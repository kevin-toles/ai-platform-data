{
  "metadata": {
    "title": "Designing Data-Intensive Applications",
    "source_file": "Designing Data-Intensive Applications_metadata.json"
  },
  "chapters": [
    {
      "chapter_number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "summary": "Data-Intensive \nAND MAINTAINABLE SYSTEMS\nDesigning Data-Intensive Applications\nData is at the center of many challenges in system design today.\nIn this practical and comprehensive guide, author Martin Kleppmann helps\nvarious technologies for processing and storing data.\nsoftware engineers and architects will learn how to apply those ideas in\npractice, and how to make full use of data in modern applications.\nm Peer under the hood of the systems you already use, and learn\nm Understand the distributed systems research upon which\nMartin Kleppmann is a researcher in distributed systems at the University of\nDesigning Data-Intensive\ndata infrastructure and\nsystems.”\nDesigning Data-Intensive\nand Maintainable Systems\nDesigning Data-Intensive Applications\nO’Reilly books may be purchased for educational, business, or sales promotional use.\nEditors: Ann Spencer and Marie Beaugureau\nSee http://oreilly.com/catalog/errata.csp?isbn=9781449373320 for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Designing Data-Intensive Applications,\nthe cover image, and related trade dress are trademarks of O’Reilly Media, Inc. While the publisher and the author have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the author disclaim all responsibility\nUse of the information and instructions contained in this work is at your own\nIf any code samples or other technology this work contains or describes is subject to open source\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\nData, software, and communication can\nbook is dedicated to everyone working toward the good.",
      "keywords": [
        "Designing Data-Intensive Applications",
        "Martin Kleppmann",
        "Designing Data-Intensive",
        "Data-Intensive Applications",
        "Martin",
        "Applications",
        "Kleppmann",
        "Data-Intensive Applications Data",
        "SYSTEMS",
        "author Martin Kleppmann",
        "Data",
        "Designing",
        "Data-Intensive",
        "architectures Martin Kleppmann",
        "O’Reilly Media"
      ],
      "concepts": [
        "data",
        "designing",
        "systems",
        "make",
        "right",
        "martin",
        "source",
        "online",
        "powerful",
        "power"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 6,
          "title": "",
          "score": 0.726,
          "base_score": 0.576,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 3,
          "title": "",
          "score": 0.718,
          "base_score": 0.568,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "",
          "score": 0.686,
          "base_score": 0.536,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 41,
          "title": "",
          "score": 0.594,
          "base_score": 0.444,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 59,
          "title": "",
          "score": 0.577,
          "base_score": 0.427,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data intensive",
          "intensive",
          "designing",
          "designing data",
          "reilly"
        ],
        "semantic": [],
        "merged": [
          "data intensive",
          "intensive",
          "designing",
          "designing data",
          "reilly"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3252330592279376,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.200961+00:00"
      }
    },
    {
      "chapter_number": 2,
      "title": "Segment 2 (pages 9-17)",
      "start_page": 9,
      "end_page": 17,
      "summary": "Foundations of Data Systems\n1. Reliable, Scalable, and Maintainable Applications.\nThinking About Data Systems                                                                                       4\n2. Data Models and Query Languages.\nRelational Versus Document Databases Today                                                     38\nQuery Languages for Data                                                                                            42\nGraph-Like Data Models                                                                                              49\nData Structures That Power Your Database                                                              70\nTransaction Processing or Analytics?\nData Warehousing                                                                                                     91\nAggregation: Data Cubes and Materialized Views                                             101\nFormats for Encoding Data                                                                                        112\nDataflow Through Databases                                                                                 129\nDistributed Data\n5. Replication.\nReading Your Own Writes                                                                                     162\nUse Cases for Multi-Leader Replication                                                               168\nWriting to the Database When a Node Is Down                                                177\n6. Partitioning.\nPartitioning and Replication                                                                                      200\nPartitioning of Key-Value Data                                                                                 201\nPartitioning Secondary Indexes by Document                                                    206\nImplementing Linearizable Systems                                                                     332\nDerived Data\nBatch Processing with Unix Tools                                                                            391\nComparing Hadoop to Distributed Databases                                                    414\nStream Processing.\nDatabases and Streams                                                                                                451\nChange Data Capture                                                                                              454\nProcessing Streams                                                                                                      464\nUses of Stream Processing                                                                                      465\nThe Future of Data Systems.\nData Integration                                                                                                           490\nCombining Specialized Tools by Deriving Data                                                 490\nBatch and Stream Processing                                                                                 494\nComposing Data Storage Technologies                                                                499\nwords relating to storage and processing of data.\nBig Data!\ntributed systems, and in the ways we build applications on top of them.\nMicrosoft, and Twitter are handling huge volumes of data and traffic, forcing\nmarket insights by keeping development cycles short and data models flexible.\n• Even if you work on a small team, you can now build systems that are distributed\nData-intensive applications are pushing the boundaries of what is possible by making\nWe call an application data-intensive if data\nis its primary challenge—the quantity of data, the complexity of data, or the speed at\nThe tools and technologies that help data-intensive applications store and process\ndata have been rapidly adapting to these changes.\nNew types of database systems\nindexes, frameworks for batch and stream processing, and related technologies are\nof technologies for processing and storing data.\nof successful data systems: technologies that form the foundation of many popular\nto find useful ways of thinking about data systems—not just how they work, but also\nAfter reading this book, you will be in a great position to decide which kind of tech‐\nbuild your own database storage engine from scratch, but fortunately that is rarely\nWho Should Read This Book?\ncessing data, and your applications use the internet (e.g., web applications, mobile\nThis book is for software engineers, software architects, and technical managers who\nture of the systems you work on—for example, if you need to choose tools for solving\nchoice over your tools, this book will help you better understand their strengths and\nYou should have some experience building web-based applications or network serv‐\nrelational databases and other data-related tools you know are a bonus, but not\n• You want to learn how to make data systems scalable, for example, to support\n• You need to make applications highly available (minimizing downtime) and\n• You are looking for ways of making systems easier to maintain in the long run,\ninternals of various databases and data processing systems, and it’s great fun to\nSometimes, when discussing scalable data systems, people make comments along the\nrelational database.” There is truth in that statement: building for scale that you don’t\ndata.\nmental to data systems, and we explore the different design decisions taken by differ‐",
      "keywords": [
        "Data",
        "Data Systems",
        "Systems",
        "Databases",
        "Processing",
        "book",
        "Table of Contents",
        "Replication",
        "Summary",
        "Data Models",
        "Applications",
        "data processing systems",
        "Stream Processing",
        "make data systems",
        "Tools"
      ],
      "concepts": [
        "data",
        "summary",
        "systems",
        "applications",
        "application",
        "making",
        "makes",
        "processing",
        "process",
        "relational"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 62,
          "title": "",
          "score": 0.903,
          "base_score": 0.753,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 3,
          "title": "",
          "score": 0.897,
          "base_score": 0.747,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 59,
          "title": "",
          "score": 0.882,
          "base_score": 0.732,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 60,
          "title": "",
          "score": 0.851,
          "base_score": 0.701,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 52,
          "title": "",
          "score": 0.828,
          "base_score": 0.678,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "systems",
          "processing",
          "data systems",
          "applications",
          "tools"
        ],
        "semantic": [],
        "merged": [
          "systems",
          "processing",
          "data systems",
          "applications",
          "tools"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4598935246567447,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.201047+00:00"
      }
    },
    {
      "chapter_number": 3,
      "title": "Segment 3 (pages 18-26)",
      "start_page": 18,
      "end_page": 26,
      "summary": "are reading a print copy of this book, you can look up references using a search\nWe look primarily at the architecture of data systems and the ways they are integrated\ninto data-intensive applications.\nthis book.\nMany of the technologies described in this book fall within the realm of the Big Data\nThis book has a bias toward free and open source software (FOSS), because reading,\nOutline of This Book\n1. In Part I, we discuss the fundamental ideas that underpin the design of data-\neral different data models and query languages, and see how they are appropriate\nIn Chapter 3 we talk about storage engines: how databases\nformats for data encoding (serialization) and evolution of schemas over time.\n2. In Part II, we move from data stored on one machine to data that is distributed\nmore detail on the problems with distributed systems (Chapter 8) and what it\nDerived data often occurs in heterogeneous systems: when there is no one data‐\nbase that can do everything well, applications need to integrate several different\ncessing approach to derived data, and we build upon it with stream processing in\nChapter 11.\nFinally, in Chapter 12 we put everything together and discuss\napproaches for building reliable, scalable, and maintainable applications in the\nThe references at the end of each chapter are a great resource if\nYou can access this page at http://bit.ly/designing-data-intensive-apps.\nDavid Beyer, Jim Brikman, Paul Carey, Raul Castro Fernandez, Joseph Chow, Derek\nFoundations of Data Systems\nThe first four chapters go through the fundamental ideas that apply to all data sys‐\nthroughout this book.\n2. Chapter 2 compares several different data models and query languages—the\n3. Chapter 3 turns to the internals of storage engines and looks at how databases lay\nout data on disk.\n4. Chapter 4 compares various formats for data encoding (serialization) and espe‐\nLater, Part II will turn to the particular issues of distributed data systems.\n(Data-Intensive\\)\nApplications ;\nCHAPTER 1\nMany applications today are data-intensive, as opposed to compute-intensive.\nA data-intensive application is typically built from standard building blocks that pro‐\nFor example, many applications need to:\n• Store data so that they, or another application, can find it again later (databases)\n• Allow users to search data by keyword or filter it in various ways (search indexes)\n• Periodically crunch a large amount of accumulated data (batch processing)\ning an application, most engineers wouldn’t dream of writing a new data storage\nThere are many database systems with different charac‐\nbuilding an application, we still need to figure out which tools and which approaches\nThis book is a journey through both the principles and the practicalities of data sys‐\ntems, and how you can use them to build data-intensive applications.\nachieve: reliable, scalable, and maintainable data systems.\nwe will need for later chapters.\non a data-intensive application.\nThinking About Data Systems\nboth store data for some time—they have very different access patterns, which means\nSo why should we lump them all together under an umbrella term like data systems?\nMany new tools for data storage and processing have emerged in recent years.\nrequirements that a single tool can no longer meet all of its data processing and stor‐\nthis may look like (we will go into detail in later chapters).\nChapter 1: Reliable, Scalable, and Maintainable Applications",
      "keywords": [
        "data",
        "book",
        "data systems",
        "applications",
        "systems",
        "chapters",
        "Big Data",
        "Press",
        "distributed data systems",
        "time",
        "Part",
        "maintainable applications",
        "maintainable data systems",
        "databases",
        "discuss"
      ],
      "concepts": [
        "different",
        "chapters",
        "databases",
        "applications",
        "application",
        "press",
        "book",
        "preface",
        "sams",
        "sam"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "",
          "score": 0.897,
          "base_score": 0.747,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 44,
          "title": "",
          "score": 0.793,
          "base_score": 0.643,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 41,
          "title": "",
          "score": 0.783,
          "base_score": 0.633,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 52,
          "title": "",
          "score": 0.744,
          "base_score": 0.594,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 62,
          "title": "",
          "score": 0.737,
          "base_score": 0.587,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "intensive",
          "book",
          "data intensive",
          "applications",
          "systems"
        ],
        "semantic": [],
        "merged": [
          "intensive",
          "book",
          "data intensive",
          "applications",
          "systems"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4197529232224402,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.201130+00:00"
      }
    },
    {
      "chapter_number": 4,
      "title": "Segment 4 (pages 27-35)",
      "start_page": 27,
      "end_page": 35,
      "summary": "application developer, but also a data system designer.\nware faults, and even human error).\nOver time, many different people will work on the system (engineering and oper‐\n• The application performs the function that the user expected.\n• It can tolerate the user making mistakes or using the software in unexpected\n• Its performance is good enough for the required use case, under the expected\nload and data volume.\nThe things that can go wrong are called faults, and systems that anticipate faults and\ncan cope with them are called fault-tolerant or resilient.\nChapter 1: Reliable, Scalable, and Maintainable Applications\nabout tolerating certain types of faults.\nNote that a fault is not the same as a failure [2].\nwhole stops providing the required service to the user.\nprobability of a fault to zero; therefore it is usually best to design fault-tolerance\nmechanisms that prevent faults from causing failures.\nCounterintuitively, in such fault-tolerant systems, it can make sense to increase the\nrate of faults by triggering them deliberately—for example, by randomly killing indi‐\nAlthough we generally prefer tolerating faults over preventing faults, there are cases\nmostly deals with the kinds of faults that can be cured, as described in the following\nHardware Faults\nWhen we think of causes of system failure, hardware faults quickly come to mind.\nHard disks are reported as having a mean time to failure (MTTF) of about 10 to 50\nfrom causing failures, but it is well understood and can often keep a machine running\nrestore a backup onto a new machine fairly quickly, the downtime in case of failure is\nHowever, as data volumes and applications’ computing demands have increased,\nally increases the rate of hardware faults.\nusing software fault-tolerance techniques in preference or in addition to hardware\nWe usually think of hardware faults as being random and independent from each\nSuch faults are\nmany more system failures than uncorrelated hardware faults [5].\n• A software bug that causes every instance of an application server to crash when\n• A runaway process that uses up some shared resource—CPU time, memory, disk\nChapter 1: Reliable, Scalable, and Maintainable Applications\n• Cascading failures, where a small fault in one component triggers a fault in\nanother component, which in turn triggers further faults [10].\nThe bugs that cause these kinds of software faults often lie dormant for a long time\nHumans design and build software systems, and the operators who keep the systems\nHow do we make our systems reliable, in spite of unreliable humans?\n• Design systems in a way that minimizes opportunities for error.\nreal data, without affecting real users.\nusers), and provide tools to recompute data (in case it turns out that the old com‐\n• Set up detailed and clear monitoring, such as performance metrics and error\nmore mundane applications are also expected to work reliably.\nEven in “noncritical” applications we have a responsibility to our users.\nload.\nChapter 1: Reliable, Scalable, and Maintainable Applications\nIn transaction processing systems, we use it to describe the number of requests to other services that we need\nwith a few numbers which we call load parameters.\nTo make this idea more concrete, let’s consider Twitter as an example, using data\nA user can publish a new message to their followers (4.6k requests/sec on aver‐\nA user can view tweets posted by the people they follow (300k requests/sec).\nbut due to fan-outii—each user follows many people, and each user is followed by\nWhen a user requests their home timeline, look up all the people they follow,\nfind all the tweets for each of those users, and merge them (sorted by time).\nSELECT tweets.*, users.* FROM tweets\n2. Maintain a cache for each user’s home timeline—like a mailbox of tweets for\nWhen a user posts a tweet, look up all the\npeople who follow that user, and insert the new tweet into each of their home\nTwitter’s data pipeline for delivering tweets to followers, with load parame‐\nwith the load of home timeline queries, so the company switched to approach 2.\nworks better because the average rate of published tweets is almost two orders of\nhides the fact that the number of followers per user varies wildly, and some users\nChapter 1: Reliable, Scalable, and Maintainable Applications\nIn the example of Twitter, the distribution of followers per user (maybe weighted by\nhow often those users tweet) is a key load parameter for discussing scalability, since it\nMost users’ tweets continue to be\nfanned out to home timelines at the time when they are posted, but a small number\nof users with a very large number of followers (i.e., celebrities) are excepted from this\nTweets from any celebrities that a user may follow are fetched separately and\nmerged with that user’s home timeline when it is read, like in approach 1.\n• When you increase a load parameter and keep the system resources (CPU, mem‐\n• When you increase a load parameter, how much do you need to increase the",
      "keywords": [
        "system",
        "data system",
        "faults",
        "data",
        "load",
        "user",
        "time",
        "Home timeline",
        "Hardware Faults",
        "Applications",
        "Twitter",
        "tweets",
        "number",
        "Maintainable Applications",
        "Home"
      ],
      "concepts": [
        "user",
        "faults",
        "tweet",
        "application",
        "applications",
        "reliability",
        "reliable",
        "reliably",
        "machines",
        "load"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 6,
          "title": "",
          "score": 0.788,
          "base_score": 0.638,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 31,
          "title": "",
          "score": 0.656,
          "base_score": 0.506,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 60,
          "title": "",
          "score": 0.615,
          "base_score": 0.465,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 53,
          "title": "",
          "score": 0.535,
          "base_score": 0.385,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 55,
          "title": "",
          "score": 0.523,
          "base_score": 0.523,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "faults",
          "tweets",
          "user",
          "home",
          "load"
        ],
        "semantic": [],
        "merged": [
          "faults",
          "tweets",
          "user",
          "home",
          "load"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3102191093864564,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.201190+00:00"
      }
    },
    {
      "chapter_number": 5,
      "title": "Segment 5 (pages 36-43)",
      "start_page": 36,
      "end_page": 43,
      "summary": "Latency and response time\nLatency and response time are often used synonymously, but they\nThe response time is what the client sees: besides\nthe actual time to process the request (the service time), it includes\nEven if you only make the same request over and over again, you’ll get a slightly dif‐\nferent response time on every try.\nrequests, the response time can vary a lot.\ntime not as a single number, but as a distribution of values that you can measure.\nIn Figure 1-4, each gray bar represents a request to a service, and its height shows\nthink all requests should take the same time, you get variation: random additional\nIllustrating mean and percentiles: response times for a sample of 100\nrequests to a service.\nIt’s common to see the average response time of a service reported.\ncal” response time, because it doesn’t tell you how many users actually experienced\nIf you take your list of response times and sort it\nmedian response time is 200 ms, that means half your requests return in less than\nThis makes the median a good metric if you want to know how long users typically\nhave to wait: half of user requests are served in less than the median response time,\nrequest; if the user makes several requests (over the course of a session, or because\nThey are the response time thresholds at which 95%, 99%, or 99.9% of requests are\nFor example, if the 95th percentile response time\nHigh percentiles of response times, also known as tail latencies, are important\ndescribes response time requirements for internal services in terms of the 99.9th per‐\nwith the slowest requests are often those who have the most data on their accounts\nfor them: Amazon has also observed that a 100 ms increase in response time reduces\nReducing response times at very high percentiles is difficult because they\nmedian response time of less than 200 ms and a 99th percentile under 1 s (if the\nresponse time is longer, it might as well be down), and the service may be required to\nbe up at least 99.9% of the time.\nQueueing delays often account for a large part of the response time at high percen‐\nexample, by its number of CPU cores), it only takes a small number of slow requests\nserver, the client will see a slow overall response time due to the time waiting for the\ntimes on the client side.\ngenerating client needs to keep sending requests independently of the response time.\nHigh percentiles become especially important in backend services that are called mul‐\ntiple times as part of serving a single end-user request.\nparallel, the end-user request still needs to wait for the slowest of the parallel calls to\nIt takes just one slow call to make the entire end-user request slow, as illus‐\nchance of getting a slow call increases if an end-user request requires multiple back‐\nend calls, and so a higher proportion of end-user requests end up being slow (an\nIf you want to add response time percentiles to the monitoring dashboards for your\nmay want to keep a rolling window of response times of requests in the last 10\nThe naïve implementation is to keep a list of response times for all requests within the\naggregating response time data is to add the histograms [28].\nWhen several backend calls are needed to serve a request, it takes just a sin‐\ngle slow backend request to slow down the entire end-user request.\ntimes that load.\nmore powerful machine) and scaling out (horizontal scaling, distributing the load\nDistributing load across multiple machines is also\nces when they detect a load increase, whereas other systems are scaled manually (a\nward, taking stateful data systems from a single node to a distributed setup can intro‐\nAs the tools and abstractions for distributed systems get better, this common wisdom\nThe architecture of systems that operate at large scale is usually highly specific to the\nresponse time requirements, the access patterns, or (usually) some mixture of all of\nFor example, a system that is designed to handle 100,000 requests per second, each\nYet, unfortunately, many people working on software systems dislike maintenance of\nMake it easy for operations teams to keep the system running smoothly.\nRather, we will try to think about systems with operability, simplicity,\n• Performing complex maintenance tasks, such as moving an application from one\n• Defining processes that make operations predictable and help keep the produc‐\nGood operability means making routine tasks easy, allowing the operations team to\n• Providing good documentation and an easy-to-understand operational model\nmaintainability of software, and thus simplicity should be a key goal for the systems\nFor example, high-level programming languages are abstractions that hide machine\nEvolvability: Making Change Easy\nously unanticipated use cases emerge, business priorities change, users request new",
      "keywords": [
        "response time",
        "time",
        "median response time",
        "response",
        "requests",
        "system",
        "request",
        "response time requirements",
        "percentile response time",
        "good",
        "response time data",
        "service",
        "percentiles",
        "load",
        "software"
      ],
      "concepts": [
        "request",
        "requests",
        "good",
        "complexity",
        "scaling",
        "scale",
        "operational",
        "operate",
        "operations",
        "times"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 32,
          "title": "",
          "score": 0.553,
          "base_score": 0.403,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 33,
          "title": "",
          "score": 0.551,
          "base_score": 0.401,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 34,
          "title": "",
          "score": 0.505,
          "base_score": 0.355,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 35,
          "title": "",
          "score": 0.467,
          "base_score": 0.317,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 49,
          "title": "",
          "score": 0.345,
          "base_score": 0.345,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "response",
          "response time",
          "time",
          "requests",
          "request"
        ],
        "semantic": [],
        "merged": [
          "response",
          "response time",
          "time",
          "requests",
          "request"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.19846377403210969,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.201234+00:00"
      }
    },
    {
      "chapter_number": 6,
      "title": "Segment 6 (pages 44-51)",
      "start_page": 44,
      "end_page": 51,
      "summary": "of increasing agility on the level of a larger data system, perhaps consisting of several\nsuch an important idea, we will use a different word to refer to agility on a data sys‐\nIn this chapter, we have explored some fundamental ways of thinking about data-\nChapter 1: Reliable, Scalable, and Maintainable Applications\nexamples of data systems and analyze how they work toward those goals.\nTime Has Come and Gone,” at 21st International Conference on Data Engineering\nfor System Fault Tolerance,” Technical Report CMU/SEI-92-TR-033, Software Engi‐\ntems,” at 11th USENIX Symposium on Operating Systems Design and Implementation\nally Distributed Storage Systems,” at 9th USENIX Symposium on Operating Systems\n[6] Brian Beach: “Hard Drive Reliability Update – Sep 2014,” backblaze.com, Septem‐\nBugs Live in the Cloud?,” at 5th ACM Symposium on Cloud Computing (SoCC),\nvice Disruption in the US East Region,” aws.amazon.com, April 29, 2011.\nInternet Services Fail, and What Can Be Done About It?,” at 4th USENIX Symposium\n[14] Nathan Marz: “Principles of Software Engineering, Part 1,” nathanmarz.com,\nwhen we replaced physical server?” twitter.com, November 13, 2014.\nzon’s Highly Available Key-Value Store,” at 21st ACM Symposium on Operating Sys‐\n[20] Greg Linden: “Make Data Useful,” slides from presentation at Stanford Univer‐\nChapter 1: Reliable, Scalable, and Maintainable Applications\n“Forward Decay: A Practical Time Decay Model for Streaming Systems,” at 25th\nIEEE International Conference on Data Engineering (ICDE), March 2009.\nt-Digests,” github.com, March 2014.\n[28] Baron Schwartz: “Why Percentiles Don’t Work the Way You Think,” vividcor‐\nEvolvability,” at 32nd Annual IEEE International Computer Software and Applica‐\nData Models and Query Languages\nData models are perhaps the most important part of developing software, because\nMost applications are built by layering one data model on top of another.\nterms of objects or data structures, and APIs that manipulate those data struc‐\n2. When you want to store those data structures, you express them in terms of a\ngeneral-purpose data model, such as JSON or XML documents, tables in a rela‐\ntional database, or a graph model.\n3. The engineers who built your database software decided on a way of representing\nthat JSON/XML/relational/graph data in terms of bytes in memory, on disk, or\nThe representation may allow the data to be queried, searched,\nlayers below it by providing a clean data model.\nThere are many different kinds of data models, and every data model embodies\nIt can take a lot of effort to master just one data model (think how many books there\nare on relational data modeling).\ning with just one data model and without worrying about its inner workings.\nsince the data model has such a profound effect on what the software above it can\nIn this chapter we will look at a range of general-purpose data models for data stor‐\nrelational model, the document model, and a few graph-based data models.\nwill discuss how storage engines work; that is, how these data models are actually\nThe best-known data model today is probably that of SQL, based on the relational\nmodel proposed by Edgar Codd in 1970 [1]: data is organized into relations (called\nThe relational model was a theoretical proposal, and many people at the time\nrelational database management systems (RDBMSes) and SQL had become the tools\nof choice for most people who needed to store and query data with some kind of reg‐\nThe roots of relational databases lie in business data processing, which was performed\nOther databases at that time forced application developers to think a lot about the\ninternal representation of the data in the database.\nThe goal of the relational model\nChapter 2: Data Models and Query Languages\nwere the main alternatives, but the relational model came to dominate them.\nthe relational model generated a lot of hype in its time, but it never lasted [2].\nNow, in the 2010s, NoSQL is the latest attempt to overthrow the relational model’s\n• Specialized query operations that are not well supported by the relational model\ndynamic and expressive data model [5]\nguages, which leads to a common criticism of the SQL data model: if data is stored in",
      "keywords": [
        "data model",
        "data",
        "relational model",
        "Model",
        "relational",
        "systems",
        "SQL data model",
        "Relational Model Versus",
        "relational databases",
        "software",
        "data system",
        "databases",
        "application",
        "document model",
        "Model Versus Document"
      ],
      "concepts": [
        "data",
        "database",
        "model",
        "systems",
        "relational",
        "relations",
        "application",
        "reliability",
        "reliable",
        "processes"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 4,
          "title": "",
          "score": 0.788,
          "base_score": 0.638,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "",
          "score": 0.757,
          "base_score": 0.607,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 31,
          "title": "",
          "score": 0.748,
          "base_score": 0.598,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 1,
          "title": "",
          "score": 0.726,
          "base_score": 0.576,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 3,
          "title": "",
          "score": 0.723,
          "base_score": 0.573,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "model",
          "relational",
          "relational model",
          "data model",
          "data models"
        ],
        "semantic": [],
        "merged": [
          "model",
          "relational",
          "relational model",
          "data model",
          "data models"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.40879041795320237,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.201315+00:00"
      }
    },
    {
      "chapter_number": 7,
      "title": "Segment 7 (pages 52-61)",
      "start_page": 52,
      "end_page": 61,
      "summary": "application code and the database model of tables, rows, and columns.\nuser, so they can be modeled as columns on the users table.\ndocument, store it on a text column in the database, and let the application inter‐\nChapter 2: Data Models and Query Languages\nFor a data structure like a résumé, which is mostly a self-contained document, a JSON\nDocument-oriented databases like MongoDB [9],\nRethinkDB [10], CouchDB [11], and Espresso [12] support this data model.\nRelational Model Versus Document Model \nChapter 2: Data Models and Query Languages\nRelational Model Versus Document Model \ndon’t fit nicely into the document model.\nIn relational databases, it’s normal to refer\nIn document databases, joins are\nnot needed for one-to-many tree structures, and support for joins is often weak.iii\nIf the database itself does not support joins, you have to emulate a join in application\ncode by making multiple queries to the database.\nfrom the database to the application code.)\nment model, data has a tendency of becoming more interconnected as features are\nChapter 2: Data Models and Query Languages\nreferences to organizations, schools, and other users need to be represented as refer‐\nRelational Model Versus Document Model \nAre Document Databases Repeating History?\nbases, document databases and NoSQL reopened the debate on how best to represent\nsuch relationships in a database.\nThe design of IMS used a fairly simple data model called the hierarchical model,\nwhich has some remarkable similarities to the JSON model used by document data‐\nLike document databases, IMS worked well for one-to-many relationships, but it\nmuch like the problems that developers are running into with document databases\nThe network model was standardized by a committee called the Conference on Data\nstructure of the hierarchical model, every record has exactly one parent; in the net‐\nwork model, a record could have multiple parents.\nChapter 2: Data Models and Query Languages\nrelational model.\nThe links between records in the network model were not foreign keys, but more like\nmodel, if you didn’t have a path to the data you wanted, you were in a difficult situa‐\nhandwritten database query code and rewrite it to handle the new access paths.\ndifficult to make changes to an application’s data model.\nThe relational model\nWhat the relational model did, by contrast, was to lay out all the data in the open: a\nIn a relational database, the query optimizer automatically decides which parts of the\nRelational Model Versus Document Model \nQuery optimizers for relational databases are complicated beasts, and they have con‐\nrelational model was this: you only need to build a query optimizer once, and then all\napplications that use the database can benefit from it.\nComparison to document databases\nDocument databases reverted back to the hierarchical model in one aspect: storing\nships, relational and document databases are not fundamentally different: in both\nkey in the relational model and a document reference in the document model [9].\ndocument databases have not followed the path of CODASYL.\nRelational Versus Document Databases Today\ndocument databases, including their fault-tolerance properties (see Chapter 5) and\nthe differences in the data model.\nThe main arguments in favor of the document data model are schema flexibility, bet‐\nWhich data model leads to simpler application code?\nIf the data in your application has a document-like structure (i.e., a tree of one-to-\nChapter 2: Data Models and Query Languages\nbly a good idea to use a document model.\nsplitting a document-like structure into multiple tables (like positions, education,\nThe document model has limitations: for example, you cannot refer directly to a nes‐\nitem in the list of positions for user 251” (much like an access path in the hierarchical\nmodel).\nThe poor support for joins in document databases may or may not be a problem,\nbe needed in an analytics application that uses a document database to record which\nHowever, if your application does use many-to-many relationships, the document\nIn such cases, using a document model can lead to significantly more complex appli‐\nIt’s not possible to say in general which data model leads to simpler application code;\ninterconnected data, the document model is awkward, the relational model is accept‐\nable, and graph models (see “Graph-Like Data Models” on page 49) are the most\nSchema flexibility in the document model\nMost document databases, and the JSON support in relational databases, do not\nenforce any schema on the data in documents.\nXML support in relational databases\ndata is read), in contrast with schema-on-write (the traditional approach of relational\nRelational Model Versus Document Model ",
      "keywords": [
        "Model Versus Document",
        "Document Model",
        "model",
        "Relational Model",
        "Relational Model Versus",
        "document databases",
        "data model",
        "data",
        "document",
        "document data model",
        "relational",
        "Model Versus",
        "database",
        "Versus Document",
        "data model leads"
      ],
      "concepts": [
        "data",
        "databases",
        "models",
        "relational",
        "related",
        "documents",
        "document",
        "schema",
        "querying",
        "queried"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 8,
          "title": "",
          "score": 0.843,
          "base_score": 0.693,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 3,
          "title": "",
          "score": 0.675,
          "base_score": 0.525,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 10,
          "title": "",
          "score": 0.653,
          "base_score": 0.503,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 44,
          "title": "",
          "score": 0.633,
          "base_score": 0.483,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "",
          "score": 0.603,
          "base_score": 0.453,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "model",
          "document",
          "relational",
          "relational model",
          "document databases"
        ],
        "semantic": [],
        "merged": [
          "model",
          "document",
          "relational",
          "relational model",
          "document databases"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3014257214366787,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.201371+00:00"
      }
    },
    {
      "chapter_number": 8,
      "title": "Segment 8 (pages 62-70)",
      "start_page": 62,
      "end_page": 70,
      "summary": "In a document database, you would just\nRunning the UPDATE statement on a large table is likely to be slow on any database,\na document database.\nChapter 2: Data Models and Query Languages\nData locality for queries\nThe database typically needs to load the entire document, even if you\ndocument databases are useful.\nnot limited to the document model.\nthe same locality properties in a relational data model, by allowing the schema to\nConvergence of document and relational databases\nMost relational database systems (other than MySQL) have supported XML since the\nThis includes functions to make local modifications to XML documents\nand the ability to index and query inside XML documents, which allows applications\nto use data models very similar to what they would do when using a document data‐\ndocuments within a relational schema.\npopularity of JSON for web APIs, it is likely that other relational databases will follow\nOn the document database side, RethinkDB supports relational-like joins in its query\nIt seems that relational and document databases are becoming more similar over\ntime, and that is a good thing: the data models complement each other.v If a database\nis able to handle document-like data and also perform relational queries on it, appli‐\nA hybrid of the relational and document models is a good route for databases to take\nQuery Languages for Data\nWhen the relational model was introduced, it included a new way of querying data:\nSQL is a declarative query language, whereas IMS and CODASYL queried the data‐\nChapter 2: Data Models and Query Languages\nSELECT * FROM animals WHERE family = 'Sharks';\nIn a declarative query language, like SQL or relational algebra, you just specify the\nIt is up to the database system’s query optimizer to decide which\nof the query.\nA declarative query language is attractive because it is typically more concise and eas‐\nqueries.\nCan the database do that safely, without breaking queries?\nBut if the query is written as imperative code, the database can\nQuery Languages for Data \nDeclarative Queries on the Web\nThe advantages of declarative query languages are not limited to just databases.\npage on sharks, so you mark the navigation item “Sharks” as currently selected, like\n<p>Sharks</p> is the title of the currently selected page.\nli.selected > p {\nHere the CSS selector li.selected > p declares the pattern of elements to which we\nChapter 2: Data Models and Query Languages\n<xsl:template match=\"li[@class='selected']/p\">\nHere, the XPath expression li[@class='selected']/p is equivalent to the CSS selec‐\nis that they are both declarative languages for specifying the styling of a document.\nvar liElements = document.getElementsByTagName(\"li\");\nThis JavaScript imperatively sets the element <p>Sharks</p> to have a blue back‐\nQuery Languages for Data \nIMS and CODASYL both used imperative query APIs. Applications typically used COBOL code to iterate\nSimilarly, in databases, declarative query languages\nlike SQL turned out to be much better than imperative query APIs.vi\nMapReduce Querying\nmechanism for performing read-only queries across many documents.\nMapReduce is neither a declarative query language nor a fully imperative query API,\nrecord to your database every time you see animals in the ocean.\nIn PostgreSQL you might express that query like this:\nThis query first filters the observations to only show species in the Sharks family,\nChapter 2: Data Models and Query Languages\nquery: { family: \"Sharks\" }, \nThe JavaScript function map is called once for every document that matches\nquery, with this set to the document object.\nwith the same key (i.e., the same month and year), the reduce function is called\nThe reduce function adds up the number of animals from all observations in a\nFor example, say the observations collection contains these two documents:\nQuery Languages for Data \nThe map function would be called once for each document, resulting in\nthem as input, they cannot perform additional database queries, and they must not\nHigher-level query languages like SQL can be implemented as a\nBeing able to use JavaScript code in the middle of a query is a great feature for\nadvanced queries, but it’s not limited to MapReduce—some SQL databases can be\nnated JavaScript functions, which is often harder than writing a single query.\nover, a declarative query language offers more opportunities for a query optimizer to\nfor a declarative query language called the aggregation pipeline [9].\nthe same shark-counting query looks like this:\nChapter 2: Data Models and Query Languages",
      "keywords": [
        "declarative query language",
        "Query Languages",
        "Query",
        "Data Models",
        "data",
        "document",
        "database",
        "sharks",
        "declarative query",
        "Languages",
        "SQL",
        "model",
        "document database",
        "documents",
        "relational data model"
      ],
      "concepts": [
        "document",
        "documents",
        "query",
        "queries",
        "queried",
        "functions",
        "function",
        "functionality",
        "databases",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 7,
          "title": "",
          "score": 0.843,
          "base_score": 0.693,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 3,
          "title": "",
          "score": 0.72,
          "base_score": 0.57,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "",
          "score": 0.673,
          "base_score": 0.523,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 12,
          "title": "",
          "score": 0.661,
          "base_score": 0.511,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 44,
          "title": "",
          "score": 0.659,
          "base_score": 0.509,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "query",
          "declarative",
          "query languages",
          "document",
          "declarative query"
        ],
        "semantic": [],
        "merged": [
          "query",
          "declarative",
          "query languages",
          "document",
          "declarative query"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.34289659519817983,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.201448+00:00"
      }
    },
    {
      "chapter_number": 9,
      "title": "Segment 9 (pages 71-80)",
      "start_page": 71,
      "end_page": 80,
      "summary": "Graph-Like Data Models\nmodeling your data as a graph.\nVertices are people, and edges indicate which people know each other.\nVertices are web pages, and edges indicate HTML links to other pages.\nIn the examples just given, all the vertices in a graph represent the same kind of thing\nexample, Facebook maintains a single graph with many different types of vertices and\nedges: vertices represent people, locations, events, checkins, and comments made by\nGraph-Like Data Models \nExample of graph-structured data (boxes represent vertices, arrows repre‐\nIn the property graph model, each vertex consists of:\n• The vertex at which the edge starts (the tail vertex)\nChapter 2: Data Models and Query Languages\n• The vertex at which the edge ends (the head vertex)\ndatatype to store the properties of each vertex or edge).\nstored for each edge; if you want the set of incoming or outgoing edges for a vertex,\nyou can query the edges table by head_vertex or tail_vertex, respectively.\nCREATE INDEX edges_tails ON edges (tail_vertex);\nCREATE INDEX edges_heads ON edges (head_vertex);\n1. Any vertex can have an edge connecting it with any other vertex.\nGraph-Like Data Models \nCypher is a declarative query language for property graphs, created for the Neo4j\nthe query can use those names to create edges between the vertices, using an arrow\nnotation: (Idaho) -[:WITHIN]-> (USA) creates an edge labeled WITHIN, with Idaho\nA subset of the data in Figure 2-5, represented as a Cypher query\n(USA:Location      {name:'United States', type:'country'  }),\nvertices that have a BORN_IN edge to a location within the US, and also a LIVING_IN\nedge to a location within Europe, and return the name property of each of those verti‐\nExample 2-4 shows how to express that query in Cypher.\nChapter 2: Data Models and Query Languages\nmatches any two vertices that are related by an edge labeled BORN_IN.\nof that edge is bound to the variable person, and the head vertex is left unnamed.\n1. person has an outgoing BORN_IN edge to some vertex.\nfollow a chain of outgoing WITHIN edges until eventually you reach a vertex of\n2. That same person vertex also has an outgoing LIVES_IN edge.\nincoming BORN_IN or LIVES_IN edge at one of the location vertices.\nExample 2-2 suggested that graph data can be represented in a relational database.\nBut if we put graph data in a relational structure, can we also query it using SQL?\nGraph-Like Data Models \nA person’s LIVES_IN edge may point at any kind of location: a street, a city, a\nThe same query as Example 2-4, expressed in SQL using recursive\n-- in_usa is the set of vertex IDs of all locations within the United States\nin_usa(vertex_id) AS (\nSELECT vertex_id FROM vertices WHERE properties->>'name' = 'United States' \nSELECT edges.tail_vertex FROM edges \nJOIN in_usa ON edges.head_vertex = in_usa.vertex_id\n-- in_europe is the set of vertex IDs of all locations within Europe\nSELECT vertex_id FROM vertices WHERE properties->>'name' = 'Europe' \nSELECT edges.tail_vertex FROM edges\nJOIN in_europe ON edges.head_vertex = in_europe.vertex_id\n-- born_in_usa is the set of vertex IDs of all people born in the US\nborn_in_usa(vertex_id) AS ( \nSELECT edges.tail_vertex FROM edges\nJOIN in_usa ON edges.head_vertex = in_usa.vertex_id\nChapter 2: Data Models and Query Languages\n-- lives_in_europe is the set of vertex IDs of all people living in Europe\nlives_in_europe(vertex_id) AS ( \nSELECT edges.tail_vertex FROM edges\nJOIN in_europe ON edges.head_vertex = in_europe.vertex_id\nJOIN born_in_usa     ON vertices.vertex_id = born_in_usa.vertex_id \nJOIN lives_in_europe ON vertices.vertex_id = lives_in_europe.vertex_id;\nFirst find the vertex whose name property has the value \"United States\", and\nFollow all incoming within edges from vertices in the set in_usa, and add them\nFor each of the vertices in the set in_usa, follow incoming born_in edges to find\nedges to find people who live in Europe.\nThe triple-store model is mostly equivalent to the property graph model, using differ‐\nGraph-Like Data Models \nThe subject of a triple is equivalent to a vertex in a graph.\nFor example, (lucy, age, 33) is like a vertex lucy with prop‐\n2. Another vertex in the graph.\nIn that case, the predicate is an edge in the graph,\nExample 2-6 shows the same data as in Example 2-3, written as triples in a format\nIn this example, vertices of the graph are written as _:someName.\nobject is a vertex, as in _:idaho :within _:usa.\nChapter 2: Data Models and Query Languages\n_:idaho    a :Location; :name \"Idaho\";         :type \"state\";   :within _:usa.\n_:usa      a :Location; :name \"United States\"; :type \"country\"; :within _:namerica.\nThe RDF data model\nGraph-Like Data Models \nThe data of Example 2-7, expressed using RDF/XML syntax\nChapter 2: Data Models and Query Languages",
      "keywords": [
        "Data Models",
        "vertex",
        "Data",
        "United States",
        "location",
        "Graph-Like Data Models",
        "edges",
        "query",
        "graph",
        "Europe",
        "vertices",
        "USA",
        "Idaho",
        "property graph model",
        "RDF"
      ],
      "concepts": [
        "examples",
        "graph",
        "edges",
        "data",
        "vertices",
        "locations",
        "location",
        "uses",
        "lucy",
        "models"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 10,
          "title": "",
          "score": 0.746,
          "base_score": 0.596,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 45,
          "title": "",
          "score": 0.742,
          "base_score": 0.592,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 7,
          "title": "",
          "score": 0.599,
          "base_score": 0.449,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 3,
          "title": "",
          "score": 0.556,
          "base_score": 0.406,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 41,
          "title": "",
          "score": 0.542,
          "base_score": 0.392,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "edges",
          "vertex",
          "vertices",
          "vertex_id",
          "edge"
        ],
        "semantic": [],
        "merged": [
          "edges",
          "vertex",
          "vertices",
          "vertex_id",
          "edge"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2692118098699738,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.201500+00:00"
      }
    },
    {
      "chapter_number": 10,
      "title": "Segment 10 (pages 81-88)",
      "start_page": 81,
      "end_page": 88,
      "summary": "The SPARQL query language\nSPARQL is a query language for triple-stores using the RDF data model [43].\nacronym for SPARQL Protocol and RDF Query Language, pronounced “sparkle.”) It\nThe same query as Example 2-4, expressed in SPARQL\nGraph-Like Data Models \nGraph Databases Compared to the Network Model\nIn “Are Document Databases Repeating History?” on page 36 we discussed how\nAre graph databases the second coming of CODASYL in\n• In CODASYL, a database had a schema that specified which record type could be\nIn a graph database, there is no such\nIn a graph database, you can refer directly to any vertex by\nIn a graph database, you can write your traversal in\ndeclarative query languages such as Cypher or SPARQL.\nDatalog is a much older language than SPARQL or Cypher, having been studied\nIn practice, Datalog is used in a few data systems: for example, it is the query lan‐\nChapter 2: Data Models and Query Languages\nDatalog’s data model is similar to the triple-store model, generalized a bit.\nExample 2-10 shows how to write the data from our example in Datalog.\nA subset of the data in Figure 2-5, represented as Datalog facts\nNow that we have defined the data, we can write the same query as before, as shown\nThe same query as Example 2-4, expressed in Datalog\nWe define rules that tell the database about new predicates: here, we define\nstored in the database, but instead they are derived from data or from other rules.\nGraph-Like Data Models \n1. name(namerica, 'North America') exists in the database, so rule 1 applies.\n2. within(usa, namerica) exists in the database and the previous step generated\nwithin_recursive(namerica, 'North America'), so rule 2 applies.\n3. within(idaho, usa) exists in the database and the previous step generated\nwithin_recursive(usa, 'North America'), so rule 2 applies.\ndatabase.\nDetermining that Idaho is in North America, using the Datalog rules from\nlocation LivingIn. By querying with BornIn = 'United States' and LivingIn =\nin the earlier Cypher and SPARQL queries.\nChapter 2: Data Models and Query Languages\nqueries, but it can cope better if your data is complex.\nData models are a huge subject, and in this chapter we have taken a quick look at a\n1. Document databases target use cases where data comes in self-contained docu‐\n2. Graph databases go in the opposite direction, targeting use cases where anything\n—for example, graph data can be represented in a relational database—but the result\nEach data model comes with its own query language or framework, and we discussed\n• Full-text search is arguably a kind of data model that is frequently used alongside\ndatabases.\ntrade-offs that come into play when implementing the data models described in this\nCodd: “A Relational Model of Data for Large Shared Data Banks,” Com‐\nAround,” in Readings in Database Systems, 4th edition, MIT Press, pages 2–41, 2005.\n[5] James Phillips: “Surprises in Our NoSQL Adoption Survey,” blog.couchbase.com,\n[7] “XML Data in SQL Server,” SQL Server 2012 documentation, technet.micro‐\n[8] “PostgreSQL 9.3.1 Documentation,” The PostgreSQL Global Development\nChapter 2: Data Models and Query Languages\n[10] “RethinkDB 1.11 Documentation,” rethinkdb.com, 2013.\nLinkedIn’s Distributed Data Serving Platform,” at ACM International Conference on\n[15] Sarah Mei: “Why You Should Never Use MongoDB,” sarahmei.com, November\nBell: “The CODASYL Model,” in Databases—Role\nM. D.\nof a Database System,” Foundations and Trends in Databases, volume 1, number 2,\n[20] Martin Fowler: “Schemaless Data Structures,” martinfowler.com, January 7,\nSchema-on-Write,” at Berkeley EECS\n[24] “Percona Toolkit Documentation: pt-online-schema-change,” Percona Ireland\nGlobally-Distributed Database,” at 10th USENIX Symposium on Operating System\nage System for Structured Data,” at 7th USENIX Symposium on Operating System\nrency in Software,” Dr. Dobb’s Journal, volume 30, number 3, pages 202-210, March\nLarge Clusters,” at 6th USENIX Symposium on Operating System Design and Imple‐\ntributed Data Store for the Social Graph,” at USENIX Annual Technical Conference\n[36] “Apache TinkerPop3.2.3 Documentation,” tinkerpop.apache.org, October 2016.\n[39] David Beckett and Tim Berners-Lee: “Turtle – Terse RDF Triple Language,”\n[41] W3C RDF Working Group: “Resource Description Framework (RDF),” w3.org,\nChapter 2: Data Models and Query Languages",
      "keywords": [
        "North America",
        "data",
        "Data Models",
        "SPARQL",
        "query languages",
        "database",
        "SPARQL query language",
        "query",
        "model",
        "United States",
        "Datalog",
        "RDF data model",
        "Graph Databases",
        "Query Languages Datalog",
        "Cypher"
      ],
      "concepts": [
        "databases",
        "models",
        "document",
        "documentation",
        "queries",
        "june",
        "examples",
        "graph",
        "languages",
        "difference"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 9,
          "title": "",
          "score": 0.746,
          "base_score": 0.596,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 45,
          "title": "",
          "score": 0.685,
          "base_score": 0.535,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 3,
          "title": "",
          "score": 0.662,
          "base_score": 0.512,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 7,
          "title": "",
          "score": 0.653,
          "base_score": 0.503,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 62,
          "title": "",
          "score": 0.635,
          "base_score": 0.485,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "sparql",
          "datalog",
          "query",
          "graph",
          "rdf"
        ],
        "semantic": [],
        "merged": [
          "sparql",
          "datalog",
          "query",
          "graph",
          "rdf"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.31106096068820555,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.201555+00:00"
      }
    },
    {
      "chapter_number": 11,
      "title": "Segment 11 (pages 89-108)",
      "start_page": 89,
      "end_page": 108,
      "summary": "storage engines: log-structured storage engines, and page-oriented storage engines\nThese two functions implement a key-value store.\nYou can call db_set key value,\nwhich will store key and value in the database.\nThe key and value can be (almost)\ncall db_get key, which looks up the most recent value associated with that particular\nkey-value pair, separated by a comma (roughly like a CSV file, ignoring escaping\nEvery call to db_set appends to the end of the file, so if you update a key sev‐\nlast occurrence of a key in a file to find the latest value (hence the tail -n 1 in\ndb_set does, many databases internally use a log, which is an append-only data file.\nEvery time you want to look up a key, db_get\nthe key.\nIn order to efficiently find the value for a particular key in the database, we need a\ndifferent data structure: an index.\nneed several different indexes on different parts of the data.\nAn index is an additional structure that is derived from the primary data.\nkind of index usually slows down writes, because the index also needs to be updated\nqueries, but every index slows down writes.\nLet’s start with indexes for key-value data.\nKey-value stores are quite similar to the dictionary type that you can find in most\nmemory data structures, why not use them to index our data on disk?\nLet’s say our data storage consists only of appending to a file, as in the preceding\nhash map where every key is mapped to a byte offset in the data file—the location at\nnew key-value pair to the file, you also update the hash map to reflect the offset of the\ndata you just wrote (this works both for inserting new keys and for updating existing\nkeys).\ndata file, seek to that location, and read the value.\nStoring a log of key-value pairs in a CSV-like format, indexed with an in-\nreads and writes, subject to the requirement that all the keys fit in the available RAM,\nA storage engine like Bitcask is well suited to situations where the value for each key\nthere are not too many distinct keys—you have a large number of writes per key, but\nquent writes to a new segment file.\nkeys in the log, and keeping only the most recent update for each key.\nCompaction of a key-value update log (counting the number of times each\ncat video was played), retaining only the most recent value for each key.\nkey is overwritten several times on average within one segment), we can also merge\nmerged segment is written to a new file.\ntinue to serve read and write requests as normal, using the old segment files.\nEach segment now has its own in-memory hash table, mapping keys to file offsets.\norder to find the value for a key, we first check the most recent segment’s hash map;\nIf you want to delete a key and its associated value, you have to append a special\nous values for the deleted key.\ncan restore each segment’s hash map by reading the entire segment file from\nbeginning to end and noting the offset of the most recent value for every key as\nData file segments are\n• Appending and segment merging are sequential write operations, which are gen‐\nTrees and LSM-Trees” on page 83.\n• Merging old segments avoids the problem of data files getting fragmented over\n• The hash table must fit in memory, so if you have a very large number of keys,\nIn Figure 3-3, each log-structured storage segment is a sequence of key-value pairs.\nprecedence over values for the same key earlier in the log.\nof key-value pairs in the file does not matter.\nthe sequence of key-value pairs is sorted by key.\neach key only appears once within each merged segment file (the compaction process\nat the first key in each file, copy the lowest key (according to the sort order) to\nThis produces a new merged segment file, also sorted\nby key.\nMerging several SSTable segments, retaining only the most recent value\nfor each key.\ni. If all keys and values had a fixed size, you could use binary search on a segment file and avoid the in-\nWhat if the same key appears in several input segments?\nsegment contains all the values written to the database during some period of\nWhen multiple segments contain the same key, we can keep the value\n2. In order to find a particular key in the file, you no longer need to keep an index\nof all the keys in memory.\nfind handiwork (or not, if the key is not present in the file).\nYou still need an in-memory index to tell you the offsets for some of the keys, but\nit can be sparse: one key for every few kilobytes of segment file is sufficient,\n3. Since read requests need to scan over several key-value pairs in the requested\nFine so far—but how do you get your data to be sorted by key in the first place?\nMaintaining a sorted structure on disk is possible (see “B-Trees” on page 79), but\nstructures, you can insert keys in any order and read them back in sorted order.\n• When a write comes in, add it to an in-memory balanced tree data structure (for\ntree already maintains the key-value pairs sorted by key.\n• In order to serve a read request, first try to find the key in the memtable, then in\ncombine segment files and to discard overwritten or deleted values.\n[7], key-value storage engine libraries that are designed to be embedded into other\nname Log-Structured Merge-Tree (or LSM-Tree) [10], building on earlier work on\nmerging and compacting sorted files are often called LSM storage engines.\ncomplex than a key-value index but is based on a similar idea: given a word in a\nThis is implemented with a key-value structure where the key is a\nFor example, the LSM-tree algorithm can be slow when looking up keys that do not\nkeys.)\nIn leveled compaction, the key range is split up into smaller SSTables and\nSince data is stored in sorted order, you can efficiently perform range queries (scan‐\nLike SSTables, B-trees keep key-value pairs sorted by key, which allows efficient key-\nThe log-structured indexes we saw earlier break the database down into variable-size\nBy contrast, B-trees break the database down into fixed-size blocks or\nthese page references to construct a tree of pages, as illustrated in Figure 3-6.\nLooking up a key using a B-tree index.\nOne page is designated as the root of the B-tree; whenever you want to look up a key\nThe page contains several keys and references to child\nIn the example in Figure 3-6, we are looking for the key 251, so we know that we need\nInserting a new key into a B-tree is reasonably intuitive, but deleting one (while keeping the tree balanced)\nEventually we get down to a page containing individual keys (a leaf page), which\neither contains the value for each key inline or contains references to the pages where\nThe number of references to child pages in one page of the B-tree is called the\nIf you want to update the value for an existing key in a B-tree, you search for the leaf\npage containing that key, change the value in that page, and write the page back to\nneed to find the page whose range encompasses the new key and add it to that page.\nIf there isn’t enough free space in the page to accommodate the new key, it is split\nGrowing a B-tree by splitting a page.\nThis algorithm ensures that the tree remains balanced: a B-tree with n keys always\nMost databases can fit into a B-tree that is three or four levels\n(A four-level tree of 4 KB pages with a branching factor of 500 can store up to\nThe basic underlying write operation of a B-tree is to overwrite a page on disk with\nstark contrast to log-structured indexes such as LSM-trees, which only append to files\nIn order to make the database resilient to crashes, it is common for B-tree implemen‐\ntations to include an additional data structure on disk: a write-ahead log (WAL, also\nThis is an append-only file to which every B-tree modification\nmust be written before it can be applied to the pages of the tree itself.\nbase comes back up after a crash, this log is used to restore the B-tree back to a con‐\ncontrol is required if multiple threads are going to access the B-tree at the same time\nwritten to a different location, and a new version of the parent pages in the tree is\n• We can save space in pages by not storing the entire key, but abbreviating it.\nEspecially in pages on the interior of the tree, keys only need to provide enough\npage allows the tree to have a higher branching factor, and thus fewer levels.iii\npages with nearby key ranges to be nearby on disk.\nlarge part of the key range in sorted order, that page-by-page layout can be ineffi‐\ncient, because a disk seek may be required for every page that is read.\nBy contrast, since LSM-trees rewrite large segments of the storage in one\nkeys in order without jumping back to parent pages.\n• B-tree variants such as fractal trees [22] borrow some log-structured ideas to\nAs a rule of thumb, LSM-trees are typically faster for writes, whereas B-trees\nA B-tree index must write every piece of data at least twice: once to the write-ahead\nlog, and once to the tree page itself (and perhaps again as pages are split).\nSome storage engines even overwrite the same page twice in order\nLog-structured indexes also rewrite data multiple times due to repeated compaction\nthe database can write to disk.\nmance cost: the more that a storage engine writes to disk, the fewer writes per second\nMoreover, LSM-trees are typically able to sustain higher write throughput than B-\nsequentially write compact SSTable files rather than having to overwrite several pages\nLSM-trees can be compressed better, and thus often produce smaller files on disk\nB-tree storage engines leave some disk space unused due to fragmenta‐\nSince LSM-trees are not page-oriented and periodically\ning data more compactly allows more read and write requests within the available I/O\nA downside of log-structured storage is that the compaction process can sometimes\n“Describing Performance” on page 13) the response time of queries to log-structured\nstorage engines can sometimes be quite high, and B-trees can be more predictable\nwriting to an empty database, the full disk bandwidth can be used for the initial write,\nbut the bigger the database gets, the more disk bandwidth is required for compaction.\nspace, and reads also slow down because they need to check more segment files.\nAn advantage of B-trees is that each key exists in exactly one place in the index,\nwhereas a log-structured storage engine may have multiple copies of the same key in\nThis aspect makes B-trees attractive in databases that want to\ntion is implemented using locks on ranges of keys, and in a B-tree index, those locks\nIn new datastores, log-structured indexes are becoming increasingly popular.\nSo far we have only discussed key-value indexes, which are like a primary key index in\nrecords in the database can refer to that row/document/vertex by its primary key (or\nA secondary index can easily be constructed from a key-value index.\nwith the same key.\nand log-structured indexes can be used as secondary indexes.\nStoring values within the index\nThe key in an index is the thing that queries search for, but the value can be one of\nWhen updating a value without changing the key, the heap file approach can be quite\nInnoDB storage engine, the primary key of a table is always a clustered index, and\nsecondary indexes refer to the primary key (rather than a heap file location) [31].",
      "keywords": [
        "key",
        "Data",
        "Storage",
        "Data Structures",
        "Database",
        "storage engine",
        "file",
        "index",
        "Steve Harris",
        "Andy Seaborne",
        "Eric Prud’hommeaux",
        "keys",
        "disk",
        "B-tree",
        "segment"
      ],
      "concepts": [
        "key",
        "keys",
        "databases",
        "index",
        "indexes",
        "writes",
        "writing",
        "pages",
        "trees",
        "segments"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 12,
          "title": "",
          "score": 0.711,
          "base_score": 0.561,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 51,
          "title": "",
          "score": 0.652,
          "base_score": 0.502,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 13,
          "title": "",
          "score": 0.631,
          "base_score": 0.481,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 25,
          "title": "",
          "score": 0.627,
          "base_score": 0.477,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 8,
          "title": "",
          "score": 0.619,
          "base_score": 0.469,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "key",
          "tree",
          "value",
          "file",
          "key value"
        ],
        "semantic": [],
        "merged": [
          "key",
          "tree",
          "value",
          "file",
          "key value"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.32703932425965093,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.201614+00:00"
      }
    },
    {
      "chapter_number": 12,
      "title": "Segment 12 (pages 109-118)",
      "start_page": 109,
      "end_page": 118,
      "summary": "As with any kind of duplication of data, clustered and covering indexes can speed up\nwe need to query multiple columns of a table (or multiple fields in a document)\nMulti-dimensional indexes are a more general way of querying several columns at\nData Structures That Power Your Database \nAll the indexes discussed so far assume that you have exact data and allow you to\nmemory index that tells queries at which offset in the sorted file they need to look for\nIn LevelDB, this in-memory index is a sparse collection of some of the keys,\nboth magnetic disks and SSDs, data on disk needs to be laid out carefully if you want\nmemory databases.\nmemory databases aim for durability, which can be achieved with special hardware\nWhen an in-memory database is restarted, it needs to reload its state, either from disk\nto disk, it’s still an in-memory database, because the disk is merely used as an\nProducts such as VoltDB, MemSQL, and Oracle TimesTen are in-memory databases\nimprovements by removing all the overheads associated with managing on-disk data\ndurability (using a log-structured approach for the data in memory as well as the data\nCounterintuitively, the performance advantage of in-memory databases is not due to\nthe fact that they don’t need to read from disk.\nnever need to read from disk if you have enough memory, because the operating sys‐\nbecause they can avoid the overheads of encoding in-memory data structures in a\nBesides performance, another interesting area for in-memory databases is providing\ndata models that are difficult to implement with disk-based indexes.\nRedis offers a database-like interface to various data structures such as priority\nBecause it keeps all data in memory, its implementation is compara‐\nby evicting the least recently used data from memory to disk when there is not\nData Structures That Power Your Database \napproach still requires indexes to fit entirely in memory, though (like the Bitcask\nIn the early days of business data processing, a write to the database typically corre‐\ntypically looks up a small number of records by some key, using an index.\nHowever, databases also started being increasingly used for data analytics, which has\nUsually an analytic query needs to scan over a huge\nFor example, if your data is a table of sales transactions, then analytic queries\nto differentiate this pattern of using databases from transaction processing, it has\nbeen called online analytic processing (OLAP) [47].iv The difference between OLTP\nTransaction processing systems (OLTP)\nSmall number of records per query, fetched by key\nWhat data represents\nAt first, the same databases were used for both transaction processing and analytic\nbase was called a data warehouse.\nally reluctant to let business analysts run ad hoc analytic queries on an OLTP data‐\nA data warehouse, by contrast, is a separate database that analysts can query to their\nThe data warehouse con‐\ntains a read-only copy of the data in all the various OLTP systems in the company.\nData is extracted from OLTP databases (using either a periodic data dump or a con‐\nup, and then loaded into the data warehouse.\nThis process of getting data into the\nData warehouses now exist in almost all large enterprises, but in small companies\namount of data—small enough that it can be queried in a conventional SQL database,\nA big advantage of using a separate data warehouse, rather than querying OLTP sys‐\ntems directly for analytics, is that the data warehouse can be optimized for analytic\nthis chapter work well for OLTP, but are not very good at answering analytic queries.\nThe divergence between OLTP databases and data warehouses\nThe data model of a data warehouse is most commonly relational, because SQL is\nthat generate SQL queries, visualize the results, and allow analysts to explore the data\nOn the surface, a data warehouse and a relational OLTP database look similar,\nMany database vendors now focus on supporting either transaction processing or\ntransaction processing and data warehousing in the same product.\ncial data warehouse systems.\nAs explored in Chapter 2, a wide range of different data models are used in the realm\nhand, in analytics, there is much less diversity of data models.\nMany data warehouses\nThe example schema in Figure 3-9 shows a data warehouse that might be found at a\nAt the center of the schema is a so-called fact table (in this example,\nEach row of the fact table represents an event that occurred\nExample of a star schema for use in a data warehouse.\ntransaction history in its data warehouse, most of which is in fact tables [56].\nSome of the columns in the fact table are attributes, such as the price at which the\nOther columns in the fact table are foreign key references to\nAs each row in the fact table represents an event,\nrow in the dim_product table represents one type of product that is for sale, including\nEach row in the fact_sales table uses a foreign key to indicate which prod‐\neral different products at once, they are represented as separate rows in the fact\nIn a typical data warehouse, tables are often very wide: fact tables often have over 100\nIf you have trillions of rows and petabytes of data in your fact tables, storing and\nAlthough fact tables are often over 100 columns wide, a typical data warehouse query\nonly accesses 4 or 5 of them at one time (\"SELECT *\" queries are rarely needed for\nTake the query in Example 3-1: it accesses a large number of rows\nbut it only needs to access three columns of the fact_sales table: date_key,\nIn most OLTP databases, storage is laid out in a row-oriented fashion: all the values\nIn order to process a query like Example 3-1, you may have indexes on\nfact_sales.date_key and/or fact_sales.product_sk that tell the storage engine\numn is stored in a separate file, a query only needs to read and parse those columns\nColumn storage is easiest to understand in a relational data model,\n[57] is a columnar storage format that supports a document data",
      "keywords": [
        "data",
        "data warehouse",
        "Transaction Processing",
        "OLTP",
        "fact table",
        "fact",
        "storage",
        "OLTP databases",
        "Databases",
        "Data Structures",
        "memory",
        "Processing",
        "Transaction",
        "warehouse",
        "index"
      ],
      "concepts": [
        "data",
        "query",
        "queries",
        "queried",
        "databases",
        "indexes",
        "index",
        "memory",
        "transaction",
        "transactions"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 13,
          "title": "",
          "score": 0.745,
          "base_score": 0.595,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 11,
          "title": "",
          "score": 0.711,
          "base_score": 0.561,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 51,
          "title": "",
          "score": 0.693,
          "base_score": 0.543,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 24,
          "title": "",
          "score": 0.677,
          "base_score": 0.527,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 8,
          "title": "",
          "score": 0.661,
          "base_score": 0.511,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "warehouse",
          "data warehouse",
          "oltp",
          "memory",
          "fact"
        ],
        "semantic": [],
        "merged": [
          "warehouse",
          "data warehouse",
          "oltp",
          "memory",
          "fact"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.33157699613046626,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.201670+00:00"
      }
    },
    {
      "chapter_number": 13,
      "title": "Segment 13 (pages 119-126)",
      "start_page": 119,
      "end_page": 126,
      "summary": "Storing relational data by column, rather than by row.\nThe column-oriented storage layout relies on each column file containing the rows in\nColumn Compression\nBesides only loading those columns from disk that are required for a query, we can\ncolumn-oriented storage often lends itself very well to compression.\ncolumn, different compression techniques can be used.\nlarly effective in data warehouses is bitmap encoding, illustrated in Figure 3-11.\nColumn-Oriented Storage \nCompressed, bitmap-indexed storage of a single column.\ntinct values), those bitmaps can be stored with one bit per row.\nThis works because the columns contain the rows in the same order,\nso the kth bit in one column’s bitmap corresponds to the same row as the kth bit\nin another column’s bitmap.\nColumn-oriented storage and column families\ncall them column-oriented: within each column family, they store\nall columns from a row together, along with a row key, and they do\nnot use column compression.\nFor data warehouse queries that need to scan over millions of rows, a big bottleneck\nBesides reducing the volume of data that needs to be loaded from disk, column-\nexample, the query engine can take a chunk of compressed column data that fits\numn compression allows more rows from a column to fit in the same amount of L1\ndesigned to operate on such chunks of compressed column data directly.\nSort Order in Column Storage\nIn a column store, it doesn’t necessarily matter in which order the rows are stored.\na new row just means appending to each of the column files.\nColumn-Oriented Storage \nNote that it wouldn’t make sense to sort each column independently, because then\nwe would no longer know which items in the columns belong to the same row.\ncan only reconstruct a row because we know that the kth item in one column belongs\nto the same row as the kth item in another column.\nRather, the data needs to be sorted an entire row at a time, even though it is stored by\ncolumn.\nThe administrator of the database can choose the columns by which the\nThen the query optimizer can scan only the rows from the\nA second column can determine the sort order of any rows that have the same value\nin the first column.\nAnother advantage of sorted order is that it can help with compression of columns.\nthe primary sort column does not have many distinct values, then after sorting, it will\nthat column down to a few kilobytes—even if the table has billions of rows.\nBut having the first few columns sorted is still\norders, so why not store the same data sorted in several different ways?\nYou might as well store that redundant data sorted in different ways so that\nHaving multiple sort orders in a column-oriented store is a bit similar to having mul‐\nIn a column store,\nthere normally aren’t any pointers to data elsewhere, only columns containing values.\nWriting to Column-Oriented Storage\nColumn-oriented storage, compres‐\nrow-oriented or column-oriented.\nQueries need to examine both the column data on disk and the recent writes in mem‐\nNot every data warehouse is necessarily a column store: traditional row-oriented\nAs discussed earlier, data warehouse queries often involve an aggregate\nby many different queries, it can be wasteful to crunch through the raw data every\nColumn-Oriented Storage \nThen you can apply the same aggregate along each row or column and\nThe advantage of a materialized data cube is that certain queries become very fast\nThe disadvantage is that a data cube doesn’t have the same flexibility as querying the\naggregates such as data cubes only as a performance boost for certain queries.\nWhat happens when you store data in a database, and what does the data‐\nbase do when you query for the data again later?\nsome kind of key, and the storage engine uses an index to find the data for the\nDisk bandwidth (not seek time) is often the bottleneck here, and column-\nindexing structures, and databases that are optimized for keeping all data in memory.\nWe discussed how column-oriented storage\nKey/Value Data,” Basho Technologies, April 2010.",
      "keywords": [
        "data",
        "column",
        "storage",
        "column-oriented storage",
        "row",
        "column data",
        "data warehouses",
        "rows",
        "queries",
        "compressed column data",
        "store",
        "data warehouse queries",
        "storage engines",
        "query",
        "product"
      ],
      "concepts": [
        "data",
        "column",
        "query",
        "queries",
        "storage",
        "compression",
        "compressing",
        "storing",
        "store",
        "sort"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 12,
          "title": "",
          "score": 0.745,
          "base_score": 0.595,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 3,
          "title": "",
          "score": 0.638,
          "base_score": 0.488,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 44,
          "title": "",
          "score": 0.633,
          "base_score": 0.483,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 11,
          "title": "",
          "score": 0.631,
          "base_score": 0.481,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "",
          "score": 0.629,
          "base_score": 0.479,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "column",
          "column oriented",
          "oriented",
          "row",
          "oriented storage"
        ],
        "semantic": [],
        "merged": [
          "column",
          "column oriented",
          "oriented",
          "row",
          "oriented storage"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3214064406021143,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.201735+00:00"
      }
    },
    {
      "chapter_number": 14,
      "title": "Segment 14 (pages 127-134)",
      "start_page": 127,
      "end_page": 134,
      "summary": "age System for Structured Data,” at 7th USENIX Symposium on Operating System\nStructured Merge-Tree (LSM-Tree),” Acta Informatica, volume 33, number 4, pages\nof a Log-Structured File System,” ACM Transactions on Computer Systems, volume\nErrors,” Communications of the ACM, volume 13, number 7, pages 422–426, July\n[18] Douglas Comer: “The Ubiquitous B-Tree,” ACM Computing Surveys, volume 11,\nIndex Management Method Using Write-Ahead Logging,” at ACM International\nData \n(LSM) Trees,” tokutek.com, April 22, 2014.\nAccess Methods: The RUM Conjecture,” at 19th International Conference on Extend‐\n[24] Peter Zaitsev: “Innodb Double Write,” percona.com, August 4, 2006.\n[25] Tomas Vondra: “On the Impact of Full-Page Writes,” blog.2ndquadrant.com,\n[26] Mark Callaghan: “The Advantages of an LSM vs a B-Tree,” smalldatum.blog‐\nRocksDB,” at Code Mesh, November 4, 2016.\nLevelDB,” github.com, August 2011.\na Database System Kernel,” at 26th International Conference on Very Large Data\nSearchable Key-Value Store,” at ACM SIGCOMM Conference, August 2012.\n[37] Michael McCandless: “Lucene’s FuzzyQuery Is 100 Times Faster in 4.0,”\nData Structure for String Keys,” ACM Transactions on Information Systems, volume\nAutomata,” International Journal on Document Analysis and Recognition, volume 5,\nArchitectural Era (It’s Time for a Complete Rewrite),” at 33rd International Confer‐\nMemory for DRAM-Based Storage,” at 12th USENIX Conference on File and Storage\nInternational Conference on Management of Data (SIGMOD), June 2008.\n[45] Justin DeBrabant, Andrew Pavlo, Stephen Tu, et al.: “Anti-Caching: A New\nApproach to Database Management System Architecture,” Proceedings of the VLDB\nage & Recovery Methods for Non-Volatile Memory Database Systems,” at ACM\nInternational Conference on Management of Data (SIGMOD), June 2015.\nAn IT Mandate,” E.\nand OLAP Technology,” ACM SIGMOD Record, volume 26, number 1, pages 65–74,\nServer Column Stores,” at ACM International Conference on Management of Data\n– An Architecture Overview,” IEEE Data Engineering Bulletin, volume 35, number 1,\nAbadi: “Classifying the SQL-on-Hadoop Solutions,” hadapt.com, Octo‐\nOpen-Source SQL Engine for Hadoop,” at 7th Biennial Conference on Innovative\nData Systems Research (CIDR), January 2015.\nAnalysis of Web-Scale Datasets,” at 36th International Conference on Very Large Data\nWarehouses You’ve Ever Seen,” gigaom.com, March 27, 2013.\nAbadi, Peter Boncz, Stavros Harizopoulos, et al.: “The Design and\nImplementation of Modern Column-Oriented Database Systems,” Foundations and\nTrends in Databases, volume 5, number 3, pages 197–280, December 2013.\nPipelining Query Execution,” at 2nd Biennial Conference on Innovative Data Systems\nSIMD Instructions,” at ACM International Conference on Management of Data (SIG‐\noriented DBMS,” at 31st International Conference on Very Large Data Bases (VLDB),\nlytic Database: C-Store 7 Years Later,” Proceedings of the VLDB Endowment, volume\nAggregation Operator Generalizing Group-By, Cross-Tab, and Sub-Totals,” Data\nIn most cases, a change to an application’s features also requires a change to data that\ndata needs to be presented in a new way.\ndatabase can contain a mixture of older and newer data formats written at different\nWhen a data format or schema changes, a corresponding change to application code\nThis means that old and new versions of the code, and old and new data formats,\nNewer code can read data that was written by older code.\nOlder code can read data that was written by newer code.\nyou know the format of data written by older code, and so you can explicitly handle it\nIn this chapter we will look at several formats for encoding data, including JSON,\ndle schema changes and how they support systems where old and new data and code\nFormats for Encoding Data",
      "keywords": [
        "Fay Chang",
        "Jeffrey Dean",
        "Sanjay Ghemawat",
        "ACM International Conference",
        "Data",
        "International Conference",
        "Conference",
        "doi",
        "ACM International",
        "Conference on Management",
        "ACM",
        "pages",
        "International",
        "Large Data",
        "Daniel J. Abadi"
      ],
      "concepts": [
        "data",
        "doi",
        "database",
        "coding",
        "code",
        "chang",
        "changes",
        "pages",
        "volume",
        "trees"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 58,
          "title": "",
          "score": 0.589,
          "base_score": 0.589,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 40,
          "title": "",
          "score": 0.563,
          "base_score": 0.563,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "",
          "score": 0.557,
          "base_score": 0.557,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 23,
          "title": "",
          "score": 0.54,
          "base_score": 0.54,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 59,
          "title": "",
          "score": 0.526,
          "base_score": 0.526,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "conference",
          "international",
          "acm",
          "international conference",
          "volume"
        ],
        "semantic": [],
        "merged": [
          "conference",
          "international",
          "acm",
          "international conference",
          "volume"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.40108459963056037,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:02:02.201800+00:00"
      }
    },
    {
      "chapter_number": 15,
      "title": "Segment 15 (pages 135-143)",
      "start_page": 135,
      "end_page": 143,
      "summary": "sequence-of-bytes representation looks quite different from the data structures\nlation from the in-memory representation to a byte sequence is called encoding (also\nMany programming languages come with built-in support for encoding in-memory\nFormats for Encoding Data \nFor these reasons it’s generally a bad idea to use your language’s built-in encoding for\n• There is a lot of ambiguity around the encoding of numbers.\nthis limitation by encoding the binary data as text using Base64.\n• There is optional schema support for both XML [11] and JSON [12].\nas numbers and binary strings) depends on information in the schema, applica‐\ntions that don’t use XML/JSON schemas need to potentially hardcode the appro‐\nBinary encoding\nuse a lowest-common-denominator encoding format.\nfloating-point numbers, or adding support for binary strings), but otherwise they\na schema, they need to include all the object field names within the encoded data.\nThat is, in a binary encoding of the JSON document in Example 4-1, they will need to\nFormats for Encoding Data \nExample record which we will encode in several binary formats in this\nLet’s look at an example of MessagePack, a binary encoding for JSON.\nshows the byte sequence that you get if you encode the JSON document in\nin four bits, it then gets a different type indicator, and the number of fields is\nencoded in two or four bytes.)\n4. The next seven bytes encode the six-letter string value Martin with a prefix 0xa6,\nThe binary encoding is 66 bytes long, which is only a little less than the 81 bytes taken\nby the textual JSON encoding (with whitespace removed).\nAll the binary encodings of\nExample record (Example 4-1) encoded using MessagePack.\nApache Thrift [15] and Protocol Buffers (protobuf) [16] are binary encoding libraries\nBoth Thrift and Protocol Buffers require a schema for any data that is encoded.\nencode the data in Example 4-1 in Thrift, you would describe the schema in the\nFormats for Encoding Data \nhas two different JSON-based encoding formats [19].\ngenerated code to encode or decode records of the schema.\nWhat does data encoded with this schema look like?\nferent binary encoding formats,iii called BinaryProtocol and CompactProtocol, respec‐\nEncoding Example 4-1 in that format takes\nExample record encoded using Thrift’s BinaryProtocol.\nInstead, the encoded data contains field tags, which are\ndoes this by packing the field type and tag number into a single byte, and by using\nRather than using a full eight bytes for the number 1337, it is\nencoded in two bytes, with the top bit of each byte used to indicate whether there are\nThis means numbers between –64 and 63 are encoded in\none byte, numbers between –8192 and 8191 are encoded in two bytes, etc.\nnumbers use more bytes.\nExample record encoded using Thrift’s CompactProtocol.\nFinally, Protocol Buffers (which has only one binary encoding format) encodes the\nFormats for Encoding Data \nExample record encoded using Protocol Buffers.\nrequired or optional, but this makes no difference to how the field is encoded\n(nothing in the binary data indicates whether a field was required).\nField tags and schema evolution\nAs you can see from the examples, an encoded record is just the concatenation of its\nencoded fields.\nEach field is identified by its tag number (the numbers 1, 2, 3 in the\nthat field tags are critical to the meaning of the encoded data.\nname of a field in the schema, since the encoded data never refers to field names, but\nyou cannot change a field’s tag, since that would make all existing encoded data\nYou can add new fields to the schema, provided that you give each field a new tag\nIf old code (which doesn’t know about the new tag numbers you added)\ntries to read data written by new code, including a new field with a tag number it\nAs long as each field has a unique tag number,\nnew code can always read old data, because the tag numbers still have the same\ndata written by old code, because the old code will not have written the new field that\nrequired field can never be removed), and you can never use the same tag number\nnumber, and that field must be ignored by new code).\nFormats for Encoding Data ",
      "keywords": [
        "Protocol Buffers",
        "encoding",
        "data",
        "field",
        "JSON",
        "Encoding Data",
        "Binary encoding",
        "XML",
        "schema",
        "numbers",
        "code",
        "Thrift",
        "Buffers",
        "Binary",
        "bytes"
      ],
      "concepts": [
        "data",
        "encoding",
        "encode",
        "schema",
        "string",
        "strings",
        "formats",
        "byte",
        "different",
        "difference"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 18,
          "title": "",
          "score": 0.645,
          "base_score": 0.495,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 16,
          "title": "",
          "score": 0.622,
          "base_score": 0.472,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 13,
          "title": "",
          "score": 0.594,
          "base_score": 0.444,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 8,
          "title": "",
          "score": 0.553,
          "base_score": 0.403,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 7,
          "title": "",
          "score": 0.525,
          "base_score": 0.375,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "field",
          "encoded",
          "encoding",
          "binary",
          "tag"
        ],
        "semantic": [],
        "merged": [
          "field",
          "encoded",
          "encoding",
          "binary",
          "tag"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.21571459129146164,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.201851+00:00"
      }
    },
    {
      "chapter_number": 16,
      "title": "Segment 16 (pages 144-152)",
      "start_page": 144,
      "end_page": 152,
      "summary": "Avro also uses a schema to specify the structure of the data being encoded.\nschema languages: one (Avro IDL) intended for human editing, and one (based on\nOur example schema, written in Avro IDL, might look like this:\nexample record (Example 4-1) using this schema, the Avro binary encoding is just 32\nthe schema and use the schema to tell you the datatype of each field.\nexact same schema as the code that wrote the data.\nSo, how does Avro support schema evolution?\nThe writer’s schema and the reader’s schema\nWith Avro, when an application wants to encode some data (to write it to a file or\ndatabase, to send it over the network, etc.), it encodes the data using whatever version\nThis is known as the writer’s schema.\nWhen an application wants to decode some data (read it from a file or database,\nreceive it from the network, etc.), it is expecting the data to be in some schema, which\nThat is the schema the application code is relying on\nThe key idea with Avro is that the writer’s schema and the reader’s schema don’t have\nAvro library resolves the differences by looking at the writer’s schema and the\nreader’s schema side by side and translating the data from the writer’s schema into\nthe reader’s schema.\nFor example, it’s no problem if the writer’s schema and the reader’s schema have\ntheir fields in a different order, because the schema resolution matches up the fields\nwriter’s schema but not in the reader’s schema, it is ignored.\ndata expects some field, but the writer’s schema does not contain a field of that name,\nit is filled in with a default value declared in the reader’s schema.\nAn Avro reader resolves differences between the writer’s schema and the\nreader’s schema.\nschema as writer and an old version of the schema as reader.\ncompatibility means that you can have a new version of the schema as reader and an\n(The field favoriteNumber in our Avro schema has a default value of null.)\nnew schema but not the old one.\nWhen a reader using the new schema reads a record\nwritten with the old schema, the default value is filled in for the missing field.\nread data written by old writers, so you would break backward compatibility.\nChanging the name of a field is possible but a little tricky: the reader’s schema can\ncontain aliases for field names, so it can match an old writer’s schema field names\nBut what is the writer’s schema?\nknow the writer’s schema with which a particular piece of data was encoded?\nlarge file containing millions of records, all encoded with the same schema.\nfile can just include the writer’s schema once at the beginning of the file.\nsame schema.\nning of every encoded record, and to keep a list of schema versions in your data‐\nwriter’s schema for that version number from the database.\nschema, it can decode the rest of the record.\nA database of schema versions is a useful thing to have in any case, since it acts as\nDynamically generated schemas\nThe difference is that Avro is friendlier to dynamically generated schemas.\nencode the database contents using that schema, dumping it all to an Avro object\nYou generate a record schema for each database table, and each\nNow, if the database schema changes (for example, a table has one column added and\none column removed), you can just generate a new Avro schema from the updated\ndatabase schema and export data in the new Avro schema.\nthe updated writer’s schema can still be matched up with the old reader’s schema.\ntags would likely have to be assigned by hand: every time the database schema\nschema generator would have to be very careful to not assign previously used field\nThrift and Protocol Buffers rely on code generation: after a schema has been defined,\nyou can generate code that implements this schema in a programming language of\ncally generated schema (such as an Avro schema generated from a database table),\ntainer file (which embeds the writer’s schema), you can simply open it using the Avro\nAs we saw, Protocol Buffers, Thrift, and Avro all use a schema to describe a binary\nwidespread, binary encodings based on schemas are also a viable option.\nthey can omit field names from the encoded data.\n• Keeping a database of schemas allows you to check forward and backward com‐\nfrom the schema is useful, since it enables type checking at compile time.\nschema-on-read JSON databases provide (see “Schema flexibility in the document\nIn a database, the process that writes to the database encodes the data, and the pro‐\nSay you add a field to a record schema, and the\nnewer code writes a value for that new field to the database.\nRewriting (migrating) data into a new schema is certainly possible, but it’s an expen‐\ntional databases allow simple schema changes, such as adding a new column with a\ndatabase fills in nulls for any columns that are missing from the encoded data on\nAvro’s schema evolution rules [23].",
      "keywords": [
        "schema",
        "Avro",
        "Avro schema",
        "data",
        "field",
        "database",
        "writer ’s schema",
        "Protocol Buffers",
        "schema evolution",
        "encoding",
        "writer",
        "code",
        "reader",
        "version",
        "database schema"
      ],
      "concepts": [
        "data",
        "schema",
        "database",
        "type",
        "typed",
        "fields",
        "encoded",
        "encode",
        "different",
        "differences"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 15,
          "title": "",
          "score": 0.622,
          "base_score": 0.472,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 3,
          "title": "",
          "score": 0.579,
          "base_score": 0.429,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 13,
          "title": "",
          "score": 0.539,
          "base_score": 0.389,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 41,
          "title": "",
          "score": 0.533,
          "base_score": 0.383,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 59,
          "title": "",
          "score": 0.508,
          "base_score": 0.358,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "schema",
          "writer schema",
          "avro",
          "writer",
          "reader"
        ],
        "semantic": [],
        "merged": [
          "schema",
          "writer schema",
          "avro",
          "writer",
          "reader"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2552700522889614,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.201906+00:00"
      }
    },
    {
      "chapter_number": 17,
      "title": "Segment 17 (pages 153-160)",
      "start_page": 153,
      "end_page": 160,
      "summary": "Dataflow Through Services: REST and RPC\nand the clients can connect to the servers to make requests to that API.\nexposed by the server is known as a service.\nThe web works this way: clients (web browsers) make requests to web servers, mak‐\nrequests to submit data to the server.\nmobile device or a desktop computer can also make network requests to a server, and\na client-side JavaScript application running inside a web browser can use\nbut rather data in an encoding that is convenient for further processing by the client-\nprotocol, the API implemented on top is application-specific, and the client and\nMoreover, a server can itself be a client to another service (for example, a typical web\nmakes a request to another when it requires some functionality or data from that\nother service.\nThis way of building applications has traditionally been called a service-\nIn some ways, services are similar to databases: they typically allow clients to submit\nguages we discussed in Chapter 2, services expose an application-specific API that\ntion code) of the service [33].\nA key design goal of a service-oriented/microservices architecture is to make the\napplication easier to change and maintain by making services independently deploya‐\nsions of servers and clients to be running at the same time, and so the data encoding\nused by servers and clients must be compatible across versions of the service API—\nWeb services\nWhen HTTP is used as the underlying protocol for talking to the service, it is called a\nweb service.\ndevice, or JavaScript web app using Ajax) making requests to a service over\n2. One service making requests to another service owned by the same organization,\n3. One service making requests to a service owned by a different organization, usu‐\nThere are two popular approaches to web services: REST and SOAP.\nBy contrast, SOAP is an XML-based protocol for making network API requests.vii\ncomplex multitude of related standards (the web service framework, known as WS-*)\nThe API of a SOAP web service is described using an XML-based language called the\nWeb Services Description Language, or WSDL.\nthat a client can access a remote service using local classes and method calls (which\nare encoded to XML messages and decoded again by the framework).\ning API requests over a network, many of which received a lot of hype but have seri‐\nThe Common Object Request Broker Architecture\nThe RPC model tries to make a request to a remote net‐\nwork service look the same as calling a function or method in your programming lan‐\nA network request is very different from a local function call: \nrequest or response may be lost due to a network problem, or the remote\nresponse from the remote service, you have no way of knowing whether the\n• If you retry a failed network request, it could happen that the requests are\nA network request is much slower than a function call, and its latency is\nwhen the network is congested or the remote service is overloaded it may take\nWhen you make a network request, all those parameters\n• The client and the service may be implemented in different programming lan‐\nrequest is different from a local function call.\nsimplify situations where you need to make requests to multiple services in parallel,\nSome of these frameworks also provide service discovery—that is, allowing a client to\nsimply make requests to it using a web browser or the command-line tool curl,\nfocus of RPC frameworks is on requests between services owned by the same organi‐\nData encoding and evolution for RPC\nFor evolvability, it is important that RPC clients and servers can be changed and\nthrough services: it is reasonable to assume that all the servers will be updated first,\n• In SOAP, requests and responses are specified with XML schemas.\n• RESTful APIs most commonly use JSON (without a formally specified schema)\nfor responses, and JSON or URI-encoded/form-encoded request parameters for\nService compatibility is made harder by the fact that RPC is often used for communi‐\nchange is required, the service provider often ends up maintaining multiple versions\nof the service API side by side.\nFor services that use API keys to identify a particular client, another option is to store\na client’s requested API version on the server and to allow this version selection to be\nWe have been looking at the different ways encoded data flows from one process to\nSo far, we’ve discussed REST and RPC (where one process sends a request\nand databases (where one process writes encoded data, and another process reads it\nclient’s request (usually called a message) is delivered to another process with low\nUsing a message broker has several advantages compared to direct RPC:\nHowever, a difference compared to RPC is that message-passing communication is\n(allowing a request/response dataflow, similar to RPC).\nMessage brokers typically don’t enforce any particular data model—a message is just\nSince each actor processes only one message at a time, it doesn’t\non different nodes, the message is transparently encoded into a byte sequence, sent\nmodel already assumes that messages may be lost, even within a single process.\nA distributed actor framework essentially integrates a message broker and the actor\nThree popular distributed actor frameworks handle message encoding as follows:",
      "keywords": [
        "RPC",
        "service",
        "API",
        "request",
        "data",
        "requests",
        "message",
        "web",
        "Web services",
        "network request",
        "SOAP",
        "network",
        "client",
        "SOAP web service",
        "service API"
      ],
      "concepts": [
        "services",
        "messages",
        "api",
        "apis",
        "data",
        "request",
        "requested",
        "encoded",
        "encode",
        "different"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 41,
          "title": "",
          "score": 0.555,
          "base_score": 0.405,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 61,
          "title": "",
          "score": 0.539,
          "base_score": 0.389,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "",
          "score": 0.532,
          "base_score": 0.382,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 44,
          "title": "",
          "score": 0.527,
          "base_score": 0.377,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 3,
          "title": "",
          "score": 0.52,
          "base_score": 0.37,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "service",
          "rpc",
          "web",
          "api",
          "requests"
        ],
        "semantic": [],
        "merged": [
          "service",
          "rpc",
          "web",
          "api",
          "requests"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.27231268594539515,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.201961+00:00"
      }
    },
    {
      "chapter_number": 18,
      "title": "Segment 18 (pages 161-168)",
      "start_page": 161,
      "end_page": 168,
      "summary": "• Orleans by default uses a custom data encoding format that does not support\nrolling upgrade deployments; to deploy a new version of your application, you\nIn particular, many services need to support rolling upgrades, where a new version of\nWe discussed several data encoding formats and their compatibility properties:\n• Databases, where the process writing to the database encodes the data and the\n[1] “Java Object Serialization Specification,” docs.oracle.com, 2010.\n[2] “Ruby 2.2.0 API Documentation,” ruby-doc.org, Dec 2014.\n[3] “The Python 3.4.3 Standard Library Reference Manual,” docs.python.org, Febru‐\n[4] “EsotericSoftware/kryo,” github.com, October 2014.\n[5] “CWE-502: Deserialization of Untrusted Data,” Common Weakness Enumera‐\nThis Vulnerability,” foxglovesecurity.com,\n[8] Eishay Smith: “jvm-serializers wiki,” github.com, November 2014.\n[9] “XML Is a Poor Copy of S-Expressions,” c2.com wiki.\nSchema 1.1,” W3C Recommendation, May 2001.\n[14] “MessagePack Specification,” msgpack.org.\nLanguage Services Implementation,” Facebook technical report, April 2007.\n[16] “Protocol Buffers Developer Guide,” Google, Inc., developers.google.com.\n[18] “A Matrix of the Features Each Individual Language Library Supports,”\n[19] Martin Kleppmann: “Schema Evolution in Avro, Protocol Buffers and Thrift,”\n[20] “Apache Avro 1.7.7 Documentation,” avro.apache.org, July 2014.\nject: Avro,” email thread on hadoop-general mailing list, mail-archives.apache.org,\nNew Distributed Document Store,” engineering.linkedin.com, January 21, 2015.\nData Platform (Part 2),” blog.confluent.io, February 25, 2015.\n[25] Gwen Shapira: “The Problem of Managing Schemas,” radar.oreilly.com, Novem‐\n[26] “Apache Pig 0.14.0 Documentation,” pig.apache.org, November 2014.\nX.509 Public Key Infrastructure: Certificate and CRL Profile,” IETF Network Work‐\n[30] Jesse James Garrett: “Ajax: A New Approach to Web Applications,” adaptive‐\nand Scalability,” infoq.com, May 25, 2014.\n[33] Pat Helland: “Data on the Outside Versus Data on the Inside,” at 2nd Biennial\n[35] Roy Thomas Fielding: “REST APIs Must Be Hypertext-Driven,” roy.gbiv.com,\n[36] “REST in Peace, SOAP,” royal.pingdom.com, October 15, 2010.\n[37] “Web Services Standards as of Q1 2007,” innoq.com, February 2007.\n[38] Pete Lacey: “The S Stands for Simple,” harmful.cat-v.org, November 15, 2006.\n[39] Stefan Tilkov: “Interview: Pete Lacey Criticizes Web Services,” infoq.com,\ntion) Version 2.0,” swagger.io, September 8, 2014.\nCalls,” ACM Transactions on Computer Systems (TOCS), volume 2, number 1, pages\n[46] “grpc-common Documentation,” Google, Inc., github.com, February 2015.\nServices,” ibm.com, March 28, 2007.\nDifferent Wrong Ways,” troyhunt.com, February 10, 2014.\n[49] “API Upgrades,” Stripe, Inc., April 2015.\n[50] Jonas Bonér: “Upgrade in an Akka Cluster,” email to akka-user mailing list, grok‐\nUpdating Data Structures,” email thread on erlang-questions mailing list, erlang.com,\n[54] Fred Hebert: “Postscript: Maps,” learnyousomeerlang.com, April 9, 2014.\npens if multiple machines are involved in storage and retrieval of data?\nIf your data volume, read load, or write load grows bigger than a single machine\nmachines, or the network, or an entire datacenter) goes down, you can use multi‐\nmachine [1].i\nA shared-memory architecture may offer limited fault tolerance—high-end machines\nAnother approach is the shared-disk architecture, which uses several machines with\nindependent CPUs and RAM, but stores data on an array of disks that is shared\nbetween the machines, which are connected via a fast network.ii This architecture is",
      "keywords": [
        "data",
        "rolling upgrades",
        "Protocol Buffers",
        "February",
        "machine",
        "upgrades",
        "documentation",
        "November",
        "rolling",
        "October",
        "data encoding format",
        "application",
        "data encoding",
        "support rolling upgrade",
        "network"
      ],
      "concepts": [
        "schemas",
        "machine",
        "february",
        "network",
        "november",
        "documentation",
        "document",
        "language",
        "october",
        "compatibility"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 15,
          "title": "",
          "score": 0.645,
          "base_score": 0.495,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 3,
          "title": "",
          "score": 0.512,
          "base_score": 0.362,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 59,
          "title": "",
          "score": 0.461,
          "base_score": 0.311,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 6,
          "title": "",
          "score": 0.44,
          "base_score": 0.29,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "",
          "score": 0.427,
          "base_score": 0.277,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "com",
          "2014",
          "org",
          "rolling",
          "upgrades"
        ],
        "semantic": [],
        "merged": [
          "com",
          "2014",
          "org",
          "rolling",
          "upgrades"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.1910541570996822,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.202011+00:00"
      }
    },
    {
      "chapter_number": 19,
      "title": "Segment 19 (pages 169-182)",
      "start_page": 169,
      "end_page": 182,
      "summary": "There are two common ways data is distributed across multiple nodes:\nReplication\nWe discuss replication in Chapter 5.\nReplication\nReplication means keeping a copy of the same data on multiple machines that are\nreasons why you might want to replicate data:\nchapters we will discuss various kinds of faults that can occur in a replicated data sys‐\nIf the data that you’re replicating does not change over time, then replication is easy:\nbetween nodes: single-leader, multi-leader, and leaderless replication.\nprocesses changes from the leader but doesn’t process any queries from clients.\nReplication of databases is an old topic—the principles haven’t changed much since\nLeaders and Followers\nEach node that stores a copy of the database is called a replica.\nEvery write to the database needs to be processed by every replica; otherwise, the rep‐\ncalled leader-based replication (also known as active/passive or master–slave replica‐\nleader, which first writes the new data to its local storage.\nstandbys).i Whenever the leader writes new data to its local storage, it also sends\nthe data change to all of its followers as part of a replication log or change stream.\nEach follower takes the log from the leader and updates its local copy of the data‐\non the leader.\nChapter 5: Replication\n3. When a client wants to read from the database, it can query either the leader or\nany of the followers.\nLeader-based (master–slave) replication.\nFinally, leader-based replication\nAt some point in time, the client sends the update request to the leader;\ndata change to the followers.\nuser’s client, the leader, and two followers.\nLeaders and Followers \nLeader-based replication with one synchronous and one asynchronous fol‐\nIn the example of Figure 5-2, the replication to follower 1 is synchronous: the leader\nThe replication to\nfollower 2 is asynchronous: the leader sends the message, but doesn’t wait for a\nNormally, replication is quite fast: most database systems apply changes to\nThere are circumstances when followers might fall behind the leader by several\nThe advantage of synchronous replication is that the follower is guaranteed to have\nan up-to-date copy of the data that is consistent with the leader.\ndenly fails, we can be sure that the data is still available on the follower.\nThe leader must block all writes and wait until the synchronous replica is available\nchronous replication on a database, it usually means that one of the followers is syn‐\nChapter 5: Replication\nleader and one synchronous follower.\nOften, leader-based replication is configured to be completely asynchronous.\ncase, if the leader fails and is not recoverable, any writes that have not yet been repli‐\ntion has the advantage that the leader can continue processing writes, even if all of its\nIt can be a serious problem for asynchronously replicated systems to lose data if the\nleader fails, so researchers have continued investigating replication methods that do\nhas an accurate copy of the leader’s data?\nLeaders and Followers \n1. Take a consistent snapshot of the leader’s database at some point in time—if pos‐\n2. Copy the snapshot to the new follower node.\n3. The follower connects to the leader and requests all the data changes that have\nated with an exact position in the leader’s replication log.\n4. When the follower has processed the backlog of data changes since the snapshot,\nleader as they happen.\nHow do you achieve high availability with leader-based replication?\nOn its local disk, each follower keeps a log of the data changes it has received from\nthe leader.\nIf a follower crashes and is restarted, or if the network between the leader\nThus, the follower can connect to the leader and request all the data changes that\nChapter 5: Replication\nHandling a failure of the leader is trickier: one of the followers needs to be promoted\nto be the new leader, clients need to be reconfigured to send their writes to the new\nleader, and the other followers need to start consuming data changes from the new\nleader.\nand takes the necessary steps to make a new leader) or automatically.\n2. Choosing a new leader.\nthe leader is chosen by a majority of the remaining replicas), or a new leader\nold leader (to minimize any data loss).\nleader is a consensus problem, discussed in detail in Chapter 9.\n3. Reconfiguring the system to use the new leader.\ntheir write requests to the new leader (we discuss this in “Request Routing” on\nneeds to ensure that the old leader becomes a follower and recognizes the new\nleader.\n• If asynchronous replication is used, the new leader may not have received all the\nwrites from the old leader before it failed.\nafter a new leader has been chosen, what should happen to those writes?\nincident at GitHub [13], an out-of-date MySQL follower was promoted to leader.\nLeaders and Followers \nflicts (see “Multi-Leader Replication” on page 168), data is likely to be lost or\nnode if two leaders are detected.ii However, if this mechanism is not carefully\nHow does leader-based replication work under the hood?\nIn the simplest case, the leader logs every write request (statement) that it executes\nand sends that statement log to its followers.\nChapter 5: Replication\nIn Chapter 3 we discussed how storage engines represent data on disk, and we found\nwriting the log to disk, the leader also sends it across the network to its followers.\nLeaders and Followers \nWhen the follower processes this log, it builds a copy of the exact same data struc‐\nthe database software on the leader and the followers.\nIf the replication protocol allows the follower to use a newer software version\nthan the leader, you can perform a zero-downtime upgrade of the database software\nupgraded nodes the new leader.\nLogical (row-based) log replication\nAn alternative is to use different log formats for replication and for the storage\nbe kept backward compatible, allowing the leader and the follower to run different\nChapter 5: Replication",
      "keywords": [
        "leader",
        "Replication",
        "data",
        "follower",
        "database",
        "log",
        "data changes",
        "replication log",
        "Replication leader",
        "nodes",
        "writes",
        "leader-based replication",
        "system",
        "asynchronous replication",
        "Replication Leader failure"
      ],
      "concepts": [
        "replication",
        "replicate",
        "replicated",
        "data",
        "leader",
        "followers",
        "database",
        "different",
        "differ",
        "difference"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 20,
          "title": "",
          "score": 0.9,
          "base_score": 0.75,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 21,
          "title": "",
          "score": 0.807,
          "base_score": 0.657,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 47,
          "title": "",
          "score": 0.782,
          "base_score": 0.632,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 23,
          "title": "",
          "score": 0.779,
          "base_score": 0.629,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 36,
          "title": "",
          "score": 0.762,
          "base_score": 0.612,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "leader",
          "replication",
          "follower",
          "new leader",
          "followers"
        ],
        "semantic": [],
        "merged": [
          "leader",
          "replication",
          "follower",
          "new leader",
          "followers"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.34698248913646007,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.202065+00:00"
      }
    },
    {
      "chapter_number": 20,
      "title": "Segment 20 (pages 183-196)",
      "start_page": 183,
      "end_page": 196,
      "summary": "database to another, or if you need conflict resolution logic (see “Handling Write\nConflicts” on page 171), then you may need to move replication up to the application\napplication by reading the database log.\nwhen a data change (write transaction) occurs in a database system.\nlogic and replicate the data change to another system.\nLeader-based replication requires all writes to go through a single node, but read-\nThis removes load from the leader and allows read requests to be served by\nconsistent: followers in an asynchronously replicated relational database have the same characteristics.\ntencies in the database: if you run the same query on the leader and a follower at the\nReading Your Own Writes\nMany applications let the user submit some data and then view what they have sub‐\nthe leader, but when the user views the data, it can be read from a follower.\nuser views the data shortly after making a write, the new data may not yet have\nA user makes a write, followed by a read from a stale replica.\nthis anomaly, we need read-after-write consistency.\nIn this situation, we need read-after-write consistency, also known as read-your-writes\nHow can we implement read-after-write consistency in a system with leader-based\n• When reading something that the user may have modified, read it from the\nleader; otherwise, read it from a follower.\nple rule is: always read the user’s own profile from the leader, and any other\n• If most things in the application are potentially editable by the user, that\nused to decide whether to read from the leader.\nfrom the leader.\ntem can ensure that the replica serving any reads for that user reflects updates at\nthat needs to be served by the leader must be routed to the datacenter that con‐\nmay want to provide cross-device read-after-write consistency: if the user enters some\nleader, you may first need to route requests from all of a user’s devices to the\nfollowers is that it’s possible for a user to see things moving backward in time.\nThis can happen if a user makes several reads from different replicas.\nFigure 5-4 shows user 2345 making the same query twice, first to a follower with little\nA user first reads from a fresh replica, then from a stale replica.\nthat if one user makes several reads in sequence, they will not see time go backward—\nOne way of achieving monotonic reads is to make sure that each user always makes\ntheir reads from the same replica (different users can read from different replicas).\nFor example, the replica can be chosen based on a hash of the user ID, rather than\nHowever, if that replica fails, the user’s queries will need to be rerouted to\nthen anyone reading those writes will see them appear in the same order.\nIf the database always applies writes in the same order, reads always see\nwrites: when a user reads from the database, they may see some parts of the database\nsuch as read-after-write.\nreads on the leader.\nIf the database is partitioned (see Chapter 6), each partition has one leader.\nMulti-Leader Replication\nleader.\nLeader-based replication has one major downside: there is only one leader, and all\nwrites must go through it.iv If you can’t connect to the leader for any reason, for\nexample due to a network interruption between you and the leader, you can’t write to\nA natural extension of the leader-based replication model is to allow more than one\nmulti-leader configuration (also known as master–master or active/active replication).\nUse Cases for Multi-Leader Replication\nIt rarely makes sense to use a multi-leader setup within a single datacenter, because\nImagine you have a database with replicas in several different datacenters (perhaps so\nWith a normal leader-based replication setup, the leader has to be in\nIn a multi-leader configuration, you can have a leader in each datacenter.\nfollower replication is used; between datacenters, each datacenter’s leader replicates\nits changes to the leaders in other datacenters.\nMulti-leader replication across multiple datacenters.\nIn a single-leader configuration, every write must go over the internet to the\ndatacenter with the leader.\nmulti-leader configuration, every write can be processed in the local datacenter\nand is replicated asynchronously to the other datacenters.\nIn a single-leader configuration, if the datacenter with the leader fails, failover\ncan promote a follower in another datacenter to be leader.\nMulti-Leader Replication \nSome databases support multi-leader configurations by default, but it is also often\nAlthough multi-leader replication has advantages, it also has a big downside: the\nsame data may be concurrently modified in two different datacenters, and those write\nAs multi-leader replication is a somewhat retrofitted feature in many databases, there\nFor this reason, multi-leader replication is often considered danger‐\nAnother situation in which multi-leader replication is appropriate is if you have an\nIn this case, every device has a local database that acts as a leader (it accepts write\nrequests), and there is an asynchronous multi-leader replication process (sync)\ntory of broken calendar sync implementations demonstrates, multi-leader replication\nWe don’t usually think of collaborative editing as a database replication problem, but\none user edits a document, the changes are instantly applied to their local replica (the\nreplicated to the server and any other users who are editing the same document.\nIf you want to guarantee that there will be no editing conflicts, the application must\nreplication with transactions on the leader.\nto edit simultaneously, but it also brings all the challenges of multi-leader replication,\nHandling Write Conflicts\nThe biggest problem with multi-leader replication is that write conflicts can occur,\nFor example, consider a wiki page that is simultaneously being edited by two users, as\nA write conflict caused by two leaders concurrently updating the same\nMulti-Leader Replication \nIn a single-leader database, the second writer will either block and wait for the first\nwrite to complete, or abort the second write transaction, forcing the user to retry the\nwrite.\nOn the other hand, in a multi-leader setup, both writes are successful, and the\nwrite to be replicated to all replicas before telling the user that the write was success‐\nconflict detection, you might as well just use single-leader replication.\nensure that all writes for a particular record go through the same leader, then con‐\nSince many implementations of multi-leader replication handle\nFor example, in an application where a user can edit their own data, you can ensure\nthat requests from a particular user are always routed to the same datacenter and use\nthe leader in that datacenter for reading and writing.\nbut from any one user’s point of view the configuration is essentially single-leader.\nyou have to deal with the possibility of concurrent writes on different leaders.\nA single-leader database applies writes in a sequential order: if there are several\nIn a multi-leader configuration, there is no defined ordering of writes, so it’s not clear\nIf each replica simply applied writes in the order that it saw the writes, the database\nleader 2.\nand write application code that resolves the conflict at some later time (perhaps\nmost multi-leader replication tools let you write conflict resolution logic using appli‐\nThat code may be executed on write or on read:\nOn write\nAs soon as the database system detects a conflict in the log of replicated changes,\nThe application may prompt the user or automatically resolve the conflict,\nand write the result back to the database.\nMulti-Leader Replication \n• Conflict-free replicated datatypes (CRDTs) [32, 38] are a family of data structures\nmultiple users, and which automatically resolve conflicts in sensible ways.\nconflict resolution could make multi-leader data synchronization much simpler for",
      "keywords": [
        "replication",
        "Multi-Leader Replication",
        "Replication Lag",
        "user",
        "conflict",
        "Write",
        "leader",
        "conflict resolution",
        "Multi-Leader",
        "data",
        "database",
        "datacenter",
        "read",
        "write conflicts",
        "Mrs. Cake"
      ],
      "concepts": [
        "replication",
        "replicate",
        "replicated",
        "write",
        "writing",
        "leader",
        "users",
        "data",
        "conflicts",
        "replicas"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 19,
          "title": "",
          "score": 0.9,
          "base_score": 0.75,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 36,
          "title": "",
          "score": 0.895,
          "base_score": 0.745,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 21,
          "title": "",
          "score": 0.872,
          "base_score": 0.722,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 23,
          "title": "",
          "score": 0.809,
          "base_score": 0.659,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 47,
          "title": "",
          "score": 0.807,
          "base_score": 0.657,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "leader",
          "multi leader",
          "replication",
          "multi",
          "user"
        ],
        "semantic": [],
        "merged": [
          "leader",
          "multi leader",
          "replication",
          "multi",
          "user"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.39892126556087687,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.202123+00:00"
      }
    },
    {
      "chapter_number": 21,
      "title": "Segment 21 (pages 197-206)",
      "start_page": 197,
      "end_page": 206,
      "summary": "A replication topology describes the communication paths along which writes are\nthere is only one plausible topology: leader 1 must send all of its writes to leader 2,\nThree example topologies in which multi-leader replication can be set up.\nwrites to every other leader.\nnode receives writes from one node and forwards those writes (plus any writes of its\nown) to one other node.\nnated root node forwards writes to all of the other nodes.\nIn circular and star topologies, a write may need to pass through several nodes before\nfrom other nodes.\nWith multi-leader replication, writes may arrive in the wrong order at some\nHowever, leader 2 may receive the writes in a different order: it may\nand multi-leader replication—are based on the idea that a client sends a write request\nto one node (the leader), and the database system takes care of copying that write to\nA leader determines the order in which writes should be processed,\nand followers apply the leader’s writes in the same order.\nleader and allowing any replica to directly accept writes from clients.\nwrites.\nWriting to the Database When a Node Is Down\nA quorum write, quorum read, and read repair after a node outage.\nNow imagine that the unavailable node comes back online, and clients start reading\nAny writes that happened while the node was down are missing from that\nnode.\nThus, if you read from that node, you may get stale (outdated) values as\nrequest to one replica: read requests are also sent to several nodes in parallel.\none node and a stale value from another.\nWhen a client makes a read from several nodes in parallel, it can detect any stale\nhas a stale value and writes the newer value back to that replica.\nQuorums for reading and writing\nMore generally, if there are n replicas, every write must be confirmed by w nodes to\nbe considered successful, and we must query at least r nodes for each read.\nvalue when reading, because at least one of the r nodes we’re reading from must be\nReads and writes that obey these r and w values are called quorum reads\nand writes [44].vii You can think of r and w as the minimum number of votes required\nfor the read or write to be valid.\nexample, a workload with few writes and many reads may benefit from setting w = n\nThis makes reads faster, but has the disadvantage that just one failed node\nThere may be more than n nodes in the cluster, but any given value\nThe quorum condition, w + r > n, allows the system to tolerate unavailable nodes as\n• If w < n, we can still process writes if a node is unavailable.\n• If r < n, we can still process reads if a node is unavailable.\n• With n = 3, w = 2, r = 2 we can tolerate one unavailable node.\n• With n = 5, w = 3, r = 3 we can tolerate two unavailable nodes.\n• Normally, reads and writes are always sent to all n replicas in parallel.\nthe n nodes need to report success before we consider the read or write to be suc‐\nIf w + r > n, at least one of the r replicas you read from must have seen the\nIf fewer than the required w or r nodes are available, writes or reads return an error.\nThat is, among the nodes you read there must be at least\none node with the latest value (illustrated in Figure 5-11).\nOften, r and w are chosen to be a majority (more than n/2) of nodes, because that\nensures w + r > n while still tolerating up to n/2 node failures.\nwrite operations overlap in at least one node.\nIn this case, reads and writes will still be sent to n nodes, but a\nWith a smaller w and r you are more likely to read stale values, because it’s more\nlikely that your read didn’t include the node with the latest value.\ncontinue processing reads and writes.\nfalls below w or r does the database become unavailable for writing or reading,\n183), the w writes may end up on different nodes than the r reads, so there is no\nlonger a guaranteed overlap between the r nodes and the w nodes [46].\n• If a write happens concurrently with a read, the write may be reflected on only\n• If a write succeeded on some replicas but failed on others (for example because\nthe disks on some nodes are full), and overall succeeded on fewer than w replicas,\nthat write [47].\n• If a node carrying a new value fails, and its data is restored from a replica carry‐\napplied to the leader and to followers in the same order, and each node has a position\nwhen w or r nodes have responded.\nnodes.\nthem, to a client that is cut off from the database nodes, they might as well be dead.\nthis situation, it’s likely that fewer than w or r reachable nodes remain, so the client\nnodes that it needs to assemble a quorum for a particular value.\nw or r nodes?\n• Or should we accept writes anyway, and write them to some nodes that are\nThe latter is known as a sloppy quorum [37]: writes and reads still require w and r\nn “home” nodes for a value.\nOnce the network interruption is fixed, any writes that one node temporarily\nw nodes are available, the database can accept writes.\nwhen w + r > n, you cannot be sure to read the latest value for a key, because the\nThere is no guarantee that a read of r nodes will see it until the hinted handoff has\nmal leaderless model: the number of replicas n includes nodes in all datacenters, and\nRiak keeps all communication between clients and database nodes local to one data‐\nDynamo-style databases allow several clients to concurrently write to the same key,\nsimilar to multi-leader replication (see “Handling Write Conflicts” on page 171),",
      "keywords": [
        "nodes",
        "writes",
        "Replication",
        "read",
        "replicas",
        "Multi-Leader Replication",
        "Detecting Concurrent Writes",
        "Leaderless Replication",
        "Concurrent Writes",
        "quorum",
        "database",
        "database nodes",
        "client",
        "leader"
      ],
      "concepts": [
        "node",
        "reads",
        "writes",
        "writing",
        "replicated",
        "replication",
        "values",
        "database",
        "topology",
        "topologies"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 20,
          "title": "",
          "score": 0.872,
          "base_score": 0.722,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 19,
          "title": "",
          "score": 0.807,
          "base_score": 0.657,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 40,
          "title": "",
          "score": 0.807,
          "base_score": 0.657,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 36,
          "title": "",
          "score": 0.744,
          "base_score": 0.594,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 23,
          "title": "",
          "score": 0.713,
          "base_score": 0.563,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "nodes",
          "writes",
          "node",
          "leader",
          "unavailable"
        ],
        "semantic": [],
        "merged": [
          "nodes",
          "writes",
          "node",
          "leader",
          "unavailable"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3353253200034539,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.202175+00:00"
      }
    },
    {
      "chapter_number": 22,
      "title": "Segment 22 (pages 207-215)",
      "start_page": 207,
      "end_page": 215,
      "summary": "If each node simply overwrote the value for a key whenever it received a write request\nLast write wins (discarding concurrent writes)\nwrite requests to the database nodes, so it’s not clear which one happened first.\nare several concurrent writes to the same key, even if they were all reported as suc‐\ncessful to the client (because they were written to w replicas), only one of the writes\nwrites that are not concurrent, as we shall discuss in “Timestamps for ordering\nHow do we decide whether two operations are concurrent or not?\n• In Figure 5-9, the two writes are not concurrent: A’s insert happens before B’s\n• On the other hand, the two writes in Figure 5-12 are concurrent: when each cli‐\noperation is the key to defining what concurrency means.\nthat two operations are concurrent if neither happens before the other (i.e., neither\nneed is an algorithm to tell us whether two operations are concurrent or not.\noperation, but if the operations are concurrent, we have a conflict that needs to be\nIt may seem that two operations should be called concurrent if they occur “at the\nLet’s look at an algorithm that determines whether two operations are concurrent, or\nFigure 5-13 shows two clients concurrently adding items to the same shopping cart.\nBetween them, the clients make five writes to the database:\nto the client, along with the version number.\n2. Client 2 adds eggs to the cart, not knowing that client 1 concurrently added milk\nversion 2 to this write, and stores eggs and milk as two separate values.\nreturns both values to the client, along with the version number of 2.\n3. Client 1, oblivious to client 2’s write, wants to add flour to the cart, so it thinks\nserver, along with the version number 1 that the server gave client 1 previously.\nThe server can tell from the version number that the write of [milk, flour]\nsupersedes the prior value of [milk] but that it is concurrent with [eggs].\nthe server assigns version 3 to [milk, flour], overwrites the version 1 value\n[milk], but keeps the version 2 value [eggs] and returns both remaining values\nClient 2 received the two values [milk] and [eggs] from the server in the\nconcurrent with [milk, flour], so the two remaining values are [milk, flour]\n[eggs] from the server at version 3, so it merges those, adds bacon, and sends the\nfinal value [milk, flour, eggs, bacon] to the server, along with the version\nwritten in the last step) but is concurrent with [eggs, milk, ham], so the server\nkeeps those two concurrent values.\nIn this example, the clients are never fully up to date with the data on the server, since\nthere is always another operation going on concurrently.\nBut old versions of the value\nNote that the server can determine whether two operations are concurrent by looking\nnumber every time that key is written, and stores the new version number along\n• When a client reads a key, the server returns all values that have not been over‐\nwriting.\n• When a client writes a key, it must include the version number from the prior\nresponse from a write request can be like a read, returning all current values,\n• When the server receives a write with a particular version number, it can over‐\nwrite all values with that version number or below (since it knows that they have\nsion number (because those values are concurrent with the incoming write).\nWhen a write includes the version number from a prior read, that tells us which pre‐\nIf you make a write without including a version\nnumber, it is concurrent with all other writes, so it will not overwrite anything—it\nMerging concurrently written values\nthat the clients do some extra work: if several operations happen concurrently, clients\nhave to clean up afterward by merging the concurrently written values.\nthese concurrent values siblings.\nMerging sibling values is essentially the same problem as conflict resolution in multi-\nleader replication, which we discussed previously (see “Handling Write Conflicts” on\nnumber or timestamp (last write wins), but that implies losing data.\nFigure 5-13 uses a single version number to capture dependencies between opera‐\nInstead, we need to use a version number per replica as well as per key.\nEach replica increments its own version number when processing a write, and also\nLike the version numbers in Figure 5-13, version vectors are sent from the database\nreplicas to clients when values are read, and need to be sent back to the database\noverwrites and concurrent writes.\nAlso, like in the single-replica example, the application may need to merge siblings.\nreferences for details [57, 60, 61].\nstate of replicas, version vectors are the right data structure to use.\nClients send all writes to a single node (the leader), which sends a stream of data\nClients send each write to one of several leader nodes, any of which can accept\nwrites.\nClients send each write to several nodes, and read from several nodes in parallel\nleaderless replication approaches: because they allow multiple writes to happen con‐",
      "keywords": [
        "version number",
        "version",
        "write",
        "Concurrent",
        "Replication",
        "client",
        "data",
        "operation",
        "number",
        "Concurrent writes",
        "server",
        "operations",
        "milk",
        "version vector",
        "Figure 5-13"
      ],
      "concepts": [
        "concurrency",
        "writing",
        "write",
        "data",
        "value",
        "version",
        "versions",
        "client",
        "replication",
        "nodes"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 38,
          "title": "",
          "score": 0.804,
          "base_score": 0.654,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 58,
          "title": "",
          "score": 0.633,
          "base_score": 0.633,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 26,
          "title": "",
          "score": 0.617,
          "base_score": 0.617,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 36,
          "title": "",
          "score": 0.612,
          "base_score": 0.612,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 20,
          "title": "",
          "score": 0.582,
          "base_score": 0.582,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "version",
          "version number",
          "concurrent",
          "milk",
          "values"
        ],
        "semantic": [],
        "merged": [
          "version",
          "version number",
          "concurrent",
          "milk",
          "values"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3861130374957738,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.202232+00:00"
      }
    },
    {
      "chapter_number": 23,
      "title": "Segment 23 (pages 216-227)",
      "start_page": 216,
      "end_page": 227,
      "summary": "LinkedIn’s Distributed Data Serving Platform,” at ACM International Conference on\nThroughput Chain Replication for Read-Mostly Workloads,” at USENIX Annual\n[21] Greg Sabino Mullane: “Version 5 of Bucardo Database Replication System,”\n[22] Werner Vogels: “Eventually Consistent,” ACM Queue, volume 6, number 6,\nTerry: “Replicated Data Consistency Explained Through Baseball,”\nWeakly Consistent Replicated Data,” at 3rd International Conference on Parallel and\n[26] “Tungsten Replicator,” Continuent, Inc., 2014.\ntributed Systems,” basho.com, November 12, 2013.\nzon’s Highly Available Key-Value Store,” at 21st ACM Symposium on Operating Sys‐\nprehensive Study of Convergent and Commutative Replicated Data Types,” INRIA\n[40] Russell Brown: “A Bluffers Guide to CRDTs in Riak,” gist.github.com, October\nGifford: “Weighted Voting for Replicated Data,” at 7th ACM Sympo‐\n[47] Joseph Blomstedt: “Bringing Consistency to Riak,” at RICON West, October\nEventual Consistency with PBS,” Communications of the ACM, volume 57, number 8,\n[53] Jonathan Ellis: “Why Cassandra Doesn’t Need Vector Clocks,” datastax.com,\ntem,” Communications of the ACM, volume 21, number 7, pages 558–565, July 1978.\n[55] Joel Jacobson: “Riak 2.0: Data Types,” blog.joeljacobson.com, March 23, 2014.\nVectors: Logical Clocks for Optimistic Replication,” arXiv:1011.5808, November 26,\n[58] Sean Cribbs: “A Brief History of Time in Riak,” at RICON, October 2014.\ni. Partitioning, as discussed in this chapter, is a way of intentionally breaking a large database down into\nPartitioning\nnot sufficient: we need to break the data up into partitions, also known as sharding.i\nNormally, partitions are defined in such a way that each piece of data (each record,\nple partitions at the same time.\nThe main reason for wanting to partition data is scalability.\nDifferent partitions can\nFor queries that operate on a single partition, each node can independently execute\nIn this chapter we will first look at different approaches for partitioning large datasets\nand observe how the indexing of data interacts with partitioning.\nPartitioning and Replication\nPartitioning is usually combined with replication so that copies of each partition are\nexactly one partition, it may still be stored on several different nodes for fault toler‐\nA node may store more than one partition.\nused, the combination of partitioning and replication can look like Figure 6-1.\npartition’s leader is assigned to one node, and its followers are assigned to other\nEach node may be the leader for some partitions and a follower for other par‐\nto replication of partitions.\nChapter 6: Partitioning\nCombining replication and partitioning: each node acts as leader for some\nPartitioning of Key-Value Data\nSay you have a large amount of data, and you want to partition it.\nOur goal with partitioning is to spread the data and the query load evenly across\nIf the partitioning is unfair, so that some partitions have more data or queries than\nIn an extreme case, all the load could end up on one partition, so 9 out of 10 nodes\nA partition with disproportion‐\nThat would distribute the data quite evenly across the nodes, but it has a\nPartitioning of Key-Value Data \nPartitioning by Key Range\nOne way of partitioning is to assign a continuous range of keys (from some mini‐\nmine which partition contains a given key.\nIf you also know which partition is\nA print encyclopedia is partitioned by key range.\nThe ranges of keys are not necessarily evenly spaced, because your data may not be\nIn order to distribute the data evenly, the partition bound‐\nThis partitioning strategy is\nWithin each partition, we can keep keys in sorted order (see “SSTables and LSM-\nthat stores data from a network of sensors, where the key is the timestamp of the\nChapter 6: Partitioning\nHowever, the downside of key range partitioning is that certain access patterns can\nIf the key is a timestamp, then the partitions correspond to ranges\nPartitioning by Hash of Key\nfunction to determine the partition for a given key.\nA good hash function takes skewed data and makes it uniformly distributed.\nFor partitioning purposes, the hash function need not be cryptographically strong:\n(as they are used for hash tables), but they may not be suitable for partitioning: for\nOnce you have a suitable hash function for keys, you can assign each partition a\npartition’s range will be stored in that partition.\nPartitioning of Key-Value Data \nPartitioning by hash of key.\nThis technique is good at distributing keys fairly among the partitions.\nThe partition\nand just call it hash partitioning instead.\nUnfortunately however, by using the hash of the key for partitioning we lose a nice\nproperty of key-range partitioning: the ability to do efficient range queries.\nhas to be sent to all partitions [4].\nOnly the first part of that key is hashed to determine the partition,\nChapter 6: Partitioning\nDifferent users may be stored on different partitions, but\nwithin each user, the updates are stored ordered by timestamp on a single partition.\nAs discussed, hashing a key to determine its partition can help reduce hot spots.\nto be distributed to different partitions.\nPartitioning of Key-Value Data ",
      "keywords": [
        "Brewing Fresh Espresso",
        "Data",
        "Lin Qiao",
        "Fresh Espresso",
        "Brewing Fresh",
        "partition",
        "Partitioning",
        "key",
        "Replication",
        "Data Serving Platform",
        "keys",
        "Replicated Data",
        "Distributed Data Serving",
        "October",
        "Range"
      ],
      "concepts": [
        "partitioning",
        "partition",
        "replication",
        "replicated",
        "data",
        "october",
        "consistency",
        "consistent",
        "consisting",
        "hash"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 40,
          "title": "",
          "score": 0.847,
          "base_score": 0.697,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 36,
          "title": "",
          "score": 0.834,
          "base_score": 0.684,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 20,
          "title": "",
          "score": 0.809,
          "base_score": 0.659,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 19,
          "title": "",
          "score": 0.779,
          "base_score": 0.629,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 21,
          "title": "",
          "score": 0.713,
          "base_score": 0.563,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "partitioning",
          "partition",
          "key",
          "partitions",
          "partitioning key"
        ],
        "semantic": [],
        "merged": [
          "partitioning",
          "partition",
          "key",
          "partitions",
          "partitioning key"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4025723623229111,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.202299+00:00"
      }
    },
    {
      "chapter_number": 24,
      "title": "Segment 24 (pages 228-238)",
      "start_page": 228,
      "end_page": 238,
      "summary": "Partitioning and Secondary Indexes\nThe partitioning schemes we have discussed so far rely on a key-value data model.\nfrom that key and use it to route read and write requests to the partition responsible\nThe problem with secondary indexes is that they don’t map neatly to partitions.\nThere are two main approaches to partitioning a database with secondary indexes:\nPartitioning Secondary Indexes by Document\nthe database by the document ID (for example, IDs 0 to 499 in partition 0, IDs 500 to\n999 in partition 1, etc.).\na red car is added to the database, the database partition automatically adds it to the\nChapter 6: Partitioning\nPartitioning secondary indexes by document.\nIn this indexing approach, each partition is completely separate: each partition main‐\ntains its own secondary indexes, covering only the documents in that partition.\ndoesn’t care what data is stored in other partitions.\ndocument-partitioned index is also known as a local index (as opposed to a global\nHowever, reading from a document-partitioned index requires care: unless you have\nsearch for red cars, you need to send the query to all partitions, and combine all the\nThis approach to querying a partitioned database is sometimes known as scatter/\nall use document-partitioned secondary indexes.\nthat you structure your partitioning scheme so that secondary index queries can be\nPartitioning and Secondary Indexes \nPartitioning secondary indexes by term.\nPartitioning Secondary Indexes by Term\nRather than each partition having its own secondary index (a local index), we can\nconstruct a global index that covers data in all partitions.\nA global index must also be partitioned, but it can be partitioned\nunder color:red in the index, but the index is partitioned so that colors starting with\nThe index on the make of car is partitioned similarly (with the partition\nmines the partition of the index.\nAs before, we can partition the index by the term itself, or using a hash of the term.\nThe advantage of a global (term-partitioned) index over a document-partitioned\nall partitions, a client only needs to make a request to the partition containing the\nChapter 6: Partitioning\npartitions of the index (every term in the document might be on a different partition,\npartitioned index, that would require a distributed transaction across all partitions\nOther uses of global term-partitioned indexes include Riak’s search feature [21] and\nWe will return to the topic of implementing term-partitioned secondary indexes\nRebalancing Partitions\nRebalancing Partitions \nThere are a few different ways of assigning partitions to nodes [23].\nFixed number of partitions\nare nodes, and assign several partitions to each node.\nning on a cluster of 10 nodes may be split into 1,000 partitions from the outset so that\napproximately 100 partitions are assigned to each node.\nNow, if a node is added to the cluster, the new node can steal a few partitions from\nevery existing node until partitions are fairly distributed once again.\nOnly entire partitions are moved between nodes.\nThe number of partitions does not\nchange, nor does the assignment of keys to partitions.\nthe assignment of partitions to nodes.\nChapter 6: Partitioning\nAdding a new node to a database cluster with multiple partitions per node.\nassigning more partitions to nodes that are more powerful, you can force those nodes\nIn this configuration, the number of partitions is usually fixed when the database is\nIf partitions are\nwhich can be hard to achieve if the number of partitions is fixed but the dataset size\nRebalancing Partitions \nDynamic partitioning\nFor databases that use key range partitioning (see “Partitioning by Key Range” on\nEach partition is assigned to one node, and each node can handle multiple partitions,\nlike in the case of a fixed number of partitions.\nAfter a large partition has been split,\nDynamic partitioning is not only suitable for key range–partitioned data, but can\nequally well be used with hash-partitioned data.\nboth key-range and hash partitioning, and it splits partitions dynamically in either\nPartitioning proportionally to nodes\nChapter 6: Partitioning\nboth of these cases, the number of partitions is independent of the number of nodes.\nincrease the number of nodes, the partitions become smaller again.\nsandra, 256 partitions per node by default), the new node ends up taking a fair share\nically when to move partitions from one node to another, without any administrator\ninteraction) and fully manual (the assignment of partitions to nodes is explicitly con‐\nRebalancing Partitions \nWe have now partitioned our dataset across multiple nodes running on multiple\nAs partitions are rebalanced,\nthe assignment of partitions to nodes changes.\nnode coincidentally owns the partition to which the request applies, it can handle\nin the assignment of partitions to nodes?\nChapter 6: Partitioning\npartitions to nodes.\nWhenever a partition changes\nUsing ZooKeeper to keep track of assignment of partitions to nodes.\nand that node forwards them to the appropriate node for the requested partition\nassignment of partitions to nodes, so it is often sufficient to use DNS for this purpose.\nscatter/gather queries in the case of document-partitioned secondary indexes).\nChapter 6: Partitioning",
      "keywords": [
        "partitions",
        "partition",
        "Secondary Indexes",
        "node",
        "Partitioning Secondary Indexes",
        "index",
        "secondary",
        "indexes",
        "number",
        "data",
        "secondary index",
        "number of partitions",
        "database",
        "Partitioning Secondary",
        "key"
      ],
      "concepts": [
        "partitioning",
        "partition",
        "node",
        "indexes",
        "indexing",
        "data",
        "key",
        "keys",
        "automatically",
        "automat"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 25,
          "title": "",
          "score": 0.977,
          "base_score": 0.827,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 12,
          "title": "",
          "score": 0.677,
          "base_score": 0.527,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 53,
          "title": "",
          "score": 0.655,
          "base_score": 0.505,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 51,
          "title": "",
          "score": 0.64,
          "base_score": 0.49,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 11,
          "title": "",
          "score": 0.619,
          "base_score": 0.469,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "partitions",
          "partitioning",
          "secondary",
          "partitions nodes",
          "secondary indexes"
        ],
        "semantic": [],
        "merged": [
          "partitions",
          "partitioning",
          "secondary",
          "partitions nodes",
          "secondary indexes"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2947247626490586,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.202355+00:00"
      }
    },
    {
      "chapter_number": 25,
      "title": "Segment 25 (pages 239-247)",
      "start_page": 239,
      "end_page": 247,
      "summary": "• Key range partitioning, where keys are sorted, and a partition owns all the keys\n• Hash partitioning, where a hash function is applied to each key, and a partition\npartitioning can also be used.\npart of the key to identify the partition and another part for the sort order.\n• Document-partitioned indexes (local indexes), where the secondary indexes are\nsingle partition needs to be updated on write, but a read of the secondary index\n• Term-partitioned indexes (global indexes), where the secondary indexes are parti‐\nten, several partitions of the secondary index need to be updated; however, a read\nto several partitions can be difficult to reason about: for example, what happens if the\nwrite to one partition succeeds, but another fails?\nPerformance Database Systems,” Communications of the ACM, volume 35, number 6,\nApache Cassandra 2.0,” datastax.com, September 12, 2013.\nChapter 6: Partitioning\n[16] Richard Low: “The Sweet Spot for Cassandra Secondary Indexing,” wentnet.com,\nBurleson: “Object Partitioning in Oracle,” dba-oracle.com, November\n[27] Brandon Williams: “Virtual Nodes in Cassandra 1.2,” datastax.com, December\n[28] Richard Jones: “libketama: Consistent Hashing Library for Memcached Clients,”\nMapReduce Systems,” Foundations and Trends in Databases, volume 5, number 1,\nTransactions\nTransactions\n• The database software or hardware may fail at any time (including in the middle\ndatabase, or one database node from another.\n• Several clients may write to the database at the same time, overwriting each\nA transaction is a way for an application to group several reads and writes\nConceptually, all the reads and writes in a transaction are\nexecuted as one operation: either the entire transaction succeeds (commit) or it fails\nWith transactions, error\naccessing a database.\nBy using transactions, the application is free to ignore certain\nNot every application needs transactions, and sometimes there are advantages to\nweakening transactional guarantees or abandoning them entirely (for example, to\nachieved without transactions.\nHow do you figure out whether you need transactions?\ntion, we first need to understand exactly what safety guarantees transactions can pro‐\nAlthough transactions seem\nconditions that can occur and how databases implement isolation levels such as read\nThis chapter applies to both single-node and distributed databases; in Chapter 8 we\ntransactions.\nChapter 7: Transactions\nof this new generation of databases abandoned transactions entirely, or redefined the\nthe details of the guarantees that transactions can provide—both in normal operation\nknown acronym ACID, which stands for Atomicity, Consistency, Isolation, and Dura‐\nHowever, in practice, one database’s implementation of ACID does not equal\nthe transaction is aborted and the database must discard or undo any writes it has\nmade so far in that transaction.\nAtomicity simplifies this problem: if a transaction was aborted, the\nThe ability to abort a transaction on error and have all writes from that transaction\n• Consistent hashing is an approach to partitioning that some systems use for reba‐\n• In the CAP theorem (see Chapter 9), the word consistency is used to mean linear‐\n• In the context of ACID, consistency refers to an application-specific notion of the\ndatabase being in a “good state.”\nChapter 7: Transactions\nIf a transaction starts with a\ndatabase that is valid according to these invariants, and any writes during the transac‐\nand it’s the application’s responsibility to define its transactions correctly so that they\nThis is not something that the database can guarantee: if you\nwrite bad data that violates your invariants, the database can’t stop you.\ndefines what data is valid or invalid—the database only stores it.)\nAtomicity, isolation, and durability are properties of the database, whereas consis‐\non the database’s atomicity and isolation properties in order to achieve consistency,\nbut it’s not up to the database alone.\nMost databases are accessed by several clients at the same time.\nthey are reading and writing different parts of the database, but if they are accessing\nthe same database records, you can run into concurrency problems (race conditions).\nincrement operation built into the database).\nIsolation in the sense of ACID means that concurrently executing transactions are\nThe classic database\ntextbooks formalize isolation as serializability, which means that each transaction can\npretend that it is the only transaction running on the entire database.\nThe database\nensures that when the transactions have committed, the result is the same as if they",
      "keywords": [
        "database",
        "Transactions",
        "ACID",
        "application",
        "partitioning",
        "partition",
        "Systems",
        "isolation",
        "Database Systems",
        "Parallel Database Systems",
        "Cassandra",
        "Consistency",
        "Atomicity",
        "data",
        "Distributed Systems"
      ],
      "concepts": [
        "database",
        "transactions",
        "transaction",
        "partitioning",
        "partition",
        "consistent",
        "consistency",
        "atom",
        "atomic",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 24,
          "title": "",
          "score": 0.977,
          "base_score": 0.827,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 12,
          "title": "",
          "score": 0.658,
          "base_score": 0.508,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 51,
          "title": "",
          "score": 0.638,
          "base_score": 0.488,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 11,
          "title": "",
          "score": 0.627,
          "base_score": 0.477,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 53,
          "title": "",
          "score": 0.608,
          "base_score": 0.458,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "transactions",
          "database",
          "transaction",
          "partitioning",
          "acid"
        ],
        "semantic": [],
        "merged": [
          "transactions",
          "database",
          "transaction",
          "partitioning",
          "acid"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.34440829964123365,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.202417+00:00"
      }
    },
    {
      "chapter_number": 26,
      "title": "Segment 26 (pages 248-255)",
      "start_page": 248,
      "end_page": 255,
      "summary": "The purpose of a database system is to provide a safe place where data can be stored\nIn a single-node database, durability typically means that the data has been written to\norder to provide a durability guarantee, a database must wait until these writes or\nreplications are complete before reporting a transaction as successfully committed.\nChapter 7: Transactions\n• If you write to disk and the machine dies, even though your data isn’t lost, it is\n• Data on disk can gradually become corrupted without this being detected [17].\nTo recap, in ACID, atomicity and isolation describe what the database should do if a\nclient makes several writes within the same transaction:\nIf an error occurs halfway through a sequence of writes, the transaction should\nConcurrently running transactions shouldn’t interfere with each other.\nexample, if one transaction makes several writes, then another transaction should\nSuch multi-object transactions are often needed if several pieces of\nChapter 7: Transactions\nIf the TCP connection is interrupted, the transaction must be aborted.\nViolating isolation: one transaction reads another transaction’s uncommit‐\ncourse of the transaction, the contents of the mailbox and the unread counter might\nIn an atomic transaction, if the update to the counter fails, the\ntransaction is aborted and the inserted email is rolled back.\nAtomicity ensures that if an error occurs any prior writes from that transac‐\nMulti-object transactions require some way of determining which read and write\noperations belong to the same transaction.\nconsidered to be part of the same transaction.iii\nSingle-object writes\nAtomicity and isolation also apply when a single object is being changed.\nple, imagine you are writing a 20 KB JSON document to a database:\n• If another client reads that document while the write is in progress, will it see a\naim to provide atomicity and isolation on the level of a single object (such as a key-\nSome databases also provide more complex atomic operations,iv such as an increment\nThese single-object operations are useful, as they can prevent lost updates when sev‐\neral clients try to write to the same object concurrently (see “Preventing Lost\nHowever, they are not transactions in the usual sense of the\nA transaction is usually understood as a mechanism for\nChapter 7: Transactions\nThe need for multi-object transactions\nMany distributed datastores have abandoned multi-object transactions because they\nnothing that fundamentally prevents transactions in a distributed database, and we\nwill discuss implementations of distributed transactions in Chapter 9.\nBut do we need multi-object transactions at all?\nany application with only a key-value data model and single-object operations?\nHowever, in many other cases writes to several different objects need to be\n• In a relational data model, a row in one table often has a foreign key reference to\nto other vertices.) Multi-object transactions allow you to ensure that these refer‐\n• In a document data model, the fields that need to be updated together are often\ntransactions are needed when updating a single document.\n• In databases with secondary indexes (almost everything except pure key-value\nindexes are different database objects from a transaction point of view: for exam‐\nple, without transaction isolation, it’s possible for a record to appear in one index\nSuch applications can still be implemented without transactions.\nA key feature of a transaction is that it can be aborted and safely retried if an error\nAlthough retrying an aborted transaction is a simple and effective error handling\n• If the transaction actually succeeded, but the network failed while the server tried\nthen retrying the transaction causes it to be performed twice—unless you have an\n• If the error is due to overload, retrying the transaction will make the problem\n• If the transaction also has side effects outside of the database, those side effects\nmay happen even if the transaction is aborted.\n• If the client process fails while retrying, any data it was trying to write to the\nChapter 7: Transactions\nIf two transactions don’t touch the same data, they can safely be run in parallel,\ncome into play when one transaction reads data that is concurrently modified by\ntion developers by providing transaction isolation.\nisolation means that the database guarantees that transactions have the same effect as\nConcurrency bugs caused by weak transaction isolation are not just a theoretical",
      "keywords": [
        "transaction",
        "isolation",
        "data",
        "database",
        "multi-object transactions",
        "transaction isolation",
        "Weak Isolation Levels",
        "n’t",
        "Isolation Levels",
        "Transactions Weak Isolation",
        "weak transaction isolation",
        "error",
        "Weak Isolation",
        "writes",
        "disk"
      ],
      "concepts": [
        "transaction",
        "transactions",
        "databases",
        "data",
        "concurrency",
        "objects",
        "atomic",
        "write",
        "writing",
        "isolated"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 54,
          "title": "",
          "score": 0.93,
          "base_score": 0.78,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 27,
          "title": "",
          "score": 0.849,
          "base_score": 0.699,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 29,
          "title": "",
          "score": 0.836,
          "base_score": 0.686,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 30,
          "title": "",
          "score": 0.826,
          "base_score": 0.676,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 28,
          "title": "",
          "score": 0.801,
          "base_score": 0.651,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "transaction",
          "transactions",
          "isolation",
          "object",
          "multi object"
        ],
        "semantic": [],
        "merged": [
          "transaction",
          "transactions",
          "isolation",
          "object",
          "multi object"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4241042388496239,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.202477+00:00"
      }
    },
    {
      "chapter_number": 27,
      "title": "Segment 27 (pages 256-263)",
      "start_page": 256,
      "end_page": 263,
      "summary": "Read Committed\nThe most basic level of transaction isolation is read committed.v It makes two guaran‐\n1. When reading from the database, you will only see data that has been committed\nTransactions running at the read committed isolation level must prevent dirty reads.\nThis means that any writes by a transaction only become visible to others when that\ntransaction commits (and then all of its writes become visible at once).\nNo dirty reads: user 2 sees the new value for x only after user 1’s transaction\n• If a transaction needs to update several objects, a dirty read means that another\ntransaction may see some of the updates but not others.\n• If a transaction aborts, any writes it has made need to be rolled back (like in\nIf the database allows dirty reads, that means a transaction may see\nWhat happens if two transactions concurrently try to update the same object in a\nHowever, what happens if the earlier write is part of a transaction that has not yet\nTransactions running at the read committed isolation level must prevent\n• If transactions update multiple objects, dirty writes can lead to a bad outcome.\nfirst transaction has committed, so it’s not a dirty write.\nWith dirty writes, conflicting writes from different transactions can be\nRead committed is a very popular isolation level.\nMost commonly, databases prevent dirty writes by using row-level locks: when a\nOnly one transaction can hold the lock for any given object; if\nanother transaction wants to write to the same object, it must wait until the first\ntransaction is committed or aborted before it can acquire the lock and continue.\nlocking is done automatically by databases in read committed mode (or stronger iso‐\nrequire any transaction that wants to read an object to briefly acquire the lock and\ntime the lock would be held by the transaction that has made the write).\nbecause one long-running write transaction can force many read-only transactions to\nof read-only transactions and is bad for operability: a slowdown in one part of an\nAt the time of writing, the only mainstream databases that use locks for read committed isolation are IBM\nmitted value and the new value set by the transaction that currently holds the write\nWhile the transaction is ongoing, any other transactions that read the object are\nOnly when the new value is committed do transactions\nfor atomicity), it prevents reading the incomplete results of transactions, and it pre‐\nwith read committed.\nRead skew: Alice observes the database in an inconsistent state.\ntransaction is being processed, she may see one account balance at a time before the\nbalance of account 1 again at the end of the transaction, she would see a different\nunder read committed isolation: the account balances that Alice saw were indeed\ncommitted at the time when she read them.\neach transaction reads from a consistent snapshot of the database—that is, the trans‐\nSnapshot isolation is a boon for long-running, read-only queries such as backups and\nWhen a transaction\nLike read committed isolation, implementations of snapshot isolation typically use\nwrite locks to prevent dirty writes (see “Implementing read committed” on page 236),\nwhich means that a transaction that makes a write can block the progress of another\ntransaction that writes to the same object.\nlong-running read queries on a consistent snapshot at the same time as processing\nIf a database only needed to provide read committed isolation, but not snapshot iso‐\nsupport snapshot isolation typically use MVCC for their read committed isolation\neach query, while snapshot isolation uses the same snapshot for an entire transaction.\nWhenever a transaction\nwrites anything to the database, the data it writes is tagged with the transaction ID of\nWhen a transaction reads from the database, transaction IDs are used to decide\n1. At the start of each transaction, the database makes a list of all the other transac‐\n2. Any writes made by aborted transactions are ignored.\ntransactions have committed.\ndeletion had not yet committed at the time when the reader’s transaction started.\ning to read values that (from other transactions’ point of view) have long been over‐\nobject versions that are not visible to the current transaction.\ntion removes old object versions that are no longer visible to any transaction, the cor‐",
      "keywords": [
        "transaction",
        "read committed isolation",
        "Read Committed",
        "read",
        "Committed",
        "isolation",
        "Snapshot Isolation",
        "dirty reads",
        "committed isolation",
        "database",
        "dirty",
        "writes",
        "committed isolation level",
        "dirty writes",
        "prevent dirty reads"
      ],
      "concepts": [
        "transaction",
        "transactions",
        "read",
        "writes",
        "writing",
        "databases",
        "committed",
        "commits",
        "snapshot",
        "indexes"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 26,
          "title": "",
          "score": 0.849,
          "base_score": 0.699,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 28,
          "title": "",
          "score": 0.788,
          "base_score": 0.638,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 30,
          "title": "",
          "score": 0.786,
          "base_score": 0.636,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 29,
          "title": "",
          "score": 0.777,
          "base_score": 0.627,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 54,
          "title": "",
          "score": 0.711,
          "base_score": 0.561,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "committed",
          "transaction",
          "read committed",
          "dirty",
          "isolation"
        ],
        "semantic": [],
        "merged": [
          "committed",
          "transaction",
          "read committed",
          "dirty",
          "isolation"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3430081167344208,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.202529+00:00"
      }
    },
    {
      "chapter_number": 28,
      "title": "Segment 28 (pages 264-271)",
      "start_page": 264,
      "end_page": 271,
      "summary": "that does not overwrite pages of the tree when they are updated, but instead creates a\ntransaction IDs because subsequent writes cannot modify an existing B-tree; they can\nSnapshot isolation is a useful isolation level, especially for read-only transactions.\nPostgreSQL and MySQL call their snapshot isolation level repeatable read\nPreventing Lost Updates\nrently writing transactions.\nThe best known of these is the lost update problem, illus‐\nThe lost update problem can occur if an application reads some value from the data‐\nbase, modifies it, and writes back the modified value (a read-modify-write cycle).\n• Incrementing a counter or updating an account balance (requires reading the\ncurrent value, calculating the new value, and writing back the updated value)\nAtomic write operations\nMany databases provide atomic update operations, which remove the need to imple‐\nment read-modify-write cycles in application code.\nwhen it is read so that no other transaction can read it until the update has been\nwrite code that performs unsafe read-modify-write cycles instead of using atomic\nAnother option for preventing lost updates, if the database’s built-in atomic opera‐\nmodify-write cycle, and if any other transaction tries to concurrently read the same\nobject, it is forced to wait until the first read-modify-write cycle has completed.\nExplicitly locking rows to prevent lost updates\nFOR UPDATE; \nUPDATE figures SET position = 'c4' WHERE id = 1234;\nThe FOR UPDATE clause indicates that the database should take a lock on all rows\nAutomatically detecting lost updates\nAtomic operations and locks are ways of preventing lost updates by forcing the read-\nin parallel and, if the transaction manager detects a lost update, abort the transaction\nand force it to retry its read-modify-write cycle.\nlost update has occurred and abort the offending transaction.\nInnoDB’s repeatable read does not detect lost updates [23].\nargue that a database must prevent lost updates in order to qualify as providing snap‐\nLost update detection is a great feature, because it doesn’t require application code to\ntion and thus introduce a bug, but lost update detection happens automatically and is\nIn databases that don’t provide transactions, you sometimes find an atomic compare-\npurpose of this operation is to avoid lost updates by allowing an update to happen\nmatch what you previously read, the update has no effect, and the read-modify-write\nFor example, to prevent two users concurrently updating the same wiki page, you\nUPDATE wiki_pages SET content = 'new content'\nthis statement may not prevent lost updates, because the condition may be true even\nthough another concurrent write is occurring.\nIn replicated databases (see Chapter 5), preventing lost updates takes on another\nto be taken to prevent lost updates.\napproach in such replicated databases is to allow concurrent writes to create several\nprevent lost updates across replicas.\nWhen a value is concurrently updated by differ‐\nupdates are lost [39].\nlost updates, as discussed in “Last write wins (discarding concurrent writes)” on page\nIn the previous sections we saw dirty writes and lost updates, two kinds of race condi‐\ntions that can occur when different transactions concurrently try to write to the same\nor atomic write operations.\nbetween concurrent writes.\nTo begin, imagine this example: you are writing an application for doctors to manage\nIt is neither a dirty write nor a lost update,\nbecause the two transactions are updating two different objects (Alice’s and Bob’s on-\nYou can think of write skew as a generalization of the lost update problem.\nskew can occur if two transactions read the same objects, and then update some of\nthose objects (different transactions may update different objects).\nwhere different transactions update the same object, you get a dirty write or lost\nWe saw that there are various different ways of preventing lost updates.\n• The automatic detection of lost updates that you find in some implementations\nof snapshot isolation unfortunately doesn’t help either: write skew is not auto‐\nmatically preventing write skew requires true serializable isolation (see “Serializa‐\nAND shift_id = 1234 FOR UPDATE; \nUPDATE doctors\nAs before, FOR UPDATE tells the database to lock all rows returned by this\nMore examples of write skew\nIn Example 7-1, we used a lock to prevent lost updates (that is, making sure that",
      "keywords": [
        "Lost Updates",
        "update",
        "prevent lost updates",
        "Lost",
        "Snapshot isolation",
        "write",
        "isolation",
        "isolation levels",
        "Write Skew",
        "Repeatable read",
        "transactions",
        "read",
        "atomic operations",
        "atomic",
        "lost update problem"
      ],
      "concepts": [
        "updated",
        "updates",
        "write",
        "writing",
        "transaction",
        "transactions",
        "concurrent",
        "concurrency",
        "different",
        "differ"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 30,
          "title": "",
          "score": 0.821,
          "base_score": 0.671,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 26,
          "title": "",
          "score": 0.801,
          "base_score": 0.651,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 27,
          "title": "",
          "score": 0.788,
          "base_score": 0.638,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 29,
          "title": "",
          "score": 0.732,
          "base_score": 0.582,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 54,
          "title": "",
          "score": 0.715,
          "base_score": 0.565,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "lost",
          "lost updates",
          "update",
          "updates",
          "lost update"
        ],
        "semantic": [],
        "merged": [
          "lost",
          "lost updates",
          "update",
          "updates",
          "lost update"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3653770963730838,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.202586+00:00"
      }
    },
    {
      "chapter_number": 29,
      "title": "Segment 29 (pages 272-282)",
      "start_page": 272,
      "end_page": 282,
      "summary": "You may use a transaction to\nDELETE) to the database and commits the transaction.\nChapter 7: Transactions\nthe rows returned in step 1, so we could make the transaction safe and avoid write\nThis effect, where a write in one transaction changes the result of a search query in\nanother transaction, is called a phantom [3].\nread-only queries, but in read-write transactions like the examples we discussed,\nNow a transaction that wants to create a booking can lock (SELECT FOR UPDATE) the\nthat even though transactions may execute in parallel, the end result is the same as if\n• Literally executing transactions in a serial order (see “Actual Serial Execution” on\n• Two-phase locking (see “Two-Phase Locking (2PL)” on page 257), which for sev‐\ndatabases; in Chapter 9 we will examine how they can be generalized to transactions\nentirely: to execute only one transaction at a time, in serial order, on a single thread.\nflicts between transactions: the resulting isolation is by definition serializable.\nChapter 7: Transactions\naround 2007—decided that a single-threaded loop for executing transactions was fea‐\nWhen all data that a transaction needs to access is in memory, transactions\n• Database designers realized that OLTP transactions are usually short and only\nThe approach of executing transactions serially is implemented in VoltDB/H-Store,\nEncapsulating transactions in stored procedures\nbase transaction needs to wait for input from a user, the database needs to support a\ntions short by avoiding interactively waiting for a user within a transaction.\nthe database and only process one transaction at a time, the throughput would be\ntion to issue the next query for the current transaction.\nnecessary to process multiple transactions concurrently in order to get reasonable\nFor this reason, systems with single-threaded serial transaction processing don’t\nthe entire transaction code to the database ahead of time, as a stored procedure.\ndata required by a transaction is in memory, the stored procedure can execute very\nThe difference between an interactive transaction and a stored procedure\n(using the example transaction of Figure 7-8).\nChapter 7: Transactions\nuses Lua. With stored procedures and in-memory data, executing all transactions on a single\nVoltDB also uses stored procedures for replication: instead of copying a transaction’s\nIf a transaction needs to use the\nExecuting all transactions serially makes concurrency control much simpler, but lim‐\nits the transaction throughput of the database to the speed of a single CPU core on a\nRead-only transactions may execute elsewhere, using snapshot isola‐\ntion, but for applications with high write throughput, the single-threaded transaction\nx. If a transaction needs to access data that’s not in memory, the best solution may be to abort the transac‐\nof partitioning your dataset so that each transaction only needs to read and write data\nwithin a single partition, then each partition can have its own transaction processing\nHowever, for any transaction that needs to access multiple partitions, the database\nvastly slower than single-partition transactions.\nWhether transactions can be single-partition depends very much on the structure of\nSerial execution of transactions has become a viable way of achieving serializable iso‐\nin a single-threaded transaction, the system would get very slow.x\n• Cross-partition transactions are possible, but there is a hard limit to the extent to\nChapter 7: Transactions\ndatabases: two-phase locking (2PL).xi\nwrites” on page 235): if two transactions concurrently try to write to the same object,\ntransaction (aborted or committed) before it may continue.\neral transactions are allowed to concurrently read the same object as long as nobody\n• If transaction A has read an object and transaction B wants to write to that\n• If transaction A has written an object and transaction B wants to read that object,\ndifference between snapshot isolation and two-phase locking.\n• If a transaction wants to read an object, it must first acquire the lock in shared\nSeveral transactions are allowed to hold the lock in shared mode simulta‐\nneously, but if another transaction already has an exclusive lock on the object,\nthese transactions must wait.\n• If a transaction wants to write to an object, it must first acquire the lock in exclu‐\nNo other transaction may hold the lock at the same time (either in\ntransaction must wait.\n• If a transaction first reads and then writes an object, it may upgrade its shared\n• After a transaction has acquired the lock, it must continue to hold the lock until\nthe end of the transaction (commit or abort).\nthe locks are acquired, and the second phase (at the end of the transaction) is\nSince so many locks are in use, it can happen quite easily that transaction A is stuck\nwaiting for transaction B to release its lock, and vice versa.\nThe database automatically detects deadlocks between transactions and\nThe aborted transaction\ntimes of queries are significantly worse under two-phase locking than under weak\nBy design, if two concurrent transactions\nChapter 7: Transactions\nIt may take just one slow transaction, or one\ntransaction that accesses a lot of data and acquires many locks, to cause the rest of the\nAlthough deadlocks can happen with the lock-based read committed isolation level,\nwhen a transaction is aborted due to deadlock and is retried, it needs to do its work\nIn the meeting room booking example this means that if one transaction has\nExample 7-2), another transaction is not allowed to concurrently insert or update\n• If transaction A wants to read objects matching some condition, like in that\nSELECT query, it must acquire a shared-mode predicate lock on the conditions of\nIf another transaction B currently has an exclusive lock on any object\n• If transaction A wants to insert, update, or delete any object, it must first check\nthere is a matching predicate lock held by transaction B, then A must wait until B\nlocking includes predicate locks, the database prevents all forms of write skew and\ntransactions, checking for matching locks becomes time-consuming.\nmost databases with 2PL actually implement index-range locking (also known as next-\nyou can approximate it by locking bookings for room 123 at any time, or you can\napproximate it by locking all rooms (not just room 123) between noon and 1 p.m. This is safe, because any write that matches the original predicate will definitely also\nindex entry, indicating that a transaction has searched for bookings of room 123.\n• Alternatively, if the database uses a time-based index to find existing bookings, it\nNow, if another transaction wants to insert, update, or delete a booking for\nChapter 7: Transactions",
      "keywords": [
        "transaction",
        "lock",
        "database",
        "time",
        "isolation",
        "write",
        "Two-phase locking",
        "object",
        "snapshot isolation",
        "room",
        "predicate lock",
        "serializable isolation",
        "data",
        "write skew",
        "query"
      ],
      "concepts": [
        "transaction",
        "transactions",
        "locking",
        "database",
        "concurrently",
        "concurrency",
        "isolation",
        "uses",
        "time",
        "timing"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 26,
          "title": "",
          "score": 0.836,
          "base_score": 0.686,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 30,
          "title": "",
          "score": 0.825,
          "base_score": 0.675,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 27,
          "title": "",
          "score": 0.777,
          "base_score": 0.627,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 39,
          "title": "",
          "score": 0.752,
          "base_score": 0.602,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 54,
          "title": "",
          "score": 0.752,
          "base_score": 0.602,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "transaction",
          "transactions",
          "lock",
          "locking",
          "transaction wants"
        ],
        "semantic": [],
        "merged": [
          "transaction",
          "transactions",
          "lock",
          "locking",
          "transaction wants"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3834691803280733,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.202644+00:00"
      }
    },
    {
      "chapter_number": 30,
      "title": "Segment 30 (pages 283-290)",
      "start_page": 283,
      "end_page": 290,
      "summary": "Serializable Snapshot Isolation (SSI)\neach transaction having an exclusive lock on the entire database (or one partition of\nthe database) for the duration of the transaction.\nBy contrast, serializable snapshot isolation is an optimistic concurrency control tech‐\nWhen a transaction wants to commit, the database checks\nOnly transactions that executed serializably\nproportion of transactions needing to abort.\nfor example, if several transactions concurrently want to increment a counter, it\nread in the same transaction), so the concurrent increments can all be applied\ntransaction are made from a consistent snapshot of the database (see “Snapshot Isola‐\nwhich transactions to abort.\nPhantoms” on page 246), we observed a recurring pattern: a transaction reads some\ntime the transaction commits, because the data may have been modified in the mean‐\nLater, when the transaction wants to commit, the original data may have\n(the premise) means that writes in that transaction may be invalid.\nChapter 7: Transactions\nwhich a transaction may have acted on an outdated premise and abort the transac‐\nWhen a transaction reads from a consistent snap‐\nthat the write that was ignored when reading from the consistent snapshot has now\nDetecting when a transaction reads outdated values from an MVCC\nIn order to prevent this anomaly, the database needs to track when a transaction\nignores another transaction’s writes due to MVCC visibility rules.\nIf so, the transaction must be aborted.\nWhy not abort transaction 43 immediately when the\nWell, if transaction 43 was a read-only transaction, it wouldn’t\ntion 43 makes its read, the database doesn’t yet know whether that transaction is\nMoreover, transaction 42 may yet abort or may still be\nuncommitted at the time when transaction 43 is committed, and so the read may\nThe second case to consider is when another transaction modifies data after it has\nIn serializable snapshot isolation, detecting when one transaction modifies\nanother transaction’s reads.\nhere, except that SSI locks don’t block other transactions.\nChapter 7: Transactions\nrecord the fact that transactions 42 and 43 read this data.\nfor a while: after a transaction has finished (committed or aborted), and all concur‐\nrent transactions have finished, the database can forget what data it read.\nWhen a transaction writes to the database, it must look in the indexes for any other\ntransactions that have recently read the affected data.\nreaders have committed, the lock acts as a tripwire: it simply notifies the transactions\nIn Figure 7-11, transaction 43 notifies transaction 42 that its prior read is outdated,\nTransaction 42 is first to commit, and it is successful: although trans‐\nHowever, when transaction 43 wants to commit, the conflicting write from 42\nPerformance of serializable snapshot isolation\nFor example, one trade-off is the granularity at which transactions’ reads and writes\nIf the database keeps track of each transaction’s activity in great detail, it\nIn some cases, it’s okay for a transaction to read information that was overwritten by\nis that one transaction doesn’t need to block waiting for locks held by another trans‐\nread and write data in multiple partitions while ensuring serializable isolation [54].\ntransaction that reads and writes data over a long period of time is likely to run into\nconflicts and abort, so SSI requires that read-write transactions be fairly short (long-\nrunning read-only transactions may be okay).\nto slow transactions than two-phase locking or serial execution.\nmanage without transactions.\nWithout transactions, it becomes very difficult to\ndiscussed several widely used isolation levels, in particular read committed, snapshot\nOne client reads another client’s writes before they have been committed.\nAlmost all transaction implementations prevent dirty writes.\nis most commonly prevented with snapshot isolation, which allows a transaction\nChapter 7: Transactions\nTwo clients concurrently perform a read-modify-write cycle.\nA transaction reads something, makes a decision based on the value it saw, and\nA transaction reads objects that match some search condition.\napproaches to implementing serializable transactions:\nSerializable snapshot isolation (SSI)\nWhen a transaction wants to commit, it is checked, and it is\n“The need for multi-object transactions” on page 231, transactions are a valuable\nTransactions in distributed databases open a new set of\nCook: “ACID Versus BASE for Database Transactions,” johndcook.com,\nIsolation Serializable,” ACM Transactions on Database Systems, volume 30, number\nChapter 7: Transactions",
      "keywords": [
        "transaction",
        "Snapshot Isolation",
        "Serializable Snapshot Isolation",
        "database",
        "Isolation",
        "Snapshot",
        "transaction reads",
        "read",
        "Serializable Snapshot",
        "SSI",
        "data",
        "write",
        "concurrency control",
        "Serializable",
        "serializable isolation"
      ],
      "concepts": [
        "transaction",
        "read",
        "database",
        "lock",
        "write",
        "performance",
        "perform",
        "concurrently",
        "data",
        "isolation"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 26,
          "title": "",
          "score": 0.826,
          "base_score": 0.676,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 29,
          "title": "",
          "score": 0.825,
          "base_score": 0.675,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 28,
          "title": "",
          "score": 0.821,
          "base_score": 0.671,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 54,
          "title": "",
          "score": 0.821,
          "base_score": 0.671,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 27,
          "title": "",
          "score": 0.786,
          "base_score": 0.636,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "transaction",
          "transactions",
          "snapshot",
          "serializable",
          "serializable snapshot"
        ],
        "semantic": [],
        "merged": [
          "transaction",
          "transactions",
          "snapshot",
          "serializable",
          "serializable snapshot"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.40412734488257873,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.202711+00:00"
      }
    },
    {
      "chapter_number": 31,
      "title": "Segment 31 (pages 291-298)",
      "start_page": 291,
      "end_page": 298,
      "summary": "Robustness of SSDs Under Power Fault,” at 11th USENIX Conference on File and\n[14] Adam Surak: “When Solid State Drives Are Not That Solid,” blog.algolia.com,\ning Crash-Consistent Applications,” at 11th USENIX Symposium on Operating\n[16] Chris Siebenmann: “Unix’s File Durability Problem,” utcc.utoronto.ca, April 14,\nAnalysis of Data Corruption in the Storage Stack,” at 6th USENIX Conference on File\nProduction: The Expected and the Unexpected,” at 14th USENIX Conference on File\n[19] Don Allison: “SSD Storage – Ignorance of Technology Is No Excuse,” blog.kore‐\n[20] Dave Scherer: “Those Are Not Transactions (Cassandra 2.0),” blog.founda‐\n[21] Kyle Kingsbury: “Call Me Maybe: Cassandra,” aphyr.com, September 24, 2013.\n[22] “ACID Support in Aerospike,” Aerospike, Inc., June 2014.\nCould Have Stolen More!,” reddit.com, February 2, 2014.\nmating the Detection of Snapshot Isolation Anomalies,” at 33rd International Confer‐\n[27] Michael Melanson: “Transactions: The Limits of Isolation,” michaelmelan‐\nIsolation Levels,” at ACM International Conference on Management of Data (SIG‐\nmentations for Distributed Transactions,” PhD Thesis, Massachusetts Institute of\nVirtues and Limitations (Extended Version),” at 40th International Conference on\nsistent Read View Works,” blogs.oracle.com, January 15, 2013.\n[33] Nikita Prokopov: “Unofficial Guide to Datomic Internals,” tonsky.me, May 6,\n[34] Baron Schwartz: “Immutability, MVCC, and Garbage Collection,” xaprb.com,\nStability, Uncommitted Read) with Examples,” mframes.blogspot.co.uk, July 4, 2013.\n[37] Steve Hilker: “Cursor Stability (CS) – IBM DB2 Community,” toadworld.com,\n[38] Nate Wiger: “An Atomic Rant,” nateware.com, February 18, 2010.\n[39] Joel Jacobson: “Riak 2.0: Data Types,” blog.joeljacobson.com, March 23, 2014.\nshot Databases,” at ACM International Conference on Management of Data (SIG‐\ngreSQL,” at 38th International Conference on Very Large Databases (VLDB), August\nConflicts in Bayou, a Weakly Connected Replicated Storage System,” at 15th ACM\n[44] Gary Fredericks: “Postgres Serializability Bug,” github.com, September 2015.\nArchitectural Era (It’s Time for a Complete Rewrite),” at 33rd International Confer‐\ning Architectures,” at Data @Scale Boston, November 2014.\nPerformance, Distributed Main Memory Transaction Processing System,” Proceed‐\n[48] Rich Hickey: “The Architecture of Datomic,” infoq.com, November 2, 2012.\n[49] John Hugg: “Debunking Myths About the VoltDB In-Memory Database,”\n[50] Joseph M.\nof a Database System,” Foundations and Trends in Databases, volume 1, number 2,\nCahill: “Serializable Isolation for Snapshot Databases,” PhD Thesis,\ntributed Databases,” at 3rd International IEEE Computer Software and Applications\nformance Modeling: Alternatives and Implications,” ACM Transactions on Database\nSystems (TODS), volume 12, number 4, pages 609–654, December 1987.\n[54] Dave Rosenthal: “Databases at 14.4MHz,” blog.foundationdb.com, December 10,\nThe Trouble with Distributed Systems\nA recurring theme in the last few chapters has been how systems handle things going\nWorking with distributed systems is fundamentally different from writing software\ngo wrong in a distributed system.\nnel panic, “blue screen of death,” failure to start up).\nIn distributed systems, we are no\nChapter 8: The Trouble with Distributed Systems\nThis nondeterminism and possibility of partial failures is what makes distributed sys‐\nThere is a spectrum of philosophies on how to build large-scale computing systems:\n• At one end of the scale is the field of high-performance computing (HPC).\nsupercomputer, a job typically checkpoints the state of its computation to durable\nThus, a supercomputer is more like a single-node\ncomputer than a distributed system: it deals with partial failure by letting it escalate\n• Supercomputers are typically built from specialized hardware, where each node\n• If the system can tolerate failed nodes and still keep working as a whole, that is a\nform a rolling upgrade (see Chapter 4), restarting one node at a time, while the\nIf we want to make distributed systems work, we must accept the possibility of partial\nEven in smaller systems consisting of only a few nodes, it’s important to think about\nChapter 8: The Trouble with Distributed Systems",
      "keywords": [
        "Mai Zheng",
        "Feng Qin",
        "Mark Lillibridge",
        "System",
        "Distributed Systems",
        "Large Data Bases",
        "ACM International Conference",
        "International Conference",
        "Data",
        "Distributed",
        "USENIX Conference",
        "Isolation",
        "Storage Technologies",
        "Conference",
        "Large Data"
      ],
      "concepts": [
        "systems",
        "blog",
        "computers",
        "computationally",
        "network",
        "data",
        "transactions",
        "transaction",
        "failures",
        "databases"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 6,
          "title": "",
          "score": 0.748,
          "base_score": 0.598,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 4,
          "title": "",
          "score": 0.656,
          "base_score": 0.506,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 1,
          "title": "",
          "score": 0.542,
          "base_score": 0.392,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "",
          "score": 0.532,
          "base_score": 0.382,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 12,
          "title": "",
          "score": 0.516,
          "base_score": 0.366,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "conference",
          "international",
          "com",
          "distributed",
          "distributed systems"
        ],
        "semantic": [],
        "merged": [
          "conference",
          "international",
          "com",
          "distributed",
          "distributed systems"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3153196071187725,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.202773+00:00"
      }
    },
    {
      "chapter_number": 32,
      "title": "Segment 32 (pages 299-308)",
      "start_page": 299,
      "end_page": 308,
      "summary": "from you, but it cannot magically remove delays in the network.\nUnreliable Networks\nbook are shared-nothing systems: i.e., a bunch of machines connected by a network.\nThe network is the only way those machines can communicate—we assume that each\nUnreliable Networks \nmachine’s memory or disk (except by making requests to a service over the network).\nThe internet and most internal networks in datacenters (often Ethernet) are asyn‐\nchronous packet networks.\nIn this kind of network, one node can send a message (a\npacket) to another node, but the network gives no guarantees as to when it will arrive,\n1. Your request may have been lost (perhaps someone unplugged a network cable).\nnetwork or the recipient is overloaded).\nlost on the network (perhaps a network switch has been misconfigured).\ndelayed and will be delivered later (perhaps the network or your own machine is\nNetwork Faults in Practice\nnetwork problems can be surprisingly common, even in controlled environments like\nfound about 12 network faults per month, of which half disconnected a single\nIt found that adding redundant networking gear doesn’t reduce faults as\nwork glitches [14], and well-managed private datacenter networks can be stabler\nple, a problem during a software upgrade for a switch could trigger a network topol‐\nogy reconfiguration, during which network packets could be delayed for more than a\ning faults include a network interface that sometimes drops all inbound packets but\nsends outbound packets successfully [19]: just because a network link works in one\nNetwork partitions\nWhen one part of the network is cut off from the rest due to a net‐\nwork fault, that is sometimes called a network partition or netsplit.\nUnreliable Networks \nEven if network faults are rare in your environment, the fact that faults can occur\nIf the error handling of network faults is not defined and tested, arbitrarily bad things\nunable to serve requests, even when the network recovers [20], or it could even delete\nHandling network faults doesn’t necessarily mean tolerating them: if your network is\nusers while your network is experiencing problems.\nhow your software reacts to network problems and ensure that the system can\n• A load balancer needs to stop sending requests to a node that is dead (i.e., take it\n• If you have access to the management interface of the network switches in your\nsame limitations as other participants of the network.\nA long timeout means a long wait until a node is declared dead (and during this time,\nsuffered a temporary slowdown (e.g., due to a load spike on the node or the network).\nnodes, which places additional load on other nodes and the network.\nUnreliable Networks \nImagine a fictitious system with a network that guaranteed a maximum delay for\ndon’t receive a response within that time, you know that either the network or the\nchronous networks have unbounded delays (that is, they try to deliver packets as\nNetwork congestion and queueing\nWhen driving a car, travel times on road networks often vary most due to traffic con‐\nSimilarly, the variability of packet delays on computer networks is most often\n• If several different nodes simultaneously try to send packets to the same destina‐\ntion, the network switch must queue them up and feed them into the destination\nnetwork link one by one (as illustrated in Figure 8-2).\nOn a busy network link, a\npacket may have to wait a while until it can get a slot (this is called network con‐\npacket is dropped, so it needs to be resent—even though the network is function‐\nbusy, the incoming request from the network is queued by the operating system\nvariability of network delays.\nnetwork link or the receiving node [27].\nsender before the data even enters the network.\nIf several machines send network traffic to the same destination, its switch\nit avoids some of the reasons for variable network delays (although it is still suscepti‐\nAll of these factors contribute to the variability of network delays.\nUnreliable Networks \ncustomers: the network links and switches, and even each machine’s network inter‐\ndistribution of network round-trip times over an extended period, and over many\nDistributed systems would be a lot simpler if we could rely on the network to deliver\nsolve this at the hardware level and make the network reliable so that the software\nTo answer this question, it’s interesting to compare datacenter networks to the tradi‐\ncomputer networks?\nWhen you make a call over the telephone network, it establishes a circuit: a fixed,\nexample, an ISDN network runs at a fixed rate of 4,000 frames per second.\nmuch adoption outside of telephone network core switches.\nmaximum end-to-end latency of the network is fixed.\nCan we not simply make network delays predictable?\nNote that a circuit in a telephone network is very different from a TCP connection: a\nwhatever network bandwidth is available.\nIf datacenter networks and the internet were circuit-switched networks, it would be\nfer from queueing and thus unbounded delays in the network.\nWhy do datacenter networks and the internet use packet switching?\nThus, using circuits for bursty data transfers wastes network capacity and\ndata transfer to the available network capacity.\nThere have been some attempts to build hybrid networks that support both circuit\nUnreliable Networks \nqueueing in the network, although it can still suffer from delays due to link conges‐\nswitching on packet networks, or provide statistically bounded delay [25, 32].\nBy contrast, the internet shares network bandwidth dynamically.\nnetwork switches decide which packet to send (i.e., the bandwidth allocation) from\nVariable delays in networks are not a law of nature, but simply the result of a cost/",
      "keywords": [
        "network",
        "node",
        "distributed systems",
        "packet",
        "Network Faults",
        "System",
        "TCP",
        "Unreliable Networks",
        "network delays",
        "remote node",
        "Distributed Systems network",
        "n’t",
        "response",
        "time",
        "delays"
      ],
      "concepts": [
        "network",
        "packets",
        "delay",
        "tcp",
        "node",
        "time",
        "machines",
        "fault",
        "switch",
        "switches"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 35,
          "title": "",
          "score": 0.645,
          "base_score": 0.495,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 33,
          "title": "",
          "score": 0.601,
          "base_score": 0.451,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 34,
          "title": "",
          "score": 0.569,
          "base_score": 0.419,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 5,
          "title": "",
          "score": 0.553,
          "base_score": 0.403,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 46,
          "title": "",
          "score": 0.505,
          "base_score": 0.505,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "network",
          "networks",
          "delays",
          "packet",
          "unreliable networks"
        ],
        "semantic": [],
        "merged": [
          "network",
          "networks",
          "delays",
          "packet",
          "unreliable networks"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2212668990256689,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.202821+00:00"
      }
    },
    {
      "chapter_number": 33,
      "title": "Segment 33 (pages 309-318)",
      "start_page": 309,
      "end_page": 318,
      "summary": "Unreliable Clocks\nClocks and time are important.\nApplications depend on clocks in various ways to\n1. Has this request timed out yet?\nExamples 1–4 measure durations (e.g., the time interval between a request being sent\nMoreover, each machine on the network has its own clock, which is an actual hard‐\nUnreliable Clocks \nv. Although the clock is called real-time, it has nothing to do with real-time operating systems, as discussed\nIt is possible to synchronize clocks to some degree:\nthe most commonly used mechanism is the Network Time Protocol (NTP), which\nallows the computer clock to be adjusted according to the time reported by a group of\nMonotonic Versus Time-of-Day Clocks\nModern computers have at least two different kinds of clocks: a time-of-day clock and\na monotonic clock.\nTime-of-day clocks\nA time-of-day clock does what you intuitively expect of a clock: it returns the current\ndate and time according to some calendar (also known as wall-clock time).\nTime-of-day clocks are usually synchronized with NTP, which means that a time‐\nHowever, time-of-day clocks also have various oddities, as described in the\nIn particular, if the local clock is too far ahead of the NTP server, it may\nwell as the fact that they often ignore leap seconds, make time-of-day clocks unsuita‐\nTime-of-day clocks have also historically had quite a coarse-grained resolution, e.g.,\nMonotonic clocks\nA monotonic clock is suitable for measuring a duration (time interval), such as a\ntimeout or a service’s response time: clock_gettime(CLOCK_MONOTONIC) on Linux\nand System.nanoTime() in Java are monotonic clocks, for example.\nday clock may jump back in time).\nYou can check the value of the monotonic clock at one point in time, do something,\nand then check the clock again at a later time.\nsense to compare monotonic clock values from two different computers, because they\nNTP may adjust the frequency at which the monotonic clock moves forward (this is\nded up or slowed down by up to 0.05%, but NTP cannot cause the monotonic clock\nThe resolution of monotonic clocks is usually quite\nIn a distributed system, using a monotonic clock for measuring elapsed time (e.g.,\nferent nodes’ clocks and is not sensitive to slight inaccuracies of measurement.\nClock Synchronization and Accuracy\nMonotonic clocks don’t need synchronization, but time-of-day clocks need to be set\naccording to an NTP server or other external time source in order to be useful.\nUnfortunately, our methods for getting a clock to tell the correct time aren’t nearly as\n• The quartz clock in a computer is not very accurate: it drifts (runs faster or\nservers [41], which is equivalent to 6 ms drift for a clock that is resynchronized\nwith a server every 30 seconds, or 17 seconds drift for a clock that is resynchron‐\n• If a computer’s clock differs too much from an NTP server, it may refuse to syn‐\nUnreliable Clocks \n• Some NTP servers are wrong or misconfigured, reporting time that is off by\npause manifests itself as the clock suddenly jumping forward [26].\nSome users deliberately set their hardware clock to an incorrect date and time,\nAs a result, the clock\nthe clock error due to drift can quickly become large.\nRelying on Synchronized Clocks\nclocks may move backward in time, and the time on one node may be quite different\nfrom the time on another node.\nThe same is true with clocks: although they\nincorrect clocks.\nis relying on an accurately synchronized clock, the result is more likely to be silent\nThus, if you use software that requires synchronized clocks, it is essential that you\non clocks: ordering of events across multiple nodes.\nFigure 8-3 illustrates a dangerous use of time-of-day clocks in a database with multi-\nUnreliable Clocks \naccording to the time-of-day clock on the node where the write originated.\nThe clock\n• Database writes can mysteriously disappear: a node with a lagging clock is unable\nto overwrite values previously written by a node with a fast clock until the clock\ndepends on a local time-of-day clock, which may well be incorrect.\nNTP-synchronized clocks, you could send a packet at timestamp 100 ms (according\nFor correct ordering, you would need the clock source to be significantly more\nLogical clocks do not measure the time\nmonotonic clocks, which measure actual elapsed time, are also known as physical\nclocks.\nClock readings have a confidence interval\nYou may be able to read a machine’s time-of-day clock with microsecond or even\nlike a range of times, within a confidence interval: for example, a system may be 95%\nUnreliable Clocks \nwith the server, plus the NTP server’s uncertainty, plus the network round-trip time\ncall clock_gettime(), the return value doesn’t tell you the expected error of the\nreports the confidence interval on the local clock.\nquartz clock was last synchronized with a more accurate clock source.\nSynchronized clocks for global snapshots\nCan we use the timestamps from synchronized time-of-day clocks as transaction IDs?\nwait time as short as possible, Spanner needs to keep the clock uncertainty as small as\ndatacenter, allowing clocks to be synchronized to within about 7 ms [41].\nUsing clock synchronization for distributed transaction semantics is an area of active\nLet’s consider another example of dangerous clock use in a distributed system.\nwhen a node obtains a lease, it knows that it is the leader for some amount of time,\nUnreliable Clocks \nthe current time plus 30 seconds, for example), and it’s being compared to the local\nsystem clock.\nIf the clocks are out of sync by more than a few seconds, this code will\nSecondly, even if we change the protocol to only use the local monotonic clock, there\ntime the request is processed, and another node has already taken over as leader.",
      "keywords": [
        "clock",
        "time",
        "NTP",
        "NTP server",
        "monotonic clock",
        "write",
        "node",
        "Unreliable Clocks",
        "der Gateway Protocol",
        "Clock Synchronization",
        "Synchronized Clocks",
        "systems",
        "network",
        "Monotonic",
        "server"
      ],
      "concepts": [
        "time",
        "timing",
        "clocks",
        "seconds",
        "transactions",
        "transaction",
        "nodes",
        "write",
        "monotonic",
        "systems"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 35,
          "title": "",
          "score": 0.653,
          "base_score": 0.503,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 34,
          "title": "",
          "score": 0.639,
          "base_score": 0.489,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 32,
          "title": "",
          "score": 0.601,
          "base_score": 0.451,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 5,
          "title": "",
          "score": 0.551,
          "base_score": 0.401,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 49,
          "title": "",
          "score": 0.429,
          "base_score": 0.429,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "clock",
          "clocks",
          "monotonic",
          "time day",
          "day"
        ],
        "semantic": [],
        "merged": [
          "clock",
          "clocks",
          "monotonic",
          "time day",
          "day"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.20884434511813235,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.202867+00:00"
      }
    },
    {
      "chapter_number": 34,
      "title": "Segment 34 (pages 319-327)",
      "start_page": 319,
      "end_page": 327,
      "summary": "pause can occur at any time in a process’s execution and can last for an arbitrary\nthe pause depends on the rate at which processes are writing to memory [67].\nmachine), the currently running thread can be paused at any arbitrary point in\n• If the application performs synchronous disk access, a thread may be paused\n• A Unix process can be paused by sending it the SIGSTOP signal, for example by\nsome later time, without the thread even noticing.\nA node in a distributed system must assume that its execution can be paused for a\npause, the rest of the world keeps moving and may even declare the paused node\nEventually, the paused node may continue running,\nResponse time guarantees\nIn many programming languages and operating systems, threads and processes may\npause for an unbounded amount of time, as discussed.\nThese are so-called hard real-time systems.\nIn embedded systems, real-time means that a system is carefully\ndata to clients and stream processing without hard response time\nProviding real-time guarantees in a system requires support from all levels of the\nsoftware stack: a real-time operating system (RTOS) that allows processes to be sched‐\nguages and tools do not provide real-time guarantees).\nreal-time systems is very expensive, and they are most commonly used in safety-\nperformance”—in fact, real-time systems may have lower throughput, since they\nFor most server-side data processing systems, real-time guarantees are simply not\nsive real-time scheduling guarantees.\nAn emerging idea is to treat GC pauses like brief planned outages of a node, and to\nlet other nodes handle requests from clients while one node is collecting its garbage.\nIf the runtime can warn the application that a node soon requires a GC pause, the\napplication can stop sending new requests to that node, wait for it to finish process‐\ntime [70, 71].\nnode can be restarted at a time, and traffic can be shifted away from the node before\nA node in the network cannot know anything for sure—it can\nnode doesn’t respond, there is no way of knowing what state it is in, because prob‐\nlems in the network cannot reliably be distinguished from problems at a node.\nImagine a network with an asymmetric fault: a node is able to receive all messages\nsent to it, but any outgoing messages from that node are dropped or delayed [19].\nEven though that node is working perfectly well, and is receiving requests from other\nnodes, the other nodes cannot hear its responses.\nnodes declare it dead, because they haven’t heard from the node.\nthe messages it is sending are not being acknowledged by other nodes, and so realize\nNevertheless, the node is wrongly declared\ndead by the other nodes, and the semi-disconnected node cannot do anything about\nAll of the node’s threads are preempted by the GC and paused for\nFinally, the GC finishes and the node’s threads continue\nThe other nodes are surprised as the supposedly dead\ntime has passed since it was last talking to the other nodes.\nA distributed system cannot exclusively rely on a single node, because a\nnode may fail at any time, potentially leaving the system stuck and unable to recover.\nnodes (see “Quorums for reading and writing” on page 179): decisions require some\nany one particular node.\nThat includes decisions about declaring nodes dead.\nIf a quorum of nodes declares\nMost commonly, the quorum is an absolute majority of more than half the nodes\ntem to continue working if individual nodes have failed (with three nodes, one failure\ncan be tolerated; with five nodes, two failures can be tolerated).\n• Only one node is allowed to be the leader for a database partition, to avoid split\nImplementing this in a distributed system requires care: even if a node believes that it\nmean a quorum of nodes agrees!\nA node may have formerly been the leader, but if\nthe other nodes declared it dead in the meantime (e.g., due to a network interruption\nIf a node continues acting as the chosen one, even though the majority of nodes have\nSuch a node could send messages to other nodes in its self-appointed capacity, and if\nother nodes believe it, the system as a whole may do something incorrect.\none client at a time, because if multiple clients tried to write to it, the file would\nthe client holding the lease is paused for too long, its lease expires.\nWhen the paused client\nin Figure 8-4, we need to ensure that a node that is under a false belief of being “the\nLet’s assume that every time the lock server grants a lock or lease, it also returns a\nfencing token, which is a number that increases every time a lock is granted (e.g.,\nWe can then require that every time a client sends a\nwrite request to the storage service, it must include its current fencing token.\nIf ZooKeeper is used as lock service, the transaction ID zxid or the node version\nFencing tokens can detect and block a node that is inadvertently acting in error (e.g.,\nHowever, if the node delib‐\nIn this book we assume that nodes are unreliable but honest: they may be slow or\nnetwork delays), but we assume that if a node does respond, it is telling the “truth”: to\nDistributed systems problems become much harder if there is a risk that nodes may\n“lie” (send arbitrary faulty or corrupted responses)—for example, if a node may claim\nof the nodes are malfunctioning and not obeying the protocol, or if malicious attack‐\ncould become corrupted by radiation, leading it to respond to other nodes in\nnode to simply trust another node’s messages, since they may be sent with mali‐\ndata systems, the cost of deploying Byzantine fault-tolerant solutions makes them\nsame software to all nodes, then a Byzantine fault-tolerant algorithm cannot save you.\nthirds of the nodes to be functioning correctly (i.e., if you have four nodes, at most",
      "keywords": [
        "node",
        "system",
        "distributed systems",
        "Byzantine Generals Problem",
        "time",
        "Byzantine",
        "distributed",
        "n’t",
        "Distributed systems problems",
        "Byzantine fault-tolerant",
        "problem",
        "node dead",
        "Byzantine Generals",
        "real-time systems",
        "operating system"
      ],
      "concepts": [
        "node",
        "time",
        "timing",
        "systems",
        "clients",
        "processes",
        "process",
        "processing",
        "pausing",
        "pause"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 35,
          "title": "",
          "score": 0.753,
          "base_score": 0.603,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 33,
          "title": "",
          "score": 0.639,
          "base_score": 0.489,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 32,
          "title": "",
          "score": 0.569,
          "base_score": 0.419,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 5,
          "title": "",
          "score": 0.505,
          "base_score": 0.355,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 60,
          "title": "",
          "score": 0.458,
          "base_score": 0.458,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "node",
          "nodes",
          "paused",
          "time",
          "real time"
        ],
        "semantic": [],
        "merged": [
          "node",
          "nodes",
          "paused",
          "time",
          "real time"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3131088611751914,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.202926+00:00"
      }
    },
    {
      "chapter_number": 35,
      "title": "Segment 35 (pages 328-340)",
      "start_page": 328,
      "end_page": 340,
      "summary": "either: in most systems, if an attacker can compromise one node, they can probably\nMany algorithms have been designed to solve distributed systems problems—for\nto be useful, these algorithms need to tolerate the various faults of distributed systems\nChapter 8: The Trouble with Distributed Systems\nWith regard to timing assumptions, three system models are in common use:\nThe synchronous model assumes bounded network delay, bounded process pau‐\nnot a realistic model of most practical systems, because (as discussed in this\nthe time, but it sometimes exceeds the bounds for network delay, process pauses,\nThis is a realistic model of many systems: most of the time,\nIn this model, an algorithm is not allowed to make any timing assumptions—in\ncommon system models for nodes are:\nIn the crash-stop model, an algorithm may assume that a node can fail in only\nIn the crash-recovery model, nodes are assumed\nFor modeling real systems, the partially synchronous model with crash-recovery\nfaults is generally the most useful model.\nwith that model?\nTo define what it means for an algorithm to be correct, we can describe its properties.\nFor example, the output of a sorting algorithm has the property that for any two dis‐\nSimilarly, we can write down the properties we want of a distributed algorithm to\nA node that requests a fencing token and does not crash eventually receives a\nAn algorithm is correct in some system model if it always satisfies its properties in all\nIf all nodes crash, or all network delays suddenly become infinitely long, then\nChapter 8: The Trouble with Distributed Systems\ntime (for example, a node may have sent a request but not yet received a\nrequire that safety properties always hold, in all possible situations of a system model\nThat is, even if all nodes crash, or the entire network fails, the algorithm must\ntion of the partially synchronous model requires that eventually the system returns to\nSafety and liveness properties and system models are very useful for reasoning about\nthe correctness of a distributed algorithm.\nFor example, algorithms in the crash-recovery model generally assume that data in\nChapter 8: The Trouble with Distributed Systems\ndistributed algorithms rely on timeouts to determine whether a remote node is still\nHowever, timeouts can’t distinguish between network and node failures,\nand variable network delay sometimes causes a node to be falsely suspected of crash‐\nnode to another is by sending it over the unreliable network.\nbeen designed to cope with all the problems in distributed systems.\nSystem,” ACM Queue, volume 11, number 4, pages 80-89, April 2013.\n[2] Jay Kreps: “Getting Real About Distributed System Reliability,” blog.empathy‐\n[4] Coda Hale: “You Can’t Sacrifice Partition Tolerance,” codahale.com, October 7,\n[5] Jeff Hodges: “Notes on Distributed Systems for Young Bloods,” somethingsimi‐\n[6] Antonio Regalado: “Who Coined ‘Cloud Computing’?,” technologyreview.com,\ntion,” Synthesis Lectures on Computer Architecture, volume 8, number 3, Morgan &\ntion of Silent Data Corruption for Large-Scale High-Performance Computing,” at\nTopologies and Centralized Control in Google’s Datacenter Network,” at Annual\nChapter 8: The Trouble with Distributed Systems\nShannon: “A Mathematical Theory of Communication,” The Bell Sys‐\n[14] Peter Bailis and Kyle Kingsbury: “The Network Is Reliable,” ACM Queue, vol‐\n“Taming Uncertainty in Distributed Systems with Help from the Network,” at 10th\nwork Failures in Data Centers: Measurement, Analysis, and Implications,” at ACM\n[17] Mark Imbriaco: “Downtime Last Saturday,” github.com, December 26, 2012.\nfirms,” slate.com, August 15, 2014.\n[20] Kyle Kingsbury: “Call Me Maybe: Elasticsearch,” aphyr.com, June 15, 2014.\nFail Scenarios,” antirez.com, October 21, 2014.\nPartition,” blog.thislongrun.com, May 25, 2015.\nin System Design,” ACM Transactions on Computer Systems, volume 2, number 4,\nMatter When You Can JUMP Them!,” at 12th USENIX Symposium on Networked\nPerformance of Amazon EC2 Data Center,” at 29th IEEE International Conference on\n[27] Van Jacobson: “Congestion Avoidance and Control,” at ACM Symposium on\n[28] Brandon Philips: “etcd: Distributed Locking and Service Discovery,” at Strange\n[29] Steve Newman: “A Systematic Look at EC2 I/O,” blog.scalyr.com, October 16,\n[31] Jeffrey Wang: “Phi Accrual Failure Detector,” ternarysearch.blogspot.co.uk,\n[33] Cisco, “Integrated Services Digital Network,” docwiki.cisco.com.\nFAQ and HOWTO,” ntp.org, November 2006.\nDNS,” blog.cloudflare.com, January 1, 2017.\n– Part I – Windows,” blogs.oracle.com, October 2, 2006.\nChapter 8: The Trouble with Distributed Systems\nGlobally-Distributed Database,” at 10th USENIX Symposium on Operating System\nClock Track the UTC Timescale Via the Internet?,” European Journal of Physics, vol‐\n[43] Nelson Minar: “A Survey of the NTP Network,” alumni.media.mit.edu, Decem‐\nlem,” blog.logentries.com, March 14, 2014.\n[45] Poul-Henning Kamp: “The One-Second War (What Time Will You Die?),”\n[46] Nelson Minar: “Leap Second Crashes Half the Internet,” somebits.com, July 3,\nand AWS,” aws.amazon.com, May 18, 2015.\nSecond,” at 17th International Conference on Passive and Active Measurement\n[50] “Timekeeping in VMware Virtual Machines,” Information Guide, VMware, Inc.,\n(Part 1),” lmax.com, November 27, 2015.\n[53] Kyle Kingsbury: “Call Me Maybe: Cassandra,” aphyr.com, September 24, 2013.\ntributed Systems,” basho.com, November 12, 2013.\n[55] Kyle Kingsbury: “The Trouble with Timestamps,” aphyr.com, October 12, 2013.\ntem,” Communications of the ACM, volume 21, number 7, pages 558–565, July 1978.\nClocks and Consistent Snapshots in Globally Distributed Databases,” State University\nSystems,” ACM Queue, volume 13, number 3, pages 36–41, March 2015.\n[59] Murat Demirbas: “Spanner: Google’s Globally-Distributed Database,” muratbuf‐\nACM SIGACT News, volume 44, number 3, pages 73–77, September 2013.\nEnforce Consistency in the Cloud,” IEEE Data Engineering Bulletin, volume 38, num‐\n[62] Spencer Kimball: “Living Without Atomic Clocks,” cockroachlabs.com, February\nanism for Distributed File Cache Consistency,” at 12th ACM Symposium on Operat‐\ncation Buffers: Part 1,” blog.cloudera.com, February 24, 2011.\nGreater,” java.dzone.com, June 28, 2011.\nMachines,” at 2nd USENIX Symposium on Symposium on Networked Systems Design\nby Background IO Traffic,” engineering.linkedin.com, February 10, 2016.\nChapter 8: The Trouble with Distributed Systems\n[70] David Terei and Amit Levy: “Blade: A Data Center Garbage Collector,” arXiv:\nCoordinating Garbage Collection in Distributed Systems,” at 15th USENIX Workshop\n[72] “Predictable Low Latency,” Cinnober Financial Technology AB, cinnober.com,\n[73] Martin Fowler: “The LMAX Architecture,” martinfowler.com, July 12, 2011.\nLaunch & How We Survived,” caitiem.com, June 23, 2015.\nProblem,” ACM Transactions on Programming Languages and Systems (TOPLAS),\nGray: “Notes on Data Base Operating Systems,” in Operating Systems: An\n[79] Brian Palmer: “How Complicated Was the Byzantine Empire?,” slate.com, Octo‐\n[80] Leslie Lamport: “My Writings,” research.microsoft.com, December 16, 2014.\n[81] John Rushby: “Bus Architectures for Safety-Critical Embedded Systems,” at 1st\nfrom Moderately-Hard Puzzles: A Model for Bitcoin,” University of Central Florida,\n[84] James Mickens: “The Saddest Moment,” USENIX ;login: logout, May 2013.\n[85] Evan Gilman: “The Discovery of Apache ZooKeeper’s Poison Packet,” pagerd‐\ngree,” at ACM Conference on Applications, Technologies, Architectures, and Protocols\n[87] Evan Jones: “How Both TCP and Ethernet Checksums Fail,” evanjones.ca, Octo‐\nence of Partial Synchrony,” Journal of the ACM, volume 35, number 2, pages 288–\nsions, and Beyond,” ACM Queue, volume 11, number 3, pages 55-63, March 2013.\nSchneider: “Defining Liveness,” Information Process‐\n[92] Scott Sanders: “January 28th Incident Report,” github.com, February 3, 2016.\n[93] Jay Kreps: “A Few Notes on Kafka and Jepsen,” blog.empathybox.com, Septem‐\nUnderstanding the Impact of Limpware on Scale-out Cloud Systems,” at 4th ACM\nCOST?,” at 15th USENIX Workshop on Hot Topics in Operating Systems (HotOS),\nChapter 8: The Trouble with Distributed Systems",
      "keywords": [
        "distributed systems",
        "systems",
        "System Model",
        "distributed",
        "Model",
        "Network",
        "operating systems",
        "distributed systems problems",
        "node",
        "doi",
        "ACM",
        "distributed algorithms",
        "ACM Queue",
        "Distributed System Reliability",
        "algorithm"
      ],
      "concepts": [
        "network",
        "systems",
        "doi",
        "blog",
        "time",
        "timing",
        "clock",
        "october",
        "distributed",
        "model"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 34,
          "title": "",
          "score": 0.753,
          "base_score": 0.603,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 33,
          "title": "",
          "score": 0.653,
          "base_score": 0.503,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 32,
          "title": "",
          "score": 0.645,
          "base_score": 0.495,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 40,
          "title": "",
          "score": 0.525,
          "base_score": 0.525,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 55,
          "title": "",
          "score": 0.503,
          "base_score": 0.503,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "com",
          "systems",
          "acm",
          "distributed",
          "distributed systems"
        ],
        "semantic": [],
        "merged": [
          "com",
          "systems",
          "acm",
          "distributed",
          "distributed systems"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3097485546924043,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.202982+00:00"
      }
    },
    {
      "chapter_number": 36,
      "title": "Segment 36 (pages 341-350)",
      "start_page": 341,
      "end_page": 350,
      "summary": "For example, say you have a database with single-leader replication.\nin time, you’re likely to see different data on the two nodes, because write requests\nMost replicated databases provide at least eventual consistency, which means that if\nyou stop writing to the database and wait for some unspecified length of time, then\neventually all read requests will return the same value [1].\nUntil the time of convergence, reads could return anything or\nFor example, if you write a value and then immediately read it again,\nthere is no guarantee that you will see the value you just wrote, because the read may\nvalue, or for the read to fail.\nWhen working with a database that provides only weak guarantees, you need to be\nIn this chapter we will explore stronger consistency models that data systems may\nIn an eventually consistent database, if you ask two different replicas the same ques‐\nIn a linearizable system, as soon as one client successfully completes a write, all cli‐\nents reading from the database must be able to see the value just written.\nthe illusion of a single copy of the data means guaranteeing that the value read is the\nFigure 9-2 shows three clients concurrently reading and writing the same key x in a\nlinearizable database.\nIf a read request is concurrent with a write request, it may return either the\nA register in which reads may return either the old or the new value if they are concurrent with a write is\n• read(x) ⇒ v means the client requested to read the value of register x, and the\ndatabase returned the value v.\n• write(x, v) ⇒ r means the client requested to set the register x to value v, and the\nIn Figure 9-2, the value of x is initially 0, and client C performs a write request to set\nread the latest value.\nread requests?\n• The first read operation by client A completes before the write begins, so it must\n• The last read by client A begins after the write has completed, so it must defi‐\nnitely return the new value 1 if the database is linearizable: we know that the\noperation, and the read must have been processed sometime between the start\nand end of the read operation.\nIf the read started after the write ended, then the\nread must have been processed after the write, and therefore it must see the new\n• Any read operations that overlap in time with the write operation might return\nthe time when the read operation is processed.\nconcurrent with a write can return either the old or the new value, then readers could\nsee a value flip back and forth between the old and the new value several times while\nAfter any one read has returned the new value, all following reads (on the\nsame or other clients) must also return the new value.\nthe start and end of the write operation) at which the value of x atomically flips from\nThus, if one client’s read returns the new value 1, all subsequent reads must\nalso return the new value, even if the write operation has not yet completed.\nto read the new value, 1.\nJust after A’s read returns, B begins a new read.\nread occurs strictly after A’s read, it must also return 1, even though the write by C is\nhas read the new value, Bob also expects to read the new value.)\nIn Figure 9-4 we add a third type of operation besides read and write:\n• cas(x, vold, vnew) ⇒ r means the client requested an atomic compare-and-set oper‐\nwrites for a register (every read must return the value set by the most recent write).\nor read, all subsequent reads see the value that was written, until it is overwritten\nVisualizing the points in time at which the reads and writes appear to have\nThe final read by B is not linearizable.\n• First client B sent a request to read x, then client D sent a request to set x to 0,\nNevertheless, the value returned to\nB’s read is 1 (the value written by A).\nprocessed D’s write, then A’s write, and finally B’s read.\nPerhaps B’s read request was slightly delayed in the net‐\n• Client B’s read returned 1 before client A received its response from the database,\nsaying that the write of the value 1 was successful.\nmean the value was read before it was written, it just means the ok response from\n• This model doesn’t assume any transaction isolation: another client may change\na value at any time.\nvalue was changed by B between the two reads.\ntime the database processes it, the value of x is no longer 0).\n• The final read by client B (in a shaded bar) is not linearizable.",
      "keywords": [
        "read",
        "NMOXDAWOP AO SMOIA",
        "write",
        "database",
        "client",
        "Consistency",
        "Consensus",
        "distributed systems",
        "time",
        "n’t",
        "system",
        "operation",
        "distributed",
        "request",
        "SWHLIIMOSTY SASNASNO’"
      ],
      "concepts": [
        "value",
        "database",
        "consistency",
        "consistent",
        "read",
        "client",
        "writing",
        "writes",
        "request",
        "requested"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 20,
          "title": "",
          "score": 0.895,
          "base_score": 0.745,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 23,
          "title": "",
          "score": 0.834,
          "base_score": 0.684,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 19,
          "title": "",
          "score": 0.762,
          "base_score": 0.612,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 21,
          "title": "",
          "score": 0.744,
          "base_score": 0.594,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 40,
          "title": "",
          "score": 0.664,
          "base_score": 0.514,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "value",
          "read",
          "new value",
          "client",
          "write"
        ],
        "semantic": [],
        "merged": [
          "value",
          "read",
          "new value",
          "client",
          "write"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.38617571289607744,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.203038+00:00"
      }
    },
    {
      "chapter_number": 37,
      "title": "Segment 37 (pages 351-358)",
      "start_page": 351,
      "end_page": 358,
      "summary": "Linearizability Versus Serializability\nLinearizability is easily confused with serializability (see “Serializability” on page 251),\nLinearizability\nLinearizability is a recency guarantee on reads and writes of a register (an indi‐\nA database may provide both serializability and linearizability, and this combination\npage 252) are typically linearizable.\non page 261) is not linearizable: by design, it makes reads from a consistent snapshot,\nthus reads from the snapshot are not linearizable.\nLinearizability \nStrictly speaking, ZooKeeper and etcd provide linearizable writes, but reads may be stale, since by default\nYou can optionally request a linearizable read: etcd calls this a\nRelying on Linearizability\nIn what circumstances is linearizability useful?\nA system that uses single-leader replication needs to ensure that there is indeed only\nNo matter how this lock is implemented, it must be linearizable: all nodes\nimplement linearizable operations in a fault-tolerant way (we discuss such algorithms\nthese linearizable locks are on the critical path of transaction execution, RAC deploy‐\nIn such cases, linearizability may\ntional databases, requires linearizability.\nkey or attribute constraints, can be implemented without requiring linearizability\nThe linearizability viola‐\nLinearizability \nIf the file storage service is linearizable, then this system should work fine.\nlinearizable, there is the risk of a race condition: the message queue (steps 3 and 4 in\nWithout the recency guarantee of linearizability, race conditions between these two\nLinearizability is not the only way of avoiding this race condition, but it’s the simplest\nImplementing Linearizable Systems\nNow that we’ve looked at a few examples in which linearizability is useful, let’s think\nabout how we might implement a system that offers linearizable semantics.\naffect linearizability, since it is only a single-object guarantee.\nbe made linearizable:\nSingle-leader replication (potentially linearizable)\nIn a system with single-leader replication (see “Leaders and Followers” on page\ntial to be linearizable.iv However, not every single-leader database is actually line‐\nlinearizability [20].\ndurability and linearizability.\nConsensus algorithms (linearizable)\nsensus algorithms can implement linearizable storage safely.\nMulti-leader replication (not linearizable)\nSystems with multi-leader replication are generally not linearizable, because they\nconcurrently process writes on multiple nodes and asynchronously replicate\nLeaderless replication (probably not linearizable)\nLinearizability \nlinearizability.\nLinearizability and quorums\nIntuitively, it seems as though strict quorum reads and writes should be linearizable\ncurrently with the write, client B reads from a different quorum of two nodes, and\nInterestingly, it is possible to make Dynamo-style quorums linearizable at the cost of\nrum reads [27], but it loses linearizability if there are multiple concurrent writes to\nMoreover, only linearizable read and write operations can be implemented in this\ntion does not provide linearizability.\nThe Cost of Linearizability\nAs some replication methods can provide linearizability and others cannot, it is inter‐\nLinearizability \nAny writes and any linearizable reads must be sent to the leader—\nthus, for any clients connected to a follower datacenter, those read and write requests\nIf the network between datacenters is interrupted in a single-leader setup, clients con‐\nwrites to the database, nor any linearizable reads.\nizable reads and writes, the network interruption causes the application to become\nlinearizable database has this problem, no matter how it is implemented.\n• If your application requires linearizability, and some replicas are disconnected\n• If your application does not require linearizability, then it can be written in a way\navailable in the face of a network problem, but its behavior is not linearizable.",
      "keywords": [
        "linearizability",
        "linearizable",
        "leader",
        "writes",
        "read",
        "replication",
        "network",
        "consensus",
        "system",
        "nodes",
        "Serializability",
        "Lock",
        "storage",
        "linearizable read",
        "Linearizability Versus Serializability"
      ],
      "concepts": [
        "replication",
        "replicate",
        "replicated",
        "write",
        "read",
        "node",
        "network",
        "quorum",
        "leader",
        "useful"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 38,
          "title": "",
          "score": 0.629,
          "base_score": 0.479,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 30,
          "title": "",
          "score": 0.56,
          "base_score": 0.56,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 22,
          "title": "",
          "score": 0.559,
          "base_score": 0.409,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 36,
          "title": "",
          "score": 0.549,
          "base_score": 0.549,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 23,
          "title": "",
          "score": 0.533,
          "base_score": 0.533,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "linearizability",
          "linearizable",
          "leader",
          "serializability",
          "reads"
        ],
        "semantic": [],
        "merged": [
          "linearizability",
          "linearizable",
          "leader",
          "serializability",
          "reads"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.36803586492930257,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.203115+00:00"
      }
    },
    {
      "chapter_number": 38,
      "title": "Segment 38 (pages 359-381)",
      "start_page": 359,
      "end_page": 381,
      "summary": "time of linearizable reads and writes is inevitably going to be high.\nThis definition implies that operations are executed in some well-defined order.\nWe illustrated the ordering in Figure 9-4 by joining up the operations in the order in\nwe have discussed ordering:\ntion is to determine the order of writes in the replication log—that is, the order in\ndue to concurrent operations (see “Handling Write Conflicts” on page 171).\nby literally executing transactions in that serial order, or by allowing concurrent\nOrdering and Causality\nRepeatable Read” on page 237), we said that a transaction reads from a consistent\nmakes it consistent with causality: the effects of all operations that happened cau‐\nFigure 7-6) means reading data in a state that violates causality.\ning the causal dependencies between transactions.\nCausality imposes an ordering on events: cause comes before effect; a message is sent\nreal life, one thing leads to another: one node reads some data and then writes some‐\nthing as a result, another node reads the thing that was written and writes something\nThese chains of causally dependent operations define the\ncausal order in the system—i.e., what happened before what.\nIf a system obeys the ordering imposed by causality, we say that it is causally consis‐\nFor example, snapshot isolation provides causal consistency: when you read\nThe causal order is not a total order\nral numbers are totally ordered: if I give you any two numbers, say 5 and 13, you can\nIn a linearizable system, we have a total order of operations: if the system behaves\nas if there is only a single copy of the data, and every operation is atomic, this\nThis total ordering is illustrated as a timeline in Figure 9-4.\nanother way, two events are ordered if they are causally related (one happened\nthat causality defines a partial order, not a total order: some operations are\ntotally ordered.\norder, but rather a jumble of different operations going on concurrently.\nin the diagram indicate causal dependencies—the partial ordering of operations.\nOften one commit\nLinearizability is stronger than causal consistency\nSo what is the relationship between the causal order and linearizability?\nThe fact that linearizability ensures causality is what makes linearizable systems sim‐\nIn order to maintain causality, you need to know which operation happened before\nThis is a partial order: concurrent operations may be pro‐\ncessed in any order, but if one operation happened before another, then they must be\nit must ensure that all causally preceding operations (all operations that happened\nIn order to determine causal dependencies, we need some way of describing the\nconcurrent writes to the same key in order to prevent lost updates.\nIn order to determine the causal ordering, the database needs to know which version\ntion (SSI)” on page 261: when a transaction wants to commit, the database checks\nSequence Number Ordering\nHowever, there is a better way: we can use sequence numbers or timestamps to order\nA total order that is inconsistent with causality is easy to create, but not very useful.\nThis is a valid total order, but the random UUIDs tell you nothing about which operation\nthey provide a total order: that is, every operation has a unique sequence number, and\nIn particular, we can create sequence numbers in a total order that is consistent with\ncausality:vii we promise that if operation A causally happened before B, then A occurs\nbefore B in the total order (A has a lower sequence number than B).\noperations may be ordered arbitrarily.\nSuch a total order captures all the causality\ninformation, but also imposes more ordering than strictly required by causality.\n152), the replication log defines a total order of write operations that is consistent\nsequence numbers for operations.\n• Each node can generate its own independent set of sequence numbers.\nrepresentation of the sequence number to contain a unique node identifier, and\nhigh resolution, they might be sufficient to totally order operations.\nFor example, node A might\nclaim the block of sequence numbers from 1 to 1,000, and node B might claim\nIt is possible to make physical clock timestamps consistent with causality: in “Synchronized clocks for\nhave a problem: the sequence numbers they generate are not consistent with causality.\nThe causality problems occur because these sequence number generators do not cor‐\nrectly capture the ordering of operations across different nodes:\n• Each node may process a different number of operations per second.\nnario in which an operation that happened causally later was actually assigned a\n• In the case of the block allocator, one operation may be given a sequence number\nidentifier, and each node keeps a counter of the number of operations it has pro‐\nLamport timestamps provide a total ordering consistent with causality.\nprovides total ordering: if you have two timestamps, the one with a greater counter\ntent with causality, is the following: every node and every client keeps track of the\nWhen a node receives a request or response with a maximum counter value\nscheme ensures that the ordering from the Lamport timestamps is consistent with\ntimestamps always enforce a total ordering.\nFrom the total ordering of Lamport time‐\nTimestamp ordering is not sufficient\nAlthough Lamport timestamps define a total order of operations that is consistent\nAt first glance, it seems as though a total ordering of operations (e.g., using Lamport\nSince timestamps are totally ordered, this comparison is always valid.\ntimestamp that other node may assign to the operation.\nIn order to be sure that no other node is in the process of concurrently creating an\nwith every other node to see what it is doing [56].\nThe problem here is that the total order of operations only emerges after you have\nIf another node has generated some operations, but\ntions: the unknown operations from the other node may need to be inserted at vari‐\nous positions in the total order.\nnames, it’s not sufficient to have a total ordering of operations—you also need to\nyour operation in the total order, then you can safely declare the operation successful.\noperations (in the sense of multi-threaded programming) or atomic registers (linearizable storage).\nThis idea of knowing when your total order is finalized is captured in the topic of\ntotal order broadcast.\nTotal Order Broadcast\nIf your program runs only on a single CPU core, it is easy to define a total ordering of\noperations: it is simply the order in which they were executed by the CPU.\nin a distributed system, getting all nodes to agree on the same total ordering of opera‐\nIn the last section we discussed ordering by timestamps or sequence\ntimestamp ordering to implement a uniqueness constraint, you cannot tolerate any\nAs discussed, single-leader replication determines a total order of operations by\nchoosing one node as the leader and sequencing all operations on a single CPU core\nture, this problem is known as total order broadcast or atomic broadcast [25, 57, 58].ix\nTotal ordering across all partitions is\nTotal order broadcast is usually described as a protocol for exchanging messages\nbetween nodes.\nnodes.\nTotally ordered delivery\nMessages are delivered to every node in the same order.\nA correct algorithm for total order broadcast must ensure that the reliability and\nordering properties are always satisfied, even if a node or the network is faulty.\nUsing total order broadcast\nConsensus services such as ZooKeeper and etcd actually implement total order\nTotal order broadcast is exactly what you need for database replication: if every mes‐\nSimilarly, total order broadcast can be used to implement serializable transactions: as\ndeterministic transaction to be executed as a stored procedure, and if every node pro‐\nAn important aspect of total order broadcast is that the order is fixed at the time the\nfact makes total order broadcast stronger than timestamp ordering.\nAnother way of looking at total order broadcast is that it is a way of creating a log (as\nin a replication log, transaction log, or write-ahead log): delivering a message is like\nSince all nodes must deliver the same messages in the same\norder, all nodes can read the log and see the same sequence of messages.\nTotal order broadcast is also useful for implementing a lock service that provides\nTotal order broadcast is equiva‐\nwhereas a linearizable read-write register can be implemented in the same system model [23, 24, 25].\nImplementing linearizable storage using total order broadcast\nAs illustrated in Figure 9-4, in a linearizable system there is a total order of opera‐\nDoes that mean linearizability is the same as total order broadcast?\nTotal order broadcast is asynchronous: messages are guaranteed to be delivered relia‐\nHowever, if you have total order broadcast, you can build linearizable storage on top\nusing total order broadcast as an append-only log [62, 63]:\ncommit the username claim (perhaps by appending another message to the log)\nBecause log entries are delivered to all nodes in the same order, if there are several\nthe conflicting writes as the winner and aborting later ones ensures that all nodes\nagree on whether a write was committed or aborted.\nImplementing total order broadcast using linearizable storage\ntotal order broadcast.\nstorage, and show how to build total order broadcast from it.\nThe algorithm is simple: for every message you want to send through total order\nwith Lamport timestamps—in fact, this is the key difference between total order\nbroadcast and timestamp ordering.\nincrement-and-get) register and total order broadcast are both equivalent to consen‐\ntransactions (Chapter 7), system models (Chapter 8), linearizability, and total order\nIn a database with single-leader replication, all nodes need to agree on which\nAtomic commit is formalized slightly differently from consensus: an atomic transaction can commit only\nHowever, atomic commit and consensus are\nAtomic commit\nIn a database that supports transactions spanning several nodes or partitions, we\nhave the problem that a transaction may fail on some nodes but succeed on oth‐\natomic commit problem.xii\nparticular, we will discuss the two-phase commit (2PC) algorithm, which is the most\nAtomic Commit and Two-Phase Commit (2PC)\nThe outcome of a transaction is either a successful commit, in which case\nall of the transaction’s writes are made durable, or an abort, in which case all of the\nFrom single-node to distributed atomic commit\nFor transactions that execute at a single database node, atomicity is commonly imple‐\nWhen the client asks the database node to commit the\ntransaction is recovered from the log when the node restarts: if the commit record\nThus, on a single node, transaction commitment crucially depends on the order in\nkey deciding moment for whether the transaction commits or aborts is the moment\nticular disk drive, attached to one particular node) that makes the commit atomic.\nHowever, what if multiple nodes are involved in a transaction?\nIn these cases, it is not sufficient to simply send a commit request to all of the nodes\nand independently commit the transaction on each one.\nhappen that the commit succeeds on some nodes and fails on other nodes, which\n• Some nodes may detect a constraint violation or conflict, making an abort neces‐\nsary, while other nodes are successfully able to commit.\n• Some of the commit requests might be lost in the network, eventually aborting\n• Some nodes may crash before the commit record is fully written and roll back on\nIf some nodes commit the transaction but others abort it, the nodes become inconsis‐\nAnd once a transaction has been committed\nanother node.\nFor this reason, a node must only commit once it is certain that all\nother nodes in the transaction are also going to commit.\nA transaction commit must be irrevocable—you are not allowed to change your\nmind and retroactively abort a transaction after it has been committed.\nbasis of read committed isolation, discussed in “Read Committed” on page 234.\ntransaction was allowed to abort after committing, any transactions that read the\n(It is possible for the effects of a committed transaction to later be undone by\nTwo-phase commit is an algorithm for achieving atomic transaction commit across\nmultiple nodes—i.e., to ensure that either all nodes commit or all nodes abort.\nas with a single-node transaction, the commit/abort process in 2PC is split into two\nprovides atomic commit in a distributed database, whereas 2PL\nA 2PC transaction begins with the application reading and writing data on multiple\nWe call these database nodes participants in the transac‐\ncommit.\natomicity, while one-phase commit across several nodes does not.\nand commit requests can just as easily be lost in the two-phase case.\n2. The application begins a single-node transaction on each of the participants, and\nattaches the globally unique transaction ID to the single-node transaction.\nreads and writes are done in one of these single-node transactions.\ngoes wrong at this stage (for example, a node crashes or a request times out), the\nnitely commit the transaction under all circumstances.\npromises to commit the transaction without error if requested.\ncommitting it.\ndefinitive decision on whether to commit or abort the transaction (committing\n6. Once the coordinator’s decision has been written to disk, the commit or abort\n(Single-node atomic\nhusband and wife,” that doesn’t change the fact that the transaction was committed.\n2PC: if any of the prepare requests fail or time out, the coordinator aborts the trans‐\naction; if any of the commit or abort requests fail, the coordinator retries them indefi‐\ncoordinator whether the transaction was committed or aborted.\nactually decided to commit, and database 2 received the commit request.\nthe coordinator crashed before it could send the commit request to database 1, and so\ndatabase 1 does not know whether to commit or abort.\ndatabase 2, which has committed.\nwhether to commit or abort.\nwhy the coordinator must write its commit or abort decision to a transaction log on\ndisk before sending commit or abort requests to participants: when the coordinator\nAny transactions that don’t have a commit record in the coordinator’s log\nThus, the commit point of 2PC comes down to a regular single-node\natomic commit on the coordinator.\nHowever, 3PC assumes a network with bounded delay and nodes with\nmay time out due to a network problem even if no node has crashed.",
      "keywords": [
        "total order broadcast",
        "total order",
        "order",
        "commit",
        "order broadcast",
        "node",
        "transaction",
        "total",
        "operations",
        "Consensus",
        "operation",
        "Atomic commit",
        "Sequence Number",
        "Ordering",
        "Lamport timestamps"
      ],
      "concepts": [
        "nodes",
        "ordering",
        "order",
        "causality",
        "causally",
        "transactions",
        "transaction",
        "commit",
        "committing",
        "operation"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 22,
          "title": "",
          "score": 0.804,
          "base_score": 0.654,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 37,
          "title": "",
          "score": 0.629,
          "base_score": 0.479,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 58,
          "title": "",
          "score": 0.62,
          "base_score": 0.62,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 48,
          "title": "",
          "score": 0.62,
          "base_score": 0.62,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 30,
          "title": "",
          "score": 0.596,
          "base_score": 0.596,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "total",
          "total order",
          "commit",
          "order",
          "order broadcast"
        ],
        "semantic": [],
        "merged": [
          "total",
          "total order",
          "commit",
          "order",
          "order broadcast"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.39534002075118185,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.203173+00:00"
      }
    },
    {
      "chapter_number": 39,
      "title": "Segment 39 (pages 382-391)",
      "start_page": 382,
      "end_page": 391,
      "summary": "Distributed Transactions in Practice\nDistributed transactions, especially those implemented with two-phase commit, have\ndistributed transactions due to the operational problems they engender [85, 86].\nSome implementations of distributed transactions carry a heavy performance penalty\n—for example, distributed transactions in MySQL are reported to be over 10 times\nslower than single-node transactions [87], so it is not surprising when people advise\nHowever, rather than dismissing distributed transactions outright, we should exam‐\ntions.” Two quite different types of distributed transactions are often conflated:\nDatabase-internal distributed transactions\nin their standard configuration) support internal transactions among the nodes\nIn this case, all the nodes participating in\nthe transaction are running the same database software.\nHeterogeneous distributed transactions\nIn a heterogeneous transaction, the participants are two or more different tech‐\nA distributed transaction across these\nDatabase-internal transactions do not have to be compatible with any other system,\nFor that reason, database-internal distributed transactions can often work\nHeterogeneous distributed transactions allow diverse systems to be integrated in\nas processed if and only if the database transaction for processing the message was\nacknowledgment and the database writes in a single transaction.\ntransaction support, this is possible, even if the message broker and the database are\nIf either the message delivery or the database transaction fails, both are aborted, and\nSuch a distributed transaction is only possible if all systems affected by the transac‐\ntransactions.\nXA transactions\nDistributed Transactions and Consensus \nThe transaction coordinator implements the XA API.\nread the log to recover the commit/abort outcome of each transaction.\nthe coordinator use the database driver’s XA callbacks to ask participants to commit\ntransaction (see “Two-Phase Locking (2PL)” on page 257).\nThe database cannot release those locks until the transaction commits or aborts\na transaction must hold onto the locks throughout the time it is in doubt.\non the database, other transactions may even be blocked from reading those rows.\nfrom the log and resolve any in-doubt transactions.\nThese transactions cannot be\nother transactions.\nin-doubt transaction, determine whether any participant has committed or aborted\nLimitations of distributed transactions\nXA transactions solve the real and important problem of keeping several participant\ncoordinator is itself a kind of database (in which transaction outcomes are stored),\nservers to block on locks held by in-doubt transactions).\nDistributed Transactions and Consensus \nare required in order to recover in-doubt transactions after a crash.\n• For database-internal distributed transactions (not XA), the limitations are not\nremains the problem that for 2PC to successfully commit a transaction, all par‐\ntransaction also fails.\nInformally, consensus means getting several nodes to agree on something.\nThe consensus problem is normally formalized as follows: one or more nodes may\npropose values, and the consensus algorithm decides on one of those values.\nIn this formalism, a consensus algorithm must satisfy the following properties [25]:xiii\nNo two nodes decide differently.\nEvery node that does not crash eventually decides some value.\ncoordinator fails, in-doubt participants cannot decide whether to commit or abort.\nThe system model of consensus assumes that when a node “crashes,” it suddenly dis‐\ncome back online.) In this system model, any algorithm that has to wait for a node to\nDistributed Transactions and Consensus \nat least a majority of nodes to be functioning correctly in order to assure termination\neven if a majority of nodes fail or there is a severe network problem [92].\ncol (for example, if it sends contradictory messages to different nodes), it may break\nConsensus algorithms and total order broadcast\nThe best-known fault-tolerant consensus algorithms are Viewstamped Replication\nwhich makes them total order broadcast algorithms, as discussed previously in this\nin the same order, to all nodes.\nseveral rounds of consensus: in each round, nodes propose the message that they\n• Due to the agreement property of consensus, all nodes decide to deliver the same\nSingle-leader replication and consensus\n(i.e., make decisions about the order of writes in the replication log), and if that node\nconfigure a different node to be the leader.\nbut it does not satisfy the termination property of consensus because it requires\nto be the new leader if the old leader fails (see “Handling Node Outages” on page\nsaid that all nodes need to agree who the leader is—otherwise two different nodes\nThus, we need consensus in order to elect a leader.\nconsensus algorithms described here are actually total order broadcast algorithms,\nDistributed Transactions and Consensus \nAll of the consensus protocols discussed so far internally use a leader in some form or\nEvery time the current leader is thought to be dead, a vote is started among the nodes\nHow does a leader know that it hasn’t been ousted by another node?\nown judgment—just because a node thinks that it is the leader, that does not neces‐\nsarily mean the other nodes accept it as their leader.\nnodes [105].\nsus algorithms only require votes from a majority of nodes, whereas 2PC requires a\nprocess by which nodes can get into a consistent state after a new leader is elected,\ncorrectness and fault tolerance of a consensus algorithm.\nConsensus algorithms are a huge breakthrough for distributed systems: they bring\nThe process by which nodes vote on proposals before they are decided is a kind of\nConsensus systems always require a strict majority to operate.\nminimum of three nodes in order to tolerate one failure (the remaining two out of\nMost consensus algorithms assume a fixed set of nodes that participate in voting,\nbership extensions to consensus algorithms allow the set of nodes in the cluster to\nConsensus systems generally rely on timeouts to detect failed nodes.\noften happens that a node falsely believes the leader to have failed due to a transient\nSometimes, consensus algorithms are particularly sensitive to network problems.\nOther consensus algorithms have similar problems, and\nDistributed Transactions and Consensus ",
      "keywords": [
        "Distributed Transactions",
        "Consensus",
        "Transactions",
        "nodes",
        "consensus algorithms",
        "leader",
        "Heterogeneous distributed transactions",
        "Distributed",
        "total order broadcast",
        "Database-internal distributed transactions",
        "message",
        "order",
        "total order",
        "order broadcast",
        "coordinator"
      ],
      "concepts": [
        "transactions",
        "transaction",
        "consensus",
        "nodes",
        "database",
        "leaders",
        "distributed",
        "network",
        "algorithm",
        "properties"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 26,
          "title": "",
          "score": 0.772,
          "base_score": 0.622,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 29,
          "title": "",
          "score": 0.752,
          "base_score": 0.602,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 54,
          "title": "",
          "score": 0.714,
          "base_score": 0.564,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 30,
          "title": "",
          "score": 0.694,
          "base_score": 0.544,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 27,
          "title": "",
          "score": 0.635,
          "base_score": 0.485,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "consensus",
          "transactions",
          "distributed transactions",
          "nodes",
          "distributed"
        ],
        "semantic": [],
        "merged": [
          "consensus",
          "transactions",
          "distributed transactions",
          "nodes",
          "distributed"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.36733707904303514,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.203229+00:00"
      }
    },
    {
      "chapter_number": 40,
      "title": "Segment 40 (pages 392-405)",
      "start_page": 392,
      "end_page": 405,
      "summary": "or “coordination and configuration services.” The API of such a service looks pretty\nacross all the nodes using a fault-tolerant total order broadcast algorithm.\nDistributed Transactions and Consensus \nsome of the work of coordinating nodes (consensus, operation ordering, and failure\nsents information like “the node running on 10.1.1.23 is the leader for partition 7,”\nand multi-leader replication systems typically do not use global consensus.\nsions, and Beyond,” ACM Queue, volume 11, number 3, pages 55-63, March 2013.\ndoi:10.1145/2460276.2462076\n[3] Alex Scotti: “Adventures in Building Your Own Database,” at All Your Base,\nVirtues and Limitations,” at 40th International Conference on Very Large Data Bases\nStorage Systems,” arXiv:1512.00168, 12 April 2016.\ndition for Concurrent Objects,” ACM Transactions on Programming Languages and\nSystems (TOPLAS), volume 12, number 3, pages 463–492, July 1990.\ndoi:\n[7] Leslie Lamport: “On interprocess communication,” Distributed Computing, vol‐\n[10] Kyle Kingsbury: “Call Me Maybe: MongoDB Stale Reads,” aphyr.com, April 20,\n[12] Peter Bailis: “Linearizability Versus Serializability,” bailis.org, September 24,\ntems,” at 7th USENIX Symposium on Operating System Design and Implementation\nDatabase Systems,” Proceedings of the VLDB Endowment, volume 8, number 3, pages\n[20] Kyle Kingsbury: “Call Me Maybe: etcd and Consul,” aphyr.com, June 9, 2014.\nPerformance Broadcast for Primary-Backup Systems,” at 41st IEEE International\ndoi:10.1109/\nsensus Algorithm (Extended Version),” at USENIX Annual Technical Conference\nMessage-Passing Systems,” Journal of the ACM, volume 42, number 1, pages 124–\ndoi:10.1145/200836.200869\nUsing Dynamic Quorum-Acknowledged Broadcasts,” at 27th Annual International\n978-3-642-15259-7, doi:10.1007/978-3-642-15260-3\nHerlihy: “Wait-Free Synchronization,” ACM Transactions on Pro‐\ngramming Languages and Systems (TOPLAS), volume 13, number 1, pages 124–149,\ndoi:10.1145/114005.102808\ntems,” at 7th Workshop on Hot Topics in Operating Systems (HotOS), March 1999.\nsistent, Available, Partition-Tolerant Web Services,” ACM SIGACT News, volume 33,\nnumber 2, pages 51–59, June 2002.\ndoi:10.1145/564585.564601\ndoi:\ntitioned Networks,” ACM Computing Surveys, volume 17, number 3, pages 341–370,\ndoi:10.1145/5505.5508\ncate Databases,” Network Working Group, January 27, 1975.\ntributed Databases,” IBM Research, Research Report RJ2571(33471), July 1979.\nAvailability of Data in an Unreliable Network,” at 1st ACM Symposium on Principles\ndoi:10.1145/588111.588124\nLynch: “A Hundred Impossibility Proofs for Distributed Computing,”\ndoi:10.1145/72981.72982\nEventually-Consistent Data Stores,” at ACM Symposium on Principles of Distributed\ndoi:10.1145/2767386.2767419\nble Programmer’s Model for x86 Multiprocessors,” Communications of the ACM,\nvolume 53, number 7, pages 89–97, July 2010.\ndoi:10.1145/1785414.1785443\nBarriers/Fences,” \nDesign,” IEEE Computer Magazine, volume 45, number 2, pages 37–42, February\nity,” ACM Transactions on Computer Systems (TOCS), volume 12, number 2, pages\ndoi:10.1145/176575.176576\ntions, Implementation, and Programming,” Distributed Computing, volume 9, num‐\n“Stronger Semantics for Low-Latency Geo-Replicated Storage,” at 10th USENIX Sym‐\nTolerant Geo-Replication Integrated All the Way to the Client Machine,” INRIA\nConsistency,” at ACM International Conference on Management of Data (SIGMOD),\nStronger Consistency at Scale,” at 15th USENIX Workshop on Hot Topics in Operat‐\n“Concise Server-Wide Causality Management for Eventually Consistent Data Stores,”\ndoi:10.1007/978-3-319-19129-4_6\ntem,” Communications of the ACM, volume 21, number 7, pages 558–565, July 1978.\ndoi:10.1145/359545.359563\nMulticast Algorithms: Taxonomy and Survey,” ACM Computing Surveys, volume 36,\ndoi:10.1145/1041680.1041682\n978-0-471-45324-6, doi:10.1002/0471478210\nShared Log Design for Flash Clusters,” at 9th USENIX Symposium on Networked Sys‐\nMachine Approach: A Tutorial,” ACM Computing Surveys, volume 22, number 4,\nDistributed Transactions for Partitioned Database Systems,” at ACM International\nData Structures over a Shared Log,” at 24th ACM Symposium on Operating Systems\ndoi:10.1145/2517349.2522732\nHigh Throughput and Availability,” at 6th USENIX Symposium on Operating System\ncutes Multiprocess Programs,” IEEE Transactions on Computers, volume 28, number\nity at the Next Level,” hortonworks.com, January 22, 2015.\nYahoo!’s Hosted Data Serving Platform,” at 34th International Conference on Very\ndoi:10.14778/1454159.1454167\nable Distributed Systems,” Journal of the ACM, volume 43, number 2, pages 225–267,\ndoi:10.1145/226643.226647\ntributed Consensus with One Faulty Process,” Journal of the ACM, volume 32, num‐\ndoi:10.1145/3149.214121\nnous Agreement Protocols,” at 2nd ACM Symposium on Principles of Distributed\ndoi:10.1145/800221.806707\nGray and Leslie Lamport: “Consensus on Transaction Commit,” ACM\nTransactions on Database Systems (TODS), volume 31, number 1, pages 133–160,\ndoi:10.1145/1132863.1132867\nCommitment and Consensus,” at 9th International Workshop on Distributed Algo‐\ning Crash-Consistent Applications,” at 11th USENIX Symposium on Operating\n[73] Jim Gray: “The Transaction Concept: Virtues and Limitations,” at 7th Interna‐\n[74] Hector Garcia-Molina and Kenneth Salem: “Sagas,” at ACM International Con‐\ndoi:10.1145/38713.38742\nthe R* Distributed Database Management System,” ACM Transactions on Database\nSystems, volume 11, number 4, pages 378–396, December 1986.\ndoi:\n[76] “Distributed Transaction Processing: The XA Specification,” X/Open Company\n[77] Mike Spille: “XA Exposed, Part II,” jroller.com, April 3, 2004.\ndoi:10.1109/\n“Formal Specification of a Web Services Protocol,” at 1st International Workshop on\n[80] Dale Skeen: “Nonblocking Commit Protocols,” at ACM International Conference\ndoi:10.1145/582318.582339\nware, volume 22, number 2, pages 64–66, March 2005.\n[82] Pat Helland: “Life Beyond Distributed Transactions: An Apostate’s Opinion,” at\n[84] Oren Eini (Ahende Rahien): “The Fallacy of Distributed Transactions,”\nEmail Discussion,” vasters.com, July 30, 2012.\nTransactions in MySQL,” at MySQL Conference and Expo, April 2013.\n[88] Mike Spille: “XA Exposed, Part I,” jroller.com, April 3, 2004.\n[89] Ajmer Dhariwal: “Orphaned MSDTC Transactions (-2 spids),” eraofdata.com,\n[90] Paul Randal: “Real World Story of DBCC PAGE Saving the Day,” sqlskills.com,\nence of Partial Synchrony,” Journal of the ACM, volume 35, number 2, pages 288–\ndoi:10.1145/42282.42283\nProactive Recovery,” ACM Transactions on Computer Systems, volume 20, number 4,\ndoi:10.1145/571637.571640\nCopy Method to Support Highly-Available Distributed Systems,” at 7th ACM Sympo‐\ndoi:\nLiskov and James Cowling: “Viewstamped Replication Revisited,”\n[96] Leslie Lamport: “The Part-Time Parliament,” ACM Transactions on Computer\nSystems, volume 16, number 2, pages 133–169, May 1998.\ndoi:10.1145/279227.279229\n[97] Leslie Lamport: “Paxos Made Simple,” ACM SIGACT News, volume 32, number\nLive – An Engineering Perspective,” at 26th ACM Symposium on Principles of Dis‐\n[99] Robbert van Renesse: “Paxos Made Moderately Complex,” cs.cornell.edu, March\n“Raft Refloated: Do We Have Consensus?,” ACM SIGOPS Operating Systems Review,\nvolume 49, number 1, pages 12–21, January 2015.\ndoi:10.1145/2723872.2723876\nZab,” IEEE Transactions on Dependable\nand Secure Computing, volume 12, number 4, pages 472–484, September 2014.\ndoi:\nnet Edge,” at Annual Conference of the ACM Special Interest Group on Data Commu‐\ndoi:10.1145/2829988.2790010\n[107] Kyle Kingsbury: “Call Me Maybe: Elasticsearch 1.5.0,” aphyr.com, April 27,\n[109] Camille Fournier: “Consensus Systems for the Skeptical Architect,” at Craft\nBirman: “A History of the Virtual Synchrony Replication Model,”",
      "keywords": [
        "doi",
        "ACM",
        "consensus",
        "systems",
        "ACM Transactions",
        "distributed",
        "ACM International Conference",
        "ACM Computing Surveys",
        "distributed systems",
        "ACM Symposium",
        "Distributed Computing",
        "Distributed Database System",
        "Database Systems",
        "Distributed Transactions",
        "ACM International"
      ],
      "concepts": [
        "doi",
        "systems",
        "consensus",
        "distributed",
        "services",
        "nodes",
        "transaction",
        "transactions",
        "consistent",
        "consistency"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 23,
          "title": "",
          "score": 0.847,
          "base_score": 0.697,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 21,
          "title": "",
          "score": 0.807,
          "base_score": 0.657,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 20,
          "title": "",
          "score": 0.784,
          "base_score": 0.634,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 19,
          "title": "",
          "score": 0.73,
          "base_score": 0.58,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 58,
          "title": "",
          "score": 0.668,
          "base_score": 0.668,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "doi 10",
          "doi",
          "10 1145",
          "1145",
          "acm"
        ],
        "semantic": [],
        "merged": [
          "doi 10",
          "doi",
          "10 1145",
          "1145",
          "acm"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4269733901221612,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.203299+00:00"
      }
    },
    {
      "chapter_number": 41,
      "title": "Segment 41 (pages 406-415)",
      "start_page": 406,
      "end_page": 415,
      "summary": "Derived Data\nIn reality, data systems are often more complex.\nto be able to access and process data in many different ways, and there is no one data‐\nsystems, etc.\ndifferent data systems, potentially with different data models and optimized for dif‐\nSystems of Record and Derived Data\nOn a high level, systems that store and process data can be grouped into two broad\nof your data.\nWhen new data comes in, e.g., as user input, it is first written here.\nDerived data systems\nA classic example is a cache: data can\npredictive summary data is often derived from usage logs.\na single source, enabling you to look at the data from different “points of view.”\nNot all systems make a clear distinction between systems of record and derived data\nThe distinction between system of record and derived data system depends not\nWe will start in Chapter 10 by examining batch-oriented dataflow systems such as\nscale data systems.\nBatch Processing\nThis style of data processing is assumed in many\nmodern data systems: you ask for something, or you send an instruction, and some\nIn such online systems, whether it’s a web browser requesting a page or a service call‐\nBatch processing systems (offline systems)\nA batch processing system takes a large amount of input data, runs a job to pro‐\ncuss batch processing in this chapter.\nStream processing systems (near-real-time systems)\ndata.\nThis difference allows stream processing systems to have lower latency than\nthe equivalent batch systems.\nAs stream processing builds upon batch process‐\nAs we shall see in this chapter, batch processing is an important building block in our\nwas subsequently implemented in various open source data systems, including\ncessing systems that were developed for data warehouses many years previously [3,\nwhy and how batch processing is useful.\nform of batch processing to compute aggregate statistics from large inputs.\nmachines that were widely used for business data processing in the 1940s and 1950s\nIn this chapter, we will look at MapReduce and several other batch processing algo‐\nrithms and frameworks, and explore how they are used in modern data systems.\nfirst, to get started, we will look at data processing using standard Unix tools.\nChapter 10: Batch Processing\nous distributed data systems.\nBatch Processing with Unix Tools\nlog file every time it serves a request.\nformat, one line of the log might look like this:\nBatch Processing with Unix Tools \nfrom each line, which happens to be the requested URL.\nrequest URL is /css/typography.css.\nAlphabetically sort the list of requested URLs. If some URL has been requested\nn times, then after sorting, the file contains the same URL repeated n times in a\nevery distinct URL, it reports how many times that URL appeared in the input.\nnumber of times the URL was requested.\nIt will process gigabytes of log files in a\nChapter 10: Batch Processing\nurl = line.split[6] \nFrom each line of the log, we take the URL to be the seventh whitespace-\nIncrement the counter for the URL in the current line of the log.\nsuch a hash table, but instead relies on sorting a list of URLs in which multiple occur‐\nnumber of distinct URLs: if there are a million log entries for a single URL, the space\nBatch Processing with Unix Tools ",
      "keywords": [
        "Batch Processing",
        "Data",
        "data systems",
        "systems",
        "processing",
        "URL",
        "Derived Data",
        "Batch",
        "Unix tools",
        "Stream processing systems",
        "Batch processing systems",
        "data processing",
        "processing systems",
        "Unix",
        "Derived data systems"
      ],
      "concepts": [
        "data",
        "systems",
        "batch",
        "logs",
        "log",
        "sort",
        "different",
        "difference",
        "process",
        "processing"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 3,
          "title": "",
          "score": 0.783,
          "base_score": 0.633,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 53,
          "title": "",
          "score": 0.737,
          "base_score": 0.587,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "",
          "score": 0.732,
          "base_score": 0.582,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 52,
          "title": "",
          "score": 0.719,
          "base_score": 0.569,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 59,
          "title": "",
          "score": 0.711,
          "base_score": 0.561,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "url",
          "batch",
          "batch processing",
          "processing",
          "systems"
        ],
        "semantic": [],
        "merged": [
          "url",
          "batch",
          "batch processing",
          "processing",
          "systems"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.39177875836970494,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.203362+00:00"
      }
    },
    {
      "chapter_number": 42,
      "title": "Segment 42 (pages 416-425)",
      "start_page": 416,
      "end_page": 425,
      "summary": "data can be sorted in memory and written out to disk as segment files, and then mul‐\nis likely to be the rate at which the input file can be read from disk.\n2. Expect the output of every program to become the input to another, as yet\nThe sort tool is a great example of a program that does one thing well.\nA Unix shell like bash lets us easily compose these small programs into surprisingly\nIf you expect the output of one program to become the input to another program,\nIf you want to be able to connect any program’s output to any pro‐\ngram’s input, that means that all programs must use the same input/output interface.\nIn Unix, that interface is a file (or, more precisely, a file descriptor).\nA file is just an\nBy convention, many (but not all) Unix programs treat this sequence of bytes as\ntheir input file as a list of records separated by the \\n (newline, ASCII 0x0A) charac‐\nBatch Processing with Unix Tools \nUnix tools commonly\nto have programs that work together as smoothly as Unix tools do.\nAnother characteristic feature of Unix tools is their use of standard input (stdin) and\nalso take input from a file and/or redirect output to a file.\nA program can still read and write files directly if it needs to, but the Unix approach\nworks best if a program doesn’t worry about particular file paths and simply uses\nThis allows a shell user to wire up the input and output in what‐\never way they want; the program doesn’t know or care where the input is coming\nlate binding [15], or inversion of control [16].) Separating the input/output wiring\nYou can even write your own programs and combine them with the tools provided\nYour program just needs to read input from stdin and write\noutput to stdout, and it can participate in data processing pipelines.\nneed multiple inputs or outputs are possible but tricky.\noutput into a network connection [17, 18].iii If a program directly opens files for read‐\nand outputs in a shell is reduced.\n• The input files to Unix commands are normally treated as immutable.\ncommand-line options, without damaging the input files.\n• You can write the output of one pipeline stage to a file and use that file as input\nMapReduce is a bit like Unix tools, but distributed across potentially thousands of\nA single MapReduce job is comparable to a single Unix process: it takes one or\nmore inputs and produces one or more outputs.\nAs with most Unix tools, running a MapReduce job normally does not modify the\ninput and does not have any side effects other than producing the output.\nWhile Unix tools use stdin and stdout as input and output, MapReduce jobs read\nReduce, that filesystem is called HDFS (Hadoop Distributed File System), an open\nIn order to tolerate machine and disk failures, file blocks are replicated on multiple\nMapReduce is a programming framework with which you can write code to process\nThe pattern of data processing in MapReduce is very similar to this\n1. Read a set of input files, and break it up into records.\n2. Call the mapper function to extract a key and value from each input record.\n4. Call the reducer function to iterate over the sorted key-value pairs.\n(reduce) are where you write your custom data processing code.\nThe mapper is called once for every input record, and its job is to extract the key\nand value from the input record.\nThe MapReduce framework takes the key-value pairs produced by the mappers,\nThe reducer can produce output records\nIn the web server log example, we had a second sort command in step 5, which\nstage, you can implement it by writing a second MapReduce job and using the output\nthe reducer is to process the data that has been sorted.\nThe main difference from pipelines of Unix commands is that MapReduce can paral‐\nThe mapper and reducer only operate on one record\nat a time; they don’t need to know where their input is coming from or their output is\nIt is possible to use standard Unix tools as mappers and reducers in a distributed\nIn Hadoop MapReduce, the mapper and reducer\nbased on partitioning (see Chapter 6): the input to a job is typically a directory in\nHDFS, and each file or file block within the input directory is considered to be a sepa‐\nEach input file is typically hundreds of megabytes in size.\nstores a replica of the input file, provided that machine has enough spare RAM and\nputation near the data [27]: it saves copying the input file over the network, reducing\nA MapReduce job with three mappers and three reducers.\nfirst copies the code (e.g., JAR files in the case of a Java program) to the appropriate\nIt then starts the map task and begins reading the input file, passing one\nThe output of the mapper consists of key-\ntasks is determined by the number of input file blocks, the number of reduce tasks is\nThe key-value pairs must be sorted, but the dataset is likely too large to be sorted with\nFirst, each map task partitions its output by reducer, based on the\nEach of these partitions is written to a sorted file on the mapper’s\nWhenever a mapper finishes reading its input file and writing its sorted output files,\nthe MapReduce scheduler notifies the reducers that they can start fetching the output\nfiles from that mapper.\nthe files of sorted key-value pairs for their partition.\nreducer, sorting, and copying data partitions from mappers to reducers is known as\nThe reduce task takes the files from the mappers and merges them together, preserv‐\nThus, if different mappers produced records with the same key,\nthey will be adjacent in the merged reducer input.\nreducer can use arbitrary logic to process these records, and can generate any number\nof output records.\nThese output records are written to a file on the distributed filesys‐\ntem (usually, one copy on the local disk of the machine running the reducer, with\nring back to the log analysis example, a single MapReduce job could determine the\nsuch that the output of one job becomes the input to the next job.\noutput to a designated directory in HDFS, and the second job must be configured to\nChained MapReduce jobs are therefore less like pipelines of Unix commands (which\npass the output of one process as input to another process directly, using only a small\noutput is written to a temporary file, and the next command reads from the tempo‐\n(MapReduce discards the partial output of a failed job).\nmany different teams may be running different jobs that read each other’s output.\nWhen a MapReduce job is given a set of files as input, it reads the entire content of all",
      "keywords": [
        "Unix tools",
        "Unix",
        "input",
        "file",
        "input file",
        "MapReduce",
        "MapReduce job",
        "output",
        "job",
        "tools",
        "program",
        "record",
        "Unix commands",
        "data",
        "HDFS"
      ],
      "concepts": [
        "files",
        "sorted",
        "storage",
        "job",
        "jobs",
        "program",
        "programming",
        "processing",
        "process",
        "tool"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 41,
          "title": "",
          "score": 0.621,
          "base_score": 0.471,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 12,
          "title": "",
          "score": 0.568,
          "base_score": 0.418,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 3,
          "title": "",
          "score": 0.552,
          "base_score": 0.402,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "",
          "score": 0.546,
          "base_score": 0.396,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 13,
          "title": "",
          "score": 0.546,
          "base_score": 0.396,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "input",
          "output",
          "file",
          "unix",
          "mapreduce"
        ],
        "semantic": [],
        "merged": [
          "input",
          "output",
          "file",
          "unix",
          "mapreduce"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.29987333220733997,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.203420+00:00"
      }
    },
    {
      "chapter_number": 43,
      "title": "Segment 43 (pages 426-436)",
      "start_page": 426,
      "end_page": 436,
      "summary": "When we talk about joins in the context of batch processing, we mean resolving all\nExample: analysis of user activity events\nA typical example of a join in a batch job is illustrated in Figure 10-2.\nactivity events or clickstream data), and on the right is a database of users.\nA join between a log of user activity events and a database of user profiles.\nTherefore, the activity events need to be joined with the user profile database.\none and query the user database (on a remote server) for every user ID it encounters.\nThus, a better approach would be to take a copy of the user database (for example,\nYou would then have the user database in one set of files in HDFS and the\nuser activity records in another set of files, and could use MapReduce to bring\nRecall that the purpose of the mapper is to extract a key and value from each input\nIn the case of Figure 10-2, this key would be the user ID: one set of mappers\nwould go over the activity events (extracting the user ID as the key and the activity\nevent as the value), while another set of mappers would go over the user database\n(extracting the user ID as the key and the user’s date of birth as the value).\nA reduce-side sort-merge join on user ID.\nWhen the MapReduce framework partitions the mapper output by key and then sorts\nthe key-value pairs, the effect is that all the activity events and the user record with\nthe same user ID become adjacent to each other in the reducer input.\nReduce job can even arrange the records to be sorted such that the reducer always\nsees the record from the user database first, followed by the activity events in time‐\nThe reducer can then perform the actual join logic easily: the reducer function is\nexpected to be the date-of-birth record from the user database.\nSince the reducer processes all of the records for a particular user ID in one go, it only\nneeds to keep one user record in memory at any one time, and it never needs to make\nmapper output is sorted by key, and the reducers then merge together the sorted lists\nIn a sort-merge join, the mappers and the sorting process make sure that all the nec‐\nessary data to perform the join operation for a particular user ID is brought together\nrecords with the same key form a group, and the next step is often to perform some\nset up the mappers so that the key-value pairs they produce use the desired grouping\nThe partitioning and sorting process then brings together all the records with the\nsame key in the same reducer.\nthe grouping key and bringing all the activity events for a particular user together in\none place, while distributing different users’ events across different partitions.\nactive database records are known as linchpin objects [38] or hot keys.\ncomplete when all of its mappers and reducers have completed, any subsequent jobs\nIf a join input has hot keys, there are a few algorithms you can use to compensate.\nFor example, the skewed join method in Pig first runs a sampling job to determine\nrecords relating to a hot key to one of several reducers, chosen at random (in contrast\nFor the other input to the join, records relating to the hot key need\nThis technique spreads the work of handling the hot key over several reducers, which\nWhen performing a join on that table, it uses a map-\nside join (see the next section) for the hot keys.\nWhen grouping records by a hot key and aggregating them, you can perform the\nparing the input data: extracting the key and value from each input record, assigning\nthe key-value pairs to a reducer partition, and sorting by key.\ncut-down MapReduce job in which there are no reducers and no sorting.\neach mapper simply reads one input file block from the distributed filesystem and\nseveral entries with the same key, and the join operator will output all matches for a key.\nFor example, imagine in the case of Figure 10-2 that the user database is small\nuser database from the distributed filesystem into an in-memory hash table.\nthis is done, the mapper can scan over the user activity events and simply look up the\nuser ID for each event in the hash table.vi\njoin (in the example of Figure 10-2, the activity events are the large input).\nInstead of loading the small join input into an in-memory hash table, an alternative is\nto store the small join input in a read-only index on the local disk [42].\nPartitioned hash joins\nIf the inputs to the map-side join are partitioned in the same way, then the hash join\nyou might arrange for the activity events and the user database to each be partitioned\nFor example, mapper 3 first loads all users with an ID ending in 3 into a hash\ntable, and then scans over all the activity events for each user whose ID ends in 3.\neach mapper to only read one partition from each of the input datasets.\nThis approach only works if both of the join’s inputs have the same number of parti‐\ntions, with records assigned to partitions based on the same key and the same hash\nIf the inputs are generated by prior MapReduce jobs that already perform\nPartitioned hash joins are known as bucketed map joins in Hive [37].\nAnother variant of a map-side join applies if the input datasets are not only parti‐\nreading both input files incrementally, in order of ascending key, and matching\nrecords with the same key.\nIf a map-side merge join is possible, it probably means that prior MapReduce jobs\nprinciple, this join could have been performed in the reduce stage of the prior job.\nonly job, for example if the partitioned and sorted datasets are also needed for other\nMapReduce workflows with map-side joins\nWhen the output of a MapReduce join is consumed by downstream jobs, the choice\nof map-side or reduce-side join affects the structure of the output.\nreduce-side join is partitioned and sorted by the join key, whereas the output of a\nmap-side join is partitioned and sorted in the same way as the large input (since one\nmap task is started for each file block of the join’s large input, regardless of whether a\nwhich the data is stored; you must also know the number of partitions and the keys\nusing indexes, in order to present them to a user (for example, on a web page).\nprocess is a very effective way of building the indexes: the mappers partition the set of\ndocuments as needed, each reducer builds the index for its partition, and the index\nKey-value stores as batch process output\nSearch indexes are just one example of the possible outputs of a batch processing\nThe output of those batch jobs is often some kind of database: for example, a data‐\nThese databases need to be queried from the web application that handles user\noutput from the batch process get back into a database where the web application can\ndirectly within a mapper or reducer, and to write from the batch job directly to the\n• Normally, MapReduce provides a clean all-or-nothing guarantee for job output:\nwrite it as files to the job’s output directory in the distributed filesystem, just like the\nvalue stores support building database files in MapReduce jobs, including Voldemort\nBuilding these database files is a good use of MapReduce: using a mapper to extract a\nThe handling of output from MapReduce jobs follows the same philosophy.\n• The same set of files can be used as input for various different jobs, including",
      "keywords": [
        "user",
        "join",
        "user database",
        "user activity events",
        "key",
        "activity events",
        "data",
        "MapReduce",
        "input",
        "batch processing",
        "database",
        "batch",
        "user activity",
        "MapReduce jobs",
        "output"
      ],
      "concepts": [
        "joins",
        "data",
        "job",
        "jobs",
        "processing",
        "process",
        "processes",
        "reduce",
        "reducer",
        "record"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 52,
          "title": "",
          "score": 0.724,
          "base_score": 0.574,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 44,
          "title": "",
          "score": 0.714,
          "base_score": 0.564,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 62,
          "title": "",
          "score": 0.643,
          "base_score": 0.493,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "",
          "score": 0.641,
          "base_score": 0.491,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 57,
          "title": "",
          "score": 0.571,
          "base_score": 0.421,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "join",
          "user",
          "activity",
          "activity events",
          "key"
        ],
        "semantic": [],
        "merged": [
          "join",
          "user",
          "activity",
          "activity events",
          "key"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2938369254288758,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.203482+00:00"
      }
    },
    {
      "chapter_number": 44,
      "title": "Segment 44 (pages 437-445)",
      "start_page": 437,
      "end_page": 445,
      "summary": "SQL queries on a cluster of machines, while the combination of MapReduce and a\nDatabases require you to structure data according to a particular model (e.g., rela‐\nwhich can be written using any data model and encoding.\nMPP databases typically require careful up-front modeling of the data and query pat‐\nterns before importing the data into the database’s proprietary storage format.\ndesirable, because it means users of the database have better-quality data to work\ning to decide on the ideal data model up front [54].\ncareful schema design required by an MPP database slows down that centralized data\nMapReduce and Distributed Filesystems \nThere may not even be one ideal data model, but rather different views onto the data\nhousing” on page 91): data from transaction processing systems is dumped into the\ndistributed filesystem in some raw form, and then MapReduce jobs are written to\nData modeling still happens, but it is in a separate\nDiversity of processing models\nysis, you most likely need a more general model of data processing.\nIf you have HDFS and MapReduce, you can build a SQL query execution engine\nHaving two processing models, SQL and MapReduce, was not enough: even\nCrucially, those various processing models can all be run on a single shared-use clus‐\nHadoop approach, there is no need to import the data into several different special‐\nNot having to move data\nNeither HBase nor Impala uses MapReduce, but both use HDFS for\nThey are very different approaches to accessing and processing data, but they\nWhen comparing MapReduce to MPP databases, two more differences in design\nprefer to keep as much data as possible in memory (e.g., using hash joins) to avoid\nOn the other hand, MapReduce can tolerate the failure of a map or reduce task\nIt is also very eager to write data to disk, partly for fault tolerance, and\nThe MapReduce approach is more appropriate for larger jobs: jobs that process so\nmuch data and run for such a long time that they are likely to experience at least one\nMapReduce and Distributed Filesystems \nTo understand the reasons for MapReduce’s sparing use of memory and task-level\noffline batch jobs run on the same machines.\nresources they use, and higher-priority processes cost more [59].\nHowever, as MapReduce jobs run at low priority, they run the risk\nAt Google, a MapReduce task that runs for an hour has an approximately 5% risk of\nAnd this is why MapReduce is designed to tolerate frequent unexpected task termina‐\nBeyond MapReduce\nplex processing job using the raw MapReduce APIs is actually quite hard and labori‐\nof MapReduce.\nlearn, and their higher-level constructs make many common batch processing tasks\nHowever, there are also problems with the MapReduce execution model itself, which\nvery robust: you can use it to process almost arbitrarily large quantities of data on an\nAs discussed previously, every MapReduce job is independent from every other job.\nBeyond MapReduce \nPublishing data to a well-known location in the distributed filesystem\non the distributed filesystem are simply intermediate state: a means of passing data\nsystems consisting of 50 or 100 MapReduce jobs [29], there is a lot of such intermedi‐\nThe process of writing out this intermediate state to files is called materialization.\nmaterialize the intermediate state, but instead stream the output to the input incre‐\nMapReduce’s approach of fully materializing intermediate state has downsides com‐\n• A MapReduce job can only start when all tasks in the preceding jobs (that gener‐\nate its inputs) have completed, whereas processes connected by a Unix pipe are\nuntil all of the preceding job’s tasks have completed slows down the execution of\nIn order to fix these problems with MapReduce, several new execution engines for\nSince they explicitly model the flow of data through several processing stages, these\n• Because all joins and data dependencies in a workflow are explicitly declared, the\nFor example, it can try to place the task that consumes some data\non the same machine as the task that produces it, so that the data can be\nBeyond MapReduce \nMapReduce already uses this optimization for mapper output, but dataflow\n• Operators can start executing as soon as their input is ready; there is no need to\n• Existing Java Virtual Machine (JVM) processes can be reused to run new opera‐\nYou can use dataflow engines to implement the same computations as MapReduce\nthat it is durable, which makes fault tolerance fairly easy in MapReduce: if a task fails,\nmachine is lost, it is recomputed from other data that is still available (a prior inter‐\nmediary stage if possible, or otherwise the original input data, which is normally on\ndata was computed—which input partitions it used, and which operators were\ning the ancestry of data [61], while Flink checkpoints operator state, allowing it to\nWhen recomputing data, it is important to know whether the computation is deter‐\nministic: that is, given the same input data, do the operators always produce the same\nIf the operator is restarted and the recomputed data is not the same\nas the original lost data, it becomes very hard for downstream operators to resolve the\nagain on the new data.\nintermediate data is much smaller than the source data, or if the computation is very\nCPU-intensive, it is probably cheaper to materialize the intermediate data to files\nReturning to the Unix analogy, we saw that MapReduce is like writing the output of\nfor the input to be complete before starting to process it.\nA sorting operation inevitably needs to consume its entire input before it can pro‐\nWhen the job completes, its output needs to go somewhere durable so that users can\ninputs and the final outputs of a job.\nLike with MapReduce, the inputs are immutable\nBeyond MapReduce ",
      "keywords": [
        "data",
        "MapReduce",
        "processing",
        "job",
        "MPP databases",
        "distributed filesystem",
        "output",
        "Intermediate State",
        "Batch Processing",
        "input",
        "processing models",
        "task",
        "distributed",
        "MPP",
        "MapReduce jobs"
      ],
      "concepts": [
        "data",
        "processing",
        "process",
        "processes",
        "task",
        "model",
        "machine",
        "difference",
        "different",
        "differ"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 3,
          "title": "",
          "score": 0.793,
          "base_score": 0.643,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "",
          "score": 0.782,
          "base_score": 0.632,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 62,
          "title": "",
          "score": 0.727,
          "base_score": 0.577,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 43,
          "title": "",
          "score": 0.714,
          "base_score": 0.564,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 52,
          "title": "",
          "score": 0.714,
          "base_score": 0.564,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "mapreduce",
          "jobs",
          "task",
          "intermediate",
          "job"
        ],
        "semantic": [],
        "merged": [
          "mapreduce",
          "jobs",
          "task",
          "intermediate",
          "job"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3914777564733949,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.203542+00:00"
      }
    },
    {
      "chapter_number": 45,
      "title": "Segment 45 (pages 446-458)",
      "start_page": 446,
      "end_page": 458,
      "summary": "Graphs and Iterative Processing\nIn “Graph-Like Data Models” on page 49 we discussed using graphs for modeling\ndata, and using graph query languages to traverse the edges and vertices in a graph.\nIt is also interesting to look at graphs in a batch processing context, where the goal is\nto perform some kind of offline processing or analysis on an entire graph.\ngraph processing: in dataflow engines, the flow of data from one\noperator to another is structured as a graph, while the data itself\nIn graph processing, the\ndata itself has the form of a graph.\nMany graph algorithms are expressed by traversing one edge at a time, joining one\n1. An external scheduler runs a batch process to calculate one step of the algorithm.\n2. When the batch process completes, the scheduler checks whether it has finished\nround of the batch process.\nChapter 10: Batch Processing\nThe Pregel processing model\nAs an optimization for batch processing graphs, the bulk synchronous parallel (BSP)\nprocessing graphs [72].\nanother vertex, and typically those messages are sent along the edges in a graph.\nto the next, so the function only needs to process new incoming messages.\ncalled “thinking like a vertex”), the framework may partition the graph in arbitrary\nAs a result, graph algorithms often have a lot of cross-machine communication over‐\ntributed batch process [73, 74].\nthe disks of a single computer, single-machine processing using a framework such as\nrobust enough to store and process many petabytes of data on clusters of over 10,000\nAs the problem of physically operating batch processes at such scale has\nChapter 10: Batch Processing\nThe choice of join algorithm can make a big difference to the performance of a batch\nmizers that can take advantage of them during execution, batch processing frame‐\nness reporting, but that is just one among many domains in which batch processing\nimplements various algorithms for machine learning on top of MapReduce, Spark,\nChapter 10: Batch Processing\nBatch processing engines are being used for distributed execution of algorithms from\nAs batch processing systems gain built-in\nend, they are all just systems for storing and processing data.\nThe two main problems that distributed batch processing frameworks need to solve\nIn MapReduce, mappers are partitioned according to input file blocks.\nWe discussed several join algorithms for MapReduce, most of which are also inter‐\nOne of the two join inputs is small, so it is not partitioned and it can be entirely\nIf the two join inputs are partitioned in the same way (using the same key, same\nDistributed batch processing engines have a deliberately restricted programming\nThanks to the framework, your code in a batch processing job does not need to worry\nThe distinguishing feature of a batch processing job is that it reads some input data\nded—that is, you still have a job, but its inputs are never-ending streams of data.\nChapter 10: Batch Processing\nWe shall see that stream and batch processing are similar in some\n[1] Jeffrey Dean and Sanjay Ghemawat: “MapReduce: Simplified Data Processing on\nLarge Clusters,” at 6th USENIX Symposium on Operating System Design and Imple‐\n[2] Joel Spolsky: “The Perils of JavaSchools,” joelonsoftware.com, December 25, 2005.\nMapReduce Systems,” Foundations and Trends in Databases, volume 5, number 1,\nwards,” originally published at databasecolumn.vertica.com, January 17, 2008.\nReduce at Google,” the-paper-trail.org, June 25, 2014.\n[6] “The Hollerith Machine,” United States Census Bureau, census.gov.\nCluster,” aadrake.com, January 25, 2014.\nData,” martin.kleppmann.com, August 5, 2015.\nRichie: “Advice from Doug McIlroy,” cm.bell-labs.com.\nForeword,” The Bell System Technical Journal, volume 57, number 6, pages 1899–\nDelimited Text,” ronaldduncan.wordpress.com, October 31, 2009.\n[15] Alan Kay: “Is ‘Software Engineering’ an Oxymoron?,” tinlizzie.org.\n[16] Martin Fowler: “InversionOfControl,” martinfowler.com, June 26, 2005.\ntems,” Bell Labs Technical Journal, volume 4, number 2, pages 146–152, April 1999.\ntem,” at 19th ACM Symposium on Operating Systems Principles (SOSP), October\nSystem,” Proceedings of the VLDB Endowment, volume 6, number 11, pages 1092–\nCoding in Apache Hadoop,” blog.cloudera.com, September 23, 2015.\n[23] Peter Cnudde: “Hadoop Turns 10,” yahoohadoop.tumblr.com, February 5, 2016.\ngies,” hortonworks.com, July 25, 2012.\n[25] Brendan Gregg: “Manta: Unix Meets Map Reduce,” dtrace.org, June 25, 2013.\nLinkedIn,” at ACM International Conference on Management of Data (SIGMOD),\nDataflow System on Top of Map-Reduce: The Pig Experience,” at 35th International\nData Warehouse Using Hadoop,” at 26th IEEE International Conference on Data\nChapter 10: Batch Processing\n[33] “Apache Crunch User Guide,” Apache Software Foundation, crunch.apache.org.\ncient Data-Parallel Pipelines,” at 31st ACM SIGPLAN Conference on Programming\n[35] Jay Kreps: “Why Local State is a Fundamental Primitive in Stream Processing,”\nStronger Consistency at Scale,” at 15th USENIX Workshop on Hot Topics in Operat‐\n[39] Sriranjan Manjunath: “Skewed Join,” wiki.apache.org, 2009.\n“Practical Skew Handling in Parallel Joins,” at 18th International Conference on Very\nOpen-Source SQL Engine for Hadoop,” at 7th Biennial Conference on Innovative\ning Side Data,” engineering.linkedin.com, October 26, 2015.\ntributed Transactions and Notifications,” at 9th USENIX conference on Operating Sys‐\n[44] ““Cloudera Search User Guide,” Cloudera, Inc., September 2015.\nat LinkedIn,” at 6th Workshop on Recommender Systems and the Social Web\nted Data with Project Voldemort,” at 10th USENIX Conference on File and Storage\nated Data,” engineering.pinterest.com, September 14, 2015.\n[50] Nathan Marz: “How to Beat the CAP Theorem,” nathanmarz.com, October 13,\nHigh Performance Database Systems,” Communications of the ACM, volume 35,\nPractices for Big Data,” Proceedings of the VLDB Endowment, volume 2, number 2,\nData Lake Question,” adaptivesystemsinc.com, July 2, 2015.\n[57] Bobby Johnson and Joseph Adler: “The Sushi Principle: Raw Data Is Better,” at\nHadoop YARN: Yet Another Resource Negotiator,” at 4th ACM Symposium on\nManagement at Google with Borg,” at 10th European Conference on Computer Sys‐\n[60] Malte Schwarzkopf: “The Evolution of Cluster Scheduler Architectures,” firma‐\ntributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing,”\nChapter 10: Batch Processing\ning,” at Hadoop Summit, June 2014.\nwork for Modeling and Building Data Processing Applications,” at ACM Interna‐\nPlatform for Big Data Analytics,” The VLDB Journal, volume 23, number 6, pages\n[67] Michael Isard, Mihai Budiu, Yuan Yu, et al.: “Dryad: Distributed Data-Parallel\nPrograms from Sequential Building Blocks,” at European Conference on Computer\n[68] Daniel Warneke and Odej Kao: “Nephele: Efficient Parallel Data Processing in\nthe Cloud,” at 2nd Workshop on Many-Task Computing on Grids and Supercomputers\nValiant: “A Bridging Model for Parallel Computation,” Communica‐\nning Fast Iterative Data Flows,” Proceedings of the VLDB Endowment, volume 5,\nfor Large-Scale Graph Processing,” at ACM International Conference on Management\nCOST?,” at 15th USENIX Workshop on Hot Topics in Operating Systems (HotOS),\nOne for All in Data Processing Systems,” at 10th European Conference on Computer\nComputation on Just a PC,” at 10th USENIX Symposium on Operating Systems\nics,” Communications of the ACM, volume 59, number 5, pages 78–87, May 2016.\n[77] Fabian Hüske: “Peeking into Apache Flink’s Engine Room,” flink.apache.org,\nview,” hortonworks.com, March 2, 2015.\nData Processing in Spark,” at ACM International Conference on Management of Data\ncessing,” blog.cloudera.com, April 6, 2016.\nChapter 10: Batch Processing",
      "keywords": [
        "batch processing",
        "Data",
        "Processing",
        "Data Processing Systems",
        "distributed batch processing",
        "Data Processing",
        "Batch processing engines",
        "batch processing systems",
        "batch processing graphs",
        "batch",
        "graph",
        "batch processing job",
        "Data Systems Research",
        "ACM International Conference",
        "systems"
      ],
      "concepts": [
        "data",
        "processing",
        "process",
        "processes",
        "systems",
        "doi",
        "joining",
        "engines",
        "engineering",
        "october"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 60,
          "title": "",
          "score": 0.755,
          "base_score": 0.605,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 9,
          "title": "",
          "score": 0.742,
          "base_score": 0.592,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 52,
          "title": "",
          "score": 0.706,
          "base_score": 0.556,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 62,
          "title": "",
          "score": 0.695,
          "base_score": 0.545,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 10,
          "title": "",
          "score": 0.685,
          "base_score": 0.535,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "batch",
          "processing",
          "batch processing",
          "graph",
          "conference"
        ],
        "semantic": [],
        "merged": [
          "batch",
          "processing",
          "batch processing",
          "graph",
          "conference"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3574473913503398,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.203604+00:00"
      }
    },
    {
      "chapter_number": 46,
      "title": "Segment 46 (pages 459-473)",
      "start_page": 459,
      "end_page": 473,
      "summary": "Messaging Systems\nA common approach for notifying consumers about new events is to use a messaging\nsystem: a producer sends a message containing the event, which is then pushed to\nducer and consumer would be a simple way of implementing a messaging system.\nsystem allows multiple producer nodes to send messages to the same topic and allows\nmultiple consumer nodes to receive messages in a topic.\n1. What happens if the producers send messages faster than the consumers can pro‐\ncontrol; i.e., blocking the producer from sending more messages).\nIf messages are buffered in a queue, it is important to understand what happens\nory, or does it write messages to disk?\nperformance of the messaging system [6]?\nDirect messaging from producers to consumers\nA number of messaging systems use direct network communication between produc‐\n131) to push messages to the consumer.\nAlthough these direct messaging systems work well in the situations for which they\nof message loss.\nIf a consumer is offline, it may miss messages that were sent while it is unreachable.\nSome protocols allow the producer to retry failed message deliveries, but this\napproach may break down if the producer crashes, losing the buffer of messages that\nMessage brokers\nA widely used alternative is to send messages via a message broker (also known as a\nmessage queue), which is essentially a kind of database that is optimized for handling\nmessage streams [13].\nProducers write messages to the broker, and consumers receive them\nSome message brokers only keep messages in memory,\na producer sends a message, it normally only waits for the broker to confirm that it\nhas buffered the message and does not wait for the message to be processed by con‐\nMessage brokers compared to databases\nSome message brokers can even participate in two-phase commit protocols using XA\ndifferences between message brokers and databases:\n• Databases usually keep data until it is explicitly deleted, whereas most message\nbrokers automatically delete a message when it has been successfully delivered to\nSuch message brokers are not suitable for long-term data storage.\n• Since they quickly delete messages, most message brokers assume that their\na lot of messages because the consumers are slow (perhaps spilling messages to\ndata, while message brokers often support some way of subscribing to a subset of\nmessage brokers do not support arbitrary queries, but they do notify clients when\ndata changes (i.e., when new messages become available).\nThis is the traditional view of message brokers, which is encapsulated in standards\nWhen multiple consumers read messages in the same topic, two main patterns of\nmessaging are used, as illustrated in Figure 11-1:\nEach message is delivered to one of the consumers, so the consumers can share\nthe work of processing the messages in the topic.\nThis pattern is useful when the messages are\nEach message is delivered to all of the consumers.\npendent consumers to each “tune in” to the same broadcast of messages, without\nsumers; (b) fan-out: delivering each message to multiple consumers.\nbut within each group only one of the nodes receives each message.\nConsumers may crash at any time, so it could happen that a broker delivers a mes‐\nIn order to ensure that the message is not lost, message brokers use\ning a message so that the broker can remove it from the queue.\nacknowledgment, it assumes that the message was not processed, and therefore it\ndelivers the message again to another consumer.\nmessage actually was fully processed, but the acknowledgment was lost in the net‐\neffect on the ordering of messages.\nmessages in the order they were sent by producers.\nwhile processing message m3, at the same time as consumer 1 is processing message\nThe unacknowledged message m3 is subsequently redelivered to consumer 1,\nwith the result that consumer 1 processes messages in the order m4, m3, m5.\nEven if the message broker otherwise tries to preserve the order of messages (as\nEven message brokers that durably write messages to disk quickly delete them\nIf you add a new consumer to a messaging system, it typically only starts receiving\nbased message brokers.\nUsing logs for message storage\nThe same structure can be used to implement a message broker: a producer sends a\nmessage by appending it to the end of the log, and a consumer receives messages by\nnotification that a new message has been appended.\nmessages within a partition are totally ordered.\nProducers send messages by appending them to a topic-partition file, and\n[20, 21] are log-based message brokers that work like this.\nEven though these message brokers write all messages to disk, they are able to achieve\nthroughput of millions of messages per second by partitioning across multiple\nLogs compared to traditional messaging\nThe log-based approach trivially supports fan-out messaging, because several con‐\nconsumers, instead of assigning individual messages to consumer clients, the broker\nEach client then consumes all the messages in the partitions it has been assigned.\nTypically, when a consumer has been assigned a log partition, it reads the messages in\nmessage processing over a thread pool, but that approach complicates consumer offset management.\nnumber of log partitions in that topic, because messages within the same parti‐\n• If a single message is slow to process, it holds up the processing of subsequent\nThus, in situations where messages may be expensive to process and you want to par‐\nallelize processing on a message-by-message basis, and where message ordering is not\nso important, the JMS/AMQP style of message broker is preferable.\ncess and where message ordering is important, the log-based approach works very\nConsuming a partition sequentially makes it easy to tell which messages have been\nprocessed: all messages with an offset less than a consumer’s current offset have\nalready been processed, and all messages with a greater offset have not yet been seen.\nconsumer’s partitions, and it starts consuming messages at the last recorded offset.\nthe consumer had processed subsequent messages but not yet recorded their offset,\nthose messages will be processed a second time upon restart.\nThis means that if a slow consumer cannot keep up with the rate of messages, and it\nsome of the messages.\nThus, the disk can buffer 11 hours’ worth of messages, after which it will start\nmessages.\nRegardless of how long you retain messages, the throughput of a log remains more or\nless constant, since every message is written to disk anyway [18].\nIf a consumer falls so far behind that the messages it requires are older than what is\nmessages.\nEven if a consumer does fall too far behind and starts missing messages, only that\nThis behavior also contrasts with traditional message brokers, where you need to be\nWe noted previously that with AMQP- and JMS-style message brokers, processing\nOn the other hand, in a log-based message broker, con‐\nsuming messages is more like reading from a file: it is a read-only operation that does\nThe only side effect of processing, besides any output of the consumer, is that the\nThis aspect makes log-based messaging more like the batch processes of the last\nWe have drawn some comparisons between message brokers and databases.\nthat log-based message brokers have been successful in taking ideas from databases\nand applying them to messaging.",
      "keywords": [
        "messages",
        "Message brokers",
        "consumer",
        "Stream Processing",
        "Processing",
        "event streams",
        "Transmitting Event Streams",
        "streams",
        "event",
        "broker",
        "data",
        "log",
        "Databases",
        "Messaging",
        "Stream Processing Databases"
      ],
      "concepts": [
        "messaging",
        "message",
        "consumers",
        "consumes",
        "processing",
        "process",
        "processes",
        "log",
        "logs",
        "logging"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 50,
          "title": "",
          "score": 0.685,
          "base_score": 0.535,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 32,
          "title": "",
          "score": 0.505,
          "base_score": 0.505,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 39,
          "title": "",
          "score": 0.496,
          "base_score": 0.496,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 60,
          "title": "",
          "score": 0.487,
          "base_score": 0.487,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 58,
          "title": "",
          "score": 0.486,
          "base_score": 0.486,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "messages",
          "message",
          "consumer",
          "message brokers",
          "brokers"
        ],
        "semantic": [],
        "merged": [
          "messages",
          "message",
          "consumer",
          "message brokers",
          "brokers"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3276703850722112,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.203657+00:00"
      }
    },
    {
      "chapter_number": 47,
      "title": "Segment 47 (pages 474-481)",
      "start_page": 474,
      "end_page": 481,
      "summary": "stream of database write events, produced by the leader as it processes transactions.\nThe followers apply that stream of writes to their own copy of the database and thus\nThe events in the replication log\ndescribe the data changes that occurred.\ncast” on page 348, which states: if every event represents a write to the database, and\nwhen data changes: for example, first writing to the database, then updating the\nclients first write the new value to the database, then write it to the search index.\nDatabases and Streams \nChange Data Capture\nThe problem with most databases’ replication logs is that they have long been consid‐\nare supposed to query the database through its data model and query language, not\nparse the replication logs and try to extract data from them.\nlog of changes written to them.\nMore recently, there has been growing interest in change data capture (CDC), which\nis the process of observing all data changes written to a database and extracting them\nFor example, you can capture the changes in a database and continually apply the\nIf the log of changes is applied in the same order, you\ncan expect the data in the search index to match the data in the database.\nindex and any other derived data systems are just consumers of the change stream, as\nTaking data in the order it was written to one database, and applying the\nImplementing change data capture\nWe can call the log consumers derived data systems, as discussed in the introduction\nChange data capture is a mechanism for\nEssentially, change data capture makes one database the leader (the one from which\nbroker is well suited for transporting the change events from the source database,\nDatabase triggers can be used to implement change data capture (see “Trigger-based\nreplication” on page 161) by registering triggers that observe all changes to data\nLike message brokers, change data capture is usually asynchronous: the system of\nrecord database does not wait for the change to be applied to consumers before com‐\nIf you have the log of all changes that were ever made to a database, you can recon‐\nstruct the entire state of the database by replaying the log.\n—it is not sufficient to only apply a log of recent changes, since it would be missing\nDatabases and Streams \nThe same idea works in the context of log-based message brokers and change data\nIncreasingly, databases are beginning to support change streams as a first-class inter‐\nthe MongoDB oplog to subscribe to data changes and update the user interface [39].\nVoltDB allows transactions to continuously export data from a database in the form\nThe database represents an output stream in the relational data\ncan asynchronously consume this log and use it to update derived data systems.\nKafka Connect [41] is an effort to integrate change data capture tools for a wide\nOnce the stream of change events is in Kafka, it\ncan be used to update derived data systems such as search indexes, and also feed into\nSimilarly to change data capture, event sourcing involves storing all changes to the\napplication state as a log of change events.\n• In change data capture, the application uses the database in a mutable way,\nThe log of changes is extracted from the\ndatabase at a low level (e.g., by parsing the replication log), which ensures that\nwriting to the database does not need to be aware that CDC is occurring.\nble events that are written to an event log.\nEvent sourcing is a powerful technique for data modeling: from an application point\nDatabases and Streams \nEvent sourcing is similar to the chronicle data model [45], and there are also similari‐\nties between an event log and the fact table that you find in a star schema (see “Stars\nSpecialized databases such as Event Store [46] have been developed to support appli‐\nA conventional database or a log-based message broker can also be used\nDeriving current state from the event log\nAn event log by itself is not very useful, because users generally expect to see the cur‐\nThus, applications that use event sourcing need to take the log of events (representing\nrun it again and derive the same application state from the event log.\nLike with change data capture, replaying the event log allows you to reconstruct the\nmost recent event for that primary key, and log compaction can discard previous\nApplications that use event sourcing typically have some mechanism for storing\nsnapshots of the current state that is derived from the log of events, so they don’t\nable to store all raw events forever and reprocess the full event log whenever required.\nsumer sees the event, it is already an immutable part of the log, and it may have\nevent sourcing and change data capture so powerful.\nThe nature of state is that it changes, so databases support updating and\nDatabases and Streams ",
      "keywords": [
        "data",
        "event",
        "database",
        "log",
        "Event Sourcing",
        "derived data systems",
        "Data Capture",
        "event log",
        "data systems",
        "search index",
        "derived data",
        "database write events",
        "stream",
        "Systems",
        "Log compaction"
      ],
      "concepts": [
        "events",
        "databases",
        "data",
        "processes",
        "processing",
        "process",
        "changes",
        "logs",
        "application",
        "applications"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 20,
          "title": "",
          "score": 0.807,
          "base_score": 0.657,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 19,
          "title": "",
          "score": 0.782,
          "base_score": 0.632,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 21,
          "title": "",
          "score": 0.686,
          "base_score": 0.536,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 23,
          "title": "",
          "score": 0.678,
          "base_score": 0.528,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 40,
          "title": "",
          "score": 0.659,
          "base_score": 0.509,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "log",
          "event",
          "change",
          "capture",
          "data capture"
        ],
        "semantic": [],
        "merged": [
          "log",
          "event",
          "change",
          "capture",
          "data capture"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.38077597099139243,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.203714+00:00"
      }
    },
    {
      "chapter_number": 48,
      "title": "Segment 48 (pages 482-489)",
      "start_page": 482,
      "end_page": 489,
      "summary": "Whenever you have state that changes, that state is the result of the events that muta‐\nNo matter how the state changes, there was always a sequence of events that caused\nyou get when you integrate an event stream over time, and a change stream is what\nThe relationship between the current application state and an event\nstream.\nIf you consider the log of events to be your system of record, and any\nTransaction logs record all the changes made to the database.\nChapter 11: Stream Processing\nrecorded in an append-only ledger, which is essentially a log of events describing\nImmutable events also capture more information than just the current state.\ntion is recorded in an event log, but would be lost in a database that deletes items\nDeriving several views from the same event log\nMoreover, by separating mutable state from the immutable event log, you can derive\nseveral different read-oriented representations from the same log of events.\nworks just like having multiple consumers of a stream (Figure 11-5): for example, the\nHaving an explicit translation step from an event log to a database makes it easier to\npresents your existing data in some new way, you can use the event log to build a\nDatabases and Streams \nevent log to read-optimized application state: it is entirely reasonable to denormalize\ndata in the read-optimized views, as the translation process gives you a mechanism\nfor keeping it consistent with the event log.\nThe biggest downside of event sourcing and change data capture is that the consum‐\ners of the event log are usually asynchronous, so there is a possibility that a user may\nappending the event to the log.\nan atomic unit, so either you need to keep the event log and the read view in the same\nOn the other hand, deriving the current state from an event log also simplifies some\nChapter 11: Stream Processing\nIf the event log and the application state are partitioned in the same way (for exam‐\nple, processing an event for a customer in partition 3 only requires updating partition\nno concurrency control for writes—by construction, it only processes a single event\nan event touches multiple state partitions, a bit more work is required, which we will\nIn these circumstances, it’s not sufficient to just append another event to the log to\nDatabases and Streams \nProcessing Streams\nevents, sensors, and writes to databases), and we have talked about how streams are\n1. You can take the data in the events and write it to a database, cache, search index,\n2. You can push the events to users in some way, for example by sending email\nalerts or push notifications, or by streaming the events to a real-time dashboard\nstream.\n3. You can process one or more input streams to produce one or more output\nstreams.\nIn the rest of this chapter, we will discuss option 3: processing streams to produce\nother, derived streams.\ndiscussed in Chapter 10, and the pattern of dataflow is similar: a stream processor\nChapter 11: Stream Processing\nUses of Stream Processing\nHowever, other uses of stream processing have also emerged over time.\nComplex event processing\nComplex event processing (CEP) is an approach developed in the 1990s for analyzing\nevent streams, especially geared toward the kind of application that requires search‐\nspecify rules to search for certain patterns of events in a stream.\nqueries are submitted to a processing engine that consumes the input streams and\ntransient: when a query comes in, the database searches for data matching the query,\nroles: queries are stored long-term, and events from the input streams continuously\nflow past them in search of a query that matches an event pattern [68].\nProcessing Streams \n• Measuring the rate of some type of event (how often it occurs per time interval)\nStream analytics systems sometimes use probabilistic algorithms, such as Bloom fil‐\ntion algorithms sometimes leads people to believe that stream processing systems are\nabout stream processing, and probabilistic algorithms are merely an optimization\nChapter 11: Stream Processing\nWe saw in “Databases and Streams” on page 451 that a stream of changes to a data‐\nbase can be used to keep derived data systems, such as caches, search indexes, and\nSimilarly, in event sourcing, application state is maintained by applying a log of\nevents; here the application state is also a kind of materialized view.\ntime window: building the materialized view potentially requires all events over an\narbitrary time period, apart from any obsolete events that may be discarded by log\nSearch on streams\nBesides CEP, which allows searching for patterns consisting of multiple events, there\nis also sometimes a need to search for individual events based on complex criteria,\nand then continually matching the stream of news items against this query.\nBy contrast, searching a stream turns the processing on its head: the queries\nProcessing Streams ",
      "keywords": [
        "event log",
        "Stream Processing",
        "stream",
        "event",
        "log",
        "data",
        "state",
        "Processing",
        "event stream",
        "Stream analytics",
        "time",
        "stream processing systems",
        "database",
        "systems",
        "application state"
      ],
      "concepts": [
        "data",
        "streams",
        "events",
        "processed",
        "process",
        "processes",
        "database",
        "systems",
        "time",
        "search"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 38,
          "title": "",
          "score": 0.62,
          "base_score": 0.62,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 50,
          "title": "",
          "score": 0.587,
          "base_score": 0.587,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 47,
          "title": "",
          "score": 0.541,
          "base_score": 0.541,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 55,
          "title": "",
          "score": 0.52,
          "base_score": 0.52,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 49,
          "title": "",
          "score": 0.498,
          "base_score": 0.498,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "event",
          "stream",
          "event log",
          "events",
          "log"
        ],
        "semantic": [],
        "merged": [
          "event",
          "stream",
          "event log",
          "events",
          "log"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.37055984668524494,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:02:02.203782+00:00"
      }
    },
    {
      "chapter_number": 49,
      "title": "Segment 49 (pages 490-497)",
      "start_page": 490,
      "end_page": 497,
      "summary": "events, we normally don’t think of them as stream processors:\nallows user queries to be farmed out to a set of nodes that also process event streams;\nthese queries are then interleaved with events from the input streams, and results can\ncess needs to look at the timestamp embedded in each event.\ntime at which the process is run has nothing to do with the time at which the events\nA batch process may read a year’s worth of historical events within a few minutes; in\nMoreover, using the timestamps in the events allows the processing to be\non the processing machine (the processing time) to determine windowing [79].\nlater than the time at which the event actually occurred.\nEvent time versus processing time\nNow stream processors will first see the B event and then the A event,\nthe event timestamp, and the date when you watched the movie is the processing\nConfusing event time and processing time leads to bad data.\nminute and process the backlog of events when it comes back up.\nProcessing Streams \nA tricky problem when defining windows in terms of event time is that you can never\nbe sure when you have received all of the events for a particular window, or whether\nFor example, say you’re grouping events into one-minute windows so that you can\nYou can time out and declare a window ready after you have not seen any new events\nhandle such straggler events that arrive after the window has already been declared\nany consumers of this stream, the events will appear as extremely delayed stragglers.\nIn this context, the timestamp on the events should really be the time at which the\nThe time at which the event was received by the server (according to the\n• The time at which the event occurred, according to the device clock\n• The time at which the event was sent to the server, according to the device clock\n• The time at which the event was received by the server, according to the server\nthe event timestamp, and thus estimate the true time at which the event actually\noccurred (assuming the device clock offset did not change between the time the event\nProcessing Streams \nA tumbling window has a fixed length, and every event belongs to exactly one\nFor example, if you have a 1-minute tumbling window, all the events\nevents between 10:04:00 and 10:04:59 into the next window, and so on.\ncould implement a 1-minute tumbling window by taking each event timestamp\nthe next window would cover events between 10:04:00 and 10:08:59, and so on.\nA sliding window contains all the events that occur within some interval of each\nFor example, a 5-minute sliding window would cover events at 10:03:39\nhopping 5-minute windows would not have put these two events in the same\nit is defined by grouping together all events for the same user that occur closely\ntime (for example, if there have been no events for 30 minutes).\nsame need for joins on streams.\nHowever, the fact that new events can appear anytime on a stream makes joins on\nStream-stream join (window join)\nin searched-for URLs. Every time someone types a search query, you log an event\nsearch results, you log another event recording the click.\nsearch event.\njoining the events: doing so would only tell you about the cases where the user\nwhich you need both the search events and the click events.\nTo implement this type of join, a stream processor needs to maintain state: for exam‐\nstream processor also checks the other index to see if another event for the same ses‐\nexample of a batch job joining two datasets: a set of user activity events and a data‐\nIt is natural to think of the user activity events as a stream, and\nProcessing Streams \nstream of activity events containing a user ID, and the output is a stream of activity\nevents in which the user ID has been augmented with profile information about the\nThis process is sometimes known as enriching the activity events with informa‐\nTo perform this join, the stream process needs to look at one activity event at a time,\nlook up the event’s user ID in the database, and add the profile information to the\nactivity event.\nthe database are likely to change over time, so the stream processor’s local copy of the\nas the stream of activity events.\nence is that for the table changelog stream, the join uses a window that reaches back\nand maintaining this cache requires the following event processing:\ntables u·v, something interesting happens: the stream of changes to the materialized join follows the product\nThe stream process needs to maintain a database containing the set of\n(search and click events, user profiles, or follower list) based on one join input, and\nThis raises a question: if events on different streams happen around a similar time, in\nIn the stream-table join example, if a user updates\ntheir profile, which activity events are joined with the old profile (processed before\nProcessing Streams ",
      "keywords": [
        "events",
        "stream",
        "Time",
        "stream processing",
        "window",
        "stream processor",
        "processing",
        "user",
        "activity events",
        "user activity events",
        "processing time",
        "join",
        "Event time",
        "process event streams",
        "stream process"
      ],
      "concepts": [
        "events",
        "windows",
        "stream",
        "processing",
        "process",
        "time",
        "timing",
        "users",
        "joins",
        "tweets"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 50,
          "title": "",
          "score": 0.609,
          "base_score": 0.609,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 52,
          "title": "",
          "score": 0.556,
          "base_score": 0.556,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 48,
          "title": "",
          "score": 0.498,
          "base_score": 0.498,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 62,
          "title": "",
          "score": 0.493,
          "base_score": 0.493,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 33,
          "title": "",
          "score": 0.429,
          "base_score": 0.429,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "events",
          "event",
          "window",
          "stream",
          "activity"
        ],
        "semantic": [],
        "merged": [
          "events",
          "event",
          "window",
          "stream",
          "activity"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.28763305549066487,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:02:02.203832+00:00"
      }
    },
    {
      "chapter_number": 50,
      "title": "Segment 50 (pages 498-509)",
      "start_page": 498,
      "end_page": 509,
      "summary": "If the ordering of events across streams is undetermined, the join becomes nondeter‐\nmay in fact be processed multiple times, the visible effect in the output is as if they\nThe same issue of fault tolerance arises in stream processing, but it is less straightfor‐\nan option, because a stream is infinite and so you can never finish processing it.\nChapter 11: Stream Processing\nStreaming [91].\nriers in the message stream, similar to the boundaries between microbatches, but\nWithin the confines of the stream processing framework, the microbatching and\nHowever, as soon as output leaves the stream processor (for example, by writing\nwe need to ensure that all outputs and side effects of processing an event take effect if\nProcessing Streams \nthem internal by managing both state changes and messaging within the stream pro‐\nprocessing several input messages within a single transaction.\nAny stream process that requires state—for example, any windowed aggregations\nChapter 11: Stream Processing\n“Stream-table join (stream enrichment)” on page 473.\nThen, when the stream\nto durable storage such as HDFS [92, 93]; Samza and Kafka Streams replicate state\nIn some ways, stream processing is very much like the batch pro‐\nbrokers and event logs serve as the streaming equivalent of a filesystem.\nold messages again after they have been processed.\nlast message they have processed.\napproach is especially appropriate for stream processing systems that consume input\nstreams and generate derived state or derived output streams.\nThe facilities for maintaining state as streams and replaying messages are also the\nWe discussed several purposes of stream processing, includ‐\naggregations (stream analytics), and keeping derived data systems up to date (materi‐\nWe distinguished three types of joins that may appear in stream processes:\nBoth input streams consist of activity events, and the join operator searches for\nChapter 11: Stream Processing\nStream-table joins\nOne input stream consists of activity events, while the other is a database change‐\nBoth input streams are database changelogs.\nHowever, since a stream process is long-running\nUnbounded, Out-of-Order Data Processing,” Proceedings of the VLDB Endowment,\nMIT Press, 2005.\nNew Class of Data Management Applications,” at 28th International Conference on\n[10] Ian Malpass: “Measure Anything, Measure Everything,” codeascraft.com, Febru‐\n[16] “Google Cloud Pub/Sub: A Google-Scale Messaging Service,” cloud.google.com,\n[17] “Apache Kafka 0.9 Documentation,” kafka.apache.org, November 2015.\ntem for Log Processing,” at 6th International Workshop on Networking Meets Data‐\n[19] “Amazon Kinesis Streams Developer Guide,” docs.aws.amazon.com, April 2016.\nPerformance Replicated Log Service,” blog.twitter.com, September 16, 2015.\nThree Cheap Machines),” engineering.linkedin.com, April 27, 2014.\nLinkedIn,” engineering.linkedin.com, September 2, 2015.\nTime Data’s Unifying Abstraction,” engineering.linkedin.com, December 16, 2013.\nChapter 11: Stream Processing\nNarayan: “Sherpa Update,” developer.yahoo.com, June 8, .\nKafka,” martin.kleppmann.com, April 23, 2015.\nKafka,” engineeringblog.yelp.com, August 1, 2016.\n[32] “Mongoriver,” Stripe, Inc., github.com, September 2014.\n[33] Dan Harvey: “Change Data Capture with Mongo + Kafka,” at Hadoop Users\n[36] Slava Akhmechet: “Advancing the Realtime Web,” rethinkdb.com, January 27,\n[37] “Firebase Realtime Database Documentation,” Google, Inc., firebase.google.com,\nOplog Instead of Poll-and-Diff,” info.meteor.com, December 17, 2013.\nImporting and Exporting Live Data,” VoltDB 6.4 User Manual,\nLatency Data Pipelines,” confluent.io, February 18, 2016.\n[43] Martin Fowler: “Event Sourcing,” martinfowler.com, December 12, 2005.\nMaintenance Issues for the Chronicle Data Model,” at 14th ACM SIGACT-SIGMOD-\n[46] “Event Store 3.5.0 Documentation,” Event Store LLP, docs.geteventstore.com,\n[47] Martin Kleppmann: Making Sense of Stream Processing.\nMIT Press, 1999.\nDuplicates,” at ACM International Conference on Management of Data (SIGMOD),\n[54] Pat Helland: “Accountants Don’t Use Erasers,” blogs.msdn.com, June 14, 2007.\namarkets,” metamarkets.com, June 3, 2015.\nfor Fastest Cloud Compute,” yahoohadoop.tumblr.com, April 13, 2015.\nLambda,” engineering.linkedin.com, June 27, 2016.\n[58] Martin Fowler: “CQRS,” martinfowler.com, July 14, 2011.\n[59] Greg Young: “CQRS Documents,” cqrs.files.wordpress.com, November 2010.\nChapter 11: Stream Processing\nInside-out with Apache Samza,” Hacker News discussion, news.ycombinator.com,\n[62] “Datomic Development Resources: Excision,” Cognitect, Inc., docs.datomic.com.\ndeleting data is surprisingly hard,” twitter.com, March 30, 2015.\n[66] Srinath Perera: “How Is Stream Processing and Complex Event Processing\n(CEP) Different?,” quora.com, December 3, 2015.\nthe Web 2.0 Data Crunch,” ACM Queue, volume 7, number 11, December 2009.\n[69] “Esper Reference, Version 5.4.0,” EsperTech, Inc., espertech.com, April 2016.\nand Storms,” IBM technical report, developer.ibm.com, April 2014.\nFast Data Management with Streaming SQL,” at IEEE International Workshop on\n[73] Jay Kreps: “Questioning the Lambda Architecture,” oreilly.com, July 2, 2014.\n[74] Ian Hellström: “An Overview of Apache Streaming Technologies,” database‐\n[75] Jay Kreps: “Why Local State Is a Fundamental Primitive in Stream Processing,”\nLuwak and Samza,” martin.kleppmann.com, April 13, 2015.\n[79] Tyler Akidau: “The World Beyond Batch: Streaming 102,” oreilly.com, January\n[80] Stephan Ewen: “Streaming Analytics with Apache Flink,” at Kafka Summit, April\nStream Processing at Internet Scale,” at 39th International Conference on Very Large\n[83] “Windowing (Azure Stream Analytics),” Microsoft Azure Reference,\nFault-Tolerant and Scalable Joining of Continuous Data Streams,” at ACM Interna‐\n[86] Martin Kleppmann: “Samza Newsfeed Demo,” github.com, September 2014.\n[87] Ben Kirwin: “Doing the Impossible: Exactly-Once Messaging Patterns in Kafka,”\n[88] Pat Helland: “Data on the Outside Versus Data on the Inside,” at 2nd Biennial\nwith at-least-once + idempotent operations,” twitter.com, October 20, 2016.\ncient and Fault-Tolerant Model for Stream Processing on Large Clusters,” at 4th\nLatency, and Exactly-Once Stream Processing with Apache Flink,” data-artisans.com,\nChapter 11: Stream Processing\nGuozhang Wang: “KIP-98 – Exactly Once Delivery and Transactional Messaging,”\n“A Survey of Rollback-Recovery Protocols in Message-Passing Systems,” ACM Com‐\nscape?,” softwaremill.com, June 1, 2016.",
      "keywords": [
        "stream processing",
        "stream",
        "data",
        "processing",
        "stream processing systems",
        "June",
        "state",
        "input streams",
        "Jay Kreps",
        "Exactly-Once Stream Processing",
        "data systems",
        "Stream Processing Microbatching",
        "Continuous Data Streams",
        "stream processor",
        "database"
      ],
      "concepts": [
        "data",
        "streams",
        "processed",
        "process",
        "processes",
        "message",
        "june",
        "kafka",
        "events",
        "database"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 46,
          "title": "",
          "score": 0.685,
          "base_score": 0.535,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 52,
          "title": "",
          "score": 0.668,
          "base_score": 0.668,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 62,
          "title": "",
          "score": 0.643,
          "base_score": 0.643,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 49,
          "title": "",
          "score": 0.609,
          "base_score": 0.609,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 48,
          "title": "",
          "score": 0.587,
          "base_score": 0.587,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "com",
          "stream",
          "stream processing",
          "processing",
          "kafka"
        ],
        "semantic": [],
        "merged": [
          "com",
          "stream",
          "stream processing",
          "processing",
          "kafka"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.41325485667914036,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.204257+00:00"
      }
    },
    {
      "chapter_number": 51,
      "title": "Segment 51 (pages 510-517)",
      "start_page": 510,
      "end_page": 517,
      "summary": "The Future of Data Systems\nstances for their use, there is another challenge: in complex applications, data is often\nsuitable for all the different circumstances in which the data is used, so you inevitably\nend up having to cobble together several different pieces of software in order to pro‐\nCombining Specialized Tools by Deriving Data\nsearch index in order to handle queries for arbitrary keywords.\npage 452.\nAs the number of different representations of the data increases, the inte‐\nChapter 12: The Future of Data Systems\nyou need to keep copies of the data in analytics systems (data warehouses, or batch\nthat were derived from the original data; pass the data through machine learning,\nWhen copies of the same data need to be maintained in several storage systems in\norder to satisfy different access patterns, you need to be very clear about the inputs\nand outputs: where is data written first, and which representations are derived from\npage 454) and then applying the changes to the search index in the same order.\nchange data capture (CDC) is the only way of updating the index, you can be confi‐\nAllowing the application to directly write to both the search index and the database\nconflicting writes, and the two storage systems process them in a different order.\nan ordering for all writes, it becomes much easier to derive other representations of\nthe data by processing the writes in the same order.\nmachine replication approach that we saw in “Total Order Broadcast” on page 348.\nWhether you use change data capture or an event sourcing log is less important than\nUpdating a derived data system based on an event log can often be made determinis‐\nDerived data versus distributed transactions\nThe classic approach for keeping different data systems consistent with each other\nHow does the approach of using derived data systems\n“Two-Phase Locking (2PL)” on page 257), while CDC and event sourcing use a log\nfor ordering.\nderived data systems are often updated asynchronously, and so they do not by default\nbelieve that log-based derived data is the most promising approach for integrating\ndifferent data systems.\nChapter 12: The Future of Data Systems\nThe limits of total ordering\nWith systems that are small enough, constructing a totally ordered event log is\n• In most cases, constructing a totally ordered log requires all events to pass\nThe order of events in\nfor example in order to tolerate an entire datacenter going offline, you typically\non page 168).\nThis implies an undefined ordering of events that originate in two\nWhen two events originate in different services, there is no\ndefined order for those events.\ncations, clients and servers are very likely to see events in different orders.\nIn formal terms, deciding on a total order of events is known as total order broadcast,\nthe throughput of a single node is sufficient to process the entire stream of events,\nwork of ordering the events.\nOrdering events to capture causality\nIn cases where there is no causal link between events, the lack of a total order is not a\nbig problem, since concurrent events can be ordered arbitrarily.\ncan be totally ordered by routing all updates for a particular object ID to the same log\nanother place, that ordering dependency between the unfriend event and the message-\nnotifications about new messages may process the message-send event before the\ndle events that are delivered out of order, and they require additional metadata to\n• If you can log an event to record the state of the system that the user saw before\ncan reference that event identifier in order to record the causal dependency [4].\nWe will return to this idea in “Reads are events too” on page 513.\n174) help with processing events that are delivered in an unexpected order.\ncorrectly, without forcing all events to go through the bottleneck of total order\nI would say that the goal of data integration is to make sure that data ends up in the\nChapter 12: The Future of Data Systems\nThe outputs of batch and stream processes are derived datasets such as search\ndata is a search index, a statistical model, or a cache, it is helpful to think in terms of\ndata pipelines that derive one thing from another, pushing state changes in one sys‐\ntem through functional application code and applying the effects to derived systems.\nIn principle, derived data systems could be maintained synchronously, just like a\ntems based on event logs robust: it allows a fault in one part of the system to be",
      "keywords": [
        "Data Systems",
        "Data",
        "derived data systems",
        "Systems",
        "derived data",
        "order",
        "Data Integration",
        "Total Order",
        "events",
        "distributed transactions",
        "stream processing",
        "Wet NMONIN",
        "derived",
        "Total Order Broadcast",
        "processing"
      ],
      "concepts": [
        "wet",
        "level",
        "abstraction",
        "abstract",
        "ices",
        "course",
        "subjective",
        "right",
        "ces",
        "effectively"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 3,
          "title": "",
          "score": 0.735,
          "base_score": 0.585,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 53,
          "title": "",
          "score": 0.704,
          "base_score": 0.554,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 41,
          "title": "",
          "score": 0.7,
          "base_score": 0.55,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 12,
          "title": "",
          "score": 0.693,
          "base_score": 0.543,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "",
          "score": 0.691,
          "base_score": 0.541,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "events",
          "order",
          "derived",
          "total",
          "total order"
        ],
        "semantic": [],
        "merged": [
          "events",
          "order",
          "derived",
          "total",
          "total order"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.34537569737143925,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.204333+00:00"
      }
    },
    {
      "chapter_number": 52,
      "title": "Segment 52 (pages 518-525)",
      "start_page": 518,
      "end_page": 525,
      "summary": "nously [8] (see also “Multi-partition data processing” on page 514).\nReprocessing data for application evolution\nWhen maintaining derived data, batch and stream processing are both useful.\nprocessing allows changes in the input to be reflected in derived views with low delay,\nwhereas batch processing allows large amounts of accumulated historical data to be\nreprocessed in order to derive new views onto an existing dataset.\nIn particular, reprocessing existing data provides a good mechanism for maintaining\nto exist side by side, makes it possible to change the gauge gradually over the course\nChapter 12: The Future of Data Systems\nIf batch processing is used to reprocess historical data, and stream processing is used\nThe lambda architecture proposes running two different systems in parallel:\na batch processing system such as Hadoop MapReduce, and a separate stream-\nIn the lambda approach, the stream processor consumes the events and quickly pro‐\nstreams of immutable events and reprocessing events when needed.\n• Having to maintain the same logic to run both in a batch and in a stream pro‐\nData Integration \nneeds to be set up to process incremental batches (e.g., an hour’s worth of data at\nUnifying batch and stream processing\ndata) and stream computations (processing events as they arrive) to be implemented\nUnifying batch and stream processing in one system requires the following features,\nLike with batch processing, this requires discarding the\nChapter 12: The Future of Data Systems\nUnbundling Databases\nAt a most abstract level, databases, Hadoop, and operating systems all perform the\nsame functions: they store some data, and they allow you to process and query that\ndata [16].\nA database stores data in records of some data model (rows in tables, docu‐\nments, vertices in a graph, etc.) while an operating system’s filesystem stores data in\ntheless, the similarities and differences between operating systems and databases are\nUnix and relational databases have approached the information management prob‐\nlow-level abstractions to the domain of distributed OLTP data storage.\nComposing Data Storage Technologies\nUnbundling Databases \n“Aggregation: Data Cubes and Materialized Views” on page 101)\ndatabases [1]\nsearch indexes (see “The Output of Batch Workflows” on page 411), about material‐\nabout replicating changes from a database to derived data systems (see “Change Data\nthe derived data systems that people are building with batch and stream processors.\nOnce that is done, the database must continue to keep the index up\nUp New Followers” on page 155), and also very similar to bootstrapping change data\nWhenever you run CREATE INDEX, the database essentially reprocesses the existing\ndataset (as discussed in “Reprocessing data for application evolution” on page 496)\nand derives the index as a new view onto the existing data.\nThe existing data may be a\nWhenever a batch, stream, or ETL process transports data\nChapter 12: The Future of Data Systems\nViewed like this, batch and stream processors are like elaborate implementations of\ndata systems they maintain are like different index types.\nture of derived data systems, instead of implementing those facilities as features of a\nsingle integrated database product, they are provided by various different pieces of\nthat there is no single data model or storage format that is suitable for all access pat‐\nApplications that need a specialized data model or\nUnbundled databases: unifying writes\nWhile federation addresses read-only querying across several different systems, it\nsaid that within a single database, creating a consistent index is a built-in feature.\neasier to reliably plug together storage systems (e.g., through change data capture\nand event logs) is like unbundling a database’s index-maintenance features in a\nUnbundling Databases \nquerying requires mapping one data model into another, which takes some thought\n“Derived data versus distributed transactions” on page 492).\nsingle storage or stream processing system are feasible, but when data crosses the\nFor example, distributed transactions are used within some stream processors to ach‐\nwritten by different groups of people (e.g., when data is written from a stream pro‐\n1. At a system level, asynchronous event streams make the system as a whole more\n2. At a human level, unbundling data systems allows different software components\nbe applicable to almost any kind of data.\nDatabases are still\nChapter 12: The Future of Data Systems\nrequired for maintaining state in stream processors, and in order to serve queries for\nThe tools for composing data systems are getting better, but I think one major part is\nhigh-level language for composing storage and processing systems in a simple and\nUnbundling Databases ",
      "keywords": [
        "data",
        "Data Systems",
        "derived data systems",
        "systems",
        "stream",
        "batch",
        "derived data",
        "Databases",
        "stream processing",
        "processing",
        "Multi-partition data processing",
        "stream processors",
        "batch processing",
        "index",
        "stream processing system"
      ],
      "concepts": [
        "data",
        "databases",
        "index",
        "indexes",
        "systems",
        "query",
        "queries",
        "different",
        "differences",
        "processing"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "",
          "score": 0.828,
          "base_score": 0.678,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 62,
          "title": "",
          "score": 0.822,
          "base_score": 0.672,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 3,
          "title": "",
          "score": 0.744,
          "base_score": 0.594,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 43,
          "title": "",
          "score": 0.724,
          "base_score": 0.574,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 41,
          "title": "",
          "score": 0.719,
          "base_score": 0.569,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "stream",
          "batch",
          "processing",
          "systems",
          "unbundling"
        ],
        "semantic": [],
        "merged": [
          "stream",
          "batch",
          "processing",
          "systems",
          "unbundling"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.39411609475840287,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.204399+00:00"
      }
    },
    {
      "chapter_number": 53,
      "title": "Segment 53 (pages 526-536)",
      "start_page": 526,
      "end_page": 536,
      "summary": "ies, including recursive queries on graphs (see “Graph-Like Data Models” on page\ncessing systems with application code is also becoming known as the “database\nchanges, we want any index for that record to be automatically updated, and any\nThus, I think that most data systems still have something to learn from the features\ntoday’s data systems need to be fault-tolerant, scalable, and store data durably.\nApplication code as a derivation function\nThe derivation function for a secondary index is so commonly required that it is built\ntion like creating a secondary index, custom code is required to handle the\nuser-defined functions, which can be used to execute application code within the\nSeparation of application code and state\nbetter than a database that provides execution of user-defined functions as one of its\nMost web applications today are deployed as stateless services, in which any user\nfrom state management (databases): not putting application logic in the database and\nnot putting persistent state in the application [36].\nIn this typical web application model, the database acts as a kind of mutable shared\nread and update the variable, and the database takes care of making it durable, pro‐\nDatabases have inherited this passive approach to mutable data: if you want to find\nDataflow: Interplay between state changes and application code\nship between application code and state management.\nabout the interplay and collaboration between state, state changes, and code that pro‐\nApplication code responds to state changes in one place by triggering\nstate changes in another place.\ncussed treating the log of changes to a database as a stream of events that we can sub‐\ncesses that observe state changes and react to them [38, 39].\ndata change, or when a secondary index is updated to reflect a change in the table\ncreation of derived datasets outside of the primary database: caches, full-text search\n• When maintaining derived data, the order of state changes is often important (if\nseveral views are derived from an event log, they need to process the events in the\ndelivery and derived state updates must be reliable.\nability guarantees at scale, and they allow application code to be run as stream\nneed to periodically poll for updated exchange rates, or subscribe to a stream of changes—which is exactly\nThis application code can do the arbitrary processing that built-in derivation func‐\ntakes streams of state changes as input, and produces other streams of state changes\nComposing stream operators into dataflow systems has a lot of similar characteristics\nably query an exchange-rate service or database in order to obtain the current\nstream of exchange rate updates ahead of time, and record the current rate in a\nlocal database whenever it changes.\nonly needs to query the local database.\nwith a query to a local database (which may be on the same machine, even in the\nevents and exchange rate update events (see “Stream-table join (stream enrichment)”\nNo matter whether you query a service or subscribe to a stream of exchange\nSubscribing to a stream of changes, rather than querying the current state when\npiece of data changes, any derived data that depends on it can swiftly be updated.\ncess for creating derived datasets (such as search indexes, materialized views, and\nIn a search index, writes (document updates) meet reads (queries).\nThis is the read path: when serving a user\nrequest you read from the derived dataset, perhaps perform some more processing\nTaken together, the write path and the read path encompass the whole journey of the\nThe derived dataset is the place where the write path and the read path meet, as illus‐\nto be done at write time and the amount that needs to be done at read time.\nA full-text search index is a good example: the write path updates the index, and the\nread path searches the index for keywords.\nBoth reads and writes need to do some\nWrites need to update the index entries for all terms that appear in the docu‐\nReads need to search for each of the words in the query, and apply Boolean\nNo index means less work on the write path (no index to update), but a lot more\nwork on the read path.\nbetween the write path and the read path.\nsimple: they shift the boundary between the read path and the write path.\nus to do more work on the write path, by precomputing results, in order to save effort\non the read path.\nShifting the boundary between work done on the write path and the read path was in\nand read path might be drawn differently for celebrities compared to ordinary users.\nI find the idea of a boundary between write and read paths interesting because we can\ndatabase and toward state that is maintained on end-user devices, a world of new\nIn particular, we can think of the on-device state as a cache of\nPushing state changes to clients\nThe browser only reads the data at one point in time, assuming\nuser client about any changes to the state it has stored locally, reducing the staleness\nIn terms of our model of write path and read path, actively pushing state changes all\nthe way to client devices means extending the write path all the way to the end user.\nWhen a client is first initialized, it would still need to use a read path to get its initial\nstate, but thereafter it could rely on a stream of state changes sent by the server.\nideas we discussed around stream processing and messaging are not restricted to run‐\nstate changes from the server during that time.\nRecent tools for developing stateful clients and user interfaces, such as the Elm lan‐\ninternal client-side state by subscribing to a stream of events representing user input\npush state-change events into this client-side event pipeline.\nThus, state changes\ntriggers a state change, via event logs and through several derived data systems and\nstream processors, all the way to the user interface of a person observing the state on\nMany datastores support read and write operations where a request returns one\nthat returns a stream of responses over time (see “API support for change streams”\nIn order to extend the write path all the way to the end user, we would need to funda‐\nmind the option of subscribing to changes, not just querying the current state.\nWe discussed that when a stream processor writes derived data to a store (database,\ncache, or index), and when user requests query that store, the store acts as the bound‐\nary between the write path and the read path.\nqueries to the data that would otherwise require scanning the whole event log.\nstream processors also need to maintain state to perform aggregations and joins (see\ning the stream processor itself into a kind of simple database.\nthrough an event log, while reads are transient network requests that go directly to\nIt is also possible to represent read requests as streams of events,\nand send both the read events and the write events through a stream processor; the\nWhen both the writes and the reads are represented as events, and routed to the same\nbetween the stream of read queries and the database.\nThe read event needs to be sent\nto the database partition holding the data (see “Request Routing” on page 214), just\nA one-off read request just passes the request through the join operator\nconnection, you need to record the result of the user’s query of the shipping and\nWriting read events to durable storage thus enables better tracking of causal depen‐\nBut if you already log read requests for operational",
      "keywords": [
        "read path",
        "write path",
        "Data",
        "data systems",
        "state",
        "stream",
        "read",
        "path",
        "database",
        "application code",
        "systems",
        "application",
        "derived data",
        "write",
        "Data Systems Application"
      ],
      "concepts": [
        "databases",
        "data",
        "likely",
        "state",
        "application",
        "applications",
        "message",
        "messaging",
        "streams",
        "index"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 41,
          "title": "",
          "score": 0.737,
          "base_score": 0.587,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 51,
          "title": "",
          "score": 0.704,
          "base_score": 0.554,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 24,
          "title": "",
          "score": 0.655,
          "base_score": 0.505,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 3,
          "title": "",
          "score": 0.649,
          "base_score": 0.499,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "",
          "score": 0.628,
          "base_score": 0.478,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "path",
          "read path",
          "state",
          "write path",
          "changes"
        ],
        "semantic": [],
        "merged": [
          "path",
          "read path",
          "state",
          "write path",
          "changes"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3645637673871625,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.204462+00:00"
      }
    },
    {
      "chapter_number": 54,
      "title": "Segment 54 (pages 537-545)",
      "start_page": 537,
      "end_page": 545,
      "summary": "form this kind of multi-partition join, it is probably simpler to use a database that\nrun a particular application at a particular transaction isolation level or replication\nproducts like databases were free from problems, application code would still need to\nthe last word in making applications correct and resilient to faults.\nJust because an application uses a data system that provides comparatively strong\nsafety properties, such as serializable transactions, that does not mean the application\nFor example, if an application\ntle example of data corruption that can occur.\nFor example, TCP uses sequence numbers on packets to\nduplicates are removed by the TCP stack before it hands the data to an application.\nis currently executing the transaction in Example 12-1.\nThe client can reconnect to the database and retry the transaction, but now it is out‐\nSince the transaction in Example 12-1\nThus, even though Example 12-1 is a standard example for transaction atomicity, it is\nwork fault, and tell it whether to commit or abort an in-doubt transaction.\nEven if we can suppress duplicate transactions between the database client and\nserver, we still need to worry about the network between the end-user device and the\nFor example, if the end-user client is a web browser, it probably\nuses an HTTP POST request to submit an instruction to the server.\nhelp if the POST request times out.) From the web server’s point of view the retry is a\nseparate request, and from the database’s point of view it is a separate transaction.\nyou need to consider the end-to-end flow of the request.\nFor example, you could generate a unique identifier for an operation (such as a\nsubmits the POST request twice, the two requests will have the same operation ID.\nYou can then pass that operation ID all the way through to the database and check\nthat you only ever execute one operation with a given ID, as shown in Example 12-2.\nSuppressing duplicate requests using a unique ID\nALTER TABLE requests ADD UNIQUE (request_id);\n(request_id, from_account, to_account, amount)\nExample 12-2 relies on a uniqueness constraint on the request_id column.\nerally maintain a uniqueness constraint correctly, even at weak isolation levels\nBesides suppressing duplicate requests, the requests table in Example 12-2 acts as a\nprocessed exactly once, which can again be enforced using the request ID.\nThis scenario of suppressing duplicate transactions is just one example of a more\nIn our example, the function in question was duplicate suppression.\nsuppresses duplicate packets at the TCP connection level, and some stream process‐\nis not enough to prevent a user from submitting a duplicate request if the first one\nBy themselves, TCP, database transactions, and stream processors cannot\ntion: a transaction identifier that is passed all the way from the end-user client to the\nreceiving ends of the network connection, or corruption on the disks where the data\nAlthough the low-level features (TCP duplicate suppression, Ethernet checksums,\nexample, HTTP requests would often get mangled if we didn’t have TCP putting the\ntions, that does not mean the application is guaranteed to be free from data loss or\nThe application itself needs to take end-to-end measures, such as dupli‐\nuse distributed transactions because they are too expensive, we end up having to\nit easy to provide application-specific end-to-end correctness properties, but also\nsion can be achieved with a request ID that is passed all the way from the client to the\neral other examples of application features that need to enforce uniqueness: a user‐\nOther kinds of constraints are very similar: for example, ensuring that an account\nrequires consensus: if there are several concurrent requests with the same value, the\nUniqueness checking can be scaled out by partitioning based on the value that needs\nFor example, if you need to ensure uniqueness by request ID, as in\nExample 12-2, you can ensure all requests with the same request ID are routed to the\nIf you need usernames to be unique, you can partition\nmessaging, we can use a very similar approach to enforce uniqueness constraints.\nA stream processor consumes all the messages in a log partition sequentially on a sin‐\nlog is partitioned based on the value that needs to be unique, a stream processor can\n1. Every request for a username is encoded as a message, and appended to a parti‐\n2. A stream processor sequentially reads the requests in the log, using a local data‐\n3. The client that requested the username watches the output stream and waits for a\na conflict may depend on the application, but the stream processor can use arbitrary\nMulti-partition request processing\nthere are potentially three partitions: the one containing the request ID, the one con‐\nIn the traditional approach to databases, executing this transaction would require an\nwith respect to all other transactions on any of those partitions.\n1. The request to transfer money from account A to account B is given a unique\nrequest ID by the client, and appended to a log partition based on the request ID.\n2. A stream processor reads the log of requests.\nFor each request message it emits\nThe original request ID is included in those emitted messages.\nplicate by request ID, and apply the changes to the account balances.\ninstructions, it would require an atomic commit across those two partitions to ensure\nTo avoid the need for a distributed transaction,\nwe first durably log the request as a single message, and then derive the credit and\nall data systems (see “Single-object writes” on page 230), and so the request either\nappears in the log or it doesn’t, without any need for a multi-partition atomic com‐\nIn doing so, it does not skip any request messages, but it may process requests\nstep 3 can easily deduplicate them using the end-to-end request ID.\ncan additionally have a stream processor (partitioned by payer account number) that\nBy breaking down the multi-partition transaction into two differently partitioned\nstages and using the end-to-end request ID, we have achieved the same correctness\nproperty (every request is applied exactly once to both the payer and payee accounts),",
      "keywords": [
        "request",
        "data",
        "transaction",
        "TCP",
        "Data Systems",
        "UPDATE accounts SET",
        "account",
        "application",
        "requests",
        "database",
        "accounts SET balance",
        "TCP connection",
        "Correctness",
        "stream",
        "stream processor"
      ],
      "concepts": [
        "transaction",
        "transactions",
        "request",
        "requests",
        "end",
        "ends",
        "applications",
        "application",
        "data",
        "databases"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 26,
          "title": "",
          "score": 0.93,
          "base_score": 0.78,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 30,
          "title": "",
          "score": 0.821,
          "base_score": 0.671,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 29,
          "title": "",
          "score": 0.752,
          "base_score": 0.602,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 28,
          "title": "",
          "score": 0.715,
          "base_score": 0.565,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 39,
          "title": "",
          "score": 0.714,
          "base_score": 0.564,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "request",
          "request id",
          "id",
          "duplicate",
          "example 12"
        ],
        "semantic": [],
        "merged": [
          "request",
          "request id",
          "id",
          "duplicate",
          "example 12"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.42327210898470785,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.204523+00:00"
      }
    },
    {
      "chapter_number": 55,
      "title": "Segment 55 (pages 546-554)",
      "start_page": 546,
      "end_page": 554,
      "summary": "“Multi-partition data processing” on page 514 (see also “Concurrency control” on\nTimeliness and Integrity\nearizability” on page 324): that is, a writer waits until a transaction is committed, and\nness in log-based messaging” on page 522 when checking whether a uniqueness con‐\nIn this example, the correctness of the uniqueness check does not depend on whether\nIntegrity\nIntegrity means absence of corruption; i.e., no data loss, and no contradictory or\nsome underlying data (see “Deriving current state from the event log” on page\nChapter 12: The Future of Data Systems\nIf integrity is violated, the inconsistency is permanent: waiting and trying again is\napplication-specific notion of integrity.\nI am going to assert that in most applications, integrity is much more important than\nintegrity can be catastrophic.\nviolations of the integrity of the system.\nCorrectness of dataflow systems\nACID transactions usually provide both timeliness (e.g., linearizability) and integrity\nThus, if you approach application correctness from\nthe point of view of ACID transactions, the distinction between timeliness and integ‐\nwe have discussed in this chapter is that they decouple timeliness and integrity.\nprocessing event streams asynchronously, there is no guarantee of timeliness, unless\nintegrity is in fact central to streaming systems.\nmechanism for preserving integrity.\ntwice, the integrity of a data system could be violated.\nmaintaining the integrity of a data system in the face of faults.\nrity without requiring distributed transactions and an atomic commit protocol,\n• Deriving all other state updates from that single message using deterministic der‐\n• Making messages immutable and allowing derived data to be reprocessed from\ntransaction [59, 60].\nChapter 12: The Future of Data Systems\noverbooking, apology and compensation processes would be needed in order to\nthe traditional model of checking all constraints before even writing the data is\nThese applications do require integrity: you would not want to lose a reservation, or\nCoordination-avoiding data systems\n1. Dataflow systems can maintain integrity guarantees on derived data without\n2. Although strict uniqueness constraints require timeliness and coordination,\nily violated and fixed up later, as long as integrity is preserved throughout.\nTaken together, these observations mean that dataflow systems can provide the data\nstill giving strong integrity guarantees.\nSuch coordination-avoiding data systems have\nIn this context, serializable transactions are still useful as part of maintaining derived\nto pay the cost of coordination if only a small part of an application needs it [43].\nAll of our discussion of correctness, integrity, and fault-tolerance has been under the\npage 309): for example, we should assume that processes can crash, machines can\nChapter 12: The Future of Data Systems\n(see “Replication and Durability” on page 227), and data corruption on the network\nMaintaining integrity in the face of software bugs\nduring which such bugs can corrupt data.\nMany applications don’t even correctly use the features that databases\noffer for preserving integrity, such as foreign key or uniqueness constraints [36].\nidea that the database starts off in a consistent state, and a transaction transforms it\nin some way, for example using a weak isolation level unsafely, the integrity of the\nthem to be, it seems that data corruption is inevitable sooner or later.\nshould at least have a way of finding out if data has been corrupted so that we can fix\nChecking the integrity of data is\nAs discussed in “Advantages of immutable events” on page 460, auditing is not just\nMature systems similarly tend to consider the possibility of unlikely things going\nSystems like HDFS and S3 still have to assume that disks work correctly most of the\nness guarantees are absolute and make no provision for the possibility of rare data\nsystems that continually check their own integrity, rather than relying on blind trust\non the basis of blindly trusting technology (such as a transaction mechanism), and\nChapter 12: The Future of Data Systems\nIf a transaction mutates several objects in a database, it is difficult to tell after the fact\nData Capture” on page 454), the insertions, updates, and deletions in various tables\nBy contrast, event-based systems can provide better auditability.\n413) makes the provenance of data much clearer, which makes integrity checking\nFor the event log, we can use hashes to check that the event stor‐\nprocessors that derived it from the event log in order to check whether we get the\nsoftware is bug-free—then we must at least periodically check the integrity of our\ndata.\nChecking the integrity of data systems is best done in an end-to-end fashion (see\n“The End-to-End Argument for Databases” on page 516): the more systems we can\ninclude in an integrity check, the fewer opportunities there are for corruption to go\nIf we can check that an entire derived data\nHaving continuous end-to-end integrity checks gives you increased confidence about\nTools for auditable data systems\nAt present, not many data systems make auditability a top-level concern.\nto a separate audit table, but guaranteeing the integrity of the audit log and the data‐\nIt would be interesting to use cryptographic tools to prove the integrity of a system in\nHowever, from a data systems point of view they\nEssentially, they are distributed databases, with a data\nHowever, the integrity checking aspects are interesting.\nCryptographic auditing and integrity checking often relies on Merkle trees [74],\nChapter 12: The Future of Data Systems",
      "keywords": [
        "Data Systems",
        "data",
        "Integrity",
        "Systems",
        "Multi-partition data processing",
        "Multi-partition data",
        "Data Systems things",
        "Timeliness",
        "Coordination-avoiding data systems",
        "transactions",
        "data systems make",
        "Concurrency control",
        "event",
        "ACID transactions",
        "check"
      ],
      "concepts": [
        "data",
        "transactions",
        "transaction",
        "systems",
        "auditing",
        "application",
        "applications",
        "processing",
        "processes",
        "process"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 54,
          "title": "",
          "score": 0.706,
          "base_score": 0.706,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 26,
          "title": "",
          "score": 0.695,
          "base_score": 0.695,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 30,
          "title": "",
          "score": 0.66,
          "base_score": 0.66,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 58,
          "title": "",
          "score": 0.64,
          "base_score": 0.64,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 38,
          "title": "",
          "score": 0.593,
          "base_score": 0.593,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "integrity",
          "systems",
          "timeliness",
          "data systems",
          "integrity data"
        ],
        "semantic": [],
        "merged": [
          "integrity",
          "systems",
          "timeliness",
          "data systems",
          "integrity data"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4280475264402028,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:02:02.204583+00:00"
      }
    },
    {
      "chapter_number": 56,
      "title": "Segment 56 (pages 555-562)",
      "start_page": 555,
      "end_page": 562,
      "summary": "transparency and distributed ledgers, becoming more widely used in data systems in\nbook we have examined a wide range of different architectures for data systems, eval‐\nWe talk about data as an abstract thing, but remember that many datasets are about\nWe must treat such data with\nUsing data\nHowever, as algorithmic decision-making becomes more widespread, someone who\nThere is hope that basing decisions on data, rather than subjective and instinc‐\nlearned by these systems are opaque: even if there is some correlation in the data, we\nOther features of a person’s data may be analyzed, but what happens if they\nbiased data as input and produce fair and impartial output from it [85].\noften seems to be implied by proponents of data-driven decision making, an attitude\nChapter 12: The Future of Data Systems\nAutomated decision making opens the question of responsibility and accountability\nIf a human makes a mistake, they can be held accountable, and the person affec‐\nCredit rating agencies are an old example of collecting data to make decisions about\npeople.\nanalytics usually work on the basis of “Who is similar to you, and how did people like\ndecision is incorrect due to erroneous data, recourse is almost impossible [87].\nMuch data is statistical in nature, which means that even if the probability distribu‐\nA blind belief in the supremacy of data for making decisions is not only delusional, it\nAs data-driven decision making becomes more widespread,\nWe will also need to figure out how to prevent data being used to harm people, and\nWhen services become good at predicting what content users want to see, they\nWhen predictive analytics affect people’s lives, particularly pernicious problems arise\nmathematical rigor and data.\nWe can try to understand how a data analysis system responds to dif‐\nBesides the problems of predictive analytics—i.e., using data to make automated\ndecisions about people—there are ethical problems with data collection itself.\nthe relationship between the organizations collecting data and the people whose data\nWhen a system only stores data that a user has explicitly entered, because they want\nChapter 12: The Future of Data Systems\nTracking behavioral data has become increasingly important for user-facing features\nusers discover interesting and useful things; A/B tests and user flow analysis can help\nTracking data becomes more\nNow the relationship between the company and the user whose data is being collec‐\nThe user is given a free service and is coaxed into\nAs a thought experiment, try replacing the word data with surveillance, and observe if\nence is just that the data is being collected by corporations rather than government\nNot all data collection necessarily qualifies as surveillance, but examining it as such\ncan help us understand our relationship with the data collector.\nMoreover, data analysis can reveal surprisingly intrusive things: for example, the\nWe might assert that users voluntarily choose to use a service that tracks their activ‐\ndata collection.\nWe might even claim that users are receiving a valuable service in\nreturn for the data they provide, and that the tracking is necessary in order to provide\nUsers have little knowledge of what data they are feeding into our databases, or how\nWithout understanding what happens to their data, users cannot give any\nOften, data from one user also says things about other people\nwho are not users of the service and who have not agreed to any terms.\nuser base may have been combined with behavioral tracking and external data sour‐\nces—are precisely the kinds of data of which users cannot have any meaningful\nMoreover, data is extracted from users through a one-way process, not a relationship\nusers to negotiate how much data they provide and what service they receive in\nChapter 12: The Future of Data Systems\nreturn: the relationship between the service and the user is very asymmetric and one-\nThe terms are set by the service, not by the user [99].\nFor a user who does not consent to surveillance, the only real alternative is simply not\nEspecially when a service has network effects, there is a social cost to people\nDeclining to use a service due to its tracking of users is only an option for the small\nPrivacy and use of data\nSometimes people claim that “privacy is dead” on the grounds that some users are\nWhen data is extracted from people through surveillance infrastructure, privacy\nrights are not necessarily eroded, but rather transferred to the data collector.\nnies that acquire data essentially say “trust us to do the right thing with your data,”\nIntimate information about users is only revealed indirectly, for example in the form\nEven if particular users cannot be personally reidentified from the bucket of people\nIt is not the user\nof how intrusive their data collection actually is, and instead focusing on managing\nWith any kind of data we should expect the\nPrivacy settings that allow a user of an online service to control which aspects of their\ndata other users can see are a starting point for handing back some control to users.\ndata, and is free to use it in any way permitted by the privacy policy.\nrights to process and analyze the data internally, often going much further than what\ntheir attorney—but in these cases the use of data has been strictly governed by ethical,\nmassive scale without users understanding what is happening to their private data.\nData as assets and power\nSince behavioral data is a byproduct of users interacting with a service, it is some‐\nview, if targeted advertising is what pays for a service, then behavioral data about\npeople is the service’s core asset.\nChapter 12: The Future of Data Systems",
      "keywords": [
        "data",
        "people",
        "data systems",
        "Users",
        "service",
        "systems",
        "surveillance",
        "Predictive Analytics",
        "privacy",
        "Tracking",
        "predictive analytics systems",
        "data collection",
        "Data Systems past",
        "Thing",
        "decision"
      ],
      "concepts": [
        "users",
        "people",
        "surveillance",
        "make",
        "making",
        "social",
        "thing",
        "privacy",
        "algorithmic",
        "algorithm"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 57,
          "title": "",
          "score": 0.697,
          "base_score": 0.547,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 41,
          "title": "",
          "score": 0.616,
          "base_score": 0.466,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 3,
          "title": "",
          "score": 0.566,
          "base_score": 0.416,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 6,
          "title": "",
          "score": 0.565,
          "base_score": 0.415,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 51,
          "title": "",
          "score": 0.564,
          "base_score": 0.414,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "users",
          "people",
          "service",
          "user",
          "tracking"
        ],
        "semantic": [],
        "merged": [
          "users",
          "people",
          "service",
          "user",
          "tracking"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.29363332267398945,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.204640+00:00"
      }
    },
    {
      "chapter_number": 57,
      "title": "Segment 57 (pages 563-574)",
      "start_page": 563,
      "end_page": 574,
      "summary": "Because the data is valuable, many people want it.\ncompany goes bankrupt, the personal data it has collected is one of the assets that get\nble of preventing abuse of data, whenever we collect data, we need to balance the ben‐\nWhen collecting data, we need to consider not just today’s political environment, but\nI believe that the collection and use of data is one of those problems.\nData is the pollution problem of the information age, and protecting privacy is the\nData protection laws might be able to help preserve individuals’ rights.\nway incompatible with those purposes,” and furthermore that data must be “ade‐\nChapter 12: The Future of Data Systems\nCompanies that collect lots of data about people oppose regulation as being a burden\npersonal data.\nself-regulate our data collection and processing practices in order to establish and\ntake it upon ourselves to educate end users about how their data is used, rather than\nretain data forever, but purge it as soon as it is no longer needed [111, 112].\nIn this chapter we discussed new approaches to designing data systems, and I\nproblem by using batch processing and event streams to let data changes flow\nIn this approach, certain systems are designated as systems of record, and other data\nDerived state can be updated by observing changes in the underlying data.\nAs software and data are having such a large impact on the world, we engineers must\nChapter 12: The Future of Data Systems\n[1] Rachid Belaid: “Postgres Full-Text Search is Good Enough!,” rachbelaid.com, July\nference on Innovative Data Systems Research (CIDR), January 2009.\n[4] Jessica Kerr: “Provenance and Causality in Distributed Systems,” blog.jessi‐\n[5] Kostas Tzoumas: “Batch Is a Special Case of Streaming,” data-artisans.com, Sep‐\nTime Data’s Unifying Abstraction,” engineering.linkedin.com, December 16, 2013.\n3rd Biennial Conference on Innovative Data Systems Research (CIDR), January 2007.\n[10] Jacqueline Xu: “Online Migrations at Scale,” stripe.com, February 2, 2017.\nble Real-Time Data Systems.\nFramework for Integrating Batch and Online MapReduce Computations,” at 40th\nInternational Conference on Very Large Data Bases (VLDB), September 2014.\n[14] Jay Kreps: “Questioning the Lambda Architecture,” oreilly.com, July 2, 2014.\nline and Offline Big Data Integration,” at 7th Biennial Conference on Innovative Data\nRitchie and Ken Thompson: “The UNIX Time-Sharing System,”\n[18] Michael Stonebraker: “The Case for Polystores,” wp.sigmod.org, July 13, 2015.\nPolystore System,” ACM SIGMOD Record, volume 44, number 2, pages 11–16, June\n[20] Patrycja Dybka: “Foreign Data Wrappers for PostgreSQL,” vertabelo.com, March\ndling Transaction Services in the Cloud,” at 4th Biennial Conference on Innovative\nData Systems Research (CIDR), January 2009.\nDistributed Data,” IEEE Data Engineering Bulletin, volume 38, number 4, pages 4–14,\nential Dataflow,” at 6th Biennial Conference on Innovative Data Systems Research\nflow System,” at 24th ACM Symposium on Operating Systems Principles (SOSP),\nChapter 12: The Future of Data Systems\ngramming for GUIs,” at 34th ACM SIGPLAN Conference on Programming Language\n“Consistency Analysis in Bloom: A CALM and Collected Approach,” at 5th Biennial\nConference on Innovative Data Systems Research (CIDR), January 2011.\n[36] Peter Bailis, Alan Fekete, Michael J Franklin, et al.: “Feral Concurrency Control:\nAn Empirical Investigation of Modern Application Integrity,” at ACM International\nData \n[38] David Gelernter: “Generative Communication in Linda,” ACM Transactions on\nmarrec: “The Many Faces of Publish/Subscribe,” ACM Computing Surveys, volume\nAuthority,” blog.christianposta.com, May 27, 2016.\nand Relay,” medium.com, December 3, 2015.\n[46] Frank McSherry: “Dataflow as Database,” github.com, July 17, 2016.\n[47] Peter Alvaro: “I See What You Mean,” at Strange Loop, September 2015.\nand Friends,” at Merchant Risk Council MRC Vegas Conference, March 2016.\nCorrectness at Different Isolation Levels,” at 16th International Conference on Data\n[54] Michael Jouravlev: “Redirect After Post,” theserverside.com, August 1, 2004.\nin System Design,” ACM Transactions on Computer Systems, volume 2, number 4,\nDatabase Systems,” Proceedings of the VLDB Endowment, volume 8, number 3, pages\n[57] Alex Yarmula: “Strong Consistency in Manhattan,” blog.twitter.com, March 17,\nChapter 12: The Future of Data Systems\n[59] Jim Gray: “The Transaction Concept: Virtues and Limitations,” at 7th Interna‐\nference on Management of Data (SIGMOD), May 1987.\n[61] Pat Helland: “Memories, Guesses, and Apologies,” blogs.msdn.com, May 15,\nondary Index Because of Fix of Bug#68021,” bugs.mysql.com, July 2014.\n[66] Gary Fredericks: “Postgres Serializability Bug,” github.com, September 2015.\n[68] Jay Kreps: “Getting Real About Distributed System Reliability,” blog.empathy‐\n[69] Martin Fowler: “The LMAX Architecture,” martinfowler.com, July 12, 2011.\nScalable Blockchain Database,” bigchaindb.com, June 8, 2016.\nFunction,” at CRYPTO ’87, August 1987.\n[75] Ben Laurie: “Certificate Transparency,” ACM Queue, volume 12, number 8,\nMail,” at Network and Distributed System Security Symposium (NDSS), February\n[77] “Software Engineering Code of Ethics and Professional Practice,” Association for\nchoices,” twitter.com, October 30, 2016.\n[81] Logan Kugler: “What Happens When Big Data Blunders?,” Communications of\n[82] Bill Davidow: “Welcome to Algorithmic Prison,” theatlantic.com, February 20,\n[83] Don Peck: “They’re Watching You at Work,” theatlantic.com, December 2013.\n[85] Jesse Emspak: “How a Machine Learns Prejudice,” scientificamerican.com,\n[86] Maciej Cegłowski: “The Moral Economy of Tech,” idlewords.com, June 2016.\n[88] Julia Angwin: “Make Algorithms Accountable,” nytimes.com, August 1, 2016.\nmic Decision-Making and a ‘Right to Explanation’,” arXiv:1606.08813, August 31,\n[90] “A Review of the Data Broker Industry: Collection, Use, and Sale of Consumer\nData for Marketing Purposes,” Staff Report, United States Senate Committee on Com‐\nTrump Elected?” theguardian.com, November 10, 2016.\nChapter 12: The Future of Data Systems\nBernstein: “Listening to a ‘big data’/‘data science’ talk,” twitter.com,\nWorse,” arstechnica.com, January 23, 2016.\n[97] The Grugq: “Nothing to Hide,” grugq.tumblr.com, April 15, 2016.\nInformation Civilization,” Journal of Information Technology, volume 30, number 1,\nDunn: “The UK’s 15 Most Infamous Data Breaches,” techworld.com,\nwant,” twitter.com, March 6, 2016.\n[104] Bruce Schneier: “Mission Creep: When Everything Is Terrorism,” schneier.com,\n[105] Lena Ulbricht and Maximilian von Grafenstein: “Big Data: Big Power Shifts?,”\nSecretive Empires We’ve Ever Known,” theguardian.com, September 28, 2016.\nsibility and Risk Among Actors Involved in Personal Data Processing,” Thesis, KU\nsumer Protection Law,” Internet Policy Review, volume 5, number 1, March 2016.\nEven Imagine,” fastcoexist.com, March 15, 2016.\n[111] Maciej Cegłowski: “Haunted by Data,” idlewords.com, October 2015.\nProtect Privacy,” theguardian.com, January 13, 2016.\n[114] Phillip Rogaway: “The Moral Character of Cryptographic Work,” Cryptology\nChapter 12: The Future of Data Systems",
      "keywords": [
        "Data Systems Research",
        "data",
        "Data Systems",
        "Innovative Data Systems",
        "systems",
        "Big Data",
        "Systems Research",
        "Innovative Data",
        "personal data",
        "Data Systems Companies",
        "Large Data Bases",
        "Data Systems References",
        "Data protection",
        "doi",
        "December"
      ],
      "concepts": [
        "data",
        "doi",
        "systems",
        "december",
        "blog",
        "march",
        "july",
        "august",
        "november",
        "processes"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 3,
          "title": "",
          "score": 0.716,
          "base_score": 0.566,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "",
          "score": 0.698,
          "base_score": 0.548,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 56,
          "title": "",
          "score": 0.697,
          "base_score": 0.547,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 41,
          "title": "",
          "score": 0.693,
          "base_score": 0.543,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 52,
          "title": "",
          "score": 0.681,
          "base_score": 0.531,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "com",
          "2016",
          "data systems",
          "systems",
          "innovative data"
        ],
        "semantic": [],
        "merged": [
          "com",
          "2016",
          "data systems",
          "systems",
          "innovative data"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3280968396307177,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.204700+00:00"
      }
    },
    {
      "chapter_number": 58,
      "title": "Segment 58 (pages 575-582)",
      "start_page": 575,
      "end_page": 582,
      "summary": "(e.g., sending data over the network to\npage 284, and “System Model and Reality”\non page 306.\nanother concurrent process can never\nSee “Atomicity” on page 223\nCommit (2PC)” on page 354.\n“Messaging Systems” on page 441.\nusually large) set of data as input and pro‐\nduces some other data as output, without\nded Delays” on page 281) and datasets\n(see the introduction to Chapter 11).\nother nodes.\npage 304.\nused data in order to speed up future\nreads of the same data.\ncomplete: thus, if some data is missing\nsome underlying, slower data storage\ndata.\nCAP theorem” on page 336.\nrelationship and concurrency” on page\n186 and “Ordering and Causality” on page\nple, which node should be the leader for a\n“Fault-Tolerant Consensus” on page 364.\ndata warehouse\nA database in which data from several dif‐\nSee “Data Warehousing” on page\non page 42.\ndancy or duplication in a normalized\nindex, in order to speed up reads.\nObject Operations” on page 228 and\nevent log” on page 461.\nderived data\ndata through a repeatable process, which\nderived data is needed to speed up a par‐\nticular kind of read access to the data.\nare examples of derived data.\nRunning on several nodes connected by a\nknow what exactly is broken.\nand Partial Failures” on page 274.\nStoring data in a way such that you\nSee “Durability” on page 226.\nThe process of\nextracting data from a source database,\ninto a data warehouse or batch processing\nSee “Data Warehousing” on page\n“Handling Node Outages” on page 156.\nability” on page 6.\nwrites from clients, but only processes\ndata changes that it receives from a leader.\nFollowers” on page 152.\npage 88.\nA data structure consisting of vertices\n(things that you can refer to, also known\n“Graph-Like Data Models” on page 49.\nHash of Key” on page 203.\nSee “Idempotence” on page\nA data structure that lets you efficiently\npage 70.\npage 225.\nMany-to-Many Relationships” on page 33\npage 403.\nWhen data or a service is replicated across\nseveral nodes, the leader is the designated\npage 152.\nof data in the system, which is updated by\non page 324.\neral pieces of data in the same place if they\nSee “Data locality for queries” on page 41.\nthread, node, or transaction can access\n(2PL)” on page 257 and “The leader and\nthe lock” on page 301.\nAn append-only file for storing data.\ning B-trees reliable” on page 82), a log-\nand LSM-Trees” on page 76), a replication\nlog is used to copy writes from a leader to\npage 152), and an event log can represent\na data stream (see “Partitioned Logs” on\npage 446).\n“Aggregation: Data Cubes and Material‐\ntion of Intermediate State” on page 419.\nnode\nother nodes via a network in order to\nized database, when some piece of data\nMany Relationships” on page 33.\nAnalytics?” on page 90.\nOnline transaction processing.\nread or write a small number of records,\nProcessing or Analytics?” on page 90.\npage 13.\nThe minimum number of nodes that need\nreading and writing” on page 179.\nTo move data or services from one node\npage 209.\nKeeping a copy of the same data on sev‐\ndata, including its fields and datatypes.\nWhether some data conforms to a schema\nthe document model” on page 39), and a\nAn additional data structure that is main‐\ntained alongside the primary data storage\ntures” on page 85 and “Partitioning and\nSecondary Indexes” on page 206.\non page 251.\nor data, and others have much less.\nloads and Relieving Hot Spots” on page\n205 and “Handling skew” on page 407.\non page 237, write skew in “Write Skew\nand Phantoms” on page 246, and clock\non page 291.\nages” on page 156 and “The Truth Is\nDefined by the Majority” on page 300.\nthe transaction.\ntion” on page 252.\ntative version of some data, also known as\nDelays” on page 281.\norder is not a total order” on page 341.\ntransaction\ntransaction.\nTwo-Phase Commit (2PC)” on page 354.\nAn algorithm for achieving serializable\nisolation that works by a transaction\nacquiring a lock on all data it reads or\nof the transaction.\ning (2PL)” on page 257.\ntransaction\ndata cubes and materialized views, 101\nchecking data integrity, 530\nmessage ordering, 446\ncomparison to transaction processing, 91\nfor distributed transactions, 361\narchival storage, data from databases, 131\ndata loss on failover, 157\natomicity (transactions), 223, 228, 553\nmaintaining derived data, 453",
      "keywords": [
        "data",
        "Apache",
        "transaction",
        "Data Warehousing",
        "node",
        "System",
        "order",
        "leader",
        "processing",
        "derived data",
        "isolation",
        "database",
        "Glossary",
        "Commit",
        "atomic"
      ],
      "concepts": [
        "apache",
        "data",
        "database",
        "transactions",
        "transaction",
        "index",
        "indexes",
        "node",
        "query",
        "queries"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 40,
          "title": "",
          "score": 0.668,
          "base_score": 0.668,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 62,
          "title": "",
          "score": 0.663,
          "base_score": 0.663,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 23,
          "title": "",
          "score": 0.647,
          "base_score": 0.647,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 55,
          "title": "",
          "score": 0.64,
          "base_score": 0.64,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 26,
          "title": "",
          "score": 0.638,
          "base_score": 0.638,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "page",
          "transaction",
          "leader",
          "page 152",
          "node"
        ],
        "semantic": [],
        "merged": [
          "page",
          "transaction",
          "leader",
          "page 152",
          "node"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.49029170728901944,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:02:02.204763+00:00"
      }
    },
    {
      "chapter_number": 59,
      "title": "Segment 59 (pages 583-590)",
      "start_page": 583,
      "end_page": 590,
      "summary": "for multi-object transactions, 229\ntools for auditable data systems, 532\nAvro (data format), 122-127\ndatabase snapshot for replication, 156\nuse for ETL processes, 405\ncombining with stream processing\ncomparison to stream processing, 464\nfor data integration, 494-498\nBigtable data model, 41, 99\nbinary data encodings, 115-128\nBottled Water (change data capture), 455\nin networks, 285\nIndex \nBTM (transaction coordinator), 356\nbusiness data processing, 28, 90, 390\nbyte sequence, encoding data in, 112\nas derived data, 386, 499-504\ndatabase as cache of transaction log, 460\ncolumn-family data model, 41, 99\nin transactions, 262\nin serializable transactions, 262-265\nordering events to capture, 493\nCEP (see complex event processing)\nchange data capture, 160, 454\nchange data capture, 454\nin stream joins, 474\nchronicle data model, 458\nclickstream data, analysis of, 404\nclocks, 287-299\nIndex\n(see also network model)\ncommits (transactions), 222\n(see also atomicity; transactions)\nfor stream operator state, 479\nrelation to transactions, 230\nusing databases, 129-131\ncompensating transactions, 355, 461, 526\ncomplex event processing (CEP), 465\ncomposing data systems (see unbundling data‐\nin replicated systems, 161-191, 324-338\nordering of operations, 326, 341\ntransaction isolation, 225\nwrite skew (transaction isolation), 246-251\nIndex \nin log-based systems, 351, 521\nby aborting transactions, 261\nrelation to operation ordering, 339\nwrite skew (transaction isolation), 246-251\ndistributed transactions, 352-375\nXA transactions, 361-364\nrelation to replication, 155, 349\nin ACID transactions, 224, 529\nread-after-write, 162-164\nconstraints (databases), 225, 248\nin log-based systems, 521-524\nrelation to event ordering, 347\ncross-partition ordering, 256, 294, 348, 523\nin XA transactions, 361-364\nIndex\nin log-based systems, 521-524\nof algorithm within system model, 308\nof compensating transactions, 355\nof derived data, 497, 531\nof immutable data, 461\nof personal data, 535, 540\nof transactions, 225, 515, 529\ncorruption of data\ndue to weak transaction isolation, 233\ndocument data model, 31\nproving integrity of data, 532\ndata cubes, 102\ndata formats (see encoding)\ndata integration, 490-498, 543\nbatch and stream processing, 494-498\nreprocessing data, 496\ncombining tools by deriving data, 490-494\nderived data versus distributed transac‐\nordering events to capture causality, 493\ndata lakes, 415\ndata locality (see locality)\ndata models, 27-64\nrelational model versus document model,\ndata protection regulations, 542\ndata systems, 3\nIndex \ndata integration, 490-498\ndata warehousing, 91-95, 554\ncomparison to data lakes, 415\nkeeping data systems in sync, 452\ndata-intensive applications, 3\ndatabase-internal distributed transactions, 360,\ndatabases\nrelation to event streams, 451-464\nchange data capture, 454-457\ncomposing data storage technologies,\nnetwork architecture, 276\nnetwork faults, 279\ncorrectness of dataflow systems, 525\nthrough databases, 129\ncomparison to stream processing, 464\ndata model, 50, 57\nexcision (deleting data), 463\nlanguages for transactions, 255\nDebezium (change data capture), 455\nIndex\nbounded network delays, 285\ndeleting data, 463\ndenormalization (data representation), 34, 554\nin derived data systems, 386\nupdating derived data, 228, 231, 490\nderived data, 386, 439, 554\nfrom change data capture, 454\noutputs of batch and stream processing, 495\nversus distributed transactions, 492\ncomputing derived data, 495, 526, 531\ndirty reads (transaction isolation), 234\ndirty writes (transaction isolation), 235\nindiscriminately dumping data into, 415\ndetecting network faults, 280\nlimitations of distributed transactions, 363\nsystem models, 306-310\nuse of clocks and time, 287\ndistributed transactions (see transactions)\ndocument data model, 30-42\ncomparison to relational model, 38-42\nmulti-object transactions, need for, 231\nversus relational model\ndata locality, 41\ndocument-partitioned indexes, 206, 217, 411\ndurability (transactions), 226, 554\nIndex \ndocument-partitioned indexes, 207\nencodings (data formats), 111-128\nusing databases, 129-131\nrepresentations of data, 112\nfor network faults, 280\nin transactions, 231\ndata as assets and power, 540\nmeaning of privacy, 539\ncomparison to change data capture, 457\nderiving current state from event log, 458\nlarge, reliable data systems, 519, 526\nEvent Store (database), 458\nevent streams (see streams)\nevents, 440\nderiving views from event log, 461\nevent time versus processing time, 469, 477,\ntimestamp of, in stream processing, 471\ngraph-structured data, 52\nof databases, 40, 129-131, 461, 497\nreprocessing data, 496, 498\nIndex",
      "keywords": [
        "data",
        "transactions",
        "change data capture",
        "systems",
        "processing",
        "data systems",
        "event",
        "derived data",
        "model",
        "transaction isolation",
        "data capture",
        "change data",
        "data model",
        "stream",
        "databases"
      ],
      "concepts": [
        "data",
        "databases",
        "transactions",
        "transaction",
        "indexes",
        "index",
        "processes",
        "processing",
        "process",
        "model"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 62,
          "title": "",
          "score": 0.929,
          "base_score": 0.779,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 60,
          "title": "",
          "score": 0.899,
          "base_score": 0.749,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "",
          "score": 0.882,
          "base_score": 0.732,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 61,
          "title": "",
          "score": 0.869,
          "base_score": 0.719,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 3,
          "title": "",
          "score": 0.725,
          "base_score": 0.575,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "transactions",
          "capture",
          "model",
          "data capture",
          "change data"
        ],
        "semantic": [],
        "merged": [
          "transactions",
          "capture",
          "model",
          "data capture",
          "change data"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.42724430055598683,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.204827+00:00"
      }
    },
    {
      "chapter_number": 60,
      "title": "Segment 60 (pages 591-598)",
      "start_page": 591,
      "end_page": 598,
      "summary": "transactions)\nWormhole (change data capture), 455\n(see also leader-based replication)\namplification by distributed transactions,\nfaults versus, 7\npartial failures in distributed systems,\nfan-out (messaging systems), 11, 445\nuse of replication, 367\nin batch processing, 406, 414, 422, 425\nin log-based systems, 520, 524-526\nin stream processing, 476-479\nof distributed transactions, 362-364\nfaults, 6\nhandled by transactions, 221\nin batch processing versus distributed data‐\nin distributed systems, 274-277\nnetwork faults, 279-281\nstream processors writing to databases, 478,\nfinancial data, 460\nFlink (processing framework), 421-423\nGelly API (graph processing), 425\nintegration of batch and stream processing,\nstream processing, 466\n(see also leader-based replication)\nIndex \nserializable transactions, 261, 265, 364\nprocess pauses for, 14, 296-299, 301\n(see also process pauses)\nGiraph (graph processing), 425\nGoldenGate (change data capture), 161, 170,\nMapReduce (batch processing), 390\nPregel (graph processing), 425\ngovernment use of data, 541\nGraphChi (graph processing), 426\nas data models, 49-63\nexample of graph-structured data, 49\nversus the network model, 60\nprocessing and analysis, 424-426\nPregel processing model, 425\ncomparison to distributed databases, 390\ncomparison to MPP databases, 414-418\ndiverse processing models in ecosystem, 417\njoin algorithms, 403-410\nIndex\nfaults in, 7, 227\nhash indexes, 72-75\nbroadcast hash joins, 409\npartitioned hash joins, 409\nhash partitioning, 203-205, 217\ncolumn-family data model, 41, 99\nchecking data integrity, 530\nheterogeneous distributed transactions, 360,\nfor data warehouses, 93\nhopping windows (stream processing), 472\ndistributed transaction support, 361\nfor time-series data, 203\nin batch processing, 407\ndistributed transaction support, 361\ndistributed transaction support, 361\nIndex \nfor data warehouses, 93\nhash joins, 409\nin-memory databases, 88\ndata corruption on hard disks, 227\ndata loss due to last-write-wins, 173, 292\ndata on disks unreadable, 309\nerrors in transaction serializability, 529\nnetwork faults, 279\nnetwork partitions and whole-datacenter\npoor handling of network faults, 280\nindexes, 71, 555\nas derived data, 386, 499-504\nbuilding in batch processes, 411\nindex-range locking, 260\npartitioning and secondary indexes,\nupdating when data changes, 452, 467\nclustered index on primary key, 86\nintegrating different data systems (see data\ncoordination-avoiding data systems, 528\nIndex\nisolation (in transactions), 225, 228, 555\ndistributed transaction support, 361\nJava Message Service (JMS), 444\n(see also messaging systems)\ncomparison to log-based messaging, 448,\ndistributed transaction support, 361\nmessage ordering, 446\nprocess reuse in batch processors, 422\nin MapReduce querying, 46\njoins, 555\nbroadcast hash joins, 409\npartitioned hash joins, 409\nstream joins, 472-476\nstream-table join, 473\nin relational databases, 30, 42\nleader-based replication, 153\nmessage offsets, 447, 478\ntransaction support, 477\nas batch process output, 412\nhash indexes, 72-75\nIndex \nleader-based replication, 152-161\nimplementation of replication logs\nchange data capture, 454-457\ndetecting concurrent writes, 184-191\nsloppy quorums and hinted handoff, 183\nof derived data systems, 492, 524\nDatabus (change data capture), 161, 455\nlocal indexes (see document-partitioned\nindexes)\nlocality (data access), 32, 41, 555\nIndex\nin batch processing, 400, 405, 421\nin stream processing, 474, 478, 508, 522\ndistributed locking, 301-304, 330\nfor transaction isolation\nin-doubt transactions holding locks, 362\nfor read-after-write consistency, 164\nlogs (data structure), 71, 556\nfor stream operator state, 479\nlog-based messaging, 446-451\nusing logs for message storage, 447\nlog-structured merge tree (see LSM-\nchange data capture, 454-457\nLSM-trees (indexes), 78-79\nbuilding indexes in batch processes, 411\nmodels derived from training data, 505\nMapReduce (batch processing), 390, 399-400\ncomparison to distributed databases\ndiversity of processing models, 416\nIndex \ncomparison to stream processing, 464\nmap-side processing, 408-410\nbroadcast hash joins, 409\npartitioned hash joins, 409\nreduce-side processing, 403-408\nintermediate state (batch processing),\nas derived data, 386, 499-504\nmaintaining, using stream processing,\nMaxwell (change data capture), 455\nin-memory databases, 88\nin-memory representation of data, 112\nuse by indexes, 72, 77\nmerge joins, MapReduce map-side, 410\nmessages\nmessaging systems, 440-451\nuniqueness in log-based messaging, 522\nIndex",
      "keywords": [
        "data",
        "processing",
        "distributed",
        "replication",
        "messaging",
        "joins",
        "stream processing",
        "indexes",
        "transaction",
        "batch processing",
        "fault tolerance",
        "batch",
        "systems",
        "database",
        "comparison"
      ],
      "concepts": [
        "data",
        "databases",
        "messaging",
        "message",
        "index",
        "indexes",
        "processing",
        "process",
        "processes",
        "query"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 61,
          "title": "",
          "score": 0.953,
          "base_score": 0.803,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 62,
          "title": "",
          "score": 0.93,
          "base_score": 0.78,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 59,
          "title": "",
          "score": 0.899,
          "base_score": 0.749,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "",
          "score": 0.851,
          "base_score": 0.701,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 45,
          "title": "",
          "score": 0.755,
          "base_score": 0.605,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "processing",
          "batch",
          "joins",
          "joins 409",
          "409"
        ],
        "semantic": [],
        "merged": [
          "processing",
          "batch",
          "joins",
          "joins 409",
          "409"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4382723684447839,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.204893+00:00"
      }
    },
    {
      "chapter_number": 61,
      "title": "Segment 61 (pages 599-606)",
      "start_page": 599,
      "end_page": 606,
      "summary": "(see also services)\nAzure Service Bus (messaging), 444\ndocument data model, 31\nkey-range partitioning, 202\nleader-based replication, 153\npartition splitting, 212\nmulti-leader replication, 168-177\n(see also replication)\nmulti-datacenter replication, 168, 335\nmulti-object transactions, 228\nmulti-version concurrency control (MVCC),\ndetecting stale MVCC reads, 263\nindexes and snapshot isolation, 241\nbinlog parsing for change data capture, 455\ndistributed transaction support, 361\nleader-based replication, 153\nrow-based replication, 160\nTungsten Replicator (multi-leader replica‐\n(see also stream processing)\nCypher query language, 52\ngraph data model, 50\nnetwork model, 36\nIndex \ngraph databases versus, 60\nnetwork partitions, 279, 337\n(see also read skew)\ntransactions and, 223\nunsafe read-modify-write cycle code, 244\nmessages in partitioned logs, 447\nOLTP (online transaction processing), 90, 556\nanalytics queries versus, 411\n(see also services)\noperating systems versus databases, 499\nin stream processing, 464\ndistributed transaction support, 361\nleader-based replication, 153\npartitioned indexes, 209\nread committed isolation, 236\nIndex\nqueries in MPP databases, 216\npartitioning, 199-218, 556\nand replication, 200\nmulti-partition operations, 514\nof key-value data, 201-205\nrebalancing partitions, 209-214\nusing dynamic partitioning, 212\nusing fixed number of partitions, 210\nreplication and, 147\ndocument-based partitioning, 206\nterm-based partitioning, 208\nserial execution of transactions and, 255\nof distributed transactions, 360\nof in-memory databases, 89\nof multi-leader replication, 169\nreplicated joins, 409\nBDR (multi-leader replication), 170\ncausal ordering of writes, 177\ndistributed transaction support, 361\nleader-based replication, 153\nread committed isolation, 236\nIndex \nprimary-secondary replication (see leader-\nbased replication)\nprocessing time (of events), 469\nCypher query language, 52\nProtocol Buffers (data format), 117-121\nquery languages, 42-48\nmulti-datacenter replication, 184\nleader-based replication, 153\nIndex\nread committed isolation level, 234-237\nread path (derived data), 509\nread repair (leaderless replication), 178\nread replicas (see leader-based replication)\nread skew (transaction isolation), 238, 266\nread-after-write consistency, 163, 524\nread-modify-write cycle, 243\nnear-real-time processing, 390\n(see also stream processing)\nrebalancing partitions, 209-214, 556\n(see also partitioning)\ndynamic partitioning, 212\nfixed number of partitions, 210\nevents in stream processing, 440\nof derived data, 386\n(see also derived data)\nrelational data model, 28-42\nin-memory databases with, 89\nmulti-object transactions, need for, 231\nrelational databases\nleader-based replication, 153\nof messaging systems, 442\nIndex \n(see also services)\ndata encoding and evolution, 136\nrepeatable reads (transaction isolation), 242\nreplication, 151-193, 556\nconsistent prefix reads, 165\nreading your own writes, 162\npartitioning and, 147, 200\nimplementation of replication logs,\nwith heterogeneous data systems, 453\nfrom log-based messaging, 451\nmean and percentiles, 14\n(see also services)\ndocument data model, 31\ndynamic partitioning, 212\nkey-range partitioning, 202\nleader-based replication, 153\nmulti-datacenter support, 184\nrow-based replication, 160\nIndex\nrules (Datalog), 61\nin transactions, 222\nstreaming SQL support, 466\npartitioning and, 199\nreplication and, 161\nscatter/gather approach, querying partitioned\ndatabases, 207\nschema-on-read, 39\nschema-on-write, 39\nschemaless databases (see schema-on-read)\nin databases, 129-131\nbuilding search indexes in batch processes,\npartitioned secondary indexes, 206\nsecondaries (see leader-based replication)\npartitioning, 206-209, 217\ndocument-partitioned, 206\nupdating, transaction isolation and, 231\npartitioning, 255\ndetecting stale MVCC reads, 263\ndetecting writes that affect prior reads,\nIndex \n(see also services)\nservices, 131-136\nsession windows (stream processing), 472\n(see also replication)\npartitioning, 199\nsingle-leader replication (see leader-based rep‐\nin stream processing, 448, 463, 522\nin transaction isolation\nread skew, 238, 266\nfor time-series data, 203\nslaves (see leader-based replication)\nsnapshots (databases)\nin change data capture, 455\nsnapshot isolation and repeatable read,\n(see also services)\nbuilding indexes in batch processes, 411\ndocument-partitioned indexes, 207\nIndex",
      "keywords": [
        "replication",
        "data",
        "leader-based replication",
        "support",
        "processing",
        "isolation",
        "partitioning",
        "snapshot isolation",
        "distributed transaction support",
        "indexes",
        "stream processing",
        "Distributed",
        "index",
        "SQL",
        "snapshot isolation support"
      ],
      "concepts": [
        "data",
        "replication",
        "replicated",
        "partitioning",
        "partition",
        "database",
        "indexes",
        "index",
        "reads",
        "processing"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 60,
          "title": "",
          "score": 0.953,
          "base_score": 0.803,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 59,
          "title": "",
          "score": 0.869,
          "base_score": 0.719,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 62,
          "title": "",
          "score": 0.841,
          "base_score": 0.691,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "",
          "score": 0.812,
          "base_score": 0.662,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 40,
          "title": "",
          "score": 0.63,
          "base_score": 0.63,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "replication",
          "based replication",
          "leader based",
          "leader",
          "partitioning"
        ],
        "semantic": [],
        "merged": [
          "replication",
          "based replication",
          "leader based",
          "leader",
          "partitioning"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4060702569631618,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.204957+00:00"
      }
    },
    {
      "chapter_number": 62,
      "title": "Segment 62 (pages 607-613)",
      "start_page": 607,
      "end_page": 613,
      "summary": "sort order in column storage, 99\ndata locality, 41\nsnapshot isolation using clocks, 295\nSpark (processing framework), 421-423\nfor data warehouses, 93\nGraphX API (graph processing), 425\nSpark Streaming, 466\nstream processing on top of batch process‐\nin consensus algorithms, 352, 367\nSQL (Structured Query Language), 21, 28, 43\ndistributed query execution, 48\nSQL Server (database)\ndata warehousing support, 93\ndistributed transaction support, 361\nread committed isolation, 236\nserializable isolation, 257\nsnapshot isolation support, 239\nSQLstream (stream analytics), 466\nStar Wars analogy (event time versus process‐\nstate\nderived from log of immutable events, 459\nderiving current state from the event log,\nstream joins, 473\nrebuilding after stream processor failure,\nstate machine replication, 349, 452\ncomposing data storage technologies,\nwriting to, 101\ncomparing requirements for transaction\nprocessing and analytics, 90-96\nin-memory storage, 88\nsimilarity to stream processors, 505\nstream processing, 464-481, 557\ncomparison to batch processing, 464\ncomplex event processing (CEP), 465\nfor data integration, 494-498\nevent time versus processing time, 469,\nrelation to databases (see streams)\nrelation to services, 508\nsearch on streams, 467\nstream analytics, 466\nstream joins, 472-476\nstream-stream join, 473\nstream-table join, 473\nstreams, 440-451\nprocessing (see stream processing)\nrelation to databases, 451-464\nAPI support for change streams, 456\nchange data capture, 454-457\nderivative of state by time, 460\nsubscribers (message streams), 440\nchange data capture, 454, 491\nTableau (data visualization software), 416\nuse for transaction sessions, 229\nThrift (data format), 117-121\nEnterprise Message Service, 444\nStreamBase (stream analytics), 466\nin distributed systems, 287-299\nprocess pauses, 295-299\nreasoning about, in stream processors,\nevent time versus processing time, 469,\ntimestamp of events, 471\nsystem models for distributed systems, 307\ntime-dependence in stream joins, 475\ncoordination-avoiding data systems, 528\nassigning to events in stream processing,\nfor read-after-write consistency, 163\nfor transaction ordering, 295\nordering events, 291, 345\nconsensus algorithms and, 366-368\ntransaction manager (see coordinator)\ntransaction processing, 28, 90-95\ncomparison to data warehousing, 93\ntransactions, 221-267, 558\nisolation, 225\ndistributed transactions, 352-364\nneed for multi-object transactions, 231\ntrie (data structure), 88\nimplementing change data capture, 455\ntumbling windows (stream processing), 472\nTurtle (RDF data format), 56\ntransactions holding locks, 362\n(see also streams)\nprocess pauses, 296\ncomposing data storage technologies,\nmulti-partition data processing, 514\n(see also consensus)\nUnix pipes versus dataflow engines, 423\ncomparison to relational databases, 499, 501\ncomparison to stream processing, 464\natomic write operations, 243\nvectorized processing, 99, 428\ntools for auditable data systems, 532\ndata, 463\nversus vector clocks, 191\nversus memory management by databases,\nbuilding read-only stores in batch processes,\nin-memory storage, 89\noutput streams, 456\nserial execution of transactions, 253\ntransactions in stream processing, 477\nwindows (stream processing), 466, 468-472\nstream joins within a window, 473\nwrite path (derived data), 509\nwrite skew (transaction isolation), 246-251\nwrites (database)\natomic write operations, 243\n(see also services)\nencoding RDF data, 57\nfor application data, issues with, 114\nin relational databases, 30, 41\nZab (consensus algorithm), 366\nZooKeeper (coordination service), 370-373\npanies including LinkedIn and Rapportive, where he worked on large-scale data\nThe animal on the cover of Designing Data-Intensive Applications is an Indian wild\nWild boars are also\nBoars",
      "keywords": [
        "stream",
        "stream processing",
        "data",
        "processing",
        "systems",
        "database",
        "state",
        "replication",
        "Sorted String Tables",
        "SQL",
        "stream joins",
        "storage",
        "isolation",
        "versus",
        "event"
      ],
      "concepts": [
        "databases",
        "processing",
        "process",
        "processes",
        "transaction",
        "transactions",
        "streaming",
        "indexes",
        "index",
        "write"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 60,
          "title": "",
          "score": 0.93,
          "base_score": 0.78,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 59,
          "title": "",
          "score": 0.929,
          "base_score": 0.779,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "",
          "score": 0.903,
          "base_score": 0.753,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 61,
          "title": "",
          "score": 0.841,
          "base_score": 0.691,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 52,
          "title": "",
          "score": 0.822,
          "base_score": 0.672,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "stream",
          "processing",
          "stream processing",
          "466",
          "stream joins"
        ],
        "semantic": [],
        "merged": [
          "stream",
          "processing",
          "stream processing",
          "466",
          "stream joins"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.46729575321621125,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:02:02.205021+00:00"
      }
    }
  ],
  "total_chapters": 62,
  "enrichment_provenance": {
    "taxonomy_id": "none",
    "taxonomy_version": "none",
    "taxonomy_path": "none",
    "taxonomy_checksum": "sha256:none",
    "source_metadata_file": "Designing Data-Intensive Applications_metadata.json",
    "enrichment_date": "2025-12-17T23:02:02.213761+00:00",
    "enrichment_method": "msep",
    "model_version": "ai-agents-msep-v1",
    "processing_time_ms": 4913.4642940007325,
    "total_similar_chapters": 310
  }
}