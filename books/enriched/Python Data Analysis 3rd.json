{
  "metadata": {
    "title": "Python Data Analysis 3rd",
    "author": "Wes McKinney",
    "publisher": "O'Reilly Media",
    "edition": "3rd Edition",
    "isbn": "978-1-098-10403-0",
    "total_pages": 582,
    "conversion_date": "2025-11-05T18:42:58.102560",
    "conversion_method": "PyMuPDF + OCR fallback"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Preliminaries",
      "start_page": 19,
      "end_page": 34,
      "detection_method": "regex_chapter",
      "chapter_number": 1,
      "summary": "This chapter covers preliminaries. Key topics include python, data, and likely. This book is concerned with the nuts and bolts of manipulating, processing, cleaning,\nand crunching data in Python.",
      "keywords": [
        "Python",
        "data",
        "data analysis",
        "Python data",
        "Python data analysis",
        "Python programming language",
        "Book",
        "Essential Python Libraries",
        "Python programming",
        "Python Libraries",
        "analysis",
        "Python code",
        "Python data structures",
        "Miniconda",
        "code"
      ],
      "concepts": [
        "python",
        "data",
        "likely",
        "installation",
        "languages",
        "computing",
        "computations",
        "analysis",
        "packages",
        "numpy"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 44,
          "title": "Segment 44 (pages 362-366)",
          "relevance_score": 0.78,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Cookbook 3rd",
          "chapter": 1,
          "title": "Data Structures and Algorithms",
          "relevance_score": 0.74,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 2,
          "title": "Segment 2 (pages 20-40)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 2,
      "title": "Python Language Basics, IPython,",
      "start_page": 35,
      "end_page": 64,
      "detection_method": "regex_chapter",
      "chapter_number": 2,
      "summary": "So, I will only present roughly enough information in this chapter and\nChapter 3 to enable you to follow along with the rest of the book Key topics include python, types, and typing.",
      "keywords": [
        "Python Language Basics",
        "Python Language",
        "Python",
        "Language Basics",
        "Jupyter Notebooks",
        "Jupyter",
        "Python Interpreter",
        "Language",
        "string",
        "Basics",
        "Python objects",
        "IPython",
        "Type",
        "Python string type",
        "object"
      ],
      "concepts": [
        "python",
        "types",
        "typing",
        "typed",
        "string",
        "strings",
        "object",
        "functionality",
        "functions",
        "function"
      ],
      "similar_chapters": [
        {
          "book": "Fluent Python 2nd",
          "chapter": 1,
          "title": "Segment 1 (pages 1-19)",
          "relevance_score": 0.73,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.73,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 4,
          "title": "Introducing Python Object Types",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 40,
          "title": "Segment 40 (pages 420-428)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 3,
      "title": "Built-In Data Structures,",
      "start_page": 65,
      "end_page": 100,
      "detection_method": "regex_chapter",
      "chapter_number": 3,
      "summary": "CHAPTER 3\nBuilt-In Data Structures,\nFunctions, and Files\nThis chapter discusses capabilities built into the Python language that will be used\nubiquitously throughout the book Key topics include functions, functionality, and function. Covers function.",
      "keywords": [
        "Built-In Data Structures",
        "Data Structures",
        "list",
        "Python",
        "function",
        "Data",
        "file",
        "Functions",
        "Python data structure",
        "Built-In Data",
        "Structures",
        "Elements",
        "Python file",
        "sequence",
        "Python file objects"
      ],
      "concepts": [
        "functions",
        "functionality",
        "function",
        "lists",
        "python",
        "files",
        "data",
        "values",
        "sets",
        "setting"
      ],
      "similar_chapters": [
        {
          "book": "Python Cookbook 3rd",
          "chapter": 1,
          "title": "Data Structures and Algorithms",
          "relevance_score": 0.74,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 11,
          "title": "Segment 11 (pages 99-114)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 25,
          "title": "Miscellaneous Library Modules",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 26,
          "title": "Extending and Embedding Python",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 9,
          "title": "Tuples, Files, and Everything Else",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 4,
      "title": "NumPy Basics: Arrays and",
      "start_page": 101,
      "end_page": 133,
      "detection_method": "regex_chapter",
      "chapter_number": 4,
      "summary": "Also, pandas provides some more\ndomain-specific functionality like time series manipulation, which is not present in\nNumPy Key topics include arrays, numpy, and data.",
      "keywords": [
        "array",
        "Multidimensional Array Object",
        "data",
        "Boolean array",
        "NumPy",
        "Python",
        "arr",
        "multidimensional array",
        "Array Object",
        "Vectorized Computation",
        "NumPy Basics",
        "NumPy arrays",
        "data type",
        "Boolean",
        "type"
      ],
      "concepts": [
        "arrays",
        "numpy",
        "data",
        "computation",
        "computations",
        "compute",
        "python",
        "values",
        "functions",
        "function"
      ],
      "similar_chapters": [
        {
          "book": "Python Cookbook 3rd",
          "chapter": 1,
          "title": "Data Structures and Algorithms",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 5,
          "title": "Segment 5 (pages 84-106)",
          "relevance_score": 0.48,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 4,
          "title": "Introducing Python Object Types",
          "relevance_score": 0.45,
          "method": "sentence_transformers"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 44,
          "title": "Segment 44 (pages 362-366)",
          "relevance_score": 0.44,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 22,
          "title": "Segment 22 (pages 175-182)",
          "relevance_score": 0.42,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 5,
      "title": "for much",
      "start_page": 134,
      "end_page": 140,
      "detection_method": "regex_chapter",
      "chapter_number": 5,
      "summary": "This chapter covers for much. Key topics include array, walks, and random. In this section I discuss only NumPy’s built-in binary format, since most users will\nprefer pandas and other tools for loading text or tabular data (see Chapter 6 for much\nmore).",
      "keywords": [
        "array",
        "Random Walks",
        "Random",
        "Walks",
        "Compute",
        "matrix",
        "Boolean array",
        "square matrix",
        "steps",
        "nsteps",
        "Linear Algebra",
        "arr",
        "NumPy",
        "Vectorized Computation",
        "crossing"
      ],
      "concepts": [
        "array",
        "walks",
        "random",
        "compute",
        "computing",
        "numpy",
        "value",
        "steps",
        "sized",
        "size"
      ],
      "similar_chapters": [
        {
          "book": "Python Cookbook 3rd",
          "chapter": 1,
          "title": "Data Structures and Algorithms",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 5,
          "title": "Segment 5 (pages 84-106)",
          "relevance_score": 0.49,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.44,
          "method": "sentence_transformers"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 44,
          "title": "Segment 44 (pages 362-366)",
          "relevance_score": 0.44,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 30,
          "title": "Segment 30 (pages 315-323)",
          "relevance_score": 0.42,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 6,
      "title": "Getting Started with pandas",
      "start_page": 141,
      "end_page": 146,
      "detection_method": "regex_chapter",
      "chapter_number": 6,
      "summary": "This chapter covers getting started with pandas. Key topics include array, series, and index. It\ncontains data structures and data manipulation tools designed to make data cleaning\nand analysis fast and convenient in Python.",
      "keywords": [
        "Series",
        "data",
        "pandas",
        "pandas Data Structures",
        "dtype",
        "index",
        "Ohio",
        "data structures",
        "Texas False dtype",
        "Oregon",
        "Texas",
        "California",
        "NumPy",
        "pandas Data",
        "Ohio False"
      ],
      "concepts": [
        "array",
        "series",
        "index",
        "pandas",
        "missing",
        "typed",
        "types",
        "communities",
        "tool",
        "values"
      ],
      "similar_chapters": [
        {
          "book": "Python Cookbook 3rd",
          "chapter": 1,
          "title": "Data Structures and Algorithms",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 44,
          "title": "Segment 44 (pages 362-366)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.48,
          "method": "sentence_transformers"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 1,
          "title": "Segment 1 (pages 1-19)",
          "relevance_score": 0.46,
          "method": "sentence_transformers"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 5,
          "title": "Segment 5 (pages 84-106)",
          "relevance_score": 0.46,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 7,
      "title": "and an",
      "start_page": 147,
      "end_page": 220,
      "detection_method": "regex_chapter",
      "chapter_number": 7,
      "summary": "Bob      4\nSteve    7\nJeff    -5\nRyan     3\ndtype: int64\nDataFrame\nA DataFrame represents a rectangular table of data and contains an ordered, named\ncollection of columns, each of which can be a different value type (numeric, string,\nBoolean, etc.) Key topics include data, index, and indexes.",
      "keywords": [
        "data",
        "index",
        "Ohio",
        "Series",
        "DataFrame",
        "columns",
        "Ohio Nevada year",
        "binary data formats",
        "Data Loading",
        "Ohio Nevada",
        "File Formats",
        "Nevada"
      ],
      "concepts": [
        "data",
        "index",
        "indexes",
        "values",
        "method",
        "pandas",
        "columns",
        "format",
        "formatted",
        "files"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 13,
          "title": "Segment 13 (pages 119-126)",
          "relevance_score": 0.46,
          "method": "sentence_transformers"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 10,
          "title": "Segment 10 (pages 82-90)",
          "relevance_score": 0.43,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 4,
          "title": "Segment 4 (pages 27-34)",
          "relevance_score": 0.42,
          "method": "sentence_transformers"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 16,
          "title": "Segment 16 (pages 312-333)",
          "relevance_score": 0.42,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 4,
          "title": "Segment 4 (pages 27-35)",
          "relevance_score": 0.42,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 8,
      "title": "Data Cleaning and Preparation",
      "start_page": 221,
      "end_page": 298,
      "detection_method": "regex_chapter",
      "chapter_number": 8,
      "summary": "Fortunately, pandas, along with the\nbuilt-in Python language features, provides you with a high-level, flexible, and fast set\nof tools to enable you to manipulate data into the right form Key topics include data, value, and indexing.",
      "keywords": [
        "Data",
        "missing data",
        "Data Cleaning",
        "Data Wrangling",
        "index",
        "Categorical Data",
        "string",
        "dtype",
        "Ohio",
        "data types",
        "Extension Data Types",
        "columns",
        "Cleaning and Preparation",
        "Series",
        "Data Transformation"
      ],
      "concepts": [
        "data",
        "value",
        "indexing",
        "indexes",
        "string",
        "strings",
        "pandas",
        "columns",
        "method",
        "uses"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 44,
          "title": "Segment 44 (pages 362-366)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 34,
          "title": "Segment 34 (pages 283-291)",
          "relevance_score": 0.46,
          "method": "sentence_transformers"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 1,
          "title": "Segment 1 (pages 1-19)",
          "relevance_score": 0.44,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 7,
          "title": "String Fundamentals",
          "relevance_score": 0.44,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Cookbook 3rd",
          "chapter": 1,
          "title": "Data Structures and Algorithms",
          "relevance_score": 0.44,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 9,
      "title": "Plotting and Visualization",
      "start_page": 299,
      "end_page": 318,
      "detection_method": "regex_chapter",
      "chapter_number": 9,
      "summary": "One of these is seaborn, which we\nexplore later in this chapter Key topics include plotting, plots, and figures. It may be a part of the exploratory process—for example,\nto help identify outliers or needed data transformations, or as a way of generating\nideas for models.",
      "keywords": [
        "matplotlib API Primer",
        "plot",
        "matplotlib",
        "matplotlib API",
        "API Primer",
        "Plotting",
        "Visualization",
        "subplot",
        "data",
        "labels",
        "line plot",
        "data visualization",
        "Jupyter",
        "line",
        "Plotting and Visualization"
      ],
      "concepts": [
        "plotting",
        "plots",
        "figures",
        "data",
        "labels",
        "options",
        "optional",
        "color",
        "visualization",
        "visualizations"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 36,
          "title": "Segment 36 (pages 300-307)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 40,
          "title": "Segment 40 (pages 363-372)",
          "relevance_score": 0.49,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 21,
          "title": "Segment 21 (pages 167-174)",
          "relevance_score": 0.44,
          "method": "sentence_transformers"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 23,
          "title": "Segment 23 (pages 441-462)",
          "relevance_score": 0.43,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 24,
          "title": "Segment 24 (pages 231-239)",
          "relevance_score": 0.4,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 10,
      "title": "Bar Plots",
      "start_page": 319,
      "end_page": 336,
      "detection_method": "regex_chapter",
      "chapter_number": 10,
      "summary": "This chapter covers bar plots. Key topics include plot, plotting, and data. Horizonal and vertical bar plot\nWith a DataFrame, bar plots group the values in each row in bars, side by side, for\neach value.",
      "keywords": [
        "Sun Dinner",
        "plot",
        "data",
        "plotting",
        "Sun",
        "Visualization",
        "day",
        "tip",
        "seaborn",
        "Dinner",
        "tips",
        "data visualization",
        "pct",
        "Bar",
        "Plotting and Visualization"
      ],
      "concepts": [
        "plot",
        "plotting",
        "data",
        "tipping",
        "tips",
        "bar",
        "bars",
        "day",
        "visualization",
        "visualize"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 36,
          "title": "Segment 36 (pages 300-307)",
          "relevance_score": 0.45,
          "method": "sentence_transformers"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 11,
          "title": "Segment 11 (pages 88-98)",
          "relevance_score": 0.43,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 36,
          "title": "Segment 36 (pages 347-355)",
          "relevance_score": 0.42,
          "method": "sentence_transformers"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 23,
          "title": "Segment 23 (pages 441-462)",
          "relevance_score": 0.41,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 42,
          "title": "Segment 42 (pages 401-412)",
          "relevance_score": 0.4,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 11,
      "title": "Data Aggregation and Group Operations",
      "start_page": 337,
      "end_page": 422,
      "detection_method": "regex_chapter",
      "chapter_number": 11,
      "summary": "pandas provides a\nversatile groupby interface, enabling you to slice, dice, and summarize datasets in a\nnatural way Key topics include data, time, and group.",
      "keywords": [
        "time series",
        "time series data",
        "time",
        "Sun Dinner",
        "Group",
        "series",
        "Group Operations",
        "Data",
        "Data Aggregation",
        "time day Dinner",
        "time zone",
        "Time Series Basics",
        "series data",
        "smoker day time",
        "dtype"
      ],
      "concepts": [
        "data",
        "time",
        "group",
        "value",
        "index",
        "indexes",
        "frequency",
        "frequencies",
        "function",
        "functions"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 44,
          "title": "Segment 44 (pages 362-366)",
          "relevance_score": 0.41,
          "method": "sentence_transformers"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 34,
          "title": "Segment 34 (pages 283-291)",
          "relevance_score": 0.4,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 13,
          "title": "[ 443 ]",
          "relevance_score": 0.38,
          "method": "sentence_transformers"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 13,
          "title": "Segment 13 (pages 119-126)",
          "relevance_score": 0.37,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Cookbook 3rd",
          "chapter": 1,
          "title": "Data Structures and Algorithms",
          "relevance_score": 0.36,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 12,
      "title": "Introduction to Modeling",
      "start_page": 423,
      "end_page": 442,
      "detection_method": "regex_chapter",
      "chapter_number": 12,
      "summary": "In this chapter, I will review some features of pandas that may be helpful when\nyou’re crossing back and forth between data wrangling with pandas and model fitting\nand scoring Key topics include modeling, data, and column.",
      "keywords": [
        "model",
        "data",
        "Modeling Libraries",
        "Python",
        "Intercept",
        "column",
        "Patsy",
        "Introduction to Modeling",
        "linear models",
        "Introduction",
        "Libraries in Python",
        "Creating Model Descriptions",
        "Python modeling libraries",
        "Libraries",
        "machine learning"
      ],
      "concepts": [
        "modeling",
        "data",
        "column",
        "statistical",
        "statistics",
        "patsy",
        "training",
        "python",
        "statsmodels",
        "methods"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 44,
          "title": "Segment 44 (pages 362-366)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 33,
          "title": "Segment 33 (pages 281-289)",
          "relevance_score": 0.51,
          "method": "sentence_transformers"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 28,
          "title": "Segment 28 (pages 237-245)",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 20,
          "title": "Segment 20 (pages 168-178)",
          "relevance_score": 0.49,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 13,
      "title": "Data Analysis Examples",
      "start_page": 443,
      "end_page": 582,
      "detection_method": "regex_chapter",
      "chapter_number": 13,
      "summary": "This chapter contains a collection of\nmiscellaneous example datasets that you can use for practice with the tools in this\nbook Key topics include data, numpy, and object.",
      "keywords": [
        "Data",
        "Data Analysis",
        "array",
        "Python",
        "data type",
        "extension data type",
        "missing data",
        "numpy",
        "IPython",
        "Advanced NumPy",
        "time",
        "IPython System",
        "code",
        "Python objects",
        "Python code"
      ],
      "concepts": [
        "data",
        "numpy",
        "object",
        "python",
        "pythonic",
        "index",
        "indexes",
        "function",
        "functions",
        "functionality"
      ],
      "similar_chapters": [
        {
          "book": "Python Cookbook 3rd",
          "chapter": 1,
          "title": "Data Structures and Algorithms",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 44,
          "title": "Segment 44 (pages 362-366)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 10,
          "title": "Segment 10 (pages 192-209)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 8,
          "title": "Lists and Dictionaries",
          "relevance_score": 0.49,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 4,
          "title": "Segment 4 (pages 27-35)",
          "relevance_score": 0.49,
          "method": "sentence_transformers"
        }
      ]
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "chapter": null,
      "content": "Python  \n for Data Analysis\nData Wrangling with pandas, NumPy & Jupyter\nWes McKinney\nThird  \nEdition\npowered by",
      "content_length": 111,
      "extraction_method": "Direct"
    },
    {
      "page_number": 2,
      "chapter": null,
      "content": "DATA\n“With this new edition, \nWes has updated his \nbook to ensure it remains \nthe go-to resource for \nall things related to data \nanalysis with Python \nand pandas. I cannot \nrecommend this book \nhighly enough.”\n—Paul Barry\nLecturer and author of O’Reilly’s  \nHead First Python\nPython for Data Analysis\n9\n781098 104030\n5 6 9 9 9\nUS $69.99\t\n CAN $87.99\nISBN: 978-1-098-10403-0\nTwitter: @oreillymedia\nlinkedin.com/company/oreilly-media\nyoutube.com/oreillymedia \nGet the definitive handbook for manipulating, processing, \ncleaning, and crunching datasets in Python. Updated for \nPython 3.10 and pandas 1.4, the third edition of this hands-\non guide is packed with practical case studies that show you \nhow to solve a broad set of data analysis problems effectively. \nYou’ll learn the latest versions of pandas, NumPy, and Jupyter \nin the process.\nWritten by Wes McKinney, the creator of the Python pandas \nproject, this book is a practical, modern introduction to \ndata science tools in Python. It’s ideal for analysts new to \nPython and for Python programmers new to data science \nand scientific computing. Data files and related material are \navailable on GitHub.\n•\t Use the Jupyter notebook and the IPython shell for \nexploratory computing\n•\t Learn basic and advanced features in NumPy\n•\t Get started with data analysis tools in the pandas library\n•\t Use flexible tools to load, clean, transform, merge, and \nreshape data\n•\t Create informative visualizations with matplotlib\n•\t Apply the pandas groupBy facility to slice, dice, and \nsummarize datasets\n•\t Analyze and manipulate regular and irregular time series \ndata\n•\t Learn how to solve real-world data analysis problems with \nthorough, detailed examples\nWes McKinney, cofounder and chief \ntechnology officer of Voltron Data, is \nan active member of the Python data \ncommunity and an advocate for Python \nuse in data analysis, finance, and \nstatistical computing applications. A \ngraduate of MIT, he’s also a member of \nthe project management committees \nfor the Apache Software Foundation’s \nApache Arrow and Apache Parquet \nprojects.",
      "content_length": 2087,
      "extraction_method": "Direct"
    },
    {
      "page_number": 3,
      "chapter": null,
      "content": "Wes McKinney\nPython for Data Analysis\nData Wrangling with pandas,\nNumPy, and Jupyter\nTHIRD EDITION\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing",
      "content_length": 178,
      "extraction_method": "Direct"
    },
    {
      "page_number": 4,
      "chapter": null,
      "content": "978-1-098-10403-0\n[LSI]\nPython for Data Analysis\nby Wes McKinney\nCopyright © 2022 Wesley McKinney. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles (http://oreilly.com). For more information, contact our corporate/institutional\nsales department: 800-998-9938 or corporate@oreilly.com.\nAcquisitions Editor: Jessica Haberman\nDevelopment Editor: Angela Rufino\nProduction Editor: Christopher Faucher\nCopyeditor: Sonia Saruba\nProofreader: Piper Editorial Consulting, LLC\nIndexer: Sue Klefstad\nInterior Designer: David Futato\nCover Designer: Karen Montgomery\nIllustrator: Kate Dullea\nOctober 2012:\n First Edition\nOctober 2017:\n Second Edition\nAugust 2022:\n Third Edition\nRevision History for the Third Edition\n2022-08-12: First Release\nSee https://www.oreilly.com/catalog/errata.csp?isbn=0636920519829 for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Python for Data Analysis, the cover\nimage, and related trade dress are trademarks of O’Reilly Media, Inc.\nWhile the publisher and the author have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the author disclaim all responsibility\nfor errors or omissions, including without limitation responsibility for damages resulting from the use\nof or reliance on this work. Use of the information and instructions contained in this work is at your\nown risk. If any code samples or other technology this work contains or describes is subject to open\nsource licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\nthereof complies with such licenses and/or rights.",
      "content_length": 1906,
      "extraction_method": "Direct"
    },
    {
      "page_number": 5,
      "chapter": null,
      "content": "Table of Contents\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi\n1. Preliminaries. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\n1.1 What Is This Book About?                                                                                         1\nWhat Kinds of Data?                                                                                                      1\n1.2 Why Python for Data Analysis?                                                                                 2\nPython as Glue                                                                                                                3\nSolving the “Two-Language” Problem                                                                        3\nWhy Not Python?                                                                                                           3\n1.3 Essential Python Libraries                                                                                          4\nNumPy                                                                                                                             4\npandas                                                                                                                              5\nmatplotlib                                                                                                                        6\nIPython and Jupyter                                                                                                       6\nSciPy                                                                                                                                 7\nscikit-learn                                                                                                                       8\nstatsmodels                                                                                                                      8\nOther Packages                                                                                                               9\n1.4 Installation and Setup                                                                                                 9\nMiniconda on Windows                                                                                                9\nGNU/Linux                                                                                                                   10\nMiniconda on macOS                                                                                                  11\nInstalling Necessary Packages                                                                                    11\nIntegrated Development Environments and Text Editors                                     12\n1.5 Community and Conferences                                                                                  13\n1.6 Navigating This Book                                                                                                14\nCode Examples                                                                                                             15\niii",
      "content_length": 3267,
      "extraction_method": "Direct"
    },
    {
      "page_number": 6,
      "chapter": null,
      "content": "Data for Examples                                                                                                        15\nImport Conventions                                                                                                     16\n2. Python Language Basics, IPython, and Jupyter Notebooks. . . . . . . . . . . . . . . . . . . . . . . .  17\n2.1 The Python Interpreter                                                                                             18\n2.2 IPython Basics                                                                                                            19\nRunning the IPython Shell                                                                                          19\nRunning the Jupyter Notebook                                                                                  20\nTab Completion                                                                                                            23\nIntrospection                                                                                                                 25\n2.3 Python Language Basics                                                                                           26\nLanguage Semantics                                                                                                     26\nScalar Types                                                                                                                   34\nControl Flow                                                                                                                 42\n2.4 Conclusion                                                                                                                  45\n3. Built-In Data Structures, Functions, and Files. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  47\n3.1 Data Structures and Sequences                                                                                47\nTuple                                                                                                                               47\nList                                                                                                                                  51\nDictionary                                                                                                                     55\nSet                                                                                                                                   59\nBuilt-In Sequence Functions                                                                                      62\nList, Set, and Dictionary Comprehensions                                                               63\n3.2 Functions                                                                                                                    65\nNamespaces, Scope, and Local Functions                                                                 67\nReturning Multiple Values                                                                                          68\nFunctions Are Objects                                                                                                 69\nAnonymous (Lambda) Functions                                                                              70\nGenerators                                                                                                                     71\nErrors and Exception Handling                                                                                 74\n3.3 Files and the Operating System                                                                               76\nBytes and Unicode with Files                                                                                      80\n3.4 Conclusion                                                                                                                  82\n4. NumPy Basics: Arrays and Vectorized Computation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  83\n4.1 The NumPy ndarray: A Multidimensional Array Object                                    85\nCreating ndarrays                                                                                                         86\nData Types for ndarrays                                                                                              88\nArithmetic with NumPy Arrays                                                                                 91\nBasic Indexing and Slicing                                                                                          92\niv \n| \nTable of Contents",
      "content_length": 4622,
      "extraction_method": "Direct"
    },
    {
      "page_number": 7,
      "chapter": null,
      "content": "Boolean Indexing                                                                                                         97\nFancy Indexing                                                                                                           100\nTransposing Arrays and Swapping Axes                                                                102\n4.2 Pseudorandom Number Generation                                                                    103\n4.3 Universal Functions: Fast Element-Wise Array Functions                               105\n4.4 Array-Oriented Programming with Arrays                                                         108\nExpressing Conditional Logic as Array Operations                                              110\nMathematical and Statistical Methods                                                                    111\nMethods for Boolean Arrays                                                                                    113\nSorting                                                                                                                          114\nUnique and Other Set Logic                                                                                     115\n4.5 File Input and Output with Arrays                                                                       116\n4.6 Linear Algebra                                                                                                          116\n4.7 Example: Random Walks                                                                                        118\nSimulating Many Random Walks at Once                                                              120\n4.8 Conclusion                                                                                                                121\n5. Getting Started with pandas. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  123\n5.1 Introduction to pandas Data Structures                                                               124\nSeries                                                                                                                            124\nDataFrame                                                                                                                   129\nIndex Objects                                                                                                              136\n5.2 Essential Functionality                                                                                            138\nReindexing                                                                                                                  138\nDropping Entries from an Axis                                                                                141\nIndexing, Selection, and Filtering                                                                            142\nArithmetic and Data Alignment                                                                              152\nFunction Application and Mapping                                                                        158\nSorting and Ranking                                                                                                  160\nAxis Indexes with Duplicate Labels                                                                         164\n5.3 Summarizing and Computing Descriptive Statistics                                         165\nCorrelation and Covariance                                                                                     168\nUnique Values, Value Counts, and Membership                                                   170\n5.4 Conclusion                                                                                                                173\n6. Data Loading, Storage, and File Formats. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  175\n6.1 Reading and Writing Data in Text Format                                                           175\nReading Text Files in Pieces                                                                                      182\nWriting Data to Text Format                                                                                    184\nWorking with Other Delimited Formats                                                                185\nJSON Data                                                                                                                   187\nTable of Contents \n| \nv",
      "content_length": 4527,
      "extraction_method": "Direct"
    },
    {
      "page_number": 8,
      "chapter": null,
      "content": "XML and HTML: Web Scraping                                                                              189\n6.2 Binary Data Formats                                                                                               193\nReading Microsoft Excel Files                                                                                  194\nUsing HDF5 Format                                                                                                  195\n6.3 Interacting with Web APIs                                                                                     197\n6.4 Interacting with Databases                                                                                     199\n6.5 Conclusion                                                                                                                201\n7. Data Cleaning and Preparation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  203\n7.1 Handling Missing Data                                                                                           203\nFiltering Out Missing Data                                                                                       205\nFilling In Missing Data                                                                                              207\n7.2 Data Transformation                                                                                               209\nRemoving Duplicates                                                                                                 209\nTransforming Data Using a Function or Mapping                                                211\nReplacing Values                                                                                                         212\nRenaming Axis Indexes                                                                                             214\nDiscretization and Binning                                                                                       215\nDetecting and Filtering Outliers                                                                              217\nPermutation and Random Sampling                                                                       219\nComputing Indicator/Dummy Variables                                                               221\n7.3 Extension Data Types                                                                                              224\n7.4 String Manipulation                                                                                                227\nPython Built-In String Object Methods                                                                  227\nRegular Expressions                                                                                                   229\nString Functions in pandas                                                                                       232\n7.5 Categorical Data                                                                                                       235\nBackground and Motivation                                                                                     236\nCategorical Extension Type in pandas                                                                    237\nComputations with Categoricals                                                                              240\nCategorical Methods                                                                                                  242\n7.6 Conclusion                                                                                                                245\n8. Data Wrangling: Join, Combine, and Reshape. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  247\n8.1 Hierarchical Indexing                                                                                             247\nReordering and Sorting Levels                                                                                 250\nSummary Statistics by Level                                                                                     251\nIndexing with a DataFrame’s columns                                                                    252\n8.2 Combining and Merging Datasets                                                                        253\nDatabase-Style DataFrame Joins                                                                              254\nMerging on Index                                                                                                       259\nvi \n| \nTable of Contents",
      "content_length": 4586,
      "extraction_method": "Direct"
    },
    {
      "page_number": 9,
      "chapter": null,
      "content": "Concatenating Along an Axis                                                                                   263\nCombining Data with Overlap                                                                                 268\n8.3 Reshaping and Pivoting                                                                                          270\nReshaping with Hierarchical Indexing                                                                   270\nPivoting “Long” to “Wide” Format                                                                          273\nPivoting “Wide” to “Long” Format                                                                          277\n8.4 Conclusion                                                                                                                279\n9. Plotting and Visualization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  281\n9.1 A Brief matplotlib API Primer                                                                               282\nFigures and Subplots                                                                                                  283\nColors, Markers, and Line Styles                                                                             288\nTicks, Labels, and Legends                                                                                        290\nAnnotations and Drawing on a Subplot                                                                 294\nSaving Plots to File                                                                                                     296\nmatplotlib Configuration                                                                                          297\n9.2 Plotting with pandas and seaborn                                                                         298\nLine Plots                                                                                                                     298\nBar Plots                                                                                                                       301\nHistograms and Density Plots                                                                                  309\nScatter or Point Plots                                                                                                 311\nFacet Grids and Categorical Data                                                                            314\n9.3 Other Python Visualization Tools                                                                         317\n9.4 Conclusion                                                                                                                317\n10. Data Aggregation and Group Operations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  319\n10.1 How to Think About Group Operations                                                           320\nIterating over Groups                                                                                                324\nSelecting a Column or Subset of Columns                                                             326\nGrouping with Dictionaries and Series                                                                   327\nGrouping with Functions                                                                                          328\nGrouping by Index Levels                                                                                         328\n10.2 Data Aggregation                                                                                                   329\nColumn-Wise and Multiple Function Application                                               331\nReturning Aggregated Data Without Row Indexes                                              335\n10.3 Apply: General split-apply-combine                                                                  335\nSuppressing the Group Keys                                                                                    338\nQuantile and Bucket Analysis                                                                                  338\nExample: Filling Missing Values with Group-Specific Values                             340\nExample: Random Sampling and Permutation                                                     343\nExample: Group Weighted Average and Correlation                                           344\nTable of Contents \n| \nvii",
      "content_length": 4473,
      "extraction_method": "Direct"
    },
    {
      "page_number": 10,
      "chapter": null,
      "content": "Example: Group-Wise Linear Regression                                                               347\n10.4 Group Transforms and “Unwrapped” GroupBys                                             347\n10.5 Pivot Tables and Cross-Tabulation                                                                     351\nCross-Tabulations: Crosstab                                                                                     354\n10.6 Conclusion                                                                                                             355\n11. Time Series. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  357\n11.1 Date and Time Data Types and Tools                                                                 358\nConverting Between String and Datetime                                                              359\n11.2 Time Series Basics                                                                                                  361\nIndexing, Selection, Subsetting                                                                                363\nTime Series with Duplicate Indices                                                                         365\n11.3 Date Ranges, Frequencies, and Shifting                                                             366\nGenerating Date Ranges                                                                                            367\nFrequencies and Date Offsets                                                                                   370\nShifting (Leading and Lagging) Data                                                                      371\n11.4 Time Zone Handling                                                                                             374\nTime Zone Localization and Conversion                                                               375\nOperations with Time Zone-Aware Timestamp Objects                                     377\nOperations Between Different Time Zones                                                            378\n11.5 Periods and Period Arithmetic                                                                            379\nPeriod Frequency Conversion                                                                                  380\nQuarterly Period Frequencies                                                                                   382\nConverting Timestamps to Periods (and Back)                                                     384\nCreating a PeriodIndex from Arrays                                                                       385\n11.6 Resampling and Frequency Conversion                                                            387\nDownsampling                                                                                                           388\nUpsampling and Interpolation                                                                                 391\nResampling with Periods                                                                                           392\nGrouped Time Resampling                                                                                       394\n11.7 Moving Window Functions                                                                                 396\nExponentially Weighted Functions                                                                          399\nBinary Moving Window Functions                                                                         401\nUser-Defined Moving Window Functions                                                             402\n11.8 Conclusion                                                                                                             403\n12. Introduction to Modeling Libraries in Python. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  405\n12.1 Interfacing Between pandas and Model Code                                                  405\n12.2 Creating Model Descriptions with Patsy                                                            408\nData Transformations in Patsy Formulas                                                               410\nCategorical Data and Patsy                                                                                       412\nviii \n| \nTable of Contents",
      "content_length": 4388,
      "extraction_method": "Direct"
    },
    {
      "page_number": 11,
      "chapter": null,
      "content": "12.3 Introduction to statsmodels                                                                                 415\nEstimating Linear Models                                                                                         415\nEstimating Time Series Processes                                                                            419\n12.4 Introduction to scikit-learn                                                                                  420\n12.5 Conclusion                                                                                                             423\n13. Data Analysis Examples. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  425\n13.1 Bitly Data from 1.USA.gov                                                                                   425\nCounting Time Zones in Pure Python                                                                    426\nCounting Time Zones with pandas                                                                         428\n13.2 MovieLens 1M Dataset                                                                                         435\nMeasuring Rating Disagreement                                                                             439\n13.3 US Baby Names 1880–2010                                                                                 443\nAnalyzing Naming Trends                                                                                        448\n13.4 USDA Food Database                                                                                           457\n13.5 2012 Federal Election Commission Database                                                   463\nDonation Statistics by Occupation and Employer                                                466\nBucketing Donation Amounts                                                                                 469\nDonation Statistics by State                                                                                      471\n13.6 Conclusion                                                                                                             472\nA. Advanced NumPy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  473\nA.1 ndarray Object Internals                                                                                      473\nNumPy Data Type Hierarchy                                                                                  474\nA.2 Advanced Array Manipulation                                                                            476\nReshaping Arrays                                                                                                      476\nC Versus FORTRAN Order                                                                                     478\nConcatenating and Splitting Arrays                                                                       479\nRepeating Elements: tile and repeat                                                                       481\nFancy Indexing Equivalents: take and put                                                            483\nA.3 Broadcasting                                                                                                           484\nBroadcasting over Other Axes                                                                                487\nSetting Array Values by Broadcasting                                                                    489\nA.4 Advanced ufunc Usage                                                                                         490\nufunc Instance Methods                                                                                           490\nWriting New ufuncs in Python                                                                               493\nA.5 Structured and Record Arrays                                                                             493\nNested Data Types and Multidimensional Fields                                                 494\nWhy Use Structured Arrays?                                                                                   495\nA.6 More About Sorting                                                                                              495\nIndirect Sorts: argsort and lexsort                                                                          497\nTable of Contents \n| \nix",
      "content_length": 4496,
      "extraction_method": "Direct"
    },
    {
      "page_number": 12,
      "chapter": null,
      "content": "Alternative Sort Algorithms                                                                                    498\nPartially Sorting Arrays                                                                                            499\nnumpy.searchsorted: Finding Elements in a Sorted Array                                 500\nA.7 Writing Fast NumPy Functions with Numba                                                    501\nCreating Custom numpy.ufunc Objects with Numba                                         502\nA.8 Advanced Array Input and Output                                                                    503\nMemory-Mapped Files                                                                                             503\nHDF5 and Other Array Storage Options                                                              504\nA.9 Performance Tips                                                                                                  505\nThe Importance of Contiguous Memory                                                              505\nB. More on the IPython System. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  509\nB.1 Terminal Keyboard Shortcuts                                                                              509\nB.2 About Magic Commands                                                                                      510\nThe %run Command                                                                                                512\nExecuting Code from the Clipboard                                                                      513\nB.3 Using the Command History                                                                               514\nSearching and Reusing the Command History                                                    514\nInput and Output Variables                                                                                     515\nB.4 Interacting with the Operating System                                                               516\nShell Commands and Aliases                                                                                  517\nDirectory Bookmark System                                                                                   518\nB.5 Software Development Tools                                                                                519\nInteractive Debugger                                                                                                519\nTiming Code: %time and %timeit                                                                          523\nBasic Profiling: %prun and %run -p                                                                      525\nProfiling a Function Line by Line                                                                           527\nB.6 Tips for Productive Code Development Using IPython                                  529\nReloading Module Dependencies                                                                           529\nCode Design Tips                                                                                                      530\nB.7 Advanced IPython Features                                                                                 532\nProfiles and Configuration                                                                                      532\nB.8 Conclusion                                                                                                              533\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  535\nx \n| \nTable of Contents",
      "content_length": 3717,
      "extraction_method": "Direct"
    },
    {
      "page_number": 13,
      "chapter": null,
      "content": "Preface\nThe first edition of this book was published in 2012, during a time when open source\ndata analysis libraries for Python, especially pandas, were very new and developing\nrapidly. When the time came to write the second edition in 2016 and 2017, I needed\nto update the book not only for Python 3.6 (the first edition used Python 2.7) but also\nfor the many changes in pandas that had occurred over the previous five years. Now\nin 2022, there are fewer Python language changes (we are now at Python 3.10, with\n3.11 coming out at the end of 2022), but pandas has continued to evolve.\nIn this third edition, my goal is to bring the content up to date with current versions\nof Python, NumPy, pandas, and other projects, while also remaining relatively con‐\nservative about discussing newer Python projects that have appeared in the last few\nyears. Since this book has become an important resource for many university courses\nand working professionals, I will try to avoid topics that are at risk of falling out of\ndate within a year or two. That way paper copies won’t be too difficult to follow in\n2023 or 2024 or beyond.\nA new feature of the third edition is the open access online version hosted on my\nwebsite at https://wesmckinney.com/book, to serve as a resource and convenience for\nowners of the print and digital editions. I intend to keep the content reasonably up to\ndate there, so if you own the paper book and run into something that doesn’t work\nproperly, you should check there for the latest content changes.\nConventions Used in This Book\nThe following typographical conventions are used in this book:\nItalic\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\nxi",
      "content_length": 1702,
      "extraction_method": "Direct"
    },
    {
      "page_number": 14,
      "chapter": null,
      "content": "Constant width\nUsed for program listings, as well as within paragraphs to refer to program\nelements such as variable or function names, databases, data types, environment\nvariables, statements, and keywords.\nConstant width bold\nShows commands or other text that should be typed literally by the user.\nConstant width italic\nShows text that should be replaced with user-supplied values or by values deter‐\nmined by context.\nThis element signifies a tip or suggestion.\nThis element signifies a general note.\nThis element indicates a warning or caution.\nUsing Code Examples\nYou can find data files and related material for each chapter in this book’s GitHub\nrepository at https://github.com/wesm/pydata-book, which is mirrored to Gitee (for\nthose who cannot access GitHub) at https://gitee.com/wesmckinn/pydata-book.\nThis book is here to help you get your job done. In general, if example code is\noffered with this book, you may use it in your programs and documentation. You\ndo not need to contact us for permission unless you’re reproducing a significant\nportion of the code. For example, writing a program that uses several chunks of code\nfrom this book does not require permission. Selling or distributing examples from\nO’Reilly books does require permission. Answering a question by citing this book\nand quoting example code does not require permission. Incorporating a significant\namount of example code from this book into your product’s documentation does\nrequire permission.\nxii \n| \nPreface",
      "content_length": 1495,
      "extraction_method": "Direct"
    },
    {
      "page_number": 15,
      "chapter": null,
      "content": "We appreciate, but do not require, attribution. An attribution usually includes the\ntitle, author, publisher, and ISBN. For example: “Python for Data Analysis by Wes\nMcKinney (O’Reilly). Copyright 2022 Wes McKinney, 978-1-098-10403-0.”\nIf you feel your use of code examples falls outside fair use or the permission given\nabove, feel free to contact us at permissions@oreilly.com.\nO’Reilly Online Learning\nFor more than 40 years, O’Reilly Media has provided technol‐\nogy and business training, knowledge, and insight to help\ncompanies succeed.\nOur unique network of experts and innovators share their knowledge and expertise\nthrough books, articles, and our online learning platform. O’Reilly’s online learning\nplatform gives you on-demand access to live training courses, in-depth learning\npaths, interactive coding environments, and a vast collection of text and video from\nO’Reilly and 200+ other publishers. For more information, visit http://oreilly.com.\nHow to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nWe have a web page for this book, where we list errata, examples, and any additional\ninformation. You can access this page at https://oreil.ly/python-data-analysis-3e.\nEmail bookquestions@oreilly.com to comment or ask technical questions about this\nbook.\nFor news and information about our books and courses, visit http://oreilly.com.\nFind us on LinkedIn: https://linkedin.com/company/oreilly-media.\nFollow us on Twitter: http://twitter.com/oreillymedia.\nWatch us on YouTube: http://youtube.com/oreillymedia.\nPreface \n| \nxiii",
      "content_length": 1755,
      "extraction_method": "Direct"
    },
    {
      "page_number": 16,
      "chapter": null,
      "content": "Acknowledgments\nThis work is the product of many years of fruitful discussions and collaborations\nwith, and assistance from many people around the world. I’d like to thank a few of\nthem.\nIn Memoriam: John D. Hunter (1968–2012)\nOur dear friend and colleague John D. Hunter passed away after a battle with colon\ncancer on August 28, 2012. This was only a short time after I’d completed the final\nmanuscript for this book’s first edition.\nJohn’s impact and legacy in the Python scientific and data communities would be\nhard to overstate. In addition to developing matplotlib in the early 2000s (a time\nwhen Python was not nearly so popular), he helped shape the culture of a critical\ngeneration of open source developers who’ve become pillars of the Python ecosystem\nthat we now often take for granted.\nI was lucky enough to connect with John early in my open source career in January\n2010, just after releasing pandas 0.1. His inspiration and mentorship helped me push\nforward, even in the darkest of times, with my vision for pandas and Python as a\nfirst-class data analysis language.\nJohn was very close with Fernando Pérez and Brian Granger, pioneers of IPython,\nJupyter, and many other initiatives in the Python community. We had hoped to work\non a book together, the four of us, but I ended up being the one with the most free\ntime. I am sure he would be proud of what we’ve accomplished, as individuals and as\na community, over the last nine years.\nAcknowledgments for the Third Edition (2022)\nIt has more than a decade since I started writing the first edition of this book and\nmore than 15 years since I originally started my journey as a Python prorammer.\nA lot has changed since then! Python has evolved from a relatively niche language\nfor data analysis to the most popular and most widely used language powering\nthe plurality (if not the majority!) of data science, machine learning, and artificial\nintelligence work.\nI have not been an active contributor to the pandas open source project since 2013,\nbut its worldwide developer community has continued to thrive, serving as a model\nof community-centric open source software development. Many “next-generation”\nPython projects that deal with tabular data are modeling their user interfaces directly\nafter pandas, so the project has proved to have an enduring influence on the future\ntrajectory of the Python data science ecosystem.\nxiv \n| \nPreface",
      "content_length": 2408,
      "extraction_method": "Direct"
    },
    {
      "page_number": 17,
      "chapter": null,
      "content": "I hope that this book continues to serve as a valuable resource for students and\nindividuals who want to learn about working with data in Python.\nI’m especially thankful to O’Reilly for allowing me to publish an “open access” version\nof this book on my website at https://wesmckinney.com/book, where I hope it will\nreach even more people and help expand opportunity in the world of data analysis.\nJ.J. Allaire was a lifesaver in making this possible by helping me “port” the book from\nDocbook XML to Quarto, a wonderful new scientific and technical publishing system\nfor print and web.\nSpecial thanks to my technical reviewers Paul Barry, Jean-Christophe Leyder, Abdul‐\nlah Karasan, and William Jamir, whose thorough feedback has greatly improved the\nreadability, clarity, and understandability of the content.\nAcknowledgments for the Second Edition (2017)\nIt has been five years almost to the day since I completed the manuscript for\nthis book’s first edition in July 2012. A lot has changed. The Python community\nhas grown immensely, and the ecosystem of open source software around it has\nflourished.\nThis new edition of the book would not exist if not for the tireless efforts of the\npandas core developers, who have grown the project and its user community into\none of the cornerstones of the Python data science ecosystem. These include, but are\nnot limited to, Tom Augspurger, Joris van den Bossche, Chris Bartak, Phillip Cloud,\ngfyoung, Andy Hayden, Masaaki Horikoshi, Stephan Hoyer, Adam Klein, Wouter\nOvermeire, Jeff Reback, Chang She, Skipper Seabold, Jeff Tratner, and y-p.\nOn the actual writing of this second edition, I would like to thank the O’Reilly staff\nwho helped me patiently with the writing process. This includes Marie Beaugureau,\nBen Lorica, and Colleen Toporek. I again had outstanding technical reviewers with\nTom Augspurger, Paul Barry, Hugh Brown, Jonathan Coe, and Andreas Müller con‐\ntributing. Thank you.\nThis book’s first edition has been translated into many foreign languages, including\nChinese, French, German, Japanese, Korean, and Russian. Translating all this content\nand making it available to a broader audience is a huge and often thankless effort.\nThank you for helping more people in the world learn how to program and use data\nanalysis tools.\nI am also lucky to have had support for my continued open source development\nefforts from Cloudera and Two Sigma Investments over the last few years. With open\nsource software projects more thinly resourced than ever relative to the size of user\nbases, it is becoming increasingly important for businesses to provide support for\ndevelopment of key open source projects. It’s the right thing to do.\nPreface \n| \nxv",
      "content_length": 2700,
      "extraction_method": "Direct"
    },
    {
      "page_number": 18,
      "chapter": null,
      "content": "Acknowledgments for the First Edition (2012)\nIt would have been difficult for me to write this book without the support of a large\nnumber of people.\nOn the O’Reilly staff, I’m very grateful for my editors, Meghan Blanchette and Julie\nSteele, who guided me through the process. Mike Loukides also worked with me in\nthe proposal stages and helped make the book a reality.\nI received a wealth of technical review from a large cast of characters. In particu‐\nlar, Martin Blais and Hugh Brown were incredibly helpful in improving the book’s\nexamples, clarity, and organization from cover to cover. James Long, Drew Conway,\nFernando Pérez, Brian Granger, Thomas Kluyver, Adam Klein, Josh Klein, Chang\nShe, and Stéfan van der Walt each reviewed one or more chapters, providing pointed\nfeedback from many different perspectives.\nI got many great ideas for examples and datasets from friends and colleagues in the\ndata community, among them: Mike Dewar, Jeff Hammerbacher, James Johndrow,\nKristian Lum, Adam Klein, Hilary Mason, Chang She, and Ashley Williams.\nI am of course indebted to the many leaders in the open source scientific Python\ncommunity who’ve built the foundation for my development work and gave encour‐\nagement while I was writing this book: the IPython core team (Fernando Pérez,\nBrian Granger, Min Ragan-Kelly, Thomas Kluyver, and others), John Hunter, Skipper\nSeabold, Travis Oliphant, Peter Wang, Eric Jones, Robert Kern, Josef Perktold, Fran‐\ncesc Alted, Chris Fonnesbeck, and too many others to mention. Several other people\nprovided a great deal of support, ideas, and encouragement along the way: Drew\nConway, Sean Taylor, Giuseppe Paleologo, Jared Lander, David Epstein, John Krowas,\nJoshua Bloom, Den Pilsworth, John Myles-White, and many others I’ve forgotten.\nI’d also like to thank a number of people from my formative years. First, my former\nAQR colleagues who’ve cheered me on in my pandas work over the years: Alex Reyf‐\nman, Michael Wong, Tim Sargen, Oktay Kurbanov, Matthew Tschantz, Roni Israelov,\nMichael Katz, Ari Levine, Chris Uga, Prasad Ramanan, Ted Square, and Hoon Kim.\nLastly, my academic advisors Haynes Miller (MIT) and Mike West (Duke).\nI received significant help from Phillip Cloud and Joris van den Bossche in 2014 to\nupdate the book’s code examples and fix some other inaccuracies due to changes in\npandas.\nOn the personal side, Casey provided invaluable day-to-day support during the\nwriting process, tolerating my highs and lows as I hacked together the final draft on\ntop of an already overcommitted schedule. Lastly, my parents, Bill and Kim, taught\nme to always follow my dreams and to never settle for less.\nxvi \n| \nPreface",
      "content_length": 2672,
      "extraction_method": "Direct"
    },
    {
      "page_number": 19,
      "chapter": null,
      "content": "CHAPTER 1\nPreliminaries\n1.1 What Is This Book About?\nThis book is concerned with the nuts and bolts of manipulating, processing, cleaning,\nand crunching data in Python. My goal is to offer a guide to the parts of the Python\nprogramming language and its data-oriented library ecosystem and tools that will\nequip you to become an effective data analyst. While “data analysis” is in the title\nof the book, the focus is specifically on Python programming, libraries, and tools as\nopposed to data analysis methodology. This is the Python programming you need for\ndata analysis.\nSometime after I originally published this book in 2012, people started using the\nterm data science as an umbrella description for everything from simple descriptive\nstatistics to more advanced statistical analysis and machine learning. The Python\nopen source ecosystem for doing data analysis (or data science) has also expanded\nsignificantly since then. There are now many other books which focus specifically on\nthese more advanced methodologies. My hope is that this book serves as adequate\npreparation to enable you to move on to a more domain-specific resource.\nSome might characterize much of the content of the book as “data\nmanipulation” as opposed to “data analysis.” We also use the terms\nwrangling or munging to refer to data manipulation.\nWhat Kinds of Data?\nWhen I say “data,” what am I referring to exactly? The primary focus is on structured\ndata, a deliberately vague term that encompasses many different common forms of\ndata, such as:\n1",
      "content_length": 1527,
      "extraction_method": "Direct"
    },
    {
      "page_number": 20,
      "chapter": null,
      "content": "• Tabular or spreadsheet-like data in which each column may be a different type\n•\n(string, numeric, date, or otherwise). This includes most kinds of data commonly\nstored in relational databases or tab- or comma-delimited text files.\n• Multidimensional arrays (matrices).\n•\n• Multiple tables of data interrelated by key columns (what would be primary or\n•\nforeign keys for a SQL user).\n• Evenly or unevenly spaced time series.\n•\nThis is by no means a complete list. Even though it may not always be obvious, a\nlarge percentage of datasets can be transformed into a structured form that is more\nsuitable for analysis and modeling. If not, it may be possible to extract features from\na dataset into a structured form. As an example, a collection of news articles could\nbe processed into a word frequency table, which could then be used to perform\nsentiment analysis.\nMost users of spreadsheet programs like Microsoft Excel, perhaps the most widely\nused data analysis tool in the world, will not be strangers to these kinds of data.\n1.2 Why Python for Data Analysis?\nFor many people, the Python programming language has strong appeal. Since its\nfirst appearance in 1991, Python has become one of the most popular interpreted\nprogramming languages, along with Perl, Ruby, and others. Python and Ruby have\nbecome especially popular since 2005 or so for building websites using their numer‐\nous web frameworks, like Rails (Ruby) and Django (Python). Such languages are\noften called scripting languages, as they can be used to quickly write small programs,\nor scripts to automate other tasks. I don’t like the term “scripting languages,” as it\ncarries a connotation that they cannot be used for building serious software. Among\ninterpreted languages, for various historical and cultural reasons, Python has devel‐\noped a large and active scientific computing and data analysis community. In the last\n20 years, Python has gone from a bleeding-edge or “at your own risk” scientific com‐\nputing language to one of the most important languages for data science, machine\nlearning, and general software development in academia and industry.\nFor data analysis and interactive computing and data visualization, Python will inevi‐\ntably draw comparisons with other open source and commercial programming lan‐\nguages and tools in wide use, such as R, MATLAB, SAS, Stata, and others. In recent\nyears, Python’s improved open source libraries (such as pandas and scikit-learn) have\nmade it a popular choice for data analysis tasks. Combined with Python’s overall\nstrength for general-purpose software engineering, it is an excellent option as a\nprimary language for building data applications.\n2 \n| \nChapter 1: Preliminaries",
      "content_length": 2703,
      "extraction_method": "Direct"
    },
    {
      "page_number": 21,
      "chapter": null,
      "content": "Python as Glue\nPart of Python’s success in scientific computing is the ease of integrating C, C++,\nand FORTRAN code. Most modern computing environments share a similar set of\nlegacy FORTRAN and C libraries for doing linear algebra, optimization, integration,\nfast Fourier transforms, and other such algorithms. The same story has held true for\nmany companies and national labs that have used Python to glue together decades’\nworth of legacy software.\nMany programs consist of small portions of code where most of the time is spent,\nwith large amounts of “glue code” that doesn’t run often. In many cases, the execution\ntime of the glue code is insignificant; effort is most fruitfully invested in optimizing\nthe computational bottlenecks, sometimes by moving the code to a lower-level lan‐\nguage like C.\nSolving the “Two-Language” Problem\nIn many organizations, it is common to research, prototype, and test new ideas using\na more specialized computing language like SAS or R and then later port those\nideas to be part of a larger production system written in, say, Java, C#, or C++.\nWhat people are increasingly finding is that Python is a suitable language not only\nfor doing research and prototyping but also for building the production systems.\nWhy maintain two development environments when one will suffice? I believe that\nmore and more companies will go down this path, as there are often significant\norganizational benefits to having both researchers and software engineers using the\nsame set of programming tools.\nOver the last decade some new approaches to solving the “two-language” problem\nhave appeared, such as the Julia programming language. Getting the most out of\nPython in many cases will require programming in a low-level language like C or\nC++ and creating Python bindings to that code. That said, “just-in-time” (JIT) com‐\npiler technology provided by libraries like Numba have provided a way to achieve\nexcellent performance in many computational algorithms without having to leave the\nPython programming environment.\nWhy Not Python?\nWhile Python is an excellent environment for building many kinds of analytical\napplications and general-purpose systems, there are a number of uses for which\nPython may be less suitable.\nAs Python is an interpreted programming language, in general most Python code\nwill run substantially slower than code written in a compiled language like Java or\nC++. As programmer time is often more valuable than CPU time, many are happy to\nmake this trade-off. However, in an application with very low latency or demanding\n1.2 Why Python for Data Analysis? \n| \n3",
      "content_length": 2608,
      "extraction_method": "Direct"
    },
    {
      "page_number": 22,
      "chapter": null,
      "content": "resource utilization requirements (e.g., a high-frequency trading system), the time\nspent programming in a lower-level (but also lower-productivity) language like C++\nto achieve the maximum possible performance might be time well spent.\nPython can be a challenging language for building highly concurrent, multithreaded\napplications, particularly applications with many CPU-bound threads. The reason for\nthis is that it has what is known as the global interpreter lock (GIL), a mechanism that\nprevents the interpreter from executing more than one Python instruction at a time.\nThe technical reasons for why the GIL exists are beyond the scope of this book. While\nit is true that in many big data processing applications, a cluster of computers may be\nrequired to process a dataset in a reasonable amount of time, there are still situations\nwhere a single-process, multithreaded system is desirable.\nThis is not to say that Python cannot execute truly multithreaded, parallel code.\nPython C extensions that use native multithreading (in C or C++) can run code in\nparallel without being impacted by the GIL, as long as they do not need to regularly\ninteract with Python objects.\n1.3 Essential Python Libraries\nFor those who are less familiar with the Python data ecosystem and the libraries used\nthroughout the book, I will give a brief overview of some of them.\nNumPy\nNumPy, short for Numerical Python, has long been a cornerstone of numerical\ncomputing in Python. It provides the data structures, algorithms, and library glue\nneeded for most scientific applications involving numerical data in Python. NumPy\ncontains, among other things:\n• A fast and efficient multidimensional array object ndarray\n•\n• Functions for performing element-wise computations with arrays or mathemati‐\n•\ncal operations between arrays\n• Tools for reading and writing array-based datasets to disk\n•\n• Linear algebra operations, Fourier transform, and random number generation\n•\n• A mature C API to enable Python extensions and native C or C++ code to access\n•\nNumPy’s data structures and computational facilities\nBeyond the fast array-processing capabilities that NumPy adds to Python, one of\nits primary uses in data analysis is as a container for data to be passed between\nalgorithms and libraries. For numerical data, NumPy arrays are more efficient for\nstoring and manipulating data than the other built-in Python data structures. Also,\nlibraries written in a lower-level language, such as C or FORTRAN, can operate on\n4 \n| \nChapter 1: Preliminaries",
      "content_length": 2529,
      "extraction_method": "Direct"
    },
    {
      "page_number": 23,
      "chapter": null,
      "content": "the data stored in a NumPy array without copying data into some other memory\nrepresentation. Thus, many numerical computing tools for Python either assume\nNumPy arrays as a primary data structure or else target interoperability with NumPy.\npandas\npandas provides high-level data structures and functions designed to make working\nwith structured or tabular data intuitive and flexible. Since its emergence in 2010, it\nhas helped enable Python to be a powerful and productive data analysis environment.\nThe primary objects in pandas that will be used in this book are the DataFrame, a\ntabular, column-oriented data structure with both row and column labels, and the\nSeries, a one-dimensional labeled array object.\npandas blends the array-computing ideas of NumPy with the kinds of data manipu‐\nlation capabilities found in spreadsheets and relational databases (such as SQL). It\nprovides convenient indexing functionality to enable you to reshape, slice and dice,\nperform aggregations, and select subsets of data. Since data manipulation, prepara‐\ntion, and cleaning are such important skills in data analysis, pandas is one of the\nprimary focuses of this book.\nAs a bit of background, I started building pandas in early 2008 during my tenure at\nAQR Capital Management, a quantitative investment management firm. At the time,\nI had a distinct set of requirements that were not well addressed by any single tool at\nmy disposal:\n• Data structures with labeled axes supporting automatic or explicit data align‐\n•\nment—this prevents common errors resulting from misaligned data and working\nwith differently indexed data coming from different sources\n• Integrated time series functionality\n•\n• The same data structures handle both time series data and non-time series data\n•\n• Arithmetic operations and reductions that preserve metadata\n•\n• Flexible handling of missing data\n•\n• Merge and other relational operations found in popular databases (SQL-based,\n•\nfor example)\nI wanted to be able to do all of these things in one place, preferably in a language\nwell suited to general-purpose software development. Python was a good candidate\nlanguage for this, but at that time an integrated set of data structures and tools\nproviding this functionality did not exist. As a result of having been built initially\nto solve finance and business analytics problems, pandas features especially deep\ntime series functionality and tools well suited for working with time-indexed data\ngenerated by business processes.\n1.3 Essential Python Libraries \n| \n5",
      "content_length": 2534,
      "extraction_method": "Direct"
    },
    {
      "page_number": 24,
      "chapter": null,
      "content": "I spent a large part of 2011 and 2012 expanding pandas’s capabilities with some of\nmy former AQR colleagues, Adam Klein and Chang She. In 2013, I stopped being\nas involved in day-to-day project development, and pandas has since become a fully\ncommunity-owned and community-maintained project with well over two thousand\nunique contributors around the world.\nFor users of the R language for statistical computing, the DataFrame name will be\nfamiliar, as the object was named after the similar R data.frame object. Unlike\nPython, data frames are built into the R programming language and its standard\nlibrary. As a result, many features found in pandas are typically either part of the R\ncore implementation or provided by add-on packages.\nThe pandas name itself is derived from panel data, an econometrics term for multidi‐\nmensional structured datasets, and a play on the phrase Python data analysis.\nmatplotlib\nmatplotlib is the most popular Python library for producing plots and other two-\ndimensional data visualizations. It was originally created by John D. Hunter and\nis now maintained by a large team of developers. It is designed for creating plots\nsuitable for publication. While there are other visualization libraries available to\nPython programmers, matplotlib is still widely used and integrates reasonably well\nwith the rest of the ecosystem. I think it is a safe choice as a default visualization tool.\nIPython and Jupyter\nThe IPython project began in 2001 as Fernando Pérez’s side project to make a\nbetter interactive Python interpreter. Over the subsequent 20 years it has become\none of the most important tools in the modern Python data stack. While it does\nnot provide any computational or data analytical tools by itself, IPython is designed\nfor both interactive computing and software development work. It encourages an\nexecute-explore workflow instead of the typical edit-compile-run workflow of many\nother programming languages. It also provides integrated access to your operating\nsystem’s shell and filesystem; this reduces the need to switch between a terminal\nwindow and a Python session in many cases. Since much of data analysis coding\ninvolves exploration, trial and error, and iteration, IPython can help you get the job\ndone faster.\nIn 2014, Fernando and the IPython team announced the Jupyter project, a broader\ninitiative to design language-agnostic interactive computing tools. The IPython web\nnotebook became the Jupyter notebook, with support now for over 40 programming\nlanguages. The IPython system can now be used as a kernel (a programming language\nmode) for using Python with Jupyter.\n6 \n| \nChapter 1: Preliminaries",
      "content_length": 2657,
      "extraction_method": "Direct"
    },
    {
      "page_number": 25,
      "chapter": null,
      "content": "IPython itself has become a component of the much broader Jupyter open source\nproject, which provides a productive environment for interactive and exploratory\ncomputing. Its oldest and simplest “mode” is as an enhanced Python shell designed\nto accelerate the writing, testing, and debugging of Python code. You can also use the\nIPython system through the Jupyter notebook.\nThe Jupyter notebook system also allows you to author content in Markdown and\nHTML, providing you a means to create rich documents with code and text.\nI personally use IPython and Jupyter regularly in my Python work, whether running,\ndebugging, or testing code.\nIn the accompanying book materials on GitHub, you will find Jupyter notebooks\ncontaining all the code examples from each chapter. If you cannot access GitHub\nwhere you are, you can try the mirror on Gitee.\nSciPy\nSciPy is a collection of packages addressing a number of foundational problems in\nscientific computing. Here are some of the tools it contains in its various modules:\nscipy.integrate\nNumerical integration routines and differential equation solvers\nscipy.linalg\nLinear algebra routines and matrix decompositions extending beyond those pro‐\nvided in numpy.linalg\nscipy.optimize\nFunction optimizers (minimizers) and root finding algorithms\nscipy.signal\nSignal processing tools\nscipy.sparse\nSparse matrices and sparse linear system solvers\nscipy.special\nWrapper around SPECFUN, a FORTRAN library implementing many common\nmathematical functions, such as the gamma function\nscipy.stats\nStandard continuous and discrete probability distributions (density functions,\nsamplers, continuous distribution functions), various statistical tests, and more\ndescriptive statistics\n1.3 Essential Python Libraries \n| \n7",
      "content_length": 1747,
      "extraction_method": "Direct"
    },
    {
      "page_number": 26,
      "chapter": null,
      "content": "Together, NumPy and SciPy form a reasonably complete and mature computational\nfoundation for many traditional scientific computing applications.\nscikit-learn\nSince the project’s inception in 2007, scikit-learn has become the premier general-\npurpose machine learning toolkit for Python programmers. As of this writing, more\nthan two thousand different individuals have contributed code to the project. It\nincludes submodules for such models as:\n• Classification: SVM, nearest neighbors, random forest, logistic regression, etc.\n•\n• Regression: Lasso, ridge regression, etc.\n•\n• Clustering: k-means, spectral clustering, etc.\n•\n• Dimensionality reduction: PCA, feature selection, matrix factorization, etc.\n•\n• Model selection: Grid search, cross-validation, metrics\n•\n• Preprocessing: Feature extraction, normalization\n•\nAlong with pandas, statsmodels, and IPython, scikit-learn has been critical for ena‐\nbling Python to be a productive data science programming language. While I won’t\nbe able to include a comprehensive guide to scikit-learn in this book, I will give a\nbrief introduction to some of its models and how to use them with the other tools\npresented in the book.\nstatsmodels\nstatsmodels is a statistical analysis package that was seeded by work from Stanford\nUniversity statistics professor Jonathan Taylor, who implemented a number of regres‐\nsion analysis models popular in the R programming language. Skipper Seabold and\nJosef Perktold formally created the new statsmodels project in 2010 and since then\nhave grown the project to a critical mass of engaged users and contributors. Nathaniel\nSmith developed the Patsy project, which provides a formula or model specification\nframework for statsmodels inspired by R’s formula system.\nCompared with scikit-learn, statsmodels contains algorithms for classical (primarily\nfrequentist) statistics and econometrics. This includes such submodules as:\n• Regression models: linear regression, generalized linear models, robust linear\n•\nmodels, linear mixed effects models, etc.\n• Analysis of variance (ANOVA)\n•\n• Time series analysis: AR, ARMA, ARIMA, VAR, and other models\n•\n• Nonparametric methods: Kernel density estimation, kernel regression\n•\n8 \n| \nChapter 1: Preliminaries",
      "content_length": 2235,
      "extraction_method": "Direct"
    },
    {
      "page_number": 27,
      "chapter": null,
      "content": "• Visualization of statistical model results\n•\nstatsmodels is more focused on statistical inference, providing uncertainty estimates\nand p-values for parameters. scikit-learn, by contrast, is more prediction focused.\nAs with scikit-learn, I will give a brief introduction to statsmodels and how to use it\nwith NumPy and pandas.\nOther Packages\nIn 2022, there are many other Python libraries which might be discussed in a book\nabout data science. This includes some newer projects like TensorFlow or PyTorch,\nwhich have become popular for machine learning or artificial intelligence work. Now\nthat there are other books out there that focus more specifically on those projects, I\nwould recommend using this book to build a foundation in general-purpose Python\ndata wrangling. Then, you should be well prepared to move on to a more advanced\nresource that may assume a certain level of expertise.\n1.4 Installation and Setup\nSince everyone uses Python for different applications, there is no single solution for\nsetting up Python and obtaining the necessary add-on packages. Many readers will\nnot have a complete Python development environment suitable for following along\nwith this book, so here I will give detailed instructions to get set up on each operating\nsystem. I will be using Miniconda, a minimal installation of the conda package\nmanager, along with conda-forge, a community-maintained software distribution\nbased on conda. This book uses Python 3.10 throughout, but if you’re reading in the\nfuture, you are welcome to install a newer version of Python.\nIf for some reason these instructions become out-of-date by the time you are reading\nthis, you can check out my website for the book which I will endeavor to keep up to\ndate with the latest installation instructions.\nMiniconda on Windows\nTo get started on Windows, download the Miniconda installer for the latest Python\nversion available (currently 3.9) from https://conda.io. I recommend following the\ninstallation instructions for Windows available on the conda website, which may have\nchanged between the time this book was published and when you are reading this.\nMost people will want the 64-bit version, but if that doesn’t run on your Windows\nmachine, you can install the 32-bit version instead.\nWhen prompted whether to install for just yourself or for all users on your system,\nchoose the option that’s most appropriate for you. Installing just for yourself will be\nsufficient to follow along with the book. It will also ask you whether you want to\n1.4 Installation and Setup \n| \n9",
      "content_length": 2551,
      "extraction_method": "Direct"
    },
    {
      "page_number": 28,
      "chapter": null,
      "content": "add Miniconda to the system PATH environment variable. If you select this (I usually\ndo), then this Miniconda installation may override other versions of Python you have\ninstalled. If you do not, then you will need to use the Window Start menu shortcut\nthat’s installed to be able to use this Miniconda. This Start menu entry may be called\n“Anaconda3 (64-bit).”\nI’ll assume that you haven’t added Miniconda to your system PATH. To verify that\nthings are configured correctly, open the “Anaconda Prompt (Miniconda3)” entry\nunder “Anaconda3 (64-bit)” in the Start menu. Then try launching the Python inter‐\npreter by typing python. You should see a message like this:\n(base) C:\\Users\\Wes>python\nPython 3.9 [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>>\nTo exit the Python shell, type exit() and press Enter.\nGNU/Linux\nLinux details will vary a bit depending on your Linux distribution type, but here I\ngive details for such distributions as Debian, Ubuntu, CentOS, and Fedora. Setup is\nsimilar to macOS with the exception of how Miniconda is installed. Most readers will\nwant to download the default 64-bit installer file, which is for x86 architecture (but\nit’s possible in the future more users will have aarch64-based Linux machines). The\ninstaller is a shell script that must be executed in the terminal. You will then have\na file named something similar to Miniconda3-latest-Linux-x86_64.sh. To install it,\nexecute this script with bash:\n$ bash Miniconda3-latest-Linux-x86_64.sh\nSome Linux distributions have all the required Python packages\n(although outdated versions, in some cases) in their package man‐\nagers and can be installed using a tool like apt. The setup described\nhere uses Miniconda, as it’s both easily reproducible across distri‐\nbutions and simpler to upgrade packages to their latest versions.\nYou will have a choice of where to put the Miniconda files. I recommend installing\nthe files in the default location in your home directory; for example, /home/$USER/\nminiconda (with your username, naturally).\nThe installer will ask if you wish to modify your shell scripts to automatically activate\nMiniconda. I recommend doing this (select “yes”) as a matter of convenience.\nAfter completing the installation, start a new terminal process and verify that you are\npicking up the new Miniconda installation:\n10 \n| \nChapter 1: Preliminaries",
      "content_length": 2438,
      "extraction_method": "Direct"
    },
    {
      "page_number": 29,
      "chapter": null,
      "content": "(base) $ python\nPython 3.9 | (main) [GCC 10.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>>\nTo exit the Python shell, type exit() and press Enter or press Ctrl-D.\nMiniconda on macOS\nDownload the macOS Miniconda installer, which should be named something\nlike Miniconda3-latest-MacOSX-arm64.sh for Apple Silicon-based macOS computers\nreleased from 2020 onward, or Miniconda3-latest-MacOSX-x86_64.sh for Intel-based\nMacs released before 2020. Open the Terminal application in macOS, and install by\nexecuting the installer (most likely in your Downloads directory) with bash:\n$ bash $HOME/Downloads/Miniconda3-latest-MacOSX-arm64.sh\nWhen the installer runs, by default it automatically configures Miniconda in your\ndefault shell environment in your default shell profile. This is probably located\nat /Users/$USER/.zshrc. I recommend letting it do this; if you do not want to allow\nthe installer to modify your default shell environment, you will need to consult the\nMiniconda documentation to be able to proceed.\nTo verify everything is working, try launching Python in the system shell (open the\nTerminal application to get a command prompt):\n$ python\nPython 3.9 (main) [Clang 12.0.1 ] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>>\nTo exit the shell, press Ctrl-D or type exit() and press Enter.\nInstalling Necessary Packages\nNow that we have set up Miniconda on your system, it’s time to install the main\npackages we will be using in this book. The first step is to configure conda-forge as\nyour default package channel by running the following commands in a shell:\n(base) $ conda config --add channels conda-forge\n(base) $ conda config --set channel_priority strict\nNow, we will create a new conda “environment” with the conda create command\nusing Python 3.10:\n(base) $ conda create -y -n pydata-book python=3.10\nAfter the installation completes, activate the environment with conda activate:\n(base) $ conda activate pydata-book\n(pydata-book) $\n1.4 Installation and Setup \n| \n11",
      "content_length": 2058,
      "extraction_method": "Direct"
    },
    {
      "page_number": 30,
      "chapter": null,
      "content": "It is necessary to use conda activate to activate your environment\neach time you open a new terminal. You can see information about\nthe active conda environment at any time from the terminal by\nrunning conda info.\nNow, we will install the essential packages used throughout the book (along with their\ndependencies) with conda install:\n(pydata-book) $ conda install -y pandas jupyter matplotlib\nWe will be using some other packages, too, but these can be installed later once they\nare needed. There are two ways to install packages: with conda install and with\npip install. conda install should always be preferred when using Miniconda, but\nsome packages are not available through conda, so if conda install $package_name\nfails, try pip install $package_name.\nIf you want to install all of the packages used in the rest of the\nbook, you can do that now by running:\nconda install lxml beautifulsoup4 html5lib openpyxl \\\n               requests sqlalchemy seaborn scipy statsmodels \\\n               patsy scikit-learn pyarrow pytables numba\nOn Windows, substitute a carat ^ for the line continuation \\ used\non Linux and macOS.\nYou can update packages by using the conda update command:\nconda update package_name\npip also supports upgrades using the --upgrade flag:\npip install --upgrade package_name\nYou will have several opportunities to try out these commands throughout the book.\nWhile you can use both conda and pip to install packages, you\nshould avoid updating packages originally installed with conda\nusing pip (and vice versa), as doing so can lead to environment\nproblems. I recommend sticking to conda if you can and falling\nback on pip only for packages that are unavailable with conda\ninstall.\nIntegrated Development Environments and Text Editors\nWhen asked about my standard development environment, I almost always say “IPy‐\nthon plus a text editor.” I typically write a program and iteratively test and debug each\npiece of it in IPython or Jupyter notebooks. It is also useful to be able to play around\n12 \n| \nChapter 1: Preliminaries",
      "content_length": 2046,
      "extraction_method": "Direct"
    },
    {
      "page_number": 31,
      "chapter": null,
      "content": "with data interactively and visually verify that a particular set of data manipulations is\ndoing the right thing. Libraries like pandas and NumPy are designed to be productive\nto use in the shell.\nWhen building software, however, some users may prefer to use a more richly\nfeatured integrated development environment (IDE) and rather than an editor like\nEmacs or Vim which provide a more minimal environment out of the box. Here are\nsome that you can explore:\n• PyDev (free), an IDE built on the Eclipse platform\n•\n• PyCharm from JetBrains (subscription-based for commercial users, free for open\n•\nsource developers)\n• Python Tools for Visual Studio (for Windows users)\n•\n• Spyder (free), an IDE currently shipped with Anaconda\n•\n• Komodo IDE (commercial)\n•\nDue to the popularity of Python, most text editors, like VS Code and Sublime Text 2,\nhave excellent Python support.\n1.5 Community and Conferences\nOutside of an internet search, the various scientific and data-related Python mailing\nlists are generally helpful and responsive to questions. Some to take a look at include:\n• pydata: A Google Group list for questions related to Python for data analysis and\n•\npandas\n• pystatsmodels: For statsmodels or pandas-related questions\n•\n• Mailing list for scikit-learn (scikit-learn@python.org) and machine learning in\n•\nPython, generally\n• numpy-discussion: For NumPy-related questions\n•\n• scipy-user: For general SciPy or scientific Python questions\n•\nI deliberately did not post URLs for these in case they change. They can be easily\nlocated via an internet search.\nEach year many conferences are held all over the world for Python programmers.\nIf you would like to connect with other Python programmers who share your inter‐\nests, I encourage you to explore attending one, if possible. Many conferences have\nfinancial support available for those who cannot afford admission or travel to the\nconference. Here are some to consider:\n1.5 Community and Conferences \n| \n13",
      "content_length": 1968,
      "extraction_method": "Direct"
    },
    {
      "page_number": 32,
      "chapter": null,
      "content": "• PyCon and EuroPython: The two main general Python conferences in North\n•\nAmerica and Europe, respectively\n• SciPy and EuroSciPy: Scientific-computing-oriented conferences in North Amer‐\n•\nica and Europe, respectively\n• PyData: A worldwide series of regional conferences targeted at data science and\n•\ndata analysis use cases\n• International and regional PyCon conferences (see https://pycon.org for a com‐\n•\nplete listing)\n1.6 Navigating This Book\nIf you have never programmed in Python before, you will want to spend some time\nin Chapters 2 and 3, where I have placed a condensed tutorial on Python language\nfeatures and the IPython shell and Jupyter notebooks. These things are prerequisite\nknowledge for the remainder of the book. If you have Python experience already, you\nmay instead choose to skim or skip these chapters.\nNext, I give a short introduction to the key features of NumPy, leaving more\nadvanced NumPy use for Appendix A. Then, I introduce pandas and devote the\nrest of the book to data analysis topics applying pandas, NumPy, and matplotlib\n(for visualization). I have structured the material in an incremental fashion, though\nthere is occasionally some minor crossover between chapters, with a few cases where\nconcepts are used that haven’t been introduced yet.\nWhile readers may have many different end goals for their work, the tasks required\ngenerally fall into a number of different broad groups:\nInteracting with the outside world\nReading and writing with a variety of file formats and data stores\nPreparation\nCleaning, munging, combining, normalizing, reshaping, slicing and dicing, and\ntransforming data for analysis\nTransformation\nApplying mathematical and statistical operations to groups of datasets to derive\nnew datasets (e.g., aggregating a large table by group variables)\nModeling and computation\nConnecting your data to statistical models, machine learning algorithms, or other\ncomputational tools\nPresentation\nCreating interactive or static graphical visualizations or textual summaries\n14 \n| \nChapter 1: Preliminaries",
      "content_length": 2056,
      "extraction_method": "Direct"
    },
    {
      "page_number": 33,
      "chapter": null,
      "content": "Code Examples\nMost of the code examples in the book are shown with input and output as it would\nappear executed in the IPython shell or in Jupyter notebooks:\nIn [5]: CODE EXAMPLE\nOut[5]: OUTPUT\nWhen you see a code example like this, the intent is for you to type the example code\nin the In block in your coding environment and execute it by pressing the Enter key\n(or Shift-Enter in Jupyter). You should see output similar to what is shown in the Out\nblock.\nI changed the default console output settings in NumPy and pandas to improve\nreadability and brevity throughout the book. For example, you may see more digits\nof precision printed in numeric data. To exactly match the output shown in the book,\nyou can execute the following Python code before running the code examples:\nimport numpy as np\nimport pandas as pd\npd.options.display.max_columns = 20\npd.options.display.max_rows = 20\npd.options.display.max_colwidth = 80\nnp.set_printoptions(precision=4, suppress=True)\nData for Examples\nDatasets for the examples in each chapter are hosted in a GitHub repository (or in a\nmirror on Gitee if you cannot access GitHub). You can download this data either by\nusing the Git version control system on the command line or by downloading a zip\nfile of the repository from the website. If you run into problems, navigate to the book\nwebsite for up-to-date instructions about obtaining the book materials.\nIf you download a zip file containing the example datasets, you must then fully\nextract the contents of the zip file to a directory and navigate to that directory from\nthe terminal before proceeding with running the book’s code examples:\n$ pwd\n/home/wesm/book-materials\n$ ls\nappa.ipynb  ch05.ipynb  ch09.ipynb  ch13.ipynb  README.md\nch02.ipynb  ch06.ipynb  ch10.ipynb  COPYING     requirements.txt\nch03.ipynb  ch07.ipynb  ch11.ipynb  datasets\nch04.ipynb  ch08.ipynb  ch12.ipynb  examples\n1.6 Navigating This Book \n| \n15",
      "content_length": 1917,
      "extraction_method": "Direct"
    },
    {
      "page_number": 34,
      "chapter": null,
      "content": "I have made every effort to ensure that the GitHub repository contains everything\nnecessary to reproduce the examples, but I may have made some mistakes or omis‐\nsions. If so, please send me an email: book@wesmckinney.com. The best way to report\nerrors in the book is on the errata page on the O’Reilly website.\nImport Conventions\nThe Python community has adopted a number of naming conventions for commonly\nused modules:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels as sm\nThis means that when you see np.arange, this is a reference to the arange function\nin NumPy. This is done because it’s considered bad practice in Python software\ndevelopment to import everything (from numpy import *) from a large package like\nNumPy.\n16 \n| \nChapter 1: Preliminaries",
      "content_length": 821,
      "extraction_method": "Direct"
    },
    {
      "page_number": 35,
      "chapter": null,
      "content": "CHAPTER 2\nPython Language Basics, IPython,\nand Jupyter Notebooks\nWhen I wrote the first edition of this book in 2011 and 2012, there were fewer\nresources available for learning about doing data analysis in Python. This was\npartially a chicken-and-egg problem; many libraries that we now take for granted,\nlike pandas, scikit-learn, and statsmodels, were comparatively immature back then.\nNow in 2022, there is now a growing literature on data science, data analysis, and\nmachine learning, supplementing the prior works on general-purpose scientific com‐\nputing geared toward computational scientists, physicists, and professionals in other\nresearch fields. There are also excellent books about learning the Python program‐\nming language itself and becoming an effective software engineer.\nAs this book is intended as an introductory text in working with data in Python, I\nfeel it is valuable to have a self-contained overview of some of the most important\nfeatures of Python’s built-in data structures and libraries from the perspective of data\nmanipulation. So, I will only present roughly enough information in this chapter and\nChapter 3 to enable you to follow along with the rest of the book.\nMuch of this book focuses on table-based analytics and data preparation tools for\nworking with datasets that are small enough to fit on your personal computer. To\nuse these tools you must sometimes do some wrangling to arrange messy data into\na more nicely tabular (or structured) form. Fortunately, Python is an ideal language\nfor doing this. The greater your facility with the Python language and its built-in data\ntypes, the easier it will be for you to prepare new datasets for analysis.\nSome of the tools in this book are best explored from a live IPython or Jupyter\nsession. Once you learn how to start up IPython and Jupyter, I recommend that you\nfollow along with the examples so you can experiment and try different things. As\n17",
      "content_length": 1935,
      "extraction_method": "Direct"
    },
    {
      "page_number": 36,
      "chapter": null,
      "content": "with any keyboard-driven console-like environment, developing familiarity with the\ncommon commands is also part of the learning curve.\nThere are introductory Python concepts that this chapter does not\ncover, like classes and object-oriented programming, which you\nmay find useful in your foray into data analysis in Python.\nTo deepen your Python language knowledge, I recommend that\nyou supplement this chapter with the official Python tutorial and\npotentially one of the many excellent books on general-purpose\nPython programming. Some recommendations to get you started\ninclude:\n• Python Cookbook, Third Edition, by David Beazley and Brian\n•\nK. Jones (O’Reilly)\n• Fluent Python by Luciano Ramalho (O’Reilly)\n•\n• Effective Python, Second Edition, by Brett Slatkin (Addison-\n•\nWesley)\n2.1 The Python Interpreter\nPython is an interpreted language. The Python interpreter runs a program by execut‐\ning one statement at a time. The standard interactive Python interpreter can be\ninvoked on the command line with the python command:\n$ python\nPython 3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:38:57)\n[GCC 10.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> a = 5\n>>> print(a)\n5\nThe >>> you see is the prompt after which you’ll type code expressions. To exit the\nPython interpreter, you can either type exit() or press Ctrl-D (works on Linux and\nmacOS only).\nRunning Python programs is as simple as calling python with a .py file as its first\nargument. Suppose we had created hello_world.py with these contents:\nprint(\"Hello world\")\nYou can run it by executing the following command (the hello_world.py file must be\nin your current working terminal directory):\n$ python hello_world.py\nHello world\n18 \n| \nChapter 2: Python Language Basics, IPython, and Jupyter Notebooks",
      "content_length": 1818,
      "extraction_method": "Direct"
    },
    {
      "page_number": 37,
      "chapter": null,
      "content": "While some Python programmers execute all of their Python code in this way,\nthose doing data analysis or scientific computing make use of IPython, an enhanced\nPython interpreter, or Jupyter notebooks, web-based code notebooks originally cre‐\nated within the IPython project. I give an introduction to using IPython and Jupyter\nin this chapter and have included a deeper look at IPython functionality in Appen‐\ndix A. When you use the %run command, IPython executes the code in the specified\nfile in the same process, enabling you to explore the results interactively when it’s\ndone:\n$ ipython\nPython 3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:38:57)\nType 'copyright', 'credits' or 'license' for more information\nIPython 7.31.1 -- An enhanced Interactive Python. Type '?' for help.\nIn [1]: %run hello_world.py\nHello world\nIn [2]:\nThe default IPython prompt adopts the numbered In [2]: style, compared with the\nstandard >>> prompt.\n2.2 IPython Basics\nIn this section, I’ll get you up and running with the IPython shell and Jupyter\nnotebook, and introduce you to some of the essential concepts.\nRunning the IPython Shell\nYou can launch the IPython shell on the command line just like launching the regular\nPython interpreter except with the ipython command:\n$ ipython\nPython 3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:38:57)\nType 'copyright', 'credits' or 'license' for more information\nIPython 7.31.1 -- An enhanced Interactive Python. Type '?' for help.\nIn [1]: a = 5\nIn [2]: a\nOut[2]: 5\nYou can execute arbitrary Python statements by typing them and pressing Return (or\nEnter). When you type just a variable into IPython, it renders a string representation\nof the object:\nIn [5]: import numpy as np\nIn [6]: data = [np.random.standard_normal() for i in range(7)]\n2.2 IPython Basics \n| \n19",
      "content_length": 1817,
      "extraction_method": "Direct"
    },
    {
      "page_number": 38,
      "chapter": null,
      "content": "In [7]: data\nOut[7]: \n[-0.20470765948471295,\n 0.47894333805754824,\n -0.5194387150567381,\n -0.55573030434749,\n 1.9657805725027142,\n 1.3934058329729904,\n 0.09290787674371767]\nThe first two lines are Python code statements; the second statement creates a vari‐\nable named data that refers to a newly created Python dictionary. The last line prints\nthe value of data in the console.\nMany kinds of Python objects are formatted to be more readable, or pretty-printed,\nwhich is distinct from normal printing with print. If you printed the above data\nvariable in the standard Python interpreter, it would be much less readable:\n>>> import numpy as np\n>>> data = [np.random.standard_normal() for i in range(7)]\n>>> print(data)\n>>> data\n[-0.5767699931966723, -0.1010317773535111, -1.7841005313329152,\n-1.524392126408841, 0.22191374220117385, -1.9835710588082562,\n-1.6081963964963528]\nIPython also provides facilities to execute arbitrary blocks of code (via a somewhat\nglorified copy-and-paste approach) and whole Python scripts. You can also use the\nJupyter notebook to work with larger blocks of code, as we will soon see.\nRunning the Jupyter Notebook\nOne of the major components of the Jupyter project is the notebook, a type of\ninteractive document for code, text (including Markdown), data visualizations, and\nother output. The Jupyter notebook interacts with kernels, which are implementations\nof the Jupyter interactive computing protocol specific to different programming\nlanguages. The Python Jupyter kernel uses the IPython system for its underlying\nbehavior.\nTo start up Jupyter, run the command jupyter notebook in a terminal:\n$ jupyter notebook\n[I 15:20:52.739 NotebookApp] Serving notebooks from local directory:\n/home/wesm/code/pydata-book\n[I 15:20:52.739 NotebookApp] 0 active kernels\n[I 15:20:52.739 NotebookApp] The Jupyter Notebook is running at:\nhttp://localhost:8888/?token=0a77b52fefe52ab83e3c35dff8de121e4bb443a63f2d...\n[I 15:20:52.740 NotebookApp] Use Control-C to stop this server and shut down\nall kernels (twice to skip confirmation).\nCreated new window in existing browser session.\n20 \n| \nChapter 2: Python Language Basics, IPython, and Jupyter Notebooks",
      "content_length": 2172,
      "extraction_method": "Direct"
    },
    {
      "page_number": 39,
      "chapter": null,
      "content": "To access the notebook, open this file in a browser:\n        file:///home/wesm/.local/share/jupyter/runtime/nbserver-185259-open.html\n    Or copy and paste one of these URLs:\n        http://localhost:8888/?token=0a77b52fefe52ab83e3c35dff8de121e4...\n     or http://127.0.0.1:8888/?token=0a77b52fefe52ab83e3c35dff8de121e4...\nOn many platforms, Jupyter will automatically open in your default web browser\n(unless you start it with --no-browser). Otherwise, you can navigate to the HTTP\naddress printed when you started the notebook, here http://localhost:8888/?\ntoken=0a77b52fefe52ab83e3c35dff8de121e4bb443a63f2d3055. See Figure 2-1 for\nwhat this looks like in Google Chrome.\nMany people use Jupyter as a local computing environment, but\nit can also be deployed on servers and accessed remotely. I won’t\ncover those details here, but I encourage you to explore this topic\non the internet if it’s relevant to your needs.\nFigure 2-1. Jupyter notebook landing page\n2.2 IPython Basics \n| \n21",
      "content_length": 984,
      "extraction_method": "Direct"
    },
    {
      "page_number": 40,
      "chapter": null,
      "content": "To create a new notebook, click the New button and select the “Python 3” option.\nYou should see something like Figure 2-2. If this is your first time, try clicking on\nthe empty code “cell” and entering a line of Python code. Then press Shift-Enter to\nexecute it.\nFigure 2-2. Jupyter new notebook view\nWhen you save the notebook (see “Save and Checkpoint” under the notebook File\nmenu), it creates a file with the extension .ipynb. This is a self-contained file format\nthat contains all of the content (including any evaluated code output) currently in the\nnotebook. These can be loaded and edited by other Jupyter users.\nTo rename an open notebook, click on the notebook title at the top of the page and\ntype the new title, pressing Enter when you are finished.\nTo load an existing notebook, put the file in the same directory where you started the\nnotebook process (or in a subfolder within it), then click the name from the landing\npage. You can try it out with the notebooks from my wesm/pydata-book repository on\nGitHub. See Figure 2-3.\nWhen you want to close a notebook, click the File menu and select “Close and Halt.”\nIf you simply close the browser tab, the Python process associated with the notebook\nwill keep running in the background.\nWhile the Jupyter notebook may feel like a distinct experience from the IPython\nshell, nearly all of the commands and tools in this chapter can be used in either\nenvironment.\n22 \n| \nChapter 2: Python Language Basics, IPython, and Jupyter Notebooks",
      "content_length": 1494,
      "extraction_method": "Direct"
    },
    {
      "page_number": 41,
      "chapter": null,
      "content": "Figure 2-3. Jupyter example view for an existing notebook\nTab Completion\nOn the surface, the IPython shell looks like a cosmetically different version of the\nstandard terminal Python interpreter (invoked with python). One of the major\nimprovements over the standard Python shell is tab completion, found in many IDEs\nor other interactive computing analysis environments. While entering expressions in\nthe shell, pressing the Tab key will search the namespace for any variables (objects,\nfunctions, etc.) matching the characters you have typed so far and show the results in\na convenient drop-down menu:\nIn [1]: an_apple = 27\nIn [2]: an_example = 42\nIn [3]: an<Tab>\nan_apple   an_example  any\nIn this example, note that IPython displayed both of the two variables I defined, as\nwell as the built-in function any. Also, you can also complete methods and attributes\non any object after typing a period:\n2.2 IPython Basics \n| \n23",
      "content_length": 925,
      "extraction_method": "Direct"
    },
    {
      "page_number": 42,
      "chapter": null,
      "content": "In [3]: b = [1, 2, 3]\nIn [4]: b.<Tab>\nappend()  count()   insert()  reverse()\nclear()   extend()  pop()     sort()\ncopy()    index()   remove()\nThe same is true for modules:\nIn [1]: import datetime\nIn [2]: datetime.<Tab>\ndate          MAXYEAR       timedelta\ndatetime      MINYEAR       timezone\ndatetime_CAPI time          tzinfo\nNote that IPython by default hides methods and attributes starting\nwith underscores, such as magic methods and internal “private”\nmethods and attributes, in order to avoid cluttering the display\n(and confusing novice users!). These, too, can be tab-completed,\nbut you must first type an underscore to see them. If you prefer\nto always see such methods in tab completion, you can change this\nsetting in the IPython configuration. See the IPython documenta‐\ntion to find out how to do this.\nTab completion works in many contexts outside of searching the interactive name‐\nspace and completing object or module attributes. When typing anything that looks\nlike a file path (even in a Python string), pressing the Tab key will complete anything\non your computer’s filesystem matching what you’ve typed.\nCombined with the %run command (see “The %run Command” on page 512), this\nfunctionality can save you many keystrokes.\nAnother area where tab completion saves time is in the completion of function\nkeyword arguments (including the = sign!). See Figure 2-4.\nFigure 2-4. Autocomplete function keywords in a Jupyter notebook\nWe’ll have a closer look at functions in a little bit.\n24 \n| \nChapter 2: Python Language Basics, IPython, and Jupyter Notebooks",
      "content_length": 1576,
      "extraction_method": "Direct"
    },
    {
      "page_number": 43,
      "chapter": null,
      "content": "Introspection\nUsing a question mark (?) before or after a variable will display some general infor‐\nmation about the object:\nIn [1]: b = [1, 2, 3]\nIn [2]: b?\nType:        list\nString form: [1, 2, 3]\nLength:      3\nDocstring:\nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\nIn [3]: print?\nDocstring:\nprint(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False)\nPrints the values to a stream, or to sys.stdout by default.\nOptional keyword arguments:\nfile:  a file-like object (stream); defaults to the current sys.stdout.\nsep:   string inserted between values, default a space.\nend:   string appended after the last value, default a newline.\nflush: whether to forcibly flush the stream.\nType:      builtin_function_or_method\nThis is referred to as object introspection. If the object is a function or instance\nmethod, the docstring, if defined, will also be shown. Suppose we’d written the\nfollowing function (which you can reproduce in IPython or Jupyter):\ndef add_numbers(a, b):\n    \"\"\"\n    Add two numbers together\n    Returns\n    -------\n    the_sum : type of arguments\n    \"\"\"\n    return a + b\nThen using ? shows us the docstring:\nIn [6]: add_numbers?\nSignature: add_numbers(a, b)\nDocstring:\nAdd two numbers together\nReturns\n-------\nthe_sum : type of arguments\n2.2 IPython Basics \n| \n25",
      "content_length": 1388,
      "extraction_method": "Direct"
    },
    {
      "page_number": 44,
      "chapter": null,
      "content": "File:      <ipython-input-9-6a548a216e27>\nType:      function\n? has a final usage, which is for searching the IPython namespace in a manner similar\nto the standard Unix or Windows command line. A number of characters combined\nwith the wildcard (*) will show all names matching the wildcard expression. For\nexample, we could get a list of all functions in the top-level NumPy namespace\ncontaining load:\nIn [9]: import numpy as np\nIn [10]: np.*load*?\nnp.__loader__\nnp.load\nnp.loads\nnp.loadtxt\n2.3 Python Language Basics\nIn this section, I will give you an overview of essential Python programming concepts\nand language mechanics. In the next chapter, I will go into more detail about Python\ndata structures, functions, and other built-in tools.\nLanguage Semantics\nThe Python language design is distinguished by its emphasis on readability, simplic‐\nity, and explicitness. Some people go so far as to liken it to “executable pseudocode.”\nIndentation, not braces\nPython uses whitespace (tabs or spaces) to structure code instead of using braces as in\nmany other languages like R, C++, Java, and Perl. Consider a for loop from a sorting\nalgorithm:\nfor x in array:\n    if x < pivot:\n        less.append(x)\n    else:\n        greater.append(x)\nA colon denotes the start of an indented code block after which all of the code must\nbe indented by the same amount until the end of the block.\nLove it or hate it, significant whitespace is a fact of life for Python programmers.\nWhile it may seem foreign at first, you will hopefully grow accustomed to it in time.\n26 \n| \nChapter 2: Python Language Basics, IPython, and Jupyter Notebooks",
      "content_length": 1623,
      "extraction_method": "Direct"
    },
    {
      "page_number": 45,
      "chapter": null,
      "content": "I strongly recommend using four spaces as your default indentation\nand replacing tabs with four spaces. Many text editors have a\nsetting that will replace tab stops with spaces automatically (do\nthis!). IPython and Jupyter notebooks will automatically insert four\nspaces on new lines following a colon and replace tabs by four\nspaces.\nAs you can see by now, Python statements also do not need to be terminated by\nsemicolons. Semicolons can be used, however, to separate multiple statements on a\nsingle line:\na = 5; b = 6; c = 7\nPutting multiple statements on one line is generally discouraged in Python as it can\nmake code less readable.\nEverything is an object\nAn important characteristic of the Python language is the consistency of its object\nmodel. Every number, string, data structure, function, class, module, and so on exists\nin the Python interpreter in its own “box,” which is referred to as a Python object.\nEach object has an associated type (e.g., integer, string, or function) and internal data.\nIn practice this makes the language very flexible, as even functions can be treated like\nany other object.\nComments\nAny text preceded by the hash mark (pound sign) # is ignored by the Python\ninterpreter. This is often used to add comments to code. At times you may also want\nto exclude certain blocks of code without deleting them. One solution is to comment\nout the code:\nresults = []\nfor line in file_handle:\n    # keep the empty lines for now\n    # if len(line) == 0:\n    #   continue\n    results.append(line.replace(\"foo\", \"bar\"))\nComments can also occur after a line of executed code. While some programmers\nprefer comments to be placed in the line preceding a particular line of code, this can\nbe useful at times:\nprint(\"Reached this line\")  # Simple status report\n2.3 Python Language Basics \n| \n27",
      "content_length": 1813,
      "extraction_method": "Direct"
    },
    {
      "page_number": 46,
      "chapter": null,
      "content": "Function and object method calls\nYou call functions using parentheses and passing zero or more arguments, optionally\nassigning the returned value to a variable:\nresult = f(x, y, z)\ng()\nAlmost every object in Python has attached functions, known as methods, that have\naccess to the object’s internal contents. You can call them using the following syntax:\nobj.some_method(x, y, z)\nFunctions can take both positional and keyword arguments:\nresult = f(a, b, c, d=5, e=\"foo\")\nWe will look at this in more detail later.\nVariables and argument passing\nWhen assigning a variable (or name) in Python, you are creating a reference to the\nobject shown on the righthand side of the equals sign. In practical terms, consider a\nlist of integers:\nIn [8]: a = [1, 2, 3]\nSuppose we assign a to a new variable b:\nIn [9]: b = a\nIn [10]: b\nOut[10]: [1, 2, 3]\nIn some languages, the assignment if b will cause the data [1, 2, 3] to be copied. In\nPython, a and b actually now refer to the same object, the original list [1, 2, 3] (see\nFigure 2-5 for a mock-up). You can prove this to yourself by appending an element to\na and then examining b:\nIn [11]: a.append(4)\nIn [12]: b\nOut[12]: [1, 2, 3, 4]\nFigure 2-5. Two references for the same object\nUnderstanding the semantics of references in Python, and when, how, and why data\nis copied, is especially critical when you are working with larger datasets in Python.\n28 \n| \nChapter 2: Python Language Basics, IPython, and Jupyter Notebooks",
      "content_length": 1464,
      "extraction_method": "Direct"
    },
    {
      "page_number": 47,
      "chapter": null,
      "content": "Assignment is also referred to as binding, as we are binding a\nname to an object. Variable names that have been assigned may\noccasionally be referred to as bound variables.\nWhen you pass objects as arguments to a function, new local variables are created\nreferencing the original objects without any copying. If you bind a new object to a\nvariable inside a function, that will not overwrite a variable of the same name in the\n“scope” outside of the function (the “parent scope”). It is therefore possible to alter the\ninternals of a mutable argument. Suppose we had the following function:\nIn [13]: def append_element(some_list, element):\n   ....:     some_list.append(element)\nThen we have:\nIn [14]: data = [1, 2, 3]\nIn [15]: append_element(data, 4)\nIn [16]: data\nOut[16]: [1, 2, 3, 4]\nDynamic references, strong types\nVariables in Python have no inherent type associated with them; a variable can refer\nto a different type of object simply by doing an assignment. There is no problem with\nthe following:\nIn [17]: a = 5\nIn [18]: type(a)\nOut[18]: int\nIn [19]: a = \"foo\"\nIn [20]: type(a)\nOut[20]: str\nVariables are names for objects within a particular namespace; the type information is\nstored in the object itself. Some observers might hastily conclude that Python is not a\n“typed language.” This is not true; consider this example:\nIn [21]: \"5\" + 5\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-21-7fe5aa79f268> in <module>\n----> 1 \"5\" + 5\nTypeError: can only concatenate str (not \"int\") to str\n2.3 Python Language Basics \n| \n29",
      "content_length": 1651,
      "extraction_method": "Direct"
    },
    {
      "page_number": 48,
      "chapter": null,
      "content": "In some languages, the string '5' might get implicitly converted (or cast) to an\ninteger, thus yielding 10. In other languages the integer 5 might be cast to a string,\nyielding the concatenated string '55'. In Python, such implicit casts are not allowed.\nIn this regard we say that Python is a strongly typed language, which means that every\nobject has a specific type (or class), and implicit conversions will occur only in certain\npermitted circumstances, such as:\nIn [22]: a = 4.5\nIn [23]: b = 2\n# String formatting, to be visited later\nIn [24]: print(f\"a is {type(a)}, b is {type(b)}\")\na is <class 'float'>, b is <class 'int'>\nIn [25]: a / b\nOut[25]: 2.25\nHere, even though b is an integer, it is implicitly converted to a float for the division\noperation.\nKnowing the type of an object is important, and it’s useful to be able to write\nfunctions that can handle many different kinds of input. You can check that an object\nis an instance of a particular type using the isinstance function:\nIn [26]: a = 5\nIn [27]: isinstance(a, int)\nOut[27]: True\nisinstance can accept a tuple of types if you want to check that an object’s type is\namong those present in the tuple:\nIn [28]: a = 5; b = 4.5\nIn [29]: isinstance(a, (int, float))\nOut[29]: True\nIn [30]: isinstance(b, (int, float))\nOut[30]: True\nAttributes and methods\nObjects in Python typically have both attributes (other Python objects stored\n“inside” the object) and methods (functions associated with an object that can\nhave access to the object’s internal data). Both of them are accessed via the syntax\nobj.attribute_name:\nIn [1]: a = \"foo\"\nIn [2]: a.<Press Tab>\n30 \n| \nChapter 2: Python Language Basics, IPython, and Jupyter Notebooks",
      "content_length": 1693,
      "extraction_method": "Direct"
    },
    {
      "page_number": 49,
      "chapter": null,
      "content": "capitalize() index()        isspace()      removesuffix()  startswith()\ncasefold()   isprintable()  istitle()      replace()       strip()\ncenter()     isalnum()      isupper()      rfind()         swapcase()\ncount()      isalpha()      join()         rindex()        title()\nencode()     isascii()      ljust()        rjust()         translate()\nendswith()   isdecimal()    lower()        rpartition()\nexpandtabs() isdigit()      lstrip()       rsplit()\nfind()       isidentifier() maketrans()    rstrip()\nformat()     islower()      partition()    split()\nformat_map() isnumeric()    removeprefix() splitlines()\nAttributes and methods can also be accessed by name via the getattr function:\nIn [32]: getattr(a, \"split\")\nOut[32]: <function str.split(sep=None, maxsplit=-1)>\nWhile we will not extensively use the functions getattr and related functions\nhasattr and setattr in this book, they can be used very effectively to write generic,\nreusable code.\nDuck typing\nOften you may not care about the type of an object but rather only whether it has\ncertain methods or behavior. This is sometimes called duck typing, after the saying “If\nit walks like a duck and quacks like a duck, then it’s a duck.” For example, you can\nverify that an object is iterable if it implements the iterator protocol. For many objects,\nthis means it has an __iter__ “magic method,” though an alternative and better way\nto check is to try using the iter function:\nIn [33]: def isiterable(obj):\n   ....:     try:\n   ....:         iter(obj)\n   ....:         return True\n   ....:     except TypeError: # not iterable\n   ....:         return False\nThis function would return True for strings as well as most Python collection types:\nIn [34]: isiterable(\"a string\")\nOut[34]: True\nIn [35]: isiterable([1, 2, 3])\nOut[35]: True\nIn [36]: isiterable(5)\nOut[36]: False\n2.3 Python Language Basics \n| \n31",
      "content_length": 1866,
      "extraction_method": "Direct"
    },
    {
      "page_number": 50,
      "chapter": null,
      "content": "Imports\nIn Python, a module is simply a file with the .py extension containing Python code.\nSuppose we had the following module:\n# some_module.py\nPI = 3.14159\ndef f(x):\n    return x + 2\ndef g(a, b):\n    return a + b\nIf we wanted to access the variables and functions defined in some_module.py, from\nanother file in the same directory we could do:\nimport some_module\nresult = some_module.f(5)\npi = some_module.PI\nOr alternately:\nfrom some_module import g, PI\nresult = g(5, PI)\nBy using the as keyword, you can give imports different variable names:\nimport some_module as sm\nfrom some_module import PI as pi, g as gf\nr1 = sm.f(pi)\nr2 = gf(6, pi)\nBinary operators and comparisons\nMost of the binary math operations and comparisons use familiar mathematical\nsyntax used in other programming languages:\nIn [37]: 5 - 7\nOut[37]: -2\nIn [38]: 12 + 21.5\nOut[38]: 33.5\nIn [39]: 5 <= 2\nOut[39]: False\nSee Table 2-1 for all of the available binary operators.\n32 \n| \nChapter 2: Python Language Basics, IPython, and Jupyter Notebooks",
      "content_length": 1018,
      "extraction_method": "Direct"
    },
    {
      "page_number": 51,
      "chapter": null,
      "content": "Table 2-1. Binary operators\nOperation\nDescription\na + b\nAdd a and b\na - b\nSubtract b from a\na * b\nMultiply a by b\na / b\nDivide a by b\na // b\nFloor-divide a by b, dropping any fractional remainder\na ** b\nRaise a to the b power\na & b\nTrue if both a and b are True; for integers, take the bitwise AND\na | b\nTrue if either a or b is True; for integers, take the bitwise OR\na ^ b\nFor Booleans, True if a or b is True, but not both; for integers, take the bitwise EXCLUSIVE-OR\na == b\nTrue if a equals b\na != b\nTrue if a is not equal to b\na < b, a <= b\nTrue if a is less than (less than or equal to) b\na > b, a >= b\nTrue if a is greater than (greater than or equal to) b\na is b\nTrue if a and b reference the same Python object\na is not b\nTrue if a and b reference different Python objects\nTo check if two variables refer to the same object, use the is keyword. Use is not to\ncheck that two objects are not the same:\nIn [40]: a = [1, 2, 3]\nIn [41]: b = a\nIn [42]: c = list(a)\nIn [43]: a is b\nOut[43]: True\nIn [44]: a is not c\nOut[44]: True\nSince the list function always creates a new Python list (i.e., a copy), we can be\nsure that c is distinct from a. Comparing with is is not the same as the == operator,\nbecause in this case we have:\nIn [45]: a == c\nOut[45]: True\n2.3 Python Language Basics \n| \n33",
      "content_length": 1294,
      "extraction_method": "Direct"
    },
    {
      "page_number": 52,
      "chapter": null,
      "content": "A common use of is and is not is to check if a variable is None, since there is only\none instance of None:\nIn [46]: a = None\nIn [47]: a is None\nOut[47]: True\nMutable and immutable objects\nMany objects in Python, such as lists, dictionaries, NumPy arrays, and most user-\ndefined types (classes), are mutable. This means that the object or values that they\ncontain can be modified:\nIn [48]: a_list = [\"foo\", 2, [4, 5]]\nIn [49]: a_list[2] = (3, 4)\nIn [50]: a_list\nOut[50]: ['foo', 2, (3, 4)]\nOthers, like strings and tuples, are immutable, which means their internal data\ncannot be changed:\nIn [51]: a_tuple = (3, 5, (4, 5))\nIn [52]: a_tuple[1] = \"four\"\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-52-cd2a018a7529> in <module>\n----> 1 a_tuple[1] = \"four\"\nTypeError: 'tuple' object does not support item assignment\nRemember that just because you can mutate an object does not mean that you always\nshould. Such actions are known as side effects. For example, when writing a function,\nany side effects should be explicitly communicated to the user in the function’s\ndocumentation or comments. If possible, I recommend trying to avoid side effects\nand favor immutability, even though there may be mutable objects involved.\nScalar Types\nPython has a small set of built-in types for handling numerical data, strings, Boolean\n(True or False) values, and dates and time. These “single value” types are sometimes\ncalled scalar types, and we refer to them in this book as scalars . See Table 2-2 for a list\nof the main scalar types. Date and time handling will be discussed separately, as these\nare provided by the datetime module in the standard library.\n34 \n| \nChapter 2: Python Language Basics, IPython, and Jupyter Notebooks",
      "content_length": 1838,
      "extraction_method": "Direct"
    },
    {
      "page_number": 53,
      "chapter": null,
      "content": "Table 2-2. Standard Python scalar types\nType\nDescription\nNone\nThe Python “null” value (only one instance of the None object exists)\nstr\nString type; holds Unicode strings\nbytes\nRaw binary data\nfloat\nDouble-precision floating-point number (note there is no separate double type)\nbool\nA Boolean True or False value\nint\nArbitrary precision integer\nNumeric types\nThe primary Python types for numbers are int and float. An int can store arbitrar‐\nily large numbers:\nIn [53]: ival = 17239871\nIn [54]: ival ** 6\nOut[54]: 26254519291092456596965462913230729701102721\nFloating-point numbers are represented with the Python float type. Under the hood,\neach one is a double-precision value. They can also be expressed with scientific\nnotation:\nIn [55]: fval = 7.243\nIn [56]: fval2 = 6.78e-5\nInteger division not resulting in a whole number will always yield a floating-point\nnumber:\nIn [57]: 3 / 2\nOut[57]: 1.5\nTo get C-style integer division (which drops the fractional part if the result is not a\nwhole number), use the floor division operator //:\nIn [58]: 3 // 2\nOut[58]: 1\nStrings\nMany people use Python for its built-in string handling capabilities. You can write\nstring literals using either single quotes ' or double quotes \" (double quotes are\ngenerally favored):\na = 'one way of writing a string'\nb = \"another way\"\nThe Python string type is str.\n2.3 Python Language Basics \n| \n35",
      "content_length": 1377,
      "extraction_method": "Direct"
    },
    {
      "page_number": 54,
      "chapter": null,
      "content": "For multiline strings with line breaks, you can use triple quotes, either ''' or \"\"\":\nc = \"\"\"\nThis is a longer string that\nspans multiple lines\n\"\"\"\nIt may surprise you that this string c actually contains four lines of text; the line\nbreaks after \"\"\" and after lines are included in the string. We can count the new line\ncharacters with the count method on c:\nIn [60]: c.count(\"\\n\")\nOut[60]: 3\nPython strings are immutable; you cannot modify a string:\nIn [61]: a = \"this is a string\"\nIn [62]: a[10] = \"f\"\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-62-3b2d95f10db4> in <module>\n----> 1 a[10] = \"f\"\nTypeError: 'str' object does not support item assignment\nTo interpret this error message, read from the bottom up. We tried to replace the\ncharacter (the “item”) at position 10 with the letter \"f\", but this is not allowed for\nstring objects. If we need to modify a string, we have to use a function or method that\ncreates a new string, such as the string replace method:\nIn [63]: b = a.replace(\"string\", \"longer string\")\nIn [64]: b\nOut[64]: 'this is a longer string'\nAfer this operation, the variable a is unmodified:\nIn [65]: a\nOut[65]: 'this is a string'\nMany Python objects can be converted to a string using the str function:\nIn [66]: a = 5.6\nIn [67]: s = str(a)\nIn [68]: print(s)\n5.6\nStrings are a sequence of Unicode characters and therefore can be treated like other\nsequences, such as lists and tuples:\nIn [69]: s = \"python\"\n36 \n| \nChapter 2: Python Language Basics, IPython, and Jupyter Notebooks",
      "content_length": 1626,
      "extraction_method": "Direct"
    },
    {
      "page_number": 55,
      "chapter": null,
      "content": "In [70]: list(s)\nOut[70]: ['p', 'y', 't', 'h', 'o', 'n']\nIn [71]: s[:3]\nOut[71]: 'pyt'\nThe syntax s[:3] is called slicing and is implemented for many kinds of Python\nsequences. This will be explained in more detail later on, as it is used extensively in\nthis book.\nThe backslash character \\ is an escape character, meaning that it is used to specify\nspecial characters like newline \\n or Unicode characters. To write a string literal with\nbackslashes, you need to escape them:\nIn [72]: s = \"12\\\\34\"\nIn [73]: print(s)\n12\\34\nIf you have a string with a lot of backslashes and no special characters, you might find\nthis a bit annoying. Fortunately you can preface the leading quote of the string with r,\nwhich means that the characters should be interpreted as is:\nIn [74]: s = r\"this\\has\\no\\special\\characters\"\nIn [75]: s\nOut[75]: 'this\\\\has\\\\no\\\\special\\\\characters'\nThe r stands for raw.\nAdding two strings together concatenates them and produces a new string:\nIn [76]: a = \"this is the first half \"\nIn [77]: b = \"and this is the second half\"\nIn [78]: a + b\nOut[78]: 'this is the first half and this is the second half'\nString templating or formatting is another important topic. The number of ways to\ndo so has expanded with the advent of Python 3, and here I will briefly describe\nthe mechanics of one of the main interfaces. String objects have a format method\nthat can be used to substitute formatted arguments into the string, producing a new\nstring:\nIn [79]: template = \"{0:.2f} {1:s} are worth US${2:d}\"\nIn this string:\n• {0:.2f} means to format the first argument as a floating-point number with two\n•\ndecimal places.\n2.3 Python Language Basics \n| \n37",
      "content_length": 1659,
      "extraction_method": "Direct"
    },
    {
      "page_number": 56,
      "chapter": null,
      "content": "• {1:s} means to format the second argument as a string.\n•\n• {2:d} means to format the third argument as an exact integer.\n•\nTo substitute arguments for these format parameters, we pass a sequence of argu‐\nments to the format method:\nIn [80]: template.format(88.46, \"Argentine Pesos\", 1)\nOut[80]: '88.46 Argentine Pesos are worth US$1'\nPython 3.6 introduced a new feature called f-strings (short for formatted string literals)\nwhich can make creating formatted strings even more convenient. To create an f-\nstring, write the character f immediately preceding a string literal. Within the string,\nenclose Python expressions in curly braces to substitute the value of the expression\ninto the formatted string:\nIn [81]: amount = 10\nIn [82]: rate = 88.46\nIn [83]: currency = \"Pesos\"\nIn [84]: result = f\"{amount} {currency} is worth US${amount / rate}\"\nFormat specifiers can be added after each expression using the same syntax as with\nthe string templates above:\nIn [85]: f\"{amount} {currency} is worth US${amount / rate:.2f}\"\nOut[85]: '10 Pesos is worth US$0.11'\nString formatting is a deep topic; there are multiple methods and numerous options\nand tweaks available to control how values are formatted in the resulting string. To\nlearn more, consult the official Python documentation.\nBytes and Unicode\nIn modern Python (i.e., Python 3.0 and up), Unicode has become the first-class\nstring type to enable more consistent handling of ASCII and non-ASCII text. In older\nversions of Python, strings were all bytes without any explicit Unicode encoding. You\ncould convert to Unicode assuming you knew the character encoding. Here is an\nexample Unicode string with non-ASCII characters:\nIn [86]: val = \"español\"\nIn [87]: val\nOut[87]: 'español'\nWe can convert this Unicode string to its UTF-8 bytes representation using the\nencode method:\nIn [88]: val_utf8 = val.encode(\"utf-8\")\n38 \n| \nChapter 2: Python Language Basics, IPython, and Jupyter Notebooks",
      "content_length": 1942,
      "extraction_method": "Direct"
    },
    {
      "page_number": 57,
      "chapter": null,
      "content": "In [89]: val_utf8\nOut[89]: b'espa\\xc3\\xb1ol'\nIn [90]: type(val_utf8)\nOut[90]: bytes\nAssuming you know the Unicode encoding of a bytes object, you can go back using\nthe decode method:\nIn [91]: val_utf8.decode(\"utf-8\")\nOut[91]: 'español'\nWhile it is now preferable to use UTF-8 for any encoding, for historical reasons you\nmay encounter data in any number of different encodings:\nIn [92]: val.encode(\"latin1\")\nOut[92]: b'espa\\xf1ol'\nIn [93]: val.encode(\"utf-16\")\nOut[93]: b'\\xff\\xfee\\x00s\\x00p\\x00a\\x00\\xf1\\x00o\\x00l\\x00'\nIn [94]: val.encode(\"utf-16le\")\nOut[94]: b'e\\x00s\\x00p\\x00a\\x00\\xf1\\x00o\\x00l\\x00'\nIt is most common to encounter bytes objects in the context of working with files,\nwhere implicitly decoding all data to Unicode strings may not be desired.\nBooleans\nThe two Boolean values in Python are written as True and False. Comparisons and\nother conditional expressions evaluate to either True or False. Boolean values are\ncombined with the and and or keywords:\nIn [95]: True and True\nOut[95]: True\nIn [96]: False or True\nOut[96]: True\nWhen converted to numbers, False becomes 0 and True becomes 1:\nIn [97]: int(False)\nOut[97]: 0\nIn [98]: int(True)\nOut[98]: 1\nThe keyword not flips a Boolean value from True to False or vice versa:\nIn [99]: a = True\nIn [100]: b = False\n2.3 Python Language Basics \n| \n39",
      "content_length": 1312,
      "extraction_method": "Direct"
    },
    {
      "page_number": 58,
      "chapter": null,
      "content": "In [101]: not a\nOut[101]: False\nIn [102]: not b\nOut[102]: True\nType casting\nThe str, bool, int, and float types are also functions that can be used to cast values\nto those types:\nIn [103]: s = \"3.14159\"\nIn [104]: fval = float(s)\nIn [105]: type(fval)\nOut[105]: float\nIn [106]: int(fval)\nOut[106]: 3\nIn [107]: bool(fval)\nOut[107]: True\nIn [108]: bool(0)\nOut[108]: False\nNote that most nonzero values when cast to bool become True.\nNone\nNone is the Python null value type:\nIn [109]: a = None\nIn [110]: a is None\nOut[110]: True\nIn [111]: b = 5\nIn [112]: b is not None\nOut[112]: True\nNone is also a common default value for function arguments:\ndef add_and_maybe_multiply(a, b, c=None):\n    result = a + b\n    if c is not None:\n        result = result * c\n    return result\n40 \n| \nChapter 2: Python Language Basics, IPython, and Jupyter Notebooks",
      "content_length": 840,
      "extraction_method": "Direct"
    },
    {
      "page_number": 59,
      "chapter": null,
      "content": "Dates and times\nThe built-in Python datetime module provides datetime, date, and time types. The\ndatetime type combines the information stored in date and time and is the most\ncommonly used:\nIn [113]: from datetime import datetime, date, time\nIn [114]: dt = datetime(2011, 10, 29, 20, 30, 21)\nIn [115]: dt.day\nOut[115]: 29\nIn [116]: dt.minute\nOut[116]: 30\nGiven a datetime instance, you can extract the equivalent date and time objects by\ncalling methods on the datetime of the same name:\nIn [117]: dt.date()\nOut[117]: datetime.date(2011, 10, 29)\nIn [118]: dt.time()\nOut[118]: datetime.time(20, 30, 21)\nThe strftime method formats a datetime as a string:\nIn [119]: dt.strftime(\"%Y-%m-%d %H:%M\")\nOut[119]: '2011-10-29 20:30'\nStrings can be converted (parsed) into datetime objects with the strptime function:\nIn [120]: datetime.strptime(\"20091031\", \"%Y%m%d\")\nOut[120]: datetime.datetime(2009, 10, 31, 0, 0)\nSee Table 11-2 for a full list of format specifications.\nWhen you are aggregating or otherwise grouping time series data, it will occasionally\nbe useful to replace time fields of a series of datetimes—for example, replacing the\nminute and second fields with zero:\nIn [121]: dt_hour = dt.replace(minute=0, second=0)\nIn [122]: dt_hour\nOut[122]: datetime.datetime(2011, 10, 29, 20, 0)\nSince datetime.datetime is an immutable type, methods like these always produce\nnew objects. So in the previous example, dt is not modified by replace:\nIn [123]: dt\nOut[123]: datetime.datetime(2011, 10, 29, 20, 30, 21)\nThe difference of two datetime objects produces a datetime.timedelta type:\n2.3 Python Language Basics \n| \n41",
      "content_length": 1615,
      "extraction_method": "Direct"
    },
    {
      "page_number": 60,
      "chapter": null,
      "content": "In [124]: dt2 = datetime(2011, 11, 15, 22, 30)\nIn [125]: delta = dt2 - dt\nIn [126]: delta\nOut[126]: datetime.timedelta(days=17, seconds=7179)\nIn [127]: type(delta)\nOut[127]: datetime.timedelta\nThe output timedelta(17, 7179) indicates that the timedelta encodes an offset of\n17 days and 7,179 seconds.\nAdding a timedelta to a datetime produces a new shifted datetime:\nIn [128]: dt\nOut[128]: datetime.datetime(2011, 10, 29, 20, 30, 21)\nIn [129]: dt + delta\nOut[129]: datetime.datetime(2011, 11, 15, 22, 30)\nControl Flow\nPython has several built-in keywords for conditional logic, loops, and other standard\ncontrol flow concepts found in other programming languages.\nif, elif, and else\nThe if statement is one of the most well-known control flow statement types. It\nchecks a condition that, if True, evaluates the code in the block that follows:\nx = -5\nif x < 0:\n    print(\"It's negative\")\nAn if statement can be optionally followed by one or more elif blocks and a catchall\nelse block if all of the conditions are False:\nif x < 0:\n    print(\"It's negative\")\nelif x == 0:\n    print(\"Equal to zero\")\nelif 0 < x < 5:\n    print(\"Positive but smaller than 5\")\nelse:\n    print(\"Positive and larger than or equal to 5\")\nIf any of the conditions are True, no further elif or else blocks will be reached.\nWith a compound condition using and or or, conditions are evaluated left to right\nand will short-circuit:\n42 \n| \nChapter 2: Python Language Basics, IPython, and Jupyter Notebooks",
      "content_length": 1472,
      "extraction_method": "Direct"
    },
    {
      "page_number": 61,
      "chapter": null,
      "content": "In [130]: a = 5; b = 7\nIn [131]: c = 8; d = 4\nIn [132]: if a < b or c > d:\n   .....:     print(\"Made it\")\nMade it\nIn this example, the comparison c > d never gets evaluated because the first compar‐\nison was True.\nIt is also possible to chain comparisons:\nIn [133]: 4 > 3 > 2 > 1\nOut[133]: True\nfor loops\nfor loops are for iterating over a collection (like a list or tuple) or an iterater. The\nstandard syntax for a for loop is:\nfor value in collection:\n    # do something with value\nYou can advance a for loop to the next iteration, skipping the remainder of the block,\nusing the continue keyword. Consider this code, which sums up integers in a list and\nskips None values:\nsequence = [1, 2, None, 4, None, 5]\ntotal = 0\nfor value in sequence:\n    if value is None:\n        continue\n    total += value\nA for loop can be exited altogether with the break keyword. This code sums ele‐\nments of the list until a 5 is reached:\nsequence = [1, 2, 0, 4, 6, 5, 2, 1]\ntotal_until_5 = 0\nfor value in sequence:\n    if value == 5:\n        break\n    total_until_5 += value\nThe break keyword only terminates the innermost for loop; any outer for loops will\ncontinue to run:\nIn [134]: for i in range(4):\n   .....:     for j in range(4):\n   .....:         if j > i:\n   .....:             break\n   .....:         print((i, j))\n2.3 Python Language Basics \n| \n43",
      "content_length": 1342,
      "extraction_method": "Direct"
    },
    {
      "page_number": 62,
      "chapter": null,
      "content": ".....:\n(0, 0)\n(1, 0)\n(1, 1)\n(2, 0)\n(2, 1)\n(2, 2)\n(3, 0)\n(3, 1)\n(3, 2)\n(3, 3)\nAs we will see in more detail, if the elements in the collection or iterator are sequen‐\nces (tuples or lists, say), they can be conveniently unpacked into variables in the for\nloop statement:\nfor a, b, c in iterator:\n    # do something\nwhile loops\nA while loop specifies a condition and a block of code that is to be executed until the\ncondition evaluates to False or the loop is explicitly ended with break:\nx = 256\ntotal = 0\nwhile x > 0:\n    if total > 500:\n        break\n    total += x\n    x = x // 2\npass\npass is the “no-op” (or “do nothing”) statement in Python. It can be used in blocks\nwhere no action is to be taken (or as a placeholder for code not yet implemented); it\nis required only because Python uses whitespace to delimit blocks:\nif x < 0:\n    print(\"negative!\")\nelif x == 0:\n    # TODO: put something smart here\n    pass\nelse:\n    print(\"positive!\")\nrange\nThe range function generates a sequence of evenly spaced integers:\nIn [135]: range(10)\nOut[135]: range(0, 10)\n44 \n| \nChapter 2: Python Language Basics, IPython, and Jupyter Notebooks",
      "content_length": 1133,
      "extraction_method": "Direct"
    },
    {
      "page_number": 63,
      "chapter": null,
      "content": "In [136]: list(range(10))\nOut[136]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nA start, end, and step (which may be negative) can be given:\nIn [137]: list(range(0, 20, 2))\nOut[137]: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\nIn [138]: list(range(5, 0, -1))\nOut[138]: [5, 4, 3, 2, 1]\nAs you can see, range produces integers up to but not including the endpoint. A\ncommon use of range is for iterating through sequences by index:\nIn [139]: seq = [1, 2, 3, 4]\nIn [140]: for i in range(len(seq)):\n   .....:     print(f\"element {i}: {seq[i]}\")\nelement 0: 1\nelement 1: 2\nelement 2: 3\nelement 3: 4\nWhile you can use functions like list to store all the integers generated by range in\nsome other data structure, often the default iterator form will be what you want. This\nsnippet sums all numbers from 0 to 99,999 that are multiples of 3 or 5:\nIn [141]: total = 0\nIn [142]: for i in range(100_000):\n   .....:     # % is the modulo operator\n   .....:     if i % 3 == 0 or i % 5 == 0:\n   .....:         total += i\nIn [143]: print(total)\n2333316668\nWhile the range generated can be arbitrarily large, the memory use at any given time\nmay be very small.\n2.4 Conclusion\nThis chapter provided a brief introduction to some basic Python language concepts\nand the IPython and Jupyter programming environments. In the next chapter, I will\ndiscuss many built-in data types, functions, and input-output utilities that will be\nused continuously throughout the rest of the book.\n2.4 Conclusion \n| \n45",
      "content_length": 1459,
      "extraction_method": "Direct"
    },
    {
      "page_number": 64,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 65,
      "chapter": null,
      "content": "CHAPTER 3\nBuilt-In Data Structures,\nFunctions, and Files\nThis chapter discusses capabilities built into the Python language that will be used\nubiquitously throughout the book. While add-on libraries like pandas and NumPy\nadd advanced computational functionality for larger datasets, they are designed to be\nused together with Python’s built-in data manipulation tools.\nWe’ll start with Python’s workhorse data structures: tuples, lists, dictionaries, and sets.\nThen, we’ll discuss creating your own reusable Python functions. Finally, we’ll look at\nthe mechanics of Python file objects and interacting with your local hard drive.\n3.1 Data Structures and Sequences\nPython’s data structures are simple but powerful. Mastering their use is a critical\npart of becoming a proficient Python programmer. We start with tuple, list, and\ndictionary, which are some of the most frequently used sequence types.\nTuple\nA tuple is a fixed-length, immutable sequence of Python objects which, once assigned,\ncannot be changed. The easiest way to create one is with a comma-separated\nsequence of values wrapped in parentheses:\nIn [2]: tup = (4, 5, 6)\nIn [3]: tup\nOut[3]: (4, 5, 6)\n47",
      "content_length": 1165,
      "extraction_method": "Direct"
    },
    {
      "page_number": 66,
      "chapter": null,
      "content": "In many contexts, the parentheses can be omitted, so here we could also have written:\nIn [4]: tup = 4, 5, 6\nIn [5]: tup\nOut[5]: (4, 5, 6)\nYou can convert any sequence or iterator to a tuple by invoking tuple:\nIn [6]: tuple([4, 0, 2])\nOut[6]: (4, 0, 2)\nIn [7]: tup = tuple('string')\nIn [8]: tup\nOut[8]: ('s', 't', 'r', 'i', 'n', 'g')\nElements can be accessed with square brackets [] as with most other sequence types.\nAs in C, C++, Java, and many other languages, sequences are 0-indexed in Python:\nIn [9]: tup[0]\nOut[9]: 's'\nWhen you’re defining tuples within more complicated expressions, it’s often neces‐\nsary to enclose the values in parentheses, as in this example of creating a tuple of\ntuples:\nIn [10]: nested_tup = (4, 5, 6), (7, 8)\nIn [11]: nested_tup\nOut[11]: ((4, 5, 6), (7, 8))\nIn [12]: nested_tup[0]\nOut[12]: (4, 5, 6)\nIn [13]: nested_tup[1]\nOut[13]: (7, 8)\nWhile the objects stored in a tuple may be mutable themselves, once the tuple is\ncreated it’s not possible to modify which object is stored in each slot:\nIn [14]: tup = tuple(['foo', [1, 2], True])\nIn [15]: tup[2] = False\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-15-b89d0c4ae599> in <module>\n----> 1 tup[2] = False\nTypeError: 'tuple' object does not support item assignment\nIf an object inside a tuple is mutable, such as a list, you can modify it in place:\nIn [16]: tup[1].append(3)\n48 \n| \nChapter 3: Built-In Data Structures, Functions, and Files",
      "content_length": 1545,
      "extraction_method": "Direct"
    },
    {
      "page_number": 67,
      "chapter": null,
      "content": "In [17]: tup\nOut[17]: ('foo', [1, 2, 3], True)\nYou can concatenate tuples using the + operator to produce longer tuples:\nIn [18]: (4, None, 'foo') + (6, 0) + ('bar',)\nOut[18]: (4, None, 'foo', 6, 0, 'bar')\nMultiplying a tuple by an integer, as with lists, has the effect of concatenating that\nmany copies of the tuple:\nIn [19]: ('foo', 'bar') * 4\nOut[19]: ('foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'bar')\nNote that the objects themselves are not copied, only the references to them.\nUnpacking tuples\nIf you try to assign to a tuple-like expression of variables, Python will attempt to\nunpack the value on the righthand side of the equals sign:\nIn [20]: tup = (4, 5, 6)\nIn [21]: a, b, c = tup\nIn [22]: b\nOut[22]: 5\nEven sequences with nested tuples can be unpacked:\nIn [23]: tup = 4, 5, (6, 7)\nIn [24]: a, b, (c, d) = tup\nIn [25]: d\nOut[25]: 7\nUsing this functionality you can easily swap variable names, a task that in many\nlanguages might look like:\ntmp = a\na = b\nb = tmp\nBut, in Python, the swap can be done like this:\nIn [26]: a, b = 1, 2\nIn [27]: a\nOut[27]: 1\nIn [28]: b\nOut[28]: 2\nIn [29]: b, a = a, b\n3.1 Data Structures and Sequences \n| \n49",
      "content_length": 1155,
      "extraction_method": "Direct"
    },
    {
      "page_number": 68,
      "chapter": null,
      "content": "In [30]: a\nOut[30]: 2\nIn [31]: b\nOut[31]: 1\nA common use of variable unpacking is iterating over sequences of tuples or lists:\nIn [32]: seq = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\nIn [33]: for a, b, c in seq:\n   ....:     print(f'a={a}, b={b}, c={c}')\na=1, b=2, c=3\na=4, b=5, c=6\na=7, b=8, c=9\nAnother common use is returning multiple values from a function. I’ll cover this in\nmore detail later.\nThere are some situations where you may want to “pluck” a few elements from the\nbeginning of a tuple. There is a special syntax that can do this, *rest, which is also\nused in function signatures to capture an arbitrarily long list of positional arguments:\nIn [34]: values = 1, 2, 3, 4, 5\nIn [35]: a, b, *rest = values\nIn [36]: a\nOut[36]: 1\nIn [37]: b\nOut[37]: 2\nIn [38]: rest\nOut[38]: [3, 4, 5]\nThis rest bit is sometimes something you want to discard; there is nothing special\nabout the rest name. As a matter of convention, many Python programmers will use\nthe underscore (_) for unwanted variables:\nIn [39]: a, b, *_ = values\nTuple methods\nSince the size and contents of a tuple cannot be modified, it is very light on instance\nmethods. A particularly useful one (also available on lists) is count, which counts the\nnumber of occurrences of a value:\nIn [40]: a = (1, 2, 2, 2, 3, 4, 2)\nIn [41]: a.count(2)\nOut[41]: 4\n50 \n| \nChapter 3: Built-In Data Structures, Functions, and Files",
      "content_length": 1376,
      "extraction_method": "Direct"
    },
    {
      "page_number": 69,
      "chapter": null,
      "content": "List\nIn contrast with tuples, lists are variable length and their contents can be modified in\nplace. Lists are mutable. You can define them using square brackets [] or using the\nlist type function:\nIn [42]: a_list = [2, 3, 7, None]\nIn [43]: tup = (\"foo\", \"bar\", \"baz\")\nIn [44]: b_list = list(tup)\nIn [45]: b_list\nOut[45]: ['foo', 'bar', 'baz']\nIn [46]: b_list[1] = \"peekaboo\"\nIn [47]: b_list\nOut[47]: ['foo', 'peekaboo', 'baz']\nLists and tuples are semantically similar (though tuples cannot be modified) and can\nbe used interchangeably in many functions.\nThe list built-in function is frequently used in data processing as a way to material‐\nize an iterator or generator expression:\nIn [48]: gen = range(10)\nIn [49]: gen\nOut[49]: range(0, 10)\nIn [50]: list(gen)\nOut[50]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nAdding and removing elements\nElements can be appended to the end of the list with the append method:\nIn [51]: b_list.append(\"dwarf\")\nIn [52]: b_list\nOut[52]: ['foo', 'peekaboo', 'baz', 'dwarf']\nUsing insert you can insert an element at a specific location in the list:\nIn [53]: b_list.insert(1, \"red\")\nIn [54]: b_list\nOut[54]: ['foo', 'red', 'peekaboo', 'baz', 'dwarf']\nThe insertion index must be between 0 and the length of the list, inclusive.\n3.1 Data Structures and Sequences \n| \n51",
      "content_length": 1290,
      "extraction_method": "Direct"
    },
    {
      "page_number": 70,
      "chapter": null,
      "content": "insert is computationally expensive compared with append,\nbecause references to subsequent elements have to be shifted inter‐\nnally to make room for the new element. If you need to insert\nelements at both the beginning and end of a sequence, you may\nwish to explore collections.deque, a double-ended queue, which\nis optimized for this purpose and found in the Python Standard\nLibrary.\nThe inverse operation to insert is pop, which removes and returns an element at a\nparticular index:\nIn [55]: b_list.pop(2)\nOut[55]: 'peekaboo'\nIn [56]: b_list\nOut[56]: ['foo', 'red', 'baz', 'dwarf']\nElements can be removed by value with remove, which locates the first such value and\nremoves it from the list:\nIn [57]: b_list.append(\"foo\")\nIn [58]: b_list\nOut[58]: ['foo', 'red', 'baz', 'dwarf', 'foo']\nIn [59]: b_list.remove(\"foo\")\nIn [60]: b_list\nOut[60]: ['red', 'baz', 'dwarf', 'foo']\nIf performance is not a concern, by using append and remove, you can use a Python\nlist as a set-like data structure (although Python has actual set objects, discussed\nlater).\nCheck if a list contains a value using the in keyword:\nIn [61]: \"dwarf\" in b_list\nOut[61]: True\nThe keyword not can be used to negate in:\nIn [62]: \"dwarf\" not in b_list\nOut[62]: False\nChecking whether a list contains a value is a lot slower than doing so with diction‐\naries and sets (to be introduced shortly), as Python makes a linear scan across the\nvalues of the list, whereas it can check the others (based on hash tables) in constant\ntime.\n52 \n| \nChapter 3: Built-In Data Structures, Functions, and Files",
      "content_length": 1559,
      "extraction_method": "Direct"
    },
    {
      "page_number": 71,
      "chapter": null,
      "content": "Concatenating and combining lists\nSimilar to tuples, adding two lists together with + concatenates them:\nIn [63]: [4, None, \"foo\"] + [7, 8, (2, 3)]\nOut[63]: [4, None, 'foo', 7, 8, (2, 3)]\nIf you have a list already defined, you can append multiple elements to it using the\nextend method:\nIn [64]: x = [4, None, \"foo\"]\nIn [65]: x.extend([7, 8, (2, 3)])\nIn [66]: x\nOut[66]: [4, None, 'foo', 7, 8, (2, 3)]\nNote that list concatenation by addition is a comparatively expensive operation since\na new list must be created and the objects copied over. Using extend to append\nelements to an existing list, especially if you are building up a large list, is usually\npreferable. Thus:\neverything = []\nfor chunk in list_of_lists:\n    everything.extend(chunk)\nis faster than the concatenative alternative:\neverything = []\nfor chunk in list_of_lists:\n    everything = everything + chunk\nSorting\nYou can sort a list in place (without creating a new object) by calling its sort\nfunction:\nIn [67]: a = [7, 2, 5, 1, 3]\nIn [68]: a.sort()\nIn [69]: a\nOut[69]: [1, 2, 3, 5, 7]\nsort has a few options that will occasionally come in handy. One is the ability to\npass a secondary sort key—that is, a function that produces a value to use to sort the\nobjects. For example, we could sort a collection of strings by their lengths:\nIn [70]: b = [\"saw\", \"small\", \"He\", \"foxes\", \"six\"]\nIn [71]: b.sort(key=len)\nIn [72]: b\nOut[72]: ['He', 'saw', 'six', 'small', 'foxes']\n3.1 Data Structures and Sequences \n| \n53",
      "content_length": 1480,
      "extraction_method": "Direct"
    },
    {
      "page_number": 72,
      "chapter": null,
      "content": "Soon, we’ll look at the sorted function, which can produce a sorted copy of a general\nsequence.\nSlicing\nYou can select sections of most sequence types by using slice notation, which in its\nbasic form consists of start:stop passed to the indexing operator []:\nIn [73]: seq = [7, 2, 3, 7, 5, 6, 0, 1]\nIn [74]: seq[1:5]\nOut[74]: [2, 3, 7, 5]\nSlices can also be assigned with a sequence:\nIn [75]: seq[3:5] = [6, 3]\nIn [76]: seq\nOut[76]: [7, 2, 3, 6, 3, 6, 0, 1]\nWhile the element at the start index is included, the stop index is not included, so\nthat the number of elements in the result is stop - start.\nEither the start or stop can be omitted, in which case they default to the start of the\nsequence and the end of the sequence, respectively:\nIn [77]: seq[:5]\nOut[77]: [7, 2, 3, 6, 3]\nIn [78]: seq[3:]\nOut[78]: [6, 3, 6, 0, 1]\nNegative indices slice the sequence relative to the end:\nIn [79]: seq[-4:]\nOut[79]: [3, 6, 0, 1]\nIn [80]: seq[-6:-2]\nOut[80]: [3, 6, 3, 6]\nSlicing semantics takes a bit of getting used to, especially if you’re coming from R\nor MATLAB. See Figure 3-1 for a helpful illustration of slicing with positive and\nnegative integers. In the figure, the indices are shown at the “bin edges” to help show\nwhere the slice selections start and stop using positive or negative indices.\n54 \n| \nChapter 3: Built-In Data Structures, Functions, and Files",
      "content_length": 1362,
      "extraction_method": "Direct"
    },
    {
      "page_number": 73,
      "chapter": null,
      "content": "Figure 3-1. Illustration of Python slicing conventions\nA step can also be used after a second colon to, say, take every other element:\nIn [81]: seq[::2]\nOut[81]: [7, 3, 3, 0]\nA clever use of this is to pass -1, which has the useful effect of reversing a list or tuple:\nIn [82]: seq[::-1]\nOut[82]: [1, 0, 6, 3, 6, 3, 2, 7]\nDictionary\nThe dictionary or dict may be the most important built-in Python data structure.\nIn other programming languages, dictionaries are sometimes called hash maps or\nassociative arrays. A dictionary stores a collection of key-value pairs, where key and\nvalue are Python objects. Each key is associated with a value so that a value can\nbe conveniently retrieved, inserted, modified, or deleted given a particular key. One\napproach for creating a dictionary is to use curly braces {} and colons to separate\nkeys and values:\nIn [83]: empty_dict = {}\nIn [84]: d1 = {\"a\": \"some value\", \"b\": [1, 2, 3, 4]}\nIn [85]: d1\nOut[85]: {'a': 'some value', 'b': [1, 2, 3, 4]}\nYou can access, insert, or set elements using the same syntax as for accessing elements\nof a list or tuple:\nIn [86]: d1[7] = \"an integer\"\nIn [87]: d1\nOut[87]: {'a': 'some value', 'b': [1, 2, 3, 4], 7: 'an integer'}\nIn [88]: d1[\"b\"]\nOut[88]: [1, 2, 3, 4]\n3.1 Data Structures and Sequences \n| \n55",
      "content_length": 1281,
      "extraction_method": "Direct"
    },
    {
      "page_number": 74,
      "chapter": null,
      "content": "You can check if a dictionary contains a key using the same syntax used for checking\nwhether a list or tuple contains a value:\nIn [89]: \"b\" in d1\nOut[89]: True\nYou can delete values using either the del keyword or the pop method (which\nsimultaneously returns the value and deletes the key):\nIn [90]: d1[5] = \"some value\"\nIn [91]: d1\nOut[91]: \n{'a': 'some value',\n 'b': [1, 2, 3, 4],\n 7: 'an integer',\n 5: 'some value'}\nIn [92]: d1[\"dummy\"] = \"another value\"\nIn [93]: d1\nOut[93]: \n{'a': 'some value',\n 'b': [1, 2, 3, 4],\n 7: 'an integer',\n 5: 'some value',\n 'dummy': 'another value'}\nIn [94]: del d1[5]\nIn [95]: d1\nOut[95]: \n{'a': 'some value',\n 'b': [1, 2, 3, 4],\n 7: 'an integer',\n 'dummy': 'another value'}\nIn [96]: ret = d1.pop(\"dummy\")\nIn [97]: ret\nOut[97]: 'another value'\nIn [98]: d1\nOut[98]: {'a': 'some value', 'b': [1, 2, 3, 4], 7: 'an integer'}\nThe keys and values method gives you iterators of the dictionary’s keys and values,\nrespectively. The order of the keys depends on the order of their insertion, and these\nfunctions output the keys and values in the same respective order:\nIn [99]: list(d1.keys())\nOut[99]: ['a', 'b', 7]\n56 \n| \nChapter 3: Built-In Data Structures, Functions, and Files",
      "content_length": 1205,
      "extraction_method": "Direct"
    },
    {
      "page_number": 75,
      "chapter": null,
      "content": "In [100]: list(d1.values())\nOut[100]: ['some value', [1, 2, 3, 4], 'an integer']\nIf you need to iterate over both the keys and values, you can use the items method to\niterate over the keys and values as 2-tuples:\nIn [101]: list(d1.items())\nOut[101]: [('a', 'some value'), ('b', [1, 2, 3, 4]), (7, 'an integer')]\nYou can merge one dictionary into another using the update method:\nIn [102]: d1.update({\"b\": \"foo\", \"c\": 12})\nIn [103]: d1\nOut[103]: {'a': 'some value', 'b': 'foo', 7: 'an integer', 'c': 12}\nThe update method changes dictionaries in place, so any existing keys in the data\npassed to update will have their old values discarded.\nCreating dictionaries from sequences\nIt’s common to occasionally end up with two sequences that you want to pair up\nelement-wise in a dictionary. As a first cut, you might write code like this:\nmapping = {}\nfor key, value in zip(key_list, value_list):\n    mapping[key] = value\nSince a dictionary is essentially a collection of 2-tuples, the dict function accepts a\nlist of 2-tuples:\nIn [104]: tuples = zip(range(5), reversed(range(5)))\nIn [105]: tuples\nOut[105]: <zip at 0x7fefe4553a00>\nIn [106]: mapping = dict(tuples)\nIn [107]: mapping\nOut[107]: {0: 4, 1: 3, 2: 2, 3: 1, 4: 0}\nLater we’ll talk about dictionary comprehensions, which are another way to construct\ndictionaries.\nDefault values\nIt’s common to have logic like:\nif key in some_dict:\n    value = some_dict[key]\nelse:\n    value = default_value\n3.1 Data Structures and Sequences \n| \n57",
      "content_length": 1485,
      "extraction_method": "Direct"
    },
    {
      "page_number": 76,
      "chapter": null,
      "content": "Thus, the dictionary methods get and pop can take a default value to be returned, so\nthat the above if-else block can be written simply as:\nvalue = some_dict.get(key, default_value)\nget by default will return None if the key is not present, while pop will raise an\nexception. With setting values, it may be that the values in a dictionary are another\nkind of collection, like a list. For example, you could imagine categorizing a list of\nwords by their first letters as a dictionary of lists:\nIn [108]: words = [\"apple\", \"bat\", \"bar\", \"atom\", \"book\"]\nIn [109]: by_letter = {}\nIn [110]: for word in words:\n   .....:     letter = word[0]\n   .....:     if letter not in by_letter:\n   .....:         by_letter[letter] = [word]\n   .....:     else:\n   .....:         by_letter[letter].append(word)\n   .....:\nIn [111]: by_letter\nOut[111]: {'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']}\nThe setdefault dictionary method can be used to simplify this workflow. The\npreceding for loop can be rewritten as:\nIn [112]: by_letter = {}\nIn [113]: for word in words:\n   .....:     letter = word[0]\n   .....:     by_letter.setdefault(letter, []).append(word)\n   .....:\nIn [114]: by_letter\nOut[114]: {'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']}\nThe built-in collections module has a useful class, defaultdict, which makes this\neven easier. To create one, you pass a type or function for generating the default value\nfor each slot in the dictionary:\nIn [115]: from collections import defaultdict\nIn [116]: by_letter = defaultdict(list)\nIn [117]: for word in words:\n   .....:     by_letter[word[0]].append(word)\n58 \n| \nChapter 3: Built-In Data Structures, Functions, and Files",
      "content_length": 1670,
      "extraction_method": "Direct"
    },
    {
      "page_number": 77,
      "chapter": null,
      "content": "Valid dictionary key types\nWhile the values of a dictionary can be any Python object, the keys generally have to\nbe immutable objects like scalar types (int, float, string) or tuples (all the objects in\nthe tuple need to be immutable, too). The technical term here is hashability. You can\ncheck whether an object is hashable (can be used as a key in a dictionary) with the\nhash function:\nIn [118]: hash(\"string\")\nOut[118]: 3634226001988967898\nIn [119]: hash((1, 2, (2, 3)))\nOut[119]: -9209053662355515447\nIn [120]: hash((1, 2, [2, 3])) # fails because lists are mutable\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-120-473c35a62c0b> in <module>\n----> 1 hash((1, 2, [2, 3])) # fails because lists are mutable\nTypeError: unhashable type: 'list'\nThe hash values you see when using the hash function in general will depend on the\nPython version you are using.\nTo use a list as a key, one option is to convert it to a tuple, which can be hashed as\nlong as its elements also can be:\nIn [121]: d = {}\nIn [122]: d[tuple([1, 2, 3])] = 5\nIn [123]: d\nOut[123]: {(1, 2, 3): 5}\nSet\nA set is an unordered collection of unique elements. A set can be created in two ways:\nvia the set function or via a set literal with curly braces:\nIn [124]: set([2, 2, 2, 1, 3, 3])\nOut[124]: {1, 2, 3}\nIn [125]: {2, 2, 2, 1, 3, 3}\nOut[125]: {1, 2, 3}\nSets support mathematical set operations like union, intersection, difference, and\nsymmetric difference. Consider these two example sets:\nIn [126]: a = {1, 2, 3, 4, 5}\nIn [127]: b = {3, 4, 5, 6, 7, 8}\n3.1 Data Structures and Sequences \n| \n59",
      "content_length": 1683,
      "extraction_method": "Direct"
    },
    {
      "page_number": 78,
      "chapter": null,
      "content": "The union of these two sets is the set of distinct elements occurring in either set. This\ncan be computed with either the union method or the | binary operator:\nIn [128]: a.union(b)\nOut[128]: {1, 2, 3, 4, 5, 6, 7, 8}\nIn [129]: a | b\nOut[129]: {1, 2, 3, 4, 5, 6, 7, 8}\nThe intersection contains the elements occurring in both sets. The & operator or the\nintersection method can be used:\nIn [130]: a.intersection(b)\nOut[130]: {3, 4, 5}\nIn [131]: a & b\nOut[131]: {3, 4, 5}\nSee Table 3-1 for a list of commonly used set methods.\nTable 3-1. Python set operations\nFunction\nAlternative\nsyntax\nDescription\na.add(x)\nN/A\nAdd element x to set a\na.clear()\nN/A\nReset set a to an empty state, discarding all of its\nelements\na.remove(x)\nN/A\nRemove element x from set a\na.pop()\nN/A\nRemove an arbitrary element from set a, raising\nKeyError if the set is empty\na.union(b)\na | b\nAll of the unique elements in a and b\na.update(b)\na |= b\nSet the contents of a to be the union of the elements\nin a and b\na.intersection(b)\na & b\nAll of the elements in both a and b\na.intersection_update(b)\na &= b\nSet the contents of a to be the intersection of the\nelements in a and b\na.difference(b)\na - b\nThe elements in a that are not in b\na.difference_update(b)\na -= b\nSet a to the elements in a that are not in b\na.symmetric_difference(b)\na ^ b\nAll of the elements in either a or b but not both\na.symmetric_difference_update(b)\na ^= b\nSet a to contain the elements in either a or b but\nnot both\na.issubset(b)\n<=\nTrue if the elements of a are all contained in b\na.issuperset(b)\n>=\nTrue if the elements of b are all contained in a\na.isdisjoint(b)\nN/A\nTrue if a and b have no elements in common\n60 \n| \nChapter 3: Built-In Data Structures, Functions, and Files",
      "content_length": 1722,
      "extraction_method": "Direct"
    },
    {
      "page_number": 79,
      "chapter": null,
      "content": "If you pass an input that is not a set to methods like union and\nintersection, Python will convert the input to a set before execut‐\ning the operation. When using the binary operators, both objects\nmust already be sets.\nAll of the logical set operations have in-place counterparts, which enable you to\nreplace the contents of the set on the left side of the operation with the result. For\nvery large sets, this may be more efficient:\nIn [132]: c = a.copy()\nIn [133]: c |= b\nIn [134]: c\nOut[134]: {1, 2, 3, 4, 5, 6, 7, 8}\nIn [135]: d = a.copy()\nIn [136]: d &= b\nIn [137]: d\nOut[137]: {3, 4, 5}\nLike dictionary keys, set elements generally must be immutable, and they must be\nhashable (which means that calling hash on a value does not raise an exception). In\norder to store list-like elements (or other mutable sequences) in a set, you can convert\nthem to tuples:\nIn [138]: my_data = [1, 2, 3, 4]\nIn [139]: my_set = {tuple(my_data)}\nIn [140]: my_set\nOut[140]: {(1, 2, 3, 4)}\nYou can also check if a set is a subset of (is contained in) or a superset of (contains all\nelements of) another set:\nIn [141]: a_set = {1, 2, 3, 4, 5}\nIn [142]: {1, 2, 3}.issubset(a_set)\nOut[142]: True\nIn [143]: a_set.issuperset({1, 2, 3})\nOut[143]: True\nSets are equal if and only if their contents are equal:\nIn [144]: {1, 2, 3} == {3, 2, 1}\nOut[144]: True\n3.1 Data Structures and Sequences \n| \n61",
      "content_length": 1374,
      "extraction_method": "Direct"
    },
    {
      "page_number": 80,
      "chapter": null,
      "content": "Built-In Sequence Functions\nPython has a handful of useful sequence functions that you should familiarize your‐\nself with and use at any opportunity.\nenumerate\nIt’s common when iterating over a sequence to want to keep track of the index of the\ncurrent item. A do-it-yourself approach would look like:\nindex = 0\nfor value in collection:\n   # do something with value\n   index += 1\nSince this is so common, Python has a built-in function, enumerate, which returns a\nsequence of (i, value) tuples:\nfor index, value in enumerate(collection):\n   # do something with value\nsorted\nThe sorted function returns a new sorted list from the elements of any sequence:\nIn [145]: sorted([7, 1, 2, 6, 0, 3, 2])\nOut[145]: [0, 1, 2, 2, 3, 6, 7]\nIn [146]: sorted(\"horse race\")\nOut[146]: [' ', 'a', 'c', 'e', 'e', 'h', 'o', 'r', 'r', 's']\nThe sorted function accepts the same arguments as the sort method on lists.\nzip\nzip “pairs” up the elements of a number of lists, tuples, or other sequences to create a\nlist of tuples:\nIn [147]: seq1 = [\"foo\", \"bar\", \"baz\"]\nIn [148]: seq2 = [\"one\", \"two\", \"three\"]\nIn [149]: zipped = zip(seq1, seq2)\nIn [150]: list(zipped)\nOut[150]: [('foo', 'one'), ('bar', 'two'), ('baz', 'three')]\nzip can take an arbitrary number of sequences, and the number of elements it\nproduces is determined by the shortest sequence:\nIn [151]: seq3 = [False, True]\n62 \n| \nChapter 3: Built-In Data Structures, Functions, and Files",
      "content_length": 1424,
      "extraction_method": "Direct"
    },
    {
      "page_number": 81,
      "chapter": null,
      "content": "In [152]: list(zip(seq1, seq2, seq3))\nOut[152]: [('foo', 'one', False), ('bar', 'two', True)]\nA common use of zip is simultaneously iterating over multiple sequences, possibly\nalso combined with enumerate:\nIn [153]: for index, (a, b) in enumerate(zip(seq1, seq2)):\n   .....:     print(f\"{index}: {a}, {b}\")\n   .....:\n0: foo, one\n1: bar, two\n2: baz, three\nreversed\nreversed iterates over the elements of a sequence in reverse order:\nIn [154]: list(reversed(range(10)))\nOut[154]: [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\nKeep in mind that reversed is a generator (to be discussed in some more detail later),\nso it does not create the reversed sequence until materialized (e.g., with list or a for\nloop).\nList, Set, and Dictionary Comprehensions\nList comprehensions are a convenient and widely used Python language feature. They\nallow you to concisely form a new list by filtering the elements of a collection,\ntransforming the elements passing the filter into one concise expression. They take\nthe basic form:\n[expr for value in collection if condition]\nThis is equivalent to the following for loop:\nresult = []\nfor value in collection:\n    if condition:\n        result.append(expr)\nThe filter condition can be omitted, leaving only the expression. For example, given\na list of strings, we could filter out strings with length 2 or less and convert them to\nuppercase like this:\nIn [155]: strings = [\"a\", \"as\", \"bat\", \"car\", \"dove\", \"python\"]\nIn [156]: [x.upper() for x in strings if len(x) > 2]\nOut[156]: ['BAT', 'CAR', 'DOVE', 'PYTHON']\nSet and dictionary comprehensions are a natural extension, producing sets and dic‐\ntionaries in an idiomatically similar way instead of lists.\n3.1 Data Structures and Sequences \n| \n63",
      "content_length": 1709,
      "extraction_method": "Direct"
    },
    {
      "page_number": 82,
      "chapter": null,
      "content": "A dictionary comprehension looks like this:\ndict_comp = {key-expr: value-expr for value in collection\n             if condition}\nA set comprehension looks like the equivalent list comprehension except with curly\nbraces instead of square brackets:\nset_comp = {expr for value in collection if condition}\nLike list comprehensions, set and dictionary comprehensions are mostly convenien‐\nces, but they similarly can make code both easier to write and read. Consider the\nlist of strings from before. Suppose we wanted a set containing just the lengths of\nthe strings contained in the collection; we could easily compute this using a set\ncomprehension:\nIn [157]: unique_lengths = {len(x) for x in strings}\nIn [158]: unique_lengths\nOut[158]: {1, 2, 3, 4, 6}\nWe could also express this more functionally using the map function, introduced\nshortly:\nIn [159]: set(map(len, strings))\nOut[159]: {1, 2, 3, 4, 6}\nAs a simple dictionary comprehension example, we could create a lookup map of\nthese strings for their locations in the list:\nIn [160]: loc_mapping = {value: index for index, value in enumerate(strings)}\nIn [161]: loc_mapping\nOut[161]: {'a': 0, 'as': 1, 'bat': 2, 'car': 3, 'dove': 4, 'python': 5}\nNested list comprehensions\nSuppose we have a list of lists containing some English and Spanish names:\nIn [162]: all_data = [[\"John\", \"Emily\", \"Michael\", \"Mary\", \"Steven\"],\n   .....:             [\"Maria\", \"Juan\", \"Javier\", \"Natalia\", \"Pilar\"]]\nSuppose we wanted to get a single list containing all names with two or more a’s in\nthem. We could certainly do this with a simple for loop:\nIn [163]: names_of_interest = []\nIn [164]: for names in all_data:\n   .....:     enough_as = [name for name in names if name.count(\"a\") >= 2]\n   .....:     names_of_interest.extend(enough_as)\n   .....:\nIn [165]: names_of_interest\nOut[165]: ['Maria', 'Natalia']\n64 \n| \nChapter 3: Built-In Data Structures, Functions, and Files",
      "content_length": 1904,
      "extraction_method": "Direct"
    },
    {
      "page_number": 83,
      "chapter": null,
      "content": "You can actually wrap this whole operation up in a single nested list comprehension,\nwhich will look like:\nIn [166]: result = [name for names in all_data for name in names\n   .....:           if name.count(\"a\") >= 2]\nIn [167]: result\nOut[167]: ['Maria', 'Natalia']\nAt first, nested list comprehensions are a bit hard to wrap your head around. The for\nparts of the list comprehension are arranged according to the order of nesting, and\nany filter condition is put at the end as before. Here is another example where we\n“flatten” a list of tuples of integers into a simple list of integers:\nIn [168]: some_tuples = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\nIn [169]: flattened = [x for tup in some_tuples for x in tup]\nIn [170]: flattened\nOut[170]: [1, 2, 3, 4, 5, 6, 7, 8, 9]\nKeep in mind that the order of the for expressions would be the same if you wrote a\nnested for loop instead of a list comprehension:\nflattened = []\nfor tup in some_tuples:\n    for x in tup:\n        flattened.append(x)\nYou can have arbitrarily many levels of nesting, though if you have more than two\nor three levels of nesting, you should probably start to question whether this makes\nsense from a code readability standpoint. It’s important to distinguish the syntax just\nshown from a list comprehension inside a list comprehension, which is also perfectly\nvalid:\nIn [172]: [[x for x in tup] for tup in some_tuples]\nOut[172]: [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nThis produces a list of lists, rather than a flattened list of all of the inner elements.\n3.2 Functions\nFunctions are the primary and most important method of code organization and\nreuse in Python. As a rule of thumb, if you anticipate needing to repeat the same\nor very similar code more than once, it may be worth writing a reusable function.\nFunctions can also help make your code more readable by giving a name to a group\nof Python statements.\n3.2 Functions \n| \n65",
      "content_length": 1896,
      "extraction_method": "Direct"
    },
    {
      "page_number": 84,
      "chapter": null,
      "content": "Functions are declared with the def keyword. A function contains a block of code\nwith an optional use of the return keyword:\nIn [173]: def my_function(x, y):\n   .....:     return x + y\nWhen a line with return is reached, the value or expression after return is sent to the\ncontext where the function was called, for example:\nIn [174]: my_function(1, 2)\nOut[174]: 3\nIn [175]: result = my_function(1, 2)\nIn [176]: result\nOut[176]: 3\nThere is no issue with having multiple return statements. If Python reaches the end\nof a function without encountering a return statement, None is returned automati‐\ncally. For example:\nIn [177]: def function_without_return(x):\n   .....:     print(x)\nIn [178]: result = function_without_return(\"hello!\")\nhello!\nIn [179]: print(result)\nNone\nEach function can have positional arguments and keyword arguments. Keyword argu‐\nments are most commonly used to specify default values or optional arguments. Here\nwe will define a function with an optional z argument with the default value 1.5:\ndef my_function2(x, y, z=1.5):\n    if z > 1:\n        return z * (x + y)\n    else:\n        return z / (x + y)\nWhile keyword arguments are optional, all positional arguments must be specified\nwhen calling a function.\nYou can pass values to the z argument with or without the keyword provided, though\nusing the keyword is encouraged:\nIn [181]: my_function2(5, 6, z=0.7)\nOut[181]: 0.06363636363636363\nIn [182]: my_function2(3.14, 7, 3.5)\nOut[182]: 35.49\n66 \n| \nChapter 3: Built-In Data Structures, Functions, and Files",
      "content_length": 1531,
      "extraction_method": "Direct"
    },
    {
      "page_number": 85,
      "chapter": null,
      "content": "In [183]: my_function2(10, 20)\nOut[183]: 45.0\nThe main restriction on function arguments is that the keyword arguments must\nfollow the positional arguments (if any). You can specify keyword arguments in any\norder. This frees you from having to remember the order in which the function\narguments were specified. You need to remember only what their names are.\nNamespaces, Scope, and Local Functions\nFunctions can access variables created inside the function as well as those outside\nthe function in higher (or even global) scopes. An alternative and more descriptive\nname describing a variable scope in Python is a namespace. Any variables that are\nassigned within a function by default are assigned to the local namespace. The local\nnamespace is created when the function is called and is immediately populated by the\nfunction’s arguments. After the function is finished, the local namespace is destroyed\n(with some exceptions that are outside the purview of this chapter). Consider the\nfollowing function:\ndef func():\n    a = []\n    for i in range(5):\n        a.append(i)\nWhen func() is called, the empty list a is created, five elements are appended, and\nthen a is destroyed when the function exits. Suppose instead we had declared a as\nfollows:\nIn [184]: a = []\nIn [185]: def func():\n   .....:     for i in range(5):\n   .....:         a.append(i)\nEach call to func will modify list a:\nIn [186]: func()\nIn [187]: a\nOut[187]: [0, 1, 2, 3, 4]\nIn [188]: func()\nIn [189]: a\nOut[189]: [0, 1, 2, 3, 4, 0, 1, 2, 3, 4]\nAssigning variables outside of the function’s scope is possible, but those variables\nmust be declared explicitly using either the global or nonlocal keywords:\nIn [190]: a = None\n3.2 Functions \n| \n67",
      "content_length": 1711,
      "extraction_method": "Direct"
    },
    {
      "page_number": 86,
      "chapter": null,
      "content": "In [191]: def bind_a_variable():\n   .....:     global a\n   .....:     a = []\n   .....: bind_a_variable()\n   .....:\nIn [192]: print(a)\n[]\nnonlocal allows a function to modify variables defined in a higher-level scope that is\nnot global. Since its use is somewhat esoteric (I never use it in this book), I refer you\nto the Python documentation to learn more about it.\nI generally discourage use of the global keyword. Typically, global\nvariables are used to store some kind of state in a system. If you\nfind yourself using a lot of them, it may indicate a need for object-\noriented programming (using classes).\nReturning Multiple Values\nWhen I first programmed in Python after having programmed in Java and C++, one\nof my favorite features was the ability to return multiple values from a function with\nsimple syntax. Here’s an example:\ndef f():\n    a = 5\n    b = 6\n    c = 7\n    return a, b, c\na, b, c = f()\nIn data analysis and other scientific applications, you may find yourself doing this\noften. What’s happening here is that the function is actually just returning one object,\na tuple, which is then being unpacked into the result variables. In the preceding\nexample, we could have done this instead:\nreturn_value = f()\nIn this case, return_value would be a 3-tuple with the three returned variables. A\npotentially attractive alternative to returning multiple values like before might be to\nreturn a dictionary instead:\ndef f():\n    a = 5\n    b = 6\n    c = 7\n    return {\"a\" : a, \"b\" : b, \"c\" : c}\nThis alternative technique can be useful depending on what you are trying to do.\n68 \n| \nChapter 3: Built-In Data Structures, Functions, and Files",
      "content_length": 1647,
      "extraction_method": "Direct"
    },
    {
      "page_number": 87,
      "chapter": null,
      "content": "Functions Are Objects\nSince Python functions are objects, many constructs can be easily expressed that are\ndifficult to do in other languages. Suppose we were doing some data cleaning and\nneeded to apply a bunch of transformations to the following list of strings:\nIn [193]: states = [\"   Alabama \", \"Georgia!\", \"Georgia\", \"georgia\", \"FlOrIda\",\n   .....:           \"south   carolina##\", \"West virginia?\"]\nAnyone who has ever worked with user-submitted survey data has seen messy results\nlike these. Lots of things need to happen to make this list of strings uniform and\nready for analysis: stripping whitespace, removing punctuation symbols, and stand‐\nardizing proper capitalization. One way to do this is to use built-in string methods\nalong with the re standard library module for regular expressions:\nimport re\ndef clean_strings(strings):\n    result = []\n    for value in strings:\n        value = value.strip()\n        value = re.sub(\"[!#?]\", \"\", value)\n        value = value.title()\n        result.append(value)\n    return result\nThe result looks like this:\nIn [195]: clean_strings(states)\nOut[195]: \n['Alabama',\n 'Georgia',\n 'Georgia',\n 'Georgia',\n 'Florida',\n 'South   Carolina',\n 'West Virginia']\nAn alternative approach that you may find useful is to make a list of the operations\nyou want to apply to a particular set of strings:\ndef remove_punctuation(value):\n    return re.sub(\"[!#?]\", \"\", value)\nclean_ops = [str.strip, remove_punctuation, str.title]\ndef clean_strings(strings, ops):\n    result = []\n    for value in strings:\n        for func in ops:\n            value = func(value)\n        result.append(value)\n    return result\n3.2 Functions \n| \n69",
      "content_length": 1663,
      "extraction_method": "Direct"
    },
    {
      "page_number": 88,
      "chapter": null,
      "content": "Then we have the following:\nIn [197]: clean_strings(states, clean_ops)\nOut[197]: \n['Alabama',\n 'Georgia',\n 'Georgia',\n 'Georgia',\n 'Florida',\n 'South   Carolina',\n 'West Virginia']\nA more functional pattern like this enables you to easily modify how the strings\nare transformed at a very high level. The clean_strings function is also now more\nreusable and generic.\nYou can use functions as arguments to other functions like the built-in map function,\nwhich applies a function to a sequence of some kind:\nIn [198]: for x in map(remove_punctuation, states):\n   .....:     print(x)\nAlabama \nGeorgia\nGeorgia\ngeorgia\nFlOrIda\nsouth   carolina\nWest virginia\nmap can be used as an alternative to list comprehensions without any filter.\nAnonymous (Lambda) Functions\nPython has support for so-called anonymous or lambda functions, which are a way\nof writing functions consisting of a single statement, the result of which is the return\nvalue. They are defined with the lambda keyword, which has no meaning other than\n“we are declaring an anonymous function”:\nIn [199]: def short_function(x):\n   .....:     return x * 2\nIn [200]: equiv_anon = lambda x: x * 2\nI usually refer to these as lambda functions in the rest of the book. They are especially\nconvenient in data analysis because, as you’ll see, there are many cases where data\ntransformation functions will take functions as arguments. It’s often less typing (and\nclearer) to pass a lambda function as opposed to writing a full-out function declara‐\ntion or even assigning the lambda function to a local variable. Consider this example:\nIn [201]: def apply_to_list(some_list, f):\n   .....:     return [f(x) for x in some_list]\n70 \n| \nChapter 3: Built-In Data Structures, Functions, and Files",
      "content_length": 1737,
      "extraction_method": "Direct"
    },
    {
      "page_number": 89,
      "chapter": null,
      "content": "In [202]: ints = [4, 0, 1, 5, 6]\nIn [203]: apply_to_list(ints, lambda x: x * 2)\nOut[203]: [8, 0, 2, 10, 12]\nYou could also have written [x * 2 for x in ints], but here we were able to\nsuccinctly pass a custom operator to the apply_to_list function.\nAs another example, suppose you wanted to sort a collection of strings by the number\nof distinct letters in each string:\nIn [204]: strings = [\"foo\", \"card\", \"bar\", \"aaaa\", \"abab\"]\nHere we could pass a lambda function to the list’s sort method:\nIn [205]: strings.sort(key=lambda x: len(set(x)))\nIn [206]: strings\nOut[206]: ['aaaa', 'foo', 'abab', 'bar', 'card']\nGenerators\nMany objects in Python support iteration, such as over objects in a list or lines in a\nfile. This is accomplished by means of the iterator protocol, a generic way to make\nobjects iterable. For example, iterating over a dictionary yields the dictionary keys:\nIn [207]: some_dict = {\"a\": 1, \"b\": 2, \"c\": 3}\nIn [208]: for key in some_dict:\n   .....:     print(key)\na\nb\nc\nWhen you write for key in some_dict, the Python interpreter first attempts to\ncreate an iterator out of some_dict:\nIn [209]: dict_iterator = iter(some_dict)\nIn [210]: dict_iterator\nOut[210]: <dict_keyiterator at 0x7fefe45465c0>\nAn iterator is any object that will yield objects to the Python interpreter when used\nin a context like a for loop. Most methods expecting a list or list-like object will also\naccept any iterable object. This includes built-in methods such as min, max, and sum,\nand type constructors like list and tuple:\nIn [211]: list(dict_iterator)\nOut[211]: ['a', 'b', 'c']\n3.2 Functions \n| \n71",
      "content_length": 1598,
      "extraction_method": "Direct"
    },
    {
      "page_number": 90,
      "chapter": null,
      "content": "A generator is a convenient way, similar to writing a normal function, to construct a\nnew iterable object. Whereas normal functions execute and return a single result at\na time, generators can return a sequence of multiple values by pausing and resuming\nexecution each time the generator is used. To create a generator, use the yield\nkeyword instead of return in a function:\ndef squares(n=10):\n    print(f\"Generating squares from 1 to {n ** 2}\")\n    for i in range(1, n + 1):\n        yield i ** 2\nWhen you actually call the generator, no code is immediately executed:\nIn [213]: gen = squares()\nIn [214]: gen\nOut[214]: <generator object squares at 0x7fefe437d620>\nIt is not until you request elements from the generator that it begins executing its\ncode:\nIn [215]: for x in gen:\n   .....:     print(x, end=\" \")\nGenerating squares from 1 to 100\n1 4 9 16 25 36 49 64 81 100\nSince generators produce output one element at a time versus an\nentire list all at once, it can help your program use less memory.\nGenerator expressions\nAnother way to make a generator is by using a generator expression. This is a genera‐\ntor analogue to list, dictionary, and set comprehensions. To create one, enclose what\nwould otherwise be a list comprehension within parentheses instead of brackets:\nIn [216]: gen = (x ** 2 for x in range(100))\nIn [217]: gen\nOut[217]: <generator object <genexpr> at 0x7fefe437d000>\nThis is equivalent to the following more verbose generator:\ndef _make_gen():\n    for x in range(100):\n        yield x ** 2\ngen = _make_gen()\nGenerator expressions can be used instead of list comprehensions as function argu‐\nments in some cases:\n72 \n| \nChapter 3: Built-In Data Structures, Functions, and Files",
      "content_length": 1701,
      "extraction_method": "Direct"
    },
    {
      "page_number": 91,
      "chapter": null,
      "content": "In [218]: sum(x ** 2 for x in range(100))\nOut[218]: 328350\nIn [219]: dict((i, i ** 2) for i in range(5))\nOut[219]: {0: 0, 1: 1, 2: 4, 3: 9, 4: 16}\nDepending on the number of elements produced by the comprehension expression,\nthe generator version can sometimes be meaningfully faster.\nitertools module\nThe standard library itertools module has a collection of generators for many\ncommon data algorithms. For example, groupby takes any sequence and a function,\ngrouping consecutive elements in the sequence by return value of the function. Here’s\nan example:\nIn [220]: import itertools\nIn [221]: def first_letter(x):\n   .....:     return x[0]\nIn [222]: names = [\"Alan\", \"Adam\", \"Wes\", \"Will\", \"Albert\", \"Steven\"]\nIn [223]: for letter, names in itertools.groupby(names, first_letter):\n   .....:     print(letter, list(names)) # names is a generator\nA ['Alan', 'Adam']\nW ['Wes', 'Will']\nA ['Albert']\nS ['Steven']\nSee Table 3-2 for a list of a few other itertools functions I’ve frequently found\nhelpful. You may like to check out the official Python documentation for more on this\nuseful built-in utility module.\nTable 3-2. Some useful itertools functions\nFunction\nDescription\nchain(*iterables)\nGenerates a sequence by chaining iterators together. Once elements from the\nfirst iterator are exhausted, elements from the next iterator are returned, and\nso on.\ncombinations(iterable, k)\nGenerates a sequence of all possible k-tuples of elements in the iterable,\nignoring order and without replacement (see also the companion function\ncombinations_with_replacement).\npermutations(iterable, k)\nGenerates a sequence of all possible k-tuples of elements in the iterable,\nrespecting order.\ngroupby(iterable[, keyfunc])\nGenerates (key, sub-iterator) for each unique key.\nproduct(*iterables, repeat=1) Generates the Cartesian product of the input iterables as tuples, similar to a\nnested for loop.\n3.2 Functions \n| \n73",
      "content_length": 1905,
      "extraction_method": "Direct"
    },
    {
      "page_number": 92,
      "chapter": null,
      "content": "Errors and Exception Handling\nHandling Python errors or exceptions gracefully is an important part of building\nrobust programs. In data analysis applications, many functions work only on certain\nkinds of input. As an example, Python’s float function is capable of casting a string\nto a floating-point number, but it fails with ValueError on improper inputs:\nIn [224]: float(\"1.2345\")\nOut[224]: 1.2345\nIn [225]: float(\"something\")\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-225-5ccfe07933f4> in <module>\n----> 1 float(\"something\")\nValueError: could not convert string to float: 'something'\nSuppose we wanted a version of float that fails gracefully, returning the input\nargument. We can do this by writing a function that encloses the call to float in a\ntry/except block (execute this code in IPython):\ndef attempt_float(x):\n    try:\n        return float(x)\n    except:\n        return x\nThe code in the except part of the block will only be executed if float(x) raises an\nexception:\nIn [227]: attempt_float(\"1.2345\")\nOut[227]: 1.2345\nIn [228]: attempt_float(\"something\")\nOut[228]: 'something'\nYou might notice that float can raise exceptions other than ValueError:\nIn [229]: float((1, 2))\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-229-82f777b0e564> in <module>\n----> 1 float((1, 2))\nTypeError: float() argument must be a string or a real number, not 'tuple'\nYou might want to suppress only ValueError, since a TypeError (the input was not a\nstring or numeric value) might indicate a legitimate bug in your program. To do that,\nwrite the exception type after except:\ndef attempt_float(x):\n    try:\n        return float(x)\n74 \n| \nChapter 3: Built-In Data Structures, Functions, and Files",
      "content_length": 1933,
      "extraction_method": "Direct"
    },
    {
      "page_number": 93,
      "chapter": null,
      "content": "except ValueError:\n        return x\nWe have then:\nIn [231]: attempt_float((1, 2))\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-231-8b0026e9e6b7> in <module>\n----> 1 attempt_float((1, 2))\n<ipython-input-230-6209ddecd2b5> in attempt_float(x)\n      1 def attempt_float(x):\n      2     try:\n----> 3         return float(x)\n      4     except ValueError:\n      5         return x\nTypeError: float() argument must be a string or a real number, not 'tuple'\nYou can catch multiple exception types by writing a tuple of exception types instead\n(the parentheses are required):\ndef attempt_float(x):\n    try:\n        return float(x)\n    except (TypeError, ValueError):\n        return x\nIn some cases, you may not want to suppress an exception, but you want some code\nto be executed regardless of whether or not the code in the try block succeeds. To do\nthis, use finally:\nf = open(path, mode=\"w\")\ntry:\n    write_to_file(f)\nfinally:\n    f.close()\nHere, the file object f will always get closed. Similarly, you can have code that\nexecutes only if the try: block succeeds using else:\nf = open(path, mode=\"w\")\ntry:\n    write_to_file(f)\nexcept:\n    print(\"Failed\")\nelse:\n    print(\"Succeeded\")\nfinally:\n    f.close()\n3.2 Functions \n| \n75",
      "content_length": 1343,
      "extraction_method": "Direct"
    },
    {
      "page_number": 94,
      "chapter": null,
      "content": "Exceptions in IPython\nIf an exception is raised while you are %run-ing a script or executing any statement,\nIPython will by default print a full call stack trace (traceback) with a few lines of\ncontext around the position at each point in the stack:\nIn [10]: %run examples/ipython_bug.py\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n/home/wesm/code/pydata-book/examples/ipython_bug.py in <module>()\n     13     throws_an_exception()\n     14\n---> 15 calling_things()\n/home/wesm/code/pydata-book/examples/ipython_bug.py in calling_things()\n     11 def calling_things():\n     12     works_fine()\n---> 13     throws_an_exception()\n     14\n     15 calling_things()\n/home/wesm/code/pydata-book/examples/ipython_bug.py in throws_an_exception()\n      7     a = 5\n      8     b = 6\n----> 9     assert(a + b == 10)\n     10\n     11 def calling_things():\nAssertionError:\nHaving additional context by itself is a big advantage over the standard Python\ninterpreter (which does not provide any additional context). You can control the\namount of context shown using the %xmode magic command, from Plain (same as\nthe standard Python interpreter) to Verbose (which inlines function argument values\nand more). As you will see later in Appendix B, you can step into the stack (using\nthe %debug or %pdb magics) after an error has occurred for interactive postmortem\ndebugging.\n3.3 Files and the Operating System\nMost of this book uses high-level tools like pandas.read_csv to read data files from\ndisk into Python data structures. However, it’s important to understand the basics of\nhow to work with files in Python. Fortunately, it’s relatively straightforward, which is\none reason Python is so popular for text and file munging.\nTo open a file for reading or writing, use the built-in open function with either a\nrelative or absolute file path and an optional file encoding:\n76 \n| \nChapter 3: Built-In Data Structures, Functions, and Files",
      "content_length": 2027,
      "extraction_method": "Direct"
    },
    {
      "page_number": 95,
      "chapter": null,
      "content": "In [233]: path = \"examples/segismundo.txt\"\nIn [234]: f = open(path, encoding=\"utf-8\")\nHere, I pass encoding=\"utf-8\" as a best practice because the default Unicode encod‐\ning for reading files varies from platform to platform.\nBy default, the file is opened in read-only mode \"r\". We can then treat the file object\nf like a list and iterate over the lines like so:\nfor line in f:\n    print(line)\nThe lines come out of the file with the end-of-line (EOL) markers intact, so you’ll\noften see code to get an EOL-free list of lines in a file like:\nIn [235]: lines = [x.rstrip() for x in open(path, encoding=\"utf-8\")]\nIn [236]: lines\nOut[236]: \n['Sueña el rico en su riqueza,',\n 'que más cuidados le ofrece;',\n '',\n 'sueña el pobre que padece',\n 'su miseria y su pobreza;',\n '',\n 'sueña el que a medrar empieza,',\n 'sueña el que afana y pretende,',\n 'sueña el que agravia y ofende,',\n '',\n 'y en el mundo, en conclusión,',\n 'todos sueñan lo que son,',\n 'aunque ninguno lo entiende.',\n '']\nWhen you use open to create file objects, it is recommended to close the file when\nyou are finished with it. Closing the file releases its resources back to the operating\nsystem:\nIn [237]: f.close()\nOne of the ways to make it easier to clean up open files is to use the with statement:\nIn [238]: with open(path, encoding=\"utf-8\") as f:\n   .....:     lines = [x.rstrip() for x in f]\nThis will automatically close the file f when exiting the with block. Failing to ensure\nthat files are closed will not cause problems in many small programs or scripts, but it\ncan be an issue in programs that need to interact with a large number of files.\nIf we had typed f = open(path, \"w\"), a new file at examples/segismundo.txt would\nhave been created (be careful!), overwriting any file in its place. There is also the\n3.3 Files and the Operating System \n| \n77",
      "content_length": 1829,
      "extraction_method": "Direct"
    },
    {
      "page_number": 96,
      "chapter": null,
      "content": "\"x\" file mode, which creates a writable file but fails if the file path already exists. See\nTable 3-3 for a list of all valid file read/write modes.\nTable 3-3. Python file modes\nMode\nDescription\nr\nRead-only mode\nw\nWrite-only mode; creates a new file (erasing the data for any file with the same name)\nx\nWrite-only mode; creates a new file but fails if the file path already exists\na\nAppend to existing file (creates the file if it does not already exist)\nr+\nRead and write\nb\nAdd to mode for binary files (i.e., \"rb\" or \"wb\")\nt\nText mode for files (automatically decoding bytes to Unicode); this is the default if not specified\nFor readable files, some of the most commonly used methods are read, seek, and\ntell. read returns a certain number of characters from the file. What constitutes a\n“character” is determined by the file encoding or simply raw bytes if the file is opened\nin binary mode:\nIn [239]: f1 = open(path)\nIn [240]: f1.read(10)\nOut[240]: 'Sueña el r'\nIn [241]: f2 = open(path, mode=\"rb\")  # Binary mode\nIn [242]: f2.read(10)\nOut[242]: b'Sue\\xc3\\xb1a el '\nThe read method advances the file object position by the number of bytes read. tell\ngives you the current position:\nIn [243]: f1.tell()\nOut[243]: 11\nIn [244]: f2.tell()\nOut[244]: 10\nEven though we read 10 characters from the file f1 opened in text mode, the position\nis 11 because it took that many bytes to decode 10 characters using the default\nencoding. You can check the default encoding in the sys module:\nIn [245]: import sys\nIn [246]: sys.getdefaultencoding()\nOut[246]: 'utf-8'\nTo get consistent behavior across platforms, it is best to pass an encoding (such as\nencoding=\"utf-8\", which is widely used) when opening files.\n78 \n| \nChapter 3: Built-In Data Structures, Functions, and Files",
      "content_length": 1764,
      "extraction_method": "Direct"
    },
    {
      "page_number": 97,
      "chapter": null,
      "content": "seek changes the file position to the indicated byte in the file:\nIn [247]: f1.seek(3)\nOut[247]: 3\nIn [248]: f1.read(1)\nOut[248]: 'ñ'\nIn [249]: f1.tell()\nOut[249]: 5\nLastly, we remember to close the files:\nIn [250]: f1.close()\nIn [251]: f2.close()\nTo write text to a file, you can use the file’s write or writelines methods. For\nexample, we could create a version of examples/segismundo.txt with no blank lines\nlike so:\nIn [252]: path\nOut[252]: 'examples/segismundo.txt'\nIn [253]: with open(\"tmp.txt\", mode=\"w\") as handle:\n   .....:     handle.writelines(x for x in open(path) if len(x) > 1)\nIn [254]: with open(\"tmp.txt\") as f:\n   .....:     lines = f.readlines()\nIn [255]: lines\nOut[255]: \n['Sueña el rico en su riqueza,\\n',\n 'que más cuidados le ofrece;\\n',\n 'sueña el pobre que padece\\n',\n 'su miseria y su pobreza;\\n',\n 'sueña el que a medrar empieza,\\n',\n 'sueña el que afana y pretende,\\n',\n 'sueña el que agravia y ofende,\\n',\n 'y en el mundo, en conclusión,\\n',\n 'todos sueñan lo que son,\\n',\n 'aunque ninguno lo entiende.\\n']\nSee Table 3-4 for many of the most commonly used file methods.\nTable 3-4. Important Python file methods or attributes\nMethod/attribute\nDescription\nread([size])\nReturn data from file as bytes or string depending on the file mode, with optional size\nargument indicating the number of bytes or string characters to read\nreadable()\nReturn True if the file supports read operations\nreadlines([size])\nReturn list of lines in the file, with optional size argument\n3.3 Files and the Operating System \n| \n79",
      "content_length": 1534,
      "extraction_method": "Direct"
    },
    {
      "page_number": 98,
      "chapter": null,
      "content": "Method/attribute\nDescription\nwrite(string)\nWrite passed string to file\nwritable()\nReturn True if the file supports write operations\nwritelines(strings) Write passed sequence of strings to the file\nclose()\nClose the file object\nflush()\nFlush the internal I/O buffer to disk\nseek(pos)\nMove to indicated file position (integer)\nseekable()\nReturn True if the file object supports seeking and thus random access (some file-like objects\ndo not)\ntell()\nReturn current file position as integer\nclosed\nTrue if the file is closed\nencoding\nThe encoding used to interpret bytes in the file as Unicode (typically UTF-8)\nBytes and Unicode with Files\nThe default behavior for Python files (whether readable or writable) is text mode,\nwhich means that you intend to work with Python strings (i.e., Unicode). This\ncontrasts with binary mode, which you can obtain by appending b to the file mode.\nRevisiting the file (which contains non-ASCII characters with UTF-8 encoding) from\nthe previous section, we have:\nIn [258]: with open(path) as f:\n   .....:     chars = f.read(10)\nIn [259]: chars\nOut[259]: 'Sueña el r'\nIn [260]: len(chars)\nOut[260]: 10\nUTF-8 is a variable-length Unicode encoding, so when I request some number of\ncharacters from the file, Python reads enough bytes (which could be as few as 10 or\nas many as 40 bytes) from the file to decode that many characters. If I open the file in\n\"rb\" mode instead, read requests that exact number of bytes:\nIn [261]: with open(path, mode=\"rb\") as f:\n   .....:     data = f.read(10)\nIn [262]: data\nOut[262]: b'Sue\\xc3\\xb1a el '\nDepending on the text encoding, you may be able to decode the bytes to a str object\nyourself, but only if each of the encoded Unicode characters is fully formed:\nIn [263]: data.decode(\"utf-8\")\nOut[263]: 'Sueña el '\nIn [264]: data[:4].decode(\"utf-8\")\n80 \n| \nChapter 3: Built-In Data Structures, Functions, and Files",
      "content_length": 1877,
      "extraction_method": "Direct"
    },
    {
      "page_number": 99,
      "chapter": null,
      "content": "---------------------------------------------------------------------------\nUnicodeDecodeError                        Traceback (most recent call last)\n<ipython-input-264-846a5c2fed34> in <module>\n----> 1 data[:4].decode(\"utf-8\")\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xc3 in position 3: unexpecte\nd end of data\nText mode, combined with the encoding option of open, provides a convenient way\nto convert from one Unicode encoding to another:\nIn [265]: sink_path = \"sink.txt\"\nIn [266]: with open(path) as source:\n   .....:     with open(sink_path, \"x\", encoding=\"iso-8859-1\") as sink:\n   .....:         sink.write(source.read())\nIn [267]: with open(sink_path, encoding=\"iso-8859-1\") as f:\n   .....:     print(f.read(10))\nSueña el r\nBeware using seek when opening files in any mode other than binary. If the file\nposition falls in the middle of the bytes defining a Unicode character, then subsequent\nreads will result in an error:\nIn [269]: f = open(path, encoding='utf-8')\nIn [270]: f.read(5)\nOut[270]: 'Sueña'\nIn [271]: f.seek(4)\nOut[271]: 4\nIn [272]: f.read(1)\n---------------------------------------------------------------------------\nUnicodeDecodeError                        Traceback (most recent call last)\n<ipython-input-272-5a354f952aa4> in <module>\n----> 1 f.read(1)\n/miniconda/envs/book-env/lib/python3.10/codecs.py in decode(self, input, final)\n    320         # decode input (taking the buffer into account)\n    321         data = self.buffer + input\n--> 322         (result, consumed) = self._buffer_decode(data, self.errors, final\n)\n    323         # keep undecoded input until the next call\n    324         self.buffer = data[consumed:]\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xb1 in position 0: invalid s\ntart byte\nIn [273]: f.close()\nIf you find yourself regularly doing data analysis on non-ASCII text data, mastering\nPython’s Unicode functionality will prove valuable. See Python’s online documenta‐\ntion for much more.\n3.3 Files and the Operating System \n| \n81",
      "content_length": 2008,
      "extraction_method": "Direct"
    },
    {
      "page_number": 100,
      "chapter": null,
      "content": "3.4 Conclusion\nWith some of the basics of the Python environment and language now under your\nbelt, it is time to move on and learn about NumPy and array-oriented computing in\nPython.\n82 \n| \nChapter 3: Built-In Data Structures, Functions, and Files",
      "content_length": 247,
      "extraction_method": "Direct"
    },
    {
      "page_number": 101,
      "chapter": null,
      "content": "CHAPTER 4\nNumPy Basics: Arrays and\nVectorized Computation\nNumPy, short for Numerical Python, is one of the most important foundational pack‐\nages for numerical computing in Python. Many computational packages providing\nscientific functionality use NumPy’s array objects as one of the standard interface\nlingua francas for data exchange. Much of the knowledge about NumPy that I cover is\ntransferable to pandas as well.\nHere are some of the things you’ll find in NumPy:\n• ndarray, an efficient multidimensional array providing fast array-oriented arith‐\n•\nmetic operations and flexible broadcasting capabilities\n• Mathematical functions for fast operations on entire arrays of data without hav‐\n•\ning to write loops\n• Tools for reading/writing array data to disk and working with memory-mapped\n•\nfiles\n• Linear algebra, random number generation, and Fourier transform capabilities\n•\n• A C API for connecting NumPy with libraries written in C, C++, or FORTRAN\n•\nBecause NumPy provides a comprehensive and well-documented C API, it is\nstraightforward to pass data to external libraries written in a low-level language,\nand for external libraries to return data to Python as NumPy arrays. This feature\nhas made Python a language of choice for wrapping legacy C, C++, or FORTRAN\ncodebases and giving them a dynamic and accessible interface.\nWhile NumPy by itself does not provide modeling or scientific functionality, having\nan understanding of NumPy arrays and array-oriented computing will help you use\ntools with array computing semantics, like pandas, much more effectively. Since\n83",
      "content_length": 1582,
      "extraction_method": "Direct"
    },
    {
      "page_number": 102,
      "chapter": null,
      "content": "NumPy is a large topic, I will cover many advanced NumPy features like broadcasting\nin more depth later (see Appendix A). Many of these advanced features are not\nneeded to follow the rest of this book, but they may help you as you go deeper into\nscientific computing in Python.\nFor most data analysis applications, the main areas of functionality I’ll focus on are:\n• Fast array-based operations for data munging and cleaning, subsetting and filter‐\n•\ning, transformation, and any other kind of computation\n• Common array algorithms like sorting, unique, and set operations\n•\n• Efficient descriptive statistics and aggregating/summarizing data\n•\n• Data alignment and relational data manipulations for merging and joining heter‐\n•\nogeneous datasets\n• Expressing conditional logic as array expressions instead of loops with if-elif-\n•\nelse branches\n• Group-wise data manipulations (aggregation, transformation, and function\n•\napplication)\nWhile NumPy provides a computational foundation for general numerical data\nprocessing, many readers will want to use pandas as the basis for most kinds of\nstatistics or analytics, especially on tabular data. Also, pandas provides some more\ndomain-specific functionality like time series manipulation, which is not present in\nNumPy.\nArray-oriented computing in Python traces its roots back to 1995,\nwhen Jim Hugunin created the Numeric library. Over the next\n10 years, many scientific programming communities began doing\narray programming in Python, but the library ecosystem had\nbecome fragmented in the early 2000s. In 2005, Travis Oliphant\nwas able to forge the NumPy project from the then Numeric and\nNumarray projects to bring the community together around a sin‐\ngle array computing framework.\nOne of the reasons NumPy is so important for numerical computations in Python is\nbecause it is designed for efficiency on large arrays of data. There are a number of\nreasons for this:\n• NumPy internally stores data in a contiguous block of memory, independent of\n•\nother built-in Python objects. NumPy’s library of algorithms written in the C lan‐\nguage can operate on this memory without any type checking or other overhead.\nNumPy arrays also use much less memory than built-in Python sequences.\n84 \n| \nChapter 4: NumPy Basics: Arrays and Vectorized Computation",
      "content_length": 2298,
      "extraction_method": "Direct"
    },
    {
      "page_number": 103,
      "chapter": null,
      "content": "• NumPy operations perform complex computations on entire arrays without the\n•\nneed for Python for loops, which can be slow for large sequences. NumPy is\nfaster than regular Python code because its C-based algorithms avoid overhead\npresent with regular interpreted Python code.\nTo give you an idea of the performance difference, consider a NumPy array of one\nmillion integers, and the equivalent Python list:\nIn [7]: import numpy as np\nIn [8]: my_arr = np.arange(1_000_000)\nIn [9]: my_list = list(range(1_000_000))\nNow let’s multiply each sequence by 2:\nIn [10]: %timeit my_arr2 = my_arr * 2\n715 us +- 13.2 us per loop (mean +- std. dev. of 7 runs, 1000 loops each)\nIn [11]: %timeit my_list2 = [x * 2 for x in my_list]\n48.8 ms +- 298 us per loop (mean +- std. dev. of 7 runs, 10 loops each)\nNumPy-based algorithms are generally 10 to 100 times faster (or more) than their\npure Python counterparts and use significantly less memory.\n4.1 The NumPy ndarray: A Multidimensional Array Object\nOne of the key features of NumPy is its N-dimensional array object, or ndarray,\nwhich is a fast, flexible container for large datasets in Python. Arrays enable you to\nperform mathematical operations on whole blocks of data using similar syntax to the\nequivalent operations between scalar elements.\nTo give you a flavor of how NumPy enables batch computations with similar syntax\nto scalar values on built-in Python objects, I first import NumPy and create a small\narray:\nIn [12]: import numpy as np\nIn [13]: data = np.array([[1.5, -0.1, 3], [0, -3, 6.5]])\nIn [14]: data\nOut[14]: \narray([[ 1.5, -0.1,  3. ],\n       [ 0. , -3. ,  6.5]])\nI then write mathematical operations with data:\nIn [15]: data * 10\nOut[15]: \narray([[ 15.,  -1.,  30.],\n       [  0., -30.,  65.]])\n4.1 The NumPy ndarray: A Multidimensional Array Object \n| \n85",
      "content_length": 1815,
      "extraction_method": "Direct"
    },
    {
      "page_number": 104,
      "chapter": null,
      "content": "In [16]: data + data\nOut[16]: \narray([[ 3. , -0.2,  6. ],\n       [ 0. , -6. , 13. ]])\nIn the first example, all of the elements have been multiplied by 10. In the second, the\ncorresponding values in each “cell” in the array have been added to each other.\nIn this chapter and throughout the book, I use the standard\nNumPy convention of always using import numpy as np. It would\nbe possible to put from numpy import * in your code to avoid\nhaving to write np., but I advise against making a habit of this.\nThe numpy namespace is large and contains a number of functions\nwhose names conflict with built-in Python functions (like min and\nmax). Following standard conventions like these is almost always a\ngood idea.\nAn ndarray is a generic multidimensional container for homogeneous data; that is, all\nof the elements must be the same type. Every array has a shape, a tuple indicating the\nsize of each dimension, and a dtype, an object describing the data type of the array:\nIn [17]: data.shape\nOut[17]: (2, 3)\nIn [18]: data.dtype\nOut[18]: dtype('float64')\nThis chapter will introduce you to the basics of using NumPy arrays, and it should\nbe sufficient for following along with the rest of the book. While it’s not necessary to\nhave a deep understanding of NumPy for many data analytical applications, becom‐\ning proficient in array-oriented programming and thinking is a key step along the\nway to becoming a scientific Python guru.\nWhenever you see “array,” “NumPy array,” or “ndarray” in the book\ntext, in most cases they all refer to the ndarray object.\nCreating ndarrays\nThe easiest way to create an array is to use the array function. This accepts any\nsequence-like object (including other arrays) and produces a new NumPy array\ncontaining the passed data. For example, a list is a good candidate for conversion:\nIn [19]: data1 = [6, 7.5, 8, 0, 1]\n86 \n| \nChapter 4: NumPy Basics: Arrays and Vectorized Computation",
      "content_length": 1915,
      "extraction_method": "Direct"
    },
    {
      "page_number": 105,
      "chapter": null,
      "content": "In [20]: arr1 = np.array(data1)\nIn [21]: arr1\nOut[21]: array([6. , 7.5, 8. , 0. , 1. ])\nNested sequences, like a list of equal-length lists, will be converted into a multidimen‐\nsional array:\nIn [22]: data2 = [[1, 2, 3, 4], [5, 6, 7, 8]]\nIn [23]: arr2 = np.array(data2)\nIn [24]: arr2\nOut[24]: \narray([[1, 2, 3, 4],\n       [5, 6, 7, 8]])\nSince data2 was a list of lists, the NumPy array arr2 has two dimensions, with\nshape inferred from the data. We can confirm this by inspecting the ndim and shape\nattributes:\nIn [25]: arr2.ndim\nOut[25]: 2\nIn [26]: arr2.shape\nOut[26]: (2, 4)\nUnless explicitly specified (discussed in “Data Types for ndarrays” on page 88),\nnumpy.array tries to infer a good data type for the array that it creates. The data\ntype is stored in a special dtype metadata object; for example, in the previous two\nexamples we have:\nIn [27]: arr1.dtype\nOut[27]: dtype('float64')\nIn [28]: arr2.dtype\nOut[28]: dtype('int64')\nIn addition to numpy.array, there are a number of other functions for creating\nnew arrays. As examples, numpy.zeros and numpy.ones create arrays of 0s or 1s,\nrespectively, with a given length or shape. numpy.empty creates an array without\ninitializing its values to any particular value. To create a higher dimensional array\nwith these methods, pass a tuple for the shape:\nIn [29]: np.zeros(10)\nOut[29]: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\nIn [30]: np.zeros((3, 6))\nOut[30]: \narray([[0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.]])\n4.1 The NumPy ndarray: A Multidimensional Array Object \n| \n87",
      "content_length": 1583,
      "extraction_method": "Direct"
    },
    {
      "page_number": 106,
      "chapter": null,
      "content": "In [31]: np.empty((2, 3, 2))\nOut[31]: \narray([[[0., 0.],\n        [0., 0.],\n        [0., 0.]],\n       [[0., 0.],\n        [0., 0.],\n        [0., 0.]]])\nIt’s not safe to assume that numpy.empty will return an array of all\nzeros. This function returns uninitialized memory and thus may\ncontain nonzero “garbage” values. You should use this function\nonly if you intend to populate the new array with data.\nnumpy.arange is an array-valued version of the built-in Python range function:\nIn [32]: np.arange(15)\nOut[32]: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\nSee Table 4-1 for a short list of standard array creation functions. Since NumPy is\nfocused on numerical computing, the data type, if not specified, will in many cases be\nfloat64 (floating point).\nTable 4-1. Some important NumPy array creation functions\nFunction\nDescription\narray\nConvert input data (list, tuple, array, or other sequence type) to an ndarray either by inferring a data\ntype or explicitly specifying a data type; copies the input data by default\nasarray\nConvert input to ndarray, but do not copy if the input is already an ndarray\narange\nLike the built-in range but returns an ndarray instead of a list\nones, \nones_like\nProduce an array of all 1s with the given shape and data type; ones_like takes another array and\nproduces a ones array of the same shape and data type\nzeros, \nzeros_like\nLike ones and ones_like but producing arrays of 0s instead\nempty, \nempty_like\nCreate new arrays by allocating new memory, but do not populate with any values like ones and\nzeros\nfull, \nfull_like\nProduce an array of the given shape and data type with all values set to the indicated “fill value”;\nfull_like takes another array and produces a filled array of the same shape and data type\neye, identity\nCreate a square N × N identity matrix (1s on the diagonal and 0s elsewhere)\nData Types for ndarrays\nThe data type or dtype is a special object containing the information (or metadata,\ndata about data) the ndarray needs to interpret a chunk of memory as a particular\ntype of data:\n88 \n| \nChapter 4: NumPy Basics: Arrays and Vectorized Computation",
      "content_length": 2131,
      "extraction_method": "Direct"
    },
    {
      "page_number": 107,
      "chapter": null,
      "content": "In [33]: arr1 = np.array([1, 2, 3], dtype=np.float64)\nIn [34]: arr2 = np.array([1, 2, 3], dtype=np.int32)\nIn [35]: arr1.dtype\nOut[35]: dtype('float64')\nIn [36]: arr2.dtype\nOut[36]: dtype('int32')\nData types are a source of NumPy’s flexibility for interacting with data coming from\nother systems. In most cases they provide a mapping directly onto an underlying\ndisk or memory representation, which makes it possible to read and write binary\nstreams of data to disk and to connect to code written in a low-level language like\nC or FORTRAN. The numerical data types are named the same way: a type name,\nlike float or int, followed by a number indicating the number of bits per element.\nA standard double-precision floating-point value (what’s used under the hood in\nPython’s float object) takes up 8 bytes or 64 bits. Thus, this type is known in NumPy\nas float64. See Table 4-2 for a full listing of NumPy’s supported data types.\nDon’t worry about memorizing the NumPy data types, especially if\nyou’re a new user. It’s often only necessary to care about the general\nkind of data you’re dealing with, whether floating point, complex,\ninteger, Boolean, string, or general Python object. When you need\nmore control over how data is stored in memory and on disk,\nespecially large datasets, it is good to know that you have control\nover the storage type.\nTable 4-2. NumPy data types\nType\nType code\nDescription\nint8, uint8\ni1, u1\nSigned and unsigned 8-bit (1 byte) integer types\nint16, uint16\ni2, u2\nSigned and unsigned 16-bit integer types\nint32, uint32\ni4, u4\nSigned and unsigned 32-bit integer types\nint64, uint64\ni8, u8\nSigned and unsigned 64-bit integer types\nfloat16\nf2\nHalf-precision floating point\nfloat32\nf4 or f\nStandard single-precision floating point; compatible with C float\nfloat64\nf8 or d\nStandard double-precision floating point; compatible with C double and\nPython float object\nfloat128\nf16 or g\nExtended-precision floating point\ncomplex64,\ncomplex128,\ncomplex256\nc8, c16, \nc32\nComplex numbers represented by two 32, 64, or 128 floats, respectively\nbool\n?\nBoolean type storing True and False values\nobject\nO\nPython object type; a value can be any Python object\n4.1 The NumPy ndarray: A Multidimensional Array Object \n| \n89",
      "content_length": 2231,
      "extraction_method": "Direct"
    },
    {
      "page_number": 108,
      "chapter": null,
      "content": "Type\nType code\nDescription\nstring_\nS\nFixed-length ASCII string type (1 byte per character); for example, to create a\nstring data type with length 10, use 'S10'\nunicode_\nU\nFixed-length Unicode type (number of bytes platform specific); same\nspecification semantics as string_ (e.g., 'U10')\nThere are both signed and unsigned integer types, and many readers\nwill not be familiar with this terminology. A signed integer can\nrepresent both positive and negative integers, while an unsigned\ninteger can only represent nonzero integers. For example, int8\n(signed 8-bit integer) can represent integers from -128 to 127\n(inclusive), while uint8 (unsigned 8-bit integer) can represent 0\nthrough 255.\nYou can explicitly convert or cast an array from one data type to another using\nndarray’s astype method:\nIn [37]: arr = np.array([1, 2, 3, 4, 5])\nIn [38]: arr.dtype\nOut[38]: dtype('int64')\nIn [39]: float_arr = arr.astype(np.float64)\nIn [40]: float_arr\nOut[40]: array([1., 2., 3., 4., 5.])\nIn [41]: float_arr.dtype\nOut[41]: dtype('float64')\nIn this example, integers were cast to floating point. If I cast some floating-point\nnumbers to be of integer data type, the decimal part will be truncated:\nIn [42]: arr = np.array([3.7, -1.2, -2.6, 0.5, 12.9, 10.1])\nIn [43]: arr\nOut[43]: array([ 3.7, -1.2, -2.6,  0.5, 12.9, 10.1])\nIn [44]: arr.astype(np.int32)\nOut[44]: array([ 3, -1, -2,  0, 12, 10], dtype=int32)\nIf you have an array of strings representing numbers, you can use astype to convert\nthem to numeric form:\nIn [45]: numeric_strings = np.array([\"1.25\", \"-9.6\", \"42\"], dtype=np.string_)\nIn [46]: numeric_strings.astype(float)\nOut[46]: array([ 1.25, -9.6 , 42.  ])\n90 \n| \nChapter 4: NumPy Basics: Arrays and Vectorized Computation",
      "content_length": 1723,
      "extraction_method": "Direct"
    },
    {
      "page_number": 109,
      "chapter": null,
      "content": "Be cautious when using the numpy.string_ type, as string data in\nNumPy is fixed size and may truncate input without warning. pan‐\ndas has more intuitive out-of-the-box behavior on non-numeric\ndata.\nIf casting were to fail for some reason (like a string that cannot be converted to\nfloat64), a ValueError will be raised. Before, I was a bit lazy and wrote float\ninstead of np.float64; NumPy aliases the Python types to its own equivalent data\ntypes.\nYou can also use another array’s dtype attribute:\nIn [47]: int_array = np.arange(10)\nIn [48]: calibers = np.array([.22, .270, .357, .380, .44, .50], dtype=np.float64)\nIn [49]: int_array.astype(calibers.dtype)\nOut[49]: array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\nThere are shorthand type code strings you can also use to refer to a dtype:\nIn [50]: zeros_uint32 = np.zeros(8, dtype=\"u4\")\nIn [51]: zeros_uint32\nOut[51]: array([0, 0, 0, 0, 0, 0, 0, 0], dtype=uint32)\nCalling astype always creates a new array (a copy of the data), even\nif the new data type is the same as the old data type.\nArithmetic with NumPy Arrays\nArrays are important because they enable you to express batch operations on data\nwithout writing any for loops. NumPy users call this vectorization. Any arithmetic\noperations between equal-size arrays apply the operation element-wise:\nIn [52]: arr = np.array([[1., 2., 3.], [4., 5., 6.]])\nIn [53]: arr\nOut[53]: \narray([[1., 2., 3.],\n       [4., 5., 6.]])\nIn [54]: arr * arr\nOut[54]: \narray([[ 1.,  4.,  9.],\n       [16., 25., 36.]])\n4.1 The NumPy ndarray: A Multidimensional Array Object \n| \n91",
      "content_length": 1563,
      "extraction_method": "Direct"
    },
    {
      "page_number": 110,
      "chapter": null,
      "content": "In [55]: arr - arr\nOut[55]: \narray([[0., 0., 0.],\n       [0., 0., 0.]])\nArithmetic operations with scalars propagate the scalar argument to each element in\nthe array:\nIn [56]: 1 / arr\nOut[56]: \narray([[1.    , 0.5   , 0.3333],\n       [0.25  , 0.2   , 0.1667]])\nIn [57]: arr ** 2\nOut[57]: \narray([[ 1.,  4.,  9.],\n       [16., 25., 36.]])\nComparisons between arrays of the same size yield Boolean arrays:\nIn [58]: arr2 = np.array([[0., 4., 1.], [7., 2., 12.]])\nIn [59]: arr2\nOut[59]: \narray([[ 0.,  4.,  1.],\n       [ 7.,  2., 12.]])\nIn [60]: arr2 > arr\nOut[60]: \narray([[False,  True, False],\n       [ True, False,  True]])\nEvaluating operations between differently sized arrays is called broadcasting and\nwill be discussed in more detail in Appendix A. Having a deep understanding of\nbroadcasting is not necessary for most of this book.\nBasic Indexing and Slicing\nNumPy array indexing is a deep topic, as there are many ways you may want to select\na subset of your data or individual elements. One-dimensional arrays are simple; on\nthe surface they act similarly to Python lists:\nIn [61]: arr = np.arange(10)\nIn [62]: arr\nOut[62]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\nIn [63]: arr[5]\nOut[63]: 5\nIn [64]: arr[5:8]\nOut[64]: array([5, 6, 7])\n92 \n| \nChapter 4: NumPy Basics: Arrays and Vectorized Computation",
      "content_length": 1306,
      "extraction_method": "Direct"
    },
    {
      "page_number": 111,
      "chapter": null,
      "content": "In [65]: arr[5:8] = 12\nIn [66]: arr\nOut[66]: array([ 0,  1,  2,  3,  4, 12, 12, 12,  8,  9])\nAs you can see, if you assign a scalar value to a slice, as in arr[5:8] = 12, the value is\npropagated (or broadcast henceforth) to the entire selection.\nAn important first distinction from Python’s built-in lists is that\narray slices are views on the original array. This means that the data\nis not copied, and any modifications to the view will be reflected in\nthe source array.\nTo give an example of this, I first create a slice of arr:\nIn [67]: arr_slice = arr[5:8]\nIn [68]: arr_slice\nOut[68]: array([12, 12, 12])\nNow, when I change values in arr_slice, the mutations are reflected in the original\narray arr:\nIn [69]: arr_slice[1] = 12345\nIn [70]: arr\nOut[70]: \narray([    0,     1,     2,     3,     4,    12, 12345,    12,     8,\n           9])\nThe “bare” slice [:] will assign to all values in an array:\nIn [71]: arr_slice[:] = 64\nIn [72]: arr\nOut[72]: array([ 0,  1,  2,  3,  4, 64, 64, 64,  8,  9])\nIf you are new to NumPy, you might be surprised by this, especially if you have used\nother array programming languages that copy data more eagerly. As NumPy has been\ndesigned to be able to work with very large arrays, you could imagine performance\nand memory problems if NumPy insisted on always copying data.\nIf you want a copy of a slice of an ndarray instead of a\nview, you will need to explicitly copy the array—for example,\narr[5:8].copy(). As you will see, pandas works this way, too.\n4.1 The NumPy ndarray: A Multidimensional Array Object \n| \n93",
      "content_length": 1552,
      "extraction_method": "Direct"
    },
    {
      "page_number": 112,
      "chapter": null,
      "content": "With higher dimensional arrays, you have many more options. In a two-dimensional\narray, the elements at each index are no longer scalars but rather one-dimensional\narrays:\nIn [73]: arr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nIn [74]: arr2d[2]\nOut[74]: array([7, 8, 9])\nThus, individual elements can be accessed recursively. But that is a bit too much\nwork, so you can pass a comma-separated list of indices to select individual elements.\nSo these are equivalent:\nIn [75]: arr2d[0][2]\nOut[75]: 3\nIn [76]: arr2d[0, 2]\nOut[76]: 3\nSee Figure 4-1 for an illustration of indexing on a two-dimensional array. I find it\nhelpful to think of axis 0 as the “rows” of the array and axis 1 as the “columns.”\nFigure 4-1. Indexing elements in a NumPy array\nIn multidimensional arrays, if you omit later indices, the returned object will be a\nlower dimensional ndarray consisting of all the data along the higher dimensions. So\nin the 2 × 2 × 3 array arr3d:\nIn [77]: arr3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\nIn [78]: arr3d\nOut[78]: \narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n       [[ 7,  8,  9],\n        [10, 11, 12]]])\narr3d[0] is a 2 × 3 array:\n94 \n| \nChapter 4: NumPy Basics: Arrays and Vectorized Computation",
      "content_length": 1237,
      "extraction_method": "Direct"
    },
    {
      "page_number": 113,
      "chapter": null,
      "content": "In [79]: arr3d[0]\nOut[79]: \narray([[1, 2, 3],\n       [4, 5, 6]])\nBoth scalar values and arrays can be assigned to arr3d[0]:\nIn [80]: old_values = arr3d[0].copy()\nIn [81]: arr3d[0] = 42\nIn [82]: arr3d\nOut[82]: \narray([[[42, 42, 42],\n        [42, 42, 42]],\n       [[ 7,  8,  9],\n        [10, 11, 12]]])\nIn [83]: arr3d[0] = old_values\nIn [84]: arr3d\nOut[84]: \narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n       [[ 7,  8,  9],\n        [10, 11, 12]]])\nSimilarly, arr3d[1, 0] gives you all of the values whose indices start with (1, 0),\nforming a one-dimensional array:\nIn [85]: arr3d[1, 0]\nOut[85]: array([7, 8, 9])\nThis expression is the same as though we had indexed in two steps:\nIn [86]: x = arr3d[1]\nIn [87]: x\nOut[87]: \narray([[ 7,  8,  9],\n       [10, 11, 12]])\nIn [88]: x[0]\nOut[88]: array([7, 8, 9])\nNote that in all of these cases where subsections of the array have been selected, the\nreturned arrays are views.\nThis multidimensional indexing syntax for NumPy arrays will not\nwork with regular Python objects, such as lists of lists.\n4.1 The NumPy ndarray: A Multidimensional Array Object \n| \n95",
      "content_length": 1102,
      "extraction_method": "Direct"
    },
    {
      "page_number": 114,
      "chapter": null,
      "content": "Indexing with slices\nLike one-dimensional objects such as Python lists, ndarrays can be sliced with the\nfamiliar syntax:\nIn [89]: arr\nOut[89]: array([ 0,  1,  2,  3,  4, 64, 64, 64,  8,  9])\nIn [90]: arr[1:6]\nOut[90]: array([ 1,  2,  3,  4, 64])\nConsider the two-dimensional array from before, arr2d. Slicing this array is a bit\ndifferent:\nIn [91]: arr2d\nOut[91]: \narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\nIn [92]: arr2d[:2]\nOut[92]: \narray([[1, 2, 3],\n       [4, 5, 6]])\nAs you can see, it has sliced along axis 0, the first axis. A slice, therefore, selects a\nrange of elements along an axis. It can be helpful to read the expression arr2d[:2] as\n“select the first two rows of arr2d.”\nYou can pass multiple slices just like you can pass multiple indexes:\nIn [93]: arr2d[:2, 1:]\nOut[93]: \narray([[2, 3],\n       [5, 6]])\nWhen slicing like this, you always obtain array views of the same number of dimen‐\nsions. By mixing integer indexes and slices, you get lower dimensional slices.\nFor example, I can select the second row but only the first two columns, like so:\nIn [94]: lower_dim_slice = arr2d[1, :2]\nHere, while arr2d is two-dimensional, lower_dim_slice is one-dimensional, and its\nshape is a tuple with one axis size:\nIn [95]: lower_dim_slice.shape\nOut[95]: (2,)\nSimilarly, I can select the third column but only the first two rows, like so:\nIn [96]: arr2d[:2, 2]\nOut[96]: array([3, 6])\n96 \n| \nChapter 4: NumPy Basics: Arrays and Vectorized Computation",
      "content_length": 1472,
      "extraction_method": "Direct"
    },
    {
      "page_number": 115,
      "chapter": null,
      "content": "See Figure 4-2 for an illustration. Note that a colon by itself means to take the entire\naxis, so you can slice only higher dimensional axes by doing:\nIn [97]: arr2d[:, :1]\nOut[97]: \narray([[1],\n       [4],\n       [7]])\nOf course, assigning to a slice expression assigns to the whole selection:\nIn [98]: arr2d[:2, 1:] = 0\nIn [99]: arr2d\nOut[99]: \narray([[1, 0, 0],\n       [4, 0, 0],\n       [7, 8, 9]])\nFigure 4-2. Two-dimensional array slicing\nBoolean Indexing\nLet’s consider an example where we have some data in an array and an array of names\nwith duplicates:\n4.1 The NumPy ndarray: A Multidimensional Array Object \n| \n97",
      "content_length": 623,
      "extraction_method": "Direct"
    },
    {
      "page_number": 116,
      "chapter": null,
      "content": "In [100]: names = np.array([\"Bob\", \"Joe\", \"Will\", \"Bob\", \"Will\", \"Joe\", \"Joe\"])\nIn [101]: data = np.array([[4, 7], [0, 2], [-5, 6], [0, 0], [1, 2],\n   .....:                  [-12, -4], [3, 4]])\nIn [102]: names\nOut[102]: array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'], dtype='<U4')\nIn [103]: data\nOut[103]: \narray([[  4,   7],\n       [  0,   2],\n       [ -5,   6],\n       [  0,   0],\n       [  1,   2],\n       [-12,  -4],\n       [  3,   4]])\nSuppose each name corresponds to a row in the data array and we wanted to\nselect all the rows with the corresponding name \"Bob\". Like arithmetic operations,\ncomparisons (such as ==) with arrays are also vectorized. Thus, comparing names\nwith the string \"Bob\" yields a Boolean array:\nIn [104]: names == \"Bob\"\nOut[104]: array([ True, False, False,  True, False, False, False])\nThis Boolean array can be passed when indexing the array:\nIn [105]: data[names == \"Bob\"]\nOut[105]: \narray([[4, 7],\n       [0, 0]])\nThe Boolean array must be of the same length as the array axis it’s indexing. You can\neven mix and match Boolean arrays with slices or integers (or sequences of integers;\nmore on this later).\nIn these examples, I select from the rows where names == \"Bob\" and index the\ncolumns, too:\nIn [106]: data[names == \"Bob\", 1:]\nOut[106]: \narray([[7],\n       [0]])\nIn [107]: data[names == \"Bob\", 1]\nOut[107]: array([7, 0])\nTo select everything but \"Bob\" you can either use != or negate the condition using ~:\nIn [108]: names != \"Bob\"\nOut[108]: array([False,  True,  True, False,  True,  True,  True])\n98 \n| \nChapter 4: NumPy Basics: Arrays and Vectorized Computation",
      "content_length": 1614,
      "extraction_method": "Direct"
    },
    {
      "page_number": 117,
      "chapter": null,
      "content": "In [109]: ~(names == \"Bob\")\nOut[109]: array([False,  True,  True, False,  True,  True,  True])\nIn [110]: data[~(names == \"Bob\")]\nOut[110]: \narray([[  0,   2],\n       [ -5,   6],\n       [  1,   2],\n       [-12,  -4],\n       [  3,   4]])\nThe ~ operator can be useful when you want to invert a Boolean array referenced by a\nvariable:\nIn [111]: cond = names == \"Bob\"\nIn [112]: data[~cond]\nOut[112]: \narray([[  0,   2],\n       [ -5,   6],\n       [  1,   2],\n       [-12,  -4],\n       [  3,   4]])\nTo select two of the three names to combine multiple Boolean conditions, use\nBoolean arithmetic operators like & (and) and | (or):\nIn [113]: mask = (names == \"Bob\") | (names == \"Will\")\nIn [114]: mask\nOut[114]: array([ True, False,  True,  True,  True, False, False])\nIn [115]: data[mask]\nOut[115]: \narray([[ 4,  7],\n       [-5,  6],\n       [ 0,  0],\n       [ 1,  2]])\nSelecting data from an array by Boolean indexing and assigning the result to a new\nvariable always creates a copy of the data, even if the returned array is unchanged.\nThe Python keywords and and or do not work with Boolean arrays.\nUse & (and) and | (or) instead.\nSetting values with Boolean arrays works by substituting the value or values on the\nrighthand side into the locations where the Boolean array’s values are True. To set all\nof the negative values in data to 0, we need only do:\n4.1 The NumPy ndarray: A Multidimensional Array Object \n| \n99",
      "content_length": 1411,
      "extraction_method": "Direct"
    },
    {
      "page_number": 118,
      "chapter": null,
      "content": "In [116]: data[data < 0] = 0\nIn [117]: data\nOut[117]: \narray([[4, 7],\n       [0, 2],\n       [0, 6],\n       [0, 0],\n       [1, 2],\n       [0, 0],\n       [3, 4]])\nYou can also set whole rows or columns using a one-dimensional Boolean array:\nIn [118]: data[names != \"Joe\"] = 7\nIn [119]: data\nOut[119]: \narray([[7, 7],\n       [0, 2],\n       [7, 7],\n       [7, 7],\n       [7, 7],\n       [0, 0],\n       [3, 4]])\nAs we will see later, these types of operations on two-dimensional data are convenient\nto do with pandas.\nFancy Indexing\nFancy indexing is a term adopted by NumPy to describe indexing using integer arrays.\nSuppose we had an 8 × 4 array:\nIn [120]: arr = np.zeros((8, 4))\nIn [121]: for i in range(8):\n   .....:     arr[i] = i\nIn [122]: arr\nOut[122]: \narray([[0., 0., 0., 0.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [4., 4., 4., 4.],\n       [5., 5., 5., 5.],\n       [6., 6., 6., 6.],\n       [7., 7., 7., 7.]])\nTo select a subset of the rows in a particular order, you can simply pass a list or\nndarray of integers specifying the desired order:\n100 \n| \nChapter 4: NumPy Basics: Arrays and Vectorized Computation",
      "content_length": 1156,
      "extraction_method": "Direct"
    },
    {
      "page_number": 119,
      "chapter": null,
      "content": "In [123]: arr[[4, 3, 0, 6]]\nOut[123]: \narray([[4., 4., 4., 4.],\n       [3., 3., 3., 3.],\n       [0., 0., 0., 0.],\n       [6., 6., 6., 6.]])\nHopefully this code did what you expected! Using negative indices selects rows from\nthe end:\nIn [124]: arr[[-3, -5, -7]]\nOut[124]: \narray([[5., 5., 5., 5.],\n       [3., 3., 3., 3.],\n       [1., 1., 1., 1.]])\nPassing multiple index arrays does something slightly different; it selects a one-\ndimensional array of elements corresponding to each tuple of indices:\nIn [125]: arr = np.arange(32).reshape((8, 4))\nIn [126]: arr\nOut[126]: \narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12, 13, 14, 15],\n       [16, 17, 18, 19],\n       [20, 21, 22, 23],\n       [24, 25, 26, 27],\n       [28, 29, 30, 31]])\nIn [127]: arr[[1, 5, 7, 2], [0, 3, 1, 2]]\nOut[127]: array([ 4, 23, 29, 10])\nTo learn more about the reshape method, have a look at Appendix A.\nHere the elements (1, 0), (5, 3), (7, 1), and (2, 2) were selected. The\nresult of fancy indexing with as many integer arrays as there are axes is always\none-dimensional.\nThe behavior of fancy indexing in this case is a bit different from what some users\nmight have expected (myself included), which is the rectangular region formed by\nselecting a subset of the matrix’s rows and columns. Here is one way to get that:\nIn [128]: arr[[1, 5, 7, 2]][:, [0, 3, 1, 2]]\nOut[128]: \narray([[ 4,  7,  5,  6],\n       [20, 23, 21, 22],\n       [28, 31, 29, 30],\n       [ 8, 11,  9, 10]])\n4.1 The NumPy ndarray: A Multidimensional Array Object \n| \n101",
      "content_length": 1552,
      "extraction_method": "Direct"
    },
    {
      "page_number": 120,
      "chapter": null,
      "content": "Keep in mind that fancy indexing, unlike slicing, always copies the data into a new\narray when assigning the result to a new variable. If you assign values with fancy\nindexing, the indexed values will be modified:\nIn [129]: arr[[1, 5, 7, 2], [0, 3, 1, 2]]\nOut[129]: array([ 4, 23, 29, 10])\nIn [130]: arr[[1, 5, 7, 2], [0, 3, 1, 2]] = 0\nIn [131]: arr\nOut[131]: \narray([[ 0,  1,  2,  3],\n       [ 0,  5,  6,  7],\n       [ 8,  9,  0, 11],\n       [12, 13, 14, 15],\n       [16, 17, 18, 19],\n       [20, 21, 22,  0],\n       [24, 25, 26, 27],\n       [28,  0, 30, 31]])\nTransposing Arrays and Swapping Axes\nTransposing is a special form of reshaping that similarly returns a view on the\nunderlying data without copying anything. Arrays have the transpose method and\nthe special T attribute:\nIn [132]: arr = np.arange(15).reshape((3, 5))\nIn [133]: arr\nOut[133]: \narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])\nIn [134]: arr.T\nOut[134]: \narray([[ 0,  5, 10],\n       [ 1,  6, 11],\n       [ 2,  7, 12],\n       [ 3,  8, 13],\n       [ 4,  9, 14]])\nWhen doing matrix computations, you may do this very often—for example, when\ncomputing the inner matrix product using numpy.dot:\nIn [135]: arr = np.array([[0, 1, 0], [1, 2, -2], [6, 3, 2], [-1, 0, -1], [1, 0, 1\n]])\nIn [136]: arr\nOut[136]: \narray([[ 0,  1,  0],\n       [ 1,  2, -2],\n102 \n| \nChapter 4: NumPy Basics: Arrays and Vectorized Computation",
      "content_length": 1423,
      "extraction_method": "Direct"
    },
    {
      "page_number": 121,
      "chapter": null,
      "content": "[ 6,  3,  2],\n       [-1,  0, -1],\n       [ 1,  0,  1]])\nIn [137]: np.dot(arr.T, arr)\nOut[137]: \narray([[39, 20, 12],\n       [20, 14,  2],\n       [12,  2, 10]])\nThe @ infix operator is another way to do matrix multiplication:\nIn [138]: arr.T @ arr\nOut[138]: \narray([[39, 20, 12],\n       [20, 14,  2],\n       [12,  2, 10]])\nSimple transposing with .T is a special case of swapping axes. ndarray has the method\nswapaxes, which takes a pair of axis numbers and switches the indicated axes to\nrearrange the data:\nIn [139]: arr\nOut[139]: \narray([[ 0,  1,  0],\n       [ 1,  2, -2],\n       [ 6,  3,  2],\n       [-1,  0, -1],\n       [ 1,  0,  1]])\nIn [140]: arr.swapaxes(0, 1)\nOut[140]: \narray([[ 0,  1,  6, -1,  1],\n       [ 1,  2,  3,  0,  0],\n       [ 0, -2,  2, -1,  1]])\nswapaxes similarly returns a view on the data without making a copy.\n4.2 Pseudorandom Number Generation\nThe numpy.random module supplements the built-in Python random module with\nfunctions for efficiently generating whole arrays of sample values from many kinds of\nprobability distributions. For example, you can get a 4 × 4 array of samples from the\nstandard normal distribution using numpy.random.standard_normal:\nIn [141]: samples = np.random.standard_normal(size=(4, 4))\nIn [142]: samples\nOut[142]: \narray([[-0.2047,  0.4789, -0.5194, -0.5557],\n       [ 1.9658,  1.3934,  0.0929,  0.2817],\n4.2 Pseudorandom Number Generation \n| \n103",
      "content_length": 1404,
      "extraction_method": "Direct"
    },
    {
      "page_number": 122,
      "chapter": null,
      "content": "[ 0.769 ,  1.2464,  1.0072, -1.2962],\n       [ 0.275 ,  0.2289,  1.3529,  0.8864]])\nPython’s built-in random module, by contrast, samples only one value at a time. As\nyou can see from this benchmark, numpy.random is well over an order of magnitude\nfaster for generating very large samples:\nIn [143]: from random import normalvariate\nIn [144]: N = 1_000_000\nIn [145]: %timeit samples = [normalvariate(0, 1) for _ in range(N)]\n1.04 s +- 11.4 ms per loop (mean +- std. dev. of 7 runs, 1 loop each)\nIn [146]: %timeit np.random.standard_normal(N)\n21.9 ms +- 155 us per loop (mean +- std. dev. of 7 runs, 10 loops each)\nThese random numbers are not truly random (rather, pseudorandom) but instead\nare generated by a configurable random number generator that determines determin‐\nistically what values are created. Functions like numpy.random.standard_normal use\nthe numpy.random module’s default random number generator, but your code can be\nconfigured to use an explicit generator:\nIn [147]: rng = np.random.default_rng(seed=12345)\nIn [148]: data = rng.standard_normal((2, 3))\nThe seed argument is what determines the initial state of the generator, and the state\nchanges each time the rng object is used to generate data. The generator object rng is\nalso isolated from other code which might use the numpy.random module:\nIn [149]: type(rng)\nOut[149]: numpy.random._generator.Generator\nSee Table 4-3 for a partial list of methods available on random generator objects like\nrng. I will use the rng object I created above to generate random data throughout the\nrest of the chapter.\nTable 4-3. NumPy random number generator methods\nMethod\nDescription\npermutation\nReturn a random permutation of a sequence, or return a permuted range\nshuffle\nRandomly permute a sequence in place\nuniform\nDraw samples from a uniform distribution\nintegers\nDraw random integers from a given low-to-high range\nstandard_normal Draw samples from a normal distribution with mean 0 and standard deviation 1\nbinomial\nDraw samples from a binomial distribution\nnormal\nDraw samples from a normal (Gaussian) distribution\nbeta\nDraw samples from a beta distribution\n104 \n| \nChapter 4: NumPy Basics: Arrays and Vectorized Computation",
      "content_length": 2191,
      "extraction_method": "Direct"
    },
    {
      "page_number": 123,
      "chapter": null,
      "content": "Method\nDescription\nchisquare\nDraw samples from a chi-square distribution\ngamma\nDraw samples from a gamma distribution\nuniform\nDraw samples from a uniform [0, 1) distribution\n4.3 Universal Functions: Fast Element-Wise Array\nFunctions\nA universal function, or ufunc, is a function that performs element-wise operations\non data in ndarrays. You can think of them as fast vectorized wrappers for simple\nfunctions that take one or more scalar values and produce one or more scalar results.\nMany ufuncs are simple element-wise transformations, like numpy.sqrt or\nnumpy.exp:\nIn [150]: arr = np.arange(10)\nIn [151]: arr\nOut[151]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\nIn [152]: np.sqrt(arr)\nOut[152]: \narray([0.    , 1.    , 1.4142, 1.7321, 2.    , 2.2361, 2.4495, 2.6458,\n       2.8284, 3.    ])\nIn [153]: np.exp(arr)\nOut[153]: \narray([   1.    ,    2.7183,    7.3891,   20.0855,   54.5982,  148.4132,\n        403.4288, 1096.6332, 2980.958 , 8103.0839])\nThese are referred to as unary ufuncs. Others, such as numpy.add or numpy.maximum,\ntake two arrays (thus, binary ufuncs) and return a single array as the result:\nIn [154]: x = rng.standard_normal(8)\nIn [155]: y = rng.standard_normal(8)\nIn [156]: x\nOut[156]: \narray([-1.3678,  0.6489,  0.3611, -1.9529,  2.3474,  0.9685, -0.7594,\n        0.9022])\nIn [157]: y\nOut[157]: \narray([-0.467 , -0.0607,  0.7888, -1.2567,  0.5759,  1.399 ,  1.3223,\n       -0.2997])\nIn [158]: np.maximum(x, y)\nOut[158]: \n4.3 Universal Functions: Fast Element-Wise Array Functions \n| \n105",
      "content_length": 1507,
      "extraction_method": "Direct"
    },
    {
      "page_number": 124,
      "chapter": null,
      "content": "array([-0.467 ,  0.6489,  0.7888, -1.2567,  2.3474,  1.399 ,  1.3223,\n        0.9022])\nIn this example, numpy.maximum computed the element-wise maximum of the ele‐\nments in x and y.\nWhile not common, a ufunc can return multiple arrays. numpy.modf is one example:\na vectorized version of the built-in Python math.modf, it returns the fractional and\nintegral parts of a floating-point array:\nIn [159]: arr = rng.standard_normal(7) * 5\nIn [160]: arr\nOut[160]: array([ 4.5146, -8.1079, -0.7909,  2.2474, -6.718 , -0.4084,  8.6237])\nIn [161]: remainder, whole_part = np.modf(arr)\nIn [162]: remainder\nOut[162]: array([ 0.5146, -0.1079, -0.7909,  0.2474, -0.718 , -0.4084,  0.6237])\nIn [163]: whole_part\nOut[163]: array([ 4., -8., -0.,  2., -6., -0.,  8.])\nUfuncs accept an optional out argument that allows them to assign their results into\nan existing array rather than create a new one:\nIn [164]: arr\nOut[164]: array([ 4.5146, -8.1079, -0.7909,  2.2474, -6.718 , -0.4084,  8.6237])\nIn [165]: out = np.zeros_like(arr)\nIn [166]: np.add(arr, 1)\nOut[166]: array([ 5.5146, -7.1079,  0.2091,  3.2474, -5.718 ,  0.5916,  9.6237])\nIn [167]: np.add(arr, 1, out=out)\nOut[167]: array([ 5.5146, -7.1079,  0.2091,  3.2474, -5.718 ,  0.5916,  9.6237])\nIn [168]: out\nOut[168]: array([ 5.5146, -7.1079,  0.2091,  3.2474, -5.718 ,  0.5916,  9.6237])\nSee Tables 4-4 and 4-5 for a listing of some of NumPy’s ufuncs. New ufuncs continue\nto be added to NumPy, so consulting the online NumPy documentation is the best\nway to get a comprehensive listing and stay up to date.\n106 \n| \nChapter 4: NumPy Basics: Arrays and Vectorized Computation",
      "content_length": 1614,
      "extraction_method": "Direct"
    },
    {
      "page_number": 125,
      "chapter": null,
      "content": "Table 4-4. Some unary universal functions\nFunction\nDescription\nabs, fabs\nCompute the absolute value element-wise for integer, floating-point, or complex values\nsqrt\nCompute the square root of each element (equivalent to arr ** 0.5)\nsquare\nCompute the square of each element (equivalent to arr ** 2)\nexp\nCompute the exponent ex of each element\nlog, log10, \nlog2, log1p\nNatural logarithm (base e), log base 10, log base 2, and log(1 + x), respectively\nsign\nCompute the sign of each element: 1 (positive), 0 (zero), or –1 (negative)\nceil\nCompute the ceiling of each element (i.e., the smallest integer greater than or equal to that\nnumber)\nfloor\nCompute the floor of each element (i.e., the largest integer less than or equal to each element)\nrint\nRound elements to the nearest integer, preserving the dtype\nmodf\nReturn fractional and integral parts of array as separate arrays\nisnan\nReturn Boolean array indicating whether each value is NaN (Not a Number)\nisfinite, isinf\nReturn Boolean array indicating whether each element is finite (non-inf, non-NaN) or infinite,\nrespectively\ncos, cosh, sin, \nsinh, tan, tanh\nRegular and hyperbolic trigonometric functions\narccos, arccosh, \narcsin, arcsinh, \narctan, arctanh\nInverse trigonometric functions\nlogical_not\nCompute truth value of not x element-wise (equivalent to ~arr)\nTable 4-5. Some binary universal functions\nFunction\nDescription\nadd\nAdd corresponding elements in arrays\nsubtract\nSubtract elements in second array from first array\nmultiply\nMultiply array elements\ndivide, floor_divide\nDivide or floor divide (truncating the remainder)\npower\nRaise elements in first array to powers indicated in second array\nmaximum, fmax\nElement-wise maximum; fmax ignores NaN\nminimum, fmin\nElement-wise minimum; fmin ignores NaN\nmod\nElement-wise modulus (remainder of division)\ncopysign\nCopy sign of values in second argument to values in first argument\ngreater, \ngreater_equal, less, \nless_equal, equal, \nnot_equal\nPerform element-wise comparison, yielding Boolean array (equivalent to infix operators\n>, >=, <, <=, ==, !=)\nlogical_and\nCompute element-wise truth value of AND (&) logical operation\nlogical_or\nCompute element-wise truth value of OR (|) logical operation\nlogical_xor\nCompute element-wise truth value of XOR (^) logical operation\n4.3 Universal Functions: Fast Element-Wise Array Functions \n| \n107",
      "content_length": 2346,
      "extraction_method": "Direct"
    },
    {
      "page_number": 126,
      "chapter": null,
      "content": "4.4 Array-Oriented Programming with Arrays\nUsing NumPy arrays enables you to express many kinds of data processing tasks as\nconcise array expressions that might otherwise require writing loops. This practice\nof replacing explicit loops with array expressions is referred to by some people\nas vectorization. In general, vectorized array operations will usually be significantly\nfaster than their pure Python equivalents, with the biggest impact in any kind of\nnumerical computations. Later, in Appendix A, I explain broadcasting, a powerful\nmethod for vectorizing computations.\nAs a simple example, suppose we wished to evaluate the function sqrt(x^2 +\ny^2) across a regular grid of values. The numpy.meshgrid function takes two one-\ndimensional arrays and produces two two-dimensional matrices corresponding to all\npairs of (x, y) in the two arrays:\nIn [169]: points = np.arange(-5, 5, 0.01) # 100 equally spaced points\nIn [170]: xs, ys = np.meshgrid(points, points)\nIn [171]: ys\nOut[171]: \narray([[-5.  , -5.  , -5.  , ..., -5.  , -5.  , -5.  ],\n       [-4.99, -4.99, -4.99, ..., -4.99, -4.99, -4.99],\n       [-4.98, -4.98, -4.98, ..., -4.98, -4.98, -4.98],\n       ...,\n       [ 4.97,  4.97,  4.97, ...,  4.97,  4.97,  4.97],\n       [ 4.98,  4.98,  4.98, ...,  4.98,  4.98,  4.98],\n       [ 4.99,  4.99,  4.99, ...,  4.99,  4.99,  4.99]])\nNow, evaluating the function is a matter of writing the same expression you would\nwrite with two points:\nIn [172]: z = np.sqrt(xs ** 2 + ys ** 2)\nIn [173]: z\nOut[173]: \narray([[7.0711, 7.064 , 7.0569, ..., 7.0499, 7.0569, 7.064 ],\n       [7.064 , 7.0569, 7.0499, ..., 7.0428, 7.0499, 7.0569],\n       [7.0569, 7.0499, 7.0428, ..., 7.0357, 7.0428, 7.0499],\n       ...,\n       [7.0499, 7.0428, 7.0357, ..., 7.0286, 7.0357, 7.0428],\n       [7.0569, 7.0499, 7.0428, ..., 7.0357, 7.0428, 7.0499],\n       [7.064 , 7.0569, 7.0499, ..., 7.0428, 7.0499, 7.0569]])\nAs a preview of Chapter 9, I use matplotlib to create visualizations of this two-\ndimensional array:\nIn [174]: import matplotlib.pyplot as plt\nIn [175]: plt.imshow(z, cmap=plt.cm.gray, extent=[-5, 5, -5, 5])\nOut[175]: <matplotlib.image.AxesImage at 0x7f624ae73b20>\n108 \n| \nChapter 4: NumPy Basics: Arrays and Vectorized Computation",
      "content_length": 2225,
      "extraction_method": "Direct"
    },
    {
      "page_number": 127,
      "chapter": null,
      "content": "In [176]: plt.colorbar()\nOut[176]: <matplotlib.colorbar.Colorbar at 0x7f6253e43ee0>\nIn [177]: plt.title(\"Image plot of $\\sqrt{x^2 + y^2}$ for a grid of values\")\nOut[177]: Text(0.5, 1.0, 'Image plot of $\\\\sqrt{x^2 + y^2}$ for a grid of values'\n)\nIn Figure 4-3, I used the matplotlib function imshow to create an image plot from a\ntwo-dimensional array of function values.\nFigure 4-3. Plot of function evaluated on a grid\nIf you’re working in IPython, you can close all open plot windows by executing\nplt.close(\"all\"):\nIn [179]: plt.close(\"all\")\n4.4 Array-Oriented Programming with Arrays \n| \n109",
      "content_length": 594,
      "extraction_method": "Direct"
    },
    {
      "page_number": 128,
      "chapter": null,
      "content": "The term vectorization is used to describe some other computer\nscience concepts, but in this book I use it to describe operations on\nwhole arrays of data at once rather than going value by value using\na Python for loop.\nExpressing Conditional Logic as Array Operations\nThe numpy.where function is a vectorized version of the ternary expression x if\ncondition else y. Suppose we had a Boolean array and two arrays of values:\nIn [180]: xarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])\nIn [181]: yarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])\nIn [182]: cond = np.array([True, False, True, True, False])\nSuppose we wanted to take a value from xarr whenever the corresponding value in\ncond is True, and otherwise take the value from yarr. A list comprehension doing\nthis might look like:\nIn [183]: result = [(x if c else y)\n   .....:           for x, y, c in zip(xarr, yarr, cond)]\nIn [184]: result\nOut[184]: [1.1, 2.2, 1.3, 1.4, 2.5]\nThis has multiple problems. First, it will not be very fast for large arrays (because all\nthe work is being done in interpreted Python code). Second, it will not work with\nmultidimensional arrays. With numpy.where you can do this with a single function\ncall:\nIn [185]: result = np.where(cond, xarr, yarr)\nIn [186]: result\nOut[186]: array([1.1, 2.2, 1.3, 1.4, 2.5])\nThe second and third arguments to numpy.where don’t need to be arrays; one or\nboth of them can be scalars. A typical use of where in data analysis is to produce a\nnew array of values based on another array. Suppose you had a matrix of randomly\ngenerated data and you wanted to replace all positive values with 2 and all negative\nvalues with –2. This is possible to do with numpy.where:\nIn [187]: arr = rng.standard_normal((4, 4))\nIn [188]: arr\nOut[188]: \narray([[ 2.6182,  0.7774,  0.8286, -0.959 ],\n       [-1.2094, -1.4123,  0.5415,  0.7519],\n       [-0.6588, -1.2287,  0.2576,  0.3129],\n       [-0.1308,  1.27  , -0.093 , -0.0662]])\n110 \n| \nChapter 4: NumPy Basics: Arrays and Vectorized Computation",
      "content_length": 1987,
      "extraction_method": "Direct"
    },
    {
      "page_number": 129,
      "chapter": null,
      "content": "In [189]: arr > 0\nOut[189]: \narray([[ True,  True,  True, False],\n       [False, False,  True,  True],\n       [False, False,  True,  True],\n       [False,  True, False, False]])\nIn [190]: np.where(arr > 0, 2, -2)\nOut[190]: \narray([[ 2,  2,  2, -2],\n       [-2, -2,  2,  2],\n       [-2, -2,  2,  2],\n       [-2,  2, -2, -2]])\nYou can combine scalars and arrays when using numpy.where. For example, I can\nreplace all positive values in arr with the constant 2, like so:\nIn [191]: np.where(arr > 0, 2, arr) # set only positive values to 2\nOut[191]: \narray([[ 2.    ,  2.    ,  2.    , -0.959 ],\n       [-1.2094, -1.4123,  2.    ,  2.    ],\n       [-0.6588, -1.2287,  2.    ,  2.    ],\n       [-0.1308,  2.    , -0.093 , -0.0662]])\nMathematical and Statistical Methods\nA set of mathematical functions that compute statistics about an entire array or\nabout the data along an axis are accessible as methods of the array class. You can\nuse aggregations (sometimes called reductions) like sum, mean, and std (standard\ndeviation) either by calling the array instance method or using the top-level NumPy\nfunction. When you use the NumPy function, like numpy.sum, you have to pass the\narray you want to aggregate as the first argument.\nHere I generate some normally distributed random data and compute some aggregate\nstatistics:\nIn [192]: arr = rng.standard_normal((5, 4))\nIn [193]: arr\nOut[193]: \narray([[-1.1082,  0.136 ,  1.3471,  0.0611],\n       [ 0.0709,  0.4337,  0.2775,  0.5303],\n       [ 0.5367,  0.6184, -0.795 ,  0.3   ],\n       [-1.6027,  0.2668, -1.2616, -0.0713],\n       [ 0.474 , -0.4149,  0.0977, -1.6404]])\nIn [194]: arr.mean()\nOut[194]: -0.08719744457434529\nIn [195]: np.mean(arr)\n4.4 Array-Oriented Programming with Arrays \n| \n111",
      "content_length": 1738,
      "extraction_method": "Direct"
    },
    {
      "page_number": 130,
      "chapter": null,
      "content": "Out[195]: -0.08719744457434529\nIn [196]: arr.sum()\nOut[196]: -1.743948891486906\nFunctions like mean and sum take an optional axis argument that computes the\nstatistic over the given axis, resulting in an array with one less dimension:\nIn [197]: arr.mean(axis=1)\nOut[197]: array([ 0.109 ,  0.3281,  0.165 , -0.6672, -0.3709])\nIn [198]: arr.sum(axis=0)\nOut[198]: array([-1.6292,  1.0399, -0.3344, -0.8203])\nHere, arr.mean(axis=1) means “compute mean across the columns,” where\narr.sum(axis=0) means “compute sum down the rows.”\nOther methods like cumsum and cumprod do not aggregate, instead producing an array\nof the intermediate results:\nIn [199]: arr = np.array([0, 1, 2, 3, 4, 5, 6, 7])\nIn [200]: arr.cumsum()\nOut[200]: array([ 0,  1,  3,  6, 10, 15, 21, 28])\nIn multidimensional arrays, accumulation functions like cumsum return an array of\nthe same size but with the partial aggregates computed along the indicated axis\naccording to each lower dimensional slice:\nIn [201]: arr = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\nIn [202]: arr\nOut[202]: \narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\nThe expression arr.cumsum(axis=0) computes the cumulative sum along the rows,\nwhile arr.cumsum(axis=1) computes the sums along the columns:\nIn [203]: arr.cumsum(axis=0)\nOut[203]: \narray([[ 0,  1,  2],\n       [ 3,  5,  7],\n       [ 9, 12, 15]])\nIn [204]: arr.cumsum(axis=1)\nOut[204]: \narray([[ 0,  1,  3],\n       [ 3,  7, 12],\n       [ 6, 13, 21]])\n112 \n| \nChapter 4: NumPy Basics: Arrays and Vectorized Computation",
      "content_length": 1521,
      "extraction_method": "Direct"
    },
    {
      "page_number": 131,
      "chapter": null,
      "content": "See Table 4-6 for a full listing. We’ll see many examples of these methods in action in\nlater chapters.\nTable 4-6. Basic array statistical methods\nMethod\nDescription\nsum\nSum of all the elements in the array or along an axis; zero-length arrays have sum 0\nmean\nArithmetic mean; invalid (returns NaN) on zero-length arrays\nstd, var\nStandard deviation and variance, respectively\nmin, max\nMinimum and maximum\nargmin, argmax Indices of minimum and maximum elements, respectively\ncumsum\nCumulative sum of elements starting from 0\ncumprod\nCumulative product of elements starting from 1\nMethods for Boolean Arrays\nBoolean values are coerced to 1 (True) and 0 (False) in the preceding methods. Thus,\nsum is often used as a means of counting True values in a Boolean array:\nIn [205]: arr = rng.standard_normal(100)\nIn [206]: (arr > 0).sum() # Number of positive values\nOut[206]: 48\nIn [207]: (arr <= 0).sum() # Number of non-positive values\nOut[207]: 52\nThe parentheses here in the expression (arr > 0).sum() are necessary to be able to\ncall sum() on the temporary result of arr > 0.\nTwo additional methods, any and all, are useful especially for Boolean arrays. any\ntests whether one or more values in an array is True, while all checks if every value is\nTrue:\nIn [208]: bools = np.array([False, False, True, False])\nIn [209]: bools.any()\nOut[209]: True\nIn [210]: bools.all()\nOut[210]: False\nThese methods also work with non-Boolean arrays, where nonzero elements are\ntreated as True.\n4.4 Array-Oriented Programming with Arrays \n| \n113",
      "content_length": 1526,
      "extraction_method": "Direct"
    },
    {
      "page_number": 132,
      "chapter": null,
      "content": "Sorting\nLike Python’s built-in list type, NumPy arrays can be sorted in place with the sort\nmethod:\nIn [211]: arr = rng.standard_normal(6)\nIn [212]: arr\nOut[212]: array([ 0.0773, -0.6839, -0.7208,  1.1206, -0.0548, -0.0824])\nIn [213]: arr.sort()\nIn [214]: arr\nOut[214]: array([-0.7208, -0.6839, -0.0824, -0.0548,  0.0773,  1.1206])\nYou can sort each one-dimensional section of values in a multidimensional array in\nplace along an axis by passing the axis number to sort. In this example data:\nIn [215]: arr = rng.standard_normal((5, 3))\nIn [216]: arr\nOut[216]: \narray([[ 0.936 ,  1.2385,  1.2728],\n       [ 0.4059, -0.0503,  0.2893],\n       [ 0.1793,  1.3975,  0.292 ],\n       [ 0.6384, -0.0279,  1.3711],\n       [-2.0528,  0.3805,  0.7554]])\narr.sort(axis=0) sorts the values within each column, while arr.sort(axis=1)\nsorts across each row:\nIn [217]: arr.sort(axis=0)\nIn [218]: arr\nOut[218]: \narray([[-2.0528, -0.0503,  0.2893],\n       [ 0.1793, -0.0279,  0.292 ],\n       [ 0.4059,  0.3805,  0.7554],\n       [ 0.6384,  1.2385,  1.2728],\n       [ 0.936 ,  1.3975,  1.3711]])\nIn [219]: arr.sort(axis=1)\nIn [220]: arr\nOut[220]: \narray([[-2.0528, -0.0503,  0.2893],\n       [-0.0279,  0.1793,  0.292 ],\n       [ 0.3805,  0.4059,  0.7554],\n       [ 0.6384,  1.2385,  1.2728],\n       [ 0.936 ,  1.3711,  1.3975]])\nThe top-level method numpy.sort returns a sorted copy of an array (like the Python\nbuilt-in function sorted) instead of modifying the array in place. For example:\n114 \n| \nChapter 4: NumPy Basics: Arrays and Vectorized Computation",
      "content_length": 1538,
      "extraction_method": "Direct"
    },
    {
      "page_number": 133,
      "chapter": null,
      "content": "In [221]: arr2 = np.array([5, -10, 7, 1, 0, -3])\nIn [222]: sorted_arr2 = np.sort(arr2)\nIn [223]: sorted_arr2\nOut[223]: array([-10,  -3,   0,   1,   5,   7])\nFor more details on using NumPy’s sorting methods, and more advanced techniques\nlike indirect sorts, see Appendix A. Several other kinds of data manipulations related\nto sorting (e.g., sorting a table of data by one or more columns) can also be found in\npandas.\nUnique and Other Set Logic\nNumPy has some basic set operations for one-dimensional ndarrays. A commonly\nused one is numpy.unique, which returns the sorted unique values in an array:\nIn [224]: names = np.array([\"Bob\", \"Will\", \"Joe\", \"Bob\", \"Will\", \"Joe\", \"Joe\"])\nIn [225]: np.unique(names)\nOut[225]: array(['Bob', 'Joe', 'Will'], dtype='<U4')\nIn [226]: ints = np.array([3, 3, 3, 2, 2, 1, 1, 4, 4])\nIn [227]: np.unique(ints)\nOut[227]: array([1, 2, 3, 4])\nContrast numpy.unique with the pure Python alternative:\nIn [228]: sorted(set(names))\nOut[228]: ['Bob', 'Joe', 'Will']\nIn many cases, the NumPy version is faster and returns a NumPy array rather than a\nPython list.\nAnother function, numpy.in1d, tests membership of the values in one array in\nanother, returning a Boolean array:\nIn [229]: values = np.array([6, 0, 0, 3, 2, 5, 6])\nIn [230]: np.in1d(values, [2, 3, 6])\nOut[230]: array([ True, False, False,  True,  True, False,  True])\nSee Table 4-7 for a listing of array set operations in NumPy.\nTable 4-7. Array set operations\nMethod\nDescription\nunique(x)\nCompute the sorted, unique elements in x\nintersect1d(x, y)\nCompute the sorted, common elements in x and y\nunion1d(x, y)\nCompute the sorted union of elements\n4.4 Array-Oriented Programming with Arrays \n| \n115",
      "content_length": 1684,
      "extraction_method": "Direct"
    },
    {
      "page_number": 134,
      "chapter": null,
      "content": "Method\nDescription\nin1d(x, y)\nCompute a Boolean array indicating whether each element of x is contained in y\nsetdiff1d(x, y)\nSet difference, elements in x that are not in y\nsetxor1d(x, y)\nSet symmetric differences; elements that are in either of the arrays, but not both\n4.5 File Input and Output with Arrays\nNumPy is able to save and load data to and from disk in some text or binary formats.\nIn this section I discuss only NumPy’s built-in binary format, since most users will\nprefer pandas and other tools for loading text or tabular data (see Chapter 6 for much\nmore).\nnumpy.save and numpy.load are the two workhorse functions for efficiently saving\nand loading array data on disk. Arrays are saved by default in an uncompressed raw\nbinary format with file extension .npy:\nIn [231]: arr = np.arange(10)\nIn [232]: np.save(\"some_array\", arr)\nIf the file path does not already end in .npy, the extension will be appended. The array\non disk can then be loaded with numpy.load:\nIn [233]: np.load(\"some_array.npy\")\nOut[233]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\nYou can save multiple arrays in an uncompressed archive using numpy.savez and\npassing the arrays as keyword arguments:\nIn [234]: np.savez(\"array_archive.npz\", a=arr, b=arr)\nWhen loading an .npz file, you get back a dictionary-like object that loads the\nindividual arrays lazily:\nIn [235]: arch = np.load(\"array_archive.npz\")\nIn [236]: arch[\"b\"]\nOut[236]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\nIf your data compresses well, you may wish to use numpy.savez_compressed instead:\nIn [237]: np.savez_compressed(\"arrays_compressed.npz\", a=arr, b=arr)\n4.6 Linear Algebra\nLinear algebra operations, like matrix multiplication, decompositions, determinants,\nand other square matrix math, are an important part of many array libraries. Multi‐\nplying two two-dimensional arrays with * is an element-wise product, while matrix\n116 \n| \nChapter 4: NumPy Basics: Arrays and Vectorized Computation",
      "content_length": 1941,
      "extraction_method": "Direct"
    },
    {
      "page_number": 135,
      "chapter": null,
      "content": "multiplications require using a function. Thus, there is a function dot, both an array\nmethod and a function in the numpy namespace, for matrix multiplication:\nIn [241]: x = np.array([[1., 2., 3.], [4., 5., 6.]])\nIn [242]: y = np.array([[6., 23.], [-1, 7], [8, 9]])\nIn [243]: x\nOut[243]: \narray([[1., 2., 3.],\n       [4., 5., 6.]])\nIn [244]: y\nOut[244]: \narray([[ 6., 23.],\n       [-1.,  7.],\n       [ 8.,  9.]])\nIn [245]: x.dot(y)\nOut[245]: \narray([[ 28.,  64.],\n       [ 67., 181.]])\nx.dot(y) is equivalent to np.dot(x, y):\nIn [246]: np.dot(x, y)\nOut[246]: \narray([[ 28.,  64.],\n       [ 67., 181.]])\nA matrix product between a two-dimensional array and a suitably sized one-\ndimensional array results in a one-dimensional array:\nIn [247]: x @ np.ones(3)\nOut[247]: array([ 6., 15.])\nnumpy.linalg has a standard set of matrix decompositions and things like inverse\nand determinant:\nIn [248]: from numpy.linalg import inv, qr\nIn [249]: X = rng.standard_normal((5, 5))\nIn [250]: mat = X.T @ X\nIn [251]: inv(mat)\nOut[251]: \narray([[  3.4993,   2.8444,   3.5956, -16.5538,   4.4733],\n       [  2.8444,   2.5667,   2.9002, -13.5774,   3.7678],\n       [  3.5956,   2.9002,   4.4823, -18.3453,   4.7066],\n       [-16.5538, -13.5774, -18.3453,  84.0102, -22.0484],\n       [  4.4733,   3.7678,   4.7066, -22.0484,   6.0525]])\n4.6 Linear Algebra \n| \n117",
      "content_length": 1344,
      "extraction_method": "Direct"
    },
    {
      "page_number": 136,
      "chapter": null,
      "content": "In [252]: mat @ inv(mat)\nOut[252]: \narray([[ 1.,  0., -0.,  0., -0.],\n       [ 0.,  1.,  0.,  0., -0.],\n       [ 0., -0.,  1., -0., -0.],\n       [ 0., -0.,  0.,  1., -0.],\n       [ 0., -0.,  0., -0.,  1.]])\nThe expression X.T.dot(X) computes the dot product of X with its transpose X.T.\nSee Table 4-8 for a list of some of the most commonly used linear algebra functions.\nTable 4-8. Commonly used numpy.linalg functions\nFunction\nDescription\ndiag\nReturn the diagonal (or off-diagonal) elements of a square matrix as a 1D array, or convert a 1D array into a\nsquare matrix with zeros on the off-diagonal\ndot\nMatrix multiplication\ntrace\nCompute the sum of the diagonal elements\ndet\nCompute the matrix determinant\neig\nCompute the eigenvalues and eigenvectors of a square matrix\ninv\nCompute the inverse of a square matrix\npinv\nCompute the Moore-Penrose pseudoinverse of a matrix\nqr\nCompute the QR decomposition\nsvd\nCompute the singular value decomposition (SVD)\nsolve\nSolve the linear system Ax = b for x, where A is a square matrix\nlstsq\nCompute the least-squares solution to Ax = b\n4.7 Example: Random Walks\nThe simulation of random walks provides an illustrative application of utilizing array\noperations. Let’s first consider a simple random walk starting at 0 with steps of 1 and\n–1 occurring with equal probability.\nHere is a pure Python way to implement a single random walk with 1,000 steps using\nthe built-in random module:\n#! blockstart\nimport random\nposition = 0\nwalk = [position]\nnsteps = 1000\nfor _ in range(nsteps):\n    step = 1 if random.randint(0, 1) else -1\n    position += step\n    walk.append(position)\n#! blockend\n118 \n| \nChapter 4: NumPy Basics: Arrays and Vectorized Computation",
      "content_length": 1694,
      "extraction_method": "Direct"
    },
    {
      "page_number": 137,
      "chapter": null,
      "content": "See Figure 4-4 for an example plot of the first 100 values on one of these random\nwalks:\nIn [255]: plt.plot(walk[:100])\nFigure 4-4. A simple random walk\nYou might make the observation that walk is the cumulative sum of the random steps\nand could be evaluated as an array expression. Thus, I use the numpy.random module\nto draw 1,000 coin flips at once, set these to 1 and –1, and compute the cumulative\nsum:\nIn [256]: nsteps = 1000\nIn [257]: rng = np.random.default_rng(seed=12345)  # fresh random generator\nIn [258]: draws = rng.integers(0, 2, size=nsteps)\nIn [259]: steps = np.where(draws == 0, 1, -1)\nIn [260]: walk = steps.cumsum()\nFrom this we can begin to extract statistics like the minimum and maximum value\nalong the walk’s trajectory:\nIn [261]: walk.min()\nOut[261]: -8\nIn [262]: walk.max()\nOut[262]: 50\n4.7 Example: Random Walks \n| \n119",
      "content_length": 846,
      "extraction_method": "Direct"
    },
    {
      "page_number": 138,
      "chapter": null,
      "content": "A more complicated statistic is the first crossing time, the step at which the random\nwalk reaches a particular value. Here we might want to know how long it took the\nrandom walk to get at least 10 steps away from the origin 0 in either direction.\nnp.abs(walk) >= 10 gives us a Boolean array indicating where the walk has reached\nor exceeded 10, but we want the index of the first 10 or –10. Turns out, we can\ncompute this using argmax, which returns the first index of the maximum value in\nthe Boolean array (True is the maximum value):\nIn [263]: (np.abs(walk) >= 10).argmax()\nOut[263]: 155\nNote that using argmax here is not always efficient because it always makes a full\nscan of the array. In this special case, once a True is observed we know it to be the\nmaximum value.\nSimulating Many Random Walks at Once\nIf your goal was to simulate many random walks, say five thousand of them, you can\ngenerate all of the random walks with minor modifications to the preceding code. If\npassed a 2-tuple, the numpy.random functions will generate a two-dimensional array\nof draws, and we can compute the cumulative sum for each row to compute all five\nthousand random walks in one shot:\nIn [264]: nwalks = 5000\nIn [265]: nsteps = 1000\nIn [266]: draws = rng.integers(0, 2, size=(nwalks, nsteps)) # 0 or 1\nIn [267]: steps = np.where(draws > 0, 1, -1)\nIn [268]: walks = steps.cumsum(axis=1)\nIn [269]: walks\nOut[269]: \narray([[  1,   2,   3, ...,  22,  23,  22],\n       [  1,   0,  -1, ..., -50, -49, -48],\n       [  1,   2,   3, ...,  50,  49,  48],\n       ...,\n       [ -1,  -2,  -1, ..., -10,  -9, -10],\n       [ -1,  -2,  -3, ...,   8,   9,   8],\n       [ -1,   0,   1, ...,  -4,  -3,  -2]])\nNow, we can compute the maximum and minimum values obtained over all of the\nwalks:\nIn [270]: walks.max()\nOut[270]: 114\n120 \n| \nChapter 4: NumPy Basics: Arrays and Vectorized Computation",
      "content_length": 1869,
      "extraction_method": "Direct"
    },
    {
      "page_number": 139,
      "chapter": null,
      "content": "In [271]: walks.min()\nOut[271]: -120\nOut of these walks, let’s compute the minimum crossing time to 30 or –30. This is\nslightly tricky because not all 5,000 of them reach 30. We can check this using the any\nmethod:\nIn [272]: hits30 = (np.abs(walks) >= 30).any(axis=1)\nIn [273]: hits30\nOut[273]: array([False,  True,  True, ...,  True, False,  True])\nIn [274]: hits30.sum() # Number that hit 30 or -30\nOut[274]: 3395\nWe can use this Boolean array to select the rows of walks that actually cross the\nabsolute 30 level, and call argmax across axis 1 to get the crossing times:\nIn [275]: crossing_times = (np.abs(walks[hits30]) >= 30).argmax(axis=1)\nIn [276]: crossing_times\nOut[276]: array([201, 491, 283, ..., 219, 259, 541])\nLastly, we compute the average minimum crossing time:\nIn [277]: crossing_times.mean()\nOut[277]: 500.5699558173785\nFeel free to experiment with other distributions for the steps other than equal-\nsized coin flips. You need only use a different random generator method, like stan\ndard_normal to generate normally distributed steps with some mean and standard\ndeviation:\nIn [278]: draws = 0.25 * rng.standard_normal((nwalks, nsteps))\nKeep in mind that this vectorized approach requires creating an\narray with nwalks * nsteps elements, which may use a large\namount of memory for large simulations. If memory is more con‐\nstrained, then a different approach will be required.\n4.8 Conclusion\nWhile much of the rest of the book will focus on building data wrangling skills\nwith pandas, we will continue to work in a similar array-based style. In Appendix A,\nwe will dig deeper into NumPy features to help you further develop your array\ncomputing skills.\n4.8 Conclusion \n| \n121",
      "content_length": 1693,
      "extraction_method": "Direct"
    },
    {
      "page_number": 140,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 141,
      "chapter": null,
      "content": "CHAPTER 5\nGetting Started with pandas\npandas will be a major tool of interest throughout much of the rest of the book. It\ncontains data structures and data manipulation tools designed to make data cleaning\nand analysis fast and convenient in Python. pandas is often used in tandem with\nnumerical computing tools like NumPy and SciPy, analytical libraries like statsmo‐\ndels and scikit-learn, and data visualization libraries like matplotlib. pandas adopts\nsignificant parts of NumPy’s idiomatic style of array-based computing, especially\narray-based functions and a preference for data processing without for loops.\nWhile pandas adopts many coding idioms from NumPy, the biggestabout difference\nis that pandas is designed for working with tabular or heterogeneous data. NumPy, by\ncontrast, is best suited for working with homogeneously typed numerical array data.\nSince becoming an open source project in 2010, pandas has matured into a quite\nlarge library that’s applicable in a broad set of real-world use cases. The developer\ncommunity has grown to over 2,500 distinct contributors, who’ve been helping build\nthe project as they used it to solve their day-to-day data problems. The vibrant pandas\ndeveloper and user communities have been a key part of its success.\nMany people don’t know that I haven’t been actively involved in\nday-to-day pandas development since 2013; it has been an entirely\ncommunity-managed project since then. Be sure to pass on your\nthanks to the core development and all the contributors for their\nhard work!\n123",
      "content_length": 1540,
      "extraction_method": "Direct"
    },
    {
      "page_number": 142,
      "chapter": null,
      "content": "Throughout the rest of the book, I use the following import conventions for NumPy\nand pandas:\nIn [1]: import numpy as np\nIn [2]: import pandas as pd\nThus, whenever you see pd. in code, it’s referring to pandas. You may also find it\neasier to import Series and DataFrame into the local namespace since they are so\nfrequently used:\nIn [3]: from pandas import Series, DataFrame\n5.1 Introduction to pandas Data Structures\nTo get started with pandas, you will need to get comfortable with its two workhorse\ndata structures: Series and DataFrame. While they are not a universal solution for\nevery problem, they provide a solid foundation for a wide variety of data tasks.\nSeries\nA Series is a one-dimensional array-like object containing a sequence of values (of\nsimilar types to NumPy types) of the same type and an associated array of data labels,\ncalled its index. The simplest Series is formed from only an array of data:\nIn [14]: obj = pd.Series([4, 7, -5, 3])\nIn [15]: obj\nOut[15]: \n0    4\n1    7\n2   -5\n3    3\ndtype: int64\nThe string representation of a Series displayed interactively shows the index on the\nleft and the values on the right. Since we did not specify an index for the data, a\ndefault one consisting of the integers 0 through N - 1 (where N is the length of the\ndata) is created. You can get the array representation and index object of the Series via\nits array and index attributes, respectively:\nIn [16]: obj.array\nOut[16]: \n<PandasArray>\n[4, 7, -5, 3]\nLength: 4, dtype: int64\nIn [17]: obj.index\nOut[17]: RangeIndex(start=0, stop=4, step=1)\n124 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 1605,
      "extraction_method": "Direct"
    },
    {
      "page_number": 143,
      "chapter": null,
      "content": "The result of the .array attribute is a PandasArray which usually wraps a NumPy\narray but can also contain special extension array types which will be discussed more\nin Section 7.3, “Extension Data Types,” on page 224.\nOften, you’ll want to create a Series with an index identifying each data point with a\nlabel:\nIn [18]: obj2 = pd.Series([4, 7, -5, 3], index=[\"d\", \"b\", \"a\", \"c\"])\nIn [19]: obj2\nOut[19]: \nd    4\nb    7\na   -5\nc    3\ndtype: int64\nIn [20]: obj2.index\nOut[20]: Index(['d', 'b', 'a', 'c'], dtype='object')\nCompared with NumPy arrays, you can use labels in the index when selecting single\nvalues or a set of values:\nIn [21]: obj2[\"a\"]\nOut[21]: -5\nIn [22]: obj2[\"d\"] = 6\nIn [23]: obj2[[\"c\", \"a\", \"d\"]]\nOut[23]: \nc    3\na   -5\nd    6\ndtype: int64\nHere [\"c\", \"a\", \"d\"] is interpreted as a list of indices, even though it contains\nstrings instead of integers.\nUsing NumPy functions or NumPy-like operations, such as filtering with a Boolean\narray, scalar multiplication, or applying math functions, will preserve the index-value\nlink:\nIn [24]: obj2[obj2 > 0]\nOut[24]: \nd    6\nb    7\nc    3\ndtype: int64\nIn [25]: obj2 * 2\nOut[25]: \nd    12\n5.1 Introduction to pandas Data Structures \n| \n125",
      "content_length": 1198,
      "extraction_method": "Direct"
    },
    {
      "page_number": 144,
      "chapter": null,
      "content": "b    14\na   -10\nc     6\ndtype: int64\nIn [26]: import numpy as np\nIn [27]: np.exp(obj2)\nOut[27]: \nd     403.428793\nb    1096.633158\na       0.006738\nc      20.085537\ndtype: float64\nAnother way to think about a Series is as a fixed-length, ordered dictionary, as it is a\nmapping of index values to data values. It can be used in many contexts where you\nmight use a dictionary:\nIn [28]: \"b\" in obj2\nOut[28]: True\nIn [29]: \"e\" in obj2\nOut[29]: False\nShould you have data contained in a Python dictionary, you can create a Series from\nit by passing the dictionary:\nIn [30]: sdata = {\"Ohio\": 35000, \"Texas\": 71000, \"Oregon\": 16000, \"Utah\": 5000}\nIn [31]: obj3 = pd.Series(sdata)\nIn [32]: obj3\nOut[32]: \nOhio      35000\nTexas     71000\nOregon    16000\nUtah       5000\ndtype: int64\nA Series can be converted back to a dictionary with its to_dict method:\nIn [33]: obj3.to_dict()\nOut[33]: {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}\nWhen you are only passing a dictionary, the index in the resulting Series will respect\nthe order of the keys according to the dictionary’s keys method, which depends on\nthe key insertion order. You can override this by passing an index with the dictionary\nkeys in the order you want them to appear in the resulting Series:\nIn [34]: states = [\"California\", \"Ohio\", \"Oregon\", \"Texas\"]\nIn [35]: obj4 = pd.Series(sdata, index=states)\n126 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 1418,
      "extraction_method": "Direct"
    },
    {
      "page_number": 145,
      "chapter": null,
      "content": "In [36]: obj4\nOut[36]: \nCalifornia        NaN\nOhio          35000.0\nOregon        16000.0\nTexas         71000.0\ndtype: float64\nHere, three values found in sdata were placed in the appropriate locations, but since\nno value for \"California\" was found, it appears as NaN (Not a Number), which is\nconsidered in pandas to mark missing or NA values. Since \"Utah\" was not included\nin states, it is excluded from the resulting object.\nI will use the terms “missing,” “NA,” or “null” interchangeably to refer to missing data.\nThe isna and notna functions in pandas should be used to detect missing data:\nIn [37]: pd.isna(obj4)\nOut[37]: \nCalifornia     True\nOhio          False\nOregon        False\nTexas         False\ndtype: bool\nIn [38]: pd.notna(obj4)\nOut[38]: \nCalifornia    False\nOhio           True\nOregon         True\nTexas          True\ndtype: bool\nSeries also has these as instance methods:\nIn [39]: obj4.isna()\nOut[39]: \nCalifornia     True\nOhio          False\nOregon        False\nTexas         False\ndtype: bool\nI discuss working with missing data in more detail in Chapter 7.\nA useful Series feature for many applications is that it automatically aligns by index\nlabel in arithmetic operations:\nIn [40]: obj3\nOut[40]: \nOhio      35000\nTexas     71000\nOregon    16000\n5.1 Introduction to pandas Data Structures \n| \n127",
      "content_length": 1318,
      "extraction_method": "Direct"
    },
    {
      "page_number": 146,
      "chapter": null,
      "content": "Utah       5000\ndtype: int64\nIn [41]: obj4\nOut[41]: \nCalifornia        NaN\nOhio          35000.0\nOregon        16000.0\nTexas         71000.0\ndtype: float64\nIn [42]: obj3 + obj4\nOut[42]: \nCalifornia         NaN\nOhio           70000.0\nOregon         32000.0\nTexas         142000.0\nUtah               NaN\ndtype: float64\nData alignment features will be addressed in more detail later. If you have experience\nwith databases, you can think about this as being similar to a join operation.\nBoth the Series object itself and its index have a name attribute, which integrates with\nother areas of pandas functionality:\nIn [43]: obj4.name = \"population\"\nIn [44]: obj4.index.name = \"state\"\nIn [45]: obj4\nOut[45]: \nstate\nCalifornia        NaN\nOhio          35000.0\nOregon        16000.0\nTexas         71000.0\nName: population, dtype: float64\nA Series’s index can be altered in place by assignment:\nIn [46]: obj\nOut[46]: \n0    4\n1    7\n2   -5\n3    3\ndtype: int64\nIn [47]: obj.index = [\"Bob\", \"Steve\", \"Jeff\", \"Ryan\"]\nIn [48]: obj\nOut[48]: \n128 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 1072,
      "extraction_method": "Direct"
    },
    {
      "page_number": 147,
      "chapter": null,
      "content": "Bob      4\nSteve    7\nJeff    -5\nRyan     3\ndtype: int64\nDataFrame\nA DataFrame represents a rectangular table of data and contains an ordered, named\ncollection of columns, each of which can be a different value type (numeric, string,\nBoolean, etc.). The DataFrame has both a row and column index; it can be thought of\nas a dictionary of Series all sharing the same index.\nWhile a DataFrame is physically two-dimensional, you can use it\nto represent higher dimensional data in a tabular format using\nhierarchical indexing, a subject we will discuss in Chapter 8 and an\ningredient in some of the more advanced data-handling features in\npandas.\nThere are many ways to construct a DataFrame, though one of the most common is\nfrom a dictionary of equal-length lists or NumPy arrays:\ndata = {\"state\": [\"Ohio\", \"Ohio\", \"Ohio\", \"Nevada\", \"Nevada\", \"Nevada\"],\n        \"year\": [2000, 2001, 2002, 2001, 2002, 2003],\n        \"pop\": [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}\nframe = pd.DataFrame(data)\nThe resulting DataFrame will have its index assigned automatically, as with Series,\nand the columns are placed according to the order of the keys in data (which\ndepends on their insertion order in the dictionary):\nIn [50]: frame\nOut[50]: \n    state  year  pop\n0    Ohio  2000  1.5\n1    Ohio  2001  1.7\n2    Ohio  2002  3.6\n3  Nevada  2001  2.4\n4  Nevada  2002  2.9\n5  Nevada  2003  3.2\nIf you are using the Jupyter notebook, pandas DataFrame objects\nwill be displayed as a more browser-friendly HTML table. See\nFigure 5-1 for an example.\n5.1 Introduction to pandas Data Structures \n| \n129",
      "content_length": 1567,
      "extraction_method": "Direct"
    },
    {
      "page_number": 148,
      "chapter": null,
      "content": "Figure 5-1. How pandas DataFrame objects look in Jupyter\nFor large DataFrames, the head method selects only the first five rows:\nIn [51]: frame.head()\nOut[51]: \n    state  year  pop\n0    Ohio  2000  1.5\n1    Ohio  2001  1.7\n2    Ohio  2002  3.6\n3  Nevada  2001  2.4\n4  Nevada  2002  2.9\nSimilarly, tail returns the last five rows:\nIn [52]: frame.tail()\nOut[52]: \n    state  year  pop\n1    Ohio  2001  1.7\n2    Ohio  2002  3.6\n3  Nevada  2001  2.4\n4  Nevada  2002  2.9\n5  Nevada  2003  3.2\nIf you specify a sequence of columns, the DataFrame’s columns will be arranged in\nthat order:\nIn [53]: pd.DataFrame(data, columns=[\"year\", \"state\", \"pop\"])\nOut[53]: \n   year   state  pop\n0  2000    Ohio  1.5\n1  2001    Ohio  1.7\n2  2002    Ohio  3.6\n3  2001  Nevada  2.4\n4  2002  Nevada  2.9\n5  2003  Nevada  3.2\n130 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 848,
      "extraction_method": "Direct"
    },
    {
      "page_number": 149,
      "chapter": null,
      "content": "If you pass a column that isn’t contained in the dictionary, it will appear with missing\nvalues in the result:\nIn [54]: frame2 = pd.DataFrame(data, columns=[\"year\", \"state\", \"pop\", \"debt\"])\nIn [55]: frame2\nOut[55]: \n   year   state  pop debt\n0  2000    Ohio  1.5  NaN\n1  2001    Ohio  1.7  NaN\n2  2002    Ohio  3.6  NaN\n3  2001  Nevada  2.4  NaN\n4  2002  Nevada  2.9  NaN\n5  2003  Nevada  3.2  NaN\nIn [56]: frame2.columns\nOut[56]: Index(['year', 'state', 'pop', 'debt'], dtype='object')\nA column in a DataFrame can be retrieved as a Series either by dictionary-like\nnotation or by using the dot attribute notation:\nIn [57]: frame2[\"state\"]\nOut[57]: \n0      Ohio\n1      Ohio\n2      Ohio\n3    Nevada\n4    Nevada\n5    Nevada\nName: state, dtype: object\nIn [58]: frame2.year\nOut[58]: \n0    2000\n1    2001\n2    2002\n3    2001\n4    2002\n5    2003\nName: year, dtype: int64\nAttribute-like access (e.g., frame2.year) and tab completion of\ncolumn names in IPython are provided as a convenience.\nframe2[column] works for any column name, but frame2.column\nworks only when the column name is a valid Python variable name\nand does not conflict with any of the method names in DataFrame.\nFor example, if a column’s name contains whitespace or symbols\nother than underscores, it cannot be accessed with the dot attribute\nmethod.\n5.1 Introduction to pandas Data Structures \n| \n131",
      "content_length": 1363,
      "extraction_method": "Direct"
    },
    {
      "page_number": 150,
      "chapter": null,
      "content": "Note that the returned Series have the same index as the DataFrame, and their name\nattribute has been appropriately set.\nRows can also be retrieved by position or name with the special iloc and loc\nattributes (more on this later in “Selection on DataFrame with loc and iloc” on page\n147):\nIn [59]: frame2.loc[1]\nOut[59]: \nyear     2001\nstate    Ohio\npop       1.7\ndebt      NaN\nName: 1, dtype: object\nIn [60]: frame2.iloc[2]\nOut[60]: \nyear     2002\nstate    Ohio\npop       3.6\ndebt      NaN\nName: 2, dtype: object\nColumns can be modified by assignment. For example, the empty debt column could\nbe assigned a scalar value or an array of values:\nIn [61]: frame2[\"debt\"] = 16.5\nIn [62]: frame2\nOut[62]: \n   year   state  pop  debt\n0  2000    Ohio  1.5  16.5\n1  2001    Ohio  1.7  16.5\n2  2002    Ohio  3.6  16.5\n3  2001  Nevada  2.4  16.5\n4  2002  Nevada  2.9  16.5\n5  2003  Nevada  3.2  16.5\nIn [63]: frame2[\"debt\"] = np.arange(6.)\nIn [64]: frame2\nOut[64]: \n   year   state  pop  debt\n0  2000    Ohio  1.5   0.0\n1  2001    Ohio  1.7   1.0\n2  2002    Ohio  3.6   2.0\n3  2001  Nevada  2.4   3.0\n4  2002  Nevada  2.9   4.0\n5  2003  Nevada  3.2   5.0\n132 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 1191,
      "extraction_method": "Direct"
    },
    {
      "page_number": 151,
      "chapter": null,
      "content": "When you are assigning lists or arrays to a column, the value’s length must match the\nlength of the DataFrame. If you assign a Series, its labels will be realigned exactly to\nthe DataFrame’s index, inserting missing values in any index values not present:\nIn [65]: val = pd.Series([-1.2, -1.5, -1.7], index=[\"two\", \"four\", \"five\"])\nIn [66]: frame2[\"debt\"] = val\nIn [67]: frame2\nOut[67]: \n   year   state  pop  debt\n0  2000    Ohio  1.5   NaN\n1  2001    Ohio  1.7   NaN\n2  2002    Ohio  3.6   NaN\n3  2001  Nevada  2.4   NaN\n4  2002  Nevada  2.9   NaN\n5  2003  Nevada  3.2   NaN\nAssigning a column that doesn’t exist will create a new column.\nThe del keyword will delete columns like with a dictionary. As an example, I first add\na new column of Boolean values where the state column equals \"Ohio\":\nIn [68]: frame2[\"eastern\"] = frame2[\"state\"] == \"Ohio\"\nIn [69]: frame2\nOut[69]: \n   year   state  pop  debt  eastern\n0  2000    Ohio  1.5   NaN     True\n1  2001    Ohio  1.7   NaN     True\n2  2002    Ohio  3.6   NaN     True\n3  2001  Nevada  2.4   NaN    False\n4  2002  Nevada  2.9   NaN    False\n5  2003  Nevada  3.2   NaN    False\nNew columns cannot be created with the frame2.eastern dot\nattribute notation.\nThe del method can then be used to remove this column:\nIn [70]: del frame2[\"eastern\"]\nIn [71]: frame2.columns\nOut[71]: Index(['year', 'state', 'pop', 'debt'], dtype='object')\n5.1 Introduction to pandas Data Structures \n| \n133",
      "content_length": 1433,
      "extraction_method": "Direct"
    },
    {
      "page_number": 152,
      "chapter": null,
      "content": "The column returned from indexing a DataFrame is a view on the\nunderlying data, not a copy. Thus, any in-place modifications to\nthe Series will be reflected in the DataFrame. The column can be\nexplicitly copied with the Series’s copy method.\nAnother common form of data is a nested dictionary of dictionaries:\nIn [72]: populations = {\"Ohio\": {2000: 1.5, 2001: 1.7, 2002: 3.6},\n   ....:                \"Nevada\": {2001: 2.4, 2002: 2.9}}\nIf the nested dictionary is passed to the DataFrame, pandas will interpret the outer\ndictionary keys as the columns, and the inner keys as the row indices:\nIn [73]: frame3 = pd.DataFrame(populations)\nIn [74]: frame3\nOut[74]: \n      Ohio  Nevada\n2000   1.5     NaN\n2001   1.7     2.4\n2002   3.6     2.9\nYou can transpose the DataFrame (swap rows and columns) with similar syntax to a\nNumPy array:\nIn [75]: frame3.T\nOut[75]: \n        2000  2001  2002\nOhio     1.5   1.7   3.6\nNevada   NaN   2.4   2.9\nNote that transposing discards the column data types if the col‐\numns do not all have the same data type, so transposing and then\ntransposing back may lose the previous type information. The col‐\numns become arrays of pure Python objects in this case.\nThe keys in the inner dictionaries are combined to form the index in the result. This\nisn’t true if an explicit index is specified:\nIn [76]: pd.DataFrame(populations, index=[2001, 2002, 2003])\nOut[76]: \n      Ohio  Nevada\n2001   1.7     2.4\n2002   3.6     2.9\n2003   NaN     NaN\nDictionaries of Series are treated in much the same way:\nIn [77]: pdata = {\"Ohio\": frame3[\"Ohio\"][:-1],\n   ....:          \"Nevada\": frame3[\"Nevada\"][:2]}\n134 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 1665,
      "extraction_method": "Direct"
    },
    {
      "page_number": 153,
      "chapter": null,
      "content": "In [78]: pd.DataFrame(pdata)\nOut[78]: \n      Ohio  Nevada\n2000   1.5     NaN\n2001   1.7     2.4\nFor a list of many of the things you can pass to the DataFrame constructor, see\nTable 5-1.\nTable 5-1. Possible data inputs to the DataFrame constructor\nType\nNotes\n2D ndarray\nA matrix of data, passing optional row and column labels\nDictionary of arrays, lists, or\ntuples\nEach sequence becomes a column in the DataFrame; all sequences must be the same length\nNumPy structured/record\narray\nTreated as the “dictionary of arrays” case\nDictionary of Series\nEach value becomes a column; indexes from each Series are unioned together to form the\nresult’s row index if no explicit index is passed\nDictionary of dictionaries\nEach inner dictionary becomes a column; keys are unioned to form the row index as in the\n“dictionary of Series” case\nList of dictionaries or Series\nEach item becomes a row in the DataFrame; unions of dictionary keys or Series indexes\nbecome the DataFrame’s column labels\nList of lists or tuples\nTreated as the “2D ndarray” case\nAnother DataFrame\nThe DataFrame’s indexes are used unless different ones are passed\nNumPy MaskedArray\nLike the “2D ndarray” case except masked values are missing in the DataFrame result\nIf a DataFrame’s index and columns have their name attributes set, these will also be\ndisplayed:\nIn [79]: frame3.index.name = \"year\"\nIn [80]: frame3.columns.name = \"state\"\nIn [81]: frame3\nOut[81]: \nstate  Ohio  Nevada\nyear               \n2000    1.5     NaN\n2001    1.7     2.4\n2002    3.6     2.9\nUnlike Series, DataFrame does not have a name attribute. DataFrame’s to_numpy\nmethod returns the data contained in the DataFrame as a two-dimensional ndarray:\nIn [82]: frame3.to_numpy()\nOut[82]: \narray([[1.5, nan],\n5.1 Introduction to pandas Data Structures \n| \n135",
      "content_length": 1788,
      "extraction_method": "Direct"
    },
    {
      "page_number": 154,
      "chapter": null,
      "content": "[1.7, 2.4],\n       [3.6, 2.9]])\nIf the DataFrame’s columns are different data types, the data type of the returned\narray will be chosen to accommodate all of the columns:\nIn [83]: frame2.to_numpy()\nOut[83]: \narray([[2000, 'Ohio', 1.5, nan],\n       [2001, 'Ohio', 1.7, nan],\n       [2002, 'Ohio', 3.6, nan],\n       [2001, 'Nevada', 2.4, nan],\n       [2002, 'Nevada', 2.9, nan],\n       [2003, 'Nevada', 3.2, nan]], dtype=object)\nIndex Objects\npandas’s Index objects are responsible for holding the axis labels (including a Data‐\nFrame’s column names) and other metadata (like the axis name or names). Any array\nor other sequence of labels you use when constructing a Series or DataFrame is\ninternally converted to an Index:\nIn [84]: obj = pd.Series(np.arange(3), index=[\"a\", \"b\", \"c\"])\nIn [85]: index = obj.index\nIn [86]: index\nOut[86]: Index(['a', 'b', 'c'], dtype='object')\nIn [87]: index[1:]\nOut[87]: Index(['b', 'c'], dtype='object')\nIndex objects are immutable and thus can’t be modified by the user:\nindex[1] = \"d\"  # TypeError\nImmutability makes it safer to share Index objects among data structures:\nIn [88]: labels = pd.Index(np.arange(3))\nIn [89]: labels\nOut[89]: Int64Index([0, 1, 2], dtype='int64')\nIn [90]: obj2 = pd.Series([1.5, -2.5, 0], index=labels)\nIn [91]: obj2\nOut[91]: \n0    1.5\n1   -2.5\n2    0.0\ndtype: float64\n136 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 1377,
      "extraction_method": "Direct"
    },
    {
      "page_number": 155,
      "chapter": null,
      "content": "In [92]: obj2.index is labels\nOut[92]: True\nSome users will not often take advantage of the capabilities pro‐\nvided by an Index, but because some operations will yield results\ncontaining indexed data, it’s important to understand how they\nwork.\nIn addition to being array-like, an Index also behaves like a fixed-size set:\nIn [93]: frame3\nOut[93]: \nstate  Ohio  Nevada\nyear               \n2000    1.5     NaN\n2001    1.7     2.4\n2002    3.6     2.9\nIn [94]: frame3.columns\nOut[94]: Index(['Ohio', 'Nevada'], dtype='object', name='state')\nIn [95]: \"Ohio\" in frame3.columns\nOut[95]: True\nIn [96]: 2003 in frame3.index\nOut[96]: False\nUnlike Python sets, a pandas Index can contain duplicate labels:\nIn [97]: pd.Index([\"foo\", \"foo\", \"bar\", \"bar\"])\nOut[97]: Index(['foo', 'foo', 'bar', 'bar'], dtype='object')\nSelections with duplicate labels will select all occurrences of that label.\nEach Index has a number of methods and properties for set logic, which answer other\ncommon questions about the data it contains. Some useful ones are summarized in\nTable 5-2.\nTable 5-2. Some Index methods and properties\nMethod/Property\nDescription\nappend()\nConcatenate with additional Index objects, producing a new Index\ndifference()\nCompute set difference as an Index\nintersection()\nCompute set intersection\nunion()\nCompute set union\nisin()\nCompute Boolean array indicating whether each value is contained in the passed collection\ndelete()\nCompute new Index with element at Index i deleted\ndrop()\nCompute new Index by deleting passed values\ninsert()\nCompute new Index by inserting element at Index i\n5.1 Introduction to pandas Data Structures \n| \n137",
      "content_length": 1633,
      "extraction_method": "Direct"
    },
    {
      "page_number": 156,
      "chapter": null,
      "content": "Method/Property\nDescription\nis_monotonic\nReturns True if each element is greater than or equal to the previous element\nis_unique\nReturns True if the Index has no duplicate values\nunique()\nCompute the array of unique values in the Index\n5.2 Essential Functionality\nThis section will walk you through the fundamental mechanics of interacting with\nthe data contained in a Series or DataFrame. In the chapters to come, we will delve\nmore deeply into data analysis and manipulation topics using pandas. This book is\nnot intended to serve as exhaustive documentation for the pandas library; instead,\nwe’ll focus on familiarizing you with heavily used features, leaving the less common\n(i.e., more esoteric) things for you to learn more about by reading the online pandas\ndocumentation.\nReindexing\nAn important method on pandas objects is reindex, which means to create a new\nobject with the values rearranged to align with the new index. Consider an example:\nIn [98]: obj = pd.Series([4.5, 7.2, -5.3, 3.6], index=[\"d\", \"b\", \"a\", \"c\"])\nIn [99]: obj\nOut[99]: \nd    4.5\nb    7.2\na   -5.3\nc    3.6\ndtype: float64\nCalling reindex on this Series rearranges the data according to the new index,\nintroducing missing values if any index values were not already present:\nIn [100]: obj2 = obj.reindex([\"a\", \"b\", \"c\", \"d\", \"e\"])\nIn [101]: obj2\nOut[101]: \na   -5.3\nb    7.2\nc    3.6\nd    4.5\ne    NaN\ndtype: float64\nFor ordered data like time series, you may want to do some interpolation or filling of\nvalues when reindexing. The method option allows us to do this, using a method such\nas ffill, which forward-fills the values:\n138 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 1656,
      "extraction_method": "Direct"
    },
    {
      "page_number": 157,
      "chapter": null,
      "content": "In [102]: obj3 = pd.Series([\"blue\", \"purple\", \"yellow\"], index=[0, 2, 4])\nIn [103]: obj3\nOut[103]: \n0      blue\n2    purple\n4    yellow\ndtype: object\nIn [104]: obj3.reindex(np.arange(6), method=\"ffill\")\nOut[104]: \n0      blue\n1      blue\n2    purple\n3    purple\n4    yellow\n5    yellow\ndtype: object\nWith DataFrame, reindex can alter the (row) index, columns, or both. When passed\nonly a sequence, it reindexes the rows in the result:\nIn [105]: frame = pd.DataFrame(np.arange(9).reshape((3, 3)),\n   .....:                      index=[\"a\", \"c\", \"d\"],\n   .....:                      columns=[\"Ohio\", \"Texas\", \"California\"])\nIn [106]: frame\nOut[106]: \n   Ohio  Texas  California\na     0      1           2\nc     3      4           5\nd     6      7           8\nIn [107]: frame2 = frame.reindex(index=[\"a\", \"b\", \"c\", \"d\"])\nIn [108]: frame2\nOut[108]: \n   Ohio  Texas  California\na   0.0    1.0         2.0\nb   NaN    NaN         NaN\nc   3.0    4.0         5.0\nd   6.0    7.0         8.0\nThe columns can be reindexed with the columns keyword:\nIn [109]: states = [\"Texas\", \"Utah\", \"California\"]\nIn [110]: frame.reindex(columns=states)\nOut[110]: \n   Texas  Utah  California\na      1   NaN           2\nc      4   NaN           5\nd      7   NaN           8\n5.2 Essential Functionality \n| \n139",
      "content_length": 1281,
      "extraction_method": "Direct"
    },
    {
      "page_number": 158,
      "chapter": null,
      "content": "Because \"Ohio\" was not in states, the data for that column is dropped from the\nresult.\nAnother way to reindex a particular axis is to pass the new axis labels as a positional\nargument and then specify the axis to reindex with the axis keyword:\nIn [111]: frame.reindex(states, axis=\"columns\")\nOut[111]: \n   Texas  Utah  California\na      1   NaN           2\nc      4   NaN           5\nd      7   NaN           8\nSee Table 5-3 for more about the arguments to reindex.\nTable 5-3. reindex function arguments\nArgument\nDescription\nlabels\nNew sequence to use as an index. Can be Index instance or any other sequence-like Python data structure.\nAn Index will be used exactly as is without any copying.\nindex\nUse the passed sequence as the new index labels.\ncolumns\nUse the passed sequence as the new column labels.\naxis\nThe axis to reindex, whether \"index\" (rows) or \"columns\". The default is \"index\". You can\nalternately do reindex(index=new_labels) or reindex(columns=new_labels).\nmethod\nInterpolation (fill) method; \"ffill\" fills forward, while \"bfill\" fills backward.\nfill_value\nSubstitute value to use when introducing missing data by reindexing. Use fill_value=\"missing\"\n(the default behavior) when you want absent labels to have null values in the result.\nlimit\nWhen forward filling or backfilling, the maximum size gap (in number of elements) to fill.\ntolerance\nWhen forward filling or backfilling, the maximum size gap (in absolute numeric distance) to fill for inexact\nmatches.\nlevel\nMatch simple Index on level of MultiIndex; otherwise select subset of.\ncopy\nIf True, always copy underlying data even if the new index is equivalent to the old index; if False, do not\ncopy the data when the indexes are equivalent.\nAs we’ll explore later in “Selection on DataFrame with loc and iloc” on page 147, you\ncan also reindex by using the loc operator, and many users prefer to always do it this\nway. This works only if all of the new index labels already exist in the DataFrame\n(whereas reindex will insert missing data for new labels):\nIn [112]: frame.loc[[\"a\", \"d\", \"c\"], [\"California\", \"Texas\"]]\nOut[112]: \n   California  Texas\na           2      1\nd           8      7\nc           5      4\n140 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 2235,
      "extraction_method": "Direct"
    },
    {
      "page_number": 159,
      "chapter": null,
      "content": "Dropping Entries from an Axis\nDropping one or more entries from an axis is simple if you already have an index\narray or list without those entries, since you can use the reindex method or .loc-\nbased indexing. As that can require a bit of munging and set logic, the drop method\nwill return a new object with the indicated value or values deleted from an axis:\nIn [113]: obj = pd.Series(np.arange(5.), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\nIn [114]: obj\nOut[114]: \na    0.0\nb    1.0\nc    2.0\nd    3.0\ne    4.0\ndtype: float64\nIn [115]: new_obj = obj.drop(\"c\")\nIn [116]: new_obj\nOut[116]: \na    0.0\nb    1.0\nd    3.0\ne    4.0\ndtype: float64\nIn [117]: obj.drop([\"d\", \"c\"])\nOut[117]: \na    0.0\nb    1.0\ne    4.0\ndtype: float64\nWith DataFrame, index values can be deleted from either axis. To illustrate this, we\nfirst create an example DataFrame:\nIn [118]: data = pd.DataFrame(np.arange(16).reshape((4, 4)),\n   .....:                     index=[\"Ohio\", \"Colorado\", \"Utah\", \"New York\"],\n   .....:                     columns=[\"one\", \"two\", \"three\", \"four\"])\nIn [119]: data\nOut[119]: \n          one  two  three  four\nOhio        0    1      2     3\nColorado    4    5      6     7\nUtah        8    9     10    11\nNew York   12   13     14    15\n5.2 Essential Functionality \n| \n141",
      "content_length": 1268,
      "extraction_method": "Direct"
    },
    {
      "page_number": 160,
      "chapter": null,
      "content": "Calling drop with a sequence of labels will drop values from the row labels (axis 0):\nIn [120]: data.drop(index=[\"Colorado\", \"Ohio\"])\nOut[120]: \n          one  two  three  four\nUtah        8    9     10    11\nNew York   12   13     14    15\nTo drop labels from the columns, instead use the columns keyword:\nIn [121]: data.drop(columns=[\"two\"])\nOut[121]: \n          one  three  four\nOhio        0      2     3\nColorado    4      6     7\nUtah        8     10    11\nNew York   12     14    15\nYou can also drop values from the columns by passing axis=1 (which is like NumPy)\nor axis=\"columns\":\nIn [122]: data.drop(\"two\", axis=1)\nOut[122]: \n          one  three  four\nOhio        0      2     3\nColorado    4      6     7\nUtah        8     10    11\nNew York   12     14    15\nIn [123]: data.drop([\"two\", \"four\"], axis=\"columns\")\nOut[123]: \n          one  three\nOhio        0      2\nColorado    4      6\nUtah        8     10\nNew York   12     14\nIndexing, Selection, and Filtering\nSeries indexing (obj[...]) works analogously to NumPy array indexing, except you\ncan use the Series’s index values instead of only integers. Here are some examples of\nthis:\nIn [124]: obj = pd.Series(np.arange(4.), index=[\"a\", \"b\", \"c\", \"d\"])\nIn [125]: obj\nOut[125]: \na    0.0\nb    1.0\nc    2.0\nd    3.0\ndtype: float64\n142 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 1340,
      "extraction_method": "Direct"
    },
    {
      "page_number": 161,
      "chapter": null,
      "content": "In [126]: obj[\"b\"]\nOut[126]: 1.0\nIn [127]: obj[1]\nOut[127]: 1.0\nIn [128]: obj[2:4]\nOut[128]: \nc    2.0\nd    3.0\ndtype: float64\nIn [129]: obj[[\"b\", \"a\", \"d\"]]\nOut[129]: \nb    1.0\na    0.0\nd    3.0\ndtype: float64\nIn [130]: obj[[1, 3]]\nOut[130]: \nb    1.0\nd    3.0\ndtype: float64\nIn [131]: obj[obj < 2]\nOut[131]: \na    0.0\nb    1.0\ndtype: float64\nWhile you can select data by label this way, the preferred way to select index values is\nwith the special loc operator:\nIn [132]: obj.loc[[\"b\", \"a\", \"d\"]]\nOut[132]: \nb    1.0\na    0.0\nd    3.0\ndtype: float64\nThe reason to prefer loc is because of the different treatment of integers when\nindexing with []. Regular []-based indexing will treat integers as labels if the index\ncontains integers, so the behavior differs depending on the data type of the index. For\nexample:\nIn [133]: obj1 = pd.Series([1, 2, 3], index=[2, 0, 1])\nIn [134]: obj2 = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\nIn [135]: obj1\nOut[135]: \n5.2 Essential Functionality \n| \n143",
      "content_length": 993,
      "extraction_method": "Direct"
    },
    {
      "page_number": 162,
      "chapter": null,
      "content": "2    1\n0    2\n1    3\ndtype: int64\nIn [136]: obj2\nOut[136]: \na    1\nb    2\nc    3\ndtype: int64\nIn [137]: obj1[[0, 1, 2]]\nOut[137]: \n0    2\n1    3\n2    1\ndtype: int64\nIn [138]: obj2[[0, 1, 2]]\nOut[138]: \na    1\nb    2\nc    3\ndtype: int64\nWhen using loc, the expression obj.loc[[0, 1, 2]] will fail when the index does\nnot contain integers:\nIn [134]: obj2.loc[[0, 1]]\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n/tmp/ipykernel_804589/4185657903.py in <module>\n----> 1 obj2.loc[[0, 1]]\n^ LONG EXCEPTION ABBREVIATED ^\nKeyError: \"None of [Int64Index([0, 1], dtype=\"int64\")] are in the [index]\"\nSince loc operator indexes exclusively with labels, there is also an iloc operator\nthat indexes exclusively with integers to work consistently whether or not the index\ncontains integers:\nIn [139]: obj1.iloc[[0, 1, 2]]\nOut[139]: \n2    1\n0    2\n1    3\ndtype: int64\nIn [140]: obj2.iloc[[0, 1, 2]]\nOut[140]: \na    1\n144 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 1054,
      "extraction_method": "Direct"
    },
    {
      "page_number": 163,
      "chapter": null,
      "content": "b    2\nc    3\ndtype: int64\nYou can also slice with labels, but it works differently from normal\nPython slicing in that the endpoint is inclusive:\nIn [141]: obj2.loc[\"b\":\"c\"]\nOut[141]: \nb    2\nc    3\ndtype: int64\nAssigning values using these methods modifies the corresponding section of the\nSeries:\nIn [142]: obj2.loc[\"b\":\"c\"] = 5\nIn [143]: obj2\nOut[143]: \na    1\nb    5\nc    5\ndtype: int64\nIt can be a common newbie error to try to call loc or iloc like\nfunctions rather than “indexing into” them with square brackets.\nThe square bracket notation is used to enable slice operations and\nto allow for indexing on multiple axes with DataFrame objects.\nIndexing into a DataFrame retrieves one or more columns either with a single value\nor sequence:\nIn [144]: data = pd.DataFrame(np.arange(16).reshape((4, 4)),\n   .....:                     index=[\"Ohio\", \"Colorado\", \"Utah\", \"New York\"],\n   .....:                     columns=[\"one\", \"two\", \"three\", \"four\"])\nIn [145]: data\nOut[145]: \n          one  two  three  four\nOhio        0    1      2     3\nColorado    4    5      6     7\nUtah        8    9     10    11\nNew York   12   13     14    15\nIn [146]: data[\"two\"]\nOut[146]: \nOhio         1\nColorado     5\nUtah         9\n5.2 Essential Functionality \n| \n145",
      "content_length": 1255,
      "extraction_method": "Direct"
    },
    {
      "page_number": 164,
      "chapter": null,
      "content": "New York    13\nName: two, dtype: int64\nIn [147]: data[[\"three\", \"one\"]]\nOut[147]: \n          three  one\nOhio          2    0\nColorado      6    4\nUtah         10    8\nNew York     14   12\nIndexing like this has a few special cases. The first is slicing or selecting data with a\nBoolean array:\nIn [148]: data[:2]\nOut[148]: \n          one  two  three  four\nOhio        0    1      2     3\nColorado    4    5      6     7\nIn [149]: data[data[\"three\"] > 5]\nOut[149]: \n          one  two  three  four\nColorado    4    5      6     7\nUtah        8    9     10    11\nNew York   12   13     14    15\nThe row selection syntax data[:2] is provided as a convenience. Passing a single\nelement or a list to the [] operator selects columns.\nAnother use case is indexing with a Boolean DataFrame, such as one produced by\na scalar comparison. Consider a DataFrame with all Boolean values produced by\ncomparing with a scalar value:\nIn [150]: data < 5\nOut[150]: \n            one    two  three   four\nOhio       True   True   True   True\nColorado   True  False  False  False\nUtah      False  False  False  False\nNew York  False  False  False  False\nWe can use this DataFrame to assign the value 0 to each location with the value True,\nlike so:\nIn [151]: data[data < 5] = 0\nIn [152]: data\nOut[152]: \n          one  two  three  four\nOhio        0    0      0     0\nColorado    0    5      6     7\n146 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 1422,
      "extraction_method": "Direct"
    },
    {
      "page_number": 165,
      "chapter": null,
      "content": "Utah        8    9     10    11\nNew York   12   13     14    15\nSelection on DataFrame with loc and iloc\nLike Series, DataFrame has special attributes loc and iloc for label-based and\ninteger-based indexing, respectively. Since DataFrame is two-dimensional, you can\nselect a subset of the rows and columns with NumPy-like notation using either axis\nlabels (loc) or integers (iloc).\nAs a first example, let’s select a single row by label:\nIn [153]: data\nOut[153]: \n          one  two  three  four\nOhio        0    0      0     0\nColorado    0    5      6     7\nUtah        8    9     10    11\nNew York   12   13     14    15\nIn [154]: data.loc[\"Colorado\"]\nOut[154]: \none      0\ntwo      5\nthree    6\nfour     7\nName: Colorado, dtype: int64\nThe result of selecting a single row is a Series with an index that contains the\nDataFrame’s column labels. To select multiple roles, creating a new DataFrame, pass a\nsequence of labels:\nIn [155]: data.loc[[\"Colorado\", \"New York\"]]\nOut[155]: \n          one  two  three  four\nColorado    0    5      6     7\nNew York   12   13     14    15\nYou can combine both row and column selection in loc by separating the selections\nwith a comma:\nIn [156]: data.loc[\"Colorado\", [\"two\", \"three\"]]\nOut[156]: \ntwo      5\nthree    6\nName: Colorado, dtype: int64\nWe’ll then perform some similar selections with integers using iloc:\nIn [157]: data.iloc[2]\nOut[157]: \none       8\ntwo       9\n5.2 Essential Functionality \n| \n147",
      "content_length": 1447,
      "extraction_method": "Direct"
    },
    {
      "page_number": 166,
      "chapter": null,
      "content": "three    10\nfour     11\nName: Utah, dtype: int64\nIn [158]: data.iloc[[2, 1]]\nOut[158]: \n          one  two  three  four\nUtah        8    9     10    11\nColorado    0    5      6     7\nIn [159]: data.iloc[2, [3, 0, 1]]\nOut[159]: \nfour    11\none      8\ntwo      9\nName: Utah, dtype: int64\nIn [160]: data.iloc[[1, 2], [3, 0, 1]]\nOut[160]: \n          four  one  two\nColorado     7    0    5\nUtah        11    8    9\nBoth indexing functions work with slices in addition to single labels or lists of labels:\nIn [161]: data.loc[:\"Utah\", \"two\"]\nOut[161]: \nOhio        0\nColorado    5\nUtah        9\nName: two, dtype: int64\nIn [162]: data.iloc[:, :3][data.three > 5]\nOut[162]: \n          one  two  three\nColorado    0    5      6\nUtah        8    9     10\nNew York   12   13     14\nBoolean arrays can be used with loc but not iloc:\nIn [163]: data.loc[data.three >= 2]\nOut[163]: \n          one  two  three  four\nColorado    0    5      6     7\nUtah        8    9     10    11\nNew York   12   13     14    15\nThere are many ways to select and rearrange the data contained in a pandas object.\nFor DataFrame, Table 5-4 provides a short summary of many of them. As you will see\nlater, there are a number of additional options for working with hierarchical indexes.\n148 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 1296,
      "extraction_method": "Direct"
    },
    {
      "page_number": 167,
      "chapter": null,
      "content": "Table 5-4. Indexing options with DataFrame\nType\nNotes\ndf[column]\nSelect single column or sequence of columns from the DataFrame; special case conveniences:\nBoolean array (filter rows), slice (slice rows), or Boolean DataFrame (set values based on some\ncriterion)\ndf.loc[rows]\nSelect single row or subset of rows from the DataFrame by label\ndf.loc[:, cols]\nSelect single column or subset of columns by label\ndf.loc[rows, cols]\nSelect both row(s) and column(s) by label\ndf.iloc[rows]\nSelect single row or subset of rows from the DataFrame by integer position\ndf.iloc[:, cols]\nSelect single column or subset of columns by integer position\ndf.iloc[rows, cols] Select both row(s) and column(s) by integer position\ndf.at[row, col]\nSelect a single scalar value by row and column label\ndf.iat[row, col]\nSelect a single scalar value by row and column position (integers)\nreindex method\nSelect either rows or columns by labels\nInteger indexing pitfalls\nWorking with pandas objects indexed by integers can be a stumbling block for new\nusers since they work differently from built-in Python data structures like lists and\ntuples. For example, you might not expect the following code to generate an error:\nIn [164]: ser = pd.Series(np.arange(3.))\nIn [165]: ser\nOut[165]: \n0    0.0\n1    1.0\n2    2.0\ndtype: float64\nIn [166]: ser[-1]\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/miniconda/envs/book-env/lib/python3.10/site-packages/pandas/core/indexes/range.p\ny in get_loc(self, key, method, tolerance)\n    384                 try:\n--> 385                     return self._range.index(new_key)\n    386                 except ValueError as err:\nValueError: -1 is not in range\nThe above exception was the direct cause of the following exception:\nKeyError                                  Traceback (most recent call last)\n<ipython-input-166-44969a759c20> in <module>\n----> 1 ser[-1]\n/miniconda/envs/book-env/lib/python3.10/site-packages/pandas/core/series.py in __\ngetitem__(self, key)\n    956 \n    957         elif key_is_scalar:\n--> 958             return self._get_value(key)\n5.2 Essential Functionality \n| \n149",
      "content_length": 2205,
      "extraction_method": "Direct"
    },
    {
      "page_number": 168,
      "chapter": null,
      "content": "959 \n    960         if is_hashable(key):\n/miniconda/envs/book-env/lib/python3.10/site-packages/pandas/core/series.py in _g\net_value(self, label, takeable)\n   1067 \n   1068         # Similar to Index.get_value, but we do not fall back to position\nal\n-> 1069         loc = self.index.get_loc(label)\n   1070         return self.index._get_values_for_loc(self, loc, label)\n   1071 \n/miniconda/envs/book-env/lib/python3.10/site-packages/pandas/core/indexes/range.p\ny in get_loc(self, key, method, tolerance)\n    385                     return self._range.index(new_key)\n    386                 except ValueError as err:\n--> 387                     raise KeyError(key) from err\n    388             self._check_indexing_error(key)\n    389             raise KeyError(key)\nKeyError: -1\nIn this case, pandas could “fall back” on integer indexing, but it is difficult to do\nthis in general without introducing subtle bugs into the user code. Here we have an\nindex containing 0, 1, and 2, but pandas does not want to guess what the user wants\n(label-based indexing or position-based):\nIn [167]: ser\nOut[167]: \n0    0.0\n1    1.0\n2    2.0\ndtype: float64\nOn the other hand, with a noninteger index, there is no such ambiguity:\nIn [168]: ser2 = pd.Series(np.arange(3.), index=[\"a\", \"b\", \"c\"])\nIn [169]: ser2[-1]\nOut[169]: 2.0\nIf you have an axis index containing integers, data selection will always be label\noriented. As I said above, if you use loc (for labels) or iloc (for integers) you will get\nexactly what you want:\nIn [170]: ser.iloc[-1]\nOut[170]: 2.0\nOn the other hand, slicing with integers is always integer oriented:\nIn [171]: ser[:2]\nOut[171]: \n0    0.0\n1    1.0\ndtype: float64\n150 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 1722,
      "extraction_method": "Direct"
    },
    {
      "page_number": 169,
      "chapter": null,
      "content": "As a result of these pitfalls, it is best to always prefer indexing with loc and iloc to\navoid ambiguity.\nPitfalls with chained indexing\nIn the previous section we looked at how you can do flexible selections on a Data‐\nFrame using loc and iloc. These indexing attributes can also be used to modify\nDataFrame objects in place, but doing so requires some care.\nFor example, in the example DataFrame above, we can assign to a column or row by\nlabel or integer position:\nIn [172]: data.loc[:, \"one\"] = 1\nIn [173]: data\nOut[173]: \n          one  two  three  four\nOhio        1    0      0     0\nColorado    1    5      6     7\nUtah        1    9     10    11\nNew York    1   13     14    15\nIn [174]: data.iloc[2] = 5\nIn [175]: data\nOut[175]: \n          one  two  three  four\nOhio        1    0      0     0\nColorado    1    5      6     7\nUtah        5    5      5     5\nNew York    1   13     14    15\nIn [176]: data.loc[data[\"four\"] > 5] = 3\nIn [177]: data\nOut[177]: \n          one  two  three  four\nOhio        1    0      0     0\nColorado    3    3      3     3\nUtah        5    5      5     5\nNew York    3    3      3     3\nA common gotcha for new pandas users is to chain selections when assigning, like\nthis:\nIn [177]: data.loc[data.three == 5][\"three\"] = 6\n<ipython-input-11-0ed1cf2155d5>:1: SettingWithCopyWarning:\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\nDepending on the data contents, this may print a special SettingWithCopyWarning,\nwhich warns you that you are trying to modify a temporary value (the nonempty\n5.2 Essential Functionality \n| \n151",
      "content_length": 1639,
      "extraction_method": "Direct"
    },
    {
      "page_number": 170,
      "chapter": null,
      "content": "result of data.loc[data.three == 5]) instead of the original DataFrame data,\nwhich might be what you were intending. Here, data was unmodified:\nIn [179]: data\nOut[179]: \n          one  two  three  four\nOhio        1    0      0     0\nColorado    3    3      3     3\nUtah        5    5      5     5\nNew York    3    3      3     3\nIn these scenarios, the fix is to rewrite the chained assignment to use a single loc\noperation:\nIn [180]: data.loc[data.three == 5, \"three\"] = 6\nIn [181]: data\nOut[181]: \n          one  two  three  four\nOhio        1    0      0     0\nColorado    3    3      3     3\nUtah        5    5      6     5\nNew York    3    3      3     3\nA good rule of thumb is to avoid chained indexing when doing assignments. There\nare other cases where pandas will generate SettingWithCopyWarning that have to do\nwith chained indexing. I refer you to this topic in the online pandas documentation.\nArithmetic and Data Alignment\npandas can make it much simpler to work with objects that have different indexes.\nFor example, when you add objects, if any index pairs are not the same, the respective\nindex in the result will be the union of the index pairs. Let’s look at an example:\nIn [182]: s1 = pd.Series([7.3, -2.5, 3.4, 1.5], index=[\"a\", \"c\", \"d\", \"e\"])\nIn [183]: s2 = pd.Series([-2.1, 3.6, -1.5, 4, 3.1],\n   .....:                index=[\"a\", \"c\", \"e\", \"f\", \"g\"])\nIn [184]: s1\nOut[184]: \na    7.3\nc   -2.5\nd    3.4\ne    1.5\ndtype: float64\nIn [185]: s2\nOut[185]: \na   -2.1\nc    3.6\ne   -1.5\n152 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 1549,
      "extraction_method": "Direct"
    },
    {
      "page_number": 171,
      "chapter": null,
      "content": "f    4.0\ng    3.1\ndtype: float64\nAdding these yields:\nIn [186]: s1 + s2\nOut[186]: \na    5.2\nc    1.1\nd    NaN\ne    0.0\nf    NaN\ng    NaN\ndtype: float64\nThe internal data alignment introduces missing values in the label locations that don’t\noverlap. Missing values will then propagate in further arithmetic computations.\nIn the case of DataFrame, alignment is performed on both rows and columns:\nIn [187]: df1 = pd.DataFrame(np.arange(9.).reshape((3, 3)), columns=list(\"bcd\"),\n   .....:                    index=[\"Ohio\", \"Texas\", \"Colorado\"])\nIn [188]: df2 = pd.DataFrame(np.arange(12.).reshape((4, 3)), columns=list(\"bde\"),\n   .....:                    index=[\"Utah\", \"Ohio\", \"Texas\", \"Oregon\"])\nIn [189]: df1\nOut[189]: \n            b    c    d\nOhio      0.0  1.0  2.0\nTexas     3.0  4.0  5.0\nColorado  6.0  7.0  8.0\nIn [190]: df2\nOut[190]: \n          b     d     e\nUtah    0.0   1.0   2.0\nOhio    3.0   4.0   5.0\nTexas   6.0   7.0   8.0\nOregon  9.0  10.0  11.0\nAdding these returns a DataFrame with index and columns that are the unions of the\nones in each DataFrame:\nIn [191]: df1 + df2\nOut[191]: \n            b   c     d   e\nColorado  NaN NaN   NaN NaN\nOhio      3.0 NaN   6.0 NaN\nOregon    NaN NaN   NaN NaN\nTexas     9.0 NaN  12.0 NaN\nUtah      NaN NaN   NaN NaN\n5.2 Essential Functionality \n| \n153",
      "content_length": 1303,
      "extraction_method": "Direct"
    },
    {
      "page_number": 172,
      "chapter": null,
      "content": "Since the \"c\" and \"e\" columns are not found in both DataFrame objects, they appear\nas missing in the result. The same holds for the rows with labels that are not common\nto both objects.\nIf you add DataFrame objects with no column or row labels in common, the result\nwill contain all nulls:\nIn [192]: df1 = pd.DataFrame({\"A\": [1, 2]})\nIn [193]: df2 = pd.DataFrame({\"B\": [3, 4]})\nIn [194]: df1\nOut[194]: \n   A\n0  1\n1  2\nIn [195]: df2\nOut[195]: \n   B\n0  3\n1  4\nIn [196]: df1 + df2\nOut[196]: \n    A   B\n0 NaN NaN\n1 NaN NaN\nArithmetic methods with fill values\nIn arithmetic operations between differently indexed objects, you might want to fill\nwith a special value, like 0, when an axis label is found in one object but not the other.\nHere is an example where we set a particular value to NA (null) by assigning np.nan\nto it:\nIn [197]: df1 = pd.DataFrame(np.arange(12.).reshape((3, 4)),\n   .....:                    columns=list(\"abcd\"))\nIn [198]: df2 = pd.DataFrame(np.arange(20.).reshape((4, 5)),\n   .....:                    columns=list(\"abcde\"))\nIn [199]: df2.loc[1, \"b\"] = np.nan\nIn [200]: df1\nOut[200]: \n     a    b     c     d\n0  0.0  1.0   2.0   3.0\n1  4.0  5.0   6.0   7.0\n2  8.0  9.0  10.0  11.0\n154 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 1249,
      "extraction_method": "Direct"
    },
    {
      "page_number": 173,
      "chapter": null,
      "content": "In [201]: df2\nOut[201]: \n      a     b     c     d     e\n0   0.0   1.0   2.0   3.0   4.0\n1   5.0   NaN   7.0   8.0   9.0\n2  10.0  11.0  12.0  13.0  14.0\n3  15.0  16.0  17.0  18.0  19.0\nAdding these results in missing values in the locations that don’t overlap:\nIn [202]: df1 + df2\nOut[202]: \n      a     b     c     d   e\n0   0.0   2.0   4.0   6.0 NaN\n1   9.0   NaN  13.0  15.0 NaN\n2  18.0  20.0  22.0  24.0 NaN\n3   NaN   NaN   NaN   NaN NaN\nUsing the add method on df1, I pass df2 and an argument to fill_value, which\nsubstitutes the passed value for any missing values in the operation:\nIn [203]: df1.add(df2, fill_value=0)\nOut[203]: \n      a     b     c     d     e\n0   0.0   2.0   4.0   6.0   4.0\n1   9.0   5.0  13.0  15.0   9.0\n2  18.0  20.0  22.0  24.0  14.0\n3  15.0  16.0  17.0  18.0  19.0\nSee Table 5-5 for a listing of Series and DataFrame methods for arithmetic. Each has\na counterpart, starting with the letter r, that has arguments reversed. So these two\nstatements are equivalent:\nIn [204]: 1 / df1\nOut[204]: \n       a         b         c         d\n0    inf  1.000000  0.500000  0.333333\n1  0.250  0.200000  0.166667  0.142857\n2  0.125  0.111111  0.100000  0.090909\nIn [205]: df1.rdiv(1)\nOut[205]: \n       a         b         c         d\n0    inf  1.000000  0.500000  0.333333\n1  0.250  0.200000  0.166667  0.142857\n2  0.125  0.111111  0.100000  0.090909\nRelatedly, when reindexing a Series or DataFrame, you can also specify a different fill\nvalue:\nIn [206]: df1.reindex(columns=df2.columns, fill_value=0)\nOut[206]: \n     a    b     c     d  e\n0  0.0  1.0   2.0   3.0  0\n5.2 Essential Functionality \n| \n155",
      "content_length": 1620,
      "extraction_method": "Direct"
    },
    {
      "page_number": 174,
      "chapter": null,
      "content": "1  4.0  5.0   6.0   7.0  0\n2  8.0  9.0  10.0  11.0  0\nTable 5-5. Flexible arithmetic methods\nMethod\nDescription\nadd, radd\nMethods for addition (+)\nsub, rsub\nMethods for subtraction (-)\ndiv, rdiv\nMethods for division (/)\nfloordiv, rfloordiv\nMethods for floor division (//)\nmul, rmul\nMethods for multiplication (*)\npow, rpow\nMethods for exponentiation (**)\nOperations between DataFrame and Series\nAs with NumPy arrays of different dimensions, arithmetic between DataFrame and\nSeries is also defined. First, as a motivating example, consider the difference between\na two-dimensional array and one of its rows:\nIn [207]: arr = np.arange(12.).reshape((3, 4))\nIn [208]: arr\nOut[208]: \narray([[ 0.,  1.,  2.,  3.],\n       [ 4.,  5.,  6.,  7.],\n       [ 8.,  9., 10., 11.]])\nIn [209]: arr[0]\nOut[209]: array([0., 1., 2., 3.])\nIn [210]: arr - arr[0]\nOut[210]: \narray([[0., 0., 0., 0.],\n       [4., 4., 4., 4.],\n       [8., 8., 8., 8.]])\nWhen we subtract arr[0] from arr, the subtraction is performed once for each row.\nThis is referred to as broadcasting and is explained in more detail as it relates to\ngeneral NumPy arrays in Appendix A. Operations between a DataFrame and a Series\nare similar:\nIn [211]: frame = pd.DataFrame(np.arange(12.).reshape((4, 3)),\n   .....:                      columns=list(\"bde\"),\n   .....:                      index=[\"Utah\", \"Ohio\", \"Texas\", \"Oregon\"])\nIn [212]: series = frame.iloc[0]\nIn [213]: frame\nOut[213]: \n          b     d     e\n156 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 1507,
      "extraction_method": "Direct"
    },
    {
      "page_number": 175,
      "chapter": null,
      "content": "Utah    0.0   1.0   2.0\nOhio    3.0   4.0   5.0\nTexas   6.0   7.0   8.0\nOregon  9.0  10.0  11.0\nIn [214]: series\nOut[214]: \nb    0.0\nd    1.0\ne    2.0\nName: Utah, dtype: float64\nBy default, arithmetic between DataFrame and Series matches the index of the Series\non the columns of the DataFrame, broadcasting down the rows:\nIn [215]: frame - series\nOut[215]: \n          b    d    e\nUtah    0.0  0.0  0.0\nOhio    3.0  3.0  3.0\nTexas   6.0  6.0  6.0\nOregon  9.0  9.0  9.0\nIf an index value is not found in either the DataFrame’s columns or the Series’s index,\nthe objects will be reindexed to form the union:\nIn [216]: series2 = pd.Series(np.arange(3), index=[\"b\", \"e\", \"f\"])\nIn [217]: series2\nOut[217]: \nb    0\ne    1\nf    2\ndtype: int64\nIn [218]: frame + series2\nOut[218]: \n          b   d     e   f\nUtah    0.0 NaN   3.0 NaN\nOhio    3.0 NaN   6.0 NaN\nTexas   6.0 NaN   9.0 NaN\nOregon  9.0 NaN  12.0 NaN\nIf you want to instead broadcast over the columns, matching on the rows, you have to\nuse one of the arithmetic methods and specify to match over the index. For example:\nIn [219]: series3 = frame[\"d\"]\nIn [220]: frame\nOut[220]: \n          b     d     e\nUtah    0.0   1.0   2.0\nOhio    3.0   4.0   5.0\n5.2 Essential Functionality \n| \n157",
      "content_length": 1237,
      "extraction_method": "Direct"
    },
    {
      "page_number": 176,
      "chapter": null,
      "content": "Texas   6.0   7.0   8.0\nOregon  9.0  10.0  11.0\nIn [221]: series3\nOut[221]: \nUtah       1.0\nOhio       4.0\nTexas      7.0\nOregon    10.0\nName: d, dtype: float64\nIn [222]: frame.sub(series3, axis=\"index\")\nOut[222]: \n          b    d    e\nUtah   -1.0  0.0  1.0\nOhio   -1.0  0.0  1.0\nTexas  -1.0  0.0  1.0\nOregon -1.0  0.0  1.0\nThe axis that you pass is the axis to match on. In this case we mean to match on the\nDataFrame’s row index (axis=\"index\") and broadcast across the columns.\nFunction Application and Mapping\nNumPy ufuncs (element-wise array methods) also work with pandas objects:\nIn [223]: frame = pd.DataFrame(np.random.standard_normal((4, 3)),\n   .....:                      columns=list(\"bde\"),\n   .....:                      index=[\"Utah\", \"Ohio\", \"Texas\", \"Oregon\"])\nIn [224]: frame\nOut[224]: \n               b         d         e\nUtah   -0.204708  0.478943 -0.519439\nOhio   -0.555730  1.965781  1.393406\nTexas   0.092908  0.281746  0.769023\nOregon  1.246435  1.007189 -1.296221\nIn [225]: np.abs(frame)\nOut[225]: \n               b         d         e\nUtah    0.204708  0.478943  0.519439\nOhio    0.555730  1.965781  1.393406\nTexas   0.092908  0.281746  0.769023\nOregon  1.246435  1.007189  1.296221\nAnother frequent operation is applying a function on one-dimensional arrays to each\ncolumn or row. DataFrame’s apply method does exactly this:\nIn [226]: def f1(x):\n   .....:     return x.max() - x.min()\n158 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 1460,
      "extraction_method": "Direct"
    },
    {
      "page_number": 177,
      "chapter": null,
      "content": "In [227]: frame.apply(f1)\nOut[227]: \nb    1.802165\nd    1.684034\ne    2.689627\ndtype: float64\nHere the function f, which computes the difference between the maximum and\nminimum of a Series, is invoked once on each column in frame. The result is a Series\nhaving the columns of frame as its index.\nIf you pass axis=\"columns\" to apply, the function will be invoked once per row\ninstead. A helpful way to think about this is as “apply across the columns”:\nIn [228]: frame.apply(f1, axis=\"columns\")\nOut[228]: \nUtah      0.998382\nOhio      2.521511\nTexas     0.676115\nOregon    2.542656\ndtype: float64\nMany of the most common array statistics (like sum and mean) are DataFrame meth‐\nods, so using apply is not necessary.\nThe function passed to apply need not return a scalar value; it can also return a Series\nwith multiple values:\nIn [229]: def f2(x):\n   .....:     return pd.Series([x.min(), x.max()], index=[\"min\", \"max\"])\nIn [230]: frame.apply(f2)\nOut[230]: \n            b         d         e\nmin -0.555730  0.281746 -1.296221\nmax  1.246435  1.965781  1.393406\nElement-wise Python functions can be used, too. Suppose you wanted to compute\na formatted string from each floating-point value in frame. You can do this with\napplymap:\nIn [231]: def my_format(x):\n   .....:     return f\"{x:.2f}\"\nIn [232]: frame.applymap(my_format)\nOut[232]: \n            b     d      e\nUtah    -0.20  0.48  -0.52\nOhio    -0.56  1.97   1.39\nTexas    0.09  0.28   0.77\nOregon   1.25  1.01  -1.30\n5.2 Essential Functionality \n| \n159",
      "content_length": 1505,
      "extraction_method": "Direct"
    },
    {
      "page_number": 178,
      "chapter": null,
      "content": "The reason for the name applymap is that Series has a map method for applying an\nelement-wise function:\nIn [233]: frame[\"e\"].map(my_format)\nOut[233]: \nUtah      -0.52\nOhio       1.39\nTexas      0.77\nOregon    -1.30\nName: e, dtype: object\nSorting and Ranking\nSorting a dataset by some criterion is another important built-in operation. To sort\nlexicographically by row or column label, use the sort_index method, which returns\na new, sorted object:\nIn [234]: obj = pd.Series(np.arange(4), index=[\"d\", \"a\", \"b\", \"c\"])\nIn [235]: obj\nOut[235]: \nd    0\na    1\nb    2\nc    3\ndtype: int64\nIn [236]: obj.sort_index()\nOut[236]: \na    1\nb    2\nc    3\nd    0\ndtype: int64\nWith a DataFrame, you can sort by index on either axis:\nIn [237]: frame = pd.DataFrame(np.arange(8).reshape((2, 4)),\n   .....:                      index=[\"three\", \"one\"],\n   .....:                      columns=[\"d\", \"a\", \"b\", \"c\"])\nIn [238]: frame\nOut[238]: \n       d  a  b  c\nthree  0  1  2  3\none    4  5  6  7\nIn [239]: frame.sort_index()\nOut[239]: \n       d  a  b  c\none    4  5  6  7\nthree  0  1  2  3\n160 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 1115,
      "extraction_method": "Direct"
    },
    {
      "page_number": 179,
      "chapter": null,
      "content": "In [240]: frame.sort_index(axis=\"columns\")\nOut[240]: \n       a  b  c  d\nthree  1  2  3  0\none    5  6  7  4\nThe data is sorted in ascending order by default but can be sorted in descending\norder, too:\nIn [241]: frame.sort_index(axis=\"columns\", ascending=False)\nOut[241]: \n       d  c  b  a\nthree  0  3  2  1\none    4  7  6  5\nTo sort a Series by its values, use its sort_values method:\nIn [242]: obj = pd.Series([4, 7, -3, 2])\nIn [243]: obj.sort_values()\nOut[243]: \n2   -3\n3    2\n0    4\n1    7\ndtype: int64\nAny missing values are sorted to the end of the Series by default:\nIn [244]: obj = pd.Series([4, np.nan, 7, np.nan, -3, 2])\nIn [245]: obj.sort_values()\nOut[245]: \n4   -3.0\n5    2.0\n0    4.0\n2    7.0\n1    NaN\n3    NaN\ndtype: float64\nMissing values can be sorted to the start instead by using the na_position option:\nIn [246]: obj.sort_values(na_position=\"first\")\nOut[246]: \n1    NaN\n3    NaN\n4   -3.0\n5    2.0\n0    4.0\n2    7.0\ndtype: float64\n5.2 Essential Functionality \n| \n161",
      "content_length": 984,
      "extraction_method": "Direct"
    },
    {
      "page_number": 180,
      "chapter": null,
      "content": "When sorting a DataFrame, you can use the data in one or more columns as the sort\nkeys. To do so, pass one or more column names to sort_values:\nIn [247]: frame = pd.DataFrame({\"b\": [4, 7, -3, 2], \"a\": [0, 1, 0, 1]})\nIn [248]: frame\nOut[248]: \n   b  a\n0  4  0\n1  7  1\n2 -3  0\n3  2  1\nIn [249]: frame.sort_values(\"b\")\nOut[249]: \n   b  a\n2 -3  0\n3  2  1\n0  4  0\n1  7  1\nTo sort by multiple columns, pass a list of names:\nIn [250]: frame.sort_values([\"a\", \"b\"])\nOut[250]: \n   b  a\n2 -3  0\n0  4  0\n3  2  1\n1  7  1\nRanking assigns ranks from one through the number of valid data points in an array,\nstarting from the lowest value. The rank methods for Series and DataFrame are the\nplace to look; by default, rank breaks ties by assigning each group the mean rank:\nIn [251]: obj = pd.Series([7, -5, 7, 4, 2, 0, 4])\nIn [252]: obj.rank()\nOut[252]: \n0    6.5\n1    1.0\n2    6.5\n3    4.5\n4    3.0\n5    2.0\n6    4.5\ndtype: float64\n162 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 964,
      "extraction_method": "Direct"
    },
    {
      "page_number": 181,
      "chapter": null,
      "content": "Ranks can also be assigned according to the order in which they’re observed in the\ndata:\nIn [253]: obj.rank(method=\"first\")\nOut[253]: \n0    6.0\n1    1.0\n2    7.0\n3    4.0\n4    3.0\n5    2.0\n6    5.0\ndtype: float64\nHere, instead of using the average rank 6.5 for the entries 0 and 2, they instead have\nbeen set to 6 and 7 because label 0 precedes label 2 in the data.\nYou can rank in descending order, too:\nIn [254]: obj.rank(ascending=False)\nOut[254]: \n0    1.5\n1    7.0\n2    1.5\n3    3.5\n4    5.0\n5    6.0\n6    3.5\ndtype: float64\nSee Table 5-6 for a list of tie-breaking methods available.\nDataFrame can compute ranks over the rows or the columns:\nIn [255]: frame = pd.DataFrame({\"b\": [4.3, 7, -3, 2], \"a\": [0, 1, 0, 1],\n   .....:                       \"c\": [-2, 5, 8, -2.5]})\nIn [256]: frame\nOut[256]: \n     b  a    c\n0  4.3  0 -2.0\n1  7.0  1  5.0\n2 -3.0  0  8.0\n3  2.0  1 -2.5\nIn [257]: frame.rank(axis=\"columns\")\nOut[257]: \n     b    a    c\n0  3.0  2.0  1.0\n1  3.0  1.0  2.0\n2  1.0  2.0  3.0\n3  3.0  2.0  1.0\n5.2 Essential Functionality \n| \n163",
      "content_length": 1047,
      "extraction_method": "Direct"
    },
    {
      "page_number": 182,
      "chapter": null,
      "content": "Table 5-6. Tie-breaking methods with rank\nMethod\nDescription\n\"average\"\nDefault: assign the average rank to each entry in the equal group\n\"min\"\nUse the minimum rank for the whole group\n\"max\"\nUse the maximum rank for the whole group\n\"first\"\nAssign ranks in the order the values appear in the data\n\"dense\"\nLike method=\"min\", but ranks always increase by 1 between groups rather than the number of equal\nelements in a group\nAxis Indexes with Duplicate Labels\nUp until now almost all of the examples we have looked at have unique axis labels\n(index values). While many pandas functions (like reindex) require that the labels be\nunique, it’s not mandatory. Let’s consider a small Series with duplicate indices:\nIn [258]: obj = pd.Series(np.arange(5), index=[\"a\", \"a\", \"b\", \"b\", \"c\"])\nIn [259]: obj\nOut[259]: \na    0\na    1\nb    2\nb    3\nc    4\ndtype: int64\nThe is_unique property of the index can tell you whether or not its labels are unique:\nIn [260]: obj.index.is_unique\nOut[260]: False\nData selection is one of the main things that behaves differently with duplicates.\nIndexing a label with multiple entries returns a Series, while single entries return a\nscalar value:\nIn [261]: obj[\"a\"]\nOut[261]: \na    0\na    1\ndtype: int64\nIn [262]: obj[\"c\"]\nOut[262]: 4\nThis can make your code more complicated, as the output type from indexing can\nvary based on whether or not a label is repeated.\nThe same logic extends to indexing rows (or columns) in a DataFrame:\n164 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 1500,
      "extraction_method": "Direct"
    },
    {
      "page_number": 183,
      "chapter": null,
      "content": "In [263]: df = pd.DataFrame(np.random.standard_normal((5, 3)),\n   .....:                   index=[\"a\", \"a\", \"b\", \"b\", \"c\"])\nIn [264]: df\nOut[264]: \n          0         1         2\na  0.274992  0.228913  1.352917\na  0.886429 -2.001637 -0.371843\nb  1.669025 -0.438570 -0.539741\nb  0.476985  3.248944 -1.021228\nc -0.577087  0.124121  0.302614\nIn [265]: df.loc[\"b\"]\nOut[265]: \n          0         1         2\nb  1.669025 -0.438570 -0.539741\nb  0.476985  3.248944 -1.021228\nIn [266]: df.loc[\"c\"]\nOut[266]: \n0   -0.577087\n1    0.124121\n2    0.302614\nName: c, dtype: float64\n5.3 Summarizing and Computing Descriptive Statistics\npandas objects are equipped with a set of common mathematical and statistical meth‐\nods. Most of these fall into the category of reductions or summary statistics, methods\nthat extract a single value (like the sum or mean) from a Series, or a Series of values\nfrom the rows or columns of a DataFrame. Compared with the similar methods\nfound on NumPy arrays, they have built-in handling for missing data. Consider a\nsmall DataFrame:\nIn [267]: df = pd.DataFrame([[1.4, np.nan], [7.1, -4.5],\n   .....:                    [np.nan, np.nan], [0.75, -1.3]],\n   .....:                   index=[\"a\", \"b\", \"c\", \"d\"],\n   .....:                   columns=[\"one\", \"two\"])\nIn [268]: df\nOut[268]: \n    one  two\na  1.40  NaN\nb  7.10 -4.5\nc   NaN  NaN\nd  0.75 -1.3\n5.3 Summarizing and Computing Descriptive Statistics \n| \n165",
      "content_length": 1428,
      "extraction_method": "Direct"
    },
    {
      "page_number": 184,
      "chapter": null,
      "content": "Calling DataFrame’s sum method returns a Series containing column sums:\nIn [269]: df.sum()\nOut[269]: \none    9.25\ntwo   -5.80\ndtype: float64\nPassing axis=\"columns\" or axis=1 sums across the columns instead:\nIn [270]: df.sum(axis=\"columns\")\nOut[270]: \na    1.40\nb    2.60\nc    0.00\nd   -0.55\ndtype: float64\nWhen an entire row or column contains all NA values, the sum is 0, whereas if any\nvalue is not NA, then the result is NA. This can be disabled with the skipna option, in\nwhich case any NA value in a row or column names the corresponding result NA:\nIn [271]: df.sum(axis=\"index\", skipna=False)\nOut[271]: \none   NaN\ntwo   NaN\ndtype: float64\nIn [272]: df.sum(axis=\"columns\", skipna=False)\nOut[272]: \na     NaN\nb    2.60\nc     NaN\nd   -0.55\ndtype: float64\nSome aggregations, like mean, require at least one non-NA value to yield a value\nresult, so here we have:\nIn [273]: df.mean(axis=\"columns\")\nOut[273]: \na    1.400\nb    1.300\nc      NaN\nd   -0.275\ndtype: float64\nSee Table 5-7 for a list of common options for each reduction method.\n166 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 1084,
      "extraction_method": "Direct"
    },
    {
      "page_number": 185,
      "chapter": null,
      "content": "Table 5-7. Options for reduction methods\nMethod\nDescription\naxis\nAxis to reduce over; “index” for DataFrame’s rows and “columns” for columns\nskipna\nExclude missing values; True by default\nlevel\nReduce grouped by level if the axis is hierarchically indexed (MultiIndex)\nSome methods, like idxmin and idxmax, return indirect statistics, like the index value\nwhere the minimum or maximum values are attained:\nIn [274]: df.idxmax()\nOut[274]: \none    b\ntwo    d\ndtype: object\nOther methods are accumulations:\nIn [275]: df.cumsum()\nOut[275]: \n    one  two\na  1.40  NaN\nb  8.50 -4.5\nc   NaN  NaN\nd  9.25 -5.8\nSome methods are neither reductions nor accumulations. describe is one such\nexample, producing multiple summary statistics in one shot:\nIn [276]: df.describe()\nOut[276]: \n            one       two\ncount  3.000000  2.000000\nmean   3.083333 -2.900000\nstd    3.493685  2.262742\nmin    0.750000 -4.500000\n25%    1.075000 -3.700000\n50%    1.400000 -2.900000\n75%    4.250000 -2.100000\nmax    7.100000 -1.300000\nOn nonnumeric data, describe produces alternative summary statistics:\nIn [277]: obj = pd.Series([\"a\", \"a\", \"b\", \"c\"] * 4)\nIn [278]: obj.describe()\nOut[278]: \ncount     16\nunique     3\ntop        a\nfreq       8\ndtype: object\n5.3 Summarizing and Computing Descriptive Statistics \n| \n167",
      "content_length": 1291,
      "extraction_method": "Direct"
    },
    {
      "page_number": 186,
      "chapter": null,
      "content": "See Table 5-8 for a full list of summary statistics and related methods.\nTable 5-8. Descriptive and summary statistics\nMethod\nDescription\ncount\nNumber of non-NA values\ndescribe\nCompute set of summary statistics\nmin, max\nCompute minimum and maximum values\nargmin, argmax Compute index locations (integers) at which minimum or maximum value is obtained, respectively;\nnot available on DataFrame objects\nidxmin, idxmax Compute index labels at which minimum or maximum value is obtained, respectively\nquantile\nCompute sample quantile ranging from 0 to 1 (default: 0.5)\nsum\nSum of values\nmean\nMean of values\nmedian\nArithmetic median (50% quantile) of values\nmad\nMean absolute deviation from mean value\nprod\nProduct of all values\nvar\nSample variance of values\nstd\nSample standard deviation of values\nskew\nSample skewness (third moment) of values\nkurt\nSample kurtosis (fourth moment) of values\ncumsum\nCumulative sum of values\ncummin, cummax\nCumulative minimum or maximum of values, respectively\ncumprod\nCumulative product of values\ndiff\nCompute first arithmetic difference (useful for time series)\npct_change\nCompute percent changes\nCorrelation and Covariance\nSome summary statistics, like correlation and covariance, are computed from pairs\nof arguments. Let’s consider some DataFrames of stock prices and volumes originally\nobtained from Yahoo! Finance and available in binary Python pickle files you can\nfind in the accompanying datasets for the book:\nIn [279]: price = pd.read_pickle(\"examples/yahoo_price.pkl\")\nIn [280]: volume = pd.read_pickle(\"examples/yahoo_volume.pkl\")\nI now compute percent changes of the prices, a time series operation that will be\nexplored further in Chapter 11:\nIn [281]: returns = price.pct_change()\nIn [282]: returns.tail()\nOut[282]: \n                AAPL      GOOG       IBM      MSFT\n168 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 1858,
      "extraction_method": "Direct"
    },
    {
      "page_number": 187,
      "chapter": null,
      "content": "Date                                              \n2016-10-17 -0.000680  0.001837  0.002072 -0.003483\n2016-10-18 -0.000681  0.019616 -0.026168  0.007690\n2016-10-19 -0.002979  0.007846  0.003583 -0.002255\n2016-10-20 -0.000512 -0.005652  0.001719 -0.004867\n2016-10-21 -0.003930  0.003011 -0.012474  0.042096\nThe corr method of Series computes the correlation of the overlapping, non-NA,\naligned-by-index values in two Series. Relatedly, cov computes the covariance:\nIn [283]: returns[\"MSFT\"].corr(returns[\"IBM\"])\nOut[283]: 0.49976361144151144\nIn [284]: returns[\"MSFT\"].cov(returns[\"IBM\"])\nOut[284]: 8.870655479703546e-05\nSince MSFT is a valid Python variable name, we can also select these columns using\nmore concise syntax:\nIn [285]: returns[\"MSFT\"].corr(returns[\"IBM\"])\nOut[285]: 0.49976361144151144\nDataFrame’s corr and cov methods, on the other hand, return a full correlation or\ncovariance matrix as a DataFrame, respectively:\nIn [286]: returns.corr()\nOut[286]: \n          AAPL      GOOG       IBM      MSFT\nAAPL  1.000000  0.407919  0.386817  0.389695\nGOOG  0.407919  1.000000  0.405099  0.465919\nIBM   0.386817  0.405099  1.000000  0.499764\nMSFT  0.389695  0.465919  0.499764  1.000000\nIn [287]: returns.cov()\nOut[287]: \n          AAPL      GOOG       IBM      MSFT\nAAPL  0.000277  0.000107  0.000078  0.000095\nGOOG  0.000107  0.000251  0.000078  0.000108\nIBM   0.000078  0.000078  0.000146  0.000089\nMSFT  0.000095  0.000108  0.000089  0.000215\nUsing DataFrame’s corrwith method, you can compute pair-wise correlations\nbetween a DataFrame’s columns or rows with another Series or DataFrame. Passing a\nSeries returns a Series with the correlation value computed for each column:\nIn [288]: returns.corrwith(returns[\"IBM\"])\nOut[288]: \nAAPL    0.386817\nGOOG    0.405099\nIBM     1.000000\nMSFT    0.499764\ndtype: float64\n5.3 Summarizing and Computing Descriptive Statistics \n| \n169",
      "content_length": 1881,
      "extraction_method": "Direct"
    },
    {
      "page_number": 188,
      "chapter": null,
      "content": "Passing a DataFrame computes the correlations of matching column names. Here, I\ncompute correlations of percent changes with volume:\nIn [289]: returns.corrwith(volume)\nOut[289]: \nAAPL   -0.075565\nGOOG   -0.007067\nIBM    -0.204849\nMSFT   -0.092950\ndtype: float64\nPassing axis=\"columns\" does things row-by-row instead. In all cases, the data points\nare aligned by label before the correlation is computed.\nUnique Values, Value Counts, and Membership\nAnother class of related methods extracts information about the values contained in a\none-dimensional Series. To illustrate these, consider this example:\nIn [290]: obj = pd.Series([\"c\", \"a\", \"d\", \"a\", \"a\", \"b\", \"b\", \"c\", \"c\"])\nThe first function is unique, which gives you an array of the unique values in a Series:\nIn [291]: uniques = obj.unique()\nIn [292]: uniques\nOut[292]: array(['c', 'a', 'd', 'b'], dtype=object)\nThe unique values are not necessarily returned in the order in which they first\nappear, and not in sorted order, but they could be sorted after the fact if needed\n(uniques.sort()). Relatedly, value_counts computes a Series containing value fre‐\nquencies:\nIn [293]: obj.value_counts()\nOut[293]: \nc    3\na    3\nb    2\nd    1\ndtype: int64\nThe Series is sorted by value in descending order as a convenience. value_counts is\nalso available as a top-level pandas method that can be used with NumPy arrays or\nother Python sequences:\nIn [294]: pd.value_counts(obj.to_numpy(), sort=False)\nOut[294]: \nc    3\na    3\nd    1\nb    2\ndtype: int64\n170 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 1545,
      "extraction_method": "Direct"
    },
    {
      "page_number": 189,
      "chapter": null,
      "content": "isin performs a vectorized set membership check and can be useful in filtering a\ndataset down to a subset of values in a Series or column in a DataFrame:\nIn [295]: obj\nOut[295]: \n0    c\n1    a\n2    d\n3    a\n4    a\n5    b\n6    b\n7    c\n8    c\ndtype: object\nIn [296]: mask = obj.isin([\"b\", \"c\"])\nIn [297]: mask\nOut[297]: \n0     True\n1    False\n2    False\n3    False\n4    False\n5     True\n6     True\n7     True\n8     True\ndtype: bool\nIn [298]: obj[mask]\nOut[298]: \n0    c\n5    b\n6    b\n7    c\n8    c\ndtype: object\nRelated to isin is the Index.get_indexer method, which gives you an index array\nfrom an array of possibly nondistinct values into another array of distinct values:\nIn [299]: to_match = pd.Series([\"c\", \"a\", \"b\", \"b\", \"c\", \"a\"])\nIn [300]: unique_vals = pd.Series([\"c\", \"b\", \"a\"])\nIn [301]: indices = pd.Index(unique_vals).get_indexer(to_match)\nIn [302]: indices\nOut[302]: array([0, 2, 1, 1, 0, 2])\nSee Table 5-9 for a reference on these methods.\n5.3 Summarizing and Computing Descriptive Statistics \n| \n171",
      "content_length": 1015,
      "extraction_method": "Direct"
    },
    {
      "page_number": 190,
      "chapter": null,
      "content": "Table 5-9. Unique, value counts, and set membership methods\nMethod\nDescription\nisin\nCompute a Boolean array indicating whether each Series or DataFrame value is contained in the passed\nsequence of values\nget_indexer\nCompute integer indices for each value in an array into another array of distinct values; helpful for data\nalignment and join-type operations\nunique\nCompute an array of unique values in a Series, returned in the order observed\nvalue_counts Return a Series containing unique values as its index and frequencies as its values, ordered count in\ndescending order\nIn some cases, you may want to compute a histogram on multiple related columns in\na DataFrame. Here’s an example:\nIn [303]: data = pd.DataFrame({\"Qu1\": [1, 3, 4, 3, 4],\n   .....:                      \"Qu2\": [2, 3, 1, 2, 3],\n   .....:                      \"Qu3\": [1, 5, 2, 4, 4]})\nIn [304]: data\nOut[304]: \n   Qu1  Qu2  Qu3\n0    1    2    1\n1    3    3    5\n2    4    1    2\n3    3    2    4\n4    4    3    4\nWe can compute the value counts for a single column, like so:\nIn [305]: data[\"Qu1\"].value_counts().sort_index()\nOut[305]: \n1    1\n3    2\n4    2\nName: Qu1, dtype: int64\nTo compute this for all columns, pass pandas.value_counts to the DataFrame’s\napply method:\nIn [306]: result = data.apply(pd.value_counts).fillna(0)\nIn [307]: result\nOut[307]: \n   Qu1  Qu2  Qu3\n1  1.0  1.0  1.0\n2  0.0  2.0  1.0\n3  2.0  2.0  0.0\n4  2.0  0.0  2.0\n5  0.0  0.0  1.0\nHere, the row labels in the result are the distinct values occurring in all of the\ncolumns. The values are the respective counts of these values in each column.\n172 \n| \nChapter 5: Getting Started with pandas",
      "content_length": 1636,
      "extraction_method": "Direct"
    },
    {
      "page_number": 191,
      "chapter": null,
      "content": "There is also a DataFrame.value_counts method, but it computes counts considering\neach row of the DataFrame as a tuple to determine the number of occurrences of each\ndistinct row:\nIn [308]: data = pd.DataFrame({\"a\": [1, 1, 1, 2, 2], \"b\": [0, 0, 1, 0, 0]})\nIn [309]: data\nOut[309]: \n   a  b\n0  1  0\n1  1  0\n2  1  1\n3  2  0\n4  2  0\nIn [310]: data.value_counts()\nOut[310]: \na  b\n1  0    2\n2  0    2\n1  1    1\ndtype: int64\nIn this case, the result has an index representing the distinct rows as a hierarchical\nindex, a topic we will explore in greater detail in Chapter 8.\n5.4 Conclusion\nIn the next chapter, we will discuss tools for reading (or loading) and writing datasets\nwith pandas. After that, we will dig deeper into data cleaning, wrangling, analysis, and\nvisualization tools using pandas.\n5.4 Conclusion \n| \n173",
      "content_length": 818,
      "extraction_method": "Direct"
    },
    {
      "page_number": 192,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 193,
      "chapter": null,
      "content": "CHAPTER 6\nData Loading, Storage, and File Formats\nReading data and making it accessible (often called data loading) is a necessary first\nstep for using most of the tools in this book. The term parsing is also sometimes used\nto describe loading text data and interpreting it as tables and different data types. I’m\ngoing to focus on data input and output using pandas, though there are numerous\ntools in other libraries to help with reading and writing data in various formats.\nInput and output typically fall into a few main categories: reading text files and other\nmore efficient on-disk formats, loading data from databases, and interacting with\nnetwork sources like web APIs.\n6.1 Reading and Writing Data in Text Format\npandas features a number of functions for reading tabular data as a DataFrame\nobject. Table 6-1 summarizes some of them; pandas.read_csv is one of the most\nfrequently used in this book. We will look at binary data formats later in Section 6.2,\n“Binary Data Formats,” on page 193.\nTable 6-1. Text and binary data loading functions in pandas\nFunction\nDescription\nread_csv\nLoad delimited data from a file, URL, or file-like object; use comma as default delimiter\nread_fwf\nRead data in fixed-width column format (i.e., no delimiters)\nread_clipboard\nVariation of read_csv that reads data from the clipboard; useful for converting tables from web\npages\nread_excel\nRead tabular data from an Excel XLS or XLSX file\nread_hdf\nRead HDF5 files written by pandas\nread_html\nRead all tables found in the given HTML document\nread_json\nRead data from a JSON (JavaScript Object Notation) string representation, file, URL, or file-like object\n175",
      "content_length": 1650,
      "extraction_method": "Direct"
    },
    {
      "page_number": 194,
      "chapter": null,
      "content": "Function\nDescription\nread_feather\nRead the Feather binary file format\nread_orc\nRead the Apache ORC binary file format\nread_parquet\nRead the Apache Parquet binary file format\nread_pickle\nRead an object stored by pandas using the Python pickle format\nread_sas\nRead a SAS dataset stored in one of the SAS system’s custom storage formats\nread_spss\nRead a data file created by SPSS\nread_sql\nRead the results of a SQL query (using SQLAlchemy)\nread_sql_table\nRead a whole SQL table (using SQLAlchemy); equivalent to using a query that selects everything in\nthat table using read_sql\nread_stata\nRead a dataset from Stata file format\nread_xml\nRead a table of data from an XML file\nI’ll give an overview of the mechanics of these functions, which are meant to convert\ntext data into a DataFrame. The optional arguments for these functions may fall into\na few categories:\nIndexing\nCan treat one or more columns as the returned DataFrame, and whether to get\ncolumn names from the file, arguments you provide, or not at all.\nType inference and data conversion\nIncludes the user-defined value conversions and custom list of missing value\nmarkers.\nDate and time parsing\nIncludes a combining capability, including combining date and time information\nspread over multiple columns into a single column in the result.\nIterating\nSupport for iterating over chunks of very large files.\nUnclean data issues\nIncludes skipping rows or a footer, comments, or other minor things like\nnumeric data with thousands separated by commas.\nBecause of how messy data in the real world can be, some of the data loading\nfunctions (especially pandas.read_csv) have accumulated a long list of optional\narguments over time. It’s normal to feel overwhelmed by the number of different\nparameters (pandas.read_csv has around 50). The online pandas documentation\nhas many examples about how each of these works, so if you’re struggling to read a\nparticular file, there might be a similar enough example to help you find the right\nparameters.\n176 \n| \nChapter 6: Data Loading, Storage, and File Formats",
      "content_length": 2056,
      "extraction_method": "Direct"
    },
    {
      "page_number": 195,
      "chapter": null,
      "content": "Some of these functions perform type inference, because the column data types are\nnot part of the data format. That means you don’t necessarily have to specify which\ncolumns are numeric, integer, Boolean, or string. Other data formats, like HDF5,\nORC, and Parquet, have the data type information embedded in the format.\nHandling dates and other custom types can require extra effort.\nLet’s start with a small comma-separated values (CSV) text file:\nIn [10]: !cat examples/ex1.csv\na,b,c,d,message\n1,2,3,4,hello\n5,6,7,8,world\n9,10,11,12,foo\nHere I used the Unix cat shell command to print the raw contents\nof the file to the screen. If you’re on Windows, you can use type\ninstead of cat to achieve the same effect within a Windows termi‐\nnal (or command line).\nSince this is comma-delimited, we can then use pandas.read_csv to read it into a\nDataFrame:\nIn [11]: df = pd.read_csv(\"examples/ex1.csv\")\nIn [12]: df\nOut[12]: \n   a   b   c   d message\n0  1   2   3   4   hello\n1  5   6   7   8   world\n2  9  10  11  12     foo\nA file will not always have a header row. Consider this file:\nIn [13]: !cat examples/ex2.csv\n1,2,3,4,hello\n5,6,7,8,world\n9,10,11,12,foo\nTo read this file, you have a couple of options. You can allow pandas to assign default\ncolumn names, or you can specify names yourself:\nIn [14]: pd.read_csv(\"examples/ex2.csv\", header=None)\nOut[14]: \n   0   1   2   3      4\n0  1   2   3   4  hello\n1  5   6   7   8  world\n2  9  10  11  12    foo\nIn [15]: pd.read_csv(\"examples/ex2.csv\", names=[\"a\", \"b\", \"c\", \"d\", \"message\"])\nOut[15]: \n6.1 Reading and Writing Data in Text Format \n| \n177",
      "content_length": 1593,
      "extraction_method": "Direct"
    },
    {
      "page_number": 196,
      "chapter": null,
      "content": "a   b   c   d message\n0  1   2   3   4   hello\n1  5   6   7   8   world\n2  9  10  11  12     foo\nSuppose you wanted the message column to be the index of the returned DataFrame.\nYou can either indicate you want the column at index 4 or named \"message\" using\nthe index_col argument:\nIn [16]: names = [\"a\", \"b\", \"c\", \"d\", \"message\"]\nIn [17]: pd.read_csv(\"examples/ex2.csv\", names=names, index_col=\"message\")\nOut[17]: \n         a   b   c   d\nmessage               \nhello    1   2   3   4\nworld    5   6   7   8\nfoo      9  10  11  12\nIf you want to form a hierarchical index (discussed in Section 8.1, “Hierarchical\nIndexing,” on page 247) from multiple columns, pass a list of column numbers or\nnames:\nIn [18]: !cat examples/csv_mindex.csv\nkey1,key2,value1,value2\none,a,1,2\none,b,3,4\none,c,5,6\none,d,7,8\ntwo,a,9,10\ntwo,b,11,12\ntwo,c,13,14\ntwo,d,15,16\nIn [19]: parsed = pd.read_csv(\"examples/csv_mindex.csv\",\n   ....:                      index_col=[\"key1\", \"key2\"])\nIn [20]: parsed\nOut[20]: \n           value1  value2\nkey1 key2                \none  a          1       2\n     b          3       4\n     c          5       6\n     d          7       8\ntwo  a          9      10\n     b         11      12\n     c         13      14\n     d         15      16\n178 \n| \nChapter 6: Data Loading, Storage, and File Formats",
      "content_length": 1308,
      "extraction_method": "Direct"
    },
    {
      "page_number": 197,
      "chapter": null,
      "content": "In some cases, a table might not have a fixed delimiter, using whitespace or some\nother pattern to separate fields. Consider a text file that looks like this:\nIn [21]: !cat examples/ex3.txt\nA         B         C\naaa -0.264438 -1.026059 -0.619500\nbbb  0.927272  0.302904 -0.032399\nccc -0.264273 -0.386314 -0.217601\nddd -0.871858 -0.348382  1.100491\nWhile you could do some munging by hand, the fields here are separated by a\nvariable amount of whitespace. In these cases, you can pass a regular expression as a\ndelimiter for pandas.read_csv. This can be expressed by the regular expression \\s+,\nso we have then:\nIn [22]: result = pd.read_csv(\"examples/ex3.txt\", sep=\"\\s+\")\nIn [23]: result\nOut[23]: \n            A         B         C\naaa -0.264438 -1.026059 -0.619500\nbbb  0.927272  0.302904 -0.032399\nccc -0.264273 -0.386314 -0.217601\nddd -0.871858 -0.348382  1.100491\nBecause there was one fewer column name than the number of data rows,\npandas.read_csv infers that the first column should be the DataFrame’s index in\nthis special case.\nThe file parsing functions have many additional arguments to help you handle the\nwide variety of exception file formats that occur (see a partial listing in Table 6-2). For\nexample, you can skip the first, third, and fourth rows of a file with skiprows:\nIn [24]: !cat examples/ex4.csv\n# hey!\na,b,c,d,message\n# just wanted to make things more difficult for you\n# who reads CSV files with computers, anyway?\n1,2,3,4,hello\n5,6,7,8,world\n9,10,11,12,foo\nIn [25]: pd.read_csv(\"examples/ex4.csv\", skiprows=[0, 2, 3])\nOut[25]: \n   a   b   c   d message\n0  1   2   3   4   hello\n1  5   6   7   8   world\n2  9  10  11  12     foo\nHandling missing values is an important and frequently nuanced part of the file\nreading process. Missing data is usually either not present (empty string) or marked\n6.1 Reading and Writing Data in Text Format \n| \n179",
      "content_length": 1873,
      "extraction_method": "Direct"
    },
    {
      "page_number": 198,
      "chapter": null,
      "content": "by some sentinel (placeholder) value. By default, pandas uses a set of commonly\noccurring sentinels, such as NA and NULL:\nIn [26]: !cat examples/ex5.csv\nsomething,a,b,c,d,message\none,1,2,3,4,NA\ntwo,5,6,,8,world\nthree,9,10,11,12,foo\nIn [27]: result = pd.read_csv(\"examples/ex5.csv\")\nIn [28]: result\nOut[28]: \n  something  a   b     c   d message\n0       one  1   2   3.0   4     NaN\n1       two  5   6   NaN   8   world\n2     three  9  10  11.0  12     foo\nRecall that pandas outputs missing values as NaN, so we have two null or missing\nvalues in result:\nIn [29]: pd.isna(result)\nOut[29]: \n   something      a      b      c      d  message\n0      False  False  False  False  False     True\n1      False  False  False   True  False    False\n2      False  False  False  False  False    False\nThe na_values option accepts a sequence of strings to add to the default list of strings\nrecognized as missing:\nIn [30]: result = pd.read_csv(\"examples/ex5.csv\", na_values=[\"NULL\"])\nIn [31]: result\nOut[31]: \n  something  a   b     c   d message\n0       one  1   2   3.0   4     NaN\n1       two  5   6   NaN   8   world\n2     three  9  10  11.0  12     foo\npandas.read_csv has a list of many default NA value representations, but these\ndefaults can be disabled with the keep_default_na option:\nIn [32]: result2 = pd.read_csv(\"examples/ex5.csv\", keep_default_na=False)\nIn [33]: result2\nOut[33]: \n  something  a   b   c   d message\n0       one  1   2   3   4      NA\n1       two  5   6       8   world\n2     three  9  10  11  12     foo\nIn [34]: result2.isna()\nOut[34]: \n180 \n| \nChapter 6: Data Loading, Storage, and File Formats",
      "content_length": 1616,
      "extraction_method": "Direct"
    },
    {
      "page_number": 199,
      "chapter": null,
      "content": "something      a      b      c      d  message\n0      False  False  False  False  False    False\n1      False  False  False  False  False    False\n2      False  False  False  False  False    False\nIn [35]: result3 = pd.read_csv(\"examples/ex5.csv\", keep_default_na=False,\n   ....:                       na_values=[\"NA\"])\nIn [36]: result3\nOut[36]: \n  something  a   b   c   d message\n0       one  1   2   3   4     NaN\n1       two  5   6       8   world\n2     three  9  10  11  12     foo\nIn [37]: result3.isna()\nOut[37]: \n   something      a      b      c      d  message\n0      False  False  False  False  False     True\n1      False  False  False  False  False    False\n2      False  False  False  False  False    False\nDifferent NA sentinels can be specified for each column in a dictionary:\nIn [38]: sentinels = {\"message\": [\"foo\", \"NA\"], \"something\": [\"two\"]}\nIn [39]: pd.read_csv(\"examples/ex5.csv\", na_values=sentinels,\n   ....:             keep_default_na=False)\nOut[39]: \n  something  a   b   c   d message\n0       one  1   2   3   4     NaN\n1       NaN  5   6       8   world\n2     three  9  10  11  12     NaN\nTable 6-2 lists some frequently used options in pandas.read_csv.\nTable 6-2. Some pandas.read_csv function arguments\nArgument\nDescription\npath\nString indicating filesystem location, URL, or file-like object.\nsep or delimiter\nCharacter sequence or regular expression to use to split fields in each row.\nheader\nRow number to use as column names; defaults to 0 (first row), but should be None if there is no\nheader row.\nindex_col\nColumn numbers or names to use as the row index in the result; can be a single name/number or a\nlist of them for a hierarchical index.\nnames\nList of column names for result.\nskiprows\nNumber of rows at beginning of file to ignore or list of row numbers (starting from 0) to skip.\nna_values\nSequence of values to replace with NA. They are added to the default list unless\nkeep_default_na=False is passed.\nkeep_default_na\nWhether to use the default NA value list or not (True by default).\n6.1 Reading and Writing Data in Text Format \n| \n181",
      "content_length": 2083,
      "extraction_method": "Direct"
    },
    {
      "page_number": 200,
      "chapter": null,
      "content": "Argument\nDescription\ncomment\nCharacter(s) to split comments off the end of lines.\nparse_dates\nAttempt to parse data to datetime; False by default. If True, will attempt to parse all\ncolumns. Otherwise, can specify a list of column numbers or names to parse. If element of list is\ntuple or list, will combine multiple columns together and parse to date (e.g., if date/time split across\ntwo columns).\nkeep_date_col\nIf joining columns to parse date, keep the joined columns; False by default.\nconverters\nDictionary containing column number or name mapping to functions (e.g., {\"foo\": f} would\napply the function f to all values in the \"foo\" column).\ndayfirst\nWhen parsing potentially ambiguous dates, treat as international format (e.g., 7/6/2012 -> June 7,\n2012); False by default.\ndate_parser\nFunction to use to parse dates.\nnrows\nNumber of rows to read from beginning of file (not counting the header).\niterator\nReturn a TextFileReader object for reading the file piecemeal. This object can also be used\nwith the with statement.\nchunksize\nFor iteration, size of file chunks.\nskip_footer\nNumber of lines to ignore at end of file.\nverbose\nPrint various parsing information, like the time spent in each stage of the file conversion and\nmemory use information.\nencoding\nText encoding (e.g., \"utf-8 for UTF-8 encoded text). Defaults to \"utf-8\" if None.\nsqueeze\nIf the parsed data contains only one column, return a Series.\nthousands\nSeparator for thousands (e.g., \",\" or \".\"); default is None.\ndecimal\nDecimal separator in numbers (e.g., \".\" or \",\"); default is \".\".\nengine\nCSV parsing and conversion engine to use; can be one of \"c\", \"python\", or \"pyarrow\". The\ndefault is \"c\", though the newer \"pyarrow\" engine can parse some files much faster. The\n\"python\" engine is slower but supports some features that the other engines do not.\nReading Text Files in Pieces\nWhen processing very large files or figuring out the right set of arguments to cor‐\nrectly process a large file, you may want to read only a small piece of a file or iterate\nthrough smaller chunks of the file.\nBefore we look at a large file, we make the pandas display settings more compact:\nIn [40]: pd.options.display.max_rows = 10\nNow we have:\nIn [41]: result = pd.read_csv(\"examples/ex6.csv\")\nIn [42]: result\nOut[42]: \n           one       two     three      four key\n0     0.467976 -0.038649 -0.295344 -1.824726   L\n1    -0.358893  1.404453  0.704965 -0.200638   B\n2    -0.501840  0.659254 -0.421691 -0.057688   G\n182 \n| \nChapter 6: Data Loading, Storage, and File Formats",
      "content_length": 2536,
      "extraction_method": "Direct"
    },
    {
      "page_number": 201,
      "chapter": null,
      "content": "3     0.204886  1.074134  1.388361 -0.982404   R\n4     0.354628 -0.133116  0.283763 -0.837063   Q\n...        ...       ...       ...       ...  ..\n9995  2.311896 -0.417070 -1.409599 -0.515821   L\n9996 -0.479893 -0.650419  0.745152 -0.646038   E\n9997  0.523331  0.787112  0.486066  1.093156   K\n9998 -0.362559  0.598894 -1.843201  0.887292   G\n9999 -0.096376 -1.012999 -0.657431 -0.573315   0\n[10000 rows x 5 columns]\nThe elipsis marks ... indicate that rows in the middle of the DataFrame have been\nomitted.\nIf you want to read only a small number of rows (avoiding reading the entire file),\nspecify that with nrows:\nIn [43]: pd.read_csv(\"examples/ex6.csv\", nrows=5)\nOut[43]: \n        one       two     three      four key\n0  0.467976 -0.038649 -0.295344 -1.824726   L\n1 -0.358893  1.404453  0.704965 -0.200638   B\n2 -0.501840  0.659254 -0.421691 -0.057688   G\n3  0.204886  1.074134  1.388361 -0.982404   R\n4  0.354628 -0.133116  0.283763 -0.837063   Q\nTo read a file in pieces, specify a chunksize as a number of rows:\nIn [44]: chunker = pd.read_csv(\"examples/ex6.csv\", chunksize=1000)\nIn [45]: type(chunker)\nOut[45]: pandas.io.parsers.readers.TextFileReader\nThe TextFileReader object returned by pandas.read_csv allows you to iterate over\nthe parts of the file according to the chunksize. For example, we can iterate over\nex6.csv, aggregating the value counts in the \"key\" column, like so:\nchunker = pd.read_csv(\"examples/ex6.csv\", chunksize=1000)\ntot = pd.Series([], dtype='int64')\nfor piece in chunker:\n    tot = tot.add(piece[\"key\"].value_counts(), fill_value=0)\ntot = tot.sort_values(ascending=False)\nWe have then:\nIn [47]: tot[:10]\nOut[47]: \nE    368.0\nX    364.0\nL    346.0\nO    343.0\nQ    340.0\nM    338.0\n6.1 Reading and Writing Data in Text Format \n| \n183",
      "content_length": 1766,
      "extraction_method": "Direct"
    },
    {
      "page_number": 202,
      "chapter": null,
      "content": "J    337.0\nF    335.0\nK    334.0\nH    330.0\ndtype: float64\nTextFileReader is also equipped with a get_chunk method that enables you to read\npieces of an arbitrary size.\nWriting Data to Text Format\nData can also be exported to a delimited format. Let’s consider one of the CSV files\nread before:\nIn [48]: data = pd.read_csv(\"examples/ex5.csv\")\nIn [49]: data\nOut[49]: \n  something  a   b     c   d message\n0       one  1   2   3.0   4     NaN\n1       two  5   6   NaN   8   world\n2     three  9  10  11.0  12     foo\nUsing DataFrame’s to_csv method, we can write the data out to a comma-separated\nfile:\nIn [50]: data.to_csv(\"examples/out.csv\")\nIn [51]: !cat examples/out.csv\n,something,a,b,c,d,message\n0,one,1,2,3.0,4,\n1,two,5,6,,8,world\n2,three,9,10,11.0,12,foo\nOther delimiters can be used, of course (writing to sys.stdout so it prints the text\nresult to the console rather than a file):\nIn [52]: import sys\nIn [53]: data.to_csv(sys.stdout, sep=\"|\")\n|something|a|b|c|d|message\n0|one|1|2|3.0|4|\n1|two|5|6||8|world\n2|three|9|10|11.0|12|foo\nMissing values appear as empty strings in the output. You might want to denote them\nby some other sentinel value:\nIn [54]: data.to_csv(sys.stdout, na_rep=\"NULL\")\n,something,a,b,c,d,message\n0,one,1,2,3.0,4,NULL\n1,two,5,6,NULL,8,world\n2,three,9,10,11.0,12,foo\n184 \n| \nChapter 6: Data Loading, Storage, and File Formats",
      "content_length": 1355,
      "extraction_method": "Direct"
    },
    {
      "page_number": 203,
      "chapter": null,
      "content": "With no other options specified, both the row and column labels are written. Both of\nthese can be disabled:\nIn [55]: data.to_csv(sys.stdout, index=False, header=False)\none,1,2,3.0,4,\ntwo,5,6,,8,world\nthree,9,10,11.0,12,foo\nYou can also write only a subset of the columns, and in an order of your choosing:\nIn [56]: data.to_csv(sys.stdout, index=False, columns=[\"a\", \"b\", \"c\"])\na,b,c\n1,2,3.0\n5,6,\n9,10,11.0\nWorking with Other Delimited Formats\nIt’s possible to load most forms of tabular data from disk using functions like pan\ndas.read_csv. In some cases, however, some manual processing may be necessary.\nIt’s not uncommon to receive a file with one or more malformed lines that trip up\npandas.read_csv. To illustrate the basic tools, consider a small CSV file:\nIn [57]: !cat examples/ex7.csv\n\"a\",\"b\",\"c\"\n\"1\",\"2\",\"3\"\n\"1\",\"2\",\"3\"\nFor any file with a single-character delimiter, you can use Python’s built-in csv\nmodule. To use it, pass any open file or file-like object to csv.reader:\nIn [58]: import csv\nIn [59]: f = open(\"examples/ex7.csv\")\nIn [60]: reader = csv.reader(f)\nIterating through the reader like a file yields lists of values with any quote characters\nremoved:\nIn [61]: for line in reader:\n   ....:     print(line)\n['a', 'b', 'c']\n['1', '2', '3']\n['1', '2', '3']\nIn [62]: f.close()\nFrom there, it’s up to you to do the wrangling necessary to put the data in the form\nthat you need. Let’s take this step by step. First, we read the file into a list of lines:\nIn [63]: with open(\"examples/ex7.csv\") as f:\n   ....:     lines = list(csv.reader(f))\n6.1 Reading and Writing Data in Text Format \n| \n185",
      "content_length": 1608,
      "extraction_method": "Direct"
    },
    {
      "page_number": 204,
      "chapter": null,
      "content": "Then we split the lines into the header line and the data lines:\nIn [64]: header, values = lines[0], lines[1:]\nThen we can create a dictionary of data columns using a dictionary comprehension\nand the expression zip(*values) (beware that this will use a lot of memory on large\nfiles), which transposes rows to columns:\nIn [65]: data_dict = {h: v for h, v in zip(header, zip(*values))}\nIn [66]: data_dict\nOut[66]: {'a': ('1', '1'), 'b': ('2', '2'), 'c': ('3', '3')}\nCSV files come in many different flavors. To define a new format with a different\ndelimiter, string quoting convention, or line terminator, we could define a simple\nsubclass of csv.Dialect:\nclass my_dialect(csv.Dialect):\n    lineterminator = \"\\n\"\n    delimiter = \";\"\n    quotechar = '\"'\n    quoting = csv.QUOTE_MINIMAL\nreader = csv.reader(f, dialect=my_dialect)\nWe could also give individual CSV dialect parameters as keywords to csv.reader\nwithout having to define a subclass:\nreader = csv.reader(f, delimiter=\"|\")\nThe possible options (attributes of csv.Dialect) and what they do can be found in\nTable 6-3.\nTable 6-3. CSV dialect options\nArgument\nDescription\ndelimiter\nOne-character string to separate fields; defaults to \",\".\nlineterminator\nLine terminator for writing; defaults to \"\\r\\n\". Reader ignores this and recognizes cross-platform\nline terminators.\nquotechar\nQuote character for fields with special characters (like a delimiter); default is '\"'.\nquoting\nQuoting convention. Options include csv.QUOTE_ALL (quote all fields), csv.QUOTE_MINI\nMAL (only fields with special characters like the delimiter), csv.QUOTE_NONNUMERIC, and\ncsv.QUOTE_NONE (no quoting). See Python’s documentation for full details. Defaults to\nQUOTE_MINIMAL.\nskipinitialspace Ignore whitespace after each delimiter; default is False.\ndoublequote\nHow to handle quoting character inside a field; if True, it is doubled (see online documentation\nfor full detail and behavior).\nescapechar\nString to escape the delimiter if quoting is set to csv.QUOTE_NONE; disabled by default.\n186 \n| \nChapter 6: Data Loading, Storage, and File Formats",
      "content_length": 2077,
      "extraction_method": "Direct"
    },
    {
      "page_number": 205,
      "chapter": null,
      "content": "For files with more complicated or fixed multicharacter delimiters,\nyou will not be able to use the csv module. In those cases, you’ll\nhave to do the line splitting and other cleanup using the string’s\nsplit method or the regular expression method re.split. Thank‐\nfully, pandas.read_csv is capable of doing almost anything you\nneed if you pass the necessary options, so you only rarely will have\nto parse files by hand.\nTo write delimited files manually, you can use csv.writer. It accepts an open, writa‐\nble file object and the same dialect and format options as csv.reader:\nwith open(\"mydata.csv\", \"w\") as f:\n    writer = csv.writer(f, dialect=my_dialect)\n    writer.writerow((\"one\", \"two\", \"three\"))\n    writer.writerow((\"1\", \"2\", \"3\"))\n    writer.writerow((\"4\", \"5\", \"6\"))\n    writer.writerow((\"7\", \"8\", \"9\"))\nJSON Data\nJSON (short for JavaScript Object Notation) has become one of the standard formats\nfor sending data by HTTP request between web browsers and other applications. It\nis a much more free-form data format than a tabular text form like CSV. Here is an\nexample:\nobj = \"\"\"\n{\"name\": \"Wes\",\n \"cities_lived\": [\"Akron\", \"Nashville\", \"New York\", \"San Francisco\"],\n \"pet\": null,\n \"siblings\": [{\"name\": \"Scott\", \"age\": 34, \"hobbies\": [\"guitars\", \"soccer\"]},\n              {\"name\": \"Katie\", \"age\": 42, \"hobbies\": [\"diving\", \"art\"]}]\n}\n\"\"\"\nJSON is very nearly valid Python code with the exception of its null value null and\nsome other nuances (such as disallowing trailing commas at the end of lists). The\nbasic types are objects (dictionaries), arrays (lists), strings, numbers, Booleans, and\nnulls. All of the keys in an object must be strings. There are several Python libraries\nfor reading and writing JSON data. I’ll use json here, as it is built into the Python\nstandard library. To convert a JSON string to Python form, use json.loads:\nIn [68]: import json\nIn [69]: result = json.loads(obj)\nIn [70]: result\nOut[70]: \n{'name': 'Wes',\n 'cities_lived': ['Akron', 'Nashville', 'New York', 'San Francisco'],\n6.1 Reading and Writing Data in Text Format \n| \n187",
      "content_length": 2071,
      "extraction_method": "Direct"
    },
    {
      "page_number": 206,
      "chapter": null,
      "content": "'pet': None,\n 'siblings': [{'name': 'Scott',\n   'age': 34,\n   'hobbies': ['guitars', 'soccer']},\n  {'name': 'Katie', 'age': 42, 'hobbies': ['diving', 'art']}]}\njson.dumps, on the other hand, converts a Python object back to JSON:\nIn [71]: asjson = json.dumps(result)\nIn [72]: asjson\nOut[72]: '{\"name\": \"Wes\", \"cities_lived\": [\"Akron\", \"Nashville\", \"New York\", \"San\n Francisco\"], \"pet\": null, \"siblings\": [{\"name\": \"Scott\", \"age\": 34, \"hobbies\": [\n\"guitars\", \"soccer\"]}, {\"name\": \"Katie\", \"age\": 42, \"hobbies\": [\"diving\", \"art\"]}\n]}'\nHow you convert a JSON object or list of objects to a DataFrame or some other\ndata structure for analysis will be up to you. Conveniently, you can pass a list of\ndictionaries (which were previously JSON objects) to the DataFrame constructor and\nselect a subset of the data fields:\nIn [73]: siblings = pd.DataFrame(result[\"siblings\"], columns=[\"name\", \"age\"])\nIn [74]: siblings\nOut[74]: \n    name  age\n0  Scott   34\n1  Katie   42\nThe pandas.read_json can automatically convert JSON datasets in specific arrange‐\nments into a Series or DataFrame. For example:\nIn [75]: !cat examples/example.json\n[{\"a\": 1, \"b\": 2, \"c\": 3},\n {\"a\": 4, \"b\": 5, \"c\": 6},\n {\"a\": 7, \"b\": 8, \"c\": 9}]\nThe default options for pandas.read_json assume that each object in the JSON array\nis a row in the table:\nIn [76]: data = pd.read_json(\"examples/example.json\")\nIn [77]: data\nOut[77]: \n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\nFor an extended example of reading and manipulating JSON data (including nested\nrecords), see the USDA food database example in Chapter 13.\n188 \n| \nChapter 6: Data Loading, Storage, and File Formats",
      "content_length": 1634,
      "extraction_method": "Direct"
    },
    {
      "page_number": 207,
      "chapter": null,
      "content": "1 For the full list, see https://www.fdic.gov/bank/individual/failed/banklist.html.\nIf you need to export data from pandas to JSON, one way is to use the to_json\nmethods on Series and DataFrame:\nIn [78]: data.to_json(sys.stdout)\n{\"a\":{\"0\":1,\"1\":4,\"2\":7},\"b\":{\"0\":2,\"1\":5,\"2\":8},\"c\":{\"0\":3,\"1\":6,\"2\":9}}\nIn [79]: data.to_json(sys.stdout, orient=\"records\")\n[{\"a\":1,\"b\":2,\"c\":3},{\"a\":4,\"b\":5,\"c\":6},{\"a\":7,\"b\":8,\"c\":9}]\nXML and HTML: Web Scraping\nPython has many libraries for reading and writing data in the ubiquitous HTML and\nXML formats. Examples include lxml, Beautiful Soup, and html5lib. While lxml is\ncomparatively much faster in general, the other libraries can better handle malformed\nHTML or XML files.\npandas has a built-in function, pandas.read_html, which uses all of these libraries to\nautomatically parse tables out of HTML files as DataFrame objects. To show how this\nworks, I downloaded an HTML file (used in the pandas documentation) from the US\nFDIC showing bank failures.1 First, you must install some additional libraries used by\nread_html:\nconda install lxml beautifulsoup4 html5lib\nIf you are not using conda, pip install lxml should also work.\nThe pandas.read_html function has a number of options, but by default it searches\nfor and attempts to parse all tabular data contained within <table> tags. The result is\na list of DataFrame objects:\nIn [80]: tables = pd.read_html(\"examples/fdic_failed_bank_list.html\")\nIn [81]: len(tables)\nOut[81]: 1\nIn [82]: failures = tables[0]\nIn [83]: failures.head()\nOut[83]: \n                      Bank Name             City  ST   CERT  \\\n0                   Allied Bank         Mulberry  AR     91   \n1  The Woodbury Banking Company         Woodbury  GA  11297   \n2        First CornerStone Bank  King of Prussia  PA  35312   \n3            Trust Company Bank          Memphis  TN   9956   \n4    North Milwaukee State Bank        Milwaukee  WI  20364   \n                 Acquiring Institution        Closing Date       Updated Date  \n0                         Today's Bank  September 23, 2016  November 17, 2016  \n1                          United Bank     August 19, 2016  November 17, 2016  \n6.1 Reading and Writing Data in Text Format \n| \n189",
      "content_length": 2201,
      "extraction_method": "Direct"
    },
    {
      "page_number": 208,
      "chapter": null,
      "content": "2  First-Citizens Bank & Trust Company         May 6, 2016  September 6, 2016  \n3           The Bank of Fayette County      April 29, 2016  September 6, 2016  \n4  First-Citizens Bank & Trust Company      March 11, 2016      June 16, 2016  \nBecause failures has many columns, pandas inserts a line break character \\.\nAs you will learn in later chapters, from here we could proceed to do some data\ncleaning and analysis, like computing the number of bank failures by year:\nIn [84]: close_timestamps = pd.to_datetime(failures[\"Closing Date\"])\nIn [85]: close_timestamps.dt.year.value_counts()\nOut[85]: \n2010    157\n2009    140\n2011     92\n2012     51\n2008     25\n       ... \n2004      4\n2001      4\n2007      3\n2003      3\n2000      2\nName: Closing Date, Length: 15, dtype: int64\nParsing XML with lxml.objectify\nXML is another common structured data format supporting hierarchical, nested data\nwith metadata. The book you are currently reading was actually created from a series\nof large XML documents.\nEarlier, I showed the pandas.read_html function, which uses either lxml or Beautiful\nSoup under the hood to parse data from HTML. XML and HTML are structurally\nsimilar, but XML is more general. Here, I will show an example of how to use lxml to\nparse data from a more general XML format.\nFor many years, the New York Metropolitan Transportation Authority (MTA) pub‐\nlished a number of data series about its bus and train services in XML format. Here\nwe’ll look at the performance data, which is contained in a set of XML files. Each train\nor bus service has a different file (like Performance_MNR.xml for the Metro-North\nRailroad) containing monthly data as a series of XML records that look like this:\n<INDICATOR>\n  <INDICATOR_SEQ>373889</INDICATOR_SEQ>\n  <PARENT_SEQ></PARENT_SEQ>\n  <AGENCY_NAME>Metro-North Railroad</AGENCY_NAME>\n  <INDICATOR_NAME>Escalator Availability</INDICATOR_NAME>\n  <DESCRIPTION>Percent of the time that escalators are operational\n  systemwide. The availability rate is based on physical observations performed\n  the morning of regular business days only. This is a new indicator the agency\n190 \n| \nChapter 6: Data Loading, Storage, and File Formats",
      "content_length": 2175,
      "extraction_method": "Direct"
    },
    {
      "page_number": 209,
      "chapter": null,
      "content": "began reporting in 2009.</DESCRIPTION>\n  <PERIOD_YEAR>2011</PERIOD_YEAR>\n  <PERIOD_MONTH>12</PERIOD_MONTH>\n  <CATEGORY>Service Indicators</CATEGORY>\n  <FREQUENCY>M</FREQUENCY>\n  <DESIRED_CHANGE>U</DESIRED_CHANGE>\n  <INDICATOR_UNIT>%</INDICATOR_UNIT>\n  <DECIMAL_PLACES>1</DECIMAL_PLACES>\n  <YTD_TARGET>97.00</YTD_TARGET>\n  <YTD_ACTUAL></YTD_ACTUAL>\n  <MONTHLY_TARGET>97.00</MONTHLY_TARGET>\n  <MONTHLY_ACTUAL></MONTHLY_ACTUAL>\n</INDICATOR>\nUsing lxml.objectify, we parse the file and get a reference to the root node of the\nXML file with getroot:\nIn [86]: from lxml import objectify\nIn [87]: path = \"datasets/mta_perf/Performance_MNR.xml\"\nIn [88]: with open(path) as f:\n   ....:     parsed = objectify.parse(f)\nIn [89]: root = parsed.getroot()\nroot.INDICATOR returns a generator yielding each <INDICATOR> XML element. For\neach record, we can populate a dictionary of tag names (like YTD_ACTUAL) to data\nvalues (excluding a few tags) by running the following code:\ndata = []\nskip_fields = [\"PARENT_SEQ\", \"INDICATOR_SEQ\",\n               \"DESIRED_CHANGE\", \"DECIMAL_PLACES\"]\nfor elt in root.INDICATOR:\n    el_data = {}\n    for child in elt.getchildren():\n        if child.tag in skip_fields:\n            continue\n        el_data[child.tag] = child.pyval\n    data.append(el_data)\nLastly, convert this list of dictionaries into a DataFrame:\nIn [91]: perf = pd.DataFrame(data)\nIn [92]: perf.head()\nOut[92]: \n            AGENCY_NAME                        INDICATOR_NAME  \\\n0  Metro-North Railroad  On-Time Performance (West of Hudson)   \n1  Metro-North Railroad  On-Time Performance (West of Hudson)   \n2  Metro-North Railroad  On-Time Performance (West of Hudson)   \n6.1 Reading and Writing Data in Text Format \n| \n191",
      "content_length": 1710,
      "extraction_method": "Direct"
    },
    {
      "page_number": 210,
      "chapter": null,
      "content": "3  Metro-North Railroad  On-Time Performance (West of Hudson)   \n4  Metro-North Railroad  On-Time Performance (West of Hudson)   \n                                                                  DESCRIPTION  \\\n0  Percent of commuter trains that arrive at their destinations within 5 m...   \n1  Percent of commuter trains that arrive at their destinations within 5 m...   \n2  Percent of commuter trains that arrive at their destinations within 5 m...   \n3  Percent of commuter trains that arrive at their destinations within 5 m...   \n4  Percent of commuter trains that arrive at their destinations within 5 m...   \n   PERIOD_YEAR  PERIOD_MONTH            CATEGORY FREQUENCY INDICATOR_UNIT  \\\n0         2008             1  Service Indicators         M              %   \n1         2008             2  Service Indicators         M              %   \n2         2008             3  Service Indicators         M              %   \n3         2008             4  Service Indicators         M              %   \n4         2008             5  Service Indicators         M              %   \n  YTD_TARGET YTD_ACTUAL MONTHLY_TARGET MONTHLY_ACTUAL  \n0       95.0       96.9           95.0           96.9  \n1       95.0       96.0           95.0           95.0  \n2       95.0       96.3           95.0           96.9  \n3       95.0       96.8           95.0           98.3  \n4       95.0       96.6           95.0           95.8  \npandas’s pandas.read_xml function turns this process into a one-line expression:\nIn [93]: perf2 = pd.read_xml(path)\nIn [94]: perf2.head()\nOut[94]: \n   INDICATOR_SEQ  PARENT_SEQ           AGENCY_NAME  \\\n0          28445         NaN  Metro-North Railroad   \n1          28445         NaN  Metro-North Railroad   \n2          28445         NaN  Metro-North Railroad   \n3          28445         NaN  Metro-North Railroad   \n4          28445         NaN  Metro-North Railroad   \n                         INDICATOR_NAME  \\\n0  On-Time Performance (West of Hudson)   \n1  On-Time Performance (West of Hudson)   \n2  On-Time Performance (West of Hudson)   \n3  On-Time Performance (West of Hudson)   \n4  On-Time Performance (West of Hudson)   \n                                                                  DESCRIPTION  \\\n0  Percent of commuter trains that arrive at their destinations within 5 m...   \n1  Percent of commuter trains that arrive at their destinations within 5 m...   \n2  Percent of commuter trains that arrive at their destinations within 5 m...   \n3  Percent of commuter trains that arrive at their destinations within 5 m...   \n4  Percent of commuter trains that arrive at their destinations within 5 m...   \n   PERIOD_YEAR  PERIOD_MONTH            CATEGORY FREQUENCY DESIRED_CHANGE  \\\n0         2008             1  Service Indicators         M              U   \n1         2008             2  Service Indicators         M              U   \n2         2008             3  Service Indicators         M              U   \n3         2008             4  Service Indicators         M              U   \n4         2008             5  Service Indicators         M              U   \n  INDICATOR_UNIT  DECIMAL_PLACES YTD_TARGET YTD_ACTUAL MONTHLY_TARGET  \\\n0              %               1      95.00      96.90          95.00   \n192 \n| \nChapter 6: Data Loading, Storage, and File Formats",
      "content_length": 3296,
      "extraction_method": "Direct"
    },
    {
      "page_number": 211,
      "chapter": null,
      "content": "1              %               1      95.00      96.00          95.00   \n2              %               1      95.00      96.30          95.00   \n3              %               1      95.00      96.80          95.00   \n4              %               1      95.00      96.60          95.00   \n  MONTHLY_ACTUAL  \n0          96.90  \n1          95.00  \n2          96.90  \n3          98.30  \n4          95.80  \nFor more complex XML documents, refer to the docstring for pandas.read_xml\nwhich describes how to do selections and filters to extract a particular table of\ninterest.\n6.2 Binary Data Formats\nOne simple way to store (or serialize) data in binary format is using Python’s built-in\npickle module. pandas objects all have a to_pickle method that writes the data to\ndisk in pickle format:\nIn [95]: frame = pd.read_csv(\"examples/ex1.csv\")\nIn [96]: frame\nOut[96]: \n   a   b   c   d message\n0  1   2   3   4   hello\n1  5   6   7   8   world\n2  9  10  11  12     foo\nIn [97]: frame.to_pickle(\"examples/frame_pickle\")\nPickle files are in general readable only in Python. You can read any “pickled” object\nstored in a file by using the built-in pickle directly, or even more conveniently using\npandas.read_pickle:\nIn [98]: pd.read_pickle(\"examples/frame_pickle\")\nOut[98]: \n   a   b   c   d message\n0  1   2   3   4   hello\n1  5   6   7   8   world\n2  9  10  11  12     foo\npickle is recommended only as a short-term storage format. The\nproblem is that it is hard to guarantee that the format will be\nstable over time; an object pickled today may not unpickle with a\nlater version of a library. pandas has tried to maintain backward\ncompatibility when possible, but at some point in the future it may\nbe necessary to “break” the pickle format.\n6.2 Binary Data Formats \n| \n193",
      "content_length": 1769,
      "extraction_method": "Direct"
    },
    {
      "page_number": 212,
      "chapter": null,
      "content": "pandas has built-in support for several other open source binary data formats, such\nas HDF5, ORC, and Apache Parquet. For example, if you install the pyarrow package\n(conda install pyarrow), then you can read Parquet files with pandas.read_par\nquet:\nIn [100]: fec = pd.read_parquet('datasets/fec/fec.parquet')\nI will give some HDF5 examples in “Using HDF5 Format” on page 195. I encourage\nyou to explore different file formats to see how fast they are and how well they work\nfor your analysis.\nReading Microsoft Excel Files\npandas also supports reading tabular data stored in Excel 2003 (and higher) files\nusing either the pandas.ExcelFile class or pandas.read_excel function. Internally,\nthese tools use the add-on packages xlrd and openpyxl to read old-style XLS and\nnewer XLSX files, respectively. These must be installed separately from pandas using\npip or conda:\nconda install openpyxl xlrd\nTo use pandas.ExcelFile, create an instance by passing a path to an xls or xlsx file:\nIn [101]: xlsx = pd.ExcelFile(\"examples/ex1.xlsx\")\nThis object can show you the list of available sheet names in the file:\nIn [102]: xlsx.sheet_names\nOut[102]: ['Sheet1']\nData stored in a sheet can then be read into DataFrame with parse:\nIn [103]: xlsx.parse(sheet_name=\"Sheet1\")\nOut[103]: \n   Unnamed: 0  a   b   c   d message\n0           0  1   2   3   4   hello\n1           1  5   6   7   8   world\n2           2  9  10  11  12     foo\nThis Excel table has an index column, so we can indicate that with the index_col\nargument:\nIn [104]: xlsx.parse(sheet_name=\"Sheet1\", index_col=0)\nOut[104]: \n   a   b   c   d message\n0  1   2   3   4   hello\n1  5   6   7   8   world\n2  9  10  11  12     foo\nIf you are reading multiple sheets in a file, then it is faster to create the pandas.Excel\nFile, but you can also simply pass the filename to pandas.read_excel:\n194 \n| \nChapter 6: Data Loading, Storage, and File Formats",
      "content_length": 1897,
      "extraction_method": "Direct"
    },
    {
      "page_number": 213,
      "chapter": null,
      "content": "In [105]: frame = pd.read_excel(\"examples/ex1.xlsx\", sheet_name=\"Sheet1\")\nIn [106]: frame\nOut[106]: \n   Unnamed: 0  a   b   c   d message\n0           0  1   2   3   4   hello\n1           1  5   6   7   8   world\n2           2  9  10  11  12     foo\nTo write pandas data to Excel format, you must first create an ExcelWriter, then\nwrite data to it using the pandas object’s to_excel method:\nIn [107]: writer = pd.ExcelWriter(\"examples/ex2.xlsx\")\nIn [108]: frame.to_excel(writer, \"Sheet1\")\nIn [109]: writer.save()\nYou can also pass a file path to to_excel and avoid the ExcelWriter:\nIn [110]: frame.to_excel(\"examples/ex2.xlsx\")\nUsing HDF5 Format\nHDF5 is a respected file format intended for storing large quantities of scientific array\ndata. It is available as a C library, and it has interfaces available in many other lan‐\nguages, including Java, Julia, MATLAB, and Python. The “HDF” in HDF5 stands for\nhierarchical data format. Each HDF5 file can store multiple datasets and supporting\nmetadata. Compared with simpler formats, HDF5 supports on-the-fly compression\nwith a variety of compression modes, enabling data with repeated patterns to be\nstored more efficiently. HDF5 can be a good choice for working with datasets that\ndon’t fit into memory, as you can efficiently read and write small sections of much\nlarger arrays.\nTo get started with HDF5 and pandas, you must first install PyTables by installing the\ntables package with conda:\nconda install pytables\nNote that the PyTables package is called “tables” in PyPI, so if you\ninstall with pip you will have to run pip install tables.\nWhile it’s possible to directly access HDF5 files using either the PyTables or h5py\nlibraries, pandas provides a high-level interface that simplifies storing Series and\nDataFrame objects. The HDFStore class works like a dictionary and handles the\nlow-level details:\n6.2 Binary Data Formats \n| \n195",
      "content_length": 1888,
      "extraction_method": "Direct"
    },
    {
      "page_number": 214,
      "chapter": null,
      "content": "In [113]: frame = pd.DataFrame({\"a\": np.random.standard_normal(100)})\nIn [114]: store = pd.HDFStore(\"examples/mydata.h5\")\nIn [115]: store[\"obj1\"] = frame\nIn [116]: store[\"obj1_col\"] = frame[\"a\"]\nIn [117]: store\nOut[117]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: examples/mydata.h5\nObjects contained in the HDF5 file can then be retrieved with the same dictionary-\nlike API:\nIn [118]: store[\"obj1\"]\nOut[118]: \n           a\n0  -0.204708\n1   0.478943\n2  -0.519439\n3  -0.555730\n4   1.965781\n..       ...\n95  0.795253\n96  0.118110\n97 -0.748532\n98  0.584970\n99  0.152677\n[100 rows x 1 columns]\nHDFStore supports two storage schemas, \"fixed\" and \"table\" (the default is\n\"fixed\"). The latter is generally slower, but it supports query operations using a\nspecial syntax:\nIn [119]: store.put(\"obj2\", frame, format=\"table\")\nIn [120]: store.select(\"obj2\", where=[\"index >= 10 and index <= 15\"])\nOut[120]: \n           a\n10  1.007189\n11 -1.296221\n12  0.274992\n13  0.228913\n14  1.352917\n15  0.886429\nIn [121]: store.close()\nThe put is an explicit version of the store[\"obj2\"] = frame method but allows us to\nset other options like the storage format.\n196 \n| \nChapter 6: Data Loading, Storage, and File Formats",
      "content_length": 1203,
      "extraction_method": "Direct"
    },
    {
      "page_number": 215,
      "chapter": null,
      "content": "The pandas.read_hdf function gives you a shortcut to these tools:\nIn [122]: frame.to_hdf(\"examples/mydata.h5\", \"obj3\", format=\"table\")\nIn [123]: pd.read_hdf(\"examples/mydata.h5\", \"obj3\", where=[\"index < 5\"])\nOut[123]: \n          a\n0 -0.204708\n1  0.478943\n2 -0.519439\n3 -0.555730\n4  1.965781\nIf you’d like, you can delete the HDF5 file you created, like so:\nIn [124]: import os\nIn [125]: os.remove(\"examples/mydata.h5\")\nIf you are processing data that is stored on remote servers, like\nAmazon S3 or HDFS, using a different binary format designed for\ndistributed storage like Apache Parquet may be more suitable.\nIf you work with large quantities of data locally, I would encourage you to explore\nPyTables and h5py to see how they can suit your needs. Since many data analysis\nproblems are I/O-bound (rather than CPU-bound), using a tool like HDF5 can\nmassively accelerate your applications.\nHDF5 is not a database. It is best suited for write-once, read-many\ndatasets. While data can be added to a file at any time, if multiple\nwriters do so simultaneously, the file can become corrupted.\n6.3 Interacting with Web APIs\nMany websites have public APIs providing data feeds via JSON or some other format.\nThere are a number of ways to access these APIs from Python; one method that I\nrecommend is the requests package, which can be installed with pip or conda:\nconda install requests\nTo find the last 30 GitHub issues for pandas on GitHub, we can make a GET HTTP\nrequest using the add-on requests library:\nIn [126]: import requests\nIn [127]: url = \"https://api.github.com/repos/pandas-dev/pandas/issues\"\n6.3 Interacting with Web APIs \n| \n197",
      "content_length": 1637,
      "extraction_method": "Direct"
    },
    {
      "page_number": 216,
      "chapter": null,
      "content": "In [128]: resp = requests.get(url)\nIn [129]: resp.raise_for_status()\nIn [130]: resp\nOut[130]: <Response [200]>\nIt’s a good practice to always call raise_for_status after using requests.get to\ncheck for HTTP errors.\nThe response object’s json method will return a Python object containing the parsed\nJSON data as a dictionary or list (depending on what JSON is returned):\nIn [131]: data = resp.json()\nIn [132]: data[0][\"title\"]\nOut[132]: 'REF: make copy keyword non-stateful'\nSince the results retrieved are based on real-time data, what you see when you run\nthis code will almost definitely be different.\nEach element in data is a dictionary containing all of the data found on a GitHub\nissue page (except for the comments). We can pass data directly to pandas.Data\nFrame and extract fields of interest:\nIn [133]: issues = pd.DataFrame(data, columns=[\"number\", \"title\",\n   .....:                                      \"labels\", \"state\"])\nIn [134]: issues\nOut[134]: \n    number  \\\n0    48062   \n1    48061   \n2    48060   \n3    48059   \n4    48058   \n..     ...   \n25   48032   \n26   48030   \n27   48028   \n28   48027   \n29   48026   \n                                                                         title  \\\n0                                          REF: make copy keyword non-stateful   \n1                                                        STYLE: upgrade flake8   \n2   DOC: \"Creating a Python environment\" in \"Creating a development environ...   \n3                                        REGR: Avoid overflow with groupby sum   \n4   REGR: fix reset_index (Index.insert) regression with custom Index subcl...   \n..                                                                         ...   \n25                   BUG: Union of multi index with EA types can lose EA dtype   \n26                                                     ENH: Add rolling.prod()   \n198 \n| \nChapter 6: Data Loading, Storage, and File Formats",
      "content_length": 1929,
      "extraction_method": "Direct"
    },
    {
      "page_number": 217,
      "chapter": null,
      "content": "27                                       CLN: Refactor groupby's _make_wrapper   \n28                                          ENH: Support masks in groupby prod   \n29                                             DEP: Add pip to environment.yml   \n                                                                        labels  \\\n0                                                                           []   \n1   [{'id': 106935113, 'node_id': 'MDU6TGFiZWwxMDY5MzUxMTM=', 'url': 'https...   \n2   [{'id': 134699, 'node_id': 'MDU6TGFiZWwxMzQ2OTk=', 'url': 'https://api....   \n3   [{'id': 233160, 'node_id': 'MDU6TGFiZWwyMzMxNjA=', 'url': 'https://api....   \n4   [{'id': 32815646, 'node_id': 'MDU6TGFiZWwzMjgxNTY0Ng==', 'url': 'https:...   \n..                                                                         ...   \n25  [{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ==', 'url': 'https://api.g...   \n26  [{'id': 76812, 'node_id': 'MDU6TGFiZWw3NjgxMg==', 'url': 'https://api.g...   \n27  [{'id': 233160, 'node_id': 'MDU6TGFiZWwyMzMxNjA=', 'url': 'https://api....   \n28  [{'id': 233160, 'node_id': 'MDU6TGFiZWwyMzMxNjA=', 'url': 'https://api....   \n29  [{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ==', 'url': 'https://api.g...   \n   state  \n0   open  \n1   open  \n2   open  \n3   open  \n4   open  \n..   ...  \n25  open  \n26  open  \n27  open  \n28  open  \n29  open  \n[30 rows x 4 columns]\nWith a bit of elbow grease, you can create some higher-level interfaces to common\nweb APIs that return DataFrame objects for more convenient analysis.\n6.4 Interacting with Databases\nIn a business setting, a lot of data may not be stored in text or Excel files. SQL-based\nrelational databases (such as SQL Server, PostgreSQL, and MySQL) are in wide use,\nand many alternative databases have become quite popular. The choice of database\nis usually dependent on the performance, data integrity, and scalability needs of an\napplication.\npandas has some functions to simplify loading the results of a SQL query into a\nDataFrame. As an example, I’ll create a SQLite3 database using Python’s built-in\nsqlite3 driver:\nIn [135]: import sqlite3\nIn [136]: query = \"\"\"\n   .....: CREATE TABLE test\n   .....: (a VARCHAR(20), b VARCHAR(20),\n   .....:  c REAL,        d INTEGER\n6.4 Interacting with Databases \n| \n199",
      "content_length": 2283,
      "extraction_method": "Direct"
    },
    {
      "page_number": 218,
      "chapter": null,
      "content": ".....: );\"\"\"\nIn [137]: con = sqlite3.connect(\"mydata.sqlite\")\nIn [138]: con.execute(query)\nOut[138]: <sqlite3.Cursor at 0x7fdfd73b69c0>\nIn [139]: con.commit()\nThen, insert a few rows of data:\nIn [140]: data = [(\"Atlanta\", \"Georgia\", 1.25, 6),\n   .....:         (\"Tallahassee\", \"Florida\", 2.6, 3),\n   .....:         (\"Sacramento\", \"California\", 1.7, 5)]\nIn [141]: stmt = \"INSERT INTO test VALUES(?, ?, ?, ?)\"\nIn [142]: con.executemany(stmt, data)\nOut[142]: <sqlite3.Cursor at 0x7fdfd73a00c0>\nIn [143]: con.commit()\nMost Python SQL drivers return a list of tuples when selecting data from a table:\nIn [144]: cursor = con.execute(\"SELECT * FROM test\")\nIn [145]: rows = cursor.fetchall()\nIn [146]: rows\nOut[146]: \n[('Atlanta', 'Georgia', 1.25, 6),\n ('Tallahassee', 'Florida', 2.6, 3),\n ('Sacramento', 'California', 1.7, 5)]\nYou can pass the list of tuples to the DataFrame constructor, but you also need\nthe column names, contained in the cursor’s description attribute. Note that for\nSQLite3, the cursor description only provides column names (the other fields,\nwhich are part of Python’s Database API specification, are None), but for some other\ndatabase drivers, more column information is provided:\nIn [147]: cursor.description\nOut[147]: \n(('a', None, None, None, None, None, None),\n ('b', None, None, None, None, None, None),\n ('c', None, None, None, None, None, None),\n ('d', None, None, None, None, None, None))\nIn [148]: pd.DataFrame(rows, columns=[x[0] for x in cursor.description])\nOut[148]: \n             a           b     c  d\n0      Atlanta     Georgia  1.25  6\n1  Tallahassee     Florida  2.60  3\n2   Sacramento  California  1.70  5\n200 \n| \nChapter 6: Data Loading, Storage, and File Formats",
      "content_length": 1701,
      "extraction_method": "Direct"
    },
    {
      "page_number": 219,
      "chapter": null,
      "content": "This is quite a bit of munging that you’d rather not repeat each time you query the\ndatabase. The SQLAlchemy project is a popular Python SQL toolkit that abstracts\naway many of the common differences between SQL databases. pandas has a\nread_sql function that enables you to read data easily from a general SQLAlchemy\nconnection. You can install SQLAlchemy with conda like so:\nconda install sqlalchemy\nNow, we’ll connect to the same SQLite database with SQLAlchemy and read data\nfrom the table created before:\nIn [149]: import sqlalchemy as sqla\nIn [150]: db = sqla.create_engine(\"sqlite:///mydata.sqlite\")\nIn [151]: pd.read_sql(\"SELECT * FROM test\", db)\nOut[151]: \n             a           b     c  d\n0      Atlanta     Georgia  1.25  6\n1  Tallahassee     Florida  2.60  3\n2   Sacramento  California  1.70  5\n6.5 Conclusion\nGetting access to data is frequently the first step in the data analysis process. We have\nlooked at a number of useful tools in this chapter that should help you get started.\nIn the upcoming chapters we will dig deeper into data wrangling, data visualization,\ntime series analysis, and other topics.\n6.5 Conclusion \n| \n201",
      "content_length": 1146,
      "extraction_method": "Direct"
    },
    {
      "page_number": 220,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 221,
      "chapter": null,
      "content": "CHAPTER 7\nData Cleaning and Preparation\nDuring the course of doing data analysis and modeling, a significant amount of time\nis spent on data preparation: loading, cleaning, transforming, and rearranging. Such\ntasks are often reported to take up 80% or more of an analyst’s time. Sometimes the\nway that data is stored in files or databases is not in the right format for a particular\ntask. Many researchers choose to do ad hoc processing of data from one form to\nanother using a general-purpose programming language, like Python, Perl, R, or Java,\nor Unix text-processing tools like sed or awk. Fortunately, pandas, along with the\nbuilt-in Python language features, provides you with a high-level, flexible, and fast set\nof tools to enable you to manipulate data into the right form.\nIf you identify a type of data manipulation that isn’t anywhere in this book or\nelsewhere in the pandas library, feel free to share your use case on one of the\nPython mailing lists or on the pandas GitHub site. Indeed, much of the design and\nimplementation of pandas have been driven by the needs of real-world applications.\nIn this chapter I discuss tools for missing data, duplicate data, string manipulation,\nand some other analytical data transformations. In the next chapter, I focus on\ncombining and rearranging datasets in various ways.\n7.1 Handling Missing Data\nMissing data occurs commonly in many data analysis applications. One of the goals\nof pandas is to make working with missing data as painless as possible. For example,\nall of the descriptive statistics on pandas objects exclude missing data by default.\nThe way that missing data is represented in pandas objects is somewhat imperfect,\nbut it is sufficient for most real-world use. For data with float64 dtype, pandas uses\nthe floating-point value NaN (Not a Number) to represent missing data.\n203",
      "content_length": 1848,
      "extraction_method": "Direct"
    },
    {
      "page_number": 222,
      "chapter": null,
      "content": "We call this a sentinel value: when present, it indicates a missing (or null) value:\nIn [14]: float_data = pd.Series([1.2, -3.5, np.nan, 0])\nIn [15]: float_data\nOut[15]: \n0    1.2\n1   -3.5\n2    NaN\n3    0.0\ndtype: float64\nThe isna method gives us a Boolean Series with True where values are null:\nIn [16]: float_data.isna()\nOut[16]: \n0    False\n1    False\n2     True\n3    False\ndtype: bool\nIn pandas, we’ve adopted a convention used in the R programming language by refer‐\nring to missing data as NA, which stands for not available. In statistics applications,\nNA data may either be data that does not exist or that exists but was not observed\n(through problems with data collection, for example). When cleaning up data for\nanalysis, it is often important to do analysis on the missing data itself to identify data\ncollection problems or potential biases in the data caused by missing data.\nThe built-in Python None value is also treated as NA:\nIn [17]: string_data = pd.Series([\"aardvark\", np.nan, None, \"avocado\"])\nIn [18]: string_data\nOut[18]: \n0    aardvark\n1         NaN\n2        None\n3     avocado\ndtype: object\nIn [19]: string_data.isna()\nOut[19]: \n0    False\n1     True\n2     True\n3    False\ndtype: bool\nIn [20]: float_data = pd.Series([1, 2, None], dtype='float64')\nIn [21]: float_data\nOut[21]: \n204 \n| \nChapter 7: Data Cleaning and Preparation",
      "content_length": 1353,
      "extraction_method": "Direct"
    },
    {
      "page_number": 223,
      "chapter": null,
      "content": "0    1.0\n1    2.0\n2    NaN\ndtype: float64\nIn [22]: float_data.isna()\nOut[22]: \n0    False\n1    False\n2     True\ndtype: bool\nThe pandas project has attempted to make working with missing data consistent\nacross data types. Functions like pandas.isna abstract away many of the annoying\ndetails. See Table 7-1 for a list of some functions related to missing data handling.\nTable 7-1. NA handling object methods\nMethod\nDescription\ndropna\nFilter axis labels based on whether values for each label have missing data, with varying thresholds for how much\nmissing data to tolerate.\nfillna\nFill in missing data with some value or using an interpolation method such as \"ffill\" or \"bfill\".\nisna\nReturn Boolean values indicating which values are missing/NA.\nnotna\nNegation of isna, returns True for non-NA values and False for NA values.\nFiltering Out Missing Data\nThere are a few ways to filter out missing data. While you always have the option to\ndo it by hand using pandas.isna and Boolean indexing, dropna can be helpful. On a\nSeries, it returns the Series with only the nonnull data and index values:\nIn [23]: data = pd.Series([1, np.nan, 3.5, np.nan, 7])\nIn [24]: data.dropna()\nOut[24]: \n0    1.0\n2    3.5\n4    7.0\ndtype: float64\nThis is the same thing as doing:\nIn [25]: data[data.notna()]\nOut[25]: \n0    1.0\n2    3.5\n4    7.0\ndtype: float64\n7.1 Handling Missing Data \n| \n205",
      "content_length": 1370,
      "extraction_method": "Direct"
    },
    {
      "page_number": 224,
      "chapter": null,
      "content": "With DataFrame objects, there are different ways to remove missing data. You may\nwant to drop rows or columns that are all NA, or only those rows or columns\ncontaining any NAs at all. dropna by default drops any row containing a missing\nvalue:\nIn [26]: data = pd.DataFrame([[1., 6.5, 3.], [1., np.nan, np.nan],\n   ....:                      [np.nan, np.nan, np.nan], [np.nan, 6.5, 3.]])\nIn [27]: data\nOut[27]: \n     0    1    2\n0  1.0  6.5  3.0\n1  1.0  NaN  NaN\n2  NaN  NaN  NaN\n3  NaN  6.5  3.0\nIn [28]: data.dropna()\nOut[28]: \n     0    1    2\n0  1.0  6.5  3.0\nPassing how=\"all\" will drop only rows that are all NA:\nIn [29]: data.dropna(how=\"all\")\nOut[29]: \n     0    1    2\n0  1.0  6.5  3.0\n1  1.0  NaN  NaN\n3  NaN  6.5  3.0\nKeep in mind that these functions return new objects by default and do not modify\nthe contents of the original object.\nTo drop columns in the same way, pass axis=\"columns\":\nIn [30]: data[4] = np.nan\nIn [31]: data\nOut[31]: \n     0    1    2   4\n0  1.0  6.5  3.0 NaN\n1  1.0  NaN  NaN NaN\n2  NaN  NaN  NaN NaN\n3  NaN  6.5  3.0 NaN\nIn [32]: data.dropna(axis=\"columns\", how=\"all\")\nOut[32]: \n     0    1    2\n0  1.0  6.5  3.0\n1  1.0  NaN  NaN\n2  NaN  NaN  NaN\n3  NaN  6.5  3.0\n206 \n| \nChapter 7: Data Cleaning and Preparation",
      "content_length": 1247,
      "extraction_method": "Direct"
    },
    {
      "page_number": 225,
      "chapter": null,
      "content": "Suppose you want to keep only rows containing at most a certain number of missing\nobservations. You can indicate this with the thresh argument:\nIn [33]: df = pd.DataFrame(np.random.standard_normal((7, 3)))\nIn [34]: df.iloc[:4, 1] = np.nan\nIn [35]: df.iloc[:2, 2] = np.nan\nIn [36]: df\nOut[36]: \n          0         1         2\n0 -0.204708       NaN       NaN\n1 -0.555730       NaN       NaN\n2  0.092908       NaN  0.769023\n3  1.246435       NaN -1.296221\n4  0.274992  0.228913  1.352917\n5  0.886429 -2.001637 -0.371843\n6  1.669025 -0.438570 -0.539741\nIn [37]: df.dropna()\nOut[37]: \n          0         1         2\n4  0.274992  0.228913  1.352917\n5  0.886429 -2.001637 -0.371843\n6  1.669025 -0.438570 -0.539741\nIn [38]: df.dropna(thresh=2)\nOut[38]: \n          0         1         2\n2  0.092908       NaN  0.769023\n3  1.246435       NaN -1.296221\n4  0.274992  0.228913  1.352917\n5  0.886429 -2.001637 -0.371843\n6  1.669025 -0.438570 -0.539741\nFilling In Missing Data\nRather than filtering out missing data (and potentially discarding other data along\nwith it), you may want to fill in the “holes” in any number of ways. For most\npurposes, the fillna method is the workhorse function to use. Calling fillna with a\nconstant replaces missing values with that value:\nIn [39]: df.fillna(0)\nOut[39]: \n          0         1         2\n0 -0.204708  0.000000  0.000000\n1 -0.555730  0.000000  0.000000\n2  0.092908  0.000000  0.769023\n3  1.246435  0.000000 -1.296221\n4  0.274992  0.228913  1.352917\n7.1 Handling Missing Data \n| \n207",
      "content_length": 1517,
      "extraction_method": "Direct"
    },
    {
      "page_number": 226,
      "chapter": null,
      "content": "5  0.886429 -2.001637 -0.371843\n6  1.669025 -0.438570 -0.539741\nCalling fillna with a dictionary, you can use a different fill value for each column:\nIn [40]: df.fillna({1: 0.5, 2: 0})\nOut[40]: \n          0         1         2\n0 -0.204708  0.500000  0.000000\n1 -0.555730  0.500000  0.000000\n2  0.092908  0.500000  0.769023\n3  1.246435  0.500000 -1.296221\n4  0.274992  0.228913  1.352917\n5  0.886429 -2.001637 -0.371843\n6  1.669025 -0.438570 -0.539741\nThe same interpolation methods available for reindexing (see Table 5-3) can be used\nwith fillna:\nIn [41]: df = pd.DataFrame(np.random.standard_normal((6, 3)))\nIn [42]: df.iloc[2:, 1] = np.nan\nIn [43]: df.iloc[4:, 2] = np.nan\nIn [44]: df\nOut[44]: \n          0         1         2\n0  0.476985  3.248944 -1.021228\n1 -0.577087  0.124121  0.302614\n2  0.523772       NaN  1.343810\n3 -0.713544       NaN -2.370232\n4 -1.860761       NaN       NaN\n5 -1.265934       NaN       NaN\nIn [45]: df.fillna(method=\"ffill\")\nOut[45]: \n          0         1         2\n0  0.476985  3.248944 -1.021228\n1 -0.577087  0.124121  0.302614\n2  0.523772  0.124121  1.343810\n3 -0.713544  0.124121 -2.370232\n4 -1.860761  0.124121 -2.370232\n5 -1.265934  0.124121 -2.370232\nIn [46]: df.fillna(method=\"ffill\", limit=2)\nOut[46]: \n          0         1         2\n0  0.476985  3.248944 -1.021228\n1 -0.577087  0.124121  0.302614\n2  0.523772  0.124121  1.343810\n3 -0.713544  0.124121 -2.370232\n208 \n| \nChapter 7: Data Cleaning and Preparation",
      "content_length": 1453,
      "extraction_method": "Direct"
    },
    {
      "page_number": 227,
      "chapter": null,
      "content": "4 -1.860761       NaN -2.370232\n5 -1.265934       NaN -2.370232\nWith fillna you can do lots of other things such as simple data imputation using the\nmedian or mean statistics:\nIn [47]: data = pd.Series([1., np.nan, 3.5, np.nan, 7])\nIn [48]: data.fillna(data.mean())\nOut[48]: \n0    1.000000\n1    3.833333\n2    3.500000\n3    3.833333\n4    7.000000\ndtype: float64\nSee Table 7-2 for a reference on fillna function arguments.\nTable 7-2. fillna function arguments\nArgument\nDescription\nvalue\nScalar value or dictionary-like object to use to fill missing values\nmethod\nInterpolation method: one of \"bfill\" (backward fill) or \"ffill\" (forward fill); default is None\naxis\nAxis to fill on (\"index\" or \"columns\"); default is axis=\"index\"\nlimit\nFor forward and backward filling, maximum number of consecutive periods to fill\n7.2 Data Transformation\nSo far in this chapter we’ve been concerned with handling missing data. Filtering,\ncleaning, and other transformations are another class of important operations.\nRemoving Duplicates\nDuplicate rows may be found in a DataFrame for any number of reasons. Here is an\nexample:\nIn [49]: data = pd.DataFrame({\"k1\": [\"one\", \"two\"] * 3 + [\"two\"],\n   ....:                      \"k2\": [1, 1, 2, 3, 3, 4, 4]})\nIn [50]: data\nOut[50]: \n    k1  k2\n0  one   1\n1  two   1\n2  one   2\n3  two   3\n4  one   3\n5  two   4\n6  two   4\n7.2 Data Transformation \n| \n209",
      "content_length": 1377,
      "extraction_method": "Direct"
    },
    {
      "page_number": 228,
      "chapter": null,
      "content": "The DataFrame method duplicated returns a Boolean Series indicating whether\neach row is a duplicate (its column values are exactly equal to those in an earlier row)\nor not:\nIn [51]: data.duplicated()\nOut[51]: \n0    False\n1    False\n2    False\n3    False\n4    False\n5    False\n6     True\ndtype: bool\nRelatedly, drop_duplicates returns a DataFrame with rows where the duplicated\narray is False filtered out:\nIn [52]: data.drop_duplicates()\nOut[52]: \n    k1  k2\n0  one   1\n1  two   1\n2  one   2\n3  two   3\n4  one   3\n5  two   4\nBoth methods by default consider all of the columns; alternatively, you can specify\nany subset of them to detect duplicates. Suppose we had an additional column of\nvalues and wanted to filter duplicates based only on the \"k1\" column:\nIn [53]: data[\"v1\"] = range(7)\nIn [54]: data\nOut[54]: \n    k1  k2  v1\n0  one   1   0\n1  two   1   1\n2  one   2   2\n3  two   3   3\n4  one   3   4\n5  two   4   5\n6  two   4   6\nIn [55]: data.drop_duplicates(subset=[\"k1\"])\nOut[55]: \n    k1  k2  v1\n0  one   1   0\n1  two   1   1\n210 \n| \nChapter 7: Data Cleaning and Preparation",
      "content_length": 1082,
      "extraction_method": "Direct"
    },
    {
      "page_number": 229,
      "chapter": null,
      "content": "duplicated and drop_duplicates by default keep the first observed value combina‐\ntion. Passing keep=\"last\" will return the last one:\nIn [56]: data.drop_duplicates([\"k1\", \"k2\"], keep=\"last\")\nOut[56]: \n    k1  k2  v1\n0  one   1   0\n1  two   1   1\n2  one   2   2\n3  two   3   3\n4  one   3   4\n6  two   4   6\nTransforming Data Using a Function or Mapping\nFor many datasets, you may wish to perform some transformation based on the\nvalues in an array, Series, or column in a DataFrame. Consider the following hypo‐\nthetical data collected about various kinds of meat:\nIn [57]: data = pd.DataFrame({\"food\": [\"bacon\", \"pulled pork\", \"bacon\",\n   ....:                               \"pastrami\", \"corned beef\", \"bacon\",\n   ....:                               \"pastrami\", \"honey ham\", \"nova lox\"],\n   ....:                      \"ounces\": [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})\nIn [58]: data\nOut[58]: \n          food  ounces\n0        bacon     4.0\n1  pulled pork     3.0\n2        bacon    12.0\n3     pastrami     6.0\n4  corned beef     7.5\n5        bacon     8.0\n6     pastrami     3.0\n7    honey ham     5.0\n8     nova lox     6.0\nSuppose you wanted to add a column indicating the type of animal that each food\ncame from. Let’s write down a mapping of each distinct meat type to the kind of\nanimal:\nmeat_to_animal = {\n  \"bacon\": \"pig\",\n  \"pulled pork\": \"pig\",\n  \"pastrami\": \"cow\",\n  \"corned beef\": \"cow\",\n  \"honey ham\": \"pig\",\n  \"nova lox\": \"salmon\"\n}\n7.2 Data Transformation \n| \n211",
      "content_length": 1466,
      "extraction_method": "Direct"
    },
    {
      "page_number": 230,
      "chapter": null,
      "content": "The map method on a Series (also discussed in “Function Application and Mapping”\non page 158) accepts a function or dictionary-like object containing a mapping to do\nthe transformation of values:\nIn [60]: data[\"animal\"] = data[\"food\"].map(meat_to_animal)\nIn [61]: data\nOut[61]: \n          food  ounces  animal\n0        bacon     4.0     pig\n1  pulled pork     3.0     pig\n2        bacon    12.0     pig\n3     pastrami     6.0     cow\n4  corned beef     7.5     cow\n5        bacon     8.0     pig\n6     pastrami     3.0     cow\n7    honey ham     5.0     pig\n8     nova lox     6.0  salmon\nWe could also have passed a function that does all the work:\nIn [62]: def get_animal(x):\n   ....:     return meat_to_animal[x]\nIn [63]: data[\"food\"].map(get_animal)\nOut[63]: \n0       pig\n1       pig\n2       pig\n3       cow\n4       cow\n5       pig\n6       cow\n7       pig\n8    salmon\nName: food, dtype: object\nUsing map is a convenient way to perform element-wise transformations and other\ndata cleaning-related operations.\nReplacing Values\nFilling in missing data with the fillna method is a special case of more general value\nreplacement. As you’ve already seen, map can be used to modify a subset of values\nin an object, but replace provides a simpler and more flexible way to do so. Let’s\nconsider this Series:\nIn [64]: data = pd.Series([1., -999., 2., -999., -1000., 3.])\nIn [65]: data\nOut[65]: \n0       1.0\n212 \n| \nChapter 7: Data Cleaning and Preparation",
      "content_length": 1449,
      "extraction_method": "Direct"
    },
    {
      "page_number": 231,
      "chapter": null,
      "content": "1    -999.0\n2       2.0\n3    -999.0\n4   -1000.0\n5       3.0\ndtype: float64\nThe -999 values might be sentinel values for missing data. To replace these with NA\nvalues that pandas understands, we can use replace, producing a new Series:\nIn [66]: data.replace(-999, np.nan)\nOut[66]: \n0       1.0\n1       NaN\n2       2.0\n3       NaN\n4   -1000.0\n5       3.0\ndtype: float64\nIf you want to replace multiple values at once, you instead pass a list and then the\nsubstitute value:\nIn [67]: data.replace([-999, -1000], np.nan)\nOut[67]: \n0    1.0\n1    NaN\n2    2.0\n3    NaN\n4    NaN\n5    3.0\ndtype: float64\nTo use a different replacement for each value, pass a list of substitutes:\nIn [68]: data.replace([-999, -1000], [np.nan, 0])\nOut[68]: \n0    1.0\n1    NaN\n2    2.0\n3    NaN\n4    0.0\n5    3.0\ndtype: float64\nThe argument passed can also be a dictionary:\nIn [69]: data.replace({-999: np.nan, -1000: 0})\nOut[69]: \n0    1.0\n1    NaN\n2    2.0\n3    NaN\n4    0.0\n7.2 Data Transformation \n| \n213",
      "content_length": 979,
      "extraction_method": "Direct"
    },
    {
      "page_number": 232,
      "chapter": null,
      "content": "5    3.0\ndtype: float64\nThe data.replace method is distinct from data.str.replace,\nwhich performs element-wise string substitution. We look at these\nstring methods on Series later in the chapter.\nRenaming Axis Indexes\nLike values in a Series, axis labels can be similarly transformed by a function or\nmapping of some form to produce new, differently labeled objects. You can also\nmodify the axes in place without creating a new data structure. Here’s a simple\nexample:\nIn [70]: data = pd.DataFrame(np.arange(12).reshape((3, 4)),\n   ....:                     index=[\"Ohio\", \"Colorado\", \"New York\"],\n   ....:                     columns=[\"one\", \"two\", \"three\", \"four\"])\nLike a Series, the axis indexes have a map method:\nIn [71]: def transform(x):\n   ....:     return x[:4].upper()\nIn [72]: data.index.map(transform)\nOut[72]: Index(['OHIO', 'COLO', 'NEW '], dtype='object')\nYou can assign to the index attribute, modifying the DataFrame in place:\nIn [73]: data.index = data.index.map(transform)\nIn [74]: data\nOut[74]: \n      one  two  three  four\nOHIO    0    1      2     3\nCOLO    4    5      6     7\nNEW     8    9     10    11\nIf you want to create a transformed version of a dataset without modifying the\noriginal, a useful method is rename:\nIn [75]: data.rename(index=str.title, columns=str.upper)\nOut[75]: \n      ONE  TWO  THREE  FOUR\nOhio    0    1      2     3\nColo    4    5      6     7\nNew     8    9     10    11\nNotably, rename can be used in conjunction with a dictionary-like object, providing\nnew values for a subset of the axis labels:\n214 \n| \nChapter 7: Data Cleaning and Preparation",
      "content_length": 1600,
      "extraction_method": "Direct"
    },
    {
      "page_number": 233,
      "chapter": null,
      "content": "In [76]: data.rename(index={\"OHIO\": \"INDIANA\"},\n   ....:             columns={\"three\": \"peekaboo\"})\nOut[76]: \n         one  two  peekaboo  four\nINDIANA    0    1         2     3\nCOLO       4    5         6     7\nNEW        8    9        10    11\nrename saves you from the chore of copying the DataFrame manually and assigning\nnew values to its index and columns attributes.\nDiscretization and Binning\nContinuous data is often discretized or otherwise separated into “bins” for analysis.\nSuppose you have data about a group of people in a study, and you want to group\nthem into discrete age buckets:\nIn [77]: ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]\nLet’s divide these into bins of 18 to 25, 26 to 35, 36 to 60, and finally 61 and older. To\ndo so, you have to use pandas.cut:\nIn [78]: bins = [18, 25, 35, 60, 100]\nIn [79]: age_categories = pd.cut(ages, bins)\nIn [80]: age_categories\nOut[80]: \n[(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35,\n 60], (35, 60], (25, 35]]\nLength: 12\nCategories (4, interval[int64, right]): [(18, 25] < (25, 35] < (35, 60] < (60, 10\n0]]\nThe object pandas returns is a special Categorical object. The output you see\ndescribes the bins computed by pandas.cut. Each bin is identified by a special\n(unique to pandas) interval value type containing the lower and upper limit of each\nbin:\nIn [81]: age_categories.codes\nOut[81]: array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1], dtype=int8)\nIn [82]: age_categories.categories\nOut[82]: IntervalIndex([(18, 25], (25, 35], (35, 60], (60, 100]], dtype='interval\n[int64, right]')\nIn [83]: age_categories.categories[0]\nOut[83]: Interval(18, 25, closed='right')\nIn [84]: pd.value_counts(age_categories)\nOut[84]: \n(18, 25]     5\n7.2 Data Transformation \n| \n215",
      "content_length": 1764,
      "extraction_method": "Direct"
    },
    {
      "page_number": 234,
      "chapter": null,
      "content": "(25, 35]     3\n(35, 60]     3\n(60, 100]    1\ndtype: int64\nNote that pd.value_counts(categories) are the bin counts for the result of\npandas.cut.\nIn the string representation of an interval, a parenthesis means that the side is open\n(exclusive), while the square bracket means it is closed (inclusive). You can change\nwhich side is closed by passing right=False:\nIn [85]: pd.cut(ages, bins, right=False)\nOut[85]: \n[[18, 25), [18, 25), [25, 35), [25, 35), [18, 25), ..., [25, 35), [60, 100), [35,\n 60), [35, 60), [25, 35)]\nLength: 12\nCategories (4, interval[int64, left]): [[18, 25) < [25, 35) < [35, 60) < [60, 100\n)]\nYou can override the default interval-based bin labeling by passing a list or array to\nthe labels option:\nIn [86]: group_names = [\"Youth\", \"YoungAdult\", \"MiddleAged\", \"Senior\"]\nIn [87]: pd.cut(ages, bins, labels=group_names)\nOut[87]: \n['Youth', 'Youth', 'Youth', 'YoungAdult', 'Youth', ..., 'YoungAdult', 'Senior', '\nMiddleAged', 'MiddleAged', 'YoungAdult']\nLength: 12\nCategories (4, object): ['Youth' < 'YoungAdult' < 'MiddleAged' < 'Senior']\nIf you pass an integer number of bins to pandas.cut instead of explicit bin edges, it\nwill compute equal-length bins based on the minimum and maximum values in the\ndata. Consider the case of some uniformly distributed data chopped into fourths:\nIn [88]: data = np.random.uniform(size=20)\nIn [89]: pd.cut(data, 4, precision=2)\nOut[89]: \n[(0.34, 0.55], (0.34, 0.55], (0.76, 0.97], (0.76, 0.97], (0.34, 0.55], ..., (0.34\n, 0.55], (0.34, 0.55], (0.55, 0.76], (0.34, 0.55], (0.12, 0.34]]\nLength: 20\nCategories (4, interval[float64, right]): [(0.12, 0.34] < (0.34, 0.55] < (0.55, 0\n.76] <\n                                           (0.76, 0.97]]\nThe precision=2 option limits the decimal precision to two digits.\nA closely related function, pandas.qcut, bins the data based on sample quantiles.\nDepending on the distribution of the data, using pandas.cut will not usually result\n216 \n| \nChapter 7: Data Cleaning and Preparation",
      "content_length": 1982,
      "extraction_method": "Direct"
    },
    {
      "page_number": 235,
      "chapter": null,
      "content": "in each bin having the same number of data points. Since pandas.qcut uses sample\nquantiles instead, you will obtain roughly equally sized bins:\nIn [90]: data = np.random.standard_normal(1000)\nIn [91]: quartiles = pd.qcut(data, 4, precision=2)\nIn [92]: quartiles\nOut[92]: \n[(-0.026, 0.62], (0.62, 3.93], (-0.68, -0.026], (0.62, 3.93], (-0.026, 0.62], ...\n, (-0.68, -0.026], (-0.68, -0.026], (-2.96, -0.68], (0.62, 3.93], (-0.68, -0.026]\n]\nLength: 1000\nCategories (4, interval[float64, right]): [(-2.96, -0.68] < (-0.68, -0.026] < (-0\n.026, 0.62] <\n                                           (0.62, 3.93]]\nIn [93]: pd.value_counts(quartiles)\nOut[93]: \n(-2.96, -0.68]     250\n(-0.68, -0.026]    250\n(-0.026, 0.62]     250\n(0.62, 3.93]       250\ndtype: int64\nSimilar to pandas.cut, you can pass your own quantiles (numbers between 0 and 1,\ninclusive):\nIn [94]: pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.]).value_counts()\nOut[94]: \n(-2.9499999999999997, -1.187]    100\n(-1.187, -0.0265]                400\n(-0.0265, 1.286]                 400\n(1.286, 3.928]                   100\ndtype: int64\nWe’ll return to pandas.cut and pandas.qcut later in the chapter during our dis‐\ncussion of aggregation and group operations, as these discretization functions are\nespecially useful for quantile and group analysis.\nDetecting and Filtering Outliers\nFiltering or transforming outliers is largely a matter of applying array operations.\nConsider a DataFrame with some normally distributed data:\nIn [95]: data = pd.DataFrame(np.random.standard_normal((1000, 4)))\nIn [96]: data.describe()\nOut[96]: \n                 0            1            2            3\ncount  1000.000000  1000.000000  1000.000000  1000.000000\nmean      0.049091     0.026112    -0.002544    -0.051827\n7.2 Data Transformation \n| \n217",
      "content_length": 1778,
      "extraction_method": "Direct"
    },
    {
      "page_number": 236,
      "chapter": null,
      "content": "std       0.996947     1.007458     0.995232     0.998311\nmin      -3.645860    -3.184377    -3.745356    -3.428254\n25%      -0.599807    -0.612162    -0.687373    -0.747478\n50%       0.047101    -0.013609    -0.022158    -0.088274\n75%       0.756646     0.695298     0.699046     0.623331\nmax       2.653656     3.525865     2.735527     3.366626\nSuppose you wanted to find values in one of the columns exceeding 3 in absolute\nvalue:\nIn [97]: col = data[2]\nIn [98]: col[col.abs() > 3]\nOut[98]: \n41    -3.399312\n136   -3.745356\nName: 2, dtype: float64\nTo select all rows having a value exceeding 3 or –3, you can use the any method on a\nBoolean DataFrame:\nIn [99]: data[(data.abs() > 3).any(axis=\"columns\")]\nOut[99]: \n            0         1         2         3\n41   0.457246 -0.025907 -3.399312 -0.974657\n60   1.951312  3.260383  0.963301  1.201206\n136  0.508391 -0.196713 -3.745356 -1.520113\n235 -0.242459 -3.056990  1.918403 -0.578828\n258  0.682841  0.326045  0.425384 -3.428254\n322  1.179227 -3.184377  1.369891 -1.074833\n544 -3.548824  1.553205 -2.186301  1.277104\n635 -0.578093  0.193299  1.397822  3.366626\n782 -0.207434  3.525865  0.283070  0.544635\n803 -3.645860  0.255475 -0.549574 -1.907459\nThe parentheses around data.abs() > 3 are necessary in order to call the any\nmethod on the result of the comparison operation.\nValues can be set based on these criteria. Here is code to cap values outside the\ninterval –3 to 3:\nIn [100]: data[data.abs() > 3] = np.sign(data) * 3\nIn [101]: data.describe()\nOut[101]: \n                 0            1            2            3\ncount  1000.000000  1000.000000  1000.000000  1000.000000\nmean      0.050286     0.025567    -0.001399    -0.051765\nstd       0.992920     1.004214     0.991414     0.995761\nmin      -3.000000    -3.000000    -3.000000    -3.000000\n25%      -0.599807    -0.612162    -0.687373    -0.747478\n50%       0.047101    -0.013609    -0.022158    -0.088274\n218 \n| \nChapter 7: Data Cleaning and Preparation",
      "content_length": 1971,
      "extraction_method": "Direct"
    },
    {
      "page_number": 237,
      "chapter": null,
      "content": "75%       0.756646     0.695298     0.699046     0.623331\nmax       2.653656     3.000000     2.735527     3.000000\nThe statement np.sign(data) produces 1 and –1 values based on whether the values\nin data are positive or negative:\nIn [102]: np.sign(data).head()\nOut[102]: \n     0    1    2    3\n0 -1.0  1.0 -1.0  1.0\n1  1.0 -1.0  1.0 -1.0\n2  1.0  1.0  1.0 -1.0\n3 -1.0 -1.0  1.0 -1.0\n4 -1.0  1.0 -1.0 -1.0\nPermutation and Random Sampling\nPermuting (randomly reordering) a Series or the rows in a DataFrame is possible\nusing the numpy.random.permutation function. Calling permutation with the length\nof the axis you want to permute produces an array of integers indicating the new\nordering:\nIn [103]: df = pd.DataFrame(np.arange(5 * 7).reshape((5, 7)))\nIn [104]: df\nOut[104]: \n    0   1   2   3   4   5   6\n0   0   1   2   3   4   5   6\n1   7   8   9  10  11  12  13\n2  14  15  16  17  18  19  20\n3  21  22  23  24  25  26  27\n4  28  29  30  31  32  33  34\nIn [105]: sampler = np.random.permutation(5)\nIn [106]: sampler\nOut[106]: array([3, 1, 4, 2, 0])\nThat array can then be used in iloc-based indexing or the equivalent take function:\nIn [107]: df.take(sampler)\nOut[107]: \n    0   1   2   3   4   5   6\n3  21  22  23  24  25  26  27\n1   7   8   9  10  11  12  13\n4  28  29  30  31  32  33  34\n2  14  15  16  17  18  19  20\n0   0   1   2   3   4   5   6\nIn [108]: df.iloc[sampler]\nOut[108]: \n    0   1   2   3   4   5   6\n7.2 Data Transformation \n| \n219",
      "content_length": 1452,
      "extraction_method": "Direct"
    },
    {
      "page_number": 238,
      "chapter": null,
      "content": "3  21  22  23  24  25  26  27\n1   7   8   9  10  11  12  13\n4  28  29  30  31  32  33  34\n2  14  15  16  17  18  19  20\n0   0   1   2   3   4   5   6\nBy invoking take with axis=\"columns\", we could also select a permutation of the\ncolumns:\nIn [109]: column_sampler = np.random.permutation(7)\nIn [110]: column_sampler\nOut[110]: array([4, 6, 3, 2, 1, 0, 5])\nIn [111]: df.take(column_sampler, axis=\"columns\")\nOut[111]: \n    4   6   3   2   1   0   5\n0   4   6   3   2   1   0   5\n1  11  13  10   9   8   7  12\n2  18  20  17  16  15  14  19\n3  25  27  24  23  22  21  26\n4  32  34  31  30  29  28  33\nTo select a random subset without replacement (the same row cannot appear twice),\nyou can use the sample method on Series and DataFrame:\nIn [112]: df.sample(n=3)\nOut[112]: \n    0   1   2   3   4   5   6\n2  14  15  16  17  18  19  20\n4  28  29  30  31  32  33  34\n0   0   1   2   3   4   5   6\nTo generate a sample with replacement (to allow repeat choices), pass replace=True\nto sample:\nIn [113]: choices = pd.Series([5, 7, -1, 6, 4])\nIn [114]: choices.sample(n=10, replace=True)\nOut[114]: \n2   -1\n0    5\n3    6\n1    7\n4    4\n0    5\n4    4\n0    5\n4    4\n4    4\ndtype: int64\n220 \n| \nChapter 7: Data Cleaning and Preparation",
      "content_length": 1218,
      "extraction_method": "Direct"
    },
    {
      "page_number": 239,
      "chapter": null,
      "content": "Computing Indicator/Dummy Variables\nAnother type of transformation for statistical modeling or machine learning applica‐\ntions is converting a categorical variable into a dummy or indicator matrix. If a col‐\numn in a DataFrame has k distinct values, you would derive a matrix or DataFrame\nwith k columns containing all 1s and 0s. pandas has a pandas.get_dummies function\nfor doing this, though you could also devise one yourself. Let’s consider an example\nDataFrame:\nIn [115]: df = pd.DataFrame({\"key\": [\"b\", \"b\", \"a\", \"c\", \"a\", \"b\"],\n   .....:                    \"data1\": range(6)})\nIn [116]: df\nOut[116]: \n  key  data1\n0   b      0\n1   b      1\n2   a      2\n3   c      3\n4   a      4\n5   b      5\nIn [117]: pd.get_dummies(df[\"key\"])\nOut[117]: \n   a  b  c\n0  0  1  0\n1  0  1  0\n2  1  0  0\n3  0  0  1\n4  1  0  0\n5  0  1  0\nIn some cases, you may want to add a prefix to the columns in the indicator Data‐\nFrame, which can then be merged with the other data. pandas.get_dummies has a\nprefix argument for doing this:\nIn [118]: dummies = pd.get_dummies(df[\"key\"], prefix=\"key\")\nIn [119]: df_with_dummy = df[[\"data1\"]].join(dummies)\nIn [120]: df_with_dummy\nOut[120]: \n   data1  key_a  key_b  key_c\n0      0      0      1      0\n1      1      0      1      0\n2      2      1      0      0\n3      3      0      0      1\n4      4      1      0      0\n5      5      0      1      0\nThe DataFrame.join method will be explained in more detail in the next chapter.\n7.2 Data Transformation \n| \n221",
      "content_length": 1485,
      "extraction_method": "Direct"
    },
    {
      "page_number": 240,
      "chapter": null,
      "content": "If a row in a DataFrame belongs to multiple categories, we have to use a different\napproach to create the dummy variables. Let’s look at the MovieLens 1M dataset,\nwhich is investigated in more detail in Chapter 13:\nIn [121]: mnames = [\"movie_id\", \"title\", \"genres\"]\nIn [122]: movies = pd.read_table(\"datasets/movielens/movies.dat\", sep=\"::\",\n   .....:                        header=None, names=mnames, engine=\"python\")\nIn [123]: movies[:10]\nOut[123]: \n   movie_id                               title                        genres\n0         1                    Toy Story (1995)   Animation|Children's|Comedy\n1         2                      Jumanji (1995)  Adventure|Children's|Fantasy\n2         3             Grumpier Old Men (1995)                Comedy|Romance\n3         4            Waiting to Exhale (1995)                  Comedy|Drama\n4         5  Father of the Bride Part II (1995)                        Comedy\n5         6                         Heat (1995)         Action|Crime|Thriller\n6         7                      Sabrina (1995)                Comedy|Romance\n7         8                 Tom and Huck (1995)          Adventure|Children's\n8         9                 Sudden Death (1995)                        Action\n9        10                    GoldenEye (1995)     Action|Adventure|Thriller\npandas has implemented a special Series method str.get_dummies (methods that\nstart with str. are discussed in more detail later in Section 7.4, “String Manipula‐\ntion,” on page 227) that handles this scenario of multiple group membership encoded\nas a delimited string:\nIn [124]: dummies = movies[\"genres\"].str.get_dummies(\"|\")\nIn [125]: dummies.iloc[:10, :6]\nOut[125]: \n   Action  Adventure  Animation  Children's  Comedy  Crime\n0       0          0          1           1       1      0\n1       0          1          0           1       0      0\n2       0          0          0           0       1      0\n3       0          0          0           0       1      0\n4       0          0          0           0       1      0\n5       1          0          0           0       0      1\n6       0          0          0           0       1      0\n7       0          1          0           1       0      0\n8       1          0          0           0       0      0\n9       1          1          0           0       0      0\nThen, as before, you can combine this with movies while adding a \"Genre_\" to the\ncolumn names in the dummies DataFrame with the add_prefix method:\nIn [126]: movies_windic = movies.join(dummies.add_prefix(\"Genre_\"))\nIn [127]: movies_windic.iloc[0]\nOut[127]: \nmovie_id                                       1\n222 \n| \nChapter 7: Data Cleaning and Preparation",
      "content_length": 2684,
      "extraction_method": "Direct"
    },
    {
      "page_number": 241,
      "chapter": null,
      "content": "title                           Toy Story (1995)\ngenres               Animation|Children's|Comedy\nGenre_Action                                   0\nGenre_Adventure                                0\nGenre_Animation                                1\nGenre_Children's                               1\nGenre_Comedy                                   1\nGenre_Crime                                    0\nGenre_Documentary                              0\nGenre_Drama                                    0\nGenre_Fantasy                                  0\nGenre_Film-Noir                                0\nGenre_Horror                                   0\nGenre_Musical                                  0\nGenre_Mystery                                  0\nGenre_Romance                                  0\nGenre_Sci-Fi                                   0\nGenre_Thriller                                 0\nGenre_War                                      0\nGenre_Western                                  0\nName: 0, dtype: object\nFor much larger data, this method of constructing indicator vari‐\nables with multiple membership is not especially speedy. It would\nbe better to write a lower-level function that writes directly to a\nNumPy array, and then wrap the result in a DataFrame.\nA useful recipe for statistical applications is to combine pandas.get_dummies with a\ndiscretization function like pandas.cut:\nIn [128]: np.random.seed(12345) # to make the example repeatable\nIn [129]: values = np.random.uniform(size=10)\nIn [130]: values\nOut[130]: \narray([0.9296, 0.3164, 0.1839, 0.2046, 0.5677, 0.5955, 0.9645, 0.6532,\n       0.7489, 0.6536])\nIn [131]: bins = [0, 0.2, 0.4, 0.6, 0.8, 1]\nIn [132]: pd.get_dummies(pd.cut(values, bins))\nOut[132]: \n   (0.0, 0.2]  (0.2, 0.4]  (0.4, 0.6]  (0.6, 0.8]  (0.8, 1.0]\n0           0           0           0           0           1\n1           0           1           0           0           0\n2           1           0           0           0           0\n3           0           1           0           0           0\n4           0           0           1           0           0\n5           0           0           1           0           0\n7.2 Data Transformation \n| \n223",
      "content_length": 2184,
      "extraction_method": "Direct"
    },
    {
      "page_number": 242,
      "chapter": null,
      "content": "6           0           0           0           0           1\n7           0           0           0           1           0\n8           0           0           0           1           0\n9           0           0           0           1           0\nWe will look again at pandas.get_dummies later in “Creating dummy variables for\nmodeling” on page 245.\n7.3 Extension Data Types\nThis is a newer and more advanced topic that many pandas users\ndo not need to know a lot about, but I present it here for complete‐\nness since I will reference and use extension data types in various\nplaces in the upcoming chapters.\npandas was originally built upon the capabilities present in NumPy, an array comput‐\ning library used primarily for working with numerical data. Many pandas concepts,\nsuch as missing data, were implemented using what was available in NumPy while\ntrying to maximize compatibility between libraries that used NumPy and pandas\ntogether.\nBuilding on NumPy led to a number of shortcomings, such as:\n• Missing data handling for some numerical data types, such as integers and Boo‐\n•\nleans, was incomplete. As a result, when missing data was introduced into such\ndata, pandas converted the data type to float64 and used np.nan to represent\nnull values. This had compounding effects by introducing subtle issues into many\npandas algorithms.\n• Datasets with a lot of string data were computationally expensive and used a lot\n•\nof memory.\n• Some data types, like time intervals, timedeltas, and timestamps with time zones,\n•\ncould not be supported efficiently without using computationally expensive\narrays of Python objects.\nMore recently, pandas has developed an extension type system allowing for new data\ntypes to be added even if they are not supported natively by NumPy. These new data\ntypes can be treated as first class alongside data coming from NumPy arrays.\nLet’s look at an example where we create a Series of integers with a missing value:\nIn [133]: s = pd.Series([1, 2, 3, None])\nIn [134]: s\nOut[134]: \n0    1.0\n224 \n| \nChapter 7: Data Cleaning and Preparation",
      "content_length": 2073,
      "extraction_method": "Direct"
    },
    {
      "page_number": 243,
      "chapter": null,
      "content": "1    2.0\n2    3.0\n3    NaN\ndtype: float64\nIn [135]: s.dtype\nOut[135]: dtype('float64')\nMainly for backward compatibility reasons, Series uses the legacy behavior of using\na float64 data type and np.nan for the missing value. We could create this Series\ninstead using pandas.Int64Dtype:\nIn [136]: s = pd.Series([1, 2, 3, None], dtype=pd.Int64Dtype())\nIn [137]: s\nOut[137]: \n0       1\n1       2\n2       3\n3    <NA>\ndtype: Int64\nIn [138]: s.isna()\nOut[138]: \n0    False\n1    False\n2    False\n3     True\ndtype: bool\nIn [139]: s.dtype\nOut[139]: Int64Dtype()\nThe output <NA> indicates that a value is missing for an extension type array. This\nuses the special pandas.NA sentinel value:\nIn [140]: s[3]\nOut[140]: <NA>\nIn [141]: s[3] is pd.NA\nOut[141]: True\nWe also could have used the shorthand \"Int64\" instead of pd.Int64Dtype() to\nspecify the type. The capitalization is necessary, otherwise it will be a NumPy-based\nnonextension type:\nIn [142]: s = pd.Series([1, 2, 3, None], dtype=\"Int64\")\n7.3 Extension Data Types \n| \n225",
      "content_length": 1018,
      "extraction_method": "Direct"
    },
    {
      "page_number": 244,
      "chapter": null,
      "content": "pandas also has an extension type specialized for string data that does not use\nNumPy object arrays (it requires the pyarrow library, which you may need to install\nseparately):\nIn [143]: s = pd.Series(['one', 'two', None, 'three'], dtype=pd.StringDtype())\nIn [144]: s\nOut[144]: \n0      one\n1      two\n2     <NA>\n3    three\ndtype: string\nThese string arrays generally use much less memory and are frequently computation‐\nally more efficient for doing operations on large datasets.\nAnother important extension type is Categorical, which we discuss in more detail in\nSection 7.5, “Categorical Data,” on page 235. A reasonably complete list of extension\ntypes available as of this writing is in Table 7-3.\nExtension types can be passed to the Series astype method, allowing you to convert\neasily as part of your data cleaning process:\nIn [145]: df = pd.DataFrame({\"A\": [1, 2, None, 4],\n   .....:                    \"B\": [\"one\", \"two\", \"three\", None],\n   .....:                    \"C\": [False, None, False, True]})\nIn [146]: df\nOut[146]: \n     A      B      C\n0  1.0    one  False\n1  2.0    two   None\n2  NaN  three  False\n3  4.0   None   True\nIn [147]: df[\"A\"] = df[\"A\"].astype(\"Int64\")\nIn [148]: df[\"B\"] = df[\"B\"].astype(\"string\")\nIn [149]: df[\"C\"] = df[\"C\"].astype(\"boolean\")\nIn [150]: df\nOut[150]: \n      A      B      C\n0     1    one  False\n1     2    two   <NA>\n2  <NA>  three  False\n3     4   <NA>   True\n226 \n| \nChapter 7: Data Cleaning and Preparation",
      "content_length": 1456,
      "extraction_method": "Direct"
    },
    {
      "page_number": 245,
      "chapter": null,
      "content": "Table 7-3. pandas extension data types\nExtension type\nDescription\nBooleanDtype\nNullable Boolean data, use \"boolean\" when passing as string\nCategoricalDtype\nCategorical data type, use \"category\" when passing as string\nDatetimeTZDtype\nDatetime with time zone\nFloat32Dtype\n32-bit nullable floating point, use \"Float32\" when passing as string\nFloat64Dtype\n64-bit nullable floating point, use \"Float64\" when passing as string\nInt8Dtype\n8-bit nullable signed integer, use \"Int8\" when passing as string\nInt16Dtype\n16-bit nullable signed integer, use \"Int16\" when passing as string\nInt32Dtype\n32-bit nullable signed integer, use \"Int32\" when passing as string\nInt64Dtype\n64-bit nullable signed integer, use \"Int64\" when passing as string\nUInt8Dtype\n8-bit nullable unsigned integer, use \"UInt8\" when passing as string\nUInt16Dtype\n16-bit nullable unsigned integer, use \"UInt16\" when passing as string\nUInt32Dtype\n32-bit nullable unsigned integer, use \"UInt32\" when passing as string\nUInt64Dtype\n64-bit nullable unsigned integer, use \"UInt64\" when passing as string\n7.4 String Manipulation\nPython has long been a popular raw data manipulation language in part due to its\nease of use for string and text processing. Most text operations are made simple\nwith the string object’s built-in methods. For more complex pattern matching and\ntext manipulations, regular expressions may be needed. pandas adds to the mix by\nenabling you to apply string and regular expressions concisely on whole arrays of\ndata, additionally handling the annoyance of missing data.\nPython Built-In String Object Methods\nIn many string munging and scripting applications, built-in string methods are\nsufficient. As an example, a comma-separated string can be broken into pieces with\nsplit:\nIn [151]: val = \"a,b,  guido\"\nIn [152]: val.split(\",\")\nOut[152]: ['a', 'b', '  guido']\nsplit is often combined with strip to trim whitespace (including line breaks):\nIn [153]: pieces = [x.strip() for x in val.split(\",\")]\nIn [154]: pieces\nOut[154]: ['a', 'b', 'guido']\n7.4 String Manipulation \n| \n227",
      "content_length": 2050,
      "extraction_method": "Direct"
    },
    {
      "page_number": 246,
      "chapter": null,
      "content": "These substrings could be concatenated together with a two-colon delimiter using\naddition:\nIn [155]: first, second, third = pieces\nIn [156]: first + \"::\" + second + \"::\" + third\nOut[156]: 'a::b::guido'\nBut this isn’t a practical generic method. A faster and more Pythonic way is to pass a\nlist or tuple to the join method on the string \"::\":\nIn [157]: \"::\".join(pieces)\nOut[157]: 'a::b::guido'\nOther methods are concerned with locating substrings. Using Python’s in keyword is\nthe best way to detect a substring, though index and find can also be used:\nIn [158]: \"guido\" in val\nOut[158]: True\nIn [159]: val.index(\",\")\nOut[159]: 1\nIn [160]: val.find(\":\")\nOut[160]: -1\nNote that the difference between find and index is that index raises an exception if\nthe string isn’t found (versus returning –1):\nIn [161]: val.index(\":\")\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-161-bea4c4c30248> in <module>\n----> 1 val.index(\":\")\nValueError: substring not found\nRelatedly, count returns the number of occurrences of a particular substring:\nIn [162]: val.count(\",\")\nOut[162]: 2\nreplace will substitute occurrences of one pattern for another. It is commonly used\nto delete patterns, too, by passing an empty string:\nIn [163]: val.replace(\",\", \"::\")\nOut[163]: 'a::b::  guido'\nIn [164]: val.replace(\",\", \"\")\nOut[164]: 'ab  guido'\nSee Table 7-4 for a listing of some of Python’s string methods.\nRegular expressions can also be used with many of these operations, as you’ll see.\n228 \n| \nChapter 7: Data Cleaning and Preparation",
      "content_length": 1634,
      "extraction_method": "Direct"
    },
    {
      "page_number": 247,
      "chapter": null,
      "content": "Table 7-4. Python built-in string methods\nMethod\nDescription\ncount\nReturn the number of nonoverlapping occurrences of substring in the string\nendswith\nReturn True if string ends with suffix\nstartswith\nReturn True if string starts with prefix\njoin\nUse string as delimiter for concatenating a sequence of other strings\nindex\nReturn starting index of the first occurrence of passed substring if found in the string; otherwise, raises\nValueError if not found\nfind\nReturn position of first character of first occurrence of substring in the string; like index, but returns –1 if\nnot found\nrfind\nReturn position of first character of last occurrence of substring in the string; returns –1 if not found\nreplace\nReplace occurrences of string with another string\nstrip, \nrstrip, \nlstrip\nTrim whitespace, including newlines on both sides, on the right side, or on the left side, respectively\nsplit\nBreak string into list of substrings using passed delimiter\nlower\nConvert alphabet characters to lowercase\nupper\nConvert alphabet characters to uppercase\ncasefold\nConvert characters to lowercase, and convert any region-specific variable character combinations to a\ncommon comparable form\nljust, \nrjust\nLeft justify or right justify, respectively; pad opposite side of string with spaces (or some other fill\ncharacter) to return a string with a minimum width\nRegular Expressions\nRegular expressions provide a flexible way to search or match (often more complex)\nstring patterns in text. A single expression, commonly called a regex, is a string\nformed according to the regular expression language. Python’s built-in re module is\nresponsible for applying regular expressions to strings; I’ll give a number of examples\nof its use here.\nThe art of writing regular expressions could be a chapter of its own\nand thus is outside the book’s scope. There are many excellent tuto‐\nrials and references available on the internet and in other books.\nThe re module functions fall into three categories: pattern matching, substitution,\nand splitting. Naturally these are all related; a regex describes a pattern to locate in\nthe text, which can then be used for many purposes. Let’s look at a simple example:\nsuppose we wanted to split a string with a variable number of whitespace characters\n(tabs, spaces, and newlines).\n7.4 String Manipulation \n| \n229",
      "content_length": 2327,
      "extraction_method": "Direct"
    },
    {
      "page_number": 248,
      "chapter": null,
      "content": "The regex describing one or more whitespace characters is \\s+:\nIn [165]: import re\nIn [166]: text = \"foo    bar\\t baz  \\tqux\"\nIn [167]: re.split(r\"\\s+\", text)\nOut[167]: ['foo', 'bar', 'baz', 'qux']\nWhen you call re.split(r\"\\s+\", text), the regular expression is first compiled, and\nthen its split method is called on the passed text. You can compile the regex yourself\nwith re.compile, forming a reusable regex object:\nIn [168]: regex = re.compile(r\"\\s+\")\nIn [169]: regex.split(text)\nOut[169]: ['foo', 'bar', 'baz', 'qux']\nIf, instead, you wanted to get a list of all patterns matching the regex, you can use the\nfindall method:\nIn [170]: regex.findall(text)\nOut[170]: ['    ', '\\t ', '  \\t']\nTo avoid unwanted escaping with \\ in a regular expression, use raw\nstring literals like r\"C:\\x\" instead of the equivalent \"C:\\\\x\".\nCreating a regex object with re.compile is highly recommended if you intend to\napply the same expression to many strings; doing so will save CPU cycles.\nmatch and search are closely related to findall. While findall returns all matches\nin a string, search returns only the first match. More rigidly, match only matches at\nthe beginning of the string. As a less trivial example, let’s consider a block of text and\na regular expression capable of identifying most email addresses:\ntext = \"\"\"Dave dave@google.com\nSteve steve@gmail.com\nRob rob@gmail.com\nRyan ryan@yahoo.com\"\"\"\npattern = r\"[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,4}\"\n# re.IGNORECASE makes the regex case insensitive\nregex = re.compile(pattern, flags=re.IGNORECASE)\nUsing findall on the text produces a list of the email addresses:\nIn [172]: regex.findall(text)\nOut[172]: \n['dave@google.com',\n230 \n| \nChapter 7: Data Cleaning and Preparation",
      "content_length": 1722,
      "extraction_method": "Direct"
    },
    {
      "page_number": 249,
      "chapter": null,
      "content": "'steve@gmail.com',\n 'rob@gmail.com',\n 'ryan@yahoo.com']\nsearch returns a special match object for the first email address in the text. For the\npreceding regex, the match object can only tell us the start and end position of the\npattern in the string:\nIn [173]: m = regex.search(text)\nIn [174]: m\nOut[174]: <re.Match object; span=(5, 20), match='dave@google.com'>\nIn [175]: text[m.start():m.end()]\nOut[175]: 'dave@google.com'\nregex.match returns None, as it will match only if the pattern occurs at the start of the\nstring:\nIn [176]: print(regex.match(text))\nNone\nRelatedly, sub will return a new string with occurrences of the pattern replaced by a\nnew string:\nIn [177]: print(regex.sub(\"REDACTED\", text))\nDave REDACTED\nSteve REDACTED\nRob REDACTED\nRyan REDACTED\nSuppose you wanted to find email addresses and simultaneously segment each\naddress into its three components: username, domain name, and domain suffix. To\ndo this, put parentheses around the parts of the pattern to segment:\nIn [178]: pattern = r\"([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\.([A-Z]{2,4})\"\nIn [179]: regex = re.compile(pattern, flags=re.IGNORECASE)\nA match object produced by this modified regex returns a tuple of the pattern\ncomponents with its groups method:\nIn [180]: m = regex.match(\"wesm@bright.net\")\nIn [181]: m.groups()\nOut[181]: ('wesm', 'bright', 'net')\nfindall returns a list of tuples when the pattern has groups:\nIn [182]: regex.findall(text)\nOut[182]: \n[('dave', 'google', 'com'),\n ('steve', 'gmail', 'com'),\n7.4 String Manipulation \n| \n231",
      "content_length": 1518,
      "extraction_method": "Direct"
    },
    {
      "page_number": 250,
      "chapter": null,
      "content": "('rob', 'gmail', 'com'),\n ('ryan', 'yahoo', 'com')]\nsub also has access to groups in each match using special symbols like \\1 and \\2. The\nsymbol \\1 corresponds to the first matched group, \\2 corresponds to the second, and\nso forth:\nIn [183]: print(regex.sub(r\"Username: \\1, Domain: \\2, Suffix: \\3\", text))\nDave Username: dave, Domain: google, Suffix: com\nSteve Username: steve, Domain: gmail, Suffix: com\nRob Username: rob, Domain: gmail, Suffix: com\nRyan Username: ryan, Domain: yahoo, Suffix: com\nThere is much more to regular expressions in Python, most of which is outside the\nbook’s scope. Table 7-5 provides a brief summary.\nTable 7-5. Regular expression methods\nMethod\nDescription\nfindall\nReturn all nonoverlapping matching patterns in a string as a list\nfinditer\nLike findall, but returns an iterator\nmatch\nMatch pattern at start of string and optionally segment pattern components into groups; if the pattern\nmatches, return a match object, and otherwise None\nsearch\nScan string for match to pattern, returning a match object if so; unlike match, the match can be anywhere in\nthe string as opposed to only at the beginning\nsplit\nBreak string into pieces at each occurrence of pattern\nsub, subn\nReplace all (sub) or first n occurrences (subn) of pattern in string with replacement expression; use symbols\n\\1, \\2, ... to refer to match group elements in the replacement string\nString Functions in pandas\nCleaning up a messy dataset for analysis often requires a lot of string manipulation.\nTo complicate matters, a column containing strings will sometimes have missing data:\nIn [184]: data = {\"Dave\": \"dave@google.com\", \"Steve\": \"steve@gmail.com\",\n   .....:         \"Rob\": \"rob@gmail.com\", \"Wes\": np.nan}\nIn [185]: data = pd.Series(data)\nIn [186]: data\nOut[186]: \nDave     dave@google.com\nSteve    steve@gmail.com\nRob        rob@gmail.com\nWes                  NaN\ndtype: object\nIn [187]: data.isna()\nOut[187]: \nDave     False\n232 \n| \nChapter 7: Data Cleaning and Preparation",
      "content_length": 1981,
      "extraction_method": "Direct"
    },
    {
      "page_number": 251,
      "chapter": null,
      "content": "Steve    False\nRob      False\nWes       True\ndtype: bool\nString and regular expression methods can be applied (passing a lambda or other\nfunction) to each value using data.map, but it will fail on the NA (null) values.\nTo cope with this, Series has array-oriented methods for string operations that skip\nover and propagate NA values. These are accessed through Series’s str attribute;\nfor example, we could check whether each email address has \"gmail\" in it with\nstr.contains:\nIn [188]: data.str.contains(\"gmail\")\nOut[188]: \nDave     False\nSteve     True\nRob       True\nWes        NaN\ndtype: object\nNote that the result of this operation has an object dtype. pandas has extension types\nthat provide for specialized treatment of strings, integers, and Boolean data which\nuntil recently have had some rough edges when working with missing data:\nIn [189]: data_as_string_ext = data.astype('string')\nIn [190]: data_as_string_ext\nOut[190]: \nDave     dave@google.com\nSteve    steve@gmail.com\nRob        rob@gmail.com\nWes                 <NA>\ndtype: string\nIn [191]: data_as_string_ext.str.contains(\"gmail\")\nOut[191]: \nDave     False\nSteve     True\nRob       True\nWes       <NA>\ndtype: boolean\nExtension types are discussed in more detail in Section 7.3, “Extension Data Types,”\non page 224.\nRegular expressions can be used, too, along with any re options like IGNORECASE:\nIn [192]: pattern = r\"([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\.([A-Z]{2,4})\"\nIn [193]: data.str.findall(pattern, flags=re.IGNORECASE)\nOut[193]: \nDave     [(dave, google, com)]\n7.4 String Manipulation \n| \n233",
      "content_length": 1564,
      "extraction_method": "Direct"
    },
    {
      "page_number": 252,
      "chapter": null,
      "content": "Steve    [(steve, gmail, com)]\nRob        [(rob, gmail, com)]\nWes                        NaN\ndtype: object\nThere are a couple of ways to do vectorized element retrieval. Either use str.get or\nindex into the str attribute:\nIn [194]: matches = data.str.findall(pattern, flags=re.IGNORECASE).str[0]\nIn [195]: matches\nOut[195]: \nDave     (dave, google, com)\nSteve    (steve, gmail, com)\nRob        (rob, gmail, com)\nWes                      NaN\ndtype: object\nIn [196]: matches.str.get(1)\nOut[196]: \nDave     google\nSteve     gmail\nRob       gmail\nWes         NaN\ndtype: object\nYou can similarly slice strings using this syntax:\nIn [197]: data.str[:5]\nOut[197]: \nDave     dave@\nSteve    steve\nRob      rob@g\nWes        NaN\ndtype: object\nThe str.extract method will return the captured groups of a regular expression as a\nDataFrame:\nIn [198]: data.str.extract(pattern, flags=re.IGNORECASE)\nOut[198]: \n           0       1    2\nDave    dave  google  com\nSteve  steve   gmail  com\nRob      rob   gmail  com\nWes      NaN     NaN  NaN\nSee Table 7-6 for more pandas string methods.\n234 \n| \nChapter 7: Data Cleaning and Preparation",
      "content_length": 1119,
      "extraction_method": "Direct"
    },
    {
      "page_number": 253,
      "chapter": null,
      "content": "Table 7-6. Partial listing of Series string methods\nMethod\nDescription\ncat\nConcatenate strings element-wise with optional delimiter\ncontains\nReturn Boolean array if each string contains pattern/regex\ncount\nCount occurrences of pattern\nextract\nUse a regular expression with groups to extract one or more strings from a Series of strings; the result\nwill be a DataFrame with one column per group\nendswith\nEquivalent to x.endswith(pattern) for each element\nstartswith\nEquivalent to x.startswith(pattern) for each element\nfindall\nCompute list of all occurrences of pattern/regex for each string\nget\nIndex into each element (retrieve i-th element)\nisalnum\nEquivalent to built-in str.alnum\nisalpha\nEquivalent to built-in str.isalpha\nisdecimal\nEquivalent to built-in str.isdecimal\nisdigit\nEquivalent to built-in str.isdigit\nislower\nEquivalent to built-in str.islower\nisnumeric\nEquivalent to built-in str.isnumeric\nisupper\nEquivalent to built-in str.isupper\njoin\nJoin strings in each element of the Series with passed separator\nlen\nCompute length of each string\nlower, upper\nConvert cases; equivalent to x.lower() or x.upper() for each element\nmatch\nUse re.match with the passed regular expression on each element, returning True or False\nwhether it matches\npad\nAdd whitespace to left, right, or both sides of strings\ncenter\nEquivalent to pad(side=\"both\")\nrepeat\nDuplicate values (e.g., s.str.repeat(3) is equivalent to x * 3 for each string)\nreplace\nReplace occurrences of pattern/regex with some other string\nslice\nSlice each string in the Series\nsplit\nSplit strings on delimiter or regular expression\nstrip\nTrim whitespace from both sides, including newlines\nrstrip\nTrim whitespace on right side\nlstrip\nTrim whitespace on left side\n7.5 Categorical Data\nThis section introduces the pandas Categorical type. I will show how you can\nachieve better performance and memory use in some pandas operations by using it.\nI also introduce some tools that may help with using categorical data in statistics and\nmachine learning applications.\n7.5 Categorical Data \n| \n235",
      "content_length": 2053,
      "extraction_method": "Direct"
    },
    {
      "page_number": 254,
      "chapter": null,
      "content": "Background and Motivation\nFrequently, a column in a table may contain repeated instances of a smaller set of\ndistinct values. We have already seen functions like unique and value_counts, which\nenable us to extract the distinct values from an array and compute their frequencies,\nrespectively:\nIn [199]: values = pd.Series(['apple', 'orange', 'apple',\n   .....:                     'apple'] * 2)\nIn [200]: values\nOut[200]: \n0     apple\n1    orange\n2     apple\n3     apple\n4     apple\n5    orange\n6     apple\n7     apple\ndtype: object\nIn [201]: pd.unique(values)\nOut[201]: array(['apple', 'orange'], dtype=object)\nIn [202]: pd.value_counts(values)\nOut[202]: \napple     6\norange    2\ndtype: int64\nMany data systems (for data warehousing, statistical computing, or other uses) have\ndeveloped specialized approaches for representing data with repeated values for more\nefficient storage and computation. In data warehousing, a best practice is to use\nso-called dimension tables containing the distinct values and storing the primary\nobservations as integer keys referencing the dimension table:\nIn [203]: values = pd.Series([0, 1, 0, 0] * 2)\nIn [204]: dim = pd.Series(['apple', 'orange'])\nIn [205]: values\nOut[205]: \n0    0\n1    1\n2    0\n3    0\n4    0\n5    1\n6    0\n7    0\n236 \n| \nChapter 7: Data Cleaning and Preparation",
      "content_length": 1315,
      "extraction_method": "Direct"
    },
    {
      "page_number": 255,
      "chapter": null,
      "content": "dtype: int64\nIn [206]: dim\nOut[206]: \n0     apple\n1    orange\ndtype: object\nWe can use the take method to restore the original Series of strings:\nIn [207]: dim.take(values)\nOut[207]: \n0     apple\n1    orange\n0     apple\n0     apple\n0     apple\n1    orange\n0     apple\n0     apple\ndtype: object\nThis representation as integers is called the categorical or dictionary-encoded repre‐\nsentation. The array of distinct values can be called the categories, dictionary, or levels\nof the data. In this book we will use the terms categorical and categories. The integer\nvalues that reference the categories are called the category codes or simply codes.\nThe categorical representation can yield significant performance improvements when\nyou are doing analytics. You can also perform transformations on the categories while\nleaving the codes unmodified. Some example transformations that can be made at\nrelatively low cost are:\n• Renaming categories\n•\n• Appending a new category without changing the order or position of the existing\n•\ncategories\nCategorical Extension Type in pandas\npandas has a special Categorical extension type for holding data that uses the\ninteger-based categorical representation or encoding. This is a popular data compres‐\nsion technique for data with many occurrences of similar values and can provide\nsignificantly faster performance with lower memory use, especially for string data.\nLet’s consider the example Series from before:\nIn [208]: fruits = ['apple', 'orange', 'apple', 'apple'] * 2\nIn [209]: N = len(fruits)\nIn [210]: rng = np.random.default_rng(seed=12345)\n7.5 Categorical Data \n| \n237",
      "content_length": 1615,
      "extraction_method": "Direct"
    },
    {
      "page_number": 256,
      "chapter": null,
      "content": "In [211]: df = pd.DataFrame({'fruit': fruits,\n   .....:                    'basket_id': np.arange(N),\n   .....:                    'count': rng.integers(3, 15, size=N),\n   .....:                    'weight': rng.uniform(0, 4, size=N)},\n   .....:                   columns=['basket_id', 'fruit', 'count', 'weight'])\nIn [212]: df\nOut[212]: \n   basket_id   fruit  count    weight\n0          0   apple     11  1.564438\n1          1  orange      5  1.331256\n2          2   apple     12  2.393235\n3          3   apple      6  0.746937\n4          4   apple      5  2.691024\n5          5  orange     12  3.767211\n6          6   apple     10  0.992983\n7          7   apple     11  3.795525\nHere, df['fruit'] is an array of Python string objects. We can convert it to categori‐\ncal by calling:\nIn [213]: fruit_cat = df['fruit'].astype('category')\nIn [214]: fruit_cat\nOut[214]: \n0     apple\n1    orange\n2     apple\n3     apple\n4     apple\n5    orange\n6     apple\n7     apple\nName: fruit, dtype: category\nCategories (2, object): ['apple', 'orange']\nThe values for fruit_cat are now an instance of pandas.Categorical, which you\ncan access via the .array attribute:\nIn [215]: c = fruit_cat.array\nIn [216]: type(c)\nOut[216]: pandas.core.arrays.categorical.Categorical\nThe Categorical object has categories and codes attributes:\nIn [217]: c.categories\nOut[217]: Index(['apple', 'orange'], dtype='object')\nIn [218]: c.codes\nOut[218]: array([0, 1, 0, 0, 0, 1, 0, 0], dtype=int8)\n238 \n| \nChapter 7: Data Cleaning and Preparation",
      "content_length": 1509,
      "extraction_method": "Direct"
    },
    {
      "page_number": 257,
      "chapter": null,
      "content": "These can be accessed more easily using the cat accessor, which will be explained\nsoon in “Categorical Methods” on page 242.\nA useful trick to get a mapping between codes and categories is:\nIn [219]: dict(enumerate(c.categories))\nOut[219]: {0: 'apple', 1: 'orange'}\nYou can convert a DataFrame column to categorical by assigning the converted result:\nIn [220]: df['fruit'] = df['fruit'].astype('category')\nIn [221]: df[\"fruit\"]\nOut[221]: \n0     apple\n1    orange\n2     apple\n3     apple\n4     apple\n5    orange\n6     apple\n7     apple\nName: fruit, dtype: category\nCategories (2, object): ['apple', 'orange']\nYou can also create pandas.Categorical directly from other types of Python\nsequences:\nIn [222]: my_categories = pd.Categorical(['foo', 'bar', 'baz', 'foo', 'bar'])\nIn [223]: my_categories\nOut[223]: \n['foo', 'bar', 'baz', 'foo', 'bar']\nCategories (3, object): ['bar', 'baz', 'foo']\nIf you have obtained categorical encoded data from another source, you can use the\nalternative from_codes constructor:\nIn [224]: categories = ['foo', 'bar', 'baz']\nIn [225]: codes = [0, 1, 2, 0, 0, 1]\nIn [226]: my_cats_2 = pd.Categorical.from_codes(codes, categories)\nIn [227]: my_cats_2\nOut[227]: \n['foo', 'bar', 'baz', 'foo', 'foo', 'bar']\nCategories (3, object): ['foo', 'bar', 'baz']\nUnless explicitly specified, categorical conversions assume no specific ordering of the\ncategories. So the categories array may be in a different order depending on the\nordering of the input data. When using from_codes or any of the other constructors,\nyou can indicate that the categories have a meaningful ordering:\n7.5 Categorical Data \n| \n239",
      "content_length": 1623,
      "extraction_method": "Direct"
    },
    {
      "page_number": 258,
      "chapter": null,
      "content": "In [228]: ordered_cat = pd.Categorical.from_codes(codes, categories,\n   .....:                                         ordered=True)\nIn [229]: ordered_cat\nOut[229]: \n['foo', 'bar', 'baz', 'foo', 'foo', 'bar']\nCategories (3, object): ['foo' < 'bar' < 'baz']\nThe output [foo < bar < baz] indicates that 'foo' precedes 'bar' in the ordering,\nand so on. An unordered categorical instance can be made ordered with as_ordered:\nIn [230]: my_cats_2.as_ordered()\nOut[230]: \n['foo', 'bar', 'baz', 'foo', 'foo', 'bar']\nCategories (3, object): ['foo' < 'bar' < 'baz']\nAs a last note, categorical data need not be strings, even though I have shown only\nstring examples. A categorical array can consist of any immutable value types.\nComputations with Categoricals\nUsing Categorical in pandas compared with the nonencoded version (like an array\nof strings) generally behaves the same way. Some parts of pandas, like the groupby\nfunction, perform better when working with categoricals. There are also some func‐\ntions that can utilize the ordered flag.\nLet’s consider some random numeric data and use the pandas.qcut binning func‐\ntion. This returns pandas.Categorical; we used pandas.cut earlier in the book but\nglossed over the details of how categoricals work:\nIn [231]: rng = np.random.default_rng(seed=12345)\nIn [232]: draws = rng.standard_normal(1000)\nIn [233]: draws[:5]\nOut[233]: array([-1.4238,  1.2637, -0.8707, -0.2592, -0.0753])\nLet’s compute a quartile binning of this data and extract some statistics:\nIn [234]: bins = pd.qcut(draws, 4)\nIn [235]: bins\nOut[235]: \n[(-3.121, -0.675], (0.687, 3.211], (-3.121, -0.675], (-0.675, 0.0134], (-0.675, 0\n.0134], ..., (0.0134, 0.687], (0.0134, 0.687], (-0.675, 0.0134], (0.0134, 0.687],\n (-0.675, 0.0134]]\nLength: 1000\nCategories (4, interval[float64, right]): [(-3.121, -0.675] < (-0.675, 0.0134] < \n(0.0134, 0.687] <\n                                           (0.687, 3.211]]\n240 \n| \nChapter 7: Data Cleaning and Preparation",
      "content_length": 1964,
      "extraction_method": "Direct"
    },
    {
      "page_number": 259,
      "chapter": null,
      "content": "While useful, the exact sample quartiles may be less useful for producing a report\nthan quartile names. We can achieve this with the labels argument to qcut:\nIn [236]: bins = pd.qcut(draws, 4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\nIn [237]: bins\nOut[237]: \n['Q1', 'Q4', 'Q1', 'Q2', 'Q2', ..., 'Q3', 'Q3', 'Q2', 'Q3', 'Q2']\nLength: 1000\nCategories (4, object): ['Q1' < 'Q2' < 'Q3' < 'Q4']\nIn [238]: bins.codes[:10]\nOut[238]: array([0, 3, 0, 1, 1, 0, 0, 2, 2, 0], dtype=int8)\nThe labeled bins categorical does not contain information about the bin edges in the\ndata, so we can use groupby to extract some summary statistics:\nIn [239]: bins = pd.Series(bins, name='quartile')\nIn [240]: results = (pd.Series(draws)\n   .....:            .groupby(bins)\n   .....:            .agg(['count', 'min', 'max'])\n   .....:            .reset_index())\nIn [241]: results\nOut[241]: \n  quartile  count       min       max\n0       Q1    250 -3.119609 -0.678494\n1       Q2    250 -0.673305  0.008009\n2       Q3    250  0.018753  0.686183\n3       Q4    250  0.688282  3.211418\nThe 'quartile' column in the result retains the original categorical information,\nincluding ordering, from bins:\nIn [242]: results['quartile']\nOut[242]: \n0    Q1\n1    Q2\n2    Q3\n3    Q4\nName: quartile, dtype: category\nCategories (4, object): ['Q1' < 'Q2' < 'Q3' < 'Q4']\nBetter performance with categoricals\nAt the beginning of the section, I said that categorical types can improve performance\nand memory use, so let’s look at some examples. Consider some Series with 10\nmillion elements and a small number of distinct categories:\n7.5 Categorical Data \n| \n241",
      "content_length": 1608,
      "extraction_method": "Direct"
    },
    {
      "page_number": 260,
      "chapter": null,
      "content": "In [243]: N = 10_000_000\nIn [244]: labels = pd.Series(['foo', 'bar', 'baz', 'qux'] * (N // 4))\nNow we convert labels to categorical:\nIn [245]: categories = labels.astype('category')\nNow we note that labels uses significantly more memory than categories:\nIn [246]: labels.memory_usage(deep=True)\nOut[246]: 600000128\nIn [247]: categories.memory_usage(deep=True)\nOut[247]: 10000540\nThe conversion to category is not free, of course, but it is a one-time cost:\nIn [248]: %time _ = labels.astype('category')\nCPU times: user 469 ms, sys: 106 ms, total: 574 ms\nWall time: 577 ms\nGroupBy operations can be significantly faster with categoricals because the underly‐\ning algorithms use the integer-based codes array instead of an array of strings. Here\nwe compare the performance of value_counts(), which internally uses the GroupBy\nmachinery:\nIn [249]: %timeit labels.value_counts()\n840 ms +- 10.9 ms per loop (mean +- std. dev. of 7 runs, 1 loop each)\nIn [250]: %timeit categories.value_counts()\n30.1 ms +- 549 us per loop (mean +- std. dev. of 7 runs, 10 loops each)\nCategorical Methods\nSeries containing categorical data have several special methods similar to the Ser\nies.str specialized string methods. This also provides convenient access to the\ncategories and codes. Consider the Series:\nIn [251]: s = pd.Series(['a', 'b', 'c', 'd'] * 2)\nIn [252]: cat_s = s.astype('category')\nIn [253]: cat_s\nOut[253]: \n0    a\n1    b\n2    c\n3    d\n4    a\n5    b\n6    c\n7    d\n242 \n| \nChapter 7: Data Cleaning and Preparation",
      "content_length": 1507,
      "extraction_method": "Direct"
    },
    {
      "page_number": 261,
      "chapter": null,
      "content": "dtype: category\nCategories (4, object): ['a', 'b', 'c', 'd']\nThe special accessor attribute cat provides access to categorical methods:\nIn [254]: cat_s.cat.codes\nOut[254]: \n0    0\n1    1\n2    2\n3    3\n4    0\n5    1\n6    2\n7    3\ndtype: int8\nIn [255]: cat_s.cat.categories\nOut[255]: Index(['a', 'b', 'c', 'd'], dtype='object')\nSuppose that we know the actual set of categories for this data extends beyond the\nfour values observed in the data. We can use the set_categories method to change\nthem:\nIn [256]: actual_categories = ['a', 'b', 'c', 'd', 'e']\nIn [257]: cat_s2 = cat_s.cat.set_categories(actual_categories)\nIn [258]: cat_s2\nOut[258]: \n0    a\n1    b\n2    c\n3    d\n4    a\n5    b\n6    c\n7    d\ndtype: category\nCategories (5, object): ['a', 'b', 'c', 'd', 'e']\nWhile it appears that the data is unchanged, the new categories will be reflected\nin operations that use them. For example, value_counts respects the categories, if\npresent:\nIn [259]: cat_s.value_counts()\nOut[259]: \na    2\nb    2\nc    2\nd    2\ndtype: int64\n7.5 Categorical Data \n| \n243",
      "content_length": 1050,
      "extraction_method": "Direct"
    },
    {
      "page_number": 262,
      "chapter": null,
      "content": "In [260]: cat_s2.value_counts()\nOut[260]: \na    2\nb    2\nc    2\nd    2\ne    0\ndtype: int64\nIn large datasets, categoricals are often used as a convenient tool for memory sav‐\nings and better performance. After you filter a large DataFrame or Series, many\nof the categories may not appear in the data. To help with this, we can use the\nremove_unused_categories method to trim unobserved categories:\nIn [261]: cat_s3 = cat_s[cat_s.isin(['a', 'b'])]\nIn [262]: cat_s3\nOut[262]: \n0    a\n1    b\n4    a\n5    b\ndtype: category\nCategories (4, object): ['a', 'b', 'c', 'd']\nIn [263]: cat_s3.cat.remove_unused_categories()\nOut[263]: \n0    a\n1    b\n4    a\n5    b\ndtype: category\nCategories (2, object): ['a', 'b']\nSee Table 7-7 for a listing of available categorical methods.\nTable 7-7. Categorical methods for Series in pandas\nMethod\nDescription\nadd_categories\nAppend new (unused) categories at end of existing categories\nas_ordered\nMake categories ordered\nas_unordered\nMake categories unordered\nremove_categories\nRemove categories, setting any removed values to null\nremove_unused_categories\nRemove any category values that do not appear in the data\nrename_categories\nReplace categories with indicated set of new category names; cannot change the\nnumber of categories\nreorder_categories\nBehaves like rename_categories, but can also change the result to have ordered\ncategories\nset_categories\nReplace the categories with the indicated set of new categories; can add or remove\ncategories\n244 \n| \nChapter 7: Data Cleaning and Preparation",
      "content_length": 1524,
      "extraction_method": "Direct"
    },
    {
      "page_number": 263,
      "chapter": null,
      "content": "Creating dummy variables for modeling\nWhen you’re using statistics or machine learning tools, you’ll often transform catego‐\nrical data into dummy variables, also known as one-hot encoding. This involves creat‐\ning a DataFrame with a column for each distinct category; these columns contain 1s\nfor occurrences of a given category and 0 otherwise.\nConsider the previous example:\nIn [264]: cat_s = pd.Series(['a', 'b', 'c', 'd'] * 2, dtype='category')\nAs mentioned previously in this chapter, the pandas.get_dummies function converts\nthis one-dimensional categorical data into a DataFrame containing the dummy\nvariable:\nIn [265]: pd.get_dummies(cat_s)\nOut[265]: \n   a  b  c  d\n0  1  0  0  0\n1  0  1  0  0\n2  0  0  1  0\n3  0  0  0  1\n4  1  0  0  0\n5  0  1  0  0\n6  0  0  1  0\n7  0  0  0  1\n7.6 Conclusion\nEffective data preparation can significantly improve productivity by enabling you to\nspend more time analyzing data and less time getting it ready for analysis. We have\nexplored a number of tools in this chapter, but the coverage here is by no means\ncomprehensive. In the next chapter, we will explore pandas’s joining and grouping\nfunctionality.\n7.6 Conclusion \n| \n245",
      "content_length": 1171,
      "extraction_method": "Direct"
    },
    {
      "page_number": 264,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 265,
      "chapter": null,
      "content": "CHAPTER 8\nData Wrangling: Join,\nCombine, and Reshape\nIn many applications, data may be spread across a number of files or databases, or be\narranged in a form that is not convenient to analyze. This chapter focuses on tools to\nhelp combine, join, and rearrange data.\nFirst, I introduce the concept of hierarchical indexing in pandas, which is used exten‐\nsively in some of these operations. I then dig into the particular data manipulations.\nYou can see various applied usages of these tools in Chapter 13.\n8.1 Hierarchical Indexing\nHierarchical indexing is an important feature of pandas that enables you to have\nmultiple (two or more) index levels on an axis. Another way of thinking about it\nis that it provides a way for you to work with higher dimensional data in a lower\ndimensional form. Let’s start with a simple example: create a Series with a list of lists\n(or arrays) as the index:\nIn [11]: data = pd.Series(np.random.uniform(size=9),\n   ....:                  index=[[\"a\", \"a\", \"a\", \"b\", \"b\", \"c\", \"c\", \"d\", \"d\"],\n   ....:                         [1, 2, 3, 1, 3, 1, 2, 2, 3]])\nIn [12]: data\nOut[12]: \na  1    0.929616\n   2    0.316376\n   3    0.183919\nb  1    0.204560\n   3    0.567725\nc  1    0.595545\n   2    0.964515\n247",
      "content_length": 1234,
      "extraction_method": "Direct"
    },
    {
      "page_number": 266,
      "chapter": null,
      "content": "d  2    0.653177\n   3    0.748907\ndtype: float64\nWhat you’re seeing is a prettified view of a Series with a MultiIndex as its index. The\n“gaps” in the index display mean “use the label directly above”:\nIn [13]: data.index\nOut[13]: \nMultiIndex([('a', 1),\n            ('a', 2),\n            ('a', 3),\n            ('b', 1),\n            ('b', 3),\n            ('c', 1),\n            ('c', 2),\n            ('d', 2),\n            ('d', 3)],\n           )\nWith a hierarchically indexed object, so-called partial indexing is possible, enabling\nyou to concisely select subsets of the data:\nIn [14]: data[\"b\"]\nOut[14]: \n1    0.204560\n3    0.567725\ndtype: float64\nIn [15]: data[\"b\":\"c\"]\nOut[15]: \nb  1    0.204560\n   3    0.567725\nc  1    0.595545\n   2    0.964515\ndtype: float64\nIn [16]: data.loc[[\"b\", \"d\"]]\nOut[16]: \nb  1    0.204560\n   3    0.567725\nd  2    0.653177\n   3    0.748907\ndtype: float64\nSelection is even possible from an “inner” level. Here I select all of the values having\nthe value 2 from the second index level:\nIn [17]: data.loc[:, 2]\nOut[17]: \na    0.316376\nc    0.964515\n248 \n| \nChapter 8: Data Wrangling: Join, Combine, and Reshape",
      "content_length": 1140,
      "extraction_method": "Direct"
    },
    {
      "page_number": 267,
      "chapter": null,
      "content": "d    0.653177\ndtype: float64\nHierarchical indexing plays an important role in reshaping data and in group-based\noperations like forming a pivot table. For example, you can rearrange this data into a\nDataFrame using its unstack method:\nIn [18]: data.unstack()\nOut[18]: \n          1         2         3\na  0.929616  0.316376  0.183919\nb  0.204560       NaN  0.567725\nc  0.595545  0.964515       NaN\nd       NaN  0.653177  0.748907\nThe inverse operation of unstack is stack:\nIn [19]: data.unstack().stack()\nOut[19]: \na  1    0.929616\n   2    0.316376\n   3    0.183919\nb  1    0.204560\n   3    0.567725\nc  1    0.595545\n   2    0.964515\nd  2    0.653177\n   3    0.748907\ndtype: float64\nstack and unstack will be explored in more detail later in Section 8.3, “Reshaping\nand Pivoting,” on page 270.\nWith a DataFrame, either axis can have a hierarchical index:\nIn [20]: frame = pd.DataFrame(np.arange(12).reshape((4, 3)),\n   ....:                      index=[[\"a\", \"a\", \"b\", \"b\"], [1, 2, 1, 2]],\n   ....:                      columns=[[\"Ohio\", \"Ohio\", \"Colorado\"],\n   ....:                               [\"Green\", \"Red\", \"Green\"]])\nIn [21]: frame\nOut[21]: \n     Ohio     Colorado\n    Green Red    Green\na 1     0   1        2\n  2     3   4        5\nb 1     6   7        8\n  2     9  10       11\nThe hierarchical levels can have names (as strings or any Python objects). If so, these\nwill show up in the console output:\nIn [22]: frame.index.names = [\"key1\", \"key2\"]\n8.1 Hierarchical Indexing \n| \n249",
      "content_length": 1491,
      "extraction_method": "Direct"
    },
    {
      "page_number": 268,
      "chapter": null,
      "content": "In [23]: frame.columns.names = [\"state\", \"color\"]\nIn [24]: frame\nOut[24]: \nstate      Ohio     Colorado\ncolor     Green Red    Green\nkey1 key2                   \na    1        0   1        2\n     2        3   4        5\nb    1        6   7        8\n     2        9  10       11\nThese names supersede the name attribute, which is used only with single-level\nindexes.\nBe careful to note that the index names \"state\" and \"color\" are\nnot part of the row labels (the frame.index values).\nYou can see how many levels an index has by accessing its nlevels attribute:\nIn [25]: frame.index.nlevels\nOut[25]: 2\nWith partial column indexing you can similarly select groups of columns:\nIn [26]: frame[\"Ohio\"]\nOut[26]: \ncolor      Green  Red\nkey1 key2            \na    1         0    1\n     2         3    4\nb    1         6    7\n     2         9   10\nA MultiIndex can be created by itself and then reused; the columns in the preceding\nDataFrame with level names could also be created like this:\npd.MultiIndex.from_arrays([[\"Ohio\", \"Ohio\", \"Colorado\"],\n                          [\"Green\", \"Red\", \"Green\"]],\n                          names=[\"state\", \"color\"])\nReordering and Sorting Levels\nAt times you may need to rearrange the order of the levels on an axis or sort the data\nby the values in one specific level. The swaplevel method takes two level numbers\nor names and returns a new object with the levels interchanged (but the data is\notherwise unaltered):\n250 \n| \nChapter 8: Data Wrangling: Join, Combine, and Reshape",
      "content_length": 1507,
      "extraction_method": "Direct"
    },
    {
      "page_number": 269,
      "chapter": null,
      "content": "In [27]: frame.swaplevel(\"key1\", \"key2\")\nOut[27]: \nstate      Ohio     Colorado\ncolor     Green Red    Green\nkey2 key1                   \n1    a        0   1        2\n2    a        3   4        5\n1    b        6   7        8\n2    b        9  10       11\nsort_index by default sorts the data lexicographically using all the index levels, but\nyou can choose to use only a single level or a subset of levels to sort by passing the\nlevel argument. For example:\nIn [28]: frame.sort_index(level=1)\nOut[28]: \nstate      Ohio     Colorado\ncolor     Green Red    Green\nkey1 key2                   \na    1        0   1        2\nb    1        6   7        8\na    2        3   4        5\nb    2        9  10       11\nIn [29]: frame.swaplevel(0, 1).sort_index(level=0)\nOut[29]: \nstate      Ohio     Colorado\ncolor     Green Red    Green\nkey2 key1                   \n1    a        0   1        2\n     b        6   7        8\n2    a        3   4        5\n     b        9  10       11\nData selection performance is much better on hierarchically\nindexed objects if the index is lexicographically sorted start‐\ning with the outermost level—that is, the result of calling\nsort_index(level=0) or sort_index().\nSummary Statistics by Level\nMany descriptive and summary statistics on DataFrame and Series have a level\noption in which you can specify the level you want to aggregate by on a particular\naxis. Consider the above DataFrame; we can aggregate by level on either the rows or\ncolumns, like so:\nIn [30]: frame.groupby(level=\"key2\").sum()\nOut[30]: \nstate  Ohio     Colorado\ncolor Green Red    Green\n8.1 Hierarchical Indexing \n| \n251",
      "content_length": 1616,
      "extraction_method": "Direct"
    },
    {
      "page_number": 270,
      "chapter": null,
      "content": "key2                    \n1         6   8       10\n2        12  14       16\nIn [31]: frame.groupby(level=\"color\", axis=\"columns\").sum()\nOut[31]: \ncolor      Green  Red\nkey1 key2            \na    1         2    1\n     2         8    4\nb    1        14    7\n     2        20   10\nWe will discuss groupby in much more detail later in Chapter 10.\nIndexing with a DataFrame’s columns\nIt’s not unusual to want to use one or more columns from a DataFrame as the\nrow index; alternatively, you may wish to move the row index into the DataFrame’s\ncolumns. Here’s an example DataFrame:\nIn [32]: frame = pd.DataFrame({\"a\": range(7), \"b\": range(7, 0, -1),\n   ....:                       \"c\": [\"one\", \"one\", \"one\", \"two\", \"two\",\n   ....:                             \"two\", \"two\"],\n   ....:                       \"d\": [0, 1, 2, 0, 1, 2, 3]})\nIn [33]: frame\nOut[33]: \n   a  b    c  d\n0  0  7  one  0\n1  1  6  one  1\n2  2  5  one  2\n3  3  4  two  0\n4  4  3  two  1\n5  5  2  two  2\n6  6  1  two  3\nDataFrame’s set_index function will create a new DataFrame using one or more of\nits columns as the index:\nIn [34]: frame2 = frame.set_index([\"c\", \"d\"])\nIn [35]: frame2\nOut[35]: \n       a  b\nc   d      \none 0  0  7\n    1  1  6\n    2  2  5\ntwo 0  3  4\n    1  4  3\n252 \n| \nChapter 8: Data Wrangling: Join, Combine, and Reshape",
      "content_length": 1302,
      "extraction_method": "Direct"
    },
    {
      "page_number": 271,
      "chapter": null,
      "content": "2  5  2\n    3  6  1\nBy default, the columns are removed from the DataFrame, though you can leave\nthem in by passing drop=False to set_index:\nIn [36]: frame.set_index([\"c\", \"d\"], drop=False)\nOut[36]: \n       a  b    c  d\nc   d              \none 0  0  7  one  0\n    1  1  6  one  1\n    2  2  5  one  2\ntwo 0  3  4  two  0\n    1  4  3  two  1\n    2  5  2  two  2\n    3  6  1  two  3\nreset_index, on the other hand, does the opposite of set_index; the hierarchical\nindex levels are moved into the columns:\nIn [37]: frame2.reset_index()\nOut[37]: \n     c  d  a  b\n0  one  0  0  7\n1  one  1  1  6\n2  one  2  2  5\n3  two  0  3  4\n4  two  1  4  3\n5  two  2  5  2\n6  two  3  6  1\n8.2 Combining and Merging Datasets\nData contained in pandas objects can be combined in a number of ways:\npandas.merge\nConnect rows in DataFrames based on one or more keys. This will be familiar\nto users of SQL or other relational databases, as it implements database join\noperations.\npandas.concat\nConcatenate or “stack” objects together along an axis.\ncombine_first\nSplice together overlapping data to fill in missing values in one object with values\nfrom another.\nI will address each of these and give a number of examples. They’ll be utilized in\nexamples throughout the rest of the book.\n8.2 Combining and Merging Datasets \n| \n253",
      "content_length": 1303,
      "extraction_method": "Direct"
    },
    {
      "page_number": 272,
      "chapter": null,
      "content": "Database-Style DataFrame Joins\nMerge or join operations combine datasets by linking rows using one or more keys.\nThese operations are particularly important in relational databases (e.g., SQL-based).\nThe pandas.merge function in pandas is the main entry point for using these algo‐\nrithms on your data.\nLet’s start with a simple example:\nIn [38]: df1 = pd.DataFrame({\"key\": [\"b\", \"b\", \"a\", \"c\", \"a\", \"a\", \"b\"],\n   ....:                     \"data1\": pd.Series(range(7), dtype=\"Int64\")})\nIn [39]: df2 = pd.DataFrame({\"key\": [\"a\", \"b\", \"d\"],\n   ....:                     \"data2\": pd.Series(range(3), dtype=\"Int64\")})\nIn [40]: df1\nOut[40]: \n  key  data1\n0   b      0\n1   b      1\n2   a      2\n3   c      3\n4   a      4\n5   a      5\n6   b      6\nIn [41]: df2\nOut[41]: \n  key  data2\n0   a      0\n1   b      1\n2   d      2\nHere I am using pandas’s Int64 extension type for nullable integers, discussed in\nSection 7.3, “Extension Data Types,” on page 224.\nThis is an example of a many-to-one join; the data in df1 has multiple rows labeled\na and b, whereas df2 has only one row for each value in the key column. Calling\npandas.merge with these objects, we obtain:\nIn [42]: pd.merge(df1, df2)\nOut[42]: \n  key  data1  data2\n0   b      0      1\n1   b      1      1\n2   b      6      1\n3   a      2      0\n4   a      4      0\n5   a      5      0\n254 \n| \nChapter 8: Data Wrangling: Join, Combine, and Reshape",
      "content_length": 1395,
      "extraction_method": "Direct"
    },
    {
      "page_number": 273,
      "chapter": null,
      "content": "Note that I didn’t specify which column to join on. If that information is not\nspecified, pandas.merge uses the overlapping column names as the keys. It’s a good\npractice to specify explicitly, though:\nIn [43]: pd.merge(df1, df2, on=\"key\")\nOut[43]: \n  key  data1  data2\n0   b      0      1\n1   b      1      1\n2   b      6      1\n3   a      2      0\n4   a      4      0\n5   a      5      0\nIn general, the order of column output in pandas.merge operations is unspecified.\nIf the column names are different in each object, you can specify them separately:\nIn [44]: df3 = pd.DataFrame({\"lkey\": [\"b\", \"b\", \"a\", \"c\", \"a\", \"a\", \"b\"],\n   ....:                     \"data1\": pd.Series(range(7), dtype=\"Int64\")})\nIn [45]: df4 = pd.DataFrame({\"rkey\": [\"a\", \"b\", \"d\"],\n   ....:                     \"data2\": pd.Series(range(3), dtype=\"Int64\")})\nIn [46]: pd.merge(df3, df4, left_on=\"lkey\", right_on=\"rkey\")\nOut[46]: \n  lkey  data1 rkey  data2\n0    b      0    b      1\n1    b      1    b      1\n2    b      6    b      1\n3    a      2    a      0\n4    a      4    a      0\n5    a      5    a      0\nYou may notice that the \"c\" and \"d\" values and associated data are missing from\nthe result. By default, pandas.merge does an \"inner\" join; the keys in the result are\nthe intersection, or the common set found in both tables. Other possible options are\n\"left\", \"right\", and \"outer\". The outer join takes the union of the keys, combining\nthe effect of applying both left and right joins:\nIn [47]: pd.merge(df1, df2, how=\"outer\")\nOut[47]: \n  key  data1  data2\n0   b      0      1\n1   b      1      1\n2   b      6      1\n3   a      2      0\n4   a      4      0\n5   a      5      0\n6   c      3   <NA>\n7   d   <NA>      2\n8.2 Combining and Merging Datasets \n| \n255",
      "content_length": 1744,
      "extraction_method": "Direct"
    },
    {
      "page_number": 274,
      "chapter": null,
      "content": "In [48]: pd.merge(df3, df4, left_on=\"lkey\", right_on=\"rkey\", how=\"outer\")\nOut[48]: \n  lkey  data1 rkey  data2\n0    b      0    b      1\n1    b      1    b      1\n2    b      6    b      1\n3    a      2    a      0\n4    a      4    a      0\n5    a      5    a      0\n6    c      3  NaN   <NA>\n7  NaN   <NA>    d      2\nIn an outer join, rows from the left or right DataFrame objects that do not match\non keys in the other DataFrame will appear with NA values in the other DataFrame’s\ncolumns for the nonmatching rows.\nSee Table 8-1 for a summary of the options for how.\nTable 8-1. Different join types with the how argument\nOption\nBehavior\nhow=\"inner\" Use only the key combinations observed in both tables\nhow=\"left\"\nUse all key combinations found in the left table\nhow=\"right\"\nUse all key combinations found in the right table\nhow=\"outer\" Use all key combinations observed in both tables together\nMany-to-many merges form the Cartesian product of the matching keys. Here’s an\nexample:\nIn [49]: df1 = pd.DataFrame({\"key\": [\"b\", \"b\", \"a\", \"c\", \"a\", \"b\"],\n   ....:                     \"data1\": pd.Series(range(6), dtype=\"Int64\")})\nIn [50]: df2 = pd.DataFrame({\"key\": [\"a\", \"b\", \"a\", \"b\", \"d\"],\n   ....:                     \"data2\": pd.Series(range(5), dtype=\"Int64\")})\nIn [51]: df1\nOut[51]: \n  key  data1\n0   b      0\n1   b      1\n2   a      2\n3   c      3\n4   a      4\n5   b      5\nIn [52]: df2\nOut[52]: \n  key  data2\n0   a      0\n1   b      1\n2   a      2\n256 \n| \nChapter 8: Data Wrangling: Join, Combine, and Reshape",
      "content_length": 1516,
      "extraction_method": "Direct"
    },
    {
      "page_number": 275,
      "chapter": null,
      "content": "3   b      3\n4   d      4\nIn [53]: pd.merge(df1, df2, on=\"key\", how=\"left\")\nOut[53]: \n   key  data1  data2\n0    b      0      1\n1    b      0      3\n2    b      1      1\n3    b      1      3\n4    a      2      0\n5    a      2      2\n6    c      3   <NA>\n7    a      4      0\n8    a      4      2\n9    b      5      1\n10   b      5      3\nSince there were three \"b\" rows in the left DataFrame and two in the right one, there\nare six \"b\" rows in the result. The join method passed to the how keyword argument\naffects only the distinct key values appearing in the result:\nIn [54]: pd.merge(df1, df2, how=\"inner\")\nOut[54]: \n  key  data1  data2\n0   b      0      1\n1   b      0      3\n2   b      1      1\n3   b      1      3\n4   b      5      1\n5   b      5      3\n6   a      2      0\n7   a      2      2\n8   a      4      0\n9   a      4      2\nTo merge with multiple keys, pass a list of column names:\nIn [55]: left = pd.DataFrame({\"key1\": [\"foo\", \"foo\", \"bar\"],\n   ....:                      \"key2\": [\"one\", \"two\", \"one\"],\n   ....:                      \"lval\": pd.Series([1, 2, 3], dtype='Int64')})\nIn [56]: right = pd.DataFrame({\"key1\": [\"foo\", \"foo\", \"bar\", \"bar\"],\n   ....:                       \"key2\": [\"one\", \"one\", \"one\", \"two\"],\n   ....:                       \"rval\": pd.Series([4, 5, 6, 7], dtype='Int64')})\nIn [57]: pd.merge(left, right, on=[\"key1\", \"key2\"], how=\"outer\")\nOut[57]: \n  key1 key2  lval  rval\n0  foo  one     1     4\n1  foo  one     1     5\n2  foo  two     2  <NA>\n8.2 Combining and Merging Datasets \n| \n257",
      "content_length": 1527,
      "extraction_method": "Direct"
    },
    {
      "page_number": 276,
      "chapter": null,
      "content": "3  bar  one     3     6\n4  bar  two  <NA>     7\nTo determine which key combinations will appear in the result depending on the\nchoice of merge method, think of the multiple keys as forming an array of tuples to\nbe used as a single join key.\nWhen you’re joining columns on columns, the indexes on the\npassed DataFrame objects are discarded. If you need to preserve\nthe index values, you can use reset_index to append the index to\nthe columns.\nA last issue to consider in merge operations is the treatment of overlapping column\nnames. For example:\nIn [58]: pd.merge(left, right, on=\"key1\")\nOut[58]: \n  key1 key2_x  lval key2_y  rval\n0  foo    one     1    one     4\n1  foo    one     1    one     5\n2  foo    two     2    one     4\n3  foo    two     2    one     5\n4  bar    one     3    one     6\n5  bar    one     3    two     7\nWhile you can address the overlap manually (see the section “Renaming Axis\nIndexes” on page 214 for renaming axis labels), pandas.merge has a suffixes option\nfor specifying strings to append to overlapping names in the left and right DataFrame\nobjects:\nIn [59]: pd.merge(left, right, on=\"key1\", suffixes=(\"_left\", \"_right\"))\nOut[59]: \n  key1 key2_left  lval key2_right  rval\n0  foo       one     1        one     4\n1  foo       one     1        one     5\n2  foo       two     2        one     4\n3  foo       two     2        one     5\n4  bar       one     3        one     6\n5  bar       one     3        two     7\nSee Table 8-2 for an argument reference on pandas.merge. The next section covers\njoining using the DataFrame’s row index.\nTable 8-2. pandas.merge function arguments\nArgument\nDescription\nleft\nDataFrame to be merged on the left side.\nright\nDataFrame to be merged on the right side.\nhow\nType of join to apply: one of \"inner\", \"outer\", \"left\", or \"right\"; defaults to \"inner\".\n258 \n| \nChapter 8: Data Wrangling: Join, Combine, and Reshape",
      "content_length": 1878,
      "extraction_method": "Direct"
    },
    {
      "page_number": 277,
      "chapter": null,
      "content": "Argument\nDescription\non\nColumn names to join on. Must be found in both DataFrame objects. If not specified and no other join keys\ngiven, will use the intersection of the column names in left and right as the join keys.\nleft_on\nColumns in left DataFrame to use as join keys. Can be a single column name or a list of column names.\nright_on\nAnalogous to left_on for right DataFrame.\nleft_index\nUse row index in left as its join key (or keys, if a MultiIndex).\nright_index\nAnalogous to left_index.\nsort\nSort merged data lexicographically by join keys; False by default.\nsuffixes\nTuple of string values to append to column names in case of overlap; defaults to (\"_x\", \"_y\") (e.g., if\n\"data\" in both DataFrame objects, would appear as \"data_x\" and \"data_y\" in result).\ncopy\nIf False, avoid copying data into resulting data structure in some exceptional cases; by default always\ncopies.\nvalidate\nVerifies if the merge is of the specified type, whether one-to-one, one-to-many, or many-to-many. See the\ndocstring for full details on the options.\nindicator\nAdds a special column _merge that indicates the source of each row; values will be \"left_only\",\n\"right_only\", or \"both\" based on the origin of the joined data in each row.\nMerging on Index\nIn some cases, the merge key(s) in a DataFrame will be found in its index (row\nlabels). In this case, you can pass left_index=True or right_index=True (or both) to\nindicate that the index should be used as the merge key:\nIn [60]: left1 = pd.DataFrame({\"key\": [\"a\", \"b\", \"a\", \"a\", \"b\", \"c\"],\n   ....:                       \"value\": pd.Series(range(6), dtype=\"Int64\")})\nIn [61]: right1 = pd.DataFrame({\"group_val\": [3.5, 7]}, index=[\"a\", \"b\"])\nIn [62]: left1\nOut[62]: \n  key  value\n0   a      0\n1   b      1\n2   a      2\n3   a      3\n4   b      4\n5   c      5\nIn [63]: right1\nOut[63]: \n   group_val\na        3.5\nb        7.0\nIn [64]: pd.merge(left1, right1, left_on=\"key\", right_index=True)\nOut[64]: \n  key  value  group_val\n0   a      0        3.5\n8.2 Combining and Merging Datasets \n| \n259",
      "content_length": 2026,
      "extraction_method": "Direct"
    },
    {
      "page_number": 278,
      "chapter": null,
      "content": "2   a      2        3.5\n3   a      3        3.5\n1   b      1        7.0\n4   b      4        7.0\nIf you look carefully here, you will see that the index values for\nleft1 have been preserved, whereas in other examples above, the\nindexes of the input DataFrame objects are dropped. Because the\nindex of right1 is unique, this “many-to-one” merge (with the\ndefault how=\"inner\" method) can preserve the index values from\nleft1 that correspond to rows in the output.\nSince the default merge method is to intersect the join keys, you can instead form the\nunion of them with an outer join:\nIn [65]: pd.merge(left1, right1, left_on=\"key\", right_index=True, how=\"outer\")\nOut[65]: \n  key  value  group_val\n0   a      0        3.5\n2   a      2        3.5\n3   a      3        3.5\n1   b      1        7.0\n4   b      4        7.0\n5   c      5        NaN\nWith hierarchically indexed data, things are more complicated, as joining on index is\nequivalent to a multiple-key merge:\nIn [66]: lefth = pd.DataFrame({\"key1\": [\"Ohio\", \"Ohio\", \"Ohio\",\n   ....:                                \"Nevada\", \"Nevada\"],\n   ....:                       \"key2\": [2000, 2001, 2002, 2001, 2002],\n   ....:                       \"data\": pd.Series(range(5), dtype=\"Int64\")})\nIn [67]: righth_index = pd.MultiIndex.from_arrays(\n   ....:     [\n   ....:         [\"Nevada\", \"Nevada\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\"],\n   ....:         [2001, 2000, 2000, 2000, 2001, 2002]\n   ....:     ]\n   ....: )\nIn [68]: righth = pd.DataFrame({\"event1\": pd.Series([0, 2, 4, 6, 8, 10], dtype=\"I\nnt64\",\n   ....:                                            index=righth_index),\n   ....:                        \"event2\": pd.Series([1, 3, 5, 7, 9, 11], dtype=\"I\nnt64\",\n   ....:                                            index=righth_index)})\nIn [69]: lefth\nOut[69]: \n     key1  key2  data\n0    Ohio  2000     0\n260 \n| \nChapter 8: Data Wrangling: Join, Combine, and Reshape",
      "content_length": 1905,
      "extraction_method": "Direct"
    },
    {
      "page_number": 279,
      "chapter": null,
      "content": "1    Ohio  2001     1\n2    Ohio  2002     2\n3  Nevada  2001     3\n4  Nevada  2002     4\nIn [70]: righth\nOut[70]: \n             event1  event2\nNevada 2001       0       1\n       2000       2       3\nOhio   2000       4       5\n       2000       6       7\n       2001       8       9\n       2002      10      11\nIn this case, you have to indicate multiple columns to merge on as a list (note the\nhandling of duplicate index values with how=\"outer\"):\nIn [71]: pd.merge(lefth, righth, left_on=[\"key1\", \"key2\"], right_index=True)\nOut[71]: \n     key1  key2  data  event1  event2\n0    Ohio  2000     0       4       5\n0    Ohio  2000     0       6       7\n1    Ohio  2001     1       8       9\n2    Ohio  2002     2      10      11\n3  Nevada  2001     3       0       1\nIn [72]: pd.merge(lefth, righth, left_on=[\"key1\", \"key2\"],\n   ....:          right_index=True, how=\"outer\")\nOut[72]: \n     key1  key2  data  event1  event2\n0    Ohio  2000     0       4       5\n0    Ohio  2000     0       6       7\n1    Ohio  2001     1       8       9\n2    Ohio  2002     2      10      11\n3  Nevada  2001     3       0       1\n4  Nevada  2002     4    <NA>    <NA>\n4  Nevada  2000  <NA>       2       3\nUsing the indexes of both sides of the merge is also possible:\nIn [73]: left2 = pd.DataFrame([[1., 2.], [3., 4.], [5., 6.]],\n   ....:                      index=[\"a\", \"c\", \"e\"],\n   ....:                      columns=[\"Ohio\", \"Nevada\"]).astype(\"Int64\")\nIn [74]: right2 = pd.DataFrame([[7., 8.], [9., 10.], [11., 12.], [13, 14]],\n   ....:                       index=[\"b\", \"c\", \"d\", \"e\"],\n   ....:                       columns=[\"Missouri\", \"Alabama\"]).astype(\"Int64\")\nIn [75]: left2\nOut[75]: \n   Ohio  Nevada\na     1       2\n8.2 Combining and Merging Datasets \n| \n261",
      "content_length": 1751,
      "extraction_method": "Direct"
    },
    {
      "page_number": 280,
      "chapter": null,
      "content": "c     3       4\ne     5       6\nIn [76]: right2\nOut[76]: \n   Missouri  Alabama\nb         7        8\nc         9       10\nd        11       12\ne        13       14\nIn [77]: pd.merge(left2, right2, how=\"outer\", left_index=True, right_index=True)\nOut[77]: \n   Ohio  Nevada  Missouri  Alabama\na     1       2      <NA>     <NA>\nb  <NA>    <NA>         7        8\nc     3       4         9       10\nd  <NA>    <NA>        11       12\ne     5       6        13       14\nDataFrame has a join instance method to simplify merging by index. It can also be\nused to combine many DataFrame objects having the same or similar indexes but\nnonoverlapping columns. In the prior example, we could have written:\nIn [78]: left2.join(right2, how=\"outer\")\nOut[78]: \n   Ohio  Nevada  Missouri  Alabama\na     1       2      <NA>     <NA>\nb  <NA>    <NA>         7        8\nc     3       4         9       10\nd  <NA>    <NA>        11       12\ne     5       6        13       14\nCompared with pandas.merge, DataFrame’s join method performs a left join on the\njoin keys by default. It also supports joining the index of the passed DataFrame on\none of the columns of the calling DataFrame:\nIn [79]: left1.join(right1, on=\"key\")\nOut[79]: \n  key  value  group_val\n0   a      0        3.5\n1   b      1        7.0\n2   a      2        3.5\n3   a      3        3.5\n4   b      4        7.0\n5   c      5        NaN\nYou can think of this method as joining data “into” the object whose join method\nwas called.\n262 \n| \nChapter 8: Data Wrangling: Join, Combine, and Reshape",
      "content_length": 1533,
      "extraction_method": "Direct"
    },
    {
      "page_number": 281,
      "chapter": null,
      "content": "Lastly, for simple index-on-index merges, you can pass a list of DataFrames to join\nas an alternative to using the more general pandas.concat function described in the\nnext section:\nIn [80]: another = pd.DataFrame([[7., 8.], [9., 10.], [11., 12.], [16., 17.]],\n   ....:                        index=[\"a\", \"c\", \"e\", \"f\"],\n   ....:                        columns=[\"New York\", \"Oregon\"])\nIn [81]: another\nOut[81]: \n   New York  Oregon\na       7.0     8.0\nc       9.0    10.0\ne      11.0    12.0\nf      16.0    17.0\nIn [82]: left2.join([right2, another])\nOut[82]: \n   Ohio  Nevada  Missouri  Alabama  New York  Oregon\na     1       2      <NA>     <NA>       7.0     8.0\nc     3       4         9       10       9.0    10.0\ne     5       6        13       14      11.0    12.0\nIn [83]: left2.join([right2, another], how=\"outer\")\nOut[83]: \n   Ohio  Nevada  Missouri  Alabama  New York  Oregon\na     1       2      <NA>     <NA>       7.0     8.0\nc     3       4         9       10       9.0    10.0\ne     5       6        13       14      11.0    12.0\nb  <NA>    <NA>         7        8       NaN     NaN\nd  <NA>    <NA>        11       12       NaN     NaN\nf  <NA>    <NA>      <NA>     <NA>      16.0    17.0\nConcatenating Along an Axis\nAnother kind of data combination operation is referred to interchangeably as concat‐\nenation or stacking. NumPy’s concatenate function can do this with NumPy arrays:\nIn [84]: arr = np.arange(12).reshape((3, 4))\nIn [85]: arr\nOut[85]: \narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\nIn [86]: np.concatenate([arr, arr], axis=1)\nOut[86]: \narray([[ 0,  1,  2,  3,  0,  1,  2,  3],\n       [ 4,  5,  6,  7,  4,  5,  6,  7],\n       [ 8,  9, 10, 11,  8,  9, 10, 11]])\n8.2 Combining and Merging Datasets \n| \n263",
      "content_length": 1764,
      "extraction_method": "Direct"
    },
    {
      "page_number": 282,
      "chapter": null,
      "content": "In the context of pandas objects such as Series and DataFrame, having labeled axes\nenable you to further generalize array concatenation. In particular, you have a num‐\nber of additional concerns:\n• If the objects are indexed differently on the other axes, should we combine the\n•\ndistinct elements in these axes or use only the values in common?\n• Do the concatenated chunks of data need to be identifiable as such in the result‐\n•\ning object?\n• Does the “concatenation axis” contain data that needs to be preserved? In\n•\nmany cases, the default integer labels in a DataFrame are best discarded during\nconcatenation.\nThe concat function in pandas provides a consistent way to address each of these\nquestions. I’ll give a number of examples to illustrate how it works. Suppose we have\nthree Series with no index overlap:\nIn [87]: s1 = pd.Series([0, 1], index=[\"a\", \"b\"], dtype=\"Int64\")\nIn [88]: s2 = pd.Series([2, 3, 4], index=[\"c\", \"d\", \"e\"], dtype=\"Int64\")\nIn [89]: s3 = pd.Series([5, 6], index=[\"f\", \"g\"], dtype=\"Int64\")\nCalling pandas.concat with these objects in a list glues together the values and\nindexes:\nIn [90]: s1\nOut[90]: \na    0\nb    1\ndtype: Int64\nIn [91]: s2\nOut[91]: \nc    2\nd    3\ne    4\ndtype: Int64\nIn [92]: s3\nOut[92]: \nf    5\ng    6\ndtype: Int64\nIn [93]: pd.concat([s1, s2, s3])\nOut[93]: \na    0\nb    1\nc    2\n264 \n| \nChapter 8: Data Wrangling: Join, Combine, and Reshape",
      "content_length": 1392,
      "extraction_method": "Direct"
    },
    {
      "page_number": 283,
      "chapter": null,
      "content": "d    3\ne    4\nf    5\ng    6\ndtype: Int64\nBy default, pandas.concat works along axis=\"index\", producing another Series. If\nyou pass axis=\"columns\", the result will instead be a DataFrame:\nIn [94]: pd.concat([s1, s2, s3], axis=\"columns\")\nOut[94]: \n      0     1     2\na     0  <NA>  <NA>\nb     1  <NA>  <NA>\nc  <NA>     2  <NA>\nd  <NA>     3  <NA>\ne  <NA>     4  <NA>\nf  <NA>  <NA>     5\ng  <NA>  <NA>     6\nIn this case there is no overlap on the other axis, which as you can see is the\nunion (the \"outer\" join) of the indexes. You can instead intersect them by passing\njoin=\"inner\":\nIn [95]: s4 = pd.concat([s1, s3])\nIn [96]: s4\nOut[96]: \na    0\nb    1\nf    5\ng    6\ndtype: Int64\nIn [97]: pd.concat([s1, s4], axis=\"columns\")\nOut[97]: \n      0  1\na     0  0\nb     1  1\nf  <NA>  5\ng  <NA>  6\nIn [98]: pd.concat([s1, s4], axis=\"columns\", join=\"inner\")\nOut[98]: \n   0  1\na  0  0\nb  1  1\nIn this last example, the \"f\" and \"g\" labels disappeared because of the join=\"inner\"\noption.\n8.2 Combining and Merging Datasets \n| \n265",
      "content_length": 1018,
      "extraction_method": "Direct"
    },
    {
      "page_number": 284,
      "chapter": null,
      "content": "A potential issue is that the concatenated pieces are not identifiable in the result.\nSuppose instead you wanted to create a hierarchical index on the concatenation axis.\nTo do this, use the keys argument:\nIn [99]: result = pd.concat([s1, s1, s3], keys=[\"one\", \"two\", \"three\"])\nIn [100]: result\nOut[100]: \none    a    0\n       b    1\ntwo    a    0\n       b    1\nthree  f    5\n       g    6\ndtype: Int64\nIn [101]: result.unstack()\nOut[101]: \n          a     b     f     g\none       0     1  <NA>  <NA>\ntwo       0     1  <NA>  <NA>\nthree  <NA>  <NA>     5     6\nIn the case of combining Series along axis=\"columns\", the keys become the Data‐\nFrame column headers:\nIn [102]: pd.concat([s1, s2, s3], axis=\"columns\", keys=[\"one\", \"two\", \"three\"])\nOut[102]: \n    one   two  three\na     0  <NA>   <NA>\nb     1  <NA>   <NA>\nc  <NA>     2   <NA>\nd  <NA>     3   <NA>\ne  <NA>     4   <NA>\nf  <NA>  <NA>      5\ng  <NA>  <NA>      6\nThe same logic extends to DataFrame objects:\nIn [103]: df1 = pd.DataFrame(np.arange(6).reshape(3, 2), index=[\"a\", \"b\", \"c\"],\n   .....:                    columns=[\"one\", \"two\"])\nIn [104]: df2 = pd.DataFrame(5 + np.arange(4).reshape(2, 2), index=[\"a\", \"c\"],\n   .....:                    columns=[\"three\", \"four\"])\nIn [105]: df1\nOut[105]: \n   one  two\na    0    1\nb    2    3\nc    4    5\nIn [106]: df2\n266 \n| \nChapter 8: Data Wrangling: Join, Combine, and Reshape",
      "content_length": 1383,
      "extraction_method": "Direct"
    },
    {
      "page_number": 285,
      "chapter": null,
      "content": "Out[106]: \n   three  four\na      5     6\nc      7     8\nIn [107]: pd.concat([df1, df2], axis=\"columns\", keys=[\"level1\", \"level2\"])\nOut[107]: \n  level1     level2     \n     one two  three four\na      0   1    5.0  6.0\nb      2   3    NaN  NaN\nc      4   5    7.0  8.0\nHere the keys argument is used to create a hierarchical index where the first level can\nbe used to identify each of the concatenated DataFrame objects.\nIf you pass a dictionary of objects instead of a list, the dictionary’s keys will be used\nfor the keys option:\nIn [108]: pd.concat({\"level1\": df1, \"level2\": df2}, axis=\"columns\")\nOut[108]: \n  level1     level2     \n     one two  three four\na      0   1    5.0  6.0\nb      2   3    NaN  NaN\nc      4   5    7.0  8.0\nThere are additional arguments governing how the hierarchical index is created\n(see Table 8-3). For example, we can name the created axis levels with the names\nargument:\nIn [109]: pd.concat([df1, df2], axis=\"columns\", keys=[\"level1\", \"level2\"],\n   .....:           names=[\"upper\", \"lower\"])\nOut[109]: \nupper level1     level2     \nlower    one two  three four\na          0   1    5.0  6.0\nb          2   3    NaN  NaN\nc          4   5    7.0  8.0\nA last consideration concerns DataFrames in which the row index does not contain\nany relevant data:\nIn [110]: df1 = pd.DataFrame(np.random.standard_normal((3, 4)),\n   .....:                    columns=[\"a\", \"b\", \"c\", \"d\"])\nIn [111]: df2 = pd.DataFrame(np.random.standard_normal((2, 3)),\n   .....:                    columns=[\"b\", \"d\", \"a\"])\nIn [112]: df1\nOut[112]: \n          a         b         c         d\n0  1.248804  0.774191 -0.319657 -0.624964\n8.2 Combining and Merging Datasets \n| \n267",
      "content_length": 1673,
      "extraction_method": "Direct"
    },
    {
      "page_number": 286,
      "chapter": null,
      "content": "1  1.078814  0.544647  0.855588  1.343268\n2 -0.267175  1.793095 -0.652929 -1.886837\nIn [113]: df2\nOut[113]: \n          b         d         a\n0  1.059626  0.644448 -0.007799\n1 -0.449204  2.448963  0.667226\nIn this case, you can pass ignore_index=True, which discards the indexes from each\nDataFrame and concatenates the data in the columns only, assigning a new default\nindex:\nIn [114]: pd.concat([df1, df2], ignore_index=True)\nOut[114]: \n          a         b         c         d\n0  1.248804  0.774191 -0.319657 -0.624964\n1  1.078814  0.544647  0.855588  1.343268\n2 -0.267175  1.793095 -0.652929 -1.886837\n3 -0.007799  1.059626       NaN  0.644448\n4  0.667226 -0.449204       NaN  2.448963\nTable 8-3 describes the pandas.concat function arguments.\nTable 8-3. pandas.concat function arguments\nArgument\nDescription\nobjs\nList or dictionary of pandas objects to be concatenated; this is the only required argument\naxis\nAxis to concatenate along; defaults to concatenating along rows (axis=\"index\")\njoin\nEither \"inner\" or \"outer\" (\"outer\" by default); whether to intersect (inner) or union\n(outer) indexes along the other axes\nkeys\nValues to associate with objects being concatenated, forming a hierarchical index along the\nconcatenation axis; can be a list or array of arbitrary values, an array of tuples, or a list of arrays (if\nmultiple-level arrays passed in levels)\nlevels\nSpecific indexes to use as hierarchical index level or levels if keys passed\nnames\nNames for created hierarchical levels if keys and/or levels passed\nverify_integrity Check new axis in concatenated object for duplicates and raise an exception if so; by default\n(False) allows duplicates\nignore_index\nDo not preserve indexes along concatenation axis, instead produce a new\nrange(total_length) index\nCombining Data with Overlap\nThere is another data combination situation that can’t be expressed as either a merge\nor concatenation operation. You may have two datasets with indexes that overlap in\nfull or in part. As a motivating example, consider NumPy’s where function, which\nperforms the array-oriented equivalent of an if-else expression:\n268 \n| \nChapter 8: Data Wrangling: Join, Combine, and Reshape",
      "content_length": 2176,
      "extraction_method": "Direct"
    },
    {
      "page_number": 287,
      "chapter": null,
      "content": "In [115]: a = pd.Series([np.nan, 2.5, 0.0, 3.5, 4.5, np.nan],\n   .....:               index=[\"f\", \"e\", \"d\", \"c\", \"b\", \"a\"])\nIn [116]: b = pd.Series([0., np.nan, 2., np.nan, np.nan, 5.],\n   .....:               index=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"])\nIn [117]: a\nOut[117]: \nf    NaN\ne    2.5\nd    0.0\nc    3.5\nb    4.5\na    NaN\ndtype: float64\nIn [118]: b\nOut[118]: \na    0.0\nb    NaN\nc    2.0\nd    NaN\ne    NaN\nf    5.0\ndtype: float64\nIn [119]: np.where(pd.isna(a), b, a)\nOut[119]: array([0. , 2.5, 0. , 3.5, 4.5, 5. ])\nHere, whenever values in a are null, values from b are selected, otherwise the non-\nnull values from a are selected. Using numpy.where does not check whether the index\nlabels are aligned or not (and does not even require the objects to be the same\nlength), so if you want to line up values by index, use the Series combine_first\nmethod:\nIn [120]: a.combine_first(b)\nOut[120]: \na    0.0\nb    4.5\nc    3.5\nd    0.0\ne    2.5\nf    5.0\ndtype: float64\nWith DataFrames, combine_first does the same thing column by column, so you\ncan think of it as “patching” missing data in the calling object with data from the\nobject you pass:\nIn [121]: df1 = pd.DataFrame({\"a\": [1., np.nan, 5., np.nan],\n   .....:                     \"b\": [np.nan, 2., np.nan, 6.],\n   .....:                     \"c\": range(2, 18, 4)})\n8.2 Combining and Merging Datasets \n| \n269",
      "content_length": 1357,
      "extraction_method": "Direct"
    },
    {
      "page_number": 288,
      "chapter": null,
      "content": "In [122]: df2 = pd.DataFrame({\"a\": [5., 4., np.nan, 3., 7.],\n   .....:                     \"b\": [np.nan, 3., 4., 6., 8.]})\nIn [123]: df1\nOut[123]: \n     a    b   c\n0  1.0  NaN   2\n1  NaN  2.0   6\n2  5.0  NaN  10\n3  NaN  6.0  14\nIn [124]: df2\nOut[124]: \n     a    b\n0  5.0  NaN\n1  4.0  3.0\n2  NaN  4.0\n3  3.0  6.0\n4  7.0  8.0\nIn [125]: df1.combine_first(df2)\nOut[125]: \n     a    b     c\n0  1.0  NaN   2.0\n1  4.0  2.0   6.0\n2  5.0  4.0  10.0\n3  3.0  6.0  14.0\n4  7.0  8.0   NaN\nThe output of combine_first with DataFrame objects will have the union of all the\ncolumn names.\n8.3 Reshaping and Pivoting\nThere are a number of basic operations for rearranging tabular data. These are\nreferred to as reshape or pivot operations.\nReshaping with Hierarchical Indexing\nHierarchical indexing provides a consistent way to rearrange data in a DataFrame.\nThere are two primary actions:\nstack\nThis “rotates” or pivots from the columns in the data to the rows.\nunstack\nThis pivots from the rows into the columns.\n270 \n| \nChapter 8: Data Wrangling: Join, Combine, and Reshape",
      "content_length": 1059,
      "extraction_method": "Direct"
    },
    {
      "page_number": 289,
      "chapter": null,
      "content": "I’ll illustrate these operations through a series of examples. Consider a small Data‐\nFrame with string arrays as row and column indexes:\nIn [126]: data = pd.DataFrame(np.arange(6).reshape((2, 3)),\n   .....:                     index=pd.Index([\"Ohio\", \"Colorado\"], name=\"state\"),\n   .....:                     columns=pd.Index([\"one\", \"two\", \"three\"],\n   .....:                     name=\"number\"))\nIn [127]: data\nOut[127]: \nnumber    one  two  three\nstate                    \nOhio        0    1      2\nColorado    3    4      5\nUsing the stack method on this data pivots the columns into the rows, producing a\nSeries:\nIn [128]: result = data.stack()\nIn [129]: result\nOut[129]: \nstate     number\nOhio      one       0\n          two       1\n          three     2\nColorado  one       3\n          two       4\n          three     5\ndtype: int64\nFrom a hierarchically indexed Series, you can rearrange the data back into a Data‐\nFrame with unstack:\nIn [130]: result.unstack()\nOut[130]: \nnumber    one  two  three\nstate                    \nOhio        0    1      2\nColorado    3    4      5\nBy default, the innermost level is unstacked (same with stack). You can unstack a\ndifferent level by passing a level number or name:\nIn [131]: result.unstack(level=0)\nOut[131]: \nstate   Ohio  Colorado\nnumber                \none        0         3\ntwo        1         4\nthree      2         5\nIn [132]: result.unstack(level=\"state\")\n8.3 Reshaping and Pivoting \n| \n271",
      "content_length": 1452,
      "extraction_method": "Direct"
    },
    {
      "page_number": 290,
      "chapter": null,
      "content": "Out[132]: \nstate   Ohio  Colorado\nnumber                \none        0         3\ntwo        1         4\nthree      2         5\nUnstacking might introduce missing data if all of the values in the level aren’t found\nin each subgroup:\nIn [133]: s1 = pd.Series([0, 1, 2, 3], index=[\"a\", \"b\", \"c\", \"d\"], dtype=\"Int64\")\nIn [134]: s2 = pd.Series([4, 5, 6], index=[\"c\", \"d\", \"e\"], dtype=\"Int64\")\nIn [135]: data2 = pd.concat([s1, s2], keys=[\"one\", \"two\"])\nIn [136]: data2\nOut[136]: \none  a    0\n     b    1\n     c    2\n     d    3\ntwo  c    4\n     d    5\n     e    6\ndtype: Int64\nStacking filters out missing data by default, so the operation is more easily invertible:\nIn [137]: data2.unstack()\nOut[137]: \n        a     b  c  d     e\none     0     1  2  3  <NA>\ntwo  <NA>  <NA>  4  5     6\nIn [138]: data2.unstack().stack()\nOut[138]: \none  a    0\n     b    1\n     c    2\n     d    3\ntwo  c    4\n     d    5\n     e    6\ndtype: Int64\nIn [139]: data2.unstack().stack(dropna=False)\nOut[139]: \none  a       0\n     b       1\n     c       2\n     d       3\n     e    <NA>\n272 \n| \nChapter 8: Data Wrangling: Join, Combine, and Reshape",
      "content_length": 1116,
      "extraction_method": "Direct"
    },
    {
      "page_number": 291,
      "chapter": null,
      "content": "two  a    <NA>\n     b    <NA>\n     c       4\n     d       5\n     e       6\ndtype: Int64\nWhen you unstack in a DataFrame, the level unstacked becomes the lowest level in\nthe result:\nIn [140]: df = pd.DataFrame({\"left\": result, \"right\": result + 5},\n   .....:                   columns=pd.Index([\"left\", \"right\"], name=\"side\"))\nIn [141]: df\nOut[141]: \nside             left  right\nstate    number             \nOhio     one        0      5\n         two        1      6\n         three      2      7\nColorado one        3      8\n         two        4      9\n         three      5     10\nIn [142]: df.unstack(level=\"state\")\nOut[142]: \nside   left          right         \nstate  Ohio Colorado  Ohio Colorado\nnumber                             \none       0        3     5        8\ntwo       1        4     6        9\nthree     2        5     7       10\nAs with unstack, when calling stack we can indicate the name of the axis to stack:\nIn [143]: df.unstack(level=\"state\").stack(level=\"side\")\nOut[143]: \nstate         Colorado  Ohio\nnumber side                 \none    left          3     0\n       right         8     5\ntwo    left          4     1\n       right         9     6\nthree  left          5     2\n       right        10     7\nPivoting “Long” to “Wide” Format\nA common way to store multiple time series in databases and CSV files is what\nis sometimes called long or stacked format. In this format, individual values are\nrepresented by a single row in a table rather than multiple values per row.\n8.3 Reshaping and Pivoting \n| \n273",
      "content_length": 1530,
      "extraction_method": "Direct"
    },
    {
      "page_number": 292,
      "chapter": null,
      "content": "Let’s load some example data and do a small amount of time series wrangling and\nother data cleaning:\nIn [144]: data = pd.read_csv(\"examples/macrodata.csv\")\nIn [145]: data = data.loc[:, [\"year\", \"quarter\", \"realgdp\", \"infl\", \"unemp\"]]\nIn [146]: data.head()\nOut[146]: \n   year  quarter   realgdp  infl  unemp\n0  1959        1  2710.349  0.00    5.8\n1  1959        2  2778.801  2.34    5.1\n2  1959        3  2775.488  2.74    5.3\n3  1959        4  2785.204  0.27    5.6\n4  1960        1  2847.699  2.31    5.2\nFirst, I use pandas.PeriodIndex (which represents time intervals rather than points\nin time), discussed in more detail in Chapter 11, to combine the year and quarter\ncolumns to set the index to consist of datetime values at the end of each quarter:\nIn [147]: periods = pd.PeriodIndex(year=data.pop(\"year\"),\n   .....:                          quarter=data.pop(\"quarter\"),\n   .....:                          name=\"date\")\nIn [148]: periods\nOut[148]: \nPeriodIndex(['1959Q1', '1959Q2', '1959Q3', '1959Q4', '1960Q1', '1960Q2',\n             '1960Q3', '1960Q4', '1961Q1', '1961Q2',\n             ...\n             '2007Q2', '2007Q3', '2007Q4', '2008Q1', '2008Q2', '2008Q3',\n             '2008Q4', '2009Q1', '2009Q2', '2009Q3'],\n            dtype='period[Q-DEC]', name='date', length=203)\nIn [149]: data.index = periods.to_timestamp(\"D\")\nIn [150]: data.head()\nOut[150]: \n             realgdp  infl  unemp\ndate                             \n1959-01-01  2710.349  0.00    5.8\n1959-04-01  2778.801  2.34    5.1\n1959-07-01  2775.488  2.74    5.3\n1959-10-01  2785.204  0.27    5.6\n1960-01-01  2847.699  2.31    5.2\nHere I used the pop method on the DataFrame, which returns a column while\ndeleting it from the DataFrame at the same time.\nThen, I select a subset of columns and give the columns index the name \"item\":\nIn [151]: data = data.reindex(columns=[\"realgdp\", \"infl\", \"unemp\"])\nIn [152]: data.columns.name = \"item\"\n274 \n| \nChapter 8: Data Wrangling: Join, Combine, and Reshape",
      "content_length": 1973,
      "extraction_method": "Direct"
    },
    {
      "page_number": 293,
      "chapter": null,
      "content": "In [153]: data.head()\nOut[153]: \nitem         realgdp  infl  unemp\ndate                             \n1959-01-01  2710.349  0.00    5.8\n1959-04-01  2778.801  2.34    5.1\n1959-07-01  2775.488  2.74    5.3\n1959-10-01  2785.204  0.27    5.6\n1960-01-01  2847.699  2.31    5.2\nLastly, I reshape with stack, turn the new index levels into columns with\nreset_index, and finally give the column containing the data values the name\n\"value\":\nIn [154]: long_data = (data.stack()\n   .....:              .reset_index()\n   .....:              .rename(columns={0: \"value\"}))\nNow, ldata looks like:\nIn [155]: long_data[:10]\nOut[155]: \n        date     item     value\n0 1959-01-01  realgdp  2710.349\n1 1959-01-01     infl     0.000\n2 1959-01-01    unemp     5.800\n3 1959-04-01  realgdp  2778.801\n4 1959-04-01     infl     2.340\n5 1959-04-01    unemp     5.100\n6 1959-07-01  realgdp  2775.488\n7 1959-07-01     infl     2.740\n8 1959-07-01    unemp     5.300\n9 1959-10-01  realgdp  2785.204\nIn this so-called long format for multiple time series, each row in the table represents\na single observation.\nData is frequently stored this way in relational SQL databases, as a fixed schema (col‐\numn names and data types) allows the number of distinct values in the item column\nto change as data is added to the table. In the previous example, date and item would\nusually be the primary keys (in relational database parlance), offering both relational\nintegrity and easier joins. In some cases, the data may be more difficult to work with\nin this format; you might prefer to have a DataFrame containing one column per\ndistinct item value indexed by timestamps in the date column. DataFrame’s pivot\nmethod performs exactly this transformation:\nIn [156]: pivoted = long_data.pivot(index=\"date\", columns=\"item\",\n   .....:                           values=\"value\")\nIn [157]: pivoted.head()\nOut[157]: \n8.3 Reshaping and Pivoting \n| \n275",
      "content_length": 1904,
      "extraction_method": "Direct"
    },
    {
      "page_number": 294,
      "chapter": null,
      "content": "item        infl   realgdp  unemp\ndate                             \n1959-01-01  0.00  2710.349    5.8\n1959-04-01  2.34  2778.801    5.1\n1959-07-01  2.74  2775.488    5.3\n1959-10-01  0.27  2785.204    5.6\n1960-01-01  2.31  2847.699    5.2\nThe first two values passed are the columns to be used, respectively, as the row and\ncolumn index, then finally an optional value column to fill the DataFrame. Suppose\nyou had two value columns that you wanted to reshape simultaneously:\nIn [158]: long_data[\"value2\"] = np.random.standard_normal(len(long_data))\nIn [159]: long_data[:10]\nOut[159]: \n        date     item     value    value2\n0 1959-01-01  realgdp  2710.349  0.802926\n1 1959-01-01     infl     0.000  0.575721\n2 1959-01-01    unemp     5.800  1.381918\n3 1959-04-01  realgdp  2778.801  0.000992\n4 1959-04-01     infl     2.340 -0.143492\n5 1959-04-01    unemp     5.100 -0.206282\n6 1959-07-01  realgdp  2775.488 -0.222392\n7 1959-07-01     infl     2.740 -1.682403\n8 1959-07-01    unemp     5.300  1.811659\n9 1959-10-01  realgdp  2785.204 -0.351305\nBy omitting the last argument, you obtain a DataFrame with hierarchical columns:\nIn [160]: pivoted = long_data.pivot(index=\"date\", columns=\"item\")\nIn [161]: pivoted.head()\nOut[161]: \n           value                    value2                    \nitem        infl   realgdp unemp      infl   realgdp     unemp\ndate                                                          \n1959-01-01  0.00  2710.349   5.8  0.575721  0.802926  1.381918\n1959-04-01  2.34  2778.801   5.1 -0.143492  0.000992 -0.206282\n1959-07-01  2.74  2775.488   5.3 -1.682403 -0.222392  1.811659\n1959-10-01  0.27  2785.204   5.6  0.128317 -0.351305 -1.313554\n1960-01-01  2.31  2847.699   5.2 -0.615939  0.498327  0.174072\nIn [162]: pivoted[\"value\"].head()\nOut[162]: \nitem        infl   realgdp  unemp\ndate                             \n1959-01-01  0.00  2710.349    5.8\n1959-04-01  2.34  2778.801    5.1\n1959-07-01  2.74  2775.488    5.3\n1959-10-01  0.27  2785.204    5.6\n1960-01-01  2.31  2847.699    5.2\n276 \n| \nChapter 8: Data Wrangling: Join, Combine, and Reshape",
      "content_length": 2078,
      "extraction_method": "Direct"
    },
    {
      "page_number": 295,
      "chapter": null,
      "content": "Note that pivot is equivalent to creating a hierarchical index using set_index fol‐\nlowed by a call to unstack:\nIn [163]: unstacked = long_data.set_index([\"date\", \"item\"]).unstack(level=\"item\")\nIn [164]: unstacked.head()\nOut[164]: \n           value                    value2                    \nitem        infl   realgdp unemp      infl   realgdp     unemp\ndate                                                          \n1959-01-01  0.00  2710.349   5.8  0.575721  0.802926  1.381918\n1959-04-01  2.34  2778.801   5.1 -0.143492  0.000992 -0.206282\n1959-07-01  2.74  2775.488   5.3 -1.682403 -0.222392  1.811659\n1959-10-01  0.27  2785.204   5.6  0.128317 -0.351305 -1.313554\n1960-01-01  2.31  2847.699   5.2 -0.615939  0.498327  0.174072\nPivoting “Wide” to “Long” Format\nAn inverse operation to pivot for DataFrames is pandas.melt. Rather than trans‐\nforming one column into many in a new DataFrame, it merges multiple columns into\none, producing a DataFrame that is longer than the input. Let’s look at an example:\nIn [166]: df = pd.DataFrame({\"key\": [\"foo\", \"bar\", \"baz\"],\n   .....:                    \"A\": [1, 2, 3],\n   .....:                    \"B\": [4, 5, 6],\n   .....:                    \"C\": [7, 8, 9]})\nIn [167]: df\nOut[167]: \n   key  A  B  C\n0  foo  1  4  7\n1  bar  2  5  8\n2  baz  3  6  9\nThe \"key\" column may be a group indicator, and the other columns are data values.\nWhen using pandas.melt, we must indicate which columns (if any) are group indica‐\ntors. Let’s use \"key\" as the only group indicator here:\nIn [168]: melted = pd.melt(df, id_vars=\"key\")\nIn [169]: melted\nOut[169]: \n   key variable  value\n0  foo        A      1\n1  bar        A      2\n2  baz        A      3\n3  foo        B      4\n4  bar        B      5\n5  baz        B      6\n6  foo        C      7\n7  bar        C      8\n8  baz        C      9\n8.3 Reshaping and Pivoting \n| \n277",
      "content_length": 1855,
      "extraction_method": "Direct"
    },
    {
      "page_number": 296,
      "chapter": null,
      "content": "Using pivot, we can reshape back to the original layout:\nIn [170]: reshaped = melted.pivot(index=\"key\", columns=\"variable\",\n   .....:                         values=\"value\")\nIn [171]: reshaped\nOut[171]: \nvariable  A  B  C\nkey              \nbar       2  5  8\nbaz       3  6  9\nfoo       1  4  7\nSince the result of pivot creates an index from the column used as the row labels, we\nmay want to use reset_index to move the data back into a column:\nIn [172]: reshaped.reset_index()\nOut[172]: \nvariable  key  A  B  C\n0         bar  2  5  8\n1         baz  3  6  9\n2         foo  1  4  7\nYou can also specify a subset of columns to use as value columns:\nIn [173]: pd.melt(df, id_vars=\"key\", value_vars=[\"A\", \"B\"])\nOut[173]: \n   key variable  value\n0  foo        A      1\n1  bar        A      2\n2  baz        A      3\n3  foo        B      4\n4  bar        B      5\n5  baz        B      6\npandas.melt can be used without any group identifiers, too:\nIn [174]: pd.melt(df, value_vars=[\"A\", \"B\", \"C\"])\nOut[174]: \n  variable  value\n0        A      1\n1        A      2\n2        A      3\n3        B      4\n4        B      5\n5        B      6\n6        C      7\n7        C      8\n8        C      9\nIn [175]: pd.melt(df, value_vars=[\"key\", \"A\", \"B\"])\nOut[175]: \n  variable value\n0      key   foo\n278 \n| \nChapter 8: Data Wrangling: Join, Combine, and Reshape",
      "content_length": 1338,
      "extraction_method": "Direct"
    },
    {
      "page_number": 297,
      "chapter": null,
      "content": "1      key   bar\n2      key   baz\n3        A     1\n4        A     2\n5        A     3\n6        B     4\n7        B     5\n8        B     6\n8.4 Conclusion\nNow that you have some pandas basics for data import, cleaning, and reorganization\nunder your belt, we are ready to move on to data visualization with matplotlib. We\nwill return to explore other areas of pandas later in the book when we discuss more\nadvanced analytics.\n8.4 Conclusion \n| \n279",
      "content_length": 443,
      "extraction_method": "Direct"
    },
    {
      "page_number": 298,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 299,
      "chapter": null,
      "content": "CHAPTER 9\nPlotting and Visualization\nMaking informative visualizations (sometimes called plots) is one of the most impor‐\ntant tasks in data analysis. It may be a part of the exploratory process—for example,\nto help identify outliers or needed data transformations, or as a way of generating\nideas for models. For others, building an interactive visualization for the web may\nbe the end goal. Python has many add-on libraries for making static or dynamic\nvisualizations, but I’ll be mainly focused on matplotlib and libraries that build on top\nof it.\nmatplotlib is a desktop plotting package designed for creating plots and figures\nsuitable for publication. The project was started by John Hunter in 2002 to enable\na MATLAB-like plotting interface in Python. The matplotlib and IPython commun‐\nities have collaborated to simplify interactive plotting from the IPython shell (and\nnow, Jupyter notebook). matplotlib supports various GUI backends on all operating\nsystems and can export visualizations to all of the common vector and raster graphics\nformats (PDF, SVG, JPG, PNG, BMP, GIF, etc.). With the exception of a few diagrams,\nnearly all of the graphics in this book were produced using matplotlib.\nOver time, matplotlib has spawned a number of add-on toolkits for data visualization\nthat use matplotlib for their underlying plotting. One of these is seaborn, which we\nexplore later in this chapter.\nThe simplest way to follow the code examples in the chapter is to output plots in\nthe Jupyter notebook. To set this up, execute the following statement in a Jupyter\nnotebook:\n%matplotlib inline\n281",
      "content_length": 1601,
      "extraction_method": "Direct"
    },
    {
      "page_number": 300,
      "chapter": null,
      "content": "Since this book’s first edition in 2012, many new data visualization\nlibraries have been created, some of which (like Bokeh and Altair)\ntake advantage of modern web technology to create interactive\nvisualizations that integrate well with the Jupyter notebook. Rather\nthan use multiple visualization tools in this book, I decided to stick\nwith matplotlib for teaching the fundamentals, in particular since\npandas has good integration with matplotlib. You can adapt the\nprinciples from this chapter to learn how to use other visualization\nlibraries as well.\n9.1 A Brief matplotlib API Primer\nWith matplotlib, we use the following import convention:\nIn [13]: import matplotlib.pyplot as plt\nAfter running %matplotlib notebook in Jupyter (or simply %matplotlib in IPy‐\nthon), we can try creating a simple plot. If everything is set up right, a line plot like\nFigure 9-1 should appear:\nIn [14]: data = np.arange(10)\nIn [15]: data\nOut[15]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\nIn [16]: plt.plot(data)\nFigure 9-1. Simple line plot\n282 \n| \nChapter 9: Plotting and Visualization",
      "content_length": 1070,
      "extraction_method": "Direct"
    },
    {
      "page_number": 301,
      "chapter": null,
      "content": "While libraries like seaborn and pandas’s built-in plotting functions will deal with\nmany of the mundane details of making plots, should you wish to customize them\nbeyond the function options provided, you will need to learn a bit about the matplot‐\nlib API.\nThere is not enough room in the book to give comprehensive\ntreatment of the breadth and depth of functionality in matplotlib.\nIt should be enough to teach you the ropes to get up and running.\nThe matplotlib gallery and documentation are the best resource for\nlearning advanced features.\nFigures and Subplots\nPlots in matplotlib reside within a Figure object. You can create a new figure with\nplt.figure:\nIn [17]: fig = plt.figure()\nIn IPython, if you first run %matplotlib to set up the matplotlib integration, an\nempty plot window will appear, but in Jupyter nothing will be shown until we use a\nfew more commands.\nplt.figure has a number of options; notably, figsize will guarantee the figure has a\ncertain size and aspect ratio if saved to disk.\nYou can’t make a plot with a blank figure. You have to create one or more subplots\nusing add_subplot:\nIn [18]: ax1 = fig.add_subplot(2, 2, 1)\nThis means that the figure should be 2 × 2 (so up to four plots in total), and we’re\nselecting the first of four subplots (numbered from 1). If you create the next two\nsubplots, you’ll end up with a visualization that looks like Figure 9-2:\nIn [19]: ax2 = fig.add_subplot(2, 2, 2)\nIn [20]: ax3 = fig.add_subplot(2, 2, 3)\n9.1 A Brief matplotlib API Primer \n| \n283",
      "content_length": 1512,
      "extraction_method": "Direct"
    },
    {
      "page_number": 302,
      "chapter": null,
      "content": "Figure 9-2. An empty matplotlib figure with three subplots\nOne nuance of using Jupyter notebooks is that plots are reset after\neach cell is evaluated, so you must put all of the plotting commands\nin a single notebook cell.\nHere we run all of these commands in the same cell:\nfig = plt.figure()\nax1 = fig.add_subplot(2, 2, 1)\nax2 = fig.add_subplot(2, 2, 2)\nax3 = fig.add_subplot(2, 2, 3)\nThese plot axis objects have various methods that create different types of plots,\nand it is preferred to use the axis methods over the top-level plotting functions\nlike plt.plot. For example, we could make a line plot with the plot method (see\nFigure 9-3):\nIn [21]: ax3.plot(np.random.standard_normal(50).cumsum(), color=\"black\",\n   ....:          linestyle=\"dashed\")\n284 \n| \nChapter 9: Plotting and Visualization",
      "content_length": 801,
      "extraction_method": "Direct"
    },
    {
      "page_number": 303,
      "chapter": null,
      "content": "Figure 9-3. Data visualization after a single plot\nYou may notice output like <matplotlib.lines.Line2D at ...> when you run this.\nmatplotlib returns objects that reference the plot subcomponent that was just added.\nA lot of the time you can safely ignore this output, or you can put a semicolon at the\nend of the line to suppress the output.\nThe additional options instruct matplotlib to plot a black dashed line. The objects\nreturned by fig.add_subplot here are AxesSubplot objects, on which you can\ndirectly plot on the other empty subplots by calling each one’s instance method (see\nFigure 9-4):\nIn [22]: ax1.hist(np.random.standard_normal(100), bins=20, color=\"black\", alpha=0\n.3);\nIn [23]: ax2.scatter(np.arange(30), np.arange(30) + 3 * np.random.standard_normal\n(30));\n9.1 A Brief matplotlib API Primer \n| \n285",
      "content_length": 816,
      "extraction_method": "Direct"
    },
    {
      "page_number": 304,
      "chapter": null,
      "content": "Figure 9-4. Data visualization after additional plots\nThe style option alpha=0.3 sets the transparency of the overlaid plot.\nYou can find a comprehensive catalog of plot types in the matplotlib documentation.\nTo make creating a grid of subplots more convenient, matplotlib includes a plt.sub\nplots method that creates a new figure and returns a NumPy array containing the\ncreated subplot objects:\nIn [25]: fig, axes = plt.subplots(2, 3)\nIn [26]: axes\nOut[26]: \narray([[<AxesSubplot:>, <AxesSubplot:>, <AxesSubplot:>],\n       [<AxesSubplot:>, <AxesSubplot:>, <AxesSubplot:>]], dtype=object)\nThe axes array can then be indexed like a two-dimensional array; for example,\naxes[0, 1] refers to the subplot in the top row at the center. You can also indi‐\ncate that subplots should have the same x- or y-axis using sharex and sharey,\nrespectively. This can be useful when you’re comparing data on the same scale;\notherwise, matplotlib autoscales plot limits independently. See Table 9-1 for more on\nthis method.\n286 \n| \nChapter 9: Plotting and Visualization",
      "content_length": 1051,
      "extraction_method": "Direct"
    },
    {
      "page_number": 305,
      "chapter": null,
      "content": "Table 9-1. matplotlib.pyplot.subplots options\nArgument\nDescription\nnrows\nNumber of rows of subplots\nncols\nNumber of columns of subplots\nsharex\nAll subplots should use the same x-axis ticks (adjusting the xlim will affect all subplots)\nsharey\nAll subplots should use the same y-axis ticks (adjusting the ylim will affect all subplots)\nsubplot_kw\nDictionary of keywords passed to add_subplot call used to create each subplot\n**fig_kw\nAdditional keywords to subplots are used when creating the figure, such as plt.subplots(2, 2, \nfigsize=(8, 6))\nAdjusting the spacing around subplots\nBy default, matplotlib leaves a certain amount of padding around the outside of the\nsubplots and in spacing between subplots. This spacing is all specified relative to the\nheight and width of the plot, so that if you resize the plot either programmatically\nor manually using the GUI window, the plot will dynamically adjust itself. You can\nchange the spacing using the subplots_adjust method on Figure objects:\nsubplots_adjust(left=None, bottom=None, right=None, top=None,\n                wspace=None, hspace=None)\nwspace and hspace control the percent of the figure width and figure height, respec‐\ntively, to use as spacing between subplots. Here is a small example you can execute in\nJupyter where I shrink the spacing all the way to zero (see Figure 9-5):\nfig, axes = plt.subplots(2, 2, sharex=True, sharey=True)\nfor i in range(2):\n    for j in range(2):\n        axes[i, j].hist(np.random.standard_normal(500), bins=50,\n                        color=\"black\", alpha=0.5)\nfig.subplots_adjust(wspace=0, hspace=0)\n9.1 A Brief matplotlib API Primer \n| \n287",
      "content_length": 1636,
      "extraction_method": "Direct"
    },
    {
      "page_number": 306,
      "chapter": null,
      "content": "Figure 9-5. Data visualization with no inter-subplot spacing\nYou may notice that the axis labels overlap. matplotlib doesn’t check whether the\nlabels overlap, so in a case like this you would need to fix the labels yourself by\nspecifying explicit tick locations and tick labels (we’ll look at how to do this in the\nlater section “Ticks, Labels, and Legends” on page 290).\nColors, Markers, and Line Styles\nmatplotlib’s line plot function accepts arrays of x and y coordinates and optional\ncolor styling options. For example, to plot x versus y with green dashes, you would\nexecute:\nax.plot(x, y, linestyle=\"--\", color=\"green\")\nA number of color names are provided for commonly used colors, but you can use\nany color on the spectrum by specifying its hex code (e.g., \"#CECECE\"). You can\nsee some of the supported line styles by looking at the docstring for plt.plot (use\nplt.plot? in IPython or Jupyter). A more comprehensive reference is available in the\nonline documentation.\nLine plots can additionally have markers to highlight the actual data points. Since\nmatplotlib’s plot function creates a continuous line plot, interpolating between\npoints, it can occasionally be unclear where the points lie. The marker can be\nsupplied as an additional styling option (see Figure 9-6):\n288 \n| \nChapter 9: Plotting and Visualization",
      "content_length": 1324,
      "extraction_method": "Direct"
    },
    {
      "page_number": 307,
      "chapter": null,
      "content": "In [31]: ax = fig.add_subplot()\nIn [32]: ax.plot(np.random.standard_normal(30).cumsum(), color=\"black\",\n   ....:         linestyle=\"dashed\", marker=\"o\");\nFigure 9-6. Line plot with markers\nFor line plots, you will notice that subsequent points are linearly interpolated by\ndefault. This can be altered with the drawstyle option (see Figure 9-7):\nIn [34]: fig = plt.figure()\nIn [35]: ax = fig.add_subplot()\nIn [36]: data = np.random.standard_normal(30).cumsum()\nIn [37]: ax.plot(data, color=\"black\", linestyle=\"dashed\", label=\"Default\");\nIn [38]: ax.plot(data, color=\"black\", linestyle=\"dashed\",\n   ....:         drawstyle=\"steps-post\", label=\"steps-post\");\nIn [39]: ax.legend()\n9.1 A Brief matplotlib API Primer \n| \n289",
      "content_length": 719,
      "extraction_method": "Direct"
    },
    {
      "page_number": 308,
      "chapter": null,
      "content": "Figure 9-7. Line plot with different drawstyle options\nHere, since we passed the label arguments to plot, we are able to create a plot legend\nto identify each line using ax.legend. I discuss legends more in “Ticks, Labels, and\nLegends” on page 290.\nYou must call ax.legend to create the legend, whether or not you\npassed the label options when plotting the data.\nTicks, Labels, and Legends\nMost kinds of plot decorations can be accessed through methods on matplotlib axes\nobjects. This includes methods like xlim, xticks, and xticklabels. These control the\nplot range, tick locations, and tick labels, respectively. They can be used in two ways:\n• Called with no arguments returns the current parameter value (e.g., ax.xlim()\n•\nreturns the current x-axis plotting range)\n• Called with parameters sets the parameter value (e.g., ax.xlim([0, 10]) sets the\n•\nx-axis range to 0 to 10)\n290 \n| \nChapter 9: Plotting and Visualization",
      "content_length": 926,
      "extraction_method": "Direct"
    },
    {
      "page_number": 309,
      "chapter": null,
      "content": "All such methods act on the active or most recently created AxesSubplot. Each\ncorresponds to two methods on the subplot object itself; in the case of xlim, these are\nax.get_xlim and ax.set_xlim.\nSetting the title, axis labels, ticks, and tick labels\nTo illustrate customizing the axes, I’ll create a simple figure and plot of a random\nwalk (see Figure 9-8):\nIn [40]: fig, ax = plt.subplots()\nIn [41]: ax.plot(np.random.standard_normal(1000).cumsum());\nFigure 9-8. Simple plot for illustrating xticks (with default labels)\nTo change the x-axis ticks, it’s easiest to use set_xticks and set_xticklabels. The\nformer instructs matplotlib where to place the ticks along the data range; by default\nthese locations will also be the labels. But we can set any other values as the labels\nusing set_xticklabels:\nIn [42]: ticks = ax.set_xticks([0, 250, 500, 750, 1000])\nIn [43]: labels = ax.set_xticklabels([\"one\", \"two\", \"three\", \"four\", \"five\"],\n   ....:                             rotation=30, fontsize=8)\n9.1 A Brief matplotlib API Primer \n| \n291",
      "content_length": 1040,
      "extraction_method": "Direct"
    },
    {
      "page_number": 310,
      "chapter": null,
      "content": "The rotation option sets the x tick labels at a 30-degree rotation. Lastly, set_xlabel\ngives a name to the x-axis, and set_title is the subplot title (see Figure 9-9 for the\nresulting figure):\nIn [44]: ax.set_xlabel(\"Stages\")\nOut[44]: Text(0.5, 6.666666666666652, 'Stages')\nIn [45]: ax.set_title(\"My first matplotlib plot\")\nFigure 9-9. Simple plot for illustrating custom xticks\nModifying the y-axis consists of the same process, substituting y for x in this example.\nThe axes class has a set method that allows batch setting of plot properties. From the\nprior example, we could also have written:\nax.set(title=\"My first matplotlib plot\", xlabel=\"Stages\")\nAdding legends\nLegends are another critical element for identifying plot elements. There are a couple\nof ways to add one. The easiest is to pass the label argument when adding each piece\nof the plot:\nIn [46]: fig, ax = plt.subplots()\nIn [47]: ax.plot(np.random.randn(1000).cumsum(), color=\"black\", label=\"one\");\nIn [48]: ax.plot(np.random.randn(1000).cumsum(), color=\"black\", linestyle=\"dashed\n292 \n| \nChapter 9: Plotting and Visualization",
      "content_length": 1095,
      "extraction_method": "Direct"
    },
    {
      "page_number": 311,
      "chapter": null,
      "content": "\",\n   ....:         label=\"two\");\nIn [49]: ax.plot(np.random.randn(1000).cumsum(), color=\"black\", linestyle=\"dotted\n\",\n   ....:         label=\"three\");\nOnce you’ve done this, you can call ax.legend() to automatically create a legend.\nThe resulting plot is in Figure 9-10:\nIn [50]: ax.legend()\nFigure 9-10. Simple plot with three lines and legend\nThe legend method has several other choices for the location loc argument. See the\ndocstring (with ax.legend?) for more information.\nThe loc legend option tells matplotlib where to place the plot. The default is \"best\",\nwhich tries to choose a location that is most out of the way. To exclude one or more\nelements from the legend, pass no label or label=\"_nolegend_\".\n9.1 A Brief matplotlib API Primer \n| \n293",
      "content_length": 755,
      "extraction_method": "Direct"
    },
    {
      "page_number": 312,
      "chapter": null,
      "content": "Annotations and Drawing on a Subplot\nIn addition to the standard plot types, you may wish to draw your own plot annota‐\ntions, which could consist of text, arrows, or other shapes. You can add annotations\nand text using the text, arrow, and annotate functions. text draws text at given\ncoordinates (x, y) on the plot with optional custom styling:\nax.text(x, y, \"Hello world!\",\n        family=\"monospace\", fontsize=10)\nAnnotations can draw both text and arrows arranged appropriately. As an example,\nlet’s plot the closing S&P 500 index price since 2007 (obtained from Yahoo! Finance)\nand annotate it with some of the important dates from the 2008–2009 financial crisis.\nYou can run this code example in a single cell in a Jupyter notebook. See Figure 9-11\nfor the result:\nfrom datetime import datetime\nfig, ax = plt.subplots()\ndata = pd.read_csv(\"examples/spx.csv\", index_col=0, parse_dates=True)\nspx = data[\"SPX\"]\nspx.plot(ax=ax, color=\"black\")\ncrisis_data = [\n    (datetime(2007, 10, 11), \"Peak of bull market\"),\n    (datetime(2008, 3, 12), \"Bear Stearns Fails\"),\n    (datetime(2008, 9, 15), \"Lehman Bankruptcy\")\n]\nfor date, label in crisis_data:\n    ax.annotate(label, xy=(date, spx.asof(date) + 75),\n                xytext=(date, spx.asof(date) + 225),\n                arrowprops=dict(facecolor=\"black\", headwidth=4, width=2,\n                                headlength=4),\n                horizontalalignment=\"left\", verticalalignment=\"top\")\n# Zoom in on 2007-2010\nax.set_xlim([\"1/1/2007\", \"1/1/2011\"])\nax.set_ylim([600, 1800])\nax.set_title(\"Important dates in the 2008-2009 financial crisis\")\n294 \n| \nChapter 9: Plotting and Visualization",
      "content_length": 1643,
      "extraction_method": "Direct"
    },
    {
      "page_number": 313,
      "chapter": null,
      "content": "Figure 9-11. Important dates in the 2008–2009 financial crisis\nThere are a couple of important points to highlight in this plot. The ax.annotate\nmethod can draw labels at the indicated x and y coordinates. We use the set_xlim\nand set_ylim methods to manually set the start and end boundaries for the plot\nrather than using matplotlib’s default. Lastly, ax.set_title adds a main title to the\nplot.\nSee the online matplotlib gallery for many more annotation examples to learn from.\nDrawing shapes requires some more care. matplotlib has objects that represent many\ncommon shapes, referred to as patches. Some of these, like Rectangle and Circle, are\nfound in matplotlib.pyplot, but the full set is located in matplotlib.patches.\nTo add a shape to a plot, you create the patch object and add it to a subplot ax by\npassing the patch to ax.add_patch (see Figure 9-12):\nfig, ax = plt.subplots()\nrect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color=\"black\", alpha=0.3)\ncirc = plt.Circle((0.7, 0.2), 0.15, color=\"blue\", alpha=0.3)\npgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]],\n                   color=\"green\", alpha=0.5)\nax.add_patch(rect)\nax.add_patch(circ)\nax.add_patch(pgon)\n9.1 A Brief matplotlib API Primer \n| \n295",
      "content_length": 1224,
      "extraction_method": "Direct"
    },
    {
      "page_number": 314,
      "chapter": null,
      "content": "Figure 9-12. Data visualization composed from three different patches\nIf you look at the implementation of many familiar plot types, you will see that they\nare assembled from patches.\nSaving Plots to File\nYou can save the active figure to file using the figure object’s savefig instance\nmethod. For example, to save an SVG version of a figure, you need only type:\nfig.savefig(\"figpath.svg\")\nThe file type is inferred from the file extension. So if you used .pdf instead, you\nwould get a PDF. One important option that I use frequently for publishing graphics\nis dpi, which controls the dots-per-inch resolution. To get the same plot as a PNG at\n400 DPI, you would do:\nfig.savefig(\"figpath.png\", dpi=400)\nSee Table 9-2 for a list of some other options for savefig. For a comprehensive\nlisting, refer to the docstring in IPython or Jupyter.\n296 \n| \nChapter 9: Plotting and Visualization",
      "content_length": 884,
      "extraction_method": "Direct"
    },
    {
      "page_number": 315,
      "chapter": null,
      "content": "Table 9-2. Some fig.savefig options\nArgument\nDescription\nfname\nString containing a filepath or a Python file-like object. The figure format is inferred from the file\nextension (e.g., .pdf for PDF or .png for PNG).\ndpi\nThe figure resolution in dots per inch; defaults to 100 in IPython or 72 in Jupyter out of the box but can be\nconfigured.\nfacecolor, \nedgecolor\nThe color of the figure background outside of the subplots; \"w\" (white), by default.\nformat\nThe explicit file format to use (\"png\", \"pdf\", \"svg\", \"ps\", \"eps\", ...).\nmatplotlib Configuration\nmatplotlib comes configured with color schemes and defaults that are geared primar‐\nily toward preparing figures for publication. Fortunately, nearly all of the default\nbehavior can be customized via global parameters governing figure size, subplot\nspacing, colors, font sizes, grid styles, and so on. One way to modify the configuration\nprogrammatically from Python is to use the rc method; for example, to set the global\ndefault figure size to be 10 × 10, you could enter:\nplt.rc(\"figure\", figsize=(10, 10))\nAll of the current configuration settings are found in the plt.rcParams dictionary,\nand they can be restored to their default values by calling the plt.rcdefaults()\nfunction.\nThe first argument to rc is the component you wish to customize, such as \"figure\",\n\"axes\", \"xtick\", \"ytick\", \"grid\", \"legend\", or many others. After that can follow a\nsequence of keyword arguments indicating the new parameters. A convenient way to\nwrite down the options in your program is as a dictionary:\nplt.rc(\"font\", family=\"monospace\", weight=\"bold\", size=8)\nFor more extensive customization and to see a list of all the options, matplotlib\ncomes with a configuration file matplotlibrc in the matplotlib/mpl-data directory. If\nyou customize this file and place it in your home directory titled .matplotlibrc, it will\nbe loaded each time you use matplotlib.\nAs we’ll see in the next section, the seaborn package has several built-in plot themes\nor styles that use matplotlib’s configuration system internally.\n9.1 A Brief matplotlib API Primer \n| \n297",
      "content_length": 2093,
      "extraction_method": "Direct"
    },
    {
      "page_number": 316,
      "chapter": null,
      "content": "9.2 Plotting with pandas and seaborn\nmatplotlib can be a fairly low-level tool. You assemble a plot from its base compo‐\nnents: the data display (i.e., the type of plot: line, bar, box, scatter, contour, etc.),\nlegend, title, tick labels, and other annotations.\nIn pandas, we may have multiple columns of data, along with row and column labels.\npandas itself has built-in methods that simplify creating visualizations from Data‐\nFrame and Series objects. Another library is seaborn, a high-level statistical graphics\nlibrary built on matplotlib. seaborn simplifies creating many common visualization\ntypes.\nLine Plots\nSeries and DataFrame have a plot attribute for making some basic plot types. By\ndefault, plot() makes line plots (see Figure 9-13):\nIn [61]: s = pd.Series(np.random.standard_normal(10).cumsum(), index=np.arange(0,\n 100, 10))\nIn [62]: s.plot()\nFigure 9-13. Simple Series plot\nThe Series object’s index is passed to matplotlib for plotting on the x-axis, though\nyou can disable this by passing use_index=False. The x-axis ticks and limits can be\nadjusted with the xticks and xlim options, and the y-axis respectively with yticks\n298 \n| \nChapter 9: Plotting and Visualization",
      "content_length": 1190,
      "extraction_method": "Direct"
    },
    {
      "page_number": 317,
      "chapter": null,
      "content": "and ylim. See Table 9-3 for a partial listing of plot options. I’ll comment on a few\nmore of them throughout this section and leave the rest for you to explore.\nTable 9-3. Series.plot method arguments\nArgument\nDescription\nlabel\nLabel for plot legend\nax\nmatplotlib subplot object to plot on; if nothing passed, uses active matplotlib subplot\nstyle\nStyle string, like \"ko--\", to be passed to matplotlib\nalpha\nThe plot fill opacity (from 0 to 1)\nkind\nCan be \"area\", \"bar\", \"barh\", \"density\", \"hist\", \"kde\", \"line\", or \"pie\"; defaults to\n\"line\"\nfigsize\nSize of the figure object to create\nlogx\nPass True for logarithmic scaling on the x axis; pass \"sym\" for symmetric logarithm that permits negative\nvalues\nlogy\nPass True for logarithmic scaling on the y axis; pass \"sym\" for symmetric logarithm that permits negative\nvalues\ntitle\nTitle to use for the plot\nuse_index\nUse the object index for tick labels\nrot\nRotation of tick labels (0 through 360)\nxticks\nValues to use for x-axis ticks\nyticks\nValues to use for y-axis ticks\nxlim\nx-axis limits (e.g., [0, 10])\nylim\ny-axis limits\ngrid\nDisplay axis grid (off by default)\nMost of pandas’s plotting methods accept an optional ax parameter, which can be a\nmatplotlib subplot object. This gives you more flexible placement of subplots in a grid\nlayout.\n9.2 Plotting with pandas and seaborn \n| \n299",
      "content_length": 1336,
      "extraction_method": "Direct"
    },
    {
      "page_number": 318,
      "chapter": null,
      "content": "DataFrame’s plot method plots each of its columns as a different line on the same\nsubplot, creating a legend automatically (see Figure 9-14):\nIn [63]: df = pd.DataFrame(np.random.standard_normal((10, 4)).cumsum(0),\n   ....:                   columns=[\"A\", \"B\", \"C\", \"D\"],\n   ....:                   index=np.arange(0, 100, 10))\nIn [64]: plt.style.use('grayscale')\nIn [65]: df.plot()\nFigure 9-14. Simple DataFrame plot\nHere I used plt.style.use('grayscale') to switch to a color\nscheme more suitable for black and white publication, since some\nreaders will not be able to see the full color plots.\nThe plot attribute contains a “family” of methods for different plot types. For exam‐\nple, df.plot() is equivalent to df.plot.line(). We’ll explore some of these methods\nnext.\nAdditional keyword arguments to plot are passed through to the\nrespective matplotlib plotting function, so you can further custom‐\nize these plots by learning more about the matplotlib API.\n300 \n| \nChapter 9: Plotting and Visualization",
      "content_length": 1008,
      "extraction_method": "Direct"
    },
    {
      "page_number": 319,
      "chapter": null,
      "content": "DataFrame has a number of options allowing some flexibility for how the columns\nare handled, for example, whether to plot them all on the same subplot or to create\nseparate subplots. See Table 9-4 for more on these.\nTable 9-4. DataFrame-specific plot arguments\nArgument\nDescription\nsubplots\nPlot each DataFrame column in a separate subplot\nlayouts\n2-tuple (rows, columns) providing layout of subplots\nsharex\nIf subplots=True, share the same x-axis, linking ticks and limits\nsharey\nIf subplots=True, share the same y-axis\nlegend\nAdd a subplot legend (True by default)\nsort_columns\nPlot columns in alphabetical order; by default uses existing column order\nFor time series plotting, see Chapter 11.\nBar Plots\nThe plot.bar() and plot.barh() make vertical and horizontal bar plots, respec‐\ntively. In this case, the Series or DataFrame index will be used as the x (bar) or y\n(barh) ticks (see Figure 9-15):\nIn [66]: fig, axes = plt.subplots(2, 1)\nIn [67]: data = pd.Series(np.random.uniform(size=16), index=list(\"abcdefghijklmno\np\"))\nIn [68]: data.plot.bar(ax=axes[0], color=\"black\", alpha=0.7)\nOut[68]: <AxesSubplot:>\nIn [69]: data.plot.barh(ax=axes[1], color=\"black\", alpha=0.7)\n9.2 Plotting with pandas and seaborn \n| \n301",
      "content_length": 1220,
      "extraction_method": "Direct"
    },
    {
      "page_number": 320,
      "chapter": null,
      "content": "Figure 9-15. Horizonal and vertical bar plot\nWith a DataFrame, bar plots group the values in each row in bars, side by side, for\neach value. See Figure 9-16:\nIn [71]: df = pd.DataFrame(np.random.uniform(size=(6, 4)),\n   ....:                   index=[\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"],\n   ....:                   columns=pd.Index([\"A\", \"B\", \"C\", \"D\"], name=\"Genus\"))\nIn [72]: df\nOut[72]: \nGenus         A         B         C         D\none    0.370670  0.602792  0.229159  0.486744\ntwo    0.420082  0.571653  0.049024  0.880592\nthree  0.814568  0.277160  0.880316  0.431326\nfour   0.374020  0.899420  0.460304  0.100843\nfive   0.433270  0.125107  0.494675  0.961825\nsix    0.601648  0.478576  0.205690  0.560547\nIn [73]: df.plot.bar()\n302 \n| \nChapter 9: Plotting and Visualization",
      "content_length": 791,
      "extraction_method": "Direct"
    },
    {
      "page_number": 321,
      "chapter": null,
      "content": "Figure 9-16. DataFrame bar plot\nNote that the name “Genus” on the DataFrame’s columns is used to title the legend.\n9.2 Plotting with pandas and seaborn \n| \n303",
      "content_length": 159,
      "extraction_method": "Direct"
    },
    {
      "page_number": 322,
      "chapter": null,
      "content": "We create stacked bar plots from a DataFrame by passing stacked=True, resulting in\nthe value in each row being stacked together horizontally (see Figure 9-17):\nIn [75]: df.plot.barh(stacked=True, alpha=0.5)\nFigure 9-17. DataFrame stacked bar plot\nA useful recipe for bar plots is to visualize a Series’s value frequency\nusing value_counts: s.value_counts().plot.bar().\nLet’s have a look at an example dataset about restaurant tipping. Suppose we wanted\nto make a stacked bar plot showing the percentage of data points for each party size\nfor each day. I load the data using read_csv and make a cross-tabulation by day and\nparty size. The pandas.crosstab function is a convenient way to compute a simple\nfrequency table from two DataFrame columns:\nIn [77]: tips = pd.read_csv(\"examples/tips.csv\")\nIn [78]: tips.head()\nOut[78]: \n   total_bill   tip smoker  day    time  size\n0       16.99  1.01     No  Sun  Dinner     2\n1       10.34  1.66     No  Sun  Dinner     3\n2       21.01  3.50     No  Sun  Dinner     3\n3       23.68  3.31     No  Sun  Dinner     2\n4       24.59  3.61     No  Sun  Dinner     4\n304 \n| \nChapter 9: Plotting and Visualization",
      "content_length": 1148,
      "extraction_method": "Direct"
    },
    {
      "page_number": 323,
      "chapter": null,
      "content": "In [79]: party_counts = pd.crosstab(tips[\"day\"], tips[\"size\"])\nIn [80]: party_counts = party_counts.reindex(index=[\"Thur\", \"Fri\", \"Sat\", \"Sun\"])\nIn [81]: party_counts\nOut[81]: \nsize  1   2   3   4  5  6\nday                      \nThur  1  48   4   5  1  3\nFri   1  16   1   1  0  0\nSat   2  53  18  13  1  0\nSun   0  39  15  18  3  1\nSince there are not many one- and six-person parties, I remove them here:\nIn [82]: party_counts = party_counts.loc[:, 2:5]\nThen, normalize so that each row sums to 1, and make the plot (see Figure 9-18):\n# Normalize to sum to 1\nIn [83]: party_pcts = party_counts.div(party_counts.sum(axis=\"columns\"),\n   ....:                               axis=\"index\")\nIn [84]: party_pcts\nOut[84]: \nsize         2         3         4         5\nday                                         \nThur  0.827586  0.068966  0.086207  0.017241\nFri   0.888889  0.055556  0.055556  0.000000\nSat   0.623529  0.211765  0.152941  0.011765\nSun   0.520000  0.200000  0.240000  0.040000\nIn [85]: party_pcts.plot.bar(stacked=True)\n9.2 Plotting with pandas and seaborn \n| \n305",
      "content_length": 1074,
      "extraction_method": "Direct"
    },
    {
      "page_number": 324,
      "chapter": null,
      "content": "Figure 9-18. Fraction of parties by size within each day\nSo you can see that party sizes appear to increase on the weekend in this dataset.\nWith data that requires aggregation or summarization before making a plot, using the\nseaborn package can make things much simpler (install it with conda install sea\nborn). Let’s look now at the tipping percentage by day with seaborn (see Figure 9-19\nfor the resulting plot):\nIn [87]: import seaborn as sns\nIn [88]: tips[\"tip_pct\"] = tips[\"tip\"] / (tips[\"total_bill\"] - tips[\"tip\"])\nIn [89]: tips.head()\nOut[89]: \n   total_bill   tip smoker  day    time  size   tip_pct\n0       16.99  1.01     No  Sun  Dinner     2  0.063204\n1       10.34  1.66     No  Sun  Dinner     3  0.191244\n2       21.01  3.50     No  Sun  Dinner     3  0.199886\n3       23.68  3.31     No  Sun  Dinner     2  0.162494\n4       24.59  3.61     No  Sun  Dinner     4  0.172069\nIn [90]: sns.barplot(x=\"tip_pct\", y=\"day\", data=tips, orient=\"h\")\n306 \n| \nChapter 9: Plotting and Visualization",
      "content_length": 1000,
      "extraction_method": "Direct"
    },
    {
      "page_number": 325,
      "chapter": null,
      "content": "Figure 9-19. Tipping percentage by day with error bars\nPlotting functions in seaborn take a data argument, which can be a pandas Data‐\nFrame. The other arguments refer to column names. Because there are multiple\nobservations for each value in the day, the bars are the average value of tip_pct.\nThe black lines drawn on the bars represent the 95% confidence interval (this can be\nconfigured through optional arguments).\n9.2 Plotting with pandas and seaborn \n| \n307",
      "content_length": 464,
      "extraction_method": "Direct"
    },
    {
      "page_number": 326,
      "chapter": null,
      "content": "seaborn.barplot has a hue option that enables us to split by an additional categorical\nvalue (see Figure 9-20):\nIn [92]: sns.barplot(x=\"tip_pct\", y=\"day\", hue=\"time\", data=tips, orient=\"h\")\nFigure 9-20. Tipping percentage by day and time\nNotice that seaborn has automatically changed the aesthetics of plots: the default\ncolor palette, plot background, and grid line colors. You can switch between different\nplot appearances using seaborn.set_style:\nIn [94]: sns.set_style(\"whitegrid\")\nWhen producing plots for black-and-white print medium, you may find it useful to\nset a greyscale color palette, like so:\nsns.set_palette(\"Greys_r\")\n308 \n| \nChapter 9: Plotting and Visualization",
      "content_length": 679,
      "extraction_method": "Direct"
    },
    {
      "page_number": 327,
      "chapter": null,
      "content": "Histograms and Density Plots\nA histogram is a kind of bar plot that gives a discretized display of value frequency.\nThe data points are split into discrete, evenly spaced bins, and the number of data\npoints in each bin is plotted. Using the tipping data from before, we can make a\nhistogram of tip percentages of the total bill using the plot.hist method on the\nSeries (see Figure 9-21):\nIn [96]: tips[\"tip_pct\"].plot.hist(bins=50)\nFigure 9-21. Histogram of tip percentages\nA related plot type is a density plot, which is formed by computing an estimate of a\ncontinuous probability distribution that might have generated the observed data. The\nusual procedure is to approximate this distribution as a mixture of “kernels”—that is,\nsimpler distributions like the normal distribution. Thus, density plots are also known\nas kernel density estimate (KDE) plots. Using plot.density makes a density plot\nusing the conventional mixture-of-normals estimate (see Figure 9-22):\nIn [98]: tips[\"tip_pct\"].plot.density()\n9.2 Plotting with pandas and seaborn \n| \n309",
      "content_length": 1052,
      "extraction_method": "Direct"
    },
    {
      "page_number": 328,
      "chapter": null,
      "content": "Figure 9-22. Density plot of tip percentages\nThis kind of plot requires SciPy, so if you do not have it installed already, you can\npause and do that now:\nconda install scipy\nseaborn makes histograms and density plots even easier through its histplot\nmethod, which can plot both a histogram and a continuous density estimate simulta‐\nneously. As an example, consider a bimodal distribution consisting of draws from\ntwo different standard normal distributions (see Figure 9-23):\nIn [100]: comp1 = np.random.standard_normal(200)\nIn [101]: comp2 = 10 + 2 * np.random.standard_normal(200)\nIn [102]: values = pd.Series(np.concatenate([comp1, comp2]))\nIn [103]: sns.histplot(values, bins=100, color=\"black\")\n310 \n| \nChapter 9: Plotting and Visualization",
      "content_length": 746,
      "extraction_method": "Direct"
    },
    {
      "page_number": 329,
      "chapter": null,
      "content": "Figure 9-23. Normalized histogram of normal mixture\nScatter or Point Plots\nPoint plots or scatter plots can be a useful way of examining the relationship between\ntwo one-dimensional data series. For example, here we load the macrodata dataset\nfrom the statsmodels project, select a few variables, then compute log differences:\nIn [104]: macro = pd.read_csv(\"examples/macrodata.csv\")\nIn [105]: data = macro[[\"cpi\", \"m1\", \"tbilrate\", \"unemp\"]]\nIn [106]: trans_data = np.log(data).diff().dropna()\nIn [107]: trans_data.tail()\nOut[107]: \n          cpi        m1  tbilrate     unemp\n198 -0.007904  0.045361 -0.396881  0.105361\n199 -0.021979  0.066753 -2.277267  0.139762\n200  0.002340  0.010286  0.606136  0.160343\n201  0.008419  0.037461 -0.200671  0.127339\n202  0.008894  0.012202 -0.405465  0.042560\n9.2 Plotting with pandas and seaborn \n| \n311",
      "content_length": 841,
      "extraction_method": "Direct"
    },
    {
      "page_number": 330,
      "chapter": null,
      "content": "We can then use seaborn’s regplot method, which makes a scatter plot and fits a\nlinear regression line (see Figure 9-24):\nIn [109]: ax = sns.regplot(x=\"m1\", y=\"unemp\", data=trans_data)\nIn [110]: ax.title(\"Changes in log(m1) versus log(unemp)\")\nFigure 9-24. A seaborn regression/scatter plot\nIn exploratory data analysis, it’s helpful to be able to look at all the scatter plots\namong a group of variables; this is known as a pairs plot or scatter plot matrix. Mak‐\ning such a plot from scratch is a bit of work, so seaborn has a convenient pairplot\nfunction that supports placing histograms or density estimates of each variable along\nthe diagonal (see Figure 9-25 for the resulting plot):\nIn [111]: sns.pairplot(trans_data, diag_kind=\"kde\", plot_kws={\"alpha\": 0.2})\n312 \n| \nChapter 9: Plotting and Visualization",
      "content_length": 812,
      "extraction_method": "Direct"
    },
    {
      "page_number": 331,
      "chapter": null,
      "content": "Figure 9-25. Pair plot matrix of statsmodels macro data\nYou may notice the plot_kws argument. This enables us to pass down configuration\noptions to the individual plotting calls on the off-diagonal elements. Check out the\nseaborn.pairplot docstring for more granular configuration options.\n9.2 Plotting with pandas and seaborn \n| \n313",
      "content_length": 334,
      "extraction_method": "Direct"
    },
    {
      "page_number": 332,
      "chapter": null,
      "content": "Facet Grids and Categorical Data\nWhat about datasets where we have additional grouping dimensions? One way to\nvisualize data with many categorical variables is to use a facet grid, which is a two-\ndimensional layout of plots where the data is split across the plots on each axis based\non the distinct values of a certain variable. seaborn has a useful built-in function cat\nplot that simplifies making many kinds of faceted plots split by categorical variables\n(see Figure 9-26 for the resulting plot):\nIn [112]: sns.catplot(x=\"day\", y=\"tip_pct\", hue=\"time\", col=\"smoker\",\n   .....:             kind=\"bar\", data=tips[tips.tip_pct < 1])\nFigure 9-26. Tipping percentage by day/time/smoker\nInstead of grouping by \"time\" by different bar colors within a facet, we can also\nexpand the facet grid by adding one row per time value (see Figure 9-27):\nIn [113]: sns.catplot(x=\"day\", y=\"tip_pct\", row=\"time\",\n   .....:             col=\"smoker\",\n   .....:             kind=\"bar\", data=tips[tips.tip_pct < 1])\n314 \n| \nChapter 9: Plotting and Visualization",
      "content_length": 1043,
      "extraction_method": "Direct"
    },
    {
      "page_number": 333,
      "chapter": null,
      "content": "Figure 9-27. Tipping percentage by day split by time/smoker\ncatplot supports other plot types that may be useful depending on what you are\ntrying to display. For example, box plots (which show the median, quartiles, and\noutliers) can be an effective visualization type (see Figure 9-28):\nIn [114]: sns.catplot(x=\"tip_pct\", y=\"day\", kind=\"box\",\n   .....:             data=tips[tips.tip_pct < 0.5])\n9.2 Plotting with pandas and seaborn \n| \n315",
      "content_length": 441,
      "extraction_method": "Direct"
    },
    {
      "page_number": 334,
      "chapter": null,
      "content": "Figure 9-28. Box plot of tipping percentage by day\nYou can create your own facet grid plots using the more general seaborn.FacetGrid\nclass. See the seaborn documentation for more.\n316 \n| \nChapter 9: Plotting and Visualization",
      "content_length": 225,
      "extraction_method": "Direct"
    },
    {
      "page_number": 335,
      "chapter": null,
      "content": "9.3 Other Python Visualization Tools\nAs is common with open source, there many options for creating graphics in Python\n(too many to list). Since 2010, much development effort has been focused on creating\ninteractive graphics for publication on the web. With tools like Altair, Bokeh, and\nPlotly, it’s now possible to specify dynamic, interactive graphics in Python that are\nintended for use with web browsers.\nFor creating static graphics for print or web, I recommend using matplotlib and\nlibraries that build on matplotlib, like pandas and seaborn, for your needs. For other\ndata visualization requirements, it may be useful to learn how to use one of the other\navailable tools. I encourage you to explore the ecosystem as it continues to evolve and\ninnovate into the future.\nAn excellent book on data visualization is Fundamentals of Data Visualization by\nClaus O. Wilke (O’Reilly), which is available in print or on Claus’s website at https://\nclauswilke.com/dataviz.\n9.4 Conclusion\nThe goal of this chapter was to get your feet wet with some basic data visualization\nusing pandas, matplotlib, and seaborn. If visually communicating the results of data\nanalysis is important in your work, I encourage you to seek out resources to learn\nmore about effective data visualization. It is an active field of research, and you can\npractice with many excellent learning resources available online and in print.\nIn the next chapter, we turn our attention to data aggregation and group operations\nwith pandas.\n9.3 Other Python Visualization Tools \n| \n317",
      "content_length": 1548,
      "extraction_method": "Direct"
    },
    {
      "page_number": 336,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 337,
      "chapter": null,
      "content": "CHAPTER 10\nData Aggregation and Group Operations\nCategorizing a dataset and applying a function to each group, whether an aggregation\nor transformation, can be a critical component of a data analysis workflow. After\nloading, merging, and preparing a dataset, you may need to compute group statistics\nor possibly pivot tables for reporting or visualization purposes. pandas provides a\nversatile groupby interface, enabling you to slice, dice, and summarize datasets in a\nnatural way.\nOne reason for the popularity of relational databases and SQL (which stands for\n“structured query language”) is the ease with which data can be joined, filtered,\ntransformed, and aggregated. However, query languages like SQL impose certain\nlimitations on the kinds of group operations that can be performed. As you will see,\nwith the expressiveness of Python and pandas, we can perform quite complex group\noperations by expressing them as custom Python functions that manipulate the data\nassociated with each group. In this chapter, you will learn how to:\n• Split a pandas object into pieces using one or more keys (in the form of func‐\n•\ntions, arrays, or DataFrame column names)\n• Calculate group summary statistics, like count, mean, or standard deviation, or a\n•\nuser-defined function\n• Apply within-group transformations or other manipulations, like normalization,\n•\nlinear regression, rank, or subset selection\n• Compute pivot tables and cross-tabulations\n•\n• Perform quantile analysis and other statistical group analyses\n•\n319",
      "content_length": 1517,
      "extraction_method": "Direct"
    },
    {
      "page_number": 338,
      "chapter": null,
      "content": "Time-based aggregation of time series data, a special use case of\ngroupby, is referred to as resampling in this book and will receive\nseparate treatment in Chapter 11.\nAs with the rest of the chapters, we start by importing NumPy and pandas:\nIn [12]: import numpy as np\nIn [13]: import pandas as pd\n10.1 How to Think About Group Operations\nHadley Wickham, an author of many popular packages for the R programming\nlanguage, coined the term split-apply-combine for describing group operations. In the\nfirst stage of the process, data contained in a pandas object, whether a Series, Data‐\nFrame, or otherwise, is split into groups based on one or more keys that you provide.\nThe splitting is performed on a particular axis of an object. For example, a DataFrame\ncan be grouped on its rows (axis=\"index\") or its columns (axis=\"columns\"). Once\nthis is done, a function is applied to each group, producing a new value. Finally,\nthe results of all those function applications are combined into a result object. The\nform of the resulting object will usually depend on what’s being done to the data. See\nFigure 10-1 for a mockup of a simple group aggregation.\nEach grouping key can take many forms, and the keys do not have to be all of the\nsame type:\n• A list or array of values that is the same length as the axis being grouped\n•\n• A value indicating a column name in a DataFrame\n•\n• A dictionary or Series giving a correspondence between the values on the axis\n•\nbeing grouped and the group names\n• A function to be invoked on the axis index or the individual labels in the index\n•\n320 \n| \nChapter 10: Data Aggregation and Group Operations",
      "content_length": 1633,
      "extraction_method": "Direct"
    },
    {
      "page_number": 339,
      "chapter": null,
      "content": "Figure 10-1. Illustration of a group aggregation\nNote that the latter three methods are shortcuts for producing an array of values to\nbe used to split up the object. Don’t worry if this all seems abstract. Throughout this\nchapter, I will give many examples of all these methods. To get started, here is a small\ntabular dataset as a DataFrame:\nIn [14]: df = pd.DataFrame({\"key1\" : [\"a\", \"a\", None, \"b\", \"b\", \"a\", None],\n   ....:                    \"key2\" : pd.Series([1, 2, 1, 2, 1, None, 1], dtype=\"I\nnt64\"),\n   ....:                    \"data1\" : np.random.standard_normal(7),\n   ....:                    \"data2\" : np.random.standard_normal(7)})\nIn [15]: df\nOut[15]: \n   key1  key2     data1     data2\n0     a     1 -0.204708  0.281746\n1     a     2  0.478943  0.769023\n2  None     1 -0.519439  1.246435\n3     b     2 -0.555730  1.007189\n4     b     1  1.965781 -1.296221\n5     a  <NA>  1.393406  0.274992\n6  None     1  0.092908  0.228913\n10.1 How to Think About Group Operations \n| \n321",
      "content_length": 988,
      "extraction_method": "Direct"
    },
    {
      "page_number": 340,
      "chapter": null,
      "content": "Suppose you wanted to compute the mean of the data1 column using the labels from\nkey1. There are a number of ways to do this. One is to access data1 and call groupby\nwith the column (a Series) at key1:\nIn [16]: grouped = df[\"data1\"].groupby(df[\"key1\"])\nIn [17]: grouped\nOut[17]: <pandas.core.groupby.generic.SeriesGroupBy object at 0x7fa9270e0a00>\nThis grouped variable is now a special “GroupBy” object. It has not actually computed\nanything yet except for some intermediate data about the group key df[\"key1\"].\nThe idea is that this object has all of the information needed to then apply some\noperation to each of the groups. For example, to compute group means we can call\nthe GroupBy’s mean method:\nIn [18]: grouped.mean()\nOut[18]: \nkey1\na    0.555881\nb    0.705025\nName: data1, dtype: float64\nLater in Section 10.2, “Data Aggregation,” on page 329, I’ll explain more about what\nhappens when you call .mean(). The important thing here is that the data (a Series)\nhas been aggregated by splitting the data on the group key, producing a new Series\nthat is now indexed by the unique values in the key1 column. The result index has the\nname \"key1\" because the DataFrame column df[\"key1\"] did.\nIf instead we had passed multiple arrays as a list, we’d get something different:\nIn [19]: means = df[\"data1\"].groupby([df[\"key1\"], df[\"key2\"]]).mean()\nIn [20]: means\nOut[20]: \nkey1  key2\na     1      -0.204708\n      2       0.478943\nb     1       1.965781\n      2      -0.555730\nName: data1, dtype: float64\nHere we grouped the data using two keys, and the resulting Series now has a hier‐\narchical index consisting of the unique pairs of keys observed:\nIn [21]: means.unstack()\nOut[21]: \nkey2         1         2\nkey1                    \na    -0.204708  0.478943\nb     1.965781 -0.555730\n322 \n| \nChapter 10: Data Aggregation and Group Operations",
      "content_length": 1839,
      "extraction_method": "Direct"
    },
    {
      "page_number": 341,
      "chapter": null,
      "content": "In this example, the group keys are all Series, though they could be any arrays of the\nright length:\nIn [22]: states = np.array([\"OH\", \"CA\", \"CA\", \"OH\", \"OH\", \"CA\", \"OH\"])\nIn [23]: years = [2005, 2005, 2006, 2005, 2006, 2005, 2006]\nIn [24]: df[\"data1\"].groupby([states, years]).mean()\nOut[24]: \nCA  2005    0.936175\n    2006   -0.519439\nOH  2005   -0.380219\n    2006    1.029344\nName: data1, dtype: float64\nFrequently, the grouping information is found in the same DataFrame as the data you\nwant to work on. In that case, you can pass column names (whether those are strings,\nnumbers, or other Python objects) as the group keys:\nIn [25]: df.groupby(\"key1\").mean()\nOut[25]: \n      key2     data1     data2\nkey1                          \na      1.5  0.555881  0.441920\nb      1.5  0.705025 -0.144516\nIn [26]: df.groupby(\"key2\").mean()\nOut[26]: \n         data1     data2\nkey2                    \n1     0.333636  0.115218\n2    -0.038393  0.888106\nIn [27]: df.groupby([\"key1\", \"key2\"]).mean()\nOut[27]: \n              data1     data2\nkey1 key2                    \na    1    -0.204708  0.281746\n     2     0.478943  0.769023\nb    1     1.965781 -1.296221\n     2    -0.555730  1.007189\nYou may have noticed in the second case, df.groupby(\"key2\").mean(), that there is\nno key1 column in the result. Because df[\"key1\"] is not numeric data, it is said to\nbe a nuisance column, which is therefore automatically excluded from the result. By\ndefault, all of the numeric columns are aggregated, though it is possible to filter down\nto a subset, as you’ll see soon.\nRegardless of the objective in using groupby, a generally useful GroupBy method is\nsize, which returns a Series containing group sizes:\n10.1 How to Think About Group Operations \n| \n323",
      "content_length": 1734,
      "extraction_method": "Direct"
    },
    {
      "page_number": 342,
      "chapter": null,
      "content": "In [28]: df.groupby([\"key1\", \"key2\"]).size()\nOut[28]: \nkey1  key2\na     1       1\n      2       1\nb     1       1\n      2       1\ndtype: int64\nNote that any missing values in a group key are excluded from the result by default.\nThis behavior can be disabled by passing dropna=False to groupby:\nIn [29]: df.groupby(\"key1\", dropna=False).size()\nOut[29]: \nkey1\na      3\nb      2\nNaN    2\ndtype: int64\nIn [30]: df.groupby([\"key1\", \"key2\"], dropna=False).size()\nOut[30]: \nkey1  key2\na     1       1\n      2       1\n      <NA>    1\nb     1       1\n      2       1\nNaN   1       2\ndtype: int64\nA group function similar in spirit to size is count, which computes the number of\nnonnull values in each group:\nIn [31]: df.groupby(\"key1\").count()\nOut[31]: \n      key2  data1  data2\nkey1                    \na        2      3      3\nb        2      2      2\nIterating over Groups\nThe object returned by groupby supports iteration, generating a sequence of 2-tuples\ncontaining the group name along with the chunk of data. Consider the following:\nIn [32]: for name, group in df.groupby(\"key1\"):\n   ....:     print(name)\n   ....:     print(group)\n   ....:\na\n  key1  key2     data1     data2\n0    a     1 -0.204708  0.281746\n324 \n| \nChapter 10: Data Aggregation and Group Operations",
      "content_length": 1265,
      "extraction_method": "Direct"
    },
    {
      "page_number": 343,
      "chapter": null,
      "content": "1    a     2  0.478943  0.769023\n5    a  <NA>  1.393406  0.274992\nb\n  key1  key2     data1     data2\n3    b     2 -0.555730  1.007189\n4    b     1  1.965781 -1.296221\nIn the case of multiple keys, the first element in the tuple will be a tuple of key values:\nIn [33]: for (k1, k2), group in df.groupby([\"key1\", \"key2\"]):\n   ....:     print((k1, k2))\n   ....:     print(group)\n   ....:\n('a', 1)\n  key1  key2     data1     data2\n0    a     1 -0.204708  0.281746\n('a', 2)\n  key1  key2     data1     data2\n1    a     2  0.478943  0.769023\n('b', 1)\n  key1  key2     data1     data2\n4    b     1  1.965781 -1.296221\n('b', 2)\n  key1  key2    data1     data2\n3    b     2 -0.55573  1.007189\nOf course, you can choose to do whatever you want with the pieces of data. A recipe\nyou may find useful is computing a dictionary of the data pieces as a one-liner:\nIn [34]: pieces = {name: group for name, group in df.groupby(\"key1\")}\nIn [35]: pieces[\"b\"]\nOut[35]: \n  key1  key2     data1     data2\n3    b     2 -0.555730  1.007189\n4    b     1  1.965781 -1.296221\nBy default groupby groups on axis=\"index\", but you can group on any of the other\naxes. For example, we could group the columns of our example df here by whether\nthey start with \"key\" or \"data\":\nIn [36]: grouped = df.groupby({\"key1\": \"key\", \"key2\": \"key\",\n   ....:                       \"data1\": \"data\", \"data2\": \"data\"}, axis=\"columns\")\nWe can print out the groups like so:\nIn [37]: for group_key, group_values in grouped:\n   ....:     print(group_key)\n   ....:     print(group_values)\n   ....:\ndata\n      data1     data2\n0 -0.204708  0.281746\n1  0.478943  0.769023\n10.1 How to Think About Group Operations \n| \n325",
      "content_length": 1662,
      "extraction_method": "Direct"
    },
    {
      "page_number": 344,
      "chapter": null,
      "content": "2 -0.519439  1.246435\n3 -0.555730  1.007189\n4  1.965781 -1.296221\n5  1.393406  0.274992\n6  0.092908  0.228913\nkey\n   key1  key2\n0     a     1\n1     a     2\n2  None     1\n3     b     2\n4     b     1\n5     a  <NA>\n6  None     1\nSelecting a Column or Subset of Columns\nIndexing a GroupBy object created from a DataFrame with a column name or array\nof column names has the effect of column subsetting for aggregation. This means\nthat:\ndf.groupby(\"key1\")[\"data1\"]\ndf.groupby(\"key1\")[[\"data2\"]]\nare conveniences for:\ndf[\"data1\"].groupby(df[\"key1\"])\ndf[[\"data2\"]].groupby(df[\"key1\"])\nEspecially for large datasets, it may be desirable to aggregate only a few columns. For\nexample, in the preceding dataset, to compute the means for just the data2 column\nand get the result as a DataFrame, we could write:\nIn [38]: df.groupby([\"key1\", \"key2\"])[[\"data2\"]].mean()\nOut[38]: \n              data2\nkey1 key2          \na    1     0.281746\n     2     0.769023\nb    1    -1.296221\n     2     1.007189\nThe object returned by this indexing operation is a grouped DataFrame if a list or\narray is passed, or a grouped Series if only a single column name is passed as a scalar:\nIn [39]: s_grouped = df.groupby([\"key1\", \"key2\"])[\"data2\"]\nIn [40]: s_grouped\nOut[40]: <pandas.core.groupby.generic.SeriesGroupBy object at 0x7fa9270e3520>\nIn [41]: s_grouped.mean()\nOut[41]: \nkey1  key2\n326 \n| \nChapter 10: Data Aggregation and Group Operations",
      "content_length": 1416,
      "extraction_method": "Direct"
    },
    {
      "page_number": 345,
      "chapter": null,
      "content": "a     1       0.281746\n      2       0.769023\nb     1      -1.296221\n      2       1.007189\nName: data2, dtype: float64\nGrouping with Dictionaries and Series\nGrouping information may exist in a form other than an array. Let’s consider another\nexample DataFrame:\nIn [42]: people = pd.DataFrame(np.random.standard_normal((5, 5)),\n   ....:                       columns=[\"a\", \"b\", \"c\", \"d\", \"e\"],\n   ....:                       index=[\"Joe\", \"Steve\", \"Wanda\", \"Jill\", \"Trey\"])\nIn [43]: people.iloc[2:3, [1, 2]] = np.nan # Add a few NA values\nIn [44]: people\nOut[44]: \n              a         b         c         d         e\nJoe    1.352917  0.886429 -2.001637 -0.371843  1.669025\nSteve -0.438570 -0.539741  0.476985  3.248944 -1.021228\nWanda -0.577087       NaN       NaN  0.523772  0.000940\nJill   1.343810 -0.713544 -0.831154 -2.370232 -1.860761\nTrey  -0.860757  0.560145 -1.265934  0.119827 -1.063512\nNow, suppose I have a group correspondence for the columns and want to sum the\ncolumns by group:\nIn [45]: mapping = {\"a\": \"red\", \"b\": \"red\", \"c\": \"blue\",\n   ....:            \"d\": \"blue\", \"e\": \"red\", \"f\" : \"orange\"}\nNow, you could construct an array from this dictionary to pass to groupby, but\ninstead we can just pass the dictionary (I included the key \"f\" to highlight that\nunused grouping keys are OK):\nIn [46]: by_column = people.groupby(mapping, axis=\"columns\")\nIn [47]: by_column.sum()\nOut[47]: \n           blue       red\nJoe   -2.373480  3.908371\nSteve  3.725929 -1.999539\nWanda  0.523772 -0.576147\nJill  -3.201385 -1.230495\nTrey  -1.146107 -1.364125\nThe same functionality holds for Series, which can be viewed as a fixed-size mapping:\nIn [48]: map_series = pd.Series(mapping)\nIn [49]: map_series\nOut[49]: \n10.1 How to Think About Group Operations \n| \n327",
      "content_length": 1764,
      "extraction_method": "Direct"
    },
    {
      "page_number": 346,
      "chapter": null,
      "content": "a       red\nb       red\nc      blue\nd      blue\ne       red\nf    orange\ndtype: object\nIn [50]: people.groupby(map_series, axis=\"columns\").count()\nOut[50]: \n       blue  red\nJoe       2    3\nSteve     2    3\nWanda     1    2\nJill      2    3\nTrey      2    3\nGrouping with Functions\nUsing Python functions is a more generic way of defining a group mapping compared\nwith a dictionary or Series. Any function passed as a group key will be called once\nper index value (or once per column value if using axis=\"columns\"), with the\nreturn values being used as the group names. More concretely, consider the example\nDataFrame from the previous section, which has people’s first names as index values.\nSuppose you wanted to group by name length. While you could compute an array of\nstring lengths, it’s simpler to just pass the len function:\nIn [51]: people.groupby(len).sum()\nOut[51]: \n          a         b         c         d         e\n3  1.352917  0.886429 -2.001637 -0.371843  1.669025\n4  0.483052 -0.153399 -2.097088 -2.250405 -2.924273\n5 -1.015657 -0.539741  0.476985  3.772716 -1.020287\nMixing functions with arrays, dictionaries, or Series is not a problem, as everything\ngets converted to arrays internally:\nIn [52]: key_list = [\"one\", \"one\", \"one\", \"two\", \"two\"]\nIn [53]: people.groupby([len, key_list]).min()\nOut[53]: \n              a         b         c         d         e\n3 one  1.352917  0.886429 -2.001637 -0.371843  1.669025\n4 two -0.860757 -0.713544 -1.265934 -2.370232 -1.860761\n5 one -0.577087 -0.539741  0.476985  0.523772 -1.021228\nGrouping by Index Levels\nA final convenience for hierarchically indexed datasets is the ability to aggregate\nusing one of the levels of an axis index. Let’s look at an example:\n328 \n| \nChapter 10: Data Aggregation and Group Operations",
      "content_length": 1780,
      "extraction_method": "Direct"
    },
    {
      "page_number": 347,
      "chapter": null,
      "content": "In [54]: columns = pd.MultiIndex.from_arrays([[\"US\", \"US\", \"US\", \"JP\", \"JP\"],\n   ....:                                     [1, 3, 5, 1, 3]],\n   ....:                                     names=[\"cty\", \"tenor\"])\nIn [55]: hier_df = pd.DataFrame(np.random.standard_normal((4, 5)), columns=column\ns)\nIn [56]: hier_df\nOut[56]: \ncty          US                            JP          \ntenor         1         3         5         1         3\n0      0.332883 -2.359419 -0.199543 -1.541996 -0.970736\n1     -1.307030  0.286350  0.377984 -0.753887  0.331286\n2      1.349742  0.069877  0.246674 -0.011862  1.004812\n3      1.327195 -0.919262 -1.549106  0.022185  0.758363\nTo group by level, pass the level number or name using the level keyword:\nIn [57]: hier_df.groupby(level=\"cty\", axis=\"columns\").count()\nOut[57]: \ncty  JP  US\n0     2   3\n1     2   3\n2     2   3\n3     2   3\n10.2 Data Aggregation\nAggregations refer to any data transformation that produces scalar values from arrays.\nThe preceding examples have used several of them, including mean, count, min, and\nsum. You may wonder what is going on when you invoke mean() on a GroupBy\nobject. Many common aggregations, such as those found in Table 10-1, have opti‐\nmized implementations. However, you are not limited to only this set of methods.\nTable 10-1. Optimized groupby methods\nFunction name\nDescription\nany, all\nReturn True if any (one or more values) or all non-NA values are “truthy”\ncount\nNumber of non-NA values\ncummin, cummax Cumulative minimum and maximum of non-NA values\ncumsum\nCumulative sum of non-NA values\ncumprod\nCumulative product of non-NA values\nfirst, last\nFirst and last non-NA values\nmean\nMean of non-NA values\nmedian\nArithmetic median of non-NA values\nmin, max\nMinimum and maximum of non-NA values\nnth\nRetrieve value that would appear at position n with the data in sorted order\nohlc\nCompute four “open-high-low-close” statistics for time series-like data\n10.2 Data Aggregation \n| \n329",
      "content_length": 1955,
      "extraction_method": "Direct"
    },
    {
      "page_number": 348,
      "chapter": null,
      "content": "Function name\nDescription\nprod\nProduct of non-NA values\nquantile\nCompute sample quantile\nrank\nOrdinal ranks of non-NA values, like calling Series.rank\nsize\nCompute group sizes, returning result as a Series\nsum\nSum of non-NA values\nstd, var\nSample standard deviation and variance\nYou can use aggregations of your own devising and additionally call any method\nthat is also defined on the object being grouped. For example, the nsmallest\nSeries method selects the smallest requested number of values from the data.\nWhile nsmallest is not explicitly implemented for GroupBy, we can still use it\nwith a nonoptimized implementation. Internally, GroupBy slices up the Series, calls\npiece.nsmallest(n) for each piece, and then assembles those results into the result\nobject:\nIn [58]: df\nOut[58]: \n   key1  key2     data1     data2\n0     a     1 -0.204708  0.281746\n1     a     2  0.478943  0.769023\n2  None     1 -0.519439  1.246435\n3     b     2 -0.555730  1.007189\n4     b     1  1.965781 -1.296221\n5     a  <NA>  1.393406  0.274992\n6  None     1  0.092908  0.228913\nIn [59]: grouped = df.groupby(\"key1\")\nIn [60]: grouped[\"data1\"].nsmallest(2)\nOut[60]: \nkey1   \na     0   -0.204708\n      1    0.478943\nb     3   -0.555730\n      4    1.965781\nName: data1, dtype: float64\nTo use your own aggregation functions, pass any function that aggregates an array to\nthe aggregate method or its short alias agg:\nIn [61]: def peak_to_peak(arr):\n   ....:     return arr.max() - arr.min()\nIn [62]: grouped.agg(peak_to_peak)\nOut[62]: \n      key2     data1     data2\nkey1                          \n330 \n| \nChapter 10: Data Aggregation and Group Operations",
      "content_length": 1632,
      "extraction_method": "Direct"
    },
    {
      "page_number": 349,
      "chapter": null,
      "content": "a        1  1.598113  0.494031\nb        1  2.521511  2.303410\nYou may notice that some methods, like describe, also work, even though they are\nnot aggregations, strictly speaking:\nIn [63]: grouped.describe()\nOut[63]: \n      key2                                           data1            ...  \\\n     count mean       std  min   25%  50%   75%  max count      mean  ...   \nkey1                                                                  ...   \na      2.0  1.5  0.707107  1.0  1.25  1.5  1.75  2.0   3.0  0.555881  ...   \nb      2.0  1.5  0.707107  1.0  1.25  1.5  1.75  2.0   2.0  0.705025  ...   \n                         data2                                          \\\n           75%       max count      mean       std       min       25%   \nkey1                                                                     \na     0.936175  1.393406   3.0  0.441920  0.283299  0.274992  0.278369   \nb     1.335403  1.965781   2.0 -0.144516  1.628757 -1.296221 -0.720368   \n                                    \n           50%       75%       max  \nkey1                                \na     0.281746  0.525384  0.769023  \nb    -0.144516  0.431337  1.007189  \n[2 rows x 24 columns]\nI will explain in more detail what has happened here in Section 10.3, “Apply: General\nsplit-apply-combine,” on page 335.\nCustom aggregation functions are generally much slower than the\noptimized functions found in Table 10-1. This is because there\nis some extra overhead (function calls, data rearrangement) in\nconstructing the intermediate group data chunks.\nColumn-Wise and Multiple Function Application\nLet’s return to the tipping dataset used in the last chapter. After loading it with\npandas.read_csv, we add a tipping percentage column:\nIn [64]: tips = pd.read_csv(\"examples/tips.csv\")\nIn [65]: tips.head()\nOut[65]: \n   total_bill   tip smoker  day    time  size\n0       16.99  1.01     No  Sun  Dinner     2\n1       10.34  1.66     No  Sun  Dinner     3\n2       21.01  3.50     No  Sun  Dinner     3\n3       23.68  3.31     No  Sun  Dinner     2\n4       24.59  3.61     No  Sun  Dinner     4\n10.2 Data Aggregation \n| \n331",
      "content_length": 2108,
      "extraction_method": "Direct"
    },
    {
      "page_number": 350,
      "chapter": null,
      "content": "Now I will add a tip_pct column with the tip percentage of the total bill:\nIn [66]: tips[\"tip_pct\"] = tips[\"tip\"] / tips[\"total_bill\"]\nIn [67]: tips.head()\nOut[67]: \n   total_bill   tip smoker  day    time  size   tip_pct\n0       16.99  1.01     No  Sun  Dinner     2  0.059447\n1       10.34  1.66     No  Sun  Dinner     3  0.160542\n2       21.01  3.50     No  Sun  Dinner     3  0.166587\n3       23.68  3.31     No  Sun  Dinner     2  0.139780\n4       24.59  3.61     No  Sun  Dinner     4  0.146808\nAs you’ve already seen, aggregating a Series or all of the columns of a DataFrame is\na matter of using aggregate (or agg) with the desired function or calling a method\nlike mean or std. However, you may want to aggregate using a different function,\ndepending on the column, or multiple functions at once. Fortunately, this is possible\nto do, which I’ll illustrate through a number of examples. First, I’ll group the tips by\nday and smoker:\nIn [68]: grouped = tips.groupby([\"day\", \"smoker\"])\nNote that for descriptive statistics like those in Table 10-1, you can pass the name of\nthe function as a string:\nIn [69]: grouped_pct = grouped[\"tip_pct\"]\nIn [70]: grouped_pct.agg(\"mean\")\nOut[70]: \nday   smoker\nFri   No        0.151650\n      Yes       0.174783\nSat   No        0.158048\n      Yes       0.147906\nSun   No        0.160113\n      Yes       0.187250\nThur  No        0.160298\n      Yes       0.163863\nName: tip_pct, dtype: float64\nIf you pass a list of functions or function names instead, you get back a DataFrame\nwith column names taken from the functions:\nIn [71]: grouped_pct.agg([\"mean\", \"std\", peak_to_peak])\nOut[71]: \n                 mean       std  peak_to_peak\nday  smoker                                  \nFri  No      0.151650  0.028123      0.067349\n     Yes     0.174783  0.051293      0.159925\nSat  No      0.158048  0.039767      0.235193\n     Yes     0.147906  0.061375      0.290095\nSun  No      0.160113  0.042347      0.193226\n332 \n| \nChapter 10: Data Aggregation and Group Operations",
      "content_length": 2008,
      "extraction_method": "Direct"
    },
    {
      "page_number": 351,
      "chapter": null,
      "content": "Yes     0.187250  0.154134      0.644685\nThur No      0.160298  0.038774      0.193350\n     Yes     0.163863  0.039389      0.151240\nHere we passed a list of aggregation functions to agg to evaluate independently on the\ndata groups.\nYou don’t need to accept the names that GroupBy gives to the columns; notably,\nlambda functions have the name \"<lambda>\", which makes them hard to identify\n(you can see for yourself by looking at a function’s __name__ attribute). Thus, if you\npass a list of (name, function) tuples, the first element of each tuple will be used\nas the DataFrame column names (you can think of a list of 2-tuples as an ordered\nmapping):\nIn [72]: grouped_pct.agg([(\"average\", \"mean\"), (\"stdev\", np.std)])\nOut[72]: \n              average     stdev\nday  smoker                    \nFri  No      0.151650  0.028123\n     Yes     0.174783  0.051293\nSat  No      0.158048  0.039767\n     Yes     0.147906  0.061375\nSun  No      0.160113  0.042347\n     Yes     0.187250  0.154134\nThur No      0.160298  0.038774\n     Yes     0.163863  0.039389\nWith a DataFrame you have more options, as you can specify a list of functions\nto apply to all of the columns or different functions per column. To start, suppose\nwe wanted to compute the same three statistics for the tip_pct and total_bill\ncolumns:\nIn [73]: functions = [\"count\", \"mean\", \"max\"]\nIn [74]: result = grouped[[\"tip_pct\", \"total_bill\"]].agg(functions)\nIn [75]: result\nOut[75]: \n            tip_pct                     total_bill                  \n              count      mean       max      count       mean    max\nday  smoker                                                         \nFri  No           4  0.151650  0.187735          4  18.420000  22.75\n     Yes         15  0.174783  0.263480         15  16.813333  40.17\nSat  No          45  0.158048  0.291990         45  19.661778  48.33\n     Yes         42  0.147906  0.325733         42  21.276667  50.81\nSun  No          57  0.160113  0.252672         57  20.506667  48.17\n     Yes         19  0.187250  0.710345         19  24.120000  45.35\nThur No          45  0.160298  0.266312         45  17.113111  41.19\n     Yes         17  0.163863  0.241255         17  19.190588  43.11\n10.2 Data Aggregation \n| \n333",
      "content_length": 2227,
      "extraction_method": "Direct"
    },
    {
      "page_number": 352,
      "chapter": null,
      "content": "As you can see, the resulting DataFrame has hierarchical columns, the same as you\nwould get aggregating each column separately and using concat to glue the results\ntogether using the column names as the keys argument:\nIn [76]: result[\"tip_pct\"]\nOut[76]: \n             count      mean       max\nday  smoker                           \nFri  No          4  0.151650  0.187735\n     Yes        15  0.174783  0.263480\nSat  No         45  0.158048  0.291990\n     Yes        42  0.147906  0.325733\nSun  No         57  0.160113  0.252672\n     Yes        19  0.187250  0.710345\nThur No         45  0.160298  0.266312\n     Yes        17  0.163863  0.241255\nAs before, a list of tuples with custom names can be passed:\nIn [77]: ftuples = [(\"Average\", \"mean\"), (\"Variance\", np.var)]\nIn [78]: grouped[[\"tip_pct\", \"total_bill\"]].agg(ftuples)\nOut[78]: \n              tip_pct           total_bill            \n              Average  Variance    Average    Variance\nday  smoker                                           \nFri  No      0.151650  0.000791  18.420000   25.596333\n     Yes     0.174783  0.002631  16.813333   82.562438\nSat  No      0.158048  0.001581  19.661778   79.908965\n     Yes     0.147906  0.003767  21.276667  101.387535\nSun  No      0.160113  0.001793  20.506667   66.099980\n     Yes     0.187250  0.023757  24.120000  109.046044\nThur No      0.160298  0.001503  17.113111   59.625081\n     Yes     0.163863  0.001551  19.190588   69.808518\nNow, suppose you wanted to apply potentially different functions to one or more of\nthe columns. To do this, pass a dictionary to agg that contains a mapping of column\nnames to any of the function specifications listed so far:\nIn [79]: grouped.agg({\"tip\" : np.max, \"size\" : \"sum\"})\nOut[79]: \n               tip  size\nday  smoker             \nFri  No       3.50     9\n     Yes      4.73    31\nSat  No       9.00   115\n     Yes     10.00   104\nSun  No       6.00   167\n     Yes      6.50    49\nThur No       6.70   112\n     Yes      5.00    40\nIn [80]: grouped.agg({\"tip_pct\" : [\"min\", \"max\", \"mean\", \"std\"],\n334 \n| \nChapter 10: Data Aggregation and Group Operations",
      "content_length": 2104,
      "extraction_method": "Direct"
    },
    {
      "page_number": 353,
      "chapter": null,
      "content": "....:              \"size\" : \"sum\"})\nOut[80]: \n              tip_pct                               size\n                  min       max      mean       std  sum\nday  smoker                                             \nFri  No      0.120385  0.187735  0.151650  0.028123    9\n     Yes     0.103555  0.263480  0.174783  0.051293   31\nSat  No      0.056797  0.291990  0.158048  0.039767  115\n     Yes     0.035638  0.325733  0.147906  0.061375  104\nSun  No      0.059447  0.252672  0.160113  0.042347  167\n     Yes     0.065660  0.710345  0.187250  0.154134   49\nThur No      0.072961  0.266312  0.160298  0.038774  112\n     Yes     0.090014  0.241255  0.163863  0.039389   40\nA DataFrame will have hierarchical columns only if multiple functions are applied to\nat least one column.\nReturning Aggregated Data Without Row Indexes\nIn all of the examples up until now, the aggregated data comes back with an index,\npotentially hierarchical, composed from the unique group key combinations. Since\nthis isn’t always desirable, you can disable this behavior in most cases by passing\nas_index=False to groupby:\nIn [81]: tips.groupby([\"day\", \"smoker\"], as_index=False).mean()\nOut[81]: \n    day smoker  total_bill       tip      size   tip_pct\n0   Fri     No   18.420000  2.812500  2.250000  0.151650\n1   Fri    Yes   16.813333  2.714000  2.066667  0.174783\n2   Sat     No   19.661778  3.102889  2.555556  0.158048\n3   Sat    Yes   21.276667  2.875476  2.476190  0.147906\n4   Sun     No   20.506667  3.167895  2.929825  0.160113\n5   Sun    Yes   24.120000  3.516842  2.578947  0.187250\n6  Thur     No   17.113111  2.673778  2.488889  0.160298\n7  Thur    Yes   19.190588  3.030000  2.352941  0.163863\nOf course, it’s always possible to obtain the result in this format by calling\nreset_index on the result. Using the as_index=False argument avoids some unnec‐\nessary computations.\n10.3 Apply: General split-apply-combine\nThe most general-purpose GroupBy method is apply, which is the subject of this\nsection. apply splits the object being manipulated into pieces, invokes the passed\nfunction on each piece, and then attempts to concatenate the pieces.\nReturning to the tipping dataset from before, suppose you wanted to select the top\nfive tip_pct values by group. First, write a function that selects the rows with the\nlargest values in a particular column:\n10.3 Apply: General split-apply-combine \n| \n335",
      "content_length": 2392,
      "extraction_method": "Direct"
    },
    {
      "page_number": 354,
      "chapter": null,
      "content": "In [82]: def top(df, n=5, column=\"tip_pct\"):\n   ....:     return df.sort_values(column, ascending=False)[:n]\nIn [83]: top(tips, n=6)\nOut[83]: \n     total_bill   tip smoker  day    time  size   tip_pct\n172        7.25  5.15    Yes  Sun  Dinner     2  0.710345\n178        9.60  4.00    Yes  Sun  Dinner     2  0.416667\n67         3.07  1.00    Yes  Sat  Dinner     1  0.325733\n232       11.61  3.39     No  Sat  Dinner     2  0.291990\n183       23.17  6.50    Yes  Sun  Dinner     4  0.280535\n109       14.31  4.00    Yes  Sat  Dinner     2  0.279525\nNow, if we group by smoker, say, and call apply with this function, we get the\nfollowing:\nIn [84]: tips.groupby(\"smoker\").apply(top)\nOut[84]: \n            total_bill   tip smoker   day    time  size   tip_pct\nsmoker                                                           \nNo     232       11.61  3.39     No   Sat  Dinner     2  0.291990\n       149        7.51  2.00     No  Thur   Lunch     2  0.266312\n       51        10.29  2.60     No   Sun  Dinner     2  0.252672\n       185       20.69  5.00     No   Sun  Dinner     5  0.241663\n       88        24.71  5.85     No  Thur   Lunch     2  0.236746\nYes    172        7.25  5.15    Yes   Sun  Dinner     2  0.710345\n       178        9.60  4.00    Yes   Sun  Dinner     2  0.416667\n       67         3.07  1.00    Yes   Sat  Dinner     1  0.325733\n       183       23.17  6.50    Yes   Sun  Dinner     4  0.280535\n       109       14.31  4.00    Yes   Sat  Dinner     2  0.279525\nWhat has happened here? First, the tips DataFrame is split into groups based on the\nvalue of smoker. Then the top function is called on each group, and the results of\neach function call are glued together using pandas.concat, labeling the pieces with\nthe group names. The result therefore has a hierarchical index with an inner level that\ncontains index values from the original DataFrame.\nIf you pass a function to apply that takes other arguments or keywords, you can pass\nthese after the function:\nIn [85]: tips.groupby([\"smoker\", \"day\"]).apply(top, n=1, column=\"total_bill\")\nOut[85]: \n                 total_bill    tip smoker   day    time  size   tip_pct\nsmoker day                                                             \nNo     Fri  94        22.75   3.25     No   Fri  Dinner     2  0.142857\n       Sat  212       48.33   9.00     No   Sat  Dinner     4  0.186220\n       Sun  156       48.17   5.00     No   Sun  Dinner     6  0.103799\n       Thur 142       41.19   5.00     No  Thur   Lunch     5  0.121389\nYes    Fri  95        40.17   4.73    Yes   Fri  Dinner     4  0.117750\n       Sat  170       50.81  10.00    Yes   Sat  Dinner     3  0.196812\n       Sun  182       45.35   3.50    Yes   Sun  Dinner     3  0.077178\n       Thur 197       43.11   5.00    Yes  Thur   Lunch     4  0.115982\n336 \n| \nChapter 10: Data Aggregation and Group Operations",
      "content_length": 2850,
      "extraction_method": "Direct"
    },
    {
      "page_number": 355,
      "chapter": null,
      "content": "Beyond these basic usage mechanics, getting the most out of apply may require some\ncreativity. What occurs inside the function passed is up to you; it must either return\na pandas object or a scalar value. The rest of this chapter will consist mainly of\nexamples showing you how to solve various problems using groupby.\nFor example, you may recall that I earlier called describe on a GroupBy object:\nIn [86]: result = tips.groupby(\"smoker\")[\"tip_pct\"].describe()\nIn [87]: result\nOut[87]: \n        count      mean       std       min       25%       50%       75%  \\\nsmoker                                                                      \nNo      151.0  0.159328  0.039910  0.056797  0.136906  0.155625  0.185014   \nYes      93.0  0.163196  0.085119  0.035638  0.106771  0.153846  0.195059   \n             max  \nsmoker            \nNo      0.291990  \nYes     0.710345  \nIn [88]: result.unstack(\"smoker\")\nOut[88]: \n       smoker\ncount  No        151.000000\n       Yes        93.000000\nmean   No          0.159328\n       Yes         0.163196\nstd    No          0.039910\n       Yes         0.085119\nmin    No          0.056797\n       Yes         0.035638\n25%    No          0.136906\n       Yes         0.106771\n50%    No          0.155625\n       Yes         0.153846\n75%    No          0.185014\n       Yes         0.195059\nmax    No          0.291990\n       Yes         0.710345\ndtype: float64\nInside GroupBy, when you invoke a method like describe, it is actually just a\nshortcut for:\ndef f(group):\n    return group.describe()\ngrouped.apply(f)\n10.3 Apply: General split-apply-combine \n| \n337",
      "content_length": 1591,
      "extraction_method": "Direct"
    },
    {
      "page_number": 356,
      "chapter": null,
      "content": "Suppressing the Group Keys\nIn the preceding examples, you see that the resulting object has a hierarchical index\nformed from the group keys, along with the indexes of each piece of the original\nobject. You can disable this by passing group_keys=False to groupby:\nIn [89]: tips.groupby(\"smoker\", group_keys=False).apply(top)\nOut[89]: \n     total_bill   tip smoker   day    time  size   tip_pct\n232       11.61  3.39     No   Sat  Dinner     2  0.291990\n149        7.51  2.00     No  Thur   Lunch     2  0.266312\n51        10.29  2.60     No   Sun  Dinner     2  0.252672\n185       20.69  5.00     No   Sun  Dinner     5  0.241663\n88        24.71  5.85     No  Thur   Lunch     2  0.236746\n172        7.25  5.15    Yes   Sun  Dinner     2  0.710345\n178        9.60  4.00    Yes   Sun  Dinner     2  0.416667\n67         3.07  1.00    Yes   Sat  Dinner     1  0.325733\n183       23.17  6.50    Yes   Sun  Dinner     4  0.280535\n109       14.31  4.00    Yes   Sat  Dinner     2  0.279525\nQuantile and Bucket Analysis\nAs you may recall from Chapter 8, pandas has some tools, in particular pandas.cut\nand pandas.qcut, for slicing data up into buckets with bins of your choosing, or by\nsample quantiles. Combining these functions with groupby makes it convenient to\nperform bucket or quantile analysis on a dataset. Consider a simple random dataset\nand an equal-length bucket categorization using pandas.cut:\nIn [90]: frame = pd.DataFrame({\"data1\": np.random.standard_normal(1000),\n   ....:                       \"data2\": np.random.standard_normal(1000)})\nIn [91]: frame.head()\nOut[91]: \n      data1     data2\n0 -0.660524 -0.612905\n1  0.862580  0.316447\n2 -0.010032  0.838295\n3  0.050009 -1.034423\n4  0.670216  0.434304\nIn [92]: quartiles = pd.cut(frame[\"data1\"], 4)\nIn [93]: quartiles.head(10)\nOut[93]: \n0     (-1.23, 0.489]\n1     (0.489, 2.208]\n2     (-1.23, 0.489]\n3     (-1.23, 0.489]\n4     (0.489, 2.208]\n5     (0.489, 2.208]\n6     (-1.23, 0.489]\n338 \n| \nChapter 10: Data Aggregation and Group Operations",
      "content_length": 2001,
      "extraction_method": "Direct"
    },
    {
      "page_number": 357,
      "chapter": null,
      "content": "7     (-1.23, 0.489]\n8    (-2.956, -1.23]\n9     (-1.23, 0.489]\nName: data1, dtype: category\nCategories (4, interval[float64, right]): [(-2.956, -1.23] < (-1.23, 0.489] < (0.\n489, 2.208] <\n                                           (2.208, 3.928]]\nThe Categorical object returned by cut can be passed directly to groupby. So we\ncould compute a set of group statistics for the quartiles, like so:\nIn [94]: def get_stats(group):\n   ....:     return pd.DataFrame(\n   ....:         {\"min\": group.min(), \"max\": group.max(),\n   ....:         \"count\": group.count(), \"mean\": group.mean()}\n   ....:     )\nIn [95]: grouped = frame.groupby(quartiles)\nIn [96]: grouped.apply(get_stats)\nOut[96]: \n                            min       max  count      mean\ndata1                                                     \n(-2.956, -1.23] data1 -2.949343 -1.230179     94 -1.658818\n                data2 -3.399312  1.670835     94 -0.033333\n(-1.23, 0.489]  data1 -1.228918  0.488675    598 -0.329524\n                data2 -2.989741  3.260383    598 -0.002622\n(0.489, 2.208]  data1  0.489965  2.200997    298  1.065727\n                data2 -3.745356  2.954439    298  0.078249\n(2.208, 3.928]  data1  2.212303  3.927528     10  2.644253\n                data2 -1.929776  1.765640     10  0.024750\nKeep in mind the same result could have been computed more simply with:\nIn [97]: grouped.agg([\"min\", \"max\", \"count\", \"mean\"])\nOut[97]: \n                    data1                               data2                  \\\n                      min       max count      mean       min       max count   \ndata1                                                                           \n(-2.956, -1.23] -2.949343 -1.230179    94 -1.658818 -3.399312  1.670835    94   \n(-1.23, 0.489]  -1.228918  0.488675   598 -0.329524 -2.989741  3.260383   598   \n(0.489, 2.208]   0.489965  2.200997   298  1.065727 -3.745356  2.954439   298   \n(2.208, 3.928]   2.212303  3.927528    10  2.644253 -1.929776  1.765640    10   \n                           \n                     mean  \ndata1                      \n(-2.956, -1.23] -0.033333  \n(-1.23, 0.489]  -0.002622  \n(0.489, 2.208]   0.078249  \n(2.208, 3.928]   0.024750  \nThese were equal-length buckets; to compute equal-size buckets based on sample\nquantiles, use pandas.qcut. We can pass 4 as the number of bucket compute sam‐\n10.3 Apply: General split-apply-combine \n| \n339",
      "content_length": 2378,
      "extraction_method": "Direct"
    },
    {
      "page_number": 358,
      "chapter": null,
      "content": "ple quartiles, and pass labels=False to obtain just the quartile indices instead of\nintervals:\nIn [98]: quartiles_samp = pd.qcut(frame[\"data1\"], 4, labels=False)\nIn [99]: quartiles_samp.head()\nOut[99]: \n0    1\n1    3\n2    2\n3    2\n4    3\nName: data1, dtype: int64\nIn [100]: grouped = frame.groupby(quartiles_samp)\nIn [101]: grouped.apply(get_stats)\nOut[101]: \n                  min       max  count      mean\ndata1                                           \n0     data1 -2.949343 -0.685484    250 -1.212173\n      data2 -3.399312  2.628441    250 -0.027045\n1     data1 -0.683066 -0.030280    250 -0.368334\n      data2 -2.630247  3.260383    250 -0.027845\n2     data1 -0.027734  0.618965    250  0.295812\n      data2 -3.056990  2.458842    250  0.014450\n3     data1  0.623587  3.927528    250  1.248875\n      data2 -3.745356  2.954439    250  0.115899\nExample: Filling Missing Values with Group-Specific Values\nWhen cleaning up missing data, in some cases you will remove data observations\nusing dropna, but in others you may want to fill in the null (NA) values using a\nfixed value or some value derived from the data. fillna is the right tool to use; for\nexample, here I fill in the null values with the mean:\nIn [102]: s = pd.Series(np.random.standard_normal(6))\nIn [103]: s[::2] = np.nan\nIn [104]: s\nOut[104]: \n0         NaN\n1    0.227290\n2         NaN\n3   -2.153545\n4         NaN\n5   -0.375842\ndtype: float64\nIn [105]: s.fillna(s.mean())\n340 \n| \nChapter 10: Data Aggregation and Group Operations",
      "content_length": 1498,
      "extraction_method": "Direct"
    },
    {
      "page_number": 359,
      "chapter": null,
      "content": "Out[105]: \n0   -0.767366\n1    0.227290\n2   -0.767366\n3   -2.153545\n4   -0.767366\n5   -0.375842\ndtype: float64\nSuppose you need the fill value to vary by group. One way to do this is to group the\ndata and use apply with a function that calls fillna on each data chunk. Here is\nsome sample data on US states divided into eastern and western regions:\nIn [106]: states = [\"Ohio\", \"New York\", \"Vermont\", \"Florida\",\n   .....:           \"Oregon\", \"Nevada\", \"California\", \"Idaho\"]\nIn [107]: group_key = [\"East\", \"East\", \"East\", \"East\",\n   .....:              \"West\", \"West\", \"West\", \"West\"]\nIn [108]: data = pd.Series(np.random.standard_normal(8), index=states)\nIn [109]: data\nOut[109]: \nOhio          0.329939\nNew York      0.981994\nVermont       1.105913\nFlorida      -1.613716\nOregon        1.561587\nNevada        0.406510\nCalifornia    0.359244\nIdaho        -0.614436\ndtype: float64\nLet’s set some values in the data to be missing:\nIn [110]: data[[\"Vermont\", \"Nevada\", \"Idaho\"]] = np.nan\nIn [111]: data\nOut[111]: \nOhio          0.329939\nNew York      0.981994\nVermont            NaN\nFlorida      -1.613716\nOregon        1.561587\nNevada             NaN\nCalifornia    0.359244\nIdaho              NaN\ndtype: float64\nIn [112]: data.groupby(group_key).size()\nOut[112]: \nEast    4\nWest    4\n10.3 Apply: General split-apply-combine \n| \n341",
      "content_length": 1328,
      "extraction_method": "Direct"
    },
    {
      "page_number": 360,
      "chapter": null,
      "content": "dtype: int64\nIn [113]: data.groupby(group_key).count()\nOut[113]: \nEast    3\nWest    2\ndtype: int64\nIn [114]: data.groupby(group_key).mean()\nOut[114]: \nEast   -0.100594\nWest    0.960416\ndtype: float64\nWe can fill the NA values using the group means, like so:\nIn [115]: def fill_mean(group):\n   .....:     return group.fillna(group.mean())\nIn [116]: data.groupby(group_key).apply(fill_mean)\nOut[116]: \nOhio          0.329939\nNew York      0.981994\nVermont      -0.100594\nFlorida      -1.613716\nOregon        1.561587\nNevada        0.960416\nCalifornia    0.359244\nIdaho         0.960416\ndtype: float64\nIn another case, you might have predefined fill values in your code that vary by\ngroup. Since the groups have a name attribute set internally, we can use that:\nIn [117]: fill_values = {\"East\": 0.5, \"West\": -1}\nIn [118]: def fill_func(group):\n   .....:     return group.fillna(fill_values[group.name])\nIn [119]: data.groupby(group_key).apply(fill_func)\nOut[119]: \nOhio          0.329939\nNew York      0.981994\nVermont       0.500000\nFlorida      -1.613716\nOregon        1.561587\nNevada       -1.000000\nCalifornia    0.359244\nIdaho        -1.000000\ndtype: float64\n342 \n| \nChapter 10: Data Aggregation and Group Operations",
      "content_length": 1218,
      "extraction_method": "Direct"
    },
    {
      "page_number": 361,
      "chapter": null,
      "content": "Example: Random Sampling and Permutation\nSuppose you wanted to draw a random sample (with or without replacement) from a\nlarge dataset for Monte Carlo simulation purposes or some other application. There\nare a number of ways to perform the “draws”; here we use the sample method for\nSeries.\nTo demonstrate, here’s a way to construct a deck of English-style playing cards:\nsuits = [\"H\", \"S\", \"C\", \"D\"]  # Hearts, Spades, Clubs, Diamonds\ncard_val = (list(range(1, 11)) + [10] * 3) * 4\nbase_names = [\"A\"] + list(range(2, 11)) + [\"J\", \"K\", \"Q\"]\ncards = []\nfor suit in suits:\n    cards.extend(str(num) + suit for num in base_names)\ndeck = pd.Series(card_val, index=cards)\nNow we have a Series of length 52 whose index contains card names, and values are\nthe ones used in blackjack and other games (to keep things simple, I let the ace \"A\"\nbe 1):\nIn [121]: deck.head(13)\nOut[121]: \nAH      1\n2H      2\n3H      3\n4H      4\n5H      5\n6H      6\n7H      7\n8H      8\n9H      9\n10H    10\nJH     10\nKH     10\nQH     10\ndtype: int64\nNow, based on what I said before, drawing a hand of five cards from the deck could\nbe written as:\nIn [122]: def draw(deck, n=5):\n   .....:     return deck.sample(n)\nIn [123]: draw(deck)\nOut[123]: \n4D     4\nQH    10\n8S     8\n7D     7\n10.3 Apply: General split-apply-combine \n| \n343",
      "content_length": 1299,
      "extraction_method": "Direct"
    },
    {
      "page_number": 362,
      "chapter": null,
      "content": "9C     9\ndtype: int64\nSuppose you wanted two random cards from each suit. Because the suit is the last\ncharacter of each card name, we can group based on this and use apply:\nIn [124]: def get_suit(card):\n   .....:     # last letter is suit\n   .....:     return card[-1]\nIn [125]: deck.groupby(get_suit).apply(draw, n=2)\nOut[125]: \nC  6C     6\n   KC    10\nD  7D     7\n   3D     3\nH  7H     7\n   9H     9\nS  2S     2\n   QS    10\ndtype: int64\nAlternatively, we could pass group_keys=False to drop the outer suit index, leaving\nin just the selected cards:\nIn [126]: deck.groupby(get_suit, group_keys=False).apply(draw, n=2)\nOut[126]: \nAC      1\n3C      3\n5D      5\n4D      4\n10H    10\n7H      7\nQS     10\n7S      7\ndtype: int64\nExample: Group Weighted Average and Correlation\nUnder the split-apply-combine paradigm of groupby, operations between columns in\na DataFrame or two Series, such as a group weighted average, are possible. As an\nexample, take this dataset containing group keys, values, and some weights:\nIn [127]: df = pd.DataFrame({\"category\": [\"a\", \"a\", \"a\", \"a\",\n   .....:                                 \"b\", \"b\", \"b\", \"b\"],\n   .....:                    \"data\": np.random.standard_normal(8),\n   .....:                    \"weights\": np.random.uniform(size=8)})\nIn [128]: df\nOut[128]: \n  category      data   weights\n0        a -1.691656  0.955905\n344 \n| \nChapter 10: Data Aggregation and Group Operations",
      "content_length": 1413,
      "extraction_method": "Direct"
    },
    {
      "page_number": 363,
      "chapter": null,
      "content": "1        a  0.511622  0.012745\n2        a -0.401675  0.137009\n3        a  0.968578  0.763037\n4        b -1.818215  0.492472\n5        b  0.279963  0.832908\n6        b -0.200819  0.658331\n7        b -0.217221  0.612009\nThe weighted average by category would then be:\nIn [129]: grouped = df.groupby(\"category\")\nIn [130]: def get_wavg(group):\n   .....:     return np.average(group[\"data\"], weights=group[\"weights\"])\nIn [131]: grouped.apply(get_wavg)\nOut[131]: \ncategory\na   -0.495807\nb   -0.357273\ndtype: float64\nAs another example, consider a financial dataset originally obtained from Yahoo!\nFinance containing end-of-day prices for a few stocks and the S&P 500 index (the SPX\nsymbol):\nIn [132]: close_px = pd.read_csv(\"examples/stock_px.csv\", parse_dates=True,\n   .....:                        index_col=0)\nIn [133]: close_px.info()\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 2214 entries, 2003-01-02 to 2011-10-14\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   AAPL    2214 non-null   float64\n 1   MSFT    2214 non-null   float64\n 2   XOM     2214 non-null   float64\n 3   SPX     2214 non-null   float64\ndtypes: float64(4)\nmemory usage: 86.5 KB\nIn [134]: close_px.tail(4)\nOut[134]: \n              AAPL   MSFT    XOM      SPX\n2011-10-11  400.29  27.00  76.27  1195.54\n2011-10-12  402.19  26.96  77.16  1207.25\n2011-10-13  408.43  27.18  76.37  1203.66\n2011-10-14  422.00  27.27  78.11  1224.58\nThe DataFrame info() method here is a convenient way to get an overview of the\ncontents of a DataFrame.\n10.3 Apply: General split-apply-combine \n| \n345",
      "content_length": 1618,
      "extraction_method": "Direct"
    },
    {
      "page_number": 364,
      "chapter": null,
      "content": "One task of interest might be to compute a DataFrame consisting of the yearly\ncorrelations of daily returns (computed from percent changes) with SPX. As one way\nto do this, we first create a function that computes the pair-wise correlation of each\ncolumn with the \"SPX\" column:\nIn [135]: def spx_corr(group):\n   .....:     return group.corrwith(group[\"SPX\"])\nNext, we compute percent change on close_px using pct_change:\nIn [136]: rets = close_px.pct_change().dropna()\nLastly, we group these percent changes by year, which can be extracted from each row\nlabel with a one-line function that returns the year attribute of each datetime label:\nIn [137]: def get_year(x):\n   .....:     return x.year\nIn [138]: by_year = rets.groupby(get_year)\nIn [139]: by_year.apply(spx_corr)\nOut[139]: \n          AAPL      MSFT       XOM  SPX\n2003  0.541124  0.745174  0.661265  1.0\n2004  0.374283  0.588531  0.557742  1.0\n2005  0.467540  0.562374  0.631010  1.0\n2006  0.428267  0.406126  0.518514  1.0\n2007  0.508118  0.658770  0.786264  1.0\n2008  0.681434  0.804626  0.828303  1.0\n2009  0.707103  0.654902  0.797921  1.0\n2010  0.710105  0.730118  0.839057  1.0\n2011  0.691931  0.800996  0.859975  1.0\nYou could also compute intercolumn correlations. Here we compute the annual\ncorrelation between Apple and Microsoft:\nIn [140]: def corr_aapl_msft(group):\n   .....:     return group[\"AAPL\"].corr(group[\"MSFT\"])\nIn [141]: by_year.apply(corr_aapl_msft)\nOut[141]: \n2003    0.480868\n2004    0.259024\n2005    0.300093\n2006    0.161735\n2007    0.417738\n2008    0.611901\n2009    0.432738\n2010    0.571946\n2011    0.581987\ndtype: float64\n346 \n| \nChapter 10: Data Aggregation and Group Operations",
      "content_length": 1669,
      "extraction_method": "Direct"
    },
    {
      "page_number": 365,
      "chapter": null,
      "content": "Example: Group-Wise Linear Regression\nIn the same theme as the previous example, you can use groupby to perform more\ncomplex group-wise statistical analysis, as long as the function returns a pandas\nobject or scalar value. For example, I can define the following regress function\n(using the statsmodels econometrics library), which executes an ordinary least\nsquares (OLS) regression on each chunk of data:\nimport statsmodels.api as sm\ndef regress(data, yvar=None, xvars=None):\n    Y = data[yvar]\n    X = data[xvars]\n    X[\"intercept\"] = 1.\n    result = sm.OLS(Y, X).fit()\n    return result.params\nYou can install statsmodels with conda if you don’t have it already:\nconda install statsmodels\nNow, to run a yearly linear regression of AAPL on SPX returns, execute:\nIn [143]: by_year.apply(regress, yvar=\"AAPL\", xvars=[\"SPX\"])\nOut[143]: \n           SPX  intercept\n2003  1.195406   0.000710\n2004  1.363463   0.004201\n2005  1.766415   0.003246\n2006  1.645496   0.000080\n2007  1.198761   0.003438\n2008  0.968016  -0.001110\n2009  0.879103   0.002954\n2010  1.052608   0.001261\n2011  0.806605   0.001514\n10.4 Group Transforms and “Unwrapped” GroupBys\nIn Section 10.3, “Apply: General split-apply-combine,” on page 335, we looked at\nthe apply method in grouped operations for performing transformations. There is\nanother built-in method called transform, which is similar to apply but imposes\nmore constraints on the kind of function you can use:\n• It can produce a scalar value to be broadcast to the shape of the group.\n•\n• It can produce an object of the same shape as the input group.\n•\n• It must not mutate its input.\n•\nLet’s consider a simple example for illustration:\n10.4 Group Transforms and “Unwrapped” GroupBys \n| \n347",
      "content_length": 1721,
      "extraction_method": "Direct"
    },
    {
      "page_number": 366,
      "chapter": null,
      "content": "In [144]: df = pd.DataFrame({'key': ['a', 'b', 'c'] * 4,\n   .....:                    'value': np.arange(12.)})\nIn [145]: df\nOut[145]: \n   key  value\n0    a    0.0\n1    b    1.0\n2    c    2.0\n3    a    3.0\n4    b    4.0\n5    c    5.0\n6    a    6.0\n7    b    7.0\n8    c    8.0\n9    a    9.0\n10   b   10.0\n11   c   11.0\nHere are the group means by key:\nIn [146]: g = df.groupby('key')['value']\nIn [147]: g.mean()\nOut[147]: \nkey\na    4.5\nb    5.5\nc    6.5\nName: value, dtype: float64\nSuppose instead we wanted to produce a Series of the same shape as df['value'] but\nwith values replaced by the average grouped by 'key'. We can pass a function that\ncomputes the mean of a single group to transform:\nIn [148]: def get_mean(group):\n   .....:     return group.mean()\nIn [149]: g.transform(get_mean)\nOut[149]: \n0     4.5\n1     5.5\n2     6.5\n3     4.5\n4     5.5\n5     6.5\n6     4.5\n7     5.5\n8     6.5\n9     4.5\n10    5.5\n11    6.5\nName: value, dtype: float64\n348 \n| \nChapter 10: Data Aggregation and Group Operations",
      "content_length": 1009,
      "extraction_method": "Direct"
    },
    {
      "page_number": 367,
      "chapter": null,
      "content": "For built-in aggregation functions, we can pass a string alias as with the GroupBy agg\nmethod:\nIn [150]: g.transform('mean')\nOut[150]: \n0     4.5\n1     5.5\n2     6.5\n3     4.5\n4     5.5\n5     6.5\n6     4.5\n7     5.5\n8     6.5\n9     4.5\n10    5.5\n11    6.5\nName: value, dtype: float64\nLike apply, transform works with functions that return Series, but the result must\nbe the same size as the input. For example, we can multiply each group by 2 using a\nhelper function:\nIn [151]: def times_two(group):\n   .....:     return group * 2\nIn [152]: g.transform(times_two)\nOut[152]: \n0      0.0\n1      2.0\n2      4.0\n3      6.0\n4      8.0\n5     10.0\n6     12.0\n7     14.0\n8     16.0\n9     18.0\n10    20.0\n11    22.0\nName: value, dtype: float64\nAs a more complicated example, we can compute the ranks in descending order for\neach group:\nIn [153]: def get_ranks(group):\n   .....:     return group.rank(ascending=False)\nIn [154]: g.transform(get_ranks)\nOut[154]: \n0     4.0\n1     4.0\n2     4.0\n10.4 Group Transforms and “Unwrapped” GroupBys \n| \n349",
      "content_length": 1036,
      "extraction_method": "Direct"
    },
    {
      "page_number": 368,
      "chapter": null,
      "content": "3     3.0\n4     3.0\n5     3.0\n6     2.0\n7     2.0\n8     2.0\n9     1.0\n10    1.0\n11    1.0\nName: value, dtype: float64\nConsider a group transformation function composed from simple aggregations:\nIn [155]: def normalize(x):\n   .....:     return (x - x.mean()) / x.std()\nWe can obtain equivalent results in this case using either transform or apply:\nIn [156]: g.transform(normalize)\nOut[156]: \n0    -1.161895\n1    -1.161895\n2    -1.161895\n3    -0.387298\n4    -0.387298\n5    -0.387298\n6     0.387298\n7     0.387298\n8     0.387298\n9     1.161895\n10    1.161895\n11    1.161895\nName: value, dtype: float64\nIn [157]: g.apply(normalize)\nOut[157]: \n0    -1.161895\n1    -1.161895\n2    -1.161895\n3    -0.387298\n4    -0.387298\n5    -0.387298\n6     0.387298\n7     0.387298\n8     0.387298\n9     1.161895\n10    1.161895\n11    1.161895\nName: value, dtype: float64\nBuilt-in aggregate functions like 'mean' or 'sum' are often much faster than a general\napply function. These also have a “fast path” when used with transform. This allows\nus to perform what is called an unwrapped group operation:\n350 \n| \nChapter 10: Data Aggregation and Group Operations",
      "content_length": 1134,
      "extraction_method": "Direct"
    },
    {
      "page_number": 369,
      "chapter": null,
      "content": "In [158]: g.transform('mean')\nOut[158]: \n0     4.5\n1     5.5\n2     6.5\n3     4.5\n4     5.5\n5     6.5\n6     4.5\n7     5.5\n8     6.5\n9     4.5\n10    5.5\n11    6.5\nName: value, dtype: float64\nIn [159]: normalized = (df['value'] - g.transform('mean')) / g.transform('std')\nIn [160]: normalized\nOut[160]: \n0    -1.161895\n1    -1.161895\n2    -1.161895\n3    -0.387298\n4    -0.387298\n5    -0.387298\n6     0.387298\n7     0.387298\n8     0.387298\n9     1.161895\n10    1.161895\n11    1.161895\nName: value, dtype: float64\nHere, we are doing arithmetic between the outputs of multiple GroupBy operations\ninstead of writing a function and passing it to groupby(...).apply. That is what is\nmeant by “unwrapped.”\nWhile an unwrapped group operation may involve multiple group aggregations, the\noverall benefit of vectorized operations often outweighs this.\n10.5 Pivot Tables and Cross-Tabulation\nA pivot table is a data summarization tool frequently found in spreadsheet programs\nand other data analysis software. It aggregates a table of data by one or more keys,\narranging the data in a rectangle with some of the group keys along the rows and\nsome along the columns. Pivot tables in Python with pandas are made possible\nthrough the groupby facility described in this chapter, combined with reshape opera‐\ntions utilizing hierarchical indexing. DataFrame also has a pivot_table method, and\n10.5 Pivot Tables and Cross-Tabulation \n| \n351",
      "content_length": 1420,
      "extraction_method": "Direct"
    },
    {
      "page_number": 370,
      "chapter": null,
      "content": "there is also a top-level pandas.pivot_table function. In addition to providing a\nconvenience interface to groupby, pivot_table can add partial totals, also known as\nmargins.\nReturning to the tipping dataset, suppose you wanted to compute a table of group\nmeans (the default pivot_table aggregation type) arranged by day and smoker on\nthe rows:\nIn [161]: tips.head()\nOut[161]: \n   total_bill   tip smoker  day    time  size   tip_pct\n0       16.99  1.01     No  Sun  Dinner     2  0.059447\n1       10.34  1.66     No  Sun  Dinner     3  0.160542\n2       21.01  3.50     No  Sun  Dinner     3  0.166587\n3       23.68  3.31     No  Sun  Dinner     2  0.139780\n4       24.59  3.61     No  Sun  Dinner     4  0.146808\nIn [162]: tips.pivot_table(index=[\"day\", \"smoker\"])\nOut[162]: \n                 size       tip   tip_pct  total_bill\nday  smoker                                          \nFri  No      2.250000  2.812500  0.151650   18.420000\n     Yes     2.066667  2.714000  0.174783   16.813333\nSat  No      2.555556  3.102889  0.158048   19.661778\n     Yes     2.476190  2.875476  0.147906   21.276667\nSun  No      2.929825  3.167895  0.160113   20.506667\n     Yes     2.578947  3.516842  0.187250   24.120000\nThur No      2.488889  2.673778  0.160298   17.113111\n     Yes     2.352941  3.030000  0.163863   19.190588\nThis could have been produced with groupby directly, using tips.groupby([\"day\",\n\"smoker\"]).mean(). Now, suppose we want to take the average of only tip_pct and\nsize, and additionally group by time. I’ll put smoker in the table columns and time\nand day in the rows:\nIn [163]: tips.pivot_table(index=[\"time\", \"day\"], columns=\"smoker\",\n   .....:                  values=[\"tip_pct\", \"size\"])\nOut[163]: \n                 size             tip_pct          \nsmoker             No       Yes        No       Yes\ntime   day                                         \nDinner Fri   2.000000  2.222222  0.139622  0.165347\n       Sat   2.555556  2.476190  0.158048  0.147906\n       Sun   2.929825  2.578947  0.160113  0.187250\n       Thur  2.000000       NaN  0.159744       NaN\nLunch  Fri   3.000000  1.833333  0.187735  0.188937\n       Thur  2.500000  2.352941  0.160311  0.163863\nWe could augment this table to include partial totals by passing margins=True. This\nhas the effect of adding All row and column labels, with corresponding values being\nthe group statistics for all the data within a single tier:\n352 \n| \nChapter 10: Data Aggregation and Group Operations",
      "content_length": 2469,
      "extraction_method": "Direct"
    },
    {
      "page_number": 371,
      "chapter": null,
      "content": "In [164]: tips.pivot_table(index=[\"time\", \"day\"], columns=\"smoker\",\n   .....:                  values=[\"tip_pct\", \"size\"], margins=True)\nOut[164]: \n                 size                       tip_pct                    \nsmoker             No       Yes       All        No       Yes       All\ntime   day                                                             \nDinner Fri   2.000000  2.222222  2.166667  0.139622  0.165347  0.158916\n       Sat   2.555556  2.476190  2.517241  0.158048  0.147906  0.153152\n       Sun   2.929825  2.578947  2.842105  0.160113  0.187250  0.166897\n       Thur  2.000000       NaN  2.000000  0.159744       NaN  0.159744\nLunch  Fri   3.000000  1.833333  2.000000  0.187735  0.188937  0.188765\n       Thur  2.500000  2.352941  2.459016  0.160311  0.163863  0.161301\nAll          2.668874  2.408602  2.569672  0.159328  0.163196  0.160803\nHere, the All values are means without taking into account smoker versus non-\nsmoker (the All columns) or any of the two levels of grouping on the rows (the All\nrow).\nTo use an aggregation function other than mean, pass it to the aggfunc keyword\nargument. For example, \"count\" or len will give you a cross-tabulation (count or\nfrequency) of group sizes (though \"count\" will exclude null values from the count\nwithin data groups, while len will not):\nIn [165]: tips.pivot_table(index=[\"time\", \"smoker\"], columns=\"day\",\n   .....:                  values=\"tip_pct\", aggfunc=len, margins=True)\nOut[165]: \nday             Fri   Sat   Sun  Thur  All\ntime   smoker                             \nDinner No       3.0  45.0  57.0   1.0  106\n       Yes      9.0  42.0  19.0   NaN   70\nLunch  No       1.0   NaN   NaN  44.0   45\n       Yes      6.0   NaN   NaN  17.0   23\nAll            19.0  87.0  76.0  62.0  244\nIf some combinations are empty (or otherwise NA), you may wish to pass a\nfill_value:\nIn [166]: tips.pivot_table(index=[\"time\", \"size\", \"smoker\"], columns=\"day\",\n   .....:                  values=\"tip_pct\", fill_value=0)\nOut[166]: \nday                      Fri       Sat       Sun      Thur\ntime   size smoker                                        \nDinner 1    No      0.000000  0.137931  0.000000  0.000000\n            Yes     0.000000  0.325733  0.000000  0.000000\n       2    No      0.139622  0.162705  0.168859  0.159744\n            Yes     0.171297  0.148668  0.207893  0.000000\n       3    No      0.000000  0.154661  0.152663  0.000000\n...                      ...       ...       ...       ...\nLunch  3    Yes     0.000000  0.000000  0.000000  0.204952\n       4    No      0.000000  0.000000  0.000000  0.138919\n            Yes     0.000000  0.000000  0.000000  0.155410\n       5    No      0.000000  0.000000  0.000000  0.121389\n10.5 Pivot Tables and Cross-Tabulation \n| \n353",
      "content_length": 2755,
      "extraction_method": "Direct"
    },
    {
      "page_number": 372,
      "chapter": null,
      "content": "6    No      0.000000  0.000000  0.000000  0.173706\n[21 rows x 4 columns]\nSee Table 10-2 for a summary of pivot_table options.\nTable 10-2. pivot_table options\nArgument\nDescription\nvalues\nColumn name or names to aggregate; by default, aggregates all numeric columns\nindex\nColumn names or other group keys to group on the rows of the resulting pivot table\ncolumns\nColumn names or other group keys to group on the columns of the resulting pivot table\naggfunc\nAggregation function or list of functions (\"mean\" by default); can be any function valid in a groupby\ncontext\nfill_value\nReplace missing values in the result table\ndropna\nIf True, do not include columns whose entries are all NA\nmargins\nAdd row/column subtotals and grand total (False by default)\nmargins_name\nName to use for the margin row/column labels when passing margins=True; defaults to \"All\"\nobserved\nWith Categorical group keys, if True, show only the observed category values in the keys rather than all\ncategories\nCross-Tabulations: Crosstab\nA cross-tabulation (or crosstab for short) is a special case of a pivot table that com‐\nputes group frequencies. Here is an example:\nIn [167]: from io import StringIO\nIn [168]: data = \"\"\"Sample  Nationality  Handedness\n   .....: 1   USA  Right-handed\n   .....: 2   Japan    Left-handed\n   .....: 3   USA  Right-handed\n   .....: 4   Japan    Right-handed\n   .....: 5   Japan    Left-handed\n   .....: 6   Japan    Right-handed\n   .....: 7   USA  Right-handed\n   .....: 8   USA  Left-handed\n   .....: 9   Japan    Right-handed\n   .....: 10  USA  Right-handed\"\"\"\n   .....:\nIn [169]: data = pd.read_table(StringIO(data), sep=\"\\s+\")\nIn [170]: data\nOut[170]: \n   Sample Nationality    Handedness\n0       1         USA  Right-handed\n1       2       Japan   Left-handed\n2       3         USA  Right-handed\n3       4       Japan  Right-handed\n4       5       Japan   Left-handed\n354 \n| \nChapter 10: Data Aggregation and Group Operations",
      "content_length": 1934,
      "extraction_method": "Direct"
    },
    {
      "page_number": 373,
      "chapter": null,
      "content": "5       6       Japan  Right-handed\n6       7         USA  Right-handed\n7       8         USA   Left-handed\n8       9       Japan  Right-handed\n9      10         USA  Right-handed\nAs part of some survey analysis, we might want to summarize this data by nationality\nand handedness. You could use pivot_table to do this, but the pandas.crosstab\nfunction can be more convenient:\nIn [171]: pd.crosstab(data[\"Nationality\"], data[\"Handedness\"], margins=True)\nOut[171]: \nHandedness   Left-handed  Right-handed  All\nNationality                                \nJapan                  2             3    5\nUSA                    1             4    5\nAll                    3             7   10\nThe first two arguments to crosstab can each be an array or Series or a list of arrays.\nAs in the tips data:\nIn [172]: pd.crosstab([tips[\"time\"], tips[\"day\"]], tips[\"smoker\"], margins=True)\nOut[172]: \nsmoker        No  Yes  All\ntime   day                \nDinner Fri     3    9   12\n       Sat    45   42   87\n       Sun    57   19   76\n       Thur    1    0    1\nLunch  Fri     1    6    7\n       Thur   44   17   61\nAll          151   93  244\n10.6 Conclusion\nMastering pandas’s data grouping tools can help with data cleaning and modeling or\nstatistical analysis work. In Chapter 13 we will look at several more example use cases\nfor groupby on real data.\nIn the next chapter, we turn our attention to time series data.\n10.6 Conclusion \n| \n355",
      "content_length": 1428,
      "extraction_method": "Direct"
    },
    {
      "page_number": 374,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 375,
      "chapter": null,
      "content": "CHAPTER 11\nTime Series\nTime series data is an important form of structured data in many different fields, such\nas finance, economics, ecology, neuroscience, and physics. Anything that is recorded\nrepeatedly at many points in time forms a time series. Many time series are fixed\nfrequency, which is to say that data points occur at regular intervals according to some\nrule, such as every 15 seconds, every 5 minutes, or once per month. Time series can\nalso be irregular without a fixed unit of time or offset between units. How you mark\nand refer to time series data depends on the application, and you may have one of the\nfollowing:\nTimestamps\nSpecific instants in time.\nFixed periods\nSuch as the whole month of January 2017, or the whole year 2020.\nIntervals of time\nIndicated by a start and end timestamp. Periods can be thought of as special cases\nof intervals.\nExperiment or elapsed time\nEach timestamp is a measure of time relative to a particular start time (e.g., the\ndiameter of a cookie baking each second since being placed in the oven), starting\nfrom 0.\nIn this chapter, I am mainly concerned with time series in the first three categories,\nthough many of the techniques can be applied to experimental time series where the\nindex may be an integer or floating-point number indicating elapsed time from the\nstart of the experiment. The simplest kind of time series is indexed by timestamp.\n357",
      "content_length": 1403,
      "extraction_method": "Direct"
    },
    {
      "page_number": 376,
      "chapter": null,
      "content": "pandas also supports indexes based on timedeltas, which can be a\nuseful way of representing experiment or elapsed time. We do not\nexplore timedelta indexes in this book, but you can learn more in\nthe pandas documentation.\npandas provides many built-in time series tools and algorithms. You can efficiently\nwork with large time series, and slice and dice, aggregate, and resample irregular-\nand fixed-frequency time series. Some of these tools are useful for financial and\neconomics applications, but you could certainly use them to analyze server log data,\ntoo.\nAs with the rest of the chapters, we start by importing NumPy and pandas:\nIn [12]: import numpy as np\nIn [13]: import pandas as pd\n11.1 Date and Time Data Types and Tools\nThe Python standard library includes data types for date and time data, as well as\ncalendar-related functionality. The datetime, time, and calendar modules are the\nmain places to start. The datetime.datetime type, or simply datetime, is widely\nused:\nIn [14]: from datetime import datetime\nIn [15]: now = datetime.now()\nIn [16]: now\nOut[16]: datetime.datetime(2022, 8, 12, 14, 9, 11, 337033)\nIn [17]: now.year, now.month, now.day\nOut[17]: (2022, 8, 12)\ndatetime stores both the date and time down to the microsecond. datetime.time\ndelta, or simply timedelta, represents the temporal difference between two date\ntime objects:\nIn [18]: delta = datetime(2011, 1, 7) - datetime(2008, 6, 24, 8, 15)\nIn [19]: delta\nOut[19]: datetime.timedelta(days=926, seconds=56700)\nIn [20]: delta.days\nOut[20]: 926\nIn [21]: delta.seconds\nOut[21]: 56700\n358 \n| \nChapter 11: Time Series",
      "content_length": 1596,
      "extraction_method": "Direct"
    },
    {
      "page_number": 377,
      "chapter": null,
      "content": "You can add (or subtract) a timedelta or multiple thereof to a datetime object to\nyield a new shifted object:\nIn [22]: from datetime import timedelta\nIn [23]: start = datetime(2011, 1, 7)\nIn [24]: start + timedelta(12)\nOut[24]: datetime.datetime(2011, 1, 19, 0, 0)\nIn [25]: start - 2 * timedelta(12)\nOut[25]: datetime.datetime(2010, 12, 14, 0, 0)\nTable 11-1 summarizes the data types in the datetime module. While this chapter\nis mainly concerned with the data types in pandas and higher-level time series\nmanipulation, you may encounter the datetime-based types in many other places in\nPython in the wild.\nTable 11-1. Types in the datetime module\nType\nDescription\ndate\nStore calendar date (year, month, day) using the Gregorian calendar\ntime\nStore time of day as hours, minutes, seconds, and microseconds\ndatetime\nStore both date and time\ntimedelta\nThe difference between two datetime values (as days, seconds, and microseconds)\ntzinfo\nBase type for storing time zone information\nConverting Between String and Datetime\nYou can format datetime objects and pandas Timestamp objects, which I’ll introduce\nlater, as strings using str or the strftime method, passing a format specification:\nIn [26]: stamp = datetime(2011, 1, 3)\nIn [27]: str(stamp)\nOut[27]: '2011-01-03 00:00:00'\nIn [28]: stamp.strftime(\"%Y-%m-%d\")\nOut[28]: '2011-01-03'\nSee Table 11-2 for a complete list of the format codes.\nTable 11-2. datetime format specification (ISO C89 compatible)\nType\nDescription\n%Y\nFour-digit year\n%y\nTwo-digit year\n%m\nTwo-digit month [01, 12]\n11.1 Date and Time Data Types and Tools \n| \n359",
      "content_length": 1582,
      "extraction_method": "Direct"
    },
    {
      "page_number": 378,
      "chapter": null,
      "content": "Type\nDescription\n%d\nTwo-digit day [01, 31]\n%H\nHour (24-hour clock) [00, 23]\n%I\nHour (12-hour clock) [01, 12]\n%M\nTwo-digit minute [00, 59]\n%S\nSecond [00, 61] (seconds 60, 61 account for leap seconds)\n%f\nMicrosecond as an integer, zero-padded (from 000000 to 999999)\n%j\nDay of the year as a zero-padded integer (from 001 to 336)\n%w\nWeekday as an integer [0 (Sunday), 6]\n%u\nWeekday as an integer starting from 1, where 1 is Monday.\n%U\nWeek number of the year [00, 53]; Sunday is considered the first day of the week, and days before the first Sunday of\nthe year are “week 0”\n%W\nWeek number of the year [00, 53]; Monday is considered the first day of the week, and days before the first Monday of\nthe year are “week 0”\n%z\nUTC time zone offset as +HHMM or -HHMM; empty if time zone naive\n%Z\nTime zone name as a string, or empty string if no time zone\n%F\nShortcut for %Y-%m-%d (e.g., 2012-4-18)\n%D\nShortcut for %m/%d/%y (e.g., 04/18/12)\nYou can use many of the same format codes to convert strings to dates using date\ntime.strptime (but some codes, like %F, cannot be used):\nIn [29]: value = \"2011-01-03\"\nIn [30]: datetime.strptime(value, \"%Y-%m-%d\")\nOut[30]: datetime.datetime(2011, 1, 3, 0, 0)\nIn [31]: datestrs = [\"7/6/2011\", \"8/6/2011\"]\nIn [32]: [datetime.strptime(x, \"%m/%d/%Y\") for x in datestrs]\nOut[32]: \n[datetime.datetime(2011, 7, 6, 0, 0),\n datetime.datetime(2011, 8, 6, 0, 0)]\ndatetime.strptime is one way to parse a date with a known format.\npandas is generally oriented toward working with arrays of dates, whether used as\nan axis index or a column in a DataFrame. The pandas.to_datetime method parses\nmany different kinds of date representations. Standard date formats like ISO 8601 can\nbe parsed quickly:\nIn [33]: datestrs = [\"2011-07-06 12:00:00\", \"2011-08-06 00:00:00\"]\nIn [34]: pd.to_datetime(datestrs)\nOut[34]: DatetimeIndex(['2011-07-06 12:00:00', '2011-08-06 00:00:00'], dtype='dat\netime64[ns]', freq=None)\n360 \n| \nChapter 11: Time Series",
      "content_length": 1954,
      "extraction_method": "Direct"
    },
    {
      "page_number": 379,
      "chapter": null,
      "content": "It also handles values that should be considered missing (None, empty string, etc.):\nIn [35]: idx = pd.to_datetime(datestrs + [None])\nIn [36]: idx\nOut[36]: DatetimeIndex(['2011-07-06 12:00:00', '2011-08-06 00:00:00', 'NaT'], dty\npe='datetime64[ns]', freq=None)\nIn [37]: idx[2]\nOut[37]: NaT\nIn [38]: pd.isna(idx)\nOut[38]: array([False, False,  True])\nNaT (Not a Time) is pandas’s null value for timestamp data.\ndateutil.parser is a useful but imperfect tool. Notably, it will\nrecognize some strings as dates that you might prefer that it didn’t;\nfor example, \"42\" will be parsed as the year 2042 with today’s\ncalendar date.\ndatetime objects also have a number of locale-specific formatting options for systems\nin other countries or languages. For example, the abbreviated month names will\nbe different on German or French systems compared with English systems. See\nTable 11-3 for a listing.\nTable 11-3. Locale-specific date formatting\nType\nDescription\n%a\nAbbreviated weekday name\n%A\nFull weekday name\n%b\nAbbreviated month name\n%B\nFull month name\n%c\nFull date and time (e.g., ‘Tue 01 May 2012 04:20:57 PM’)\n%p\nLocale equivalent of AM or PM\n%x\nLocale-appropriate formatted date (e.g., in the United States, May 1, 2012 yields ’05/01/2012’)\n%X\nLocale-appropriate time (e.g., ’04:24:12 PM’)\n11.2 Time Series Basics\nA basic kind of time series object in pandas is a Series indexed by timestamps, which\nis often represented outside of pandas as Python strings or datetime objects:\nIn [39]: dates = [datetime(2011, 1, 2), datetime(2011, 1, 5),\n   ....:          datetime(2011, 1, 7), datetime(2011, 1, 8),\n   ....:          datetime(2011, 1, 10), datetime(2011, 1, 12)]\n11.2 Time Series Basics \n| \n361",
      "content_length": 1693,
      "extraction_method": "Direct"
    },
    {
      "page_number": 380,
      "chapter": null,
      "content": "In [40]: ts = pd.Series(np.random.standard_normal(6), index=dates)\nIn [41]: ts\nOut[41]: \n2011-01-02   -0.204708\n2011-01-05    0.478943\n2011-01-07   -0.519439\n2011-01-08   -0.555730\n2011-01-10    1.965781\n2011-01-12    1.393406\ndtype: float64\nUnder the hood, these datetime objects have been put in a DatetimeIndex:\nIn [42]: ts.index\nOut[42]: \nDatetimeIndex(['2011-01-02', '2011-01-05', '2011-01-07', '2011-01-08',\n               '2011-01-10', '2011-01-12'],\n              dtype='datetime64[ns]', freq=None)\nLike other Series, arithmetic operations between differently indexed time series auto‐\nmatically align on the dates:\nIn [43]: ts + ts[::2]\nOut[43]: \n2011-01-02   -0.409415\n2011-01-05         NaN\n2011-01-07   -1.038877\n2011-01-08         NaN\n2011-01-10    3.931561\n2011-01-12         NaN\ndtype: float64\nRecall that ts[::2] selects every second element in ts.\npandas stores timestamps using NumPy’s datetime64 data type at the nanosecond\nresolution:\nIn [44]: ts.index.dtype\nOut[44]: dtype('<M8[ns]')\nScalar values from a DatetimeIndex are pandas Timestamp objects:\nIn [45]: stamp = ts.index[0]\nIn [46]: stamp\nOut[46]: Timestamp('2011-01-02 00:00:00')\nA pandas.Timestamp can be substituted most places where you would use a datetime\nobject. The reverse is not true, however, because pandas.Timestamp can store nano‐\nsecond precision data, while datetime stores only up to microseconds. Additionally,\npandas.Timestamp can store frequency information (if any) and understands how to\n362 \n| \nChapter 11: Time Series",
      "content_length": 1516,
      "extraction_method": "Direct"
    },
    {
      "page_number": 381,
      "chapter": null,
      "content": "do time zone conversions and other kinds of manipulations. More on both of these\nthings later in Section 11.4, “Time Zone Handling,” on page 374.\nIndexing, Selection, Subsetting\nTime series behaves like any other Series when you are indexing and selecting data\nbased on the label:\nIn [47]: stamp = ts.index[2]\nIn [48]: ts[stamp]\nOut[48]: -0.5194387150567381\nAs a convenience, you can also pass a string that is interpretable as a date:\nIn [49]: ts[\"2011-01-10\"]\nOut[49]: 1.9657805725027142\nFor longer time series, a year or only a year and month can be passed to easily select\nslices of data (pandas.date_range is discussed in more detail in “Generating Date\nRanges” on page 367):\nIn [50]: longer_ts = pd.Series(np.random.standard_normal(1000),\n   ....:                       index=pd.date_range(\"2000-01-01\", periods=1000))\nIn [51]: longer_ts\nOut[51]: \n2000-01-01    0.092908\n2000-01-02    0.281746\n2000-01-03    0.769023\n2000-01-04    1.246435\n2000-01-05    1.007189\n                ...   \n2002-09-22    0.930944\n2002-09-23   -0.811676\n2002-09-24   -1.830156\n2002-09-25   -0.138730\n2002-09-26    0.334088\nFreq: D, Length: 1000, dtype: float64\nIn [52]: longer_ts[\"2001\"]\nOut[52]: \n2001-01-01    1.599534\n2001-01-02    0.474071\n2001-01-03    0.151326\n2001-01-04   -0.542173\n2001-01-05   -0.475496\n                ...   \n2001-12-27    0.057874\n2001-12-28   -0.433739\n2001-12-29    0.092698\n2001-12-30   -1.397820\n11.2 Time Series Basics \n| \n363",
      "content_length": 1443,
      "extraction_method": "Direct"
    },
    {
      "page_number": 382,
      "chapter": null,
      "content": "2001-12-31    1.457823\nFreq: D, Length: 365, dtype: float64\nHere, the string \"2001\" is interpreted as a year and selects that time period. This also\nworks if you specify the month:\nIn [53]: longer_ts[\"2001-05\"]\nOut[53]: \n2001-05-01   -0.622547\n2001-05-02    0.936289\n2001-05-03    0.750018\n2001-05-04   -0.056715\n2001-05-05    2.300675\n                ...   \n2001-05-27    0.235477\n2001-05-28    0.111835\n2001-05-29   -1.251504\n2001-05-30   -2.949343\n2001-05-31    0.634634\nFreq: D, Length: 31, dtype: float64\nSlicing with datetime objects works as well:\nIn [54]: ts[datetime(2011, 1, 7):]\nOut[54]: \n2011-01-07   -0.519439\n2011-01-08   -0.555730\n2011-01-10    1.965781\n2011-01-12    1.393406\ndtype: float64\nIn [55]: ts[datetime(2011, 1, 7):datetime(2011, 1, 10)]\nOut[55]: \n2011-01-07   -0.519439\n2011-01-08   -0.555730\n2011-01-10    1.965781\ndtype: float64\nBecause most time series data is ordered chronologically, you can slice with time‐\nstamps not contained in a time series to perform a range query:\nIn [56]: ts\nOut[56]: \n2011-01-02   -0.204708\n2011-01-05    0.478943\n2011-01-07   -0.519439\n2011-01-08   -0.555730\n2011-01-10    1.965781\n2011-01-12    1.393406\ndtype: float64\nIn [57]: ts[\"2011-01-06\":\"2011-01-11\"]\nOut[57]: \n2011-01-07   -0.519439\n364 \n| \nChapter 11: Time Series",
      "content_length": 1282,
      "extraction_method": "Direct"
    },
    {
      "page_number": 383,
      "chapter": null,
      "content": "2011-01-08   -0.555730\n2011-01-10    1.965781\ndtype: float64\nAs before, you can pass a string date, datetime, or timestamp. Remember that slicing\nin this manner produces views on the source time series, like slicing NumPy arrays.\nThis means that no data is copied, and modifications on the slice will be reflected in\nthe original data.\nThere is an equivalent instance method, truncate, that slices a Series between two\ndates:\nIn [58]: ts.truncate(after=\"2011-01-09\")\nOut[58]: \n2011-01-02   -0.204708\n2011-01-05    0.478943\n2011-01-07   -0.519439\n2011-01-08   -0.555730\ndtype: float64\nAll of this holds true for DataFrame as well, indexing on its rows:\nIn [59]: dates = pd.date_range(\"2000-01-01\", periods=100, freq=\"W-WED\")\nIn [60]: long_df = pd.DataFrame(np.random.standard_normal((100, 4)),\n   ....:                        index=dates,\n   ....:                        columns=[\"Colorado\", \"Texas\",\n   ....:                                 \"New York\", \"Ohio\"])\nIn [61]: long_df.loc[\"2001-05\"]\nOut[61]: \n            Colorado     Texas  New York      Ohio\n2001-05-02 -0.006045  0.490094 -0.277186 -0.707213\n2001-05-09 -0.560107  2.735527  0.927335  1.513906\n2001-05-16  0.538600  1.273768  0.667876 -0.969206\n2001-05-23  1.676091 -0.817649  0.050188  1.951312\n2001-05-30  3.260383  0.963301  1.201206 -1.852001\nTime Series with Duplicate Indices\nIn some applications, there may be multiple data observations falling on a particular\ntimestamp. Here is an example:\nIn [62]: dates = pd.DatetimeIndex([\"2000-01-01\", \"2000-01-02\", \"2000-01-02\",\n   ....:                           \"2000-01-02\", \"2000-01-03\"])\nIn [63]: dup_ts = pd.Series(np.arange(5), index=dates)\nIn [64]: dup_ts\nOut[64]: \n2000-01-01    0\n2000-01-02    1\n11.2 Time Series Basics \n| \n365",
      "content_length": 1747,
      "extraction_method": "Direct"
    },
    {
      "page_number": 384,
      "chapter": null,
      "content": "2000-01-02    2\n2000-01-02    3\n2000-01-03    4\ndtype: int64\nWe can tell that the index is not unique by checking its is_unique property:\nIn [65]: dup_ts.index.is_unique\nOut[65]: False\nIndexing into this time series will now either produce scalar values or slices, depend‐\ning on whether a timestamp is duplicated:\nIn [66]: dup_ts[\"2000-01-03\"]  # not duplicated\nOut[66]: 4\nIn [67]: dup_ts[\"2000-01-02\"]  # duplicated\nOut[67]: \n2000-01-02    1\n2000-01-02    2\n2000-01-02    3\ndtype: int64\nSuppose you wanted to aggregate the data having nonunique timestamps. One way to\ndo this is to use groupby and pass level=0 (the one and only level):\nIn [68]: grouped = dup_ts.groupby(level=0)\nIn [69]: grouped.mean()\nOut[69]: \n2000-01-01    0.0\n2000-01-02    2.0\n2000-01-03    4.0\ndtype: float64\nIn [70]: grouped.count()\nOut[70]: \n2000-01-01    1\n2000-01-02    3\n2000-01-03    1\ndtype: int64\n11.3 Date Ranges, Frequencies, and Shifting\nGeneric time series in pandas are assumed to be irregular; that is, they have no fixed\nfrequency. For many applications this is sufficient. However, it’s often desirable to\nwork relative to a fixed frequency, such as daily, monthly, or every 15 minutes, even\nif that means introducing missing values into a time series. Fortunately, pandas has\na full suite of standard time series frequencies and tools for resampling (discussed in\nmore detail later in Section 11.6, “Resampling and Frequency Conversion,” on page\n366 \n| \nChapter 11: Time Series",
      "content_length": 1470,
      "extraction_method": "Direct"
    },
    {
      "page_number": 385,
      "chapter": null,
      "content": "387), inferring frequencies, and generating fixed-frequency date ranges. For example,\nyou can convert the sample time series to fixed daily frequency by calling resample:\nIn [71]: ts\nOut[71]: \n2011-01-02   -0.204708\n2011-01-05    0.478943\n2011-01-07   -0.519439\n2011-01-08   -0.555730\n2011-01-10    1.965781\n2011-01-12    1.393406\ndtype: float64\nIn [72]: resampler = ts.resample(\"D\")\nIn [73]: resampler\nOut[73]: <pandas.core.resample.DatetimeIndexResampler object at 0x7febd896bc40>\nThe string \"D\" is interpreted as daily frequency.\nConversion between frequencies or resampling is a big enough topic to have its own\nsection later (Section 11.6, “Resampling and Frequency Conversion,” on page 387).\nHere, I’ll show you how to use the base frequencies and multiples thereof.\nGenerating Date Ranges\nWhile I used it previously without explanation, pandas.date_range is responsible\nfor generating a DatetimeIndex with an indicated length according to a particular\nfrequency:\nIn [74]: index = pd.date_range(\"2012-04-01\", \"2012-06-01\")\nIn [75]: index\nOut[75]: \nDatetimeIndex(['2012-04-01', '2012-04-02', '2012-04-03', '2012-04-04',\n               '2012-04-05', '2012-04-06', '2012-04-07', '2012-04-08',\n               '2012-04-09', '2012-04-10', '2012-04-11', '2012-04-12',\n               '2012-04-13', '2012-04-14', '2012-04-15', '2012-04-16',\n               '2012-04-17', '2012-04-18', '2012-04-19', '2012-04-20',\n               '2012-04-21', '2012-04-22', '2012-04-23', '2012-04-24',\n               '2012-04-25', '2012-04-26', '2012-04-27', '2012-04-28',\n               '2012-04-29', '2012-04-30', '2012-05-01', '2012-05-02',\n               '2012-05-03', '2012-05-04', '2012-05-05', '2012-05-06',\n               '2012-05-07', '2012-05-08', '2012-05-09', '2012-05-10',\n               '2012-05-11', '2012-05-12', '2012-05-13', '2012-05-14',\n               '2012-05-15', '2012-05-16', '2012-05-17', '2012-05-18',\n               '2012-05-19', '2012-05-20', '2012-05-21', '2012-05-22',\n               '2012-05-23', '2012-05-24', '2012-05-25', '2012-05-26',\n               '2012-05-27', '2012-05-28', '2012-05-29', '2012-05-30',\n               '2012-05-31', '2012-06-01'],\n              dtype='datetime64[ns]', freq='D')\n11.3 Date Ranges, Frequencies, and Shifting \n| \n367",
      "content_length": 2262,
      "extraction_method": "Direct"
    },
    {
      "page_number": 386,
      "chapter": null,
      "content": "By default, pandas.date_range generates daily timestamps. If you pass only a start or\nend date, you must pass a number of periods to generate:\nIn [76]: pd.date_range(start=\"2012-04-01\", periods=20)\nOut[76]: \nDatetimeIndex(['2012-04-01', '2012-04-02', '2012-04-03', '2012-04-04',\n               '2012-04-05', '2012-04-06', '2012-04-07', '2012-04-08',\n               '2012-04-09', '2012-04-10', '2012-04-11', '2012-04-12',\n               '2012-04-13', '2012-04-14', '2012-04-15', '2012-04-16',\n               '2012-04-17', '2012-04-18', '2012-04-19', '2012-04-20'],\n              dtype='datetime64[ns]', freq='D')\nIn [77]: pd.date_range(end=\"2012-06-01\", periods=20)\nOut[77]: \nDatetimeIndex(['2012-05-13', '2012-05-14', '2012-05-15', '2012-05-16',\n               '2012-05-17', '2012-05-18', '2012-05-19', '2012-05-20',\n               '2012-05-21', '2012-05-22', '2012-05-23', '2012-05-24',\n               '2012-05-25', '2012-05-26', '2012-05-27', '2012-05-28',\n               '2012-05-29', '2012-05-30', '2012-05-31', '2012-06-01'],\n              dtype='datetime64[ns]', freq='D')\nThe start and end dates define strict boundaries for the generated date index. For\nexample, if you wanted a date index containing the last business day of each month,\nyou would pass the \"BM\" frequency (business end of month; see a more complete\nlisting of frequencies in Table 11-4), and only dates falling on or inside the date\ninterval will be included:\nIn [78]: pd.date_range(\"2000-01-01\", \"2000-12-01\", freq=\"BM\")\nOut[78]: \nDatetimeIndex(['2000-01-31', '2000-02-29', '2000-03-31', '2000-04-28',\n               '2000-05-31', '2000-06-30', '2000-07-31', '2000-08-31',\n               '2000-09-29', '2000-10-31', '2000-11-30'],\n              dtype='datetime64[ns]', freq='BM')\nTable 11-4. Base time series frequencies (not comprehensive)\nAlias\nOffset type\nDescription\nD\nDay\nCalendar daily\nB\nBusinessDay\nBusiness daily\nH\nHour\nHourly\nT or min\nMinute\nOnce a minute\nS\nSecond\nOnce a second\nL or ms\nMilli\nMillisecond (1/1,000 of 1 second)\nU\nMicro\nMicrosecond (1/1,000,000 of 1 second)\nM\nMonthEnd\nLast calendar day of month\nBM\nBusinessMonthEnd\nLast business day (weekday) of month\nMS\nMonthBegin\nFirst calendar day of month\nBMS\nBusinessMonthBegin\nFirst weekday of month\n368 \n| \nChapter 11: Time Series",
      "content_length": 2272,
      "extraction_method": "Direct"
    },
    {
      "page_number": 387,
      "chapter": null,
      "content": "Alias\nOffset type\nDescription\nW-MON, W-TUE, ...\nWeek\nWeekly on given day of week (MON, TUE, WED, THU,\nFRI, SAT, or SUN)\nWOM-1MON, WOM-2MON, ...\nWeekOfMonth\nGenerate weekly dates in the first, second, third, or\nfourth week of the month (e.g., WOM-3FRI for the\nthird Friday of each month)\nQ-JAN, Q-FEB, ...\nQuarterEnd\nQuarterly dates anchored on last calendar day of each\nmonth, for year ending in indicated month (JAN, FEB,\nMAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, NOV, or DEC)\nBQ-JAN, BQ-FEB, ...\nBusinessQuarterEnd\nQuarterly dates anchored on last weekday day of each\nmonth, for year ending in indicated month\nQS-JAN, QS-FEB, ...\nQuarterBegin\nQuarterly dates anchored on first calendar day of each\nmonth, for year ending in indicated month\nBQS-JAN, BQS-FEB, ...\nBusinessQuarterBegin\nQuarterly dates anchored on first weekday day of each\nmonth, for year ending in indicated month\nA-JAN, A-FEB, ...\nYearEnd\nAnnual dates anchored on last calendar day of given\nmonth (JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP,\nOCT, NOV, or DEC)\nBA-JAN, BA-FEB, ...\nBusinessYearEnd\nAnnual dates anchored on last weekday of given\nmonth\nAS-JAN, AS-FEB, ...\nYearBegin\nAnnual dates anchored on first day of given month\nBAS-JAN, BAS-FEB, ...\nBusinessYearBegin\nAnnual dates anchored on first weekday of given\nmonth\npandas.date_range by default preserves the time (if any) of the start or end time‐\nstamp:\nIn [79]: pd.date_range(\"2012-05-02 12:56:31\", periods=5)\nOut[79]: \nDatetimeIndex(['2012-05-02 12:56:31', '2012-05-03 12:56:31',\n               '2012-05-04 12:56:31', '2012-05-05 12:56:31',\n               '2012-05-06 12:56:31'],\n              dtype='datetime64[ns]', freq='D')\nSometimes you will have start or end dates with time information but want to\ngenerate a set of timestamps normalized to midnight as a convention. To do this,\nthere is a normalize option:\nIn [80]: pd.date_range(\"2012-05-02 12:56:31\", periods=5, normalize=True)\nOut[80]: \nDatetimeIndex(['2012-05-02', '2012-05-03', '2012-05-04', '2012-05-05',\n               '2012-05-06'],\n              dtype='datetime64[ns]', freq='D')\n11.3 Date Ranges, Frequencies, and Shifting \n| \n369",
      "content_length": 2126,
      "extraction_method": "Direct"
    },
    {
      "page_number": 388,
      "chapter": null,
      "content": "Frequencies and Date Offsets\nFrequencies in pandas are composed of a base frequency and a multiplier. Base\nfrequencies are typically referred to by a string alias, like \"M\" for monthly or \"H\" for\nhourly. For each base frequency, there is an object referred to as a date offset. For\nexample, hourly frequency can be represented with the Hour class:\nIn [81]: from pandas.tseries.offsets import Hour, Minute\nIn [82]: hour = Hour()\nIn [83]: hour\nOut[83]: <Hour>\nYou can define a multiple of an offset by passing an integer:\nIn [84]: four_hours = Hour(4)\nIn [85]: four_hours\nOut[85]: <4 * Hours>\nIn most applications, you would never need to explicitly create one of these objects;\ninstead you’d use a string alias like \"H\" or \"4H\". Putting an integer before the base\nfrequency creates a multiple:\nIn [86]: pd.date_range(\"2000-01-01\", \"2000-01-03 23:59\", freq=\"4H\")\nOut[86]: \nDatetimeIndex(['2000-01-01 00:00:00', '2000-01-01 04:00:00',\n               '2000-01-01 08:00:00', '2000-01-01 12:00:00',\n               '2000-01-01 16:00:00', '2000-01-01 20:00:00',\n               '2000-01-02 00:00:00', '2000-01-02 04:00:00',\n               '2000-01-02 08:00:00', '2000-01-02 12:00:00',\n               '2000-01-02 16:00:00', '2000-01-02 20:00:00',\n               '2000-01-03 00:00:00', '2000-01-03 04:00:00',\n               '2000-01-03 08:00:00', '2000-01-03 12:00:00',\n               '2000-01-03 16:00:00', '2000-01-03 20:00:00'],\n              dtype='datetime64[ns]', freq='4H')\nMany offsets can be combined by addition:\nIn [87]: Hour(2) + Minute(30)\nOut[87]: <150 * Minutes>\nSimilarly, you can pass frequency strings, like \"1h30min\", that will effectively be\nparsed to the same expression:\nIn [88]: pd.date_range(\"2000-01-01\", periods=10, freq=\"1h30min\")\nOut[88]: \nDatetimeIndex(['2000-01-01 00:00:00', '2000-01-01 01:30:00',\n               '2000-01-01 03:00:00', '2000-01-01 04:30:00',\n               '2000-01-01 06:00:00', '2000-01-01 07:30:00',\n               '2000-01-01 09:00:00', '2000-01-01 10:30:00',\n370 \n| \nChapter 11: Time Series",
      "content_length": 2032,
      "extraction_method": "Direct"
    },
    {
      "page_number": 389,
      "chapter": null,
      "content": "'2000-01-01 12:00:00', '2000-01-01 13:30:00'],\n              dtype='datetime64[ns]', freq='90T')\nSome frequencies describe points in time that are not evenly spaced. For example,\n\"M\" (calendar month end) and \"BM\" (last business/weekday of month) depend on the\nnumber of days in a month and, in the latter case, whether the month ends on a\nweekend or not. We refer to these as anchored offsets.\nRefer to Table 11-4 for a listing of frequency codes and date offset classes available in\npandas.\nUsers can define their own custom frequency classes to provide\ndate logic not available in pandas, though the full details of that are\noutside the scope of this book.\nWeek of month dates\nOne useful frequency class is “week of month,” starting with WOM. This enables you to\nget dates like the third Friday of each month:\nIn [89]: monthly_dates = pd.date_range(\"2012-01-01\", \"2012-09-01\", freq=\"WOM-3FRI\n\")\nIn [90]: list(monthly_dates)\nOut[90]: \n[Timestamp('2012-01-20 00:00:00', freq='WOM-3FRI'),\n Timestamp('2012-02-17 00:00:00', freq='WOM-3FRI'),\n Timestamp('2012-03-16 00:00:00', freq='WOM-3FRI'),\n Timestamp('2012-04-20 00:00:00', freq='WOM-3FRI'),\n Timestamp('2012-05-18 00:00:00', freq='WOM-3FRI'),\n Timestamp('2012-06-15 00:00:00', freq='WOM-3FRI'),\n Timestamp('2012-07-20 00:00:00', freq='WOM-3FRI'),\n Timestamp('2012-08-17 00:00:00', freq='WOM-3FRI')]\nShifting (Leading and Lagging) Data\nShifting refers to moving data backward and forward through time. Both Series and\nDataFrame have a shift method for doing naive shifts forward or backward, leaving\nthe index unmodified:\nIn [91]: ts = pd.Series(np.random.standard_normal(4),\n   ....:                index=pd.date_range(\"2000-01-01\", periods=4, freq=\"M\"))\nIn [92]: ts\nOut[92]: \n2000-01-31   -0.066748\n2000-02-29    0.838639\n2000-03-31   -0.117388\n11.3 Date Ranges, Frequencies, and Shifting \n| \n371",
      "content_length": 1850,
      "extraction_method": "Direct"
    },
    {
      "page_number": 390,
      "chapter": null,
      "content": "2000-04-30   -0.517795\nFreq: M, dtype: float64\nIn [93]: ts.shift(2)\nOut[93]: \n2000-01-31         NaN\n2000-02-29         NaN\n2000-03-31   -0.066748\n2000-04-30    0.838639\nFreq: M, dtype: float64\nIn [94]: ts.shift(-2)\nOut[94]: \n2000-01-31   -0.117388\n2000-02-29   -0.517795\n2000-03-31         NaN\n2000-04-30         NaN\nFreq: M, dtype: float64\nWhen we shift like this, missing data is introduced either at the start or the end of the\ntime series.\nA common use of shift is computing consecutive percent changes in a time series or\nmultiple time series as DataFrame columns. This is expressed as:\nts / ts.shift(1) - 1\nBecause naive shifts leave the index unmodified, some data is discarded. Thus if the\nfrequency is known, it can be passed to shift to advance the timestamps instead of\nsimply the data:\nIn [95]: ts.shift(2, freq=\"M\")\nOut[95]: \n2000-03-31   -0.066748\n2000-04-30    0.838639\n2000-05-31   -0.117388\n2000-06-30   -0.517795\nFreq: M, dtype: float64\nOther frequencies can be passed, too, giving you some flexibility in how to lead and\nlag the data:\nIn [96]: ts.shift(3, freq=\"D\")\nOut[96]: \n2000-02-03   -0.066748\n2000-03-03    0.838639\n2000-04-03   -0.117388\n2000-05-03   -0.517795\ndtype: float64\nIn [97]: ts.shift(1, freq=\"90T\")\nOut[97]: \n2000-01-31 01:30:00   -0.066748\n372 \n| \nChapter 11: Time Series",
      "content_length": 1309,
      "extraction_method": "Direct"
    },
    {
      "page_number": 391,
      "chapter": null,
      "content": "2000-02-29 01:30:00    0.838639\n2000-03-31 01:30:00   -0.117388\n2000-04-30 01:30:00   -0.517795\ndtype: float64\nThe T here stands for minutes. Note that the freq parameter here indicates the offset\nto apply to the timestamps, but it does not change the underlying frequency of the\ndata, if any.\nShifting dates with offsets\nThe pandas date offsets can also be used with datetime or Timestamp objects:\nIn [98]: from pandas.tseries.offsets import Day, MonthEnd\nIn [99]: now = datetime(2011, 11, 17)\nIn [100]: now + 3 * Day()\nOut[100]: Timestamp('2011-11-20 00:00:00')\nIf you add an anchored offset like MonthEnd, the first increment will “roll forward” a\ndate to the next date according to the frequency rule:\nIn [101]: now + MonthEnd()\nOut[101]: Timestamp('2011-11-30 00:00:00')\nIn [102]: now + MonthEnd(2)\nOut[102]: Timestamp('2011-12-31 00:00:00')\nAnchored offsets can explicitly “roll” dates forward or backward by simply using their\nrollforward and rollback methods, respectively:\nIn [103]: offset = MonthEnd()\nIn [104]: offset.rollforward(now)\nOut[104]: Timestamp('2011-11-30 00:00:00')\nIn [105]: offset.rollback(now)\nOut[105]: Timestamp('2011-10-31 00:00:00')\nA creative use of date offsets is to use these methods with groupby:\nIn [106]: ts = pd.Series(np.random.standard_normal(20),\n   .....:                index=pd.date_range(\"2000-01-15\", periods=20, freq=\"4D\")\n)\nIn [107]: ts\nOut[107]: \n2000-01-15   -0.116696\n2000-01-19    2.389645\n2000-01-23   -0.932454\n2000-01-27   -0.229331\n2000-01-31   -1.140330\n11.3 Date Ranges, Frequencies, and Shifting \n| \n373",
      "content_length": 1562,
      "extraction_method": "Direct"
    },
    {
      "page_number": 392,
      "chapter": null,
      "content": "2000-02-04    0.439920\n2000-02-08   -0.823758\n2000-02-12   -0.520930\n2000-02-16    0.350282\n2000-02-20    0.204395\n2000-02-24    0.133445\n2000-02-28    0.327905\n2000-03-03    0.072153\n2000-03-07    0.131678\n2000-03-11   -1.297459\n2000-03-15    0.997747\n2000-03-19    0.870955\n2000-03-23   -0.991253\n2000-03-27    0.151699\n2000-03-31    1.266151\nFreq: 4D, dtype: float64\nIn [108]: ts.groupby(MonthEnd().rollforward).mean()\nOut[108]: \n2000-01-31   -0.005833\n2000-02-29    0.015894\n2000-03-31    0.150209\ndtype: float64\nOf course, an easier and faster way to do this is with resample (we’ll discuss this in\nmuch more depth in Section 11.6, “Resampling and Frequency Conversion,” on page\n387):\nIn [109]: ts.resample(\"M\").mean()\nOut[109]: \n2000-01-31   -0.005833\n2000-02-29    0.015894\n2000-03-31    0.150209\nFreq: M, dtype: float64\n11.4 Time Zone Handling\nWorking with time zones can be one of the most unpleasant parts of time series\nmanipulation. As a result, many time series users choose to work with time series in\ncoordinated universal time or UTC, which is the geography-independent international\nstandard. Time zones are expressed as offsets from UTC; for example, New York is\nfour hours behind UTC during daylight saving time (DST) and five hours behind the\nrest of the year.\nIn Python, time zone information comes from the third-party pytz library (installa‐\nble with pip or conda), which exposes the Olson database, a compilation of world\ntime zone information. This is especially important for historical data because the\nDST transition dates (and even UTC offsets) have been changed numerous times\n374 \n| \nChapter 11: Time Series",
      "content_length": 1638,
      "extraction_method": "Direct"
    },
    {
      "page_number": 393,
      "chapter": null,
      "content": "depending on the regional laws. In the United States, the DST transition times have\nbeen changed many times since 1900!\nFor detailed information about the pytz library, you’ll need to look at that library’s\ndocumentation. As far as this book is concerned, pandas wraps pytz’s functionality\nso you can ignore its API outside of the time zone names. Since pandas has a hard\ndependency on pytz, it isn’t necessary to install it separately. Time zone names can be\nfound interactively and in the docs:\nIn [110]: import pytz\nIn [111]: pytz.common_timezones[-5:]\nOut[111]: ['US/Eastern', 'US/Hawaii', 'US/Mountain', 'US/Pacific', 'UTC']\nTo get a time zone object from pytz, use pytz.timezone:\nIn [112]: tz = pytz.timezone(\"America/New_York\")\nIn [113]: tz\nOut[113]: <DstTzInfo 'America/New_York' LMT-1 day, 19:04:00 STD>\nMethods in pandas will accept either time zone names or these objects.\nTime Zone Localization and Conversion\nBy default, time series in pandas are time zone naive. For example, consider the\nfollowing time series:\nIn [114]: dates = pd.date_range(\"2012-03-09 09:30\", periods=6)\nIn [115]: ts = pd.Series(np.random.standard_normal(len(dates)), index=dates)\nIn [116]: ts\nOut[116]: \n2012-03-09 09:30:00   -0.202469\n2012-03-10 09:30:00    0.050718\n2012-03-11 09:30:00    0.639869\n2012-03-12 09:30:00    0.597594\n2012-03-13 09:30:00   -0.797246\n2012-03-14 09:30:00    0.472879\nFreq: D, dtype: float64\nThe index’s tz field is None:\nIn [117]: print(ts.index.tz)\nNone\nDate ranges can be generated with a time zone set:\nIn [118]: pd.date_range(\"2012-03-09 09:30\", periods=10, tz=\"UTC\")\nOut[118]: \nDatetimeIndex(['2012-03-09 09:30:00+00:00', '2012-03-10 09:30:00+00:00',\n               '2012-03-11 09:30:00+00:00', '2012-03-12 09:30:00+00:00',\n11.4 Time Zone Handling \n| \n375",
      "content_length": 1775,
      "extraction_method": "Direct"
    },
    {
      "page_number": 394,
      "chapter": null,
      "content": "'2012-03-13 09:30:00+00:00', '2012-03-14 09:30:00+00:00',\n               '2012-03-15 09:30:00+00:00', '2012-03-16 09:30:00+00:00',\n               '2012-03-17 09:30:00+00:00', '2012-03-18 09:30:00+00:00'],\n              dtype='datetime64[ns, UTC]', freq='D')\nConversion from naive to localized (reinterpreted as having been observed in a\nparticular time zone) is handled by the tz_localize method:\nIn [119]: ts\nOut[119]: \n2012-03-09 09:30:00   -0.202469\n2012-03-10 09:30:00    0.050718\n2012-03-11 09:30:00    0.639869\n2012-03-12 09:30:00    0.597594\n2012-03-13 09:30:00   -0.797246\n2012-03-14 09:30:00    0.472879\nFreq: D, dtype: float64\nIn [120]: ts_utc = ts.tz_localize(\"UTC\")\nIn [121]: ts_utc\nOut[121]: \n2012-03-09 09:30:00+00:00   -0.202469\n2012-03-10 09:30:00+00:00    0.050718\n2012-03-11 09:30:00+00:00    0.639869\n2012-03-12 09:30:00+00:00    0.597594\n2012-03-13 09:30:00+00:00   -0.797246\n2012-03-14 09:30:00+00:00    0.472879\nFreq: D, dtype: float64\nIn [122]: ts_utc.index\nOut[122]: \nDatetimeIndex(['2012-03-09 09:30:00+00:00', '2012-03-10 09:30:00+00:00',\n               '2012-03-11 09:30:00+00:00', '2012-03-12 09:30:00+00:00',\n               '2012-03-13 09:30:00+00:00', '2012-03-14 09:30:00+00:00'],\n              dtype='datetime64[ns, UTC]', freq='D')\nOnce a time series has been localized to a particular time zone, it can be converted to\nanother time zone with tz_convert:\nIn [123]: ts_utc.tz_convert(\"America/New_York\")\nOut[123]: \n2012-03-09 04:30:00-05:00   -0.202469\n2012-03-10 04:30:00-05:00    0.050718\n2012-03-11 05:30:00-04:00    0.639869\n2012-03-12 05:30:00-04:00    0.597594\n2012-03-13 05:30:00-04:00   -0.797246\n2012-03-14 05:30:00-04:00    0.472879\nFreq: D, dtype: float64\nIn the case of the preceding time series, which straddles a DST transition in the\nAmerica/New_York time zone, we could localize to US Eastern time and convert to,\nsay, UTC or Berlin time:\n376 \n| \nChapter 11: Time Series",
      "content_length": 1918,
      "extraction_method": "Direct"
    },
    {
      "page_number": 395,
      "chapter": null,
      "content": "In [124]: ts_eastern = ts.tz_localize(\"America/New_York\")\nIn [125]: ts_eastern.tz_convert(\"UTC\")\nOut[125]: \n2012-03-09 14:30:00+00:00   -0.202469\n2012-03-10 14:30:00+00:00    0.050718\n2012-03-11 13:30:00+00:00    0.639869\n2012-03-12 13:30:00+00:00    0.597594\n2012-03-13 13:30:00+00:00   -0.797246\n2012-03-14 13:30:00+00:00    0.472879\ndtype: float64\nIn [126]: ts_eastern.tz_convert(\"Europe/Berlin\")\nOut[126]: \n2012-03-09 15:30:00+01:00   -0.202469\n2012-03-10 15:30:00+01:00    0.050718\n2012-03-11 14:30:00+01:00    0.639869\n2012-03-12 14:30:00+01:00    0.597594\n2012-03-13 14:30:00+01:00   -0.797246\n2012-03-14 14:30:00+01:00    0.472879\ndtype: float64\ntz_localize and tz_convert are also instance methods on DatetimeIndex:\nIn [127]: ts.index.tz_localize(\"Asia/Shanghai\")\nOut[127]: \nDatetimeIndex(['2012-03-09 09:30:00+08:00', '2012-03-10 09:30:00+08:00',\n               '2012-03-11 09:30:00+08:00', '2012-03-12 09:30:00+08:00',\n               '2012-03-13 09:30:00+08:00', '2012-03-14 09:30:00+08:00'],\n              dtype='datetime64[ns, Asia/Shanghai]', freq=None)\nLocalizing naive timestamps also checks for ambiguous or non-\nexistent times around daylight saving time transitions.\nOperations with Time Zone-Aware Timestamp Objects\nSimilar to time series and date ranges, individual Timestamp objects similarly can\nbe localized from naive to time zone-aware and converted from one time zone to\nanother:\nIn [128]: stamp = pd.Timestamp(\"2011-03-12 04:00\")\nIn [129]: stamp_utc = stamp.tz_localize(\"utc\")\nIn [130]: stamp_utc.tz_convert(\"America/New_York\")\nOut[130]: Timestamp('2011-03-11 23:00:00-0500', tz='America/New_York')\n11.4 Time Zone Handling \n| \n377",
      "content_length": 1658,
      "extraction_method": "Direct"
    },
    {
      "page_number": 396,
      "chapter": null,
      "content": "You can also pass a time zone when creating the Timestamp:\nIn [131]: stamp_moscow = pd.Timestamp(\"2011-03-12 04:00\", tz=\"Europe/Moscow\")\nIn [132]: stamp_moscow\nOut[132]: Timestamp('2011-03-12 04:00:00+0300', tz='Europe/Moscow')\nTime zone-aware Timestamp objects internally store a UTC timestamp value as nano‐\nseconds since the Unix epoch (January 1, 1970), so changing the time zone does not\nalter the internal UTC value:\nIn [133]: stamp_utc.value\nOut[133]: 1299902400000000000\nIn [134]: stamp_utc.tz_convert(\"America/New_York\").value\nOut[134]: 1299902400000000000\nWhen performing time arithmetic using pandas’s DateOffset objects, pandas\nrespects daylight saving time transitions where possible. Here we construct time‐\nstamps that occur right before DST transitions (forward and backward). First, 30\nminutes before transitioning to DST:\nIn [135]: stamp = pd.Timestamp(\"2012-03-11 01:30\", tz=\"US/Eastern\")\nIn [136]: stamp\nOut[136]: Timestamp('2012-03-11 01:30:00-0500', tz='US/Eastern')\nIn [137]: stamp + Hour()\nOut[137]: Timestamp('2012-03-11 03:30:00-0400', tz='US/Eastern')\nThen, 90 minutes before transitioning out of DST:\nIn [138]: stamp = pd.Timestamp(\"2012-11-04 00:30\", tz=\"US/Eastern\")\nIn [139]: stamp\nOut[139]: Timestamp('2012-11-04 00:30:00-0400', tz='US/Eastern')\nIn [140]: stamp + 2 * Hour()\nOut[140]: Timestamp('2012-11-04 01:30:00-0500', tz='US/Eastern')\nOperations Between Different Time Zones\nIf two time series with different time zones are combined, the result will be UTC.\nSince the timestamps are stored under the hood in UTC, this is a straightforward\noperation and requires no conversion:\nIn [141]: dates = pd.date_range(\"2012-03-07 09:30\", periods=10, freq=\"B\")\nIn [142]: ts = pd.Series(np.random.standard_normal(len(dates)), index=dates)\nIn [143]: ts\nOut[143]: \n378 \n| \nChapter 11: Time Series",
      "content_length": 1820,
      "extraction_method": "Direct"
    },
    {
      "page_number": 397,
      "chapter": null,
      "content": "2012-03-07 09:30:00    0.522356\n2012-03-08 09:30:00   -0.546348\n2012-03-09 09:30:00   -0.733537\n2012-03-12 09:30:00    1.302736\n2012-03-13 09:30:00    0.022199\n2012-03-14 09:30:00    0.364287\n2012-03-15 09:30:00   -0.922839\n2012-03-16 09:30:00    0.312656\n2012-03-19 09:30:00   -1.128497\n2012-03-20 09:30:00   -0.333488\nFreq: B, dtype: float64\nIn [144]: ts1 = ts[:7].tz_localize(\"Europe/London\")\nIn [145]: ts2 = ts1[2:].tz_convert(\"Europe/Moscow\")\nIn [146]: result = ts1 + ts2\nIn [147]: result.index\nOut[147]: \nDatetimeIndex(['2012-03-07 09:30:00+00:00', '2012-03-08 09:30:00+00:00',\n               '2012-03-09 09:30:00+00:00', '2012-03-12 09:30:00+00:00',\n               '2012-03-13 09:30:00+00:00', '2012-03-14 09:30:00+00:00',\n               '2012-03-15 09:30:00+00:00'],\n              dtype='datetime64[ns, UTC]', freq=None)\nOperations between time zone-naive and time zone-aware data are not supported\nand will raise an exception.\n11.5 Periods and Period Arithmetic\nPeriods represent time spans, like days, months, quarters, or years. The pan\ndas.Period class represents this data type, requiring a string or integer and a sup‐\nported frequency from Table 11-4:\nIn [148]: p = pd.Period(\"2011\", freq=\"A-DEC\")\nIn [149]: p\nOut[149]: Period('2011', 'A-DEC')\nIn this case, the Period object represents the full time span from January 1, 2011,\nto December 31, 2011, inclusive. Conveniently, adding and subtracting integers from\nperiods has the effect of shifting their frequency:\nIn [150]: p + 5\nOut[150]: Period('2016', 'A-DEC')\nIn [151]: p - 2\nOut[151]: Period('2009', 'A-DEC')\n11.5 Periods and Period Arithmetic \n| \n379",
      "content_length": 1621,
      "extraction_method": "Direct"
    },
    {
      "page_number": 398,
      "chapter": null,
      "content": "If two periods have the same frequency, their difference is the number of units\nbetween them as a date offset:\nIn [152]: pd.Period(\"2014\", freq=\"A-DEC\") - p\nOut[152]: <3 * YearEnds: month=12>\nRegular ranges of periods can be constructed with the period_range function:\nIn [153]: periods = pd.period_range(\"2000-01-01\", \"2000-06-30\", freq=\"M\")\nIn [154]: periods\nOut[154]: PeriodIndex(['2000-01', '2000-02', '2000-03', '2000-04', '2000-05', '20\n00-06'], dtype='period[M]')\nThe PeriodIndex class stores a sequence of periods and can serve as an axis index in\nany pandas data structure:\nIn [155]: pd.Series(np.random.standard_normal(6), index=periods)\nOut[155]: \n2000-01   -0.514551\n2000-02   -0.559782\n2000-03   -0.783408\n2000-04   -1.797685\n2000-05   -0.172670\n2000-06    0.680215\nFreq: M, dtype: float64\nIf you have an array of strings, you can also use the PeriodIndex class, where all of its\nvalues are periods:\nIn [156]: values = [\"2001Q3\", \"2002Q2\", \"2003Q1\"]\nIn [157]: index = pd.PeriodIndex(values, freq=\"Q-DEC\")\nIn [158]: index\nOut[158]: PeriodIndex(['2001Q3', '2002Q2', '2003Q1'], dtype='period[Q-DEC]')\nPeriod Frequency Conversion\nPeriods and PeriodIndex objects can be converted to another frequency with their\nasfreq method. As an example, suppose we had an annual period and wanted to\nconvert it into a monthly period either at the start or end of the year. This can be\ndone like so:\nIn [159]: p = pd.Period(\"2011\", freq=\"A-DEC\")\nIn [160]: p\nOut[160]: Period('2011', 'A-DEC')\nIn [161]: p.asfreq(\"M\", how=\"start\")\nOut[161]: Period('2011-01', 'M')\nIn [162]: p.asfreq(\"M\", how=\"end\")\n380 \n| \nChapter 11: Time Series",
      "content_length": 1623,
      "extraction_method": "Direct"
    },
    {
      "page_number": 399,
      "chapter": null,
      "content": "Out[162]: Period('2011-12', 'M')\nIn [163]: p.asfreq(\"M\")\nOut[163]: Period('2011-12', 'M')\nYou can think of Period(\"2011\", \"A-DEC\") as being a sort of cursor pointing to a\nspan of time, subdivided by monthly periods. See Figure 11-1 for an illustration of\nthis. For a fiscal year ending on a month other than December, the corresponding\nmonthly subperiods are different:\nIn [164]: p = pd.Period(\"2011\", freq=\"A-JUN\")\nIn [165]: p\nOut[165]: Period('2011', 'A-JUN')\nIn [166]: p.asfreq(\"M\", how=\"start\")\nOut[166]: Period('2010-07', 'M')\nIn [167]: p.asfreq(\"M\", how=\"end\")\nOut[167]: Period('2011-06', 'M')\nFigure 11-1. Period frequency conversion illustration\nWhen you are converting from high to low frequency, pandas determines the subper‐\niod, depending on where the superperiod “belongs.” For example, in A-JUN frequency,\nthe month Aug-2011 is actually part of the 2012 period:\nIn [168]: p = pd.Period(\"Aug-2011\", \"M\")\nIn [169]: p.asfreq(\"A-JUN\")\nOut[169]: Period('2012', 'A-JUN')\nWhole PeriodIndex objects or time series can be similarly converted with the same\nsemantics:\nIn [170]: periods = pd.period_range(\"2006\", \"2009\", freq=\"A-DEC\")\nIn [171]: ts = pd.Series(np.random.standard_normal(len(periods)), index=periods)\nIn [172]: ts\nOut[172]: \n11.5 Periods and Period Arithmetic \n| \n381",
      "content_length": 1285,
      "extraction_method": "Direct"
    },
    {
      "page_number": 400,
      "chapter": null,
      "content": "2006    1.607578\n2007    0.200381\n2008   -0.834068\n2009   -0.302988\nFreq: A-DEC, dtype: float64\nIn [173]: ts.asfreq(\"M\", how=\"start\")\nOut[173]: \n2006-01    1.607578\n2007-01    0.200381\n2008-01   -0.834068\n2009-01   -0.302988\nFreq: M, dtype: float64\nHere, the annual periods are replaced with monthly periods corresponding to the first\nmonth falling within each annual period. If we instead wanted the last business day\nof each year, we can use the \"B\" frequency and indicate that we want the end of the\nperiod:\nIn [174]: ts.asfreq(\"B\", how=\"end\")\nOut[174]: \n2006-12-29    1.607578\n2007-12-31    0.200381\n2008-12-31   -0.834068\n2009-12-31   -0.302988\nFreq: B, dtype: float64\nQuarterly Period Frequencies\nQuarterly data is standard in accounting, finance, and other fields. Much quarterly\ndata is reported relative to a fiscal year end, typically the last calendar or business day\nof one of the 12 months of the year. Thus, the period 2012Q4 has a different meaning\ndepending on fiscal year end. pandas supports all 12 possible quarterly frequencies as\nQ-JAN through Q-DEC:\nIn [175]: p = pd.Period(\"2012Q4\", freq=\"Q-JAN\")\nIn [176]: p\nOut[176]: Period('2012Q4', 'Q-JAN')\nIn the case of a fiscal year ending in January, 2012Q4 runs from November 2011\nthrough January 2012, which you can check by converting to daily frequency:\nIn [177]: p.asfreq(\"D\", how=\"start\")\nOut[177]: Period('2011-11-01', 'D')\nIn [178]: p.asfreq(\"D\", how=\"end\")\nOut[178]: Period('2012-01-31', 'D')\nSee Figure 11-2 for an illustration.\n382 \n| \nChapter 11: Time Series",
      "content_length": 1535,
      "extraction_method": "Direct"
    },
    {
      "page_number": 401,
      "chapter": null,
      "content": "Figure 11-2. Different quarterly frequency conventions\nThus, it’s possible to do convenient period arithmetic; for example, to get the time‐\nstamp at 4 P.M. on the second-to-last business day of the quarter, you could do:\nIn [179]: p4pm = (p.asfreq(\"B\", how=\"end\") - 1).asfreq(\"T\", how=\"start\") + 16 * 6\n0\nIn [180]: p4pm\nOut[180]: Period('2012-01-30 16:00', 'T')\nIn [181]: p4pm.to_timestamp()\nOut[181]: Timestamp('2012-01-30 16:00:00')\nThe to_timestamp method returns the Timestamp at the start of the period by default.\nYou can generate quarterly ranges using pandas.period_range. The arithmetic is\nidentical, too:\nIn [182]: periods = pd.period_range(\"2011Q3\", \"2012Q4\", freq=\"Q-JAN\")\nIn [183]: ts = pd.Series(np.arange(len(periods)), index=periods)\nIn [184]: ts\nOut[184]: \n2011Q3    0\n2011Q4    1\n2012Q1    2\n2012Q2    3\n2012Q3    4\n2012Q4    5\nFreq: Q-JAN, dtype: int64\nIn [185]: new_periods = (periods.asfreq(\"B\", \"end\") - 1).asfreq(\"H\", \"start\") + 1\n6\n11.5 Periods and Period Arithmetic \n| \n383",
      "content_length": 999,
      "extraction_method": "Direct"
    },
    {
      "page_number": 402,
      "chapter": null,
      "content": "In [186]: ts.index = new_periods.to_timestamp()\nIn [187]: ts\nOut[187]: \n2010-10-28 16:00:00    0\n2011-01-28 16:00:00    1\n2011-04-28 16:00:00    2\n2011-07-28 16:00:00    3\n2011-10-28 16:00:00    4\n2012-01-30 16:00:00    5\ndtype: int64\nConverting Timestamps to Periods (and Back)\nSeries and DataFrame objects indexed by timestamps can be converted to periods\nwith the to_period method:\nIn [188]: dates = pd.date_range(\"2000-01-01\", periods=3, freq=\"M\")\nIn [189]: ts = pd.Series(np.random.standard_normal(3), index=dates)\nIn [190]: ts\nOut[190]: \n2000-01-31    1.663261\n2000-02-29   -0.996206\n2000-03-31    1.521760\nFreq: M, dtype: float64\nIn [191]: pts = ts.to_period()\nIn [192]: pts\nOut[192]: \n2000-01    1.663261\n2000-02   -0.996206\n2000-03    1.521760\nFreq: M, dtype: float64\nSince periods refer to nonoverlapping time spans, a timestamp can only belong to a\nsingle period for a given frequency. While the frequency of the new PeriodIndex is\ninferred from the timestamps by default, you can specify any supported frequency\n(most of those listed in Table 11-4 are supported). There is also no problem with\nhaving duplicate periods in the result:\nIn [193]: dates = pd.date_range(\"2000-01-29\", periods=6)\nIn [194]: ts2 = pd.Series(np.random.standard_normal(6), index=dates)\nIn [195]: ts2\nOut[195]: \n2000-01-29    0.244175\n2000-01-30    0.423331\n2000-01-31   -0.654040\n384 \n| \nChapter 11: Time Series",
      "content_length": 1397,
      "extraction_method": "Direct"
    },
    {
      "page_number": 403,
      "chapter": null,
      "content": "2000-02-01    2.089154\n2000-02-02   -0.060220\n2000-02-03   -0.167933\nFreq: D, dtype: float64\nIn [196]: ts2.to_period(\"M\")\nOut[196]: \n2000-01    0.244175\n2000-01    0.423331\n2000-01   -0.654040\n2000-02    2.089154\n2000-02   -0.060220\n2000-02   -0.167933\nFreq: M, dtype: float64\nTo convert back to timestamps, use the to_timestamp method, which returns a\nDatetimeIndex:\nIn [197]: pts = ts2.to_period()\nIn [198]: pts\nOut[198]: \n2000-01-29    0.244175\n2000-01-30    0.423331\n2000-01-31   -0.654040\n2000-02-01    2.089154\n2000-02-02   -0.060220\n2000-02-03   -0.167933\nFreq: D, dtype: float64\nIn [199]: pts.to_timestamp(how=\"end\")\nOut[199]: \n2000-01-29 23:59:59.999999999    0.244175\n2000-01-30 23:59:59.999999999    0.423331\n2000-01-31 23:59:59.999999999   -0.654040\n2000-02-01 23:59:59.999999999    2.089154\n2000-02-02 23:59:59.999999999   -0.060220\n2000-02-03 23:59:59.999999999   -0.167933\nFreq: D, dtype: float64\nCreating a PeriodIndex from Arrays\nFixed frequency datasets are sometimes stored with time span information spread\nacross multiple columns. For example, in this macroeconomic dataset, the year and\nquarter are in different columns:\nIn [200]: data = pd.read_csv(\"examples/macrodata.csv\")\nIn [201]: data.head(5)\nOut[201]: \n   year  quarter   realgdp  realcons  realinv  realgovt  realdpi    cpi  \\\n0  1959        1  2710.349    1707.4  286.898   470.045   1886.9  28.98   \n11.5 Periods and Period Arithmetic \n| \n385",
      "content_length": 1424,
      "extraction_method": "Direct"
    },
    {
      "page_number": 404,
      "chapter": null,
      "content": "1  1959        2  2778.801    1733.7  310.859   481.301   1919.7  29.15   \n2  1959        3  2775.488    1751.8  289.226   491.260   1916.4  29.35   \n3  1959        4  2785.204    1753.7  299.356   484.052   1931.3  29.37   \n4  1960        1  2847.699    1770.5  331.722   462.199   1955.5  29.54   \n      m1  tbilrate  unemp      pop  infl  realint  \n0  139.7      2.82    5.8  177.146  0.00     0.00  \n1  141.7      3.08    5.1  177.830  2.34     0.74  \n2  140.5      3.82    5.3  178.657  2.74     1.09  \n3  140.0      4.33    5.6  179.386  0.27     4.06  \n4  139.6      3.50    5.2  180.007  2.31     1.19  \nIn [202]: data[\"year\"]\nOut[202]: \n0      1959\n1      1959\n2      1959\n3      1959\n4      1960\n       ... \n198    2008\n199    2008\n200    2009\n201    2009\n202    2009\nName: year, Length: 203, dtype: int64\nIn [203]: data[\"quarter\"]\nOut[203]: \n0      1\n1      2\n2      3\n3      4\n4      1\n      ..\n198    3\n199    4\n200    1\n201    2\n202    3\nName: quarter, Length: 203, dtype: int64\nBy passing these arrays to PeriodIndex with a frequency, you can combine them to\nform an index for the DataFrame:\nIn [204]: index = pd.PeriodIndex(year=data[\"year\"], quarter=data[\"quarter\"],\n   .....:                        freq=\"Q-DEC\")\nIn [205]: index\nOut[205]: \nPeriodIndex(['1959Q1', '1959Q2', '1959Q3', '1959Q4', '1960Q1', '1960Q2',\n             '1960Q3', '1960Q4', '1961Q1', '1961Q2',\n             ...\n             '2007Q2', '2007Q3', '2007Q4', '2008Q1', '2008Q2', '2008Q3',\n386 \n| \nChapter 11: Time Series",
      "content_length": 1505,
      "extraction_method": "Direct"
    },
    {
      "page_number": 405,
      "chapter": null,
      "content": "'2008Q4', '2009Q1', '2009Q2', '2009Q3'],\n            dtype='period[Q-DEC]', length=203)\nIn [206]: data.index = index\nIn [207]: data[\"infl\"]\nOut[207]: \n1959Q1    0.00\n1959Q2    2.34\n1959Q3    2.74\n1959Q4    0.27\n1960Q1    2.31\n          ... \n2008Q3   -3.16\n2008Q4   -8.79\n2009Q1    0.94\n2009Q2    3.37\n2009Q3    3.56\nFreq: Q-DEC, Name: infl, Length: 203, dtype: float64\n11.6 Resampling and Frequency Conversion\nResampling refers to the process of converting a time series from one frequency\nto another. Aggregating higher frequency data to lower frequency is called downsam‐\npling, while converting lower frequency to higher frequency is called upsampling.\nNot all resampling falls into either of these categories; for example, converting W-WED\n(weekly on Wednesday) to W-FRI is neither upsampling nor downsampling.\npandas objects are equipped with a resample method, which is the workhorse func‐\ntion for all frequency conversion. resample has a similar API to groupby; you call\nresample to group the data, then call an aggregation function:\nIn [208]: dates = pd.date_range(\"2000-01-01\", periods=100)\nIn [209]: ts = pd.Series(np.random.standard_normal(len(dates)), index=dates)\nIn [210]: ts\nOut[210]: \n2000-01-01    0.631634\n2000-01-02   -1.594313\n2000-01-03   -1.519937\n2000-01-04    1.108752\n2000-01-05    1.255853\n                ...   \n2000-04-05   -0.423776\n2000-04-06    0.789740\n2000-04-07    0.937568\n2000-04-08   -2.253294\n2000-04-09   -1.772919\nFreq: D, Length: 100, dtype: float64\n11.6 Resampling and Frequency Conversion \n| \n387",
      "content_length": 1540,
      "extraction_method": "Direct"
    },
    {
      "page_number": 406,
      "chapter": null,
      "content": "In [211]: ts.resample(\"M\").mean()\nOut[211]: \n2000-01-31   -0.165893\n2000-02-29    0.078606\n2000-03-31    0.223811\n2000-04-30   -0.063643\nFreq: M, dtype: float64\nIn [212]: ts.resample(\"M\", kind=\"period\").mean()\nOut[212]: \n2000-01   -0.165893\n2000-02    0.078606\n2000-03    0.223811\n2000-04   -0.063643\nFreq: M, dtype: float64\nresample is a flexible method that can be used to process large time series. The exam‐\nples in the following sections illustrate its semantics and use. Table 11-5 summarizes\nsome of its options.\nTable 11-5. resample method arguments\nArgument\nDescription\nrule\nString, DateOffset, or timedelta indicating desired resampled frequency (for example, ‘M', ’5min', or\nSecond(15))\naxis\nAxis to resample on; default axis=0\nfill_method\nHow to interpolate when upsampling, as in \"ffill\" or \"bfill\"; by default does no interpolation\nclosed\nIn downsampling, which end of each interval is closed (inclusive), \"right\" or \"left\"\nlabel\nIn downsampling, how to label the aggregated result, with the \"right\" or \"left\" bin edge (e.g., the\n9:30 to 9:35 five-minute interval could be labeled 9:30 or 9:35)\nlimit\nWhen forward or backward filling, the maximum number of periods to fill\nkind\nAggregate to periods (\"period\") or timestamps (\"timestamp\"); defaults to the type of index the\ntime series has\nconvention\nWhen resampling periods, the convention (\"start\" or \"end\") for converting the low-frequency period\nto high frequency; defaults to \"start\"\norigin\nThe “base” timestamp from which to determine the resampling bin edges; can also be one of \"epoch\",\n\"start\", \"start_day\", \"end\", or \"end_day\"; see the resample docstring for full details\noffset\nAn offset timedelta added to the origin; defaults to None\nDownsampling\nDownsampling is aggregating data to a regular, lower frequency. The data you’re\naggregating doesn’t need to be fixed frequently; the desired frequency defines bin\nedges that are used to slice the time series into pieces to aggregate. For example,\nto convert to monthly, \"M\" or \"BM\", you need to chop up the data into one-month\nintervals. Each interval is said to be half-open; a data point can belong only to one\n388 \n| \nChapter 11: Time Series",
      "content_length": 2167,
      "extraction_method": "Direct"
    },
    {
      "page_number": 407,
      "chapter": null,
      "content": "1 The choice of the default values for closed and label might seem a bit odd to some users. The default\nis closed=\"left\" for all but a specific set (\"M\", \"A\", \"Q\", \"BM\", \"BQ\", and \"W\") for which the default is\nclosed=\"right\". The defaults were chosen to make the results more intuitive, but it is worth knowing that\nthe default is not always one or the other.\ninterval, and the union of the intervals must make up the whole time frame. There\nare a couple things to think about when using resample to downsample data:\n• Which side of each interval is closed\n•\n• How to label each aggregated bin, either with the start of the interval or the end\n•\nTo illustrate, let’s look at some one-minute frequency data:\nIn [213]: dates = pd.date_range(\"2000-01-01\", periods=12, freq=\"T\")\nIn [214]: ts = pd.Series(np.arange(len(dates)), index=dates)\nIn [215]: ts\nOut[215]: \n2000-01-01 00:00:00     0\n2000-01-01 00:01:00     1\n2000-01-01 00:02:00     2\n2000-01-01 00:03:00     3\n2000-01-01 00:04:00     4\n2000-01-01 00:05:00     5\n2000-01-01 00:06:00     6\n2000-01-01 00:07:00     7\n2000-01-01 00:08:00     8\n2000-01-01 00:09:00     9\n2000-01-01 00:10:00    10\n2000-01-01 00:11:00    11\nFreq: T, dtype: int64\nSuppose you wanted to aggregate this data into five-minute chunks or bars by taking\nthe sum of each group:\nIn [216]: ts.resample(\"5min\").sum()\nOut[216]: \n2000-01-01 00:00:00    10\n2000-01-01 00:05:00    35\n2000-01-01 00:10:00    21\nFreq: 5T, dtype: int64\nThe frequency you pass defines bin edges in five-minute increments. For this fre‐\nquency, by default the left bin edge is inclusive, so the 00:00 value is included in the\n00:00 to 00:05 interval, and the 00:05 value is excluded from that interval.1\nIn [217]: ts.resample(\"5min\", closed=\"right\").sum()\nOut[217]: \n1999-12-31 23:55:00     0\n11.6 Resampling and Frequency Conversion \n| \n389",
      "content_length": 1835,
      "extraction_method": "Direct"
    },
    {
      "page_number": 408,
      "chapter": null,
      "content": "2000-01-01 00:00:00    15\n2000-01-01 00:05:00    40\n2000-01-01 00:10:00    11\nFreq: 5T, dtype: int64\nThe resulting time series is labeled by the timestamps from the left side of each bin.\nBy passing label=\"right\" you can label them with the right bin edge:\nIn [218]: ts.resample(\"5min\", closed=\"right\", label=\"right\").sum()\nOut[218]: \n2000-01-01 00:00:00     0\n2000-01-01 00:05:00    15\n2000-01-01 00:10:00    40\n2000-01-01 00:15:00    11\nFreq: 5T, dtype: int64\nSee Figure 11-3 for an illustration of minute frequency data being resampled to\nfive-minute frequency.\nFigure 11-3. Five-minute resampling illustration of closed, label conventions\nLastly, you might want to shift the result index by some amount, say subtracting one\nsecond from the right edge to make it more clear which interval the timestamp refers\nto. To do this, add an offset to the resulting index:\nIn [219]: from pandas.tseries.frequencies import to_offset\nIn [220]: result = ts.resample(\"5min\", closed=\"right\", label=\"right\").sum()\nIn [221]: result.index = result.index + to_offset(\"-1s\")\nIn [222]: result\nOut[222]: \n1999-12-31 23:59:59     0\n2000-01-01 00:04:59    15\n2000-01-01 00:09:59    40\n2000-01-01 00:14:59    11\nFreq: 5T, dtype: int64\n390 \n| \nChapter 11: Time Series",
      "content_length": 1245,
      "extraction_method": "Direct"
    },
    {
      "page_number": 409,
      "chapter": null,
      "content": "Open-high-low-close (OHLC) resampling\nIn finance, a popular way to aggregate a time series is to compute four values for\neach bucket: the first (open), last (close), maximum (high), and minimal (low) values.\nBy using the ohlc aggregate function, you will obtain a DataFrame having columns\ncontaining these four aggregates, which are efficiently computed in a single function\ncall:\nIn [223]: ts = pd.Series(np.random.permutation(np.arange(len(dates))), index=date\ns)\nIn [224]: ts.resample(\"5min\").ohlc()\nOut[224]: \n                     open  high  low  close\n2000-01-01 00:00:00     8     8    1      5\n2000-01-01 00:05:00     6    11    2      2\n2000-01-01 00:10:00     0     7    0      7\nUpsampling and Interpolation\nUpsampling is converting from a lower frequency to a higher frequency, where no\naggregation is needed. Let’s consider a DataFrame with some weekly data:\nIn [225]: frame = pd.DataFrame(np.random.standard_normal((2, 4)),\n   .....:                      index=pd.date_range(\"2000-01-01\", periods=2,\n   .....:                                          freq=\"W-WED\"),\n   .....:                      columns=[\"Colorado\", \"Texas\", \"New York\", \"Ohio\"])\nIn [226]: frame\nOut[226]: \n            Colorado     Texas  New York      Ohio\n2000-01-05 -0.896431  0.927238  0.482284 -0.867130\n2000-01-12  0.493841 -0.155434  1.397286  1.507055\nWhen you are using an aggregation function with this data, there is only one value\nper group, and missing values result in the gaps. We use the asfreq method to\nconvert to the higher frequency without any aggregation:\nIn [227]: df_daily = frame.resample(\"D\").asfreq()\nIn [228]: df_daily\nOut[228]: \n            Colorado     Texas  New York      Ohio\n2000-01-05 -0.896431  0.927238  0.482284 -0.867130\n2000-01-06       NaN       NaN       NaN       NaN\n2000-01-07       NaN       NaN       NaN       NaN\n2000-01-08       NaN       NaN       NaN       NaN\n2000-01-09       NaN       NaN       NaN       NaN\n2000-01-10       NaN       NaN       NaN       NaN\n2000-01-11       NaN       NaN       NaN       NaN\n2000-01-12  0.493841 -0.155434  1.397286  1.507055\n11.6 Resampling and Frequency Conversion \n| \n391",
      "content_length": 2147,
      "extraction_method": "Direct"
    },
    {
      "page_number": 410,
      "chapter": null,
      "content": "Suppose you wanted to fill forward each weekly value on the non-Wednesdays. The\nsame filling or interpolation methods available in the fillna and reindex methods\nare available for resampling:\nIn [229]: frame.resample(\"D\").ffill()\nOut[229]: \n            Colorado     Texas  New York      Ohio\n2000-01-05 -0.896431  0.927238  0.482284 -0.867130\n2000-01-06 -0.896431  0.927238  0.482284 -0.867130\n2000-01-07 -0.896431  0.927238  0.482284 -0.867130\n2000-01-08 -0.896431  0.927238  0.482284 -0.867130\n2000-01-09 -0.896431  0.927238  0.482284 -0.867130\n2000-01-10 -0.896431  0.927238  0.482284 -0.867130\n2000-01-11 -0.896431  0.927238  0.482284 -0.867130\n2000-01-12  0.493841 -0.155434  1.397286  1.507055\nYou can similarly choose to only fill a certain number of periods forward to limit how\nfar to continue using an observed value:\nIn [230]: frame.resample(\"D\").ffill(limit=2)\nOut[230]: \n            Colorado     Texas  New York      Ohio\n2000-01-05 -0.896431  0.927238  0.482284 -0.867130\n2000-01-06 -0.896431  0.927238  0.482284 -0.867130\n2000-01-07 -0.896431  0.927238  0.482284 -0.867130\n2000-01-08       NaN       NaN       NaN       NaN\n2000-01-09       NaN       NaN       NaN       NaN\n2000-01-10       NaN       NaN       NaN       NaN\n2000-01-11       NaN       NaN       NaN       NaN\n2000-01-12  0.493841 -0.155434  1.397286  1.507055\nNotably, the new date index need not coincide with the old one at all:\nIn [231]: frame.resample(\"W-THU\").ffill()\nOut[231]: \n            Colorado     Texas  New York      Ohio\n2000-01-06 -0.896431  0.927238  0.482284 -0.867130\n2000-01-13  0.493841 -0.155434  1.397286  1.507055\nResampling with Periods\nResampling data indexed by periods is similar to timestamps:\nIn [232]: frame = pd.DataFrame(np.random.standard_normal((24, 4)),\n   .....:                      index=pd.period_range(\"1-2000\", \"12-2001\",\n   .....:                                            freq=\"M\"),\n   .....:                      columns=[\"Colorado\", \"Texas\", \"New York\", \"Ohio\"])\nIn [233]: frame.head()\nOut[233]: \n         Colorado     Texas  New York      Ohio\n2000-01 -1.179442  0.443171  1.395676 -0.529658\n2000-02  0.787358  0.248845  0.743239  1.267746\n392 \n| \nChapter 11: Time Series",
      "content_length": 2201,
      "extraction_method": "Direct"
    },
    {
      "page_number": 411,
      "chapter": null,
      "content": "2000-03  1.302395 -0.272154 -0.051532 -0.467740\n2000-04 -1.040816  0.426419  0.312945 -1.115689\n2000-05  1.234297 -1.893094 -1.661605 -0.005477\nIn [234]: annual_frame = frame.resample(\"A-DEC\").mean()\nIn [235]: annual_frame\nOut[235]: \n      Colorado     Texas  New York      Ohio\n2000  0.487329  0.104466  0.020495 -0.273945\n2001  0.203125  0.162429  0.056146 -0.103794\nUpsampling is more nuanced, as before resampling you must make a decision about\nwhich end of the time span in the new frequency to place the values. The convention\nargument defaults to \"start\" but can also be \"end\":\n# Q-DEC: Quarterly, year ending in December\nIn [236]: annual_frame.resample(\"Q-DEC\").ffill()\nOut[236]: \n        Colorado     Texas  New York      Ohio\n2000Q1  0.487329  0.104466  0.020495 -0.273945\n2000Q2  0.487329  0.104466  0.020495 -0.273945\n2000Q3  0.487329  0.104466  0.020495 -0.273945\n2000Q4  0.487329  0.104466  0.020495 -0.273945\n2001Q1  0.203125  0.162429  0.056146 -0.103794\n2001Q2  0.203125  0.162429  0.056146 -0.103794\n2001Q3  0.203125  0.162429  0.056146 -0.103794\n2001Q4  0.203125  0.162429  0.056146 -0.103794\nIn [237]: annual_frame.resample(\"Q-DEC\", convention=\"end\").asfreq()\nOut[237]: \n        Colorado     Texas  New York      Ohio\n2000Q4  0.487329  0.104466  0.020495 -0.273945\n2001Q1       NaN       NaN       NaN       NaN\n2001Q2       NaN       NaN       NaN       NaN\n2001Q3       NaN       NaN       NaN       NaN\n2001Q4  0.203125  0.162429  0.056146 -0.103794\nSince periods refer to time spans, the rules about upsampling and downsampling are\nmore rigid:\n• In downsampling, the target frequency must be a subperiod of the source\n•\nfrequency.\n• In upsampling, the target frequency must be a superperiod of the source\n•\nfrequency.\n11.6 Resampling and Frequency Conversion \n| \n393",
      "content_length": 1790,
      "extraction_method": "Direct"
    },
    {
      "page_number": 412,
      "chapter": null,
      "content": "If these rules are not satisfied, an exception will be raised. This mainly affects the\nquarterly, annual, and weekly frequencies; for example, the time spans defined by\nQ-MAR only line up with A-MAR, A-JUN, A-SEP, and A-DEC:\nIn [238]: annual_frame.resample(\"Q-MAR\").ffill()\nOut[238]: \n        Colorado     Texas  New York      Ohio\n2000Q4  0.487329  0.104466  0.020495 -0.273945\n2001Q1  0.487329  0.104466  0.020495 -0.273945\n2001Q2  0.487329  0.104466  0.020495 -0.273945\n2001Q3  0.487329  0.104466  0.020495 -0.273945\n2001Q4  0.203125  0.162429  0.056146 -0.103794\n2002Q1  0.203125  0.162429  0.056146 -0.103794\n2002Q2  0.203125  0.162429  0.056146 -0.103794\n2002Q3  0.203125  0.162429  0.056146 -0.103794\nGrouped Time Resampling\nFor time series data, the resample method is semantically a group operation based on\na time intervalization. Here’s a small example table:\nIn [239]: N = 15\nIn [240]: times = pd.date_range(\"2017-05-20 00:00\", freq=\"1min\", periods=N)\nIn [241]: df = pd.DataFrame({\"time\": times,\n   .....:                    \"value\": np.arange(N)})\nIn [242]: df\nOut[242]: \n                  time  value\n0  2017-05-20 00:00:00      0\n1  2017-05-20 00:01:00      1\n2  2017-05-20 00:02:00      2\n3  2017-05-20 00:03:00      3\n4  2017-05-20 00:04:00      4\n5  2017-05-20 00:05:00      5\n6  2017-05-20 00:06:00      6\n7  2017-05-20 00:07:00      7\n8  2017-05-20 00:08:00      8\n9  2017-05-20 00:09:00      9\n10 2017-05-20 00:10:00     10\n11 2017-05-20 00:11:00     11\n12 2017-05-20 00:12:00     12\n13 2017-05-20 00:13:00     13\n14 2017-05-20 00:14:00     14\n394 \n| \nChapter 11: Time Series",
      "content_length": 1596,
      "extraction_method": "Direct"
    },
    {
      "page_number": 413,
      "chapter": null,
      "content": "Here, we can index by \"time\" and then resample:\nIn [243]: df.set_index(\"time\").resample(\"5min\").count()\nOut[243]: \n                     value\ntime                      \n2017-05-20 00:00:00      5\n2017-05-20 00:05:00      5\n2017-05-20 00:10:00      5\nSuppose that a DataFrame contains multiple time series, marked by an additional\ngroup key column:\nIn [244]: df2 = pd.DataFrame({\"time\": times.repeat(3),\n   .....:                     \"key\": np.tile([\"a\", \"b\", \"c\"], N),\n   .....:                     \"value\": np.arange(N * 3.)})\nIn [245]: df2.head(7)\nOut[245]: \n                 time key  value\n0 2017-05-20 00:00:00   a    0.0\n1 2017-05-20 00:00:00   b    1.0\n2 2017-05-20 00:00:00   c    2.0\n3 2017-05-20 00:01:00   a    3.0\n4 2017-05-20 00:01:00   b    4.0\n5 2017-05-20 00:01:00   c    5.0\n6 2017-05-20 00:02:00   a    6.0\nTo do the same resampling for each value of \"key\", we introduce the pandas.Grouper\nobject:\nIn [246]: time_key = pd.Grouper(freq=\"5min\")\nWe can then set the time index, group by \"key\" and time_key, and aggregate:\nIn [247]: resampled = (df2.set_index(\"time\")\n   .....:              .groupby([\"key\", time_key])\n   .....:              .sum())\nIn [248]: resampled\nOut[248]: \n                         value\nkey time                      \na   2017-05-20 00:00:00   30.0\n    2017-05-20 00:05:00  105.0\n    2017-05-20 00:10:00  180.0\nb   2017-05-20 00:00:00   35.0\n    2017-05-20 00:05:00  110.0\n    2017-05-20 00:10:00  185.0\nc   2017-05-20 00:00:00   40.0\n    2017-05-20 00:05:00  115.0\n    2017-05-20 00:10:00  190.0\nIn [249]: resampled.reset_index()\n11.6 Resampling and Frequency Conversion \n| \n395",
      "content_length": 1618,
      "extraction_method": "Direct"
    },
    {
      "page_number": 414,
      "chapter": null,
      "content": "Out[249]: \n  key                time  value\n0   a 2017-05-20 00:00:00   30.0\n1   a 2017-05-20 00:05:00  105.0\n2   a 2017-05-20 00:10:00  180.0\n3   b 2017-05-20 00:00:00   35.0\n4   b 2017-05-20 00:05:00  110.0\n5   b 2017-05-20 00:10:00  185.0\n6   c 2017-05-20 00:00:00   40.0\n7   c 2017-05-20 00:05:00  115.0\n8   c 2017-05-20 00:10:00  190.0\nOne constraint with using pandas.Grouper is that the time must be the index of the\nSeries or DataFrame.\n11.7 Moving Window Functions\nAn important class of array transformations used for time series operations are\nstatistics and other functions evaluated over a sliding window or with exponentially\ndecaying weights. This can be useful for smoothing noisy or gappy data. I call these\nmoving window functions, even though they include functions without a fixed-length\nwindow like exponentially weighted moving average. Like other statistical functions,\nthese also automatically exclude missing data.\nBefore digging in, we can load up some time series data and resample it to business\nday frequency:\nIn [250]: close_px_all = pd.read_csv(\"examples/stock_px.csv\",\n   .....:                            parse_dates=True, index_col=0)\nIn [251]: close_px = close_px_all[[\"AAPL\", \"MSFT\", \"XOM\"]]\nIn [252]: close_px = close_px.resample(\"B\").ffill()\nI now introduce the rolling operator, which behaves similarly to resample and\ngroupby. It can be called on a Series or DataFrame along with a window (expressed as\na number of periods; see Figure 11-4 for the plot created):\nIn [253]: close_px[\"AAPL\"].plot()\nOut[253]: <AxesSubplot:>\nIn [254]: close_px[\"AAPL\"].rolling(250).mean().plot()\n396 \n| \nChapter 11: Time Series",
      "content_length": 1646,
      "extraction_method": "Direct"
    },
    {
      "page_number": 415,
      "chapter": null,
      "content": "Figure 11-4. Apple price with 250-day moving average\nThe expression rolling(250) is similar in behavior to groupby, but instead of group‐\ning, it creates an object that enables grouping over a 250-day sliding window. So here\nwe have the 250-day moving window average of Apple’s stock price.\nBy default, rolling functions require all of the values in the window to be non-NA.\nThis behavior can be changed to account for missing data and, in particular, the fact\nthat you will have fewer than window periods of data at the beginning of the time\nseries (see Figure 11-5):\nIn [255]: plt.figure()\nOut[255]: <Figure size 1000x600 with 0 Axes>\nIn [256]: std250 = close_px[\"AAPL\"].pct_change().rolling(250, min_periods=10).std\n()\nIn [257]: std250[5:12]\nOut[257]: \n2003-01-09         NaN\n2003-01-10         NaN\n2003-01-13         NaN\n2003-01-14         NaN\n2003-01-15         NaN\n2003-01-16    0.009628\n2003-01-17    0.013818\nFreq: B, Name: AAPL, dtype: float64\nIn [258]: std250.plot()\n11.7 Moving Window Functions \n| \n397",
      "content_length": 1013,
      "extraction_method": "Direct"
    },
    {
      "page_number": 416,
      "chapter": null,
      "content": "Figure 11-5. Apple 250-day daily return standard deviation\nTo compute an expanding window mean, use the expanding operator instead of\nrolling. The expanding mean starts the time window from the same point as the\nrolling window and increases the size of the window until it encompasses the whole\nseries. An expanding window mean on the std250 time series looks like this:\nIn [259]: expanding_mean = std250.expanding().mean()\nCalling a moving window function on a DataFrame applies the transformation to\neach column (see Figure 11-6):\nIn [261]: plt.style.use('grayscale')\nIn [262]: close_px.rolling(60).mean().plot(logy=True)\n398 \n| \nChapter 11: Time Series",
      "content_length": 655,
      "extraction_method": "Direct"
    },
    {
      "page_number": 417,
      "chapter": null,
      "content": "Figure 11-6. Stock prices 60-day moving average (log y-axis)\nThe rolling function also accepts a string indicating a fixed-size time offset rolling()\nin moving window functions rather than a set number of periods. Using this notation\ncan be useful for irregular time series. These are the same strings that you can pass to\nresample. For example, we could compute a 20-day rolling mean like so:\nIn [263]: close_px.rolling(\"20D\").mean()\nOut[263]: \n                  AAPL       MSFT        XOM\n2003-01-02    7.400000  21.110000  29.220000\n2003-01-03    7.425000  21.125000  29.230000\n2003-01-06    7.433333  21.256667  29.473333\n2003-01-07    7.432500  21.425000  29.342500\n2003-01-08    7.402000  21.402000  29.240000\n...                ...        ...        ...\n2011-10-10  389.351429  25.602143  72.527857\n2011-10-11  388.505000  25.674286  72.835000\n2011-10-12  388.531429  25.810000  73.400714\n2011-10-13  388.826429  25.961429  73.905000\n2011-10-14  391.038000  26.048667  74.185333\n[2292 rows x 3 columns]\nExponentially Weighted Functions\nAn alternative to using a fixed window size with equally weighted observations is\nto specify a constant decay factor to give more weight to more recent observations.\nThere are a couple of ways to specify the decay factor. A popular one is using a\n11.7 Moving Window Functions \n| \n399",
      "content_length": 1326,
      "extraction_method": "Direct"
    },
    {
      "page_number": 418,
      "chapter": null,
      "content": "span, which makes the result comparable to a simple moving window function with\nwindow size equal to the span.\nSince an exponentially weighted statistic places more weight on more recent observa‐\ntions, it “adapts” faster to changes compared with the equal-weighted version.\npandas has the ewm operator (which stands for exponentially weighted moving) to go\nalong with rolling and expanding. Here’s an example comparing a 30-day moving\naverage of Apple’s stock price with an exponentially weighted (EW) moving average\nwith span=60 (see Figure 11-7):\nIn [265]: aapl_px = close_px[\"AAPL\"][\"2006\":\"2007\"]\nIn [266]: ma30 = aapl_px.rolling(30, min_periods=20).mean()\nIn [267]: ewma30 = aapl_px.ewm(span=30).mean()\nIn [268]: aapl_px.plot(style=\"k-\", label=\"Price\")\nOut[268]: <AxesSubplot:>\nIn [269]: ma30.plot(style=\"k--\", label=\"Simple Moving Avg\")\nOut[269]: <AxesSubplot:>\nIn [270]: ewma30.plot(style=\"k-\", label=\"EW MA\")\nOut[270]: <AxesSubplot:>\nIn [271]: plt.legend()\nFigure 11-7. Simple moving average versus exponentially weighted\n400 \n| \nChapter 11: Time Series",
      "content_length": 1062,
      "extraction_method": "Direct"
    },
    {
      "page_number": 419,
      "chapter": null,
      "content": "Binary Moving Window Functions\nSome statistical operators, like correlation and covariance, need to operate on two\ntime series. As an example, financial analysts are often interested in a stock’s correla‐\ntion to a benchmark index like the S&P 500. To have a look at this, we first compute\nthe percent change for all of our time series of interest:\nIn [273]: spx_px = close_px_all[\"SPX\"]\nIn [274]: spx_rets = spx_px.pct_change()\nIn [275]: returns = close_px.pct_change()\nAfter we call rolling, the corr aggregation function can then compute the rolling\ncorrelation with spx_rets (see Figure 11-8 for the resulting plot):\nIn [276]: corr = returns[\"AAPL\"].rolling(125, min_periods=100).corr(spx_rets)\nIn [277]: corr.plot()\nFigure 11-8. Six-month AAPL return correlation to S&P 500\nSuppose you wanted to compute the rolling correlation of the S&P 500 index with\nmany stocks at once. You could write a loop computing this for each stock like we did\nfor Apple above, but if each stock is a column in a single DataFrame, we can compute\nall of the rolling correlations in one shot by calling rolling on the DataFrame and\npassing the spx_rets Series.\n11.7 Moving Window Functions \n| \n401",
      "content_length": 1179,
      "extraction_method": "Direct"
    },
    {
      "page_number": 420,
      "chapter": null,
      "content": "See Figure 11-9 for the plot of the result:\nIn [279]: corr = returns.rolling(125, min_periods=100).corr(spx_rets)\nIn [280]: corr.plot()\nFigure 11-9. Six-month return correlations to S&P 500\nUser-Defined Moving Window Functions\nThe apply method on rolling and related methods provides a way to apply an array\nfunction of your own creation over a moving window. The only requirement is that\nthe function produce a single value (a reduction) from each piece of the array. For\nexample, while we can compute sample quantiles using rolling(...).quantile(q),\nwe might be interested in the percentile rank of a particular value over the sample.\nThe scipy.stats.percentileofscore function does just this (see Figure 11-10 for\nthe resulting plot):\nIn [282]: from scipy.stats import percentileofscore\nIn [283]: def score_at_2percent(x):\n   .....:     return percentileofscore(x, 0.02)\nIn [284]: result = returns[\"AAPL\"].rolling(250).apply(score_at_2percent)\nIn [285]: result.plot()\n402 \n| \nChapter 11: Time Series",
      "content_length": 1002,
      "extraction_method": "Direct"
    },
    {
      "page_number": 421,
      "chapter": null,
      "content": "Figure 11-10. Percentile rank of 2% AAPL return over one-year window\nIf you don’t have SciPy installed already, you can install it with conda or pip:\nconda install scipy\n11.8 Conclusion\nTime series data calls for different types of analysis and data transformation tools\nthan the other types of data we have explored in previous chapters.\nIn the following chapter, we will show how to start using modeling libraries like\nstatsmodels and scikit-learn.\n11.8 Conclusion \n| \n403",
      "content_length": 474,
      "extraction_method": "Direct"
    },
    {
      "page_number": 422,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 423,
      "chapter": null,
      "content": "CHAPTER 12\nIntroduction to Modeling\nLibraries in Python\nIn this book, I have focused on providing a programming foundation for doing\ndata analysis in Python. Since data analysts and scientists often report spending a\ndisproportionate amount of time with data wrangling and preparation, the book’s\nstructure reflects the importance of mastering these techniques.\nWhich library you use for developing models will depend on the application. Many\nstatistical problems can be solved by simpler techniques like ordinary least squares\nregression, while other problems may call for more advanced machine learning\nmethods. Fortunately, Python has become one of the languages of choice for imple‐\nmenting analytical methods, so there are many tools you can explore after completing\nthis book.\nIn this chapter, I will review some features of pandas that may be helpful when\nyou’re crossing back and forth between data wrangling with pandas and model fitting\nand scoring. I will then give short introductions to two popular modeling toolkits,\nstatsmodels and scikit-learn. Since each of these projects is large enough to warrant\nits own dedicated book, I make no effort to be comprehensive and instead direct you\nto both projects’ online documentation along with some other Python-based books\non data science, statistics, and machine learning.\n12.1 Interfacing Between pandas and Model Code\nA common workflow for model development is to use pandas for data loading and\ncleaning before switching over to a modeling library to build the model itself. An\nimportant part of the model development process is called feature engineering in\nmachine learning. This can describe any data transformation or analytics that extract\n405",
      "content_length": 1710,
      "extraction_method": "Direct"
    },
    {
      "page_number": 424,
      "chapter": null,
      "content": "information from a raw dataset that may be useful in a modeling context. The data\naggregation and GroupBy tools we have explored in this book are used often in a\nfeature engineering context.\nWhile details of “good” feature engineering are out of scope for this book, I will\nshow some methods to make switching between data manipulation with pandas and\nmodeling as painless as possible.\nThe point of contact between pandas and other analysis libraries is usually NumPy\narrays. To turn a DataFrame into a NumPy array, use the to_numpy method:\nIn [12]: data = pd.DataFrame({\n   ....:     'x0': [1, 2, 3, 4, 5],\n   ....:     'x1': [0.01, -0.01, 0.25, -4.1, 0.],\n   ....:     'y': [-1.5, 0., 3.6, 1.3, -2.]})\nIn [13]: data\nOut[13]: \n   x0    x1    y\n0   1  0.01 -1.5\n1   2 -0.01  0.0\n2   3  0.25  3.6\n3   4 -4.10  1.3\n4   5  0.00 -2.0\nIn [14]: data.columns\nOut[14]: Index(['x0', 'x1', 'y'], dtype='object')\nIn [15]: data.to_numpy()\nOut[15]: \narray([[ 1.  ,  0.01, -1.5 ],\n       [ 2.  , -0.01,  0.  ],\n       [ 3.  ,  0.25,  3.6 ],\n       [ 4.  , -4.1 ,  1.3 ],\n       [ 5.  ,  0.  , -2.  ]])\nTo convert back to a DataFrame, as you may recall from earlier chapters, you can pass\na two-dimensional ndarray with optional column names:\nIn [16]: df2 = pd.DataFrame(data.to_numpy(), columns=['one', 'two', 'three'])\nIn [17]: df2\nOut[17]: \n   one   two  three\n0  1.0  0.01   -1.5\n1  2.0 -0.01    0.0\n2  3.0  0.25    3.6\n3  4.0 -4.10    1.3\n4  5.0  0.00   -2.0\n406 \n| \nChapter 12: Introduction to Modeling Libraries in Python",
      "content_length": 1513,
      "extraction_method": "Direct"
    },
    {
      "page_number": 425,
      "chapter": null,
      "content": "The to_numpy method is intended to be used when your data is homogeneous—for\nexample, all numeric types. If you have heterogeneous data, the result will be an\nndarray of Python objects:\nIn [18]: df3 = data.copy()\nIn [19]: df3['strings'] = ['a', 'b', 'c', 'd', 'e']\nIn [20]: df3\nOut[20]: \n   x0    x1    y strings\n0   1  0.01 -1.5       a\n1   2 -0.01  0.0       b\n2   3  0.25  3.6       c\n3   4 -4.10  1.3       d\n4   5  0.00 -2.0       e\nIn [21]: df3.to_numpy()\nOut[21]: \narray([[1, 0.01, -1.5, 'a'],\n       [2, -0.01, 0.0, 'b'],\n       [3, 0.25, 3.6, 'c'],\n       [4, -4.1, 1.3, 'd'],\n       [5, 0.0, -2.0, 'e']], dtype=object)\nFor some models, you may wish to use only a subset of the columns. I recommend\nusing loc indexing with to_numpy:\nIn [22]: model_cols = ['x0', 'x1']\nIn [23]: data.loc[:, model_cols].to_numpy()\nOut[23]: \narray([[ 1.  ,  0.01],\n       [ 2.  , -0.01],\n       [ 3.  ,  0.25],\n       [ 4.  , -4.1 ],\n       [ 5.  ,  0.  ]])\nSome libraries have native support for pandas and do some of this work for you\nautomatically: converting to NumPy from DataFrame and attaching model parameter\nnames to the columns of output tables or Series. In other cases, you will have to\nperform this “metadata management” manually.\nIn Section 7.5, “Categorical Data,” on page 235, we looked at pandas’s Categorical\ntype and the pandas.get_dummies function. Suppose we had a nonnumeric column\nin our example dataset:\nIn [24]: data['category'] = pd.Categorical(['a', 'b', 'a', 'a', 'b'],\n   ....:                                   categories=['a', 'b'])\nIn [25]: data\nOut[25]: \n12.1 Interfacing Between pandas and Model Code \n| \n407",
      "content_length": 1631,
      "extraction_method": "Direct"
    },
    {
      "page_number": 426,
      "chapter": null,
      "content": "x0    x1    y category\n0   1  0.01 -1.5        a\n1   2 -0.01  0.0        b\n2   3  0.25  3.6        a\n3   4 -4.10  1.3        a\n4   5  0.00 -2.0        b\nIf we wanted to replace the 'category' column with dummy variables, we create\ndummy variables, drop the 'category' column, and then join the result:\nIn [26]: dummies = pd.get_dummies(data.category, prefix='category')\nIn [27]: data_with_dummies = data.drop('category', axis=1).join(dummies)\nIn [28]: data_with_dummies\nOut[28]: \n   x0    x1    y  category_a  category_b\n0   1  0.01 -1.5           1           0\n1   2 -0.01  0.0           0           1\n2   3  0.25  3.6           1           0\n3   4 -4.10  1.3           1           0\n4   5  0.00 -2.0           0           1\nThere are some nuances to fitting certain statistical models with dummy variables.\nIt may be simpler and less error-prone to use Patsy (the subject of the next section)\nwhen you have more than simple numeric columns.\n12.2 Creating Model Descriptions with Patsy\nPatsy is a Python library for describing statistical models (especially linear models)\nwith a string-based “formula syntax,” which is inspired by (but not exactly the same\nas) the formula syntax used by the R and S statistical programming languages. It is\ninstalled automatically when you install statsmodels:\nconda install statsmodels\nPatsy is well supported for specifying linear models in statsmodels, so I will focus\non some of the main features to help you get up and running. Patsy’s formulas are a\nspecial string syntax that looks like:\ny ~ x0 + x1\nThe syntax a + b does not mean to add a to b, but rather that these are terms in the\ndesign matrix created for the model. The patsy.dmatrices function takes a formula\nstring along with a dataset (which can be a DataFrame or a dictionary of arrays) and\nproduces design matrices for a linear model:\nIn [29]: data = pd.DataFrame({\n   ....:     'x0': [1, 2, 3, 4, 5],\n   ....:     'x1': [0.01, -0.01, 0.25, -4.1, 0.],\n   ....:     'y': [-1.5, 0., 3.6, 1.3, -2.]})\n408 \n| \nChapter 12: Introduction to Modeling Libraries in Python",
      "content_length": 2067,
      "extraction_method": "Direct"
    },
    {
      "page_number": 427,
      "chapter": null,
      "content": "In [30]: data\nOut[30]: \n   x0    x1    y\n0   1  0.01 -1.5\n1   2 -0.01  0.0\n2   3  0.25  3.6\n3   4 -4.10  1.3\n4   5  0.00 -2.0\nIn [31]: import patsy\nIn [32]: y, X = patsy.dmatrices('y ~ x0 + x1', data)\nNow we have:\nIn [33]: y\nOut[33]: \nDesignMatrix with shape (5, 1)\n     y\n  -1.5\n   0.0\n   3.6\n   1.3\n  -2.0\n  Terms:\n    'y' (column 0)\nIn [34]: X\nOut[34]: \nDesignMatrix with shape (5, 3)\n  Intercept  x0     x1\n          1   1   0.01\n          1   2  -0.01\n          1   3   0.25\n          1   4  -4.10\n          1   5   0.00\n  Terms:\n    'Intercept' (column 0)\n    'x0' (column 1)\n    'x1' (column 2)\nThese Patsy DesignMatrix instances are NumPy ndarrays with additional metadata:\nIn [35]: np.asarray(y)\nOut[35]: \narray([[-1.5],\n       [ 0. ],\n       [ 3.6],\n       [ 1.3],\n       [-2. ]])\nIn [36]: np.asarray(X)\n12.2 Creating Model Descriptions with Patsy \n| \n409",
      "content_length": 865,
      "extraction_method": "Direct"
    },
    {
      "page_number": 428,
      "chapter": null,
      "content": "Out[36]: \narray([[ 1.  ,  1.  ,  0.01],\n       [ 1.  ,  2.  , -0.01],\n       [ 1.  ,  3.  ,  0.25],\n       [ 1.  ,  4.  , -4.1 ],\n       [ 1.  ,  5.  ,  0.  ]])\nYou might wonder where the Intercept term came from. This is a convention for\nlinear models like ordinary least squares (OLS) regression. You can suppress the\nintercept by adding the term + 0 to the model:\nIn [37]: patsy.dmatrices('y ~ x0 + x1 + 0', data)[1]\nOut[37]: \nDesignMatrix with shape (5, 2)\n  x0     x1\n   1   0.01\n   2  -0.01\n   3   0.25\n   4  -4.10\n   5   0.00\n  Terms:\n    'x0' (column 0)\n    'x1' (column 1)\nThe Patsy objects can be passed directly into algorithms like numpy.linalg.lstsq,\nwhich performs an ordinary least squares regression:\nIn [38]: coef, resid, _, _ = np.linalg.lstsq(X, y)\nThe model metadata is retained in the design_info attribute, so you can reattach the\nmodel column names to the fitted coefficients to obtain a Series, for example:\nIn [39]: coef\nOut[39]: \narray([[ 0.3129],\n       [-0.0791],\n       [-0.2655]])\nIn [40]: coef = pd.Series(coef.squeeze(), index=X.design_info.column_names)\nIn [41]: coef\nOut[41]: \nIntercept    0.312910\nx0          -0.079106\nx1          -0.265464\ndtype: float64\nData Transformations in Patsy Formulas\nYou can mix Python code into your Patsy formulas; when evaluating the formula, the\nlibrary will try to find the functions you use in the enclosing scope:\n410 \n| \nChapter 12: Introduction to Modeling Libraries in Python",
      "content_length": 1449,
      "extraction_method": "Direct"
    },
    {
      "page_number": 429,
      "chapter": null,
      "content": "In [42]: y, X = patsy.dmatrices('y ~ x0 + np.log(np.abs(x1) + 1)', data)\nIn [43]: X\nOut[43]: \nDesignMatrix with shape (5, 3)\n  Intercept  x0  np.log(np.abs(x1) + 1)\n          1   1                 0.00995\n          1   2                 0.00995\n          1   3                 0.22314\n          1   4                 1.62924\n          1   5                 0.00000\n  Terms:\n    'Intercept' (column 0)\n    'x0' (column 1)\n    'np.log(np.abs(x1) + 1)' (column 2)\nSome commonly used variable transformations include standardizing (to mean 0 and\nvariance 1) and centering (subtracting the mean). Patsy has built-in functions for this\npurpose:\nIn [44]: y, X = patsy.dmatrices('y ~ standardize(x0) + center(x1)', data)\nIn [45]: X\nOut[45]: \nDesignMatrix with shape (5, 3)\n  Intercept  standardize(x0)  center(x1)\n          1         -1.41421        0.78\n          1         -0.70711        0.76\n          1          0.00000        1.02\n          1          0.70711       -3.33\n          1          1.41421        0.77\n  Terms:\n    'Intercept' (column 0)\n    'standardize(x0)' (column 1)\n    'center(x1)' (column 2)\nAs part of a modeling process, you may fit a model on one dataset, then evaluate\nthe model based on another. This might be a hold-out portion or new data that\nis observed later. When applying transformations like center and standardize, you\nshould be careful when using the model to form predications based on new data.\nThese are called stateful transformations, because you must use statistics like the\nmean or standard deviation of the original dataset when transforming a new dataset.\nThe patsy.build_design_matrices function can apply transformations to new out-\nof-sample data using the saved information from the original in-sample dataset:\nIn [46]: new_data = pd.DataFrame({\n   ....:     'x0': [6, 7, 8, 9],\n   ....:     'x1': [3.1, -0.5, 0, 2.3],\n   ....:     'y': [1, 2, 3, 4]})\nIn [47]: new_X = patsy.build_design_matrices([X.design_info], new_data)\n12.2 Creating Model Descriptions with Patsy \n| \n411",
      "content_length": 2019,
      "extraction_method": "Direct"
    },
    {
      "page_number": 430,
      "chapter": null,
      "content": "In [48]: new_X\nOut[48]: \n[DesignMatrix with shape (4, 3)\n   Intercept  standardize(x0)  center(x1)\n           1          2.12132        3.87\n           1          2.82843        0.27\n           1          3.53553        0.77\n           1          4.24264        3.07\n   Terms:\n     'Intercept' (column 0)\n     'standardize(x0)' (column 1)\n     'center(x1)' (column 2)]\nBecause the plus symbol (+) in the context of Patsy formulas does not mean addition,\nwhen you want to add columns from a dataset by name, you must wrap them in the\nspecial I function:\nIn [49]: y, X = patsy.dmatrices('y ~ I(x0 + x1)', data)\nIn [50]: X\nOut[50]: \nDesignMatrix with shape (5, 2)\n  Intercept  I(x0 + x1)\n          1        1.01\n          1        1.99\n          1        3.25\n          1       -0.10\n          1        5.00\n  Terms:\n    'Intercept' (column 0)\n    'I(x0 + x1)' (column 1)\nPatsy has several other built-in transforms in the patsy.builtins module. See the\nonline documentation for more.\nCategorical data has a special class of transformations, which I explain next.\nCategorical Data and Patsy\nNonnumeric data can be transformed for a model design matrix in many different\nways. A complete treatment of this topic is outside the scope of this book and would\nbe studied best along with a course in statistics.\nWhen you use nonnumeric terms in a Patsy formula, they are converted to dummy\nvariables by default. If there is an intercept, one of the levels will be left out to avoid\ncollinearity:\nIn [51]: data = pd.DataFrame({\n   ....:     'key1': ['a', 'a', 'b', 'b', 'a', 'b', 'a', 'b'],\n   ....:     'key2': [0, 1, 0, 1, 0, 1, 0, 0],\n   ....:     'v1': [1, 2, 3, 4, 5, 6, 7, 8],\n412 \n| \nChapter 12: Introduction to Modeling Libraries in Python",
      "content_length": 1737,
      "extraction_method": "Direct"
    },
    {
      "page_number": 431,
      "chapter": null,
      "content": "....:     'v2': [-1, 0, 2.5, -0.5, 4.0, -1.2, 0.2, -1.7]\n   ....: })\nIn [52]: y, X = patsy.dmatrices('v2 ~ key1', data)\nIn [53]: X\nOut[53]: \nDesignMatrix with shape (8, 2)\n  Intercept  key1[T.b]\n          1          0\n          1          0\n          1          1\n          1          1\n          1          0\n          1          1\n          1          0\n          1          1\n  Terms:\n    'Intercept' (column 0)\n    'key1' (column 1)\nIf you omit the intercept from the model, then columns for each category value will\nbe included in the model design matrix:\nIn [54]: y, X = patsy.dmatrices('v2 ~ key1 + 0', data)\nIn [55]: X\nOut[55]: \nDesignMatrix with shape (8, 2)\n  key1[a]  key1[b]\n        1        0\n        1        0\n        0        1\n        0        1\n        1        0\n        0        1\n        1        0\n        0        1\n  Terms:\n    'key1' (columns 0:2)\nNumeric columns can be interpreted as categorical with the C function:\nIn [56]: y, X = patsy.dmatrices('v2 ~ C(key2)', data)\nIn [57]: X\nOut[57]: \nDesignMatrix with shape (8, 2)\n  Intercept  C(key2)[T.1]\n          1             0\n          1             1\n          1             0\n          1             1\n12.2 Creating Model Descriptions with Patsy \n| \n413",
      "content_length": 1231,
      "extraction_method": "Direct"
    },
    {
      "page_number": 432,
      "chapter": null,
      "content": "1             0\n          1             1\n          1             0\n          1             0\n  Terms:\n    'Intercept' (column 0)\n    'C(key2)' (column 1)\nWhen you’re using multiple categorical terms in a model, things can be more compli‐\ncated, as you can include interaction terms of the form key1:key2, which can be\nused, for example, in analysis of variance (ANOVA) models:\nIn [58]: data['key2'] = data['key2'].map({0: 'zero', 1: 'one'})\nIn [59]: data\nOut[59]: \n  key1  key2  v1   v2\n0    a  zero   1 -1.0\n1    a   one   2  0.0\n2    b  zero   3  2.5\n3    b   one   4 -0.5\n4    a  zero   5  4.0\n5    b   one   6 -1.2\n6    a  zero   7  0.2\n7    b  zero   8 -1.7\nIn [60]: y, X = patsy.dmatrices('v2 ~ key1 + key2', data)\nIn [61]: X\nOut[61]: \nDesignMatrix with shape (8, 3)\n  Intercept  key1[T.b]  key2[T.zero]\n          1          0             1\n          1          0             0\n          1          1             1\n          1          1             0\n          1          0             1\n          1          1             0\n          1          0             1\n          1          1             1\n  Terms:\n    'Intercept' (column 0)\n    'key1' (column 1)\n    'key2' (column 2)\nIn [62]: y, X = patsy.dmatrices('v2 ~ key1 + key2 + key1:key2', data)\nIn [63]: X\nOut[63]: \nDesignMatrix with shape (8, 4)\n  Intercept  key1[T.b]  key2[T.zero]  key1[T.b]:key2[T.zero]\n          1          0             1                       0\n414 \n| \nChapter 12: Introduction to Modeling Libraries in Python",
      "content_length": 1495,
      "extraction_method": "Direct"
    },
    {
      "page_number": 433,
      "chapter": null,
      "content": "1          0             0                       0\n          1          1             1                       1\n          1          1             0                       0\n          1          0             1                       0\n          1          1             0                       0\n          1          0             1                       0\n          1          1             1                       1\n  Terms:\n    'Intercept' (column 0)\n    'key1' (column 1)\n    'key2' (column 2)\n    'key1:key2' (column 3)\nPatsy provides for other ways to transform categorical data, including transforma‐\ntions for terms with a particular ordering. See the online documentation for more.\n12.3 Introduction to statsmodels\nstatsmodels is a Python library for fitting many kinds of statistical models, perform‐\ning statistical tests, and data exploration and visualization. statsmodels contains more\n“classical” frequentist statistical methods, while Bayesian methods and machine learn‐\ning models are found in other libraries.\nSome kinds of models found in statsmodels include:\n• Linear models, generalized linear models, and robust linear models\n•\n• Linear mixed effects models\n•\n• Analysis of variance (ANOVA) methods\n•\n• Time series processes and state space models\n•\n• Generalized method of moments\n•\nIn the next few pages, we will use a few basic tools in statsmodels and explore how\nto use the modeling interfaces with Patsy formulas and pandas DataFrame objects.\nIf you didn’t install statsmodels in the Patsy discussion earlier, you can install it now\nwith:\nconda install statsmodels\nEstimating Linear Models\nThere are several kinds of linear regression models in statsmodels, from the more\nbasic (e.g., ordinary least squares) to more complex (e.g., iteratively reweighted least\nsquares).\nLinear models in statsmodels have two different main interfaces: array based and\nformula based. These are accessed through these API module imports:\n12.3 Introduction to statsmodels \n| \n415",
      "content_length": 1987,
      "extraction_method": "Direct"
    },
    {
      "page_number": 434,
      "chapter": null,
      "content": "import statsmodels.api as sm\nimport statsmodels.formula.api as smf\nTo show how to use these, we generate a linear model from some random data. Run\nthe following code in a Jupyter cell:\n# To make the example reproducible\nrng = np.random.default_rng(seed=12345)\ndef dnorm(mean, variance, size=1):\n    if isinstance(size, int):\n        size = size,\n    return mean + np.sqrt(variance) * rng.standard_normal(*size)\nN = 100\nX = np.c_[dnorm(0, 0.4, size=N),\n          dnorm(0, 0.6, size=N),\n          dnorm(0, 0.2, size=N)]\neps = dnorm(0, 0.1, size=N)\nbeta = [0.1, 0.3, 0.5]\ny = np.dot(X, beta) + eps\nHere, I wrote down the “true” model with known parameters beta. In this case, dnorm\nis a helper function for generating normally distributed data with a particular mean\nand variance. So now we have:\nIn [66]: X[:5]\nOut[66]: \narray([[-0.9005, -0.1894, -1.0279],\n       [ 0.7993, -1.546 , -0.3274],\n       [-0.5507, -0.1203,  0.3294],\n       [-0.1639,  0.824 ,  0.2083],\n       [-0.0477, -0.2131, -0.0482]])\nIn [67]: y[:5]\nOut[67]: array([-0.5995, -0.5885,  0.1856, -0.0075, -0.0154])\nA linear model is generally fitted with an intercept term, as we saw before with Patsy.\nThe sm.add_constant function can add an intercept column to an existing matrix:\nIn [68]: X_model = sm.add_constant(X)\nIn [69]: X_model[:5]\nOut[69]: \narray([[ 1.    , -0.9005, -0.1894, -1.0279],\n       [ 1.    ,  0.7993, -1.546 , -0.3274],\n       [ 1.    , -0.5507, -0.1203,  0.3294],\n       [ 1.    , -0.1639,  0.824 ,  0.2083],\n       [ 1.    , -0.0477, -0.2131, -0.0482]])\nThe sm.OLS class can fit an ordinary least squares linear regression:\nIn [70]: model = sm.OLS(y, X)\n416 \n| \nChapter 12: Introduction to Modeling Libraries in Python",
      "content_length": 1704,
      "extraction_method": "Direct"
    },
    {
      "page_number": 435,
      "chapter": null,
      "content": "The model’s fit method returns a regression results object containing estimated\nmodel parameters and other diagnostics:\nIn [71]: results = model.fit()\nIn [72]: results.params\nOut[72]: array([0.0668, 0.268 , 0.4505])\nThe summary method on results can print a model detailing diagnostic output of the\nmodel:\nIn [73]: print(results.summary())\nOLS Regression Results                                \n=================================================================================\n======\nDep. Variable:                      y   R-squared (uncentered):                  \n 0.469\nModel:                            OLS   Adj. R-squared (uncentered):             \n 0.452\nMethod:                 Least Squares   F-statistic:                             \n 28.51\nDate:                Fri, 12 Aug 2022   Prob (F-statistic):                    2.\n66e-13\nTime:                        14:09:18   Log-Likelihood:                         -\n25.611\nNo. Observations:                 100   AIC:                                     \n 57.22\nDf Residuals:                      97   BIC:                                     \n 65.04\nDf Model:                           3                                            \n      \nCovariance Type:            nonrobust                                            \n      \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1             0.0668      0.054      1.243      0.217      -0.040       0.174\nx2             0.2680      0.042      6.313      0.000       0.184       0.352\nx3             0.4505      0.068      6.605      0.000       0.315       0.586\n==============================================================================\nOmnibus:                        0.435   Durbin-Watson:                   1.869\nProb(Omnibus):                  0.805   Jarque-Bera (JB):                0.301\nSkew:                           0.134   Prob(JB):                        0.860\nKurtosis:                       2.995   Cond. No.                         1.64\n==============================================================================\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contai\nn a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly \nspecified.\n12.3 Introduction to statsmodels \n| \n417",
      "content_length": 2469,
      "extraction_method": "Direct"
    },
    {
      "page_number": 436,
      "chapter": null,
      "content": "The parameter names here have been given the generic names x1, x2, and so on.\nSuppose instead that all of the model parameters are in a DataFrame:\nIn [74]: data = pd.DataFrame(X, columns=['col0', 'col1', 'col2'])\nIn [75]: data['y'] = y\nIn [76]: data[:5]\nOut[76]: \n       col0      col1      col2         y\n0 -0.900506 -0.189430 -1.027870 -0.599527\n1  0.799252 -1.545984 -0.327397 -0.588454\n2 -0.550655 -0.120254  0.329359  0.185634\n3 -0.163916  0.824040  0.208275 -0.007477\n4 -0.047651 -0.213147 -0.048244 -0.015374\nNow we can use the statsmodels formula API and Patsy formula strings:\nIn [77]: results = smf.ols('y ~ col0 + col1 + col2', data=data).fit()\nIn [78]: results.params\nOut[78]: \nIntercept   -0.020799\ncol0         0.065813\ncol1         0.268970\ncol2         0.449419\ndtype: float64\nIn [79]: results.tvalues\nOut[79]: \nIntercept   -0.652501\ncol0         1.219768\ncol1         6.312369\ncol2         6.567428\ndtype: float64\nObserve how statsmodels has returned results as Series with the DataFrame column\nnames attached. We also do not need to use add_constant when using formulas and\npandas objects.\nGiven new out-of-sample data, you can compute predicted values given the estimated\nmodel parameters:\nIn [80]: results.predict(data[:5])\nOut[80]: \n0   -0.592959\n1   -0.531160\n2    0.058636\n3    0.283658\n4   -0.102947\ndtype: float64\n418 \n| \nChapter 12: Introduction to Modeling Libraries in Python",
      "content_length": 1403,
      "extraction_method": "Direct"
    },
    {
      "page_number": 437,
      "chapter": null,
      "content": "There are many additional tools for analysis, diagnostics, and visualization of linear\nmodel results in statsmodels that you can explore. There are also other kinds of linear\nmodels beyond ordinary least squares.\nEstimating Time Series Processes\nAnother class of models in statsmodels is for time series analysis. Among these\nare autoregressive processes, Kalman filtering and other state space models, and\nmultivariate autoregressive models.\nLet’s simulate some time series data with an autoregressive structure and noise. Run\nthe following in Jupyter:\ninit_x = 4\nvalues = [init_x, init_x]\nN = 1000\nb0 = 0.8\nb1 = -0.4\nnoise = dnorm(0, 0.1, N)\nfor i in range(N):\n    new_x = values[-1] * b0 + values[-2] * b1 + noise[i]\n    values.append(new_x)\nThis data has an AR(2) structure (two lags) with parameters 0.8 and –0.4. When you\nfit an AR model, you may not know the number of lagged terms to include, so you\ncan fit the model with some larger number of lags:\nIn [82]: from statsmodels.tsa.ar_model import AutoReg\nIn [83]: MAXLAGS = 5\nIn [84]: model = AutoReg(values, MAXLAGS)\nIn [85]: results = model.fit()\nThe estimated parameters in the results have the intercept first, and the estimates for\nthe first two lags next:\nIn [86]: results.params\nOut[86]: array([ 0.0235,  0.8097, -0.4287, -0.0334,  0.0427, -0.0567])\nDeeper details of these models and how to interpret their results are beyond what\nI can cover in this book, but there’s plenty more to discover in the statsmodels\ndocumentation.\n12.3 Introduction to statsmodels \n| \n419",
      "content_length": 1533,
      "extraction_method": "Direct"
    },
    {
      "page_number": 438,
      "chapter": null,
      "content": "12.4 Introduction to scikit-learn\nscikit-learn is one of the most widely used and trusted general-purpose Python\nmachine learning toolkits. It contains a broad selection of standard supervised and\nunsupervised machine learning methods, with tools for model selection and evalua‐\ntion, data transformation, data loading, and model persistence. These models can\nbe used for classification, clustering, prediction, and other common tasks. You can\ninstall scikit-learn from conda like so:\nconda install scikit-learn\nThere are excellent online and print resources for learning about machine learning\nand how to apply libraries like scikit-learn to solve real-world problems. In this\nsection, I will give a brief flavor of the scikit-learn API style.\npandas integration in scikit-learn has improved significantly in recent years, and by\nthe time you are reading this it may have improved even more. I encourage you to\ncheck out the latest project documentation.\nAs an example for this chapter, I use a now-classic dataset from a Kaggle competition\nabout passenger survival rates on the Titanic in 1912. We load the training and test\ndatasets using pandas:\nIn [87]: train = pd.read_csv('datasets/titanic/train.csv')\nIn [88]: test = pd.read_csv('datasets/titanic/test.csv')\nIn [89]: train.head(4)\nOut[89]: \n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n                                                  Name     Sex   Age  SibSp  \\\n0                              Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Thayer)  female  38.0      1   \n2                               Heikkinen, Miss. Laina  female  26.0      0   \n3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \nLibraries like statsmodels and scikit-learn generally cannot be fed missing data, so we\nlook at the columns to see if there are any that contain missing data:\nIn [90]: train.isna().sum()\nOut[90]: \nPassengerId      0\n420 \n| \nChapter 12: Introduction to Modeling Libraries in Python",
      "content_length": 2419,
      "extraction_method": "Direct"
    },
    {
      "page_number": 439,
      "chapter": null,
      "content": "Survived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\nIn [91]: test.isna().sum()\nOut[91]: \nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64\nIn statistics and machine learning examples like this one, a typical task is to predict\nwhether a passenger would survive based on features in the data. A model is fitted on\na training dataset and then evaluated on an out-of-sample testing dataset.\nI would like to use Age as a predictor, but it has missing data. There are a number of\nways to do missing data imputation, but I will do a simple one and use the median of\nthe training dataset to fill the nulls in both tables:\nIn [92]: impute_value = train['Age'].median()\nIn [93]: train['Age'] = train['Age'].fillna(impute_value)\nIn [94]: test['Age'] = test['Age'].fillna(impute_value)\nNow we need to specify our models. I add a column IsFemale as an encoded version\nof the 'Sex' column:\nIn [95]: train['IsFemale'] = (train['Sex'] == 'female').astype(int)\nIn [96]: test['IsFemale'] = (test['Sex'] == 'female').astype(int)\nThen we decide on some model variables and create NumPy arrays:\nIn [97]: predictors = ['Pclass', 'IsFemale', 'Age']\n12.4 Introduction to scikit-learn \n| \n421",
      "content_length": 1510,
      "extraction_method": "Direct"
    },
    {
      "page_number": 440,
      "chapter": null,
      "content": "In [98]: X_train = train[predictors].to_numpy()\nIn [99]: X_test = test[predictors].to_numpy()\nIn [100]: y_train = train['Survived'].to_numpy()\nIn [101]: X_train[:5]\nOut[101]: \narray([[ 3.,  0., 22.],\n       [ 1.,  1., 38.],\n       [ 3.,  1., 26.],\n       [ 1.,  1., 35.],\n       [ 3.,  0., 35.]])\nIn [102]: y_train[:5]\nOut[102]: array([0, 1, 1, 1, 0])\nI make no claims that this is a good model or that these features are engineered\nproperly. We use the LogisticRegression model from scikit-learn and create a\nmodel instance:\nIn [103]: from sklearn.linear_model import LogisticRegression\nIn [104]: model = LogisticRegression()\nWe can fit this model to the training data using the model’s fit method:\nIn [105]: model.fit(X_train, y_train)\nOut[105]: LogisticRegression()\nNow, we can form predictions for the test dataset using model.predict:\nIn [106]: y_predict = model.predict(X_test)\nIn [107]: y_predict[:10]\nOut[107]: array([0, 0, 0, 0, 1, 0, 1, 0, 1, 0])\nIf you had the true values for the test dataset, you could compute an accuracy\npercentage or some other error metric:\n(y_true == y_predict).mean()\nIn practice, there are often many additional layers of complexity in model training.\nMany models have parameters that can be tuned, and there are techniques such as\ncross-validation that can be used for parameter tuning to avoid overfitting to the\ntraining data. This can often yield better predictive performance or robustness on\nnew data.\nCross-validation works by splitting the training data to simulate out-of-sample pre‐\ndiction. Based on a model accuracy score like mean squared error, you can perform\na grid search on model parameters. Some models, like logistic regression, have esti‐\nmator classes with built-in cross-validation. For example, the LogisticRegressionCV\n422 \n| \nChapter 12: Introduction to Modeling Libraries in Python",
      "content_length": 1845,
      "extraction_method": "Direct"
    },
    {
      "page_number": 441,
      "chapter": null,
      "content": "class can be used with a parameter indicating how fine-grained of a grid search to do\non the model regularization parameter C:\nIn [108]: from sklearn.linear_model import LogisticRegressionCV\nIn [109]: model_cv = LogisticRegressionCV(Cs=10)\nIn [110]: model_cv.fit(X_train, y_train)\nOut[110]: LogisticRegressionCV()\nTo do cross-validation by hand, you can use the cross_val_score helper function,\nwhich handles the data splitting process. For example, to cross-validate our model\nwith four nonoverlapping splits of the training data, we can do:\nIn [111]: from sklearn.model_selection import cross_val_score\nIn [112]: model = LogisticRegression(C=10)\nIn [113]: scores = cross_val_score(model, X_train, y_train, cv=4)\nIn [114]: scores\nOut[114]: array([0.7758, 0.7982, 0.7758, 0.7883])\nThe default scoring metric is model dependent, but it is possible to choose an explicit\nscoring function. Cross-validated models take longer to train but can often yield\nbetter model performance.\n12.5 Conclusion\nWhile I have only skimmed the surface of some Python modeling libraries, there\nare more and more frameworks for various kinds of statistics and machine learning\neither implemented in Python or with a Python user interface.\nThis book is focused especially on data wrangling, but there are many others dedica‐\nted to modeling and data science tools. Some excellent ones are:\n• Introduction to Machine Learning with Python by Andreas Müller and Sarah\n•\nGuido (O’Reilly)\n• Python Data Science Handbook by Jake VanderPlas (O’Reilly)\n•\n• Data Science from Scratch: First Principles with Python by Joel Grus (O’Reilly)\n•\n• Python Machine Learning by Sebastian Raschka and Vahid Mirjalili (Packt\n•\nPublishing)\n• Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by\n•\nAurélien Géron (O’Reilly)\n12.5 Conclusion \n| \n423",
      "content_length": 1818,
      "extraction_method": "Direct"
    },
    {
      "page_number": 442,
      "chapter": null,
      "content": "While books can be valuable resources for learning, they can sometimes grow out of\ndate when the underlying open source software changes. It’s a good idea to be familiar\nwith the documentation for the various statistics or machine learning frameworks to\nstay up to date on the latest features and API.\n424 \n| \nChapter 12: Introduction to Modeling Libraries in Python",
      "content_length": 366,
      "extraction_method": "Direct"
    },
    {
      "page_number": 443,
      "chapter": null,
      "content": "CHAPTER 13\nData Analysis Examples\nNow that we’ve reached the final chapter of this book, we’re going to take a look at\na number of real-world datasets. For each dataset, we’ll use the techniques presented\nin this book to extract meaning from the raw data. The demonstrated techniques\ncan be applied to all manner of other datasets. This chapter contains a collection of\nmiscellaneous example datasets that you can use for practice with the tools in this\nbook.\nThe example datasets are found in the book’s accompanying GitHub repository. If\nyou are unable to access GitHub, you can also get them from the repository mirror on\nGitee.\n13.1 Bitly Data from 1.USA.gov\nIn 2011, the URL shortening service Bitly partnered with the US government website\nUSA.gov to provide a feed of anonymous data gathered from users who shorten links\nending with .gov or .mil. In 2011, a live feed as well as hourly snapshots were available\nas downloadable text files. This service is shut down at the time of this writing (2022),\nbut we preserved one of the data files for the book’s examples.\nIn the case of the hourly snapshots, each line in each file contains a common form of\nweb data known as JSON, which stands for JavaScript Object Notation. For example,\nif we read just the first line of a file, we may see something like this:\nIn [5]: path = \"datasets/bitly_usagov/example.txt\"\nIn [6]: with open(path) as f:\n   ...:     print(f.readline())\n   ...:\n{ \"a\": \"Mozilla\\\\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\\\\/535.11\n(KHTML, like Gecko) Chrome\\\\/17.0.963.78 Safari\\\\/535.11\", \"c\": \"US\", \"nk\": 1,\n425",
      "content_length": 1584,
      "extraction_method": "Direct"
    },
    {
      "page_number": 444,
      "chapter": null,
      "content": "\"tz\": \"America\\\\/New_York\", \"gr\": \"MA\", \"g\": \"A6qOVH\", \"h\": \"wfLQtf\", \"l\":\n\"orofrog\", \"al\": \"en-US,en;q=0.8\", \"hh\": \"1.usa.gov\", \"r\":\n\"http:\\\\/\\\\/www.facebook.com\\\\/l\\\\/7AQEFzjSi\\\\/1.usa.gov\\\\/wfLQtf\", \"u\":\n\"http:\\\\/\\\\/www.ncbi.nlm.nih.gov\\\\/pubmed\\\\/22415991\", \"t\": 1331923247, \"hc\":\n1331822918, \"cy\": \"Danvers\", \"ll\": [ 42.576698, -70.954903 ] }\nPython has both built-in and third-party libraries for converting a JSON string into a\nPython dictionary. Here we’ll use the json module and its loads function invoked on\neach line in the sample file we downloaded:\nimport json\nwith open(path) as f:\n    records = [json.loads(line) for line in f]\nThe resulting object records is now a list of Python dictionaries:\nIn [18]: records[0]\nOut[18]:\n{'a': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko)\nChrome/17.0.963.78 Safari/535.11',\n 'al': 'en-US,en;q=0.8',\n 'c': 'US',\n 'cy': 'Danvers',\n 'g': 'A6qOVH',\n 'gr': 'MA',\n 'h': 'wfLQtf',\n 'hc': 1331822918,\n 'hh': '1.usa.gov',\n 'l': 'orofrog',\n 'll': [42.576698, -70.954903],\n 'nk': 1,\n 'r': 'http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/wfLQtf',\n 't': 1331923247,\n 'tz': 'America/New_York',\n 'u': 'http://www.ncbi.nlm.nih.gov/pubmed/22415991'}\nCounting Time Zones in Pure Python\nSuppose we were interested in finding the time zones that occur most often in the\ndataset (the tz field). There are many ways we could do this. First, let’s extract a list of\ntime zones again using a list comprehension:\nIn [15]: time_zones = [rec[\"tz\"] for rec in records]\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n<ipython-input-15-abdeba901c13> in <module>\n----> 1 time_zones = [rec[\"tz\"] for rec in records]\n<ipython-input-15-abdeba901c13> in <listcomp>(.0)\n----> 1 time_zones = [rec[\"tz\"] for rec in records]\nKeyError: 'tz'\n426 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 1930,
      "extraction_method": "Direct"
    },
    {
      "page_number": 445,
      "chapter": null,
      "content": "Oops! Turns out that not all of the records have a time zone field. We can handle this\nby adding the check if \"tz\" in rec at the end of the list comprehension:\nIn [16]: time_zones = [rec[\"tz\"] for rec in records if \"tz\" in rec]\nIn [17]: time_zones[:10]\nOut[17]: \n['America/New_York',\n 'America/Denver',\n 'America/New_York',\n 'America/Sao_Paulo',\n 'America/New_York',\n 'America/New_York',\n 'Europe/Warsaw',\n '',\n '',\n '']\nJust looking at the first 10 time zones, we see that some of them are unknown (empty\nstring). You can filter these out also, but I’ll leave them in for now. Next, to produce\ncounts by time zone, I’ll show two approaches: a harder way (using just the Python\nstandard library) and a simpler way (using pandas). One way to do the counting is to\nuse a dictionary to store counts while we iterate through the time zones:\ndef get_counts(sequence):\n    counts = {}\n    for x in sequence:\n        if x in counts:\n            counts[x] += 1\n        else:\n            counts[x] = 1\n    return counts\nUsing more advanced tools in the Python standard library, you can write the same\nthing more briefly:\nfrom collections import defaultdict\ndef get_counts2(sequence):\n    counts = defaultdict(int) # values will initialize to 0\n    for x in sequence:\n        counts[x] += 1\n    return counts\nI put this logic in a function just to make it more reusable. To use it on the time\nzones, just pass the time_zones list:\nIn [20]: counts = get_counts(time_zones)\nIn [21]: counts[\"America/New_York\"]\nOut[21]: 1251\n13.1 Bitly Data from 1.USA.gov \n| \n427",
      "content_length": 1550,
      "extraction_method": "Direct"
    },
    {
      "page_number": 446,
      "chapter": null,
      "content": "In [22]: len(time_zones)\nOut[22]: 3440\nIf we wanted the top 10 time zones and their counts, we can make a list of tuples by\n(count, timezone) and sort it:\ndef top_counts(count_dict, n=10):\n    value_key_pairs = [(count, tz) for tz, count in count_dict.items()]\n    value_key_pairs.sort()\n    return value_key_pairs[-n:]\nWe have then:\nIn [24]: top_counts(counts)\nOut[24]: \n[(33, 'America/Sao_Paulo'),\n (35, 'Europe/Madrid'),\n (36, 'Pacific/Honolulu'),\n (37, 'Asia/Tokyo'),\n (74, 'Europe/London'),\n (191, 'America/Denver'),\n (382, 'America/Los_Angeles'),\n (400, 'America/Chicago'),\n (521, ''),\n (1251, 'America/New_York')]\nIf you search the Python standard library, you may find the collections.Counter\nclass, which makes this task even simpler:\nIn [25]: from collections import Counter\nIn [26]: counts = Counter(time_zones)\nIn [27]: counts.most_common(10)\nOut[27]: \n[('America/New_York', 1251),\n ('', 521),\n ('America/Chicago', 400),\n ('America/Los_Angeles', 382),\n ('America/Denver', 191),\n ('Europe/London', 74),\n ('Asia/Tokyo', 37),\n ('Pacific/Honolulu', 36),\n ('Europe/Madrid', 35),\n ('America/Sao_Paulo', 33)]\nCounting Time Zones with pandas\nYou can create a DataFrame from the original set of records by passing the list of\nrecords to pandas.DataFrame:\nIn [28]: frame = pd.DataFrame(records)\n428 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 1339,
      "extraction_method": "Direct"
    },
    {
      "page_number": 447,
      "chapter": null,
      "content": "We can look at some basic information about this new DataFrame, such as column\nnames, inferred column types, or number of missing values, using frame.info():\nIn [29]: frame.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3560 entries, 0 to 3559\nData columns (total 18 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   a            3440 non-null   object \n 1   c            2919 non-null   object \n 2   nk           3440 non-null   float64\n 3   tz           3440 non-null   object \n 4   gr           2919 non-null   object \n 5   g            3440 non-null   object \n 6   h            3440 non-null   object \n 7   l            3440 non-null   object \n 8   al           3094 non-null   object \n 9   hh           3440 non-null   object \n 10  r            3440 non-null   object \n 11  u            3440 non-null   object \n 12  t            3440 non-null   float64\n 13  hc           3440 non-null   float64\n 14  cy           2919 non-null   object \n 15  ll           2919 non-null   object \n 16  _heartbeat_  120 non-null    float64\n 17  kw           93 non-null     object \ndtypes: float64(4), object(14)\nmemory usage: 500.8+ KB\nIn [30]: frame[\"tz\"].head()\nOut[30]: \n0     America/New_York\n1       America/Denver\n2     America/New_York\n3    America/Sao_Paulo\n4     America/New_York\nName: tz, dtype: object\nThe output shown for the frame is the summary view, shown for large DataFrame\nobjects. We can then use the value_counts method for the Series:\nIn [31]: tz_counts = frame[\"tz\"].value_counts()\nIn [32]: tz_counts.head()\nOut[32]: \nAmerica/New_York       1251\n                        521\nAmerica/Chicago         400\nAmerica/Los_Angeles     382\nAmerica/Denver          191\nName: tz, dtype: int64\n13.1 Bitly Data from 1.USA.gov \n| \n429",
      "content_length": 1787,
      "extraction_method": "Direct"
    },
    {
      "page_number": 448,
      "chapter": null,
      "content": "We can visualize this data using matplotlib. We can make the plots a bit nicer by\nfilling in a substitute value for unknown or missing time zone data in the records. We\nreplace the missing values with the fillna method and use Boolean array indexing\nfor the empty strings:\nIn [33]: clean_tz = frame[\"tz\"].fillna(\"Missing\")\nIn [34]: clean_tz[clean_tz == \"\"] = \"Unknown\"\nIn [35]: tz_counts = clean_tz.value_counts()\nIn [36]: tz_counts.head()\nOut[36]: \nAmerica/New_York       1251\nUnknown                 521\nAmerica/Chicago         400\nAmerica/Los_Angeles     382\nAmerica/Denver          191\nName: tz, dtype: int64\nAt this point, we can use the seaborn package to make a horizontal bar plot (see\nFigure 13-1 for the resulting visualization):\nIn [38]: import seaborn as sns\nIn [39]: subset = tz_counts.head()\nIn [40]: sns.barplot(y=subset.index, x=subset.to_numpy())\nFigure 13-1. Top time zones in the 1.usa.gov sample data\nThe a field contains information about the browser, device, or application used to\nperform the URL shortening:\nIn [41]: frame[\"a\"][1]\nOut[41]: 'GoogleMaps/RochesterNY'\nIn [42]: frame[\"a\"][50]\n430 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 1155,
      "extraction_method": "Direct"
    },
    {
      "page_number": 449,
      "chapter": null,
      "content": "Out[42]: 'Mozilla/5.0 (Windows NT 5.1; rv:10.0.2) Gecko/20100101 Firefox/10.0.2'\nIn [43]: frame[\"a\"][51][:50]  # long line\nOut[43]: 'Mozilla/5.0 (Linux; U; Android 2.2.2; en-us; LG-P9'\nParsing all of the interesting information in these “agent” strings may seem like\na daunting task. One possible strategy is to split off the first token in the string\n(corresponding roughly to the browser capability) and make another summary of the\nuser behavior:\nIn [44]: results = pd.Series([x.split()[0] for x in frame[\"a\"].dropna()])\nIn [45]: results.head(5)\nOut[45]: \n0               Mozilla/5.0\n1    GoogleMaps/RochesterNY\n2               Mozilla/4.0\n3               Mozilla/5.0\n4               Mozilla/5.0\ndtype: object\nIn [46]: results.value_counts().head(8)\nOut[46]: \nMozilla/5.0                 2594\nMozilla/4.0                  601\nGoogleMaps/RochesterNY       121\nOpera/9.80                    34\nTEST_INTERNET_AGENT           24\nGoogleProducer                21\nMozilla/6.0                    5\nBlackBerry8520/5.0.0.681       4\ndtype: int64\nNow, suppose you wanted to decompose the top time zones into Windows and\nnon-Windows users. As a simplification, let’s say that a user is on Windows if the\nstring \"Windows\" is in the agent string. Since some of the agents are missing, we’ll\nexclude these from the data:\nIn [47]: cframe = frame[frame[\"a\"].notna()].copy()\nWe want to then compute a value for whether or not each row is Windows:\nIn [48]: cframe[\"os\"] = np.where(cframe[\"a\"].str.contains(\"Windows\"),\n   ....:                         \"Windows\", \"Not Windows\")\nIn [49]: cframe[\"os\"].head(5)\nOut[49]: \n0        Windows\n1    Not Windows\n2        Windows\n3    Not Windows\n4        Windows\nName: os, dtype: object\n13.1 Bitly Data from 1.USA.gov \n| \n431",
      "content_length": 1748,
      "extraction_method": "Direct"
    },
    {
      "page_number": 450,
      "chapter": null,
      "content": "Then, you can group the data by its time zone column and this new list of operating\nsystems:\nIn [50]: by_tz_os = cframe.groupby([\"tz\", \"os\"])\nThe group counts, analogous to the value_counts function, can be computed with\nsize. This result is then reshaped into a table with unstack:\nIn [51]: agg_counts = by_tz_os.size().unstack().fillna(0)\nIn [52]: agg_counts.head()\nOut[52]: \nos                   Not Windows  Windows\ntz                                       \n                           245.0    276.0\nAfrica/Cairo                 0.0      3.0\nAfrica/Casablanca            0.0      1.0\nAfrica/Ceuta                 0.0      2.0\nAfrica/Johannesburg          0.0      1.0\nFinally, let’s select the top overall time zones. To do so, I construct an indirect index\narray from the row counts in agg_counts. After computing the row counts with\nagg_counts.sum(\"columns\"), I can call argsort() to obtain an index array that can\nbe used to sort in ascending order:\nIn [53]: indexer = agg_counts.sum(\"columns\").argsort()\nIn [54]: indexer.values[:10]\nOut[54]: array([24, 20, 21, 92, 87, 53, 54, 57, 26, 55])\nI use take to select the rows in that order, then slice off the last 10 rows (largest\nvalues):\nIn [55]: count_subset = agg_counts.take(indexer[-10:])\nIn [56]: count_subset\nOut[56]: \nos                   Not Windows  Windows\ntz                                       \nAmerica/Sao_Paulo           13.0     20.0\nEurope/Madrid               16.0     19.0\nPacific/Honolulu             0.0     36.0\nAsia/Tokyo                   2.0     35.0\nEurope/London               43.0     31.0\nAmerica/Denver             132.0     59.0\nAmerica/Los_Angeles        130.0    252.0\nAmerica/Chicago            115.0    285.0\n                           245.0    276.0\nAmerica/New_York           339.0    912.0\n432 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 1826,
      "extraction_method": "Direct"
    },
    {
      "page_number": 451,
      "chapter": null,
      "content": "pandas has a convenience method called nlargest that does the same thing:\nIn [57]: agg_counts.sum(axis=\"columns\").nlargest(10)\nOut[57]: \ntz\nAmerica/New_York       1251.0\n                        521.0\nAmerica/Chicago         400.0\nAmerica/Los_Angeles     382.0\nAmerica/Denver          191.0\nEurope/London            74.0\nAsia/Tokyo               37.0\nPacific/Honolulu         36.0\nEurope/Madrid            35.0\nAmerica/Sao_Paulo        33.0\ndtype: float64\nThen, this can be plotted in a grouped bar plot comparing the number of Windows\nand non-Windows users, using seaborn’s barplot function (see Figure 13-2). I first\ncall count_subset.stack() and reset the index to rearrange the data for better\ncompatibility with seaborn:\nIn [59]: count_subset = count_subset.stack()\nIn [60]: count_subset.name = \"total\"\nIn [61]: count_subset = count_subset.reset_index()\nIn [62]: count_subset.head(10)\nOut[62]: \n                  tz           os  total\n0  America/Sao_Paulo  Not Windows   13.0\n1  America/Sao_Paulo      Windows   20.0\n2      Europe/Madrid  Not Windows   16.0\n3      Europe/Madrid      Windows   19.0\n4   Pacific/Honolulu  Not Windows    0.0\n5   Pacific/Honolulu      Windows   36.0\n6         Asia/Tokyo  Not Windows    2.0\n7         Asia/Tokyo      Windows   35.0\n8      Europe/London  Not Windows   43.0\n9      Europe/London      Windows   31.0\nIn [63]: sns.barplot(x=\"total\", y=\"tz\", hue=\"os\",  data=count_subset)\n13.1 Bitly Data from 1.USA.gov \n| \n433",
      "content_length": 1458,
      "extraction_method": "Direct"
    },
    {
      "page_number": 452,
      "chapter": null,
      "content": "Figure 13-2. Top time zones by Windows and non-Windows users\nIt is a bit difficult to see the relative percentage of Windows users in the smaller\ngroups, so let’s normalize the group percentages to sum to 1:\ndef norm_total(group):\n    group[\"normed_total\"] = group[\"total\"] / group[\"total\"].sum()\n    return group\nresults = count_subset.groupby(\"tz\").apply(norm_total)\nThen plot this in Figure 13-3:\nIn [66]: sns.barplot(x=\"normed_total\", y=\"tz\", hue=\"os\",  data=results)\n434 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 514,
      "extraction_method": "Direct"
    },
    {
      "page_number": 453,
      "chapter": null,
      "content": "Figure 13-3. Percentage Windows and non-Windows users in top occurring time zones\nWe could have computed the normalized sum more efficiently by using the trans\nform method with groupby:\nIn [67]: g = count_subset.groupby(\"tz\")\nIn [68]: results2 = count_subset[\"total\"] / g[\"total\"].transform(\"sum\")\n13.2 MovieLens 1M Dataset\nGroupLens Research provides a number of collections of movie ratings data collected\nfrom users of MovieLens in the late 1990s and early 2000s. The data provides movie\nratings, movie metadata (genres and year), and demographic data about the users\n(age, zip code, gender identification, and occupation). Such data is often of interest in\nthe development of recommendation systems based on machine learning algorithms.\nWhile we do not explore machine learning techniques in detail in this book, I will\nshow you how to slice and dice datasets like these into the exact form you need.\nThe MovieLens 1M dataset contains one million ratings collected from six thousand\nusers on four thousand movies. It’s spread across three tables: ratings, user informa‐\ntion, and movie information. We can load each table into a pandas DataFrame object\nusing pandas.read_table. Run the following code in a Jupyter cell:\nunames = [\"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]\nusers = pd.read_table(\"datasets/movielens/users.dat\", sep=\"::\",\n                      header=None, names=unames, engine=\"python\")\nrnames = [\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\nratings = pd.read_table(\"datasets/movielens/ratings.dat\", sep=\"::\",\n13.2 MovieLens 1M Dataset \n| \n435",
      "content_length": 1570,
      "extraction_method": "Direct"
    },
    {
      "page_number": 454,
      "chapter": null,
      "content": "header=None, names=rnames, engine=\"python\")\nmnames = [\"movie_id\", \"title\", \"genres\"]\nmovies = pd.read_table(\"datasets/movielens/movies.dat\", sep=\"::\",\n                       header=None, names=mnames, engine=\"python\")\nYou can verify that everything succeeded by looking at each DataFrame:\nIn [70]: users.head(5)\nOut[70]: \n   user_id gender  age  occupation    zip\n0        1      F    1          10  48067\n1        2      M   56          16  70072\n2        3      M   25          15  55117\n3        4      M   45           7  02460\n4        5      M   25          20  55455\nIn [71]: ratings.head(5)\nOut[71]: \n   user_id  movie_id  rating  timestamp\n0        1      1193       5  978300760\n1        1       661       3  978302109\n2        1       914       3  978301968\n3        1      3408       4  978300275\n4        1      2355       5  978824291\nIn [72]: movies.head(5)\nOut[72]: \n   movie_id                               title                        genres\n0         1                    Toy Story (1995)   Animation|Children's|Comedy\n1         2                      Jumanji (1995)  Adventure|Children's|Fantasy\n2         3             Grumpier Old Men (1995)                Comedy|Romance\n3         4            Waiting to Exhale (1995)                  Comedy|Drama\n4         5  Father of the Bride Part II (1995)                        Comedy\nIn [73]: ratings\nOut[73]: \n         user_id  movie_id  rating  timestamp\n0              1      1193       5  978300760\n1              1       661       3  978302109\n2              1       914       3  978301968\n3              1      3408       4  978300275\n4              1      2355       5  978824291\n...          ...       ...     ...        ...\n1000204     6040      1091       1  956716541\n1000205     6040      1094       5  956704887\n1000206     6040       562       5  956704746\n1000207     6040      1096       4  956715648\n1000208     6040      1097       4  956715569\n[1000209 rows x 4 columns]\nNote that ages and occupations are coded as integers indicating groups described\nin the dataset’s README file. Analyzing the data spread across three tables is not\n436 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 2163,
      "extraction_method": "Direct"
    },
    {
      "page_number": 455,
      "chapter": null,
      "content": "a simple task; for example, suppose you wanted to compute mean ratings for a\nparticular movie by gender identity and age. As you will see, this is more convenient\nto do with all of the data merged together into a single table. Using pandas’s merge\nfunction, we first merge ratings with users and then merge that result with the\nmovies data. pandas infers which columns to use as the merge (or join) keys based on\noverlapping names:\nIn [74]: data = pd.merge(pd.merge(ratings, users), movies)\nIn [75]: data\nOut[75]: \n         user_id  movie_id  rating  timestamp gender  age  occupation    zip  \\\n0              1      1193       5  978300760      F    1          10  48067   \n1              2      1193       5  978298413      M   56          16  70072   \n2             12      1193       4  978220179      M   25          12  32793   \n3             15      1193       4  978199279      M   25           7  22903   \n4             17      1193       5  978158471      M   50           1  95350   \n...          ...       ...     ...        ...    ...  ...         ...    ...   \n1000204     5949      2198       5  958846401      M   18          17  47901   \n1000205     5675      2703       3  976029116      M   35          14  30030   \n1000206     5780      2845       1  958153068      M   18          17  92886   \n1000207     5851      3607       5  957756608      F   18          20  55410   \n1000208     5938      2909       4  957273353      M   25           1  35401   \n                                               title                genres  \n0             One Flew Over the Cuckoo's Nest (1975)                 Drama  \n1             One Flew Over the Cuckoo's Nest (1975)                 Drama  \n2             One Flew Over the Cuckoo's Nest (1975)                 Drama  \n3             One Flew Over the Cuckoo's Nest (1975)                 Drama  \n4             One Flew Over the Cuckoo's Nest (1975)                 Drama  \n...                                              ...                   ...  \n1000204                           Modulations (1998)           Documentary  \n1000205                        Broken Vessels (1998)                 Drama  \n1000206                            White Boys (1999)                 Drama  \n1000207                     One Little Indian (1973)  Comedy|Drama|Western  \n1000208  Five Wives, Three Secretaries and Me (1998)           Documentary  \n[1000209 rows x 10 columns]\nIn [76]: data.iloc[0]\nOut[76]: \nuser_id                                            1\nmovie_id                                        1193\nrating                                             5\ntimestamp                                  978300760\ngender                                             F\nage                                                1\noccupation                                        10\nzip                                            48067\ntitle         One Flew Over the Cuckoo's Nest (1975)\ngenres                                         Drama\nName: 0, dtype: object\n13.2 MovieLens 1M Dataset \n| \n437",
      "content_length": 3045,
      "extraction_method": "Direct"
    },
    {
      "page_number": 456,
      "chapter": null,
      "content": "To get mean movie ratings for each film grouped by gender, we can use the\npivot_table method:\nIn [77]: mean_ratings = data.pivot_table(\"rating\", index=\"title\",\n   ....:                                 columns=\"gender\", aggfunc=\"mean\")\nIn [78]: mean_ratings.head(5)\nOut[78]: \ngender                                F         M\ntitle                                            \n$1,000,000 Duck (1971)         3.375000  2.761905\n'Night Mother (1986)           3.388889  3.352941\n'Til There Was You (1997)      2.675676  2.733333\n'burbs, The (1989)             2.793478  2.962085\n...And Justice for All (1979)  3.828571  3.689024\nThis produced another DataFrame containing mean ratings with movie titles as row\nlabels (the “index”) and gender as column labels. I first filter down to movies that\nreceived at least 250 ratings (an arbitrary number); to do this, I group the data by\ntitle, and use size() to get a Series of group sizes for each title:\nIn [79]: ratings_by_title = data.groupby(\"title\").size()\nIn [80]: ratings_by_title.head()\nOut[80]: \ntitle\n$1,000,000 Duck (1971)            37\n'Night Mother (1986)              70\n'Til There Was You (1997)         52\n'burbs, The (1989)               303\n...And Justice for All (1979)    199\ndtype: int64\nIn [81]: active_titles = ratings_by_title.index[ratings_by_title >= 250]\nIn [82]: active_titles\nOut[82]: \nIndex([''burbs, The (1989)', '10 Things I Hate About You (1999)',\n       '101 Dalmatians (1961)', '101 Dalmatians (1996)', '12 Angry Men (1957)',\n       '13th Warrior, The (1999)', '2 Days in the Valley (1996)',\n       '20,000 Leagues Under the Sea (1954)', '2001: A Space Odyssey (1968)',\n       '2010 (1984)',\n       ...\n       'X-Men (2000)', 'Year of Living Dangerously (1982)',\n       'Yellow Submarine (1968)', 'You've Got Mail (1998)',\n       'Young Frankenstein (1974)', 'Young Guns (1988)',\n       'Young Guns II (1990)', 'Young Sherlock Holmes (1985)',\n       'Zero Effect (1998)', 'eXistenZ (1999)'],\n      dtype='object', name='title', length=1216)\nThe index of titles receiving at least 250 ratings can then be used to select rows from\nmean_ratings using .loc:\n438 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 2171,
      "extraction_method": "Direct"
    },
    {
      "page_number": 457,
      "chapter": null,
      "content": "In [83]: mean_ratings = mean_ratings.loc[active_titles]\nIn [84]: mean_ratings\nOut[84]: \ngender                                    F         M\ntitle                                                \n'burbs, The (1989)                 2.793478  2.962085\n10 Things I Hate About You (1999)  3.646552  3.311966\n101 Dalmatians (1961)              3.791444  3.500000\n101 Dalmatians (1996)              3.240000  2.911215\n12 Angry Men (1957)                4.184397  4.328421\n...                                     ...       ...\nYoung Guns (1988)                  3.371795  3.425620\nYoung Guns II (1990)               2.934783  2.904025\nYoung Sherlock Holmes (1985)       3.514706  3.363344\nZero Effect (1998)                 3.864407  3.723140\neXistenZ (1999)                    3.098592  3.289086\n[1216 rows x 2 columns]\nTo see the top films among female viewers, we can sort by the F column in descend‐\ning order:\nIn [86]: top_female_ratings = mean_ratings.sort_values(\"F\", ascending=False)\nIn [87]: top_female_ratings.head()\nOut[87]: \ngender                                                         F         M\ntitle                                                                     \nClose Shave, A (1995)                                   4.644444  4.473795\nWrong Trousers, The (1993)                              4.588235  4.478261\nSunset Blvd. (a.k.a. Sunset Boulevard) (1950)           4.572650  4.464589\nWallace & Gromit: The Best of Aardman Animation (1996)  4.563107  4.385075\nSchindler's List (1993)                                 4.562602  4.491415\nMeasuring Rating Disagreement\nSuppose you wanted to find the movies that are most divisive between male and\nfemale viewers. One way is to add a column to mean_ratings containing the differ‐\nence in means, then sort by that:\nIn [88]: mean_ratings[\"diff\"] = mean_ratings[\"M\"] - mean_ratings[\"F\"]\nSorting by \"diff\" yields the movies with the greatest rating difference so that we can\nsee which ones were preferred by women:\nIn [89]: sorted_by_diff = mean_ratings.sort_values(\"diff\")\nIn [90]: sorted_by_diff.head()\nOut[90]: \ngender                            F         M      diff\ntitle                                                  \nDirty Dancing (1987)       3.790378  2.959596 -0.830782\n13.2 MovieLens 1M Dataset \n| \n439",
      "content_length": 2277,
      "extraction_method": "Direct"
    },
    {
      "page_number": 458,
      "chapter": null,
      "content": "Jumpin' Jack Flash (1986)  3.254717  2.578358 -0.676359\nGrease (1978)              3.975265  3.367041 -0.608224\nLittle Women (1994)        3.870588  3.321739 -0.548849\nSteel Magnolias (1989)     3.901734  3.365957 -0.535777\nReversing the order of the rows and again slicing off the top 10 rows, we get the\nmovies preferred by men that women didn’t rate as highly:\nIn [91]: sorted_by_diff[::-1].head()\nOut[91]: \ngender                                         F         M      diff\ntitle                                                               \nGood, The Bad and The Ugly, The (1966)  3.494949  4.221300  0.726351\nKentucky Fried Movie, The (1977)        2.878788  3.555147  0.676359\nDumb & Dumber (1994)                    2.697987  3.336595  0.638608\nLongest Day, The (1962)                 3.411765  4.031447  0.619682\nCable Guy, The (1996)                   2.250000  2.863787  0.613787\nSuppose instead you wanted the movies that elicited the most disagreement among\nviewers, independent of gender identification. Disagreement can be measured by the\nvariance or standard deviation of the ratings. To get this, we first compute the rating\nstandard deviation by title and then filter down to the active titles:\nIn [92]: rating_std_by_title = data.groupby(\"title\")[\"rating\"].std()\nIn [93]: rating_std_by_title = rating_std_by_title.loc[active_titles]\nIn [94]: rating_std_by_title.head()\nOut[94]: \ntitle\n'burbs, The (1989)                   1.107760\n10 Things I Hate About You (1999)    0.989815\n101 Dalmatians (1961)                0.982103\n101 Dalmatians (1996)                1.098717\n12 Angry Men (1957)                  0.812731\nName: rating, dtype: float64\nThen, we sort in descending order and select the first 10 rows, which are roughly the\n10 most divisively rated movies:\nIn [95]: rating_std_by_title.sort_values(ascending=False)[:10]\nOut[95]: \ntitle\nDumb & Dumber (1994)                     1.321333\nBlair Witch Project, The (1999)          1.316368\nNatural Born Killers (1994)              1.307198\nTank Girl (1995)                         1.277695\nRocky Horror Picture Show, The (1975)    1.260177\nEyes Wide Shut (1999)                    1.259624\nEvita (1996)                             1.253631\nBilly Madison (1995)                     1.249970\nFear and Loathing in Las Vegas (1998)    1.246408\n440 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 2356,
      "extraction_method": "Direct"
    },
    {
      "page_number": 459,
      "chapter": null,
      "content": "Bicentennial Man (1999)                  1.245533\nName: rating, dtype: float64\nYou may have noticed that movie genres are given as a pipe-separated (|) string, since\na single movie can belong to multiple genres. To help us group the ratings data by\ngenre, we can use the explode method on DataFrame. Let’s take a look at how this\nworks. First, we can split the genres string into a list of genres using the str.split\nmethod on the Series:\nIn [96]: movies[\"genres\"].head()\nOut[96]: \n0     Animation|Children's|Comedy\n1    Adventure|Children's|Fantasy\n2                  Comedy|Romance\n3                    Comedy|Drama\n4                          Comedy\nName: genres, dtype: object\nIn [97]: movies[\"genres\"].head().str.split(\"|\")\nOut[97]: \n0     [Animation, Children's, Comedy]\n1    [Adventure, Children's, Fantasy]\n2                   [Comedy, Romance]\n3                     [Comedy, Drama]\n4                            [Comedy]\nName: genres, dtype: object\nIn [98]: movies[\"genre\"] = movies.pop(\"genres\").str.split(\"|\")\nIn [99]: movies.head()\nOut[99]: \n   movie_id                               title  \\\n0         1                    Toy Story (1995)   \n1         2                      Jumanji (1995)   \n2         3             Grumpier Old Men (1995)   \n3         4            Waiting to Exhale (1995)   \n4         5  Father of the Bride Part II (1995)   \n                              genre  \n0   [Animation, Children's, Comedy]  \n1  [Adventure, Children's, Fantasy]  \n2                 [Comedy, Romance]  \n3                   [Comedy, Drama]  \n4                          [Comedy]  \nNow, calling movies.explode(\"genre\") generates a new DataFrame with one row for\neach “inner” element in each list of movie genres. For example, if a movie is classified\nas both a comedy and a romance, then there will be two rows in the result, one with\njust \"Comedy\" and the other with just \"Romance\":\nIn [100]: movies_exploded = movies.explode(\"genre\")\nIn [101]: movies_exploded[:10]\n13.2 MovieLens 1M Dataset \n| \n441",
      "content_length": 2004,
      "extraction_method": "Direct"
    },
    {
      "page_number": 460,
      "chapter": null,
      "content": "Out[101]: \n   movie_id                     title       genre\n0         1          Toy Story (1995)   Animation\n0         1          Toy Story (1995)  Children's\n0         1          Toy Story (1995)      Comedy\n1         2            Jumanji (1995)   Adventure\n1         2            Jumanji (1995)  Children's\n1         2            Jumanji (1995)     Fantasy\n2         3   Grumpier Old Men (1995)      Comedy\n2         3   Grumpier Old Men (1995)     Romance\n3         4  Waiting to Exhale (1995)      Comedy\n3         4  Waiting to Exhale (1995)       Drama\nNow, we can merge all three tables together and group by genre:\nIn [102]: ratings_with_genre = pd.merge(pd.merge(movies_exploded, ratings), users\n)\nIn [103]: ratings_with_genre.iloc[0]\nOut[103]: \nmovie_id                     1\ntitle         Toy Story (1995)\ngenre                Animation\nuser_id                      1\nrating                       5\ntimestamp            978824268\ngender                       F\nage                          1\noccupation                  10\nzip                      48067\nName: 0, dtype: object\nIn [104]: genre_ratings = (ratings_with_genre.groupby([\"genre\", \"age\"])\n   .....:                  [\"rating\"].mean()\n   .....:                  .unstack(\"age\"))\nIn [105]: genre_ratings[:10]\nOut[105]: \nage                1         18        25        35        45        50  \\\ngenre                                                                     \nAction       3.506385  3.447097  3.453358  3.538107  3.528543  3.611333   \nAdventure    3.449975  3.408525  3.443163  3.515291  3.528963  3.628163   \nAnimation    3.476113  3.624014  3.701228  3.740545  3.734856  3.780020   \nChildren's   3.241642  3.294257  3.426873  3.518423  3.527593  3.556555   \nComedy       3.497491  3.460417  3.490385  3.561984  3.591789  3.646868   \nCrime        3.710170  3.668054  3.680321  3.733736  3.750661  3.810688   \nDocumentary  3.730769  3.865865  3.946690  3.953747  3.966521  3.908108   \nDrama        3.794735  3.721930  3.726428  3.782512  3.784356  3.878415   \nFantasy      3.317647  3.353778  3.452484  3.482301  3.532468  3.581570   \nFilm-Noir    4.145455  3.997368  4.058725  4.064910  4.105376  4.175401   \nage                56  \ngenre                  \nAction       3.610709  \n442 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 2305,
      "extraction_method": "Direct"
    },
    {
      "page_number": 461,
      "chapter": null,
      "content": "Adventure    3.649064  \nAnimation    3.756233  \nChildren's   3.621822  \nComedy       3.650949  \nCrime        3.832549  \nDocumentary  3.961538  \nDrama        3.933465  \nFantasy      3.532700  \nFilm-Noir    4.125932  \n13.3 US Baby Names 1880–2010\nThe United States Social Security Administration (SSA) has made available data on\nthe frequency of baby names from 1880 through the present. Hadley Wickham, an\nauthor of several popular R packages, has this dataset in illustrating data manipula‐\ntion in R.\nWe need to do some data wrangling to load this dataset, but once we do that we will\nhave a DataFrame that looks like this:\nIn [4]: names.head(10)\nOut[4]:\n        name sex  births  year\n0       Mary   F    7065  1880\n1       Anna   F    2604  1880\n2       Emma   F    2003  1880\n3  Elizabeth   F    1939  1880\n4     Minnie   F    1746  1880\n5   Margaret   F    1578  1880\n6        Ida   F    1472  1880\n7      Alice   F    1414  1880\n8     Bertha   F    1320  1880\n9      Sarah   F    1288  1880\nThere are many things you might want to do with the dataset:\n• Visualize the proportion of babies given a particular name (your own, or another\n•\nname) over time\n• Determine the relative rank of a name\n•\n• Determine the most popular names in each year or the names whose popularity\n•\nhas advanced or declined the most\n• Analyze trends in names: vowels, consonants, length, overall diversity, changes in\n•\nspelling, first and last letters\n• Analyze external sources of trends: biblical names, celebrities, demographics\n•\nWith the tools in this book, many of these kinds of analyses are within reach, so I will\nwalk you through some of them.\n13.3 US Baby Names 1880–2010 \n| \n443",
      "content_length": 1673,
      "extraction_method": "Direct"
    },
    {
      "page_number": 462,
      "chapter": null,
      "content": "As of this writing, the US Social Security Administration makes available data files,\none per year, containing the total number of births for each sex/name combination.\nYou can download the raw archive of these files.\nIf this page has been moved by the time you’re reading this, it can most likely be\nlocated again with an internet search. After downloading the “National data” file\nnames.zip and unzipping it, you will have a directory containing a series of files like\nyob1880.txt. I use the Unix head command to look at the first 10 lines of one of the\nfiles (on Windows, you can use the more command or open it in a text editor):\nIn [106]: !head -n 10 datasets/babynames/yob1880.txt\nMary,F,7065\nAnna,F,2604\nEmma,F,2003\nElizabeth,F,1939\nMinnie,F,1746\nMargaret,F,1578\nIda,F,1472\nAlice,F,1414\nBertha,F,1320\nSarah,F,1288\nAs this is already in comma-separated form, it can be loaded into a DataFrame with\npandas.read_csv:\nIn [107]: names1880 = pd.read_csv(\"datasets/babynames/yob1880.txt\",\n   .....:                         names=[\"name\", \"sex\", \"births\"])\nIn [108]: names1880\nOut[108]: \n           name sex  births\n0          Mary   F    7065\n1          Anna   F    2604\n2          Emma   F    2003\n3     Elizabeth   F    1939\n4        Minnie   F    1746\n...         ...  ..     ...\n1995     Woodie   M       5\n1996     Worthy   M       5\n1997     Wright   M       5\n1998       York   M       5\n1999  Zachariah   M       5\n[2000 rows x 3 columns]\n444 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 1489,
      "extraction_method": "Direct"
    },
    {
      "page_number": 463,
      "chapter": null,
      "content": "These files only contain names with at least five occurrences in each year, so for\nsimplicity’s sake we can use the sum of the births column by sex as the total number\nof births in that year:\nIn [109]: names1880.groupby(\"sex\")[\"births\"].sum()\nOut[109]: \nsex\nF     90993\nM    110493\nName: births, dtype: int64\nSince the dataset is split into files by year, one of the first things to do is to assemble\nall of the data into a single DataFrame and further add a year field. You can do this\nusing pandas.concat. Run the following in a Jupyter cell:\npieces = []\nfor year in range(1880, 2011):\n    path = f\"datasets/babynames/yob{year}.txt\"\n    frame = pd.read_csv(path, names=[\"name\", \"sex\", \"births\"])\n    # Add a column for the year\n    frame[\"year\"] = year\n    pieces.append(frame)\n# Concatenate everything into a single DataFrame\nnames = pd.concat(pieces, ignore_index=True)\nThere are a couple things to note here. First, remember that concat combines the\nDataFrame objects by row by default. Second, you have to pass ignore_index=True\nbecause we’re not interested in preserving the original row numbers returned from\npandas.read_csv. So we now have a single DataFrame containing all of the names\ndata across all years:\nIn [111]: names\nOut[111]: \n              name sex  births  year\n0             Mary   F    7065  1880\n1             Anna   F    2604  1880\n2             Emma   F    2003  1880\n3        Elizabeth   F    1939  1880\n4           Minnie   F    1746  1880\n...            ...  ..     ...   ...\n1690779    Zymaire   M       5  2010\n1690780     Zyonne   M       5  2010\n1690781  Zyquarius   M       5  2010\n1690782      Zyran   M       5  2010\n1690783      Zzyzx   M       5  2010\n[1690784 rows x 4 columns]\n13.3 US Baby Names 1880–2010 \n| \n445",
      "content_length": 1753,
      "extraction_method": "Direct"
    },
    {
      "page_number": 464,
      "chapter": null,
      "content": "With this data in hand, we can already start aggregating the data at the year and sex\nlevel using groupby or pivot_table (see Figure 13-4):\nIn [112]: total_births = names.pivot_table(\"births\", index=\"year\",\n   .....:                                  columns=\"sex\", aggfunc=sum)\nIn [113]: total_births.tail()\nOut[113]: \nsex         F        M\nyear                  \n2006  1896468  2050234\n2007  1916888  2069242\n2008  1883645  2032310\n2009  1827643  1973359\n2010  1759010  1898382\nIn [114]: total_births.plot(title=\"Total births by sex and year\")\nFigure 13-4. Total births by sex and year\nNext, let’s insert a column prop with the fraction of babies given each name relative\nto the total number of births. A prop value of 0.02 would indicate that 2 out of every\n100 babies were given a particular name. Thus, we group the data by year and sex,\nthen add the new column to each group:\ndef add_prop(group):\n    group[\"prop\"] = group[\"births\"] / group[\"births\"].sum()\n446 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 1005,
      "extraction_method": "Direct"
    },
    {
      "page_number": 465,
      "chapter": null,
      "content": "return group\nnames = names.groupby([\"year\", \"sex\"]).apply(add_prop)\nThe resulting complete dataset now has the following columns:\nIn [116]: names\nOut[116]: \n              name sex  births  year      prop\n0             Mary   F    7065  1880  0.077643\n1             Anna   F    2604  1880  0.028618\n2             Emma   F    2003  1880  0.022013\n3        Elizabeth   F    1939  1880  0.021309\n4           Minnie   F    1746  1880  0.019188\n...            ...  ..     ...   ...       ...\n1690779    Zymaire   M       5  2010  0.000003\n1690780     Zyonne   M       5  2010  0.000003\n1690781  Zyquarius   M       5  2010  0.000003\n1690782      Zyran   M       5  2010  0.000003\n1690783      Zzyzx   M       5  2010  0.000003\n[1690784 rows x 5 columns]\nWhen performing a group operation like this, it’s often valuable to do a sanity check,\nlike verifying that the prop column sums to 1 within all the groups:\nIn [117]: names.groupby([\"year\", \"sex\"])[\"prop\"].sum()\nOut[117]: \nyear  sex\n1880  F      1.0\n      M      1.0\n1881  F      1.0\n      M      1.0\n1882  F      1.0\n            ... \n2008  M      1.0\n2009  F      1.0\n      M      1.0\n2010  F      1.0\n      M      1.0\nName: prop, Length: 262, dtype: float64\nNow that this is done, I’m going to extract a subset of the data to facilitate further\nanalysis: the top 1,000 names for each sex/year combination. This is yet another\ngroup operation:\nIn [118]: def get_top1000(group):\n   .....:     return group.sort_values(\"births\", ascending=False)[:1000]\nIn [119]: grouped = names.groupby([\"year\", \"sex\"])\nIn [120]: top1000 = grouped.apply(get_top1000)\nIn [121]: top1000.head()\nOut[121]: \n13.3 US Baby Names 1880–2010 \n| \n447",
      "content_length": 1669,
      "extraction_method": "Direct"
    },
    {
      "page_number": 466,
      "chapter": null,
      "content": "name sex  births  year      prop\nyear sex                                         \n1880 F   0       Mary   F    7065  1880  0.077643\n         1       Anna   F    2604  1880  0.028618\n         2       Emma   F    2003  1880  0.022013\n         3  Elizabeth   F    1939  1880  0.021309\n         4     Minnie   F    1746  1880  0.019188\nWe can drop the group index since we don’t need it for our analysis:\nIn [122]: top1000 = top1000.reset_index(drop=True)\nThe resulting dataset is now quite a bit smaller:\nIn [123]: top1000.head()\nOut[123]: \n        name sex  births  year      prop\n0       Mary   F    7065  1880  0.077643\n1       Anna   F    2604  1880  0.028618\n2       Emma   F    2003  1880  0.022013\n3  Elizabeth   F    1939  1880  0.021309\n4     Minnie   F    1746  1880  0.019188\nWe’ll use this top one thousand dataset in the following investigations into the data.\nAnalyzing Naming Trends\nWith the full dataset and the top one thousand dataset in hand, we can start analyzing\nvarious naming trends of interest. First, we can split the top one thousand names into\nthe boy and girl portions:\nIn [124]: boys = top1000[top1000[\"sex\"] == \"M\"]\nIn [125]: girls = top1000[top1000[\"sex\"] == \"F\"]\nSimple time series, like the number of Johns or Marys for each year, can be plotted\nbut require some manipulation to be more useful. Let’s form a pivot table of the total\nnumber of births by year and name:\nIn [126]: total_births = top1000.pivot_table(\"births\", index=\"year\",\n   .....:                                    columns=\"name\",\n   .....:                                    aggfunc=sum)\nNow, this can be plotted for a handful of names with DataFrame’s plot method\n(Figure 13-5 shows the result):\nIn [127]: total_births.info()\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 131 entries, 1880 to 2010\nColumns: 6868 entries, Aaden to Zuri\ndtypes: float64(6868)\nmemory usage: 6.9 MB\nIn [128]: subset = total_births[[\"John\", \"Harry\", \"Mary\", \"Marilyn\"]]\n448 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 1995,
      "extraction_method": "Direct"
    },
    {
      "page_number": 467,
      "chapter": null,
      "content": "In [129]: subset.plot(subplots=True, figsize=(12, 10),\n   .....:             title=\"Number of births per year\")\nFigure 13-5. A few boy and girl names over time\nOn looking at this, you might conclude that these names have grown out of favor\nwith the American population. But the story is actually more complicated than that,\nas will be explored in the next section.\nMeasuring the increase in naming diversity\nOne explanation for the decrease in plots is that fewer parents are choosing common\nnames for their children. This hypothesis can be explored and confirmed in the data.\nOne measure is the proportion of births represented by the top 1,000 most popular\nnames, which I aggregate and plot by year and sex (Figure 13-6 shows the resulting\nplot):\n13.3 US Baby Names 1880–2010 \n| \n449",
      "content_length": 785,
      "extraction_method": "Direct"
    },
    {
      "page_number": 468,
      "chapter": null,
      "content": "In [131]: table = top1000.pivot_table(\"prop\", index=\"year\",\n   .....:                             columns=\"sex\", aggfunc=sum)\nIn [132]: table.plot(title=\"Sum of table1000.prop by year and sex\",\n   .....:            yticks=np.linspace(0, 1.2, 13))\nFigure 13-6. Proportion of births represented in top one thousand names by sex\nYou can see that, indeed, there appears to be increasing name diversity (decreasing\ntotal proportion in the top one thousand). Another interesting metric is the number\nof distinct names, taken in order of popularity from highest to lowest, in the top 50%\nof births. This number is trickier to compute. Let’s consider just the boy names from\n2010:\nIn [133]: df = boys[boys[\"year\"] == 2010]\nIn [134]: df\nOut[134]: \n           name sex  births  year      prop\n260877    Jacob   M   21875  2010  0.011523\n260878    Ethan   M   17866  2010  0.009411\n260879  Michael   M   17133  2010  0.009025\n260880   Jayden   M   17030  2010  0.008971\n260881  William   M   16870  2010  0.008887\n...         ...  ..     ...   ...       ...\n261872   Camilo   M     194  2010  0.000102\n261873   Destin   M     194  2010  0.000102\n261874   Jaquan   M     194  2010  0.000102\n450 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 1221,
      "extraction_method": "Direct"
    },
    {
      "page_number": 469,
      "chapter": null,
      "content": "261875   Jaydan   M     194  2010  0.000102\n261876   Maxton   M     193  2010  0.000102\n[1000 rows x 5 columns]\nAfter sorting prop in descending order, we want to know how many of the most\npopular names it takes to reach 50%. You could write a for loop to do this, but a\nvectorized NumPy way is more computationally efficient. Taking the cumulative sum,\ncumsum, of prop and then calling the method searchsorted returns the position in\nthe cumulative sum at which 0.5 would need to be inserted to keep it in sorted order:\nIn [135]: prop_cumsum = df[\"prop\"].sort_values(ascending=False).cumsum()\nIn [136]: prop_cumsum[:10]\nOut[136]: \n260877    0.011523\n260878    0.020934\n260879    0.029959\n260880    0.038930\n260881    0.047817\n260882    0.056579\n260883    0.065155\n260884    0.073414\n260885    0.081528\n260886    0.089621\nName: prop, dtype: float64\nIn [137]: prop_cumsum.searchsorted(0.5)\nOut[137]: 116\nSince arrays are zero-indexed, adding 1 to this result gives you a result of 117. By\ncontrast, in 1900 this number was much smaller:\nIn [138]: df = boys[boys.year == 1900]\nIn [139]: in1900 = df.sort_values(\"prop\", ascending=False).prop.cumsum()\nIn [140]: in1900.searchsorted(0.5) + 1\nOut[140]: 25\nYou can now apply this operation to each year/sex combination, groupby those fields,\nand apply a function returning the count for each group:\ndef get_quantile_count(group, q=0.5):\n    group = group.sort_values(\"prop\", ascending=False)\n    return group.prop.cumsum().searchsorted(q) + 1\ndiversity = top1000.groupby([\"year\", \"sex\"]).apply(get_quantile_count)\ndiversity = diversity.unstack()\nThis resulting DataFrame diversity now has two time series, one for each sex,\nindexed by year. This can be inspected and plotted as before (see Figure 13-7):\n13.3 US Baby Names 1880–2010 \n| \n451",
      "content_length": 1783,
      "extraction_method": "Direct"
    },
    {
      "page_number": 470,
      "chapter": null,
      "content": "In [143]: diversity.head()\nOut[143]: \nsex    F   M\nyear        \n1880  38  14\n1881  38  14\n1882  38  15\n1883  39  15\n1884  39  16\nIn [144]: diversity.plot(title=\"Number of popular names in top 50%\")\nFigure 13-7. Plot of diversity metric by year\nAs you can see, girl names have always been more diverse than boy names, and they\nhave only become more so over time. Further analysis of what exactly is driving the\ndiversity, like the increase of alternative spellings, is left to the reader.\nThe “last letter” revolution\nIn 2007, baby name researcher Laura Wattenberg pointed out that the distribution of\nboy names by final letter has changed significantly over the last 100 years. To see this,\nwe first aggregate all of the births in the full dataset by year, sex, and final letter:\ndef get_last_letter(x):\n    return x[-1]\nlast_letters = names[\"name\"].map(get_last_letter)\n452 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 913,
      "extraction_method": "Direct"
    },
    {
      "page_number": 471,
      "chapter": null,
      "content": "last_letters.name = \"last_letter\"\ntable = names.pivot_table(\"births\", index=last_letters,\n                          columns=[\"sex\", \"year\"], aggfunc=sum)\nThen we select three representative years spanning the history and print the first few\nrows:\nIn [146]: subtable = table.reindex(columns=[1910, 1960, 2010], level=\"year\")\nIn [147]: subtable.head()\nOut[147]: \nsex                 F                            M                    \nyear             1910      1960      2010     1910      1960      2010\nlast_letter                                                           \na            108376.0  691247.0  670605.0    977.0    5204.0   28438.0\nb                 NaN     694.0     450.0    411.0    3912.0   38859.0\nc                 5.0      49.0     946.0    482.0   15476.0   23125.0\nd              6750.0    3729.0    2607.0  22111.0  262112.0   44398.0\ne            133569.0  435013.0  313833.0  28655.0  178823.0  129012.0\nNext, normalize the table by total births to compute a new table containing the\nproportion of total births for each sex ending in each letter:\nIn [148]: subtable.sum()\nOut[148]: \nsex  year\nF    1910     396416.0\n     1960    2022062.0\n     2010    1759010.0\nM    1910     194198.0\n     1960    2132588.0\n     2010    1898382.0\ndtype: float64\nIn [149]: letter_prop = subtable / subtable.sum()\nIn [150]: letter_prop\nOut[150]: \nsex                 F                             M                    \nyear             1910      1960      2010      1910      1960      2010\nlast_letter                                                            \na            0.273390  0.341853  0.381240  0.005031  0.002440  0.014980\nb                 NaN  0.000343  0.000256  0.002116  0.001834  0.020470\nc            0.000013  0.000024  0.000538  0.002482  0.007257  0.012181\nd            0.017028  0.001844  0.001482  0.113858  0.122908  0.023387\ne            0.336941  0.215133  0.178415  0.147556  0.083853  0.067959\n...               ...       ...       ...       ...       ...       ...\nv                 NaN  0.000060  0.000117  0.000113  0.000037  0.001434\nw            0.000020  0.000031  0.001182  0.006329  0.007711  0.016148\nx            0.000015  0.000037  0.000727  0.003965  0.001851  0.008614\ny            0.110972  0.152569  0.116828  0.077349  0.160987  0.058168\n13.3 US Baby Names 1880–2010 \n| \n453",
      "content_length": 2326,
      "extraction_method": "Direct"
    },
    {
      "page_number": 472,
      "chapter": null,
      "content": "z            0.002439  0.000659  0.000704  0.000170  0.000184  0.001831\n[26 rows x 6 columns]\nWith the letter proportions now in hand, we can make bar plots for each sex, broken\ndown by year (see Figure 13-8):\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(2, 1, figsize=(10, 8))\nletter_prop[\"M\"].plot(kind=\"bar\", rot=0, ax=axes[0], title=\"Male\")\nletter_prop[\"F\"].plot(kind=\"bar\", rot=0, ax=axes[1], title=\"Female\",\n                      legend=False)\nFigure 13-8. Proportion of boy and girl names ending in each letter\nAs you can see, boy names ending in n have experienced significant growth since\nthe 1960s. Going back to the full table created before, I again normalize by year and\nsex and select a subset of letters for the boy names, finally transposing to make each\ncolumn a time series:\nIn [153]: letter_prop = table / table.sum()\n454 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 891,
      "extraction_method": "Direct"
    },
    {
      "page_number": 473,
      "chapter": null,
      "content": "In [154]: dny_ts = letter_prop.loc[[\"d\", \"n\", \"y\"], \"M\"].T\nIn [155]: dny_ts.head()\nOut[155]: \nlast_letter         d         n         y\nyear                                     \n1880         0.083055  0.153213  0.075760\n1881         0.083247  0.153214  0.077451\n1882         0.085340  0.149560  0.077537\n1883         0.084066  0.151646  0.079144\n1884         0.086120  0.149915  0.080405\nWith this DataFrame of time series in hand, I can make a plot of the trends over time\nagain with its plot method (see Figure 13-9):\nIn [158]: dny_ts.plot()\nFigure 13-9. Proportion of boys born with names ending in d/n/y over time\nBoy names that became girl names (and vice versa)\nAnother fun trend is looking at names that were more popular with one gender\nearlier in the sample but have become preferred as a name for the other gender\nover time. One example is the name Lesley or Leslie. Going back to the top1000\nDataFrame, I compute a list of names occurring in the dataset starting with “Lesl”:\nIn [159]: all_names = pd.Series(top1000[\"name\"].unique())\nIn [160]: lesley_like = all_names[all_names.str.contains(\"Lesl\")]\n13.3 US Baby Names 1880–2010 \n| \n455",
      "content_length": 1147,
      "extraction_method": "Direct"
    },
    {
      "page_number": 474,
      "chapter": null,
      "content": "In [161]: lesley_like\nOut[161]: \n632     Leslie\n2294    Lesley\n4262    Leslee\n4728     Lesli\n6103     Lesly\ndtype: object\nFrom there, we can filter down to just those names and sum births grouped by name\nto see the relative frequencies:\nIn [162]: filtered = top1000[top1000[\"name\"].isin(lesley_like)]\nIn [163]: filtered.groupby(\"name\")[\"births\"].sum()\nOut[163]: \nname\nLeslee      1082\nLesley     35022\nLesli        929\nLeslie    370429\nLesly      10067\nName: births, dtype: int64\nNext, let’s aggregate by sex and year, and normalize within year:\nIn [164]: table = filtered.pivot_table(\"births\", index=\"year\",\n   .....:                              columns=\"sex\", aggfunc=\"sum\")\nIn [165]: table = table.div(table.sum(axis=\"columns\"), axis=\"index\")\nIn [166]: table.tail()\nOut[166]: \nsex     F   M\nyear         \n2006  1.0 NaN\n2007  1.0 NaN\n2008  1.0 NaN\n2009  1.0 NaN\n2010  1.0 NaN\nLastly, it’s now possible to make a plot of the breakdown by sex over time (see\nFigure 13-10):\nIn [168]: table.plot(style={\"M\": \"k-\", \"F\": \"k--\"})\n456 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 1068,
      "extraction_method": "Direct"
    },
    {
      "page_number": 475,
      "chapter": null,
      "content": "Figure 13-10. Proportion of male/female Lesley-like names over time\n13.4 USDA Food Database\nThe US Department of Agriculture (USDA) makes available a database of food\nnutrient information. Programmer Ashley Williams created a version of this database\nin JSON format. The records look like this:\n{\n  \"id\": 21441,\n  \"description\": \"KENTUCKY FRIED CHICKEN, Fried Chicken, EXTRA CRISPY,\nWing, meat and skin with breading\",\n  \"tags\": [\"KFC\"],\n  \"manufacturer\": \"Kentucky Fried Chicken\",\n  \"group\": \"Fast Foods\",\n  \"portions\": [\n    {\n      \"amount\": 1,\n      \"unit\": \"wing, with skin\",\n      \"grams\": 68.0\n    },\n    ...\n  ],\n  \"nutrients\": [\n    {\n      \"value\": 20.8,\n      \"units\": \"g\",\n13.4 USDA Food Database \n| \n457",
      "content_length": 716,
      "extraction_method": "Direct"
    },
    {
      "page_number": 476,
      "chapter": null,
      "content": "\"description\": \"Protein\",\n      \"group\": \"Composition\"\n    },\n    ...\n  ]\n}\nEach food has a number of identifying attributes along with two lists of nutrients and\nportion sizes. Data in this form is not particularly amenable to analysis, so we need to\ndo some work to wrangle the data into a better form.\nYou can load this file into Python with any JSON library of your choosing. I’ll use the\nbuilt-in Python json module:\nIn [169]: import json\nIn [170]: db = json.load(open(\"datasets/usda_food/database.json\"))\nIn [171]: len(db)\nOut[171]: 6636\nEach entry in db is a dictionary containing all the data for a single food. The\n\"nutrients\" field is a list of dictionaries, one for each nutrient:\nIn [172]: db[0].keys()\nOut[172]: dict_keys(['id', 'description', 'tags', 'manufacturer', 'group', 'porti\nons', 'nutrients'])\nIn [173]: db[0][\"nutrients\"][0]\nOut[173]: \n{'value': 25.18,\n 'units': 'g',\n 'description': 'Protein',\n 'group': 'Composition'}\nIn [174]: nutrients = pd.DataFrame(db[0][\"nutrients\"])\nIn [175]: nutrients.head(7)\nOut[175]: \n     value units                  description        group\n0    25.18     g                      Protein  Composition\n1    29.20     g            Total lipid (fat)  Composition\n2     3.06     g  Carbohydrate, by difference  Composition\n3     3.28     g                          Ash        Other\n4   376.00  kcal                       Energy       Energy\n5    39.28     g                        Water  Composition\n6  1573.00    kJ                       Energy       Energy\nWhen converting a list of dictionaries to a DataFrame, we can specify a list of fields to\nextract. We’ll take the food names, group, ID, and manufacturer:\n458 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 1707,
      "extraction_method": "Direct"
    },
    {
      "page_number": 477,
      "chapter": null,
      "content": "In [176]: info_keys = [\"description\", \"group\", \"id\", \"manufacturer\"]\nIn [177]: info = pd.DataFrame(db, columns=info_keys)\nIn [178]: info.head()\nOut[178]: \n                          description                   group    id  \\\n0                     Cheese, caraway  Dairy and Egg Products  1008   \n1                     Cheese, cheddar  Dairy and Egg Products  1009   \n2                        Cheese, edam  Dairy and Egg Products  1018   \n3                        Cheese, feta  Dairy and Egg Products  1019   \n4  Cheese, mozzarella, part skim milk  Dairy and Egg Products  1028   \n  manufacturer  \n0               \n1               \n2               \n3               \n4               \nIn [179]: info.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 6636 entries, 0 to 6635\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   description   6636 non-null   object\n 1   group         6636 non-null   object\n 2   id            6636 non-null   int64 \n 3   manufacturer  5195 non-null   object\ndtypes: int64(1), object(3)\nmemory usage: 207.5+ KB\nFrom the output of info.info(), we can see that there is missing data in the manufac\nturer column.\nYou can see the distribution of food groups with value_counts:\nIn [180]: pd.value_counts(info[\"group\"])[:10]\nOut[180]: \nVegetables and Vegetable Products    812\nBeef Products                        618\nBaked Products                       496\nBreakfast Cereals                    403\nLegumes and Legume Products          365\nFast Foods                           365\nLamb, Veal, and Game Products        345\nSweets                               341\nFruits and Fruit Juices              328\nPork Products                        328\nName: group, dtype: int64\nNow, to do some analysis on all of the nutrient data, it’s easiest to assemble the\nnutrients for each food into a single large table. To do so, we need to take several\n13.4 USDA Food Database \n| \n459",
      "content_length": 1969,
      "extraction_method": "Direct"
    },
    {
      "page_number": 478,
      "chapter": null,
      "content": "steps. First, I’ll convert each list of food nutrients to a DataFrame, add a column for\nthe food id, and append the DataFrame to a list. Then, these can be concatenated\nwith concat. Run the following code in a Jupyter cell:\nnutrients = []\nfor rec in db:\n    fnuts = pd.DataFrame(rec[\"nutrients\"])\n    fnuts[\"id\"] = rec[\"id\"]\n    nutrients.append(fnuts)\nnutrients = pd.concat(nutrients, ignore_index=True)\nIf all goes well, nutrients should look like this:\nIn [182]: nutrients\nOut[182]: \n          value units                         description        group     id\n0        25.180     g                             Protein  Composition   1008\n1        29.200     g                   Total lipid (fat)  Composition   1008\n2         3.060     g         Carbohydrate, by difference  Composition   1008\n3         3.280     g                                 Ash        Other   1008\n4       376.000  kcal                              Energy       Energy   1008\n...         ...   ...                                 ...          ...    ...\n389350    0.000   mcg                 Vitamin B-12, added     Vitamins  43546\n389351    0.000    mg                         Cholesterol        Other  43546\n389352    0.072     g        Fatty acids, total saturated        Other  43546\n389353    0.028     g  Fatty acids, total monounsaturated        Other  43546\n389354    0.041     g  Fatty acids, total polyunsaturated        Other  43546\n[389355 rows x 5 columns]\nI noticed that there are duplicates in this DataFrame, so it makes things easier to drop\nthem:\nIn [183]: nutrients.duplicated().sum()  # number of duplicates\nOut[183]: 14179\nIn [184]: nutrients = nutrients.drop_duplicates()\nSince \"group\" and \"description\" are in both DataFrame objects, we can rename for\nclarity:\nIn [185]: col_mapping = {\"description\" : \"food\",\n   .....:                \"group\"       : \"fgroup\"}\nIn [186]: info = info.rename(columns=col_mapping, copy=False)\nIn [187]: info.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 6636 entries, 0 to 6635\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n460 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 2179,
      "extraction_method": "Direct"
    },
    {
      "page_number": 479,
      "chapter": null,
      "content": "0   food          6636 non-null   object\n 1   fgroup        6636 non-null   object\n 2   id            6636 non-null   int64 \n 3   manufacturer  5195 non-null   object\ndtypes: int64(1), object(3)\nmemory usage: 207.5+ KB\nIn [188]: col_mapping = {\"description\" : \"nutrient\",\n   .....:                \"group\" : \"nutgroup\"}\nIn [189]: nutrients = nutrients.rename(columns=col_mapping, copy=False)\nIn [190]: nutrients\nOut[190]: \n          value units                            nutrient     nutgroup     id\n0        25.180     g                             Protein  Composition   1008\n1        29.200     g                   Total lipid (fat)  Composition   1008\n2         3.060     g         Carbohydrate, by difference  Composition   1008\n3         3.280     g                                 Ash        Other   1008\n4       376.000  kcal                              Energy       Energy   1008\n...         ...   ...                                 ...          ...    ...\n389350    0.000   mcg                 Vitamin B-12, added     Vitamins  43546\n389351    0.000    mg                         Cholesterol        Other  43546\n389352    0.072     g        Fatty acids, total saturated        Other  43546\n389353    0.028     g  Fatty acids, total monounsaturated        Other  43546\n389354    0.041     g  Fatty acids, total polyunsaturated        Other  43546\n[375176 rows x 5 columns]\nWith all of this done, we’re ready to merge info with nutrients:\nIn [191]: ndata = pd.merge(nutrients, info, on=\"id\")\nIn [192]: ndata.info()\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 375176 entries, 0 to 375175\nData columns (total 8 columns):\n #   Column        Non-Null Count   Dtype  \n---  ------        --------------   -----  \n 0   value         375176 non-null  float64\n 1   units         375176 non-null  object \n 2   nutrient      375176 non-null  object \n 3   nutgroup      375176 non-null  object \n 4   id            375176 non-null  int64  \n 5   food          375176 non-null  object \n 6   fgroup        375176 non-null  object \n 7   manufacturer  293054 non-null  object \ndtypes: float64(1), int64(1), object(6)\nmemory usage: 25.8+ MB\nIn [193]: ndata.iloc[30000]\nOut[193]: \nvalue                                             0.04\nunits                                                g\n13.4 USDA Food Database \n| \n461",
      "content_length": 2318,
      "extraction_method": "Direct"
    },
    {
      "page_number": 480,
      "chapter": null,
      "content": "nutrient                                       Glycine\nnutgroup                                   Amino Acids\nid                                                6158\nfood            Soup, tomato bisque, canned, condensed\nfgroup                      Soups, Sauces, and Gravies\nmanufacturer                                          \nName: 30000, dtype: object\nWe could now make a plot of median values by food group and nutrient type (see\nFigure 13-11):\nIn [195]: result = ndata.groupby([\"nutrient\", \"fgroup\"])[\"value\"].quantile(0.5)\nIn [196]: result[\"Zinc, Zn\"].sort_values().plot(kind=\"barh\")\nFigure 13-11. Median zinc values by food group\nUsing the idxmax or argmax Series methods, you can find which food is most dense in\neach nutrient. Run the following in a Jupyter cell:\nby_nutrient = ndata.groupby([\"nutgroup\", \"nutrient\"])\ndef get_maximum(x):\n    return x.loc[x.value.idxmax()]\nmax_foods = by_nutrient.apply(get_maximum)[[\"value\", \"food\"]]\n# make the food a little smaller\nmax_foods[\"food\"] = max_foods[\"food\"].str[:50]\nThe resulting DataFrame is a bit too large to display in the book; here is only the\n\"Amino Acids\" nutrient group:\nIn [198]: max_foods.loc[\"Amino Acids\"][\"food\"]\nOut[198]: \n462 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 1240,
      "extraction_method": "Direct"
    },
    {
      "page_number": 481,
      "chapter": null,
      "content": "nutrient\nAlanine                            Gelatins, dry powder, unsweetened\nArginine                                Seeds, sesame flour, low-fat\nAspartic acid                                    Soy protein isolate\nCystine                 Seeds, cottonseed flour, low fat (glandless)\nGlutamic acid                                    Soy protein isolate\nGlycine                            Gelatins, dry powder, unsweetened\nHistidine                 Whale, beluga, meat, dried (Alaska Native)\nHydroxyproline    KENTUCKY FRIED CHICKEN, Fried Chicken, ORIGINAL RE\nIsoleucine        Soy protein isolate, PROTEIN TECHNOLOGIES INTERNAT\nLeucine           Soy protein isolate, PROTEIN TECHNOLOGIES INTERNAT\nLysine            Seal, bearded (Oogruk), meat, dried (Alaska Native\nMethionine                     Fish, cod, Atlantic, dried and salted\nPhenylalanine     Soy protein isolate, PROTEIN TECHNOLOGIES INTERNAT\nProline                            Gelatins, dry powder, unsweetened\nSerine            Soy protein isolate, PROTEIN TECHNOLOGIES INTERNAT\nThreonine         Soy protein isolate, PROTEIN TECHNOLOGIES INTERNAT\nTryptophan          Sea lion, Steller, meat with fat (Alaska Native)\nTyrosine          Soy protein isolate, PROTEIN TECHNOLOGIES INTERNAT\nValine            Soy protein isolate, PROTEIN TECHNOLOGIES INTERNAT\nName: food, dtype: object\n13.5 2012 Federal Election Commission Database\nThe US Federal Election Commission (FEC) publishes data on contributions to polit‐\nical campaigns. This includes contributor names, occupation and employer, address,\nand contribution amount. The contribution data from the 2012 US presidential\nelection was available as a single 150-megabyte CSV file P00000001-ALL.csv (see the\nbook’s data repository), which can be loaded with pandas.read_csv:\nIn [199]: fec = pd.read_csv(\"datasets/fec/P00000001-ALL.csv\", low_memory=False)\nIn [200]: fec.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1001731 entries, 0 to 1001730\nData columns (total 16 columns):\n #   Column             Non-Null Count    Dtype  \n---  ------             --------------    -----  \n 0   cmte_id            1001731 non-null  object \n 1   cand_id            1001731 non-null  object \n 2   cand_nm            1001731 non-null  object \n 3   contbr_nm          1001731 non-null  object \n 4   contbr_city        1001712 non-null  object \n 5   contbr_st          1001727 non-null  object \n 6   contbr_zip         1001620 non-null  object \n 7   contbr_employer    988002 non-null   object \n 8   contbr_occupation  993301 non-null   object \n 9   contb_receipt_amt  1001731 non-null  float64\n 10  contb_receipt_dt   1001731 non-null  object \n 11  receipt_desc       14166 non-null    object \n 12  memo_cd            92482 non-null    object \n13.5 2012 Federal Election Commission Database \n| \n463",
      "content_length": 2805,
      "extraction_method": "Direct"
    },
    {
      "page_number": 482,
      "chapter": null,
      "content": "13  memo_text          97770 non-null    object \n 14  form_tp            1001731 non-null  object \n 15  file_num           1001731 non-null  int64  \ndtypes: float64(1), int64(1), object(14)\nmemory usage: 122.3+ MB\nSeveral people asked me to update the dataset from the 2012\nelection to the 2016 or 2020 elections. Unfortunately, the more\nrecent datasets provided by the FEC have become larger and more\ncomplex, and I decided that working with them here would be a\ndistraction from the analysis techniques that I wanted to illustrate.\nA sample record in the DataFrame looks like this:\nIn [201]: fec.iloc[123456]\nOut[201]: \ncmte_id                             C00431445\ncand_id                             P80003338\ncand_nm                         Obama, Barack\ncontbr_nm                         ELLMAN, IRA\ncontbr_city                             TEMPE\ncontbr_st                                  AZ\ncontbr_zip                          852816719\ncontbr_employer      ARIZONA STATE UNIVERSITY\ncontbr_occupation                   PROFESSOR\ncontb_receipt_amt                        50.0\ncontb_receipt_dt                    01-DEC-11\nreceipt_desc                              NaN\nmemo_cd                                   NaN\nmemo_text                                 NaN\nform_tp                                 SA17A\nfile_num                               772372\nName: 123456, dtype: object\nYou may think of some ways to start slicing and dicing this data to extract informative\nstatistics about donors and patterns in the campaign contributions. I’ll show you a\nnumber of different analyses that apply the techniques in this book.\nYou can see that there are no political party affiliations in the data, so this would be\nuseful to add. You can get a list of all the unique political candidates using unique:\nIn [202]: unique_cands = fec[\"cand_nm\"].unique()\nIn [203]: unique_cands\nOut[203]: \narray(['Bachmann, Michelle', 'Romney, Mitt', 'Obama, Barack',\n       \"Roemer, Charles E. 'Buddy' III\", 'Pawlenty, Timothy',\n       'Johnson, Gary Earl', 'Paul, Ron', 'Santorum, Rick',\n       'Cain, Herman', 'Gingrich, Newt', 'McCotter, Thaddeus G',\n       'Huntsman, Jon', 'Perry, Rick'], dtype=object)\n464 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 2231,
      "extraction_method": "Direct"
    },
    {
      "page_number": 483,
      "chapter": null,
      "content": "1 This makes the simplifying assumption that Gary Johnson is a Republican even though he later became the\nLibertarian party candidate.\nIn [204]: unique_cands[2]\nOut[204]: 'Obama, Barack'\nOne way to indicate party affiliation is using a dictionary:1\nparties = {\"Bachmann, Michelle\": \"Republican\",\n           \"Cain, Herman\": \"Republican\",\n           \"Gingrich, Newt\": \"Republican\",\n           \"Huntsman, Jon\": \"Republican\",\n           \"Johnson, Gary Earl\": \"Republican\",\n           \"McCotter, Thaddeus G\": \"Republican\",\n           \"Obama, Barack\": \"Democrat\",\n           \"Paul, Ron\": \"Republican\",\n           \"Pawlenty, Timothy\": \"Republican\",\n           \"Perry, Rick\": \"Republican\",\n           \"Roemer, Charles E. 'Buddy' III\": \"Republican\",\n           \"Romney, Mitt\": \"Republican\",\n           \"Santorum, Rick\": \"Republican\"}\nNow, using this mapping and the map method on Series objects, you can compute an\narray of political parties from the candidate names:\nIn [206]: fec[\"cand_nm\"][123456:123461]\nOut[206]: \n123456    Obama, Barack\n123457    Obama, Barack\n123458    Obama, Barack\n123459    Obama, Barack\n123460    Obama, Barack\nName: cand_nm, dtype: object\nIn [207]: fec[\"cand_nm\"][123456:123461].map(parties)\nOut[207]: \n123456    Democrat\n123457    Democrat\n123458    Democrat\n123459    Democrat\n123460    Democrat\nName: cand_nm, dtype: object\n# Add it as a column\nIn [208]: fec[\"party\"] = fec[\"cand_nm\"].map(parties)\nIn [209]: fec[\"party\"].value_counts()\nOut[209]: \nDemocrat      593746\nRepublican    407985\nName: party, dtype: int64\n13.5 2012 Federal Election Commission Database \n| \n465",
      "content_length": 1592,
      "extraction_method": "Direct"
    },
    {
      "page_number": 484,
      "chapter": null,
      "content": "A couple of data preparation points. First, this data includes both contributions and\nrefunds (negative contribution amount):\nIn [210]: (fec[\"contb_receipt_amt\"] > 0).value_counts()\nOut[210]: \nTrue     991475\nFalse     10256\nName: contb_receipt_amt, dtype: int64\nTo simplify the analysis, I’ll restrict the dataset to positive contributions:\nIn [211]: fec = fec[fec[\"contb_receipt_amt\"] > 0]\nSince Barack Obama and Mitt Romney were the main two candidates, I’ll also\nprepare a subset that just has contributions to their campaigns:\nIn [212]: fec_mrbo = fec[fec[\"cand_nm\"].isin([\"Obama, Barack\", \"Romney, Mitt\"])]\nDonation Statistics by Occupation and Employer\nDonations by occupation is another oft-studied statistic. For example, attorneys tend\nto donate more money to Democrats, while business executives tend to donate more\nto Republicans. You have no reason to believe me; you can see for yourself in the\ndata. First, the total number of donations by occupation can be computed with\nvalue_counts:\nIn [213]: fec[\"contbr_occupation\"].value_counts()[:10]\nOut[213]: \nRETIRED                                   233990\nINFORMATION REQUESTED                      35107\nATTORNEY                                   34286\nHOMEMAKER                                  29931\nPHYSICIAN                                  23432\nINFORMATION REQUESTED PER BEST EFFORTS     21138\nENGINEER                                   14334\nTEACHER                                    13990\nCONSULTANT                                 13273\nPROFESSOR                                  12555\nName: contbr_occupation, dtype: int64\nYou will notice by looking at the occupations that many refer to the same basic job\ntype, or there are several variants of the same thing. The following code snippet\nillustrates a technique for cleaning up a few of them by mapping from one occupation\nto another; note the “trick” of using dict.get to allow occupations with no mapping\nto “pass through”:\nocc_mapping = {\n   \"INFORMATION REQUESTED PER BEST EFFORTS\" : \"NOT PROVIDED\",\n   \"INFORMATION REQUESTED\" : \"NOT PROVIDED\",\n   \"INFORMATION REQUESTED (BEST EFFORTS)\" : \"NOT PROVIDED\",\n   \"C.E.O.\": \"CEO\"\n}\n466 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 2195,
      "extraction_method": "Direct"
    },
    {
      "page_number": 485,
      "chapter": null,
      "content": "def get_occ(x):\n    # If no mapping provided, return x\n    return occ_mapping.get(x, x)\nfec[\"contbr_occupation\"] = fec[\"contbr_occupation\"].map(get_occ)\nI’ll also do the same thing for employers:\nemp_mapping = {\n   \"INFORMATION REQUESTED PER BEST EFFORTS\" : \"NOT PROVIDED\",\n   \"INFORMATION REQUESTED\" : \"NOT PROVIDED\",\n   \"SELF\" : \"SELF-EMPLOYED\",\n   \"SELF EMPLOYED\" : \"SELF-EMPLOYED\",\n}\ndef get_emp(x):\n    # If no mapping provided, return x\n    return emp_mapping.get(x, x)\nfec[\"contbr_employer\"] = fec[\"contbr_employer\"].map(f)\nNow, you can use pivot_table to aggregate the data by party and occupation, then\nfilter down to the subset that donated at least $2 million overall:\nIn [216]: by_occupation = fec.pivot_table(\"contb_receipt_amt\",\n   .....:                                 index=\"contbr_occupation\",\n   .....:                                 columns=\"party\", aggfunc=\"sum\")\nIn [217]: over_2mm = by_occupation[by_occupation.sum(axis=\"columns\") > 2000000]\nIn [218]: over_2mm\nOut[218]: \nparty                 Democrat   Republican\ncontbr_occupation                          \nATTORNEY           11141982.97   7477194.43\nCEO                 2074974.79   4211040.52\nCONSULTANT          2459912.71   2544725.45\nENGINEER             951525.55   1818373.70\nEXECUTIVE           1355161.05   4138850.09\nHOMEMAKER           4248875.80  13634275.78\nINVESTOR             884133.00   2431768.92\nLAWYER              3160478.87    391224.32\nMANAGER              762883.22   1444532.37\nNOT PROVIDED        4866973.96  20565473.01\nOWNER               1001567.36   2408286.92\nPHYSICIAN           3735124.94   3594320.24\nPRESIDENT           1878509.95   4720923.76\nPROFESSOR           2165071.08    296702.73\nREAL ESTATE          528902.09   1625902.25\nRETIRED            25305116.38  23561244.49\nSELF-EMPLOYED        672393.40   1640252.54\n13.5 2012 Federal Election Commission Database \n| \n467",
      "content_length": 1886,
      "extraction_method": "Direct"
    },
    {
      "page_number": 486,
      "chapter": null,
      "content": "It can be easier to look at this data graphically as a bar plot (\"barh\" means horizontal\nbar plot; see Figure 13-12):\nIn [220]: over_2mm.plot(kind=\"barh\")\nFigure 13-12. Total donations by party for top occupations\nYou might be interested in the top donor occupations or top companies that donated\nto Obama and Romney. To do this, you can group by candidate name and use a\nvariant of the top method from earlier in the chapter:\ndef get_top_amounts(group, key, n=5):\n    totals = group.groupby(key)[\"contb_receipt_amt\"].sum()\n    return totals.nlargest(n)\nThen aggregate by occupation and employer:\nIn [222]: grouped = fec_mrbo.groupby(\"cand_nm\")\nIn [223]: grouped.apply(get_top_amounts, \"contbr_occupation\", n=7)\nOut[223]: \ncand_nm        contbr_occupation                     \nObama, Barack  RETIRED                                   25305116.38\n               ATTORNEY                                  11141982.97\n               INFORMATION REQUESTED                      4866973.96\n               HOMEMAKER                                  4248875.80\n               PHYSICIAN                                  3735124.94\n               LAWYER                                     3160478.87\n               CONSULTANT                                 2459912.71\nRomney, Mitt   RETIRED                                   11508473.59\n               INFORMATION REQUESTED PER BEST EFFORTS    11396894.84\n               HOMEMAKER                                  8147446.22\n468 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 1509,
      "extraction_method": "Direct"
    },
    {
      "page_number": 487,
      "chapter": null,
      "content": "ATTORNEY                                   5364718.82\n               PRESIDENT                                  2491244.89\n               EXECUTIVE                                  2300947.03\n               C.E.O.                                     1968386.11\nName: contb_receipt_amt, dtype: float64\nIn [224]: grouped.apply(get_top_amounts, \"contbr_employer\", n=10)\nOut[224]: \ncand_nm        contbr_employer                       \nObama, Barack  RETIRED                                   22694358.85\n               SELF-EMPLOYED                             17080985.96\n               NOT EMPLOYED                               8586308.70\n               INFORMATION REQUESTED                      5053480.37\n               HOMEMAKER                                  2605408.54\n               SELF                                       1076531.20\n               SELF EMPLOYED                               469290.00\n               STUDENT                                     318831.45\n               VOLUNTEER                                   257104.00\n               MICROSOFT                                   215585.36\nRomney, Mitt   INFORMATION REQUESTED PER BEST EFFORTS    12059527.24\n               RETIRED                                   11506225.71\n               HOMEMAKER                                  8147196.22\n               SELF-EMPLOYED                              7409860.98\n               STUDENT                                     496490.94\n               CREDIT SUISSE                               281150.00\n               MORGAN STANLEY                              267266.00\n               GOLDMAN SACH & CO.                          238250.00\n               BARCLAYS CAPITAL                            162750.00\n               H.I.G. CAPITAL                              139500.00\nName: contb_receipt_amt, dtype: float64\nBucketing Donation Amounts\nA useful way to analyze this data is to use the cut function to discretize the contribu‐\ntor amounts into buckets by contribution size:\nIn [225]: bins = np.array([0, 1, 10, 100, 1000, 10000,\n   .....:                  100_000, 1_000_000, 10_000_000])\nIn [226]: labels = pd.cut(fec_mrbo[\"contb_receipt_amt\"], bins)\nIn [227]: labels\nOut[227]: \n411         (10, 100]\n412       (100, 1000]\n413       (100, 1000]\n414         (10, 100]\n415         (10, 100]\n             ...     \n701381      (10, 100]\n701382    (100, 1000]\n701383        (1, 10]\n13.5 2012 Federal Election Commission Database \n| \n469",
      "content_length": 2473,
      "extraction_method": "Direct"
    },
    {
      "page_number": 488,
      "chapter": null,
      "content": "701384      (10, 100]\n701385    (100, 1000]\nName: contb_receipt_amt, Length: 694282, dtype: category\nCategories (8, interval[int64, right]): [(0, 1] < (1, 10] < (10, 100] < (100, 100\n0] <\n                                         (1000, 10000] < (10000, 100000] < (10000\n0, 1000000] <\n                                         (1000000, 10000000]]\nWe can then group the data for Obama and Romney by name and bin label to get a\nhistogram by donation size:\nIn [228]: grouped = fec_mrbo.groupby([\"cand_nm\", labels])\nIn [229]: grouped.size().unstack(level=0)\nOut[229]: \ncand_nm              Obama, Barack  Romney, Mitt\ncontb_receipt_amt                               \n(0, 1]                         493            77\n(1, 10]                      40070          3681\n(10, 100]                   372280         31853\n(100, 1000]                 153991         43357\n(1000, 10000]                22284         26186\n(10000, 100000]                  2             1\n(100000, 1000000]                3             0\n(1000000, 10000000]              4             0\nThis data shows that Obama received a significantly larger number of small donations\nthan Romney. You can also sum the contribution amounts and normalize within\nbuckets to visualize the percentage of total donations of each size by candidate\n(Figure 13-13 shows the resulting plot):\nIn [231]: bucket_sums = grouped[\"contb_receipt_amt\"].sum().unstack(level=0)\nIn [232]: normed_sums = bucket_sums.div(bucket_sums.sum(axis=\"columns\"),\n   .....:                               axis=\"index\")\nIn [233]: normed_sums\nOut[233]: \ncand_nm              Obama, Barack  Romney, Mitt\ncontb_receipt_amt                               \n(0, 1]                    0.805182      0.194818\n(1, 10]                   0.918767      0.081233\n(10, 100]                 0.910769      0.089231\n(100, 1000]               0.710176      0.289824\n(1000, 10000]             0.447326      0.552674\n(10000, 100000]           0.823120      0.176880\n(100000, 1000000]         1.000000      0.000000\n(1000000, 10000000]       1.000000      0.000000\nIn [234]: normed_sums[:-2].plot(kind=\"barh\")\n470 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 2150,
      "extraction_method": "Direct"
    },
    {
      "page_number": 489,
      "chapter": null,
      "content": "Figure 13-13. Percentage of total donations received by candidates for each donation size\nI excluded the two largest bins, as these are not donations by individuals.\nThis analysis can be refined and improved in many ways. For example, you could\naggregate donations by donor name and zip code to adjust for donors who gave many\nsmall amounts versus one or more large donations. I encourage you to explore the\ndataset yourself.\nDonation Statistics by State\nWe can start by aggregating the data by candidate and state:\nIn [235]: grouped = fec_mrbo.groupby([\"cand_nm\", \"contbr_st\"])\nIn [236]: totals = grouped[\"contb_receipt_amt\"].sum().unstack(level=0).fillna(0)\nIn [237]: totals = totals[totals.sum(axis=\"columns\") > 100000]\nIn [238]: totals.head(10)\nOut[238]: \ncand_nm    Obama, Barack  Romney, Mitt\ncontbr_st                             \nAK             281840.15      86204.24\nAL             543123.48     527303.51\nAR             359247.28     105556.00\nAZ            1506476.98    1888436.23\nCA           23824984.24   11237636.60\nCO            2132429.49    1506714.12\nCT            2068291.26    3499475.45\nDC            4373538.80    1025137.50\n13.5 2012 Federal Election Commission Database \n| \n471",
      "content_length": 1204,
      "extraction_method": "Direct"
    },
    {
      "page_number": 490,
      "chapter": null,
      "content": "DE             336669.14      82712.00\nFL            7318178.58    8338458.81\nIf you divide each row by the total contribution amount, you get the relative percent‐\nage of total donations by state for each candidate:\nIn [239]: percent = totals.div(totals.sum(axis=\"columns\"), axis=\"index\")\nIn [240]: percent.head(10)\nOut[240]: \ncand_nm    Obama, Barack  Romney, Mitt\ncontbr_st                             \nAK              0.765778      0.234222\nAL              0.507390      0.492610\nAR              0.772902      0.227098\nAZ              0.443745      0.556255\nCA              0.679498      0.320502\nCO              0.585970      0.414030\nCT              0.371476      0.628524\nDC              0.810113      0.189887\nDE              0.802776      0.197224\nFL              0.467417      0.532583\n13.6 Conclusion\nWe’ve reached the end of this book. I have included some additional content you may\nfind useful in the appendixes.\nIn the 10 years since the first edition of this book was published, Python has become\na popular and widespread language for data analysis. The programming skills you\nhave developed here will stay relevant for a long time into the future. I hope the\nprogramming tools and libraries we’ve explored will serve you well.\n472 \n| \nChapter 13: Data Analysis Examples",
      "content_length": 1286,
      "extraction_method": "Direct"
    },
    {
      "page_number": 491,
      "chapter": null,
      "content": "APPENDIX A\nAdvanced NumPy\nIn this appendix, I will go deeper into the NumPy library for array computing. This\nwill include more internal details about the ndarray type and more advanced array\nmanipulations and algorithms.\nThis appendix contains miscellaneous topics and does not necessarily need to be read\nlinearly. Throughout the chapters, I will generate random data for many examples\nthat will use the default random number generator in the numpy.random module:\nIn [11]: rng = np.random.default_rng(seed=12345)\nA.1 ndarray Object Internals\nThe NumPy ndarray provides a way to interpret a block of homogeneously typed data\n(either contiguous or strided) as a multidimensional array object. The data type, or\ndtype, determines how the data is interpreted as being floating point, integer, Boolean,\nor any of the other types we’ve been looking at.\nPart of what makes ndarray flexible is that every array object is a strided view on a\nblock of data. You might wonder, for example, how the array view arr[::2, ::-1]\ndoes not copy any data. The reason is that the ndarray is more than just a chunk\nof memory and a data type; it also has striding information that enables the array to\nmove through memory with varying step sizes. More precisely, the ndarray internally\nconsists of the following:\n• A pointer to data—that is, a block of data in RAM or in a memory-mapped file\n•\n• The data type or dtype describing fixed-size value cells in the array\n•\n• A tuple indicating the array’s shape\n•\n473",
      "content_length": 1492,
      "extraction_method": "Direct"
    },
    {
      "page_number": 492,
      "chapter": null,
      "content": "• A tuple of strides—integers indicating the number of bytes to “step” in order to\n•\nadvance one element along a dimension\nSee Figure A-1 for a simple mock-up of the ndarray innards.\nFigure A-1. The NumPy ndarray object\nFor example, a 10 × 5 array would have the shape (10, 5):\nIn [12]: np.ones((10, 5)).shape\nOut[12]: (10, 5)\nA typical (C order) 3 × 4 × 5 array of float64 (8-byte) values has the strides (160,\n40, 8) (knowing about the strides can be useful because, in general, the larger the\nstrides on a particular axis, the more costly it is to perform computation along that\naxis):\nIn [13]: np.ones((3, 4, 5), dtype=np.float64).strides\nOut[13]: (160, 40, 8)\nWhile it is rare that a typical NumPy user would be interested in the array strides,\nthey are needed to construct “zero-copy” array views. Strides can even be negative,\nwhich enables an array to move “backward” through memory (this would be the case,\nfor example, in a slice like obj[::-1] or obj[:, ::-1]).\nNumPy Data Type Hierarchy\nYou may occasionally have code that needs to check whether an array contains inte‐\ngers, floating-point numbers, strings, or Python objects. Because there are multiple\ntypes of floating-point numbers (float16 through float128), checking that the data\ntype is among a list of types would be very verbose. Fortunately, the data types\nhave superclasses, such as np.integer and np.floating, which can be used with the\nnp.issubdtype function:\nIn [14]: ints = np.ones(10, dtype=np.uint16)\nIn [15]: floats = np.ones(10, dtype=np.float32)\nIn [16]: np.issubdtype(ints.dtype, np.integer)\nOut[16]: True\n474 \n| \nAppendix A: Advanced NumPy",
      "content_length": 1625,
      "extraction_method": "Direct"
    },
    {
      "page_number": 493,
      "chapter": null,
      "content": "1 Some of the data types have trailing underscores in their names. These are there to avoid variable name\nconflicts between the NumPy-specific types and the Python built-in ones.\nIn [17]: np.issubdtype(floats.dtype, np.floating)\nOut[17]: True\nYou can see all of the parent classes of a specific data type by calling the type’s mro\nmethod:\nIn [18]: np.float64.mro()\nOut[18]: \n[numpy.float64,\n numpy.floating,\n numpy.inexact,\n numpy.number,\n numpy.generic,\n float,\n object]\nTherefore, we also have:\nIn [19]: np.issubdtype(ints.dtype, np.number)\nOut[19]: True\nMost NumPy users will never have to know about this, but it is occasionally use‐\nful. See Figure A-2 for a graph of the data type hierarchy and parent–subclass\nrelationships.1\nFigure A-2. The NumPy data type class hierarchy\nAdvanced NumPy \n| \n475",
      "content_length": 803,
      "extraction_method": "Direct"
    },
    {
      "page_number": 494,
      "chapter": null,
      "content": "A.2 Advanced Array Manipulation\nThere are many ways to work with arrays beyond fancy indexing, slicing, and\nBoolean subsetting. While much of the heavy lifting for data analysis applications\nis handled by higher-level functions in pandas, you may at some point need to write a\ndata algorithm that is not found in one of the existing libraries.\nReshaping Arrays\nIn many cases, you can convert an array from one shape to another without copying\nany data. To do this, pass a tuple indicating the new shape to the reshape array\ninstance method. For example, suppose we had a one-dimensional array of values\nthat we wished to rearrange into a matrix (this is illustrated in Figure A-3):\nIn [20]: arr = np.arange(8)\nIn [21]: arr\nOut[21]: array([0, 1, 2, 3, 4, 5, 6, 7])\nIn [22]: arr.reshape((4, 2))\nOut[22]: \narray([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7]])\nFigure A-3. Reshaping in C (row major) or FORTRAN (column major) order\n476 \n| \nAppendix A: Advanced NumPy",
      "content_length": 969,
      "extraction_method": "Direct"
    },
    {
      "page_number": 495,
      "chapter": null,
      "content": "A multidimensional array can also be reshaped:\nIn [23]: arr.reshape((4, 2)).reshape((2, 4))\nOut[23]: \narray([[0, 1, 2, 3],\n       [4, 5, 6, 7]])\nOne of the passed shape dimensions can be –1, in which case the value used for that\ndimension will be inferred from the data:\nIn [24]: arr = np.arange(15)\nIn [25]: arr.reshape((5, -1))\nOut[25]: \narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11],\n       [12, 13, 14]])\nSince an array’s shape attribute is a tuple, it can be passed to reshape, too:\nIn [26]: other_arr = np.ones((3, 5))\nIn [27]: other_arr.shape\nOut[27]: (3, 5)\nIn [28]: arr.reshape(other_arr.shape)\nOut[28]: \narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])\nThe opposite operation of reshape from one-dimensional to a higher dimension is\ntypically known as flattening or raveling:\nIn [29]: arr = np.arange(15).reshape((5, 3))\nIn [30]: arr\nOut[30]: \narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11],\n       [12, 13, 14]])\nIn [31]: arr.ravel()\nOut[31]: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\nravel does not produce a copy of the underlying values if the values in the result\nwere contiguous in the original array.\nAdvanced NumPy \n| \n477",
      "content_length": 1278,
      "extraction_method": "Direct"
    },
    {
      "page_number": 496,
      "chapter": null,
      "content": "The flatten method behaves like ravel except it always returns a copy of the data:\nIn [32]: arr.flatten()\nOut[32]: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\nThe data can be reshaped or raveled in different orders. This is a slightly nuanced\ntopic for new NumPy users and is therefore the next subtopic.\nC Versus FORTRAN Order\nNumPy is able to adapt to many different layouts of your data in memory. By default,\nNumPy arrays are created in row major order. Spatially this means that if you have\na two-dimensional array of data, the items in each row of the array are stored in\nadjacent memory locations. The alternative to row major ordering is column major\norder, which means that values within each column of data are stored in adjacent\nmemory locations.\nFor historical reasons, row and column major order are also known as C and FOR‐\nTRAN order, respectively. In the FORTRAN 77 language, matrices are all column\nmajor.\nFunctions like reshape and ravel accept an order argument indicating the order to\nuse the data in the array. This is usually set to 'C' or 'F' in most cases (there are also\nless commonly used options 'A' and 'K'; see the NumPy documentation, and refer\nback to Figure A-3 for an illustration of these options):\nIn [33]: arr = np.arange(12).reshape((3, 4))\nIn [34]: arr\nOut[34]: \narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\nIn [35]: arr.ravel()\nOut[35]: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\nIn [36]: arr.ravel('F')\nOut[36]: array([ 0,  4,  8,  1,  5,  9,  2,  6, 10,  3,  7, 11])\nReshaping arrays with more than two dimensions can be a bit mind-bending (see\nFigure A-3). The key difference between C and FORTRAN order is the way in which\nthe dimensions are walked:\nC/row major order\nTraverse higher dimensions first (e.g., axis 1 before advancing on axis 0).\nFORTRAN/column major order\nTraverse higher dimensions last (e.g., axis 0 before advancing on axis 1).\n478 \n| \nAppendix A: Advanced NumPy",
      "content_length": 1991,
      "extraction_method": "Direct"
    },
    {
      "page_number": 497,
      "chapter": null,
      "content": "Concatenating and Splitting Arrays\nnumpy.concatenate takes a sequence (tuple, list, etc.) of arrays and joins them in\norder along the input axis:\nIn [37]: arr1 = np.array([[1, 2, 3], [4, 5, 6]])\nIn [38]: arr2 = np.array([[7, 8, 9], [10, 11, 12]])\nIn [39]: np.concatenate([arr1, arr2], axis=0)\nOut[39]: \narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]])\nIn [40]: np.concatenate([arr1, arr2], axis=1)\nOut[40]: \narray([[ 1,  2,  3,  7,  8,  9],\n       [ 4,  5,  6, 10, 11, 12]])\nThere are some convenience functions, like vstack and hstack, for common kinds of\nconcatenation. The preceding operations could have been expressed as:\nIn [41]: np.vstack((arr1, arr2))\nOut[41]: \narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]])\nIn [42]: np.hstack((arr1, arr2))\nOut[42]: \narray([[ 1,  2,  3,  7,  8,  9],\n       [ 4,  5,  6, 10, 11, 12]])\nsplit, on the other hand, slices an array into multiple arrays along an axis:\nIn [43]: arr = rng.standard_normal((5, 2))\nIn [44]: arr\nOut[44]: \narray([[-1.4238,  1.2637],\n       [-0.8707, -0.2592],\n       [-0.0753, -0.7409],\n       [-1.3678,  0.6489],\n       [ 0.3611, -1.9529]])\nIn [45]: first, second, third = np.split(arr, [1, 3])\nIn [46]: first\nOut[46]: array([[-1.4238,  1.2637]])\nAdvanced NumPy \n| \n479",
      "content_length": 1309,
      "extraction_method": "Direct"
    },
    {
      "page_number": 498,
      "chapter": null,
      "content": "In [47]: second\nOut[47]: \narray([[-0.8707, -0.2592],\n       [-0.0753, -0.7409]])\nIn [48]: third\nOut[48]: \narray([[-1.3678,  0.6489],\n       [ 0.3611, -1.9529]])\nThe value [1, 3] passed to np.split indicates the indices at which to split the array\ninto pieces.\nSee Table A-1 for a list of all relevant concatenation and splitting functions, some of\nwhich are provided only as a convenience of the very general-purpose concatenate.\nTable A-1. Array concatenation functions\nFunction\nDescription\nconcatenate\nMost general function, concatenate collection of arrays along one axis\nvstack, row_stack\nStack arrays by rows (along axis 0)\nhstack\nStack arrays by columns (along axis 1)\ncolumn_stack\nLike hstack, but convert 1D arrays to 2D column vectors first\ndstack\nStack arrays by “depth” (along axis 2)\nsplit\nSplit array at passed locations along a particular axis\nhsplit/vsplit\nConvenience functions for splitting on axis 0 and 1, respectively\nStacking helpers: r_ and c_\nThere are two special objects in the NumPy namespace, r_ and c_, that make stacking\narrays more concise:\nIn [49]: arr = np.arange(6)\nIn [50]: arr1 = arr.reshape((3, 2))\nIn [51]: arr2 = rng.standard_normal((3, 2))\nIn [52]: np.r_[arr1, arr2]\nOut[52]: \narray([[ 0.    ,  1.    ],\n       [ 2.    ,  3.    ],\n       [ 4.    ,  5.    ],\n       [ 2.3474,  0.9685],\n       [-0.7594,  0.9022],\n       [-0.467 , -0.0607]])\nIn [53]: np.c_[np.r_[arr1, arr2], arr]\nOut[53]: \narray([[ 0.    ,  1.    ,  0.    ],\n480 \n| \nAppendix A: Advanced NumPy",
      "content_length": 1498,
      "extraction_method": "Direct"
    },
    {
      "page_number": 499,
      "chapter": null,
      "content": "[ 2.    ,  3.    ,  1.    ],\n       [ 4.    ,  5.    ,  2.    ],\n       [ 2.3474,  0.9685,  3.    ],\n       [-0.7594,  0.9022,  4.    ],\n       [-0.467 , -0.0607,  5.    ]])\nThese additionally can translate slices to arrays:\nIn [54]: np.c_[1:6, -10:-5]\nOut[54]: \narray([[  1, -10],\n       [  2,  -9],\n       [  3,  -8],\n       [  4,  -7],\n       [  5,  -6]])\nSee the docstring for more on what you can do with c_ and r_.\nRepeating Elements: tile and repeat\nTwo useful tools for repeating or replicating arrays to produce larger arrays are the\nrepeat and tile functions. repeat replicates each element in an array some number\nof times, producing a larger array:\nIn [55]: arr = np.arange(3)\nIn [56]: arr\nOut[56]: array([0, 1, 2])\nIn [57]: arr.repeat(3)\nOut[57]: array([0, 0, 0, 1, 1, 1, 2, 2, 2])\nThe need to replicate or repeat arrays can be less common with\nNumPy than it is with other array programming frameworks like\nMATLAB. One reason for this is that broadcasting often fills this\nneed better, which is the subject of the next section.\nBy default, if you pass an integer, each element will be repeated that number of times.\nIf you pass an array of integers, each element can be repeated a different number of\ntimes:\nIn [58]: arr.repeat([2, 3, 4])\nOut[58]: array([0, 0, 1, 1, 1, 2, 2, 2, 2])\nMultidimensional arrays can have their elements repeated along a particular axis:\nIn [59]: arr = rng.standard_normal((2, 2))\nIn [60]: arr\nOut[60]: \narray([[ 0.7888, -1.2567],\nAdvanced NumPy \n| \n481",
      "content_length": 1493,
      "extraction_method": "Direct"
    },
    {
      "page_number": 500,
      "chapter": null,
      "content": "[ 0.5759,  1.399 ]])\nIn [61]: arr.repeat(2, axis=0)\nOut[61]: \narray([[ 0.7888, -1.2567],\n       [ 0.7888, -1.2567],\n       [ 0.5759,  1.399 ],\n       [ 0.5759,  1.399 ]])\nNote that if no axis is passed, the array will be flattened first, which is likely not what\nyou want. Similarly, you can pass an array of integers when repeating a multidimen‐\nsional array to repeat a given slice a different number of times:\nIn [62]: arr.repeat([2, 3], axis=0)\nOut[62]: \narray([[ 0.7888, -1.2567],\n       [ 0.7888, -1.2567],\n       [ 0.5759,  1.399 ],\n       [ 0.5759,  1.399 ],\n       [ 0.5759,  1.399 ]])\nIn [63]: arr.repeat([2, 3], axis=1)\nOut[63]: \narray([[ 0.7888,  0.7888, -1.2567, -1.2567, -1.2567],\n       [ 0.5759,  0.5759,  1.399 ,  1.399 ,  1.399 ]])\ntile, on the other hand, is a shortcut for stacking copies of an array along an axis.\nVisually you can think of it as being akin to “laying down tiles”:\nIn [64]: arr\nOut[64]: \narray([[ 0.7888, -1.2567],\n       [ 0.5759,  1.399 ]])\nIn [65]: np.tile(arr, 2)\nOut[65]: \narray([[ 0.7888, -1.2567,  0.7888, -1.2567],\n       [ 0.5759,  1.399 ,  0.5759,  1.399 ]])\nThe second argument is the number of tiles; with a scalar, the tiling is made row by\nrow, rather than column by column. The second argument to tile can be a tuple\nindicating the layout of the “tiling”:\nIn [66]: arr\nOut[66]: \narray([[ 0.7888, -1.2567],\n       [ 0.5759,  1.399 ]])\nIn [67]: np.tile(arr, (2, 1))\nOut[67]: \narray([[ 0.7888, -1.2567],\n       [ 0.5759,  1.399 ],\n       [ 0.7888, -1.2567],\n482 \n| \nAppendix A: Advanced NumPy",
      "content_length": 1542,
      "extraction_method": "Direct"
    },
    {
      "page_number": 501,
      "chapter": null,
      "content": "[ 0.5759,  1.399 ]])\nIn [68]: np.tile(arr, (3, 2))\nOut[68]: \narray([[ 0.7888, -1.2567,  0.7888, -1.2567],\n       [ 0.5759,  1.399 ,  0.5759,  1.399 ],\n       [ 0.7888, -1.2567,  0.7888, -1.2567],\n       [ 0.5759,  1.399 ,  0.5759,  1.399 ],\n       [ 0.7888, -1.2567,  0.7888, -1.2567],\n       [ 0.5759,  1.399 ,  0.5759,  1.399 ]])\nFancy Indexing Equivalents: take and put\nAs you may recall from Chapter 4, one way to get and set subsets of arrays is by fancy\nindexing using integer arrays:\nIn [69]: arr = np.arange(10) * 100\nIn [70]: inds = [7, 1, 2, 6]\nIn [71]: arr[inds]\nOut[71]: array([700, 100, 200, 600])\nThere are alternative ndarray methods that are useful in the special case of making a\nselection only on a single axis:\nIn [72]: arr.take(inds)\nOut[72]: array([700, 100, 200, 600])\nIn [73]: arr.put(inds, 42)\nIn [74]: arr\nOut[74]: array([  0,  42,  42, 300, 400, 500,  42,  42, 800, 900])\nIn [75]: arr.put(inds, [40, 41, 42, 43])\nIn [76]: arr\nOut[76]: array([  0,  41,  42, 300, 400, 500,  43,  40, 800, 900])\nTo use take along other axes, you can pass the axis keyword:\nIn [77]: inds = [2, 0, 2, 1]\nIn [78]: arr = rng.standard_normal((2, 4))\nIn [79]: arr\nOut[79]: \narray([[ 1.3223, -0.2997,  0.9029, -1.6216],\n       [-0.1582,  0.4495, -1.3436, -0.0817]])\nIn [80]: arr.take(inds, axis=1)\nOut[80]: \nAdvanced NumPy \n| \n483",
      "content_length": 1330,
      "extraction_method": "Direct"
    },
    {
      "page_number": 502,
      "chapter": null,
      "content": "array([[ 0.9029,  1.3223,  0.9029, -0.2997],\n       [-1.3436, -0.1582, -1.3436,  0.4495]])\nput does not accept an axis argument but rather indexes into the flattened (one-\ndimensional, C order) version of the array. Thus, when you need to set elements\nusing an index array on other axes, it is best to use []-based indexing.\nA.3 Broadcasting\nBroadcasting governs how operations work between arrays of different shapes. It can\nbe a powerful feature, but it can cause confusion, even for experienced users. The\nsimplest example of broadcasting occurs when combining a scalar value with an\narray:\nIn [81]: arr = np.arange(5)\nIn [82]: arr\nOut[82]: array([0, 1, 2, 3, 4])\nIn [83]: arr * 4\nOut[83]: array([ 0,  4,  8, 12, 16])\nHere we say that the scalar value 4 has been broadcast to all of the other elements in\nthe multiplication operation.\nFor example, we can demean each column of an array by subtracting the column\nmeans. In this case, it is necessary only to subtract an array containing the mean of\neach column:\nIn [84]: arr = rng.standard_normal((4, 3))\nIn [85]: arr.mean(0)\nOut[85]: array([0.1206, 0.243 , 0.1444])\nIn [86]: demeaned = arr - arr.mean(0)\nIn [87]: demeaned\nOut[87]: \narray([[ 1.6042,  2.3751,  0.633 ],\n       [ 0.7081, -1.202 , -1.3538],\n       [-1.5329,  0.2985,  0.6076],\n       [-0.7793, -1.4717,  0.1132]])\nIn [88]: demeaned.mean(0)\nOut[88]: array([ 0., -0.,  0.])\n484 \n| \nAppendix A: Advanced NumPy",
      "content_length": 1422,
      "extraction_method": "Direct"
    },
    {
      "page_number": 503,
      "chapter": null,
      "content": "See Figure A-4 for an illustration of this operation. Demeaning the rows as a broad‐\ncast operation requires a bit more care. Fortunately, broadcasting potentially lower\ndimensional values across any dimension of an array (like subtracting the row means\nfrom each column of a two-dimensional array) is possible as long as you follow the\nrules.\nThis brings us to the broadcasting rule.\nThe Broadcasting Rule\nTwo arrays are compatible for broadcasting if for each trailing dimension (i.e., starting\nfrom the end) the axis lengths match or if either of the lengths is 1. Broadcasting is\nthen performed over the missing or length 1 dimensions.\nFigure A-4. Broadcasting over axis 0 with a 1D array\nEven as an experienced NumPy user, I often find myself having to pause and draw\na diagram as I think about the broadcasting rule. Consider the last example and sup‐\npose we wished instead to subtract the mean value from each row. Since arr.mean(0)\nhas length 3, it is compatible for broadcasting across axis 0 because the trailing\ndimension in arr is 3 and therefore matches. According to the rules, to subtract over\naxis 1 (i.e., subtract the row mean from each row), the smaller array must have the\nshape (4, 1):\nIn [89]: arr\nOut[89]: \narray([[ 1.7247,  2.6182,  0.7774],\n       [ 0.8286, -0.959 , -1.2094],\n       [-1.4123,  0.5415,  0.7519],\n       [-0.6588, -1.2287,  0.2576]])\nIn [90]: row_means = arr.mean(1)\nAdvanced NumPy \n| \n485",
      "content_length": 1431,
      "extraction_method": "Direct"
    },
    {
      "page_number": 504,
      "chapter": null,
      "content": "In [91]: row_means.shape\nOut[91]: (4,)\nIn [92]: row_means.reshape((4, 1))\nOut[92]: \narray([[ 1.7068],\n       [-0.4466],\n       [-0.0396],\n       [-0.5433]])\nIn [93]: demeaned = arr - row_means.reshape((4, 1))\nIn [94]: demeaned.mean(1)\nOut[94]: array([-0.,  0.,  0.,  0.])\nSee Figure A-5 for an illustration of this operation.\nFigure A-5. Broadcasting over axis 1 of a 2D array\nSee Figure A-6 for another illustration, this time adding a two-dimensional array to a\nthree-dimensional one across axis 0.\n486 \n| \nAppendix A: Advanced NumPy",
      "content_length": 535,
      "extraction_method": "Direct"
    },
    {
      "page_number": 505,
      "chapter": null,
      "content": "Figure A-6. Broadcasting over axis 0 of a 3D array\nBroadcasting over Other Axes\nBroadcasting with higher dimensional arrays can seem even more mind-bending, but\nit is really a matter of following the rules. If you don’t, you’ll get an error like this:\nIn [95]: arr - arr.mean(1)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-95-8b8ada26fac0> in <module>\n----> 1 arr - arr.mean(1)\nValueError: operands could not be broadcast together with shapes (4,3) (4,) \nIt’s quite common to want to perform an arithmetic operation with a lower dimen‐\nsional array across axes other than axis 0. According to the broadcasting rule, the\n“broadcast dimensions” must be 1 in the smaller array. In the example of row\ndemeaning shown here, this means reshaping the row to be shape (4, 1) instead\nof (4,):\nIn [96]: arr - arr.mean(1).reshape((4, 1))\nOut[96]: \narray([[ 0.018 ,  0.9114, -0.9294],\n       [ 1.2752, -0.5124, -0.7628],\n       [-1.3727,  0.5811,  0.7915],\n       [-0.1155, -0.6854,  0.8009]])\nIn the three-dimensional case, broadcasting over any of the three dimensions is only\na matter of reshaping the data to be shape compatible. Figure A-7 nicely visualizes the\nshapes required to broadcast over each axis of a three-dimensional array.\nAdvanced NumPy \n| \n487",
      "content_length": 1374,
      "extraction_method": "Direct"
    },
    {
      "page_number": 506,
      "chapter": null,
      "content": "Figure A-7. Compatible 2D array shapes for broadcasting over a 3D array\nA common problem, therefore, is needing to add a new axis with length 1 specifi‐\ncally for broadcasting purposes. Using reshape is one option, but inserting an axis\nrequires constructing a tuple indicating the new shape. This often can be a tedious\nexercise. Thus, NumPy arrays offer a special syntax for inserting new axes by index‐\ning. We use the special np.newaxis attribute along with “full” slices to insert the new\naxis:\nIn [97]: arr = np.zeros((4, 4))\nIn [98]: arr_3d = arr[:, np.newaxis, :]\nIn [99]: arr_3d.shape\nOut[99]: (4, 1, 4)\nIn [100]: arr_1d = rng.standard_normal(3)\nIn [101]: arr_1d[:, np.newaxis]\nOut[101]: \narray([[ 0.3129],\n       [-0.1308],\n       [ 1.27  ]])\nIn [102]: arr_1d[np.newaxis, :]\nOut[102]: array([[ 0.3129, -0.1308,  1.27  ]])\nThus, if we had a three-dimensional array and wanted to demean axis 2, we would\nneed to write:\n488 \n| \nAppendix A: Advanced NumPy",
      "content_length": 961,
      "extraction_method": "Direct"
    },
    {
      "page_number": 507,
      "chapter": null,
      "content": "In [103]: arr = rng.standard_normal((3, 4, 5))\nIn [104]: depth_means = arr.mean(2)\nIn [105]: depth_means\nOut[105]: \narray([[ 0.0431,  0.2747, -0.1885, -0.2014],\n       [-0.5732, -0.5467,  0.1183, -0.6301],\n       [ 0.0972,  0.5954,  0.0331, -0.6002]])\nIn [106]: depth_means.shape\nOut[106]: (3, 4)\nIn [107]: demeaned = arr - depth_means[:, :, np.newaxis]\nIn [108]: demeaned.mean(2)\nOut[108]: \narray([[ 0., -0.,  0., -0.],\n       [ 0., -0., -0., -0.],\n       [ 0.,  0.,  0.,  0.]])\nYou might be wondering if there’s a way to generalize demeaning over an axis without\nsacrificing performance. There is, but it requires some indexing gymnastics:\ndef demean_axis(arr, axis=0):\n    means = arr.mean(axis)\n    # This generalizes things like [:, :, np.newaxis] to N dimensions\n    indexer = [slice(None)] * arr.ndim\n    indexer[axis] = np.newaxis\n    return arr - means[indexer]\nSetting Array Values by Broadcasting\nThe same broadcasting rule governing arithmetic operations also applies to setting\nvalues via array indexing. In a simple case, we can do things like:\nIn [109]: arr = np.zeros((4, 3))\nIn [110]: arr[:] = 5\nIn [111]: arr\nOut[111]: \narray([[5., 5., 5.],\n       [5., 5., 5.],\n       [5., 5., 5.],\n       [5., 5., 5.]])\nHowever, if we had a one-dimensional array of values we wanted to set into the\ncolumns of the array, we can do that as long as the shape is compatible:\nIn [112]: col = np.array([1.28, -0.42, 0.44, 1.6])\nAdvanced NumPy \n| \n489",
      "content_length": 1448,
      "extraction_method": "Direct"
    },
    {
      "page_number": 508,
      "chapter": null,
      "content": "In [113]: arr[:] = col[:, np.newaxis]\nIn [114]: arr\nOut[114]: \narray([[ 1.28,  1.28,  1.28],\n       [-0.42, -0.42, -0.42],\n       [ 0.44,  0.44,  0.44],\n       [ 1.6 ,  1.6 ,  1.6 ]])\nIn [115]: arr[:2] = [[-1.37], [0.509]]\nIn [116]: arr\nOut[116]: \narray([[-1.37 , -1.37 , -1.37 ],\n       [ 0.509,  0.509,  0.509],\n       [ 0.44 ,  0.44 ,  0.44 ],\n       [ 1.6  ,  1.6  ,  1.6  ]])\nA.4 Advanced ufunc Usage\nWhile many NumPy users will only use the fast element-wise operations provided\nby the universal functions, a number of additional features occasionally can help you\nwrite more concise code without explicit loops.\nufunc Instance Methods\nEach of NumPy’s binary ufuncs has special methods for performing certain kinds of\nspecial vectorized operations. These are summarized in Table A-2, but I’ll give a few\nconcrete examples to illustrate how they work.\nreduce takes a single array and aggregates its values, optionally along an axis, by\nperforming a sequence of binary operations. For example, an alternative way to sum\nelements in an array is to use np.add.reduce:\nIn [117]: arr = np.arange(10)\nIn [118]: np.add.reduce(arr)\nOut[118]: 45\nIn [119]: arr.sum()\nOut[119]: 45\nThe starting value (for example, 0 for add) depends on the ufunc. If an axis is passed,\nthe reduction is performed along that axis. This allows you to answer certain kinds of\nquestions in a concise way. As a less mundane example, we can use np.logical_and\nto check whether the values in each row of an array are sorted:\nIn [120]: my_rng = np.random.default_rng(12346)  # for reproducibility\nIn [121]: arr = my_rng.standard_normal((5, 5))\n490 \n| \nAppendix A: Advanced NumPy",
      "content_length": 1647,
      "extraction_method": "Direct"
    },
    {
      "page_number": 509,
      "chapter": null,
      "content": "In [122]: arr\nOut[122]: \narray([[-0.9039,  0.1571,  0.8976, -0.7622, -0.1763],\n       [ 0.053 , -1.6284, -0.1775,  1.9636,  1.7813],\n       [-0.8797, -1.6985, -1.8189,  0.119 , -0.4441],\n       [ 0.7691, -0.0343,  0.3925,  0.7589, -0.0705],\n       [ 1.0498,  1.0297, -0.4201,  0.7863,  0.9612]])\nIn [123]: arr[::2].sort(1) # sort a few rows\nIn [124]: arr[:, :-1] < arr[:, 1:]\nOut[124]: \narray([[ True,  True,  True,  True],\n       [False,  True,  True, False],\n       [ True,  True,  True,  True],\n       [False,  True,  True, False],\n       [ True,  True,  True,  True]])\nIn [125]: np.logical_and.reduce(arr[:, :-1] < arr[:, 1:], axis=1)\nOut[125]: array([ True, False,  True, False,  True])\nNote that logical_and.reduce is equivalent to the all method.\nThe accumulate ufunc method is related to reduce, like cumsum is related to sum. It\nproduces an array of the same size with the intermediate “accumulated” values:\nIn [126]: arr = np.arange(15).reshape((3, 5))\nIn [127]: np.add.accumulate(arr, axis=1)\nOut[127]: \narray([[ 0,  1,  3,  6, 10],\n       [ 5, 11, 18, 26, 35],\n       [10, 21, 33, 46, 60]])\nouter performs a pair-wise cross product between two arrays:\nIn [128]: arr = np.arange(3).repeat([1, 2, 2])\nIn [129]: arr\nOut[129]: array([0, 1, 1, 2, 2])\nIn [130]: np.multiply.outer(arr, np.arange(5))\nOut[130]: \narray([[0, 0, 0, 0, 0],\n       [0, 1, 2, 3, 4],\n       [0, 1, 2, 3, 4],\n       [0, 2, 4, 6, 8],\n       [0, 2, 4, 6, 8]])\nAdvanced NumPy \n| \n491",
      "content_length": 1459,
      "extraction_method": "Direct"
    },
    {
      "page_number": 510,
      "chapter": null,
      "content": "The output of outer will have a dimension that is the concatenation of the dimen‐\nsions of the inputs:\nIn [131]: x, y = rng.standard_normal((3, 4)), rng.standard_normal(5)\nIn [132]: result = np.subtract.outer(x, y)\nIn [133]: result.shape\nOut[133]: (3, 4, 5)\nThe last method, reduceat, performs a “local reduce,” in essence an array “group by”\noperation in which slices of the array are aggregated together. It accepts a sequence of\n“bin edges” that indicate how to split and aggregate the values:\nIn [134]: arr = np.arange(10)\nIn [135]: np.add.reduceat(arr, [0, 5, 8])\nOut[135]: array([10, 18, 17])\nThe results are the reductions (here, sums) performed over arr[0:5], arr[5:8], and\narr[8:]. As with the other methods, you can pass an axis argument:\nIn [136]: arr = np.multiply.outer(np.arange(4), np.arange(5))\nIn [137]: arr\nOut[137]: \narray([[ 0,  0,  0,  0,  0],\n       [ 0,  1,  2,  3,  4],\n       [ 0,  2,  4,  6,  8],\n       [ 0,  3,  6,  9, 12]])\nIn [138]: np.add.reduceat(arr, [0, 2, 4], axis=1)\nOut[138]: \narray([[ 0,  0,  0],\n       [ 1,  5,  4],\n       [ 2, 10,  8],\n       [ 3, 15, 12]])\nSee Table A-2 for a partial listing of ufunc methods.\nTable A-2. ufunc methods\nMethod\nDescription\naccumulate(x)\nAggregate values, preserving all partial aggregates.\nat(x, indices, b=None)\nPerform operation in place on x at the specified indices. The argument b is the second\ninput to ufuncs that requires two array inputs.\nreduce(x)\nAggregate values by successive applications of the operation.\nreduceat(x, bins)\n“Local” reduce or “group by”; reduce contiguous slices of data to produce an aggregated\narray.\nouter(x, y)\nApply operation to all pairs of elements in x and y; the resulting array has shape\nx.shape + y.shape.\n492 \n| \nAppendix A: Advanced NumPy",
      "content_length": 1755,
      "extraction_method": "Direct"
    },
    {
      "page_number": 511,
      "chapter": null,
      "content": "Writing New ufuncs in Python\nThere are a number of ways to create your own NumPy ufuncs. The most general is\nto use the NumPy C API, but that is beyond the scope of this book. In this section, we\nwill look at pure Python ufuncs.\nnumpy.frompyfunc accepts a Python function along with a specification for the num‐\nber of inputs and outputs. For example, a simple function that adds element-wise\nwould be specified as:\nIn [139]: def add_elements(x, y):\n   .....:     return x + y\nIn [140]: add_them = np.frompyfunc(add_elements, 2, 1)\nIn [141]: add_them(np.arange(8), np.arange(8))\nOut[141]: array([0, 2, 4, 6, 8, 10, 12, 14], dtype=object)\nFunctions created using frompyfunc always return arrays of Python objects, which\ncan be inconvenient. Fortunately, there is an alternative (but slightly less feature rich)\nfunction, numpy.vectorize, that allows you to specify the output type:\nIn [142]: add_them = np.vectorize(add_elements, otypes=[np.float64])\nIn [143]: add_them(np.arange(8), np.arange(8))\nOut[143]: array([ 0.,  2.,  4.,  6.,  8., 10., 12., 14.])\nThese functions provide a way to create ufunc-like functions, but they are very slow\nbecause they require a Python function call to compute each element, which is a lot\nslower than NumPy’s C-based ufunc loops:\nIn [144]: arr = rng.standard_normal(10000)\nIn [145]: %timeit add_them(arr, arr)\n2.43 ms +- 30.5 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\nIn [146]: %timeit np.add(arr, arr)\n2.88 us +- 47.9 ns per loop (mean +- std. dev. of 7 runs, 100000 loops each)\nLater in this appendix we’ll show how to create fast ufuncs in Python using the\nNumba library.\nA.5 Structured and Record Arrays\nYou may have noticed up until now that ndarray is a homogeneous data container;\nthat is, it represents a block of memory in which each element takes up the same\nnumber of bytes, as determined by the data type. On the surface, this would appear\nto not allow you to represent heterogeneous or tabular data. A structured array is an\nndarray in which each element can be thought of as representing a struct in C (hence\nthe “structured” name) or a row in a SQL table with multiple named fields:\nAdvanced NumPy \n| \n493",
      "content_length": 2171,
      "extraction_method": "Direct"
    },
    {
      "page_number": 512,
      "chapter": null,
      "content": "In [147]: dtype = [('x', np.float64), ('y', np.int32)]\nIn [148]: sarr = np.array([(1.5, 6), (np.pi, -2)], dtype=dtype)\nIn [149]: sarr\nOut[149]: array([(1.5   ,  6), (3.1416, -2)], dtype=[('x', '<f8'), ('y', '<i4')])\nThere are several ways to specify a structured data type (see the online NumPy\ndocumentation). One typical way is as a list of tuples with (field_name,\nfield_data_type). Now, the elements of the array are tuple-like objects whose\nelements can be accessed like a dictionary:\nIn [150]: sarr[0]\nOut[150]: (1.5, 6)\nIn [151]: sarr[0]['y']\nOut[151]: 6\nThe field names are stored in the dtype.names attribute. When you access a field on\nthe structured array, a strided view on the data is returned, thus copying nothing:\nIn [152]: sarr['x']\nOut[152]: array([1.5   , 3.1416])\nNested Data Types and Multidimensional Fields\nWhen specifying a structured data type, you can additionally pass a shape (as an int\nor tuple):\nIn [153]: dtype = [('x', np.int64, 3), ('y', np.int32)]\nIn [154]: arr = np.zeros(4, dtype=dtype)\nIn [155]: arr\nOut[155]: \narray([([0, 0, 0], 0), ([0, 0, 0], 0), ([0, 0, 0], 0), ([0, 0, 0], 0)],\n      dtype=[('x', '<i8', (3,)), ('y', '<i4')])\nIn this case, the x field now refers to an array of length 3 for each record:\nIn [156]: arr[0]['x']\nOut[156]: array([0, 0, 0])\nConveniently, accessing arr['x'] then returns a two-dimensional array instead of a\none-dimensional array as in prior examples:\nIn [157]: arr['x']\nOut[157]: \narray([[0, 0, 0],\n       [0, 0, 0],\n       [0, 0, 0],\n       [0, 0, 0]])\n494 \n| \nAppendix A: Advanced NumPy",
      "content_length": 1559,
      "extraction_method": "Direct"
    },
    {
      "page_number": 513,
      "chapter": null,
      "content": "This enables you to express more complicated, nested structures as a single block of\nmemory in an array. You can also nest data types to make more complex structures.\nHere is an example:\nIn [158]: dtype = [('x', [('a', 'f8'), ('b', 'f4')]), ('y', np.int32)]\nIn [159]: data = np.array([((1, 2), 5), ((3, 4), 6)], dtype=dtype)\nIn [160]: data['x']\nOut[160]: array([(1., 2.), (3., 4.)], dtype=[('a', '<f8'), ('b', '<f4')])\nIn [161]: data['y']\nOut[161]: array([5, 6], dtype=int32)\nIn [162]: data['x']['a']\nOut[162]: array([1., 3.])\npandas DataFrame does not support this feature in the same way, though it is similar\nto hierarchical indexing.\nWhy Use Structured Arrays?\nCompared with a pandas DataFrame, NumPy structured arrays are a lower level tool.\nThey provide a means to interpret a block of memory as a tabular structure with\nnested columns. Since each element in the array is represented in memory as a fixed\nnumber of bytes, structured arrays provide an efficient way of writing data to and\nfrom disk (including memory maps), transporting it over the network, and other\nsuch uses. The memory layout of each value in a structured array is based on the\nbinary representation of struct data types in the C programming language.\nAs another common use for structured arrays, writing data files as fixed-length\nrecord byte streams is a common way to serialize data in C and C++ code, which\nis sometimes found in legacy systems in industry. As long as the format of the file\nis known (the size of each record and the order, byte size, and data type of each\nelement), the data can be read into memory with np.fromfile. Specialized uses like\nthis are beyond the scope of this book, but it’s worth knowing that such things are\npossible.\nA.6 More About Sorting\nLike Python’s built-in list, the ndarray sort instance method is an in-place sort,\nmeaning that the array contents are rearranged without producing a new array:\nIn [163]: arr = rng.standard_normal(6)\nIn [164]: arr.sort()\nAdvanced NumPy \n| \n495",
      "content_length": 1996,
      "extraction_method": "Direct"
    },
    {
      "page_number": 514,
      "chapter": null,
      "content": "In [165]: arr\nOut[165]: array([-1.1553, -0.9319, -0.5218, -0.4745, -0.1649,  0.03  ])\nWhen sorting arrays in place, remember that if the array is a view on a different\nndarray, the original array will be modified:\nIn [166]: arr = rng.standard_normal((3, 5))\nIn [167]: arr\nOut[167]: \narray([[-1.1956,  0.4691, -0.3598,  1.0359,  0.2267],\n       [-0.7448, -0.5931, -1.055 , -0.0683,  0.458 ],\n       [-0.07  ,  0.1462, -0.9944,  1.1436,  0.5026]])\nIn [168]: arr[:, 0].sort()  # Sort first column values in place\nIn [169]: arr\nOut[169]: \narray([[-1.1956,  0.4691, -0.3598,  1.0359,  0.2267],\n       [-0.7448, -0.5931, -1.055 , -0.0683,  0.458 ],\n       [-0.07  ,  0.1462, -0.9944,  1.1436,  0.5026]])\nOn the other hand, numpy.sort creates a new, sorted copy of an array. Otherwise, it\naccepts the same arguments (such as kind) as ndarray’s sort method:\nIn [170]: arr = rng.standard_normal(5)\nIn [171]: arr\nOut[171]: array([ 0.8981, -1.1704, -0.2686, -0.796 ,  1.4522])\nIn [172]: np.sort(arr)\nOut[172]: array([-1.1704, -0.796 , -0.2686,  0.8981,  1.4522])\nIn [173]: arr\nOut[173]: array([ 0.8981, -1.1704, -0.2686, -0.796 ,  1.4522])\nAll of these sort methods take an axis argument for independently sorting the sec‐\ntions of data along the passed axis:\nIn [174]: arr = rng.standard_normal((3, 5))\nIn [175]: arr\nOut[175]: \narray([[-0.2535,  2.1183,  0.3634, -0.6245,  1.1279],\n       [ 1.6164, -0.2287, -0.6201, -0.1143, -1.2067],\n       [-1.0872, -2.1518, -0.6287, -1.3199,  0.083 ]])\nIn [176]: arr.sort(axis=1)\nIn [177]: arr\nOut[177]: \narray([[-0.6245, -0.2535,  0.3634,  1.1279,  2.1183],\n496 \n| \nAppendix A: Advanced NumPy",
      "content_length": 1621,
      "extraction_method": "Direct"
    },
    {
      "page_number": 515,
      "chapter": null,
      "content": "[-1.2067, -0.6201, -0.2287, -0.1143,  1.6164],\n       [-2.1518, -1.3199, -1.0872, -0.6287,  0.083 ]])\nYou may notice that none of the sort methods have an option to sort in descending\norder. This is a problem in practice because array slicing produces views, thus not\nproducing a copy or requiring any computational work. Many Python users are\nfamiliar with the “trick” that for a list of values, values[::-1] returns a list in\nreverse order. The same is true for ndarrays:\nIn [178]: arr[:, ::-1]\nOut[178]: \narray([[ 2.1183,  1.1279,  0.3634, -0.2535, -0.6245],\n       [ 1.6164, -0.1143, -0.2287, -0.6201, -1.2067],\n       [ 0.083 , -0.6287, -1.0872, -1.3199, -2.1518]])\nIndirect Sorts: argsort and lexsort\nIn data analysis you may need to reorder datasets by one or more keys. For example,\na table of data about some students might need to be sorted by last name, then by first\nname. This is an example of an indirect sort, and if you’ve read the pandas-related\nchapters, you have already seen many higher-level examples. Given a key or keys (an\narray of values or multiple arrays of values), you wish to obtain an array of integer\nindices (I refer to them colloquially as indexers) that tells you how to reorder the data\nto be in sorted order. Two methods for this are argsort and numpy.lexsort. As an\nexample:\nIn [179]: values = np.array([5, 0, 1, 3, 2])\nIn [180]: indexer = values.argsort()\nIn [181]: indexer\nOut[181]: array([1, 2, 4, 3, 0])\nIn [182]: values[indexer]\nOut[182]: array([0, 1, 2, 3, 5])\nAs a more complicated example, this code reorders a two-dimensional array by its\nfirst row:\nIn [183]: arr = rng.standard_normal((3, 5))\nIn [184]: arr[0] = values\nIn [185]: arr\nOut[185]: \narray([[ 5.    ,  0.    ,  1.    ,  3.    ,  2.    ],\n       [-0.7503, -2.1268, -1.391 , -0.4922,  0.4505],\n       [ 0.8926, -1.0479,  0.9553,  0.2936,  0.5379]])\nIn [186]: arr[:, arr[0].argsort()]\nOut[186]: \nAdvanced NumPy \n| \n497",
      "content_length": 1923,
      "extraction_method": "Direct"
    },
    {
      "page_number": 516,
      "chapter": null,
      "content": "array([[ 0.    ,  1.    ,  2.    ,  3.    ,  5.    ],\n       [-2.1268, -1.391 ,  0.4505, -0.4922, -0.7503],\n       [-1.0479,  0.9553,  0.5379,  0.2936,  0.8926]])\nlexsort is similar to argsort, but it performs an indirect lexicographical sort on\nmultiple key arrays. Suppose we wanted to sort some data identified by first and last\nnames:\nIn [187]: first_name = np.array(['Bob', 'Jane', 'Steve', 'Bill', 'Barbara'])\nIn [188]: last_name = np.array(['Jones', 'Arnold', 'Arnold', 'Jones', 'Walters'])\nIn [189]: sorter = np.lexsort((first_name, last_name))\nIn [190]: sorter\nOut[190]: array([1, 2, 3, 0, 4])\nIn [191]: list(zip(last_name[sorter], first_name[sorter]))\nOut[191]: \n[('Arnold', 'Jane'),\n ('Arnold', 'Steve'),\n ('Jones', 'Bill'),\n ('Jones', 'Bob'),\n ('Walters', 'Barbara')]\nlexsort can be a bit confusing the first time you use it, because the order in which\nthe keys are used to order the data starts with the last array passed. Here, last_name\nwas used before first_name.\nAlternative Sort Algorithms\nA stable sorting algorithm preserves the relative position of equal elements. This can\nbe especially important in indirect sorts where the relative ordering is meaningful:\nIn [192]: values = np.array(['2:first', '2:second', '1:first', '1:second',\n   .....:                    '1:third'])\nIn [193]: key = np.array([2, 2, 1, 1, 1])\nIn [194]: indexer = key.argsort(kind='mergesort')\nIn [195]: indexer\nOut[195]: array([2, 3, 4, 0, 1])\nIn [196]: values.take(indexer)\nOut[196]: \narray(['1:first', '1:second', '1:third', '2:first', '2:second'],\n      dtype='<U8')\nThe only stable sort available is mergesort, which has guaranteed O(n log n) perfor‐\nmance, but its performance is on average worse than the default quicksort method.\n498 \n| \nAppendix A: Advanced NumPy",
      "content_length": 1766,
      "extraction_method": "Direct"
    },
    {
      "page_number": 517,
      "chapter": null,
      "content": "See Table A-3 for a summary of available methods and their relative performance\n(and performance guarantees). This is not something that most users will ever have to\nthink about, but it’s useful to know that it’s there.\nTable A-3. Array sorting methods\nKind\nSpeed\nStable Work space\nWorst case\n'quicksort'\n1\nNo\n0\nO(n^2)\n'mergesort'\n2\nYes\nn / 2\nO(n log n)\n'heapsort'\n3\nNo\n0\nO(n log n)\nPartially Sorting Arrays\nOne of the goals of sorting can be to determine the largest or smallest elements\nin an array. NumPy has fast methods, numpy.partition and np.argpartition, for\npartitioning an array around the k-th smallest element:\nIn [197]: rng = np.random.default_rng(12345)\nIn [198]: arr = rng.standard_normal(20)\nIn [199]: arr\nOut[199]: \narray([-1.4238,  1.2637, -0.8707, -0.2592, -0.0753, -0.7409, -1.3678,\n        0.6489,  0.3611, -1.9529,  2.3474,  0.9685, -0.7594,  0.9022,\n       -0.467 , -0.0607,  0.7888, -1.2567,  0.5759,  1.399 ])\nIn [200]: np.partition(arr, 3)\nOut[200]: \narray([-1.9529, -1.4238, -1.3678, -1.2567, -0.8707, -0.7594, -0.7409,\n       -0.0607,  0.3611, -0.0753, -0.2592, -0.467 ,  0.5759,  0.9022,\n        0.9685,  0.6489,  0.7888,  1.2637,  1.399 ,  2.3474])\nAfter you call partition(arr, 3), the first three elements in the result are the small‐\nest three values in no particular order. numpy.argpartition, similar to numpy.arg\nsort, returns the indices that rearrange the data into the equivalent order:\nIn [201]: indices = np.argpartition(arr, 3)\nIn [202]: indices\nOut[202]: \narray([ 9,  0,  6, 17,  2, 12,  5, 15,  8,  4,  3, 14, 18, 13, 11,  7, 16,\n        1, 19, 10])\nIn [203]: arr.take(indices)\nOut[203]: \narray([-1.9529, -1.4238, -1.3678, -1.2567, -0.8707, -0.7594, -0.7409,\n       -0.0607,  0.3611, -0.0753, -0.2592, -0.467 ,  0.5759,  0.9022,\n        0.9685,  0.6489,  0.7888,  1.2637,  1.399 ,  2.3474])\nAdvanced NumPy \n| \n499",
      "content_length": 1857,
      "extraction_method": "Direct"
    },
    {
      "page_number": 518,
      "chapter": null,
      "content": "numpy.searchsorted: Finding Elements in a Sorted Array\nsearchsorted is an array method that performs a binary search on a sorted array,\nreturning the location in the array where the value would need to be inserted to\nmaintain sortedness:\nIn [204]: arr = np.array([0, 1, 7, 12, 15])\nIn [205]: arr.searchsorted(9)\nOut[205]: 3\nYou can also pass an array of values to get an array of indices back:\nIn [206]: arr.searchsorted([0, 8, 11, 16])\nOut[206]: array([0, 3, 3, 5])\nYou might have noticed that searchsorted returned 0 for the 0 element. This is\nbecause the default behavior is to return the index at the left side of a group of equal\nvalues:\nIn [207]: arr = np.array([0, 0, 0, 1, 1, 1, 1])\nIn [208]: arr.searchsorted([0, 1])\nOut[208]: array([0, 3])\nIn [209]: arr.searchsorted([0, 1], side='right')\nOut[209]: array([3, 7])\nAs another application of searchsorted, suppose we had an array of values between\n0 and 10,000, and a separate array of “bucket edges” that we wanted to use to bin the\ndata:\nIn [210]: data = np.floor(rng.uniform(0, 10000, size=50))\nIn [211]: bins = np.array([0, 100, 1000, 5000, 10000])\nIn [212]: data\nOut[212]: \narray([ 815., 1598., 3401., 4651., 2664., 8157., 1932., 1294.,  916.,\n       5985., 8547., 6016., 9319., 7247., 8605., 9293., 5461., 9376.,\n       4949., 2737., 4517., 6650., 3308., 9034., 2570., 3398., 2588.,\n       3554.,   50., 6286., 2823.,  680., 6168., 1763., 3043., 4408.,\n       1502., 2179., 4743., 4763., 2552., 2975., 2790., 2605., 4827.,\n       2119., 4956., 2462., 8384., 1801.])\nTo then get a labeling to which interval each data point belongs (where 1 would mean\nthe bucket [0, 100)), we can simply use searchsorted:\nIn [213]: labels = bins.searchsorted(data)\nIn [214]: labels\nOut[214]: \narray([2, 3, 3, 3, 3, 4, 3, 3, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 4,\n500 \n| \nAppendix A: Advanced NumPy",
      "content_length": 1846,
      "extraction_method": "Direct"
    },
    {
      "page_number": 519,
      "chapter": null,
      "content": "3, 4, 3, 3, 3, 3, 1, 4, 3, 2, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n       3, 3, 3, 3, 4, 3])\nThis, combined with pandas’s groupby, can be used to bin data:\nIn [215]: pd.Series(data).groupby(labels).mean()\nOut[215]: \n1      50.000000\n2     803.666667\n3    3079.741935\n4    7635.200000\ndtype: float64\nA.7 Writing Fast NumPy Functions with Numba\nNumba is an open source project that creates fast functions for NumPy-like data\nusing CPUs, GPUs, or other hardware. It uses the LLVM Project to translate Python\ncode into compiled machine code.\nTo introduce Numba, let’s consider a pure Python function that computes the expres‐\nsion (x - y).mean() using a for loop:\nimport numpy as np\ndef mean_distance(x, y):\n    nx = len(x)\n    result = 0.0\n    count = 0\n    for i in range(nx):\n        result += x[i] - y[i]\n        count += 1\n    return result / count\nThis function is slow:\nIn [209]: x = rng.standard_normal(10_000_000)\nIn [210]: y = rng.standard_normal(10_000_000)\nIn [211]: %timeit mean_distance(x, y)\n1 loop, best of 3: 2 s per loop\nIn [212]: %timeit (x - y).mean()\n100 loops, best of 3: 14.7 ms per loop\nThe NumPy version is over 100 times faster. We can turn this function into a\ncompiled Numba function using the numba.jit function:\nIn [213]: import numba as nb\nIn [214]: numba_mean_distance = nb.jit(mean_distance)\nAdvanced NumPy \n| \n501",
      "content_length": 1342,
      "extraction_method": "Direct"
    },
    {
      "page_number": 520,
      "chapter": null,
      "content": "We also could have written this as a decorator:\n@nb.jit\ndef numba_mean_distance(x, y):\n    nx = len(x)\n    result = 0.0\n    count = 0\n    for i in range(nx):\n        result += x[i] - y[i]\n        count += 1\n    return result / count\nThe resulting function is actually faster than the vectorized NumPy version:\nIn [215]: %timeit numba_mean_distance(x, y)\n100 loops, best of 3: 10.3 ms per loop\nNumba cannot compile all pure Python code, but it supports a significant subset of\nPython that is most useful for writing numerical algorithms.\nNumba is a deep library, supporting different kinds of hardware, modes of compila‐\ntion, and user extensions. It is also able to compile a substantial subset of the NumPy\nPython API without explicit for loops. Numba is able to recognize constructs that\ncan be compiled to machine code, while substituting calls to the CPython API for\nfunctions that it does not know how to compile. Numba’s jit function option,\nnopython=True, restricts allowed code to Python code that can be compiled to\nLLVM without any Python C API calls. jit(nopython=True) has a shorter alias,\nnumba.njit.\nIn the previous example, we could have written:\nfrom numba import float64, njit\n@njit(float64(float64[:], float64[:]))\ndef mean_distance(x, y):\n    return (x - y).mean()\nI encourage you to learn more by reading the online documentation for Numba. The\nnext section shows an example of creating custom NumPy ufunc objects.\nCreating Custom numpy.ufunc Objects with Numba\nThe numba.vectorize function creates compiled NumPy ufuncs, which behave like\nbuilt-in ufuncs. Let’s consider a Python implementation of numpy.add:\nfrom numba import vectorize\n@vectorize\ndef nb_add(x, y):\n    return x + y\nNow we have:\n502 \n| \nAppendix A: Advanced NumPy",
      "content_length": 1751,
      "extraction_method": "Direct"
    },
    {
      "page_number": 521,
      "chapter": null,
      "content": "In [13]: x = np.arange(10)\nIn [14]: nb_add(x, x)\nOut[14]: array([  0.,   2.,   4.,   6.,   8.,  10.,  12.,  14.,  16.,  18.])\nIn [15]: nb_add.accumulate(x, 0)\nOut[15]: array([  0.,   1.,   3.,   6.,  10.,  15.,  21.,  28.,  36.,  45.])\nA.8 Advanced Array Input and Output\nIn Chapter 4, we became acquainted with np.save and np.load for storing arrays in\nbinary format on disk. There are a number of additional options to consider for more\nsophisticated use. In particular, memory maps have the additional benefit of enabling\nyou to do certain operations with datasets that do not fit into RAM.\nMemory-Mapped Files\nA memory-mapped file is a method for interacting with binary data on disk as though\nit is stored in an in-memory array. NumPy implements a memmap object that is\nndarray-like, enabling small segments of a large file to be read and written without\nreading the whole array into memory. Additionally, a memmap has the same methods\nas an in-memory array and thus can be substituted into many algorithms where an\nndarray would be expected.\nTo create a new memory map, use the function np.memmap and pass a file path, data\ntype, shape, and file mode:\nIn [217]: mmap = np.memmap('mymmap', dtype='float64', mode='w+',\n   .....:                  shape=(10000, 10000))\nIn [218]: mmap\nOut[218]: \nmemmap([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]])\nSlicing a memmap returns views on the data on disk:\nIn [219]: section = mmap[:5]\nIf you assign data to these, it will be buffered in memory, which means that the\nchanges will not be immediately reflected in the on-disk file if you were to read\nthe file in a different application. Any modifications can be synchronized to disk by\ncalling flush:\nAdvanced NumPy \n| \n503",
      "content_length": 1913,
      "extraction_method": "Direct"
    },
    {
      "page_number": 522,
      "chapter": null,
      "content": "In [220]: section[:] = rng.standard_normal((5, 10000))\nIn [221]: mmap.flush()\nIn [222]: mmap\nOut[222]: \nmemmap([[-0.9074, -1.0954,  0.0071, ...,  0.2753, -1.1641,  0.8521],\n        [-0.0103, -0.0646, -1.0615, ..., -1.1003,  0.2505,  0.5832],\n        [ 0.4583,  1.2992,  1.7137, ...,  0.8691, -0.7889, -0.2431],\n        ...,\n        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]])\nIn [223]: del mmap\nWhenever a memory map falls out of scope and is garbage collected, any changes\nwill be flushed to disk also. When opening an existing memory map, you still have to\nspecify the data type and shape, as the file is only a block of binary data without any\ndata type information, shape, or strides:\nIn [224]: mmap = np.memmap('mymmap', dtype='float64', shape=(10000, 10000))\nIn [225]: mmap\nOut[225]: \nmemmap([[-0.9074, -1.0954,  0.0071, ...,  0.2753, -1.1641,  0.8521],\n        [-0.0103, -0.0646, -1.0615, ..., -1.1003,  0.2505,  0.5832],\n        [ 0.4583,  1.2992,  1.7137, ...,  0.8691, -0.7889, -0.2431],\n        ...,\n        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]])\nMemory maps also work with structured or nested data types, as described in Section\nA.5, “Structured and Record Arrays,” on page 493.\nIf you ran this example on your computer, you may want to delete the large file that\nwe created above:\nIn [226]: %xdel mmap\nIn [227]: !rm mymmap\nHDF5 and Other Array Storage Options\nPyTables and h5py are two Python projects providing NumPy-friendly interfaces for\nstoring array data in the efficient and compressible HDF5 format (HDF stands for\nhierarchical data format). You can safely store hundreds of gigabytes or even terabytes\nof data in HDF5 format. To learn more about using HDF5 with Python, I recommend\nreading the pandas online documentation.\n504 \n| \nAppendix A: Advanced NumPy",
      "content_length": 2097,
      "extraction_method": "Direct"
    },
    {
      "page_number": 523,
      "chapter": null,
      "content": "A.9 Performance Tips\nAdapting data processing code to use NumPy generally makes things much faster,\nas array operations typically replace otherwise comparatively extremely slow pure\nPython loops. Here are some tips to help get the best performance out of the library:\n• Convert Python loops and conditional logic to array operations and Boolean\n•\narray operations.\n• Use broadcasting whenever possible.\n•\n• Use arrays views (slicing) to avoid copying data.\n•\n• Utilize ufuncs and ufunc methods.\n•\nIf you can’t get the performance you require after exhausting the capabilities provided\nby NumPy alone, consider writing code in C, FORTRAN, or Cython. I use Cython\nfrequently in my own work as a way to get C-like performance, often with much less\ndevelopment time.\nThe Importance of Contiguous Memory\nWhile the full extent of this topic is a bit outside the scope of this book, in some\napplications the memory layout of an array can significantly affect the speed of\ncomputations. This is based partly on performance differences having to do with\nthe cache hierarchy of the CPU; operations accessing contiguous blocks of memory\n(e.g., summing the rows of a C order array) will generally be the fastest because the\nmemory subsystem will buffer the appropriate blocks of memory into the low latency\nL1 or L2 CPU caches. Also, certain code paths inside NumPy’s C codebase have been\noptimized for the contiguous case in which generic strided memory access can be\navoided.\nTo say that an array’s memory layout is contiguous means that the elements are stored\nin memory in the order that they appear in the array with respect to FORTRAN\n(column major) or C (row major) ordering. By default, NumPy arrays are created as\nC contiguous or just simply contiguous. A column major array, such as the transpose\nof a C-contiguous array, is thus said to be FORTRAN contiguous. These properties\ncan be explicitly checked via the flags attribute on the ndarray:\nIn [228]: arr_c = np.ones((100, 10000), order='C')\nIn [229]: arr_f = np.ones((100, 10000), order='F')\nIn [230]: arr_c.flags\nOut[230]: \n  C_CONTIGUOUS : True\n  F_CONTIGUOUS : False\n  OWNDATA : True\nAdvanced NumPy \n| \n505",
      "content_length": 2161,
      "extraction_method": "Direct"
    },
    {
      "page_number": 524,
      "chapter": null,
      "content": "WRITEABLE : True\n  ALIGNED : True\n  WRITEBACKIFCOPY : False\n  UPDATEIFCOPY : False\nIn [231]: arr_f.flags\nOut[231]: \n  C_CONTIGUOUS : False\n  F_CONTIGUOUS : True\n  OWNDATA : True\n  WRITEABLE : True\n  ALIGNED : True\n  WRITEBACKIFCOPY : False\n  UPDATEIFCOPY : False\nIn [232]: arr_f.flags.f_contiguous\nOut[232]: True\nIn this example, summing the rows of these arrays should, in theory, be faster for\narr_c than arr_f since the rows are contiguous in memory. Here, I check using\n%timeit in IPython (these results may differ on your machine):\nIn [233]: %timeit arr_c.sum(1)\n444 us +- 60.5 us per loop (mean +- std. dev. of 7 runs, 1000 loops each)\nIn [234]: %timeit arr_f.sum(1)\n581 us +- 8.16 us per loop (mean +- std. dev. of 7 runs, 1000 loops each)\nWhen you’re looking to squeeze more performance out of NumPy, this is often a\nplace to invest some effort. If you have an array that does not have the desired\nmemory order, you can use copy and pass either 'C' or 'F':\nIn [235]: arr_f.copy('C').flags\nOut[235]: \n  C_CONTIGUOUS : True\n  F_CONTIGUOUS : False\n  OWNDATA : True\n  WRITEABLE : True\n  ALIGNED : True\n  WRITEBACKIFCOPY : False\n  UPDATEIFCOPY : False\n506 \n| \nAppendix A: Advanced NumPy",
      "content_length": 1189,
      "extraction_method": "Direct"
    },
    {
      "page_number": 525,
      "chapter": null,
      "content": "When constructing a view on an array, keep in mind that the result is not guaranteed\nto be contiguous:\nIn [236]: arr_c[:50].flags.contiguous\nOut[236]: True\nIn [237]: arr_c[:, :50].flags\nOut[237]: \n  C_CONTIGUOUS : False\n  F_CONTIGUOUS : False\n  OWNDATA : False\n  WRITEABLE : True\n  ALIGNED : True\n  WRITEBACKIFCOPY : False\n  UPDATEIFCOPY : False\nAdvanced NumPy \n| \n507",
      "content_length": 368,
      "extraction_method": "Direct"
    },
    {
      "page_number": 526,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 527,
      "chapter": null,
      "content": "APPENDIX B\nMore on the IPython System\nIn Chapter 2 we looked at the basics of using the IPython shell and Jupyter notebook.\nIn this appendix, we explore some deeper functionality in the IPython system that\ncan either be used from the console or within Jupyter.\nB.1 Terminal Keyboard Shortcuts\nIPython has many keyboard shortcuts for navigating the prompt (which will be\nfamiliar to users of the Emacs text editor or the Unix bash shell) and interacting\nwith the shell’s command history. Table B-1 summarizes some of the most commonly\nused shortcuts. See Figure B-1 for an illustration of a few of these, such as cursor\nmovement.\nTable B-1. Standard IPython keyboard shortcuts\nKeyboard shortcut\nDescription\nCtrl-P or up-arrow\nSearch backward in command history for commands starting with currently entered text\nCtrl-N or down-arrow Search forward in command history for commands starting with currently entered text\nCtrl-R\nReadline-style reverse history search (partial matching)\nCtrl-Shift-V\nPaste text from clipboard\nCtrl-C\nInterrupt currently executing code\nCtrl-A\nMove cursor to beginning of line\nCtrl-E\nMove cursor to end of line\nCtrl-K\nDelete text from cursor until end of line\nCtrl-U\nDiscard all text on current line\nCtrl-F\nMove cursor forward one character\nCtrl-B\nMove cursor back one character\nCtrl-L\nClear screen\n509",
      "content_length": 1325,
      "extraction_method": "Direct"
    },
    {
      "page_number": 528,
      "chapter": null,
      "content": "Figure B-1. Illustration of some keyboard shortcuts in the IPython shell\nNote that Jupyter notebooks have a largely separate set of keyboard shortcuts for\nnavigation and editing. Since these shortcuts have evolved more rapidly than the ones\nin IPython, I encourage you to explore the integrated help system in the Jupyter\nnotebook menus.\nB.2 About Magic Commands\nSpecial commands in IPython (which are not built into Python itself) are known\nas magic commands. These are designed to facilitate common tasks and enable you\nto easily control the behavior of the IPython system. A magic command is any com‐\nmand prefixed by the percent symbol %. For example, you can check the execution\ntime of any Python statement, such as a matrix multiplication, using the %timeit\nmagic function:\nIn [20]: a = np.random.standard_normal((100, 100))\n    In [20]: %timeit np.dot(a, a)\n    92.5 µs ± 3.43 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\nMagic commands can be viewed as command-line programs to be run within the\nIPython system. Many of them have additional “command-line” options, which can\nall be viewed (as you might expect) using ?:\nIn [21]: %debug?\n    Docstring:\n    ::\n    %debug [--breakpoint FILE:LINE] [statement [statement ...]]\n    Activate the interactive debugger.\n    This magic command support two ways of activating debugger.\n    One is to activate debugger before executing code.  This way, you\n    can set a break point, to step through the code from the point.\n    You can use this mode by giving statements to execute and optionally\n    a breakpoint.\n    The other one is to activate debugger in post-mortem mode.  You can\n    activate this mode simply running %debug without any argument.\n    If an exception has just occurred, this lets you inspect its stack\n    frames interactively.  Note that this will always work only on the last\n510 \n| \nAppendix B: More on the IPython System",
      "content_length": 1910,
      "extraction_method": "Direct"
    },
    {
      "page_number": 529,
      "chapter": null,
      "content": "traceback that occurred, so you must call this quickly after an\n    exception that you wish to inspect has fired, because if another one\n    occurs, it clobbers the previous one.\n    If you want IPython to automatically do this on every exception, see\n    the %pdb magic for more details.\n    .. versionchanged:: 7.3\n    When running code, user variables are no longer expanded,\n    the magic line is always left unmodified.\n    positional arguments:\n    statement             Code to run in debugger. You can omit this in cell \n    magic mode.\n    optional arguments:\n    --breakpoint <FILE:LINE>, -b <FILE:LINE>\n    Set break point at LINE in FILE.\nMagic functions can be used by default without the percent sign, as long as no\nvariable is defined with the same name as the magic function in question. This\nfeature is called automagic and can be enabled or disabled with %automagic.\nSome magic functions behave like Python functions, and their output can be assigned\nto a variable:\nIn [22]: %pwd\n    Out[22]: '/home/wesm/code/pydata-book'\n    In [23]: foo = %pwd\n    In [24]: foo\n    Out[24]: '/home/wesm/code/pydata-book'\nSince IPython’s documentation is accessible from within the system, I encourage you\nto explore all of the special commands available by using %quickref or %magic. This\ninformation is shown in a console pager, so you will need to press q to exit from the\npager. Table B-2 highlights some of the most critical commands for being productive\nin interactive computing and Python development in IPython.\nTable B-2. Some frequently used IPython magic commands\nCommand\nDescription\n%quickref\nDisplay the IPython Quick Reference Card\n%magic\nDisplay detailed documentation for all of the available magic commands\n%debug\nEnter the interactive debugger at the bottom of the last exception traceback\n%hist\nPrint command input (and optionally output) history\n%pdb\nAutomatically enter debugger after any exception\n%paste\nExecute preformatted Python code from clipboard\nMore on the IPython System \n| \n511",
      "content_length": 2012,
      "extraction_method": "Direct"
    },
    {
      "page_number": 530,
      "chapter": null,
      "content": "Command\nDescription\n%cpaste\nOpen a special prompt for manually pasting Python code to be executed\n%reset\nDelete all variables/names defined in an interactive namespace\n%page OBJECT\nPretty-print the object and display it through a pager\n%run script.py\nRun a Python script inside IPython\n%prun statement\nExecute statement with cProfile and report the profiler output\n%time statement\nReport the execution time of a single statement\n%timeit statement\nRun a statement multiple times to compute an ensemble average execution time; useful for\ntiming code with very short execution time\n%who, %who_ls, %whos\nDisplay variables defined in interactive namespace, with varying levels of information/\nverbosity\n%xdel variable\nDelete a variable and attempt to clear any references to the object in the IPython internals\nThe %run Command\nYou can run any file as a Python program inside the environment of your IPython\nsession using the %run command. Suppose you had the following simple script stored\nin script.py:\ndef f(x, y, z):\n    return (x + y) / z\na = 5\nb = 6\nc = 7.5\nresult = f(a, b, c)\nYou can execute this by passing the filename to %run:\nIn [14]: %run script.py\nThe script is run in an empty namespace (with no imports or other variables defined),\nso that the behavior should be identical to running the program on the command\nline using python script.py. All of the variables (imports, functions, and globals)\ndefined in the file (up until an exception, if any, is raised) will then be accessible in\nthe IPython shell:\nIn [15]: c\nOut [15]: 7.5\nIn [16]: result\nOut[16]: 1.4666666666666666\nIf a Python script expects command-line arguments (to be found in sys.argv), these\ncan be passed after the file path as though run on the command line.\n512 \n| \nAppendix B: More on the IPython System",
      "content_length": 1782,
      "extraction_method": "Direct"
    },
    {
      "page_number": 531,
      "chapter": null,
      "content": "If you want to give a script access to variables already defined in the\ninteractive IPython namespace, use %run -i instead of plain %run.\nIn the Jupyter notebook, you can also use the related %load magic function, which\nimports a script into a code cell:\nIn [16]: %load script.py\n    def f(x, y, z):\n        return (x + y) / z\n    a = 5\n    b = 6\n    c = 7.5\n    result = f(a, b, c)\nInterrupting running code\nPressing Ctrl-C while any code is running, whether a script through %run or a\nlong-running command, will raise a KeyboardInterrupt. This will cause nearly all\nPython programs to stop immediately except in certain unusual cases.\nWhen a piece of Python code has called into some compiled exten‐\nsion modules, pressing Ctrl-C will not always cause the program\nexecution to stop immediately. In such cases, you will have to either\nwait until control is returned to the Python interpreter, or in more\ndire circumstances, forcibly terminate the Python process in your\noperating system (such as using Task Manager on Windows or the\nkill command on Linux).\nExecuting Code from the Clipboard\nIf you are using the Jupyter notebook, you can copy and paste code into any code cell\nand execute it. It is also possible to run code from the clipboard in the IPython shell.\nSuppose you had the following code in some other application:\nx = 5\ny = 7\nif x > 5:\n    x += 1\n    y = 8\nThe most foolproof methods are the %paste and %cpaste magic functions (note that\nthese do not work in Jupyter since you can copy and paste into a Jupyter code cell).\nMore on the IPython System \n| \n513",
      "content_length": 1572,
      "extraction_method": "Direct"
    },
    {
      "page_number": 532,
      "chapter": null,
      "content": "%paste takes whatever text is in the clipboard and executes it as a single block in the\nshell:\nIn [17]: %paste\nx = 5\ny = 7\nif x > 5:\n    x += 1\n    y = 8\n## -- End pasted text --\n%cpaste is similar, except that it gives you a special prompt for pasting code into:\nIn [18]: %cpaste\nPasting code; enter '--' alone on the line to stop or use Ctrl-D.\n:x = 5\n:y = 7\n:if x > 5:\n:    x += 1\n:\n:    y = 8\n:--\nWith the %cpaste block, you have the freedom to paste as much code as you like\nbefore executing it. You might decide to use %cpaste to look at the pasted code\nbefore executing it. If you accidentally paste the wrong code, you can break out of the\n%cpaste prompt by pressing Ctrl-C.\nB.3 Using the Command History\nIPython maintains a small on-disk database containing the text of each command\nthat you execute. This serves various purposes:\n• Searching, completing, and executing previously executed commands with mini‐\n•\nmal typing\n• Persisting the command history between sessions\n•\n• Logging the input/output history to a file\n•\nThese features are more useful in the shell than in the notebook, since the notebook\nby design keeps a log of the input and output in each code cell.\nSearching and Reusing the Command History\nThe IPython shell lets you search and execute previous code or other commands.\nThis is useful, as you may often find yourself repeating the same commands, such as a\n%run command or some other code snippet. Suppose you had run:\n514 \n| \nAppendix B: More on the IPython System",
      "content_length": 1496,
      "extraction_method": "Direct"
    },
    {
      "page_number": 533,
      "chapter": null,
      "content": "In[7]: %run first/second/third/data_script.py\nand then explored the results of the script (assuming it ran successfully), only to find\nthat you made an incorrect calculation. After figuring out the problem and modifying\ndata_script.py, you can start typing a few letters of the %run command and then press\neither the Ctrl-P key combination or the up arrow key. This will search the command\nhistory for the first prior command matching the letters you typed. Pressing either\nCtrl-P or the up arrow key multiple times will continue to search through the history.\nIf you pass over the command you wish to execute, fear not. You can move forward\nthrough the command history by pressing either Ctrl-N or the down arrow key. After\ndoing this a few times, you may start pressing these keys without thinking!\nUsing Ctrl-R gives you the same partial incremental searching capability provided by\nthe readline used in Unix-style shells, such as the bash shell. On Windows, readline\nfunctionality is emulated by IPython. To use this, press Ctrl-R and then type a few\ncharacters contained in the input line you want to search for:\nIn [1]: a_command = foo(x, y, z)\n(reverse-i-search)`com': a_command = foo(x, y, z)\nPressing Ctrl-R will cycle through the history for each line, matching the characters\nyou’ve typed.\nInput and Output Variables\nForgetting to assign the result of a function call to a variable can be very annoying.\nAn IPython session stores references to both the input commands and output Python\nobjects in special variables. The previous two outputs are stored in the _ (one under‐\nscore) and __ (two underscores) variables, respectively:\nIn [18]: 'input1'\nOut[18]: 'input1'\nIn [19]: 'input2'\nOut[19]: 'input2'\nIn [20]: __\nOut[20]: 'input1'\nIn [21]: 'input3'\nOut[21]: 'input3'\nIn [22]: _\nOut[22]: 'input3'\nInput variables are stored in variables named _iX, where X is the input line number.\nMore on the IPython System \n| \n515",
      "content_length": 1927,
      "extraction_method": "Direct"
    },
    {
      "page_number": 534,
      "chapter": null,
      "content": "For each input variable there is a corresponding output variable _X. So after input line\n27, say, there will be two new variables, _27 (for the output) and _i27 for the input:\nIn [26]: foo = 'bar'\nIn [27]: foo\nOut[27]: 'bar'\nIn [28]: _i27\nOut[28]: u'foo'\nIn [29]: _27\nOut[29]: 'bar'\nSince the input variables are strings, they can be executed again with the Python eval\nkeyword:\nIn [30]: eval(_i27)\nOut[30]: 'bar'\nHere, _i27 refers to the code input in In [27].\nSeveral magic functions allow you to work with the input and output history. %hist\nprints all or part of the input history, with or without line numbers. %reset clears the\ninteractive namespace and optionally the input and output caches. The %xdel magic\nfunction removes all references to a particular object from the IPython machinery.\nSee the documentation for these magics for more details.\nWhen working with very large datasets, keep in mind that IPy‐\nthon’s input and output history may cause objects referenced there\nto not be garbage collected (freeing up the memory), even if you\ndelete the variables from the interactive namespace using the del\nkeyword. In such cases, careful usage of %xdel and %reset can help\nyou avoid running into memory problems.\nB.4 Interacting with the Operating System\nAnother feature of IPython is that it allows you to access the filesystem and operating\nsystem shell. This means, among other things, that you can perform most standard\ncommand-line actions as you would in the Windows or Unix (Linux, macOS) shell\nwithout having to exit IPython. This includes shell commands, changing directories,\nand storing the results of a command in a Python object (list or string). There are also\ncommand aliasing and directory bookmarking features.\nSee Table B-3 for a summary of magic functions and syntax for calling shell com‐\nmands. I’ll briefly visit these features in the next few sections.\n516 \n| \nAppendix B: More on the IPython System",
      "content_length": 1932,
      "extraction_method": "Direct"
    },
    {
      "page_number": 535,
      "chapter": null,
      "content": "Table B-3. IPython system-related commands\nCommand\nDescription\n!cmd\nExecute cmd in the system shell\noutput = !cmd args\nRun cmd and store the stdout in output\n%alias alias_name cmd Define an alias for a system (shell) command\n%bookmark\nUtilize IPython’s directory bookmarking system\n%cd directory\nChange system working directory to passed directory\n%pwd\nReturn to the current system working directory\n%pushd directory\nPlace current directory on stack and change to target directory\n%popd\nChange to directory popped off the top of the stack\n%dirs\nReturn a list containing the current directory stack\n%dhist\nPrint the history of visited directories\n%env\nReturn the system environment variables as a dictionary\n%matplotlib\nConfigure matplotlib integration options\nShell Commands and Aliases\nStarting a line in IPython with an exclamation point !, or bang, tells IPython to\nexecute everything after the bang in the system shell. This means that you can delete\nfiles (using rm or del, depending on your OS), change directories, or execute any\nother process.\nYou can store the console output of a shell command in a variable by assigning the\nexpression escaped with ! to a variable. For example, on my Linux-based machine\nconnected to the internet via Ethernet, I can get my IP address as a Python variable:\nIn [1]: ip_info = !ifconfig wlan0 | grep \"inet \"\nIn [2]: ip_info[0].strip()\nOut[2]: 'inet addr:10.0.0.11  Bcast:10.0.0.255  Mask:255.255.255.0'\nThe returned Python object ip_info is actually a custom list type containing various\nversions of the console output.\nIPython can also substitute in Python values defined in the current environment\nwhen using !. To do this, preface the variable name with the dollar sign $:\nIn [3]: foo = 'test*'\nIn [4]: !ls $foo\ntest4.py  test.py  test.xml\nThe %alias magic function can define custom shortcuts for shell commands. As an\nexample:\nIn [1]: %alias ll ls -l\nMore on the IPython System \n| \n517",
      "content_length": 1932,
      "extraction_method": "Direct"
    },
    {
      "page_number": 536,
      "chapter": null,
      "content": "In [2]: ll /usr\ntotal 332\ndrwxr-xr-x   2 root root  69632 2012-01-29 20:36 bin/\ndrwxr-xr-x   2 root root   4096 2010-08-23 12:05 games/\ndrwxr-xr-x 123 root root  20480 2011-12-26 18:08 include/\ndrwxr-xr-x 265 root root 126976 2012-01-29 20:36 lib/\ndrwxr-xr-x  44 root root  69632 2011-12-26 18:08 lib32/\nlrwxrwxrwx   1 root root      3 2010-08-23 16:02 lib64 -> lib/\ndrwxr-xr-x  15 root root   4096 2011-10-13 19:03 local/\ndrwxr-xr-x   2 root root  12288 2012-01-12 09:32 sbin/\ndrwxr-xr-x 387 root root  12288 2011-11-04 22:53 share/\ndrwxrwsr-x  24 root src    4096 2011-07-17 18:38 src/\nYou can execute multiple commands just as on the command line by separating them\nwith semicolons:\nIn [558]: %alias test_alias (cd examples; ls; cd ..)\nIn [559]: test_alias\nmacrodata.csv  spx.csv\ntips.csv\nYou’ll notice that IPython “forgets” any aliases you define interactively as soon as the\nsession is closed. To create permanent aliases, you will need to use the configuration\nsystem.\nDirectory Bookmark System\nIPython has a directory bookmarking system to enable you to save aliases for com‐\nmon directories so that you can jump around easily. For example, suppose you\nwanted to create a bookmark that points to the supplementary materials for this\nbook:\nIn [6]: %bookmark py4da /home/wesm/code/pydata-book\nOnce you’ve done this, when you use the %cd magic, you can use any bookmarks\nyou’ve defined:\nIn [7]: cd py4da\n(bookmark:py4da) -> /home/wesm/code/pydata-book\n/home/wesm/code/pydata-book\nIf a bookmark name conflicts with a directory name in your current working direc‐\ntory, you can use the -b flag to override and use the bookmark location. Using the -l\noption with %bookmark lists all of your bookmarks:\nIn [8]: %bookmark -l\nCurrent bookmarks:\npy4da -> /home/wesm/code/pydata-book-source\nBookmarks, unlike aliases, are automatically persisted between IPython sessions.\n518 \n| \nAppendix B: More on the IPython System",
      "content_length": 1915,
      "extraction_method": "Direct"
    },
    {
      "page_number": 537,
      "chapter": null,
      "content": "B.5 Software Development Tools\nIn addition to being a comfortable environment for interactive computing and data\nexploration, IPython can also be a useful companion for general Python software\ndevelopment. In data analysis applications, it’s important first to have correct code.\nFortunately, IPython has closely integrated and enhanced the built-in Python pdb\ndebugger. Secondly, you want your code to be fast. For this, IPython has convenient\nintegrated code timing and profiling tools. I will give an overview of these tools in\ndetail here.\nInteractive Debugger\nIPython’s debugger enhances pdb with tab completion, syntax highlighting, and con‐\ntext for each line in exception tracebacks. One of the best times to debug code is right\nafter an error has occurred. The %debug command, when entered immediately after\nan exception, invokes the “postmortem” debugger and drops you into the stack frame\nwhere the exception was raised:\nIn [2]: run examples/ipython_bug.py\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n/home/wesm/code/pydata-book/examples/ipython_bug.py in <module>()\n     13     throws_an_exception()\n     14\n---> 15 calling_things()\n/home/wesm/code/pydata-book/examples/ipython_bug.py in calling_things()\n     11 def calling_things():\n     12     works_fine()\n---> 13     throws_an_exception()\n     14\n     15 calling_things()\n/home/wesm/code/pydata-book/examples/ipython_bug.py in throws_an_exception()\n      7     a = 5\n      8     b = 6\n----> 9     assert(a + b == 10)\n     10\n     11 def calling_things():\nAssertionError:\nIn [3]: %debug\n> /home/wesm/code/pydata-book/examples/ipython_bug.py(9)throws_an_exception()\n      8     b = 6\n----> 9     assert(a + b == 10)\n     10\nipdb>\nMore on the IPython System \n| \n519",
      "content_length": 1837,
      "extraction_method": "Direct"
    },
    {
      "page_number": 538,
      "chapter": null,
      "content": "Once inside the debugger, you can execute arbitrary Python code and explore all of\nthe objects and data (which have been “kept alive” by the interpreter) inside each\nstack frame. By default you start in the lowest level, where the error occurred. By\ntyping u (up) and d (down), you can switch between the levels of the stack trace:\nipdb> u\n> /home/wesm/code/pydata-book/examples/ipython_bug.py(13)calling_things()\n     12     works_fine()\n---> 13     throws_an_exception()\n     14\nExecuting the %pdb command makes IPython automatically invoke the debugger after\nany exception, a mode that many users will find useful.\nIt’s also helpful to use the debugger when developing code, especially when you need\nto set a breakpoint or step through the execution of a function or script to examine its\nbehavior at each step. There are several ways to accomplish this. The first is by using\n%run with the -d flag, which invokes the debugger before executing any code in the\npassed script. You must immediately type s (step) to enter the script:\nIn [5]: run -d examples/ipython_bug.py\nBreakpoint 1 at /home/wesm/code/pydata-book/examples/ipython_bug.py:1\nNOTE: Enter 'c' at the ipdb>  prompt to start your script.\n> <string>(1)<module>()\nipdb> s\n--Call--\n> /home/wesm/code/pydata-book/examples/ipython_bug.py(1)<module>()\n1---> 1 def works_fine():\n      2     a = 5\n      3     b = 6\nAfter this point, it’s up to you how you want to work your way through the file. For\nexample, in the preceding exception, we could set a breakpoint right before calling\nthe works_fine function, and run the script until we reach the breakpoint by typing c\n(continue):\nipdb> b 12\nipdb> c\n> /home/wesm/code/pydata-book/examples/ipython_bug.py(12)calling_things()\n     11 def calling_things():\n2--> 12     works_fine()\n     13     throws_an_exception()\nAt this point, you can step into works_fine() or execute works_fine() by typing n\n(next) to advance to the next line:\nipdb> n\n> /home/wesm/code/pydata-book/examples/ipython_bug.py(13)calling_things()\n2    12     works_fine()\n520 \n| \nAppendix B: More on the IPython System",
      "content_length": 2092,
      "extraction_method": "Direct"
    },
    {
      "page_number": 539,
      "chapter": null,
      "content": "---> 13     throws_an_exception()\n     14\nThen, we could step into throws_an_exception and advance to the line where the\nerror occurs and look at the variables in the scope. Note that debugger commands\ntake precedence over variable names; in such cases, preface the variables with ! to\nexamine their contents:\nipdb> s\n--Call--\n> /home/wesm/code/pydata-book/examples/ipython_bug.py(6)throws_an_exception()\n      5\n----> 6 def throws_an_exception():\n      7     a = 5\nipdb> n\n> /home/wesm/code/pydata-book/examples/ipython_bug.py(7)throws_an_exception()\n      6 def throws_an_exception():\n----> 7     a = 5\n      8     b = 6\nipdb> n\n> /home/wesm/code/pydata-book/examples/ipython_bug.py(8)throws_an_exception()\n      7     a = 5\n----> 8     b = 6\n      9     assert(a + b == 10)\nipdb> n\n> /home/wesm/code/pydata-book/examples/ipython_bug.py(9)throws_an_exception()\n      8     b = 6\n----> 9     assert(a + b == 10)\n     10\nipdb> !a\n5\nipdb> !b\n6\nIn my experience, developing proficiency with the interactive debugger takes time\nand practice. See Table B-4 for a full catalog of the debugger commands. If you are\naccustomed to using an IDE, you might find the terminal-driven debugger to be a\nbit unforgiving at first, but that will improve in time. Some of the Python IDEs have\nexcellent GUI debuggers, so most users can find something that works for them.\nTable B-4. Python debugger commands\nCommand\nAction\nh(elp)\nDisplay command list\nhelp command\nShow documentation for command\nc(ontinue)\nResume program execution\nMore on the IPython System \n| \n521",
      "content_length": 1547,
      "extraction_method": "Direct"
    },
    {
      "page_number": 540,
      "chapter": null,
      "content": "Command\nAction\nq(uit)\nExit debugger without executing any more code\nb(reak) number\nSet breakpoint at number in current file\nb path/to/file.py:number\nSet breakpoint at line number in specified file\ns(tep)\nStep into function call\nn(ext)\nExecute current line and advance to next line at current level\nu(p)/d(own)\nMove up/down in function call stack\na(rgs)\nShow arguments for current function\ndebug statement\nInvoke statement statement in new (recursive) debugger\nl(ist) statement\nShow current position and context at current level of stack\nw(here)\nPrint full stack trace with context at current position\nOther ways to use the debugger\nThere are a couple of other useful ways to invoke the debugger. The first is by using\na special set_trace function (named after pdb.set_trace), which is basically a “poor\nman’s breakpoint.” Here are two small recipes you might want to put somewhere for\nyour general use (potentially adding them to your IPython profile, as I do):\nfrom IPython.core.debugger import Pdb\ndef set_trace():\n    Pdb(.set_trace(sys._getframe().f_back)\ndef debug(f, *args, **kwargs):\n    pdb = Pdb()\n    return pdb.runcall(f, *args, **kwargs)\nThe first function, set_trace, provides a convenient way to put a breakpoint some‐\nwhere in your code. You can use a set_trace in any part of your code that you want\nto temporarily stop to examine it more closely (e.g., right before an exception occurs):\nIn [7]: run examples/ipython_bug.py\n> /home/wesm/code/pydata-book/examples/ipython_bug.py(16)calling_things()\n     15     set_trace()\n---> 16     throws_an_exception()\n     17\nTyping c (continue) will cause the code to resume normally with no harm done.\nThe debug function we just looked at enables you to invoke the interactive debugger\neasily on an arbitrary function call. Suppose we had written a function like the\nfollowing, and we wished to step through its logic:\ndef f(x, y, z=1):\n    tmp = x + y\n    return tmp / z\n522 \n| \nAppendix B: More on the IPython System",
      "content_length": 1975,
      "extraction_method": "Direct"
    },
    {
      "page_number": 541,
      "chapter": null,
      "content": "Ordinarily using f would look like f(1, 2, z=3). To instead step into f, pass f as\nthe first argument to debug, followed by the positional and keyword arguments to be\npassed to f:\nIn [6]: debug(f, 1, 2, z=3)\n> <ipython-input>(2)f()\n      1 def f(x, y, z):\n----> 2     tmp = x + y\n      3     return tmp / z\nipdb>\nThese two recipes have saved me a lot of time over the years.\nLastly, the debugger can be used in conjunction with %run. By running a script with\n%run -d, you will be dropped directly into the debugger, ready to set any breakpoints\nand start the script:\nIn [1]: %run -d examples/ipython_bug.py\nBreakpoint 1 at /home/wesm/code/pydata-book/examples/ipython_bug.py:1\nNOTE: Enter 'c' at the ipdb>  prompt to start your script.\n> <string>(1)<module>()\nipdb>\nAdding -b with a line number starts the debugger with a breakpoint set already:\nIn [2]: %run -d -b2 examples/ipython_bug.py\nBreakpoint 1 at /home/wesm/code/pydata-book/examples/ipython_bug.py:2\nNOTE: Enter 'c' at the ipdb>  prompt to start your script.\n> <string>(1)<module>()\nipdb> c\n> /home/wesm/code/pydata-book/examples/ipython_bug.py(2)works_fine()\n      1 def works_fine():\n1---> 2     a = 5\n      3     b = 6\nipdb>\nTiming Code: %time and %timeit\nFor larger-scale or longer-running data analysis applications, you may wish to meas‐\nure the execution time of various components or of individual statements or function\ncalls. You may want a report of which functions are taking up the most time in\na complex process. Fortunately, IPython enables you to get this information conven‐\niently while you are developing and testing your code.\nTiming code by hand using the built-in time module and its functions, time.clock\nand time.time, is often tedious and repetitive, as you must write the same uninterest‐\ning boilerplate code:\nMore on the IPython System \n| \n523",
      "content_length": 1831,
      "extraction_method": "Direct"
    },
    {
      "page_number": 542,
      "chapter": null,
      "content": "import time\nstart = time.time()\nfor i in range(iterations):\n    # some code to run here\nelapsed_per = (time.time() - start) / iterations\nSince this is such a common operation, IPython has two magic functions, %time and\n%timeit, to automate this process for you.\n%time runs a statement once, reporting the total execution time. Suppose we had\na large list of strings, and we wanted to compare different methods of selecting all\nstrings starting with a particular prefix. Here is a list of 600,000 strings and two\nidentical methods of selecting only the ones that start with 'foo':\n# a very large list of strings\nIn [11]: strings = ['foo', 'foobar', 'baz', 'qux',\n   ....:            'python', 'Guido Van Rossum'] * 100000\nIn [12]: method1 = [x for x in strings if x.startswith('foo')]\nIn [13]: method2 = [x for x in strings if x[:3] == 'foo']\nIt looks like they should be about the same performance-wise, right? We can check\nfor sure using %time:\nIn [14]: %time method1 = [x for x in strings if x.startswith('foo')]\nCPU times: user 52.5 ms, sys: 0 ns, total: 52.5 ms\nWall time: 52.1 ms\nIn [15]: %time method2 = [x for x in strings if x[:3] == 'foo']\nCPU times: user 65.3 ms, sys: 0 ns, total: 65.3 ms\nWall time: 64.8 ms\nThe Wall time (short for “wall-clock time”) is the main number of interest. From\nthese timings, we can infer that there is some performance difference, but it’s not\na very precise measurement. If you try %time-ing those statements multiple times\nyourself, you’ll find that the results are somewhat variable. To get a more precise\nmeasurement, use the %timeit magic function. Given an arbitrary statement, it has\na heuristic to run a statement multiple times to produce a more accurate average\nruntime (these results may be different on your system):\nIn [563]: %timeit [x for x in strings if x.startswith('foo')]\n10 loops, best of 3: 159 ms per loop\nIn [564]: %timeit [x for x in strings if x[:3] == 'foo']\n10 loops, best of 3: 59.3 ms per loop\nThis seemingly innocuous example illustrates that it is worth understanding the\nperformance characteristics of the Python standard library, NumPy, pandas, and\nother libraries used in this book. In larger-scale data analysis applications, those\nmilliseconds will start to add up!\n524 \n| \nAppendix B: More on the IPython System",
      "content_length": 2288,
      "extraction_method": "Direct"
    },
    {
      "page_number": 543,
      "chapter": null,
      "content": "%timeit is especially useful for analyzing statements and functions with very short\nexecution times, even at the level of microseconds (millionths of a second) or\nnanoseconds (billionths of a second). These may seem like insignificant amounts\nof time, but of course a 20-microsecond function invoked 1 million times takes 15\nseconds longer than a 5-microsecond function. In the preceding example, we could\nvery directly compare the two string operations to understand their performance\ncharacteristics:\nIn [565]: x = 'foobar'\nIn [566]: y = 'foo'\nIn [567]: %timeit x.startswith(y)\n1000000 loops, best of 3: 267 ns per loop\nIn [568]: %timeit x[:3] == y\n10000000 loops, best of 3: 147 ns per loop\nBasic Profiling: %prun and %run -p\nProfiling code is closely related to timing code, except it is concerned with determin‐\ning where time is spent. The main Python profiling tool is the cProfile module,\nwhich is not specific to IPython at all. cProfile executes a program or any arbitrary\nblock of code while keeping track of how much time is spent in each function.\nA common way to use cProfile is on the command line, running an entire program\nand outputting the aggregated time per function. Suppose we had a script that does\nsome linear algebra in a loop (computing the maximum absolute eigenvalues of a\nseries of 100 × 100 matrices):\nimport numpy as np\nfrom numpy.linalg import eigvals\ndef run_experiment(niter=100):\n    K = 100\n    results = []\n    for _ in range(niter):\n        mat = np.random.standard_normal((K, K))\n        max_eigenvalue = np.abs(eigvals(mat)).max()\n        results.append(max_eigenvalue)\n    return results\nsome_results = run_experiment()\nprint('Largest one we saw: {0}'.format(np.max(some_results)))\nYou can run this script through cProfile using the following in the command line:\npython -m cProfile cprof_example.py\nMore on the IPython System \n| \n525",
      "content_length": 1876,
      "extraction_method": "Direct"
    },
    {
      "page_number": 544,
      "chapter": null,
      "content": "If you try that, you’ll find that the output is sorted by function name. This makes it a\nbit hard to get an idea of where the most time is spent, so it’s useful to specify a sort\norder using the -s flag:\n$ python -m cProfile -s cumulative cprof_example.py\nLargest one we saw: 11.923204422\n    15116 function calls (14927 primitive calls) in 0.720 seconds\nOrdered by: cumulative time\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\n     1    0.001    0.001    0.721    0.721 cprof_example.py:1(<module>)\n   100    0.003    0.000    0.586    0.006 linalg.py:702(eigvals)\n   200    0.572    0.003    0.572    0.003 {numpy.linalg.lapack_lite.dgeev}\n     1    0.002    0.002    0.075    0.075 __init__.py:106(<module>)\n   100    0.059    0.001    0.059    0.001 {method 'randn')\n     1    0.000    0.000    0.044    0.044 add_newdocs.py:9(<module>)\n     2    0.001    0.001    0.037    0.019 __init__.py:1(<module>)\n     2    0.003    0.002    0.030    0.015 __init__.py:2(<module>)\n     1    0.000    0.000    0.030    0.030 type_check.py:3(<module>)\n     1    0.001    0.001    0.021    0.021 __init__.py:15(<module>)\n     1    0.013    0.013    0.013    0.013 numeric.py:1(<module>)\n     1    0.000    0.000    0.009    0.009 __init__.py:6(<module>)\n     1    0.001    0.001    0.008    0.008 __init__.py:45(<module>)\n   262    0.005    0.000    0.007    0.000 function_base.py:3178(add_newdoc)\n   100    0.003    0.000    0.005    0.000 linalg.py:162(_assertFinite)\n   ...\nOnly the first 15 rows of the output are shown. It’s easiest to read by scanning down\nthe cumtime column to see how much total time was spent inside each function. Note\nthat if a function calls some other function, the clock does not stop running. cProfile\nrecords the start and end time of each function call and uses that to produce the\ntiming.\nIn addition to the command-line usage, cProfile can also be used programmatically\nto profile arbitrary blocks of code without having to run a new process. IPython has a\nconvenient interface to this capability using the %prun command and the -p option to\n%run. %prun takes the same “command-line options” as cProfile but will profile an\narbitrary Python statement instead of a whole .py file:\nIn [4]: %prun -l 7 -s cumulative run_experiment()\n         4203 function calls in 0.643 seconds\nOrdered by: cumulative time\nList reduced from 32 to 7 due to restriction <7>\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\n     1    0.000    0.000    0.643    0.643 <string>:1(<module>)\n     1    0.001    0.001    0.643    0.643 cprof_example.py:4(run_experiment)\n   100    0.003    0.000    0.583    0.006 linalg.py:702(eigvals)\n   200    0.569    0.003    0.569    0.003 {numpy.linalg.lapack_lite.dgeev}\n526 \n| \nAppendix B: More on the IPython System",
      "content_length": 2804,
      "extraction_method": "Direct"
    },
    {
      "page_number": 545,
      "chapter": null,
      "content": "100    0.058    0.001    0.058    0.001 {method 'randn'}\n   100    0.003    0.000    0.005    0.000 linalg.py:162(_assertFinite)\n   200    0.002    0.000    0.002    0.000 {method 'all' of 'numpy.ndarray'}\nSimilarly, calling %run -p -s cumulative cprof_example.py has the same effect as\nthe command-line approach, except you never have to leave IPython.\nIn the Jupyter notebook, you can use the %%prun magic (two % signs) to profile an\nentire code block. This pops up a separate window with the profile output. This can\nbe useful in getting possibly quick answers to questions like, “Why did that code\nblock take so long to run?”\nThere are other tools available that help make profiles easier to understand when you\nare using IPython or Jupyter. One of these is SnakeViz, which produces an interactive\nvisualization of the profile results using D3.js.\nProfiling a Function Line by Line\nIn some cases, the information you obtain from %prun (or another cProfile-based\nprofile method) may not tell the whole story about a function’s execution time, or\nit may be so complex that the results, aggregated by function name, are hard to\ninterpret. For this case, there is a small library called line_profiler (obtainable via\nPyPI or one of the package management tools). It contains an IPython extension\nenabling a new magic function %lprun that computes a line-by-line-profiling of\none or more functions. You can enable this extension by modifying your IPython\nconfiguration (see the IPython documentation or the section on configuration later in\nthis appendix) to include the following line:\n# A list of dotted module names of IPython extensions to load.\nc.InteractiveShellApp.extensions = ['line_profiler']\nYou can also run the command:\n%load_ext line_profiler\nline_profiler can be used programmatically (see the full documentation), but it\nis perhaps most powerful when used interactively in IPython. Suppose you had a\nmodule prof_mod with the following code doing some NumPy array operations (if\nyou want to reproduce this example, put this code into a new file prof_mod.py):\nfrom numpy.random import randn\ndef add_and_sum(x, y):\n    added = x + y\n    summed = added.sum(axis=1)\n    return summed\ndef call_function():\n    x = randn(1000, 1000)\nMore on the IPython System \n| \n527",
      "content_length": 2275,
      "extraction_method": "Direct"
    },
    {
      "page_number": 546,
      "chapter": null,
      "content": "y = randn(1000, 1000)\n    return add_and_sum(x, y)\nIf we wanted to understand the performance of the add_and_sum function, %prun\ngives us the following:\nIn [569]: %run prof_mod\nIn [570]: x = randn(3000, 3000)\nIn [571]: y = randn(3000, 3000)\nIn [572]: %prun add_and_sum(x, y)\n         4 function calls in 0.049 seconds\n   Ordered by: internal time\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n        1    0.036    0.036    0.046    0.046 prof_mod.py:3(add_and_sum)\n        1    0.009    0.009    0.009    0.009 {method 'sum' of 'numpy.ndarray'}\n        1    0.003    0.003    0.049    0.049 <string>:1(<module>)\nThis is not especially enlightening. With the line_profiler IPython extension acti‐\nvated, a new command %lprun is available. The only difference in usage is that we\nmust instruct %lprun which function or functions we wish to profile. The general\nsyntax is:\n%lprun -f func1 -f func2 statement_to_profile\nIn this case, we want to profile add_and_sum, so we run:\nIn [573]: %lprun -f add_and_sum add_and_sum(x, y)\nTimer unit: 1e-06 s\nFile: prof_mod.py\nFunction: add_and_sum at line 3\nTotal time: 0.045936 s\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     3                                           def add_and_sum(x, y):\n     4         1        36510  36510.0     79.5      added = x + y\n     5         1         9425   9425.0     20.5      summed = added.sum(axis=1)\n     6         1            1      1.0      0.0      return summed\nThis can be much easier to interpret. In this case, we profiled the same function\nwe used in the statement. Looking at the preceding module code, we could call\ncall_function and profile that as well as add_and_sum, thus getting a full picture of\nthe performance of the code:\nIn [574]: %lprun -f add_and_sum -f call_function call_function()\nTimer unit: 1e-06 s\nFile: prof_mod.py\nFunction: add_and_sum at line 3\nTotal time: 0.005526 s\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n528 \n| \nAppendix B: More on the IPython System",
      "content_length": 2160,
      "extraction_method": "Direct"
    },
    {
      "page_number": 547,
      "chapter": null,
      "content": "3                                           def add_and_sum(x, y):\n     4         1         4375   4375.0     79.2      added = x + y\n     5         1         1149   1149.0     20.8      summed = added.sum(axis=1)\n     6         1            2      2.0      0.0      return summed\nFile: prof_mod.py\nFunction: call_function at line 8\nTotal time: 0.121016 s\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     8                                           def call_function():\n     9         1        57169  57169.0     47.2      x = randn(1000, 1000)\n    10         1        58304  58304.0     48.2      y = randn(1000, 1000)\n    11         1         5543   5543.0      4.6      return add_and_sum(x, y)\nAs a general rule of thumb, I tend to prefer %prun (cProfile) for “macro” profiling,\nand %lprun (line_profiler) for “micro” profiling. It’s worthwhile to have a good\nunderstanding of both tools.\nThe reason that you must explicitly specify the names of the func‐\ntions you want to profile with %lprun is that the overhead of\n“tracing” the execution time of each line is substantial. Tracing\nfunctions that are not of interest has the potential to significantly\nalter the profile results.\nB.6 Tips for Productive Code Development Using IPython\nWriting code in a way that makes it convenient to develop, debug, and ultimately\nuse interactively may be a paradigm shift for many users. There are procedural\ndetails like code reloading that may require some adjustment, as well as coding style\nconcerns.\nTherefore, implementing most of the strategies described in this section is more of an\nart than a science and will require some experimentation on your part to determine\na way to write your Python code that is effective for you. Ultimately you want to\nstructure your code in a way that makes it convenient to use iteratively and be able to\nexplore the results of running a program or function as effortlessly as possible. I have\nfound software designed with IPython in mind to be easier to work with than code\nintended only to be run as as standalone command-line application. This becomes\nespecially important when something goes wrong and you have to diagnose an error\nin code that you or someone else might have written months or years beforehand.\nReloading Module Dependencies\nIn Python, when you type import some_lib, the code in some_lib is executed,\nand all the variables, functions, and imports defined within are stored in the newly\ncreated some_lib module namespace. The next time you use import some_lib,\nMore on the IPython System \n| \n529",
      "content_length": 2628,
      "extraction_method": "Direct"
    },
    {
      "page_number": 548,
      "chapter": null,
      "content": "1 Since a module or package may be imported in many different places in a particular program, Python caches\na module’s code the first time it is imported rather than executing the code in the module every time.\nOtherwise, modularity and good code organization could potentially cause inefficiency in an application.\nyou will get a reference to the existing module namespace. The potential difficulty\nin interactive IPython code development comes when you, say, %run a script that\ndepends on some other module where you may have made changes. Suppose I had\nthe following code in test_script.py:\nimport some_lib\nx = 5\ny = [1, 2, 3, 4]\nresult = some_lib.get_answer(x, y)\nIf you were to execute %run test_script.py then modify some_lib.py, the next time\nyou execute %run test_script.py you will still get the old version of some_lib.py\nbecause of Python’s “load-once” module system. This behavior differs from some\nother data analysis environments, like MATLAB, which automatically propagate code\nchanges.1 To cope with this, you have a couple of options. The first way is to use the\nreload function in the importlib module in the standard library:\nimport some_lib\nimport importlib\nimportlib.reload(some_lib)\nThis attempts to give you a fresh copy of some_lib.py every time you run test_script.py\n(but there are some scenarios where it will not). Obviously, if the dependencies go\ndeeper, it might be a bit tricky to be inserting usages of reload all over the place.\nFor this problem, IPython has a special dreload function (not a magic function)\nfor “deep” (recursive) reloading of modules. If I were to run some_lib.py then use dre\nload(some_lib), it will attempt to reload some_lib as well as all of its dependencies.\nThis will not work in all cases, unfortunately, but when it does, it beats having to\nrestart IPython.\nCode Design Tips\nThere’s no simple recipe for this, but here are some high-level principles I have found\neffective in my own work.\nKeep relevant objects and data alive\nIt’s not unusual to see a program written for the command line with a structure\nsomewhat like the following:\n530 \n| \nAppendix B: More on the IPython System",
      "content_length": 2142,
      "extraction_method": "Direct"
    },
    {
      "page_number": 549,
      "chapter": null,
      "content": "from my_functions import g\ndef f(x, y):\n    return g(x + y)\ndef main():\n    x = 6\n    y = 7.5\n    result = x + y\nif __name__ == '__main__':\n    main()\nDo you see what might go wrong if we were to run this program in IPython? After\nit’s done, none of the results or objects defined in the main function will be accessible\nin the IPython shell. A better way is to have whatever code is in main execute directly\nin the module’s global namespace (or in the if __name__ == '__main__': block, if\nyou want the module to also be importable). That way, when you %run the code, you’ll\nbe able to look at all of the variables defined in main. This is equivalent to defining\ntop-level variables in cells in the Jupyter notebook.\nFlat is better than nested\nDeeply nested code makes me think about the many layers of an onion. When testing\nor debugging a function, how many layers of the onion must you peel back in order\nto reach the code of interest? The idea that “flat is better than nested” is a part of\nthe Zen of Python, and it applies generally to developing code for interactive use as\nwell. Making functions and classes as decoupled and modular as possible makes them\neasier to test (if you are writing unit tests), debug, and use interactively.\nOvercome a fear of longer files\nIf you come from a Java (or another such language) background, you may have been\ntold to keep files short. In many languages, this is sound advice; long length is usually\na bad “code smell,” indicating refactoring or reorganization may be necessary. How‐\never, while developing code using IPython, working with 10 small but interconnected\nfiles (under, say, 100 lines each) is likely to cause you more headaches in general\nthan 2 or 3 longer files. Fewer files means fewer modules to reload and less jumping\nbetween files while editing, too. I have found maintaining larger modules, each with\nhigh internal cohesion (the code all relates to solving the same kinds of problems),\nto be much more useful and Pythonic. After iterating toward a solution, of course it\nsometimes will make sense to refactor larger files into smaller ones.\nObviously, I don’t support taking this argument to the extreme, which would to be\nto put all of your code in a single monstrous file. Finding a sensible and intuitive\nmodule and package structure for a large codebase often takes a bit of work, but\nit is especially important to get right in teams. Each module should be internally\nMore on the IPython System \n| \n531",
      "content_length": 2472,
      "extraction_method": "Direct"
    },
    {
      "page_number": 550,
      "chapter": null,
      "content": "cohesive, and it should be as obvious as possible where to find functions and classes\nresponsible for each area of functionality.\nB.7 Advanced IPython Features\nMaking full use of the IPython system may lead you to write your code in a slightly\ndifferent way, or to dig into the configuration.\nProfiles and Configuration\nMost aspects of the appearance (colors, prompt, spacing between lines, etc.) and\nbehavior of the IPython and Jupyter environments are configurable through an\nextensive configuration system. Here are some things you can do via configuration:\n• Change the color scheme\n•\n• Change how the input and output prompts look, or remove the blank line after\n•\nOut and before the next In prompt\n• Execute an arbitrary list of Python statements (e.g., imports that you use all the\n•\ntime or anything else you want to happen each time you launch IPython)\n• Enable always-on IPython extensions, like the %lprun magic in line_profiler\n•\n• Enable Jupyter extensions\n•\n• Define your own magics or system aliases\n•\nConfigurations for the IPython shell are specified in special ipython_config.py files,\nwhich are usually found in the .ipython/ directory in your user home directory.\nConfiguration is performed based on a particular profile. When you start IPython\nnormally, you load up, by default, the default profile, stored in the profile_default\ndirectory. Thus, on my Linux OS, the full path to my default IPython configuration\nfile is:\n/home/wesm/.ipython/profile_default/ipython_config.py\nTo initialize this file on your system, run this in the terminal:\nipython profile create default\nI’ll spare you the complete details of what’s in this file. Fortunately, it has comments\ndescribing what each configuration option is for, so I will leave it to the reader\nto tinker and customize. One additional useful feature is that it’s possible to have\nmultiple profiles. Suppose you wanted to have an alternative IPython configuration\ntailored for a particular application or project. Creating a new profile involves typing\nthe following:\nipython profile create secret_project\n532 \n| \nAppendix B: More on the IPython System",
      "content_length": 2122,
      "extraction_method": "Direct"
    },
    {
      "page_number": 551,
      "chapter": null,
      "content": "Once you’ve done this, edit the config files in the newly created profile_secret_project\ndirectory and then launch IPython, like so:\n$ ipython --profile=secret_project\nPython 3.8.0 | packaged by conda-forge | (default, Nov 22 2019, 19:11:19)\nType 'copyright', 'credits' or 'license' for more information\nIPython 7.22.0 -- An enhanced Interactive Python. Type '?' for help.\nIPython profile: secret_project\nAs always, the online IPython documentation is an excellent resource for more on\nprofiles and configuration.\nConfiguration for Jupyter works a little differently because you can use its notebooks\nwith languages other than Python. To create an analogous Jupyter config file, run:\njupyter notebook --generate-config\nThis writes a default config file to the .jupyter/jupyter_notebook_config.py directory in\nyour home directory. After editing this to suit your needs, you may rename it to a\ndifferent file, like:\n$ mv ~/.jupyter/jupyter_notebook_config.py ~/.jupyter/my_custom_config.py\nWhen launching Jupyter, you can then add the --config argument:\njupyter notebook --config=~/.jupyter/my_custom_config.py\nB.8 Conclusion\nAs you work through the code examples in this book and grow your skills as a\nPython programmer, I encourage you to keep learning about the IPython and Jupyter\necosystems. Since these projects have been designed to assist user productivity, you\nmay discover tools that enable you to do your work more easily than using the\nPython language and its computational libraries by themselves.\nYou can also find a wealth of interesting Jupyter notebooks on the nbviewer website.\nMore on the IPython System \n| \n533",
      "content_length": 1628,
      "extraction_method": "Direct"
    },
    {
      "page_number": 552,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 553,
      "chapter": null,
      "content": "Index\nSymbols\n! (exclamation point) for shell commands, 516\n!= (not equal to), 32\n\" (double quote)\nmultiline strings, 35\nstring literals declared, 35\n# (hash mark) for comment, 27\n$ (dollar sign) for shell environment, 517\n% (percent)\ndatetime formatting, 359\nIPython magic commands, 510-511\n%bookmark, 518\nCtrl-C to interrupt running code, 513\n%debug, 519-523\nexecuting code from clipboard, 513\n%lprun, 527-529\noperating system commands, 516\n%prun, 525-527\n%run, 19, 512\n%run -p, 525-527\n%time and %timeit, 523-525\n& (ampersand)\nAND, 32\nNumPy ndarrays, 99\nintersection of two sets, 60\n' (single quote)\nmultiline strings, 35\nstring literals declared, 35\n( ) parentheses\ncalling functions and object methods, 28\nintervals open (exclusive), 216\nmethod called on results, 218\ntuples, 47-50\ntuples of exception types, 75\n* (asterisk)\nmultiplication, 32\ntuple by integer, 49\ntwo-dimensional arrays, 116\nnamespace search in IPython, 26\nraising to the power, 32\nrest in tuples, 50\n+ (plus) operator, 32\nadding strings, 37\nlists, 53\nPatsy’s formulas, 408\nI() wrapper for addition, 412\ntimedelta type, 42\ntuple concatenation, 49\n- (minus) operator, 32\nsets, 60\ntimedelta type, 41\n/ (slash) division operator, 32\nfloor (//), 35\n: (colon)\narrays returned in reverse order, 497\ncode blocks, 26\ndictionaries, 55\n; (semicolon) for multiple statements, 27\n< (less than) operator, 32\nset elements, 60\n= (equal sign)\ntest for equality, 32\nvariable assignment, 28\nunpacking tuples, 49\n> (greater than) operator, 32\nset elements, 60\n? (question mark)\nnamespace search in IPython, 26\n535",
      "content_length": 1567,
      "extraction_method": "Direct"
    },
    {
      "page_number": 554,
      "chapter": null,
      "content": "object introspection in IPython, 25\n@ (at) infix operator, 103\n[ ] (square brackets)\narrays returned in reverse order, 497\n(see also arrays)\nintervals closed (inclusive), 216\nlist definitions, 51\nslicing lists, 54\nloc and iloc operators, 143, 145\nseries indexing, 142\nstring element index, 234\nstring slicing, 234\ntuple elements, 48\n\\ (backslash) escaping in strings, 37\n\\n (newline character) counted, 36\n^ (caret) operator, 32\nsymmetric difference between sets, 60\n_ (underscore)\ndata types with trailing underscores, 475\ntab completion and, 24\nunwanted variables, 50\n{ } (curly braces)\ncode blocks surrounded by, 26\ndictionaries, 55\nformatting strings, 37\nsets, 59\n| (vertical bar)\nOR, 32\nNumPy ndarrays, 99\nunion of two sets, 60\n~ (tilde) as NumPy negation operator, 98\nA\naccumulate() (NumPy ufunc), 491\nadd() (NumPy ufunc), 105\nadd() (sets), 60\naddition (see plus (+) operator)\nadd_constant() (statsmodels), 416\nadd_subplot() (matplotlib), 283\nAxesSubplot objects returned, 285\naggregate() or agg(), 330, 349\naggregation of data, 329\nabout, 319\nagg(), 330, 349\ncolumn-wise, 331\ndescribe(), 331, 337\ndownsampling, 388-391\ngroupby() methods, 329\n(see also group operations)\nexample of group operation, 320\nindexing a GroupBy object, 326\nmoving window functions, 396-403\nmultiple functions, 331\nNumPy array methods, 111\nopen-high-low-close resampling, 391\nperformance and, 331\nperformance of aggregation functions, 350\npivot tables, 352\nabout, 351\ncross-tabulations, 354\nhierarchical indexing, 249\nreturning without row indexes, 335\nupsampling, 391\n%alias command, 517\nAltair for visualization, 317\nampersand (&)\nAND, 32\nintersection of two sets, 60\nNumPy ndarrays, 99\nanchored offsets in frequencies, 371\nshifting dates with offsets, 373\nannotate() (matplotlib), 294\nanonymous (lambda) functions, 70\nApache Parquet\nread_parquet(), 194\nremote servers for processing data, 197\nAPIs\nimporting, 32\n(see also modules)\nmatplotlib primer, 282-297\nweb API interactions, 197-199\napply() (GroupBy), 335\napply() (moving window functions), 402\narange() (NumPy), 85, 88\nargsort() (NumPy), 497\narray() (NumPy), 86\narrays\naggregations defined, 329\n(see also aggregation of data)\nassociative arrays, 55\n(see also dictionaries)\ndates, 360\nmoving window functions, 396-403\nbinary, 401\nexpanding window mean, 398\nexponentially weighted functions, 399\nrolling operator, 396, 398\nuser-defined, 402\n536 \n| \nIndex",
      "content_length": 2393,
      "extraction_method": "Direct"
    },
    {
      "page_number": 555,
      "chapter": null,
      "content": "NumPy ndarrays, 85\n(see also ndarrays)\npandas Series, 5, 124\n(see also Series)\nPandasArray, 125\nPeriodIndex created from, 385\nstriding information of ndarrays, 473\nstructured ndarrays, 493\n(see also structured ndarrays)\nasfreq(), 380, 391\nassociative arrays, 55\n(see also dictionaries)\nasterisk (*)\nmultiplication, 32\ntuple by integer, 49\ntwo-dimensional arrays, 116\nnamespace search in IPython, 26\nraising to the power, 32\nrest in tuples, 50\nastype() for extension data types, 226, 238\nat (@) infix operator, 103\nattributes of Python objects, 30\nAxesSubplot objects, 285\nB\nbaby names in US dataset, 443-456\nnaming trends analysis, 448-456\ngender and naming, 455\nincrease in diversity, 449\nlast letter revolution, 452\nbackslash (\\) escaping in strings, 37\nbang (!) for shell commands, 516\nbar plots\npandas, 301-306\nseaborn, 306\nvalue_counts() for, 304\nbarplot() (seaborn), 306\nbase frequencies, 370\nBeazley, David, 18\nbinary data formats\nabout non-pickle formats, 194\nHDF5 format, 195-197, 504\nMicrosoft Excel files, 194\nndarrays saved, 116\nmemory-mapped files, 503\nParquet (Apache)\nread_parquet(), 194\nremote servers for processing data, 197\npickle format, 193\ncaution about, 193\nplots saved to files, 296\nbinary operators of Python, 32\nbinary ufuncs, 105\nmethods, 106, 490-492\nbinding of variables, 29\nbinning data, 215, 338-340, 500\nBitly links with .gov or .mil dataset, 425-435\ncounting time zones in Python, 426\ncounting time zones with pandas, 428-435\nBokeh for visualization, 317\nbook website, 9, 15\n%bookmark command, 518\nbool scalar type, 34, 39\ntype casting, 40\nBooleanDtype extension data type, 226\nbox plots, 315\nbraces (see curly braces)\nbrackets (see square brackets)\nbreak keyword\nfor loops, 43\nwhile loops, 44\nbreakpoint in code, 522\nbroadcasting, 484-489\nabout, 92, 156\nperformance tip, 505\nbroadcasting rule, 485\nover other axes, 487\nsetting array values via, 489\nbuild_design_matrices() (Patsy), 411\nbytes scalar type, 34, 38\nC\nC/C#/C++ languages\nabout legacy libraries, 3\nHDF5 C library, 195\nNumPy arrays, 4, 83, 89\nperformance tip, 505\nCython for C-like performance, 505\ncard deck draw example, 343\ncaret (^) operator, 32\nsymmetric difference between sets, 60\ncasting and conversions of variables, 30\ncat (Unix) to print file to screen, 177, 184\ncategorical data\nany immutable data types, 240\nbackground, 236\ncategory codes, 237\nCategorical extension data type, 237\nIndex \n| \n537",
      "content_length": 2401,
      "extraction_method": "Direct"
    },
    {
      "page_number": 556,
      "chapter": null,
      "content": "cut() and groupby(), 339-340\nmodeling nonnumeric column, 407\nsequences into, 239\ncomputations with, 240\nperformance improvement with, 241\ndict() for code and category mapping, 239\nmeaningful order, 239\nmethods, 242-245\nPatsy’s formulas, 412-415\nvisualizing with facet grids, 314-316\nCategorical extension data type, 237\ncomputations with, 240\nperformance improvement with, 241\ncut() and groupby(), 339-340\nmodeling nonnumeric column, 407\nsequences into, 239\nCategoricalDtype extension data type, 226\ncatplot() (seaborn), 314\ncenter() (Patsy), 411\nchain() (itertools), 73\nCircle() (matplotlib), 295\nclear() for sets, 60\nclipboard code executed, 513\nclose() an open file, 77\nclosed property, 79\ncollections\nbuilt-in sequence functions, 62\ndictionaries, 55-59\nlist comprehensions, 63\nsets, 59-61\ncolon (:)\narrays returned in reverse order, 497\ncode blocks, 26\ndictionaries, 55\ncolors for plot(), 288\ncombinations() (itertools), 73\ncombine_first(), 269\ncommand history of IPython, 514-516\ninput and output variables, 515\nsearching and reusing, 514\ncomment preceded by hash mark (#), 27\ncomprehensions for lists, sets, dictionaries, 63\nnested, 64\nconcat() (pandas), 264-268\nconcatenate() for ndarrays, 263, 479\nr_ and c_ objects, 480\nconda\nabout Miniconda, 9\n(see also Miniconda)\nactivate to activate environment, 11\ncreate to create new environment, 11\ninstall, 12\nnecessary packages, 11\nrecommended over pip, 12\nupdating packages, 12\nconfiguration of IPython, 532\nconfiguration of Jupyter notebooks, 533\nconsole output to variable, 517\ncontiguous memory importance, 505\ncontinue keyword in for loops, 43\ncontrol flow in Python, 42-45\nNumPy array vectorized version, 110\ncorr(), 169, 346, 401\ncorrelation and covariance with pandas,\n168-170\nbinary moving window functions, 401\ngroup weighted average and correlation,\n344-346\ncount()\nnewlines in multiline string, 36\nnonnull values in groups, 324\nsubstring occurrences, 228\ntuple method, 50\ncov(), 169\ncovariance and correlation with pandas,\n168-170\ncProfile module, 525-527\ncross-validation in model training, 422\ncrosstab(), 354\nfrequency table via, 304\ncross_val_score() (scikit-learn), 423\nCSV (comma-separated values) files\nreading CSV files\nabout Python files, 73\ndefining format and delimiter, 186\nother delimited formats, 185\nreading in pieces, 182\nread_csv(), 177-181\nread_csv() arguments, 181\nwriting CSV files, 184\nother delimited formats, 187\ncsv module\nDialect, 186\noption possibilities chart, 186\nimporting, 185\nopening file with single-character delimiter,\n185\nCtrl-C to interrupt running code, 513\n538 \n| \nIndex",
      "content_length": 2572,
      "extraction_method": "Direct"
    },
    {
      "page_number": 557,
      "chapter": null,
      "content": "Ctrl-D to exit Python shell, 11, 18\ncurly braces ({ })\ncode blocks surrounded by, 26\ndictionaries, 55\nformatting strings, 37\nsets, 59\ncut() to divide data into bins, 215\ngroupby() with, 338-340\nqcut() per quantiles, 216\nCython for C-like performance, 505\nD\ndata\nabout structured data, 1\naggregation (see aggregation of data)\narrays (see arrays)\ncombining and merging datasets\nabout, 253\ncombining with overlap, 268\nconcatenating along an axis, 263-268\njoins, 254-258\nmerge key(s) in index, 259-263\ndatasets on GitHub, 15\nzip files, 15\ndictionaries, 55-59\nexample datasets\nabout, 425\nBitly links with .gov or .mil, 425-435\nFederal Election Commission (2012),\n463-472\nMovieLens, 435-442\nUS baby names, 443-456\nUSDA food database, 457-462\nfeature engineering in modeling, 405\nKaggle competition dataset, 420\nlists, 51-55\nloading, 175\n(see also reading data from a file)\nmissing data (see missing data)\npreparation (see data preparation)\nrepeated instances of values, 236\n(see also categorical data)\nreshaping and pivoting\nhierarchical indexing for, 249, 270-273\npivoting long to wide format, 273-277\npivoting wide to long format, 277-278\nscalars, 34-42\ntype casting, 40\nsequence functions built in, 62\nsets, 59-61\nshifting through time, 371-374\ntime series, 357\n(see also time series)\ntuples, 47-50\nvariables as objects, 28\ndynamic references, strong types, 29\ndata analysis\nabout Python, 2\ndrawbacks, 3\nabout the book, 1\ncategorical data background, 236\n(see also categorical data)\nemail lists, 13\nexample datasets\nabout, 425\nBitly links with .gov or .mil, 425-435\nFederal Election Commission (2012),\n463-472\nMovieLens, 435-442\nUS baby names, 443-456\nUSDA food database, 457-462\ndata loading, 175\n(see also reading data from a file)\ndata preparation\nabout, 203\ncategorical data\nbackground, 236\nCategorical extension data type, 237\ncomputations with, 240\nmeaningful order, 239\nmethods, 242-245\nperformance improvement with, 241\ncombining and merging datasets\nabout, 253\ncombining with overlap, 268\nconcatenating along an axis, 263-268\njoins, 254-258\nmerge key(s) in index, 259-263\ndata transformation\naggregations defined, 329\n(see also aggregation of data)\naxis index map(), 214\ncategorical variable into dummy matrix,\n221\ndiscretization and binning, 215, 338-340,\n500\nduplicates removed, 209\nmapping or function for, 211\nIndex \n| \n539",
      "content_length": 2333,
      "extraction_method": "Direct"
    },
    {
      "page_number": 558,
      "chapter": null,
      "content": "outlier detection and filtering, 217\npermutation, 219\nrandom sampling, 220\nreplacing values, 212\nextension data types, 224, 233\nmissing data, 203\n(see also missing data)\nreshaping and pivoting\nhierarchical indexing for, 249, 270-273\npivoting long to wide format, 273-277\npivoting wide to long format, 277-278\nstring manipulation\nbuilt-in string object methods, 227\nregular expressions, 229-232\nstring functions in pandas, 232-234\nData Science from Scratch: First Principles with\nPython (Grus), 423\ndata types\nabout NumPy, 89\nDataFrame to_numpy(), 136\ndate and time, 358\ndtype property, 86, 87, 88-91, 473\nextension data types, 224, 233\nNaN for missing data, 203\nNumPy ndarrays, 87, 88-91, 473\nhierarchy of data types, 474\ntype casting, 90\nValueError, 91\nstructured ndarrays, 493\n(see also structured ndarrays)\ntime and date, 358\ntrailing underscores in names, 475\ntype casting, 40\nNumPy ndarrays, 90\ntype inference in reading text data, 177\ndatabase interactions, 199-201\nDataFrames (pandas), 129-136\nabout, 5, 6\napply() function, 158\napplymap() for element-wise functions, 159\narithmetic, 152\nwith fill values, 154\narithmetic methods chart, 155\narithmetic with Series, 156\naxis indexes with duplicate labels, 164\ncategorical data from column, 239\nchained indexing pitfalls, 151\ncolumns retrieved as Series, 131\nconcatenating along an axis, 263-268\nconstructing, 129\npossible data inputs chart, 135\ndropping entries from an axis, 141\nduplicate rows removed, 209\nget() HTTP request data, 198\nget_dummies(), 221\nHDF5 binary data format, 195\nhead() and tail(), 130\nhierarchical indexing, 129, 248\nindexing with columns, 252\nreordering and sorting levels, 250\nreshaping data, 249, 270-273\nsummary statistics by level, 251\nimporting into local namespace, 124\nIndex objects, 136\nmap() to transform data, 214\nindexes for row and column, 129\nhierarchical indexing, 129\nindexing options chart, 148\ninteger indexing pitfalls, 149\nJSON data to and from, 188\nJupyter notebook display of, 129\nmissing data\ndropna() to filter out, 205-207\nfillna() to fill in, 207-209\nNumPy ufuncs and, 158\nobjects that have different indexes, 152\noutlier detection and filtering, 217\nranking, 162\nreading data from a file (see reading data\nfrom a file)\nreindexing, 138\nselection with loc and iloc, 147\nrow retrieval, 132\nsorting, 160\nSQL query results into, 199-201\nstatistical methods\ncorrelation and covariance, 168-170\nsummary statistics, 165-168\nsummary statistics by level, 251\nto_numpy(), 135\nunique and other set logic, 170-173\nwriting data to a file (see writing data to a\nfile)\ndate offset, 370\nperiods, 380\ndate type, 41-42, 359\ndatetime module, 41-42, 358\ndatetime type, 41-42, 359\n540 \n| \nIndex",
      "content_length": 2676,
      "extraction_method": "Direct"
    },
    {
      "page_number": 559,
      "chapter": null,
      "content": "converting between string and, 359-361\nformatting as string, 41, 359-361\nimmutable, 41\nISO 8601 format parsed, 360\nlocale-specific formatting options, 361\nshifting dates with offsets, 373\ntime series basics, 362\nDatetimeIndex, 362\nDatetimeTZDtype extension data type, 226\ndate_range(), 367\nnormalize option, 369\n%debug command, 519-523\ndebug() to call debugger, 522\ndebugger in IPython, 519-523\nchart of commands, 521\ndeck of card random sampling example, 343\ndef to declare a function, 66\ndelimiters separating text fields\nreading a CSV file, 179\nreading other delimited formats, 185\nwriting a CSV file, 184\nwriting other delimited formats, 187\ndensity plots and histograms, 309-310\ndescribe() in aggregated data, 331, 337\ndescriptive statistics with pandas, 165-168\nDesignMatrix instances of Patsy, 408\ndevelopment environment, 12\n(see also software development tools)\ndict(), 57\ncategorical data, 239\ndictionaries (dict), 55-59\nDataFrame as dictionary of Series, 129\nconstructing a DataFrame, 129\ndefaultdict(), 58\ndictionary comprehensions, 63\nreading data from delimited file, 186\nget() HTTP request data, 198\nget() versus pop() when key not present, 58\ngrouping via, 327\nHDF5 binary data format, 195\nkeys() and values(), 56\nmerging, 57\npandas Series as, 126\nSeries from and to dictionary, 126\nsequences into, 57\nsetdefault(), 58\nvalid key types, 59\ndifference() for sets, 60\nDataFrame method, 137\ndifference_update() for sets, 60\ndimension tables, 236\ndivision\ndivision operator (/), 32\nfloor (//), 32, 35\ninteger division, 35\ndmatrices() (Patsy), 408\ndocumentation online\nIPython, 24\npandas, 358\nPython\nformatting strings, 38\nitertools functions, 73\ndollar sign ($) for shell environment, 517\ndot() (NumPy) for matrix multiplication, 116\ndouble quote (\")\nmultiline strings, 35\nstring literals declared, 35\ndownsampling, 387, 388-391\ntarget period as subperiod, 393\ndropna() filter for missing data, 205-207\ndrop_duplicates() for DataFrame rows, 210\nDST (daylight saving time), 374, 378\ndtype property, 86, 87, 88-91, 473\nduck typing of objects, 31\nduplicated() for DataFrame rows, 210\nduplicates removed from data, 209\nE\nEffective Python (Slatkin), 18\nelapsed time, 357\nelse, if, elif, 42\nNumPy array vectorized version, 110\nemail lists for data-related Python, 13\nencoding of files\nencoding property, 79\nopen(), 76\nconverting between encodings, 81\nencoding of strings, 38\nend-of-line (EOL) markers, 77\nenumerate(), 62\nequal sign (=)\nset update methods, 60\ntest for equality, 32\nvariable assignment, 28\nunpacking tuples, 49\nerrors and exception handling, 74-76\nAssertionError and debugger, 519-523\nbroadcasting, 487\nIndex \n| \n541",
      "content_length": 2636,
      "extraction_method": "Direct"
    },
    {
      "page_number": 560,
      "chapter": null,
      "content": "debugger in IPython, 519\ninteger indexing, 149\nIPython exceptions, 76\nperiods and resampling, 394\nraise_for_status() for HTTP errors, 197\nSettingWithCopyWarning, 151\nsubstring find() versus index(), 228\ntime zone-aware data with naive, 379\ntry/except blocks, 74\nValueError in NumPy casting, 91\newm() for exponentially weighted moving\nfunctions, 400\nexample datasets\nabout, 425\nBitly links with .gov or .mil, 425-435\nFederal Election Commission (2012),\n463-472\nMovieLens, 435-442\nUS baby names, 443-456\nExcelFile class (pandas), 194\nexceptions (see errors and exception handling)\nexclamation point (!) for shell commands, 516\nexecuting code from clipboard, 513\nexecution time measured, 523-525\nexit() to exit Python shell, 18\nGNU/Linux, 11\nmacOS, 11\nWindows, 10\nexp() (NumPy ufunc), 105\nexpanding() in moving window functions, 398\nexperimental time series, 357\nexporting data (see writing data to a file)\nextension data types, 224, 233\nastype(), 226, 238\nextract() regex from string, 234\nF\nf-strings, 38\nfacet grids, 314-316\nFalse, 39\nfancy indexing by NumPy ndarrays, 100-102\ntake() and put(), 483\nfeature engineering in modeling, 405\nFederal Election Commission dataset, 463-472\nbucketing donation amounts, 469-471\ndonations by occupation and employer,\n466-468\ndonations by state, 471\nfigure() (matplotlib), 283\nadd_subplot() (matplotlib), 283\nAxesSubplot objects returned, 285\nsavefig(), 296\nfiles in Python, 76-81\nbinary data formats\nabout non-pickle formats, 194\nHDF5 format, 195-197, 504\nmemory-mapped files, 503\nMicrosoft Excel files, 194\nndarrays saved, 116\nParquet (Apache), 194, 197\npickle format, 193\npickle format caution, 193\nplots saved to files, 296\ndatabase interactions, 199-201\nmethods most commonly used, 79\nmodes, 78\nbinary mode, 80\nopen() default read only, 77\nopen() write-only modes, 77\ntext mode default, 80\nopen(), 76\nclose() when finished, 77\nconverting between encodings, 81\ndefault read only mode, 77\nread/write modes, 78\nwith statement for clean-up, 77\nwrite-only modes, 77\nwriting delimited files, 187\nplots saved to files, 296\nreading text data, 175-181\nCSV files, 177-181\nCSV files of other formats, 186\nJSON data, 187\nmissing data, 133\nother delimited formats, 185\nreading in pieces, 182\ntype inference, 177\nXML and HTML, 189\nwriting text data\nCSV files, 184\nJSON data, 189\nmissing data, 184\nother delimited format, 187\nsubset of columns, 185\nfillna() to fill in missing data, 205, 207-209\narguments, 209\nfilling values with mean, 340-342\nresampling, 392\n542 \n| \nIndex",
      "content_length": 2500,
      "extraction_method": "Direct"
    },
    {
      "page_number": 561,
      "chapter": null,
      "content": "filtering out missing data, 205-207\nfind() in substring, 228\nfiscal years, 381\nfiscal year end for quarterly periods, 382\nfixed frequency time series, 357\nfixed period time series, 357\nfloat scalar type, 34\nscientific notation, 35\ntype casting, 40\nFloat32Dtype extension data type, 226\nFloat64Dtype extension data type, 226\nfloor (//), 32, 35\nFluent Python (Ramalho), 18\nflush() I/O buffer, 79\nfor loops, 43\nNumPy array vectorization instead, 85, 91,\n108, 110\nperformance tip, 505\nformatting strings, 37\ndatetime type to string, 41\ndocumentation online, 38\nf-strings, 38\nFORTRAN language\nabout legacy libraries, 3\nNumPy arrays, 4, 83, 89\nperformance tip, 505\nfrequencies in periods, 379\nquarterly period frequencies, 382\nresampling and frequency conversion, 387\ndownsampling, 388-391\nfrequencies in time series chart, 368\nfrequency table via crosstab(), 304\nfromfile() (NumPy), 495\nfrompyfunc() (NumPy), 493\nfrom_codes() for Categorical, 239\nfunctions, 67\nabout, 65\nanonymous (lambda), 70\narguments, 28, 66\ndynamic references, strong types, 29\nfunctions as, 70\nkeyword arguments, 66\nNone as default value, 40\npositional arguments, 66\ncalling, 28\ndeclaring with def, 66\nerrors and exception handling, 74-76\ngenerators, 71\ngenerator expressions, 72\ngrouping via, 328\naggregating data, 331\nmethods of Python objects, 28, 30\n__name__ attribute, 333\nnamespaces, 67\nprofiling line by line, 527-529\nPython into NumPy via frompyfunc(), 493\nPython objects, 27, 69\nreturn keyword optional, 66\nreturning multiple values, 68\nyield instead of return, 71\nFundamentals of Data Visualization (Wilke),\n317\nG\ngenerators, 71\ngenerator expressions, 72\nitertools module generator collection, 73\nreversed() as, 63\nGéron, Aurélien, 423\nget() for HTTP request, 197\nget() for string element, 234\ngetattr(), 31\ngetdefaultencoding() (sys module), 78\nget_dummies(), 221, 407\nGitee for datasets, 425\nGitHub\nalternate site Gitee, 425\nbook materials\ndatasets, 15\ndatasets for last chapter, 425\nJupyter code examples, 7\nget() HTTP request, 197\nglobal interpreter lock (GIL), 4\nglobal variables\ncaution against using, 68\nfunction scope, 67\nGNU/Linux Miniconda installation, 10\nexit() or Ctrl-D to exit Python shell, 11, 18\ngreater than (>) operator, 32\nset elements, 60\ngroup operations\nabout, 319\ncross-tabulations, 354\nexamples\ngroup-wise linear regression, 347\nmissing data replacement, 340-342\nrandom sampling and permutations, 343\nIndex \n| \n543",
      "content_length": 2416,
      "extraction_method": "Direct"
    },
    {
      "page_number": 562,
      "chapter": null,
      "content": "weighted average and correlation,\n344-346\ngroup keys suppressed, 338\nhow to think about, 320-329\ndictionaries and Series for grouping, 327\nfunctions for grouping, 328\ngroup aggregation example, 320\nindex levels for grouping, 328\niterating over groups, 324\nmissing values excluded, 324\nselecting columns, 326\npivot tables, 352\nabout, 351\ncross-tabulations, 354\nhierarchical indexing, 249\nsplit-apply-combine, 320, 335\ntransform(), 347\nunwrapped group operations, 350\ngroupby() (itertools), 73, 322\naggregations chart, 329\napply(), 335\ncut() and qcut() with, 338-340\ndate offsets with, 373\ngroup keys suppressed, 338\nGroupBy object, 322\nindexing with column name(s), 326\ngrouped by key, 348\niterating over groups, 324\nlevel specified, 251\nnuisance columns excluded, 323\nsize(), 323\nsplit-apply-combine, 320, 335\ntransform(), 347\nGrus, Joel, 423\nGuido, Sarah, 423\nH\nh5py package for HDF5 files, 195, 197, 504\nHands-On Machine Learning with Scikit-\nLearn, Keras, and TensorFlow (Géron), 423\nhasattr(), 31\nhash maps, 55\n(see also dictionaries)\nhash mark (#) for comment, 27\nhashability\ndictionaries, 59\nhash() for, 59\nset elements, 61\nHDF5 binary file format, 195-197, 504\nHDFStore(), 195\nfixed versus table storage schemas, 196\nheader row in text data files, 177, 186\nhierarchical indexing, 247-253\nabout, 247\nabout DataFrames, 129\nindexing with columns, 252\nMultiIndex, 247\ncreated by itself then reused, 250\nnames for levels, 249\nnumber of levels attribute, 250\npartial indexing made possible, 248\nreading text data from a file, 178\nreordering and sorting levels, 250\nreshaping data, 249, 270-273\nsummary statistics by level, 251\nhist() (matplotlib), 285\nhistograms and density plots, 309-310\nhistplot() (seaborn), 310\nHour(), 370\nhstack() (NumPy), 479\nHTML file format, 189\nreading, 189\nHugunin, Jim, 84\nHunter, John D., 6, 281\nI\nIDEs (integrated development environments),\n12\navailable IDEs, 13\nif, elif, else, 42\nNumPy array vectorized version, 110\niloc operator\nDataFrame indexing, 147\nSeries indexing, 144\nsquare brackets ([ ]), 143, 145\nimmutable and mutable objects, 34\ndatetime types immutable, 41\nIndex objects immutable, 136\nlists mutable, 51\nset elements generally immutable, 61\nstrings immutable, 36\ntuples themselves immutable, 48\nimplicit casting and conversions, 30\nimporting data (see reading data from a file)\nimporting modules, 32\nimport csv, 185\nimport datetime, 358\nimport matplotlib.pyplot as plt, 282\n544 \n| \nIndex",
      "content_length": 2434,
      "extraction_method": "Direct"
    },
    {
      "page_number": 563,
      "chapter": null,
      "content": "import numpy as np, 16, 86, 124, 320\nimport os, 197\nimport pandas as pd, 16, 124, 320\nimport Series, DataFrame, 124\nimport patsy, 408\nimport pytz, 375\nimport requests, 197\nimport seaborn as sns, 16, 306\nimport statsmodels.api as sm, 16, 347, 415\nimport statsmodels.formula.api as smf, 415\nindentation in Python, 26\nindex levels for grouping, 328\nIndex objects (pandas), 136\nmap() to transform data, 214\nset methods available, 137\nindex() of substring, 228\ninner join of merged data, 255\ninstallation and setup of Python\nabout, 9\nabout Miniconda, 9, 10\nGNU/Linux, 10\nmacOS, 11\nnecessary packages, 11\nWindows, 9\nint scalar type, 34\ninteger division, 35\nNumPy signed and unsigned, 90\nrange(), 44\ntype casting, 40\nInt16Dtype extension data type, 226\nInt32Dtype extension data type, 226\nInt64Dtype extension data type, 226\nInt8Dtype extension data type, 226\nintegrated development environments (IDEs),\n12\navailable IDEs, 13\ninteractive debugger in IPython, 519-523\ninterpreted Python language\nabout the interpreter, 18\nglobal interpreter lock, 4\ninvoking via “python”, 18\nGNU/Linux, 10\nIPython, 19\nmacOS, 11\nWindows, 10\nIPython project, 6\n(see also IPython)\nprompt, 18\nintersection(), 60\nDataFrame method, 137\nintersection_update(), 60\nintervals\nopen versus closed, 216\nhalf-open, 388\ntime intervals, 357\nIntroduction to Machine Learning with Python\n(Müller and Guido), 423\nintrospection in IPython, 25\nIPython\nabout, 6\nadvanced features, 532\nbasics, 19\ntab completion, 23, 30\ncommand history, 514-516\ninput and output variables, 515\nsearching and reusing, 514\nconfiguration, 532\nDataFrame access, 131\ndevelopment environment, 12\n(see also software development tools)\ndocumentation link, 24\nexceptions, 76\nexecuting code from clipboard, 513\nintrospection, 25\ninvoking, 19\nkeyboard shortcuts, 509\nmagic commands, 510-511\n%alias, 517\n%bookmark, 518\nCtrl-C to interrupt running code, 513\n%debug, 519-523\nexecuting code from clipboard, 513\n%lprun, 527-529\noperating system commands, 516\n%prun, 525-527\n%run, 19, 512\noperating system commands, 516\ndirectory bookmark system, 518\nshell commands and aliases, 517\nprofiles, 532\nprompt, 19\nsoftware development tools\nabout, 519\ndebugger, 519-523\nmeasuring execution time, 523-525\nprofiling code, 525-527\nprofiling function line by line, 527-529\ntips for productive development, 529\nIndex \n| \n545",
      "content_length": 2331,
      "extraction_method": "Direct"
    },
    {
      "page_number": 564,
      "chapter": null,
      "content": "irregular time series, 357\nis operator, 32\ntest for None, 34\nisdisjoint(), 60\nisinstance(), 30\nisna() to detect NaN, 127, 204, 205\nISO 8601 date format parsed, 360\nissubset(), 60\nissuperset(), 60\nis_unique() property of indexes, 164\niterators\ndictionary keys and values, 56\nduck typing to verify, 31\nfor loops for unpacking, 44\ngenerators, 71\ngroupby() object, 324\nlist(), 51\nrange(), 45\nread_csv(), 181\nTextFileReader object, 183\ntuple(), 48\nitertools module generator collection, 73\nchain(), 73\ncombinations(), 73\ndocumentation online, 73\ngroupby(), 73\n(see also groupby())\npermutations(), 73\nproduct(), 73\nJ\njoin() for DataFrames, 262\nexample of use, 221\njoin() for string concatenation, 228\njoins of merged data, 254-258\ninner join, 255\njoin() for DataFrames, 262\nexample of use, 221\nleft and right joins, 256\nmany-to-many, 256\nmany-to-one, 254\nmerge key(s) in index, 259-263\nouter join, 255\noverlapping column names, 258\nJones, Brian K., 18\nJSON data\nabout, 187\nvalid Python code almost, 187\nget() for HTTP, 198\njson library functions, 187\nPython objects to and from JSON, 187\nreading into Series or DataFrames, 188\nwriting from Series or DataFrames, 189\nJulia programming language, 3\nJupyter notebooks\nabout, 6\nabout GitHub notebooks, 7\nabout IPython project, 6\nbasics, 20\nconfiguration, 533\ndevelopment environment, 12\nexecuting code from clipboard, 513\nimporting a script into a code cell, 513\npandas DataFrame object display, 129\nplotting and visualization, 281\nalternatives to Jupyter notebooks, 282\nplotting commands into single cell, 284\njust-in-time (JIT) compiler technology, 3\nK\nKaggle competition dataset, 420\nkernel density estimate (KDE) plots, 309\nkeyboard shortcuts for IPython, 509\nKeyboardInterrupt, 513\nkeyword arguments, 66\nKlein, Adam, 6\nKomodo IDE, 13\nL\nlambda functions, 70\nnamed “<lambda>”, 333\nleft joins of merged data, 256\nlegend() (matplotlib), 289, 293\nless than (<) operator, 32\nset elements, 60\nlexsort() (NumPy), 497\nlibraries\nessential Python libraries\nmatplotlib, 6\nNumPy, 4\npandas, 5\nscikit-learn, 8\nSciPy, 7\nstatsmodels, 8\nimport conventions, 16\nlegacy libraries, 3\nNumba for JIT compiler technology, 3\nline plots\n546 \n| \nIndex",
      "content_length": 2167,
      "extraction_method": "Direct"
    },
    {
      "page_number": 565,
      "chapter": null,
      "content": "matplotlib, 282\n(see also matplotlib)\npandas, 298-301\nlinear algebra with NumPy arrays, 116\nlinear models\ngroup-wise linear regression, 347\nintercept, 409, 412, 416\nordinary least squares linear regression, 416\nPatsy, 408-415\nabout Patsy, 408\nDesignMatrix instances, 408\nmodel metadata in design_info, 410\nobjects into NumPy, 410\nPatsy’s formulas, 408, 410\nstatsmodels estimations, 415-419\nLinux Miniconda installation, 10\nexit() or Ctrl-D to exit Python shell, 11, 18\nlist(), 51\nlists, 51-55\nadding or removing elements, 51\nappend() versus insert(), 52\nconcatenating and combining, 53\nDataFrame columns, 133\ndictionary keys from, 59\nfile open(), 77\nlist comprehensions, 63\nnested, 64\nmutable, 51\nperformance of ndarray versus, 85\nrange(), 44\nslicing, 54\nsort() in place, 53\nsorted() to new list, 62\nstrings as, 36\nLLVM Project, 501\nload() ndarray (NumPy), 116\nloc operator\nDataFrame indexing, 147\nmodeling, 407\nSeries indexing, 143\nsquare brackets ([ ]), 143, 145\nlocal namespace, 67\nlocale-specific datetime formatting, 361\nloops\nfor loops, 43\nNumPy array vectorization instead, 85, 91,\n108, 110\nperformance tip, 505\nwhile loops, 44\n%lprun command, 527-529\nlxml, 189\nobjectify parsing XML, 190\nM\nmachine learning toolkit (see scikit-learn)\nmacOS Miniconda installation, 11\nexit() or Ctrl-D to exit Python shell, 11, 18\nmany-to-many joins of merged data, 256\nmany-to-one joins of merged data, 254\nmap() to transform data, 212\naxis indexes, 214\nmath operators in Python, 32\nmatplotlib\nabout, 6, 281, 317\nAPI primer, 282-297\nabout matplotlib, 282\nannotations and drawing on subplot,\n294-296\ncolors, markers, line styles, 288-290\nfigures and subplots, 283-288\nsaving plots to file, 296\nticks, labels, legends, 290-293\nconfiguration, 297\ndocumentation online, 286\ninvoking, 282\npatch objects, 295\nplots saved to files, 296\ntwo-dimensional NumPy array, 108\nmatrix multiplication via dot() (NumPy), 116\nmaximum() (NumPy ufunc), 105\nmean(), 322\ngrouping by key, 348\nmissing data replaced with mean, 340-342\npivot table default aggregation, 352\nmedian value to fill in missing data, 421\nmelt() multiple columns into one, 277-278\nmemmap() (NumPy), 503\nmemory usage\ncontiguous memory importance, 505\ngenerators, 72\nNumPy ndarrays, 84\nrow versus column major order, 478\nstriding information, 473\nmemory-mapped files (NumPy), 503\nmerge() datasets, 254-258\nmerge key(s) in index, 259-263\nmergesort parameter for stable sorts, 498\nmetadata\nIndex \n| \n547",
      "content_length": 2440,
      "extraction_method": "Direct"
    },
    {
      "page_number": 566,
      "chapter": null,
      "content": "dtype as, 87, 88\nmodel metadata in design_info, 410\npandas preserving, 5\nMicrosoft Excel files read, 194\nMiniconda package manager\nabout, 9, 10\nconda to invoke commands (see conda)\nconda-forge, 9\nGNU/Linux installation, 10\nnecessary packages installed, 11\nWindows installation, 9\nminus (-) operator, 32\nsets, 60\ntimedelta type, 41\nMirjalili, Vahid, 423\nmissing data, 203\ncombine_first(), 269\ndatetime parsing, 361\nfilling in, 207-209\npivot tables, 353\nwith mean value, 340-342\nwith median value, 421\nfiltering out, 205-207\ngroupby() group key, 324\nintroduced during shifting, 372\nNA and NULL sentinels (pandas), 179\nNA for not available, 179, 204\nNaN (pandas), 127, 203\nfillna() to fill in missing data, 205,\n207-209\nisna() and notna() to detect, 127, 204,\n205\nNone, 34, 40, 204\ndictionary key not present, 58\nfunction without return, 66\nis operator testing for, 34\nreading text data from a file, 179\nscikit-learn not allowing, 420\nsentinel (placeholder) values, 179, 184, 203\nstatsmodels not allowing, 420\nstring data, 232-234\nwriting text data to a file, 184\nmodeling\nabout, 405\ndata\nfeature engineering, 405\nnonnumeric column, 407\nNumPy arrays for transfer, 406\npandas for loading and cleaning, 405\nintercept, 409, 412, 416\nPatsy for model descriptions, 408-415\nabout Patsy, 8, 408\ncategorical data, 412-415\ndata transformations, 410\nDesignMatrix instances, 408\nmodel metadata in design_info, 410\nobjects into NumPy, 410\nPatsy’s formulas, 408, 410\nstateful transformations, 411\nmodf() (NumPy ufunc), 106\nmodules\nabout, 32\ncsv module, 185\ndatetime module, 41-42\nimporting, 32\nimport matplotlib.pyplot as plt, 282\nimport numpy as np, 16, 86, 124, 320\nimport pandas as pd, 16, 124, 320\nimport seaborn as sns, 16\nimport Series, DataFrame, 124\nimport statsmodels as sm, 16\nitertools module, 73\nos module, 197\npickle module, 193\ncaution about, 193\nrandom modules, 103\nre for regular expressions, 229\nrequests module, 197\nMonte Carlo simulation example, 343\nMovieLens example dataset, 435-442\nmeasuring rating disagreement, 439-442\nmoving window functions, 396-403\nbinary, 401\ndecay factor, 399\nexpanding window mean, 398\nexponentially weighted functions, 399\nrolling operator, 396\nspan, 399\nuser-defined, 402\nMüller, Andreas, 423\nMultiIndex, 247\ncreated by itself then reused, 250\nmultithreading and Python, 4\nmutable and immutable objects, 34\ndatetime types immutable, 41\nIndex objects immutable, 136\nlists mutable, 51\nset elements generally immutable, 61\n548 \n| \nIndex",
      "content_length": 2467,
      "extraction_method": "Direct"
    },
    {
      "page_number": 567,
      "chapter": null,
      "content": "strings immutable, 36\ntuples themselves immutable, 48\nN\nNA for data not available, 179, 204\n(see also null values)\nnamespaces\nfunctions, 67\nimporting modules (see importing modules)\nimporting Series, DataFrame into local, 124\nIPython namespace search, 26\nscripts run in empty namespace, 512\nNaN (Not a Number; pandas), 127, 203\ndropna() filter for missing data, 205-207\nfillna() to fill in missing data, 205, 207-209\nisna() and notna() to detect, 127, 204, 205\nmissing data in file read, 179\nNaT (Not a Time), 361\nndarrays (NumPy)\n@ infix operator, 103\nabout, 85\nadvanced concepts\nbroadcasting, 92, 156, 484-489\nC order versus FORTRAN order, 478\nC versus FORTRAN order, 476\nconcatenating arrays, 263-268, 479-481\ndata type hierarchy, 474\nfancy indexing equivalents, 483\nobject internals, 473\nr_ and c_ objects, 480\nrepeating elements, 481\nreshaping arrays, 476-478\nrow major versus column major order,\n478\nrow versus column major order, 476\nsorting, 114, 495-501\nsplitting arrays, 479\nstriding information, 473\nstructured arrays, 493-495\ntiling arrays, 482\nufunc methods, 106, 490-492\nufuncs compiled via Numba, 502\nufuncs faster with Numba, 501\nufuncs written in Python, 493\narithmetic with, 91\narray-oriented programming, 108-115\nconditional logic as array operations, 110\nrandom walks, 118-121\nrandom walks, many at once, 120\nunique and other set logic, 115\nvectorization, 108, 110\nbasic indexing and slicing, 92-97\nBoolean array methods, 113\nBoolean indexing, 97-100\nbroadcasting, 484-489\nabout, 92, 156\nover other axes, 487\nperformance tip, 505\nsetting array values via, 489\nconcatenating, 263-268, 479-481\nr_ and c_ objects, 480\ncreating, 86-88\ndata types, 87, 88-91, 473\nhierarchy of, 474\ntype casting, 90\nValueError, 91\ndtype property, 86, 87, 88-91\nfancy indexing, 100-102\ntake() and put(), 483\nlinear algebra, 116\nmodel data transfer via, 406\nPatsy DesignMatrix instances, 409\nperformance of Python list versus, 85\nshape property, 86\nsorting, 114, 495-501\nalternative sort algorithms, 498\ndescending order problem, 497\nindirect sorts, 497\npartially sorting, 499\nsearching sorted arrays, 500\nstatistical methods, 111\nstructured arrays\nabout, 493\nmemory maps working with, 504\nnested data types, 494\nwhy use, 495\nswapping axes, 103\ntransposing arrays, 102\nufuncs, 105\ncompiled via Numba, 502\nfaster with Numba, 501\nmethods, 106, 490-492\npandas objects and, 158\nPython-written ufuncs, 493\nnested list comprehensions, 64\nnewline character (\\n) counted, 36\nNone, 40, 204\nabout, 34\nIndex \n| \n549",
      "content_length": 2499,
      "extraction_method": "Direct"
    },
    {
      "page_number": 568,
      "chapter": null,
      "content": "dictionary key not present, 58\nfunction without return, 66\nis operator testing for, 34\nnonlocal variables, 67\nnot equal to (!=), 32\nnotna() to detect NaN, 127, 204\nfiltering out missing data, 205\nnp (see NumPy)\nnull values\ncombining data with overlap, 268\nmissing data, 179, 203\n(see also missing data)\nNaN (pandas), 127, 203\ndropna() filter for missing data, 205-207\nfillna() to fill in, 205, 207-209\nisna() and notna() to detect, 127, 204,\n205\nmissing data in file read, 179\nNaT (Not a Time; pandas), 361\nNone, 34, 40, 204\ndictionary key not present, 58\nfunction without return, 66\nis operator testing for, 34\nnullable data types, 226, 254\npivot table fill values, 353\nstring data preparation, 233\nNumba library\nabout, 501\ncustom compiled NumPy ufuncs, 502\njust-in-time (JIT) compiler technology, 3\nnumeric types, 35\nNaN as floating-point value, 203\nnullable data types, 226, 254\nNumPy ndarrays\ndata type hierarchy, 474\ntype casting, 90\nNumPy\nabout, 4, 83-85\nshortcomings, 224\narray-oriented programming, 108-115\nconditional logic as array operations, 110\nrandom walks, 118-121\nrandom walks, many at once, 120\nunique and other set logic, 115\nvectorization, 108, 110\ndata types, 88-91\nhierarchy of, 474\nstring_ type caution, 91\ntrailing underscores in names, 475\nValueError, 91\nDataFrames to_numpy(), 135\nemail list, 13\nimport numpy as np, 16, 86, 124, 320\nndarrays, 85\n(see also ndarrays)\nPatsy objects directly into, 410\npermutation of data, 219\npseudorandom number generation, 103\nmethods available, 104\nPython functions via frompyfunc(), 493\nO\nobject introspection in IPython, 25\nobject model of Python, 27\n(see also Python objects)\nOHLC (open-high-low-close) resampling, 391\nOliphant, Travis, 84\nOlson database of time zone information, 374\nonline resources (see resources online)\nopen() a file, 76\nclose() when finished, 77\nconverting between encodings, 81\ndefault read only mode, 77\nread/write modes, 78\nwith statement for clean-up, 77\nwrite-only modes, 77\nwriting delimited files, 187\nopen-high-low-close (OHLC) resampling, 391\noperating system via IPython, 516\ndirectory bookmark system, 518\nos module, 197\nshell commands and aliases, 517\nordinary least squares linear regression, 416\nos module to remove HDF5 file, 197\nouter join of merged data, 255\nouter() (NumPy ufunc), 491\nP\npackage manager Miniconda, 9\nconda-forge, 9\npairplot() (seaborn), 312\npandas\nabout, 5, 84, 123\nbook coverage, 123\nfile input and output, 116\nnon-numeric data handling, 91\ntime series, 5\nDataFrames, 129-136\n550 \n| \nIndex",
      "content_length": 2509,
      "extraction_method": "Direct"
    },
    {
      "page_number": 569,
      "chapter": null,
      "content": "(see also DataFrames)\nabout, 5, 6\narithmetic, 152\narithmetic with fill values, 154\narithmetic with Series, 156\ncolumns retrieved as Series, 131\nconstructing, 129\nhierarchical indexing, 129\nimporting into local namespace, 124\nIndex objects, 136\nindexes for row and column, 129\nindexing options chart, 148\ninteger indexing pitfalls, 149\nJupyter notebook display of, 129\nobjects that have different indexes, 152\npossible data inputs chart, 135\nreindexing, 138\nto_numpy(), 135\ndocumentation online, 358, 504\nimport pandas as pd, 16, 124, 320\nimport Series, DataFrame, 124\nIndex objects, 136\nset methods available, 137\nmissing data representations, 203\n(see also missing data)\nmodeling with, 405\n(see also modeling)\nNaN for missing or NA values, 127, 203\ndropna() filter for missing data, 205-207\nfillna() to fill in missing data, 205,\n207-209\nisna() and notna() to detect, 127, 204,\n205\nmissing data in file read, 179\nSeries, 124-128\n(see also Series)\nabout, 5\narithmetic, 152\narithmetic methods chart, 155\narithmetic with DataFrames, 156\narithmetic with fill values, 154\narray attribute, 124\nDataFrame column retrieved as, 131\nimporting into local namespace, 124\nindex, 124\nindex attribute, 124\nIndex objects, 136\nindexing, 142\ninteger indexing pitfalls, 149\nname attribute, 128\nNumPy ufuncs and, 158\nobjects that have different indexes, 152\nPandasArray, 125\nreindexing, 138\nstatistical methods\ncorrelation and covariance, 168-170\nsummary statistics, 165-168\nsummary statistics by level, 251\nunique and other set logic, 170-173\nPandasArray, 125\nparentheses ( )\ncalling functions and object methods, 28\nintervals open (exclusive), 216\nmethod called on results, 218\ntuples, 47-50\ntuples of exception types, 75\nParquet (Apache)\nread_parquet(), 194\nremote servers for processing data, 197\nparsing a text file, 175-181\nHTML, 189\nJSON, 188\nXML with lxml.objectify, 190\nparsing date format , 360\npass statement, 44\npassenger survival scikit-learn example,\n420-423\npatch objects in matplotlib, 295\nPATH and invoking Miniconda, 9\nPatsy, 408-415\nabout, 8, 408\nIntercept, 409, 412, 416\nDesignMatrix instances, 408\nmodel metadata in design_info, 410\nobjects into NumPy, 410\nPatsy’s formulas, 408, 410\ncategorical data, 412-415\ndata transformations, 410\nstateful transformations, 411\npd (see pandas)\npdb (see debugger in IPython)\npercent (%)\ndatetime formatting, 359\nIPython magic commands, 510-511\n%alias, 517\n%bookmark, 518\nCtrl-C to interrupt running code, 513\n%debug, 519-523\nIndex \n| \n551",
      "content_length": 2477,
      "extraction_method": "Direct"
    },
    {
      "page_number": 570,
      "chapter": null,
      "content": "executing code from clipboard, 513\n%lprun, 527-529\noperating system commands, 516\n%prun, 525-527\n%run, 19, 512\n%run -p, 525-527\n%time and %timeit, 523-525\nPérez, Fernando, 6\nperformance\naggregation functions, 331, 350\ncategorical data computations, 241\nNumPy ndarray versus Python list, 85\nPython ufuncs via NumPy, 493\nfaster with Numba, 501\nsort_index() data selection, 251\ntips for, 505\ncontiguous memory importance, 505\nvalue_counts() on categoricals, 242\nPeriod object, 379\nconverted to another frequency, 380\nconverting timestamps to and from, 384\nquarterly period frequencies, 382\nPeriodIndex object, 380\nconverted to another frequency, 380, 381\ncreating from arrays, 385\nPeriodIndex(), 274\nperiods\nabout, 379\nperiod frequency conversion, 380\nresampling with, 392\nperiod_range(), 380\nPerktold, Josef, 8\npermutation of data, 219\nexample, 343\nitertools function, 73\nNumPy random generator method, 104\npermutations() (itertools), 73\npickle module, 193\nread_pickle(), 168, 193\nto_pickle(), 193\ncaution about long-term storage, 193\npip\ninstall, 12\nconda install recommended, 12\nupgrade flag, 12\npipe (|) for OR, 32\nNumPy ndarrays, 99\npivot tables, 352-354\nabout, 351\ncross-tabulations, 354\ndefault aggregation mean(), 352\naggfunc keyword for other, 353\nfill value for NA entries, 353\nhierarchical indexing, 249\nmargins, 352\npivot tables, 352\npivot(), 275\npivot_table(), 351\noptions chart, 354\nplot() (matplotlib), 284\ncolors and line styles, 288-290\nplot() (pandas objects)\nbar plots, 301-308\nvalue_counts() for, 304\ndensity plots, 309-310\nhistograms, 309-310\nline plots, 298-301\nPlotly for visualization, 317\nplotting\nabout, 281\nbook on data visualization, 317\nmatplotlib\nabout, 6, 281, 317\nAPI primer, 282\nconfiguration, 297\ndocumentation online, 286\npatch objects, 295\nplots saved to files, 296\ntwo-dimensional NumPy array, 108\nother Python tools, 317\nseaborn and pandas, 298-316\nabout seaborn, 281, 298, 306\nbar plots via pandas, 301-306\nbar plots with seaborn, 306\nbox plots, 315\ndensity plots, 309-310\ndocumentation online, 316\nfacet grids, 314-316\nhistograms, 309-310\nimport seaborn as sns, 16, 306\nline plots, 298-301\nscatter or point plots, 311-313\nplus (+) operator, 32\nadding strings, 37\nlists, 53\nPatsy’s formulas, 408\nI() wrapper for addition, 412\ntimedelta type, 42\n552 \n| \nIndex",
      "content_length": 2294,
      "extraction_method": "Direct"
    },
    {
      "page_number": 571,
      "chapter": null,
      "content": "tuple concatenation, 49\npoint or scatter plots, 311-313\nPolygon() (matplotlib), 295\npop() column from DataFrame, 274\npound sign (#) for comment, 27\npreparation of data (see data preparation)\nproduct() (itertools), 73\nprofiles in IPython, 532\nprofiling code, 525-527\nfunction line by line, 527-529\nprompt for IPython, 19\nprompt for Python interpreter, 18\n%prun, 525-527\npseudorandom number generation, 103\nmethods available, 104\nput() (NumPy), 483\npyarrow package for read_parquet(), 194\nPyCharm IDE, 13\nPyDev IDE, 13\nPyTables package for HDF5 files, 195, 197, 504\nPython\nabout data analysis, 1\nPython drawbacks, 3\nPython for, 2\nabout Python\nboth prototyping and production, 3\nlegacy libraries and, 3\nabout version used by book, 9\ncommunity, 13\nconferences, 13\ndata structures and sequences\ndictionaries, 55-59\nlists, 51-55\nsequence functions built in, 62\nsets, 59-61\ntuples, 47-50\nerrors and exception handling, 74-76\n(see also errors and exception handling)\neverything is an object, 27\nexit() to exit, 18\nGNU/Linux, 11\nmacOS, 11\nWindows, 10\nfiles, 76-81\n(see also files in Python)\ninstallation and setup\nabout, 9\nabout Miniconda, 9, 10\nGNU/Linux, 10\nmacOS, 11\nnecessary packages, 11\nWindows, 9\ninterpreted language\nabout the interpreter, 18\nglobal interpreter lock, 4\nIPython project, 6\n(see also IPython)\nspeed trade-off, 3\ninvoking, 18\nGNU/Linux, 10\nIPython, 19\nmacOS, 11\nWindows, 10\ninvoking matplotlib, 282\n(see also matplotlib)\nJSON objects to and from, 187\njust-in-time (JIT) compiler technology, 3\nlibraries that are key\nmatplotlib, 6\nNumPy, 4\npandas, 5\nscikit-learn, 8\nSciPy, 7\nstatsmodels, 8\nmodule import conventions, 16\n(see also modules)\nobjects, 27\nduck typing, 31\ndynamic references, strong types, 29\nis operator, 32\nmethods, 28, 30\nmodule imports, 32\nmutable and immutable, 34\nNone tested for, 34\nobject introspection in IPython, 25\nvariables, 28\ntutorial\nabout additional resources, 18\nabout experimentation, 17\nbinary operators, 32\ncontrol flow, 42-45\nexiting Python shell, 18\nimporting modules, 32\ninvoking Python, 18\nIPython basics, 19\nIPython introspection, 25\nIPython tab completion, 23, 30\nJupyter notebook basics, 20\nIndex \n| \n553",
      "content_length": 2154,
      "extraction_method": "Direct"
    },
    {
      "page_number": 572,
      "chapter": null,
      "content": "scalars, 34-42\nsemantics of Python, 26-34\nufuncs written in, 493\ncustom compiled NumPy ufuncs, 502\nfaster with Numba, 501\nPython Cookbook (Beazley and Jones), 18\nPython Data Science Handbook (VanderPlas),\n423\nPython Machine Learning (Raschka and Mirja‐\nlili), 423\nPython objects, 27\nattributes, 30\nconverting to strings, 36\nduck typing, 31\nfunctions, 69\nis operator, 32\ntest for None, 34\nkey-value pairs of dictionaries, 55\nmethods, 28, 30\nmutable and immutable, 34\ndatetime types immutable, 41\nIndex objects immutable, 136\nlists mutable, 51\nset elements generally immutable, 61\nstrings immutable, 36\ntuples themselves immutable, 48\nobject introspection in IPython, 25\nscalars, 34-42\nto_numpy() with heterogeneous data, 407\nvariables, 28\ndynamic references, strong types, 29\nmodule imports, 32\nPython Tools for Visual Studio (Windows), 13\npytz library for time zones, 374\nQ\nqcut() for data binning per quantiles, 216\ngroupby() with, 338-340\nquarterly period frequencies, 382\nquestion mark (?)\nnamespace search in IPython, 26\nobject introspection in IPython, 25\nquote marks\nmultiline strings, 36\nstring literals declared, 35\nR\nraise_for_status() for HTTP errors, 197\nRamalho, Luciano, 18\nrandom modules (NumPy and Python), 103\nbook use of np.random, 473\nNumPy permutation(), 219\nrandom sampling, 220\nexample, 343\nrandom walks via NumPy arrays, 118-121\nmany at once, 120\nrange(), 44\nRaschka, Sebastian, 423\nravel() (NumPy), 478\nrc() for matplotlib configuration, 297\nre module for regular expressions, 229\nread() a file, 78\nreadable() file, 79\nreading data from a file\nabout, 175\nbinary data\nabout non-pickle formats, 194\nHDF5 format, 195-197, 504\nmemory-mapped files, 503\nMicrosoft Excel files, 194\npickle format, 193\npickle format caution, 193\nCSV Dialect, 186\ndatabase interactions, 199-201\nfromfile() into ndarray, 495\ntext data, 175-181\nCSV files, 177-181\nCSV files of other formats, 186\ndelimiters separating fields, 179\nheader row, 177, 186\nJSON, 187\nmissing data, 133, 179\nother delimited formats, 185\nparsing, 175\nreading in pieces, 182\ntype inference, 177\nXML and HTML, 189\nreadlines() of a file, 79\nread_* data loading functions, 175\nread_csv(), 177-181\nabout Python files, 73\narguments commonly used, 181\nother delimited formats, 185\nreading in pieces, 182\nread_excel(), 194\nread_hdf(), 197\nread_html(), 189\nread_json(), 188\n554 \n| \nIndex",
      "content_length": 2348,
      "extraction_method": "Direct"
    },
    {
      "page_number": 573,
      "chapter": null,
      "content": "read_parquet(), 194\nread_pickle(), 168, 193\nread_sql(), 201\nread_xml(), 192\nRectangle() (matplotlib), 295\nreduce() (NumPy ufunc), 490\nreduceat() (NumPy ufunc), 492\nreduction methods for pandas objects, 165-168\nregplot() (seaborn), 312\nregular expressions (regex), 229-232\ndata preparation, 232-234\ntext file whitespace delimiter, 179\nreindex() (pandas), 138\narguments, 140\nloc operator for, 140\nresampling, 392\nremove() for sets, 60\nrename() to transform data, 214\nrepeat() (NumPy), 481\nreplace() to transform data, 212\nstring data, 228\nrequests package for web API support, 197\nresample(), 387\narguments chart, 388\nresampling with periods, 392\nresampling and frequency conversion, 387\ndownsampling, 388-391\ngrouped time resampling, 394\nopen-high-low-close resampling, 391\nupsampling, 391, 391\nopen-high-low-close resampling, 391\nreshape() (NumPy), 101, 476-478\nrow major versus column major order, 478\nresources\nbook on data visualization, 317\nbooks on modeling and data science, 423\nresources online\nbook on data visualization, 317\nIPython documentation, 24\nmatplotlib documentation, 286\npandas documentation, 358, 504\nPython documentation\nformatting strings, 38\nitertools functions, 73\nPython tutorial, 18\nseaborn documentation, 316\nvisualization tools for Python, 317\nreversed() sequence, 63\ngenerator, 63\nright joins of merged data, 256\nrolling() in moving window functions, 396\n%run command, 19, 512\n%run -p, 525-527\nS\nsample() for random sampling, 220\nsave() ndarray (NumPy), 116\nsavefig() (matplotlib), 296\nscalars, 34-42\nscatter or point plots, 311-313\nscatter() (matplotlib), 285\nscikit-learn, 420-423\nabout, 8, 420\nconda install scikit-learn, 420\ncross-validation, 422\nemail list, 13\nmissing data not allowed, 420\nfilling in missing data, 421\nSciPy\nabout, 7\nconda install scipy, 310\ndensity plots requiring, 310\nemail list, 13\nscripting languages, 2\nscripts via IPython %run command, 19, 512\nCtrl-C to interrupt running code, 513\nexecution time measured, 523\nSeabold, Skipper, 8\nseaborn and pandas, 298-316\nabout seaborn, 281, 298, 306\nbar plots via pandas, 301-306\nbar plots with seaborn, 306\nbox plots, 315\ndensity plots, 309-310\ndocumentation online, 316\nfacet grids, 314-316\nhistograms, 309\nimport seaborn as sns, 16, 306\nline plots via pandas, 298-301\nscatter or point plots, 311-313\nsearchsorted() (NumPy), 500\nseek() in a file, 78\nseekable() file, 79\nsemicolon (;) for multiple statements, 27\nsentinel (placeholder) values, 184, 203\nNA and NULL sentinels (pandas), 179\nsequences\nbuilt-in sequence functions, 62\nIndex \n| \n555",
      "content_length": 2542,
      "extraction_method": "Direct"
    },
    {
      "page_number": 574,
      "chapter": null,
      "content": "Categorical from, 239\ndictionaries from, 57\nlists, 51-55\nrange(), 44\nstrings as, 36\ntuples, 47-50\nunpacking tuples, 49\nserializing data, 193\nSeries (pandas), 124-128\nabout, 5\narithmetic, 152\nwith fill values, 154\narithmetic methods chart, 155\narithmetic with DataFrames, 156\narray attribute, 124\nPandasArray, 125\naxis indexes with duplicate labels, 164\nconcatenating along an axis, 263-268\nDataFrame columns, 131\ndictionary from and to Series, 126\ndimension tables, 236\ndropping entries from an axis, 141\nextension data types, 224, 233\nget_dummies(), 222\ngrouping via, 327\nHDF5 binary data format, 195\nimporting into local namespace, 124\nindex, 124\nindexing, 142\nloc to select index values, 143\nreindexing, 138\nindex attribute, 124\nIndex objects, 136\ninteger indexing pitfalls, 149\nJSON data to and from, 188\nmap() to transform data, 212\nmissing data\ndropna() to filter out, 205\nfillna() to fill in, 209\nMultiIndex, 247\nname attribute, 128\nNumPy ufuncs and, 158\nobjects that have different indexes, 152\nranking, 162\nreading data from a file (see reading data\nfrom a file)\nreplace() to transform data, 212\nsorting, 160\nstatistical methods\ncorrelation and covariance, 168-170\nsummary statistics, 165-168\nsummary statistics by level, 251\nstring data preparation, 232-234\nstring methods chart, 234\ntime series, 361\n(see also time series)\nunique and other set logic, 170-173\nwriting data (see writing data to a file)\nset(), 59\nsetattr(), 31\nsets\nintersection of two, 60\nlist-like elements to tuples for storage, 61\npandas DataFrame methods, 137\nset comprehensions, 64\nunion of two, 60\nset_index() to DataFrame column, 252\nset_title() (matplotlib), 292\nset_trace(), 522\nset_xlabel() (matplotlib), 292\nset_xlim() (matplotlib), 294\nset_xticklabels() (matplotlib), 291\nset_xticks() (matplotlib), 291\nset_ylim() (matplotlib), 294\nShe, Chang, 6\nshell commands and aliases, 517\nshifting data through time, 371-374\nside effects, 34\nsign() to test positive or negative, 219\nsingle quote (')\nmultiline strings, 35, 36\nstring literals declared, 35\nsize() of groups, 323\nslash (/) division operator, 32\nfloor (//), 32, 35\nSlatkin, Brett, 18\nslicing strings, 234\nsm (see statsmodels)\nSmith, Nathaniel, 8\nsns (see seaborn)\nsoftware development tools\nabout, 519\ndebugger, 519-523\nchart of commands, 521\nIDEs, 12\navailable IDEs, 13\nmeasuring execution time, 523-525\nprofiling code, 525-527\n556 \n| \nIndex",
      "content_length": 2382,
      "extraction_method": "Direct"
    },
    {
      "page_number": 575,
      "chapter": null,
      "content": "function line by line, 527-529\ntips for productive development, 529\nsort() in place, 53\nNumPy arrays, 114, 495\ndescending order problem, 497\nsorted() to new list, 62\nNumPy arrays, 114\nsorting ndarrays, 114, 495-501\nalternative sort algorithms, 498\ndescending order problem, 497\nindirect sorts, 497\npartially sorting, 499\nsearching sorted arrays, 500\nsort_index() by all or subset of levels, 251\ndata selection performance, 251\nsplit() array (NumPy), 479\nsplit() string, 227\nsplit-apply-combine group operations, 320\nSpyder IDE, 13\nSQL query results into DataFrame, 199-201\nSQLAlchemy project, 201\nSQLite3 database, 199\nsqrt() (NumPy ufunc), 105\nsquare brackets ([ ])\narrays, 85\n(see also arrays)\narrays returned in reverse order, 497\n(see also arrays)\nintervals closed (inclusive), 216\nlist definitions, 51\nslicing lists, 54\nloc and iloc operators, 143, 145\nseries indexing, 142\nstring element index, 234\nstring slicing, 234\ntuple elements, 48\nstable sorting algorithms, 498\nstack(), 249, 270-273, 275\nstacking, 263-268, 479-481\nr_ and c_ objects, 480\nvstack() and hstack(), 479\nstandardize() (Patsy), 411\nstatistical methods\ncategorical variable into dummy matrix, 221\nfrequency table via crosstab(), 304\ngroup weighted average and correlation,\n344-346\ngroup-wise linear regression, 347\ngroupby() (see groupby())\nhistogram of bimodal distribution, 310\nmean(), 322\ngrouping by key, 348\nmissing data replaced with mean,\n340-342\npivot table default aggregation, 352\nmoving window functions, 396-403\nbinary, 401\ndecay factor, 399\nexpanding window mean, 398\nexponentially weighted functions, 399\nrolling operator, 396, 398\nspan, 399\nuser-defined, 402\nNumPy arrays, 111\npandas objects\ncorrelation and covariance, 168-170\nsummary statistics, 165-168\nsummary statistics by level, 251\npermutation of data, 219\nexample, 343\nitertools function, 73\nNumPy random generator method, 104\nrandom sampling, 220, 343\nstatsmodels, 415-419\nabout, 8, 415\nabout Patsy, 408\nconda install statsmodels, 347, 415\nPatsy installed, 408\nemail list, 13\nimport statsmodels.api as sm, 16, 347, 415\nimport statsmodels.formula.api as smf, 415\nlinear regression models, 415-419\nmissing data not allowed, 420\ntime series analysis, 419\nstdout from shell command, 516\nstr (strings) scalar type, 35-38\nabout, 34\nimmutable, 36\nNumPy string_type, 91\nsequences, 36\nbackslash (\\) to escape, 37\nbuilt-in string object methods, 227\nconverting objects to, 36\ndata preparation, 232-234\ndatetime to and from, 41, 359-361\ndecoding UTF-8 to, 80\nformatting, 37\ndatetime as string, 41, 359-361\nIndex \n| \n557",
      "content_length": 2555,
      "extraction_method": "Direct"
    },
    {
      "page_number": 576,
      "chapter": null,
      "content": "documentation online, 38\nf-strings, 38\nget_dummies(), 222\nmissing data, 232-234\nmultiline strings, 36\nregular expressions, 229-232\nstring methods, 234\nsubstring methods, 228\nelement retrieval, 234\ntype casting, 40\nNumPy ndarray type casting, 90\nstr(), 36\ndatetime objects as strings, 359\ntype casting, 40\nstrftime(), 41, 359\nstriding information of ndarrays, 473\nstrip() to trim whitespace, 227\nstrptime(), 41, 360\nstructured data, 1\nstructured ndarrays\nabout, 493\nmemory maps working with, 504\nnested data types, 494\nwhy use, 495\nsubplots() (matplotlib), 286\nsubplots_adjust() (matplotlib), 287\nsubstring methods, 228\nsubtraction (see minus (-) operator)\nsummary statistics with pandas objects,\n165-168\n(see also pivot tables)\nswapaxes() (NumPy), 103\nswaplevel(), 250\nsymmetric_difference() for sets, 60\nsymmetric_difference_update() for sets, 60\nsys module for getdefaultencoding(), 78\nT\ntab completion in IPython, 23\nobject attributes and methods, 30\ntake() (NumPy), 483\nTaylor, Jonathan, 8\ntell() position in a file, 78\ntemplating strings, 37\ndocumentation online, 38\nf-strings, 38\ntext data read from a file, 175-181\nCSV files, 177-181\ndefining format and delimiter, 186\ndelimiters separating fields, 179\nJSON, 187\nmissing data, 133, 179\nother delimited formats, 185\nparsing, 175\nreading in pieces, 182\ntype inference, 177\nXML and HTML, 189\ntext data written to a file\nCSV files, 184\nJSON data, 189\nmissing data, 184\nother delimited format, 187\nsubset of columns, 185\ntext editors, 12\ntext mode default file behavior, 80\ntext() (matplotlib), 294\nTextFileReader object from read_csv(), 181, 183\ntilde (~) as NumPy negation operator, 98\ntile() (NumPy), 482\n%time(), 523-525\ntime series\nabout, 357, 366\nabout frequencies, 370\nabout pandas, 5\naggregation and zeroing time fields, 41\nbasics, 361-366\nduplicate indices, 365\nindexing, selecting, subsetting, 363\ndata types, 358\nconverting between, 359-361\nlocale-specific formatting, 361\ndate ranges, frequencies, shifting\nabout, 366\nfrequencies and date offsets, 370\nfrequencies chart, 368\ngenerating date ranges, 367-369\nshifting, 371-374\nshifting dates with offsets, 373\nweek of month dates, 371\nfixed frequency, 357\ninterpolation when reindexing, 138\nlong or stacked format, 273-277\nmoving window functions, 396-403\nbinary, 401\ndecay factor, 399\nexpanding window mean, 398\nexponentially weighted functions, 399\nrolling operator, 396, 398\n558 \n| \nIndex",
      "content_length": 2403,
      "extraction_method": "Direct"
    },
    {
      "page_number": 577,
      "chapter": null,
      "content": "span, 399\nuser-defined, 402\nperiods\nabout, 379\nconverting timestamps to and from, 384\nPeriodIndex from arrays, 385\nquarterly period frequencies, 382\nresampling and frequency conversion, 387\ndownsampling, 388-391\ngrouped time resampling, 394\nopen-high-low-close resampling, 391\nupsampling, 391\nstatsmodels for estimating, 419\nstock price percent change, 168\ntime zones (see time zones)\ntime type, 41-42, 359\ntime zones, 374-379\nabout, 374\nbetween different time zones, 378\nBitly links dataset\ncounting time zones in pandas, 428-435\ncounting time zones in Python, 426\nDST, 374, 378\nlocalization and conversion, 375-377\npytz library, 374\ntime zone-aware objects, 377\nUTC, 374, 378\ntimedelta type, 41, 359\ntimedelta() (datetime), 358\n%timeit(), 523-525\nTimestamp (pandas)\nformatting, 359-361\nshifting dates with offsets, 373\ntime series basics, 362\ntime zone-aware, 377\ntimestamps, 357, 362\nnormalized to midnight, 369\ntimezone() (pytz), 375\nTitanic passenger survival dataset, 420\nto_csv(), 184\nto_datetime(), 360\nto_excel(), 195\nto_json(), 189\nto_numpy(), 406\nconvert back to DataFrame, 406\nto_period(), 384\nto_pickle(), 193\ncaution about long-term storage, 193\nto_timestamp(), 274\ntrace function for debugger, 522\ntransform(), 347\ntranspose() with T attribute, 102\nTrue, 39\ntry/except blocks, 74\ntuple(), 48\ntuples, 47-50\nexception types, 75\nmethods, 50\nmutable and immutable, 48\nrest elements, 50\nset list-like elements to, 61\nSQL query results, 200\nstring slicing, 36\nunpacking, 49\ntype (Windows) to print file to screen, 177\ntype casting, 40\nNumPy ndarrays, 90\nValueError, 91\ntype inference in reading text data, 177\ntzinfo type, 359\ntz_convert(), 376\ntz_localize(), 376, 377\nU\nufuncs (universal functions) for ndarrays, 105\nmethods, 106, 490-492\npandas objects and, 158\nwriting new in Python, 493\ncustom compiled via Numba, 502\nfaster with Numba, 501\nUInt16Dtype extension data type, 226\nUInt32Dtype extension data type, 226\nUInt64Dtype extension data type, 226\nUInt8Dtype extension data type, 226\nunary ufuncs, 105, 106\nunderscore (_)\ndata types with trailing underscores, 475\ntab completion and, 24\nunwanted variables, 50\nUnicode characters\nbackslash (\\) to escape in strings, 37\nbytes objects and, 38\nstrings as sequences of, 36\ntext mode default file behavior, 80\nunion(), 60\nDataFrame method, 137\nunique and other set logic\nis_unique() property of indexes, 164\nIndex \n| \n559",
      "content_length": 2382,
      "extraction_method": "Direct"
    },
    {
      "page_number": 578,
      "chapter": null,
      "content": "ndarrays, 115\npandas, 170-173\nrepeated instances, 236\nUS baby names dataset, 443-456\nnaming trends analysis, 448-456\ngender and naming, 455\nincrease in diversity, 449\nlast letter revolution, 452\nUSDA food database, 457-462\nuniversal functions (see ufuncs)\nUnix\ncat to print file to screen, 177, 184\ntime zone-aware Timestamps, 378\nunstack(), 249, 270-273\nupdate() for sets, 60\nupsampling, 387, 391\ntarget period as superperiod, 393\nUTC (coordinated universal time), 374, 378\nUTF-8 encoding\nbytes encoding, 38\nopen(), 76\nV\nvalue_counts(), 170, 236\nbar plot tip, 304\ncategoricals\nnew categories, 243\nperformance of, 242\nVanderPlas, Jake, 423\nvariables, 28\nbinary operators, 32\ncommand history input and output vari‐\nables, 515\ndynamic references, strong types, 29\nduck typing, 31\nmodule imports, 32\nnamespace, 67\nNone tested for via is operator, 34\noutput of shell command, 517\nstr immutable, 36\nunderscore (_) for unwanted, 50\nvectorization with NumPy arrays, 91, 108, 110\nvectorize() (NumPy), 493\nversion of Python used by book, 9\nvertical bar (|)\nOR, 32\nNumPy ndarrays, 99\nunion of two sets, 60\nvisualization\nabout, 281\nbook on data visualization, 317\nmatplotlib\nabout, 6, 281, 317\nAPI primer, 282\nconfiguration, 297\ndocumentation online, 286\ninvoking, 282\npatch objects, 295\nplots saved to files, 296\ntwo-dimensional NumPy array, 108\nother Python tools, 317\nseaborn and pandas, 298-316\nabout seaborn, 298, 306\nbar plots, 301-308\nbox plots, 315\ndensity plots, 309-310\ndocumentation online, 316\nfacet grids, 314-316\nhistograms, 309-310\nline plots, 298-301\nscatter or point plots, 311-313\nvstack() (NumPy), 479\nW\nweb API interactions, 197-199\nwebsite for book\nbook materials, 15\ninstallation instructions, 9\nweek of month (WOM) dates, 371\nwhere() (NumPy), 268\nwhile loops, 44\nNumPy array vectorization instead, 85, 91,\n108, 110\nperformance tip, 505\nwhitespace\nPython indentation, 26\nstrip() to trim, 227\ntext file delimiter, 179\nWickham, Hadley, 320, 443\nWilke, Claus O., 317\nWilliams, Ashley, 457-462\nWindows\nexit() to exit Python shell, 10, 18\nMiniconda installation, 9\nPython Tools for Visual Studio, 13\ntype to print file to screen, 177\nwritable() file, 79\nwrite() to a file, 79\n560 \n| \nIndex",
      "content_length": 2195,
      "extraction_method": "Direct"
    },
    {
      "page_number": 579,
      "chapter": null,
      "content": "writelines() to a file, 79\nwriting CSV files, 184\nwriting data to a file\nbinary data\nExcel format, 195\nHDF5 format, 504\nmemory-mapped files, 503\nndarrays saved, 116\npickle format, 193\npickle format caution, 193\nplots saved to files, 296\ntext data\nCSV files, 184\nJSON data, 189\nmissing data, 184\nother delimited format, 187\nsubset of columns, 185\nX\nxlim() (matplotlib), 290\nXML file format, 189\nreading, 190\nY\nyield in a function, 71\nZ\nzip files of datasets on GitHub, 15\nzip() for list of tuples, 62\nIndex \n| \n561",
      "content_length": 513,
      "extraction_method": "Direct"
    },
    {
      "page_number": 580,
      "chapter": null,
      "content": "About the Author\nWes McKinney is a Nashville-based software developer and entrepreneur. After\nfinishing his undergraduate degree in mathematics at MIT in 2007, he went on to do\nquantitative finance work at AQR Capital Management in Greenwich, CT. Frustrated\nby cumbersome data analysis tools, he learned Python and started building what\nwould later become the pandas project. He’s now an active member of the Python\ndata community and is an advocate for the use of Python in data analysis, finance,\nand statistical computing applications.\nWes was later the cofounder and CEO of DataPad, whose technology assets and\nteam were acquired by Cloudera in 2014. He has since become involved in big data\ntechnology, joining the Project Management Committees for the Apache Arrow and\nApache Parquet projects in the Apache Software Foundation. In 2018, he founded\nUrsa Labs, a not-for-profit organization focused on Apache Arrow development,\nin partnership with RStudio and Two Sigma Investments. In 2021, he cofounded\ntechnology startup Voltron Data, where he currently works as the Chief Technology\nOfficer.",
      "content_length": 1099,
      "extraction_method": "Direct"
    },
    {
      "page_number": 581,
      "chapter": null,
      "content": "Colophon\nThe animal on the cover of Python for Data Analysis is a golden-tailed, or pen-tailed,\ntree shrew (Ptilocercus lowii). The golden-tailed tree shrew is the only one of its\nspecies in the genus Ptilocercus and family Ptilocercidae; all the other tree shrews\nare of the family Tupaiidae. Tree shrews are identified by their long tails and soft\nred-brown fur. As nicknamed, the golden-tailed tree shrew has a tail that resembles\nthe feather on a quill pen. Tree shrews are omnivores, feeding primarily on insects,\nfruit, seeds, and small vertebrates.\nFound predominantly in Indonesia, Malaysia, and Thailand, these wild mammals are\nknown for their chronic consumption of alcohol. Malaysian tree shrews were found\nto spend several hours consuming the naturally fermented nectar of the bertam palm,\nequalling about 10 to 12 glasses of wine with 3.8% alcohol content. Despite this, no\ngolden-tailed tree shrew has ever been intoxicated, thanks largely to their impressive\nability to break down ethanol, which includes metabolizing the alcohol in a way\nnot used by humans. Also more impressive than any of their mammal counterparts,\nincluding humans, is their brain-to-body mass ratio.\nDespite its name, the golden-tailed shrew is not a true shrew; instead it is more\nclosely related to primates. Because of their close relation, tree shrews have become\nan alternative to primates in medical experimentation for myopia, psychosocial stress,\nand hepatitis.\nThe cover image is from Cassell’s Natural History. The cover fonts are URW Type‐\nwriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font is\nAdobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.",
      "content_length": 1690,
      "extraction_method": "Direct"
    },
    {
      "page_number": 582,
      "chapter": null,
      "content": "Learn from experts.  \nBecome one yourself.\nBooks | Live online courses   \nInstant Answers | Virtual events \nVideos | Interactive learning\nGet started at oreilly.com. \n©2022 O’Reilly Media, Inc. O’Reilly is a registered trademark of O’Reilly Media, Inc. | 175",
      "content_length": 258,
      "extraction_method": "Direct"
    }
  ],
  "enrichment": {
    "version": "1.0.0",
    "generated_by": "generate_chapter_metadata.py",
    "contains": [
      "keywords",
      "concepts",
      "summary"
    ]
  }
}