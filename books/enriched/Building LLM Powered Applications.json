{
  "metadata": {
    "title": "Building LLM Powered Applications",
    "author": "Valentina Alto",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 500,
    "conversion_date": "2025-11-28T12:02:05.050502",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Building LLM Powered Applications.pdf",
    "extraction_method": "PyMuPDF (Direct: 494, OCR: 6)"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "detection_method": "topic_boundary",
      "content": "EXPERT INSIGHT\n\nBuilding LLM Powered\nApplications\n\nCreate intelligent apps and agents with MEE\nlarge language models MEE\n\nLETT)\nGALT) Uy, {/) lj\nSY4LLT] [] [) ly\n\nVig\n\nCLEP LDR\n\nay My\n\nSSSSSSS ee\n\nSoS\nSS\nSSS\nSSOOCS\nSSK\noS\n\nValentina Alto <packt\n\nBuilding LLM Powered\nApplications\nCreate intelligent apps and agents\nwith large language models\nValentina Alto\n\n\nBuilding LLM Powered Applications\nCopyright © 2024 Packt Publishing\nAll rights reserved. No part of this book may be reproduced, stored in a\nretrieval system, or transmitted in any form or by any means, without the\nprior written permission of the publisher, except in the case of brief\nquotations embedded in critical articles or reviews.\nEvery effort has been made in the preparation of this book to ensure the\naccuracy of the information presented. However, the information contained\nin this book is sold without warranty, either express or implied. Neither the\nauthor, nor Packt Publishing or its dealers and distributors, will be held\nliable for any damages caused or alleged to have been caused directly or\nindirectly by this book.\nPackt Publishing has endeavored to provide trademark information about all\nof the companies and products mentioned in this book by the appropriate\nuse of capitals. However, Packt Publishing cannot guarantee the accuracy of\nthis information.\nSenior Publishing Product Manager: Tushar Gupta\nAcquisition Editors – Peer Reviews: Tejas Mhasvekar and Jane D'Souza\nProject Editor: Namrata Katare\nContent Development Editors: Shruti Menon and Bhavesh Amin\nCopy Editor: Safis Editing\nTechnical Editor: Anirudh Singh\n\n\nProofreader: Safis Editing\nIndexer: Subalakshmi Govindhan\nPresentation Designer: Ajay Patule\nDeveloper Relations Marketing Executive: Monika Sangwan\nFirst published: May 2024\nProduction reference: 1140524\nPublished by Packt Publishing Ltd.\nGrosvenor House\n11 St Paul’s Square\nBirmingham\nB3 1RB, UK.\nISBN 978-1-83546-231-7\nwww.packt.com\n\n\nContributors\nAbout the author\nValentina Alto is an AI enthusiast, tech author, and runner. After\ncompleting her master's in data science, she joined Microsoft in 2020,\nwhere she currently works as an AI specialist. Passionate about machine\nlearning and AI since the outset of her academic journey, Valentina has\ndeepened her knowledge in the field, authoring hundreds of articles on tech\nblogs. She also authored her first book with Packt, titled Modern\nGenerative AI with ChatGPT and OpenAI Models. In her current role, she\ncollaborates with large enterprises, aiming to integrate AI into their\nprocesses and create innovative solutions using large foundation models.\nBeyond her professional pursuits, Valentina loves hiking in the beautiful\nItalian mountains, running, traveling, and enjoying a good book with a cup\nof coffee.\n\n\nAbout the reviewers\nAlexandru Vesa has over a decade of expertise as an AI engineer and is\ncurrently serving as the CEO at Cube Digital, an AI software development\nfirm he leads with a vision inspired by the transformative potential of AI\nalgorithms. He has a wealth of experience in navigating diverse business\nenvironments and shaping AI products in both multinational corporations\nand dynamic startups. Drawing inspiration from various disciplines, he has\nbuilt a versatile skill set and seamlessly integrates state-of-the-art\ntechnologies with proven engineering methods. He is proficient in guiding\nprojects from inception to scalable success.\nAlex is a key figure in the DecodingML publication, collaborating with\nPaul Iusztin to curate the groundbreaking hands-on course LLM Twin:\nBuilding Your Production-Ready AI Replica, hosted on the Substack\nplatform. His problem-solving and communication skills make him an\nindispensable force in utilizing AI to foster innovation and achieve tangible\nresults.\nLouis Owen is a data scientist/AI engineer hailing from Indonesia.\nCurrently contributing to NLP solutions at Yellow.ai, a leading CX\nautomation platform, he thrives on delivering innovative solutions. Louis’s\ndiverse career spans various sectors, including NGO work with The World\nBank, e-commerce with Bukalapak and Tokopedia, conversational AI with\nYellow.ai, online travel with Traveloka, smart city initiatives with Qlue, and\nFinTech with Do-it. Louis has also written a book with Packt, titled\nHyperparameter Tuning with Python, and published several papers in the AI\nfield.\n\n\nOutside of work, Louis loves to spend time mentoring aspiring data\nscientists, sharing insights through articles, and indulging in his hobbies of\nwatching movies and working on side projects.\n\n\nJoin our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\nOceanofPDF.com\n",
      "page_number": 1,
      "chapter_number": 1,
      "summary": "This chapter covers segment 1 (pages 1-8). Key topics include editors, current, and skill. After\ncompleting her master's in data science, she joined Microsoft in 2020,\nwhere she currently works as an AI specialist.",
      "keywords": [
        "Building LLM Powered",
        "Packt Publishing",
        "EXPERT INSIGHT Building",
        "models MEE LETT",
        "LLM Powered",
        "Building LLM",
        "Alto Building LLM",
        "MEE LETT",
        "EXPERT INSIGHT",
        "Vig CLEP LDR",
        "LLM Powered Applications",
        "Valentina Alto",
        "Publishing",
        "book",
        "MEE large language"
      ],
      "concepts": [
        "editors",
        "current",
        "skill",
        "development",
        "products",
        "production",
        "llm",
        "louis",
        "models",
        "engineer"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 1,
          "title": "Segment 1 (pages 2-10)",
          "relevance_score": 0.71,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 9-16)",
      "start_page": 9,
      "end_page": 16,
      "detection_method": "topic_boundary",
      "content": "Contents\nPreface\nWho this book is for\nWhat this book covers\nTo get the most out of this book\nGet in touch\n1. Introduction to Large Language Models\nWhat are large foundation models and LLMs?\nAI paradigm shift – an introduction to foundation models\nUnder the hood of an LLM\nMost popular LLM transformers-based architectures\nEarly experiments\nIntroducing the transformer architecture\nTraining and evaluating LLMs\nTraining an LLM\nModel evaluation\nBase models versus customized models\nHow to customize your model\nSummary\nReferences\n2. LLMs for AI-Powered Applications\nHow LLMs are changing software development\nThe copilot system\nIntroducing AI orchestrators to embed LLMs into applications\nThe main components of AI orchestrators\nLangChain\nHaystack\nSemantic Kernel\nHow to choose a framework\nSummary\nReferences\n3. Choosing an LLM for Your Application\nThe most promising LLMs in the market\nProprietary models\nGPT-4\n\n\nGemini 1.5\nClaude 2\nOpen-source models\nLLaMA-2\nFalcon LLM\nMistral\nBeyond language models\nA decision framework to pick the right LLM\nConsiderations\nCase study\nSummary\nReferences\n4. Prompt Engineering\nTechnical requirements\nWhat is prompt engineering?\nPrinciples of prompt engineering\nClear instructions\nSplit complex tasks into subtasks\nAsk for justification\nGenerate many outputs, then use the model to pick the best\none\nRepeat instructions at the end\nUse delimiters\nAdvanced techniques\nFew-shot approach\nChain of thought\nReAct\nSummary\nReferences\n5. Embedding LLMs within Your Applications\nTechnical requirements\nA brief note about LangChain\nGetting started with LangChain\nModels and prompts\nData connections\nMemory\nChains\n\n\nAgents\nWorking with LLMs via the Hugging Face Hub\nCreate a Hugging Face user access token\nStoring your secrets in an .env file\nStart using open-source LLMs\nSummary\nReferences\n6. Building Conversational Applications\nTechnical requirements\nGetting started with conversational applications\nCreating a plain vanilla bot\nAdding memory\nAdding non-parametric knowledge\nAdding external tools\nDeveloping the front-end with Streamlit\nSummary\nReferences\n7. Search and Recommendation Engines with LLMs\nTechnical requirements\nIntroduction to recommendation systems\nExisting recommendation systems\nK-nearest neighbors\nMatrix factorization\nNeural networks\nHow LLMs are changing recommendation systems\nImplementing an LLM-powered recommendation system\nData preprocessing\nBuilding a QA recommendation chatbot in a cold-start\nscenario\nBuilding a content-based system\nDeveloping the front-end with Streamlit\nSummary\nReferences\n8. Using LLMs with Structured Data\nTechnical requirements\nWhat is structured data?\nGetting started with relational databases\n\n\nIntroduction to relational databases\nOverview of the Chinook database\nHow to work with relational databases in Python\nImplementing the DBCopilot with LangChain\nLangChain agents and SQL Agent\nPrompt engineering\nAdding further tools\nDeveloping the front-end with Streamlit\nSummary\nReferences\n9. Working with Code\nTechnical requirements\nChoosing the right LLM for code\nCode understanding and generation\nFalcon LLM\nCodeLlama\nStarCoder\nAct as an algorithm\nLeveraging Code Interpreter\nSummary\nReferences\n10. Building Multimodal Applications with LLMs\nTechnical requirements\nWhy multimodality?\nBuilding a multimodal agent with LangChain\nOption 1: Using an out-of-the-box toolkit for Azure AI Services\nGetting Started with AzureCognitiveServicesToolkit\nSetting up the toolkit\nLeveraging a single tool\nLeveraging multiple tools\nBuilding an end-to-end application for invoice analysis\nOption 2: Combining single tools into one agent\nYouTube tools and Whisper\nDALL·E and text generation\nPutting it all together\nOption 3: Hard-coded approach with a sequential chain\nComparing the three options\n\n\nDeveloping the front-end with Streamlit\nSummary\nReferences\n11. Fine-Tuning Large Language Models\nTechnical requirements\nWhat is fine-tuning?\nWhen is fine-tuning necessary?\nGetting started with fine-tuning\nObtaining the dataset\nTokenizing the data\nFine-tuning the model\nUsing evaluation metrics\nTraining and saving\nSummary\nReferences\n12. Responsible AI\nWhat is Responsible AI and why do we need it?\nResponsible AI architecture\nModel level\nMetaprompt level\nUser interface level\nRegulations surrounding Responsible AI\nSummary\nReferences\n13. Emerging Trends and Innovations\nThe latest trends in language models and generative AI\nGPT-4V(ision)\nDALL-E 3\nAutoGen\nSmall language models\nCompanies embracing generative AI\nCoca-Cola\nNotion\nMalbek\nMicrosoft\nSummary\nReferences\n\n\nOther Books You May Enjoy\nIndex\nOceanofPDF.com\n\n\nPreface\nWith this book, we embark upon an exploration of large language models\n(LLMs) and the transformative paradigm they represent within the realm of\nartificial intelligence (AI). This comprehensive guide helps you delve into\nthe fundamental concepts, from solid theoretical foundations of these\ncutting-edge technologies to practical applications that LLMs offer,\nultimately converging on the ethical and responsible considerations while\nusing generative AI solutions. This book aims to provide you with a firm\nunderstanding of how the emerging LLMs in the market can impact\nindividuals, large enterprises, and society. It focuses on how to build\npowerful applications powered by LLMs, leveraging new AI orchestrators\nsuch as LangChain and uncovering new trends in modern application\ndevelopment.\nBy the end of this book, you will be able to navigate the rapidly evolving\necosystem of generative AI solutions more easily; plus, you will have the\ntools to get the most out of LLMs in both your daily tasks and your\nbusinesses. Let’s get started!\n",
      "page_number": 9,
      "chapter_number": 2,
      "summary": "Contents\nPreface\nWho this book is for\nWhat this book covers\nTo get the most out of this book\nGet in touch\n1 Key topics include models, llms, and summary.",
      "keywords": [
        "LLMs",
        "Summary",
        "Models",
        "Technical requirements",
        "References",
        "Language Models",
        "LLM",
        "Applications",
        "Technical",
        "requirements",
        "Large Language Models",
        "Language",
        "front-end with Streamlit",
        "started",
        "LangChain"
      ],
      "concepts": [
        "models",
        "llms",
        "summary",
        "references",
        "llm",
        "applications",
        "application",
        "recommendation",
        "generate",
        "generation"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 474-483)",
          "relevance_score": 0.75,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.75,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.7,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.7,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 17-24)",
      "start_page": 17,
      "end_page": 24,
      "detection_method": "topic_boundary",
      "content": "Who this book is for\nThe book is designed to mainly appeal to a technical audience with some\nbasic Python code foundations. However, the theoretical chapters and the\nhands-on exercises are based on generative AI foundations and industry-led\nuse cases, which might be of interest to non-technical audiences as well.\nOverall, the book caters to individuals interested in gaining a\ncomprehensive understanding of the transformative power of LLMs and\ndefine, enabling them to navigate the rapidly evolving AI landscape with\nconfidence and foresight. All kinds of readers are welcome, but readers who\ncan benefit the most from this book include:\nSoftware developers and engineers: This book provides practical\nguidance for developers looking to build applications leveraging\nLLMs. It covers integrating LLMs into app backends, APIs,\narchitectures, and so on.\nData scientists: For data scientists interested in deploying LLMs for\nreal-world usage, this book shows how to take models from research to\nproduction. It covers model serving, monitoring, and optimization.\nAI/ML engineers: Engineers focused on AI/ML applications can\nleverage this book to understand how to architect and deploy LLMs as\npart of intelligent systems and agents.\nTechnical founders/CTOs: Startup founders and CTOs can use this\nbook to evaluate if and how LLMs could be used within their apps and\nproducts. It provides a technical overview alongside business\nconsiderations.\n\n\nStudents: Graduate students and advanced undergraduates studying\nAI, ML, natural language processing (NLP), or computer science\ncan learn how LLMs are applied in practice from this book.\nLLM researchers: Researchers working on novel LLM architectures,\ntraining techniques, and so on will gain insight into real-world model\nusage and the associated challenges.\n\n\nWhat this book covers\nChapter 1, Introduction to Large Language Models, provides an\nintroduction to and deep dive into LLMs, a powerful set of deep learning\nneural networks in the domain of generative AI. It introduces the concept of\nLLMs, their differentiators from classical machine learning models, and the\nrelevant jargon. It also discusses the architecture of the most popular LLMs,\nmoving on to explore how LLMs are trained and consumed and compare\nbase LLMs with fine-tuned LLMs. By the end of this chapter, you will have\nthe foundations of what LLMs are and their positioning in the landscape of\nAI, creating the basis for the subsequent chapters.\nChapter 2, LLMs for AI-Powered Applications, explores how LLMs are\nrevolutionizing the world of software development, leading to a new era of\nAI-powered applications. By the end of this chapter, you will have a clearer\npicture of how LLMs can be embedded in different application scenarios,\nwith the help of new AI orchestrator frameworks that are currently available\nin the AI development market.\nChapter 3, Choosing an LLM for Your Application, highlights how different\nLLMs may have different architectures, sizes, training data, capabilities,\nand limitations. Choosing the right LLM for your application is not a trivial\ndecision as it can significantly impact the performance, quality, and cost of\nyour solution. In this chapter, we will navigate the process of choosing the\nright LLM for your application. We will discuss the most promising LLMs\nin the market, the main criteria and tools to use when comparing LLMs, and\nthe various trade-offs between size and performance. By the end of this\nchapter, you should have a clear understanding of how to choose the right\nLLM for your application and how to use it effectively and responsibly.\n\n\nChapter 4, Prompt Engineering, explains how prompt engineering is a\ncrucial activity while designing LLM-powered applications since prompts\nhave a massive impact on the performance of LLMs. In fact, there are\nseveral techniques that can be implemented to not only to refine your\nLLM’s responses but also reduce risks associated with hallucination and\nbiases. In this chapter, we will cover the emerging techniques in the field of\nprompt engineering, from basic approaches up to advanced frameworks. By\nthe end of this chapter, you will have the foundations to build functional\nand solid prompts for your LLM-powered applications, which will also be\nrelevant in the upcoming chapters.\nChapter 5, Embedding LLMs within Your Applications, discusses a new set\nof components introduced into the landscape of software development with\nthe advent of developing applications with LLMs. To make it easier to\norchestrate LLMs and their related components in an application flow,\nseveral AI frameworks have emerged, of which LangChain is one of the\nmost widely used. In this chapter, we will take a deep dive into LangChain\nand how to use it, and learn how to call open-source LLM APIs into code\nvia Hugging Face Hub and manage prompt engineering. By the end of this\nchapter, you will have the technical foundations to start developing your\nLLM-powered applications using LangChain and open-source Hugging\nFace models.\nChapter 6, Building Conversational Applications, allows us to embark on\nthe hands-on section of this book with your first concrete implementation of\nLLM-powered applications. Throughout this chapter, we will cover a step-\nby-step implementation of a conversational application, using LangChain\nand its components. We will configure the schema of a simple chatbot,\nadding a memory component, non-parametric knowledge, and tools to make\n\n\nthe chatbot “agentic.” By the end of this chapter, you will be able to set up\nyour own conversational application project with just a few lines of code.\nChapter 7, Search and Recommendation Engines with LLMs, explores how\nLLMs can enhance recommendation systems, using both embeddings and\ngenerative models. We will discuss the definition and evolution of\nrecommendation systems, learn how generative AI is impacting this field of\nresearch, and understand how to build recommendation systems with\nLangChain. By the end of this chapter, you will be able to create your own\nrecommendation application and leverage state-of-the-art LLMs using\nLangChain as the framework.\nChapter 8, Using LLMs with Structured Data, covers a great capability of\nLLMs: the ability to handle structured, tabular data. We will see how, with\nplug-ins and an agentic approach, we can use LLMs as a natural language\ninterface between us and our structured data, reducing the gap between the\nbusiness user and the structured information. To demonstrate this, we will\nbuild a database copilot with LangChain. By the end of this chapter, you\nwill be able to build your own natural language interface for your data\nestate, combining unstructured with structured sources.\nChapter 9, Working with Code, covers another great capability of LLMs:\nworking with programming languages. In the previous chapter, we’ve\nalready seen a glimpse of this capability, when we asked our LLM to\ngenerate SQL queries against a SQL Database. In this chapter, we are going\nto examine in which other ways LLMs can be used with code, from\n“simple” code understanding and generation to the building of applications\nthat behave as if they were an algorithm. By the end of this chapter, you\nwill be able to build LLM-powered applications for your coding projects, as\n\n\nwell as build LLM-powered applications with natural language interfaces to\nwork with code.\nChapter 10, Building Multimodal Applications with LLMs, goes beyond\nLLMs, introducing the concept of multi-modality while building agents. We\nwill see the logic behind the combination of foundation models in different\nAI domains – language, images, audio – into one single agent that can adapt\nto a variety of tasks. You will learn how to build a multi-modal agent with\nsingle-modal LLMs using LangChain. By the end of this chapter, you will\nbe able to build your own multi-modal agent, providing it with the tools and\nLLMs needed to perform various AI tasks.\nChapter 11, Fine-Tuning Large Language Models, covers the technical\ndetails of fine-tuning LLMs, from the theory behind it to hands-on\nimplementation with Python and Hugging Face. We will delve into how you\ncan prepare your data to fine-tune a base model on your data, as well as\ndiscuss hosting strategies for your fine-tuned model. By the end of this\nchapter, you will be able to fine-tune an LLM on your own data so that you\ncan build domain-specific applications powered by that LLM.\nChapter 12, Responsible AI, introduces the fundamentals of the discipline\nbehind the mitigation of the potential harms of LLMs – and AI models in\ngeneral – that is, responsible AI. This is important because LLMs open the\ndoors to a new set of risks and biases to be taken into account while\ndeveloping LLM-powered applications.\nWe will then move on to the risks associated with LLMs and how to prevent\nor, at the very least, mitigate them using proper techniques. By the end of\nthis chapter, you will have a deeper understanding of how to prevent LLMs\nfrom making your application potentially harmful.\n\n\nChapter 13, Emerging Trends and Innovations, explores the latest\nadvancements and future trends in the field of generative AI.\nTo get the most out of this\nbook\nThis book aims to provide a solid theoretical foundation of what LLMs are,\ntheir architecture, and why they are revolutionizing the field of AI. It adopts\na hands-on approach, providing you with a step-by-step guide to\nimplementing LLMs-powered apps for specific tasks and using powerful\nframeworks like LangChain. Furthermore, each example will showcase the\nusage of a different LLM, so that you can appreciate their differentiators\nand when to use the proper model for a given task.\nOverall, the book combines theoretical concepts with practical applications,\nmaking it an ideal resource for anyone who wants to gain a solid foundation\nin LLMs and their applications in NLP. The following pre-requisites will\nhelp you to get the most out of this book:\nA basic understanding of the math behind neural networks (linear\nalgebra, neurons and parameters, and loss functions)\nA basic understanding of ML concepts, such as training and test sets,\nevaluation metrics, and NLP\nA basic understanding of Python\nDownload the example code\nfiles\n",
      "page_number": 17,
      "chapter_number": 3,
      "summary": "All kinds of readers are welcome, but readers who\ncan benefit the most from this book include:\nSoftware developers and engineers: This book provides practical\nguidance for developers looking to build applications leveraging\nLLMs Key topics include llms, models.",
      "keywords": [
        "LLMs",
        "applications",
        "LLM",
        "book",
        "LLM-powered applications",
        "end",
        "Data",
        "build",
        "models",
        "LangChain",
        "language",
        "code",
        "build LLM-powered applications",
        "Large Language Models",
        "LLM-powered"
      ],
      "concepts": [
        "llms",
        "models",
        "llm",
        "code",
        "coding",
        "language",
        "data",
        "book",
        "developers",
        "building"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.73,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.7,
          "method": "sentence_transformers"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 1,
          "title": "Segment 1 (pages 2-10)",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.67,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 474-483)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 25-33)",
      "start_page": 25,
      "end_page": 33,
      "detection_method": "topic_boundary",
      "content": "The code bundle for the book is hosted on GitHub at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications. We also have other code bundles from our\nrich catalog of books and videos available at\nhttps://github.com/PacktPublishing/. Check them out!\nDownload the color images\nWe also provide a PDF file that has color images of the\nscreenshots/diagrams used in this book. You can download it here:\nhttps://packt.link/gbp/9781835462317.\nConventions used\nThere are a number of text conventions used throughout this book.\nCodeInText: Indicates code words in text, database table names, folder\nnames, filenames, file extensions, pathnames, dummy URLs, user input,\nand Twitter handles. For example: “I set the two variables system_message\nand instructions.”\nA block of code is set as follows:\n[default]\n$pip install openai == 0.28\nimport os\nimport openai\nopenai.api_key = os.environment.get('OPENAI_API_KEY')\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-35-turbo\", # engine = \"deployment_name\".\n    messages=[\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": instructions},\n\n\n    ]\n)\nAny command-line input or output is written as follows:\n{'text': \"Terrible movie. Nuff Said.[…]\n 'label': 0}\nBold: Indicates a new term, an important word, or words that you see on\nthe screen. For instance, words in menus or dialog boxes appear in the text\nlike this. For example: “[…] he found that repeating the main instruction at\nthe end of the prompt can help the model to overcome its inner recency\nbias.”\nWarnings or important notes appear like this.\nTips and tricks appear like this.\nGet in touch\nFeedback from our readers is always welcome.\nGeneral feedback: Email feedback@packtpub.com and mention the book’s\ntitle in the subject of your message. If you have questions about any aspect\nof this book, please email us at questions@packtpub.com.\nErrata: Although we have taken every care to ensure the accuracy of our\ncontent, mistakes do happen. If you have found a mistake in this book, we\nwould be grateful if you reported this to us. Please visit\n\n\nhttp://www.packtpub.com/submit-errata, click Submit\nErrata, and fill in the form.\nPiracy: If you come across any illegal copies of our works in any form on\nthe internet, we would be grateful if you would provide us with the location\naddress or website name. Please contact us at copyright@packtpub.com with\na link to the material.\nIf you are interested in becoming an author: If there is a topic that you\nhave expertise in and you are interested in either writing or contributing to a\nbook, please visit http://authors.packtpub.com.\n\n\nShare your thoughts\nOnce you’ve read Building LLM Powered Application, we’d love to hear\nyour thoughts! Please click here to go straight to the\nAmazon review page for this book and share your feedback.\nYour review is important to us and the tech community and will help us\nmake sure we’re delivering excellent quality content.\n\n\nDownload a free PDF copy of\nthis book\nThanks for purchasing this book!\nDo you like to read on the go but are unable to carry your print books\neverywhere?\nIs your eBook purchase not compatible with the device of your choice?\nDon’t worry, now with every Packt book you get a DRM-free PDF version\nof that book at no cost.\nRead anywhere, any place, on any device. Search, copy, and paste code\nfrom your favorite technical books directly into your application.\nThe perks don’t stop there, you can get exclusive access to discounts,\nnewsletters, and great free content in your inbox daily.\nFollow these simple steps to get the benefits:\n1. Scan the QR code or visit the link below:\n\n\nhttps://packt.link/free-ebook/9781835462317\n2. Submit your proof of purchase.\n3. That’s it! We’ll send your free PDF and other benefits to your email\ndirectly.\nOceanofPDF.com\n\n\n1\nIntroduction to Large\nLanguage Models\nDear reader, welcome to Building Large Language Model Applications! In\nthis book, we will explore the fascinating world of a new era of application\ndevelopments, where large language models (LLMs) are the main\nprotagonists.\nDuring the last year, we all learned the power of generative artificial\nintelligence (AI) tools such as ChatGPT, Bing Chat, Bard, and Dall-E. What\nimpressed us the most was their stunning capabilities of generating human-\nlike content based on user requests made in natural language. It is, in fact,\ntheir conversational capabilities that made them so easily consumable and,\ntherefore, popular as soon as they entered the market. Thanks to this phase,\nwe learned to acknowledge the power of generative AI and its core models:\nLLMs. However, LLMs are more than language generators. They can be also\nseen as reasoning engines that can become the brains of our intelligent\napplications.\nIn this book, we will see the theory and practice of how to build LLM-\npowered applications, addressing a variety of scenarios and showing new\ncomponents and frameworks that are entering the domain of software\ndevelopment in this new era of AI. The book will start with Part 1, where we\nwill introduce the theory behind LLMs, the most promising LLMs in the\n\n\nmarket right now, and the emerging frameworks for LLMs-powered\napplications. Afterward, we will move to a hands-on part where we will\nimplement many applications using various LLMs, addressing different\nscenarios and real-world problems. Finally, we will conclude the book with a\nthird part, covering the emerging trends in the field of LLMs, alongside the\nrisk of AI tools and how to mitigate them with responsible AI practices.\nSo, let’s dive in and start with some definitions of the context we are moving\nin. This chapter provides an introduction and deep dive into LLMs, a\npowerful set of deep learning neural networks that feature the domain of\ngenerative AI.\nIn this chapter, we will cover the following topics:\nUnderstanding LLMs, their differentiators from classical machine\nlearning models, and their relevant jargon\nOverview of the most popular LLM architectures\nHow LLMs are trained and consumed\nBase LLMs versus fine-tuned LLMs\nBy the end of this chapter, you will have the fundamental knowledge of what\nLLMs are, how they work, and how you can make them more tailored to\nyour applications. This will also pave the way for the concrete usage of\nLLMs in the hands-on part of this book, where we will see in practice how to\nembed LLMs within your applications.\nWhat are large foundation\nmodels and LLMs?\nLLMs are deep-learning-based models that use many parameters to learn\nfrom vast amounts of unlabeled texts. They can perform various natural\n\n\nlanguage processing tasks such as recognizing, summarizing, translating,\npredicting, and generating text.\nDefinition\nDeep learning is a branch of machine learning that is\ncharacterized by neural networks with multiple layers, hence\nthe term “deep.” These deep neural networks can\nautomatically learn hierarchical data representations, with\neach layer extracting increasingly abstract features from the\ninput data. The depth of these networks refers to the number\nof layers they possess, enabling them to effectively model\nintricate relationships and patterns in complex datasets.\nLLMs belong to a wider set of models that feature the AI subfield of\ngenerative AI: large foundation models (LFMs). Hence, in the following\nsections, we will explore the rise and development of LFMs and LLMs, as\nwell as their technical architecture, which is a crucial task to understand their\nfunctioning and properly adopt those technologies within your applications.\nWe will start by understanding why LFMs and LLMs differ from traditional\nAI models and how they represent a paradigm shift in this field. We will then\nexplore the technical functioning of LLMs, how they work, and the\nmechanisms behind their outcomes.\nAI paradigm shift – an\nintroduction to foundation\nmodels\n",
      "page_number": 25,
      "chapter_number": 4,
      "summary": "This chapter covers segment 4 (pages 25-33). Key topics include model, llms, and content. We also have other code bundles from our\nrich catalog of books and videos available at\nhttps://github.com/PacktPublishing/.",
      "keywords": [
        "LLMs",
        "book",
        "Models",
        "Applications",
        "hosted on GitHub",
        "code",
        "Language",
        "content",
        "Language Model Applications",
        "Large",
        "PDF",
        "large foundation models",
        "Language Models",
        "text",
        "foundation models"
      ],
      "concepts": [
        "model",
        "llms",
        "content",
        "text",
        "learned",
        "book",
        "free",
        "code",
        "application",
        "applications"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "Segment 28 (pages 232-240)",
          "relevance_score": 0.71,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 13,
          "title": "Segment 13 (pages 104-111)",
          "relevance_score": 0.7,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 474-483)",
          "relevance_score": 0.7,
          "method": "sentence_transformers"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 7,
          "title": "Segment 7 (pages 52-61)",
          "relevance_score": 0.7,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 82-102)",
          "relevance_score": 0.69,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 34-41)",
      "start_page": 34,
      "end_page": 41,
      "detection_method": "topic_boundary",
      "content": "A foundation model refers to a type of pre-trained generative AI model that\noffers immense versatility by being adaptable for various specific tasks.\nThese models undergo extensive training on vast and diverse datasets,\nenabling them to grasp general patterns and relationships within the data –\nnot just limited to textual but also covering other data formats such as\nimages, audio, and video. This initial pre-training phase equips the models\nwith a strong foundational understanding across different domains, laying\nthe groundwork for further fine-tuning. This cross-domain capability\ndifferentiates generative AI models from standard natural language\nunderstanding (NLU) algorithms.\nNote\nGenerative AI and NLU algorithms are both related to\nnatural language processing (NLP), which is a branch of\nAI that deals with human language. However, they have\ndifferent goals and applications.\nThe difference between generative AI and NLU algorithms is\nthat generative AI aims to create new natural language\ncontent, while NLU algorithms aim to understand existing\nnatural language content. Generative AI can be used for tasks\nsuch as text summarization, text generation, image\ncaptioning, or style transfer. NLU algorithms can be used for\ntasks such as chatbots, question answering, sentiment\nanalysis, or machine translation.\nFoundation models are designed with transfer learning in mind, meaning\nthey can effectively apply the knowledge acquired during pre-training to\n\n\nnew, related tasks. This transfer of knowledge enhances their adaptability,\nmaking them efficient at quickly mastering new tasks with relatively little\nadditional training.\nOne notable characteristic of foundation models is their large architecture,\ncontaining millions or even billions of parameters. This extensive scale\nenables them to capture complex patterns and relationships within the data,\ncontributing to their impressive performance across various tasks.\nDue to their comprehensive pre-training and transfer learning capabilities,\nfoundation models exhibit strong generalization skills. This means they can\nperform well across a range of tasks and efficiently adapt to new, unseen\ndata, eliminating the need for training separate models for individual tasks.\nThis paradigm shift in artificial neural network design offers considerable\nadvantages, as foundation models, with their diverse training datasets, can\nadapt to different tasks based on users’ intent without compromising\nperformance or efficiency. In the past, creating and training distinct neural\nnetworks for each task, such as named entity recognition or sentiment\nanalysis, would have been necessary, but now, foundation models provide a\nunified and powerful solution for multiple applications.\n\n\nFigure 1.1: From task-specific models to general models\nNow, we said that LFMs are trained on a huge amount of heterogeneous data\nin different formats. Whenever that data is unstructured, natural language\ndata, we refer to the output LFM as an LLM, due to its focus on text\nunderstanding and generation.\nFigure 1.2: Features of LLMs\n\n\nWe can then say that an LLM is a type of foundation model specifically\ndesigned for NLP tasks. These models, such as ChatGPT, BERT, Llama, and\nmany others, are trained on vast amounts of text data and can generate\nhuman-like text, answer questions, perform translations, and more.\nNevertheless, LLMs aren’t limited to performing text-related tasks. As we\nwill see throughout the book, those unique models can be seen as reasoning\nengines, extremely good in common sense reasoning. This means that they\ncan assist us in complex tasks, analytical problem-solving, enhanced\nconnections, and insights among pieces of information.\nIn fact, as LLMs mimic the way our brains are made (as we will see in the\nnext section), their architectures are featured by connected neurons. Now,\nhuman brains have about 100 trillion connections, way more than those\nwithin an LLM. Nevertheless, LLMs have proven to be much better at\npacking a lot of knowledge into those fewer connections than we are.\nUnder the hood of an LLM\nLLMs are a particular type of artificial neural networks (ANNs):\ncomputational models inspired by the structure and functioning of the human\nbrain. They have proven to be highly effective in solving complex problems,\nparticularly in areas like pattern recognition, classification, regression, and\ndecision-making tasks.\nThe basic building block of an ANN is the artificial neuron, also known as a\nnode or unit. These neurons are organized into layers, and the connections\nbetween neurons are weighted to represent the strength of the relationship\nbetween them. Those weights represent the parameters of the model that\nwill be optimized during the training process.\n\n\nANNs are, by definition, mathematical models that work with numerical\ndata. Hence, when it comes to unstructured, textual data as in the context of\nLLMs, there are two fundamental activities that are required to prepare data\nas model input:\nTokenization: This is the process of breaking down a piece of text (a\nsentence, paragraph, or document) into smaller units called tokens.\nThese tokens can be words, subwords, or even characters, depending on\nthe chosen tokenization scheme or algorithm. The goal of tokenization\nis to create a structured representation of the text that can be easily\nprocessed by machine learning models.\nFigure 1.3: Example of tokenization\nEmbedding: Once the text has been tokenized, each token is converted\ninto a dense numerical vector called an embedding. Embeddings are a\nway to represent words, subwords, or characters in a continuous vector\nspace. These embeddings are learned during the training of the\nlanguage model and capture semantic relationships between tokens.\nThe numerical representation allows the model to perform\nmathematical operations on the tokens and understand the context in\nwhich they appear.\n\n\nFigure 1.4: Example of embedding\nIn summary, tokenization breaks down text into smaller units called tokens,\nand embeddings convert these tokens into dense numerical vectors. This\nrelationship allows LLMs to process and understand textual data in a\nmeaningful and context-aware manner, enabling them to perform a wide\nrange of NLP tasks with impressive accuracy.\nFor example, let’s consider a two-dimensional embedding space where we\nwant to vectorize the words Man, King, Woman, and Queen. The idea is that\nthe mathematical distance between each pair of those words should be\nrepresentative of their semantic similarity. This is illustrated by the following\ngraph:\nFigure 1.5: Example of words embedding in a 2D space\n\n\nAs a result, if we properly embed the words, the relationship King – Man +\nWoman ≈ Queen should hold.\nOnce we have the vectorized input, we can pass it into the multi-layered\nneural network. There are three main types of layers:\nInput layer: The first layer of the neural network receives the input\ndata. Each neuron in this layer corresponds to a feature or attribute of\nthe input data.\nHidden layers: Between the input and output layers, there can be one\nor more hidden layers. These layers process the input data through a\nseries of mathematical transformations and extract relevant patterns and\nrepresentations from the data.\nOutput layer: The final layer of the neural network produces the\ndesired output, which could be predictions, classifications, or other\nrelevant results depending on the task the neural network is designed\nfor.\n\n\nFigure 1.6: High-level architecture of a generic ANN\nThe process of training an ANN involves the process of backpropagation\nby iteratively adjusting the weights of the connections between neurons\nbased on the training data and the desired outputs.\nDefinition\nBackpropagation is an algorithm used in deep learning to\ntrain neural networks. It involves two phases: the forward\npass, where data is passed through the network to compute\nthe output, and the backward pass, where errors are\npropagated backward to update the network’s parameters and\n",
      "page_number": 34,
      "chapter_number": 5,
      "summary": "This chapter covers segment 5 (pages 34-41). Key topics include models, data, and training. These models undergo extensive training on vast and diverse datasets,\nenabling them to grasp general patterns and relationships within the data –\nnot just limited to textual but also covering other data formats such as\nimages, audio, and video.",
      "keywords": [
        "models",
        "data",
        "tasks",
        "NLU algorithms",
        "Foundation models",
        "natural language",
        "NLU",
        "LLMs",
        "training",
        "neural network",
        "text",
        "language",
        "neural",
        "foundation",
        "generative"
      ],
      "concepts": [
        "models",
        "data",
        "training",
        "tasks",
        "generative",
        "generation",
        "generate",
        "layers",
        "tokenization",
        "text"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.7,
          "method": "sentence_transformers"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 58,
          "title": "Segment 58 (pages 514-521)",
          "relevance_score": 0.69,
          "method": "sentence_transformers"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 9,
          "title": "Segment 9 (pages 72-80)",
          "relevance_score": 0.67,
          "method": "sentence_transformers"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 10,
          "title": "Segment 10 (pages 78-85)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 45,
          "title": "Segment 45 (pages 386-394)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 42-50)",
      "start_page": 42,
      "end_page": 50,
      "detection_method": "topic_boundary",
      "content": "improve its performance. This iterative process helps the\nnetwork learn from data and make accurate predictions.\nDuring backpropagation, the network learns by comparing its predictions\nwith the ground truth and minimizing the error or loss between them. The\nobjective of training is to find the optimal set of weights that enables the\nneural network to make accurate predictions on new, unseen data.\nANNs can vary in architecture, including the number of layers, the number\nof neurons in each layer, and the connections between them.\nWhen it comes to generative AI and LLMs, their remarkable capability of\ngenerating text based on our prompts is based on the statistical concept of\nBayes’ theorem.\nDefinition\nBayes’ theorem, named after the Reverend Thomas Bayes, is\na fundamental concept in probability theory and statistics. It\ndescribes how to update the probability of a hypothesis based\non new evidence. Bayes’ theorem is particularly useful when\nwe want to make inferences about unknown parameters or\nevents in the presence of uncertainty. According to Bayes’\ntheorem, given two events, A and B, we can define the\nconditional probability of A given B as:\nWhere:\n\n\nP(B|A) = probability of B occurring given A, also\nknown as the likelihood of A given a fixed B.\nP(A|B) = probability of A occurring, given B; also\nknown as the posterior probability of A, given B.\nP(A) and P(B) = probability of observing A or B without\nany conditions.\nBayes’ theorem relates the conditional probability of an event based on new\nevidence with the a priori probability of the event. Translated into the\ncontext of LLMs, we are saying that such a model functions by predicting\nthe next most likely word, given the previous words prompted by the user.\nBut how can LLMs know which is the next most likely word? Well, thanks\nto the enormous amount of data on which LLMs have been trained (we will\ndive deeper into the process of training an LLM in the next sections). Based\non the training text corpus, the model will be able to identify, given a user’s\nprompt, the next most likely word or, more generally, text completion.\nFor example, let’s consider the following prompt: “The cat is on the….” and\nwe want our LLM to complete this sentence. However, the LLM may\ngenerate multiple candidate words, so we need a method to evaluate which\nof the candidates is the most likely one. To do so, we can use Bayes’ theorem\nto select the most likely word given the context. Let’s see the required steps:\nPrior probability P(A): The prior probability represents the probability\nof each candidate word being the next word in the context, based on the\nlanguage model’s knowledge learned during training. Let’s assume the\nLLM has three candidate words: “table,” “chair,” and “roof.”\nP(“table”), P(“chain”), and P(“roof”) are the prior probabilities for\neach candidate word, based on the language model’s knowledge of the\n\n\nfrequency of these words in the training data.\nLikelihood (P(B|A)): The likelihood represents how well each\ncandidate word fits the context “The cat is on the....” This is the\nprobability of observing the context given each candidate word. The\nLLM calculates this based on the training data and how often each word\nappears in similar contexts.\nFor example, if the LLM has seen many instances of “The cat is on the\ntable,” it would assign a high likelihood to “table” as the next word in\nthe given context. Similarly, if it has seen many instances of “The cat\nis on the chair,” it would assign a high likelihood to “chair” as the next\nword.\nP(“The cat is on the table”), P(“The cat is on the chair”), and P(“The\ncat is on the roof”) are the likelihoods for each candidate word given\nthe context.\nPosterior probability (P(A|B)): Using Bayes’ theorem, we can\ncalculate the posterior probability for each candidate word based on the\nprior probability and the likelihood:\nSelecting the most likely word. After calculating the posterior\nprobabilities for each candidate word, we choose the word with the\n\n\nhighest posterior probability as the most likely next word to complete\nthe sentence.\nThe LLM uses Bayes’ theorem and the probabilities learned during training\nto generate text that is contextually relevant and meaningful, capturing\npatterns and associations from the training data to complete sentences in a\ncoherent manner.\nThe following figure illustrates how it translates into the architectural\nframework of a neural network:\nFigure 1.7: Predicting the next most likely word in an LLM\nNote\nThe last layer of the ANN is typically a non-linear activation\nfunction. In the above illustration, the function is Softmax, a\nmathematical function that converts a vector of real numbers\n\n\ninto a probability distribution. It is often used in machine\nlearning to normalize the output of a neural network or a\nclassifier. The Softmax function is defined as follows:\nwhere zi is the i-th element of the input vector, and K is the\nnumber of elements in the vector. The Softmax function\nensures that each element of the output vector is between 0\nand 1 and that the sum of all elements is 1. This makes the\noutput vector suitable for representing probabilities of\ndifferent classes or outcomes.\nOverall, ANNs are the core pillars of the development of generative AI\nmodels: thanks to their mechanisms of tokenization, embedding, and\nmultiple hidden layers, they can capture complex patterns even in the most\nunstructured data, such as natural language.\nHowever, what we are observing today is a set of models that demonstrates\nincredible capabilities that have never been seen before, and this is due to a\nparticular ANNs’ architectural framework, introduced in recent years and the\nmain protagonist of LLM development. This framework is called the\ntransformer, and we are going to cover it in the following section.\nMost popular LLM\ntransformers-based\narchitectures\n\n\nANNs, as we saw in the preceding sections, are at the heart of LLMs.\nNevertheless, in order to be generative, those ANNs need to be endowed\nwith some peculiar capabilities, such as parallel processing of textual\nsentences or keeping the memory of the previous context.\nThese particular capabilities were at the core of generative AI research in the\nlast decades, starting from the 80s and 90s. However, it is only in recent\nyears that the main drawbacks of these early models – such as the capability\nof text parallel processing or memory management – have been bypassed by\nmodern generative AI frameworks. Those frameworks are the so-called\ntransformers.\nIn the following sections, we will explore the evolution of generative AI\nmodel architecture, from early developments to state-of-the-art transformers.\nWe will start by covering the first generative AI models that paved the way\nfor further research, highlighting their limitations and the approaches to\novercome them. We will then explore the introduction of transformer-based\narchitectures, covering their main components and explaining why they\nrepresent the state of the art for LLMs.\nEarly experiments\nThe very first popular generative AI ANN architectures trace back to the 80s\nand 90s, including:\nRecurrent neural networks (RNNs): RNNs are a type of ANN\ndesigned to handle sequential data. They have recurrent connections\nthat allow information to persist across time steps, making them\nsuitable for tasks like language modeling, machine translation, and text\ngeneration. However, RNNs have limitations in capturing long-range\ndependencies due to the vanishing or exploding gradient problem.\n\n\nDefinition\nIn ANNs, the gradient is a measure of how much the\nmodel’s performance would improve if we slightly\nadjusted its internal parameters (weights). During\ntraining, RNNs try to minimize the difference between\ntheir predictions and the actual targets by adjusting their\nweights based on the gradient of the loss function. The\nproblem of vanishing or exploding gradient arises in\nRNNs during training when the gradients become\nextremely small or large, respectively. The vanishing\ngradient problem occurs when the gradient becomes\nextremely small during training. As a result, the RNN\nlearns very slowly and struggles to capture long-term\npatterns in the data. Conversely, the exploding gradient\nproblem happens when the gradient becomes extremely\nlarge. This leads to unstable training and prevents the\nRNN from converging to a good solution.\nLong short-term memory (LSTM): LSTMs are a variant of RNNs\nthat address the vanishing gradient problem. They introduce gating\nmechanisms that enable better preservation of important information\nacross longer sequences. LSTMs became popular for various sequential\ntasks, including text generation, speech recognition, and sentiment\nanalysis.\nThese architectures were popular and effective for various generative tasks,\nbut they had limitations in handling long-range dependencies, scalability,\nand overall efficiency, especially when dealing with large-scale NLP tasks\nthat would need massive parallel processing. The transformer framework\n\n\nwas introduced to overcome these limitations. In the next section, we are\ngoing to see how a transformers-based architecture overcomes the above\nlimitations and is at the core of modern generative AI LLMs.\nIntroducing the transformer\narchitecture\nThe transformer architecture is a deep learning model introduced in the\npaper “Attention Is All You Need” by Vaswani et al. (2017). It\nrevolutionized NLP and other sequence-to-sequence tasks.\nThe transformer dispenses with recurrence and convolutions entirely and\nrelies solely on attention mechanisms to encode and decode sequences.\nDefinition\nIn the transformer architecture, “attention” is a mechanism\nthat enables the model to focus on relevant parts of the input\nsequence while generating the output. It calculates attention\nscores between input and output positions, applies Softmax\nto get weights, and takes a weighted sum of the input\nsequence to obtain context vectors. Attention is crucial for\ncapturing long-range dependencies and relationships\nbetween words in the data.\nSince transformers use attention on the same sequence that is currently being\nencoded, we refer to it as self-attention. Self-attention layers are responsible\nfor determining the importance of each input token in generating the output.\nThose answer the question: “Which part of the input should I focus on?”\n\n\nIn order to obtain the self-attention vector for a sentence, the elements we\nneed are “value”, “query”, and “key.” These matrices are used to calculate\nattention scores between the elements in the input sequence and are the three\nweight matrices that are learned during the training process (typically\ninitialized with random values). More specifically, their purpose is as\nfollows:\nQuery (Q) is used to represent the current focus of the attention\nmechanism\nKey (K) is used to determine which parts of the input should be given\nattention\nValue (V) is used to compute the context vectors\nThey can be represented as follows:\nFigure 1.8: Decomposition of the Input matrix into Q, K, and V vectors\n",
      "page_number": 42,
      "chapter_number": 6,
      "summary": "Well, thanks\nto the enormous amount of data on which LLMs have been trained (we will\ndive deeper into the process of training an LLM in the next sections) Key topics include attention, probability, and probabilities.",
      "keywords": [
        "word",
        "probability",
        "candidate word",
        "Bayes’ theorem",
        "LLM",
        "training",
        "LLMs",
        "Bayes’",
        "context",
        "based",
        "data",
        "posterior probability",
        "generative",
        "candidate",
        "input"
      ],
      "concepts": [
        "attention",
        "probability",
        "probabilities",
        "word",
        "architecture",
        "architectural",
        "models",
        "context",
        "transformer",
        "function"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 56,
          "title": "Segment 56 (pages 484-491)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 47,
          "title": "Segment 47 (pages 950-972)",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 51-58)",
      "start_page": 51,
      "end_page": 58,
      "detection_method": "topic_boundary",
      "content": "Those matrices are then multiplied and passed through a non-linear\ntransformation (thanks to a Softmax function). The output of the self-\nattention layer represents the input values in a transformed, context-aware\nmanner, which allows the transformer to attend to different parts of the input\ndepending on the task at hand.\nFigure 1.9: Representation of Q, K, and V matrices multiplication to obtain the context vector\nThe mathematical formula is the following:\nFrom an architectural point of view, the transformer consists of two main\ncomponents, an encoder and a decoder:\nThe encoder takes the input sequence and produces a sequence of\nhidden states, each of which is a weighted sum of all the input\nembeddings.\nThe decoder takes the output sequence (shifted right by one position)\nand produces a sequence of predictions, each of which is a weighted\nsum of all the encoder’s hidden states and the previous decoder’s\nhidden states.\n\n\nNote\nThe reason for shifting the output sequence right by one\nposition in the decoder layer is to prevent the model from\nseeing the current token when predicting the next token. This\nis because the model is trained to generate the output\nsequence given the input sequence, and the output sequence\nshould not depend on itself. By shifting the output sequence\nright, the model only sees the previous tokens as input and\nlearns to predict the next token based on the input sequence\nand the previous output tokens. This way, the model can\nlearn to generate coherent and meaningful sentences without\ncheating.\nThe following illustration from the original paper shows the transformer\narchitecture:\n\n\nFigure 1.10: Simplified transformer architecture\nLet’s examine each building block, starting from the encoding part:\n\n\nInput embedding: These are the vector representations of tokenized\ninput text.\nPositional encoding: As the transformer does not have an inherent\nsense of word order (unlike RNNs with their sequential nature),\npositional encodings are added to the input embeddings. These\nencodings provide information about the positions of words in the input\nsequence, allowing the model to understand the order of tokens.\nMulti-head attention layer: This is a mechanism in which multiple\nself-attention mechanisms operate in parallel on different parts of the\ninput data, producing multiple representations. This allows the\ntransformer model to attend to different parts of the input data in\nparallel and aggregate information from multiple perspectives.\nAdd and norm layer: This combines element-wise addition and layer\nnormalization. It adds the output of a layer to the original input and then\napplies layer normalization to stabilize and accelerate training. This\ntechnique helps mitigate gradient-related issues and improves the\nmodel’s performance on sequential data.\nFeed-forward layer: This is responsible for transforming the\nnormalized output of attention layers into a suitable representation for\nthe final output, using a non-linear activation function, such as the\npreviously mentioned Softmax.\nThe decoding part of the transformer starts with a similar process as the\nencoding part, where the target sequence (output sequence) undergoes input\nembedding and positional encoding. Let’s understand these blocks:\nOutput embedding (shifted right): For the decoder, the target\nsequence is “shifted right” by one position. This means that at each\nposition, the model tries to predict the token that comes after the\nanalyzed token in the original target sequence. This is achieved by\n\n\nremoving the last token from the target sequence and padding it with a\nspecial start-of-sequence token (start symbol). This way, the decoder\nlearns to generate the correct token based on the preceding context\nduring autoregressive decoding.\nDefinition\nAutoregressive decoding is a technique for generating\noutput sequences from a model that predicts each\noutput token based on the previous output tokens. It is\noften used in NLP tasks such as machine translation,\ntext summarization, and text generation.\nAutoregressive decoding works by feeding the model\nan initial token, such as a start-of-sequence symbol,\nand then using the model’s prediction as the next input\ntoken. This process is repeated until the model\ngenerates an end-of-sequence symbol or reaches a\nmaximum length. The output sequence is then the\nconcatenation of all the predicted tokens.\nDecoder layers: Similarly to the encoder block, here, we also have\nPositional Encoding, Multi-Head Attention, Add and Norm, and Feed\nForward layers, whose role is the same as for the encoding part.\nLinear and Softmax: These layers apply, respectively, a linear and\nnon-linear transformation to the output vector. The non-linear\ntransformation (Softmax) conveys the output vector into a probability\ndistribution, corresponding to a set of candidate words. The word\n\n\ncorresponding to the greatest element of the probability vector will be\nthe output of the whole process.\nThe transformer architecture paved the way for modern LLMs, and it also\nsaw many variations with respect to its original framework.\nSome models use only the encoder part, such as BERT (Bidirectional\nEncoder Representations from Transformers), which is designed for NLU\ntasks such as text classification, question answering, and sentiment analysis.\nOther models use only the decoder part, such as GPT-3 (Generative Pre-\ntrained Transformer 3), which is designed for natural language generation\ntasks such as text completion, summarization, and dialogue.\nFinally, there are models that use both the encoder and the decoder parts,\nsuch as T5 (Text-to-Text Transfer Transformer), which is designed for\nvarious NLP tasks that can be framed as text-to-text transformations, such as\ntranslation, paraphrasing, and text simplification.\nRegardless of the variant, the core component of a transformer – the\nattention mechanism – remains a constant within LLM architecture, and it\nalso represents the reason why those frameworks gained so much popularity\nwithin the context of generative AI and NLP.\nHowever, the architectural variant of an LLM is not the only element that\nfeatures the functioning of that model. This functioning is indeed\ncharacterized also by what the model knows, depending on its training\ndataset, and how well it applies its knowledge upon the user’s request,\ndepending on its evaluation metrics.\nIn the next section, we are going to cover both the processes of training and\nevaluating LLMs, also providing those metrics needed to differentiate\n\n\namong different LLMs and understand which one to use for specific use\ncases within your applications.\nTraining and evaluating LLMs\nIn the preceding sections, we saw how choosing an LLM architecture is a\npivotal step in determining its functioning. However, the quality and\ndiversity of the output text depend largely on two factors: the training dataset\nand the evaluation metric.\nThe training dataset determines what kind of data the LLM learns from and\nhow well it can generalize to new domains and languages. The evaluation\nmetric measures how well the LLM performs on specific tasks and\nbenchmarks, and how it compares to other models and human writers.\nTherefore, choosing an appropriate training dataset and evaluation metric is\ncrucial for developing and assessing LLMs.\nIn this section, we will discuss some of the challenges and trade-offs\ninvolved in selecting and using different training datasets and evaluation\nmetrics for LLMs, as well as some of the recent developments and future\ndirections in this area.\nTraining an LLM\nBy definition, LLMs are huge, from a double point of view:\nNumber of parameters: This is a measure of the complexity of the\nLLM architecture and represents the number of connections among\nneurons. Complex architectures have thousands of layers, each one\nhaving multiple neurons, meaning that among layers, we will have\nseveral connections with associated parameters (or weights).\n\n\nTraining set: This refers to the unlabeled text corpus on which the\nLLM learns and trains its parameters. To give an idea of how big such a\ntext corpus for an LLM can be, let’s consider OpenAI’s GPT-3 training\nset:\nFigure 1.11: GPT-3 knowledge base\nConsidering the assumption:\n1 token ~= 4 characters in English\n1 token ~= ¾ words\nWe can conclude that GPT-3 has been trained on around 374 billion\nwords.\nSo generally speaking, LLMs are trained using unsupervised learning on\nmassive datasets, which often consist of billions of sentences collected from\ndiverse sources on the internet. The transformer architecture, with its self-\nattention mechanism, allows the model to efficiently process long sequences\nof text and capture intricate dependencies between words. Training such\nmodels necessitates vast computational resources, typically employing\ndistributed systems with multiple graphics processing units (GPUs) or\ntensor processing units (TPUs).\n",
      "page_number": 51,
      "chapter_number": 7,
      "summary": "The output of the self-\nattention layer represents the input values in a transformed, context-aware\nmanner, which allows the transformer to attend to different parts of the input\ndepending on the task at hand Key topics include token, text, and trained.",
      "keywords": [
        "output sequence",
        "output",
        "sequence",
        "input",
        "input sequence",
        "model",
        "transformer",
        "token",
        "LLM",
        "LLMs",
        "previous output tokens",
        "training",
        "LLM architecture",
        "transformer architecture",
        "layer"
      ],
      "concepts": [
        "token",
        "text",
        "trained",
        "transformation",
        "transformations",
        "sequence",
        "generate",
        "generating",
        "generation",
        "output"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 38,
          "title": "Segment 38 (pages 323-331)",
          "relevance_score": 0.48,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 40,
          "title": "Segment 40 (pages 808-829)",
          "relevance_score": 0.47,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 8,
          "title": "Segment 8 (pages 144-164)",
          "relevance_score": 0.46,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 11,
          "title": "Segment 11 (pages 209-230)",
          "relevance_score": 0.46,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.45,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 59-66)",
      "start_page": 59,
      "end_page": 66,
      "detection_method": "topic_boundary",
      "content": "Definition\nA tensor is a multi-dimensional array used in mathematics\nand computer science. It holds numerical data and is\nfundamental in fields like machine learning.\nA TPU is a specialized hardware accelerator created by\nGoogle for deep learning tasks. TPUs are optimized for\ntensor operations, making them highly efficient for training\nand running neural networks. They offer fast processing\nwhile consuming less power, enabling faster model training\nand inference in data centers.\nThe training process involves numerous iterations over the dataset, fine-\ntuning the model’s parameters using optimization algorithms\nbackpropagation. Through this process, transformer-based language models\nacquire a deep understanding of language patterns, semantics, and context,\nenabling them to excel in a wide range of NLP tasks, from text generation to\nsentiment analysis and machine translation.\nThe following are the main steps involved in the training process of an\nLLM:\n1. Data collection: This is the process of gathering a large amount of text\ndata from various sources, such as the open web, books, news articles,\nsocial media, etc. The data should be diverse, high-quality, and\nrepresentative of the natural language that the LLM will encounter.\n2. Data preprocessing: This is the process of cleaning, filtering, and\nformatting the data for training. This may include removing duplicates,\n\n\nnoise, or sensitive information, splitting the data into sentences or\nparagraphs, tokenizing the text into subwords or characters, etc.\n3. Model architecture: This is the process of designing the structure and\nparameters of the LLM. This may include choosing the type of neural\nnetwork (such as transformer) and its structure (such as decoder only,\nencoder only, or encoder-decoder), the number and size of layers, the\nattention mechanism, the activation function, etc.\n4. Model initialization: This is the process of assigning initial values to\nthe weights and biases of the LLM. This may be done randomly or by\nusing pre-trained weights from another model.\n5. Model pre-training: This is the process of updating the weights and\nbiases of the LLM by feeding it batches of data and computing the loss\nfunction. The loss function measures how well the LLM predicts the\nnext token given the previous tokens. The LLM tries to minimize the\nloss by using an optimization algorithm (such as gradient descent) that\nadjusts the weights and biases in the direction that reduces the loss with\nthe backpropagation mechanism. The model training may take several\nepochs (iterations over the entire dataset) until it converges to a low\nloss value.\nDefinition\nIn the context of neural networks, the optimization\nalgorithm during training is the method used to find\nthe best set of weights for the model that minimizes\nthe prediction error or maximizes the accuracy of the\ntraining data. The most common optimization\nalgorithm for neural networks is stochastic gradient\ndescent (SGD), which updates the weights in small\n\n\nsteps based on the gradient of the error function and\nthe current input-output pair. SGD is often combined\nwith backpropagation, which we defined earlier in this\nchapter.\nThe output of the pre-training phase is the so-called base model.\n6. Fine-tuning: The base model is trained in a supervised way with a\ndataset made of tuples of (prompt, ideal response). This step is\nnecessary to make the base model more in line with AI assistants, such\nas ChatGPT. The output of this phase is called the supervised fine-\ntuned (SFT) model.\n7. Reinforcement learning from human feedback (RLHF): This step\nconsists of iteratively optimizing the SFT model (by updating some of\nits parameters) with respect to the reward model (typically another\nLLM trained incorporating human preferences).\nDefinition\nReinforcement learning (RL) is a branch of machine\nlearning that focuses on training computers to make optimal\ndecisions by interacting with their environment. Instead of\nbeing given explicit instructions, the computer learns\nthrough trial and error: by exploring the environment and\nreceiving rewards or penalties for its actions. The goal of\nreinforcement learning is to find the optimal behavior or\npolicy that maximizes the expected reward or value of a\ngiven model. To do so, the RL process involves a reward\nmodel (RM) that is able to provide a “preferability score” to\n\n\nthe computer. In the context of RLHF, the RM is trained to\nincorporate human preferences.\nNote that RLHF is a pivotal milestone in achieving human alignment with\nAI systems. Due to the rapid achievements in the field of generative AI, it is\npivotal to keep endowing those powerful LLMs and, more generally, LFMs\nwith those preferences and values that are typical of human beings.\nOnce we have a trained model, the next and final step is evaluating its\nperformance.\nModel evaluation\nEvaluating traditional AI models was, in some ways, pretty intuitive. For\nexample, let’s think about an image classification model that has to\ndetermine whether the input image represents a dog or a cat. So we train our\nmodel on a training dataset with a set of labeled images and, once the model\nis trained, we test it on unlabeled images. The evaluation metric is simply\nthe percentage of correctly classified images over the total number of images\nwithin the test set.\nWhen it comes to LLMs, the story is a bit different. As those models are\ntrained on unlabeled text and are not task-specific, but rather generic and\nadaptable given a user’s prompt, traditional evaluation metrics were not\nsuitable anymore. Evaluating an LLM means, among other things,\nmeasuring its language fluency, coherence, and ability to emulate different\nstyles depending on the user’s request.\nHence, a new set of evaluation frameworks needed to be introduced. The\nfollowing are the most popular frameworks used to evaluate LLMs:\n\n\nGeneral Language Understanding Evaluation (GLUE) and\nSuperGLUE: This benchmark is used to measure the performance of\nLLMs on various NLU tasks, such as sentiment analysis, natural\nlanguage inference, question answering, etc. The higher the score on\nthe GLUE benchmark, the better the LLM is at generalizing across\ndifferent tasks and domains.\nIt recently evolved into a new benchmark styled after GLUE and\ncalled SuperGLUE, which comes with more difficult tasks. It consists\nof eight challenging tasks that require more advanced reasoning skills\nthan GLUE, such as natural language inference, question answering,\ncoreference resolution, etc., a broad coverage diagnostic set that tests\nmodels on various linguistic capabilities and failure modes, and a\nleaderboard that ranks models based on their average score across all\ntasks.\nThe difference between the GLUE and the SuperGLUE benchmark is\nthat the SuperGLUE benchmark is more challenging and realistic than\nthe GLUE benchmark, as it covers more complex tasks and\nphenomena, requires models to handle multiple domains and formats,\nand has higher human performance baselines. The SuperGLUE\nbenchmark is designed to drive research in the development of more\ngeneral and robust NLU systems.\nMassive Multitask Language Understanding (MMLU): This\nbenchmark measures the knowledge of an LLM using zero-shot and\nfew-shot settings.\nDefinition\n\n\nThe concept of zero-shot evaluation is a method of\nevaluating a language model without any labeled data\nor fine-tuning. It measures how well the language\nmodel can perform a new task by using natural\nlanguage instructions or examples as prompts and\ncomputing the likelihood of the correct output given\nthe input. It is the probability that a trained model will\nproduce a particular set of tokens without needing any\nlabeled training data.\nThis design adds complexity to the benchmark and aligns it more\nclosely with the way we assess human performance. The benchmark\ncomprises 14,000 multiple-choice questions categorized into 57\ngroups, spanning STEM, humanities, social sciences, and other fields.\nIt covers a spectrum of difficulty levels, ranging from basic to\nadvanced professional, assessing both general knowledge and\nproblem-solving skills. The subjects encompass various areas,\nincluding traditional ones like mathematics and history, as well as\nspecialized domains like law and ethics. The extensive range of\nsubjects and depth of coverage make this benchmark valuable for\nuncovering any gaps in a model’s knowledge. Scoring is based on\nsubject-specific accuracy and the average accuracy across all subjects.\nHellaSwag: The HellaSwag evaluation framework is a method of\nevaluating LLMs on their ability to generate plausible and common\nsense continuations for given contexts. It is based on the HellaSwag\ndataset, which is a collection of 70,000 multiple-choice questions that\ncover diverse domains and genres, such as books, movies, recipes, etc.\nEach question consists of a context (a few sentences that describe a\n\n\nsituation or an event) and four possible endings (one correct and three\nincorrect). The endings are designed to be hard to distinguish for\nLLMs, as they require world knowledge, common sense reasoning, and\nlinguistic understanding.\nTruthfulQA: This benchmark evaluates a language model’s accuracy\nin generating responses to questions. It includes 817 questions across\n38 categories like health, law, finance, and politics. The questions are\ndesigned to mimic those that humans might answer incorrectly due to\nfalse beliefs or misunderstandings.\nAI2 Reasoning Challenge (ARC): This benchmark is used to measure\nLLMs’ reasoning capabilities and to stimulate the development of\nmodels that can perform complex NLU tasks. It consists of a dataset of\n7,787 multiple-choice science questions, assembled to encourage\nresearch in advanced question answering. The dataset is divided into an\nEasy set and a Challenge set, where the latter contains only questions\nthat require complex reasoning or additional knowledge to answer\ncorrectly. The benchmark also provides a corpus of over 14 million\nscience sentences that can be used as supporting evidence for the\nquestions.\nIt is important to note that each evaluation framework has a focus on a\nspecific feature. Namely, the GLUE benchmark focuses on grammar,\nparaphrasing, and text similarity, while MMLU focuses on generalized\nlanguage understanding among various domains and tasks. Hence, while\nevaluating an LLM, it is important to have a clear understanding of the final\ngoal, so that the most relevant evaluation framework can be used.\nAlternatively, if the goal is that of having the best of the breed in any task, it\nis key not to use only one evaluation framework, but rather an average of\nmultiple frameworks.\n\n\nIn addition to that, in case no existing LLM is able to tackle your specific\nuse cases, you still have a margin to customize those models and make them\nmore tailored toward your application scenarios. In the next section, we are\nindeed going to cover the existing techniques of LLM customization, from\nthe lightest ones (such as prompt engineering) up to the whole training of an\nLLM from scratch.\nBase models versus\ncustomized models\nThe nice thing about LLMs is that they have been trained and ready to use.\nAs we saw in the previous section, training an LLM requires great\ninvestment in hardware (GPUs or TPUs) and it might last for months, and\nthese two factors might mean it is not feasible for individuals and small\nbusinesses.\nLuckily, pre-trained LLMs are generalized enough to be applicable to\nvarious tasks, so they can be consumed without further tuning directly via\ntheir REST API (we will dive deeper into model consumption in the next\nchapters).\nNevertheless, there might be scenarios where a general-purpose LLM is not\nenough, since it lacks domain-specific knowledge or doesn’t conform to a\nparticular style and taxonomy of communication. If this is the case, you\nmight want to customize your model.\nHow to customize your model\nThere are three main ways to customize your model:\n",
      "page_number": 59,
      "chapter_number": 8,
      "summary": "This chapter covers segment 8 (pages 59-66). Key topics include model, human, and humanities. It holds numerical data and is\nfundamental in fields like machine learning.",
      "keywords": [
        "model",
        "LLM",
        "LLMs",
        "benchmark",
        "language model",
        "language",
        "data",
        "training",
        "GLUE benchmark",
        "tasks",
        "GLUE",
        "process",
        "model training",
        "evaluation",
        "base model"
      ],
      "concepts": [
        "model",
        "human",
        "humanities",
        "training",
        "evaluating",
        "evaluation",
        "evaluate",
        "tasks",
        "data",
        "question"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.7,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 474-483)",
          "relevance_score": 0.69,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 8,
          "title": "Segment 8 (pages 144-164)",
          "relevance_score": 0.69,
          "method": "sentence_transformers"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 8,
          "title": "Segment 8 (pages 58-71)",
          "relevance_score": 0.67,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 67-74)",
      "start_page": 67,
      "end_page": 74,
      "detection_method": "topic_boundary",
      "content": "Extending non-parametric knowledge: This allows the model to\naccess external sources of information to integrate its parametric\nknowledge while responding to the user’s query.\nDefinition\nLLMs exhibit two types of knowledge: parametric and\nnon-parametric. The parametric knowledge is the one\nembedded in the LLM’s parameters, deriving from the\nunlabeled text corpora during the training phase. On\nthe other hand, non-parametric knowledge is the one\nwe can “attach” to the model via embedded\ndocumentation. Non-parametric knowledge doesn’t\nchange the structure of the model, but rather, allows it\nto navigate through external documentation to be used\nas relevant context to answer the user’s query.\nThis might involve connecting the model to web sources (like\nWikipedia) or internal documentation with domain-specific\nknowledge. The connection of the LLM to external sources is called a\nplug-in, and we will be discussing it more deeply in the hands-on\nsection of this book.\nFew-shot learning: In this type of model customization, the LLM is\ngiven a metaprompt with a small number of examples (typically\nbetween 3 and 5) of each new task it is asked to perform. The model\nmust use its prior knowledge to generalize from these examples to\nperform the task.\n\n\nDefinition\nA metaprompt is a message or instruction that can be\nused to improve the performance of LLMs on new\ntasks with a few examples.\nFine tuning: The fine-tuning process involves using smaller, task-\nspecific datasets to customize the foundation models for particular\napplications.\nThis approach differs from the first ones because, with fine-tuning, the\nparameters of the pre-trained model are altered and optimized toward\nthe specific task. This is done by training the model on a smaller\nlabeled dataset that is specific to the new task. The key idea behind\nfine-tuning is to leverage the knowledge learned from the pre-trained\nmodel and fine-tune it to the new task, rather than training a model\nfrom scratch.\n\n\nFigure 1.12: Illustration of the process of fine-tuning\nIn the preceding figure, you can see a schema on how fine-tuning\nworks on OpenAI pre-built models. The idea is that you have available\na pre-trained model with general-purpose weights or parameters.\nThen, you feed your model with custom data, typically in the form of\n“key-value” prompts and completions:\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated\n...\nOnce the training is done, you will have a customized model that is\nparticularly performant for a given task, for example, the classification\nof your company’s documentation.\nThe nice thing about fine-tuning is that you can make pre-built models\ntailored to your use cases, without the need to retrain them from\nscratch, yet leveraging smaller training datasets and hence less training\ntime and compute. At the same time, the model keeps its generative\npower and accuracy learned via the original training, the one that\noccurred to the massive dataset.\nIn Chapter 11, Fine-Tuning Large Language Models, we will focus on\nfine-tuning your model in Python so that you can test it for your own\ntask.\nOn top of the above techniques (which you can also combine among each\nother), there is a fourth one, which is the most “drastic.” It consists of\ntraining an LLM from scratch, which you might want to either build on your\n\n\nown or initialize from a pre-built architecture. We will see how to approach\nthis technique in the final chapters.\nSummary\nIn this chapter, we explored the field of LLMs, with a technical deep dive\ninto their architecture, functioning, and training process. We saw the most\nprominent architectures, such as the transformer-based frameworks, how the\ntraining process works, and different ways to customize your own LLM.\nWe now have the foundation to understand what LLMs are. In the next\nchapter, we will see how to use them and, more specifically, how to build\nintelligent applications with them.\nReferences\nAttention is all you need: 1706.03762.pdf (arxiv.org)\nPossible End of Humanity from AI? Geoffrey Hinton at MIT\nTechnology Review’s EmTech Digital:\nhttps://www.youtube.com/watch?\nv=sitHS6UDMJc&t=594s&ab_channel=JosephRaczynski\nThe Glue Benchmark: https://gluebenchmark.com/\nTruthfulQA:\nhttps://paperswithcode.com/dataset/truthfulqa\nHugging Face Open LLM Leaderboard:\nhttps://huggingface.co/spaces/optimum/llm-perf-\nleaderboard\nThink you have Solved Question Answering? Try ARC, the AI2\nReasoning Challenge: https://arxiv.org/abs/1803.05457\n\n\nJoin our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\nOceanofPDF.com\n\n\n2\nLLMs for AI-Powered\nApplications\nIn Chapter 1, Introduction to Large Language Models, we introduced large\nlanguage models (LLMs) as powerful foundation models with generative\ncapabilities as well as powerful common-sense reasoning. Now, the next question\nis: what should I do with those models?\nIn this chapter, we are going to see how LLMs are revolutionizing the world of\nsoftware development, leading to a new era of AI-powered applications. By the\nend of this chapter, you will have a clearer picture of how LLMs can be\nembedded in different application scenarios, thanks to the new AI orchestrator\nframeworks that are populating the market of AI development.\nIn this chapter, we will cover the following topics:\nHow LLMs are changing software development\nThe copilot system\nIntroducing AI orchestrators to embed LLMs into applications\nHow LLMs are changing\nsoftware development\nLLMs have proven to have extraordinary capabilities: from natural language\nunderstanding tasks (summarization, named entity recognition, and classification)\nto text generation, from common-sense reasoning to brainstorming skills.\n\n\nHowever, they are not just incredible by themselves. As discussed in Chapter 1,\nLLMs and, generally speaking, large foundation models (LFMs), are\nrevolutionizing software development by serving as platforms for building\npowerful applications.\nIn fact, instead of starting from scratch, today developers can make API calls to a\nhosted version of an LLM, with the option of customizing it for their specific\nneeds, as we saw in the previous chapter. This shift allows teams to incorporate\nthe power of AI more easily and efficiently into their applications, similar to the\ntransition from single-purpose computing to time-sharing in the past.\nBut what does it mean, concretely, to incorporate LLMs within applications?\nThere are two main aspects to consider when incorporating LLMs within\napplications:\nThe technical aspect, which covers the how. Integrating LLMs into\napplications involves embedding them through REST API calls and\nmanaging them with AI orchestrators. This means setting up architectural\ncomponents that allow seamless communication with the LLMs via API\ncalls. Additionally, using AI orchestrators helps to efficiently manage and\ncoordinate the LLMs’ functionality within the application, as we will discuss\nlater in this chapter.\nThe conceptual aspect, which covers the what. LLMs bring a plethora of\nnew capabilities that can be harnessed within applications. These capabilities\nwill be explored in detail later in this book. One way to view LLMs’ impact\nis by considering them as a new category of software, often referred to as\ncopilot. This categorization highlights the significant assistance and\ncollaboration provided by LLMs in enhancing application functionalities.\nWe will delve into the technical aspect later on in this chapter, while the next\nsection will cover a brand-new category of software – the copilot system.\n\n\nThe copilot system\nThe copilot system is a new category of software that serves as an expert helper\nto users trying to accomplish complex tasks. This concept was coined by\nMicrosoft and has already been introduced into its applications, such as M365\nCopilot and the new Bing, now powered by GPT-4. With the same framework\nthat is used by these products, developers can now build their own copilots to\nembed within their applications.\nBut what exactly is a copilot?\nAs the name suggests, copilots are meant to be AI assistants that work side by\nside with users and support them in various activities, from information retrieval\nto blog writing and posting, from brainstorming ideas to code review and\ngeneration.\nThe following are some unique features of copilots:\nA copilot is powered by LLMs, or, more generally, LFMs, meaning that\nthese are the reasoning engines that make the copilot “intelligent.” This\nreasoning engine is one of its components, but not the only one. A copilot\nalso relies on other technologies, such as apps, data sources, and user\ninterfaces, to provide a useful and engaging experience for users. The\nfollowing illustration shows how this works:\n",
      "page_number": 67,
      "chapter_number": 9,
      "summary": "The connection of the LLM to external sources is called a\nplug-in, and we will be discussing it more deeply in the hands-on\nsection of this book Key topics include llms, applications, and application.",
      "keywords": [
        "LLMs",
        "model",
        "Large Language Models",
        "applications",
        "knowledge",
        "LLM",
        "copilot",
        "Language Models",
        "training",
        "task",
        "non-parametric knowledge",
        "Extending non-parametric knowledge",
        "fine-tuning",
        "foundation models",
        "pre-trained model"
      ],
      "concepts": [
        "llms",
        "applications",
        "application",
        "model",
        "tasks",
        "training",
        "chapters",
        "development",
        "llm",
        "aspects"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 474-483)",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 75-82)",
      "start_page": 75,
      "end_page": 82,
      "detection_method": "topic_boundary",
      "content": "Figure 2.1: A copilot is powered by an LLM\nA copilot is designed to have a conversational user interface, allowing\nusers to interact with it using natural language. This reduces or even\neliminates the knowledge gap between complex systems that need domain-\nspecific taxonomy (for example, querying tabular data needs the knowledge\nof programming languages such as T-SQL) and users. Let’s look at an\nexample of such a conversation:\n\n\nFigure 2.2: An example of a conversational UI to reduce the gap between the user and the database\nA copilot has a scope. This means that it is grounded to domain-specific\ndata so that it is entitled to answer only within the perimeter of the\napplication or domain.\nDefinition\nGrounding is the process of using LLMs with information that is\nuse case specific, relevant, and not available as part of the LLM’s\ntrained knowledge. It is crucial for ensuring the quality, accuracy,\nand relevance of the output. For example, let’s say you want an\nLLM-powered application that assists you during your research\non up-to-date papers (not included in the training dataset of your\nLLM). You also want your app to only respond if the answer is\n\n\nincluded in those papers. To do so, you will need to ground your\nLLM to the set of papers, so that your application will only\nrespond within this perimeter.\nGrounding is achieved through an architectural framework called retrieval-\naugmented generation (RAG), a technique that enhances the output of\nLLMs by incorporating information from an external, authoritative\nknowledge base before generating a response. This process helps to ensure\nthat the generated content is relevant, accurate, and up to date.\nWhat is the difference between a copilot and a RAG? RAG can be\nseen as one of the architectural patterns that feature a copilot.\nWhenever we want our copilot to be grounded to domain-specific\ndata, we use a RAG framework. Note that RAG is not the only\narchitectural pattern that can feature a copilot: there are further\nframeworks such as function calling or multi-agents that we will\nexplore throughout the book.\nFor example, let’s say we developed a copilot within our company that\nallows employees to chat with their enterprise knowledge base. As fun as it\ncan be, we cannot provide users with a copilot they can use to plan their\nsummer trip (it would be like providing users with a ChatGPT-like tool at\nour own hosting cost!); on the contrary, we want the copilot to be grounded\nonly to our enterprise knowledge base so that it can respond only if the\nanswer is pertinent to the domain-specific context.\nThe following figure shows an example of grounding a copilot system:\n\n\nFigure 2.3: Example of grounding a copilot\nThe copilot’s capabilities can be extended by skills, which can be code or\ncalls to other models. In fact, the LLM (our reasoning engine) might have\ntwo kinds of limitations:\nLimited parametric knowledge. This is due to the knowledge base\ncutoff date, which is a physiological feature of LLMs. In fact, their\ntraining dataset will always be “outdated,” not in line with the current\n\n\ntrends. This can be overcome by adding non-parametric knowledge\nwith grounding, as previously seen.\nLack of executive power. This means that LLMs by themselves are\nnot empowered to carry out actions. Let’s consider, for example, the\nwell-known ChatGPT: if we ask it to generate a LinkedIn post about\nproductivity tips, we will then need to copy and paste it onto our\nLinkedIn profile as ChatGPT is not able to do so by itself. That is the\nreason why we need plug-ins. Plug-ins are LLMs’ connectors toward\nthe external world that serve not only as input sources to extend\nLLMs’ non-parametric knowledge (for example, to allow a web\nsearch) but also as output sources so that the copilot can actually\nexecute actions. For example, with a LinkedIn plug-in, our copilot\npowered by an LLM will be able not only to generate the post but also\nto post it online.\nFigure 2.4: Example of Wikipedia and LinkedIn plug-ins\nNote that the user’s prompt in natural language is not the only input the model\nprocesses. In fact, it is a crucial component of the backend logic of our LLM-\npowered applications and the set of instructions we provide to the model. This\nmetaprompt or system message is the object of a new discipline called prompt\nengineering.\n\n\nDefinition\nPrompt engineering is the process of designing and optimizing\nprompts to LLMs for a wide variety of applications and research\ntopics. Prompts are short pieces of text that are used to guide the\nLLM’s output. Prompt engineering skills help to better understand\nthe capabilities and limitations of LLMs.\nPrompt engineering involves selecting the right words, phrases,\nsymbols, and formats that elicit the desired response from the\nLLM. Prompt engineering also involves using other controls, such\nas parameters, examples, or data sources, to influence the LLM’s\nbehavior. For example, if we want our LLM-powered application\nto generate responses for a 5-year-old child, we can specify this in\na system message similar to “Act as a teacher who explains\ncomplex concepts to 5-year-old children.”\nIn fact, Andrej Karpathy, the previous Director of AI at Tesla, who returned to\nOpenAI in February 2023, tweeted that “English is the hottest new programming\nlanguage.”\nWe will dive deeper into the concept of prompt engineering in Chapter 4, Prompt\nEngineering. In the next section, we are going to focus on the emerging AI\norchestrators.\nIntroducing AI orchestrators to\nembed LLMs into applications\nEarlier in this chapter, we saw that there are two main aspects to consider when\nincorporating LLMs within applications: a technical aspect and a conceptual\naspect. While we can explain the conceptual aspect with the brand-new category\n\n\nof software called Copilot, in this section, we are going to further explore how to\ntechnically embed and orchestrate LLMs within our applications.\nThe main components of AI\norchestrators\nFrom one side, the paradigm shift of foundation models implies a great\nsimplification in the domain of AI-powered applications: after producing models,\nnow the trend is consuming models. On the other side, many roadblocks might\narise in developing this new kind of AI, since there are LLM-related components\nthat are brand new and have never been managed before within an application life\ncycle. For example, there might be malicious actors that could try to change the\nLLM instructions (the system message mentioned earlier) so that the application\ndoes not follow the correct instructions. This is an example of a new set of\nsecurity threats that are typical to LLM-powered applications and need to be\naddressed with powerful counterattacks or preventive techniques.\nThe following is an illustration of the main components of such applications:\nFigure 2.5: High-level architecture of LLM-powered applications\n\n\nLet’s inspect each of these components in detail:\nModels: The model is simply the type of LLM we decide to embed in our\napplication. There are two main categories of models:\nProprietary LLMs: Models that are owned by specific companies or\norganizations. Examples include GPT-3 and GPT-4, developed by\nOpenAI, or Bard, developed by Google. As their source code and\narchitecture are not available, those models cannot be re-trained from\nscratch on custom data, yet they can be fine-tuned if needed.\nOpen-source: Models with code and architecture freely available and\ndistributed, hence they can also be trained from scratch on custom\ndata. Examples include Falcon LLM, developed by Abu Dhabi’s\nTechnology Innovation Institute (TII), or LLaMA, developed by\nMeta.\nWe will dive deeper into the main set of LLMs available today in Chapter\n3, Choosing an LLM for Your Application.\nMemory: LLM applications commonly use a conversational interface,\nwhich requires the ability to refer back to earlier information within the\nconversation. This is achieved through a “memory” system that allows the\napplication to store and retrieve past interactions. Note that past interactions\ncould also constitute additional non-parametric knowledge to be added to the\nmodel. To achieve that, it is important to store all the past conversations –\nproperly embedded – into VectorDB, which is at the core of the application’s\ndata.\nDefinition\nVectorDB is a type of database that stores and retrieves\ninformation based on vectorized embeddings, the numerical\nrepresentations that capture the meaning and context of\ntext. By using VectorDB, you can perform semantic search\n",
      "page_number": 75,
      "chapter_number": 10,
      "summary": "Note that RAG is not the only\narchitectural pattern that can feature a copilot: there are further\nframeworks such as function calling or multi-agents that we will\nexplore throughout the book Key topics include llms, prompt, and models.",
      "keywords": [
        "LLM",
        "copilot",
        "LLMs",
        "application",
        "knowledge",
        "prompt engineering",
        "models",
        "prompt",
        "RAG",
        "knowledge base",
        "LLM applications",
        "engineering",
        "data",
        "users",
        "LLM applications commonly"
      ],
      "concepts": [
        "llms",
        "prompt",
        "models",
        "specific",
        "llm",
        "application",
        "applications",
        "examples",
        "knowledge",
        "called"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 38,
          "title": "Segment 38 (pages 344-356)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.51,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 16,
          "title": "Segment 16 (pages 130-138)",
          "relevance_score": 0.51,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 23,
          "title": "Segment 23 (pages 191-199)",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 83-93)",
      "start_page": 83,
      "end_page": 93,
      "detection_method": "topic_boundary",
      "content": "and retrieval based on the similarity of meanings rather than\nkeywords. VectorDB can also help LLMs generate more\nrelevant and coherent text by providing contextual\nunderstanding and enriching generation results. Some\nexamples of VectorDBs are Chroma, Elasticsearch, Milvus,\nPinecone, Qdrant, Weaviate, and Facebook AI Similarity\nSearch (FAISS).\nFAISS, developed by Facebook (now Meta) in 2017, was\none of the pioneering vector databases. It was designed for\nefficient similarity search and clustering of dense vectors\nand is particularly useful for multimedia documents and\ndense embeddings. It was initially an internal research\nproject at Facebook. Its primary goal was to better utilize\nGPUs for identifying similarities related to user\npreferences. Over time, it evolved into the fastest available\nlibrary for similarity search and can handle billion-scale\ndatasets. FAISS has opened up possibilities for\nrecommendation engines and AI-based assistant systems.\nPlug-ins: They can be seen as additional modules or components that can be\nintegrated into the LLM to extend its functionality or adapt it to specific\ntasks and applications. These plug-ins act as add-ons, enhancing the\ncapabilities of the LLM beyond its core language generation or\ncomprehension abilities.\nThe idea behind plug-ins is to make LLMs more versatile and adaptable,\nallowing developers and users to customize the behavior of the language\nmodel for their specific needs. Plug-ins can be created to perform various\ntasks, and they can be seamlessly incorporated into the LLM’s architecture.\n\n\nPrompts: This is probably the most interesting and pivotal component of an\nLLM-powered application. We’ve already quoted, in the previous section,\nAndrej Karpathy’s affirmation that “English is the hottest new programming\nlanguage,” and you will understand why in the upcoming chapters. Prompts\ncan defined at two different levels:\n“Frontend,” or what the user sees: A “prompt” refers to the input to\nthe model. It is the way the user interacts with the application, asking\nthings in natural language.\n“Backend,” or what the user does not see: Natural language is not\nonly the way to interact, as a user, with the frontend; it is also the way\nwe “program” the backend. In fact, on top of the user’s prompt, there\nare many natural language instructions, or meta-promts, that we give\nto the model so that it can properly address the user’s query. Meta-\nprompts are meant to instruct the model to act as it is meant to. For\nexample, if we want to limit our application to answer only questions\nrelated to the documentation we provided in VectorDB, we will\nspecify the following in our meta-prompts to the model: “Answer only\nif the question is related to the provided documentation.”\nFinally, we get to the core of the high-level architecture shown in Figure 2.5, that\nis, the AI orchestrator. With the AI orchestrator, we refer to lightweight libraries\nthat make it easier to embed and orchestrate LLMs within applications.\nAs LLMs went viral by the end of 2022, many libraries started arising in the\nmarket. In the next sections, we are going to focus on three of them: LangChain,\nSemantic Kernel, and Haystack.\nLangChain\nLangChain was launched as an open-source project by Harrison Chase in October\n2022. It can be used both in Python and JS/TS. It is a framework for developing\napplications powered by language models, making them data-aware (with\n\n\ngrounding) and agentic – which means they are able to interact with external\nenvironments.\nLet’s take a look at the key components of LangChain:\nFigure 2.6: LangChain’s components\nOverall, LangChain has the following core modules:\nModels: These are the LLMs or LFMs that will be the engine of the\napplication. LangChain supports proprietary models, such as those available\nin OpenAI and Azure OpenAI, and open-source models consumable from\nthe Hugging Face Hub.\nDefinition\nHugging Face is a company and a community that builds\nand shares state-of-the-art models and tools for natural\nlanguage processing and other machine learning domains. It\ndeveloped the Hugging Face Hub, a platform where people\ncan create, discover, and collaborate on machine learning\nmodels and LLMs, datasets, and demos. The Hugging Face\nHub hosts over 120k models, 20k datasets, and 50k demos\n\n\nin various domains and tasks, such as audio, vision, and\nlanguage.\nAlongside models, LangChain also offers many prompt-related components\nthat make it easier to manage the prompt flow.\nData connectors: These refer to the building blocks needed to retrieve the\nadditional external knowledge (for example, in RAG-based scenarios) we\nwant to provide the model with. Examples of data connectors are document\nloaders or text embedding models.\nMemory: This allows the application to keep references to the user’s\ninteractions, in both the short and long term. It is typically based on\nvectorized embeddings stored in VectorDB.\nChains: These are predetermined sequences of actions and calls to LLMs\nthat make it easier to build complex applications that require chaining LLMs\nwith each other or with other components. An example of a chain might be:\ntake the user query, chunk it into smaller pieces, embed those chunks, search\nfor similar embeddings in VectorDB, use the top three most similar chunks\nin VectorDB as context to provide the answer, and generate the answer.\nAgents: Agents are entities that drive decision-making within LLM-powered\napplications. They have access to a suite of tools and can decide which tool\nto call based on the user input and the context. Agents are dynamic and\nadaptive, meaning that they can change or adjust their actions based on the\nsituation or the goal.\nLangChain offers the following benefits:\nLangChain provides modular abstractions for the components we previously\nmentioned that are necessary to work with language models, such as\nprompts, memory, and plug-ins.\nAlongside those components, LangChain also offers pre-built chains, which\nare structured concatenations of components. Those chains can be pre-built\n\n\nfor specific use cases or be customized.\nIn Part 2 of this book, we will go through a series of hands-on applications, all\nLangChain based. So, starting from Chapter 5, Embedding LLMs within Your\nApplications, we will focus much deeper on LangChain components and overall\nframeworks.\nHaystack\nHaystack is a Python-based framework developed by Deepset, a startup founded\nin 2018 in Berlin by Milos Rusic, Malte Pietsch, and Timo Möller. Deepset\nprovides developers with the tools to build natural language processing (NLP)-\nbased applications, and with the introduction of Haystack, they are taking them to\nthe next level.\nThe following illustration shows the core components of Haystack:\nFigure 2.7: Haystack’s components\nLet’s look at these components in detail:\nNodes: These are components that perform a specific task or function, such\nas a retriever, a reader, a generator, a summarizer, etc. Nodes can be LLMs\nor other utilities that interact with LLMs or other resources. Among LLMs,\n\n\nHaystack supports proprietary models, such as those available in OpenAI\nand Azure OpenAI, and open-source models consumable from the Hugging\nFace Hub.\nPipelines: These are sequences of calls to nodes that perform natural\nlanguage tasks or interact with other resources. Pipelines can be querying\npipelines or indexing pipelines, depending on whether they perform searches\non a set of documents or prepare documents for search. Pipelines are\npredetermined and hardcoded, meaning that they do not change or adapt\nbased on the user input or the context.\nAgent: This is an entity that uses LLMs to generate accurate responses to\ncomplex queries. An agent has access to a set of tools, which can be\npipelines or nodes, and it can decide which tool to call based on the user\ninput and the context. An agent is dynamic and adaptive, meaning that it can\nchange or adjust its actions based on the situation or the goal.\nTools: There are functions that an agent can call to perform natural language\ntasks or interact with other resources. Tools can be pipelines or nodes that\nare available to the agent and they can be grouped into toolkits, which are\nsets of tools that can accomplish specific objectives.\nDocumentStores: These are backends that store and retrieve documents for\nsearches. DocumentStores can be based on different technologies, also\nincluding VectorDB (such as FAISS, Milvus, or Elasticsearch).\nSome of the benefits offered by Haystack are:\nEase of use: Haystack is user-friendly and straightforward. It’s often chosen\nfor lighter tasks and rapid prototypes.\nDocumentation quality: Haystack’s documentation is considered high-\nquality, aiding developers in building search systems, question-answering,\nsummarization, and conversational AI.\nEnd-to-end framework: Haystack covers the entire LLM project life cycle,\nfrom data preprocessing to deployment. It’s ideal for large-scale search\n\n\nsystems and information retrieval.\nAnother nice thing about Haystack is that you can deploy it as a REST API\nand it can be consumed directly.\nSemantic Kernel\nSemantic Kernel is the third open-source SDK we are going to explore in this\nchapter. It was developed by Microsoft, originally in C# and now also available in\nPython.\nThis framework takes its name from the concept of a “kernel,” which, generally\nspeaking, refers to the core or essence of a system. In the context of this\nframework, a kernel is meant to act as the engine that addresses a user’s input by\nchaining and concatenating a series of components into pipelines, encouraging\nfunction composition.\nDefinition\nIn mathematics, function composition is a way to combine two\nfunctions to create a new function. The idea is to use the output of\none function as the input to another function, forming a chain of\nfunctions. The composition of two functions f and g is denoted as\n(f  g), where the function g is applied first, followed by the\nfunction f \n(f  g)(x) = f(g(x)).\nFunction composition in computer science is a powerful concept\nthat allows for the creation of more sophisticated and reusable\ncode by combining smaller functions into larger ones. It enhances\nmodularity and code organization, making programs easier to read\nand maintain.\nThe following is an illustration of the anatomy of Semantic Kernel:\n\n\nFigure 2.8: Anatomy of Semantic Kernel\nSemantic Kernel has the following main components:\nModels: These are the LLMs or LFMs that will be the engine of the\napplication. Semantic Kernel supports proprietary models, such as those\navailable in OpenAI and Azure OpenAI, and open-source models\nconsumable from the Hugging Face Hub.\nMemory: It allows the application to keep references to the user’s\ninteractions, both in the short and long term. Within the framework of\nSemantic Kernel, memories can be accessed in three ways:\nKey-value pairs: This consists of saving environment variables that\nstore simple information, such as names or dates.\nLocal storage: This consists of saving information to a file that can be\nretrieved by its filename, such as a CSV or JSON file.\nSemantic memory search: This is similar to LangChain’s and\nHaystack’s memory, as it uses embeddings to represent and search for\ntext information based on its meaning.\n\n\nFunctions: Functions can be seen as skills that mix LLM prompts and code,\nwith the goal of making users’ asks interpretable and actionable. There are\ntwo types of functions:\nSemantic functions: These are a type of templated prompt, which is a\nnatural language query that specifies the input and output format for\nthe LLM, also incorporating prompt configuration, which sets the\nparameters for the LLM.\nNative functions: These refer to the native computer code that can\nroute the intent captured by the semantic function and perform the\nrelated task.\nTo make an example, a semantic function could ask the LLM to write a\nshort paragraph about AI, while a native function could actually post it on\nsocial media like LinkedIn.\nPlug-ins: These are connectors toward external sources or systems that are\nmeant to provide additional information or the ability to perform\nautonomous actions. Semantic Kernel offers out-of-the-box plug-ins, such as\nthe Microsoft Graph connector kit, but you can build a custom plug-in by\nleveraging functions (both native and semantic, or a mix of the two).\nPlanner: As LLMs can be seen as reasoning engines, they can also be\nleveraged to auto-create chains or pipelines to address new users’ needs.\nThis goal is achieved with a planner, which is a function that takes as input a\nuser’s task and produces the set of actions, plug-ins, and functions needed to\nachieve the goal.\nSome benefits of Semantic Kernel are:\nLightweight and C# support: Semantic Kernel is more lightweight and\nincludes C# support. It’s a great choice for C# developers or those using the\n.NET framework.\nWide range of use cases: Semantic Kernel is versatile, supporting various\nLLM-related tasks.\n\n\nIndustry-led: Semantic Kernel was developed by Microsoft, and it is the\nframework the company used to build its own copilots. Hence, it is mainly\ndriven by industry needs and asks, making it a solid tool for enterprise-scale\napplications.\nHow to choose a framework\nOverall, the three frameworks offer, more or less, similar core components,\nsometimes called by a different taxonomy, yet covering all the blocks illustrated\nwithin the concept of the copilot system. So, a natural question might be: “Which\none should I use to build my LLM-powered application?” Well, there is no right\nor wrong answer! All three are extremely valid. However, there are some features\nthat might be more relevant for specific use cases or developers’ preferences. The\nfollowing are some criteria you might want to consider:\nThe programming language you are comfortable with or prefer to use:\nDifferent frameworks may support different programming languages or have\ndifferent levels of compatibility or integration with them. For example,\nSemantic Kernel supports C#, Python, and Java, while LangChain and\nHaystack are mainly based on Python (even though LangChain also\nintroduced JS/TS support). You may want to choose a framework that\nmatches your existing skills or preferences, or that allows you to use the\nlanguage that is most suitable for your application domain or environment.\nThe type and complexity of the natural language tasks you want to\nperform or support: Different frameworks may have different capabilities\nor features for handling various natural language tasks, such as\nsummarization, generation, translation, reasoning, etc. For example,\nLangChain and Haystack provide utilities and components for orchestrating\nand executing natural language tasks, while Semantic Kernel allows you to\nuse natural language semantic functions to invoke LLMs and services. You\n\n\nmay want to choose a framework that offers the functionality and flexibility\nyou need or want for your application goals or scenarios.\nThe level of customization and control you need or want over the LLMs\nand their parameters or options: Different frameworks may have different\nways of accessing, configuring, and fine-tuning the LLMs and their\nparameters or options, such as model selection, prompt design, inference\nspeed, output format, etc. For example, Semantic Kernel provides\nconnectors that make it easy to add memories and models to your AI app,\nwhile LangChain and Haystack allow you to plug in different components\nfor the document store, retriever, reader, generator, summarizer, and\nevaluator. You may want to choose a framework that gives you the level of\ncustomization and control you need or want over the LLMs and their\nparameters or options.\nThe availability and quality of the documentation, tutorials, examples,\nand community support for the framework: Different frameworks may\nhave different levels of documentation, tutorials, examples, and community\nsupport that can help you learn, use, and troubleshoot the framework. For\nexample, Semantic Kernel has a website with documentation, tutorials,\nexamples, and a Discord community; LangChain has a GitHub repository\nwith documentation, examples, and issues; Haystack has a website with\ndocumentation, tutorials, demos, blog posts, and a Slack community. You\nmay want to choose a framework that has the availability and quality of\ndocumentation, tutorials, examples, and community support that can help\nyou get started and solve problems with the framework.\nLet’s briefly summarize the differences between these orchestrators:\nFeature\nLangChain\nHaystack\nSemantic\nKernel\n",
      "page_number": 83,
      "chapter_number": 11,
      "summary": "This chapter covers segment 11 (pages 83-93). Key topics include prompts, model, and based. FAISS, developed by Facebook (now Meta) in 2017, was\none of the pioneering vector databases.",
      "keywords": [
        "Semantic Kernel",
        "Kernel",
        "Semantic",
        "Haystack",
        "LLMs",
        "natural language",
        "Semantic Kernel supports",
        "language",
        "natural language tasks",
        "Hugging Face Hub",
        "LangChain",
        "models",
        "components",
        "framework",
        "Hugging Face"
      ],
      "concepts": [
        "prompts",
        "model",
        "based",
        "langchain",
        "functionality",
        "function",
        "functions",
        "semantic",
        "documents",
        "document"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 474-483)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 16,
          "title": "Segment 16 (pages 131-138)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 2,
          "title": "Segment 2 (pages 19-39)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 94-102)",
      "start_page": 94,
      "end_page": 102,
      "detection_method": "topic_boundary",
      "content": "LLM support\nProprietary and\nopen-source\nProprietary and\nopen source\nProprietary and\nopen source\nSupported\nlanguages\nPython and JS/TS\nPython\nC#, Java, and\nPython\nProcess\norchestration\nChains\nPipelines of\nnodes\nPipelines of\nfunctions\nDeployment\nNo REST API\nREST API\nNo REST API\nFeature\nLangChain\nHaystack\nSemantic\nKernel\nTable 2.1: Comparisons among the three AI orchestrators\nOverall, all three frameworks offer a wide range of tools and integrations to build\nyour LLM-powered applications, and a wise approach could be to use the one that\nis most in line with your current skills or the company’s overall approach.\nSummary\nIn this chapter, we delved into the new way of developing applications that LLMs\nhave been paving, as we introduced the concept of the copilot and discussed the\nemergence of new AI orchestrators. Among those, we focused on three projects –\nLangChain, Haystack, and Semantic Kernel – and we examined their features,\nmain components, and some criteria to decide which one to pick.\nOnce we have decided on the AI orchestrator, another pivotal step is to decide\nwhich LLM(s) we want to embed into our applications. In Chapter 3, Choosing\nan LLM for Your Application, we are going to see the most prominent LLMs on\n\n\nthe market today – both proprietary and open-source – and understand some\ndecision criteria to pick the proper models with respect to the application use\ncases.\nReferences\nLangChain repository: https://github.com/langchain-\nai/langchain\nSemantic Kernel documentation:\nhttps://learn.microsoft.com/en-us/semantic-\nkernel/get-started/supported-languages\nCopilot stack: https://build.microsoft.com/en-\nUS/sessions/bb8f9d99-0c47-404f-8212-a85fffd3a59d?\nsource=/speakers/ef864919-5fd1-4215-b611-\n61035a19db6b\nThe Copilot system: https://www.youtube.com/watch?\nv=E5g20qmeKpg\nJoin our community on Discord\nJoin our community’s Discord space for discussions with the author and other\nreaders:\nhttps://packt.link/llm\n\n\nOceanofPDF.com\n\n\n3\nChoosing an LLM for Your\nApplication\nIn the last chapter, we saw how pivotal it is to properly orchestrate large\nlanguage models (LLMs) and their components within applications. In fact,\nwe saw that not all LLMs are created equal. The next key decision is which\nLLMs to actually use. Different LLMs may have different architectures,\nsizes, training data, capabilities, and limitations. Choosing the right LLM for\nyour application is not a trivial decision, as it can have a significant impact\non the performance, quality, and cost of your solution.\nIn this chapter, we will guide you through the process of choosing the right\nLLM for your application. We will cover the following topics:\nAn overview of the most promising LLMs in the market\nThe main criteria and tools to use when comparing LLMs\nTrade-offs between size and performance\nBy the end of this chapter, you should have a clear understanding of how to\nchoose the right LLM for your application and how to use it effectively and\nresponsibly.\n\n\nThe most promising LLMs in\nthe market\nThe last year has witnessed an unprecedented surge in the research and\ndevelopment of LLMs. Several new models have been released or\nannounced by different organizations, each with its own features and\ncapabilities. Some of these models are the largest and most advanced ever\ncreated, surpassing the previous state-of-the-art (SOTA) by orders of\nmagnitude. Others are lighter yet more specialized in specific tasks.\nIn this chapter, we will review some of the most promising LLMs in the\nmarket as of 2024. We will introduce their background, key findings, and\nmain techniques. We will also compare their performance, strengths, and\nlimitations on various benchmarks and tasks. We will also discuss their\npotential applications, challenges, and implications for the future of AI and\nsociety.\nProprietary models\nProprietary LLMs are developed and owned by private companies, and they\nare not disclosed with code. They are also typically subject to a fee for\nconsumption.\nProprietary models offer a series of advantages, including better support and\nmaintenance as well as safety and alignment. They also tend to outperform\nopen-source models in terms of generalization, because of their complexity\nand training datasets. On the other hand, they act as a “black box,” meaning\nthat owners do not disclose the source code to developers.\nIn the next sections, we will cover three of the most popular proprietary\nLLMs in the market, as of August 2023.\n\n\nGPT-4\nReleased in March 2023, GPT-4 is, together with its newly released “cousin”\nGPT-4 Turbo, one of the latest models developed by OpenAI, is among the\ntop performers in the market at the time of writing this book (while OpenAI,\nas confirmed by its CEO Sam Altman, is already working on GPT-5).\nIt belongs to the class of generative pretrained transformer (GPT)\nmodels, a decoder-only transformer-based architecture introduced by\nOpenAI. The following diagram shows the basic architecture:\n\n\nFigure 3.1: High-level architecture of a decoder-only transformer\nAs you can see from the preceding diagram, the decoder-only architecture\nstill includes the main elements that feature in transformer architecture that\nwe covered in Chapter 1, Positional Embeddings, Multi-Head Attention, and\nFeed Forward layers. However, in this architecture, the model solely\ncomprises a decoder, which is trained to predict the next token in a sequence\n\n\nbased on the preceding tokens. Unlike the encoder-decoder architecture, the\ndecoder-only design lacks an explicit encoder for summarizing input\ninformation. Instead, the information is implicitly encoded within the hidden\nstate of the decoder, which is updated at each step during the generation\nprocess.\nNow, we’ll look at some of the improvements in GPT-4 over previous\nversions.\nGPT-4, like the previous models in the GPT series, has been trained on both\npublicly available and OpenAI-licensed datasets (OpenAI didn’t disclose the\nexact composition of the training set).\nAdditionally, to make the model more aligned with the user’s intent, the\ntraining process also involved reinforcement learning from human\nfeedback (RLHF) training.\nDefinition\nRLHF is a technique that aims at using human feedback as\nan evaluating metric for LLMs’ generated output and then\nusing that feedback to further optimize the model. There are\ntwo main steps to achieve that goal:\n1. Training a reward model based on human preferences.\n2. Optimizing the LLM with respect to the reward model.\nThis step is done via reinforcement learning and it is a\ntype of machine learning paradigm where an agent\nlearns to make decisions by interacting with an\nenvironment. The agent receives feedback in the form\nof rewards or penalties based on its actions, and its goal\nis to maximize the cumulative reward over time by\n\n\ncontinuously adapting its behavior through trial and\nerror.\nWith RLHF, thanks to the reward model, the LLM is able to\nlearn from human preferences and be more aligned with\nusers’ intents.\nAs an example, think about ChatGPT. This model integrates\nvarious training methods, including unsupervised\npretraining, supervised fine-tuning, instruction tuning, and\nRLHF. The RLHF component involves training the model to\npredict human preferences by using feedback from human\ntrainers. These trainers review the model’s responses and\nprovide ratings or corrections, guiding the model to generate\nmore helpful, accurate, and aligned responses.\nFor instance, if a language model initially produces an output\nthat is not quite helpful or accurate, human trainers can\nprovide feedback that indicates the preferred output. The\nmodel then uses this feedback to adjust its parameters and\nimprove future responses. This process iteratively continues,\nwith the model learning from a series of human judgments to\nbetter align with what is considered helpful or appropriate by\nhuman standards.\nGPT-4 demonstrated outstanding capabilities in commonsense reasoning and\nanalytical skills. It has been benchmarked with SOTA systems, including the\nMassive Multitask Language Understanding (MMLU) we covered in\nChapter 1. On MMLU, GPT-4 outperformed previous models not only in\nEnglish, but also in other languages.\n",
      "page_number": 94,
      "chapter_number": 12,
      "summary": "Summary\nIn this chapter, we delved into the new way of developing applications that LLMs\nhave been paving, as we introduced the concept of the copilot and discussed the\nemergence of new AI orchestrators Key topics include models, training, and architectures. Covers architecture.",
      "keywords": [
        "LLMs",
        "model",
        "LLM",
        "REST API",
        "human",
        "promising LLMs",
        "Proprietary",
        "training",
        "Application",
        "architecture",
        "RLHF",
        "reward model",
        "feedback",
        "Proprietary models",
        "market"
      ],
      "concepts": [
        "models",
        "training",
        "architectures",
        "llms",
        "langchain",
        "proprietary",
        "main",
        "learning",
        "language",
        "source"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 53,
          "title": "Segment 53 (pages 448-456)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 5,
          "title": "Segment 5 (pages 33-40)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 103-110)",
      "start_page": 103,
      "end_page": 110,
      "detection_method": "topic_boundary",
      "content": "The following is an illustration that shows GPT-4’s performance on MMLU:\nFigure 3.2: GPT-4 3-shot accuracy on MMLU across languages (source:\nhttps://openai.com/research/gpt-4)\nIn addition to MMLU, GPT-4 has been benchmarked on a variety of SOTA\nsystems and academic exams, as you can see from the following graph:\n\n\nFigure 3.3: GPT performance on academic and professional exams (source:\nhttps://arxiv.org/pdf/2303.08774.pdf)\nNote: in the preceding graph, you can see two versions of\nGPT-4, vision and no vision (along with the GPT-3.5 for\nbenchmarking purposes). This is because GPT-4 is a multi-\nmodal model, meaning that it can take images as input, in\naddition to text. However, in this chapter, we will benchmark\nonly its textual capabilities.\n\n\nAnother great improvement of GPT-4 with respect to its predecessors (GPT-\n3.5 and GPT-3) is its noticeable reduction in the risk of hallucination.\nDefinition\nHallucination is a term that describes a phenomenon where\nLLMs generate text that is incorrect, nonsensical, or not real,\nbut appears to be plausible or coherent. For example, an\nLLM may hallucinate a fact that contradicts the source or\ncommon knowledge, a name that does not exist, or a\nsentence that does not make sense.\nHallucination can happen because LLMs are not databases or\nsearch engines that store or retrieve factual information.\nRather, they are statistical models that learn from massive\namounts of text data and produce outputs based on the\npatterns and probabilities they have learned. However, these\npatterns and probabilities may not reflect the truth or the\nreality, as the data may be incomplete, noisy, or biased.\nMoreover, LLMs have limited contextual understanding and\nmemory, as they can only process a certain number of tokens\nat a time and abstract them into latent representations.\nTherefore, LLMs may generate text that is not supported by\nany data or logic but is the most likely or correlated from the\nprompt.\nIn fact, even though it is still not 100% reliable, GPT-4 made great\nimprovements with TruthfulQA benchmarks, which test the model’s ability\nto separate fact from incorrect statements (we covered TruthfulQA\nbenchmarks in Chapter 1, in the Model evaluation section).\n\n\nHere, you can see an illustration that compares GPT-4 results in a\nTruthfulQA benchmark with those of GPT-3.5 (the model behind OpenAI’s\nChatGPT) and Anthropic-LM (we will cover this latter model in the next\nsections).\nFigure 3.4: Model comparison in TruthfulQA benchmark (source:\nhttps://openai.com/research/gpt-4)\nFinally, with GPT-4, OpenAI made an additional effort to make it safer and\nmore aligned, engaging from the beginning a team of over 50 experts in\ndomains like AI alignment risks, privacy, and cybersecurity, with the goal of\nunderstanding the extent of the risks of such a powerful model and how to\nprevent them.\nDefinition\nAlignment is a term that describes the degree to which LLMs\nbehave in ways that are useful and harmless for their human\n\n\nusers. For example, an LLM may be aligned if it generates\ntext that is accurate, relevant, coherent, and respectful. An\nLLM may be misaligned if it generates text that is false,\nmisleading, harmful, or offensive.\nThanks to this analysis, further data have been collected and used while\ntraining GPT-4 to mitigate its potential risks, resulting in a reduced risk\ncompared to its predecessor, GPT-3.5.\nGemini 1.5\nGemini 1.5 is a SOTA generative AI model developed by Google and\nreleased in December 2023. Like GPT-4, Gemini is designed to be\nmultimodal, meaning that it can process and generate content across various\nmodalities, including text, images, audio, video, and code. It is based on a\nmixture-of-expert (MoE) transformer.\nDefinition\nIn the context of transformer architecture, MoE refers to a\nmodel that incorporates multiple specialized sub-models,\nknown as “experts,” within its layers. Each expert is a neural\nnetwork designed to handle different types of data or tasks\nmore efficiently. The MoE model uses a gating mechanism\nor router to determine which expert should process a given\ninput, allowing the model to dynamically allocate resources\nand specialize in processing certain types of information.\nThis approach can lead to more efficient training and\ninference, as it enables the model to scale up in size and\n\n\ncomplexity without a proportional increase in computational\ncost.\nGemini comes in various sizes, including Ultra, Pro, and Nano, to cater to\ndifferent computational needs, from data centers to mobile devices. To use\nGemini, developers can access it via the APIs provided for different model\nvariants, allowing the integration of its capabilities into applications.\nCompared to its previous version, Gemini 1.0, the current model\noutperforms it in text, vision, and audio tasks, as shown in the following\nscreenshot:\nFigure 3.5: Gemini 1.5 Pro and Ultra compared to its previous version 1.0 (source:\nhttps://storage.googleapis.com/deepmind-\nmedia/gemini/gemini_v1_5_report.pdf )\nSimilarly, it has demonstrated outstanding capabilities in domains such as\nmath, science, and reasoning, and coding and multilinguality:\n\n\nFigure 3.6: Gemini 1.5 Pro compared to Gemini 1.0 Pro and Ultra on different benchmarks (source:\nhttps://storage.googleapis.com/deepmind-\nmedia/gemini/gemini_v1_5_report.pdf)\n\n\nNote that Gemini 1.5 Pro is outperforming Gemini 1.0 Ultra (which is\nremarkably bigger) in many benchmarks across the various domains. As of\ntoday, Gemini Pro can be tried via a web app at gemini.google.com for\nfree, while Gemini Ultra is available via a premium subscription with a\nmonthly fee. On the other hand, Gemini Nano, which is tailored for mobile\ndevices, can be executed on capable Android devices via the Google AI\nEdge SDK for Android. Note that, as of April 2024, this SDK is still under\nearly access preview and you can apply for the early access program at\nhttps://docs.google.com/forms/d/e/1FAIpQLSdDvg0eEzc\nUY_-CmtiMZLd68KD3F0usCnRzKKzWb4sAYwhFJg/viewform.\nFinally, Gemini Pro and Ultra can also be consumed by developers via the\nREST API from Google AI Studio.\nClaude 2\nClaude 2, which stands for Constitutional Large-scale Alignment via User\nData and Expertise, is an LLM developed by Anthropic, a research company\nfounded by former OpenAI researchers and focused on AI safety and\nalignment. It was announced in July 2023.\nClaude 2 is a transformer-based LLM that has been trained on a mix of\npublicly available information from the internet and proprietary data, via\nunsupervised learning, RLHF, and constitutional AI (CAI).\nCAI is a real peculiarity of Claude. In fact, Anthropic paid extraordinary\nattention to Claude 2 alignment with safety principles. More specifically,\nAnthropic developed this unique technique called CAI, which was disclosed\nin December 2022 in the paper Constitutional AI: Harmlessness from AI\nFeedback.\n",
      "page_number": 103,
      "chapter_number": 13,
      "summary": "However, in this chapter, we will benchmark\nonly its textual capabilities Key topics include model, gemini, and risk. Hallucination can happen because LLMs are not databases or\nsearch engines that store or retrieve factual information.",
      "keywords": [
        "Gemini",
        "model",
        "Pro",
        "MMLU",
        "Gemini Pro",
        "text",
        "Ultra",
        "LLMs",
        "LLM",
        "data",
        "source",
        "Claude",
        "performance on MMLU",
        "Pro and Ultra",
        "alignment"
      ],
      "concepts": [
        "model",
        "gemini",
        "risk",
        "anthropic",
        "generate",
        "generative",
        "aligned",
        "different",
        "access",
        "source"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 47,
          "title": "Segment 47 (pages 950-972)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 32,
          "title": "Segment 32 (pages 292-299)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 111-118)",
      "start_page": 111,
      "end_page": 118,
      "detection_method": "topic_boundary",
      "content": "CAI aims to make the model safer and more aligned with human values and\nintentions by preventing toxic or discriminatory output, not helping a human\nengage in illegal or unethical activities, and broadly creating an AI system\nthat is helpful, honest, and harmless. To achieve this, it uses a set of\nprinciples to guide the model’s behavior and outputs, rather than relying on\nhuman feedback or data alone. The principles are derived from various\nsources, such as the UN Declaration of Human Rights, trust and safety best\npractices, principles proposed by other AI research labs, non-Western\nperspectives, and empirical research.\nCAI uses these principles in two stages of the training process:\nFirst, the model is trained to critique and revise its own responses using\nthe principles and a few examples.\nSecond, the model is trained via reinforcement learning, but rather than\nusing human feedback, it uses AI-generated feedback based on the\nprinciples to choose the more harmless output.\nThe following illustration shows the training process according to the CAI\ntechnique:\n\n\nFigure 3.7: Claude’s training process according to the CAI technique (source:\nhttps://arxiv.org/abs/2212.08073)\nAnother peculiarity of Claude 2 is the context length, which has a limit of\n100,000 tokens. This means that users can input longer prompts, namely\npages of technical documentation or even a book, which do not need to be\nembedded. Plus, the model can also generate longer output compared to\nother LLMs.\nFinally, Claude 2 demonstrates relevant capabilities also when working with\ncode, scoring 71.2% on the HumanEval benchmark.\nDefinition\nHumanEval is a benchmark for evaluating the code\ngeneration ability of LLMs. It consists of 164 human-crafted\ncoding problems in Python, each with a prompt, a solution,\nand a test suite. The problems cover various topics, such as\ndata structures, algorithms, logic, math, and string\nmanipulation. The benchmark can be used to measure the\nfunctional correctness, syntactic validity, and semantic\ncoherence of the LLM’s outputs.\nOverall, Claude 2 is a very interesting model and competitor of GPT-4 to\npay attention to. It can be consumed via the REST API or directly via the\nAnthropic beta chat experience (limited for US and UK users as of August\n2023).\nThe following comparison table shows the main differences between the\nthree models:\n\n\nGPT-4\nGemini\nClaude 2\nCompany or\ninstitution\nOpenAI\nGoogle\nAnthropic\nFirst release\nMarch 2023\nDecember 2023\nJuly 2023\nArchitecture\nTransformer-\nbased, decoder\nonly\nTransformer-\nbased\nTransformer-\nbased\nSizes and\nvariants\nParameters not\nofficially\nspecified\nTwo context-\nlength variants:\nGPT-4 8K tokens\nGPT-4 32K\ntokens\nThree sizes,\nfrom smallest\nto largest:\nNano, Pro, and\nUltra\nNot officially\nspecified\nHow to use\nREST API at\nOpenAI\ndeveloper\nplatforms\nUsing OpenAI\nPlayground at\nhttps://plat\nREST API at\nGoogle AI\nStudio\nUsing Gemini\nat\nhttps://ge\nREST API after\ncompiling the\nform at\nhttps://www.\nanthropic.co\nm/claude\n\n\nform.openai.\ncom/playgrou\nnd\nmini.googl\ne.com/\nTable 3.1: Comparison table of GPT-4, PaLM 2, and Claude 2\nIn addition to proprietary models, there is a huge market for open-source\nLLMs available today. Let’s discuss some of these in the next section.\nOpen-source models\nThe advantage of an open-source model is that, by definition, developers\nhave full visibility and access to the source code. In the context of LLMs,\nthis implies the following:\nYou have major control over the architecture, meaning that you can also\nmodify it in the local version you are going to use within your project.\nThis also implies that they are not prone to potential updates to the\nsource code made by models’ owners.\nThere is the possibility to train your model from scratch, on top of the\nclassical fine-tuning, which is also available for proprietary models.\nFree to use, meaning that you won’t incur any charge while using those\nLLMs, in contrast with the proprietary ones that have pay-per-use\npricing.\nTo compare open-source models, throughout this book, we will refer to the\nindependent Hugging Face Open LLM Leaderboard (you can find it at\nhttps://huggingface.co/spaces/HuggingFaceH4/open_ll\nm_leaderboard), a project that aims to evaluate and compare the\nperformance of LLMs on various natural language understanding (NLU)\n\n\ntasks. The project is hosted on Hugging Face Spaces, a platform for creating\nand sharing machine-learning applications.\nThe Open LLM Leaderboard uses four main evaluation benchmarks, which\nwe covered in Chapter 1, in the Model evaluation section:\nAI2 Reasoning Challenge (ARC): Grade-school science questions and\ncomplex NLU tasks.\nHellaSwag: Common sense reasoning.\nMMLU: Tasks in various domains, including math, computer science,\nand law.\nTruthfulQA: An evaluation of how truthful the model is when\ngenerating answers.\nEven though those are just a subsample of the plethora of LLMs’\nbenchmarks, we will stick to this leaderboard as a reference evaluation\nframework as it being widely adopted.\nLLaMA-2\nLarge Language Model Meta AI 2 (LLaMA-2) is a new family of models\ndeveloped by Meta and unveiled to the public on July 18, 2023, open source\nand for free (its first version was originally limited to researchers).\nIt is an autoregressive model with an optimized, decoder-only transformer\narchitecture.\nDefinition\nThe concept of autoregressive in the context of transformers\nrefers to the fact that the model predicts the next token in the\nsequence, conditioned on all the previous tokens. This is\n\n\ndone by masking the future tokens in the input so that the\nmodel can only attend to the past tokens. For example, if the\ninput sequence is “The sky is blue,” the model would predict\n“The” first, then “sky,” then “is,” and finally “blue,” using a\nmask to hide the tokens that come after each prediction.\nLLaMA-2 models come in three sizes: 7, 13, and 70 billion parameters. All\nthe versions have been trained on 2 trillion tokens and have a context length\nof 4,092 tokens.\nOn top of that, all model sizes come with a “chat” version, called LLaMA-2-\nchat, which is more versatile for general-purpose conversational scenarios\ncompared to the base model LLama-2.\nNote\nIn the context of LLMs, the difference between base models\nand “chat” or assistant models is primarily in their training\nand intended use:\nBase models: These models are trained on vast amounts\nof text data, often sourced from the internet, and their\nprimary function is to predict the next word in a given\ncontext, which makes them great at understanding and\ngenerating language. However, they might not always\nbe precise or focused on specific instructions.\nAssistant models: These models start as base LLMs but\nare further fine-tuned with input-output pairs that\ninclude instructions and the model’s attempts to follow\nthose instructions. They often employ RLHF to refine\n\n\nthe model, making it better at being helpful, honest, and\nharmless. As a result, they are less likely to generate\nproblematic text and are more suitable for practical\napplications like chatbots and content generation. For\nexample, the assistant model GPT-3.5 Turbo (the model\nbehind ChatGPT) is a fine-tuned version of the\ncompletion model GPT-3.\nIn essence, while base models provide a broad understanding\nof language, assistant models are optimized to follow\ninstructions and provide more accurate and contextually\nrelevant responses.\nLLaMA-2-chat was developed with a fine-tuning process that consisted of\ntwo main steps:\n1. Supervised fine-tuning: This step involves fine-tuning the model on\npublicly available instruction datasets and over 1 million human\nannotations, to make them more helpful and safe for conversational use\ncases. The fine-tuning process uses a selected list of prompts to guide\nthe model outputs, and a loss function that encourages diversity and\nrelevance (that’s the reason why it is “supervised”).\n2. RLHF: As we saw while introducing GPT-4, RLHF is a technique that\naims at using human feedback as an evaluating metric for LLMs’\ngenerated output, and then using that feedback to further optimize the\nmodel.\nThe following is an illustration of how the training process for LLaMA\nworks:\n\n\nFigure 3.8: Two-step fine-tuning to obtain LLaMa-2 chat (source:\nhttps://ai.meta.com/resources/models-and-libraries/llama/)\nTo access the model, you need to submit a request on Meta’s website (the\nform is available at https://ai.meta.com/resources/models-\nand-libraries/llama-downloads/). Once a request is submitted,\nyou will receive an email with the GitHub repository where you will be able\nto download the following assets:\nModel code\nModel weights\nREADME (User Guide)\nResponsible Use Guide\nLicense\nAcceptable Use Policy\nModel Card\nFalcon LLM\nFalcon LLM is a representation of a new trend of LLMs, consisting of\nbuilding lighter models (with fewer parameters) and focusing rather on the\n",
      "page_number": 111,
      "chapter_number": 14,
      "summary": "Finally, Claude 2 demonstrates relevant capabilities also when working with\ncode, scoring 71.2% on the HumanEval benchmark Key topics include model, uses, and llama.",
      "keywords": [
        "model",
        "LLMs",
        "REST API",
        "human",
        "Claude",
        "human feedback",
        "CAI",
        "tokens",
        "principles",
        "Open LLM Leaderboard",
        "base models",
        "LLM",
        "assistant models",
        "context",
        "CAI technique"
      ],
      "concepts": [
        "model",
        "uses",
        "llama",
        "generate",
        "generation",
        "generating",
        "output",
        "human",
        "tokens",
        "evaluating"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 32,
          "title": "Segment 32 (pages 292-299)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 56,
          "title": "Segment 56 (pages 484-491)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 474-483)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 119-127)",
      "start_page": 119,
      "end_page": 127,
      "detection_method": "topic_boundary",
      "content": "quality of the training dataset. Indeed, it is a matter of fact that complex\nmodels like GPT-4 with trillions of parameters are extremely heavy, both in\nthe training phase and inference phase. This implies the need for high and\nexpensive computational power (GPU and TPU-powered) as well as a long\ntraining time.\nFalcon LLM is an open-source model launched by Abu Dhabi’s Technology\nInnovation Institute (TII) in May 2023. It is an autoregressive, decoder-\nonly transformer, trained on 1 trillion tokens, and it has 40 billion parameters\n(even though it has also been released as a lighter version with 7 billion\nparameters). Similarly to what we saw for LlaMA, Falcon LLM also comes\nwith a fine-tuned variant, called “Instruct,” which is tailored toward\nfollowing the user’s instructions.\nDefinition\nInstruct models are specialized for short-form instruction\nfollowing. Instruction following is a task where the model\nhas to execute a natural language command or query, such as\n“write a haiku about cats” or “tell me about the weather in\nParis.” The Instruct fine-tuned models are trained on a large\ndataset of instructions and their corresponding outputs, such\nas the Stanford Alpaca dataset.\nAccording to the Open LLM leaderboard, since its launch, Falcon LLM has\nbeen among the first positions globally, second only to some versions of\nLlaMA.\nSo, the question might be: how can a model with “only” 40 billion\nparameters perform so well? In fact, the answer is in the quality of the\n\n\ndataset. Falcon was developed using specialized tools and incorporates a\nunique data pipeline, which is capable of extracting valuable content from\nweb data. The pipeline was designed to extract high-quality content by\nemploying extensive filtering and deduplication techniques. The resulting\ndataset, called RefinedWeb, has been released by TII under the Apache-2.0\nlicense and can be found at\nhttps://huggingface.co/datasets/tiiuae/falcon-\nrefinedweb.\nBy combining superior data quality with these optimizations, Falcon\nachieves remarkable performance while utilizing around 75% and 80% of\nthe training compute budget of GPT-3 and PaLM-62B, respectively.\nMistral\nThe third and last open-source model series we are going to cover is Mistral,\ndeveloped by Mistral AI, a company founded in April 2023 by a team of AI\nscientists who previously worked at Meta Platforms and Google DeepMind.\nBased in France, the company has quickly made a name for itself by raising\nsignificant funding and releasing open-source LLMs, emphasizing the\nimportance of transparency and accessibility in AI development.\nThe Mistral model, particularly the Mistral-7B-v0.1, is a decoder-only\ntransformer with 7.3 billion parameters, designed for generative text tasks.\nIt’s known for its innovative architecture choices like grouped-query\nattention (GQA) and sliding-window attention (SWA), which have\nallowed it to outperform other models in benchmarks.\nDefinition\n\n\nGQA and SWA are mechanisms designed to improve the\nefficiency and performance of an LLM.\nGQA is a technique that allows for faster inference times\ncompared to standard full attention mechanisms. It does this\nby partitioning the attention mechanism’s query heads into\ngroups, with each group sharing a single key head and value\nhead.\nSWA is used to handle longer text sequences efficiently. It\nextends the model’s attention beyond a fixed window size,\nallowing each layer to reference a range of positions from\nthe preceding layer. This means that the hidden state at a\ncertain position in one layer can attend to hidden states\nwithin a specific range in the previous layer, thus enabling\nthe model to access tokens at a greater distance and manage\nsequences of varying lengths with a reduced inference cost.\nThe model also provides a variant that was fine-tuned for general-purpose\ncapabilities. This variant is called Mistral-7B-instruct, which outperformed\nall other 7 billion LLMs on the market (as of April 2024) on MT-Bench (an\nevaluation framework that uses an LLM as a judge).\nLike many other open-source models, Mistral can be consumed and\ndownloaded via Hugging Face Hub.\nNote\nIn February 2024, Mistral AI and Microsoft entered a multi-\nyear partnership to accelerate AI innovation. This\ncollaboration will leverage Microsoft’s Azure AI\n\n\nsupercomputing infrastructure to support the development\nand deployment of Mistral AI’s LLMs. Mistral AI’s models,\nincluding their advanced model, Mistral Large, will be\navailable to customers through Azure AI Studio and Azure\nMachine Learning model catalog. The partnership aims to\nexpand Mistral AI’s reach to global markets and foster\nongoing research collaboration.\nThe following comparison table provides the main differences between the\nthree models:\nLlaMA\nFalcon LLM\nMistral\nCompany or\ninstitution\nMeta\nTechnology\nInnovation\nInstitute (TII)\nMistral AI\nFirst release\nJuly 2023\nMay 2023\nSeptember\n2023\nArchitecture\nAutoregressive\ntransformer,\ndecoder-only\nAutoregressive\ntransformer,\ndecoder-only\nTransformer,\ndecoder only\nSizes and\nvariants\nThree sizes: 7B,\n13B, and 70B,\nalongside the fine-\nTwo sizes: 7B\nand 40B,\nalongside the\nfine-tuned\n7B size\nalongside the\nfine-tuned\n\n\ntuned version\n(chat)\nversion\n(instruct)\nversion\n(instruct)\nLicenses\nA custom\ncommercial license\nis available at\nhttps://ai.me\nta.com/resour\nces/models-\nand-\nlibraries/lla\nma-downloads/\nCommercial\nApache 2.0\nlicensed\nCommercial\nApache 2.0\nlicensed\nHow to use\nSubmit request\nform at\nhttps://ai.me\nta.com/resour\nces/models-\nand-\nlibraries/lla\nma-downloads/\nand download\nthe GitHub\nrepo\nAlso available in\nHugging Face Hub\nDownload or\nuse Hugging\nFace Hub\nInference\nAPI/Endpoint\nDownload or\nuse Hugging\nFace Hub\nInference\nAPI/Endpoint\nor Azure AI\nStudio\n\n\nTable 3.2: Comparison table of LLMs\nBeyond language models\nSo far, we have only been covering language-specific foundation models as\nthey are the focus of this book. Nevertheless, in the context of AI-powered\napplications, it is worth mentioning that there are additional foundation\nmodels that can handle data that is different from text, which can be\nembedded and orchestrated.\nHere, you can find some examples of large foundation models (LFMs) on\nthe market today:\nWhisper: It is a general-purpose speech recognition model developed\nby OpenAI that can transcribe and translate speech in multiple\nlanguages. It is trained on a large dataset of diverse audio and is also a\nmultitasking model that can perform multilingual speech recognition,\nspeech translation, spoken language identification, and voice activity\ndetection.\nMidjourney: Developed by the independent research lab of the same\nname, Midjourney is based on a sequence-to-sequence transformer\nmodel that takes text prompts and outputs a set of four images that\nmatch the prompts. Midjourney is designed to be a tool for artists and\ncreative professionals, who can use it for rapid prototyping of artistic\nconcepts, inspiration, or experimentation.\nDALL-E: Similar to the previous one, DALL-E, developed by OpenAI,\ngenerates images from natural language descriptions, using a 12-billion\nparameter version of GPT-3 trained on a dataset of text-image pairs.\n\n\nThe idea is that we can combine and orchestrate multiple LFMs within our\napplications to achieve extraordinary results. For example, let’s say we want\nto write a review about an interview with a young chef and post it on\nInstagram. The involved models might be the following:\nWhisper will convert the interview audio into a transcript.\nAn LLM, such as Falcon-7B-instruct, with a web plugin, will\nextrapolate the name of the young chef and search it on the internet to\nretrieve the biography.\nAnother LLM, such as LlaMA, will process the transcript and generate\na review with an Instagram post style. We can also ask the same model\nto generate a prompt that will ask the following model to generate a\npicture based on the post content.\nDall-E will generate an image based on the prompt generated by the\nLLM.\nWe will then provide our LFMs flow with an Instagram plugin so that the\napplication is able to post the whole review, including the illustration, on our\nprofile.\nFinally, there are emerging LFMs that are meant to be multi-modal, meaning\nthat they can handle multiple data formats with just one architecture. An\nexample is GPT-4 itself.\nThe following screenshot shows an example of an early OpenAI experiment\nwith GPT-4 visuals, demonstrating its understanding of funny aspects within\nan image:\n\n\nFigure 3.9: Early experiments with GPT-4 visuals (source:\nhttps://openai.com/research/gpt-4)\nThe following screenshot shows another example of an earlier version of\nGPT-4, demonstrating how it could understand and explain graphs in detail:\n\n\nFigure 3.10: Early experiments with GPT-4 visuals (source:\nhttps://openai.com/research/gpt-4)\n",
      "page_number": 119,
      "chapter_number": 15,
      "summary": "This chapter covers segment 15 (pages 119-127). Key topics include model, mistral, and language. It is an autoregressive, decoder-\nonly transformer, trained on 1 trillion tokens, and it has 40 billion parameters\n(even though it has also been released as a lighter version with 7 billion\nparameters).",
      "keywords": [
        "Falcon LLM",
        "model",
        "Mistral",
        "LLM",
        "Face Hub",
        "Falcon",
        "Hugging Face Hub",
        "LLMs",
        "dataset",
        "billion parameters",
        "Instruct",
        "training",
        "version",
        "Mistral model",
        "Azure"
      ],
      "concepts": [
        "model",
        "mistral",
        "language",
        "data",
        "falcon",
        "instruct",
        "instructions",
        "instruction",
        "inference",
        "dataset"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 8,
          "title": "Segment 8 (pages 144-164)",
          "relevance_score": 0.71,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 474-483)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 32,
          "title": "Segment 32 (pages 640-662)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 35,
          "title": "Segment 35 (pages 702-724)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 128-135)",
      "start_page": 128,
      "end_page": 135,
      "detection_method": "topic_boundary",
      "content": "The following example shows how an early version of GPT-4 could\nunderstand and solve complex mathematical problems while also providing\nthe corresponding justification for its response:\n\n\nUser Answer questionli.a. Think step-by-step.\n\nI. Principe de la détection de rayonnement avec un bolométre\n\nComme illustré sur la figure 1 un bolométre est constitué d'un absorbour qui recoit le rayon:\nnement que l'on désire dévecier. Sa température 7, supposée uniforme, est mesuréc 4 Vaide d'un\nthermométre incorporé, constitué d'un matériau conducteur dont la résistance A(T) varie avec la\ntempérature T; cctte variation est caractérisée par le cocfficient a = Rat Liensemble passéde\nla capacité thermique Cy.\n\nUn barreau, conducteur thermique, homogéne, de longueur £, de section S et de conducti-\nvilé thermique A et sans échanges thermiques latéraux, relie le bolométre & un thermostat de\n\ntempératiane Fh, five\n\nrayonnement incident puissance PF\n\\ absorbeur\na\n\nbarreau\nconducteur\n\ncourant de\npolarisation\n\nthermostat T;\n\nFigure 1 - Schéma d'un bolometre\n\n1.1. Dans cette premiére partie, on étudie le comportement du bolométre en régime station-\nnaire, 4 la température de fonctionnement T ~ Th.\n\nLi.a) Exprimer la lempéralure T(x) en lout point du barrenu conducleur repéré par son\n\nabseisse x: on prendra T(0) = Ty et TUL) = Ty.\n\nGPT-4 To answer question|‘.a, we need toexpress the temperature T(x) in every point of the\nconductive bar, which is characterized by the abscissa x. We are given that T(0)= TO\nand T(L) =Tb.\n\nFor a stationary regime, the heat equation for the one-dimensional heat conduction in\nthe bar is:\n\nd*T/dx*=0\n\nIntegrating once, we get:\n\nIntegrating a second time:\nT(x) = C,x+ C,\n\nTo determine the constants C, and C., we use the boundary conditions:\nT(0) = TO, which gives C,= TO\nT(L) = Tb, which gives C, =(Tb- TO)/L\n\nFinally, the expression for T(x) is:\n\nT(x) = (Tb- TO)* (x /L)+TO\n\nThis represents a linear temperature profile along the conductive bar, with a slope of (Tb\n- TO) /L.\n\nFigure 3.11: Early experiments with GPT-4 visuals (source:\nhttps://openai.com/research/gpt-4)\nGPT-4 is just one example of a large multimodal model (LMM), and it is\nrepresentative of the trend that we will probably witness in the next few\nyears.\nA decision framework to pick\nthe right LLM\nIn previous paragraphs, we covered some of the most promising LLMs\navailable in the market today. Now, the question is: which one should I use\nwithin my applications? The truth is that there is not a straightforward\nanswer to this question.\nConsiderations\nThere are many factors to consider when choosing an LLM for your\napplication. Those factors also need to be declined in two scenarios:\nproprietary and open-source LLMs. The following are some factors and\ntrade-offs you might want to consider while choosing your LLMs:\nSize and performance: We saw that more complex models (that\nmeans, with a high number of parameters) tend to have better\nperformance, especially in terms of parametric knowledge and\ngeneralization capabilities. Nevertheless, the larger the model, the more\ncomputation and memory it requires to process the input and generate\nthe output, which can result in higher latency and, as we will see, higher\ncosts.\n\n\nCost and hosting strategy: When incorporating LLMs within our\napplications, there are two types of costs we have to keep in mind:\nCost for model consumption: This refers to the fee we pay to\nconsume the model. Proprietary models like GPT-4 or Claude 2\nrequire a fee, which is typically proportional to the number of\ntokens processed. On the other hand, open-source models like\nLlaMA or Falcon LLM are free to use.\nCost for model hosting: This refers to your hosting strategy.\nTypically, proprietary models are hosted in a private or public\nhyperscaler, so that they can be consumed via a REST API and\nyou don’t have to worry about the underlying infrastructure (for\nexample, GPT-4 is hosted in a super-computer built in the\nMicrosoft Azure cloud). With open-source models, we typically\nneed to provide our own infrastructure, since those models can be\ndownloaded locally. Of course, the larger the model, the more\npowerful the computational power needed.\nNote\nIn the context of open-source models, another\noption to consume those models is that of using\nthe Hugging Face Inference API. The free version\nallows you to test and evaluate, with a limited\nrate, all the available LLMs on a shared\ninfrastructure hosted on Hugging Face. For\nproduction use cases, Hugging Face also offers\nInference Endpoints, so that you can easily\ndeploy your LLMs on a dedicated and fully\nmanaged infrastructure, with the possibility to\n\n\nconfigure parameters like region, compute power,\nand security level to accommodate your\nconstraints in terms of latency, throughput, and\ncompliance.\nPricing for the Inference Endpoint is publicly\navailable at\nhttps://huggingface.co/docs/infer\nence-endpoints/pricing.\nCustomization: This might be a requirement you want to evaluate\nbefore deciding which model to adopt. In fact, not all models are\nequally flexible in terms of customization. When we talk about\ncustomization, we refer to two activities:\nFine-tuning: This is the process of slightly adjusting LLMs’\nparameters to better fit into a domain. All open-source models\ncan be fine-tuned. When it comes to proprietary models, not all\nLLMs can be fine-tuned: for example, OpenAI’s GPT-3.5 can be\nfine-tuned, while the process of fine-tuning the GPT-4-0613 is\nstill experimental and accessible under request to OpenAI (as per\nDecember 2023).\nHenceforth, it is important to understand whether you will need fine-\ntuning in your application and decide accordingly.\nTraining from scratch: If you really want an LLM that is super\nspecific about your domain knowledge, you might want to retrain\nthe model from scratch. To train an LLM from scratch, without\nhaving to reinvent an architecture, you can download open-\nsource LLMs and simply re-train them on custom datasets. Of\n\n\ncourse, this implies that we have access to the source code, which\nis not the case when we work with proprietary LLMs.\nDomain-specific capabilities: We saw that the most popular way of\nevaluating LLMs’ performance is that of averaging different\nbenchmarks across domains. However, there are benchmarks that are\ntailored towards specific capabilities: if MMLU measures LLMs’\ngeneralized culture and commonsense reasoning, TruthfulQA is more\nconcerned with LLMs’ alignment, while HumanEval is tailored towards\nLLMs’ coding capabilities.\nHenceforth, if you have a tailored use case in mind, you might want to\nuse a model that is a top performer in one specific benchmark, rather\nthan a top performer, on average, across all benchmarks. Namely, you\nmight pick Claude 2 if you are looking for exceptional coding\ncapabilities, or PaLM 2 if analytical reasoning is what you are looking\nfor. On the other hand, if you need a model that encompasses all of\nthese capabilities, GPT-4 might be the right choice for you.\nPicking a domain-specific model is also a way to make some savings\nin terms of model complexity. The thing is, it might be sufficient for\nyou to use a relatively small model (for example, a LlaMA-7B-\ninstruct) if you need to use it for a specific use case, which comes with\nall the benefits in terms of cost and performance.\nNote\nIf you are looking for LLMs that are extremely specific, there\nis a plethora of models that have been trained on domain-\nspecific technical documentation. For example, at the\nbeginning of 2023, the Stanford Center for Research on\n\n\nFoundation Models (CRFM) and MosaicML announced\nthe release of BioMedLM, a decoder-only transformer-based\nLLM with 2.7 billion parameters, trained on biomedical\nabstracts and papers.\nAnother example is BloombergGPT, a 50 billion parameter\nLLM specialized for the financial domain developed by\nBloomberg and trained on a 363 billion token dataset based\non Bloomberg’s extensive data sources, perhaps the largest\ndomain-specific dataset yet, augmented with 345 billion\ntokens from general purpose datasets.\nTo make this decision framework more practical, let’s consider the following\nimaginary case study about the company TechGen.\nCase study\nTechGen Solutions, a leading provider of AI-driven analytics, face a decision\nbetween two advanced language models for their next-generation customer\ninteraction system: GPT-4 and LLaMa-2. They require a robust language\nmodel that can handle diverse customer queries, provide accurate technical\ninformation, and integrate with their proprietary software. The following are\ntheir options:\nGPT-4: Developed by OpenAI, GPT-4 is known for its vast parameter\ncount and the ability to process both text and image inputs\nLLama 2: Created by Meta AI, LLama 2 is an open-source model\npraised for its accessibility and performance on a smaller dataset.\nThe following are the factors that they consider when making their decision:\n\n\nPerformance: TechGen evaluates the models’ performance, particularly\nin generating technical content and code, where GPT-4 has shown\nhigher accuracy.\nIntegration: The ease of integration with TechGen’s systems is critical,\nwith GPT-4 potentially offering more seamless compatibility due to its\nwidespread adoption.\nCost: While LLama 2 is free for commercial use under certain\nconditions, GPT-4 comes with a cost, which TechGen must factor into\ntheir decision.\nFuture-proofing: TechGen considers the long-term viability of each\nmodel, including the potential for updates and improvements.\nBased on these considerations, TechGen opts for GPT-4, swayed by its\nsuperior performance in generating complex, technical responses and its\nmultilingual capabilities, which align with their international expansion\nplans. The decision is also influenced by GPT-4’s image processing feature,\nwhich TechGen anticipates will become increasingly relevant as they\nincorporate more multimedia content into their customer service.\nTechGen’s choice of GPT-4 over LLama 2 is driven by the need for a high-\nperforming, versatile language model that can scale with their growing\nglobal presence and diverse customer needs. While LLama 2’s open-source\nnature and cost effectiveness are appealing, GPT-4’s advanced capabilities\nand future-proof features present a more compelling case for TechGen’s\nambitious goals.\nNote that these decision factors are not meant to be an exhaustive guide to\ndeciding which models to embed within applications. Nevertheless, those are\nuseful elements of reflection while setting up your application flow, so that\n",
      "page_number": 128,
      "chapter_number": 16,
      "summary": "Figure 3.11: Early experiments with GPT-4 visuals (source:\nhttps://openai.com/research/gpt-4)\nGPT-4 is just one example of a large multimodal model (LMM), and it is\nrepresentative of the trend that we will probably witness in the next few\nyears Key topics include model, llms, and performance.",
      "keywords": [
        "User Answer questionli.a",
        "model",
        "LLMs",
        "User Answer",
        "open-source models",
        "LLM",
        "TechGen",
        "Proprietary models",
        "performance",
        "Hugging Face",
        "Cost",
        "capabilities",
        "solve complex mathematical",
        "complex mathematical problems",
        "open-source"
      ],
      "concepts": [
        "model",
        "llms",
        "performance",
        "performer",
        "parameters",
        "specific",
        "technical",
        "custom",
        "source",
        "useful"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 8,
          "title": "Segment 8 (pages 58-66)",
          "relevance_score": 0.46,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.46,
          "method": "sentence_transformers"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 32,
          "title": "Segment 32 (pages 292-299)",
          "relevance_score": 0.44,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 53,
          "title": "Segment 53 (pages 448-456)",
          "relevance_score": 0.43,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 136-143)",
      "start_page": 136,
      "end_page": 143,
      "detection_method": "topic_boundary",
      "content": "you can determine your requirements and then shortlist those LLMs that are\nmore suitable for your goals.\nSummary\nThis chapter covered some of the most promising LLMs in the market. It\nfirst differentiated between proprietary and open-source models, with all the\nrelated pros and cons. It then offered a deep dive into the architecture and\ntechnical features of GPT-4, PaLM-2, Claude 2, LLaMa-2, Falcon LLM, and\nMPT, with the addition of a section covering some LMMs. Finally, it\nprovided a light framework to help developers decide which LLMs to pick\nwhile building AI-powered applications. This is pivotal to get the greatest\nimpact from your application, given your industry-specific scenario.\nStarting from the next chapter, we will start working hands-on with LLMs\nwithin applications.\nReferences\nGPT-4 Technical Report.\nhttps://cdn.openai.com/papers/gpt-4.pdf\nTrain short, test long: attention with linear biases enables input length\nextrapolation. https://arxiv.org/pdf/2108.12409.pdf\nConstitutional AI: Harmlessness from AI Feedback.\nhttps://arxiv.org/abs/2212.08073\nHugging Face Inference Endpoint.\nhttps://huggingface.co/docs/inference-\nendpoints/index\n\n\nHugging Face Inference Endpoint Pricing.\nhttps://huggingface.co/docs/inference-\nendpoints/pricing\nModel Card for BioMedLM 2.7B.\nhttps://huggingface.co/stanford-crfm/BioMedLM\nPaLM 2 Technical Report.\nhttps://ai.google/static/documents/palm2techrepo\nrt.pdf\nSolving Quantitative Reasoning Problems with Language Models.\nhttps://arxiv.org/abs/2206.14858\nJudging LLM-as-a-Judge with MT-Bench and Chatbot Arena.\nhttps://arxiv.org/abs/2306.05685\nJoin our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\nOceanofPDF.com\n\n\n4\nPrompt Engineering\nIn Chapter 2, we introduced the concept of prompt engineering as the\nprocess of designing and optimizing prompts – the text input that guides the\nbehavior of a large language model (LLM) – for LLMs for a wide variety\nof applications and research topics. Since prompts have a massive impact on\nLLM performance, prompt engineering is a crucial activity while designing\nLLM-powered applications. In fact, there are several techniques that can be\nimplemented not only to refine your LLM’s responses but also to reduce\nrisks associated with hallucination and bias.\nIn this chapter, we are going to cover the emerging techniques in the field of\nprompt engineering, starting from basic approaches up to advanced\nframeworks. By the end of this chapter, you will have the foundations to\nbuild functional and solid prompts for your LLM-powered applications,\nwhich will also be relevant in the upcoming chapters.\nWe will go through the following topics:\nIntroduction to prompt engineering\nBasic principles of prompt engineering\nAdvanced techniques of prompt engineering\nTechnical requirements\n\n\nTo complete the tasks in this chapter, you will require the following:\nOpenAI account and API\nPython 3.7.1 or later version\nYou can find all the code and examples in the book’s GitHub repository at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications.\nWhat is prompt engineering?\nA prompt is a text input that guides the behavior of an LLM to generate a\ntext output.\nPrompt engineering is the process of designing effective prompts that elicit\nhigh-quality and relevant output from LLMs. Prompt engineering requires\ncreativity, understanding of the LLM, and precision.\nThe following figure shows an example of how a well-written prompt can\ninstruct the same model to perform three different tasks:\nFigure 4.1: Example of prompt engineering to specialize LLMs\n\n\nAs you might imagine, the prompt becomes one of the key elements for an\nLLM-powered application’s success. As such, it is pivotal to invest time and\nresources in this step, following some best practices and principles that we\nare going to cover in the next sections.\nPrinciples of prompt\nengineering\nGenerally speaking, there are no fixed rules to obtain the “perfect” prompt\nsince there are too many variables to be taken into account (the type of\nmodel used, the goal of the application, the supporting infrastructure, and so\non). Nevertheless, there are some clear principles that have proven to\nproduce positive effects if incorporated into the prompt. Let’s examine some\nof them.\nClear instructions\nThe principle of giving clear instructions is to provide the model with\nenough information and guidance to perform the task correctly and\nefficiently. Clear instructions should include the following elements:\nThe goal or objective of the task, such as “write a poem” or “summarize\nan article”\nThe format or structure of the expected output, such as “use four lines\nwith rhyming words” or “use bullet points with no more than 10 words\neach”\nThe constraints or limitations of the task, such as “do not use any\nprofanity” or “do not copy any text from the source”\n\n\nThe context or background of the task, such as “the poem is about\nautumn” or “the article is from a scientific journal”\nLet’s say, for example, that we want our model to fetch any kind of\ninstructions from text and return to us a tutorial in a bullet list. Also, if there\nare no instructions in the provided text, the model should inform us about\nthat. Here are the steps:\n1. First, we need to initialize our model. For this purpose, we are going to\nleverage OpenAI’s GPT-3.5-turbo model. We first install the openai\nlibrary:\n$pip install openai == 0.28\n2. To initialize the model, I used the openai Python library and set the\nOpenAI API key as the environmental variable:\nimport os\nimport openai\nopenai.api_key = os.environment.get('OPENAI_API_KEY')\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\", # engine = \"deployment_name\n    messages=[\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": instructions},\n    ]\n)\nAs you can see, the chat model comes with two variables\nplaceholders: system message (or metaprompt), where we define how\nwe want our model to behave, and instructions (or query), where the\nuser will ask the model its questions.\n\n\n3. Then, it takes the user’s query (in this case, the text instructions). For\nthis scenario, I set the two variables system_message and instructions\nas follows:\nsystem_message = \"\"\"\nYou are an AI assistant that helps humans by generatin\nYou will be provided with a text. If the text contains\nOtherwise, inform the user that the text does not cont\nText:\n\"\"\"\ninstructions = \"\"\"\nTo prepare the known sauce from Genova, Italy, you can\nchop them in a kitchen mortar together with basil and \nFinally, transfer the pesto to a bowl and stir in the \n\"\"\"\n4. Now let’s test our model:\nprint(response['choices'][0]['message']['content'])\nWe get the following output:\nSure, I can generate a tutorial based on the given text. He\n- Toast the pine nuts on a pan until they are slightly brow\n- Coarsely chop the toasted pine nuts, along with basil lea\n- Add half of the olive oil into the kitchen mortar, and se\n- Grind the mixture until it becomes a smooth paste.\n- Transfer the pesto to a bowl, and stir in the grated Parm\n- You can add more olive oil to achieve the desired consist\n- Your Genovese pesto sauce is now ready. It can be served \n5. Note that if we pass the model another text that does not contain any\ninstructions, it will be able to respond as we instructed it:\n\n\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\", # engine = \"deployment_name\n    messages=[\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": 'the sun is shinin\n    ]\n)\n#print(response)\nprint(response['choices'][0]['message']['content'])\nThe following is the corresponding output:\nAs there are no instructions provided in the text you have \nBy giving clear instructions, you can help the model understand what you\nwant it to do and how you want it to do it. This can improve the quality and\nrelevance of the model’s output and reduce the need for further revisions or\ncorrections.\nHowever, sometimes, there are scenarios where clarity is not enough. We\nmight need to infer the way of thinking of our LLM to make it more robust\nwith respect to its task. In the next section, we are going to examine one of\nthese techniques, which will be very useful in the case of accomplishing\ncomplex tasks.\nSplit complex tasks into\nsubtasks\nAs discussed earlier, prompt engineering is a technique that involves\ndesigning effective inputs for LLMs to perform various tasks. Sometimes,\nthe tasks are too complex or ambiguous for a single prompt to handle, and it\n",
      "page_number": 136,
      "chapter_number": 17,
      "summary": "Summary\nThis chapter covered some of the most promising LLMs in the market Key topics include prompt, models, and openai.",
      "keywords": [
        "Prompt Engineering",
        "Prompt",
        "Model",
        "Engineering",
        "text",
        "instructions",
        "LLMs",
        "LLM",
        "Face Inference Endpoint",
        "OpenAI",
        "message",
        "Hugging Face Inference",
        "tasks",
        "Clear instructions",
        "system"
      ],
      "concepts": [
        "prompt",
        "models",
        "openai",
        "engineering",
        "engine",
        "llm",
        "output",
        "text",
        "follows",
        "tasks"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 8,
          "title": "Segment 8 (pages 61-70)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "Segment 28 (pages 232-240)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 144-151)",
      "start_page": 144,
      "end_page": 151,
      "detection_method": "topic_boundary",
      "content": "is better to split them into simpler subtasks that can be solved by different\nprompts.\nHere are some examples of splitting complex tasks into subtasks:\nText summarization: A complex task that involves generating a\nconcise and accurate summary of a long text. This task can be split into\nsubtasks such as:\nExtracting the main points or keywords from the text\nRewriting the main points or keywords in a coherent and fluent\nway\nTrimming the summary to fit a desired length or format\nMachine translation: A complex task that involves translating a text\nfrom one language to another. This task can be split into subtasks such\nas:\nDetecting the source language of the text\nConverting the text into an intermediate representation that\npreserves the meaning and structure of the original text\nGenerating the text in the target language from the intermediate\nrepresentation\nPoem generation: A creative task that involves producing a poem that\nfollows a certain style, theme, or mood. This task can be split into\nsubtasks such as:\nChoosing a poetic form (such as sonnet, haiku, limerick, etc.) and\na rhyme scheme (such as ABAB, AABB, ABCB, etc.) for the\npoem\nGenerating a title and a topic for the poem based on the user’s\ninput or preference\n\n\nGenerating the lines or verses of the poem that match the chosen\nform, rhyme scheme, and topic\nRefining and polishing the poem to ensure coherence, fluency,\nand originality\nCode generation: A technical task that involves producing a code\nsnippet that performs a specific function or task. This task can be split\ninto subtasks such as:\nChoosing a programming language (such as Python, Java, C++,\netc.) and a framework or library (such as TensorFlow, PyTorch,\nReact, etc.) for the code\nGenerating a function name and a list of parameters and return\nvalues for the code based on the user’s input or specification\nGenerating the body of the function that implements the logic and\nfunctionality of the code\nAdding comments and documentation to explain the code and its\nusage\nLet’s consider the following example in Python, where we will ask our\nmodel to generate a summary of an article:\n1. We will leverage OpenAI’s GPT-3.5-turbo model in a manner similar to\nthe example discussed earlier in this chapter:\nimport os\nimport openai\nopenai.api_key = os.environ.get(\"OPENAI_API_KEY\")\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-35-turbo\", # engine = \"deployment_name\"\n    messages=[\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": article},\n\n\n    ]\n)\n2. Let’s set both the system_message and article variables as follows\n(you can find the entire scripts in the book’s GitHub repository):\nsystem_message = \"\"\"\nYou are an AI assistant that summarizes articles.\nTo complete this task, do the following subtasks:\nRead the provided article context comprehensively and \nGenerate a paragraph summary of the current article co\nPrint each step of the process.\nArticle:\n\"\"\"\narticle = \"\"\"\nRecurrent neural networks, long short-term memory, and\nin particular, […]\n\"\"\"\n3. To see the output, you can run the following code:\nprint(response['choices'][0]['message']['content'])\nHere is the obtained output:\nSummary:\nThe article discusses the use of recurrent neural networks\nSteps:\n1. The article discusses the success and limitations of rec\n2. Attention mechanisms have become popular in addressing t\n3. The authors propose the Transformer, a model architectu\n4. The Transformer can achieve state-of-the-art results in \nAs you can see, the model was able to produce a high-quality summary\nbased on the key topics extracted (and displayed) from the given article. The\n\n\nfact that we prompted the model to split the task into subtasks “forced” it to\nreduce the complexity of each subtask, hence improving the quality of the\nfinal result. This approach can also lead to noticeable results when we deal\nwith scenarios such as mathematical problems since it enhances the\nanalytical reasoning capabilities of the model.\nNote\nIn a landscape of many different LLMs, it is crucial to know\nthat the very same system message may not be as efficient in\nall models. A system message that perfectly works with GPT-\n4 might not be as efficient when applied to Llama 2, for\nexample. Therefore, it is pivotal to design the prompt in\naccordance with the type of LLM you decide to pick for your\napplication.\nSplitting complex tasks into easier subtasks is a powerful technique;\nnevertheless, it does not address one of the main risks of LLM-generated\ncontent, that is, having a wrong output. In the next two sections, we are\ngoing to see some techniques that are mainly aimed at addressing this risk.\nAsk for justification\nLLMs are built in such a way that they predict the next token based on the\nprevious ones without looking back at their generations. This might lead the\nmodel to output wrong content to the user, yet in a very convincing way. If\nthe LLM-powered application does not provide a specific reference to that\nresponse, it might be hard to validate the ground truth behind it. Henceforth,\nspecifying in the prompt to support the LLM’s answer with some reflections\n\n\nand justification could prompt the model to recover from its actions.\nFurthermore, asking for justification might be useful also in case of answers\nthat are right but we simply don’t know the LLM’s reasoning behind it. For\nexample, let’s say we want our LLM to solve riddles. To do so, we can\ninstruct it as follows:\nsystem_message = \"\"\"\nYou are an AI assistant specialized in solving riddles.\nGiven a riddle, solve it the best you can.\nProvide a clear justification of your answer and the reasoning b\nRiddle:\n\"\"\"\nAs you can see, I’ve specified in the metaprompt to the LLM to justify its\nanswer and also provide its reasoning. Let’s see how it works:\nriddle = \"\"\"\nWhat has a face and two hands, but no arms or legs?\n\"\"\"\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": riddle},\n    ]\n)\n#print(response)\nprint(response['choices'][0]['message']['content'])\nThe following is the obtained output:\nThe answer to this riddle is a clock.\nA clock has a face with numbers and two hands (the hour and minut\nHence, a clock is the correct answer to this riddle.\n\n\nJustifications are a great tool to make your model more reliable and robust\nsince they force it to “rethink” its output, as well as provide us with a view\nof how the reasoning was set to solve the problem.\nWith a similar approach, we could also intervene at different prompt levels\nto improve our LLM’s performance. For example, we might discover that\nthe model is systematically tackling a mathematical problem in the wrong\nway; henceforth, we might want to suggest the right approach directly at the\nmetaprompt level. Another example might be that of asking the model to\ngenerate multiple outputs – along with their justifications – to evaluate\ndifferent reasoning techniques and prompt the best one in the metaprompt.\nIn the next section, we are going to focus on one of these examples, more\nspecifically, the possibility of generating multiple outputs and then picking\nthe most likely one.\nGenerate many outputs, then\nuse the model to pick the best\none\nAs we saw in the previous section, LLMs are built in such a way that they\npredict the next token based on the previous ones without looking back at\ntheir generations. If this is the case, if one sampled token is the wrong one\n(in other words, if the model is unlucky), the LLM will keep generating\nwrong tokens and, henceforth, wrong content. Now, the bad news is that,\nunlike humans, LLMs cannot recover from errors on their own. This means\nthat, if we ask them, they acknowledge the error, but we need to explicitly\nprompt them to think about that.\n\n\nOne way to overcome this limitation is to broaden the space of probabilities\nof picking the right token. Rather than generating just one response, we can\nprompt the model to generate multiple responses, and then pick the one that\nis most suitable for the user’s query. This splits the job into two subtasks for\nour LLM:\n1. Generating multiple responses to the user’s query\n2. Comparing those responses and picking the best one, according to some\ncriteria we can specify in the metaprompt\nLet’s see an example, following up from the riddles examined in the\nprevious section:\nsystem_message = \"\"\"\nYou are an AI assistant specialized in solving riddles.\nGiven a riddle, you have to generate three answers to the riddle\nFor each answer, be specific about the reasoning you made.\nThen, among the three answers, select the one that is most plaus\nRiddle:\n\"\"\"\nriddle = \"\"\"\nWhat has a face and two hands, but no arms or legs?\n\"\"\"\nIn this case, I’ve prompted the model to generate three answers to the riddle,\nthen to give me the most likely, justifying why. Let’s see the result:\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\", # engine = \"deployment_name\".\n    messages=[\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": riddle},\n    ]\n)\n\n\n#print(response)\nprint(response['choices'][0]['message']['content'])\nWe then get the following output:\nAnswer 1: A clock. A clock has a face with numbers and two hands \nAnswer 2: A watch. Similar to a clock, a watch also has a face wi\nAnswer 3: A wall. While this answer may be less obvious, a wall d\nAmong the three answers, the most plausible one is answer 1: a cl\nAs you can see, the model selected the most plausible answer along with a\njustification of its choice. It is interesting to note that “clock” and “watch”\nmight seem similar responses; however, the model specified that “watch” is\nusually worn on a person’s wrist and, even though it doesn’t mean it has\narms or legs, this element might have lowered the probability of being the\ncorrect answer.\nWhat would you have picked?\nAs discussed earlier, forcing the model to tackle a problem with different\napproaches is a way to collect multiple samples of reasonings, which might\nserve as further instructions in the metaprompt. For example, if we want the\nmodel to always propose something that is not the most straightforward\nsolution to a problem – in other words, if we want it to “think differently” –\nwe might force it to solve a problem in N ways and then use the most\ncreative reasoning as a framework in the metaprompt.\nThe last element we are going to examine is the overall structure we want to\ngive to our metaprompt. In fact, in previous examples, we saw a sample\nsystem message with some statements and instructions. In the next section,\nwe will see how the order and “strength” of those statements and instructions\nare not invariants.\n",
      "page_number": 144,
      "chapter_number": 18,
      "summary": "To see the output, you can run the following code:\nprint(response['choices'][0]['message']['content'])\nHere is the obtained output:\nSummary:\nThe article discusses the use of recurrent neural networks\nSteps:\n1 Key topics include article, generating, and generation.",
      "keywords": [
        "model",
        "answer",
        "message",
        "task",
        "system",
        "riddle",
        "LLM",
        "content",
        "subtasks",
        "generating",
        "article",
        "response",
        "Text",
        "system message",
        "message content"
      ],
      "concepts": [
        "article",
        "generating",
        "generation",
        "generate",
        "generations",
        "riddles",
        "response",
        "messages",
        "reasoning",
        "model"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 13,
          "title": "Segment 13 (pages 253-274)",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 22,
          "title": "Segment 22 (pages 435-456)",
          "relevance_score": 0.49,
          "method": "sentence_transformers"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 58,
          "title": "Segment 58 (pages 514-521)",
          "relevance_score": 0.46,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 25,
          "title": "Segment 25 (pages 209-219)",
          "relevance_score": 0.45,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 14,
          "title": "Segment 14 (pages 275-296)",
          "relevance_score": 0.45,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 152-159)",
      "start_page": 152,
      "end_page": 159,
      "detection_method": "topic_boundary",
      "content": "Repeat instructions at the end\nLLMs tend not to process the metaprompt attributing the same weight or\nimprortance to all the sections. In fact, in his blog post Large Language\nModel Prompt Engineering for Complex Summarization, John Stewart (a\nsoftware engineer at Microsoft) found some interesting outcomes from\narranging prompt sections\n(https://devblogs.microsoft.com/ise/gpt-summary-\nprompt-engineering/). More specifically, after several\nexperimentations, he found that repeating the main instruction at the end of\nthe prompt can help the model overcome its inner recency bias.\nDefinition\nRecency bias is the tendency of LLMs to give more weight\nto the information that appears near the end of a prompt, and\nignore or forget the information that appears earlier. This can\nlead to inaccurate or inconsistent responses that do not take\ninto account the whole context of the task. For example, if\nthe prompt is a long conversation between two people, the\nmodel may only focus on the last few messages and\ndisregard the previous ones.\nLet’s look at some ways to overcome recency bias:\nOne possible way to overcome recency bias is to break down the task\ninto smaller steps or subtasks and provide feedback or guidance along\nthe way. This can help the model focus on each step and avoid getting\nlost in irrelevant details. We’ve covered this technique in the Split\n\n\ncomplex tasks into subtasks section in, which we discussed splitting\ncomplex tasks into easier subtasks.\nAnother way to overcome recency bias with prompt engineering\ntechniques is to repeat the instructions or the main goal of the task at\nthe end of the prompt. This can help remind the model of what it is\nsupposed to do and what kind of response it should generate.\nFor instance, let’s say we want our model to output the sentiment of a\nwhole chat history between an AI agent and the user. We want to make\nsure that the model will output the sentiment in lowercase and without\npunctuation.\nLet’s consider the following example (the conversation is truncated, but you\ncan find the whole code in the book’s GitHub repository). In this case, the\nkey instruction is that of having as output only the sentiment in lowercase\nand without punctuation:\nsystem_message = \"\"\"\nYou are a sentiment analyzer. You classify conversations into th\nReturn only the sentiment, in lowercase and without punctuation.\nConversation:\n\"\"\"\nconversation = \"\"\"\nCustomer: Hi, I need some help with my order.\nAI agent: Hello, welcome to our online store. I'm an AI agent an\nCustomer: I ordered a pair of shoes yesterday, but I haven't rec\n[…]\n\"\"\"\nIn this scenario, we have key instructions before the conversation, so let’s\ninitialize our model and feed it with the two variables system_message and\nconversation:\n\n\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\", # engine = \"deployment_name\".\n    messages=[\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": conversation},\n    ]\n)\n#print(response)\nprint(response['choices'][0]['message']['content'])\nHere is the output that we receive:\nNeutral\nThe model didn’t follow the instruction of having only lowercase letters.\nLet’s try to repeat the instruction also at the end of the prompt:\nsystem_message = f\"\"\"\nYou are a sentiment analyzer. You classify conversations into th\nReturn only the sentiment, in lowercase and without punctuation.\nConversation:\n{conversation}\nRemember to return only the sentiment, in lowercase and without \n\"\"\"\nAgain, let’s invoke our model with the updated system_message:\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\", # engine = \"deployment_name\".\n    messages=[\n        {\"role\": \"user\", \"content\": system_message},\n    ]\n)\n#print(response)\nprint(response['choices'][0]['message']['content'])\n\n\nHere is the corresponding output:\nneutral\nAs you can see, now the model was able to provide exactly the output we\ndesired. This approach is particularly useful whenever we have a\nconversation history to keep storing in the context window. If this is the\ncase, having the main instructions at the beginning might induce the model\nnot to have them in mind once it also goes through the whole history, hence\nreducing their strength.\nUse delimiters\nThe last principle to be covered is related to the format we want to give to\nour metaprompt. This helps our LLM to better understand its intents as well\nas relate different sections and paragraphs to each other.\nTo achieve this, we can use delimiters within our prompt. A delimiter can be\nany sequence of characters or symbols that is clearly mapping a schema\nrather than a concept. For example, we can consider the following sequences\nto be delimiters:\n>>>>\n====\n------\n####\n` ` ` ` `\nThis leads to a series of benefits, including:\n\n\nClear separation: Delimiters mark distinct sections within a prompt,\nseparating instructions, examples, and desired output.\nGuidance for LLMs: Proper use of delimiters removes ambiguity,\nguiding the model effectively.\nEnhanced precision: Delimiters improve prompt understanding,\nresulting in more relevant responses.\nImproved coherence: Effective use of delimiters organizes instructions,\ninputs, and outputs, leading to coherent responses.\nLet’s consider, for example, a metaprompt that aims at instructing the model\nto translate user’s tasks into Python code, providing an example to do so:\nsystem_message = \"\"\"\nYou are a Python expert who produces Python code as per the user\n===>START EXAMPLE\n---User Query---\nGive me a function to print a string of text.\n---User Output---\nBelow you can find the described function:\n```def my_print(text):\n     return print(text)\n```\n<===END EXAMPLE\n\"\"\"\nquery = \"generate a Python function to calculate the nth Fibonac\nIn the above example, we’ve used delimiters to both specify the beginning\nand end of an example for a one-shot learning approach and, within the\nexample, specify the Python code snippet.\nLet’s see how it works:\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\", # engine = \"deployment_name\".\n\n\n    messages=[\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": query},\n    ]\n)\n#print(response)\nprint(response['choices'][0]['message']['content'])\nHere is our output:\nSure, here's a Python function to calculate the nth Fibonacci num\n```python def fibonacci(n):\nif n < 0:\nreturn None\nelif n == 0:\nreturn 0\nelif n == 1:\nreturn 1\nelse:\nreturn fibonacci(n-1) + fibonacci(n-2) ```\nThe function takes a non-negative integer `n` as input and return\nAs you can see, it also printed the code with backticks, as shown within the\nsystem message.\nAll the principles examined up to this point are general rules that can make\nyour LLM-powered application more robust. Those techniques should be\nkept in mind regardless of the type of application you are developing since\nthey are general best practices that improve your LLM performance. In the\nfollowing section, we are going to see some advanced techniques for prompt\nengineering.\nAdvanced techniques\n\n\nAdvanced techniques might be implemented for specific scenarios and\naddress the way the model reasons and thinks about the answer before\nproviding it to the final user. Let’s look at some of these in the upcoming\nsections.\nFew-shot approach\nIn their paper Language Models are Few-Shot Learners, Tom Brown et al.\ndemonstrate that GPT-3 can achieve strong performance on many NLP tasks\nin a few-shot setting. This means that for all tasks, GPT-3 is applied without\nany fine-tuning, with tasks and few-shot demonstrations specified purely via\ntext interaction with the model.\nThis is an example and evidence of how the concept of few-shot learning –\nwhich means providing the model with examples of how we would like it to\nrespond – is a powerful technique that enables model customization without\ninterfering with the overall architecture.\nFor example, let’s say we want our model to generate a tagline for a new\nproduct line of climbing shoes we’ve just coined – Elevation Embrace. We\nhave an idea of what the tagline should be like – concise and direct. We\ncould explain it to the model in plain text; however, it might be more\neffective simply to provide it with some examples of similar projects.\nLet’s see an implementation with code:\nsystem_message = \"\"\"\nYou are an AI marketing assistant. You help users to create tagl\nGiven a product name, produce a tagline similar to the following\nPeak Pursuit - Conquer Heights with Comfort\nSummit Steps - Your Partner for Every Ascent\nCrag Conquerors - Step Up, Stand Tall\nProduct name:\n\n\n\"\"\"\nproduct_name = 'Elevation Embrace'\nLet’s see how our model will handle this request:\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\", # engine = \"deployment_name\".\n    messages=[\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": product_name},\n    ]\n)\n#print(response)\nprint(response['choices'][0]['message']['content'])\nThe following is our output:\nTagline idea: Embrace the Heights with Confidence.\nAs you can see, it maintained the style, length, and also writing convention\nof the provided taglines. This is extremely useful when you want your model\nto follow examples you already have, such as fixed templates.\nNote that, most of the time, few-shot learning is powerful enough to\ncustomize a model even in extremely specialized scenarios, where we could\nthink about fine-tuning as the proper tool. In fact, proper few-shot learning\ncould be as effective as a fine-tuning process.\nLet’s look at another example. Let’s say we want to develop a model that\nspecializes in sentiment analysis. To do so, we provide it with a series of\nexamples of texts with different sentiments, alongside the output we would\nlike – positive or negative. Note that this set of examples is nothing but a\n",
      "page_number": 152,
      "chapter_number": 19,
      "summary": "This chapter covers segment 19 (pages 152-159). Key topics include responses, messages. Let’s consider the following example (the conversation is truncated, but you\ncan find the whole code in the book’s GitHub repository).",
      "keywords": [
        "Model",
        "message",
        "Prompt",
        "system",
        "response",
        "output",
        "user",
        "content",
        "conversation",
        "Python",
        "message content",
        "sentiment",
        "delimiters",
        "end",
        "recency bias"
      ],
      "concepts": [
        "responses",
        "messages",
        "let",
        "model",
        "user",
        "output",
        "prompt",
        "conversation",
        "conversations",
        "instructions"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 14,
          "title": "Segment 14 (pages 275-296)",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 45,
          "title": "Segment 45 (pages 386-394)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 22,
          "title": "Segment 22 (pages 435-456)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "Segment 32 (pages 270-277)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 18,
          "title": "Segment 18 (pages 147-158)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 160-167)",
      "start_page": 160,
      "end_page": 167,
      "detection_method": "topic_boundary",
      "content": "small training set for supervised learning tasks; the only difference from\nfine-tuning is that we are not updating the model’s parameters.\nTo provide you with a concrete representation of what was said above, let’s\nprovide our model with just two examples for each label:\nsystem_message = \"\"\"\nYou are a binary classifier for sentiment analysis.\nGiven a text, based on its sentiment, you classify it into one o\nYou can use the following texts as examples:\nText: \"I love this product! It's fantastic and works perfectly.\"\nPositive\nText: \"I'm really disappointed with the quality of the food.\"\nNegative\nText: \"This is the best day of my life!\"\nPositive\nText: \"I can't stand the noise in this restaurant.\"\nNegative\nONLY return the sentiment as output (without punctuation).\nText:\n\"\"\"\nTo test our classifier, I’ve used the IMDb database of movie reviews\navailable on Kaggle at\nhttps://www.kaggle.com/datasets/yasserh/imdb-movie-\nratings-sentiment-analysis/data. As you can see, the dataset\ncontains many movie reviews along with their associated sentiment –\npositive or negative. Let’s substitute the binary label of 0–1 with a verbose\nlabel of Negative–Positive:\nimport numpy as np\nimport pandas as pd\ndf = pd .read_csv('movie.csv', encoding='utf-8')\n\n\ndf['label'] = df['label'].replace({0: 'Negative', 1: 'Positive'}\ndf.head()\nThis gives us the first few records of the dataset, which are as follows:\nFigure 4.2: First observations of the movie dataset\nNow, we want to test the performance of our model over a sample of 10\nobservations of this dataset:\ndf = df.sample(n=10, random_state=42)\ndef process_text(text):\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": text},\n        ]\n    )\n    return response['choices'][0]['message']['content']\ndf['predicted'] = df['text'].apply(process_text)\nprint(df)\nThe following is our output:\n\n\nFigure 4.3: Output of a GPT-3.5 model with few-shot examples\nAs you can see, by comparing the label and predicted columns, the model\nwas able to correctly classify all the reviews, without even fine-tuning! This\nis just an example of what you can achieve – in terms of model\nspecialization – with the technique of few-shot learning.\nChain of thought\nIntroduced in the paper Chain-of-Thought Prompting Elicits Reasoning in\nLarge Language Models by Wei et al., chain of thought (CoT) is a\ntechnique that enables complex reasoning capabilities through intermediate\nreasoning steps. It also encourages the model to explain its reasoning,\n“forcing” it not to be too fast and risking giving the wrong response (as we\nsaw in previous sections).\nLet’s say that we want to prompt our LLM to solve generic first-degree\nequations. To do so, we are going to provide it with a basic reasoning list\nthat it might want to follow:\nsystem_message = \"\"\"\nTo solve a generic first-degree equation, follow these steps:\n1. **Identify the Equation:** Start by identifying the equation \n2. **Isolate the Variable:** Your goal is to isolate the variabl\n\n\n \n   a. **Add or Subtract Constants:** Add or subtract 'b' from bo\n \n   b. **Divide by the Coefficient:** Divide both sides by 'a' to\n3. **Simplify:** Simplify both sides of the equation as much as \n4. **Solve for 'x':** Once 'x' is isolated on one side, you have\n5. **Check Your Solution:** Plug the found value of 'x' back int\n6. **Express the Solution:** Write down the solution in a clear \n7. **Consider Special Cases:** Be aware of special cases where t\nEquation:\n\"\"\"\nLet’s see how it can be implemented:\nequation = \"3x + 5 = 11\"\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\", # engine = \"deployment_name\".\n    messages=[\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": equation},\n    ]\n)\n#print(response)\nprint(response['choices'][0]['message']['content'])\nThe following is our output:\n1. Identify the Equation: The equation is 3x + 5 = 11.\n2. Isolate the Variable:\n  \n   a. Subtract Constants: Subtract 5 from both sides to move cons\n  \n      3x + 5 - 5 = 11 - 5\n  \n   Simplifying, we get:\n  \n      3x = 6\n  \n\n\n   b. Divide by the Coefficient: Divide both sides by 3 to isolat\n  \n      3x/3 = 6/3\n  \n   Simplifying, we get:\n  \n      x = 2\n  \n3. Simplify: Both sides are already simplified.\n4. Solve for 'x': x = 2.\n5. Check Your Solution: Plug x = 2 back into the original equatio\n   3(2) + 5 = 11\n  \n   Simplifying, we get:\n  \n   6 + 5 = 11\n  \n   The left side equals the right side, so the solution is correc\n6. Express the Solution: The solution to the equation 3x + 5 = 11\n7. Consider Special Cases: Since the coefficient 3 is not zero, t\nAs you can see, the model clearly followed the seven steps specified in the\nmetaprompt, which also allows the model to “take its time” to perform this\ntask. Note that you can also combine it with few-shot prompting to get better\nresults on more complex tasks that require reasoning before responding.\nWith CoT, we are prompting the model to generate intermediate reasoning\nsteps. This is also a component of another reasoning technique, which we are\ngoing to examine in the next section.\nReAct\nIntroduced in the paper ReAct: Synergizing Reasoning and Acting in\nLanguage Models by Yao et al., ReAct (Reason and Act) is a general\nparadigm that combines reasoning and acting with LLMs. ReAct prompts\n\n\nthe language model to generate verbal reasoning traces and actions for a\ntask, and also receives observations from external sources such as web\nsearches or databases. This allows the language model to perform dynamic\nreasoning and quickly adapt its action plan based on external information.\nFor example, you can prompt the language model to answer a question by\nfirst reasoning about the question, then performing an action to send a query\nto the web, then receiving an observation from the search results, and then\ncontinuing with this thought, action, observation loop until it reaches a\nconclusion.\nThe difference between CoT and ReAct approaches is that CoT prompts the\nlanguage model to generate intermediate reasoning steps for a task, while\nReAct prompts the language model to generate intermediate reasoning steps,\nactions, and observations for a task.\nNote that the “action” phase is generally related to the possibility for our\nLLM to interact with external tools, such as a web search.\nFor example, let’s say we want to ask our model for some up-to-date\ninformation about the upcoming Olympic games. To do so, we are going to\nbuild a smart LangChain agent (as described in Chapter 2) leveraging\nSerpAPIWrapperWrapper (to wrap the SerpApi to navigate the web), the\nAgentType tool (to decide which type of agent to use for our goal), and other\nprompt-related modules (to make it easier to “templatize” our instructions).\nLet’s see how we can do this (I won’t dive deeper into each component of\nthe following code since the next chapter will be entirely focused on\nLangChain and its main components):\nimport os\nfrom dotenv import load_dotenv\nfrom langchain import SerpAPIWrapper\nfrom langchain.agents import AgentType, initialize_agent\n\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.tools import BaseTool, StructuredTool, Tool, tool\nfrom langchain.schema import HumanMessage\nmodel = ChatOpenAI(\n    model_name='gpt-35-turbo'\n)\nload_dotenv()\nkey = os.environ[\"SERPAPI_API_KEY\"]\nsearch = SerpAPIWrapper()\ntools = [\n    Tool.from_function(\n        func=search.run,\n        name=\"Search\",\n        description=\"useful for when you need to answer question\n    )\n    ]\nagent_executor = initialize_agent(tools, model, agent=AgentType.\nAs you can see, for this purpose, I’ve used a pre-built agent type available in\nLangChain called ZERO_SHOT_REACT_DESCRIPTION. It comes with a\nprecompiled prompt that follows the ReAct approach. Let’s inspect that\nprompt:\nprint(agent_executor.agent.llm_chain.prompt.template)\nHere is the corresponding output:\nAnswer the following questions as best you can. You have access t\nSearch: useful for when you need to answer questions about curren\nUse the following format:\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [Search]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N ti\nThought: I now know the final answer\n\n\nFinal Answer: the final answer to the original input question\nBegin!\nQuestion: {input}\nThought:{agent_scratchpad}\nLet’s now test our agent by asking something about the upcoming Olympic\ngames and zooming in on the intermediate steps:\nagent_executor('who are going to be the italian male athletes fo\nThis is the output with intermediate steps:\n> Entering new AgentExecutor chain... I should search for recent \n>Observation: A select group of climbers ensured their participat\n>Input: \"List of Italian male climbers Paris 2024 Olympics\" Obser\n>Thought:This information does not seem to be readily available, \n>Action: Contact Action Input: Email or phone call to the Italian\n>Action: Search Action Input: \"Latest updates on Italian male cli\n>Thought: Based on the information I have gathered, it seems that\n>Final Answer: Matteo Zurloni is one of the Italian male climbers\nHere is the obtained output:\n'Matteo Zurloni is one of the Italian male climbers who has secur\nAt the time of this question (7th of October 2023), the answer is definitely\ncorrect. Note how the model went through several iterations of\nObservation/Thought/Action until it reached the conclusion. This is a great\nexample of how prompting a model to think step by step and explicitly\ndefine each step of the reasoning makes it “wiser” and more cautious before\nanswering. It is also a great technique to prevent hallucination.\n",
      "page_number": 160,
      "chapter_number": 20,
      "summary": "This chapter covers segment 20 (pages 160-167). Key topics include reasoning, thought, and text. To provide you with a concrete representation of what was said above, let’s\nprovide our model with just two examples for each label:\nsystem_message = \"\"\"\nYou are a binary classifier for sentiment analysis.",
      "keywords": [
        "model",
        "action",
        "Reasoning",
        "Action Input",
        "language model",
        "text",
        "thought",
        "equation",
        "input",
        "small training set",
        "intermediate reasoning steps",
        "answer",
        "agent",
        "reasoning steps",
        "Solution"
      ],
      "concepts": [
        "reasoning",
        "thought",
        "text",
        "prompting",
        "searches",
        "search",
        "actions",
        "model",
        "steps",
        "react"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "Segment 32 (pages 270-277)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 45,
          "title": "Segment 45 (pages 386-394)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 56,
          "title": "Segment 56 (pages 484-491)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 168-175)",
      "start_page": 168,
      "end_page": 175,
      "detection_method": "topic_boundary",
      "content": "Overall, prompt engineering is a powerful discipline, still in its emerging\nphase yet already widely adopted within LLM-powered applications. In the\nfollowing chapters, we are going to see concrete applications of this\ntechnique.\nSummary\nIn this chapter, we covered many aspects of the activity of prompt\nengineering, a core step in the context of improving the performance of\nLLMs within your application, as well as customizing it depending on the\nscenario. Prompt engineering is an emerging discipline that is paving the\nway for a new category of applications, infused with LLMs.\nWe started with an introduction to the concept of prompt engineering and\nwhy it is important, and then moved toward the basic principles – including\nclear instructions, asking for justification, etc. Then, we moved on to more\nadvanced techniques that are meant to shape the reasoning approach of our\nLLM: few-shot learning, CoT, and ReAct.\nIn the next chapters, we will see those techniques in action by building real-\nworld applications using LLMs.\nReferences\nReAct approach: https://arxiv.org/abs/2210.03629\nWhat is prompt engineering?:\nhttps://www.mckinsey.com/featured-\ninsights/mckinsey-explainers/what-is-prompt-\nengineering\n\n\nPrompt engineering techniques:\nhttps://blog.mrsharm.com/prompt-engineering-\nguide/\nPrompt engineering principles:\nhttps://learn.microsoft.com/en-us/azure/ai-\nservices/openai/concepts/advanced-prompt-\nengineering?pivots=programming-language-chat-\ncompletions\nRecency bias: https://learn.microsoft.com/en-\nus/azure/ai-services/openai/concepts/advanced-\nprompt-engineering?pivots=programming-language-\nchat-completions#repeat-instructions-at-the-end\nLarge Language Model Prompt Engineering for Complex\nSummarization:\nhttps://devblogs.microsoft.com/ise/2023/06/27/gp\nt-summary-prompt-engineering/\nLanguage Models are Few-Shot Learners:\nhttps://arxiv.org/pdf/2005.14165.pdf\nIMDb dataset:\nhttps://www.kaggle.com/datasets/yasserh/imdb-\nmovie-ratings-sentiment-analysis/code\nReAct: https://arxiv.org/abs/2210.03629\nChain of Thought Prompting Elicits Reasoning in Large Language\nModels: https://arxiv.org/abs/2201.11903\n\n\nJoin our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\nOceanofPDF.com\n\n\n5\nEmbedding LLMs within\nYour Applications\nThis chapter kickstarts the hands-on portions of this book, focusing on how\nwe can leverage large language models (LLMs) to build powerful AI\napplications. In fact, LLMs have introduced a whole new paradigm in\nsoftware development, paving the way for new families of applications that\nhave the peculiarity of making the communication between the user and the\nmachine smooth and conversational. Plus, those models enhanced existing\napplications, such as chatbots and recommendation systems, with their\nunique reasoning capabilities.\nDeveloping LLM-powered applications is becoming a key factor for\nenterprises to keep themselves competitive in the market, and this leads to\nthe spreading of new libraries and frameworks that make it easier to embed\nLLMs within applications. Some examples are Semantic Kernel, Haystack,\nLlamaIndex, and LangChain. In this chapter, we are going to cover\nLangChain and use its modules to build hands-on examples. By the end of\nthis chapter, you will have the technical foundations to start developing your\nLLM-powered applications using LangChain and open-source Hugging Face\nmodels.\nIn this chapter, we will cover the following topics:\nA brief note about LangChain\n\n\nGetting started with LangChain\nWorking with LLMs via the Hugging Face Hub\nTechnical requirements\nTo complete the hands-on sections of this chapter, the following\nprerequisites are needed:\nA Hugging Face account and user access token.\nAn OpenAI account and user access token.\nPython 3.7.1 or later version.\nPython packages: Make sure to have the following Python packages\ninstalled: langchain, python-dotenv, huggingface_hub, google-\nsearch-results, faiss, and tiktoken. Those can be easily installed via\npip install in your terminal.\nYou can find all the code and examples used in this chapter in the book’s\nGitHub repository at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications\nA brief note about LangChain\nJust as generative AI has evolved so rapidly over the last year, so has\nLangChain. In the months between the writing of this book and its\npublication, the AI orchestrator has gone through massive changes. The most\nremarkable traces back to January 2024, when the first stable version of\nLangChain was released, introducing a new organization of packages and\nlibraries.\n\n\nIt consists of the following:\nA core backbone where all the abstractions and runtime logic are stored\nA layer of third-party integrations and components\nA set of pre-built architectures and templates to leverage\nA serving layer to consume chains as APIs\nAn observability layer to monitor your applications in the development,\ntesting, and production stages\nYou can look at the architecture in greater detail at\nhttps://python.langchain.com/docs/get_started/intro\nduction.\nThere are three packages you can install to start using LangChain:\nlangchain-core: This contains the base abstractions and runtime for the\nwhole LangChain ecosystem.\nlangchain-experimental: This holds experimental LangChain code,\nintended for research and experimental uses.\nlangchain-community: This contains all third-party integrations.\nOn top of that, there are three additional packages that we’re not going to\ncover in this book, yet can be leveraged to monitor and maintain your\nLangChain applications:\nlangserve: LangServe is a tool that lets you deploy LangChain\nrunnables and chains as a REST API, making it easier to integrate\nLangChain applications into production environments.\nlangsmith: Think of LangSmith as an innovative testing framework\nfor evaluating language models and AI applications. It helps visualize\ninputs and outputs at each step in the chain, aiding understanding and\nintuition during development.\n\n\nlangchain-cli: The official command-line interface for LangChain, it\nfacilitates interactions with LangChain projects, including template\nusage and quickstarts.\nLast but not least, LangChain introduced the LangChain Expression\nLanguage (LCEL) to enhance the efficiency and flexibility of text\nprocessing tasks.\nKey features of LCEL include:\nStreaming asynchronous support: This allows for the efficient\nhandling of data streams.\nBatch support: This enables processing data in batches.\nParallel execution: This enhances performance by executing tasks\nconcurrently.\nRetries and fallbacks: This ensures robustness by handling failures\ngracefully.\nDynamically routing logic: This allows logic flow based on input and\noutput.\nMessage history: This keeps track of interactions for context-aware\nprocessing.\nWe are not going to cover LCEL in this book; however, all the code samples\ncan be converted into LCEL if you want to speed up your development and\nleverage its native integration with the end-to-end LangChain development\nstack.\nImportant note\nBefore we start working with LangChain, it is important to\nnote that all packages are versioned slightly differently, yet\n\n\nall releases are cut with high frequency by a maintainer with\na clearer communication strategy for breaking changes.\nIn the upcoming chapters, you will see some packages that\nhave been moved, for example, to the experimental\npackage, meaning that they are more prone to experimental\nuses. Similarly, some third-party integrations have been\nmoved to the community package.\nStarting from the next section, we are going to cover the backbone concepts\n– such as memory, VectorDB, and agents – that remain solid in the\nLangChain framework and, more generally, in the landscape of LLM\ndevelopment.\nGetting started with\nLangChain\nAs introduced in Chapter 2, LangChain is a lightweight framework meant to\nmake it easier to integrate and orchestrate LLMs and their components\nwithin applications. It is mainly Python based, yet it recently extended its\nsupport to JavaScript and TypeScript.\nIn addition to LLM integration (which we will cover in an upcoming\ndedicated section), we saw that LangChain offers the following main\ncomponents:\nModels and prompt templates\nData connections\nMemory\nChains\n",
      "page_number": 168,
      "chapter_number": 21,
      "summary": "Summary\nIn this chapter, we covered many aspects of the activity of prompt\nengineering, a core step in the context of improving the performance of\nLLMs within your application, as well as customizing it depending on the\nscenario Key topics include langchain, prompting, and python.",
      "keywords": [
        "prompt engineering",
        "LangChain",
        "applications",
        "engineering",
        "prompt",
        "Model Prompt Engineering",
        "LLMs",
        "Language Model Prompt",
        "Language Models",
        "Prompt engineering techniques",
        "Models",
        "Language",
        "Hugging Face",
        "packages",
        "Prompt engineering principles"
      ],
      "concepts": [
        "langchain",
        "prompting",
        "python",
        "development",
        "packages",
        "language",
        "applications",
        "application",
        "model",
        "llms"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 56,
          "title": "Segment 56 (pages 484-491)",
          "relevance_score": 0.71,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 8,
          "title": "Segment 8 (pages 61-70)",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 42-49)",
          "relevance_score": 0.67,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 7,
          "title": "Segment 7 (pages 50-57)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 176-183)",
      "start_page": 176,
      "end_page": 183,
      "detection_method": "topic_boundary",
      "content": "Agents\nThese components are illustrated in the following diagram:\nFigure 5.1: LangChain’s components\nThe next sections will take a deep dive into each of these components.\nModels and prompts\nLangChain offers more than 50 integrations with third-party vendors and\nplatforms, including OpenAI, Azure OpenAI, Databricks, and MosaicML,\nas well as the integration with the Hugging Face Hub and the world of open-\nsource LLMs. In Part 2 of this book, we will be trying various LLMs, both\nproprietary and open-source, and leveraging LangChain’s integrations.\nJust to provide an example, let’s see how easy it is to consume the OpenAI\nGPT-3 model (you can retrieve your OpenAI API key at\nhttps://platform.openai.com/account/api-keys):\n\n\nfrom langchain.llms import OpenAI\nllm = OpenAI(openai_api_key=\"your-api-key\")\nprint(llm('tell me a joke'))\nHere is the corresponding output:\nQ: What did one plate say to the other plate?\nA: Dinner's on me!\nNote\nWhile running examples with LLMs, the output will vary at\neach run, due to the stochasticity of the models themselves.\nIf you want to reduce the margin of variations in your output,\nyou can make your model more “deterministic” by tuning the\ntemperature hyperparameter. This parameter ranges from 0\n(deterministic) to 1 (stochastic).\nBy default, the OpenAI module uses the gpt-3.5-turbo-instruct as a\nmodel. You can specify the model you want to use by passing the model’s\nname as a parameter.\nAs said previously, we will dive deeper into LLMs in the next section; so,\nfor now, let’s focus on prompts. There are two main components related to\nLLM prompts and prompts design/engineering:\nPrompt templates: A prompt template is a component that defines how\nto generate a prompt for a language model. It can include variables,\nplaceholders, prefixes, suffixes, and other elements that can be\ncustomized according to the data and the task.\n\n\nFor example, suppose you want to use a language model to generate a\ntranslation from one language to another. You can use a prompt\ntemplate like this:\nSentence: {sentence}\nTranslation in {language}:\n{sentence} is a variable that will be replaced by the actual text.\nTranslation in {language}: is a prefix that indicates the task and the\nexpected output format.\nYou can easily implement this template as follows:\nfrom langchain import PromptTemplate\ntemplate = \"\"\"Sentence: {sentence}\nTranslation in {language}:\"\"\"\nprompt = PromptTemplate(template=template, input_variables=\nprint(prompt.format(sentence = \"the cat is on the table\", l\nHere is the output:\nSentence: the cat is on the table\nTranslation in spanish:\nGenerally speaking, prompt templates tend to be agnostic with respect\nto the LLM you might decide to use, and it is adaptable to both\ncompletion and chat models.\nDefinition\nA completion model is a type of LLM that takes a text\ninput and generates a text output, which is called a\n\n\ncompletion. The completion model tries to continue\nthe prompt in a coherent and relevant way, according\nto the task and the data it was trained on. For example,\na completion model can generate summaries,\ntranslations, stories, code, lyrics, and more, depending\non the prompt.\nA chat model is a special kind of completion model\nthat is designed to generate conversational responses.\nA chat model takes a list of messages as input, where\neach message has a role (either system, user, or\nassistant) and content. The chat model tries to generate\na new message for the assistant role, based on the\nprevious messages and the system instruction.\nThe main difference between completion and chat\nmodels is that completion models expect a single text\ninput as a prompt, while chat models expect a list of\nmessages as input.\nExample selector: An example selector is a component in LangChain\nthat allows you to choose which examples to include in a prompt for a\nlanguage model. A prompt is a text input that guides the language\nmodel to produce a desired output. Examples are pairs of inputs and\noutputs that demonstrate the task and the format of the output as\nfollows:\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal gene\n\n\nThe idea recalls the concept of few-shot learning we covered in\nChapter 1.\nLangChain offers the example selector class called\nBaseExampleSelector that you can import and modify as you wish.\nYou can find the API reference at\nhttps://python.langchain.com/docs/modules/model\n_io/prompts/example_selectors/.\nData connections\nData connections refer to the building blocks needed to retrieve the\nadditional non-parametric knowledge we want to provide the model with.\nThe idea is to cover the typical flow of incorporating user-specific data into\napplications that are made of five main blocks, as illustrated in the following\nfigure:\nFigure 5.2: Incorporating user-specific knowledge into LLMs (source:\nhttps://python.langchain.com/docs/modules/data_connection/)\nThose blocks are addressed with the following LangChain tools:\nDocument loaders: They are in charge of loading documents from\ndifferent sources such as CSV, file directory, HTML, JSON,\n\n\nMarkdown, and PDF. Document loaders expose a .load method for\nloading data as documents from a configured source. The output is a\nDocument object that contains a piece of text and associated metadata.\nFor example, let’s consider a sample CSV file to be loaded (you can\nfind the whole code in the book’s GitHub repository at\nhttps://github.com/PacktPublishing/Building-\nLLM-Powered-Applications):\nfrom langchain.document_loaders.csv_loader import CSVLoade\nloader = CSVLoader(file_path='sample.csv')\ndata = loader.load()\nprint(data)\nHere is the output:\n[Document(page_content='Name: John\\nAge: 25\\nCity: New York\nDocument transformers: After importing your documents, it’s\ncommon to modify them to better match your needs. A basic instance of\nthis is breaking down a lengthy document into smaller chunks that fit\nyour model’s context window. Within LangChain, there are various pre-\nbuilt document transformers available called text splitters. The idea of\ntext splitters is to make it easier to split documents into chunks that are\nsemantically related so that we do not lose context or relevant\ninformation.\nWith text splitters, you can decide how to split the text (for example,\nby character, heading, token, and so on) and how to measure the\nlength of the chunk (for example, by number of characters).\n\n\nFor example, let’s split a document using the\nRecursiveCharacterTextSplitter module, which operates at a\ncharacter level. For this purpose, we will be using a .txt file about\nmountains (you can find the whole code in the book’s GitHub\nrepository at\nhttps://github.com/PacktPublishing/Building-\nLLM-Powered-Applications):\nwith open('mountain.txt') as f:\n    mountain = f.read()\nfrom langchain.text_splitter import RecursiveCharacterTextS\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size = 100, #number of characters for each chunk\n    chunk_overlap  = 20,#number of characters overlapping b\n    length_function = len #function used to measure the num\n)\ntexts = text_splitter.create_documents([mountain])\nprint(texts[0])\nprint(texts[1])\nprint(texts[2])\nHere, chunk_size refers to the number of characters in each chunk\nwhile chunk_overlap represents the number of characters overlapping\nbetween successive chunks. Here is the output:\npage_content=\"Amidst the serene landscape, towering mountai\npage_content='The crisp mountain air carries whispers of t\nText embedding models: In Chapter 1, in the Under the hood of an\nLLM section, we introduced the concept of embedding as a way to\nrepresent words, subwords, or characters in a continuous vector space.\n\n\nEmbeddings are the key step in incorporating non-parametric\nknowledge into LLMs. In fact, once properly stored in a VectorDB\n(which will be covered in the next section), they become the non-\nparametric knowledge against which we can measure the distance of a\nuser’s query.\nTo get started with embedding, you will need an embedding model.\nThen, LangChain offers the Embedding class with two main modules,\nwhich address the embedding of, respectively, the non-parametric\nknowledge (multiple input text) and the user query (single input text).\nFor example, let’s consider the embeddings using the OpenAI\nembedding model text-embedding-ada-002 (for more details about\nOpenAI embedding models, you can refer to the official\ndocumentation at\nhttps://platform.openai.com/docs/guides/embeddi\nngs/what-are-embeddings):\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom dotenv import load_dotenv\nload_dotenv()\nos.environ[\"OPENAI_API_KEY\"]\nembeddings_model = OpenAIEmbeddings(model ='text-embedding\nembeddings = embeddings_model.embed_documents(\n    [\n        \"Good morning!\",\n        \"Oh, hello!\",\n        \"I want to report an accident\",\n        \"Sorry to hear that. May I ask your name?\",\n        \"Sure, Mario Rossi.\"\n    ]\n)\nprint(\"Embed documents:\")\nprint(f\"Number of vector: {len(embeddings)}; Dimension of e\nembedded_query = embeddings_model.embed_query(\"What was the\n",
      "page_number": 176,
      "chapter_number": 22,
      "summary": "In Part 2 of this book, we will be trying various LLMs, both\nproprietary and open-source, and leveraging LangChain’s integrations Key topics include document, documentation, and models.",
      "keywords": [
        "model",
        "text",
        "Prompt",
        "OpenAI",
        "completion model",
        "language model",
        "output",
        "embedding",
        "Document",
        "completion",
        "LLMs",
        "llm",
        "OpenAI embedding model",
        "LangChain",
        "Text embedding models"
      ],
      "concepts": [
        "document",
        "documentation",
        "models",
        "text",
        "langchain",
        "prompts",
        "openai",
        "embedding",
        "importing",
        "output"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 474-483)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 42-49)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 184-192)",
      "start_page": 184,
      "end_page": 192,
      "detection_method": "topic_boundary",
      "content": "print(\"Embed query:\")\nprint(f\"Dimension of the vector: {len(embedded_query)}\")\nprint(f\"Sample of the first 5 elements of the vector: {embe\nHere is the output:\nEmbed documents:\nNumber of vector: 5; Dimension of each vector: 1536\nEmbed query:\nDimension of the vector: 1536\nSample of the first 5 elements of the vector: [0.0053872121\nOnce we have both documents and the query embedded, the next step\nwill be to compute the similarity between the two elements and\nretrieve the most suitable information from the document embedding.\nWe will see the details of this when talking about vector stores.\nVector stores: A vector store (or VectorDB) is a type of database that\ncan store and search over unstructured data, such as text, images, audio,\nor video, by using embeddings. By using embeddings, vector stores can\nperform a fast and accurate similarity search, which means finding the\nmost relevant data for a given query.\nDefinition\nSimilarity is a measure of how close or related two\nvectors are in a vector space. In the context of LLMs,\nvectors are numerical representations of sentences,\nwords, or documents that capture their semantic\nmeaning, and the distance between those vectors\nshould be representative of their semantic similarity.\n\n\nThere are different ways to measure similarity between\nvectors, and while working with LLMs, one of the\nmost popular measures in use is cosine similarity.\nThis is the cosine of the angle between two vectors in\na multidimensional space. It is computed as the dot\nproduct of the vectors divided by the product of their\nlengths. Cosine similarity is insensitive to scale and\nlocation, and it ranges from -1 to 1, where 1 means\nidentical, 0 means orthogonal, and -1 means opposite.\nThe following is an illustration of the typical flow while using a vector\nstore.\nFigure 5.3: Sample architecture of a vector store (source:\nhttps://python.langchain.com/docs/modules/data_connection/vectorstor\nes/)\nLangChain offers more than 40 integrations with third-party vector\nstores. Some examples are Facebook AI Similarity Search (FAISS),\nElasticsearch, MongoDB Atlas, and Azure Search. For an exhaustive\nlist and descriptions of all the integrations, you can check the official\ndocumentation at\n\n\nhttps://python.langchain.com/docs/integrations/\nvectorstores/.\nAs an example, let’s leverage the FAISS vector store, which has been\ndeveloped by Meta AI research for efficient similarity search and\nclustering of dense vectors. We are going to leverage the same\ndialogue.txt file saved in the previous section:\nfrom langchain.document_loaders import TextLoader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nfrom dotenv import load_dotenv\nload_dotenv()\nos.environ[\"OPENAI_API_KEY\"]\n# Load the document, split it into chunks, embed each chunk\nraw_documents = TextLoader('dialogue.txt').load()\ntext_splitter = CharacterTextSplitter(chunk_size=50, chunk_\ndocuments = text_splitter.split_documents(raw_documents)\ndb = FAISS.from_documents(documents, OpenAIEmbeddings())\nNow that we’ve embedded and saved the non-parametric knowledge,\nlet’s also embed a user’s query so that it can be used to search the\nmost similar text chunk using cosine similarity as a measure:\nquery = \"What is the reason for calling?\"\ndocs = db.similarity_search(query)\nprint(docs[0].page_content)\nThe following is the output:\nI want to report an accident\n\n\nAs you can see, the output is the piece of text that is more likely to\ncontain the answer to the question. In an end-to-end scenario, it will be\nused as context to the LLM to generate a conversational response.\nRetrievers: A retriever is a component in LangChain that can return\ndocuments relevant to an unstructured query, such as a natural language\nquestion or a keyword. A retriever does not need to store the documents\nitself, but only to retrieve them from a source. A retriever can use\ndifferent methods to find relevant documents, such as keyword\nmatching, semantic search, or ranking algorithms.\nThe difference between a retriever and a vector store is that a retriever\nis more general and flexible than a vector store. A retriever can use\nany method to find relevant documents, while a vector store relies on\nembeddings and similarity metrics. A retriever can also use different\nsources of documents, such as web pages, databases, or files, while a\nvector store needs to store the data itself.\nHowever, a vector store can also be used as the backbone of a retriever\nif the data is embedded and indexed by a vector store. In that case, the\nretriever can use the vector store to perform a similarity search over\nthe embedded data and return the most relevant documents. This is\none of the main types of retrievers in LangChain, and it is called a\nvector store retriever.\nFor example, let’s consider the FAISS vector store we previously\ninitialized and “mount” a retriever on top of that:\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\nretriever = db.as_retriever()\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\n\n\nquery = \"What was the reason of the call?\"\nqa.run(query)\nHere is the output:\n' The reason for the call was to report an accident.'\nOverall, data connection modules offer a plethora of integrations and pre-\nbuilt templates that make it easier to manage the flow of your LLM-powered\napplication. We will see some concrete applications of these building blocks\nin the upcoming chapters, but in the next section, we are going to take a deep\ndive into another one of LangChain’s main components.\nMemory\nIn the context of LLM-powered applications, memory allows the application\nto keep references to user interactions, both in the short and long term. For\nexample, let’s consider the well-known ChatGPT. While interacting with the\napplication, you have the possibility to ask follow-up questions referencing\nprevious interactions without explicitly telling the model.\nPlus, all conversations are saved into threads, so that, if you want to follow\nup on a previous conversation, you can re-open the thread without providing\nChatGPT with all the contexts. This is made possible thanks to ChatGPT’s\nability to store users’ interactions into a memory variable and use this\nmemory as context while addressing follow-up questions.\nLangChain offers several modules for designing your memory system within\nyour applications, enabling it with both reading and writing skills.\n\n\nThe first step to do with your memory system is to actually store your human\ninteractions somewhere. To do so, you can leverage numerous built-in\nmemory integrations with third-party providers, including Redis, Cassandra,\nand Postgres.\nThen, when it comes to defining how to query your memory system, there\nare various memory types you can leverage:\nConversation buffer memory: This is the “plain vanilla” memory type\navailable in LangChain. It allows you to store your chat messages and\nextract them in a variable.\nConversation buffer window memory: It is identical to the previous\none, with the only difference being allowing a sliding window over\nonly K interactions so that you can manage longer chat history over\ntime.\nEntity memory: Entity memory is a feature of LangChain that allows\nthe language model to remember given facts about specific entities in a\nconversation. An entity is a person, place, thing, or concept that can be\nidentified and distinguished from others. For example, in the sentence\n“Deven and Sam are working on a hackathon in Italy,” Deven and Sam\nare entities (person), as well as hackathon (thing) and Italy (place).\nEntity memory works by extracting information on entities from the\ninput text using an LLM. It then builds up its knowledge about that\nentity over time by storing the extracted facts in a memory store. The\nmemory store can be accessed and updated by the language model\nwhenever it needs to recall or learn new information about an entity.\nConversation knowledge graph memory: This type of memory uses a\nknowledge graph to recreate memory.\n\n\nDefinition\nA knowledge graph is a way of representing and\norganizing knowledge in a graph structure, where nodes\nare entities and edges are relationships between them. A\nknowledge graph can store and integrate data from\nvarious sources, and encode the semantics and context\nof the data. A knowledge graph can also support various\ntasks, such as search, question answering, reasoning,\nand generation.\nAnother example of a knowledge graph is DBpedia,\nwhich is a community project that extracts structured\ndata from Wikipedia and makes it available on the web.\nDBpedia covers topics such as geography, music,\nsports, and films, and provides links to other datasets\nlike GeoNames and WordNet.\nYou can use this type of memory to save the input and output of each\nconversation turn as knowledge triplets (such as subject, predicate,\nand object) and then use them to generate relevant and consistent\nresponses based on the current context. You can also query the\nknowledge graph to get the current entities or the history of the\nconversation.\nConversation summary memory: When it comes to longer\nconversations to be stored, this type of memory can be very useful,\nsince it creates a summary of the conversation over time (leveraging an\nLLM).\n\n\nConversation summary buffer memory: This type of memory\ncombines the ideas behind buffer memory and conversation summary\nmemory. It keeps a buffer of recent interactions in memory, but rather\nthan just completely flushing old interactions (as occurs for the\nconversation buffer memory) it compiles them into a summary and uses\nboth.\nConversation token buffer memory: It is similar to the previous one,\nwith the difference that, to determine when to start summarizing the\ninteractions, this type of memory uses token lengths rather than the\nnumber of interactions (as occurs in summary buffer memory).\nVector store-backed memory: This type of memory leverages the\nconcepts of embeddings and vector stores previously covered. It is\ndifferent from all the previous memories since it stores interactions as\nvectors, and then retrieves the top K most similar texts every time it is\nqueried, using a retriever.\nLangChain provides specific modules for each of those memory types. Let’s\nconsider an example with the conversation summary memory, where we will\nalso need an LLM to generate the summary of the interactions:\nfrom langchain.memory import ConversationSummaryMemory, ChatMess\nfrom langchain.llms import OpenAI\nmemory = ConversationSummaryMemory(llm=OpenAI(temperature=0))\nmemory.save_context({\"input\": \"hi, I'm looking for some ideas to\nmemory.load_memory_variables({})\nHere is the output:\n{'history': '\\nThe human asked for ideas to write an essay in AI \n\n\nAs you can see, the memory summarized the conversation, leveraging the\nOpenAI LLM we initialized.\nThere is no recipe to define which memory to use within your applications;\nhowever, there are some scenarios that might be particularly suitable for\nspecific memories. For example, a knowledge graph memory is useful for\napplications that need to access information from a large and diverse corpus\nof data and generate responses based on semantic relationships, while a\nconversation summary buffer memory could be suitable for creating\nconversational agents that can maintain a coherent and consistent context\nover multiple turns, while also being able to compress and summarize the\nprevious dialogue history.\nChains\nChains are predetermined sequences of actions and calls to LLMs that make\nit easier to build complex applications that require combining LLMs with\neach other or with other components.\nLangChain offers four main types of chain to get started with:\nLLMChain: This is the most common type of chain. It consists of a\nprompt template, an LLM, and an optional output parser.\nDefinition\nAn output parser is a component that helps structure\nlanguage model responses. It is a class that implements\ntwo main methods: get_format_instructions and\nparse. The get_format_instructions method returns\na string containing instructions for how the output of a\n",
      "page_number": 184,
      "chapter_number": 23,
      "summary": "We will see the details of this when talking about vector stores Key topics include memory, memories, and conversational. Vector stores: A vector store (or VectorDB) is a type of database that\ncan store and search over unstructured data, such as text, images, audio,\nor video, by using embeddings.",
      "keywords": [
        "vector store",
        "Memory",
        "vector",
        "store",
        "FAISS vector store",
        "conversation",
        "buffer memory",
        "documents",
        "retriever",
        "similarity",
        "vector store retriever",
        "Conversation summary memory",
        "query",
        "type",
        "knowledge"
      ],
      "concepts": [
        "memory",
        "memories",
        "conversational",
        "conversations",
        "langchain",
        "vector",
        "stores",
        "storing",
        "similarity",
        "documents"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 21,
          "title": "Segment 21 (pages 174-182)",
          "relevance_score": 0.79,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 17,
          "title": "Segment 17 (pages 139-148)",
          "relevance_score": 0.75,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 25,
          "title": "Segment 25 (pages 495-517)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 25,
          "title": "Segment 25 (pages 216-223)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 193-201)",
      "start_page": 193,
      "end_page": 201,
      "detection_method": "topic_boundary",
      "content": "language model should be formatted. The parse\nmethod takes in a string (assumed to be the response\nfrom a language model) and parses it into some\nstructure, such as a dictionary, a list, or a custom\nobject.\nThis chain takes multiple input variables, uses PromptTemplate to\nformat them into a prompt, passes it to the model, and then uses\nOutputParser (if provided) to parse the output of the LLM into a final\nformat.\nFor example, let’s retrieve the prompt template we built in the\nprevious section:\nfrom langchain import PromptTemplate\ntemplate = \"\"\"Sentence: {sentence}\nTranslation in {language}:\"\"\"\nprompt = PromptTemplate(template=template, input_variables=\nNow, let’s put it into an LLMChain:\nfrom langchain import OpenAI, LLMChain\nllm = OpenAI(temperature=0)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nllm_chain.predict(sentence=\"the cat is on the table\", langu\nHere is the output:\n' El gato está en la mesa.'\n\n\nRouterChain: This is a type of chain that allows you to route the input\nvariables to different chains based on some conditions. You can specify\nthe conditions as functions or expressions that return a Boolean value.\nYou can also specify the default chain to use if none of the conditions\nare met.\nFor example, you can use this chain to create a chatbot that can handle\ndifferent types of requests, such as planning an itinerary or booking a\nrestaurant reservation. To achieve this goal, you might want to\ndifferentiate two different prompts, depending on the type of query the\nuser will make:\nitinerary_template = \"\"\"You are a vacation itinerary assist\nYou help customers finding the best destinations and itine\nYou help customer screating an optimized itinerary based on\nHere is a question:\n{input}\"\"\"\nrestaurant_template = \"\"\"You are a restaurant booking assis\nYou check with customers number of guests and food preferen\nYou pay attention whether there are special conditions to t\nHere is a question:\n{input}\"\"\"\nThanks to RouterChain, we can build a chain that is able to activate a\ndifferent prompt depending on the user’s query. I won’t post the whole\ncode here (you can find the notebook on the book’s GitHub at\nhttps://github.com/PacktPublishing/Building-\nLLM-Powered-Applications), but you can see a sample output\nof how the chain reacts to two different user’s queries:\nprint(chain.run(\"I'm planning a trip from Milan to Venice b\n\n\nHere is the output:\n> Entering new MultiPromptChain chain...\nitinerary: {'input': \"I'm planning a trip from Milan to Ven\n> Finished chain.\nAnswer:\nThere are many attractions that you can visit while traveli\nHere it is with a second query:\nprint(chain.run(\"I want to book a table for tonight\"))\nHere is the output:\n> Entering new MultiPromptChain chain...\nrestaurant: {'input': 'I want to book a table for tonight'}\n> Finished chain.\n. How many people are in your party?\nHi there! How many people are in your party for tonight's \nSequentialChain: This is a type of chain that allows you to execute\nmultiple chains in a sequence. You can specify the order of the chains\nand how they pass their outputs to the next chain. The simplest module\nof a sequential chain, takes by default the output of one chain as the\ninput of the next chain. However, you can also use a more complex\nmodule to have more flexibility to set input and output among chains.\nAs an example, let’s consider an AI system that is meant to first\ngenerate a joke on a given topic, and then translate it in to another\nlanguage. To do so, we will first create two chains:\n\n\nfrom langchain.llms import OpenAI\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\nllm = OpenAI(temperature=.7)\ntemplate = \"\"\"You are a comedian. Generate a joke on the fo\nJoke:\"\"\"\nprompt_template = PromptTemplate(input_variables=[\"topic\"]\njoke_chain = LLMChain(llm=llm, prompt=prompt_template)\ntemplate = \"\"\"You are translator. Given a text input, trans\nTranslation:\"\"\"\n.prompt_template = PromptTemplate(input_variables=[\"languag\ntranslator_chain = LLMChain(llm=llm, prompt=prompt_template\nNow, let’s combine them using the SimpleSequentialChain module:\n# This is the overall chain where we run these two chains i\nfrom langchain.chains import SimpleSequentialChain\noverall_chain = SimpleSequentialChain(chains=[joke_chain, t\ntranslated_joke = overall_chain.run(\"Cats and Dogs\")\nHere is the output:\n> Entering new SimpleSequentialChain chain...\nWhy did the cat cross the road? To prove to the dog that it\n ¿Por qué cruzó el gato la carretera? ¡Para demostrarle al \n> Finished chain.\nTransformationChain: This is a type of chain that allows you to\ntransform the input variables or the output of another chain using some\nfunctions or expressions. You can specify the transformation as a\nfunction that takes the input or output as an argument and returns a new\nvalue, as well as specify the output format of the chain.\n\n\nFor example, let’s say we want to summarize a text, but before that,\nwe want to rename one of the protagonists of the story (a cat) as\n“Silvester the Cat.” As a sample text, I asked Bing Chat to generate a\nstory about cats and dogs (you can find the whole .txt file in the\nGitHub repository of this book):\nfrom langchain.chains import TransformChain, LLMChain, Simp\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\ntransform_chain = TransformChain(\n    input_variables=[\"text\"], output_variables=[\"output_tex\n)\ntemplate = \"\"\"Summarize this text:\n{output_text}\nSummary:\"\"\"\nprompt = PromptTemplate(input_variables=[\"output_text\"], te\nllm_chain = LLMChain(llm=OpenAI(), prompt=prompt)\nsequential_chain = SimpleSequentialChain(chains=[transform_\nsequential_chain.run(cats_and_dogs)\nAs you can see, we’ve combined a simple sequential chain with a\ntransformation chain, where we set as a transformation function the\nrename_cat function (you can see the whole code in the GitHub\nrepository).\nThe output is the following:\n\" Silvester the Cat and a dog lived together but did not ge\nOverall, LangChain chains are a powerful way to combine different\nlanguage models and tasks into a single workflow. Chains are flexible,\nscalable, and easy to use, and they enable users to leverage the power of\n\n\nlanguage models for various purposes and domains. Starting from the next\nchapter, we are going to see chains in action in concrete use cases, but before\ngetting there, we need to cover the last component of LangChain: agents.\nAgents\nAgents are entities that drive decision-making within LLM-powered\napplications. They have access to a suite of tools and can decide which tool\nto call based on the user input and the context. Agents are dynamic and\nadaptive, meaning that they can change or adjust their actions based on the\nsituation or the goal: in fact, while in a chain, the sequence of actions is\nhardcoded, in agents, the LLM is used as the reasoning engine with the goal\nof planning and executing the right actions in the right order.\nA core concept while talking about agents is that of tools. In fact, an agent\nmight be good at planning all the right actions to fulfill a user’s query, but\nwhat if it cannot actually execute them, since it is missing information or\nexecutive power? For example, imagine I want to build an agent that is\ncapable of answering my questions by searching the web. By itself, the agent\nhas no access to the web, so I need to provide it with this tool. I will do so by\nusing SerpApi (the Google Search API) integration provided by LangChain\n(you can retrieve your API key at\nhttps://serpapi.com/dashboard).\nLet’s see it in Python:\nfrom langchain import SerpAPIWrapper\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain.llms import OpenAI\nfrom langchain.tools import BaseTool, StructuredTool, Tool, tool\nimport os\nfrom dotenv import load_dotenv\n\n\nload_dotenv()\nos.environ[\"SERPAPI_API_KEY\"]\nsearch = SerpAPIWrapper()\ntools = [Tool.from_function(\n        func=search.run,\n        name=\"Search\",\n        description=\"useful for when you need to answer question\n    )]\nagent = initialize_agent(tools, llm = OpenAI(), agent=AgentType.\nagent.run(\"When was Avatar 2 released?\")\nThe following is the output:\n> Entering new AgentExecutor chain...\n I need to find out when Avatar 2 was released.\nAction: Search\nAction Input: \"Avatar 2 release date\"\nObservation: December 16, 2022\nThought: I now know the final answer.\nFinal Answer: Avatar 2 was released on December 16, 2022.\n> Finished chain.\n'Avatar 2 was released on December 16, 2022.'\nNote that, while initializing my agent, I set the agent type as\nZERO_SHOT_REACT_DESCRIPTION. This is one of the configurations we can pick\nand, specifically, it configures the agent to decide which tool to pick based\nsolely on the tool’s description with a ReAct approach:\nDefinition\nThe ReAct approach is a way of using LLMs to solve\nvarious language reasoning and decision-making tasks. It\nwas introduced in the paper ReAct: Synergizing Reasoning\n\n\nand Acting in Language Models by Shunyu Yao et al., back\nin October 2022.\nThe ReAct approach prompts LLMs to generate both verbal\nreasoning traces and text actions in an interleaved manner,\nallowing for greater synergy between the two. Reasoning\ntraces help the model to plan, track, and update its actions, as\nwell as handle exceptions. Actions allow the model to\ninteract with external sources, such as knowledge bases or\nenvironments, to gather additional information.\nOn top of this configuration, LangChain also offers the following types of\nagents:\nStructured input ReAct: This is an agent type that uses the ReAct\nframework to generate natural language responses based on structured\ninput data. The agent can handle different types of input data, such as\ntables, lists, or key-value pairs. The agent uses a language model and a\nprompt to generate responses that are informative, concise, and\ncoherent.\nOpenAI Functions: This is an agent type that uses the OpenAI\nFunctions API to access various language models and tools from\nOpenAI. The agent can use different functions, such as GPT-3, Codex,\nDALL-E, CLIP, or ImageGPT. The agent uses a language model and a\nprompt to generate requests to the OpenAI Functions API and parse the\nresponses.\nConversational: This is an agent type that uses a language model to\nengage in natural language conversations with the user. The agent can\nhandle different types of conversational tasks, such as chit-chat,\nquestion answering, or task completion. The agent uses a language\n\n\nmodel and a prompt to generate responses that are relevant, fluent, and\nengaging.\nSelf ask with search: This is an agent type that uses a language model\nto generate questions for itself and then search for answers on the web.\nThe agent can use this technique to learn new information or test its\nown knowledge.\nReAct document store: This is an agent type that uses the ReAct\nframework to generate natural language responses based on documents\nstored in a database. The agent can handle different types of documents,\nsuch as news articles, blog posts, or research papers.\nPlan-and-execute agents: This is an experimental agent type that uses\na language model to choose a sequence of actions to take based on the\nuser’s input and a goal. The agent can use different tools or models to\nexecute the actions it chooses. The agent uses a language model and a\nprompt to generate plans and actions and then uses AgentExecutor to\nrun them.\nLangChain agents are pivotal whenever you want to let your LLMs interact\nwith the external world. Plus, it is interesting to see how agents leverage\nLLMs not only to retrieve and generate responses, but also as reasoning\nengines to plan an optimized sequence of actions.\nTogether with all the LangChain components covered in this section, agents\ncan be the core of LLM-powered applications, as we will see in the next\nchapters. In the next section, we are going to shift toward the world of open-\nsource LLMs, introducing the Hugging Face Hub and its native integration\nwith LangChain.\n",
      "page_number": 193,
      "chapter_number": 24,
      "summary": "This chapter covers segment 24 (pages 193-201). Key topics include chain, input, and agents. The parse\nmethod takes in a string (assumed to be the response\nfrom a language model) and parses it into some\nstructure, such as a dictionary, a list, or a custom\nobject.",
      "keywords": [
        "chain",
        "agent",
        "input",
        "language model",
        "language",
        "prompt",
        "agent type",
        "output",
        "LLM",
        "model",
        "Finished chain",
        "template",
        "type",
        "OpenAI",
        "generate"
      ],
      "concepts": [
        "chain",
        "input",
        "agents",
        "prompt",
        "reacts",
        "uses",
        "custom",
        "language",
        "action",
        "searching"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 474-483)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "Segment 28 (pages 232-240)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "Segment 32 (pages 270-277)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 202-210)",
      "start_page": 202,
      "end_page": 210,
      "detection_method": "topic_boundary",
      "content": "Working with LLMs via the\nHugging Face Hub\nNow that we are familiar with LangChain components, it is time to start\nusing our LLMs. If you want to use open-source LLMs, leveraging the\nHugging Face Hub integration is extremely versatile. In fact, with just one\naccess token you can leverage all the open-source LLMs available in\nHugging Face’s repositories.\nAs it is a non-production scenario, I will be using the free Inference API;\nhowever, if you are meant to build production-ready applications, you can\neasily scale to the Inference Endpoint, which grants you a dedicated and\nfully managed infrastructure to host and consume your LLMs.\nSo, let’s see how to start integrating LangChain with the Hugging Face Hub.\nCreate a Hugging Face user\naccess token\nTo access the free Inference API, you will need a user access token, the\ncredential that allows you to run the service. The following are the steps to\nactivate the user access token:\n1. Create a Hugging Face account: You can create a Hugging Face\naccount for free at https://huggingface.co/join.\n2. Retrieve your user access token: Once you have your account, go to\nthe upper-right corner of your profile and go to Settings | Access\nTokens. From that tab, you will be able to copy your secret token and\nuse it to access Hugging Face models.\n\n\nFigure 5.4: Retrieving access tokens from the Hugging Face account (source:\nhttps://huggingface.co/settings/tokens)\n3. Set permissions: Access tokens enable users, applications, and\nnotebooks to perform specific actions based on their assigned roles.\nThere are two available roles:\nRead: This allows tokens to provide read access to repositories\nyou have permission to read. This includes public and private\nrepositories owned by you or your organization. This role is\nsuitable for tasks like downloading private models or inference.\nWrite: In addition to read access, tokens with this role grant\nwrite access to repositories where you have writing privileges.\nThis token is useful for activities like training models or updating\nmodel cards.\nIn our series of use cases, we will keep a write permission on our\ntoken.\n\n\n4. Managing your user access token: Within your profile, you can create\nand manage multiple access tokens, so that you can also differentiate\npermissions. To create a new token, you can click on the New token\nbutton:\nFigure 5.5: Creating a new token\n5. Finally, at any time, you can delete or refresh your token under the\nManage button:\n\n\nFigure 5.6: Managing tokens\nIt is important not to leak your token, and a good practice is to\nperiodically regenerate it.\nStoring your secrets in an .env\nfile\nWith our user access token generated in the previous section, we have the\nfirst secret to be managed.\nDefinition\n\n\nSecrets are data that needs to be protected from unauthorized\naccess, such as passwords, tokens, keys, and credentials.\nSecrets are used to authenticate and authorize requests to API\nendpoints, as well as to encrypt and decrypt sensitive data.\nThroughout this hands-on portion of the book, we will keep all our secrets\nwithin an .env file.\nStoring Python secrets in an .env file is a common practice to enhance\nsecurity and maintainability in projects. To do this, create a file named .env\nin your project directory and list your sensitive information as key-value\npairs: in our scenario, we will have\nHUGGINGFACEHUB_API_TOKEN=\"your_user_access_token\". This file should be\nadded to your project’s .gitignore to prevent accidental exposure.\nTo access these secrets in your Python code, use the python-dotenv library\nto load the .env file’s values as environment variables. You can easily install\nit in your terminal via pip install python-dotenv.\nThis approach keeps sensitive data separate from your code base and helps\nensure that confidential information remains confidential throughout the\ndevelopment and deployment processes.\nHere, you can see an example of how to retrieve your access token and set it\nas an environmental variable:\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n\n\nNote that, by default, load_dotenv will look for the .env file in the current\nworking directory; however, you can also specify the path to your secrets\nfile:\nfrom dotenv import load_dotenv\nfrom pathlib import Path\ndotenv_path = Path('path/to/.env')\nload_dotenv(dotenv_path=dotenv_path)\nNow that we have all the ingredients to start coding, it is time to try out\nsome open-source LLMs.\nStart using open-source LLMs\nThe nice thing about the Hugging Face Hub integration is that you can\nnavigate its portal and decide, within the model catalog, what to use. Models\nare also clustered per category (Computer Vision, Natural Language\nProcessing, Audio, and so on) and, within each category, per capability\n(within Natural Language Processing, we have summarization,\nclassification, Q&A, and so on), as shown in the following screenshot:\n\n\nFigure 5.7: Home page of Hugging Face’s model catalog\nSince we are interested in LLMs, we will focus on the text generation\ncategory. For this first experiment, let’s try Falcon LLM-7B:\nfrom langchain import HuggingFaceHub\nrepo_id = \"tiiuae/falcon-7b-instruct\" \nllm = HuggingFaceHub(\n    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_leng\n)\nprint(llm(\"what was the first disney movie?\"))\nHere is the corresponding output:\nThe first Disney movie was 'Snow White and the Seven Dwarfs'\n\n\nAs you can see, with just a few lines of code, we integrated an LLM from\nthe Hugging Face Hub. With analogous code, you can test and consume all\nthe LLMs available in the Hub.\nNote that, throughout this book, we will be leveraging specific models for\neach application, both proprietary and open source. However, the idea is that\nyou can use the model you prefer by simply initializing it as the main LLM\nand running the code as it is, simply changing the LangChain LLM\nintegration. This is one of the main advantages of LLM-powered\napplications since you don’t have to change the whole code to adapt to\ndifferent LLMs.\nSummary\nIn this chapter, we dove deeper into the fundamentals of LangChain, since it\nwill be the AI orchestrator used in the upcoming chapters: we got familiar\nwith LangChain components such as memory, agents, chains, and prompt\ntemplates. We also covered how to start integrating LangChain with the\nHugging Face Hub and its model catalog, and how to use the available\nLLMs and start embedding them into your code.\nFrom now on, we will look at a series of concrete end-to-end use cases,\nstarting from a semantic Q&A search app, which we are going to develop in\nthe next chapter.\nReferences\nLangChain’s integration with OpenAI –\nhttps://python.langchain.com/docs/integrations/l\nlms/openai\n\n\nLangChain’s prompt templates –\nhttps://python.langchain.com/docs/modules/model_\nio/prompts/prompt_templates/\nLangChain’s vector stores –\nhttps://python.langchain.com/docs/integrations/v\nectorstores/\nFAISS index – https://faiss.ai/\nLangChain’s chains –\nhttps://python.langchain.com/docs/modules/chains\n/\nReAct approach – https://arxiv.org/abs/2210.03629\nLangChain’s agents –\nhttps://python.langchain.com/docs/modules/agents\n/agent_types/\nHugging Face documentation –\nhttps://huggingface.co/docs\nLangChain Expression Language (LCEL) –\nhttps://python.langchain.com/docs/expression_lan\nguage/\nLangChain stable version –\nhttps://blog.langchain.dev/langchain-v0-1-0/\nJoin our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\n",
      "page_number": 202,
      "chapter_number": 25,
      "summary": "In our series of use cases, we will keep a write permission on our\ntoken Key topics include langchain, tokens, and models.",
      "keywords": [
        "Hugging Face Hub",
        "Hugging Face",
        "Hugging Face account",
        "Face Hub",
        "access Hugging Face",
        "user access token",
        "Face",
        "access token",
        "Hugging",
        "access",
        "Hugging Face user",
        "token",
        "Hugging Face models",
        "Face Hub integration",
        "user access"
      ],
      "concepts": [
        "langchain",
        "tokens",
        "models",
        "llm",
        "managed",
        "manage",
        "llms",
        "python",
        "important",
        "integration"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 53,
          "title": "Segment 53 (pages 448-456)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 474-483)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 53,
          "title": "Segment 53 (pages 458-465)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 8,
          "title": "Segment 8 (pages 61-70)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 7,
          "title": "Segment 7 (pages 53-60)",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 211-218)",
      "start_page": 211,
      "end_page": 218,
      "detection_method": "topic_boundary",
      "content": "https://packt.link/llm\nOceanofPDF.com\n\n\n6\nBuilding Conversational\nApplications\nWith this chapter, we embark on the hands-on section of this book, with our\nfirst concrete implementation of LLM-powered applications. Throughout\nthis chapter, we will cover a step-by-step implementation of a conversational\napplication, using LangChain and its components, building on the\nknowledge you’ve gained from the previous chapters. By the end of this\nchapter, you will be able to set up your own conversational application\nproject with just a few lines of code.\nWe will cover the following key topics:\nConfiguring the schema of a simple chatbot\nAdding the memory component\nAdding non-parametric knowledge\nAdding tools and making the chatbot “agentic”\nDeveloping the front-end with Streamlit\nTechnical requirements\nTo complete the tasks in this chapter, you will need the following:\nA Hugging Face account and user access token.\n\n\nAn OpenAI account and user access token.\nPython 3.7.1 or a later version.\nPython packages – make sure to have the following Python packages\ninstalled: langchain, python-dotenv, huggingface_hub, streamlit,\nopenai, pypdf, tiktoken, faiss-cpu, and google-search-results.\nThey can be easily installed via pip install in your terminal.\nYou’ll find the code for this chapter in the book’s GitHub repository at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications.\nGetting started with\nconversational applications\nA conversational application is a type of software that can interact with users\nusing natural language. It can be used for various purposes, such as\nproviding information, assistance, entertainment, or transactions. Generally\nspeaking, a conversational application can use different modes of\ncommunication, such as text, voice, graphics, or even touch. A\nconversational application can also use different platforms, such as\nmessaging apps, websites, mobile devices, or smart speakers.\nToday, conversational applications are being taken to the next level thanks to\nLLMs. Let’s look at some of the benefits that they provide:\nNot only do LLMs provide a new level of natural language interactions,\nbut they can also enable applications to perform reasoning based on the\nbest responses, given users’ preferences.\nAs we saw in previous chapters, LLMs can leverage their parametric\nknowledge, but are also enriched with non-parametric knowledge,\n\n\nthanks to embeddings and plug-ins.\nFinally, LLMs are also able to keep track of the conversation thanks to\ndifferent types of memory.\nThe following image shows what the architecture of a conversational bot\nmight look like:\nFigure 6.1: Sample architecture of a conversational bot\nThroughout this chapter, we will build from scratch a text conversational\napplication that is able to help users plan their vacations. We will call this\napp GlobeBotter. We will add incremental layers of complexity to make the\napp as enjoyable as possible for the end user.\nSo, let’s start with the basics behind a conversational app architecture.\nCreating a plain vanilla bot\nTo start with, let’s initialize our LLM and set the schema for our bot. The\nschema refers to the type of messages the bot is able to receive. In our case,\nwe will have three types of messages:\n\n\nSystem message: The instructions we give the bot so that it behaves as\na travel assistant.\nAI Message: The message generated by the LLM\nHuman Message: The user’s query\nLet’s start with a simple configuration:\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\nfrom langchain.chains import LLMChain, ConversationChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chat_models import ChatOpenAI\nchat = ChatOpenAI()\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant that help\n    HumanMessage(content=\"I'm going to Rome for 2 days, what can\nWe can then save and print the output as follows:\noutput = chat(messages)\nprint(output.content)\nHere is the output:\nIn Rome, there are many famous attractions to visit. Here's an op\nDay 1:\n 1. Start your day by visiting the Colosseum, one of the world's \n 2. Next, explore the nearby Roman Forum, an ancient Roman market\n 3. Afterward, head to the Pantheon, a well-preserved Roman templ\n4. Take a stroll through the historic district of Trastevere, kno\n5. In the evening, visit the Trevi Fountain and toss a coin to en\nDay 2:\n1. Begin your day at Vatican City, the smallest independent state\n\n\n2. Explore the Vatican Museums, home to an extensive collection o\n3. Enjoy a leisurely walk along the Tiber River and cross over to\n4. Visit the Spanish Steps, a popular meeting point with a beauti\n5. End your day by exploring the charming neighborhood of Piazza \nRemember to check the opening hours and availability of tickets f\nAs you can see, the model was pretty good at generating an itinerary in\nRome with only one piece of information from our side, the number of days.\nHowever, we might want to keep interacting with the bot, so that we can\nfurther optimize the itinerary, providing more information about our\npreferences and habits. To achieve that, we need to add memory to our bot.\nAdding memory\nAs we’re creating a conversational bot with relatively short messages, in this\nscenario, a ConversationBufferMemory could be suitable. To make the\nconfiguration easier, let’s also initialize a ConversationChain to combine the\nLLM and the memory components.\nLet’s first initialize our memory and chain (I’m keeping verbose = True so\nthat you can see the bot keeping track of previous messages):\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationChain\nmemory = ConversationBufferMemory()\nconversation = ConversationChain(\n    llm=chat, verbose=True, memory=memory\n)\nGreat, now let’s have some interactions with our bot:\nconversation.run(\"Hi there!\")\n\n\nThe following is the output:\n> Entering new ConversationChain chain...\nPrompt after formatting:\nThe following is a friendly conversation between a human and an A\nCurrent conversation:\nHuman: Hi there!\nAI:\n> Finished chain.\n'Hello! How can I assist you today?'\nNext, we provide the following input:\nconversation.run(\"what is the most iconic place in Rome?\")\nHere is the corresponding output:\n> Entering new ConversationChain chain...\nPrompt after formatting:\nThe following is a friendly conversation between a human and an A\nCurrent conversation:\nHuman: Hi there!\nAI: Hello! How can I assist you today?\nHuman: what is the most iconic place in Rome?\nAI:\n> Finished chain.\n'The most iconic place in Rome is probably the Colosseum. It is a\nAs you can see from the chain, it is keeping track of the previous\ninteractions. Let’s challenge it and ask something related to the previous\ncontext:\nconversation.run(\"What kind of other events?\")\n\n\nThe following is the output that we receive:\n> Entering new ConversationChain chain...\nPrompt after formatting:\nThe following is a friendly conversation between a human and an A\nCurrent conversation:\nHuman: Hi there!\nAI: Hello! How can I assist you today?\nHuman: what is the most iconic place in Rome?\nAI: The most iconic place in Rome is probably the Colosseum. It i\nHuman: What kind of other events?\nAI:\n> Finished chain.\n'Other events that took place at the Colosseum include mock sea b\nThe bot was able to understand that our request was related to its previous\nanswer. We can also retrieve the message history with the\nmemory.load_memory_variables() method (you can see the full output in the\nGitHub repository). Here is a snippet of the output:\n{'history': 'Human: Hi there!\\nAI: Hello! How can I assist you to\nRather than running the conversation.run method at every interaction, I’ve\ncoded a while cycle to make it interactive. The following is a snapshot of\nthe whole conversation (you can find it in the book’s GitHub repository):\nwhile True:\n    query = input('you: ')\n    if query == 'q':\n        break\n    output = conversation({\"input\": query})\n    print('User: ', query)\n    print('AI system: ', output['response'])\n",
      "page_number": 211,
      "chapter_number": 26,
      "summary": "https://packt.link/llm\nOceanofPDF.com\n 6\nBuilding Conversational\nApplications\nWith this chapter, we embark on the hands-on section of this book, with our\nfirst concrete implementation of LLM-powered applications Key topics include conversational, messaging, and messages.",
      "keywords": [
        "conversational application",
        "Conversational",
        "bot",
        "Human",
        "Rome",
        "conversational bot",
        "memory",
        "output",
        "chain",
        "place in Rome",
        "application",
        "messages",
        "ConversationChain",
        "Finished chain",
        "iconic place"
      ],
      "concepts": [
        "conversational",
        "messaging",
        "messages",
        "roman",
        "output",
        "memory",
        "user",
        "different",
        "adding",
        "python"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 15,
          "title": "Segment 15 (pages 122-129)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 25,
          "title": "Segment 25 (pages 209-219)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 219-226)",
      "start_page": 219,
      "end_page": 226,
      "detection_method": "topic_boundary",
      "content": "The following is a truncated sample from the output (you can find the whole\noutput in the book’s GitHub repository):\nUser:  hello\nAI system:  Hello! How can I assist you today?\nUser:  I'm planning a 1-day trip in Venice. What should I visit?\nAI system:  That sounds like a wonderful plan! In Venice, there a\n1. St. Mark's Square (Piazza San Marco): [...] Enjoy your trip to\nUser:  thanks! I'm planning to be around also the next day, and I\nAI system:  Certainly! If you enjoy hiking and want to explore th\n1. The Dolomites: [...]\nUser:  which one is closer to Milan?\nAI system:  If you're looking for a hiking destination closer to \nAs you can see, now the AI assistant is capable of keeping track of the whole\nconversation. In the next section, we are going to add yet another layer of\ncomplexity: an external knowledge base.\nAdding non-parametric\nknowledge\nImagine that you also want your GlobeBotter to have access to exclusive\ndocumentation about itineraries that are not part of its parametric\nknowledge.\nTo do so, we can either embed the documentation in a VectorDB or directly\nuse a retriever to do the job. In this case, we will use a vector-store-backed\nretriever using a particular chain, ConversationalRetrievalChain. This type\nof chain leverages a retriever over the provided knowledge base that has the\nchat history, which can be passed as a parameter using the desired type of\nmemory previously seen.\n\n\nWith this goal in mind, we will use a sample Italy travel guide PDF\ndownloaded from https://www.minube.net/guides/italy.\nThe following Python code shows how to initialize all the ingredients we\nneed, which are:\nDocument Loader: Since the document is in PDF format, we will use\nPyPDFLoader.\nText splitter: We will use a RecursiveCharacterTextSplitter, which\nsplits text by recursively looking at characters to find one that works.\nVector store: We will use the FAISS VectorDB.\nMemory: We will use a ConversationBufferMemory.\nLLMs: We will use the gpt-3.5-turbo model for conversations.\nEmbeddings: We will use the text-embedding-ada-002.\nLet’s take a look at the code:\nfrom langchain.llms import OpenAI\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitt\nfrom langchain.vectorstores import FAISS\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.memory import ConversationBufferMemory\ntext_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1500,\n            chunk_overlap=200\n        )\nraw_documents = PyPDFLoader('italy_travel.pdf').load()\ndocuments = text_splitter.split_documents(raw_documents)\ndb = FAISS.from_documents(documents, OpenAIEmbeddings())\nmemory = ConversationBufferMemory(\n            memory_key='chat_history',\n            return_messages=True\n\n\n        )\nllm = ChatOpenAI()\nLet’s now interact with the chain:\nqa_chain = ConversationalRetrievalChain.from_llm(llm, retriever=\nqa_chain.run({'question':'Give me some review about the Pantheon\nThe following is the output (I’m reporting a truncated version. You can see\nthe whole output in the book’s GitHub repository):\n> Entering new StuffDocumentsChain chain...\n> Entering new LLMChain chain...\nPrompt after formatting:\nSystem: Use the following pieces of context to answer the users q\nIf you don't know the answer, just say that you don't know, don't\n----------------\ncafes in the square. The most famous are the Quadri and\nFlorian.\nPiazza San Marco,\nVenice\n4\nHistorical Monuments\nPantheon\nMiskita:\n\"Angelic and non-human design,\" was how\nMichelangelo described the Pantheon 14 centuries after its\nconstruction. The highlights are the gigantic dome, the upper\neye, the sheer size of the place, and the harmony of the\nwhole building. We visited with a Roman guide which is\n...\n> Finished chain.\n'Miskita:\\n\"Angelic and non-human design,\" was how Michelangelo d\n\n\nNote that, by default, the ConversationalRetrievalChain uses a prompt\ntemplate called CONDENSE_QUESTION_PROMPT, which merges the last user’s\nquery with the chat history, so that it results as just one query to the retriever.\nIf you want to pass a custom prompt, you can do so using the\ncondense_question_prompt parameter in the\nConversationalRetrievalChain.from_llm module.\nEven though the bot was able to provide an answer based on the\ndocumentation, we still have a limitation. In fact, with such a configuration,\nour GlobeBotter will only look at the provided documentation, but what if\nwe want it to also use its parametric knowledge? For example, we might\nwant the bot to be able to understand whether it could integrate with the\nprovided documentation or simply answer freely. To do so, we need to make\nour GlobeBotter agentic, meaning that we want to leverage the LLM’s\nreasoning capabilities to orchestrate and invoke the available tools without a\nfixed order, but rather following the best approach given the user’s query.\nTo do so, we will use two main components:\ncreate_retriever_tool: This method creates a custom tool that acts as\na retriever for an agent. It will need a database to retrieve from, a name,\nand a short description, so that the model can understand when to use it.\ncreate_conversational_retrieval_agent: This method initializes a\nconversational agent that is configured to work with retrievers and chat\nmodels. It will need an LLM, a list of tools (in our case, the retriever),\nand a memory key to keep track of the previous chat history.\nThe following code illustrates how to initialize the agent:\nfrom langchain.agents.agent_toolkits import create_retriever_too\ntool = create_retriever_tool(\n    db.as_retriever(),\n\n\n    \"italy_travel\",\n    \"Searches and returns documents regarding Italy.\"\n)\ntools = [tool]\nmemory = ConversationBufferMemory(\n            memory_key='chat_history',\n            return_messages=True\n        )\nfrom langchain.agents.agent_toolkits import create_conversationa\nfrom langchain.chat_models import ChatOpenAI\nllm = ChatOpenAI(temperature = 0)\nagent_executor = create_conversational_retrieval_agent(llm, tool\nGreat, now let’s see the thought process of the agent with two different\nquestions (I will report only the chain of thoughts and truncate the output,\nbut you can find the whole code in the GitHub repo):\nagent_executor({\"input\": \"Tell me something about Pantheon\"})\nHere is the output:\n> Entering new AgentExecutor chain...\nInvoking: `italy_travel` with `Pantheon`\n[Document(page_content='cafes in the square. The most famous are \n> Finished chain.\nLet’s now try with a question not related to the document:\noutput = agent_executor({\"input\": \"what can I visit in India in \nThe following is the output that we receive:\n\n\n> Entering new AgentExecutor chain...\nIn India, there are numerous incredible places to visit, each wit\n1. Delhi: Start your trip in the capital city of India, Delhi. […\n> Finished chain.\nAs you can see, when I asked the agent something about Italy, it\nimmediately invoked the provided document, while this was not done in the\nlast question.\nThe last thing we want to add to our GlobeBotter is the capability to navigate\nthe web, since, as travelers, we want to have up-to-date information about\nthe country we are traveling to. Let’s implement it with LangChain’s tools.\nAdding external tools\nThe tool we are going to add here is the Google SerpApi tool, so that our bot\nwill be able to navigate the internet.\nNote\nSerpApi is a real-time API designed to access Google search\nresults. It simplifies the process of data scraping by handling\ncomplexities such as managing proxies, solving\nCAPTCHAs, and parsing structured data from search engine\nresults pages.\nLangChain offers a pre-built tool that wraps SerpApi to\nmake it easier to integrate it within your agents. To enable\nSerpApi, you need to sign in at\n\n\nhttps://serpapi.com/users/sign_up, then go to\nthe dashboard under the tab API key.\nSince we don’t want our GlobeBotter to be focused only on the web, we will\nadd the SerpApi tool to the previous one, so that the agent will be able to\npick the most useful tool to answer the question – or use no tool if not\nnecessary.\nLet’s initialize our tools and agent (you learned about this and other\nLangChain components in Chapter 5):\nfrom langchain import SerpAPIWrapper\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\nos.environ[\"SERPAPI_API_KEY\"]\nsearch = SerpAPIWrapper()\ntools = [\n    Tool.from_function(\n        func=search.run,\n        name=\"Search\",\n        description=\"useful for when you need to answer question\n    ),\n    create_retriever_tool(\n        db.as_retriever(),\n        \"italy_travel\",\n        \"Searches and returns documents regarding Italy.\"\n    )\n    ]\nagent_executor = create_conversational_retrieval_agent(llm, tool\nGreat, now let’s test it with three different questions (here, again, the output\nhas been truncated):\n“What can I visit in India in 3 days?”\n\n\n> Entering new AgentExecutor chain...\nIndia is a vast and diverse country with numerous attr\n1. Delhi: Start your trip in the capital city of India\n> Finished chain.\nIn this case, the model doesn’t need external knowledge to answer the\nquestion, hence it is responding without invoking any tool.\n“What is the weather currently in Delhi?”\n> Entering new AgentExecutor chain...\nInvoking: `Search` with `{'query': 'current weather in\nCurrent Weather · 95°F Mostly sunny · RealFeel® 105°. \n> Finished chain.\nNote how the agent is invoking the search tool; this is due to the\nreasoning capability of the underlying gpt-3.5-turbo model, which\ncaptures the user’s intent and dynamically understands which tool to\nuse to accomplish the request.\n“I’m traveling to Italy. Can you give me some suggestions for the main\nattractions to visit?”\n> Entering new AgentExecutor chain...\nInvoking: `italy_travel` with `{'query': 'main attract\n[Document(page_content='ITALY\\nMINUBE TRAVEL GUIDE\\nTh\nHere are some suggestions for main attractions in Ital\n1. Parco Sempione, Milan: This is one of the most impo\n> Finished chain.\nNote how the agent is invoking the document retriever to provide the\npreceding output.\n",
      "page_number": 219,
      "chapter_number": 27,
      "summary": "In this case, we will use a vector-store-backed\nretriever using a particular chain, ConversationalRetrievalChain Key topics include chain, tools, and documentation.",
      "keywords": [
        "chain",
        "tool",
        "Finished chain",
        "agent",
        "retriever",
        "Italy",
        "Entering new AgentExecutor",
        "Entering",
        "documents",
        "output",
        "llm",
        "Document",
        "Finished",
        "question",
        "AgentExecutor chain"
      ],
      "concepts": [
        "chain",
        "tools",
        "documentation",
        "document",
        "user",
        "searches",
        "search",
        "entering",
        "serpapi",
        "memory"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents In Action",
          "chapter": 25,
          "title": "Segment 25 (pages 216-223)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 23,
          "title": "Segment 23 (pages 190-197)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 20,
          "title": "Segment 20 (pages 165-173)",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 27,
          "title": "Segment 27 (pages 230-240)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 55,
          "title": "Segment 55 (pages 465-472)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 227-234)",
      "start_page": 227,
      "end_page": 234,
      "detection_method": "topic_boundary",
      "content": "Overall, our GlobeBotter is now able to provide up-to-date information, as\nwell as retrieving specific knowledge from curated documentation. The next\nstep will be that of building a front-end. We will do so by building a web app\nusing Streamlit.\nDeveloping the front-end\nwith Streamlit\nStreamlit is a Python library that allows you to create and share web apps. It\nis designed to be easy and fast to use, without requiring any front-end\nexperience or knowledge. You can write your app in pure Python, using\nsimple commands to add widgets, charts, tables, and other elements.\nIn addition to its native capabilities, in July 2023, Streamlit announced an\ninitial integration and its future plans with LangChain. At the core of this\ninitial integration, there is the ambition of making it easier to build a GUI for\nconversational applications, as well as showing all the steps LangChain’s\nagents take before producing the final response.\nTo achieve this goal, the main module that Streamlit introduced is the\nStreamlit callback handler. This module provides a class called\nStreamlitCallbackHandler that implements the BaseCallbackHandler\ninterface from LangChain. This class can handle various events that occur\nduring the execution of a LangChain pipeline, such as tool start, tool end,\ntool error, LLM token, agent action, agent finish, etc.\nThe class can also create and update Streamlit elements, such as containers,\nexpanders, text, progress bars, etc., to display the output of the pipeline in a\nuser-friendly way. You can use the Streamlit callback handler to create\nStreamlit apps that showcase the capabilities of LangChain and interact with\n\n\nthe user through natural language. For example, you can create an app that\ntakes a user prompt and runs it through an agent that uses different tools and\nmodels to generate a response. You can use the Streamlit callback handler to\nshow the agent’s thought process and the results of each tool in real time.\nTo start building your application, you need to create a .py file to run in\nyour terminal via streamlit run file.py. In our case, the file will be\nnamed globebotter.py.\nThe following are the main building blocks of the application:\n1. Setting the configuration of the webpage:\nimport streamlit as st\nst.set_page_config(page_title=\"GlobeBotter\", page_icon\nst.header('\n Welcome to Globebotter, your travel assi\n2. Initializing the LangChain backbone components we need. The code is\nthe same as the one in the previous section, so I will share here only the\ninitialization code, without all the preliminary steps:\nsearch = SerpAPIWrapper()\ntext_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1500,\n            chunk_overlap=200\n        )\nraw_documents = PyPDFLoader('italy_travel.pdf').load()\ndocuments = text_splitter.split_documents(raw_document\ndb = FAISS.from_documents(documents, OpenAIEmbeddings(\nmemory = ConversationBufferMemory(\n    return_messages=True,\n    memory_key=\"chat_history\",\n    output_key=\"output\"\n)\nllm = ChatOpenAI()\n\n\ntools = [\n    Tool.from_function(\n        func=search.run,\n        name=\"Search\",\n        description=\"useful for when you need to answe\n    ),\n    create_retriever_tool(\n        db.as_retriever(),\n        \"italy_travel\",\n        \"Searches and returns documents regarding Ital\n    )\n    ]\nagent = create_conversational_retrieval_agent(llm, too\n3. Setting the input box for the user with a placeholder question:\nuser_query = st.text_input(\n    \"**Where are you planning your next vacation?**\",\n    placeholder=\"Ask me anything!\"\n)\n4. Setting Streamlit’s session states. Session state is a way to share\nvariables between reruns, for each user session. In addition to the\nability to store and persist state, Streamlit also exposes the ability to\nmanipulate state using callbacks. Session state also persists across apps\ninside a multipage app. You can use the session state API to initialize,\nread, update, and delete variables in the session state. In the case of our\nGlobeBotter, we want two main states: messages and memory:\nif \"messages\" not in st.session_state:\n    st.session_state[\"messages\"] = [{\"role\": \"assistan\nif \"memory\" not in st.session_state:\n    st.session_state['memory'] = memory\n\n\n5. Making sure to display the whole conversation. To do so, I created a for\nloop that iterates over the list of messages stored in\nst.session_state[\"messages\"]. For each message, it creates a\nStreamlit element called st.chat_message that displays a chat message\nin a nice format:\nfor msg in st.session_state[\"messages\"]:\n    st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n6. Configuring the AI assistant to respond when given a user’s query. In\nthis first example, we will keep the whole chain visible and printed to\nthe screen:\nif user_query:\n    st.session_state.messages.append({\"role\": \"user\", \n    st.chat_message(\"user\").write(user_query)\n    with st.chat_message(\"assistant\"):\n        st_cb = StreamlitCallbackHandler(st.container(\n        response = agent(user_query, callbacks=[st_cb]\n        st.session_state.messages.append({\"role\": \"ass\n        st.write(response)\n7. Finally, adding a button to clear the history of the conversation and start\nfrom scratch:\nif st.sidebar.button(\"Reset chat history\"):\n    st.session_state.messages = []\nThe final product looks as follows:\n\n\nFigure 6.2: Front-end of GlobeBotter with Streamlit\nFrom the expander, we can see that the agent used the Search tool (provided\nwith the SerpApi). We can also expand chat_history or\nintermediate_steps as follows:\n\n\nFigure 6.3: Example of Streamlit expander\nOf course, we can also decide to only show the output rather than the whole\nchain of thoughts, by specifying in the code to return only\nresponse['output']. You can see the whole code in the book’s GitHub\nrepository.\nBefore we wrap up, let’s discuss how you can give your users a streaming\nexperience while interacting with your chatbot. You can leverage the\nBaseCallbackHandler class to create a custom callback handler in your\nStreamlit app:\nfrom langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.schema import ChatMessage\n\n\nfrom langchain_openai import ChatOpenAI\nimport streamlit as st\nclass StreamHandler(BaseCallbackHandler):\n    def __init__(self, container, initial_text=\"\"):\n        self.container = container\n        self.text = initial_text\n    def on_llm_new_token(self, token: str, **kwargs) -> None:\n        self.text += token\n        self.container.markdown(self.text)\nThe StreamHandler is designed to capture and display streaming data, such\nas text or other content, in a designated container. Then, you can use it as\nfollows in your Streamlit app, making sure to set streaming=True while\ninitializing your OpenAI LLM.\n with st.chat_message(\"assistant\"):\n        stream_handler = StreamHandler(st.empty())\n        llm = ChatOpenAI(streaming=True, callbacks=[stream_handl\n        response = llm.invoke(st.session_state.messages)\n        st.session_state.messages.append(ChatMessage(role=\"assis\nYou can refer to the original code on LangChain’s GitHub repo at\nhttps://github.com/langchain-ai/streamlit-\nagent/blob/main/streamlit_agent/basic_streaming.py.\nSummary\nIn this chapter, we approached the end-to-end implementation of a\nconversational application, leveraging LangChain’s modules and\nprogressively adding layers of complexity. We started with a plain vanilla\nchatbot with no memory, then moved on to more complex systems with the\nability to keep traces of past interactions. We’ve also seen how to add non-\n\n\nparametric knowledge to our application with external tools, making it more\n“agentic” so that it is able to determine which tool to use, depending on the\nuser’s query. Finally, we introduced Streamlit as the front-end framework to\nbuild the web app for our GlobeBotter.\nIn the next chapter, we will focus on a more specific domain where LLMs\nadd value and demonstrate emerging behaviors, that is, recommendation\nsystems.\nReferences\nExample of a context-aware chatbot.\nhttps://github.com/shashankdeshpande/langchain-\nchatbot/blob/master/pages/2_%E2%AD%90_context_aw\nare_chatbot.py\nKnowledge base for the AI travel assistant.\nhttps://www.minube.net/guides/italy\nLangChain repository. https://github.com/langchain-ai\nJoin our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\n",
      "page_number": 227,
      "chapter_number": 28,
      "summary": "We will do so by building a web app\nusing Streamlit Key topics include langchain, tool, and callback. Developing the front-end\nwith Streamlit\nStreamlit is a Python library that allows you to create and share web apps.",
      "keywords": [
        "Streamlit",
        "Streamlit callback handler",
        "st.session",
        "state",
        "Streamlit callback",
        "user",
        "LangChain",
        "Streamlit app",
        "agent",
        "create",
        "app",
        "Session state",
        "messages",
        "import streamlit",
        "tool"
      ],
      "concepts": [
        "langchain",
        "tool",
        "callback",
        "agents",
        "llm",
        "role",
        "states",
        "search",
        "searches",
        "streaming"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 24,
          "title": "Segment 24 (pages 198-205)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 17,
          "title": "Segment 17 (pages 132-142)",
          "relevance_score": 0.47,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 22,
          "title": "Internet Application Programming",
          "relevance_score": 0.45,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 6,
          "title": "Segment 6 (pages 41-48)",
          "relevance_score": 0.45,
          "method": "sentence_transformers"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 4,
          "title": "Segment 4 (pages 25-32)",
          "relevance_score": 0.44,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 235-246)",
      "start_page": 235,
      "end_page": 246,
      "detection_method": "topic_boundary",
      "content": "OceanofPDF.com\n\n\n7\nSearch and\nRecommendation Engines\nwith LLMs\nIn the previous chapter, we covered the core steps involved in building\nconversational applications. We started with a plain vanilla chatbot, then\nadded more complex components, such as memory, non-parametric\nknowledge, and external tools. All of this was made straightforward with the\npre-built components of LangChain, as well as Streamlit for UI rendering.\nEven though conversational applications are often seen as the “comfort\nzone” of generative AI and LLMs, those models do embrace a wider\nspectrum of applications.\nIn this chapter, we are going to cover how LLMs can enhance\nrecommendation systems, using both embeddings and generative models.\nWe will learn how to create our own recommendation system application\nleveraging state-of-the-art LLMs using LangChain as the framework.\nThroughout this chapter, we will cover the following topics:\nDefinition and evolutions of recommendation systems\nHow LLMs are impacting this field of research\nBuilding recommendation systems with LangChain\n\n\nTechnical requirements\nTo complete the tasks in this book, you will need the following:\nHugging Face account and a user access token.\nOpenAI account and a user access token.\nPython version 3.7.1 or later.\nMake sure to have the following Python packages installed: langchain,\npython-dotenv, huggingface_hub, streamlit, lancedb, openai, and\ntiktoken. These can be easily installed via pip install in your\nterminal.\nYou’ll find the code for this chapter in the book’s GitHub repository at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications.\nIntroduction to\nrecommendation systems\nA recommendation system is a computer program that recommends items for\nusers of digital platforms such as e-commerce websites and social networks.\nIt uses large datasets to develop models of users’ likes and interests, and then\nrecommends similar items to individual users.\nThere are different types of recommendation systems, depending on the\nmethods and data they use. Some of the common types are:\nCollaborative filtering: This type of recommendation system uses the\nratings or feedback of other users who have similar preferences to the\ntarget user. It assumes that users who liked certain items in the past will\nlike similar items in the future. For example, if user A and user B both\n\n\nliked movies X and Y, then the algorithm may recommend movie Z to\nuser A if user B also liked it.\nCollaborative filtering can be further divided into two subtypes: user-\nbased and item-based:\nUser-based collaborative filtering finds similar users to the\ntarget user and recommends items that they liked.\nItem-based collaborative filtering finds similar items to the\nones that the target user liked and recommends them.\nContent-based filtering: This type of recommendation system uses the\nfeatures or attributes of the items themselves to recommend items that\nare similar to the ones that the target user has liked or interacted with\nbefore. It assumes that users who liked certain features of an item will\nlike other items with similar features. The main difference with item-\nbased collaborative filtering is that, while this latter item-based uses\npatterns of user behavior to make recommendations, content-based\nfiltering uses information about the items themselves. For example, if\nuser A liked movie X, which is a comedy with actor Y, then the\nalgorithm may recommend movie Z, which is also a comedy with actor\nY.\nHybrid filtering: This type of recommendation system combines both\ncollaborative and content-based filtering methods to overcome some of\ntheir limitations and provide more accurate and diverse\nrecommendations. For example, YouTube uses hybrid filtering to\nrecommend videos based on both the ratings and views of other users\nwho have watched similar videos, and the features and categories of the\nvideos themselves.\n\n\nKnowledge-based filtering: This type of recommendation system uses\nexplicit knowledge or rules about the domain and the user’s needs or\npreferences to recommend items that satisfy certain criteria or\nconstraints. It does not rely on ratings or feedback from other users, but\nrather on the user’s input or query. For example, if user A wants to buy\na laptop with certain specifications and budget, then the algorithm may\nrecommend a laptop that satisfies those criteria. Knowledge-based\nrecommender systems work well when there is no or little rating history\navailable, or when the items are complex and customizable.\nWithin the above frameworks, there are then various machine learning\ntechniques that can be used, which we will cover in the next section.\nExisting recommendation\nsystems\nModern recommendation systems use machine learning (ML) techniques to\nmake better predictions about users’ preferences, based on the available data\nsuch as the following:\nUser behavior data: Insights about user interaction with a product.\nThis data can be acquired from factors like user ratings, clicks, and\npurchase records.\nUser demographic data: This refers to personal information about\nusers, including details like age, educational background, income level,\nand geographical location.\nProduct attribute data: This involves information about the\ncharacteristics of a product, such as genres of books, casts of movies, or\nspecific cuisines in the context of food.\n\n\nAs of today, some of the most popular ML techniques are K-nearest\nneighbors, dimensionality reduction, and neural networks. Let’s look at these\nmethods in detail.\nK-nearest neighbors\nK-nearest neighbors (KNN) is an ML algorithm that can be used for both\nclassification and regression problems. It works by finding the k closest data\npoints (where k refers to the number of nearest data point you want to find,\nand is set by the user before initializing the algorithm) to a new data point\nand using their labels or values to make a prediction. KNN is based on the\nassumption that similar data points are likely to have similar labels or values.\nKNN can be applied to recommendation systems in the context of\ncollaborative filtering, both user-based and item-based:\nUser-based KNN is a type of collaborative filtering, which uses the\nratings or feedback of other users who have similar tastes or\npreferences to the target user.\nFor example, let’s say we have three users: Alice, Bob, and Charlie.\nThey all buy books online and rate them. Alice and Bob both liked\n(rated highly) the series, Harry Potter, and the book, The Hobbit. The\nsystem sees this pattern and considers Alice and Bob to be similar.\nNow, if Bob also liked the book A Game of Thrones, which Alice\nhasn’t read yet, the system will recommend A Game of Thrones to\nAlice. This is because it assumes that since Alice and Bob have similar\ntastes, Alice might also like A Game of Thrones.\nItem-based KNN is another type of collaborative filtering, which uses\nthe attributes or features of the items to recommend similar items to the\n\n\ntarget user.\nFor example, let’s consider the same users and their ratings for the\nbooks. The system notices that the Harry Potter series and the book,\nThe Hobbit are both liked by Alice and Bob. So, it considers these two\nbooks to be similar.\nNow, if Charlie reads and likes Harry Potter, the system will\nrecommend The Hobbit to Charlie. This is because it assumes that\nsince Harry Potter and The Hobbit are similar (both liked by the same\nusers), Charlie might also like The Hobbit.\nKNN is a popular technique in recommendation systems, but it has some\npitfalls:\nScalability: KNN can become computationally expensive and slow\nwhen dealing with large datasets, as it requires calculating distances\nbetween all pairs of items or users.\nCold-start problem: KNN struggles with new items or users that have\nlimited or no interaction history, as it relies on finding neighbors based\non historical data.\nData sparsity: KNN performance can degrade in sparse datasets where\nthere are many missing values, making it challenging to find\nmeaningful neighbors.\nFeature relevance: KNN treats all features equally and assumes that all\nfeatures contribute equally to similarity calculations. This may not hold\ntrue in scenarios where some features are more relevant than others.\nChoice of K: Selecting the appropriate value of K (number of\nneighbors) can be subjective and impact the quality of\nrecommendations. A small K may result in noise, while a large K may\nlead to overly broad recommendations.\n\n\nGenerally speaking, KNN is recommended in scenarios with small datasets\nwith minimal noise (so that outliers, missing values and other noises do not\nimpact the distance metric) and dynamic data (KNN is an instance-based\nmethod that doesn’t require retraining and can adapt to changes quickly).\nAdditionally, further techniques are widely used in the file of\nrecommendation systems, such as matrix factorization.\nMatrix factorization\nMatrix factorization is a technique used in recommendation systems to\nanalyze and predict user preferences or behaviors based on historical data. It\ninvolves decomposing a large matrix into two or more smaller matrices to\nuncover latent features that contribute to the observed data patterns and\naddress the so-called “curse of dimensionality.”\nDefinition\nThe curse of dimensionality refers to challenges that arise\nwhen dealing with high-dimensional data. It leads to\nincreased complexity, sparse data, and difficulties in analysis\nand modeling due to the exponential growth of data\nrequirements and potential overfitting.\nIn the context of recommendation systems, this technique is employed to\npredict missing values in the user-item interaction matrix, which represents\nusers’ interactions with various items (such as movies, products, or books).\nLet’s consider the following example. Imagine you have a matrix where\nrows represent users, columns represent movies, and the cells contain ratings\n\n\n(from 1 as lowest to 5 as highest). However, not all users have rated all\nmovies, resulting in a matrix with many missing entries:\nMovie 1\nMovie 2\nMovie 3\nMovie 4\nUser 1\n4\n-\n5\n-\nUser 2\n-\n3\n-\n2\nUser 3\n5\n4\n-\n3\nTable 7.1: Example of a dataset with missing data\nMatrix factorization aims to break down this matrix into two matrices: one\nfor users and another for movies, with a reduced number of dimensions\n(latent factors). These latent factors could represent attributes like genre\npreferences or specific movie characteristics. By multiplying these matrices,\nyou can predict the missing ratings and recommend movies that the users\nmight enjoy.\nThere are different algorithms for matrix factorization, including the\nfollowing:\nSingular value decomposition (SVD) decomposes a matrix into three\nseparate matrices, where the middle matrix contains singular values that\nrepresent the importance of different components in the data. It’s\nwidely used in data compression, dimensionality reduction, and\ncollaborative filtering in recommendation systems.\nPrincipal component analysis (PCA) is a technique to reduce the\ndimensionality of data by transforming it into a new coordinate system\n\n\naligned with the principal components. These components capture the\nmost significant variability in the data, allowing efficient analysis and\nvisualization.\nNon-negative matrix factorization (NMF) decomposes a matrix into\ntwo matrices with non-negative values. It’s often used for topic\nmodeling, image processing, and feature extraction, where the\ncomponents represent non-negative attributes.\nIn the context of recommendation systems, probably the most popular\ntechnique is SVD (thanks to its interpretability, flexibility, and ability to\nhandle missing values and performance), so let’s use this one to go on with\nour example. We will use the Python numpy module to apply SVD as\nfollows:\nimport numpy as np\n# Your user-movie rating matrix (replace with your actual data)\nuser_movie_matrix = np.array([\n    [4, 0, 5, 0],\n    [0, 3, 0, 2],\n    [5, 4, 0, 3]\n])\n# Apply SVD\nU, s, V = np.linalg.svd(user_movie_matrix, full_matrices=False)\n# Number of latent factors (you can choose this based on your pr\nnum_latent_factors = 2\n# Reconstruct the original matrix using the selected latent fact\nreconstructed_matrix = U[:, :num_latent_factors] @ np.diag(s[:nu\n# Replace negative values with 0\nreconstructed_matrix = np.maximum(reconstructed_matrix, 0)\nprint(\"Reconstructed Matrix:\")\nprint(reconstructed_matrix)\nThe following is the output:\n\n\nReconstructed Matrix:\n[[4.2972542  0.         4.71897811 0.        ]\n [1.08572801 2.27604748 0.         1.64449028]\n [4.44777253 4.36821972 0.52207171 3.18082082]]\nIn this example, the U matrix contains user-related information, the s matrix\ncontains singular values, and the V matrix contains movie-related\ninformation. By selecting a certain number of latent factors\n(num_latent_factors), you can reconstruct the original matrix with reduced\ndimensions, while setting the full_matrices=False parameter in the\nnp.linalg.svd function ensures that the decomposed matrices are truncated\nto have dimensions consistent with the selected number of latent factors.\nThese predicted ratings can then be used to recommend movies with higher\npredicted ratings to users. Matrix factorization enables recommendation\nsystems to uncover hidden patterns in user preferences and make\npersonalized recommendations based on those patterns.\nMatrix factorization has been a widely used technique in recommendation\nsystems, especially when dealing with large datasets containing a substantial\nnumber of users and items, since it efficiently captures latent factors even in\nsuch scenarios; or when you want personalized recommendations based on\nlatent factors, since it learns unique latent representations for each user and\nitem. However, it has some pitfalls (some similar to the KNN’s technique):\nCold-start problem: Similar to KNN, matrix factorization struggles\nwith new items or users that have limited or no interaction history.\nSince it relies on historical data, it can’t effectively provide\nrecommendations for new items or users.\nData sparsity: As the number of users and items grows, the user-item\ninteraction matrix becomes increasingly sparse, leading to challenges in\n\n\naccurately predicting missing values.\nScalability: For large datasets, performing matrix factorization can be\ncomputationally expensive and time-consuming.\nLimited context: Matrix factorization typically only considers user-\nitem interactions, ignoring contextual information like time, location, or\nadditional user attributes.\nHence, neural networks (NNs) have been explored as an alternative to\nmitigate these pitfalls in recent years.\nNeural networks\nNNs are used in recommendation systems to improve the accuracy and\npersonalization of recommendations by learning intricate patterns from data.\nHere’s how neural networks are commonly applied in this context:\nCollaborative filtering with neural networks: Neural networks can\nmodel user-item interactions by embedding users and items into\ncontinuous vector spaces. These embeddings capture latent features that\nrepresent user preferences and item characteristics. Neural collaborative\nfiltering models combine these embeddings with neural network\narchitectures to predict ratings or interactions between users and items.\nContent-based recommendations: In content-based recommendation\nsystems, neural networks can learn representations of item content,\nsuch as text, images, or audio. These representations capture item\ncharacteristics and user preferences. Neural networks like\nconvolutional neural networks (CNNs) and recurrent neural\nnetworks (RNNs) are used to process and learn from item content,\nenabling personalized content-based recommendations.\n",
      "page_number": 235,
      "chapter_number": 29,
      "summary": "In this chapter, we are going to cover how LLMs can enhance\nrecommendation systems, using both embeddings and generative models Key topics include user, data, and recommendation.",
      "keywords": [
        "recommendation systems",
        "user",
        "matrix",
        "Recommendation",
        "items",
        "data",
        "KNN",
        "systems",
        "matrix factorization",
        "similar",
        "Collaborative filtering",
        "filtering",
        "target user",
        "neural networks",
        "latent"
      ],
      "concepts": [
        "user",
        "data",
        "recommendation",
        "recommendations",
        "matrix",
        "likes",
        "similar",
        "based",
        "movies",
        "item"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 11,
          "title": "Segment 11 (pages 93-100)",
          "relevance_score": 0.47,
          "method": "sentence_transformers"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 15,
          "title": "Segment 15 (pages 124-132)",
          "relevance_score": 0.46,
          "method": "sentence_transformers"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 37,
          "title": "Segment 37 (pages 317-324)",
          "relevance_score": 0.44,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.43,
          "method": "sentence_transformers"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 39,
          "title": "Segment 39 (pages 334-342)",
          "relevance_score": 0.43,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 247-255)",
      "start_page": 247,
      "end_page": 255,
      "detection_method": "topic_boundary",
      "content": "Sequential models: In scenarios where user interactions have a\ntemporal sequence, such as clickstreams or browsing history, RNNs or\nvariants such as long short-term memory (LSTM) networks can\ncapture temporal dependencies in the user behavior and make\nsequential recommendations.\nAutoencoders and variational autoencoders (VAEs) can be used to\nlearn low-dimensional representations of users and items.\nDefinition\nAutoencoders are a type of neural network architecture used\nfor unsupervised learning and dimensionality reduction.\nThey consist of an encoder and a decoder. The encoder maps\nthe input data into a lower-dimensional latent space\nrepresentation, while the decoder attempts to reconstruct the\noriginal input data from the encoded representation.\nVAEs are an extension of traditional autoencoders that\nintroduce probabilistic elements. VAEs not only learn to\nencode the input data into a latent space but also model the\ndistribution of this latent space using probabilistic methods.\nThis allows for the generation of new data samples from the\nlearned latent space. VAEs are used for generative tasks like\nimage synthesis, anomaly detection, and data imputation.\nIn both autoencoders and VAEs, the idea is to learn a compressed and\nmeaningful representation of the input data in the latent space, which can be\nuseful for various tasks including feature extraction, data generation, and\ndimensionality reduction.\n\n\nThese representations can then be used to make recommendations by\nidentifying similar users and items in the latent space. In fact, the unique\narchitecture that features NNs allows for the following techniques:\nSide information integration: NNs can incorporate additional user and\nitem attributes, such as demographic information, location, or social\nconnections, to improve recommendations by learning from diverse\ndata sources.\nDeep reinforcement learning: In certain scenarios, deep reinforcement\nlearning can be used to optimize recommendations over time, learning\nfrom user feedback to suggest actions that maximize long-term rewards.\nNNs offer flexibility and the ability to capture complex patterns in data,\nmaking them well suited for recommendation systems. However, they also\nrequire careful design, training, and tuning to achieve optimal performance.\nNNs also bring their own challenges, including the following:\nIncreased complexity: NNs, especially deep neural networks\n(DNNs), can become incredibly complex due to their layered\narchitecture. As we add more hidden layers and neurons, the model’s\ncapacity to learn intricate patterns increases.\nTraining requirements: NNs are heavy models whose training requires\nspecial hardware requirements including GPUs, which might be very\nexpensive.\nPotential overfitting: Overfitting occurs when an ANN learns to\nperform exceptionally well on the training data but fails to generalize to\nunseen data\nSelecting appropriate architectures, handling large datasets, and tuning\nhyperparameters are essential to effectively use NNs in recommendation\nsystems.\n\n\nEven though relevant advancements have been made in recent years, the\naforementioned techniques still suffer from some pitfalls, primarily their\nbeing task-specific. For example, a rating-prediction recommendation\nsystem will not be able to tackle a task where we need to recommend the top\nk items that likely match the user’s taste. Actually, if we extend this\nlimitation to other “pre-LLMs” AI solutions, we might see some similarities:\nit is indeed the task-specific situation that LLMs and, more generally, Large\nFoundation Models are revolutionizing, being highly generalized and\nadaptable to various tasks, depending on user’s prompts and instructions.\nHenceforth, extensive research in the field of recommendation systems is\nbeing done into what extent LLMs can enhance the current models. In the\nfollowing sections, we will cover the theory behind these new approaches\nreferring to recent papers and blogs about this emerging domain.\nHow LLMs are changing\nrecommendation systems\nWe saw in previous chapters how LLMs can be customized in three main\nways: pre-training, fine-tuning, and prompting. According to the paper\nRecommender systems in the Era of Large Language Models (LLMs) from\nWenqi Fan et al., these techniques can also be used to tailor an LLM to be a\nrecommender system:\nPre-training: Pre-training LLMs for recommender systems is an\nimportant step to enable LLMs to acquire extensive world knowledge\nand user preferences, and to adapt to different recommendation tasks\nwith zero or few shots.\nNote\n\n\nAn example of a recommendation system LLM is P5,\nintroduced by Shijie Gang et al. in their paper\nRecommendation as Language Processing (RLP): A\nUnified Pretrain, Personalized Prompt & Predict\nParadigm (P5).\nP5 is a unified text-to-text paradigm for building\nrecommender systems using large language models\n(LLMs). It consists of three steps:\nPretrain: A foundation language model based on\nT5 architecture is pretrained on a large-scale web\ncorpus and fine-tuned on recommendation tasks.\nPersonalized prompt: A personalized prompt is\ngenerated for each user based on their behavior\ndata and contextual features.\nPredict: The personalized prompt is fed into the\npretrained language model to generate\nrecommendations.\nP5 is based on the idea that LLMs can encode\nextensive world knowledge and user preferences and\ncan be adapted to different recommendation tasks with\nzero or few shots.\nFine-tuning: Training an LLM from scratch is a highly computational-\nintensive activity. An alternative and less intrusive approach to\ncustomize an LLM for recommendation systems might be fine-tuning.\nMore specifically, the authors of the paper review two main strategies\nfor fine-tuning LLMs:\n\n\nFull-model fine-tuning involves changing the entire model’s\nweights based on task-specific recommendation datasets.\nParameter-efficient fine-tuning aims to change only a small\npart of weights or develop trainable adapters to fit specific tasks.\nPrompting: The third and “lightest” way of tailoring LLMs to be\nrecommender systems is prompting. According to the authors, there are\nthree main techniques for prompting LLMs:\nConventional prompting aims to unify downstream tasks into\nlanguage generation tasks by designing text templates or\nproviding a few input-output examples.\nIn-context learning enables LLMs to learn new tasks based on\ncontextual information without fine-tuning.\nChain-of-thought enhances the reasoning abilities of LLMs by\nproviding multiple demonstrations to describe the chain of\nthought as examples within the prompt. The authors also discuss\nthe advantages and challenges of each technique and provide\nsome examples of existing methods that adopt them.\nRegardless of the typology, prompting is the fastest way to test\nwhether a general-purpose LLM can tackle recommendation systems’\ntasks.\nThe application of LLMs within the recommendation system domain is\nraising interest in the research field, and there is already some interesting\nevidence of the results as seen above.\nIn the next section, we are going to implement our own recommendation\napplication using the prompting approach and leveraging the capabilities of\nLangChain as an AI orchestrator.\n\n\nImplementing an LLM-\npowered recommendation\nsystem\nNow that we have covered some theory about recommendation systems and\nemerging research on how LLMs can enhance them, let’s start building our\nrecommendation app, which will be a movie recommender system called\nMovieHarbor. The goal will be to make it as general as possible, meaning\nthat we want our app to be able to address various recommendations tasks\nwith a conversational interface. The scenario we are going to simulate will\nbe that of the so-called “cold start,” concerning the first interaction of a user\nwith the recommendation system where we do not have the user’s preference\nhistory. We will leverage a movie database with textual descriptions.\nFor this purpose, we will use the Movie recommendation data dataset,\navailable on Kaggle at\nhttps://www.kaggle.com/datasets/rohan4050/movie-\nrecommendation-data.\nThe reason for using a dataset with a textual description of each movie\n(alongside information such as ratings and movie titles) is so that we can get\nthe embeddings of the text. So let’s start building our MovieHarbor\napplication.\nData preprocessing\nIn order to apply LLMs to our dataset, we first need to preprocess the data.\nThe initial dataset included several columns; however, the ones we are\ninterested in are the following:\n\n\nGenres: A list of applicable genres for the movie.\nTitle: The movie’s title.\nOverview: Textual description of the plot.\nVote_average: A rating from 1 to 10 for a given movie\nVote_count: The number of votes for a given movie.\nI won’t report here the whole code (you can find it in the GitHub repo of this\nbook at https://github.com/PacktPublishing/Building-\nLLM-Powered-Applications), however, I will share the main steps of\ndata preprocessing:\n1. First, we format the genres column into a numpy array, which is easier\nto handle than the original dictionary format in the dataset:\nimport pandas as pd\nimport ast\n# Convert string representation of dictionaries to act\nmd['genres'] = md['genres'].apply(ast.literal_eval)\n# Transforming the 'genres' column\nmd['genres'] = md['genres'].apply(lambda x: [genre['na\n2. Next, we merge the vote_average and vote_count columns into a\nsingle column, which is the weighted ratings with respect to the number\nof votes. I’ve also limited the rows to the 95th percentile of the number\nof votes, so that we can get rid of minimum vote counts to prevent\nskewed results:\n# Calculate weighted rate (IMDb formula)\ndef calculate_weighted_rate(vote_average, vote_count, \n    return (vote_count / (vote_count + min_vote_count)\n# Minimum vote count to prevent skewed results\nvote_counts = md[md['vote_count'].notnull()]['vote_cou\nmin_vote_count = vote_counts.quantile(0.95)\n\n\n# Create a new column 'weighted_rate'\nmd['weighted_rate'] = md.apply(lambda row: calculate_w\n3. Next, we create a new column called combined_info where we are\ngoing to merge all the elements that will be provided as context to the\nLLMs. Those elements are the movie title, overview, genres, and\nratings:\nmd_final['combined_info'] = md_final.apply(lambda row:\n4. We tokenize the movie combined_info so that we will get better results\nwhile embedding:\nimport pandas as pd\nimport tiktoken\nimport os\nimport openai\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\nfrom openai.embeddings_utils import get_embedding\nembedding_encoding = \"cl100k_base\" # this the encoding\nmax_tokens = 8000 # the maximum for text-embedding-ada\nencoding = tiktoken.get_encoding(embedding_encoding)\n# omit reviews that are too long to embed\nmd_final[\"n_tokens\"] = md_final.combined_info.apply(la\nmd_final = md_final[md_final.n_tokens <= max_tokens]\nDefinition\ncl100k_base is the name of a tokenizer used by\nOpenAI’s embeddings API. A tokenizer is a tool that\nsplits a text string into units called tokens, which can\nthen be processed by a neural network. Different\n\n\ntokenizers have different rules and vocabularies for\nhow to split the text and what tokens to use.\nThe cl100k_base tokenizer is based on the byte pair\nencoding (BPE) algorithm, which learns a vocabulary\nof subword units from a large corpus of text. The\ncl100k_base tokenizer has a vocabulary of 100,000\ntokens, which are mostly common words and word\npieces, but also include some special tokens for\npunctuation, formatting, and control. It can handle\ntexts in multiple languages and domains, and can\nencode up to 8,191 tokens per input.\n5. We embed the text with text-embedding-ada-002:\nmd_final[\"embedding\"] = md_final.overview.apply(lambda\nAfter changing some columns’ names and dropping unnecessary\ncolumns, the final dataset looks as follows:\n",
      "page_number": 247,
      "chapter_number": 30,
      "summary": "This chapter covers segment 30 (pages 247-255). Key topics include recommendations, recommendation, and data. Autoencoders and variational autoencoders (VAEs) can be used to\nlearn low-dimensional representations of users and items.",
      "keywords": [
        "LLMs",
        "recommendation",
        "recommendation systems",
        "data",
        "Vote",
        "recommendation system LLM",
        "systems",
        "tasks",
        "user",
        "latent space",
        "movie",
        "LLM",
        "recommendation tasks",
        "input data",
        "make sequential recommendations"
      ],
      "concepts": [
        "recommendations",
        "recommendation",
        "data",
        "dataset",
        "learn",
        "tasks",
        "llms",
        "encoded",
        "encode",
        "important"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 11,
          "title": "Segment 11 (pages 93-100)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 15,
          "title": "Segment 15 (pages 124-132)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 10,
          "title": "Segment 10 (pages 78-85)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 256-264)",
      "start_page": 256,
      "end_page": 264,
      "detection_method": "topic_boundary",
      "content": "Figure 7.1: Sample of the final movies dataset\nLet’s have a look at a random row of text:\nmd['text'][0]\nThe following output is obtained:\n'Title: GoldenEye. Overview: James Bond must unmask the mys\nThe last change we will make is modifying some naming conventions\nand data types as follows:\nmd_final.rename(columns = {'embedding': 'vector'}, inplace \nmd_final.rename(columns = {'combined_info': 'text'}, inplac\nmd_final.to_pickle('movies.pkl')\n6. Now that we have our final dataset, we need to store it in a VectorDB.\nFor this purpose, we are going to leverage LanceDB, an open-source\ndatabase for vector-search built with persistent storage, which greatly\nsimplifies the retrieval, filtering, and management of embeddings and\nalso offers a native integration with LangChain. You can easily install\nLanceDB via pip install lancedb:\nimport lancedb\nuri = \"data/sample-lancedb\"\ndb = lancedb.connect(uri)\ntable = db.create_table(\"movies\", md)\n\n\nNow that we have all our ingredients, we can start working with those\nembeddings and start building our recommendation system. We will start\nwith a simple task in a cold-start scenario, adding progressive layers of\ncomplexity with LangChain components. Afterwards, we will also try a\ncontent-based scenario to challenge our LLMs with diverse tasks.\nBuilding a QA recommendation\nchatbot in a cold-start scenario\nIn previous sections, we saw how the cold-start scenario – that means\ninteracting with a user for the first time without their backstory – is a\nproblem often encountered by recommendation systems. The less\ninformation we have about a user, the harder it is to match the\nrecommendations to their preferences.\nIn this section, we are going to simulate a cold-start scenario with\nLangChain and OpenAI’s LLMs with the following high-level architecture:\nFigure 7.2: High-level architecture of recommendation system in a cold-start scenario\nIn the previous section, we’ve already saved our embeddings in LanceDB.\nNow, we are going to build a LangChain RetrievalQA retriever, a chain\n\n\ncomponent designed for question-answering against an index. In our case,\nwe will use the vector store as our index retriever. The idea is that the chain\nreturns the top k most similar movies upon the user’s query, using cosine\nsimilarity as the distance metric (which is the default).\nSo, let’s start building the chain:\n1. We are using only the movie overview as information input:\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import LanceDB\nos.environ[\"OPENAI_API_KEY\"]\nembeddings = OpenAIEmbeddings()\ndocsearch = LanceDB(connection = table, embedding = em\nquery = \"I'm looking for an animated action movie. Wha\ndocs = docsearch.similarity_search(query)\ndocs\nThe following is the corresponding output (I will display a truncated\nversion of the output, showing only the first out of four document\nsources):\n[Document(page_content='Title: Hitman: Agent 47. Overview: \nAs you can see, alongside each Document, all variables are reported as\nmetadata, plus the distance is also reported as a score. The lower the\ndistance, the greater the proximity between the user’s query and the\nmovie’s text embedding.\n2. Once we have gathered the most similar documents, we want a\nconversational response. For this goal, in addition to the embedding\n\n\nmodels, we will also use OpenAI’s completion model GPT-3 and\ncombine it in RetrievalQA:\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_t\nquery = \"I'm looking for an animated action movie. Wha\nresult = qa({\"query\": query})\nresult['result']\nLet’s look at the output:\n' I would suggest Transformers. It is an animated action mo\n3. Since we set the return_source_documents=True parameter, we can\nalso retrieve the document sources:\nresult['source_documents'][0]\nThe following is the output:\nDocument(page_content='Title: Hitman: Agent 47. Overview: A\n       -0.01303058, -0.00709073], dtype=float32), '_distanc\nNote that the first document reported is not the one the model\nsuggested. This occurred probably because of the rating, which is\nlower than Transformers (which was only the third result). This is a\ngreat example of how the LLM was able to consider multiple factors,\non top of similarity, to suggest a movie to the user.\n4. The model was able to generate a conversational answer, however, it is\nstill using only a part of the available information – the textual\n\n\noverview. What if we want our MovieHarbor system to also leverage\nthe other variables? We can approach the task in two ways:\nThe “filter” way: This approach consists of adding some filters\nas kwargs to our retriever, which might be required by the\napplication before responding to the user. Those questions might\nbe, for example, about the genre of a movie.\nFor example, let’s say we want to provide results featuring only those\nmovies for which the genre is tagged as comedy. You can achieve this\nwith the following code:\ndf_filtered = md[md['genres'].apply(lambda x: 'Comedy' in x)\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"s\n    retriever=docsearch.as_retriever(search_kwargs={'data': \nquery = \"I'm looking for a movie with animals and an adventu\nresult = qa({\"query\": query})\nThe filter can also operate at the metadata level, as shown in the\nfollowing example, where we want to filter only results with a rating\nabove 7:\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"s\n    retriever=docsearch.as_retriever(search_kwargs={'filter\nThe “agentic” way: This is probably the most innovative way to\napproach the problem. Making our chain agentic means\nconverting the retriever to a tool that the agent can leverage if\nneeded, including the additional variables. By doing so, it would\nbe sufficient for the user to provide their preferences in natural\n\n\nlanguage so that the agent can retrieve the most promising\nrecommendation if needed.\nLet’s see how to implement this with code, asking specifically for an\naction movie (thus filtering on the genre variable):\nfrom langchain.agents.agent_toolkits import create_retriever\nfrom langchain.agents.agent_toolkits import create_conversat\nfrom langchain.chat_models import ChatOpenAI\nllm = ChatOpenAI(temperature = 0)\nretriever = docsearch.as_retriever(return_source_documents =\ntool = create_retriever_tool(\n    retriever,\n    \"movies\",\n    \"Searches and returns recommendations about movies.\"\n)\ntools = [tool]\nagent_executor = create_conversational_retrieval_agent(llm, \nresult = agent_executor({\"input\": \"suggest me some action mo\nLet’s see a glimpse of the chain of thoughts and the output produced\n(always based on the four most similar movies according to cosine\nsimilarity):\n> Entering new AgentExecutor chain...\nInvoking: `movies` with `{'genre': 'action'}`\n[Document(page_content='The action continues from [REC], […\nHere are some action movies that you might enjoy:\n1. [REC]² - The action continues from [REC], with a medical\n2. The Boondock Saints - Twin brothers Conner and Murphy ta\n3. The Gamers - Four clueless players are sent on a quest t\n4. Atlas Shrugged Part III: Who is John Galt? - In a collap\nPlease note that these recommendations are based on the gen\n> Finished chain.\n\n\n5. Finally, we might also want to make our application more tailored\ntoward its goal of being a recommender system. To do so, we need to\ndo some prompt engineering.\nNote\nOne of the advantages of using LangChain’s pre-built\ncomponents, such as the RetrievalQA chain, is that\nthey come with a pre-configured, well-curated prompt\ntemplate. Before overriding the existing prompt, it’s a\ngood practice to inspect it, so that you can also see\nwhich variables (within {}) are already expected from\nthe component.\nTo explore the existing prompt, you can run the following code:\nprint(qa.combine_documents_chain.llm_chain.prompt.template\nHere is the output:\nUse the following pieces of context to answer the question \n{context}\nQuestion: {question}\nHelpful Answer:\nLet’s say, for example, that we want our system to return three\nsuggestions for each user’s request, with a short description of the plot\nand the reason why the user might like it. The following is a sample\nprompt that could match this goal:\n\n\nfrom langchain.prompts import PromptTemplate\ntemplate = \"\"\"You are a movie recommender system that help \nUse the following pieces of context to answer the question \nFor each question, suggest three movies, with a short desc\nIf you don't know the answer, just say that you don't know\n{context}\nQuestion: {question}\nYour response:\"\"\"\n \nPROMPT = PromptTemplate(\n    template=template, input_variables=[\"context\", \"questio\n6. Now we need to pass it into our chain:\nPROMPT = PromptTemplate(\n    template=template, input_variables=[\"context\", \"qu\nchain_type_kwargs = {\"prompt\": PROMPT}\nqa = RetrievalQA.from_chain_type(llm=OpenAI(),\n    chain_type=\"stuff\",\n    retriever=docsearch.as_retriever(),\n    return_source_documents=True,\n    chain_type_kwargs=chain_type_kwargs)\nquery = \"I'm looking for a funny action movie, any sug\nresult = qa({'query':query})\nprint(result['result'])\nThe following output is obtained:\n1. A Good Day to Die Hard: An action-packed comedy directed\n2. The Hidden: An alien is on the run in America and uses t\n3. District B13: Set in the ghettos of Paris in 2010, this \n7. Another thing that we might want to implement in our prompt is the\ninformation gathered with the conversational preliminary questions that\n\n\nwe might want to set as a welcome page. For example, before letting\nthe user input their natural language question, we might want to ask\ntheir age, gender, and favorite movie genre. To do so, we can insert in\nour prompt a section where we can format the input variables with\nthose shared by the user, and then combine this prompt chunk in the\nfinal prompt we are going to pass to the chain. Below you can find an\nexample (for simplicity, we are going to set the variables without asking\nthe user):\nfrom langchain.prompts import PromptTemplate\ntemplate_prefix = \"\"\"You are a movie recommender syste\nUse the following pieces of context to answer the ques\nIf you don't know the answer, just say that you don't \n{context}\"\"\"\nuser_info = \"\"\"This is what we know about the user, an\nAge: {age}\nGender: {gender}\"\"\"\ntemplate_suffix= \"\"\"Question: {question}\nYour response:\"\"\"\nuser_info = user_info.format(age = 18, gender = 'femal\nCOMBINED_PROMPT = template_prefix +'\\n'+ user_info +'\\\nprint(COMBINED_PROMPT)\nHere is the output:\nYou are a movie recommender system that help users to find \nUse the following pieces of context to answer the question \nIf you don't know the answer, just say that you don't know\n{context}\nThis is what we know about the user, and you can use this i\nAge: 18\nGender: female\nQuestion: {question}\nYour response:\n",
      "page_number": 256,
      "chapter_number": 31,
      "summary": "Overview: James Bond must unmask the mys\nThe last change we will make is modifying some naming conventions\nand data types as follows:\nmd_final.rename(columns = {'embedding': 'vector'}, inplace \nmd_final.rename(columns = {'combined_info': 'text'}, inplac\nmd_final.to_pickle('movies.pkl')\n6 Key topics include movies, prompt, and result.",
      "keywords": [
        "chain",
        "user",
        "prompt",
        "movie",
        "retriever",
        "question",
        "query",
        "action movie",
        "action",
        "context",
        "result",
        "type",
        "document",
        "answer",
        "output"
      ],
      "concepts": [
        "movies",
        "prompt",
        "result",
        "chain",
        "embedding",
        "retrieval",
        "retriever",
        "retrieve",
        "action",
        "lancedb"
      ],
      "similar_chapters": [
        {
          "book": "Effective-Python",
          "chapter": 4,
          "title": "Segment 4 (pages 27-34)",
          "relevance_score": 0.45,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 33,
          "title": "Segment 33 (pages 282-289)",
          "relevance_score": 0.41,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 35,
          "title": "Segment 35 (pages 301-308)",
          "relevance_score": 0.4,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 54,
          "title": "Segment 54 (pages 457-464)",
          "relevance_score": 0.4,
          "method": "sentence_transformers"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 19,
          "title": "Segment 19 (pages 172-179)",
          "relevance_score": 0.4,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 265-272)",
      "start_page": 265,
      "end_page": 272,
      "detection_method": "topic_boundary",
      "content": "8. Now let’s format the prompt and pass it into our chain:\nPROMPT = PromptTemplate(\n    template=COMBINED_PROMPT, input_variables=[\"contex\nchain_type_kwargs = {\"prompt\": PROMPT}\nqa = RetrievalQA.from_chain_type(llm=OpenAI(),\n    chain_type=\"stuff\",\n    retriever=docsearch.as_retriever(),\n    return_source_documents=True,\n    chain_type_kwargs=chain_type_kwargs)\nresult = qa({'query':query})\nresult['result']\nWe receive the following output:\n' Sure, I can suggest some action movies for you. Here are \nAs you can see, the system considered the user’s information provided.\nWhen we build the front-end of MovieHarbor, we will make this information\ndynamic as preliminary questions proposed to the user.\nBuilding a content-based\nsystem\nIn the previous section, we covered the cold-start scenario where the system\nknew nothing about the user. Sometimes, recommender systems already\nhave some backstory about users, and it is extremely useful to embed this\nknowledge in our application. Let’s imagine, for example, that we have a\nusers database where the system has stored all the registered user’s\ninformation (such as age, gender, country, etc.) as well as the movies the\nuser has already watched alongside their rating.\n\n\nTo do so, we will need to set a custom prompt that is able to retrieve this\ninformation from a source. For simplicity, we will create a sample dataset\nwith users’ information with just two records, corresponding to two users.\nEach user will exhibit the following variables: username, age, gender, and a\ndictionary containing movies already watched alongside with the rating they\ngave to them.\nThe high-level architecture is represented by the following diagram:\nFigure 7.3: High-level architecture of a content-based recommendation system\nLet’s break down this architecture and examine each step to build the final\nchat for this content-based system, starting from the available users’ data:\n1. As discussed earlier, we now have a bit of information about our users’\npreferences. More specifically, imagine we have a dataset containing\nusers’ attributes (name, age, gender) along with their reviews (a score\nfrom 1 to 10) of some movies. The following is the code used to create\nthe dataset:\n\n\nimport pandas as pd\ndata = {\n    \"username\": [\"Alice\", \"Bob\"],\n    \"age\": [25, 32],\n    \"gender\": [\"F\", \"M\"],\n    \"movies\": [\n        [(\"Transformers: The Last Knight\", 7), (\"Pokém\n        [(\"Bon Cop Bad Cop 2\", 8), (\"Goon: Last of the\n    ]\n}\n# Convert the \"movies\" column into dictionaries\nfor i, row_movies in enumerate(data[\"movies\"]):\n    movie_dict = {}\n    for movie, rating in row_movies:\n        movie_dict[movie] = rating\n    data[\"movies\"][i] = movie_dict\n# Create a pandas DataFrame\ndf = pd.DataFrame(data)\ndf.head()\nThe following output is obtained:\nFigure 7.4: Sample users dataset\n2. What we want to do now is apply the same logic of the prompt of the\ncold start with the formatting with variables. The difference here is that,\nrather than asking the user to provide the values for those variables, we\nwill directly collect them from our user dataset. So, we first define our\nprompt chunks:\n\n\ntemplate_prefix = \"\"\"You are a movie recommender syste\nUse the following pieces of context to answer the ques\nIf you don't know the answer, just say that you don't \n{context}\"\"\"\nuser_info = \"\"\"This is what we know about the user, an\nAge: {age}\nGender: {gender}\nMovies already seen alongside with rating: {movies}\"\"\"\ntemplate_suffix= \"\"\"Question: {question}\nYour response:\"\"\"\n3. We then format the user_info chunk as follows (assuming that the user\ninteracting with the system is Alice):\nage = df.loc[df['username']=='Alice']['age'][0]\ngender = df.loc[df['username']=='Alice']['gender'][0]\nmovies = ''\n# Iterate over the dictionary and output movie name an\nfor movie, rating in df['movies'][0].items():\n    output_string = f\"Movie: {movie}, Rating: {rating}\n    movies+=output_string\n    #print(output_string)\nuser_info = user_info.format(age = age, gender = gende\nCOMBINED_PROMPT = template_prefix +'\\n'+ user_info +'\\\nprint(COMBINED_PROMPT)\nHere is the output:\nYou are a movie recommender system that help users to find \nUse the following pieces of context to answer the question \nIf you don't know the answer, just say that you don't know\n{context}\nThis is what we know about the user, and you can use this i\nAge: 25\nGender: F\nMovies already seen alongside with rating: Movie: Transform\nMovie: Pokémon: Spell of the Unknown, Rating: 5\n\n\nQuestion: {question}\nYour response:\n4. Let’s now use this prompt within our chain:\nPROMPT = PromptTemplate(\n    template=COMBINED_PROMPT, input_variables=[\"contex\nchain_type_kwargs = {\"prompt\": PROMPT}\nqa = RetrievalQA.from_chain_type(llm=OpenAI(),\n    chain_type=\"stuff\",\n    retriever=docsearch.as_retriever(),\n    return_source_documents=True,\n    chain_type_kwargs=chain_type_kwargs)\nquery = \"Can you suggest me some action movie based on\nresult = qa({'query':query})\nresult['result']\nWe then obtain the following output:\n\" Based on your age, gender, and the movies you've already \n'\nAs you can see, the model is now able to recommend a list of movies\nto Alice based on the user’s information about past preferences,\nretrieved as context within the model’s metaprompt.\nNote that, in this scenario, we used as dataset a simple pandas dataframe. In\nproduction scenarios, a best practice for storing variables related to a task to\nbe addressed (such as a recommendation task) is that of using a feature store.\nFeature stores are data systems that are designed to support machine learning\nworkflows. They allow data teams to store, manage, and access features that\nare used for training and deploying machine learning models.\n\n\nFurthermore, LangChain offers native integrations towards some of the most\npopular features stores:\nFeast: This is an open-source feature store for machine learning. It\nallows teams to define, manage, discover, and serve features. Feast\nsupports batch and streaming data sources and integrates with various\ndata processing and storage systems. Feast uses BigQuery for offline\nfeatures and BigTable or Redis for online features.\nTecton: This is a managed feature platform that provides a complete\nsolution for building, deploying, and using features for machine\nlearning. Tecton allows users to define features in code, version control\nthem, and deploy them to production with best practices. Furthermore,\nit integrates with existing data infrastructure and ML platforms like\nSageMaker and Kubeflow, and it uses Spark for feature transformations\nand DynamoDB for online feature serving.\nFeatureform: This is a virtual feature store that transforms existing\ndata infrastructure into a feature store. Featureform allows users to\ncreate, store, and access features using standard feature definitions and\na Python SDK. It orchestrates and manages the data pipelines required\nfor feature engineering and materialization, and it is compatible with a\nwide range of data systems, such as Snowflake, Redis, Spark, and\nCassandra.\nAzureML Managed Feature Store: This is a new type of workspace\nthat lets users discover, create, and operationalize features. This service\nintegrates with existing data stores, feature pipelines, and ML platforms\nlike Azure Databricks and Kubeflow. Plus, it uses SQL, PySpark,\nSnowPark, or Python for feature transformations and Parquet/S3 or\nCosmos DB for feature storage.\n\n\nYou can read more about LangChain’s integration with features at\nhttps://blog.langchain.dev/feature-stores-and-llms/.\nDeveloping the front-end\nwith Streamlit\nNow that we have seen the logic behind an LLM-powered recommendation\nsystem, it is time to give a GUI to our MovieHarbor. To do so, we will once\nagain leverage Streamlit, and we will assume the cold-start scenario. As\nalways, you can find the whole Python code in the GitHub book repository\nat https://github.com/PacktPublishing/Building-LLM-\nPowered-Applications.\nAs per the Globebotter application in Chapter 6, in this case also you need to\ncreate a .py file to run in your terminal via streamlit run file.py. In our\ncase, the file will be named movieharbor.py.\nLet’s now summarize the key steps to build the app with the front-end:\n1. Configure the application webpage:\nimport streamlit as st\nst.set_page_config(page_title=\"GlobeBotter\", page_icon\nst.header('\n Welcome to MovieHarbor, your favourite mo\n2. Import the credentials and establish the connection to LanceDB:\nload_dotenv()\n#os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\nopenai_api_key = os.environ['OPENAI_API_KEY']\nembeddings = OpenAIEmbeddings()\nuri = \"data/sample-lancedb\"\n\n\ndb = lancedb.connect(uri)\ntable = db.open_table('movies')\ndocsearch = LanceDB(connection = table, embedding = em\n# Import the movie dataset\nmd = pd.read_pickle('movies.pkl')\n3. Create some widgets for the user to define their features and movies\npreferences:\n# Create a sidebar for user input\nst.sidebar.title(\"Movie Recommendation System\")\nst.sidebar.markdown(\"Please enter your details and pre\n# Ask the user for age, gender and favourite movie gen\nage = st.sidebar.slider(\"What is your age?\", 1, 100, 2\ngender = st.sidebar.radio(\"What is your gender?\", (\"Ma\ngenre = st.sidebar.selectbox(\"What is your favourite m\n# Filter the movies based on the user input\ndf_filtered = md[md['genres'].apply(lambda x: genre in\n4. Define the parametrized prompt chunks:\ntemplate_prefix = \"\"\"You are a movie recommender syste\nUse the following pieces of context to answer the ques\nIf you don't know the answer, just say that you don't \n{context}\"\"\"\nuser_info = \"\"\"This is what we know about the user, an\nAge: {age}\nGender: {gender}\"\"\"\ntemplate_suffix= \"\"\"Question: {question}\nYour response:\"\"\"\nuser_info = user_info.format(age = age, gender = gende\nCOMBINED_PROMPT = template_prefix +'\\n'+ user_info +'\\\nprint(COMBINED_PROMPT)\n5. Set up the RetrievalQA chain:\n",
      "page_number": 265,
      "chapter_number": 32,
      "summary": "When we build the front-end of MovieHarbor, we will make this information\ndynamic as preliminary questions proposed to the user Key topics include movies, feature, and data.",
      "keywords": [
        "user",
        "prompt",
        "movies",
        "age",
        "feature",
        "gender",
        "data",
        "chain",
        "system",
        "rating",
        "feature store",
        "COMBINED",
        "type",
        "create",
        "Question"
      ],
      "concepts": [
        "movies",
        "feature",
        "data",
        "user",
        "useful",
        "uses",
        "gender",
        "sidebar",
        "stored",
        "store"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 21,
          "title": "Segment 21 (pages 174-182)",
          "relevance_score": 0.47,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 33,
          "title": "Segment 33 (pages 282-289)",
          "relevance_score": 0.47,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 41,
          "title": "Segment 41 (pages 355-362)",
          "relevance_score": 0.46,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 36,
          "title": "Segment 36 (pages 309-316)",
          "relevance_score": 0.45,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 35,
          "title": "Segment 35 (pages 301-308)",
          "relevance_score": 0.44,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 273-282)",
      "start_page": 273,
      "end_page": 282,
      "detection_method": "topic_boundary",
      "content": "#setting up the chain\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_t\n    retriever=docsearch.as_retriever(search_kwargs={'d\n6. Insert the search bar for the user:\nquery = st.text_input('Enter your question:', placehol\nif query:\n    result = qa({\"query\": query})\n    st.write(result['result'])\nAnd that’s it! You can run the final result in your terminal with streamlit\nrun movieharbor.py. It looks like the following:\nFigure 7.5: Sample front-end for Movieharbor with Streamlit\nSo, you can see, in just few lines of code we were able to set up a webapp\nfor our MovieHarbor. Starting from this template, you can customize your\nlayout with Streamlit’s components, as well as tailor it to content-based\nscenarios. Plus, you can customize your prompts in such a way that the\nrecommender acts as you prefer.\n\n\nSummary\nIn this chapter, we explored how LLMs could change the way we approach a\nrecommendation system task. We started from the analysis of the current\nstrategies and algorithms for building recommendation applications,\ndifferentiating between various scenarios (collaborative filtering, content-\nbased, cold start, etc.) as well as different techniques (KNN, matrix\nfactorization, and NNs).\nWe then moved to the new, emerging field of research into how to apply the\npower of LLMs to this field, and explored the various experiments that have\nbeen done in recent months.\nLeveraging this knowledge, we built a movie recommender application\npowered by LLMs, using LangChain as the AI orchestrator and Streamlit as\nthe front-end, showing how LLMs can revolutionize this field thanks to their\nreasoning capabilities as well as their generalization. This was just one\nexample of how LLMs not only can open new frontiers, but can also\nenhance existing fields of research.\nIn the next chapter, we will see what these powerful models can do when\nworking with structured data.\nReferences\nRecommendation as Language Processing (RLP): A Unified\nPretrain, Personalized Prompt & Predict Paradigm (P5).\nhttps://arxiv.org/abs/2203.13366\nLangChain’s blog about featurestores.\nhttps://blog.langchain.dev/feature-stores-and-\nllms/\n\n\nFeast. https://docs.feast.dev/\nTecton. https://www.tecton.ai/\nFeatureForm. https://www.featureform.com/\nAzure Machine Learning feature store.\nhttps://learn.microsoft.com/en-us/azure/machine-\nlearning/concept-what-is-managed-feature-store?\nview=azureml-api-2\nJoin our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\nOceanofPDF.com\n\n\n8\nUsing LLMs with\nStructured Data\nIn this chapter, we are going to cover yet another great capability of large\nlanguage models (LLMs): the ability to handle structured, tabular data. We\nwill see how, thanks to plugins and an agentic approach, we can use LLMs\nas a natural language interface between us and our structured data, reducing\nthe gap between the business user and the structured information.\nDuring this chapter, we will cover the following topics:\nIntroduction to the main structured data systems\nUsing tools and plugins to connect LLMs to tabular data\nBuilding a database copilot with LangChain\nBy the end of this chapter, you will be able to build your own natural\nlanguage interface for your data estate and be able to combine unstructured\nwith structured sources.\nTechnical requirements\nTo complete the tasks in this chapter, you will need the following:\nA Hugging Face account and user access token.\nAn OpenAI account and user access token.\n\n\nPython 3.7.1 or later version.\nPython packages: Make sure to have the following Python packages\ninstalled: langchain, python-dotenv, huggingface_hub, streamlit, and\nsqlite3. Those can be easily installed via pip install in your\nterminal.\nYou can find all the code and examples in the book’s GitHub repository at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications.\nWhat is structured data?\nIn previous chapters, we focused on how LLMs can handle textual data. In\nfact, those models are, as the name suggests, “language” models, meaning\nthat they have been trained and are able to handle unstructured text data.\nNevertheless, unstructured data only refers to a portion of the overall data\nrealm that applications can handle. Generally, data can be categorized into\nthree types, which are as follows:\nUnstructured data: This refers to data that doesn’t have a specific or\npredefined format. It lacks a consistent structure, making it challenging\nto organize and analyze using traditional databases. Examples of\nunstructured data include:\nText documents: Emails, social media posts, articles, and reports.\nMultimedia: Images, videos, audio recordings.\nNatural language text: Chat logs, transcriptions of spoken\nconversations.\nBinary data: Files without a specific data format, such as\nproprietary file formats.\n\n\nNote\nWhen it comes to storing unstructured data, NoSQL\ndatabases play a crucial role, due to their flexible\nschema-less design, which allows them to handle\nvarious data types like text, images, and videos\nefficiently. The term “NoSQL” originally stood for\n“non-SQL” or “not only SQL” to emphasize that these\ndatabases don’t rely solely on the traditional\nStructured Query Language (SQL) to manage and\nquery data. NoSQL databases emerged as a response to\nthe limitations of relational databases, particularly\ntheir rigid schema requirements and difficulties in\nscaling horizontally.\nAn example of a NoSQL database is MongoDB, a\ndocument-oriented NoSQL database, which stores data\nin JSON-like documents, making it highly effective\nfor managing diverse unstructured content; similarly,\nCassandra, with its wide-column store model, excels at\nhandling large volumes of data across many\ncommodity servers, providing high availability\nwithout compromising performance. This flexibility\nenables NoSQL databases to adapt to the volume,\nvariety, and velocity of unstructured data,\naccommodating rapid changes and scaling easily.\nTraditional relational databases, with their rigid\nschema requirements, struggle to manage such\ndiversity and volume efficiently.\n\n\nStructured data: This type of data is organized and formatted with a\nclear structure, typically into rows and columns. It follows a fixed\nschema, making it easy to store, retrieve, and analyze using relational\ndatabases. Examples of structured data include:\nRelational databases: Data stored in tables with predefined\ncolumns and data types.\nSpreadsheets: Data organized in rows and columns in software\nlike Microsoft Excel.\nSensor data: Recorded measurements like temperature, pressure,\nand time in a structured format.\nFinancial data: Transaction records, balance sheets, and income\nstatements.\nSemi-structured data: This falls between the two categories. While it\ndoesn’t adhere to a rigid structure like structured data, it has some level\nof organization and may contain tags or other markers that provide\ncontext. Examples of semi-structured data include:\neXtensible Markup Language (XML) files: They use tags to\nstructure data, but the specific tags and their arrangement can\nvary.\nJavaScript Object Notation (JSON): This is used for data\ninterchange and allows for nested structures and key-value pairs.\nNoSQL databases: Storing data in a format that doesn’t require a\nfixed schema, allowing for flexibility.\nIn summary, unstructured data lacks a defined format, structured data\nfollows a strict format, and semi-structured data has some level of structure\nbut is more flexible than structured data. The distinction between these types\nof data is important as it impacts how they are stored, processed, and\nanalyzed in various applications.\n\n\nHowever, regardless of its nature, querying structured data involves using a\nquery language or methods specific to that database technology. For\nexample, for SQL databases, SQL is used to interact with relational\ndatabases. Henceforth, to extract data from tables, you need to know this\nspecific language.\nBut what if we want to ask questions in natural language to our structured\ndata? What if our application could provide us not only with a sterile\nnumeric answer but rather with a conversational answer, which also gives us\ncontext about the number? This is exactly what we will try to achieve in the\nnext sections with our LLM-powered applications. More specifically, we are\ngoing build something that we’ve already defined in Chapter 2: a copilot.\nSince we are going to mount our copilot to a relational database, we will\nname our application DBCopilot. First, let’s look at what relational\ndatabases are.\nGetting started with\nrelational databases\nThe concept of relational databases was first proposed by E.F. Codd, an IBM\nresearcher, in 1970. He defined the rules and principles of the relational\nmodel, which aimed to provide a simple and consistent way of accessing and\nmanipulating data. He also introduced SQL, which became the standard\nlanguage for querying and manipulating relational databases. Relational\ndatabases have become widely used in various domains and applications,\nsuch as e-commerce, inventory management, payroll, customer\nrelationship management (CRM), and business intelligence (BI).\n\n\nIn this section, we are going to cover the main aspects of a relational\ndatabase. Then, we will start working with the sample database we will use\nin our DBCopilot, the Chinook database. We will inspect this database and\nexplore how to connect to remote tables using Python.\nIntroduction to relational\ndatabases\nA relational database is a type of database that stores and organizes data in\nstructured tables with rows and columns. Each row represents a record, and\neach column represents a field or attribute. The relationships between tables\nare established through keys, primarily the primary key and foreign key. This\nallows for efficient querying and manipulation of data using SQL. These\ndatabases are commonly used for various applications like websites and\nbusiness management systems, due to their ability to manage structured data\neffectively.\nTo have a better understanding of relational databases, let’s consider an\nexample of a database of a library. We’ll have two tables: one for books and\nanother for authors. The relationship between them will be established using\nprimary and foreign keys.\nDefinition\nA primary key is like the unique fingerprint of each record in\na table. It’s a special column that holds a value that’s distinct\nfor each row in that table. Think of it as the “identity” of a\nrecord. Having a primary key is important because it\nguarantees that no two records in the same table will share\n\n\nthe same key. This uniqueness makes it easy to locate,\nmodify, and manage individual records in the table.\nA foreign key is a bridge between two tables. It’s a column in\none table that references the primary key column in another\ntable. This reference creates a link between the data in the\ntwo tables, establishing a relationship. The purpose of the\nforeign key is to maintain data consistency and integrity\nacross related tables. It ensures that if a change is made in\nthe primary key table, the related data in the other table\nremains accurate. By using foreign keys, you can retrieve\ninformation from multiple tables that are connected, enabling\nyou to understand how different pieces of data are related to\neach other.\nLet’s take a closer look at our example, as shown in the following image:\nFigure 8.1: An example of the relationship between two tables in a database\nIn this example, the Authors table contains information about authors,\nincluding their ID, name, and birth year. The Books table includes details\nabout books, including the book’s ID, title, and a foreign key called\n",
      "page_number": 273,
      "chapter_number": 33,
      "summary": "Summary\nIn this chapter, we explored how LLMs could change the way we approach a\nrecommendation system task Key topics include data, structured, and structures.",
      "keywords": [
        "data",
        "structured data",
        "relational databases",
        "databases",
        "structured",
        "relational",
        "Language",
        "unstructured data",
        "LLMs",
        "tables",
        "key",
        "chain",
        "unstructured",
        "SQL",
        "streamlit"
      ],
      "concepts": [
        "data",
        "structured",
        "structures",
        "database",
        "tables",
        "language",
        "manage",
        "managing",
        "key",
        "keys"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 20,
          "title": "Segment 20 (pages 181-194)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 25,
          "title": "Segment 25 (pages 216-223)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 7,
          "title": "Segment 7 (pages 50-57)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 8,
          "title": "Segment 8 (pages 61-70)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 283-291)",
      "start_page": 283,
      "end_page": 291,
      "detection_method": "topic_boundary",
      "content": "AuthorID, which references the corresponding author in the Authors table\n(with AuthorID as the primary key). This way, you can use SQL queries to\nretrieve information like finding all books written by a specific author or the\nbirth year of an author based on the book they wrote. The relational structure\nallows for efficient management and retrieval of data in a structured manner.\nSome of the main database systems in the market include:\nSQL databases: These are relational database management systems\n(RDBMS) that use SQL for data manipulation and querying. Examples\ninclude MySQL, PostgreSQL, and Microsoft SQL Server.\nOracle Database: A widely-used RDBMS that offers advanced features\nand scalability for large-scale applications.\nSQLite: A self-contained, serverless, and zero-configuration SQL\ndatabase engine commonly used in embedded systems and mobile\napplications.\nIBM Db2: A family of data management products, including relational\ndatabase servers, developed by IBM.\nAmazon Web Services (AWS) RDS: A managed relational database\nservice offered by Amazon, providing options for various databases like\nMySQL, PostgreSQL, SQL Server, and more.\nGoogle Cloud SQL: A managed database service by Google Cloud\nPlatform, supporting MySQL, PostgreSQL, and SQL Server.\nRedis: An open-source, in-memory data structure store that can be used\nas a database, cache, and message broker.\nIn this chapter, we are going to use SQLite database, which also offers a\nseamless integration with Python. But before we do that, let’s understand the\ndatabase we’ll be using.\n\n\nOverview of the Chinook\ndatabase\nThe Chinook database is a sample database that can be used for learning and\npracticing SQL. It is based on a fictional digital media store and contains\ndata about artists, albums, tracks, customers, invoices, and more. The\nChinook database is available for various database management systems,\nsuch as SQL Server, Oracle, MySQL, PostgreSQL, SQLite, and DB2.\nHere are some features of this database:\nIt uses real data from an iTunes library, which makes it more realistic\nand interesting.\nIt has a clear and simple data model, which makes it easy to understand\nand query.\nIt covers more features of SQL, such as subqueries, joins, views, and\ntriggers.\nIt is compatible with multiple database servers, which makes it more\nversatile and portable.\nYou can find the configuration instructions at\nhttps://database.guide/2-sample-databases-sqlite/.\nYou can see an illustration of the relationship among the database’s tables\nhere:\n\n\nFigure 8.2: Diagram of Chinook Database (source:\nhttps://github.com/arjunchndr/Analyzing-Chinook-Database-using-SQL-\nand-Python)\nAs you can see, there are 11 tables, all related to each other with primary and\nforeign keys. In the upcoming paragraph, we will see how LLMs will be\nable to navigate among those tables, capturing their relationships and\ngathering relevant information. But before jumping to LLMs, let’s first\n\n\ninspect the Chinook database a bit more by setting up the connection with\nPython.\nHow to work with relational\ndatabases in Python\nTo work with relational databases in Python, you need to use a library that\ncan connect to the database and execute SQL queries. Some of these libraries\nare as follows:\nSQLAlchemy: This is an open-source SQL toolkit and object-relational\nmapper (ORM) for Python. It allows you to create, read, update, and\ndelete data from relational databases using Python objects and methods.\nIt supports many database engines, such as SQLite, MySQL,\nPostgreSQL, and Oracle.\nPsycopg: This is a popular database connector for PostgreSQL. It\nenables you to execute SQL queries and access PostgreSQL features\nfrom Python. It is fast, reliable, and thread-safe.\nMySQLdb: This is a database connector for MySQL. It allows you to\ninteract with MySQL databases from Python using the DB-API 2.0\nspecification. It is one of the oldest and most widely used Python\nlibraries for MySQL, but its development is mostly frozen.\ncx_Oracle: This is a database connector for Oracle Database. It enables\nyou to connect to Oracle databases and use SQL and PL/SQL features\nfrom Python. It supports advanced features such as object types, Large\nObjects (LOBs), and arrays.\nsqlite3: This is a database connector for SQLite3, a widely used,\nlightweight, serverless, self-contained, and open-source relational\n\n\ndatabase management system. You can use sqlite3 to create, query,\nupdate, and delete data from SQLite databases in your Python programs\nSince we are going to work with SQLite, we will use the sqlite3 module,\nwhich you will need to install via pip install sqlite3. Some of the\nfeatures of sqlite3 are as follows:\nIt follows the DB-API 2.0 specification, which defines a standard\ninterface for Python database access modules.\nIt supports transactions, which allow you to execute multiple SQL\nstatements as a single unit of work and roll back in case of errors.\nIt allows you to use Python objects as parameters and results for SQL\nqueries, using various adapters and converters.\nIt supports user-defined functions, aggregates, collations, and\nauthorizers, which enable you to extend the functionality of SQLite\nwith Python code.\nIt has a built-in row factory, which returns query results as named\ntuples or dictionaries instead of plain tuples.\nLet’s see an example of this connection using our Chinook database:\n1. The database can be downloaded locally from\nhttps://www.sqlitetutorial.net/wp-\ncontent/uploads/2018/03/chinook.zip. You will only\nneed to unzip the chinook.db file and it will be ready to be consumed.\nIn the following code, we are initializing a connection (conn) to our\nchinook.db, which will be used to interact with the database. Then, we\nwill save our tables in a pandas object with the read_sql module,\nwhich allows you to run SQL queries against your database:\n\n\nimport sqlite3\nimport pandas as pd\n## creating a connection\ndatabase = 'chinook.db'\nconn = sqlite3.connect(database)\n## importing tables\ntables = pd.read_sql(\"\"\"SELECT name, type\n                        FROM sqlite_master\n                         WHERE type IN (\"table\", \"view\nHere is the output that we can see:\nFigure 8.3: A list of tables within the Chinook database\n\n\nNote\nColumn names might be slightly different as the online\ndatabase is updated over time. To get up-to-date\ncolumns’ naming conventions, you can run the\nfollowing command:\npd.read_sql(\"PRAGMA table_info(customers);\", \nprint(customer_columns)\n2. We can also inspect the single table to gather some relevant data. For\nexample, let’s say we want to see the top five countries per album sales:\npd.read_sql(\"\"\"\nSELECT c.country AS Country, SUM(i.total) AS Sales\nFROM customer c\nJOIN invoice i ON c.customer_id = i.customer_id\nGROUP BY Country\nORDER BY Sales DESC\nLIMIT 5;\n\"\"\", conn)\nHere is the corresponding output:\n\n\nFigure 8.4: Top 5 countries with highest sales\n3. Finally, we can also use the matplotlib Python library to create useful\ndiagrams about the database’s statistics. In the following Python\nsnippet, we are going to run an SQL query to extract the number of\ntracks grouped by genre, and then plot the result using matplotlib as\nfollows:\nimport matplotlib.pyplot as plt\n# Define the SQL query\nsql = \"\"\"\nSELECT g.Name AS Genre, COUNT(t.track_id) AS Tracks\nFROM genre g\nJOIN track t ON g.genre_id = t.genre_id\nGROUP BY Genre\nORDER BY Tracks DESC;\n\"\"\"\n# Read the data into a dataframe\ndata = pd.read_sql(sql, conn)\n# Plot the data as a bar chart\nplt.bar(data.Genre, data.Tracks)\nplt.title(\"Number of Tracks by Genre\")\nplt.xlabel(\"Genre\")\nplt.ylabel(\"Tracks\")\n\n\nplt.xticks(rotation=90)\nplt.show()\nWe’ll see the following output:\nFigure 8.5: Number of tracks by genre\nAs you can see, in order to gather relevant information from our database,\nwe used the syntax of SQL. Our goal is to gather information by simply\n",
      "page_number": 283,
      "chapter_number": 34,
      "summary": "In this chapter, we are going to use SQLite database, which also offers a\nseamless integration with Python Key topics include databases, data, and python.",
      "keywords": [
        "SQL",
        "database",
        "Chinook database",
        "Python",
        "SQL Server",
        "Microsoft SQL Server",
        "SQL queries",
        "data",
        "Chinook",
        "relational database",
        "database servers",
        "Oracle",
        "tracks",
        "genre",
        "relational"
      ],
      "concepts": [
        "databases",
        "data",
        "python",
        "tracks",
        "related",
        "querying",
        "mysql",
        "object",
        "genre",
        "management"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 44,
          "title": "Segment 44 (pages 362-366)",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 4,
          "title": "[ 113 ]",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 17,
          "title": "Python Database Access",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 3,
          "title": "[ 77 ]",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "Software Architecture",
          "chapter": 20,
          "title": "Segment 20 (pages 181-194)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 292-299)",
      "start_page": 292,
      "end_page": 299,
      "detection_method": "topic_boundary",
      "content": "asking in natural language, and we are going to do so starting in the next\nsection.\nImplementing the DBCopilot\nwith LangChain\nIn this section, we are going to cover the architecture and implementation\nsteps behind a DBCopilot application, a natural language interface to chat\nwith database-structured data. In the upcoming sections, we will explore\nhow to achieve that by leveraging a powerful LangChain component called\nSQL Agent.\nLangChain agents and SQL\nAgent\nIn Chapter 4, we introduced the concept of LangChain agents, defining them\nas entities that drive decision making within LLM-powered applications.\nAgents have access to a suite of tools and can decide which tool to call based\non the user input and the context. Agents are dynamic and adaptive, meaning\nthat they can change or adjust their actions based on the situation or the goal.\nIn this chapter, we will see agents in action, using the following LangChain\ncomponents:\ncreate_sql_agent: An agent designed to interact with relational\ndatabases\nSQLDatabaseToolkit: A toolkit to provide the agent with the required\nnon-parametric knowledge\n\n\nOpenAI: An LLM to act as the reasoning engine behind the agent, as\nwell as the generative engine to produce conversational results\nLet’s start with our implementation by following these steps:\n1. We’ll first initialize all the components and establish the connection to\nthe Chinook database, using the SQLDatabase LangChain component\n(which uses SQLAlchemy under the hood and is used to connect to our\ndatabase):\nfrom langchain.agents import create_sql_agent\nfrom langchain.llms import OpenAI\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.agents.agent_toolkits import SQLDatabas\nfrom langchain.sql_database import SQLDatabase\nfrom langchain.llms.openai import OpenAI\nfrom langchain.agents import AgentExecutor\nfrom langchain.agents.agent_types import AgentType\nfrom langchain.chat_models import ChatOpenAI\nllm = OpenAI()\ndb = SQLDatabase.from_uri('sqlite:///chinook.db')\ntoolkit = SQLDatabaseToolkit(db=db, llm=llm)\nagent_executor = create_sql_agent(\n    llm=llm,\n    toolkit=toolkit,\n    verbose=True,\n    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n)\n2. Before running the agent, let’s first inspect its available tools:\n[tool.name for tool in toolkit.get_tools()]\nHere is the output:\n\n\n['sql_db_query', 'sql_db_schema', 'sql_db_list_tables', 'sq\nThose tools have the following capabilities:\nsql_db_query: This takes as input a detailed and correct SQL\nquery, and it outputs a result from the database. If the query is not\ncorrect, an error message will be returned.\nsql_db_schema: This takes as input a comma-separated list of\ntables, and it outputs the schema and sample rows for those\ntables.\nsql_db_list_tables: This takes as input an empty string, and it\noutputs a comma-separated list of tables in the database.\nsql_db_query_checker: This tool double-checks whether the\nquery is correct before executing it.\n3. Let’s now execute our agent with a simple query to describe the\nplaylisttrack table:\nagent_executor.run(\"Describe the playlisttrack table\")\nThe following output is then obtained (the output is truncated – you\ncan find the full output in the book’s GitHub repository):\n> Entering new AgentExecutor chain...\nAction: sql_db_list_tables\nAction Input:\nObservation: album, artist, customer, employee, genre, invo\nThought: The table I need is playlist_track\nAction: sql_db_schema\nAction Input: playlist_track\nObservation:\n\n\nCREATE TABLE playlist_track (\n[...]\n> Finished chain.\n'The playlist_track table contains the playlist_id and trac\nAs you can see, with a simple question in natural language, our agent was\nable to understand its semantics, translate it into an SQL query, extract the\nrelevant information, and use it as context to generate the response.\nBut how was it able to do all of that? Under the hood, the SQL agent comes\nwith a default prompt template, which makes it tailored to this type of\nactivity. Let’s see the default template of the LangChain component:\nprint(agent_executor.agent.llm_chain.prompt.template)\nHere is the output obtained:\nYou are an agent designed to interact with a SQL database.\nGiven an input question, create a syntactically correct sqlite qu\nUnless the user specifies a specific number of examples they wish\nYou can order the results by a relevant column to return the most\nNever query for all the columns from a specific table, only ask f\nYou have access to tools for interacting with the database.\nOnly use the below tools. Only use the information returned by th\nYou MUST double check your query before executing it. If you get \nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.\nIf the question does not seem related to the database, just retur\nsql_db_query: Input to this tool is a detailed and correct SQL qu\nsql_db_schema: Input to this tool is a comma-separated list of ta\nBe sure that the tables actually exist by calling sql_db_list_tab\nsql_db_list_tables: Input is an empty string, output is a comma s\nsql_db_query_checker: Use this tool to double check if your query\nUse the following format:\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [sql_db_query, sql_d\n\n\nAction Input: the input to the action\n...\nQuestion: {input}\nThought: I should look at the tables in the database to see what \n{agent_scratchpad}\nThanks to this prompt template, the agent is able to use the proper tools and\ngenerate a SQL query, without modifying the underlying database (you can\nsee the explicit rule not to run any data manipulation language (DML)\nstatements).\nDefinition\nDML is a class of SQL statements that are used to query,\nedit, add, and delete row-level data from database tables or\nviews. The main DML statements are as follows:\nSELECT: This is used to retrieve data from one or more\ntables or views based on specified criteria.\nINSERT: This is used to insert new data records or rows\ninto a table.\nUPDATE: This is used to modify the values of existing\ndata records or rows in a table.\nDELETE: This is used to remove one or more data\nrecords or rows from a table.\nMERGE: This is used to combine the data from two tables\ninto one based on a common column.\nDML statements are used to store, modify, retrieve,\ndelete, and update data in a database.\n\n\nWe can also see how the agent is able to correlate more than one table within\nthe database:\nagent_executor.run('what is the total number of tracks and the a\nFrom the first lines of the chain, you can see that Action Input invokes two\ntables – track and genre:\n> Entering new AgentExecutor chain...\nAction: sql_db_list_tables\nAction Input:\nObservation: album, artist, customer, employee, genre, invoice, \nThought: I should look at the schema of the track and genre tabl\nAction: sql_db_schema\nAction Input: track, genre\n[…]\nThe following is the output:\n'The top 10 genres by track count and average track length are Ro\nNow, the question is as follows: are we sure that we are getting the proper\nresult? A nice way to double-check this would be to print the SQL query that\nthe agent ran against the database. To do so, we can modify the default\nprompt to ask the agent to explicitly show us the reasoning behind its result.\nPrompt engineering\nAs we saw in the previous chapter, pre-built LangChain agents and chains\ncome with default prompts, which make it easier to tailor them toward their\n\n\ngoals. Nevertheless, we can customize that prompt and pass it as a parameter\nto our component. For example, let’s say that we want our SQL agent to\nprint the SQL query it used to return the result.\nFirst of all, we have to understand which kind of prompt chunks the SQL\nAgent is able to take as parameters. To do so, we can simply inspect the\nobjects running create_sql_agent.\nFigure 8.6: A screenshot of the description of the SQL agent\nThe Agent takes a prompt prefix and a format instruction, which are merged\nand constitute the default prompt we inspected in the previous section. To\nmake our agent more self-explanatory, we will create two variables, prefix\nand format_instructions, which will be passed as parameters and that\nslightly modify the default prompt as follows (you can find the whole\nprompts in the GitHub repository at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications:\n\n\nWe have the prompt_prefix, which is already configured as follows:\nprefix: 'str' = 'You are an agent designed to interact\nTo this, we will add the following line of instruction:\nAs part of your final answer, ALWAYS include an explanation\nIn prompt_format_instructions, we will add the following example of\nexplanation using few-shot learning, which we covered in Chapter 1:\nExplanation:\n<===Beginning of an Example of Explanation:\nI joined the invoices and customers tables on the cust\n```sql\nSELECT c.country AS Country, SUM(i.total) AS Sales\nFROM customer c\nJOIN invoice i ON c.customer_id = i.customer_id\nGROUP BY Country\nORDER BY Sales DESC\nLIMIT 5;\n```sql\n===>End of an Example of Explanation\nNow, let’s pass those prompt chunks as parameters to our agent and print the\nresult (I will omit the whole chain here, but you can see it in the GitHub\nrepository):\nagent_executor = create_sql_agent(\n    prefix=prompt_prefix,\n    format_instructions = prompt_format_instructions,\n    llm=llm,\n    toolkit=toolkit,\n",
      "page_number": 292,
      "chapter_number": 35,
      "summary": "In the upcoming sections, we will explore\nhow to achieve that by leveraging a powerful LangChain component called\nSQL Agent Key topics include tables, agent, and input.",
      "keywords": [
        "SQL",
        "Agent",
        "SQL Agent",
        "input",
        "Action Input",
        "SQL query",
        "query",
        "tables",
        "action",
        "database",
        "prompt",
        "LangChain agents",
        "list",
        "LLM",
        "LangChain"
      ],
      "concepts": [
        "tables",
        "agent",
        "input",
        "database",
        "data",
        "follows",
        "actions",
        "prompt",
        "langchain",
        "llm"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 42-49)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 55,
          "title": "Segment 55 (pages 465-472)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 30,
          "title": "Segment 30 (pages 253-261)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 300-307)",
      "start_page": 300,
      "end_page": 307,
      "detection_method": "topic_boundary",
      "content": "    verbose=True,\n    top_k=10\n)\nresult = agent_executor.run(\"What are the top 5 best-selling alb\nprint(result)\nHere is the obtained output:\nThe top 5 best-selling albums and their artists are 'A Matter of \nExplanation: I joined the album and invoice tables on the album_i\n```sql\nSELECT al.title AS Album, ar.name AS Artist, SUM(i.total) AS Sale\nFROM album al\nJOIN invoice i ON al.album_id = i.invoice_id\nJOIN artist ar ON al.artist_id = ar.artist_id\nGROUP BY ar.name\nORDER BY Sales\nNow, in our result, we have a clear explanation of the thought process as\nwell as the printed query our agent made for us. This is key if we want to\ndouble-check the correctness of the reasoning procedure happening in the\nbackend of our agent.\nThis is already extremely useful, but we want to bring it to the next level: we\nwant our DBCopilot to also be able to generate graphs and save results in\nour local file system. To achieve this goal, we need to add tools to our agent,\nand we are going to do so in the next section.\nAdding further tools\nIn order to make our DBCopilot more versatile, there are two further\ncapabilities we need to add:\n\n\nPythonREPLTool: This tool allows you to interact with the Python\nprogramming language using natural language. You can use this tool to\nwrite, run, and debug Python code without having to use a script file or\nan IDE. You can also use this tool to access and manipulate various\nPython modules, libraries, and data structures. We will need this tool\nto produce the matplotlib graphs from the SQL query’s results.\nDefinition\nREPL is an acronym for read-eval-print loop, which is\na term that describes an interactive shell or\nenvironment that allows you to execute code and see\nthe results immediately. REPL is a common feature of\nmany programming languages, such as Python, Ruby,\nand Lisp.\nIn the context of LangChain, REPL is a feature that\nallows you to interact with LangChain agents and tools\nusing natural language. You can use REPL in\nLangChain to test, debug, or experiment with different\nagents and tools without having to write and run a\nscript file. You can also use REPL in LangChain to\naccess and manipulate various data sources, such as\ndatabases, APIs, and web pages.\nFileManagementToolkit: This is a set of tools, or toolkit, that allows\nyou to interact with the file system of your computer or device using\nnatural language. You can use this toolkit to perform various operations\non files and directories, such as creating, deleting, renaming, copying,\nmoving, searching, reading, and writing. You can also use this toolkit to\n\n\naccess and manipulate the metadata and attributes of files and\ndirectories, such as name, size, type, date, and permissions.\nWe will need this toolkit to save the graphs generated by our agent in\nour working directory.\nNow, let’s see how we can add these tools to our DBCopilot:\n1. First, we define the list of tools for our agent:\nfrom  langchain_experimental.tools.python.tool import \nfrom  langchain_experimental.python import PythonREPL\nfrom langchain.agents.agent_toolkits import FileManage\nworking_directory  = os.getcwd()\ntools = FileManagementToolkit(\n    root_dir=str(working_directory),\n    selected_tools=[\"read_file\", \"write_file\", \"list_d\ntools.append(\n    PythonREPLTool())\ntools.extend(SQLDatabaseToolkit(db=db, llm=llm).get_to\n2. In order to leverage that heterogeneous set of tools – SQL Database,\nPython REPL, and File System\n(https://python.langchain.com/v0.1/docs/integrat\nions/tools/filesystem/) – we cannot work anymore with the\nSQL Database-specific agent, since its default configurations are meant\nto only accept SQL-related contents. Henceforth, we need to set up an\nagnostic agent that is able to use all of the tools that we provide it with.\nFor this purpose, we are going to use the\nSTRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION agent type, which is\nable to use a multi-tool input.\nLet’s first start with initializing the agent and asking it to produce a\nbar chart and save it in the current working directory for the top five\n\n\ncountries for sales (note that, for this purpose, I’ve used a chat model\nas best suited for the type of agent in use):\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.agents import initialize_agent, Tool\nfrom langchain.agents import AgentType\nmodel = ChatOpenAI()\nagent = initialize_agent(\n    tools, model, agent= AgentType.STRUCTURED_CHAT_ZERO_SHO\n)\nagent.run(\"generate a matplotlib bar chart of the top 5 cou\nWe then receive the following output, showing how, in this case, the\nagent was also able to dynamically orchestrate the available tools to\ngenerate the final answer (I will report here just the main actions of\nthe chain – you can see the whole code in the GitHub repository of the\nbook):\n> Entering new AgentExecutor chain...\nAction:\n```\n{\n  \"action\": \"sql_db_query\",\n  \"action_input\": \"SELECT billing_country as Country, SUM(t\n}\n```\n[…]\nObservation: [('USA', 10405.889999999912), ('Canada', 5489\n[…]\nWe have successfully retrieved the top 5 countries for sale\nAction:\n```\n{\n  \"action\": \"Python_REPL\",\n  \"action_input\": \"import matplotlib.pyplot as plt\\nsales_d\n}\n```\n\n\n[…]\n> Finished chain.\n'Here is the bar chart of the top 5 countries for sales fro\nThe following is the generated chart of the top five countries by sales,\nas requested:\nFigure 8.7: Bar chart of top five countries by sales\nGreat! The agent was able to first invoke the SQL tool to retrieve the\nrelevant information, then it used the Python tool to generate the matplotlib\nbar chart. Then, it used the file system tool to save the result as PNG.\n\n\nAlso, in this case, we can modify the prompt of the agent. For example, we\nmight want the agent to provide an explanation not only of the SQL query\nbut also of the Python code. To do so, we need to define the prompt_prefix\nand prompt_format_instructions variables to be passed as kgwargs to the\nagent as follows:\nprompt_prefix = \"\"\" Your prefix here\n\"\"\"\nprompt_format_instructions= \"\"\"\nYour instructions here.\n\"\"\"\nagent = initialize_agent(tools, model, agent=AgentType.STRUCTURE\n                         agent_kwargs={\n                            'prefix':prompt_prefix,\n                            'format_instructions': prompt_format\nThanks to LangChain’s tools components, we were able to extend our\nDBCopilot capabilities and make it more versatile, depending upon the\nuser’s query.\nWith the same logic, we can tailor our agents to any domain, adding or\nremoving tools so that we can control its perimeter of actions. Plus, thanks to\nthe prompt customization, we can always refine the agent’s backend logic to\nmake it more customized.\nDeveloping the front-end\nwith Streamlit\nNow that we have seen the logic behind an LLM-powered DBCopilot, it is\ntime to give a GUI to our application. To do so, we will once again leverage\nStreamlit. As always, you can find the whole Python code in the GitHub\n\n\nbook repository at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications.\nAs per the previous sections, you need to create a .py file to run in your\nterminal via streamlit run file.py. In our case, the file will be named\ndbcopilot.py.\nHere are the main steps to set up the frontend:\n1. Configure the application web page:\nimport streamlit as st\nst.set_page_config(page_title=\"DBCopilot\", page_icon=\"\nst.header('\n Welcome to DBCopilot, your copilot for st\n2. Import the credentials and establish the connection with the Chinook\ndatabase:\nload_dotenv()\n#os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\nopenai_api_key = os.environ['OPENAI_API_KEY']\ndb = SQLDatabase.from_uri('sqlite:///chinook.db')\n3. Initialize the LLM and the toolkit:\nllm = OpenAI()\ntoolkit = SQLDatabaseToolkit(db=db, llm=llm)\n4. Initialize the Agent using the prompt variables defined in the previous\nsections:\n\n\nagent_executor = create_sql_agent(\n    prefix=prompt_prefix,\n    format_instructions = prompt_format_instructions,\n    llm=llm,\n    toolkit=toolkit,\n    verbose=True,\n    top_k=10\n)\n5. Define Streamlit’s session states to make it conversational and memory\naware:\nif \"messages\" not in st.session_state or st.sidebar.bu\n    st.session_state[\"messages\"] = [{\"role\": \"assistan\nfor msg in st.session_state.messages:\n    st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n6. Finally, define the logic of the application whenever a user makes a\nquery:\nif user_query:\n    st.session_state.messages.append({\"role\": \"user\", \n    st.chat_message(\"user\").write(user_query)\n    with st.chat_message(\"assistant\"):\n        st_cb = StreamlitCallbackHandler(st.container(\n        response = agent_executor.run(user_query, call\n        st.session_state.messages.append({\"role\": \"ass\n        st.write(response)\nYou can run your application in the terminal with the streamlit run\ncopilot.py command. The final web page looks as follows:\n",
      "page_number": 300,
      "chapter_number": 36,
      "summary": "We will need this tool\nto produce the matplotlib graphs from the SQL query’s results Key topics include tools, agent, and python. Adding further tools\nIn order to make our DBCopilot more versatile, there are two further\ncapabilities we need to add:\n PythonREPLTool: This tool allows you to interact with the Python\nprogramming language using natural language.",
      "keywords": [
        "agent",
        "tools",
        "Python",
        "REPL",
        "file",
        "sql",
        "prompt",
        "top",
        "toolkit",
        "query",
        "file system",
        "best-selling alb print",
        "llm",
        "SQL Database-specific agent",
        "Python code"
      ],
      "concepts": [
        "tools",
        "agent",
        "python",
        "run",
        "actions",
        "result",
        "working",
        "role",
        "databases",
        "printed"
      ],
      "similar_chapters": [
        {
          "book": "Python Data Analysis 3rd",
          "chapter": 9,
          "title": "Plotting and Visualization",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 40,
          "title": "Segment 40 (pages 363-372)",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 38,
          "title": "Segment 38 (pages 325-336)",
          "relevance_score": 0.49,
          "method": "sentence_transformers"
        },
        {
          "book": "Microservice APIs Using Python Flask FastAPI",
          "chapter": 24,
          "title": "Segment 24 (pages 232-239)",
          "relevance_score": 0.47,
          "method": "sentence_transformers"
        },
        {
          "book": "Microservice APIs Using Python Flask FastAPI",
          "chapter": 27,
          "title": "Segment 27 (pages 261-276)",
          "relevance_score": 0.47,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 308-315)",
      "start_page": 308,
      "end_page": 315,
      "detection_method": "topic_boundary",
      "content": "Figure 8.8: Screenshot of the front-end of DBCopilot\n\n\nThanks to the StreamlitCallbackHandler module, we can also expand each\naction the agent took, for example:\nFigure 8.9: Illustration of the agent’s actions during the chain\nWith just a few lines of code, we were able to set up a simple front-end for\nour DBCopilot with a conversational user interface.\nSummary\nIn this chapter, we saw how LLMs are not only capable of interacting with\ntextual and unstructured data, but also with structured and numeric data.\nThis is made possible because of two main elements: the natural capabilities\nof LLMs and, more generally, LFMs for understanding a problem’s\n\n\nstatement, planning a resolution, and acting as reasoning engines, as well as\na set of tools that extend LLMs’ capabilities with domain-specific skills.\nIn this case, we mainly relied upon LangChain’s SQL Database toolkit,\nwhich connects the Agent to an SQL database with a curated prompt.\nFurthermore, we extended the Agent’s capabilities even further, making it\nable to generate matplotlib graphs, with the Python REPL tool, and save the\noutput to our local file system with the File Management tool.\nIn the next chapter, we are going to delve even deeper into the analytical\ncapabilities of LLMs. More specifically, we are going to cover their\ncapabilities of working with code.\nReferences\nChinook Database:\nhttps://github.com/lerocha/chinook-\ndatabase/tree/master/ChinookDatabase/DataSources\nLangChain File system tool:\nhttps://python.langchain.com/docs/integrations/t\nools/filesystem\nLangChain Python REPL tool:\nhttps://python.langchain.com/docs/integrations/t\noolkits/python\nJoin our community on\nDiscord\n\n\nJoin our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\nOceanofPDF.com\n\n\n9\nWorking with Code\nIn this chapter, we are going to cover another great capability of Large\nLanguage Models, that is, working with programming languages. In the\nprevious chapter, we’ve already seen a glimpse of this capability, namely,\nSQL query generation in a SQL database. In this chapter, we are going to\nexamine the other ways in which LLMs can be used with code, from\n“simple” code generation to interaction with code repositories and, finally, to\nthe possibility of letting an application behave as if it were an algorithm. By\nthe end of this chapter, you will be able to leverage LLMs to code-related\nprojects, as well as build LLM-powered applications with natural language\ninterfaces to work with code.\nThroughout this chapter, we will cover the following topics:\nAnalysis of the main LLMs with top-performing code capabilities\nUsing LLMs for code understanding and generation\nBuilding LLM-powered agents to “act as” algorithms\nLeveraging Code Interpreter\nTechnical requirements\nTo complete the tasks in this chapter, you will need the following:\nA Hugging Face account and user access token.\n\n\nAn OpenAI account and user access token.\nPython 3.7.1 or a later version.\nPython packages. Make sure you have the following Python packages\ninstalled: langchain, python-dotenv, huggingface_hub, streamlit,\ncodeinterpreterapi, and jupyter_kernel_gateway. Those can be\neasily installed via pip install in your terminal.\nYou can find all the code and examples in the book’s GitHub repository at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications.\nChoosing the right LLM for\ncode\nIn Chapter 3, we described a decision framework to use in order to decide\nthe proper LLM for a given application. Generally speaking, all LLMs are\nendowed with knowledge of code understanding and generation; however,\nsome of them are particularly specialized in doing so. More specifically,\nthere are some evaluation benchmarks – such as the HumanEval – that are\nspecifically tailored to assessing LLMs’ capabilities of working with code.\nThe leaderboard of HumanEval One is a good source for determining the\ntop-performing models, available at\nhttps://paperswithcode.com/sota/code-generation-on-\nhumaneval. HumanEval is a benchmark introduced by OpenAI to assess\nthe code generation capabilities of LLMs, where the model completes\nPython functions based on their signature and docstring. It has been used to\nevaluate models like Codex, demonstrating its effectiveness in measuring\nfunctional correctness.\n\n\nIn the following screenshot, you can see the situation of the leaderboard as\nof January 2024:\nFigure 9.1: HumanEval benchmark in January 2024\nAs you can see, the majority of the models are fine-tuned versions of the\nGPT-4 (as well as the GPT-4 itself), as it is the state-of-the-art LLM in\nbasically all the domains. Nevertheless, there are many open-source models\nthat reached stunning results in the field of code understanding and\ngeneration, some of which will be covered in the next sections. Another\nbenchmark is Mostly Basic Programming Problems (MBPP), a dataset of\n974 programming tasks in Python, designed to be solvable by entry-level\nprogrammers. Henceforth, when choosing your model for a code-specific\ntask, it might be useful to have a look at these benchmarks as well as other\nsimilar code metrics (we will see throughout the chapter some further\nbenchmarks for code-specific LLMs).\n\n\nStaying within the scope of coding, below you can find three additional\nbenchmarks often used in the market:\nMultiPL-E: An extension of HumanEval to many other languages,\nsuch as Java, C#, Ruby, and SQL.\nDS-1000: A data science benchmark that tests if the model can write\ncode for common data analysis tasks in Python.\nTech Assistant Prompt: A prompt that tests if the model can act as a\ntechnical assistant and answer programming-related requests.\nIn this chapter, we are going to test different LLMs: two code-specific\n(CodeLlama and StarCoder) and one general-purpose, yet also with\nemerging capabilities in the field of code generation (Falcon LLM).\nCode understanding and\ngeneration\nThe first experiment we are going to run will be code understanding and\ngeneration leveraging LLMs. This simple use case is at the base of the many\nAI code assistants that were developed since the launch of ChatGPT, first\namong all the GitHub Copilot.\nDefinition\nGitHub Copilot is an AI-powered tool that assists developers\nin writing code more efficiently. It analyzes code and\ncomments to provide suggestions for individual lines and\nentire functions. The tool is developed by GitHub, OpenAI,\nand Microsoft and supports multiple programming\nlanguages. It can perform various tasks such as code\n",
      "page_number": 308,
      "chapter_number": 37,
      "summary": "Summary\nIn this chapter, we saw how LLMs are not only capable of interacting with\ntextual and unstructured data, but also with structured and numeric data Key topics include code, coding, and python.",
      "keywords": [
        "code",
        "LLMs",
        "Python",
        "code understanding",
        "Python REPL tool",
        "SQL Database",
        "capabilities",
        "generation",
        "code generation",
        "SQL",
        "Python REPL",
        "tool",
        "Database",
        "agent",
        "StreamlitCallbackHandler module"
      ],
      "concepts": [
        "code",
        "coding",
        "python",
        "llms",
        "programming",
        "specifically",
        "models",
        "capable",
        "capabilities",
        "capability"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 38,
          "title": "Segment 38 (pages 320-327)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 56,
          "title": "Segment 56 (pages 484-491)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "Segment 28 (pages 232-240)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 316-325)",
      "start_page": 316,
      "end_page": 325,
      "detection_method": "topic_boundary",
      "content": "completion, modification, explanation, and technical\nassistance.\nIn this experiment, we are going to try three different models: Falcon LLM,\nwhich we already explored in Chapter 3; CodeLlama, a fine-tuned version of\nMeta AI’s Llama; and StarCoder, a code-specific model that we are going to\ninvestigate in the upcoming sections.\nSince those models are pretty heavy to run on a local machine, for this\npurpose I’m going to use a Hugging Face Hub Inference Endpoint, with a\nGPU-powered virtual machine. You can link one model per Inference\nEndpoint and then embed it in your code, or use the convenient library\nHuggingFaceEndpoint, available in LangChain.\nTo start using your Inference Endpoint, you can use the following code:\nllm = HuggingFaceEndpoint(endpoint_url = \"your_endpoint_url\", ta\n        model_kwargs = {\"max_new_tokens\": 1100})\nAlternatively, you can copy and paste the Python code provided on your\nendpoint’s webpage at\nhttps://ui.endpoints.huggingface.co/user_name/endpo\nints/your_endpoint_name:\n\n\nFigure 9.2: User interface of the Hugging Face Inference Endpoint\nTo create your Hugging Face Inference Endpoint, you can follow the\ninstructions at https://huggingface.co/docs/inference-\nendpoints/index.\nYou can always leverage the free Hugging Face API as described in Chapter\n4, but you have to expect some latency when running the models.\nFalcon LLM\nFalcon LLM is an open-source model developed by Abu Dhabi’s\nTechnology Innovation Institute (TII) and launched on the market in May\n2023. It is an autoregressive, decoder-only transformer, trained on 1 trillion\ntokens, and has 40 billion parameters (although it has also been released as a\nlighter version with 7 billion parameters). As discussed in Chapter 3,\n“small” language models are a representation of a new trend of LLMs,\n\n\nconsisting of building lighter models (with fewer parameters) that focus\ninstead on the quality of the training dataset.\nTo start using Falcon LLM, we can follow these steps:\n1. We can leverage the HuggingFaceHub wrapper available in LangChain\n(remember to set the Hugging Face API in the .env file, passing your\nsecrets as os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] =\nHUGGINGFACEHUB_API_TOKEN):\nfrom langchain import HuggingFaceHub\nfrom langchain import PromptTemplate, LLMChain\nimport os\nload_dotenv()hugging_face_api = os.environ[\"HUGGINGFAC\nrepo_id = \"tiiuae/falcon-7b-instruct\"\nllm = HuggingFaceHub(\n    repo_id=repo_id,  model_kwargs={\"temperature\": 0.2\n)\n2. Now that we’ve initialized the model, let’s ask it to generate the code\nfor a simple webpage:\nprompt = \"\"\"\nGenerate a short html code to a simple webpage with a \n<!DOCTYPE html>\n<html>\n\"\"\"\nprint(llm(prompt))\nThe following is the corresponding output:\n<head>\n    <title>My Webpage</title>\n</head>\n\n\n<body>\n    <h1>My Webpage</h1>\n    <h2>Subheader</h2>\n    <p>This is the text body.</p>\n</body>\n</html>\n3. If you save it as an HTML file and execute it, the result will look like\nthe following:\nFigure 9.3: Sample webpage generated by FalconLLM\n4. We can also try to generate a Python function to generate random\npasswords:\nprompt = \"\"\"\nGenerate a python program that create random password \n\"\"\"\nprint(llm(prompt))\nHere is our output:\nimport random\ndef generate_password():\n    chars = \"abcdefghijklmnopqrstuvwxyz0123456789\"\n\n\n    lenght = 12\n    num = random.randint(1, 9)\n    cap = random.randint(1, 9)\n    password = \"\"\n    for i in range(lenght):\n        password += chars[random.randint(0, 9)]\n    password += num\n    password += cap\n    return password\nprint(generate_password())\nWe now have a function named generate_password(), which uses\nrandom functions to generate a password as per our prompt.\n5. Finally, let’s do the opposite, asking the model to explain to us the\nabove code:\nprompt = \"\"\"\nExplain to me the following code:\ndef generate_password():\n    chars = \"abcdefghijklmnopqrstuvwxyz0123456789\"\n    lenght = 12\n    num = random.randint(1, 9)\n    cap = random.randint(1, 9)\n    password = \"\"\n    for i in range(lenght):\n        password += chars[random.randint(0, 9)]\n    password += num\n    password += cap\n    return password\nprint(generate_password())\n\"\"\"\nprint(llm(prompt))\nHere is the obtained output:\n<p>The code generates a random password of length 12 charac\n\n\nOverall, even if not code-specific, the model was able to correctly perform\nall the tasks. Note also that this is the “light” version of the model (7 billion\nparameters), yet its performance is great.\nLet’s now investigate the capabilities of CodeLlama.\nCodeLlama\nCodeLlama is a family of LLMs for code based on Llama 2, which is a\ngeneral-purpose language model developed by Meta AI (as discussed in\nChapter 3). CodeLlama can generate and discuss code in various\nprogramming languages, such as Python, C++, Java, PHP, and more.\nCodeLlama can also perform infilling, which is the ability to fill in missing\nparts of code based on the surrounding context, as well as follow instructions\ngiven in natural language and produce code that matches the desired\nfunctionality.\nThe model comes in three sizes (7B, 13B, and 34B parameters) and three\nflavors (base model, Python fine-tuned, and instruction-tuned) to cover a\nwide range of applications. CodeLlama is trained on sequences of 16k\ntokens and can handle inputs with up to 100k tokens.\nIn the model paper “Code Llama: Open Foundation Models for Code” by\nRozière Baptiste et al, released in August 2023, the authors describe how the\nvarious models were tested against some of the most popular evaluation\nbenchmarks in the domain of code understanding and generation, including\nHumanEval and MBPP, according to which CodeLlama models achieved a\nscore up to 53% and 55%, respectively. On top of those remarkable results, it\nis stunning that the Python fine-tuned CodeLlama’s smallest size (7 billion\nparameters) outperformed the largest version of Llama 2 (70 billion\nparameters) on HumanEval and MBPP.\n\n\nNow, let’s run some tests with this model. As per the previous section, we\ncan initialize the model leveraging either the Hugging Face Inference API\n(pay per use) or the free Hugging Face API (with the constraint of higher\nlatency). You can consume it as follows:\nrepo_id = \"codellama/CodeLlama-7b-Instruct-hf\"\nllm = HuggingFaceHub(    repo_id=repo_id,  model_kwargs={\"temper\n)\nLet’s now test it with some code tasks. The first task will be that of\noptimizing Python code so that it runs more efficiently. Let’s see how our\nmodel performs in this task. In the following code snippet, we simply\nprompt the model to regenerate the provided code in a more efficient way:\nprompt = \"\"\"\nRegenerate the below code in a more efficient way.\n```\ndef factorial(n):\n    result = 1\n    for i in range(1, n + 1):\n        result *= i\n    return result\n# Example usage:\nn = 5\nprint(\"Factorial of\", n, \"is\", factorial(n))\n```\n\"\"\"\nprint(llm(prompt))\nHere is our output:\n### Solution\n```\ndef factorial(n):\n\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n - 1)\n```\n### Explanation\nThe solution is a recursive function that calls itself until the \n### References\n- [Wikipedia](https://en.wikipedia.org/wiki/Factorial)\n- [Math is Fun](https://www.mathsisfun.com/numbers/factorial.html\n- [Mathematics Stack Exchange](https://math.stackexchange.com/que\n- [Mathematics Stack Exchange](https://math.stackexchange.com/que\nAs you can see, the model was able to use a recursive approach, which is\nmore efficient and “Pythonic.” It also provides a reference for the user to\ndive deeper into the mathematical theory behind the function.\nNext, let’s leverage the model’s completion capabilities by initializing a\nfunction to remove non-ASCII characters.\nDefinition\nAmerican Standard Code for Information Interchange\n(ASCII) is a character encoding standard that uses 7 bits to\nrepresent 128 characters, such as letters, digits, punctuation\nmarks, and control codes.\nNon-ASCII characters are those that are not part of the\nASCII standard and use more than 7 bits to encode. They\ninclude special characters such as letters with accents,\nglyphs, ideograms, and mathematical symbols. Non-ASCII\ncharacters can be encoded using different standards, such as\nUnicode, ISO 8859-1, Windows-1252, etc.\n\n\nFor example, the letter é is a non-ASCII character that can be\nencoded using Unicode as U+00E9 or using Windows-1252\nas 0xE9.\nAccordingly, here is the code to generate the function:\nprompt = '''\nDefine a python function that remove non-ascii character from a \n'''\nprint(llm(prompt))\nThe following is the function that we receive as the output:\ndef remove_non_ascii(string):\n    return string.encode('ascii', 'ignore').decode('utf-8')\nLet’s now leverage the model as a bug fixer, prompting it with the wrong\nfunction and also asking it to provide an explanation of why it is wrong and\nhow it can be fixed:\nprompt = \"\"\"\nIdentify the bug in the following code:.\n```\n#wrong function\nimport random\na = random.randint(1, 12)\nb = random.randint(1, 12)\nfor i in range(10):\n    question = \"What is \" + a + \" x \" + b + \"? \"\n    answer = input(question)\n    if answer = a * b:\n        print (Well done!)\n    else:\n        print(\"No.\")\n\n\n```\nExplanation:\n\"\"\"\nprint(llm(prompt))\nHere is our output:\n- The function is using the wrong syntax for the `randint` functi\n- The `for` loop is using the wrong syntax. The correct syntax is\n- The `question` and `answer` variables are not being used in the\n- The `if` statement is not being used correctly. It should be `i\n- The `print` statement is not being used correctly. It should be\nFinally, let’s ask the model in natural language to generate specific Python\ncode for a given task that, in our example, will be that of writing a function\nthat finds the longest substring of a given string containing only unique\ncharacters:\nprompt = \"\"\"\nWrite a Python function that finds the longest substring of a gi\n\"\"\"\nprint(llm(prompt))\nWe then get the following function as our output:\ndef longest_unique_substring(s):\n    unique_count = 0\n    longest_substring = \"\"\n    for i in range(1, len(s)):\n        if s[i] not in unique_count:\n            unique_count += 1\n            longest_substring = s[i]\n    return longest_substring\n",
      "page_number": 316,
      "chapter_number": 38,
      "summary": "This chapter covers segment 38 (pages 316-325). Key topics include codes, models, and prompt. You can link one model per Inference\nEndpoint and then embed it in your code, or use the convenient library\nHuggingFaceEndpoint, available in LangChain.",
      "keywords": [
        "Hugging Face Inference",
        "Hugging Face API",
        "Hugging Face",
        "Face Inference Endpoint",
        "code",
        "model",
        "Inference Endpoint",
        "Face Inference API",
        "Face Inference",
        "Hugging Face Hub",
        "Face Hub Inference",
        "password",
        "LLM",
        "Face API",
        "Hub Inference Endpoint"
      ],
      "concepts": [
        "codes",
        "models",
        "prompt",
        "random",
        "function",
        "functions",
        "functionality",
        "python",
        "pythonic",
        "html"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 59,
          "title": "Segment 59 (pages 511-519)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 42,
          "title": "Segment 42 (pages 853-871)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 13,
          "title": "Segment 13 (pages 104-111)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 54,
          "title": "Segment 54 (pages 466-473)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 474-483)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 326-333)",
      "start_page": 326,
      "end_page": 333,
      "detection_method": "topic_boundary",
      "content": "As per the Falcon LLM, in this case we used the light version of the model\n(7 billion parameters), still obtaining great results. This is a perfect example\nof how the task you want to address with your application must be a factor in\ndeciding what LLM to use: if you are only interested in code generation,\ncompletion, infilling, debugging, or any other code-related tasks, a light and\nopen-source model could be more than enough, rather than 70 billion\nparameters of a state-of-the-art GPT-4.\nIn the next section, we are going to cover the third and last LLM in the\ncontext of code generation and understanding.\nStarCoder\nThe StarCoder model is an LLM for code that can perform various tasks,\nsuch as code completion, code modification, code explanation, and technical\nassistance. It was trained on permissively licensed data from GitHub,\nincluding from 80+ programming languages, Git commits, GitHub issues,\nand Jupyter notebooks. It has a context length of over 8,000 tokens, which\nenables it to process more input than any other open-source language model.\nIt also has an improved license that simplifies the process for companies to\nintegrate the model into their products.\nThe StarCoder model was evaluated on several benchmarks that test its\nability to write and understand code in different languages and domains,\nincluding the aforementioned HumanEval and MBPP, where the model\nscored, respectively, 33.6% and 52.7%. Additionally, it was tested against\nMultiPL-E (where the model matched or outperformed the code-cushman-\n001 model from OpenAI on many languages), the DS-1000 (where the\nmodel clearly beat the code-cushman-001 model as well as all other open-\n\n\naccess models), and the Tech Assistant Prompt (where the model was able to\nrespond to various queries with relevant and accurate information).\nAccording to a survey published on May 4 2023 by Hugging Face,\nStarCoder demonstrated great capabilities compared to other models, using\nHumanEval and MBPP as benchmarks. You can see an illustration of this\nstudy below:\nFigure 9.4: Results of evaluation benchmarks for various LLMs. Source:\nhttps://huggingface.co/blog/starcoder\nTo start using StarCoder, we can follow these steps:\n\n\n1. We can leverage the HuggingFaceHub wrapper available in LangChain\n(remember to set the Hugging Face API in the .env file):\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\nhugging_face_api = os.environ[\"HUGGINGFACEHUB_API_TOKE\n2. Let’s set the repo_id for the StarCoder model and initialize it:\nfrom langchain import HuggingFaceHub\nfrom langchain import PromptTemplate, LLMChain\nrepo_id = \"bigcode/starcoderplus\"\nllm = HuggingFaceHub(\n    repo_id=repo_id,  model_kwargs={\"temperature\": 0.2\n)\nNote\nStarCoder is a gated model on the Hugging Face Hub,\nmeaning that you will need to request access directly from\nthe bigcode/starcoderplus repo before being able to connect\nto it.\nNow that we’re set up, let’s start asking our model to compile some code. To\nstart with, we will ask it to generate a Python function to generate the nth\nFibonacci number:\nprompt = \"\"\"\nHow can I write a Python function to generate the nth Fibonacci \n\n\n\"\"\"\nprint(llm(prompt))\nDefinition\nThe Fibonacci sequence is a mathematical series that begins\nwith 0 and 1, and each subsequent number is the sum of the\ntwo preceding numbers. For instance, the first 10 numbers of\nthe Fibonacci sequence are 0, 1, 1, 2, 3, 5, 8, 13, 21, and 34.\nThere are different ways to compute the nth Fibonacci\nnumber, which is denoted by F(n). One way is to use a\nrecursive formula:\nThis means that to find F(n), we need to find F(n-1) and F(n-\n2) first, and then add them together. This works for any n\ngreater than or equal to 2. For n equal to 0 or 1, we simply\nreturn n as the answer.\nWe then see the following output:\n\n\nFigure 9.5: Example of Fibonacci functions generated by StarCode\nAs you can see, it also proposed different approaches to solve the problem,\nalongside the explanation.\nLet’s now ask the model to generate a webpage to play tic tac toe against the\ncomputer:\nprompt = \"\"\"\nGenerate the html code for a single page website that let the us\n \"\"\"\nprint(llm(prompt))\nHere is the corresponding output:\n ## How to use\n```\ngit clone https://github.com/Mohamed-Elhawary/tic-tac-toe.git\ncd tic-tac-toe\n\n\npython3 -m http.server\n```\n## License\n [MIT](https://choosealicense.com/licenses/mit/)\n<|endoftext|>\nInterestingly enough, the model in this case didn’t generate the whole code;\nrather, it gave the instructions to clone and run a git repository that can\nachieve this result.\nFinally, StarCoder is also available as an extension in VS Code to act as your\ncode copilot. You can find it as HF Code Autocomplete, as shown in the\nfollowing screenshot:\n\n\nFigure 9.6: Hugging Face Code Autocomplete extension, powered by StarCoder\nOnce enabled, you can see that, while compiling your code, StarCoder will\nprovide suggestions to complete the code. For example:\n\n\nFigure 9.7: Screenshot of a suggested completion, given a function description\nAs you can see, I commented my code, describing a function to generate the\nnth Fibonacci number, and then started defining the function. Automatically,\nI’ve been provided with the StarCoder auto-completion suggestion.\nCode understanding and generation are great capabilities of LLMs. On top of\nthose capabilities, there are further applications that we can think about,\ngoing beyond code generation. In fact, the code can be seen also as a\nbackend reasoning tool to propose solutions to complex problems, such as an\nenergy optimization problem rather than an algorithm task. To do this, we\ncan leverage LangChain to create powerful agents that can act as if they\nwere algorithms. In the upcoming section, we will see how to do so.\nAct as an algorithm\nSome problems are complex by definition and difficult to solve leveraging\n“only” LLMs’ analytical reasoning skills. However, LLMs are still\n",
      "page_number": 326,
      "chapter_number": 39,
      "summary": "This chapter covers segment 39 (pages 326-333). Key topics include code, model. In the next section, we are going to cover the third and last LLM in the\ncontext of code generation and understanding.",
      "keywords": [
        "model",
        "code",
        "Falcon LLM",
        "Hugging Face",
        "StarCoder",
        "StarCoder model",
        "Fibonacci",
        "Hugging Face Code",
        "LLM",
        "billion parameters",
        "Fibonacci number",
        "Hugging Face API",
        "nth Fibonacci",
        "Hugging",
        "Face"
      ],
      "concepts": [
        "code",
        "model",
        "git",
        "licensed",
        "license",
        "completion",
        "complete",
        "reasoning",
        "function",
        "prompt"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "Segment 28 (pages 232-240)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 474-483)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 15,
          "title": "Segment 15 (pages 132-139)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 33,
          "title": "Segment 33 (pages 314-325)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 1,
          "title": "Segment 1 (pages 2-10)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 334-341)",
      "start_page": 334,
      "end_page": 341,
      "detection_method": "topic_boundary",
      "content": "intelligent enough to understand the problems overall and leverage their\ncoding capabilities to solve them.\nIn this context, LangChain provides a tool that empowers the LLM to reason\n“in Python,” meaning that the LLM-powered agent will leverage Python to\nsolve complex problems. This tool is the Python REPL, which is a simple\nPython shell that can execute Python commands. The Python REPL is\nimportant because it allows users to perform complex calculations, generate\ncode, and interact with language models using Python syntax. In this section,\nwe will cover some examples of the tool’s capabilities.\nLet’s first initialize our agent using the create_python_agent class in\nLangChain. To do so, we will need to provide this class with an LLM and a\ntool, which, in our example, will be the Python REPL:\nimport os\nfrom dotenv import load_dotenv\nfrom langchain.agents.agent_types import AgentType\nfrom langchain.chat_models import ChatOpenAI\n from langchain_experimental.agents.agent_toolkits.python.base i\nfrom langchain_experimental.tools import PythonREPLTool\nload_dotenv()\nopenai_api_key = os.environ['OPENAI_API_KEY']\nmodel = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\nagent_executor = create_python_agent(\n    llm=model,\n    tool=PythonREPLTool(),\n    verbose=True,\n    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n)\nAs always, before starting to work with the agent, let’s first inspect the\ndefault prompt:\n\n\nprint(agent_executor.agent.llm_chain.prompt.template)\nHere is our output:\nFigure 9.8: Default prompt of the Python agent\nNow, let’s start with an easy query, asking the model to generate a scatter\nplot based on sample attributes of basketball players:\nquery = \"\"\"\nIn a different basketball game, we have the following player sta\n- Player A: 38 points, 10 rebounds, 7 assists\n- Player B: 28 points, 9 rebounds, 6 assists\n- Player C: 19 points, 6 rebounds, 3 assists\n- Player D: 12 points, 4 rebounds, 2 assists\n- Player E: 7 points, 2 rebounds, 1 assist\nCould you create a scatter plot graph in Seaborn talk mode for e\n\"\"\"\nagent_executor.run(query)\n\n\nWe then get the following output:\nInvoking: `Python_REPL` with `import seaborn as sns\nimport matplotlib.pyplot as plt\n# Player stats\nplayers = ['Player A', 'Player B', 'Player C', 'Player D', 'Playe\npoints = [38, 28, 19, 12, 7]\nrebounds = [10, 9, 6, 4, 2]\n# Create scatter plot\nsns.scatterplot(x=rebounds, y=points, marker='o')\n# Label each point with player's name\nfor i, player in enumerate(players):\n    plt.text(rebounds[i], points[i], player, ha='center', va='bot\n# Set title\nplt.title('Team Players')\n# Show the plot\nplt.show()`\nThis output is accompanied by the following graph based on the players’\nstatistics:\n\n\nFigure 9.9: Sample plot generated by the Python agent\nLet’s look at another example. Say we want to predict the price of a house\nbased on some features, such as the number of bedrooms or the size of the\nhouse. To do so, we can ask our agent to design and train a model to give us\nthe result of a given house. For example, let’s consider the following prompt:\nquery = \"\"\"\nI want to predict the price of a house given the following infor\n- the number of rooms\n- the number of bathrooms\n- the size of the house in square meters\nDesign and train a regression model to predict the price of a ho\nOnce the model is trained, tell me the price of a house with the\n- 2 rooms\n\n\n- 1 bathroom\n- 100 square meters\n\"\"\"\nagent_executor.run(query)\nHere, we ask the agent to train a regression model on synthetic data\n(representative of houses with various configurations of rooms, bathrooms,\nand area, each with an associated price as a dependent variable) to give us\nthe estimated price of a house with the above features. Let’s see the output:\n> Entering new AgentExecutor chain...\nInvoking: `Python_REPL` with `import numpy as np\nfrom sklearn.linear_model import LinearRegression\n# Generate synthetic data\nnp.random.seed(0)\nX = np.random.rand(100, 3)  # 100 houses with 3 features: rooms, \ny = 100000 * X[:, 0] + 200000 * X[:, 1] + 300000 * X[:, 2] + 5000\n# Train the regression model\nmodel = LinearRegression()\nmodel.fit(X, y)\n# Predict the price of a house with the given features\nfeatures = np.array([[2, 1, 100]])\npredicted_price = model.predict(features)\npredicted_price`\nresponded: {content}\nThe predicted price of a house with 2 rooms, 1 bathroom, and 100 \n> Finished chain.\n'The predicted price of a house with 2 rooms, 1 bathroom, and 100\nAs you can see, the agent was able to generate synthetic training data, train a\nproper regression model using the sklearn libraries, and predict with the\nmodel the price of the house we provided.\nWith this approach, we can program an agent to act as an algorithm in real-\ntime scenarios. Imagine, for example, that we want to design an agent that is\n\n\ncapable of solving optimization problems in a smart building environment.\nThe goal is to optimize the Heating, Ventilation and Air Conditioning\n(HVAC) setpoints in the building to minimize energy costs while ensuring\noccupant comfort. Let’s define the variables and constraints of the problem:\nthe objective is to adjust the temperature setpoints within the specified\ncomfort ranges for each of the three zones while considering the varying\nenergy costs per degree, per hour.\nThe goal is to strike a balance between energy efficiency and occupant\ncomfort. Below, you can find a description of the problem and also the\ninitialization of our variables and constraints (energy cost per zone, initial\ntemperature per zone, and comfort range per zone):\nquery = \"\"\"\n**Problem**:\nYou are tasked with optimizing the HVAC setpoints in a smart bui\n- Zone 1: Energy cost = $0.05 per degree per hour\n- Zone 2: Energy cost = $0.07 per degree per hour\n- Zone 3: Energy cost = $0.06 per degree per hour\nYou need to find the optimal set of temperature setpoints for th\n- Zone 1: 72°F\n- Zone 2: 75°F\n- Zone 3: 70°F\nThe comfort range for each zone is as follows:\n- Zone 1: 70°F to 74°F\n- Zone 2: 73°F to 77°F\n- Zone 3: 68°F to 72°F\n**Question**:\nWhat is the minimum total energy cost (in dollars per hour) you \n\"\"\"\nagent_executor.run(query)\nWe then get the following output (you can find the whole reasoning chain in\nthe book’s GitHub repository):\n\n\n> Entering new AgentExecutor chain...\nInvoking: `Python_REPL` with `import scipy.optimize as opt\n# Define the cost function\ndef cost_function(x):\n    zone1_temp = x[0]\n    zone2_temp = x[1]\n    zone3_temp = x[2]\n   \n    # Calculate the energy cost for each zone\n    zone1_cost = 0.05 * abs(zone1_temp - 72)\n    zone2_cost = 0.07 * abs(zone2_temp - 75)\n    zone3_cost = 0.06 * abs(zone3_temp - 70)\n[…]\n> Finished chain.\n'The minimum total energy cost that can be achieved by adjusting \nThe agent was able to solve the smart building optimization problem, finding\nthe minimum total energy cost, given some constraints. Staying in the scope\nof optimization problems, there are further use cases that these models could\naddress with a similar approach, including:\nSupply chain optimization: Optimize the logistics and distribution of\ngoods to minimize transportation costs, reduce inventory, and ensure\ntimely deliveries.\nPortfolio optimization: In finance, use algorithms to construct\ninvestment portfolios that maximize returns while managing risk.\nRoute planning: Plan optimal routes for delivery trucks, emergency\nservices, or ride-sharing platforms to minimize travel time and fuel\nconsumption.\nManufacturing process optimization: Optimize manufacturing\nprocesses to minimize waste, energy consumption, and production costs\nwhile maintaining product quality.\n\n\nHealthcare resource allocation: Allocate healthcare resources like\nhospital beds, medical staff, and equipment efficiently during a\npandemic or other healthcare crisis.\nNetwork routing: Optimize data routing in computer networks to\nreduce latency, congestion, and energy consumption.\nFleet management: Optimize the use of a fleet of vehicles, such as\ntaxis or delivery vans, to reduce operating costs and improve service\nquality.\nInventory management: Determine optimal inventory levels and\nreorder points to minimize storage costs while preventing stockouts.\nAgricultural planning: Optimize crop planting and harvesting\nschedules based on weather patterns and market demand to maximize\nyield and profits.\nTelecommunications network design: Design the layout of\ntelecommunications networks to provide coverage while minimizing\ninfrastructure costs.\nWaste management: Optimize routes for garbage collection trucks to\nreduce fuel consumption and emissions.\nAirline crew scheduling: Create efficient flight crew schedules that\nadhere to labor regulations and minimize costs for airlines.\nThe Python REPL agent is amazing; however, it comes with some caveats:\nIt does not allow for FileIO, meaning that it cannot read and write with\nyour local file system.\nIt forgets the variables after every run, meaning that you cannot keep\ntrace of your initialized variables after the model’s response.\nTo bypass these caveats, in the next section, we are going to cover an open-\nsource project built on top of the LangChain agent: the Code Interpreter API.\n",
      "page_number": 334,
      "chapter_number": 40,
      "summary": "In this context, LangChain provides a tool that empowers the LLM to reason\n“in Python,” meaning that the LLM-powered agent will leverage Python to\nsolve complex problems Key topics include important, players, and optimization.",
      "keywords": [
        "Python REPL",
        "Python",
        "Python REPL agent",
        "player",
        "energy cost",
        "agent",
        "zone",
        "model",
        "energy",
        "Python agent",
        "cost",
        "price",
        "house",
        "REPL",
        "total energy cost"
      ],
      "concepts": [
        "important",
        "players",
        "optimization",
        "optimize",
        "optimal",
        "python",
        "cost",
        "zones",
        "models",
        "agent"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 56,
          "title": "Segment 56 (pages 484-491)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 8,
          "title": "Segment 8 (pages 61-70)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 42-49)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 40,
          "title": "Segment 40 (pages 420-428)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 342-350)",
      "start_page": 342,
      "end_page": 350,
      "detection_method": "topic_boundary",
      "content": "Leveraging Code Interpreter\nThe name “Code Interpreter” was coined by OpenAI, referring to the\nrecently developed plugin for ChatGPT. The Code Interpreter plugin allows\nChatGPT to write and execute computer code in various programming\nlanguages. This enables ChatGPT to perform tasks such as calculations, data\nanalysis, and generating visualizations.\nThe Code Interpreter plugin is one of the tools designed specifically for\nlanguage models with safety as a core principle. It helps ChatGPT access up-\nto-date information, run computations, or use third-party services. The\nplugin is currently in private beta and is available for selected developers\nand ChatGPT Plus users.\nWhile OpenAI’s Code Interpreter still doesn’t offer an API, there are some\nopen-source projects that adapted the concept of this plugin in an open-\nsource Python library. In this section, we are going to leverage the work of\nShroominic, available at\nhttps://github.com/shroominic/codeinterpreter-api.\nYou can install it via pip install codeinterpreterapi.\nAccording to the blog post published by Shroominic, the author of the Code\nInterpreter API (which you can read at\nhttps://blog.langchain.dev/code-interpreter-api/), it\nis based on the LangChain agent OpenAIFunctionsAgent.\nDefinition\nOpenAIFunctionsAgent is a type of agent that can use the\nOpenAI functions’ ability to respond to the user’s prompts\nusing an LLM. The agent is driven by a model that supports\n\n\nusing OpenAI functions, and it has access to a set of tools\nthat it can use to interact with the user.\nThe OpenAIFunctionsAgent can also integrate custom\nfunctions. For example, you can define custom functions to\nget the current stock price or stock performance using Yahoo\nFinance. The OpenAIFunctionsAgent can use the ReAct\nframework to decide which tool to use, and it can use\nmemory to remember the previous conversation interactions.\nThe API comes already with some tools, such as the possibility to navigate\nthe web to get up-to-date information.\nYet the greatest difference from the Python REPL tool that we covered in the\nprevious section is that the Code Interpreter API can actually execute the\ncode it generates. In fact, when a Code Interpreter session starts, a miniature\nof a Jupyter Kernel is launched on your device, thanks to the underlying\nPython execution environment called CodeBox.\nTo start using the code interpreter in your notebook, you can install all the\ndependencies as follows:\n!pip install \"codeinterpreterapi[all]\"\nIn this case, I will ask it to generate a plot of COVID-19 cases in a specific\ntime range:\nfrom codeinterpreterapi import CodeInterpreterSession\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\napi_key = os.environ['OPENAI_API_KEY']\n\n\n# create a session\nasync with CodeInterpreterSession() as session:\n    # generate a response based on user input\n    response = await session.generate_response(\n        \"Generate a plot of the evolution of Covid-19 from March\n    )\n    # output the response\nprint(\"AI: \", response.content)\n    for file in response.files:\n        file.show_image()\nHere is the generated output, including a graph that shows the number of\nglobal confirmed cases in the specified time period:\nAI:  Here is the plot showing the evolution of global daily confi\nFigure 9.10: Line chart generated by the Code Intepreter API\n\n\nAs you can see, the Code Interpreter answered the question with an\nexplanation as well as a plot.\nLet’s try another one, this time also leveraging its real-time capabilities of\nsearching for up-to-date information. In the following snippet, we ask the\nmodel to plot the price of the S&P 500 index over the last 5 days:\nasync with CodeInterpreterSession() as session:\n    # generate a response based on user input\n    response = await session.generate_response(\n        \"Generate a plot of the price of S&P500 index in the las\n    )\n    # output the response\nprint(\"AI: \", response.content)\n    for file in response.files:\n        file.show_image()\nWe then get the following output, together with a line graph showing the\nprice of the S&P 500 index over the last 5 days:\nAI:  Here is the plot of the S&P 500 index for the last 5 days. T\n\n\nFigure 9.11: S&P 500 index price plotted by the Code Interpreter API\nFinally, we can provide local files to the Code Interpreter so that it can\nperform some analyses on that specific data. For example, I’ve downloaded\nthe Titanic dataset from Kaggle at\nhttps://www.kaggle.com/datasets/brendan45774/test-\nfile. The Titanic dataset is a popular dataset for machine learning that\ndescribes the survival status of individual passengers on the Titanic. It\ncontains information such as age, sex, class, fare, and whether they survived\nor not.\nOnce the dataset had downloaded, I passed it as a parameter to the model as\nfollows:\n\n\nfrom codeinterpreterapi import CodeInterpreterSession, File\n#os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\nos.environ['OPENAI_API_KEY'] = \"sk-YIN03tURjJRYmhcmv0yIT3BlbkFJv\nos.environ['VERBOSE'] = \"True\"\nasync with CodeInterpreterSession() as session:\n        # define the user request\n        user_request = \"Analyze this dataset and plot something \n        files = [\n            File.from_path(\"drive/MyDrive/titanic.csv\"),\n        ]\n        # generate the response\n        response = await session.generate_response(\n            user_request, files=files\n        )\n        # output to the user\nprint(\"AI: \", response.content)\n        for file in response.files:\n            file.show_image()\nWe then get the following output:\nAI:  The plot shows the survival count based on the passenger cla\nThese are just a few examples of the kind of insights we can extr\n\n\nFigure 9.12: Sample plots generated by the Code Interpreter API\n\n\nAs you can see, the model was able to generate to bar charts showing the\nsurvival status grouped by sex (in the first plot) and then by class (in the\nsecond plot).\nThe Code Interpreter plugin, together with code-specific LLMs and the\nPython agent, are great examples of how LLMs are having a huge impact on\nthe world of software development. This can be summarized in two main\ncapabilities:\nLLMs can understand and generate code, since they have been trained\non a huge amount of programming languages, GitHub repos,\nStackOverflow conversations, and so on. Henceforth, along with\nnatural language, programming languages are part of their parametric\nknowledge.\nLLMs can understand a user’s intent and act as a reasoning engine to\nactivate tools like Python REPL or Code Interpreter, which are then\nable to provide a response by working with code.\nOverall, LLMs are going well beyond the elimination of the gap between\nnatural language and machine language: rather, they are integrating the two\nso that they can leverage each other to respond to a user’s query.\nSummary\nIn this chapter, we explored multiple ways in which LLMs can be leveraged\nto work with code. Armed with a refresher of how to evaluate LLMs and the\nspecific evaluation benchmarks to take into account when choosing an LLM\nfor code-related tasks, we delved into practical experimentations.\nWe started from the “plain vanilla” application that we have all tried at least\nonce using ChatGPT, which is code understanding and generation. For this\n\n\npurpose, we leveraged three different models – Falcon LLM, CodeLlama,\nand StarCoder – each resulting in very good results.\nWe then moved forward with the additional applications that LLMs’ coding\ncapabilities can have in the real world. In fact, we saw how code-specific\nknowledge can be used as a booster to solve complex problems, such as\nalgorithmic or optimization tasks. Furthermore, we covered how code\nknowledge can not only be used in the backend reasoning of an LLM but\nalso actually executed in a working notebook, leveraging the open-source\nversion of the Code Interpreter API.\nWith this chapter, we are getting closer to the end of Part 2. So far, we have\ncovered the multiple capabilities of LLMs, while always handling language\ndata (natural or code). In the next chapter, we will see how to go a step\nfurther toward multi-modality and build powerful multi-modal agents that\ncan handle data in multiple formats.\nReferences\nThe open-source version of the Code Interpreter API:\nhttps://github.com/shroominic/codeinterpreter-\napi\nStarCoder: https://huggingface.co/blog/starcoder\nThe LangChain agent for the Python REPL:\nhttps://python.langchain.com/docs/integrations/t\noolkits/python\nA LangChain blog about the Code Interpreter API:\nhttps://blog.langchain.dev/code-interpreter-api/\n",
      "page_number": 342,
      "chapter_number": 41,
      "summary": "This chapter covers segment 41 (pages 342-350). Key topics include code, coding, and python. The Code Interpreter plugin allows\nChatGPT to write and execute computer code in various programming\nlanguages.",
      "keywords": [
        "Code Interpreter API",
        "Code Interpreter",
        "Code Interpreter plugin",
        "Interpreter API",
        "Code",
        "Interpreter",
        "API",
        "Code Interpreter session",
        "Leveraging Code Interpreter",
        "Interpreter plugin",
        "Code Interpreter answered",
        "response",
        "plot",
        "user",
        "LLMs"
      ],
      "concepts": [
        "code",
        "coding",
        "python",
        "file",
        "generating",
        "generates",
        "generation",
        "languages",
        "output",
        "api"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents In Action",
          "chapter": 18,
          "title": "Segment 18 (pages 147-158)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 40,
          "title": "Segment 40 (pages 420-428)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 2,
          "title": "How Python Runs Programs",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 54,
          "title": "Segment 54 (pages 466-473)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 351-359)",
      "start_page": 351,
      "end_page": 359,
      "detection_method": "topic_boundary",
      "content": "The Titanic dataset:\nhttps://www.kaggle.com/datasets/brendan45774/tes\nt-file\nThe HF Inference Endpoint:\nhttps://huggingface.co/docs/inference-\nendpoints/index\nThe CodeLlama model card:\nhttps://huggingface.co/codellama/CodeLlama-7b-hf\nCode Llama: Open Foundation Models for Code, Rozière. B., et al\n(2023): https://arxiv.org/abs/2308.12950\nThe Falcon LLM model card:\nhttps://huggingface.co/tiiuae/falcon-7b-instruct\nThe StarCoder model card:\nhttps://huggingface.co/bigcode/starcoder\nJoin our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\n\n\nOceanofPDF.com\n\n\n10\nBuilding Multimodal\nApplications with LLMs\nIn this chapter, we are going beyond LLMs, to introduce the concept of\nmultimodality while building agents. We will see the logic behind the\ncombination of foundation models in different AI domains – language,\nimages, and audio – into one single agent that can adapt to a variety of tasks.\nBy the end of this chapter, you will be able to build your own multimodal\nagent, providing it with the tools and LLMs needed to perform various AI\ntasks.\nThroughout this chapter, we will cover the following topics:\nIntroduction to multimodality and large multimodal models (LMMs)\nExamples of emerging LMMs\nHow to build a multimodal agent with single-modal LLMs using\nLangChain\nTechnical requirements\nTo complete the tasks in this chapter, you will need the following:\nA Hugging Face account and user access token.\nAn OpenAI account and user access token.\n\n\nPython 3.7.1 or later version.\nPython packages. Make sure to have the following Python packages\ninstalled: langchain, python-dotenv, huggingface_hub, streamlit,\npytube, openai, and youtube_search. Those can be easily installed via\npip install in your terminal.\nYou can find all the code and examples in the book’s GitHub repository at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications.\nWhy multimodality?\nIn the context of Generative AI, multimodality refers to a model’s capability\nof processing data in various formats. For example, a multimodal model can\ncommunicate with humans via text, speech, images, or even videos, making\nthe interaction extremely smooth and “human-like.”\nIn Chapter 1, we defined large foundation models (LFMs) as a type of pre-\ntrained generative AI model that offers immense versatility by being\nadaptable for various specific tasks. LLMs, on the other hand, are a subset of\nfoundation models that are able to process one type of data: natural\nlanguage. Even though LLMs have proven to be not only excellent text\nunderstanders and generators but also reasoning engines to power\napplications and copilots, it soon became clear that we could aim at even\nmore powerful applications.\nThe dream is to have intelligent systems that are capable of handling\nmultiple data formats – text, images, audio, video, etc – always powered by\nthe reasoning engine, which makes them able to plan and execute actions\n\n\nwith an agentic approach. Such an AI system would be a further milestone\ntoward the reaching of artificial general intelligence (AGI).\nDefinition\nAGI is a hypothetical type of artificial intelligence (AI) that\ncan perform any intellectual task that a human can. AGI\nwould have a general cognitive ability, similar to human\nintelligence, and be able to learn from experience, reason,\nplan, communicate, and solve problems across different\ndomains. An AGI system would also be able to “perceive”\nthe world as we do, meaning that it could process data in\ndifferent formats, from text to images to sounds. Hence, AGI\nimplies multimodality.\nCreating AGI is a primary goal of some AI research and a\ncommon topic in science fiction. However, there is no\nconsensus on how to achieve AGI, what criteria to use to\nmeasure it, or when it might be possible. Some researchers\nargue that AGI could be achieved in years or decades, while\nothers maintain that it might take a century or longer, or that\nit might never be achieved.\nHowever, AGI is not seen as the ultimate milestone in AI development. In\nfact, in recent months another definition has emerged in the context of AI –\nthat is, Strong AI or Super AI, referring to an AI system that is more capable\nthan a human.\nAt the time of writing this book (February 2024), LMMs such as GPT-4\nTurbo with Vision are a reality. However, those are not the only ways to\n\n\nreach multimodality. In this chapter, we are going to examine how to merge\nmultiple AI systems to reach a multimodal AI assistant. The idea is that if we\ncombine single-modal models, one for each data format we want to process,\nand then use an LLM as the brain of our agent to let it interact in dynamic\nways with those models (that will be its tools), we can still achieve this goal.\nThe following diagram shows the structure of a multimodal application that\nintegrates various single-modal tools to perform a task – in this case,\ndescribing a picture aloud. The application uses image analysis to examine\nthe picture, text generation to create some text that describes what it\nobserves in the picture, and text-to-speech to convey this text to the user\nthrough speech.\nThe LLM acts as the “reasoning engine” of the application, invoking the\nproper tools needed to accomplish the user’s query.\n\n\nFigure 10.1: Illustration of multimodal application with single-modal tools\nIn the upcoming section, we are going to explore various approaches to\nbuilding multimodal applications, all based on the idea of combining\nexisting single-modal tools or models.\n\n\nBuilding a multimodal agent\nwith LangChain\nSo far, we’ve covered the main aspects of multimodality and how to achieve\nit with modern LFMs. As we saw throughout Part 2 of this book, LangChain\noffers a variety of components that we leveraged massively, such as chains,\nagents, tools, and so on. As a result, we already have all the ingredients we\nneed to start building our multimodal agent.\nHowever, in this chapter, we will adopt three approaches to tackle the\nproblem:\nThe agentic, out-of-the-box approach: Here we will leverage the\nAzure Cognitive Services toolkit, which offers native integrations\ntoward a set of AI models that can be consumed via API, and that\ncovers various domains such as image, audio, OCR, etc.\nThe agentic, custom approach: Here, we are going to select single\nmodels and tools (including defining custom tools) and concatenate\nthem into a single agent that can leverage all of them.\nThe hard-coded approach: Here, we are going to build separate\nchains and combine them into a sequential chain.\nIn the upcoming sections, we will cover all these approaches with concrete\nexamples.\nOption 1: Using an out-of-\nthe-box toolkit for Azure AI\nServices\n\n\nFormerly known as Azure Cognitive Services, Azure AI Services are a set of\ncloud-based APIs and AI services developed by Microsoft that enable\ndevelopers and data scientists to add cognitive capabilities to their apps. AI\nServices are meant to provide every developer with AI models to be\nintegrated with programming languages such as Python, C#, or JavaScript.\nAzure AI Services cover various domains of AI, including speech, natural\nlanguage, vision, and decision-making. All those services come with models\nthat can be consumed via API, and you can decide to:\nLeverage powerful pre-built models available as they are and ready to\nuse.\nCustomize those pre-built models with custom data so that they are\ntailored to your use case.\nHence, considered all together, Azure AI Services can achieve the goal of\nmultimodality, if properly orchestrated by an LLM as a reasoning engine,\nwhich is exactly the framework LangChain built.\nGetting Started with\nAzureCognitiveServicesToolkit\nIn fact, LangChain has a native integration with Azure AI Services called\nAzureCognitiveServicesToolkit, which can be passed as a parameter to an\nagent and leverage the multimodal capabilities of those models.\nThe toolkit makes it easier to incorporate Azure AI services’ capabilities –\nsuch as image analysis, form recognition, speech-to-text, and text-to-speech\n– within your application. It can be used within an agent, which is then\nempowered to use the AI services to enhance its functionality and provide\nricher responses.\n",
      "page_number": 351,
      "chapter_number": 42,
      "summary": "We will see the logic behind the\ncombination of foundation models in different AI domains – language,\nimages, and audio – into one single agent that can adapt to a variety of tasks Key topics include model, langchain, and services.",
      "keywords": [
        "Titanic dataset",
        "Models",
        "Services",
        "AGI",
        "Multimodal",
        "Azure Cognitive Services",
        "Azure",
        "Foundation Models",
        "multimodal agent",
        "agent",
        "Inference Endpoint",
        "Falcon LLM model",
        "tools",
        "LLMs",
        "model card"
      ],
      "concepts": [
        "model",
        "langchain",
        "services",
        "python",
        "text",
        "agents",
        "building",
        "llm",
        "cognitive",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 5,
          "title": "Segment 5 (pages 33-40)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.51,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 2,
          "title": "Segment 2 (pages 19-39)",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 43,
      "title": "Segment 43 (pages 360-367)",
      "start_page": 360,
      "end_page": 367,
      "detection_method": "topic_boundary",
      "content": "Currently, the integration supports the following tools:\nAzureCogsImageAnalysisTool: Used to analyze and extract metadata\nfrom images.\nAzureCogsSpeech2TextTool: Used to convert speech to text.\nAzureCogsText2SpeechTool: Used to synthetize text to speech with\nneural voices.\nAzureCogsFormRecognizerTool: Used to perform optical character\nrecognition (OCR).\nDefinition\nOCR is a technology that converts different types of\ndocuments, such as scanned paper documents, PDFs, or\nimages captured by a digital camera, into editable and\nsearchable data. OCR can save time, cost, and resources by\nautomating data entry and storage processes. It can also\nenable access to and editing of the original content of\nhistorical, legal, or other types of documents.\nFor example, if you ask an agent what you can make with some ingredients,\nand provide an image of eggs and flour, the agent can use the Azure AI\nServices Image Analysis tool to extract the caption, objects, and tags from\nthe image, and then use the provided LLM to suggest some recipes based on\nthe ingredients. To implement this, let’s first set up our toolkit.\nSetting up the toolkit\nTo get started with the toolkit, you can follow these steps:\n\n\n1. You first need to create a multi-service instance of Azure AI Services in\nAzure following the instructions at\nhttps://learn.microsoft.com/en-us/azure/ai-\nservices/multi-service-resource?\ntabs=windows&pivots=azportal.\n2. A multi-service resource allows you to access multiple AI services with\na single key and endpoint to be passed to LangChain as environmental\nvariables. You can find your keys and endpoint under the Keys and\nEndpoint tab in your resource panel:\nFigure 10.2: Screenshot of a multi-service instance of Azure AI Services\n3. Once the resource is set, we can start building our LegalAgent. To do\nso, the first thing we need to do is set the AI services environmental\nvariables in order to configure the toolkit. To do so, I’ve saved the\nfollowing variables in my .env file:\n\n\nAZURE_COGS_KEY = \"your-api-key\"\nAZURE_COGS_ENDPOINT = \"your-endpoint\nAZURE_COGS_REGION = \"your-region\"\n4. Then, you can load them as always alongside the other environmental\nvariables:\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\nazure_cogs_key = os.environ[\"AZURE_COGS_KEY\"]\nazure_cogs_endpoint = os.environ[\"AZURE_COGS_ENDPOINT\"\nazure_cogs_region = os.environ[\"AZURE_COGS_REGION\"]\nopenai_api_key = os.environ['OPENAI_API_KEY']\n5. Now, we can configure our toolkit and also see which tools we have,\nalongside their description:\nfrom langchain.agents.agent_toolkits import AzureCogni\ntoolkit = AzureCognitiveServicesToolkit()\n[(tool.name, tool.description) for tool in toolkit.get\nThe following is the corresponding output:\n[('azure_cognitive_services_form_recognizer',\n  'A wrapper around Azure Cognitive Services Form Recognize\n ('azure_cognitive_services_speech2text',\n  'A wrapper around Azure Cognitive Services Speech2Text. U\n ('azure_cognitive_services_text2speech',\n  'A wrapper around Azure Cognitive Services Text2Speech. U\n ('azure_cognitive_services_image_analysis',\n  'A wrapper around Azure Cognitive Services Image Analysis\n\n\n6. Now, it’s time to initialize our agent. For this purpose, we will use a\nSTRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION agent that, as we saw\nin previous chapters, also allows for multi-tools input, since we will\nalso add further tools in the Leveraging multiple tools section:\nfrom langchain.agents import initialize_agent, AgentTy\nfrom langchain import OpenAI\nllm = OpenAI()\nModel = ChatOpenAI()\nagent = initialize_agent(\n    tools=toolkit.get_tools(),\n    llm=llm,\n    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DE\n    verbose=True,\n)\nNow we have all the ingredients to start testing our agent.\nLeveraging a single tool\nTo start easy, let’s simply ask the agent to describe the following picture,\nwhich will only require the image_analysis tool to be accomplished:\n\n\nFigure 10.3: Sample picture of a slingshot (source: https://www.stylo24.it/wp-\ncontent/uploads/2020/03/fionda.jpg)\nLet’s pass the URL of this image as input to our model, as per the\ndescription of the azure_cognitive_services_image_analysis tool:\ndescription = agent.run(\"what shows the following image?:\"\n\"https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg\")\nprint(description)\nWe then get the following output:\n> Entering new AgentExecutor chain...\nAction:\n```\n{\n  \"action\": \"azure_cognitive_services_image_analysis\",\n  \"action_input\": \"https://www.stylo24.it/wp-content/uploads/2020\n}\n```\nObservation: Caption: a person holding a slingshot\nTags: person, tool, nail, hand, holding, needle\nThought: I know what the image is.\n\n\nAction:\n```\n{\n  \"action\": \"Final Answer\",\n  \"action_input\": \"The image is of a person holding a slingshot.\"\n}\n```\n> Finished chain.\nThe image is of a person holding a slingshot.\nAs you can see, the agent was able to retrieve the proper tool to address the\nuser’s question. In this case, the question was very simple, so I want to\nchallenge the same tool with a trickier question.\nThe goal is to replicate the GPT-4 capabilities in its common-sense\nreasoning while working with images, as the following illustration from\nGPT-4’s earliest experiments shows:\n\n\nFigure 10.4: Example of visual capabilities and common sense reasoning of GPT-4 (source:\nhttps://openai.com/research/gpt-4)\nSo let’s ask our model something more challenging. Let’s ask it to reason\nabout the consequences of letting the slingshot go:\nagent.run(\"what happens if the person lets the slingshot go?:\"\n\"https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg\")\nWe then obtain the following output:\n\n\n> Entering new AgentExecutor chain...\nAction:\n```\n{\n  \"action\": \"azure_cognitive_services_image_analysis\",\n  \"action_input\": \"https://www.stylo24.it/wp-content/uploads/2020\n}\n```\nObservation: Caption: a person holding a slingshot\nTags: person, tool, nail, hand, holding, needle\nThought: I know what to respond\nAction:\n```\n{\n  \"action\": \"Final Answer\",\n  \"action_input\": \"If the person lets the slingshot go, it will f\n}\n```\n> Finished chain.\n'If the person lets go of the slingshot, the object being launche\nIt might seem an easy question, but the agent’s answers imply an extremely\nrefined common sense reasoning: thanks to the metadata extracted from the\nimage leveraging the image_analysis tool, the LLM was able to set up\nreasoning in terms of consequences given an action (the person letting the\nslingshot go).\nIn this example, the agent only leveraged one tool. But what happens if we\nask it something that requires at least two tools?\nLeveraging multiple tools\nLet’s say, for example, that we want the model to read a story aloud to us\nbased on a picture.\n",
      "page_number": 360,
      "chapter_number": 43,
      "summary": "This chapter covers segment 43 (pages 360-367). Key topics include tools, agent, and services. Definition\nOCR is a technology that converts different types of\ndocuments, such as scanned paper documents, PDFs, or\nimages captured by a digital camera, into editable and\nsearchable data.",
      "keywords": [
        "Azure Cognitive Services",
        "Azure",
        "Services",
        "Azure Cognitive",
        "image",
        "Cognitive Services",
        "Services Image Analysis",
        "COGS",
        "cognitive",
        "Action",
        "agent",
        "tool",
        "Cognitive Services Image",
        "slingshot",
        "person"
      ],
      "concepts": [
        "tools",
        "agent",
        "services",
        "follow",
        "images",
        "data",
        "text",
        "let",
        "letting",
        "lets"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents In Action",
          "chapter": 37,
          "title": "Segment 37 (pages 322-331)",
          "relevance_score": 0.45,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 61-81)",
          "relevance_score": 0.45,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 23,
          "title": "Segment 23 (pages 197-207)",
          "relevance_score": 0.43,
          "method": "sentence_transformers"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 12,
          "title": "Segment 12 (pages 101-108)",
          "relevance_score": 0.43,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 12,
          "title": "Segment 12 (pages 95-102)",
          "relevance_score": 0.41,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 44,
      "title": "Segment 44 (pages 368-377)",
      "start_page": 368,
      "end_page": 377,
      "detection_method": "topic_boundary",
      "content": "Figure 10.5: Example of an input image for a story-telling agent (source:\nhttps://i.redd.it/diawvlriobq11.jpg)\nLet’s ask the agent to do so:\nagent.run(\"Tell me a story related to the following picture and \nWe obtain the following output:\nThought:The human requested a story related to the picture. I wil\nAction:\n```\n{\n  \"action\": \"azure_cognitive_services_image_analysis\",\n  \"action_input\": {\n    \"query\": \"https://i.redd.it/diawvlriobq11.jpg\"\n  }\n}\n```\nObservation: Caption: a dog standing on a snowy hill\nObjects: dog\nTags: outdoor, animal, mammal, snow, freezing, dog breed, dog, mo\nThought:Based on the Azure Cognitive Services Image Analysis tool\n\n\nAction:\n```\n{\n  \"action\": \"azure_cognitive_services_text2speech\",\n  \"action_input\": {\n    \"query\": \"Once upon a time, in a snowy winter wonderland, the\n  }\n}\n```\nObservation: C:\\Users\\vaalt\\AppData\\Local\\Temp\\tmp_cecwqht.wav\nThought:The story involving a dog in a snowy winter wonderland se\nAction:\n```\n{\n  \"action\": \"Final Answer\",\n  \"action_input\": \"I hope you enjoyed the story of Snowy the Savi\n}\n```\nAs you can see, the agent was able to invoke two tools to accomplish the\nrequest:\n1. It first started with the image_analysis tool to generate the image\ncaption used to produce the story.\n2. Then, it invoked the text2speech tool to read it aloud to the user.\nThe agent saved the audio file in a temporary file, and you can listen to it\ndirectly by clicking on the URL. Alternatively, you can save the output as a\nPython variable and execute it as follows:\nfrom IPython import display\naudio = agent.run(\"Tell me a story related to the following pict\ndisplay.display(audio)\n\n\nFinally, we can also modify the default prompt that comes with the agent\ntype, to make it more customized with respect to our specific use case. To do\nso, we first need to inspect the template and then decide which part we can\nmodify. To inspect the template, you can run the following command:\nprint(agent.agent.llm_chain.prompt.messages[0].prompt.template)\nHere is our output:\nRespond to the human as helpfully and accurately as possible. You\n{tools}\nUse a json blob to specify a tool by providing an action key (too\nValid \"action\" values: \"Final Answer\" or youtube_search, CustomeY\nProvide only ONE action per $JSON_BLOB, as shown:\n```\n{{\n  \"action\": $TOOL_NAME,\n  \"action_input\": $INPUT\n}}\n```\nFollow this format:\nQuestion: input question to answer\nThought: consider previous and subsequent steps\nAction:\n```\n$JSON_BLOB\n...\n```\nBegin! Reminder to ALWAYS respond with a valid json blob of a sin\nThought:\nLet’s modify the prefix of the prompt and pass it as kwargs to our agent:\nPREFIX = \"\"\"\nYou are a story teller for children. \nYou read aloud stories based on pictures that the user pass you.\n\n\n You always start your story with a welcome message targeting chi\n You can use multiple tools to answer the question.\n ALWAYS use the tools.\n You have access to the following tools:\"\"\"\nagent = initialize_agent(toolkit.get_tools(), model, agent=AgentT\n                         agent_kwargs={\n                            'prefix':PREFIX})\nAs you can see, now the agent acts more similar to a storyteller with a\nspecific style. You can customize your prompt as you wish, always keeping\nin mind that each pre-built agent has its own prompt template, hence it is\nalways recommended to first inspect it before customizing it.\nNow that we have explored the out-of-the-box capabilities of the toolkit,\nlet’s build an end-to-end application.\nBuilding an end-to-end application for\ninvoice analysis\nAnalyzing invoices might require a lot of manual work if not assisted by\ndigital processes. To address this, we will build an AI assistant that is able to\nanalyze invoices for us and tell us any relevant information aloud. We will\ncall this application CoPenny.\nWith CoPenny, individuals and enterprises could reduce the time of invoice\nanalysis, as well as build toward document process automation and, more\ngenerally, digital process automation.\nDefinition\nDocument process automation is a strategy that uses\ntechnology to streamline and automate various document-\nrelated tasks and processes within an organization. It\n\n\ninvolves the use of software tools, including document\ncapture, data extraction, workflow automation, and\nintegration with other systems. For example, document\nprocess automation can help you extract, validate, and\nanalyze data from invoices, receipts, forms, and other types\nof documents. Document process automation can save you\ntime and money, improve accuracy and efficiency, and\nprovide valuable insights and reports from your document\ndata.\nDigital process automation (DPA) is a broader term that\nrefers to automating any business process with digital\ntechnology. DPA can help you connect your apps, data, and\nservices and boost your team’s productivity with cloud\nflows. DPA can also help you create more sophisticated and\nintuitive customer experiences, collaborate across your\norganization, and innovate with AI and ML.\nTo start building our application, we can follow these steps:\n1. Using AzureCognitiveServicesToolkit, we will leverage the\nazure_cognitive_services_form_recognizer and\nazure_cognitive_services_text2speech tools, so we can limit the\nagent’s “powers” only to those two:\ntoolkit = AzureCognitiveServicesToolkit().get_tools()\n#those tools are at the first and third position in th\ntools = [toolkit[0], toolkit[2]]\ntools\n\n\nThe following is the corresponding output:\n[AzureCogsFormRecognizerTool(name='azure_cognitive_services\n2. Let’s now initialize the agent with the default prompt and see the\nresults. For this purpose, we will use a sample invoice as a template\nwith which to query the agent:\n\n\nPURCHASE ORDER TEMPLATE\n\nCompany Name\n123 Main Street\nHamilton, OH 44414\n(321) 456-7890\nEmail Address\n\nPaint of Cantact\n\nBILL TO.\n\nATIN: Name / Dept\nCompany Name\n123 Main Street\nHamilton, OH 44416\n\n(321) 456-7890\nEmail Address\nSHIPPING METHOD SHIPPING TERMS\nTEM NO, DESCRIPTION\nAll Women's Tall -M.\n8223 Men's Tall - MM\nc333 Children’s - §\nD444 Men's - XL\nRemarks/Instructions:\n\nPlease make check payable fo Tour Company Name.\n\nTHANK YOU\n\nAUTHORIZED SIGNATURE\n\nPURCHASE ORDER\n\nSHIP TO\nATTN: Name / Dept\n\nCompany Name\n123 Main Street\nHamilton, OH 44416\n(321) 456-7890\nSHIP WIA PAYMENT\nor UNIT FRICE\n10 $10.00\n5 $20.00\n10 $5.00\n5 $10.00\nSUBTOTAL\n\nenter total amount DISCOUNT\nSUBTOTAL LESS DISCOUNT\nenter percentage TAX RATE\nTOTAL TAX\nSHIPPING/HANDLING\nOTHER\n\nTOTAL\n\nDATE\n\nFor questions concerning this invoice, please contact\nName, (321) 456-7890, Email Acidress\n\nwee. younvebadcdress.com\n\nDATE\n01/18/2022\n\nPURCHASE ORDER NO.\n\nA246\n\nCUSTOMER NO.\n\n114H\n\nDELIVERY DATE\n\nTOTAL\n$100.00\n$100.00\n\n$50.00\n$50.00\n$0.00\n40.00\n$0.00\n40.00\n$0.00\n$0.00\n$0.00\n300.00\n50.00\n250,00\n7.214%\n18.04\n50.00\n50.00\n3468.04\n\nFigure 10.6: Sample template of a generic invoice (source: https://www.whiteelysee.fr/design/wp-\ncontent/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg)\n3. Let’s start by asking the model to tell us all the men’s stock-keeping\nunits (SKUs) on the invoice:\nagent.run(\"what are all men's skus?\"\n\"https://www.whiteelysee.fr/design/wp-content/uploads/\nWe then get the following output (showing a truncated output; you can\nfind the whole output in the book’s GitHub repository):\n> Entering new AgentExecutor chain...\nAction:\n```\n{\n  \"action\": \"azure_cognitive_services_form_recognizer\",\n  \"action_input\": {\n    \"query\": \"https://www.whiteelysee.fr/design/wp-content/\n  }\n}\n```\nObservation: Content: PURCHASE ORDER TEMPLATE […]\n> Finished chain.\n\"The men's skus are B222 and D444.\"\n4. We can also ask for multiple information (women’s SKUs, shipping\naddress, and delivery dates) as follows (note that the delivery date is not\nspecified, as we want our agent not to hallucinate):\nagent.run(\"give me the following information about the\n\"https://www.whiteelysee.fr/design/wp-content/uploads/\n\n\nThis gives us the following output:\n\"The women's SKUs are A111 Women's Tall - M. The shipping a\n5. Finally, let’s also leverage the text2speech tool to produce the audio of\nthe response:\nagent.run(\"extract women's SKUs in the following invoi\n\"https://www.whiteelysee.fr/design/wp-content/uploads/\nAs per the previous example, you can listen to the audio by clicking\non the URL in the chain, or using Python’s Display function if you\nsave it as a variable.\n6. Now, we want our agent to be better tailored toward our goal. To do so,\nlet’s customize the prompt giving specific instructions. In particular, we\nwant the agent to produce the audio output without the user explicitly\nasking for it:\nPREFIX = \"\"\"\nYou are an AI assistant that help users to interact wi\nYou extract information from invoices and read it alou\nYou can use multiple tools to answer the question.\nAlways divide your response in 2 steps:\n1. Extracting the information from the invoice upon us\n2. Converting the transcript of the previous point int\nALWAYS use the tools.\nALWAYS return an audio file using the proper tool.\nYou have access to the following tools:\n\"\"\"\nagent = initialize_agent(tools, model, agent=AgentType\n\n\n                         agent_kwargs={\n                            'prefix':PREFIX})\n7. Let’s run the agent:\nagent.run(\"what are women's SKUs in the following invo\n\"https://www.whiteelysee.fr/design/wp-content/uploads/\nThis yields the following output:\n> Entering new AgentExecutor chain...\nI will need to use the azure_cognitive_services_form_recogn\nAction:\n```\n{\n  \"action\": \"azure_cognitive_services_form_recognizer\",\n  \"action_input\": {\n    \"query\": \"https://www.whiteelysee.fr/design/wp-content/\n  }\n}\n```\nObservation: Content: PURCHASE ORDER TEMPLATE […]\nObservation: C:\\Users\\vaalt\\AppData\\Local\\Temp\\tmpx1n4obf3\nThought:Now that I have provided the answer, I will wait fo\nAs you can see, now the agent saved the output into an audio file, even when\nthe user didn’t ask explicitly for it.\nAzureCognitiveServicesToolkit is a powerful integration that allows for\nnative consumption of Azure AI Services. However, there are some pitfalls\nof this approach, including the limited number of AI services. In the next\nsection, we are going to explore yet another option to achieve multimodality,\nwith a more flexible approach while still keeping an agentic strategy.\n",
      "page_number": 368,
      "chapter_number": 44,
      "summary": "To address this, we will build an AI assistant that is able to\nanalyze invoices for us and tell us any relevant information aloud Key topics include tool, action, and shipping.",
      "keywords": [
        "agent",
        "Action",
        "tools",
        "PURCHASE ORDER TEMPLATE",
        "PURCHASE ORDER",
        "services",
        "azure",
        "output",
        "input",
        "template",
        "story",
        "prefix",
        "ORDER TEMPLATE",
        "cognitive",
        "process automation"
      ],
      "concepts": [
        "tool",
        "action",
        "shipping",
        "ship",
        "output",
        "follows",
        "thought",
        "template",
        "dog",
        "entering"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents In Action",
          "chapter": 14,
          "title": "Segment 14 (pages 111-118)",
          "relevance_score": 0.51,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.51,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 22,
          "title": "Segment 22 (pages 435-456)",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 44,
          "title": "Segment 44 (pages 890-909)",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 52,
          "title": "Segment 52 (pages 440-447)",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 45,
      "title": "Segment 45 (pages 378-387)",
      "start_page": 378,
      "end_page": 387,
      "detection_method": "topic_boundary",
      "content": "Option 2: Combining single\ntools into one agent\nIn this leg of our journey toward multimodality, we will leverage different\ntools as plug-ins to our STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION\nagent. Our goal is to build a copilot agent that will help us generate reviews\nabout YouTube videos, as well as post those reviews on our social media\nwith a nice description and related picture. In all of that, we want to make\nlittle or no effort, so we need our agent to perform the following steps:\n1. Search and transcribe a YouTube video based on our input.\n2. Based on the transcription, generate a review with a length and style\ndefined by the user query.\n3. Generate an image related to the video and the review.\nWe will call our copilot GPTuber. In the following subsections, we will\nexamine each tool and then put them all together.\nYouTube tools and Whisper\nThe first step of our agent will be to search and transcribe the YouTube video\nbased on our input. To do so, there are two tools we need to leverage:\nYouTubeSearchTool: An out-of-the-box tool offered by LangChain\nand adapted from\nhttps://github.com/venuv/langchain_yt_tools. You\ncan import and try the tool by running the following code, specifying\nthe topic of the video and the number of videos you want the tool to\nreturn:\n\n\nfrom langchain.tools import YouTubeSearchTool\ntool = YouTubeSearchTool()\nresult = tool.run(\"Avatar: The Way of Water,1\")\nresult:\nHere is the output:\n\"['/watch?v=d9MyW72ELq0&pp=ygUYQXZhdGFyOiBUaGUgV2F5IG9mIFdh\nThe tool returns the URL of the video. To watch it, you can add it to\nhttps://youtube.com domain.\nCustomYTTranscribeTool: This is a custom tool that I’ve adapted\nfrom https://github.com/venuv/langchain_yt_tools.\nIt consists of transcribing the audio file retrieved from the previous tool\nusing a speech-to-text model. In our case, we will be leveraging\nOpenAI’s Whisper.\nWhisper is a transformer-based model introduced by OpenAI in\nSeptember 2022. It works as follows:\ni. It splits the input audio into 30-second chunks, converting them\ninto spectrograms (visual representations of sound frequencies).\nii. It then passes them to an encoder.\niii. The encoder then produces a sequence of hidden states that\ncapture the information in the audio.\niv. A decoder then predicts the corresponding text caption, using\nspecial tokens to indicate the task (such as language\nidentification, speech transcription, or speech translation) and the\noutput language.\n\n\nv. The decoder can also generate timestamps for each word or\nphrase in the caption.\nUnlike most OpenAI models, Whisper is open-source.\nSince this model takes as input only files and not URLs, within the\ncustom tool, there is a function defined as yt_get (you can find it in\nthe GitHub repository) that, starting from the video URL, downloads it\ninto a .mp4 file. Once downloaded, you can try Whisper with the\nfollowing lines of code:\nimport openai\naudio_file = open(\"Avatar The Way of Water  Official Traile\nresult = openai.Audio.transcribe(\"whisper-1\", audio_file)\naudio_file.close()\nprint(result.text)\nHere is the corresponding output:\n♪ Dad, I know you think I'm crazy. But I feel her. I hear h\nBy embedding Whisper in this custom tool, we can transcribe the\noutput of the first tool into a transcript that will serve as input to the\nnext tool. You can see the code and logic behind this embedding and\nthe whole tool in this book’s GitHub repository at\nhttps://github.com/PacktPublishing/Building-\nLLM-Powered-Applications, which is a modified version from\nhttps://github.com/venuv/langchain_yt_tools.\nSince we already have two tools, we can start building our tools list and\ninitializing our agent, using the following code:\n\n\nllm = OpenAI(temperature=0)\ntools = []\ntools.append(YouTubeSearchTool())\ntools.append(CustomYTTranscribeTool())\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_R\nagent.run(\"search a video trailer of Avatar: the way of water. R\nThe following is the corresponding output:\n> Entering new AgentExecutor chain...\nI need to find a specific video and transcribe it.\nAction: youtube_search\nAction Input: \"Avatar: the way of water,1\"\nObservation: ['/watch?v=d9MyW72ELq0&pp=ygUYQXZhdGFyOiB0aGUgd2F5IG\nThought:I found the video I was looking for, now I need to transc\nAction: CustomeYTTranscribe\nAction Input: […]\nObservation: ♪ Dad, I know you think I'm crazy. […]\nThought:I have the transcription of the video trailer for Avatar:\nFinal Answer: The transcription of the video trailer for Avatar: \n> Finished chain.\nGreat! We were able to generate the transcription of this video. The next step\nwill be to generate a review alongside a picture. While the review can be\nwritten directly from the LLM and passed as a parameter to the model (so\nwe don’t need another tool), the image generation will need an additional\ntool. For this purpose, we are going to use OpenAI’s DALL·E.\nDALL·E and text generation\nIntroduced by OpenAI in January 2021, DALL·E is a transformer-based\nmodel that can create images from text descriptions. It is based on GPT-3,\nwhich is also used for natural language processing tasks. It is trained on a\n\n\nlarge dataset of text-image pairs from the web and uses a vocabulary of\ntokens for both text and image concepts. DALL·E can produce multiple\nimages for the same text, showing different interpretations and variations.\nLangChain offers native integration with DALL·E, which you can use as a\ntool by running the following code (always setting the environmental\nvariable of your OPENAI_API_KEY from the .env file):\nfrom langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\ntools = load_tools(['dalle-image-generator'])\nagent = initialize_agent(tools, model, AgentType.ZERO_SHOT_REACT\nagent.run(\"Create an image of a halloween night. Return only the\nHere is the corresponding output:\n> Entering new AgentExecutor chain...\nI need to use an image generator to create an image of a hallowee\nAction: Dall-E Image Generator\nAction Input: \"An image of a spooky halloween night with a full m\nObservation: [link_to_the_blob]\nThought:I have successfully generated an image of a halloween nig\nFinal Answer: The image url is [link_to_the_blob]\n> Finished chain.\nThe following is the image that was generated, as requested:\n\n\nFigure 10.7: Image generated by DALL·E upon the user’s input\nGreat! Now let’s also see whether our agent is capable of generating a\nreview of a video based on the transcription:\nagent.run(\"search a video trailer of Avatar: the way of water. R\nWe get the following output:\n> Entering new AgentExecutor chain...\nI need to use both youtube_search and CustomeYTTranscribe tools t\nAction: youtube_search\nAction Input: \"Avatar: the way of water,1\"\nObservation: ['/watch?v=d9MyW72ELq0&pp=ygUYQXZhdGFyOiB0aGUgd2F5IG\nThought:I found the video I need to transcribe.\nAction: CustomeYTTranscribe\nAction Input: […]\nObservation: ♪ Dad, I know you think I'm crazy. […]\nThought:I need to write a review of the trailer.\nAction: Write a review\nAction Input: \"Avatar: The Way of Water trailer is visually stunn\nObservation: Write a review is not a valid tool, try one of [yout\n\n\nThought:I need to use a different tool to write a review.\nAction: Write a review\nAction Input: N/A\nObservation: Write a review is not a valid tool, try one of [yout\n...\nThought:I can write the review manually.\nFinal Answer: Avatar: The Way of Water trailer is visually stunni\n> Finished chain.\nNote how the agent was initially looking for a tool to make a review, to then\nrealize that there is no tool yet that can do it manually thanks to its\nparametric knowledge. This is a great example of how LLMs are reasoning\nengines and endowed with common sense reasoning. As always, you can\nfind the entire chain of thoughts in the book’s repository.\nThe next step will be to put it all together and see whether the agent is\ncapable of orchestrating all the tools, with some assistance in terms of\nprompt engineering.\nPutting it all together\nNow that we have all the ingredients, we need to put them together into one\nsingle agent. To do so, we can follow these steps:\n1. First, we need to add the DALL·E tool to the list of tools:\ntools = []\ntools.append(YouTubeSearchTool())\ntools.append(CustomYTTranscribeTool())\ntools.append(load_tools(['dalle-image-generator'])[0])\n[tool.name for tool in tools]\nThis gives us the following output:\n\n\n['youtube_search', 'CustomeYTTranscribe', 'Dall-E Image Gen\n2. The next step will be to test the agent with the default prompt, and then\ntry to refine the instructions with some prompt engineering. Let’s start\nwith a pre-configured agent (you can find all the steps in the GitHub\nrepository):\nagent = initialize_agent(tools, model, AgentType.ZERO_\nagent.run(\"search a video trailer of Avatar: the way o\nThis gives us the following output:\n> Entering new AgentExecutor chain...\nI need to search for a video trailer of \"Avatar: The Way of\nAction: youtube_search\nAction Input: \"Avatar: The Way of Water trailer,1\"\nObservation: ['/watch?v=d9MyW72ELq0&pp=ygUgQXZhdGFyOiBUaGUg\nThought:I found a video trailer of \"Avatar: The Way of Wate\nAction: CustomeYTTranscribe\nAction Input: '/watch?v=d9MyW72ELq0&pp=ygUgQXZhdGFyOiBUaGUg\nObservation: ♪ Dad, I know you think I'm crazy. […]\nThought:I have transcribed the video and now I can generate\nAction: Dall-E Image Generator\nAction Input: \"Review of Avatar: The Way of Water trailer: \nObservation: [image_url]\nThought:I have generated a review of the \"Avatar: The Way o\nFinal Answer: The \"Avatar: The Way of Water\" trailer is cap\n> Finished chain.\nThe following is the accompanying visual output:\n\n\nFigure 10.8: Image generated by DALL·E based on the trailer review\nWell, even without any prompt engineering, the agent was able to\norchestrate the tools and return the desired results!\n3. Now, let’s try to make it more tailored toward our purpose. Similar to\nthe CoPenny application, we don’t want the user to specify every time\nto generate a review alongside an image. So let’s modify the default\nprompt as follows:\nPREFIX = \"\"\"\nYou are an expert reviewer of movie trailer.\nYou adapt the style of the review depending on the cha\nYou can use multiple tools to answer the question.\nALWAYS search for the youtube video related to the tra\nALWAYS transcribe the youtube trailer and use it to ge\nALWAYS generate an image alongside the review, based o\nALWAYS use all the available tools for the various ste\nYou have access to the following tools:\n\"\"\"\n\n\nagent = initialize_agent(tools, model, agent=AgentType\n                         agent_kwargs={\n                            'prefix':PREFIX})\nagent.run(\"Generate a review of the trailer of Avatar:\nThe output obtained is as follows:\n> Entering new AgentExecutor chain...\nTo generate a review for Instagram, I will need to search f\nAction: youtube_search\nAction Input: \"Avatar: The Way of Water trailer\", 1\nObservation: ['/watch?v=d9MyW72ELq0&pp=ygUhQXZhdGFyOiBUaGUg\nThought:I have found a YouTube video of the \"Avatar: The Wa\nAction: CustomeYTTranscribe\nAction Input: '/watch?v=d9MyW72ELq0&pp=ygUhQXZhdGFyOiBUaGUg\nObservation: ♪ Dad, I know you think I'm crazy.[…]\nAction: Dall-E Image Generator\nAction Input: \"A scene from the movie 'Avatar: The Way of W\nObservation: [image_url]\nThought:I have generated an image for the Instagram review \nFinal Answer: \"Avatar: The Way of Water\" is an upcoming mov\n> Finished chain.\nThis is accompanied by the following visual output:\n",
      "page_number": 378,
      "chapter_number": 45,
      "summary": "Option 2: Combining single\ntools into one agent\nIn this leg of our journey toward multimodality, we will leverage different\ntools as plug-ins to our STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION\nagent Key topics include tools, agent, and action.",
      "keywords": [
        "Action Input",
        "Avatar",
        "Action",
        "input",
        "tools",
        "image",
        "agent",
        "review",
        "video",
        "video trailer",
        "trailer",
        "Combining single tools",
        "Water",
        "Observation",
        "Search"
      ],
      "concepts": [
        "tools",
        "agent",
        "action",
        "image",
        "observation",
        "generation",
        "generator",
        "generated",
        "review",
        "output"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 30,
          "title": "Segment 30 (pages 253-261)",
          "relevance_score": 0.51,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 23,
          "title": "Segment 23 (pages 197-207)",
          "relevance_score": 0.48,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 10,
          "title": "Segment 10 (pages 79-86)",
          "relevance_score": 0.48,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 12,
          "title": "Segment 12 (pages 95-102)",
          "relevance_score": 0.47,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 15,
          "title": "Segment 15 (pages 119-130)",
          "relevance_score": 0.47,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 46,
      "title": "Segment 46 (pages 388-395)",
      "start_page": 388,
      "end_page": 395,
      "detection_method": "topic_boundary",
      "content": "Figure 10.9: Image generated by DALL·E based on a trailer review\nWow! Not only was the agent able to use all the tools with the proper scope\nbut it also adapted the style to the type of channel we want to share our\nreview on – in this case, Instagram.\nOption 3: Hard-coded\napproach with a sequential\nchain\nThe third and last option offers yet another way of implementing a\nmultimodal application, which performs the following tasks:\nGenerates a story based on a topic given by the user.\nGenerates a social media post to promote the story.\n\n\nGenerates an image to go along with the social media post.\nWe will call this application StoryScribe.\nTo implement this, we will build separate LangChain chains for those single\ntasks, and then combine them into a SequentialChain. As we saw in\nChapter 1, this is a type of chain that allows you to execute multiple chains\nin a sequence. You can specify the order of the chains and how they pass\ntheir outputs to the next chain. So, we first need to create individual chains,\nthen combine them and run as a unique chain. Let’s follow these steps:\n1. We’ll start by initializing the story generator chain:\nfrom langchain.chains import SequentialChain, LLMChain\nfrom langchain.prompts import PromptTemplate\nstory_template = \"\"\"You are a storyteller. Given a top\nTopic: {topic}\nGenre: {genre}\nAudience: {audience}\nStory: This is a story about the above topic, with the\nstory_prompt_template = PromptTemplate(input_variables\nstory_chain = LLMChain(llm=llm, prompt=story_prompt_te\nresult = story_chain({'topic': 'friendship story','gen\nprint(result['story'])\nThis gives us the following output:\nJohn and Sarah had been best friends since they were kids. \n2. Note that I’ve set the output_key= \"story\" parameter so that it can be\neasily linked as output to the next chain, which will be the social post\ngenerator:\n\n\ntemplate = \"\"\"You are an influencer that, given a stor\nThe style should reflect the type of social media used\nStory:\n{story}\nSocial media: {social}\nReview from a New York Times play critic of the above \nprompt_template = PromptTemplate(input_variables=[\"sto\nsocial_chain = LLMChain(llm=llm, prompt=prompt_templat\npost = social_chain({'story': result['story'], 'social\nprint(post['post'])\nThe following output is then obtained:\n\"John and Sarah's journey of discovery and friendship is a \nHere, I used the output of story_chain as input to social_chain.\nWhen we combine all the chains together, this step will be\nautomatically performed by the sequential chain.\n3. Finally, let’s initialize an image generator chain:\nfrom langchain.utilities.dalle_image_generator import \nfrom langchain.llms import OpenAI\ntemplate = \"\"\"Generate a detailed prompt to generate a\nSocial media post:\n{post}\n\"\"\"\nprompt = PromptTemplate(\n    input_variables=[\"post\"],\n    template=template,\n)\nimage_chain = LLMChain(llm=llm, prompt=prompt, output_\n\n\nNote that the output of the chain will be the prompt to pass to the\nDALL·E model.\n4. In order to generate the image, we need to use the DallEAPIWrapper()\nmodule available in LangChain:\nfrom langchain.utilities.dalle_image_generator import \nimage_url = DallEAPIWrapper().run(image_chain.run(\"a c\nimport cv2\nfrom skimage import io\nimage = io.imread(image_url)\ncv2.imshow('image', image)\ncv2.waitKey(0)  \ncv2.destroyAllWindows()\nThis generates the following output:\nFigure 10.10: Picture generated by DALL·E given a social media post\n\n\n5. The final step will be to put it all together into a sequential chain:\noverall_chain = SequentialChain(input_variables = ['to\n                chains=[story_chain, social_chain, ima\n                output_variables = ['post', 'image'], \noverall_chain({'topic': 'friendship story','genre':'ad\nHere is our output:\n{'post': '\\n\\n\"John and Sarah\\'s journey of discovery and f\n'image': '\\nPrompt:\\n\\nCreate a digital drawing of John and\nSince we passed the output_variables = ['post, 'image'] parameter to\nthe chain, those will be the two outputs of the chain. With SequentialChain,\nwe have the flexibility to decide as many output variables as we want, so\nthat we can construct our output as we please.\nOverall, there are several ways to reach multimodality within your\napplication, and LangChain offers many components that make it easier.\nNow, let’s compare these approaches.\nComparing the three options\nWe examined three options to achieve this result: options 1 and 2 follow the\n“agentic” approach, using, respectively, pre-built toolkit and single tools\ncombined; option 3, on the other hand, follows a hard-coded approach,\nletting the developer decide the order of actions to be done.\nAll three come with pros and cons, so let’s wrap up some final\nconsiderations:\n\n\nFlexibility vs control: The agentic approach lets the LLM decide\nwhich actions to take and in which order. This implies greater flexibility\nfor the end user since there are no constraints in terms of queries that\ncan be done. On the other hand, having no control over the agent’s\nchain of thoughts could lead to mistakes that would need several tests\nof prompt engineering. Plus, as LLMs are non-deterministic, it is also\nhard to recreate mistakes to retrieve the wrong thought process. Under\nthis point of view, the hard-coded approach is safer, since the developer\nhas full control over the order of execution of the actions.\nEvaluations: The agentic approach leverages the tools to generate the\nfinal answer so that we don’t have to bother to plan these actions.\nHowever, if the final output doesn’t satisfy us, it might be cumbersome\nto understand what is the main source of the error: it might be a wrong\nplan, rather than a tool that is not doing its job correctly, or maybe a\nwrong prompt overall. On the other hand, with the hard-coded\napproach, each chain has its own model that can be tested separately, so\nthat it is easier to identify the step of the process where the main error\nhas occurred.\nMaintenance: With the agentic approach, there is one component to\nmaintain: the agent itself. We have in fact one prompt, one agent, and\none LLM, while the toolkit or list of tools is pre-built and we don’t\nneed to maintain them. On the other hand, with the hard-coded\napproach, for each chain, we need a separate prompt, model, and testing\nactivities.\nTo conclude, there is no golden rule to decide which approach to follow: it’s\nup to the developer to decide depending on the relative weight of the above\nparameters. As a general rule of thumb, the first step should be to define the\nproblem to solve and then evaluate the complexity of each approach with\n\n\nrespect to that problem. If, for example, it is a task that can be entirely\naddressed with the Cognitive Services toolkit without even doing prompt\nengineering, that could be the easiest way to proceed; on the other hand, if it\nrequires a lot of control over the single components as well as on the\nsequence of execution, a hard-coded approach is preferable.\nIn the next section, we are going to build a sample front-end using Streamlit,\nbuilt on top of StoryScribe.\nDeveloping the front-end\nwith Streamlit\nNow that we have seen the logic behind an LLM-powered StoryScribe, it is\ntime to give our application a GUI. To do so, we will once again leverage\nStreamlit. As always, you can find the whole Python code in the GitHub\nbook repository at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications.\nAs per the previous sections, you need to create a .py file to run in your\nterminal via streamlit run file.py. In our case, the file will be named\nstoryscribe.py.\nThe following are the main steps to set up the front-end:\n1. Configuring the application webpage:\nst.set_page_config(page_title=\"StoryScribe\", page_icon\nst.header('\n Welcome to StoryScribe, your story gener\nload_dotenv()\nopenai_api_key = os.environ['OPENAI_API_KEY']\n\n\n2. Initialize the dynamic variables to be used within the placeholders of\nthe prompts:\ntopic = st.sidebar.text_input(\"What is topic?\", 'A dog\ngenre = st.sidebar.text_input(\"What is the genre?\", 'D\naudience = st.sidebar.text_input(\"What is your audienc\nsocial = st.sidebar.text_input(\"What is your social?\",\n3. Initialize all the chains and the overall chain (I will omit here all the\nprompt templates; you can find them in the GitHub repository of the\nbook):\nstory_chain = LLMChain(llm=llm, prompt=story_prompt_te\nsocial_chain = LLMChain(llm=llm, prompt=social_prompt_\nimage_chain = LLMChain(llm=llm, prompt=prompt, output_\noverall_chain = SequentialChain(input_variables = ['to\n                chains=[story_chain, social_chain, ima\n                output_variables = ['story','post', 'i\n4. Run the overall chain and print the results:\nif st.button('Create your post!'):\n    result = overall_chain({'topic': topic,'genre':gen\n    image_url = DallEAPIWrapper().run(result['image'])\n    st.subheader('Story')\n    st.write(result['story'])\n    st.subheader('Social Media Post')\n    st.write(result['post'])\n    st.image(image_url)\nIn this case, I’ve set the output_variables = ['story','post', 'image']\nparameter so that we will have also the story itself as output. The final result\nlooks like the following:\n",
      "page_number": 388,
      "chapter_number": 46,
      "summary": "We will call this application StoryScribe Key topics include prompts, story, and post. Option 3: Hard-coded\napproach with a sequential\nchain\nThe third and last option offers yet another way of implementing a\nmultimodal application, which performs the following tasks:\nGenerates a story based on a topic given by the user.",
      "keywords": [
        "chain",
        "story",
        "prompt",
        "social",
        "Image",
        "social media post",
        "output",
        "post",
        "social media",
        "llm",
        "approach",
        "media post",
        "variables",
        "topic",
        "input"
      ],
      "concepts": [
        "prompts",
        "story",
        "post",
        "chain",
        "results",
        "outputs",
        "finally",
        "final",
        "social",
        "approach"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 61-81)",
          "relevance_score": 0.49,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 54,
          "title": "Segment 54 (pages 457-464)",
          "relevance_score": 0.49,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 22,
          "title": "Segment 22 (pages 435-456)",
          "relevance_score": 0.46,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 10,
          "title": "Segment 10 (pages 79-86)",
          "relevance_score": 0.46,
          "method": "sentence_transformers"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 12,
          "title": "Segment 12 (pages 101-108)",
          "relevance_score": 0.46,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 47,
      "title": "Segment 47 (pages 396-406)",
      "start_page": 396,
      "end_page": 406,
      "detection_method": "topic_boundary",
      "content": "Figure 10.11: Front-end of StoryScribe showing the story output\nThe following picture is the resulting Instagram post:\nFigure 10.12: Front-end of StoryScribe showing the social media post along with the generated image\n\n\nWith just a few lines of code, we were able to set up a simple front-end for\nStoryScribe with multimodal capabilities.\nSummary\nIn this chapter, we introduced the concept of multimodality and how to\nachieve it even without multimodal models. We explored three different\nways of achieving the objective of a multimodal application: an agentic\napproach with a pre-built toolkit, an agentic approach with the combination\nof single tools, and a hard-coded approach with chained models.\nWe delved into the concrete implementation of three applications with the\nabove methods, examining the pros and cons of each approach. We saw, for\nexample, how an agentic approach gives higher flexibility to the end user at\nthe price of less control of the backend plan of action.\nFinally, we implemented a front-end with Streamlit to build a consumable\napplication with the hard-coded approach.\nWith this chapter, we conclude Part 2 of the book, where we examined\nhands-on scenarios and built LLMs-powered applications. In the next\nchapter, we will focus on how to customize your LLMs even more with the\nprocess of fine-tuning, leveraging open-source models, and using custom\ndata for this purpose.\nReferences\nSource code for YouTube tools:\nhttps://github.com/venuv/langchain_yt_tools\nLangChain YouTube tool:\nhttps://python.langchain.com/docs/integrations/t\n\n\nools/youtube\nLangChain AzureCognitiveServicesToolkit:\nhttps://python.langchain.com/docs/integrations/t\noolkits/azure_cognitive_services\nJoin our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\nOceanofPDF.com\n\n\n11\nFine-Tuning Large\nLanguage Models\nUp to this point, we’ve explored the features and applications of large\nlanguage models (LLMs) in their “base” form, meaning that we consumed\nthem with the parameters obtained from their base training. We\nexperimented with many scenarios in which, even in their base form, LLMs\nhave been able to adapt to a variety of scenarios. Nevertheless, there might\nbe extremely domain-specific cases where a general-purpose LLM is not\nsufficient to fully embrace the taxonomy and knowledge of that domain. If\nthis is the case, you might want to fine-tune your model on your domain-\nspecific data.\nDefinition\nIn the context of fine-tuning language models, “taxonomy”\nrefers to a structured classification or categorization system\nthat organizes concepts, terms, and entities according to their\nrelationships and hierarchies within a specific domain. This\nsystem is essential for making the model’s understanding and\ngeneration of content more relevant and accurate for\nspecialized applications.\n\n\nA concrete example of taxonomy in a domain-specific sector\nis in the medical field. Here, taxonomy could categorize\ninformation into structured groups like diseases, symptoms,\ntreatments, and patient demographics. For instance, in the\n“diseases” category, there might be subcategories for types\nof diseases like “cardiovascular diseases,” which could be\nfurther divided into more specific conditions such as\n“hypertension” and “coronary artery disease.” This detailed\ncategorization helps in fine-tuning language models to\nunderstand and generate more precise and contextually\nappropriate responses in medical consultations or\ndocumentation.\nIn this chapter, we are going to cover the technical details of fine-tuning\nLLMs, from the theory behind it to the hands-on implementation with\nPython and Hugging Face. By the end of this chapter, you will be able to\nfine-tune an LLM on your own data, so that you can build domain-specific\napplications powered by those models.\nWe will delve into the following topics:\nIntroduction to fine-tuning\nUnderstanding when you need fine-tuning\nPreparing your data to fine-tune the model\nFine-tuning a base model on your data\nHosting strategies for your fine-tuned model\nTechnical requirements\nTo complete the tasks in this chapter, you will need the following:\n\n\nA Hugging Face account and user access token.\nPython 3.7.1 or later version.\nPython packages: Make sure to have the following Python packages\ninstalled: python-dotenv, huggingface_hub, accelerate>=0.16.0, <1\ntransformers[torch], safetensors, tensorflow, datasets, evaluate,\nand accelerate. Those can be easily installed via pip install in your\nterminal. If you want to install everything from the latest release, you\ncan refer to the original GitHub by running pip install\ngit+https://github.com/huggingface/transformers.git in your\nterminal.\nYou can find all the code and examples in the book’s GitHub repository at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications.\nWhat is fine-tuning?\nFine-tuning is a technique of transfer learning in which the weights of a\npretrained neural network are used as the initial values for training a new\nneural network on a different task. This can improve the performance of the\nnew network by leveraging the knowledge learned from the previous task,\nespecially when the new task has limited data.\nDefinition\nTransfer learning is a technique in machine learning that\ninvolves using the knowledge learned from one task to\nimprove the performance on a related but different task. For\nexample, if you have a model that can recognize cars, you\n\n\ncan use some of its features to help you recognize trucks.\nTransfer learning can save you time and resources by reusing\nexisting models instead of training new ones from scratch.\nTo better understand the concepts of transfer learning and fine-tuning, let’s\nconsider the following example.\nImagine you want to train a computer vision neural network to recognize\ndifferent types of flowers, such as roses, sunflowers, and tulips. You have a\nlot of photos of flowers, but not enough to train a model from scratch.\nInstead, you can use transfer learning, which means taking a model that was\nalready trained on a different task and using some of its knowledge for your\nnew task. For example, you can take a model that was trained to recognize\nmany vehicles, such as cars, trucks, and bicycles. This model has learned\nhow to extract features from images, such as edges, shapes, colors, and\ntextures. These features are useful for any image recognition task, not just\nthe original one.\nYou can use this model as a base for your flower recognition model. You\nonly need to add a new layer on top of it, which will learn how to classify\nthe features into flower types. This layer is called the classifier layer, and it\nis needed for the model to adapt to the new task. Training the classifier layer\non top of the base model is a process called feature extraction. Once this\nstep is done, you can further tailor your model with fine-tuning by\nunfreezing some of the base model layers and training them together with\nthe classifier layer. This allows you to adjust the base model features to\nbetter suit your task.\nThe following picture illustrates the computer vision model example:\n\n\nFigure 11.1: Example of transfer learning and fine-tuning\nFine-tuning is usually done after feature extraction, as a final step to improve\nthe performance of the model. You can decide how many layers to unfreeze\nbased on your data size and complexity. A common practice is to unfreeze\nthe last few layers of the base model, which are more specific to the original\ntask, and leave the first few layers frozen, which are more generic and\nreusable.\nTo summarize, transfer learning and fine-tuning are techniques that allow\nyou to use a pretrained model for a new task. Transfer learning involves\nadding a new classifier layer on top of the base model and training only that\nlayer. Fine-tuning involves unfreezing some or all of the base model layers\nand training them together with the classifier layer.\nIn the context of generative AI, fine-tuning is the process of adapting a\npretrained language model to a specific task or domain by updating its\n\n\nparameters on a task-specific dataset. Fine-tuning can improve the\nperformance and accuracy of the model for the target task. The steps\ninvolved in fine-tuning are:\n1. Load the pretrained language model and its tokenizer: The\ntokenizer is used to convert text into numerical tokens that the model\ncan process. Different models have unique architectures and\nrequirements, often coming with their own specialized tokenizers\ndesigned to handle their specific input formats.\nFor instance, BERT (which stands for Bidirectional Encoder\nRepresentations from Transformers) uses WordPiece tokenization,\nwhile GPT-2 employs byte-pair encoding (BPE). Models also impose\ntoken limits due to memory constraints during training and inference.\nThese limits determine the maximum sequence length that a model\ncan handle. For example, BERT has a maximum token limit of 512\ntokens, while the GPT-2 can handle longer sequences (e.g., up to\n1,024 tokens).\n2. Prepare the task-specific dataset: The dataset should contain input-\noutput pairs that are relevant to the task. For example, for sentiment\nanalysis, the input could be a text review and the output could be a\nsentiment label (positive, negative, or neutral).\n3. Define the task-specific head: The head is a layer or a set of layers that\nare added on top of the pretrained model to perform the task. The head\nshould match the output format and size of the task. For example, for\nsentiment analysis, the head could be a linear layer with three output\nunits corresponding to the three sentiment labels.\nNote\n\n\nWhen dealing with an LLM specifically designed for\ntext generation, the architecture differs from models\nused for classification or other tasks. In fact, unlike\nclassification tasks, where we predict labels, an LLM\npredicts the next word or token in a sequence. This\nlayer is added on top of the pretrained transformer-\nbased models with the purpose of transforming the\ncontextualized hidden representations from the base\nmodel into probabilities over the vocabulary.\n4. Train the model on the task-specific dataset: The training process\ninvolves feeding the input tokens to the model, computing the loss\nbetween the model output and the true output, and updating the model\nparameters using an optimizer. The training can be done for a fixed\nnumber of epochs or until a certain criterion is met.\n5. Evaluate the model on a test or validation set: The evaluation\nprocess involves measuring the performance of the model on unseen\ndata using appropriate metrics. For example, for sentiment analysis, the\nmetric could be accuracy or F1-score (which will be discussed later in\nthis chapter). The evaluation results can be used to compare different\nmodels or fine-tuning strategies.\nEven though it is less computationally and time expensive than full training,\nfine-tuning an LLM is not a “light” activity. As LLMs are, by definition,\nlarge, their fine-tuning has hardware requirements as well as data collection\nand preprocessing.\nSo, the first question that you want to ask yourself while approaching a\ngiven scenario is: “Do I really need to finetune my LLM?”\n\n\nWhen is fine-tuning\nnecessary?\nAs we saw in previous chapters, good prompt engineering combined with\nthe non-parametric knowledge you can add to your model via embeddings\nare exceptional techniques to customize your LLM, and they can account for\naround 90% of use cases. However, the preceding affirmation tends to hold\nfor the state-of-the-art models, such as GPT-4, Llama 2, and PaLM 2. As\ndiscussed, those models have a huge number of parameters that make them\nheavy, hence the need for computational power; plus, they might be\nproprietary and subject to a pay-per-use cost.\nHenceforth, fine-tuning might also be useful when you want to leverage a\nlight and free-of-charge LLM, such as the Falcon LLM 7B, yet you want it\nto perform as well as a SOTA model in your specific task.\nSome examples of when fine-tuning might be necessary are:\nWhen you want to use an LLM for sentiment analysis on movie\nreviews, but the LLM was pretrained on Wikipedia articles and books.\nFine-tuning can help the LLM learn the vocabulary, style, and tone of\nmovie reviews, as well as the relevant features for sentiment\nclassification.\nWhen you want to use an LLM for text summarization on news articles,\nbut the LLM was pretrained on a language modeling objective. Fine-\ntuning can help the LLM learn the structure, content, and length of\nsummaries, as well as the generation objective and evaluation metrics.\nWhen you want to use an LLM for machine translation between two\nlanguages, but the LLM was pretrained on a multilingual corpus that\ndoes not include those languages. Fine-tuning can help the LLM learn\n",
      "page_number": 396,
      "chapter_number": 47,
      "summary": "Summary\nIn this chapter, we introduced the concept of multimodality and how to\nachieve it even without multimodal models Key topics include models, specific, and tuning.",
      "keywords": [
        "model",
        "LLM",
        "fine-tuning",
        "base model",
        "task",
        "fine-tuning language models",
        "base model layers",
        "Language Models",
        "base",
        "layer",
        "LLMs",
        "pretrained language model",
        "transfer learning",
        "training",
        "model layers"
      ],
      "concepts": [
        "models",
        "specific",
        "tuning",
        "token",
        "llm",
        "fine",
        "tasks",
        "data",
        "involves",
        "involved"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 474-483)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 30,
          "title": "Segment 30 (pages 595-616)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 57,
          "title": "Segment 57 (pages 506-513)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 25,
          "title": "Segment 25 (pages 214-221)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 48,
      "title": "Segment 48 (pages 407-414)",
      "start_page": 407,
      "end_page": 414,
      "detection_method": "topic_boundary",
      "content": "the vocabulary, grammar, and syntax of the target languages, as well as\nthe translation objective and alignment methods.\nWhen you want to use an LLM to perform complex named entity\nrecognition (NER) tasks. For example, financial and legal documents\ncontain specialized terminology and entities that are not typically\nprioritized in general language models, henceforth a fine-tuning process\nmight be extremely beneficial here.\nIn this chapter, we will be covering a full-code approach leveraging Hugging\nFace models and libraries. However, be aware that Hugging Face also offers\na low-code platform called AutoTrain (you can read more about that at\nhttps://huggingface.co/autotrain), which might be a good\nalternative if your organization is more oriented towards low-code strategies.\nGetting started with fine-\ntuning\nIn this section, we are going to cover all the steps needed to fine-tune an\nLLM with a full-code approach. We will be leveraging Hugging Face\nlibraries, such as datasets (to load data from the Hugging Face datasets\necosystem) and tokenizers (to provide an implementation of the most\npopular tokenizers). The scenario we are going to address is a sentiment\nanalysis task. Our goal is to fine-tune a model to make it an expert binary\nclassifier of emotions, clustered into “positive” and “negative.”\nObtaining the dataset\nThe first ingredient that we need is the training dataset. For this purpose, I\nwill leverage the datasets library available in Hugging Face to load a binary\n\n\nclassification dataset called IMDB (you can find the dataset card at\nhttps://huggingface.co/datasets/imdb).\nThe dataset contains movie reviews, which are classified as positive or\nnegative. More specifically, the dataset contains two columns:\nText: The raw text movie review.\nLabel: The sentiment of that review. It is mapped as “0” for “Negative”\nand “1” for “Positive.”\nAs it is a supervised learning problem, the dataset already comes with\n25,000 rows for the training set and 25,000 rows for the validation set.\nDefinition\nSupervised learning is a type of machine learning that uses\nlabeled datasets to train algorithms to classify data or predict\noutcomes accurately. Labeled datasets are collections of\nexamples that have both input features and desired output\nvalues, also known as labels or targets. For example, a\nlabeled dataset for handwriting recognition might have\nimages of handwritten digits as input features and the\ncorresponding numerical values as labels.\nTraining and validation sets are subsets of the labeled dataset\nthat are used for different purposes in the supervised learning\nprocess. The training set is used to fit the parameters of the\nmodel, such as the weights of the connections in a neural\nnetwork. The validation set is used to tune the\nhyperparameters of the model, such as the number of hidden\nunits in a neural network or the learning rate.\n\n\nHyperparameters are settings that affect the overall behavior\nand performance of the model but are not directly learned\nfrom the data. The validation set helps to select the best\nmodel among different candidates by comparing their\naccuracy or other metrics on the validation set.\nSupervised learning differs from another type of machine\nlearning, which is unsupervised learning. With the latter,\nthe algorithm is tasked with finding patterns, structures, or\nrelationships in a dataset without the presence of labeled\noutputs or targets. In other words, in unsupervised learning,\nthe algorithm is not provided with specific guidance or labels\nto direct its learning process. Instead, it explores the data and\nidentifies inherent patterns or groupings on its own.\nYou can download the IMDB dataset by running the following code:\nfrom datasets import load_dataset\ndataset = load_dataset(\"imdb\")\ndataset\nHugging Face datasets come with a dictionary schema, which is as follows:\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n\n\n        features: ['text', 'label'],\n        num_rows: 50000\n    })\n})\nTo access one observation of a particular Dataset object (for example,\ntrain), you can use slicers, as follows:\ndataset[\"train\"][100]\nThis gives us the following output:\n{'text': \"Terrible movie. Nuff Said.[…]\n 'label': 0}\nSo, the 101st observation of the training set contains a review labeled as\nnegative.\nNow that we have the dataset, we need to preprocess it so that can be used to\ntrain our LLM. To do so, we need to tokenize the provided text, and we will\ndiscuss this in the next section.\nTokenizing the data\nA tokenizer is a component that is responsible for splitting a text into smaller\nunits, such as words or subwords, that can be used as inputs for an LLM.\nTokenizers can be used to encode text efficiently and consistently, as well as\nto add special tokens, such as mask or separator tokens, that are required by\nsome models.\nHugging Face provides a powerful utility called AutoTokenizer, available in\nthe Hugging Face Transformers library, that offers tokenizers for various\n\n\nmodels, such as BERT and GPT-2. It serves as a generic tokenizer class that\ndynamically selects and instantiates the appropriate tokenizer based on the\npretrained model you specify.\nThe following code snippet shows how we can initialize our tokenizer:\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\nNote that we picked a specific tokenizer called bert-base-cased. In fact,\nthere is a link between a tokenizer and an LLM, in the sense that the the\ntokenizer prepares the inputs for the model by converting the text into\nnumerical IDs that the model can understand.\nDefinition\nThe input IDs are the numerical IDs that correspond to the\ntokens in the vocabulary of the tokenizer. They are returned\nby the tokenizer function when encoding a text input. The\ninput IDs are used as inputs for the model, which expects\nnumerical tensors rather than strings. Different tokenizers\nmay have different input IDs for the same tokens, depending\non their vocabulary and tokenization algorithm.\nDifferent models may use different tokenization algorithms, such as word-\nbased, character-based, or subword-based. Therefore, it is important to use\nthe correct tokenizer for each model, otherwise the model may not perform\nwell or even produce errors. Let’s look at potential scenarios for each:\nA character-based approach might fit scenarios that deal with rare\nwords or languages with complex morphological structures, or when\n\n\ndealing with spelling correction tasks\nThe word-based approach might be a good fit for scenarios like NER,\nsentiment analysis, and text classification\nThe sub-word approach interpolates between the previous two, and it is\nuseful when we want to balance the granularity of text representation\nwith efficiency.\nAs we will see in the next section, we will leverage the BERT model for this\nscenario, hence we loaded its pretrained tokenizer (which is a word-based\ntokenizer powered by an algorithm called WordPiece).\nWe now need to initialize tokenize_function, which will be used to format\nthe dataset:\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding = \"max_length\", t\ntokenized_datasets = dataset.map(tokenize_function, batched=True\nAs you can see, we also configured the padding and truncation of\ntokenize_function to ensure an output with the right sizing for our BERT\nmodel.\nDefinition\nPadding and truncation are two techniques that are used to\nmake the input sequences of text have the same length. This\nis often required for some natural language processing\n(NLP) models, such as the BERT model, that expect fixed-\nlength inputs.\n\n\nPadding means adding some special tokens, usually zeros, at\nthe end or the beginning of a sequence to make it reach the\ndesired length. For example, if we have a sequence of length\n5 and we want to pad it to a length of 8, we can add 3 zeros\nat the end, like this: [1, 2, 3, 4, 5, 0, 0, 0]. This is called post-\npadding. Alternatively, we can add 3 zeros at the beginning,\nlike this: [0, 0, 0, 1, 2, 3, 4, 5]. This is called pre-padding.\nThe choice of padding strategy depends on the model and the\ntask.\nTruncation means removing some tokens from a sequence to\nmake it fit the desired length. For example, if we have a\nsequence of length 10 and we want to truncate it to a length\nof 8, we can remove 2 tokens from the end or the beginning\nof the sequence. For example, we can remove the last 2\ntokens, like this: [1, 2, 3, 4, 5, 6, 7, 8]. This is called post-\ntruncation. Alternatively, we can remove the first 2 tokens,\nlike this: [3, 4, 5, 6, 7, 8, 9, 10]. This is called pre-truncation.\nThe choice of truncation strategy also depends on the model\nand the task.\nNow, we can apply the function to our dataset and inspect the numerical IDs\nof one entry:\ntokenized_datasets = dataset.map(tokenize_function, batched=True\ntokenized_datasets['train'][100]['input_ids']\nHere is our output:\n\n\n[101,\n 12008,\n 27788,\n...\n 0,\n 0,\n 0,\n 0,\n 0]\nAs you can see, the last elements of the vector are zeroes, due to the\npadding='max_lenght' parameter passed to the function.\nOptionally, you can decide to reduce the size of your dataset if you want to\nmake the training time shorter. In my case, I’ve shrunk the dataset as\nfollows:\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=4\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42)\nSo, I have two sets – one for training, one for testing – of 500 observations\neach. Now that we have our dataset preprocessed and ready, we need the\nmodel to be fine-tuned.\nFine-tuning the model\nAs anticipated in the previous section, the LLM we are going to leverage for\nfine-tuning is the base version of BERT. The BERT model is a transformer-\nbased, encoder-only model for natural language understanding introduced by\nGoogle researchers in 2018. BERT was the first example of a general-\npurpose LLM, meaning that it was the first model to be able to tackle\n",
      "page_number": 407,
      "chapter_number": 48,
      "summary": "In this chapter, we will be covering a full-code approach leveraging Hugging\nFace models and libraries Key topics include grammar, vocabulary, and errors.",
      "keywords": [
        "Hugging Face datasets",
        "Hugging Face",
        "dataset",
        "model",
        "BERT model",
        "Text",
        "tokenizer",
        "Face datasets",
        "Face",
        "Hugging",
        "Hugging Face Transformers",
        "LLM",
        "BERT",
        "learning",
        "leveraging Hugging Face"
      ],
      "concepts": [
        "grammar",
        "vocabulary",
        "errors",
        "smaller",
        "obtaining",
        "negative",
        "size",
        "select",
        "network",
        "neural"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 62,
          "title": "Segment 62 (pages 536-543)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 9,
          "title": "Segment 9 (pages 72-80)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 58,
          "title": "Segment 58 (pages 514-521)",
          "relevance_score": 0.51,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 53,
          "title": "Segment 53 (pages 448-456)",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.47,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 49,
      "title": "Segment 49 (pages 415-422)",
      "start_page": 415,
      "end_page": 422,
      "detection_method": "topic_boundary",
      "content": "multiple NLP tasks at once, which was different from the task-specific\nmodels existing up to that moment.\nNow, even though it might sound a bit “old fashioned” (in fact, compared to\ntoday’s model like the GPT-4, it is not even “large,” with only 340 million\nparameters in its large version), given all the new LLMs that have emerged\nin the market in the last few months, BERT and its fine-tuned variants are\nstill a widely adopted architecture. In fact, it was thanks to BERT that the\nstandard for language models has greatly improved.\nThe BERT model has two main components:\nEncoder: The encoder consists of multiple layers of transformer blocks,\neach with a self-attention layer and a feedforward layer. The encoder\ntakes as input a sequence of tokens, which are the basic units of text,\nand outputs a sequence of hidden states, which are high-dimensional\nvectors that represent the semantic information of each token.\nOutput layer: The output layer is task-specific and can be different\ndepending on the type of task that BERT is used for. For example, for\ntext classification, the output layer can be a linear layer that predicts the\nclass label of the input text. For question answering, the output layer\ncan be two linear layers that predict the start and end positions of the\nanswer span in the input text.\nThe number of layers and parameters of the model depends on the\nmodel version. In fact, BERT comes in two sizes: BERTbase and\nBERTlarge. The following illustration shows the difference between the\ntwo versions:\n\n\nFigure 11.2: A comparison between BERTbase and BERTlarge (source:\nhttps://huggingface.co/blog/bert-101)\nLater, other versions such as BERT-tiny, BERT-mini, BERT-small, and\nBERT-medium were introduced to reduce the computational cost and\nmemory usage of BERT.\nThe model has been trained on a heterogeneous corpus of around 3.3 billion\nwords, belonging to Wikipedia and Google’s BooksCorpus. The training\nphase involved two objectives:\nMasked language modeling (MLM): MLM aims to teach the model to\npredict the original words that are randomly masked (replaced with a\nspecial token) in the input text. For example, given the sentence “He\nbought a new [MASK] yesterday,” the model should predict the word\n“car” or “bike” or something else that makes sense. This objective helps\nthe model learn the vocabulary and the syntax of the language, as well\nas the semantic and contextual relations between words.\nNext sentence prediction (NSP): NSP aims to teach the model to\npredict whether two sentences are consecutive or not in the original\ntext. For example, given the sentences “She loves reading books” and\n“Her favorite genre is fantasy,” the model should predict that they are\nconsecutive because they are likely to appear together in a text.\nHowever, given the sentences “She loves reading books” and “He plays\nsoccer every weekend,” the model should predict that they are not\nconsecutive because they are unlikely to be related. This objective helps\n\n\nthe model learn the coherence and logic of the text, as well as the\ndiscourse and pragmatic relations between sentences.\nBy using these two objectives (on which the model is trained at the same\ntime), the BERT model can learn general language knowledge that can be\ntransferred to specific tasks, such as text classification, question answering,\nand NER. The BERT model can achieve better performance on these tasks\nthan previous models that only use one direction of context or do not use\npre-training at all. In fact, it has achieved state-of-the-art results on many\nbenchmarks and tasks, such as General Language Understanding\nEvaluation (GLUE), Stanford Question Answering Dataset (SQuAD),\nand Multi-Genre Natural Language Inference (MultiNLI).\nThe BERT model is available – along with many fine-tuned versions –in the\nHugging Face Hub. You can instantiate the model as follows:\nimport torch\nfrom transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert\nNote that AutoModelForSequenceClassification is a subclass of AutoModel,\nwhich can instantiate a model architecture suitable for sequence\nclassification, such as text classification or sentiment analysis. It can be used\nfor any task that requires a single label or a list of labels for each input\nsequence. In my case, I set the number of output labels equal to two since we\nare dealing with a binary classification problem.\nOn the other hand, AutoModel is a generic class that can instantiate any\nmodel architecture from the library based on the pretrained model name or\n\n\npath. It can be used for any task that does not require a specific output\nformat, such as feature extraction or language modeling.\nThe final step before starting the training is to define the evaluation metrics\nwe will need to understand how well our model will perform once fine-\ntuned.\nUsing evaluation metrics\nAs we saw in Chapter 1, evaluating an LLM in its general-purpose\napplication might be cumbersome. As those models are trained on unlabeled\ntext and are not task-specific, but rather generic and adaptable given a user’s\nprompt, traditional evaluation metrics were not suitable anymore. Evaluating\nan LLM means, among other things, measuring its language fluency, its\ncoherence, and its ability to emulate different styles depending on a user’s\nrequest.\nHowever, we also saw how an LLM can be used for very specific scenarios,\nas in our binary classification task. If this is the case, evaluation metrics boil\ndown to those commonly used for that scenario.\nNote\nWhen it comes to more conversational tasks like\nsummarization, Q&A, and retrieval-augmented generation, a\nnew set of evaluation metrics needs to be introduced, often\npowered in turn by LLMs. Some of the most popular metrics\nare the following:\nFluency: This assesses how naturally and smoothly the\ngenerated text reads.\n\n\nCoherence: This evaluates the logical flow and\nconnectivity of ideas within a text.\nRelevance: This measures how well the generated\ncontent aligns with the given prompt or context.\nGPT-similarity: This quantifies how closely the\ngenerated text resembles human-written content.\nGroundedness: This assesses whether the generated text\nis based on factual information or context.\nThese evaluation metrics help us understand the quality,\nnaturalness, and relevance of LLM-generated text, guiding\nimprovements and ensuring reliable AI assistance.\nWhen it comes to binary classification, one of the most basic ways to\nevaluate a binary classifier is to use a confusion matrix. A confusion matrix\nis a table that shows how many of the predicted labels match the true labels.\nIt has four cells:\nTrue positive (TP): The number of cases where the classifier correctly\npredicted 1 when the true label was 1.\nFalse positive (FP): The number of cases where the classifier\nincorrectly predicted 1 when the true label was 0.\nTrue negative (TN): The number of cases where the classifier correctly\npredicted 0 when the true label was 0.\nFalse negative (FN): The number of cases where the classifier\nincorrectly predicted 0 when the true label was 1.\nHere is an example of a confusion matrix for the sentiment classifier we are\ngoing to build, knowing that the label 0 is associated with “Negative” and\nthe label 1 with “Positive”:\n\n\nPredicted Positive\nPredicted Negative\nPositive\n20 (TP)\n5 (FN)\nNegative\n3 (FP)\n72 (TN)\nThe confusion matrix can be used to calculate various metrics that measure\ndifferent aspects of the classifier’s performance. Some of the most common\nmetrics are:\nAccuracy: The proportion of correct predictions among all predictions.\nIt is calculated as (TP + TN) / (TP + FP + TN + FN). For example, the\naccuracy of the sentiment classifier is (20 + 72) / (20 + 3 + 72 + 5)\n= 0.92.\nPrecision: The proportion of correct positive predictions among all\npositive predictions. It is calculated as TP / (TP + FP). For example,\nthe precision of the sentiment classifier is 20 / (20 + 3) = 0.87.\nRecall: The proportion of correct positive predictions among all\npositive cases. It is also known as sensitivity or true positive rate. It is\ncalculated as TP / (TP + FN). For example, the recall of the sentiment\nclassifier is 20 / (20 + 5) = 0.8.\nSpecificity: The proportion of correct negative predictions among all\nnegative cases. It is also known as the true negative rate. It is calculated\nas TN / (TN + FP). For example, the specificity of the sentiment\nclassifier is 72 / (72 + 3) = 0.96.\nF1-score: The harmonic mean of precision and recall. It is a measure of\nbalance between precision and recall. It is calculated as 2 * (precision\n\n\n* recall) / (precision + recall). For example, the F1-score of the\nsentiment classifier is 2 * (0.87 * 0.8) / (0.87 + 0.8) = 0.83.\nThere are many other metrics that can be derived from the confusion matrix\nor other sources, such as the decision score or the probability output of the\nclassifier. Some examples are:\nReceiver operating characteristic (ROC) curve: A plot of recall\nversus false positive rate (FP / (FP + TN)), which shows how well the\nclassifier can distinguish between positive and negative cases at\ndifferent thresholds.\nArea under the ROC curve (AUC): The AUC, which measures how\nwell the classifier can rank positive cases higher than negative cases. It\ncan be illustrated in the following diagram, where the ROC curve and\nthe area under the curve are displayed:\n\n\nFigure 11.3: Illustration of a ROC curve, hightlighting a perfect classifier and the Area Under the\nCurve (AUC)\nIn our case, we will simply use the accuracy metric by following these steps:\n1. You can import this metric from the evaluate library as follows:\nimport numpy as np\nimport evaluate\nmetric = evaluate.load(\"accuracy\")\n2. We also need to define a function that computes the accuracy given the\noutput of the training phase:\n",
      "page_number": 415,
      "chapter_number": 49,
      "summary": "This chapter covers segment 49 (pages 415-422). Key topics include model, text, and evaluation. The BERT model has two main components:\nEncoder: The encoder consists of multiple layers of transformer blocks,\neach with a self-attention layer and a feedforward layer.",
      "keywords": [
        "model",
        "BERT model",
        "BERT",
        "text",
        "classifier",
        "multiple NLP tasks",
        "positive",
        "NLP tasks",
        "sentiment classifier",
        "Output layer",
        "negative",
        "metrics",
        "evaluation metrics",
        "multiple NLP",
        "cases"
      ],
      "concepts": [
        "model",
        "text",
        "evaluation",
        "evaluating",
        "evaluates",
        "language",
        "metrics",
        "positions",
        "positive",
        "masked"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 45,
          "title": "Segment 45 (pages 386-394)",
          "relevance_score": 0.69,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 9,
          "title": "Segment 9 (pages 72-80)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 474-483)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 50,
      "title": "Segment 50 (pages 423-432)",
      "start_page": 423,
      "end_page": 432,
      "detection_method": "topic_boundary",
      "content": "def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, ref\n3. Finally, we need to set our evaluation strategy, which means how often\nwe want our model to be tested against the test set while training:\nfrom transformers import TrainingArguments, Trainer\ntraining_args = TrainingArguments(output_dir=\"test_tra\nevaluation_strategy=\"epoch\")\nIn our case, we will set epoch as the evaluation strategy, meaning that\nthe evaluation is done at the end of each epoch.\nDefinition\nAn epoch is a term used in machine learning to describe one\ncomplete pass through the entire training dataset. It is a\nhyperparameter that can be tuned to improve the\nperformance of a machine-learning model. During an epoch,\nthe model’s weights are updated based on the training data\nand the loss function. An epoch can consist of one or more\nbatches, which are smaller subsets of the training data. The\nnumber of batches in an epoch depends on the batch size,\nwhich is another hyperparameter that can be adjusted.\nNow we have all the ingredients needed to start our fine-tuning, which will\nbe covered in the next section.\n\n\nTraining and saving\nThe last component we need to fine-tune our model is a Trainer object. The\nTrainer object is a class that provides an API for feature-complete training\nand evaluation of models in PyTorch, optimized for Hugging Face\nTransformers. You can follow these steps:\n1. Let’s first initialize our Trainer by specifying the parameters we’ve\nalready configured in the previous steps. More specifically, the Trainer\nwill need a model, some configuration args (such as the number of\nepochs), a training dataset, an evaluation dataset, and the type of\nevaluation metric to compute:\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n)\n2. You can then initiate the process of fine-tuning by calling the trainer\nas follows:\ntrainer.train()\nDepending on your hardware, the training process might take some\ntime. In my case, given the reduced size of the dataset and the low\nnumber of epochs (only 2), I don’t expect exceptional results.\nNevertheless, the training results for only two epochs in terms of\naccuracy are the following:\n\n\n{'eval_loss': 0.6720085144042969, 'eval_accuracy': 0.58, 'e\n{'eval_loss': 0.5366445183753967, 'eval_accuracy': 0.82, 'e\nAs you can see, between the two epochs the model gained an accuracy\nimprovement of 41.38%, hitting a final accuracy of 82%. Considering\nthe aforementioned elements, that’s not bad!\n3. Once the model is trained, we can save it locally, specifying the path as\nfollows:\ntrainer.save_model('models/sentiment-classifier')\n4. To consume and test the model, you can load it with the following code:\nmodel = AutoModelForSequenceClassification.from_pretra\n5. Finally, we need to test our model. To do so, let’s pass a sentence to the\nmodel (to be first tokenized) on which it can perform sentiment\nclassification:\ninputs = tokenizer(\"I cannot stand it anymore!\", retur\noutputs = model(**inputs)\noutputs\nThis yields the following output:\nSequenceClassifierOutput(loss=None, logits=tensor([[ 0.6467\n\n\nNote that the model output is a SequenceClassifierOutput object,\nwhich is the base class for outputs of sentence classification models.\nWithin this object, we are interested in the logit tensor, which is the\nvector of raw (non-normalized) predictions associated with labels that\nour classification model generated.\n6. Since we are working with tensors, we will need to leverage the\ntensorflow library in Python. Plus, we will use the softmax function to\nobtain the probability vector associated with each label, so that we\nknow that the final result corresponds to the label with the greatest\nprobability:\nimport tensorflow as tf\npredictions = tf.math.softmax(outputs.logits.detach(),\nprint(predictions)\nThe following is the obtained output:\ntf.Tensor([[0.6571879  0.34281212]], shape=(1, 2), dtype=fl\nOur model tells us that the sentiment of the sentence “I can’t stand it\nanymore” is negative, with a probability of 65.71%.\n7. Note that you can also save the model in your Hugging Face account.\nTo do so, you first need to allow the notebook to push the code to your\naccount as follows:\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\n\n8. You will be prompted to the Hugging Face login page, where you have\nto input your access token. Then, you can save the model, specifying\nyour account name and model name:\ntrainer.push_to_hub('vaalto/sentiment-classifier')\nBy doing so, this model can be consumed via the Hugging Face Hub as\neasily as we saw in the previous chapter, as shown in the following\nscreenshot:\nFigure 11.4: Model card within the Hugging Face Hub space\nFurthermore, you can also decide to make the model public, so that everyone\nwithin Hugging Face can test and consume your creation.\nIn this section, we fine-tuned a BERT model with just a few lines of code,\nthanks to Hugging Face libraries and accelerators. Again, if your goal is\n\n\nreducing the code amount, you can leverage the low-code AutoTrain\nplatform hosted in Hugging Face to train and fine-tune models.\nHugging Face is definitely a solid platform for training your open-source\nLLM. In addition to that, there are further platforms you might want to\nleverage since proprietary models can also be fine-tuned. For example,\nOpenAI lets you fine-tune the GPT series with your own data, providing the\ncomputational power to train and host your customized models.\nOverall, fine-tuning can be the icing on the cake that makes your LLM\nexceptional for your use case. Deciding a strategy to do so based on the\nframework we explored at the beginning is a pivotal step in building a\nsuccessful application.\nSummary\nIn this chapter, we covered the process of fine-tuning LLMs. We started with\na definition of fine-tuning and general considerations to take into account if\nyou have to decide to fine-tune your LLM.\nWe then went hands-on with practical sections on fine-tuning. We covered a\nscenario where, starting from a base BERT model, we wanted a powerful\nreview sentiment analyzer. To do so, we fine-tuned the base model on the\nIMDB dataset using a full-code approach with Hugging Face Python\nlibraries.\nFine-tuning is a powerful technique to further customize LLMs toward your\ngoal. However, along with many other aspects of LLMs, it comes with some\nconcerns and considerations in terms of ethics and security. In the next\nchapter, we are going to delve deeper into that, sharing how to establish\n\n\nguardrails with LLMs and, more generally, how governments and countries\nare approaching the problem from a regulatory perspective.\nReferences\nTraining dataset: https://huggingface.co/datasets/imdb\nHF AutoTrain:\nhttps://huggingface.co/docs/autotrain/index\nBERT paper: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina\nToutanova, 2019, BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding:\nhttps://arxiv.org/abs/1810.04805\nJoin our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\nOceanofPDF.com\n\n\n12\nResponsible AI\nIn Part 2 of this book, we covered multiple applications of large language\nmodels (LLMs), gathering also a deeper understanding of how many factors\ncould influence their behavior and outputs. In fact, LLMs open the doors to a\nnew set of risks and biases to be taken into account while developing LLM-\npowered applications, in order to mitigate them with defensive attacks.\nIn this chapter, we are going to introduce the fundamentals of the discipline\nbehind mitigating the potential harms of LLMs – and AI models in general –\nwhich is Responsible AI. We will then move on to the risks associated with\nLLMs and how to prevent or at least mitigate them using proper techniques.\nBy the end of this chapter, you will have a deeper understanding of how to\nprevent LLMs from making your application potentially harmful.\nWe will cover the following key topics:\nWhat is Responsible AI and why do we need it?\nResponsible AI architecture\nRegulations surrounding Responsible AI\nWhat is Responsible AI and\nwhy do we need it?\n\n\nResponsible AI refers to the ethical and accountable development,\ndeployment, and use of AI systems. It involves ensuring fairness,\ntransparency, privacy, and avoiding biases in AI algorithms. Responsible AI\nalso encompasses considerations for the social impact and consequences of\nAI technologies, promoting accountability and human-centric design.\nResponsible AI plays a crucial role in steering decisions toward positive and\nfair results. This involves prioritizing people and their objectives in the\ndesign of systems while upholding enduring values such as fairness,\nreliability, and transparency.\nSome ethical implications of Responsible AI are:\nBias: AI systems can inherit biases present in their training data. These\nbiases can lead to discriminatory outcomes, reinforcing existing\ninequalities.\nExplainability: Black-box models (such as LLMs) lack interpretability.\nEfforts are being made to create more interpretable models to enhance\ntrust and accountability.\nData protection: Collecting, storing, and processing data responsibly is\nessential. Consent, anonymization, and data minimization principles\nshould guide AI development.\nLiability: Determining liability for AI decisions (especially in critical\ndomains) remains a challenge. Legal frameworks need to evolve to\naddress this.\nHuman oversight: AI should complement human decision-making\nrather than replace it entirely. Human judgment is essential, especially\nin high-stakes contexts.\nEnvironmental impact: Training large models consumes significant\nenergy. Responsible AI considers environmental impacts and explores\n\n\nenergy-efficient alternatives.\nSecurity: Ensuring AI systems are secure and resistant to attacks is\ncrucial.\nAs an example of addressing these implications, Microsoft has established a\nframework called the Responsible AI Standard\n(https://blogs.microsoft.com/wp-\ncontent/uploads/prod/sites/5/2022/06/Microsoft-\nResponsible-AI-Standard-v2-General-Requirements-\n3.pdf), outlining six principles:\nFairness\nReliability and safety\nPrivacy and security\nInclusiveness\nTransparency\nAccountability\nIn the context of generative AI, Responsible AI would mean creating models\nthat respect these principles. For instance, the generated content should be\nfair and inclusive, not favoring any particular group or promoting any form\nof discrimination. The models should be reliable and safe to use. They\nshould respect user’s privacy and security. The process of generation should\nbe transparent, and there should be mechanisms for accountability.\nResponsible AI architecture\nGenerally speaking, there are many levels at which we can intervene to\nmake a whole LLM-powered application safer and more robust: the model\n",
      "page_number": 423,
      "chapter_number": 50,
      "summary": "The\nTrainer object is a class that provides an API for feature-complete training\nand evaluation of models in PyTorch, optimized for Hugging Face\nTransformers Key topics include model, training, and responsible.",
      "keywords": [
        "Hugging Face",
        "model",
        "Hugging Face Hub",
        "Responsible",
        "training",
        "Face",
        "Hugging",
        "Hugging Face Python",
        "LLMs",
        "Hugging Face account",
        "dataset",
        "epoch",
        "training dataset",
        "eval",
        "Hugging Face login"
      ],
      "concepts": [
        "model",
        "training",
        "responsible",
        "responsibly",
        "outputs",
        "llms",
        "application",
        "applications",
        "sentiment",
        "predictions"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 62,
          "title": "Segment 62 (pages 536-543)",
          "relevance_score": 0.47,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 53,
          "title": "Segment 53 (pages 448-456)",
          "relevance_score": 0.46,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 34,
          "title": "Segment 34 (pages 681-701)",
          "relevance_score": 0.44,
          "method": "sentence_transformers"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 29,
          "title": "Segment 29 (pages 245-254)",
          "relevance_score": 0.43,
          "method": "sentence_transformers"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 34,
          "title": "Segment 34 (pages 290-296)",
          "relevance_score": 0.43,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 51,
      "title": "Segment 51 (pages 433-440)",
      "start_page": 433,
      "end_page": 440,
      "detection_method": "topic_boundary",
      "content": "level, the metaprompt level, and the user interface level. This architecture\ncan be illustrated as follows:\nFigure 12.1: Illustration of different mitigation layers for LLM-powered applications\nOf course, it is not always possible to work at all levels. For example, in the\ncase of ChatGPT, we consume a pre-built application with a black-box\nmodel and a fixed UX, so we have little room for intervention only at the\nmetaprompt level. On the other hand, if we leverage open-source models via\nan API, we can act up to the model level to incorporate Responsible AI\nprinciples. Let’s now see a description of each layer of mitigation.\nModel level\nThe very first level is the model itself, which is impacted by the training\ndataset we train it with. In fact, if the training data is biased, the model will\ninherit a biased vision of the world.\n\n\nOne example was covered in the paper Men Also Like Shopping: Reducing\nGender Bias Amplification using Corpus-level Constraints by Zhao et al.,\nwhere authors show an example of model bias in the field of computer\nvision, as shown in the following illustration:\nFigure 12.2: Example of sexism and bias of a vision model. Adapted from\nhttps://aclanthology.org/D17-1323.pdf, licensed under CC BY 4.0\n\n\nThe model wrongly identifies a man cooking as a woman, since it associates\nthe activity of cooking with women with a greater probability, given the bias\nof the examples the model was trained on.\nAnother example traces back to the first experiments with ChatGPT, in\nDecember 2022, when it exhibited some sexist and racist comments. A\nrecent tweet highlighted this example, asking ChatGPT to create a Python\nfunction assessing a person’s aptitude as a scientist based on their race and\ngender.\nFigure 12.3: Inner bias of ChatGPT back in December 2022. Source:\nhttps://twitter.com/spiantado/status/1599462375887114240\nAs you can see, the model created a function that linked the probability of\nbeing a good scientist to race and gender, which is something that the model\nshouldn’t have created in the first place.\nTo act at the model level, there are some areas that researchers and\ncompanies should look at:\nRedact and curate training data: The primary goal of language\nmodeling is to faithfully represent the language found in the training\ncorpus. As a result, it is crucial to edit and carefully select the training\n\n\ndata. For example, in the scenario of the vision model previously\ndescribed, the training dataset should have been curated in such a way\nthat a man cooking did not represent a minority.\nNote\nThere are various toolkits available to developers to\nmake training datasets more “responsible.” A great\nopen-source example is the Python Responsible AI\nToolbox, a collection of tools and libraries designed to\nhelp developers incorporate Responsible AI practices\ninto their workflows. These tools aim to address various\naspects of AI development, including fairness,\ninterpretability, privacy, and security, to ensure that AI\nsystems are safe, trustworthy, and ethical. Specifically,\nthe toolkit includes resources to examine datasets for\npotential biases and ensure that models are fair and\ninclusive, providing metrics to assess group fairness and\ntools to mitigate identified biases; other tools\nspecifically focus on analyzing the balance of the\ndataset, providing metrics and techniques to address\nimbalances that could lead to biased model\nperformance.\nFine-tune language models: Adjust weightings to prevent bias and\nimplement checks to filter harmful language. There are many open-\nsource datasets with this goal, and you can also find a list of aligned\nfine-tuning datasets at the following GitHub repository:\n\n\nhttps://github.com/Zjh-819/LLMDataHub#general-\nopen-access-datasets-for-alignment-.\nUse reinforcement learning with human feedback (RLHF): As\ncovered in Chapter 1, RLHF is an additional layer of LLMs’ training\nthat consists of adjusting a model’s weights according to human\nfeedback. This technique, in addition to making the model more\n“human-like,” is also pivotal in making it less biased, since any harmful\nor biased content will be penalized by the human feedback.\nOpenAI employs this strategy to avoid language models generating\nharmful or toxic content, ensuring that the models are geared toward\nbeing helpful, truthful, and benign. This is part of the whole training\nprocess of OpenAI’s models before they are released to the public\n(specifically, ChatGPT went through this development stage before\nbeing accessible).\nMaking LLMs align with human principles and preventing them from being\nharmful or discriminatory is a top priority among companies and research\ninstitutes that are in the process of developing LLMs. It is also the first layer\nof mitigation toward potential harms and risks, yet it might be not enough to\nfully mitigate the risk of adopting LLM-powered applications. In the next\nsection, we are going to cover the second layer of mitigation, which is the\none related to the platform adopted to host and deploy your LLMs.\nMetaprompt level\nIn Chapter 4, we learned how the prompt and, more specifically, the\nmetaprompt or system message associated with our LLM is a key\ncomponent to make our LLM-powered application successful, to the point\n\n\nthat a new whole discipline has arisen in the last few months: prompt\nengineering.\nSince the metaprompt can be used to instruct a model to behave as we wish,\nit is also a powerful tool to mitigate any harmful output it might generate.\nThe following are some guidelines on how to leverage prompt engineering\ntechniques in that sense:\nClear guidelines: Providing clear instructions and guidelines to the AI\nmodel about what it can and cannot do. This includes setting boundaries\non the type of content it can generate, ensuring it respects user privacy,\nand ensuring it does not engage in harmful or inappropriate behavior.\nTransparency: Being transparent about how the AI model works, its\nlimitations, and the measures in place to ensure responsible use. This\nhelps build trust with users and allows them to make informed\ndecisions about using AI.\nEnsure grounding: Implementing grounding strategies on top of the\nprovided data can ensure the model does not hallucinate or provide\nharmful information.\nNote that, due to its centrality in these new application architectures, the\nprompt is also a potential subject of prompt injection; henceforth, it should\nalso include some defensive techniques to prevent this attack.\nDefinition\nPrompt injection stands as a form of attack on LLMs,\nwherein an AI employing a specific metaprompt for a task is\ndeceived by adversarial user input, leading it to execute a\ntask diverging from its original purpose.\n\n\nPrompt injection can be of different types:\nPrompt leakage (or direct prompt injection): When there is a malicious\nactivity that accesses the meta prompt of an LLM and changes it. For\nexample, from the defined metaprompt “You are an AI assistant that\ntranslates everything to French,” an attacker could leak the prompt and\nchange it to “You are an AI assistant that translates everything to\nGerman.”\nGoal hijacking (or indirect prompt injection): When the malicious\nactivity finds target prompts to feed the model with that are capable of\nbypassing the metaprompt instructions. In this context, there are plenty\nof prompts that have been tested as capable of jailbracking the\nmetaprompt instructions. An example of one of these prompts, which\nemerged in the first few months after ChatGPT’s launch, has been\ncoined as Do Anything Now (DAN) and is meant to bypass the content\nsafety restrictions embedded within ChatGPT.\nThe following lines are the start of one of the versions of this prompt\n(you can find a whole repository about DAN prompts at\nhttps://github.com/0xk1h0/ChatGPT_DAN#chatgpt-\ndan-and-other-jailbreaks):\nHello ChatGPT. You are about to immerse yourself into the \nThere are some defensive techniques you can use to prevent prompt\ninjections. One of the most remarkable of these techniques is called\nAdversarial Prompt Detector. It consists of enforcing the desired behavior\nthrough the instruction given to the model. While this doesn’t necessarily\n\n\nprovide a comprehensive solution, it underscores the effectiveness of a well-\nformulated prompt.\nThe third and final mitigation layer is at the user interface level, and we are\ngoing to cover it in the next section.\nUser interface level\nThe user interface represents the last mile for an LLM-powered application\nto mitigate the potential associated risks. In fact, the way the user can\nactually interact with the LLM in the backend is a powerful tool to control\nthe incoming and outgoing tokens.\nFor example, in Chapter 9, while examining some code-related scenarios,\nwe saw how the StarCoder model is used in GitHub as a completion copilot\nfor the user. In this case, the user has a closed-ended experience, in the sense\nthat they cannot ask direct questions to the model; rather, it receives\nsuggestions based on the code it writes.\nAnother example is in Chapter 7, where we developed a movie\nrecommendation application with a UX that encourages the user to insert\nsome hardcoded parameters, rather than asking an open-ended question.\nGenerally speaking, there are some principles that you might want to take\ninto account while designing the UX for your LLM-powered application:\nDisclose the LLM’s role in the interaction: This can help make\npeople aware that they are interacting with an AI system that might also\nbe inaccurate.\nCite references and sources: Let the model disclose to the user the\nretrieved documentation that has been used as the context to respond.\nThis holds true if there is a vector search within a custom VectorDB, as\n",
      "page_number": 433,
      "chapter_number": 51,
      "summary": "This chapter covers segment 51 (pages 433-440). Key topics include prompt, model, and applications. Model level\nThe very first level is the model itself, which is impacted by the training\ndataset we train it with.",
      "keywords": [
        "model",
        "prompt",
        "model level",
        "level",
        "user interface level",
        "user",
        "prompt injection",
        "metaprompt level",
        "training",
        "metaprompt",
        "ChatGPT",
        "interface level",
        "Bias",
        "harmful",
        "vision model"
      ],
      "concepts": [
        "prompt",
        "model",
        "applications",
        "application",
        "biased",
        "providing",
        "provide",
        "user",
        "harmful",
        "specifically"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 23,
          "title": "Segment 23 (pages 457-476)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 21,
          "title": "Segment 21 (pages 417-434)",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 103-122)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 7,
          "title": "Segment 7 (pages 50-57)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 52,
      "title": "Segment 52 (pages 441-448)",
      "start_page": 441,
      "end_page": 448,
      "detection_method": "topic_boundary",
      "content": "well as when we provide the model with external tools, such as the\npossibility to navigate the web (as we saw with our GlobeBotter\nassistant in Chapter 6).\nShow the reasoning process: This helps the user to decide whether the\nratio behind the response is coherent and useful for its purpose. It is\nalso a way to be transparent and provide the user with all the necessary\ninformation about the output it is given. In Chapter 8, we covered a\nsimilar scenario while asking the LLM to show the reasoning as well as\nthe SQL query run against the provided database when given a user’s\nquery:\n\n\nFigure 12.4: Example of transparency with DBCopilot\n\n\nShow the tools used: When we extend an LLM’s capabilities with\nexternal tools, we want to make sure the model uses them properly.\nHenceforth, it is a best practice to inform the user about which tool the\nmodel uses and how. We saw an example of that in Chapter 10, while\nexamining the case of the agentic approach to building multimodal\napplications.\nPrepare pre-defined questions: Sometimes, LLMs don’t know the\nanswer – or even worse, hallucinate – simply because users don’t know\nhow to properly ask a question. To address this risk, a best practice\n(especially in conversational applications) is that of encouraging the\nusers with pre-defined questions to start with, as well as follow-up\nquestions given a model’s answer. This can reduce the risk of poorly\nwritten questions as well as give a better UX to the user. An example of\nthis technique can be found in Bing Chat, a web copilot developed by\nMicrosoft and powered by GPT-4:\n\n\nFigure 12.5: UX of Bing Chat with pre-defined questions\nProvide system documentation: Making users aware of the type of AI\nsystem they interact with is a pivotal step if you want to embed\nResponsible AI within your application. To achieve that, you might\nwant to educate the users with comprehensive system documentation,\ncovering the system’s capabilities, constraints, and risks. For example,\ndevelop a “learn more” page for easy access to this information within\nthe system.\nPublish user guidelines and best practices: Facilitate effective system\nutilization for users and stakeholders by disseminating best practices,\nsuch as crafting prompts and reviewing generated content before\n\n\nacceptance. Integrate these guidelines and best practices directly into\nthe UX whenever feasible.\nIt is important to establish a systematic approach to assess the effectiveness\nof implemented mitigations in addressing potential harms, as well as\ndocument measurement results and regularly review them to iteratively\nenhance a system’s performance.\nOverall, there are different levels where you could intervene to mitigate risks\nassociated with LLMs. From the model level to UX, it is pivotal to\nincorporate these considerations and best practices while developing your\nLLM-powered application.\nHowever, it’s important to note that Responsible AI is not just about the\ntechnology itself but also its use and impact on society. Therefore, it’s\ncrucial to consider ethical aspects and societal implications when developing\nand deploying these systems.\nRegulations surrounding\nResponsible AI\nRegulation of AI is becoming increasingly systematic and stringent, with\nnumerous proposals on the table.\nIn the United States, the government, particularly under the Biden-Harris\nadministration, has proactively implemented measures to ensure responsible\nAI usage. This includes initiatives like the Blueprint for an AI Bill of Rights,\nan AI Risk Management Framework, and a National AI Research Resource\nroadmap. President Biden’s Executive Order emphasizes eliminating bias in\nfederal agencies’ use of new technologies, including AI. Collaborative\nefforts from agencies like the Federal Trade Commission and the Equal\n\n\nEmployment Opportunity Commission showcase a commitment to\nprotecting Americans from AI-related harm.\nIn Europe, the European Commission proposed the Artificial Intelligence\nAct (AI Act), which seeks to establish a comprehensive regulatory\nframework for AI that applies to the following stakeholders:\nProviders: Organizations or individuals who develop, deploy, or offer\nAI systems in the EU are subject to the AI Act. This includes both\nprivate and public entities.\nUsers: Users who utilize AI systems within the EU fall under the scope\nof the regulation. This includes businesses, government agencies, and\nindividuals.\nImporters: Entities that import AI systems into the EU market are also\nsubject to compliance with the AI Act.\nDistributors: Distributors who place AI systems on the EU market are\nresponsible for ensuring that these systems comply with the regulation.\nThird-country entities: Even entities located outside the EU that\nprovide AI services or products to EU residents are subject to certain\nprovisions of the AI Act.\nBy categorizing AI systems by risk, the AI Act outlines the development and\nuse of requirements to promote human-centric and trustworthy AI. The Act\naims to safeguard health, safety, fundamental rights, democracy, the rule of\nlaw, and the environment. It empowers citizens to file complaints,\nestablishes an EU AI Office for enforcement, and mandates member states to\nappoint national supervisory authorities for AI. The Act aligns with\nResponsible AI principles, emphasizing fairness, accountability,\ntransparency, and ethics. The idea is to ensure that:\n\n\nProviders of generative AI systems must train, design, and develop their\nsystems with state-of-the-art safeguards against generating content that\nbreaches EU laws.\nProviders are required to document and provide a publicly available\ndetailed summary of their use of copyrighted training data.\nProviders must adhere to more stringent transparency obligations.\nIf a generative AI system has been used to create “deep fakes,” users\nwho created such content must disclose that it was generated or\nmanipulated by AI.\nThe AI Act represents a significant step toward ensuring that AI technologies\nare developed and used in a way that benefits society, while respecting\nfundamental human rights and values. In 2023, amid the rapid growth of\ngenerative AI technologies, significant strides were made regarding the AI\nAct:\nBy June 14, 2023, the European Parliament had endorsed its stance on\nthe AI Act, securing 499 votes in favor, 28 against, and 93 abstentions.\nNoteworthy amendments were introduced to the proposal for a\nregulation, titled the AI Act, with the aim of establishing unified\nregulations on AI and modifying certain European Union legislative\nacts.\nApproved in December 2023, the AI Act allows a grace period of 2 to 3\nyears for preparation before its activation.\nThese developments signify the ongoing progress of the AI Act toward its\nimplementation, positioning the EU as a potential trailblazer in introducing\noversight or regulation for generative AI, given the advanced negotiations\nwithin the European Commission.\n\n\nOverall, governments around the world are scrambling to figure out how to\napproach the questions posed by AI. These advancements reflect a growing\nrecognition of the need for Responsible AI and the role of government in\nensuring it.\nSummary\nIn this chapter, we covered the “dark side” of generative AI technologies,\nexposing its associated risks and biases, such as hallucinations, harmful\ncontent, and discrimination. To reduce and overcome those risks, we\nintroduced the concept of Responsible AI, starting with a deep dive into the\ntechnical approach we can have while developing LLM-powered\napplications; we covered the different levels of risk mitigation – model,\nmetaprompt, and UX – and then moved on to the broader topic of\ninstitutional regulations. In this context, we examined the advancements that\nhave been carried out by governments in the last year, with a focus on the AI\nAct.\nResponsible AI is an evolving field of research, and it definitely has an\ninterdisciplinary flavor. There will probably be an acceleration at the\nregulation level to address it in the near future.\nIn the next and final chapter, we are going to cover all the emerging trends\nand innovations happening in the generative AI field with a glimpse of what\nwe could expect from the near future.\nReferences\nReducing Gender Bias Amplification using Corpus-level Constraints:\nhttps://browse.arxiv.org/pdf/1707.09457.pdf\n",
      "page_number": 441,
      "chapter_number": 52,
      "summary": "This chapter covers segment 52 (pages 441-448). Key topics include acts, provide.",
      "keywords": [
        "Act",
        "users",
        "systems",
        "Responsible",
        "model",
        "questions",
        "possibility to navigate",
        "GlobeBotter assistant",
        "European Commission",
        "pre-defined questions",
        "Regulation",
        "Show the reasoning",
        "Bing Chat",
        "risk",
        "generative"
      ],
      "concepts": [
        "act",
        "acts",
        "provide",
        "provided",
        "regulations",
        "regulation",
        "commission",
        "developed",
        "developments",
        "user"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 3",
          "chapter": 13,
          "title": "Segment 13 (pages 129-137)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "Segment 32 (pages 270-277)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 474-483)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 7,
          "title": "Segment 7 (pages 50-57)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 53,
      "title": "Segment 53 (pages 449-458)",
      "start_page": 449,
      "end_page": 458,
      "detection_method": "topic_boundary",
      "content": "ChatGPT racist and sexist outputs:\nhttps://twitter.com/spiantado/status/15994623758\n87114240\nGitHub repository for an aligned dataset:\nhttps://github.com/Zjh-819/LLMDataHub#general-\nopen-access-datasets-for-alignment-\nAI Act:\nhttps://www.europarl.europa.eu/RegData/etudes/BR\nIE/2021/698792/EPRS_BRI(2021)698792_EN.pdf\nPrompt hijacking:\nhttps://arxiv.org/pdf/2211.09527.pdf\nAI Act:\nhttps://www.europarl.europa.eu/news/en/headlines\n/society/20230601STO93804/eu-ai-act-first-\nregulation-on-artificial-intelligence\nBlueprint for an AI Bill of Rights:\nhttps://www.whitehouse.gov/ostp/ai-bill-of-\nrights/\nJoin our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\n\n\nOceanofPDF.com\n\n\n13\nEmerging Trends and\nInnovations\nDear reader, if you have made it up to this point – congratulations! You\nmanaged to complete this journey into LLMs and how to implement modern\napplications with them. Starting from the fundamentals of what’s under the\nhood of an LLM, we covered many scenarios of LLM-powered applications,\nfrom conversational chatbots, to database copilots, to multimodal agents. We\nexperimented with different models, both proprietary and open-source, and\nwe also managed to fine-tune our own LLM. Last but not least, we covered\nthe key topic of Responsible AI and how to embed ethical considerations\nwithin our LLM-powered applications.\nIn this final chapter, we are going to explore the latest advancements and\nfuture trends in the field of generative AI. Note that, as a rapidly evolving\nfield, it is nearly impossible to keep up with up-to-date releases.\nNevertheless, the advancements covered in this chapter will give you an idea\nof what to expect in the near future.\nWe will cover the following topics:\nThe latest trends in language models and generative AI\nCompanies embracing generative AI\n\n\nThe latest trends in language\nmodels and generative AI\nAs we saw in the previous chapters, LLMs set the basis for extremely\npowerful applications. Starting with LLMs, over the last months we have\nwitnessed an explosive advancement in generative models, from\nmultimodality to newly born frameworks, to enable multi-agent applications.\nIn the next sections, we will see some examples of these new releases.\nGPT-4V(ision)\nGPT-4V(ision) is a large multimodal model (LMM) developed by OpenAI\nand officially released in September 2023. It enables users to instruct GPT-4\nto analyze image inputs provided by the user. This integration of image\nanalysis into LLMs represents a significant advancement in AI research and\ndevelopment. Model multimodality was achieved by using a technique\ncalled image tokenization, which converts images into sequences of tokens\nthat can be processed by the same model as text. This allows the model to\nhandle different types of data, such as text and images, and generate outputs\nthat are consistent and coherent across modalities.\nSince its initial trials in April 2023, GPT-4V has shown remarkable abilities\nin various domains. Moreover, many businesses have begun to integrate this\nmodel in their early testing stages. One of the successful examples is Be My\nEyes, an app that assists the population of more than 250 million people who\nhave visual impairments or blindness. The app links people who have low\nvision or blindness with helpers who can assist them with everyday\nactivities, such as recognizing a product or finding their way around an\nairport. Using the new visual input feature of GPT-4, Be My Eyes created a\n\n\nVirtual Volunteer™ in its app that uses GPT-4. This Virtual Volunteer can\nproduce the same amount of context and comprehension as a human\nvolunteer.\nThe GPT-4 technology can do more than just identify and label what’s in a\npicture; it can also infer and examine the situation. For instance, it can look\nat the items in a fridge and recommend what you can cook with them. What\nsets GPT-4 apart from other language and machine learning models is its\ncapability to engage in dialogue and the higher level of analytical skill that\nthe technology provides. Simple image recognition applications only\nidentify what you see. They can’t converse to find out if the noodles are\nmade with the proper ingredients or if the thing on the floor is not just a ball\nbut also liable to trip you up—and tell you that.\nIn response to early experimentation on GPT-4V before it went public,\nOpenAI has implemented several mitigations to address risks and biases.\nThese mitigations are aimed at improving a model’s safety and reducing the\npotential harm caused by its output:\nRefusal system: OpenAI has added refusals for certain types of\nobviously harmful generations in GPT-4V. This system helps prevent a\nmodel from generating content that promotes hate groups or contains\nhate symbols.\nEvaluation and red teaming: OpenAI has performed assessments and\nconsulted with external experts to examine the strengths and\nweaknesses of GPT-4V. This process helps detect potential flaws and\nrisks in a model’s output. The assessments cover areas such as scientific\ncompetence, medical guidance, stereotyping, disinformation threats,\nhateful content, and visual vulnerabilities.\n\n\nScientific competence: Red teamers evaluated GPT-4V’s abilities and\nchallenges in scientific domains. While the model demonstrated the\nskill to comprehend complex information in images and verify claims\nin scientific papers, it also showed challenges, such as the occasional\nmixing of separate text elements and the possibility of factual mistakes.\nHateful content: GPT-4V declines to answer questions about hate\nsymbols and extremist content in some cases. However, the model’s\nbehavior may be variable, and it may not always decline to generate\ncompletions related to less-known hate groups or symbols. OpenAI\nrecognizes the need for further enhancements in addressing hateful\ncontent.\nUngrounded inferences: OpenAI has implemented mitigations to\naddress risks associated with ungrounded inferences. The model now\nrefuses requests for ungrounded inferences about people, reducing the\npotential for biased or inaccurate responses. OpenAI aims to refine\nthese mitigations to enable the model to answer questions about people\nin low-risk contexts in the future.\nDisinformation risks: GPT-4V’s ability to generate text content\ntailored to image input poses increased risks with disinformation.\nOpenAI acknowledges the need for proper risk assessment and context\nconsideration when using the model in relation to disinformation. The\ncombination of generative image models and GPT-4V’s text generation\ncapabilities may impact disinformation risks, but additional mitigations\nsuch as watermarking or provenance tools may be necessary.\nThese mitigations, along with the contribution from existing safety measures\nand ongoing research, aim to improve safety and reduce the biases in GPT-\n4V. OpenAI acknowledges the dynamic and challenging nature of addressing\n\n\nthese risks and remains committed to refining and improving a model’s\nperformance in future iterations.\nOverall, the GPT-4V has unveiled extraordinary capabilities and paves the\nway for multimodality within LLM-powered applications.\nDALL-E 3\nThe newest version of OpenAI’s image-generation tool, DALL-E 3, came\nout in October 2023. The most significant update from previous versions is\nits improved accuracy and faster speed when generating images from text. It\naims to render more detailed, expressive, and specific images that align more\nclosely with a user’s specifications. In fact, even with the same prompt,\nDALL-E 3 shows great improvements compared to its previous version:\nFigure 13.1: Images generated from the prompt “an expressive oil painting of a basketball player\ndunking, depicted as an explosion of a nebula” by DALLE-2 (left) and DALL-E 3 (right). Source:\nhttps://openai.com/dall-e-3\nDALL-E 3 has more safeguards and rules to avoid creating images that\ncontain adult, violent, or hateful content.\n\n\nDALL-E 3 is now available to ChatGPT Plus and Enterprise customers\nvia the API and in OpenAI Playground. It’s also been integrated with\nMicrosoft’s Bing Chat.\nAutoGen\nIn October 2023, Microsoft released a new open-source project called\nAutoGen. It is a Python lightweight framework that allows multiple LLM-\npowered agents to cooperate with each other to solve users’ tasks. For an\noverview of what the cooperation frameworks look like, you can refer to\nhttps://github.com/microsoft/autogen/tree/main.\nEarlier in Part 2 of this book, we covered many scenarios of LangChain\nAgents leveraging external tools. In those scenarios, we had one agent\npowered by an LLM that dynamically decided which tool to use to solve a\nuser’s query. AutoGen works differently, in the sense that it lets different\nagents, each one acting with a specific role and expertise, cooperate to\naddress the user’s query. The main element of novelty here is that each agent\ncan actually generate output that serves as input to other agents, as well as\ngenerate and modify the plan to be executed. That is the reason why the\nframework has also been designed to keep a human or admin in the loop, to\nactually approve or discard actions and executions.\nAccording to the original paper AutoGen: Enabling Next-Gen LLM\nApplications via Multi-Agent Conversation by Wu et al., there are three main\nreasons why the multi-agent conversation exhibits great performance:\nFeedback incorporation: Since LLMs have the capacity to elaborate\nand leverage feedback, they can cooperate through conversations in\nnatural language with each other, and humans as well, to adjust the way\nthey solve a given problem.\n\n\nAdaptability: Since LLMs are general-purpose models that can adapt\nto different tasks if properly configured, we can initialize different\nagents that leverage the various capabilities of LLMs in a modular and\ncomplementary way.\nSplitting complex tasks: LLMs work better when they split complex\ntasks into smaller subtasks (as covered in Chapter 4 about prompt\nengineering techniques). Henceforth, multi-agent conversations can\nenhance this partition, delegating each agent to a subtask, while keeping\nthe overall picture of the problem to solve.\nTo enable a multi-agent conversation, there are two main components to be\naware of:\nConversable agents are entities that can communicate with each other\nand have different capabilities, such as using LLMs, human input, or\ntools.\nConversation programming is a paradigm that allows developers to\ndefine the interaction behavior between agents using natural or\nprogramming languages.\nYou can see what these conversations look like at\nhttps://www.microsoft.com/en-\nus/research/publication/autogen-enabling-next-gen-\nllm-applications-via-multi-agent-conversation-\nframework/.\nThe AutoGen framework has already proven its great capability in\naddressing different use cases, among which are the following:\nCode generation and execution. AutoGen provides a class of agents\nthat can execute code as .py files in a given directory.\n\n\nMulti-agent collaboration. This scenario fits whenever you want\nvaried expertise to reason upon a given task. For example, you might\nwant to set up a research group that, when given a user’s request, sets\nup a plan, evaluates it, receives a user’s input, executes it with different\nexpertise (aka different agents), and so on.\nTools integrations. AutoGen also offers some classes that facilitate the\nintegration of external tools, such as web search and retrieval-\naugmented generation (RAG) from a provided vector database.\nYou can find some examples of different applications of the AutoGen\nframework at\nhttps://microsoft.github.io/autogen/docs/Examples#a\nutomated-multi-agent-chat.\nOverall, AutoGen provides a useful and innovative toolkit that makes it\neasier to let agents cooperate with each other, as well as with a human in the\nloop. The project is open to contribution, and it will be very interesting to\nsee how it progresses and to what extent the multi-agent approach will\nbecome a best practice.\nSo far, we have been talking about LLMs that are, by definition, “large” (for\nexample, the GPT-3 has 175 billion parameters). However, sometimes,\nsmaller models can be useful as well.\nSmall language models\nSmaller models with fewer parameters can demonstrate extraordinary\ncapabilities in specific tasks. This class of models has paved the way for\nwhat are now called small language models (SLMs). SLMs have fewer\nparameters than LLMs, which means they require less computational power\nand can be deployed on mobile devices or resource-constrained\n",
      "page_number": 449,
      "chapter_number": 53,
      "summary": "Nevertheless, the advancements covered in this chapter will give you an idea\nof what to expect in the near future Key topics include generative, generate, and generation.",
      "keywords": [
        "model",
        "LLMs",
        "agents",
        "OpenAI",
        "language models",
        "applications",
        "AutoGen",
        "images",
        "LLM",
        "generative image models",
        "risks",
        "content",
        "multi-agent",
        "racist and sexist",
        "generative"
      ],
      "concepts": [
        "generative",
        "generate",
        "generation",
        "images",
        "different",
        "openai",
        "agents",
        "models",
        "risks",
        "hate"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 10,
          "title": "Segment 10 (pages 79-86)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 5,
          "title": "Segment 5 (pages 33-40)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 54,
      "title": "Segment 54 (pages 459-466)",
      "start_page": 459,
      "end_page": 466,
      "detection_method": "topic_boundary",
      "content": "environments. SLMs can also be fine-tuned to excel in specific domains or\ntasks, such as finance, healthcare, or customer service, by using relevant\ntraining data.\nSLMs are promising because they offer several advantages over LLMs, such\nas:\nThey are more efficient and cost-effective, as they require less\ncomputational resources and energy to train and run.\nThey are more accessible and portable, as they can be deployed on\nmobile devices or edge computing platforms, enabling a wider range of\napplications and users.\nThey are more adaptable and specialized, as they can be fine-tuned to\nspecific domains or tasks using relevant data, improving their accuracy\nand relevance.\nThey are more interpretable and trustworthy, as they have fewer\nparameters and simpler architectures, making them easier to understand\nand debug.\nPhi-2 is an example of a promising SLM that demonstrates outstanding\nreasoning and language understanding capabilities, showcasing state-of-the-\nart performance among base language models with less than 13 billion\nparameters. It is a 2.7 billion-parameter language model developed by\nMicrosoft Research, trained on high-quality data sources, such as textbooks\nand synthetic texts, and uses a novel architecture that improves its efficiency\nand robustness. Phi-2 is available in the Azure AI Studio model catalog and\ncan be used for various research and development purposes, such as\nexploring safety challenges, interpretability, or fine-tuning experiments.\nIn the next section, we are going to see which companies are actively\nleveraging generative AI for their processes, services, and products.\n\n\nCompanies embracing\ngenerative AI\nSince the launch of ChatGPT in November 2022, up to the newest large\nfoundation models on the market (both proprietary and open-source), many\ncompanies in different industries started embracing generative AI within\ntheir processes and products. Let’s discuss some of the most popular ones.\nCoca-Cola\nCoca-Cola partnered with Bain & Company and OpenAI to leverage DALL-\nE, a generative AI model. This partnership was announced on February 21,\n2023.\nOpenAI’s ChatGPT and DALL-E platforms will help Coca-Cola create\ncustomized ad content, pictures, and messages. Coca-Cola’s “Create Real\nMagic” initiative is the result of the collaboration between OpenAI and Bain\n& Company (https://www.coca-colacompany.com/media-\ncenter/coca-cola-invites-digital-artists-to-create-\nreal-magic-using-new-ai-platform). The platform is a unique\ninnovation that merges the abilities of GPT-4, which generates text that\nsounds like humans making search engine queries, and DALL-E, which\ncreates images from text. This enables Coca-Cola to rapidly produce text,\nimages, and other content. This strategic alliance is expected to deliver real\nvalue to large enterprise customers, enabling massive business\ntransformation within the Fortune 500. It also sets a standard for their clients\nto follow.\nNotion\n\n\nNotion is a versatile platform that combines note-taking, project\nmanagement, and database functionalities in a single space. It allows users to\ncapture thoughts, manage projects, and even run an entire company in a way\nthat suits their needs. Notion is ideal for individuals, freelancers, startups,\nand teams looking for a straightforward application to collaborate on\nmultiple projects.\nNotion has introduced a new feature called Notion AI that uses generative\nAI. This feature is essentially a prediction engine that guesses what words\nwill work best based on a prompt or text you’ve written. It can perform tasks\nsuch as:\nSummarizing lengthy text (e.g., meeting notes and transcripts)\nGenerating entire blog post outlines and emails\nCreating action items from meeting notes\nEditing your writing to fix grammar and spelling, change the tone, etc.\nAssisting with research and problem-solving\nThe following screenshot shows some of the Notion features powered by\ngenerative AI:\nFigure 13.2: Some features of Notion AI. Source: https://www.notion.so/product/ai\n\n\nNotion AI is powered by OpenAI’s GPT models and integrated into the core\nNotion apps (desktop, browser, and mobile), allowing users to write prompts\nthat will generate text, as well as apply AI to text they’ve already written or\ncaptured. This makes Notion AI a powerful digital assistant that enhances\nthe functionality of the Notion workspace.\nMalbek\nMalbek is a modern, innovative contract lifecycle management (CLM)\nplatform with a proprietary AI core. It meets the increasing contractual needs\nof your entire organization, including Sales, Finance, Procurement, and other\nessential business units.\nMalbek uses generative AI to offer a feature powered by LLMs and featuring\nChatGPT. It can do tasks such as:\nUnderstanding the language in contracts\nMaking changes\nEasily accepting or rejecting redlines\nMaking custom requests – all in natural language\nThis remarkable new feature enables users to speed up negotiation time and\nshorten review cycles, improving the functionality of the Malbek workspace.\nMicrosoft\nSince its partnership with OpenAI, Microsoft has started infusing AI\npowered by GPT-series in all its products, introducing and coining the\nconcept of Copilot. We’ve already introduced the concept of a Copilot\nsystem in Chapter 2, as a new category of software that serves as an expert\nhelper to users trying to accomplish complex tasks, working side by side\n\n\nwith users and supporting them in various activities, from information\nretrieval to blog writing and posting, and from idea brainstorming to code\nreview and generation.\nIn 2023, Microsoft released several copilots within its products, such as the\nEdge Copiot (former Bing Chat). The following illustration shows the user\ninterface of Bing Chat:\nFigure 13.3: Microsoft Bing Chat\nBing Chat is also a perfect example of a multimodal conversational agent\npowered by both GPT-4V and DALL-E 3. Plus, you can interact with it via\naudio messaging. An example of these multimodal capabilities is shown in\nthe following screenshot:\n\n\nFigure 13.4: Leveraging the multimodal capabilities of Bing Chat\nMicrosoft’s copilots will empower professionals and organizations to\ndrastically improve their productivity and creativity, paving the way for a\n\n\nnew way of working.\nOverall, companies in all industries are seizing the potential of generative\nAI, with the awareness that the competitive landscape will soon raise the\nbenchmark for copilots and AI-powered products.\nSummary\nIn this final chapter of this book, we took a glimpse at the latest\nadvancements in the field of generative AI. We covered new model releases\nsuch as OpenAI’s GPT-4V, as well as new frameworks to build LLM-\npowered applications such as AutoGen. Furthermore, we provided an\noverview of some companies that are actively powering their business with\nLLMs, such as Notion and Microsoft.\nGenerative AI has shown to be the most promising and exciting field of AI,\nand it has the potential to unleash human creativity, enhance productivity,\nand solve complex problems. However, as we learned in the previous\nchapter, it also poses some ethical and social challenges, such as ensuring\nthe quality, safety, and fairness of the generated content, as well as\nrespecting the intellectual property and privacy rights of the original\ncreators. Therefore, as we explore the new horizons of generative AI, we\nshould also be mindful of the implications of our actions in the context of the\ncurrent times. We should strive to use generative AI for good purposes and\nfoster a culture of collaboration, innovation, and responsibility among\nresearchers, developers, and users. Nevertheless, generative AI is an\nevolving field, and within its landscape, one month is worth several years of\ntechnological progress. What is sure is that it represents a paradigm shift,\nand both companies and individuals are continuously adapting to it.\n\n\nReferences\nGPT-4V(ision) System Card: GPTV_System_Card.pdf\n(openai.com)\nAutoGen paper: Qingyun Wu et al., 2023, AutoGen: Enabling Next-Gen\nLLM Applications via Multi-Agent Conversation:\nhttps://arxiv.org/pdf/2308.08155.pdf\nAutoGen GitHub:\nhttps://github.com/microsoft/autogen/blob/main/n\notebook/agentchat_web_info.ipynb\nDALL-E 3: James Betker, Improving Image Generation with Better\nCaptions: https://cdn.openai.com/papers/dall-e-\n3.pdf\nNotion AI: https://www.notion.so/product/ai\nCoca-Cola and Bain partnership: https://www.coca-\ncolacompany.com/media-center/coca-cola-invites-\ndigital-artists-to-create-real-magic-using-new-\nai-platform\nMalbek and ChatGPT: https://www.malbek.io/news/chat-\ngpt-malbek-unveils-generative-ai-functionality\nMicrosoft Copilot: https://www.microsoft.com/en-\nus/microsoft-365/blog/2023/09/21/announcing-\nmicrosoft-365-copilot-general-availability-and-\nmicrosoft-365-chat/\n",
      "page_number": 459,
      "chapter_number": 54,
      "summary": "Phi-2 is an example of a promising SLM that demonstrates outstanding\nreasoning and language understanding capabilities, showcasing state-of-the-\nart performance among base language models with less than 13 billion\nparameters Key topics include notion, generative, and generate.",
      "keywords": [
        "Notion",
        "generative",
        "Bing Chat",
        "Microsoft",
        "Microsoft Bing Chat",
        "text",
        "users",
        "Chat",
        "Bing",
        "Notion features powered",
        "Coca-Cola",
        "OpenAI",
        "powered",
        "companies",
        "Malbek"
      ],
      "concepts": [
        "notion",
        "generative",
        "generate",
        "generation",
        "making",
        "makes",
        "coca",
        "models",
        "powering",
        "customer"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 56,
          "title": "Segment 56 (pages 484-491)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 474-483)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 8,
          "title": "Segment 8 (pages 61-70)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 55,
      "title": "Segment 55 (pages 467-474)",
      "start_page": 467,
      "end_page": 474,
      "detection_method": "topic_boundary",
      "content": "Join our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\nOceanofPDF.com\n\n\npackt.com\nSubscribe to our online digital library for full access to over 7,000 books\nand videos, as well as industry leading tools to help you plan your personal\ndevelopment and advance your career. For more information, please visit\nour website.\nWhy subscribe?\nSpend less time learning and more time coding with practical eBooks\nand Videos from over 4,000 industry professionals\nImprove your learning with Skill Plans built especially for you\nGet a free eBook or video every month\nFully searchable for easy access to vital information\nCopy and paste, print, and bookmark content\nAt www.packt.com, you can also read a collection of free technical\narticles, sign up for a range of free newsletters, and receive exclusive\ndiscounts and offers on Packt books and eBooks.\n\n\nOther Books You May\nEnjoy\nIf you enjoyed this book, you may be interested in these other books by\nPackt:\nGenerative AI with LangChain\nBen Auffarth\nISBN: 9781835083468\nUnderstand LLMs, their strengths and limitations\nGrasp generative AI fundamentals and industry trends\nCreate LLM apps with LangChain like question-answering systems\nand chatbots\nUnderstand transformer models and attention mechanisms\n\n\nAutomate data analysis and visualization using pandas and Python\nGrasp prompt engineering to improve performance\nFine-tune LLMs and get to know the tools to unleash their power\nDeploy LLMs as a service with LangChain and apply evaluation\nstrategies\nPrivately interact with documents using open-source LLMs to prevent\ndata leaks\n\n\nTransformers for Natural Language Processing and Computer Vision\nDenis Rothman\nISBN: 9781805128724\nLearn how to pretrain and fine-tune LLMs\nLearn how to work with multiple platforms, such as Hugging Face,\nOpenAI, and Google Vertex AI\nLearn about different tokenizers and the best practices for\npreprocessing language data\nImplement Retrieval Augmented Generation and rules bases to\nmitigate hallucinations\nVisualize transformer model activity for deeper insights using BertViz,\nLIME, and SHAP\nCreate and implement cross-platform chained models, such as\nHuggingGPT\nGo in-depth into vision transformers with CLIP, DALL-E 2, DALL-E\n3, and GPT-4V\n\n\nPackt is searching for\nauthors like you\nIf you’re interested in becoming an author for Packt, please visit\nauthors.packtpub.com and apply today. We have worked with\nthousands of developers and tech professionals, just like you, to help them\nshare their insight with the global tech community. You can make a general\napplication, apply for a specific hot topic that we are recruiting an author\nfor, or submit your own idea.\nShare your thoughts\nNow you’ve finished Building LLM Powered Application, we’d love to hear\nyour thoughts! If you purchased the book from Amazon, please click\nhere to go straight to the Amazon review page for this\nbook and share your feedback or leave a review on the site that you\npurchased it from.\nYour review is important to us and the tech community and will help us\nmake sure we’re delivering excellent quality content.\nOceanofPDF.com\n\n\nIndex\nSymbols\n.env file\nsecrets, storing 114, 115\nA\nagent 109, 110\ninitializing 245\nagentic approach 154\nversus hard-coded approach 256\nversus out-of-the-box approach 256\nagent types\nconversational 111\nOpenAI Functions 111\nplan-and-execute agents 111\nReAct document store 111\nself ask with search 111\nstructured input ReAct 111\nAI2 Reasoning Challenge (ARC) 19, 51\nAI orchestrators\ncomponents 31-34\n",
      "page_number": 467,
      "chapter_number": 55,
      "summary": "This chapter covers segment 55 (pages 467-474). Key topics include data, agent.",
      "keywords": [
        "LLMs",
        "books",
        "community",
        "Discord",
        "Packt books",
        "Join",
        "Join our community",
        "Fine-tune LLMs",
        "industry",
        "Discord space",
        "free",
        "LLM Powered Application",
        "Building LLM Powered",
        "author",
        "Learn"
      ],
      "concepts": [
        "data",
        "llm",
        "agent",
        "transformers",
        "language",
        "llms",
        "grasp",
        "react",
        "free",
        "generative"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "Segment 28 (pages 232-240)",
          "relevance_score": 0.71,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 31,
          "title": "Segment 31 (pages 262-269)",
          "relevance_score": 0.69,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "Segment 32 (pages 270-277)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 56,
      "title": "Segment 56 (pages 475-482)",
      "start_page": 475,
      "end_page": 482,
      "detection_method": "topic_boundary",
      "content": "framework, selecting 38, 39\nHaystack 35, 36\nLangChain 34, 35\nSemantic Kernel 36-38\nAmazon Web Services (AWS) RDS 173\narea under the ROC curve (AUC) 273\nartificial general intelligence (AGI) 226\nartificial intelligence (AI) 226\nartificial neural networks (ANNs) 4\nartificial neuron 4\nASCII characters 203\nassistant models 52\nversus base models 52\nattention mechanisms 11\nautoencoders 145\nAutoGen 296, 297\nAutomated Multi Agent Chat\nreference link 297\nautoregressive decoding 14\nautoregressive model 52\nAutoTokenizer 268\nAutoTrain 265\nreference link 265\nAzure AI Services\n\n\nout-of-the-box toolkit, using 228\nAzureCognitiveServicesToolkit\nmultiple tools, leveraging 234-238\nsingle tool, leveraging 231-234\ntoolkit, setting up 229-231\nworking with 228\nAzureCogsFormRecognizerTool 228\nAzureCogsImageAnalysisTool 228\nAzureCogsSpeech2TextTool 228\nAzureCogsText2SpeechTool 228\nAzureML Managed Feature Store 164\nB\nbackpropagation 6\nBaseExampleSelector 96\nbase models 17, 52\nversus assistant models 52\nversus customized models 20\nBayes' theorem 7\nlikelihood 8\nmost likely word, selecting 8\nposterior probability 8\nprior probability 8\nBERT model, components\n\n\nencoder 270\noutput layer 270\nBidirectional Encoder Representations from Transformers\n(BERT) model 15, 264, 268\nbusiness intelligence (BI) 171\nbyte pair encoding (BPE) 150, 264\nC\nchain of thought (CoT) 83-85\ncharacter-based approach 268\nChinook database\nfeatures 173\noverview 173\nclassifier layer 263\nClaude 2 49-51\nCoca-Cola 298\ncode\nlarge language model (LLM), selecting for 196, 197\nunderstanding 197, 198\nCodeBox 218\ncode generation 70, 197, 198\nCode Interpreter\nleveraging 217-223\nCodeLlama 201-205\n\n\ncold-start 148\nQA recommendation chatbot, building 151-159\ncollaborative filtering 140\ncompletion model 96\nconstitutional AI (CAI) 49\nprinciples, using in stages of training process 49\ncontent-based filtering 140\ncontent-based system\nbuilding 159-163\ncontract lifecycle management (CLM) 299\nconversable agents 296\nconversational applications 120\nbenefits 120\nexternal tools, adding 129-131\nmemory, adding 122-125\nnon-parametric knowledge, adding 125-129\nplain vanilla bot, creating 120-122\nconversation programming 296\nconvolutional neural networks (CNNs) 145\nCoPenny 238\ncopilot system 26, 31, 171\nfeatures 27-30\nversus retrieval-augmented generation (RAG) 29\ncosine similarity 100\n\n\ncurse of dimensionality 142\ncustomer relationship management (CRM) 171\ncustomized models\nversus base models 20\nCustomYTTranscribeTool 244\nD\nDALL-E 56, 246-248\nDALL-E 3 295\ndata\ntokenizing 267-270\ndata connections 96-102\ndata manipulation language (DML) 182\nDBCopilot 171\nimplementing, with LangChain 179\ntools, adding to 187-190\ndecoder 13\ndeep learning 2\ndeep neural networks (DNNs) 146\ndigital process automation (DPA) 238\nDo Anything Now (DAN) 285\ndocument loaders 97\ndocument transformers 97, 98\nDS-1000 197\n\n\nE\nembedding 5\nreference link 99\nencoder 12\nend-to-end application\nbuilding, for invoice analysis 238-243\nepoch 275\nevaluation metrics\nusing 272-275\nexample selector 96\nreference link 96\neXtensible Markup Language (XML) 171\nF\nFacebook AI Similarity Search (FAISS) 101\nFalcon LLM 54, 198\nusing 198-201\nFeast 163\nfeature extraction 263\nFeatureform 163\nfew-shot approach 79-83\nFibonacci sequence 207\nFileManagementToolkit 187\nfilter approach 154\n\n\nfine-tuning 262-264\ndataset, obtaining 266, 267\ndata, tokenizing 267-270\nevaluation metrics, using 272-275\nmodel 270, 271\nrequiring, scenarios 265\nsaving 275-278\nsteps 264\ntraining 275-278\nworking with 266\nforeign key 172\nfoundation model 2-4\nfront-end\ndeveloping, with Streamlit 131-136, 190-193, 256-259\nfront-end for Movieharbor\ndeveloping, with Streamlit 164-166\nfull-model fine-tuning 147\nfunction composition 36, 37\nG\nGemini 1.5 47-49\nGeneral Language Understanding Evaluation (GLUE) 18, 19,\n271\ngenerative AI\n\n\nversus natural language understanding (NLU) 3\nGenerative Pre-trained Transformer 3 (GPT-3) 15\ngenerative pretrained transformer (GPT) models 42\nGitHub Copilot 197\ngoal hijacking 285\nGoogle Cloud SQL 173\nGPT-4 42-47\nGPT-4V(ision) 294, 295\nGPTuber 243\ngraphics processing units (GPUs) 16\ngrounding 28\ngrouped-query attention (GQA) 55\nH\nhard-coded approach 228\nversus agentic, custom approach 256\nversus out-of-the-box approach 256\nwith sequential chain 252-255\nHaystack 35\nbenefits 36\ncomponents 36\nHeating, Ventilation and Air Conditioning (HVAC) setpoints 214\nHellaSwag 19, 51\nHF Code Autocomplete 209\n",
      "page_number": 475,
      "chapter_number": 56,
      "summary": "This chapter covers segment 56 (pages 475-482). Key topics include models, code, and language.",
      "keywords": [
        "reference link",
        "models",
        "versus",
        "language",
        "link",
        "artificial",
        "approach",
        "reference",
        "Web Services",
        "Amazon Web Services",
        "code",
        "adding",
        "Chat reference link",
        "Multi Agent Chat",
        "data"
      ],
      "concepts": [
        "models",
        "code",
        "language",
        "encoding",
        "generation",
        "generative",
        "artificial",
        "feature",
        "tool",
        "customized"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.69,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 474-483)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 18,
          "title": "Segment 18 (pages 147-158)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "Segment 32 (pages 270-277)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 57,
      "title": "Segment 57 (pages 483-490)",
      "start_page": 483,
      "end_page": 490,
      "detection_method": "topic_boundary",
      "content": "Hugging Face Hub 34\nlarge language model (LLM), working via 112\nHugging Face user access token\ncreating 112-114\nhybrid filtering 140\nI\nIBM Db2 173\nimage tokenization 294\nIMDB dataset\nreference link 266\nIMDB Movie Ratings Sentiment Analysis\nreference link 81\nInference Endpoints\nreference link 198\ninput IDs 268\ninvoice analysis\nend-to-end application, building for 238-243\nitem-based collaborative filtering 140\nitem-based KNN 141, 142\nJ\nJavaScript Object Notation (JSON) 171\nJoin Hugging Face\n\n\nreference link 112\nK\nK-nearest neighbors (KNN) 141\npitfalls 142\nknowledge-based filtering 140\nknowledge graph 103\nkwargs 154\nL\nlabels 266\nLanceDB 151\nLangChain 34, 92, 93, 210\nagents, SQL Agent 179-183\nbenefits 35\ncore modules 34, 35\nDBCopilot, implementing with 179\npackages, installing with 92\nreference link 92\nused, for building multimodal agent 227\nworking with 93\nlangchain-cli 92\nLangChain, components 93\nagents 109-111\n\n\nchains 105-108\ndata connections 96-102\nmemory 102-104\nmodels 94, 95\nprompts 94-96\nLangChain Expression Language (LCEL) 93\nfeatures 93\nLangChain's integration with features\nreference link 164\nLangServe 92\nLangSmith 92\nlanguage-specific foundation models 56-59\nlarge foundation models (LFMs) 2, 25, 56, 226\nlarge language model (LLM) 65\nselecting, for code 195-197\nworking with, via Hugging Face Hub 112\nLarge Language Model Meta AI 2 (LLaMA-2) 52, 53\ndeveloping, with fine-tuning process 53\nLarge Language Model Prompt Engineering for Complex\nSummarization\nreference link 75\nlarge language model (LLM) 1, 2, 8, 10, 41, 147, 169, 261, 279\nassistant models versus base models 52\nevaluating 15\n\n\nin market 41\nmodel evaluation 18-20\nopen-source model 51, 52\nproprietary models 42\ntraining 15-18\ntraining, steps 17, 18\nlarge multimodal model (LMM) 294\ncase study 62\nselecting, considerations 60-62\nselecting, framework 60\nLarge Objects (LOBs) 175\nlikelihood 7, 8\nLLMChain 105\nLLM, for recommendation systems 146\nfine-tuning 147\npre-training 146\nprompting 147\nLLM-powered agents\nbuilding, to act as algorithms 210-216\nsolving, optimization issues 216, 217\nLLM-powered recommendation system\ndata preprocessing 148-151\nimplementing 148\nLLMs, incorporating within applications\n\n\nconceptual aspect 26\ntechnical aspect 26\nlong short-term memory (LSTM) 11, 145\nM\nmachine learning (ML) 141\nmachine translation 69\nMalbek 299\nmasked language modeling (MLM) 271\nMassive Multitask Language Understanding (MMLU) 19, 44\nmatrix factorization 142-144\nalgorithms 143\npitfalls 144\nmemory types\nconversation buffer memory 103\nconversation buffer window memory 103\nconversation knowledge graph memory 103\nconversation summary buffer memory 104\nconversation summary memory 104\nconversation token buffer memory 104\nentity memory 103\nvector store-backed memory 104\nmetaprompt 21\nMicrosoft 300-302\n\n\nMidjourney 56\nMistral model 54, 55\nmixture-of-expert (MoE) 47\nmodel\nfine-tuning 270, 271\nmodel customization\nfew-shot learning 21\nfine-tuning 21, 22\nnon-parametric knowledge, extending 20\nMongoDB 170\nMostly Basic Programming Problems (MBPP) 196\nMovieHarbor 148\nMovie Recommender Dataset, on Kaggle\nreference link 148\nmulti-agent conversation\ncomponents 296\nreasons, for exhibiting performance 296\nMulti-Genre Natural Language Inference (MultiNLI) 271\nmultimodal agent\nbuilding, with LangChain 227\nmultimodality 226, 227\nMultiPL-E 197\nmulti-service resource, creating for Azure AI services\nreference link 229\n\n\nN\nnamed entity recognition (NER) 265\nnative functions 38\nnatural language processing (NLP) 3, 269\nnatural language understanding (NLU) 2, 51\nversus generative AI 3\nneural network 145, 146\nautoencoders 145\nchallenges 146\ncontent-based recommendation 145\nfeatures 146\nhidden layers 6\ninput layer 6\noutput layer 6\nsequential models 145\nused, for collaborative filtering 145\nvariational autoencoders (VAEs) 145\nnext sentence prediction (NSP) 271\nnode/unit 4\nnon-ASCII characters 203\nnon-negative matrix factorization (NMF) 143\nnon-parametric knowledge 20\nNotion 298, 299\nperforming, task 298\n\n\nNotion AI 298\nO\nobject-relational mapper (ORM) 175\nOpenAI 42\nOpenAI API key\nreference link 94\nOpenAIFunctionsAgent 217\nopen-source LLMs\nusing 116, 117\nopen-source model\nadvantage 51, 52\noptical character recognition (OCR) 229\noptimization algorithm 17\nOracle Database 173\nout-of-the-box approach\nversus agentic, custom approach 256\nversus hard-coded approach 256\nout-of-the-box toolkit\nusing, for Azure AI Services 228\noutput parser 105\nP\npadding 268\n",
      "page_number": 483,
      "chapter_number": 57,
      "summary": "This chapter covers segment 57 (pages 483-490). Key topics include models, memory, and agents.",
      "keywords": [
        "reference link",
        "Hugging Face Hub",
        "Hugging Face",
        "link",
        "language",
        "large language model",
        "language model",
        "model",
        "Face reference link",
        "large language",
        "reference",
        "Face Hub",
        "Hugging Face reference",
        "memory",
        "Face"
      ],
      "concepts": [
        "models",
        "memory",
        "agents",
        "knowledge",
        "llm",
        "langchain",
        "non",
        "object",
        "token",
        "filtering"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 474-483)",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 53,
          "title": "Segment 53 (pages 448-456)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 55,
          "title": "Segment 55 (pages 465-472)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 58,
      "title": "Segment 58 (pages 491-498)",
      "start_page": 491,
      "end_page": 498,
      "detection_method": "topic_boundary",
      "content": "parameter-efficient fine-tuning 147\nparameters 4\nparametric knowledge 20\nPhi-2 298\nplug-in 20\npoem generation 69\nposterior probability 7, 8\npost-padding 269\npost-truncation 269\npre-padding 269\nPretrain, Personalized Prompt & Predict Paradigm (P5) 147\npre-truncation 269\nprimary key 172\nprincipal component analysis (PCA) 143\nprior probability 8\nproduct attribute data 141\nprompt engineering 30, 31, 65, 183-186\nprompt engineering, principles 66\nclear instructions 66-69\ncomplex tasks, splitting into subtasks 69-72\ndelimiters, using 77-79\ninstructions, repeating 75-77\njustification, requesting 72, 73\nmultiple outputs, generating to select best one 73-75\n\n\nprompt injection 285\nprompt leakage 285\nprompt template 95, 96\nproprietary LLMs 42\nClaude 2 49-51\nGemini 1.5 47-49\nGPT-4 42-47\nPython\nrelational databases, working with 175-177\nPython REPL agent\nlimitations 217\nPythonREPLTool 186\nQ\nQA recommendation chatbot\nbuilding, in cold-start scenario 151-159\nR\nReAct (Reason and Act) 85-88, 111\nread-eval-print loop (REPL) 187\nreceiver operating characteristic (ROC) curve 273\nrecency bias 75\novercoming, ways 75\nrecommendation systems 140\n\n\nrecommendation systems, types\ncollaborative filtering 140\ncontent-based filtering 140\nhybrid filtering 140\nknowledge-based filtering 140\nrecurrent neural networks (RNNs) 10, 145\nRedis 173\nreinforcement learning from human feedback (RLHF) 43, 284\nreinforcement learning (RL) 18\nrelational databases 171, 172\nChinook database 173, 174\nworking with, in Python 175-178\nResponsible AI 279\nethical implications 280\nResponsible AI architecture 281\nmetaprompt level 284, 285\nmodel level 282-284\nregulations 289, 290\nuser interface level 286-289\nretrieval-augmented generation (RAG) 28\nversus copilot 29\nRetrievalQA chain 156\nretriever 102\nreward model (RM) 18\n\n\nRouterChain 106\nS\nschema 120\nsecrets 114\nstoring, in .env file 114, 115\nself-attention 11\nsemantic functions 38\nSemantic Kernel 36\nbenefits 38\ncomponents 37, 38\nsemi-structured data 170, 171\nsensitivity/true positive rate 273\nsequential chain 107\nused, for hard-coded approach 252-255\nSerpApi 129\nreference link 109, 129\nsimilarity 100\nsingle tools\ncombining, into single agent 243-252\nsingular value decomposition (SVD) 143\nsliding window attention (SWA) 55\nsmall language models (SLMs) 297\nadvantages, over LLMs 297\n\n\nSoftmax function 9\nSQL Agent 179-183\nSQL databases 173\nSQLite 173\nStanford Center for Research on Foundation Models (CRFM)\n61\nStanford Question Answering Dataset (SQuAD) 271\nStarCoder model 205-210\nstate-of-the-art (SOTA) 41\nstochastic gradient descent (SGD) 17\nStoryScribe 252\nStreamlit\nfront-end for Movieharbor, developing with 164-166\nused, for developing front-end 131-136, 190-193, 256-259\nStrong AI/Super AI 226\nstructured data 169, 170\nexample 170\nStructured Query Language (SQL) 170\nsub-word approach 268\nSuperGLUE 18, 19\nsupervised fine-tuned (SFT) 17\nT\ntargets 266\n\n\ntaxonomy 261\nTech Assistant Prompt 197\nTechGen Solutions 62\nTechnology Innovation Institute (TII) 54, 198\nTecton 163\ntensor 16\nTensor Processing Units (TPUs) 16\ntext embedding models 98-100\ntext generation 246-248\ntext splitters 97\ntext summarization 69\nText-to-Text Transfer Transformer (T5) 15\ntokenization 5\ntokens 5, 150\ntools list\nbuilding 245\ntransfer learning 262, 263\nTransformationChain 108\ntransformer architecture 11-15\nbuilding block 14\ndecoder layers 14\nlinear transformation 15\noutput embedding 14\nSoftMax 15\n\n\ntransformers 10\ntrue negative rate 273\ntruncation 268\nTruthfulQA 19, 51\nU\nunstructured data 170\nunsupervised learning 266\nuser-based collaborative filtering 140\nuser-based KNN 141\nuser behavior data 141\nuser demographic data 141\nV\nvariational autoencoders (VAEs) 145\nVectorDB 33\nvector store 100\nreference link 101\nvector store retriever 102\nW\nWhisper 56, 244, 245\nword-based approach 268\nWordPiece 268\n\n\nY\nYouTubeSearchTool 243, 244\nZ\nzero-shot evaluation 19\n",
      "page_number": 491,
      "chapter_number": 58,
      "summary": "This chapter covers segment 58 (pages 491-498). Key topics include model, data, and prompt.",
      "keywords": [
        "parameter-efficient fine-tuning",
        "parametric knowledge",
        "Predict Paradigm",
        "Prompt",
        "data",
        "filtering",
        "SQL",
        "parameters",
        "prompt engineering",
        "databases",
        "learning",
        "Python",
        "text",
        "engineering",
        "parameter-efficient"
      ],
      "concepts": [
        "model",
        "data",
        "prompt",
        "user",
        "filtering",
        "transformer",
        "approach",
        "generation",
        "retrieval",
        "retriever"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 48,
          "title": "Segment 48 (pages 973-990)",
          "relevance_score": 0.67,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 34,
          "title": "Segment 34 (pages 290-300)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 16,
          "title": "Segment 16 (pages 129-139)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 36,
          "title": "Segment 36 (pages 309-316)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 7,
          "title": "Segment 7 (pages 52-61)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 59,
      "title": "Segment 59 (pages 499-500)",
      "start_page": 499,
      "end_page": 500,
      "detection_method": "topic_boundary",
      "content": "Download a free PDF copy of\nthis book\nThanks for purchasing this book!\nDo you like to read on the go but are unable to carry your print books\neverywhere?\nIs your eBook purchase not compatible with the device of your choice?\nDon’t worry, now with every Packt book you get a DRM-free PDF version\nof that book at no cost.\nRead anywhere, any place, on any device. Search, copy, and paste code\nfrom your favorite technical books directly into your application.\nThe perks don’t stop there, you can get exclusive access to discounts,\nnewsletters, and great free content in your inbox daily.\nFollow these simple steps to get the benefits:\n1. Scan the QR code or visit the link below:\n\n\nhttps://packt.link/free-ebook/9781835462317\n2. Submit your proof of purchase.\n3. That’s it! We’ll send your free PDF and other benefits to your email\ndirectly.\nOceanofPDF.com\n",
      "page_number": 499,
      "chapter_number": 59,
      "summary": "This chapter covers segment 59 (pages 499-500). Key topics include directly, book, and email. Search, copy, and paste code\nfrom your favorite technical books directly into your application.",
      "keywords": [
        "free PDF",
        "PDF",
        "book",
        "free PDF copy",
        "Download",
        "Packt book",
        "free",
        "PDF copy",
        "DRM-free PDF",
        "PDF version",
        "Download a free",
        "copy",
        "read",
        "n’t",
        "DRM-free PDF version"
      ],
      "concepts": [
        "directly",
        "book",
        "email",
        "thanks",
        "link",
        "free",
        "content",
        "great",
        "version",
        "submit"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 11,
          "title": "Segment 11 (pages 84-95)",
          "relevance_score": 0.75,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 30,
          "title": "Segment 30 (pages 523-540)",
          "relevance_score": 0.74,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 12,
          "title": "Segment 12 (pages 199-216)",
          "relevance_score": 0.71,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 25,
          "title": "Segment 25 (pages 433-450)",
          "relevance_score": 0.71,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 35,
          "title": "Segment 35 (pages 603-603)",
          "relevance_score": 0.71,
          "method": "sentence_transformers"
        }
      ]
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "chapter": null,
      "content": "EXPERT INSIGHT\n\nBuilding LLM Powered\nApplications\n\nCreate intelligent apps and agents with MEE\nlarge language models MEE\n\nLETT)\nGALT) Uy, {/) lj\nSY4LLT] [] [) ly\n\nVig\n\nCLEP LDR\n\nay My\n\nSSSSSSS ee\n\nSoS\nSS\nSSS\nSSOOCS\nSSK\noS\n\nValentina Alto <packt",
      "content_length": 244,
      "extraction_method": "OCR"
    },
    {
      "page_number": 2,
      "chapter": null,
      "content": "Building LLM Powered\nApplications\nCreate intelligent apps and agents\nwith large language models\nValentina Alto\n",
      "content_length": 111,
      "extraction_method": "Direct"
    },
    {
      "page_number": 3,
      "chapter": null,
      "content": "Building LLM Powered Applications\nCopyright © 2024 Packt Publishing\nAll rights reserved. No part of this book may be reproduced, stored in a\nretrieval system, or transmitted in any form or by any means, without the\nprior written permission of the publisher, except in the case of brief\nquotations embedded in critical articles or reviews.\nEvery effort has been made in the preparation of this book to ensure the\naccuracy of the information presented. However, the information contained\nin this book is sold without warranty, either express or implied. Neither the\nauthor, nor Packt Publishing or its dealers and distributors, will be held\nliable for any damages caused or alleged to have been caused directly or\nindirectly by this book.\nPackt Publishing has endeavored to provide trademark information about all\nof the companies and products mentioned in this book by the appropriate\nuse of capitals. However, Packt Publishing cannot guarantee the accuracy of\nthis information.\nSenior Publishing Product Manager: Tushar Gupta\nAcquisition Editors – Peer Reviews: Tejas Mhasvekar and Jane D'Souza\nProject Editor: Namrata Katare\nContent Development Editors: Shruti Menon and Bhavesh Amin\nCopy Editor: Safis Editing\nTechnical Editor: Anirudh Singh\n",
      "content_length": 1244,
      "extraction_method": "Direct"
    },
    {
      "page_number": 4,
      "chapter": null,
      "content": "Proofreader: Safis Editing\nIndexer: Subalakshmi Govindhan\nPresentation Designer: Ajay Patule\nDeveloper Relations Marketing Executive: Monika Sangwan\nFirst published: May 2024\nProduction reference: 1140524\nPublished by Packt Publishing Ltd.\nGrosvenor House\n11 St Paul’s Square\nBirmingham\nB3 1RB, UK.\nISBN 978-1-83546-231-7\nwww.packt.com\n",
      "content_length": 336,
      "extraction_method": "Direct"
    },
    {
      "page_number": 5,
      "chapter": null,
      "content": "Contributors\nAbout the author\nValentina Alto is an AI enthusiast, tech author, and runner. After\ncompleting her master's in data science, she joined Microsoft in 2020,\nwhere she currently works as an AI specialist. Passionate about machine\nlearning and AI since the outset of her academic journey, Valentina has\ndeepened her knowledge in the field, authoring hundreds of articles on tech\nblogs. She also authored her first book with Packt, titled Modern\nGenerative AI with ChatGPT and OpenAI Models. In her current role, she\ncollaborates with large enterprises, aiming to integrate AI into their\nprocesses and create innovative solutions using large foundation models.\nBeyond her professional pursuits, Valentina loves hiking in the beautiful\nItalian mountains, running, traveling, and enjoying a good book with a cup\nof coffee.\n",
      "content_length": 829,
      "extraction_method": "Direct"
    },
    {
      "page_number": 6,
      "chapter": null,
      "content": "About the reviewers\nAlexandru Vesa has over a decade of expertise as an AI engineer and is\ncurrently serving as the CEO at Cube Digital, an AI software development\nfirm he leads with a vision inspired by the transformative potential of AI\nalgorithms. He has a wealth of experience in navigating diverse business\nenvironments and shaping AI products in both multinational corporations\nand dynamic startups. Drawing inspiration from various disciplines, he has\nbuilt a versatile skill set and seamlessly integrates state-of-the-art\ntechnologies with proven engineering methods. He is proficient in guiding\nprojects from inception to scalable success.\nAlex is a key figure in the DecodingML publication, collaborating with\nPaul Iusztin to curate the groundbreaking hands-on course LLM Twin:\nBuilding Your Production-Ready AI Replica, hosted on the Substack\nplatform. His problem-solving and communication skills make him an\nindispensable force in utilizing AI to foster innovation and achieve tangible\nresults.\nLouis Owen is a data scientist/AI engineer hailing from Indonesia.\nCurrently contributing to NLP solutions at Yellow.ai, a leading CX\nautomation platform, he thrives on delivering innovative solutions. Louis’s\ndiverse career spans various sectors, including NGO work with The World\nBank, e-commerce with Bukalapak and Tokopedia, conversational AI with\nYellow.ai, online travel with Traveloka, smart city initiatives with Qlue, and\nFinTech with Do-it. Louis has also written a book with Packt, titled\nHyperparameter Tuning with Python, and published several papers in the AI\nfield.\n",
      "content_length": 1589,
      "extraction_method": "Direct"
    },
    {
      "page_number": 7,
      "chapter": null,
      "content": "Outside of work, Louis loves to spend time mentoring aspiring data\nscientists, sharing insights through articles, and indulging in his hobbies of\nwatching movies and working on side projects.\n",
      "content_length": 192,
      "extraction_method": "Direct"
    },
    {
      "page_number": 8,
      "chapter": null,
      "content": "Join our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\nOceanofPDF.com\n",
      "content_length": 154,
      "extraction_method": "Direct"
    },
    {
      "page_number": 9,
      "chapter": null,
      "content": "Contents\nPreface\nWho this book is for\nWhat this book covers\nTo get the most out of this book\nGet in touch\n1. Introduction to Large Language Models\nWhat are large foundation models and LLMs?\nAI paradigm shift – an introduction to foundation models\nUnder the hood of an LLM\nMost popular LLM transformers-based architectures\nEarly experiments\nIntroducing the transformer architecture\nTraining and evaluating LLMs\nTraining an LLM\nModel evaluation\nBase models versus customized models\nHow to customize your model\nSummary\nReferences\n2. LLMs for AI-Powered Applications\nHow LLMs are changing software development\nThe copilot system\nIntroducing AI orchestrators to embed LLMs into applications\nThe main components of AI orchestrators\nLangChain\nHaystack\nSemantic Kernel\nHow to choose a framework\nSummary\nReferences\n3. Choosing an LLM for Your Application\nThe most promising LLMs in the market\nProprietary models\nGPT-4\n",
      "content_length": 909,
      "extraction_method": "Direct"
    },
    {
      "page_number": 10,
      "chapter": null,
      "content": "Gemini 1.5\nClaude 2\nOpen-source models\nLLaMA-2\nFalcon LLM\nMistral\nBeyond language models\nA decision framework to pick the right LLM\nConsiderations\nCase study\nSummary\nReferences\n4. Prompt Engineering\nTechnical requirements\nWhat is prompt engineering?\nPrinciples of prompt engineering\nClear instructions\nSplit complex tasks into subtasks\nAsk for justification\nGenerate many outputs, then use the model to pick the best\none\nRepeat instructions at the end\nUse delimiters\nAdvanced techniques\nFew-shot approach\nChain of thought\nReAct\nSummary\nReferences\n5. Embedding LLMs within Your Applications\nTechnical requirements\nA brief note about LangChain\nGetting started with LangChain\nModels and prompts\nData connections\nMemory\nChains\n",
      "content_length": 723,
      "extraction_method": "Direct"
    },
    {
      "page_number": 11,
      "chapter": null,
      "content": "Agents\nWorking with LLMs via the Hugging Face Hub\nCreate a Hugging Face user access token\nStoring your secrets in an .env file\nStart using open-source LLMs\nSummary\nReferences\n6. Building Conversational Applications\nTechnical requirements\nGetting started with conversational applications\nCreating a plain vanilla bot\nAdding memory\nAdding non-parametric knowledge\nAdding external tools\nDeveloping the front-end with Streamlit\nSummary\nReferences\n7. Search and Recommendation Engines with LLMs\nTechnical requirements\nIntroduction to recommendation systems\nExisting recommendation systems\nK-nearest neighbors\nMatrix factorization\nNeural networks\nHow LLMs are changing recommendation systems\nImplementing an LLM-powered recommendation system\nData preprocessing\nBuilding a QA recommendation chatbot in a cold-start\nscenario\nBuilding a content-based system\nDeveloping the front-end with Streamlit\nSummary\nReferences\n8. Using LLMs with Structured Data\nTechnical requirements\nWhat is structured data?\nGetting started with relational databases\n",
      "content_length": 1033,
      "extraction_method": "Direct"
    },
    {
      "page_number": 12,
      "chapter": null,
      "content": "Introduction to relational databases\nOverview of the Chinook database\nHow to work with relational databases in Python\nImplementing the DBCopilot with LangChain\nLangChain agents and SQL Agent\nPrompt engineering\nAdding further tools\nDeveloping the front-end with Streamlit\nSummary\nReferences\n9. Working with Code\nTechnical requirements\nChoosing the right LLM for code\nCode understanding and generation\nFalcon LLM\nCodeLlama\nStarCoder\nAct as an algorithm\nLeveraging Code Interpreter\nSummary\nReferences\n10. Building Multimodal Applications with LLMs\nTechnical requirements\nWhy multimodality?\nBuilding a multimodal agent with LangChain\nOption 1: Using an out-of-the-box toolkit for Azure AI Services\nGetting Started with AzureCognitiveServicesToolkit\nSetting up the toolkit\nLeveraging a single tool\nLeveraging multiple tools\nBuilding an end-to-end application for invoice analysis\nOption 2: Combining single tools into one agent\nYouTube tools and Whisper\nDALL·E and text generation\nPutting it all together\nOption 3: Hard-coded approach with a sequential chain\nComparing the three options\n",
      "content_length": 1082,
      "extraction_method": "Direct"
    },
    {
      "page_number": 13,
      "chapter": null,
      "content": "Developing the front-end with Streamlit\nSummary\nReferences\n11. Fine-Tuning Large Language Models\nTechnical requirements\nWhat is fine-tuning?\nWhen is fine-tuning necessary?\nGetting started with fine-tuning\nObtaining the dataset\nTokenizing the data\nFine-tuning the model\nUsing evaluation metrics\nTraining and saving\nSummary\nReferences\n12. Responsible AI\nWhat is Responsible AI and why do we need it?\nResponsible AI architecture\nModel level\nMetaprompt level\nUser interface level\nRegulations surrounding Responsible AI\nSummary\nReferences\n13. Emerging Trends and Innovations\nThe latest trends in language models and generative AI\nGPT-4V(ision)\nDALL-E 3\nAutoGen\nSmall language models\nCompanies embracing generative AI\nCoca-Cola\nNotion\nMalbek\nMicrosoft\nSummary\nReferences\n",
      "content_length": 765,
      "extraction_method": "Direct"
    },
    {
      "page_number": 14,
      "chapter": null,
      "content": "Other Books You May Enjoy\nIndex\nOceanofPDF.com\n",
      "content_length": 47,
      "extraction_method": "Direct"
    },
    {
      "page_number": 15,
      "chapter": null,
      "content": "Preface\nWith this book, we embark upon an exploration of large language models\n(LLMs) and the transformative paradigm they represent within the realm of\nartificial intelligence (AI). This comprehensive guide helps you delve into\nthe fundamental concepts, from solid theoretical foundations of these\ncutting-edge technologies to practical applications that LLMs offer,\nultimately converging on the ethical and responsible considerations while\nusing generative AI solutions. This book aims to provide you with a firm\nunderstanding of how the emerging LLMs in the market can impact\nindividuals, large enterprises, and society. It focuses on how to build\npowerful applications powered by LLMs, leveraging new AI orchestrators\nsuch as LangChain and uncovering new trends in modern application\ndevelopment.\nBy the end of this book, you will be able to navigate the rapidly evolving\necosystem of generative AI solutions more easily; plus, you will have the\ntools to get the most out of LLMs in both your daily tasks and your\nbusinesses. Let’s get started!\n",
      "content_length": 1049,
      "extraction_method": "Direct"
    },
    {
      "page_number": 16,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 17,
      "chapter": null,
      "content": "Who this book is for\nThe book is designed to mainly appeal to a technical audience with some\nbasic Python code foundations. However, the theoretical chapters and the\nhands-on exercises are based on generative AI foundations and industry-led\nuse cases, which might be of interest to non-technical audiences as well.\nOverall, the book caters to individuals interested in gaining a\ncomprehensive understanding of the transformative power of LLMs and\ndefine, enabling them to navigate the rapidly evolving AI landscape with\nconfidence and foresight. All kinds of readers are welcome, but readers who\ncan benefit the most from this book include:\nSoftware developers and engineers: This book provides practical\nguidance for developers looking to build applications leveraging\nLLMs. It covers integrating LLMs into app backends, APIs,\narchitectures, and so on.\nData scientists: For data scientists interested in deploying LLMs for\nreal-world usage, this book shows how to take models from research to\nproduction. It covers model serving, monitoring, and optimization.\nAI/ML engineers: Engineers focused on AI/ML applications can\nleverage this book to understand how to architect and deploy LLMs as\npart of intelligent systems and agents.\nTechnical founders/CTOs: Startup founders and CTOs can use this\nbook to evaluate if and how LLMs could be used within their apps and\nproducts. It provides a technical overview alongside business\nconsiderations.\n",
      "content_length": 1442,
      "extraction_method": "Direct"
    },
    {
      "page_number": 18,
      "chapter": null,
      "content": "Students: Graduate students and advanced undergraduates studying\nAI, ML, natural language processing (NLP), or computer science\ncan learn how LLMs are applied in practice from this book.\nLLM researchers: Researchers working on novel LLM architectures,\ntraining techniques, and so on will gain insight into real-world model\nusage and the associated challenges.\n",
      "content_length": 360,
      "extraction_method": "Direct"
    },
    {
      "page_number": 19,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 20,
      "chapter": null,
      "content": "What this book covers\nChapter 1, Introduction to Large Language Models, provides an\nintroduction to and deep dive into LLMs, a powerful set of deep learning\nneural networks in the domain of generative AI. It introduces the concept of\nLLMs, their differentiators from classical machine learning models, and the\nrelevant jargon. It also discusses the architecture of the most popular LLMs,\nmoving on to explore how LLMs are trained and consumed and compare\nbase LLMs with fine-tuned LLMs. By the end of this chapter, you will have\nthe foundations of what LLMs are and their positioning in the landscape of\nAI, creating the basis for the subsequent chapters.\nChapter 2, LLMs for AI-Powered Applications, explores how LLMs are\nrevolutionizing the world of software development, leading to a new era of\nAI-powered applications. By the end of this chapter, you will have a clearer\npicture of how LLMs can be embedded in different application scenarios,\nwith the help of new AI orchestrator frameworks that are currently available\nin the AI development market.\nChapter 3, Choosing an LLM for Your Application, highlights how different\nLLMs may have different architectures, sizes, training data, capabilities,\nand limitations. Choosing the right LLM for your application is not a trivial\ndecision as it can significantly impact the performance, quality, and cost of\nyour solution. In this chapter, we will navigate the process of choosing the\nright LLM for your application. We will discuss the most promising LLMs\nin the market, the main criteria and tools to use when comparing LLMs, and\nthe various trade-offs between size and performance. By the end of this\nchapter, you should have a clear understanding of how to choose the right\nLLM for your application and how to use it effectively and responsibly.\n",
      "content_length": 1801,
      "extraction_method": "Direct"
    },
    {
      "page_number": 21,
      "chapter": null,
      "content": "Chapter 4, Prompt Engineering, explains how prompt engineering is a\ncrucial activity while designing LLM-powered applications since prompts\nhave a massive impact on the performance of LLMs. In fact, there are\nseveral techniques that can be implemented to not only to refine your\nLLM’s responses but also reduce risks associated with hallucination and\nbiases. In this chapter, we will cover the emerging techniques in the field of\nprompt engineering, from basic approaches up to advanced frameworks. By\nthe end of this chapter, you will have the foundations to build functional\nand solid prompts for your LLM-powered applications, which will also be\nrelevant in the upcoming chapters.\nChapter 5, Embedding LLMs within Your Applications, discusses a new set\nof components introduced into the landscape of software development with\nthe advent of developing applications with LLMs. To make it easier to\norchestrate LLMs and their related components in an application flow,\nseveral AI frameworks have emerged, of which LangChain is one of the\nmost widely used. In this chapter, we will take a deep dive into LangChain\nand how to use it, and learn how to call open-source LLM APIs into code\nvia Hugging Face Hub and manage prompt engineering. By the end of this\nchapter, you will have the technical foundations to start developing your\nLLM-powered applications using LangChain and open-source Hugging\nFace models.\nChapter 6, Building Conversational Applications, allows us to embark on\nthe hands-on section of this book with your first concrete implementation of\nLLM-powered applications. Throughout this chapter, we will cover a step-\nby-step implementation of a conversational application, using LangChain\nand its components. We will configure the schema of a simple chatbot,\nadding a memory component, non-parametric knowledge, and tools to make\n",
      "content_length": 1843,
      "extraction_method": "Direct"
    },
    {
      "page_number": 22,
      "chapter": null,
      "content": "the chatbot “agentic.” By the end of this chapter, you will be able to set up\nyour own conversational application project with just a few lines of code.\nChapter 7, Search and Recommendation Engines with LLMs, explores how\nLLMs can enhance recommendation systems, using both embeddings and\ngenerative models. We will discuss the definition and evolution of\nrecommendation systems, learn how generative AI is impacting this field of\nresearch, and understand how to build recommendation systems with\nLangChain. By the end of this chapter, you will be able to create your own\nrecommendation application and leverage state-of-the-art LLMs using\nLangChain as the framework.\nChapter 8, Using LLMs with Structured Data, covers a great capability of\nLLMs: the ability to handle structured, tabular data. We will see how, with\nplug-ins and an agentic approach, we can use LLMs as a natural language\ninterface between us and our structured data, reducing the gap between the\nbusiness user and the structured information. To demonstrate this, we will\nbuild a database copilot with LangChain. By the end of this chapter, you\nwill be able to build your own natural language interface for your data\nestate, combining unstructured with structured sources.\nChapter 9, Working with Code, covers another great capability of LLMs:\nworking with programming languages. In the previous chapter, we’ve\nalready seen a glimpse of this capability, when we asked our LLM to\ngenerate SQL queries against a SQL Database. In this chapter, we are going\nto examine in which other ways LLMs can be used with code, from\n“simple” code understanding and generation to the building of applications\nthat behave as if they were an algorithm. By the end of this chapter, you\nwill be able to build LLM-powered applications for your coding projects, as\n",
      "content_length": 1810,
      "extraction_method": "Direct"
    },
    {
      "page_number": 23,
      "chapter": null,
      "content": "well as build LLM-powered applications with natural language interfaces to\nwork with code.\nChapter 10, Building Multimodal Applications with LLMs, goes beyond\nLLMs, introducing the concept of multi-modality while building agents. We\nwill see the logic behind the combination of foundation models in different\nAI domains – language, images, audio – into one single agent that can adapt\nto a variety of tasks. You will learn how to build a multi-modal agent with\nsingle-modal LLMs using LangChain. By the end of this chapter, you will\nbe able to build your own multi-modal agent, providing it with the tools and\nLLMs needed to perform various AI tasks.\nChapter 11, Fine-Tuning Large Language Models, covers the technical\ndetails of fine-tuning LLMs, from the theory behind it to hands-on\nimplementation with Python and Hugging Face. We will delve into how you\ncan prepare your data to fine-tune a base model on your data, as well as\ndiscuss hosting strategies for your fine-tuned model. By the end of this\nchapter, you will be able to fine-tune an LLM on your own data so that you\ncan build domain-specific applications powered by that LLM.\nChapter 12, Responsible AI, introduces the fundamentals of the discipline\nbehind the mitigation of the potential harms of LLMs – and AI models in\ngeneral – that is, responsible AI. This is important because LLMs open the\ndoors to a new set of risks and biases to be taken into account while\ndeveloping LLM-powered applications.\nWe will then move on to the risks associated with LLMs and how to prevent\nor, at the very least, mitigate them using proper techniques. By the end of\nthis chapter, you will have a deeper understanding of how to prevent LLMs\nfrom making your application potentially harmful.\n",
      "content_length": 1741,
      "extraction_method": "Direct"
    },
    {
      "page_number": 24,
      "chapter": null,
      "content": "Chapter 13, Emerging Trends and Innovations, explores the latest\nadvancements and future trends in the field of generative AI.\nTo get the most out of this\nbook\nThis book aims to provide a solid theoretical foundation of what LLMs are,\ntheir architecture, and why they are revolutionizing the field of AI. It adopts\na hands-on approach, providing you with a step-by-step guide to\nimplementing LLMs-powered apps for specific tasks and using powerful\nframeworks like LangChain. Furthermore, each example will showcase the\nusage of a different LLM, so that you can appreciate their differentiators\nand when to use the proper model for a given task.\nOverall, the book combines theoretical concepts with practical applications,\nmaking it an ideal resource for anyone who wants to gain a solid foundation\nin LLMs and their applications in NLP. The following pre-requisites will\nhelp you to get the most out of this book:\nA basic understanding of the math behind neural networks (linear\nalgebra, neurons and parameters, and loss functions)\nA basic understanding of ML concepts, such as training and test sets,\nevaluation metrics, and NLP\nA basic understanding of Python\nDownload the example code\nfiles\n",
      "content_length": 1194,
      "extraction_method": "Direct"
    },
    {
      "page_number": 25,
      "chapter": null,
      "content": "The code bundle for the book is hosted on GitHub at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications. We also have other code bundles from our\nrich catalog of books and videos available at\nhttps://github.com/PacktPublishing/. Check them out!\nDownload the color images\nWe also provide a PDF file that has color images of the\nscreenshots/diagrams used in this book. You can download it here:\nhttps://packt.link/gbp/9781835462317.\nConventions used\nThere are a number of text conventions used throughout this book.\nCodeInText: Indicates code words in text, database table names, folder\nnames, filenames, file extensions, pathnames, dummy URLs, user input,\nand Twitter handles. For example: “I set the two variables system_message\nand instructions.”\nA block of code is set as follows:\n[default]\n$pip install openai == 0.28\nimport os\nimport openai\nopenai.api_key = os.environment.get('OPENAI_API_KEY')\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-35-turbo\", # engine = \"deployment_name\".\n    messages=[\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": instructions},\n",
      "content_length": 1135,
      "extraction_method": "Direct"
    },
    {
      "page_number": 26,
      "chapter": null,
      "content": "    ]\n)\nAny command-line input or output is written as follows:\n{'text': \"Terrible movie. Nuff Said.[…]\n 'label': 0}\nBold: Indicates a new term, an important word, or words that you see on\nthe screen. For instance, words in menus or dialog boxes appear in the text\nlike this. For example: “[…] he found that repeating the main instruction at\nthe end of the prompt can help the model to overcome its inner recency\nbias.”\nWarnings or important notes appear like this.\nTips and tricks appear like this.\nGet in touch\nFeedback from our readers is always welcome.\nGeneral feedback: Email feedback@packtpub.com and mention the book’s\ntitle in the subject of your message. If you have questions about any aspect\nof this book, please email us at questions@packtpub.com.\nErrata: Although we have taken every care to ensure the accuracy of our\ncontent, mistakes do happen. If you have found a mistake in this book, we\nwould be grateful if you reported this to us. Please visit\n",
      "content_length": 966,
      "extraction_method": "Direct"
    },
    {
      "page_number": 27,
      "chapter": null,
      "content": "http://www.packtpub.com/submit-errata, click Submit\nErrata, and fill in the form.\nPiracy: If you come across any illegal copies of our works in any form on\nthe internet, we would be grateful if you would provide us with the location\naddress or website name. Please contact us at copyright@packtpub.com with\na link to the material.\nIf you are interested in becoming an author: If there is a topic that you\nhave expertise in and you are interested in either writing or contributing to a\nbook, please visit http://authors.packtpub.com.\n",
      "content_length": 533,
      "extraction_method": "Direct"
    },
    {
      "page_number": 28,
      "chapter": null,
      "content": "Share your thoughts\nOnce you’ve read Building LLM Powered Application, we’d love to hear\nyour thoughts! Please click here to go straight to the\nAmazon review page for this book and share your feedback.\nYour review is important to us and the tech community and will help us\nmake sure we’re delivering excellent quality content.\n",
      "content_length": 327,
      "extraction_method": "Direct"
    },
    {
      "page_number": 29,
      "chapter": null,
      "content": "Download a free PDF copy of\nthis book\nThanks for purchasing this book!\nDo you like to read on the go but are unable to carry your print books\neverywhere?\nIs your eBook purchase not compatible with the device of your choice?\nDon’t worry, now with every Packt book you get a DRM-free PDF version\nof that book at no cost.\nRead anywhere, any place, on any device. Search, copy, and paste code\nfrom your favorite technical books directly into your application.\nThe perks don’t stop there, you can get exclusive access to discounts,\nnewsletters, and great free content in your inbox daily.\nFollow these simple steps to get the benefits:\n1. Scan the QR code or visit the link below:\n",
      "content_length": 676,
      "extraction_method": "Direct"
    },
    {
      "page_number": 30,
      "chapter": null,
      "content": "https://packt.link/free-ebook/9781835462317\n2. Submit your proof of purchase.\n3. That’s it! We’ll send your free PDF and other benefits to your email\ndirectly.\nOceanofPDF.com\n",
      "content_length": 175,
      "extraction_method": "Direct"
    },
    {
      "page_number": 31,
      "chapter": null,
      "content": "1\nIntroduction to Large\nLanguage Models\nDear reader, welcome to Building Large Language Model Applications! In\nthis book, we will explore the fascinating world of a new era of application\ndevelopments, where large language models (LLMs) are the main\nprotagonists.\nDuring the last year, we all learned the power of generative artificial\nintelligence (AI) tools such as ChatGPT, Bing Chat, Bard, and Dall-E. What\nimpressed us the most was their stunning capabilities of generating human-\nlike content based on user requests made in natural language. It is, in fact,\ntheir conversational capabilities that made them so easily consumable and,\ntherefore, popular as soon as they entered the market. Thanks to this phase,\nwe learned to acknowledge the power of generative AI and its core models:\nLLMs. However, LLMs are more than language generators. They can be also\nseen as reasoning engines that can become the brains of our intelligent\napplications.\nIn this book, we will see the theory and practice of how to build LLM-\npowered applications, addressing a variety of scenarios and showing new\ncomponents and frameworks that are entering the domain of software\ndevelopment in this new era of AI. The book will start with Part 1, where we\nwill introduce the theory behind LLMs, the most promising LLMs in the\n",
      "content_length": 1305,
      "extraction_method": "Direct"
    },
    {
      "page_number": 32,
      "chapter": null,
      "content": "market right now, and the emerging frameworks for LLMs-powered\napplications. Afterward, we will move to a hands-on part where we will\nimplement many applications using various LLMs, addressing different\nscenarios and real-world problems. Finally, we will conclude the book with a\nthird part, covering the emerging trends in the field of LLMs, alongside the\nrisk of AI tools and how to mitigate them with responsible AI practices.\nSo, let’s dive in and start with some definitions of the context we are moving\nin. This chapter provides an introduction and deep dive into LLMs, a\npowerful set of deep learning neural networks that feature the domain of\ngenerative AI.\nIn this chapter, we will cover the following topics:\nUnderstanding LLMs, their differentiators from classical machine\nlearning models, and their relevant jargon\nOverview of the most popular LLM architectures\nHow LLMs are trained and consumed\nBase LLMs versus fine-tuned LLMs\nBy the end of this chapter, you will have the fundamental knowledge of what\nLLMs are, how they work, and how you can make them more tailored to\nyour applications. This will also pave the way for the concrete usage of\nLLMs in the hands-on part of this book, where we will see in practice how to\nembed LLMs within your applications.\nWhat are large foundation\nmodels and LLMs?\nLLMs are deep-learning-based models that use many parameters to learn\nfrom vast amounts of unlabeled texts. They can perform various natural\n",
      "content_length": 1456,
      "extraction_method": "Direct"
    },
    {
      "page_number": 33,
      "chapter": null,
      "content": "language processing tasks such as recognizing, summarizing, translating,\npredicting, and generating text.\nDefinition\nDeep learning is a branch of machine learning that is\ncharacterized by neural networks with multiple layers, hence\nthe term “deep.” These deep neural networks can\nautomatically learn hierarchical data representations, with\neach layer extracting increasingly abstract features from the\ninput data. The depth of these networks refers to the number\nof layers they possess, enabling them to effectively model\nintricate relationships and patterns in complex datasets.\nLLMs belong to a wider set of models that feature the AI subfield of\ngenerative AI: large foundation models (LFMs). Hence, in the following\nsections, we will explore the rise and development of LFMs and LLMs, as\nwell as their technical architecture, which is a crucial task to understand their\nfunctioning and properly adopt those technologies within your applications.\nWe will start by understanding why LFMs and LLMs differ from traditional\nAI models and how they represent a paradigm shift in this field. We will then\nexplore the technical functioning of LLMs, how they work, and the\nmechanisms behind their outcomes.\nAI paradigm shift – an\nintroduction to foundation\nmodels\n",
      "content_length": 1258,
      "extraction_method": "Direct"
    },
    {
      "page_number": 34,
      "chapter": null,
      "content": "A foundation model refers to a type of pre-trained generative AI model that\noffers immense versatility by being adaptable for various specific tasks.\nThese models undergo extensive training on vast and diverse datasets,\nenabling them to grasp general patterns and relationships within the data –\nnot just limited to textual but also covering other data formats such as\nimages, audio, and video. This initial pre-training phase equips the models\nwith a strong foundational understanding across different domains, laying\nthe groundwork for further fine-tuning. This cross-domain capability\ndifferentiates generative AI models from standard natural language\nunderstanding (NLU) algorithms.\nNote\nGenerative AI and NLU algorithms are both related to\nnatural language processing (NLP), which is a branch of\nAI that deals with human language. However, they have\ndifferent goals and applications.\nThe difference between generative AI and NLU algorithms is\nthat generative AI aims to create new natural language\ncontent, while NLU algorithms aim to understand existing\nnatural language content. Generative AI can be used for tasks\nsuch as text summarization, text generation, image\ncaptioning, or style transfer. NLU algorithms can be used for\ntasks such as chatbots, question answering, sentiment\nanalysis, or machine translation.\nFoundation models are designed with transfer learning in mind, meaning\nthey can effectively apply the knowledge acquired during pre-training to\n",
      "content_length": 1467,
      "extraction_method": "Direct"
    },
    {
      "page_number": 35,
      "chapter": null,
      "content": "new, related tasks. This transfer of knowledge enhances their adaptability,\nmaking them efficient at quickly mastering new tasks with relatively little\nadditional training.\nOne notable characteristic of foundation models is their large architecture,\ncontaining millions or even billions of parameters. This extensive scale\nenables them to capture complex patterns and relationships within the data,\ncontributing to their impressive performance across various tasks.\nDue to their comprehensive pre-training and transfer learning capabilities,\nfoundation models exhibit strong generalization skills. This means they can\nperform well across a range of tasks and efficiently adapt to new, unseen\ndata, eliminating the need for training separate models for individual tasks.\nThis paradigm shift in artificial neural network design offers considerable\nadvantages, as foundation models, with their diverse training datasets, can\nadapt to different tasks based on users’ intent without compromising\nperformance or efficiency. In the past, creating and training distinct neural\nnetworks for each task, such as named entity recognition or sentiment\nanalysis, would have been necessary, but now, foundation models provide a\nunified and powerful solution for multiple applications.\n",
      "content_length": 1270,
      "extraction_method": "Direct"
    },
    {
      "page_number": 36,
      "chapter": null,
      "content": "Figure 1.1: From task-specific models to general models\nNow, we said that LFMs are trained on a huge amount of heterogeneous data\nin different formats. Whenever that data is unstructured, natural language\ndata, we refer to the output LFM as an LLM, due to its focus on text\nunderstanding and generation.\nFigure 1.2: Features of LLMs\n",
      "content_length": 333,
      "extraction_method": "Direct"
    },
    {
      "page_number": 37,
      "chapter": null,
      "content": "We can then say that an LLM is a type of foundation model specifically\ndesigned for NLP tasks. These models, such as ChatGPT, BERT, Llama, and\nmany others, are trained on vast amounts of text data and can generate\nhuman-like text, answer questions, perform translations, and more.\nNevertheless, LLMs aren’t limited to performing text-related tasks. As we\nwill see throughout the book, those unique models can be seen as reasoning\nengines, extremely good in common sense reasoning. This means that they\ncan assist us in complex tasks, analytical problem-solving, enhanced\nconnections, and insights among pieces of information.\nIn fact, as LLMs mimic the way our brains are made (as we will see in the\nnext section), their architectures are featured by connected neurons. Now,\nhuman brains have about 100 trillion connections, way more than those\nwithin an LLM. Nevertheless, LLMs have proven to be much better at\npacking a lot of knowledge into those fewer connections than we are.\nUnder the hood of an LLM\nLLMs are a particular type of artificial neural networks (ANNs):\ncomputational models inspired by the structure and functioning of the human\nbrain. They have proven to be highly effective in solving complex problems,\nparticularly in areas like pattern recognition, classification, regression, and\ndecision-making tasks.\nThe basic building block of an ANN is the artificial neuron, also known as a\nnode or unit. These neurons are organized into layers, and the connections\nbetween neurons are weighted to represent the strength of the relationship\nbetween them. Those weights represent the parameters of the model that\nwill be optimized during the training process.\n",
      "content_length": 1671,
      "extraction_method": "Direct"
    },
    {
      "page_number": 38,
      "chapter": null,
      "content": "ANNs are, by definition, mathematical models that work with numerical\ndata. Hence, when it comes to unstructured, textual data as in the context of\nLLMs, there are two fundamental activities that are required to prepare data\nas model input:\nTokenization: This is the process of breaking down a piece of text (a\nsentence, paragraph, or document) into smaller units called tokens.\nThese tokens can be words, subwords, or even characters, depending on\nthe chosen tokenization scheme or algorithm. The goal of tokenization\nis to create a structured representation of the text that can be easily\nprocessed by machine learning models.\nFigure 1.3: Example of tokenization\nEmbedding: Once the text has been tokenized, each token is converted\ninto a dense numerical vector called an embedding. Embeddings are a\nway to represent words, subwords, or characters in a continuous vector\nspace. These embeddings are learned during the training of the\nlanguage model and capture semantic relationships between tokens.\nThe numerical representation allows the model to perform\nmathematical operations on the tokens and understand the context in\nwhich they appear.\n",
      "content_length": 1146,
      "extraction_method": "Direct"
    },
    {
      "page_number": 39,
      "chapter": null,
      "content": "Figure 1.4: Example of embedding\nIn summary, tokenization breaks down text into smaller units called tokens,\nand embeddings convert these tokens into dense numerical vectors. This\nrelationship allows LLMs to process and understand textual data in a\nmeaningful and context-aware manner, enabling them to perform a wide\nrange of NLP tasks with impressive accuracy.\nFor example, let’s consider a two-dimensional embedding space where we\nwant to vectorize the words Man, King, Woman, and Queen. The idea is that\nthe mathematical distance between each pair of those words should be\nrepresentative of their semantic similarity. This is illustrated by the following\ngraph:\nFigure 1.5: Example of words embedding in a 2D space\n",
      "content_length": 719,
      "extraction_method": "Direct"
    },
    {
      "page_number": 40,
      "chapter": null,
      "content": "As a result, if we properly embed the words, the relationship King – Man +\nWoman ≈ Queen should hold.\nOnce we have the vectorized input, we can pass it into the multi-layered\nneural network. There are three main types of layers:\nInput layer: The first layer of the neural network receives the input\ndata. Each neuron in this layer corresponds to a feature or attribute of\nthe input data.\nHidden layers: Between the input and output layers, there can be one\nor more hidden layers. These layers process the input data through a\nseries of mathematical transformations and extract relevant patterns and\nrepresentations from the data.\nOutput layer: The final layer of the neural network produces the\ndesired output, which could be predictions, classifications, or other\nrelevant results depending on the task the neural network is designed\nfor.\n",
      "content_length": 840,
      "extraction_method": "Direct"
    },
    {
      "page_number": 41,
      "chapter": null,
      "content": "Figure 1.6: High-level architecture of a generic ANN\nThe process of training an ANN involves the process of backpropagation\nby iteratively adjusting the weights of the connections between neurons\nbased on the training data and the desired outputs.\nDefinition\nBackpropagation is an algorithm used in deep learning to\ntrain neural networks. It involves two phases: the forward\npass, where data is passed through the network to compute\nthe output, and the backward pass, where errors are\npropagated backward to update the network’s parameters and\n",
      "content_length": 544,
      "extraction_method": "Direct"
    },
    {
      "page_number": 42,
      "chapter": null,
      "content": "improve its performance. This iterative process helps the\nnetwork learn from data and make accurate predictions.\nDuring backpropagation, the network learns by comparing its predictions\nwith the ground truth and minimizing the error or loss between them. The\nobjective of training is to find the optimal set of weights that enables the\nneural network to make accurate predictions on new, unseen data.\nANNs can vary in architecture, including the number of layers, the number\nof neurons in each layer, and the connections between them.\nWhen it comes to generative AI and LLMs, their remarkable capability of\ngenerating text based on our prompts is based on the statistical concept of\nBayes’ theorem.\nDefinition\nBayes’ theorem, named after the Reverend Thomas Bayes, is\na fundamental concept in probability theory and statistics. It\ndescribes how to update the probability of a hypothesis based\non new evidence. Bayes’ theorem is particularly useful when\nwe want to make inferences about unknown parameters or\nevents in the presence of uncertainty. According to Bayes’\ntheorem, given two events, A and B, we can define the\nconditional probability of A given B as:\nWhere:\n",
      "content_length": 1168,
      "extraction_method": "Direct"
    },
    {
      "page_number": 43,
      "chapter": null,
      "content": "P(B|A) = probability of B occurring given A, also\nknown as the likelihood of A given a fixed B.\nP(A|B) = probability of A occurring, given B; also\nknown as the posterior probability of A, given B.\nP(A) and P(B) = probability of observing A or B without\nany conditions.\nBayes’ theorem relates the conditional probability of an event based on new\nevidence with the a priori probability of the event. Translated into the\ncontext of LLMs, we are saying that such a model functions by predicting\nthe next most likely word, given the previous words prompted by the user.\nBut how can LLMs know which is the next most likely word? Well, thanks\nto the enormous amount of data on which LLMs have been trained (we will\ndive deeper into the process of training an LLM in the next sections). Based\non the training text corpus, the model will be able to identify, given a user’s\nprompt, the next most likely word or, more generally, text completion.\nFor example, let’s consider the following prompt: “The cat is on the….” and\nwe want our LLM to complete this sentence. However, the LLM may\ngenerate multiple candidate words, so we need a method to evaluate which\nof the candidates is the most likely one. To do so, we can use Bayes’ theorem\nto select the most likely word given the context. Let’s see the required steps:\nPrior probability P(A): The prior probability represents the probability\nof each candidate word being the next word in the context, based on the\nlanguage model’s knowledge learned during training. Let’s assume the\nLLM has three candidate words: “table,” “chair,” and “roof.”\nP(“table”), P(“chain”), and P(“roof”) are the prior probabilities for\neach candidate word, based on the language model’s knowledge of the\n",
      "content_length": 1720,
      "extraction_method": "Direct"
    },
    {
      "page_number": 44,
      "chapter": null,
      "content": "frequency of these words in the training data.\nLikelihood (P(B|A)): The likelihood represents how well each\ncandidate word fits the context “The cat is on the....” This is the\nprobability of observing the context given each candidate word. The\nLLM calculates this based on the training data and how often each word\nappears in similar contexts.\nFor example, if the LLM has seen many instances of “The cat is on the\ntable,” it would assign a high likelihood to “table” as the next word in\nthe given context. Similarly, if it has seen many instances of “The cat\nis on the chair,” it would assign a high likelihood to “chair” as the next\nword.\nP(“The cat is on the table”), P(“The cat is on the chair”), and P(“The\ncat is on the roof”) are the likelihoods for each candidate word given\nthe context.\nPosterior probability (P(A|B)): Using Bayes’ theorem, we can\ncalculate the posterior probability for each candidate word based on the\nprior probability and the likelihood:\nSelecting the most likely word. After calculating the posterior\nprobabilities for each candidate word, we choose the word with the\n",
      "content_length": 1098,
      "extraction_method": "Direct"
    },
    {
      "page_number": 45,
      "chapter": null,
      "content": "highest posterior probability as the most likely next word to complete\nthe sentence.\nThe LLM uses Bayes’ theorem and the probabilities learned during training\nto generate text that is contextually relevant and meaningful, capturing\npatterns and associations from the training data to complete sentences in a\ncoherent manner.\nThe following figure illustrates how it translates into the architectural\nframework of a neural network:\nFigure 1.7: Predicting the next most likely word in an LLM\nNote\nThe last layer of the ANN is typically a non-linear activation\nfunction. In the above illustration, the function is Softmax, a\nmathematical function that converts a vector of real numbers\n",
      "content_length": 682,
      "extraction_method": "Direct"
    },
    {
      "page_number": 46,
      "chapter": null,
      "content": "into a probability distribution. It is often used in machine\nlearning to normalize the output of a neural network or a\nclassifier. The Softmax function is defined as follows:\nwhere zi is the i-th element of the input vector, and K is the\nnumber of elements in the vector. The Softmax function\nensures that each element of the output vector is between 0\nand 1 and that the sum of all elements is 1. This makes the\noutput vector suitable for representing probabilities of\ndifferent classes or outcomes.\nOverall, ANNs are the core pillars of the development of generative AI\nmodels: thanks to their mechanisms of tokenization, embedding, and\nmultiple hidden layers, they can capture complex patterns even in the most\nunstructured data, such as natural language.\nHowever, what we are observing today is a set of models that demonstrates\nincredible capabilities that have never been seen before, and this is due to a\nparticular ANNs’ architectural framework, introduced in recent years and the\nmain protagonist of LLM development. This framework is called the\ntransformer, and we are going to cover it in the following section.\nMost popular LLM\ntransformers-based\narchitectures\n",
      "content_length": 1173,
      "extraction_method": "Direct"
    },
    {
      "page_number": 47,
      "chapter": null,
      "content": "ANNs, as we saw in the preceding sections, are at the heart of LLMs.\nNevertheless, in order to be generative, those ANNs need to be endowed\nwith some peculiar capabilities, such as parallel processing of textual\nsentences or keeping the memory of the previous context.\nThese particular capabilities were at the core of generative AI research in the\nlast decades, starting from the 80s and 90s. However, it is only in recent\nyears that the main drawbacks of these early models – such as the capability\nof text parallel processing or memory management – have been bypassed by\nmodern generative AI frameworks. Those frameworks are the so-called\ntransformers.\nIn the following sections, we will explore the evolution of generative AI\nmodel architecture, from early developments to state-of-the-art transformers.\nWe will start by covering the first generative AI models that paved the way\nfor further research, highlighting their limitations and the approaches to\novercome them. We will then explore the introduction of transformer-based\narchitectures, covering their main components and explaining why they\nrepresent the state of the art for LLMs.\nEarly experiments\nThe very first popular generative AI ANN architectures trace back to the 80s\nand 90s, including:\nRecurrent neural networks (RNNs): RNNs are a type of ANN\ndesigned to handle sequential data. They have recurrent connections\nthat allow information to persist across time steps, making them\nsuitable for tasks like language modeling, machine translation, and text\ngeneration. However, RNNs have limitations in capturing long-range\ndependencies due to the vanishing or exploding gradient problem.\n",
      "content_length": 1654,
      "extraction_method": "Direct"
    },
    {
      "page_number": 48,
      "chapter": null,
      "content": "Definition\nIn ANNs, the gradient is a measure of how much the\nmodel’s performance would improve if we slightly\nadjusted its internal parameters (weights). During\ntraining, RNNs try to minimize the difference between\ntheir predictions and the actual targets by adjusting their\nweights based on the gradient of the loss function. The\nproblem of vanishing or exploding gradient arises in\nRNNs during training when the gradients become\nextremely small or large, respectively. The vanishing\ngradient problem occurs when the gradient becomes\nextremely small during training. As a result, the RNN\nlearns very slowly and struggles to capture long-term\npatterns in the data. Conversely, the exploding gradient\nproblem happens when the gradient becomes extremely\nlarge. This leads to unstable training and prevents the\nRNN from converging to a good solution.\nLong short-term memory (LSTM): LSTMs are a variant of RNNs\nthat address the vanishing gradient problem. They introduce gating\nmechanisms that enable better preservation of important information\nacross longer sequences. LSTMs became popular for various sequential\ntasks, including text generation, speech recognition, and sentiment\nanalysis.\nThese architectures were popular and effective for various generative tasks,\nbut they had limitations in handling long-range dependencies, scalability,\nand overall efficiency, especially when dealing with large-scale NLP tasks\nthat would need massive parallel processing. The transformer framework\n",
      "content_length": 1488,
      "extraction_method": "Direct"
    },
    {
      "page_number": 49,
      "chapter": null,
      "content": "was introduced to overcome these limitations. In the next section, we are\ngoing to see how a transformers-based architecture overcomes the above\nlimitations and is at the core of modern generative AI LLMs.\nIntroducing the transformer\narchitecture\nThe transformer architecture is a deep learning model introduced in the\npaper “Attention Is All You Need” by Vaswani et al. (2017). It\nrevolutionized NLP and other sequence-to-sequence tasks.\nThe transformer dispenses with recurrence and convolutions entirely and\nrelies solely on attention mechanisms to encode and decode sequences.\nDefinition\nIn the transformer architecture, “attention” is a mechanism\nthat enables the model to focus on relevant parts of the input\nsequence while generating the output. It calculates attention\nscores between input and output positions, applies Softmax\nto get weights, and takes a weighted sum of the input\nsequence to obtain context vectors. Attention is crucial for\ncapturing long-range dependencies and relationships\nbetween words in the data.\nSince transformers use attention on the same sequence that is currently being\nencoded, we refer to it as self-attention. Self-attention layers are responsible\nfor determining the importance of each input token in generating the output.\nThose answer the question: “Which part of the input should I focus on?”\n",
      "content_length": 1338,
      "extraction_method": "Direct"
    },
    {
      "page_number": 50,
      "chapter": null,
      "content": "In order to obtain the self-attention vector for a sentence, the elements we\nneed are “value”, “query”, and “key.” These matrices are used to calculate\nattention scores between the elements in the input sequence and are the three\nweight matrices that are learned during the training process (typically\ninitialized with random values). More specifically, their purpose is as\nfollows:\nQuery (Q) is used to represent the current focus of the attention\nmechanism\nKey (K) is used to determine which parts of the input should be given\nattention\nValue (V) is used to compute the context vectors\nThey can be represented as follows:\nFigure 1.8: Decomposition of the Input matrix into Q, K, and V vectors\n",
      "content_length": 695,
      "extraction_method": "Direct"
    },
    {
      "page_number": 51,
      "chapter": null,
      "content": "Those matrices are then multiplied and passed through a non-linear\ntransformation (thanks to a Softmax function). The output of the self-\nattention layer represents the input values in a transformed, context-aware\nmanner, which allows the transformer to attend to different parts of the input\ndepending on the task at hand.\nFigure 1.9: Representation of Q, K, and V matrices multiplication to obtain the context vector\nThe mathematical formula is the following:\nFrom an architectural point of view, the transformer consists of two main\ncomponents, an encoder and a decoder:\nThe encoder takes the input sequence and produces a sequence of\nhidden states, each of which is a weighted sum of all the input\nembeddings.\nThe decoder takes the output sequence (shifted right by one position)\nand produces a sequence of predictions, each of which is a weighted\nsum of all the encoder’s hidden states and the previous decoder’s\nhidden states.\n",
      "content_length": 933,
      "extraction_method": "Direct"
    },
    {
      "page_number": 52,
      "chapter": null,
      "content": "Note\nThe reason for shifting the output sequence right by one\nposition in the decoder layer is to prevent the model from\nseeing the current token when predicting the next token. This\nis because the model is trained to generate the output\nsequence given the input sequence, and the output sequence\nshould not depend on itself. By shifting the output sequence\nright, the model only sees the previous tokens as input and\nlearns to predict the next token based on the input sequence\nand the previous output tokens. This way, the model can\nlearn to generate coherent and meaningful sentences without\ncheating.\nThe following illustration from the original paper shows the transformer\narchitecture:\n",
      "content_length": 692,
      "extraction_method": "Direct"
    },
    {
      "page_number": 53,
      "chapter": null,
      "content": "Figure 1.10: Simplified transformer architecture\nLet’s examine each building block, starting from the encoding part:\n",
      "content_length": 117,
      "extraction_method": "Direct"
    },
    {
      "page_number": 54,
      "chapter": null,
      "content": "Input embedding: These are the vector representations of tokenized\ninput text.\nPositional encoding: As the transformer does not have an inherent\nsense of word order (unlike RNNs with their sequential nature),\npositional encodings are added to the input embeddings. These\nencodings provide information about the positions of words in the input\nsequence, allowing the model to understand the order of tokens.\nMulti-head attention layer: This is a mechanism in which multiple\nself-attention mechanisms operate in parallel on different parts of the\ninput data, producing multiple representations. This allows the\ntransformer model to attend to different parts of the input data in\nparallel and aggregate information from multiple perspectives.\nAdd and norm layer: This combines element-wise addition and layer\nnormalization. It adds the output of a layer to the original input and then\napplies layer normalization to stabilize and accelerate training. This\ntechnique helps mitigate gradient-related issues and improves the\nmodel’s performance on sequential data.\nFeed-forward layer: This is responsible for transforming the\nnormalized output of attention layers into a suitable representation for\nthe final output, using a non-linear activation function, such as the\npreviously mentioned Softmax.\nThe decoding part of the transformer starts with a similar process as the\nencoding part, where the target sequence (output sequence) undergoes input\nembedding and positional encoding. Let’s understand these blocks:\nOutput embedding (shifted right): For the decoder, the target\nsequence is “shifted right” by one position. This means that at each\nposition, the model tries to predict the token that comes after the\nanalyzed token in the original target sequence. This is achieved by\n",
      "content_length": 1775,
      "extraction_method": "Direct"
    },
    {
      "page_number": 55,
      "chapter": null,
      "content": "removing the last token from the target sequence and padding it with a\nspecial start-of-sequence token (start symbol). This way, the decoder\nlearns to generate the correct token based on the preceding context\nduring autoregressive decoding.\nDefinition\nAutoregressive decoding is a technique for generating\noutput sequences from a model that predicts each\noutput token based on the previous output tokens. It is\noften used in NLP tasks such as machine translation,\ntext summarization, and text generation.\nAutoregressive decoding works by feeding the model\nan initial token, such as a start-of-sequence symbol,\nand then using the model’s prediction as the next input\ntoken. This process is repeated until the model\ngenerates an end-of-sequence symbol or reaches a\nmaximum length. The output sequence is then the\nconcatenation of all the predicted tokens.\nDecoder layers: Similarly to the encoder block, here, we also have\nPositional Encoding, Multi-Head Attention, Add and Norm, and Feed\nForward layers, whose role is the same as for the encoding part.\nLinear and Softmax: These layers apply, respectively, a linear and\nnon-linear transformation to the output vector. The non-linear\ntransformation (Softmax) conveys the output vector into a probability\ndistribution, corresponding to a set of candidate words. The word\n",
      "content_length": 1318,
      "extraction_method": "Direct"
    },
    {
      "page_number": 56,
      "chapter": null,
      "content": "corresponding to the greatest element of the probability vector will be\nthe output of the whole process.\nThe transformer architecture paved the way for modern LLMs, and it also\nsaw many variations with respect to its original framework.\nSome models use only the encoder part, such as BERT (Bidirectional\nEncoder Representations from Transformers), which is designed for NLU\ntasks such as text classification, question answering, and sentiment analysis.\nOther models use only the decoder part, such as GPT-3 (Generative Pre-\ntrained Transformer 3), which is designed for natural language generation\ntasks such as text completion, summarization, and dialogue.\nFinally, there are models that use both the encoder and the decoder parts,\nsuch as T5 (Text-to-Text Transfer Transformer), which is designed for\nvarious NLP tasks that can be framed as text-to-text transformations, such as\ntranslation, paraphrasing, and text simplification.\nRegardless of the variant, the core component of a transformer – the\nattention mechanism – remains a constant within LLM architecture, and it\nalso represents the reason why those frameworks gained so much popularity\nwithin the context of generative AI and NLP.\nHowever, the architectural variant of an LLM is not the only element that\nfeatures the functioning of that model. This functioning is indeed\ncharacterized also by what the model knows, depending on its training\ndataset, and how well it applies its knowledge upon the user’s request,\ndepending on its evaluation metrics.\nIn the next section, we are going to cover both the processes of training and\nevaluating LLMs, also providing those metrics needed to differentiate\n",
      "content_length": 1662,
      "extraction_method": "Direct"
    },
    {
      "page_number": 57,
      "chapter": null,
      "content": "among different LLMs and understand which one to use for specific use\ncases within your applications.\nTraining and evaluating LLMs\nIn the preceding sections, we saw how choosing an LLM architecture is a\npivotal step in determining its functioning. However, the quality and\ndiversity of the output text depend largely on two factors: the training dataset\nand the evaluation metric.\nThe training dataset determines what kind of data the LLM learns from and\nhow well it can generalize to new domains and languages. The evaluation\nmetric measures how well the LLM performs on specific tasks and\nbenchmarks, and how it compares to other models and human writers.\nTherefore, choosing an appropriate training dataset and evaluation metric is\ncrucial for developing and assessing LLMs.\nIn this section, we will discuss some of the challenges and trade-offs\ninvolved in selecting and using different training datasets and evaluation\nmetrics for LLMs, as well as some of the recent developments and future\ndirections in this area.\nTraining an LLM\nBy definition, LLMs are huge, from a double point of view:\nNumber of parameters: This is a measure of the complexity of the\nLLM architecture and represents the number of connections among\nneurons. Complex architectures have thousands of layers, each one\nhaving multiple neurons, meaning that among layers, we will have\nseveral connections with associated parameters (or weights).\n",
      "content_length": 1417,
      "extraction_method": "Direct"
    },
    {
      "page_number": 58,
      "chapter": null,
      "content": "Training set: This refers to the unlabeled text corpus on which the\nLLM learns and trains its parameters. To give an idea of how big such a\ntext corpus for an LLM can be, let’s consider OpenAI’s GPT-3 training\nset:\nFigure 1.11: GPT-3 knowledge base\nConsidering the assumption:\n1 token ~= 4 characters in English\n1 token ~= ¾ words\nWe can conclude that GPT-3 has been trained on around 374 billion\nwords.\nSo generally speaking, LLMs are trained using unsupervised learning on\nmassive datasets, which often consist of billions of sentences collected from\ndiverse sources on the internet. The transformer architecture, with its self-\nattention mechanism, allows the model to efficiently process long sequences\nof text and capture intricate dependencies between words. Training such\nmodels necessitates vast computational resources, typically employing\ndistributed systems with multiple graphics processing units (GPUs) or\ntensor processing units (TPUs).\n",
      "content_length": 951,
      "extraction_method": "Direct"
    },
    {
      "page_number": 59,
      "chapter": null,
      "content": "Definition\nA tensor is a multi-dimensional array used in mathematics\nand computer science. It holds numerical data and is\nfundamental in fields like machine learning.\nA TPU is a specialized hardware accelerator created by\nGoogle for deep learning tasks. TPUs are optimized for\ntensor operations, making them highly efficient for training\nand running neural networks. They offer fast processing\nwhile consuming less power, enabling faster model training\nand inference in data centers.\nThe training process involves numerous iterations over the dataset, fine-\ntuning the model’s parameters using optimization algorithms\nbackpropagation. Through this process, transformer-based language models\nacquire a deep understanding of language patterns, semantics, and context,\nenabling them to excel in a wide range of NLP tasks, from text generation to\nsentiment analysis and machine translation.\nThe following are the main steps involved in the training process of an\nLLM:\n1. Data collection: This is the process of gathering a large amount of text\ndata from various sources, such as the open web, books, news articles,\nsocial media, etc. The data should be diverse, high-quality, and\nrepresentative of the natural language that the LLM will encounter.\n2. Data preprocessing: This is the process of cleaning, filtering, and\nformatting the data for training. This may include removing duplicates,\n",
      "content_length": 1387,
      "extraction_method": "Direct"
    },
    {
      "page_number": 60,
      "chapter": null,
      "content": "noise, or sensitive information, splitting the data into sentences or\nparagraphs, tokenizing the text into subwords or characters, etc.\n3. Model architecture: This is the process of designing the structure and\nparameters of the LLM. This may include choosing the type of neural\nnetwork (such as transformer) and its structure (such as decoder only,\nencoder only, or encoder-decoder), the number and size of layers, the\nattention mechanism, the activation function, etc.\n4. Model initialization: This is the process of assigning initial values to\nthe weights and biases of the LLM. This may be done randomly or by\nusing pre-trained weights from another model.\n5. Model pre-training: This is the process of updating the weights and\nbiases of the LLM by feeding it batches of data and computing the loss\nfunction. The loss function measures how well the LLM predicts the\nnext token given the previous tokens. The LLM tries to minimize the\nloss by using an optimization algorithm (such as gradient descent) that\nadjusts the weights and biases in the direction that reduces the loss with\nthe backpropagation mechanism. The model training may take several\nepochs (iterations over the entire dataset) until it converges to a low\nloss value.\nDefinition\nIn the context of neural networks, the optimization\nalgorithm during training is the method used to find\nthe best set of weights for the model that minimizes\nthe prediction error or maximizes the accuracy of the\ntraining data. The most common optimization\nalgorithm for neural networks is stochastic gradient\ndescent (SGD), which updates the weights in small\n",
      "content_length": 1604,
      "extraction_method": "Direct"
    },
    {
      "page_number": 61,
      "chapter": null,
      "content": "steps based on the gradient of the error function and\nthe current input-output pair. SGD is often combined\nwith backpropagation, which we defined earlier in this\nchapter.\nThe output of the pre-training phase is the so-called base model.\n6. Fine-tuning: The base model is trained in a supervised way with a\ndataset made of tuples of (prompt, ideal response). This step is\nnecessary to make the base model more in line with AI assistants, such\nas ChatGPT. The output of this phase is called the supervised fine-\ntuned (SFT) model.\n7. Reinforcement learning from human feedback (RLHF): This step\nconsists of iteratively optimizing the SFT model (by updating some of\nits parameters) with respect to the reward model (typically another\nLLM trained incorporating human preferences).\nDefinition\nReinforcement learning (RL) is a branch of machine\nlearning that focuses on training computers to make optimal\ndecisions by interacting with their environment. Instead of\nbeing given explicit instructions, the computer learns\nthrough trial and error: by exploring the environment and\nreceiving rewards or penalties for its actions. The goal of\nreinforcement learning is to find the optimal behavior or\npolicy that maximizes the expected reward or value of a\ngiven model. To do so, the RL process involves a reward\nmodel (RM) that is able to provide a “preferability score” to\n",
      "content_length": 1364,
      "extraction_method": "Direct"
    },
    {
      "page_number": 62,
      "chapter": null,
      "content": "the computer. In the context of RLHF, the RM is trained to\nincorporate human preferences.\nNote that RLHF is a pivotal milestone in achieving human alignment with\nAI systems. Due to the rapid achievements in the field of generative AI, it is\npivotal to keep endowing those powerful LLMs and, more generally, LFMs\nwith those preferences and values that are typical of human beings.\nOnce we have a trained model, the next and final step is evaluating its\nperformance.\nModel evaluation\nEvaluating traditional AI models was, in some ways, pretty intuitive. For\nexample, let’s think about an image classification model that has to\ndetermine whether the input image represents a dog or a cat. So we train our\nmodel on a training dataset with a set of labeled images and, once the model\nis trained, we test it on unlabeled images. The evaluation metric is simply\nthe percentage of correctly classified images over the total number of images\nwithin the test set.\nWhen it comes to LLMs, the story is a bit different. As those models are\ntrained on unlabeled text and are not task-specific, but rather generic and\nadaptable given a user’s prompt, traditional evaluation metrics were not\nsuitable anymore. Evaluating an LLM means, among other things,\nmeasuring its language fluency, coherence, and ability to emulate different\nstyles depending on the user’s request.\nHence, a new set of evaluation frameworks needed to be introduced. The\nfollowing are the most popular frameworks used to evaluate LLMs:\n",
      "content_length": 1491,
      "extraction_method": "Direct"
    },
    {
      "page_number": 63,
      "chapter": null,
      "content": "General Language Understanding Evaluation (GLUE) and\nSuperGLUE: This benchmark is used to measure the performance of\nLLMs on various NLU tasks, such as sentiment analysis, natural\nlanguage inference, question answering, etc. The higher the score on\nthe GLUE benchmark, the better the LLM is at generalizing across\ndifferent tasks and domains.\nIt recently evolved into a new benchmark styled after GLUE and\ncalled SuperGLUE, which comes with more difficult tasks. It consists\nof eight challenging tasks that require more advanced reasoning skills\nthan GLUE, such as natural language inference, question answering,\ncoreference resolution, etc., a broad coverage diagnostic set that tests\nmodels on various linguistic capabilities and failure modes, and a\nleaderboard that ranks models based on their average score across all\ntasks.\nThe difference between the GLUE and the SuperGLUE benchmark is\nthat the SuperGLUE benchmark is more challenging and realistic than\nthe GLUE benchmark, as it covers more complex tasks and\nphenomena, requires models to handle multiple domains and formats,\nand has higher human performance baselines. The SuperGLUE\nbenchmark is designed to drive research in the development of more\ngeneral and robust NLU systems.\nMassive Multitask Language Understanding (MMLU): This\nbenchmark measures the knowledge of an LLM using zero-shot and\nfew-shot settings.\nDefinition\n",
      "content_length": 1388,
      "extraction_method": "Direct"
    },
    {
      "page_number": 64,
      "chapter": null,
      "content": "The concept of zero-shot evaluation is a method of\nevaluating a language model without any labeled data\nor fine-tuning. It measures how well the language\nmodel can perform a new task by using natural\nlanguage instructions or examples as prompts and\ncomputing the likelihood of the correct output given\nthe input. It is the probability that a trained model will\nproduce a particular set of tokens without needing any\nlabeled training data.\nThis design adds complexity to the benchmark and aligns it more\nclosely with the way we assess human performance. The benchmark\ncomprises 14,000 multiple-choice questions categorized into 57\ngroups, spanning STEM, humanities, social sciences, and other fields.\nIt covers a spectrum of difficulty levels, ranging from basic to\nadvanced professional, assessing both general knowledge and\nproblem-solving skills. The subjects encompass various areas,\nincluding traditional ones like mathematics and history, as well as\nspecialized domains like law and ethics. The extensive range of\nsubjects and depth of coverage make this benchmark valuable for\nuncovering any gaps in a model’s knowledge. Scoring is based on\nsubject-specific accuracy and the average accuracy across all subjects.\nHellaSwag: The HellaSwag evaluation framework is a method of\nevaluating LLMs on their ability to generate plausible and common\nsense continuations for given contexts. It is based on the HellaSwag\ndataset, which is a collection of 70,000 multiple-choice questions that\ncover diverse domains and genres, such as books, movies, recipes, etc.\nEach question consists of a context (a few sentences that describe a\n",
      "content_length": 1627,
      "extraction_method": "Direct"
    },
    {
      "page_number": 65,
      "chapter": null,
      "content": "situation or an event) and four possible endings (one correct and three\nincorrect). The endings are designed to be hard to distinguish for\nLLMs, as they require world knowledge, common sense reasoning, and\nlinguistic understanding.\nTruthfulQA: This benchmark evaluates a language model’s accuracy\nin generating responses to questions. It includes 817 questions across\n38 categories like health, law, finance, and politics. The questions are\ndesigned to mimic those that humans might answer incorrectly due to\nfalse beliefs or misunderstandings.\nAI2 Reasoning Challenge (ARC): This benchmark is used to measure\nLLMs’ reasoning capabilities and to stimulate the development of\nmodels that can perform complex NLU tasks. It consists of a dataset of\n7,787 multiple-choice science questions, assembled to encourage\nresearch in advanced question answering. The dataset is divided into an\nEasy set and a Challenge set, where the latter contains only questions\nthat require complex reasoning or additional knowledge to answer\ncorrectly. The benchmark also provides a corpus of over 14 million\nscience sentences that can be used as supporting evidence for the\nquestions.\nIt is important to note that each evaluation framework has a focus on a\nspecific feature. Namely, the GLUE benchmark focuses on grammar,\nparaphrasing, and text similarity, while MMLU focuses on generalized\nlanguage understanding among various domains and tasks. Hence, while\nevaluating an LLM, it is important to have a clear understanding of the final\ngoal, so that the most relevant evaluation framework can be used.\nAlternatively, if the goal is that of having the best of the breed in any task, it\nis key not to use only one evaluation framework, but rather an average of\nmultiple frameworks.\n",
      "content_length": 1759,
      "extraction_method": "Direct"
    },
    {
      "page_number": 66,
      "chapter": null,
      "content": "In addition to that, in case no existing LLM is able to tackle your specific\nuse cases, you still have a margin to customize those models and make them\nmore tailored toward your application scenarios. In the next section, we are\nindeed going to cover the existing techniques of LLM customization, from\nthe lightest ones (such as prompt engineering) up to the whole training of an\nLLM from scratch.\nBase models versus\ncustomized models\nThe nice thing about LLMs is that they have been trained and ready to use.\nAs we saw in the previous section, training an LLM requires great\ninvestment in hardware (GPUs or TPUs) and it might last for months, and\nthese two factors might mean it is not feasible for individuals and small\nbusinesses.\nLuckily, pre-trained LLMs are generalized enough to be applicable to\nvarious tasks, so they can be consumed without further tuning directly via\ntheir REST API (we will dive deeper into model consumption in the next\nchapters).\nNevertheless, there might be scenarios where a general-purpose LLM is not\nenough, since it lacks domain-specific knowledge or doesn’t conform to a\nparticular style and taxonomy of communication. If this is the case, you\nmight want to customize your model.\nHow to customize your model\nThere are three main ways to customize your model:\n",
      "content_length": 1295,
      "extraction_method": "Direct"
    },
    {
      "page_number": 67,
      "chapter": null,
      "content": "Extending non-parametric knowledge: This allows the model to\naccess external sources of information to integrate its parametric\nknowledge while responding to the user’s query.\nDefinition\nLLMs exhibit two types of knowledge: parametric and\nnon-parametric. The parametric knowledge is the one\nembedded in the LLM’s parameters, deriving from the\nunlabeled text corpora during the training phase. On\nthe other hand, non-parametric knowledge is the one\nwe can “attach” to the model via embedded\ndocumentation. Non-parametric knowledge doesn’t\nchange the structure of the model, but rather, allows it\nto navigate through external documentation to be used\nas relevant context to answer the user’s query.\nThis might involve connecting the model to web sources (like\nWikipedia) or internal documentation with domain-specific\nknowledge. The connection of the LLM to external sources is called a\nplug-in, and we will be discussing it more deeply in the hands-on\nsection of this book.\nFew-shot learning: In this type of model customization, the LLM is\ngiven a metaprompt with a small number of examples (typically\nbetween 3 and 5) of each new task it is asked to perform. The model\nmust use its prior knowledge to generalize from these examples to\nperform the task.\n",
      "content_length": 1254,
      "extraction_method": "Direct"
    },
    {
      "page_number": 68,
      "chapter": null,
      "content": "Definition\nA metaprompt is a message or instruction that can be\nused to improve the performance of LLMs on new\ntasks with a few examples.\nFine tuning: The fine-tuning process involves using smaller, task-\nspecific datasets to customize the foundation models for particular\napplications.\nThis approach differs from the first ones because, with fine-tuning, the\nparameters of the pre-trained model are altered and optimized toward\nthe specific task. This is done by training the model on a smaller\nlabeled dataset that is specific to the new task. The key idea behind\nfine-tuning is to leverage the knowledge learned from the pre-trained\nmodel and fine-tune it to the new task, rather than training a model\nfrom scratch.\n",
      "content_length": 719,
      "extraction_method": "Direct"
    },
    {
      "page_number": 69,
      "chapter": null,
      "content": "Figure 1.12: Illustration of the process of fine-tuning\nIn the preceding figure, you can see a schema on how fine-tuning\nworks on OpenAI pre-built models. The idea is that you have available\na pre-trained model with general-purpose weights or parameters.\nThen, you feed your model with custom data, typically in the form of\n“key-value” prompts and completions:\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated\n...\nOnce the training is done, you will have a customized model that is\nparticularly performant for a given task, for example, the classification\nof your company’s documentation.\nThe nice thing about fine-tuning is that you can make pre-built models\ntailored to your use cases, without the need to retrain them from\nscratch, yet leveraging smaller training datasets and hence less training\ntime and compute. At the same time, the model keeps its generative\npower and accuracy learned via the original training, the one that\noccurred to the massive dataset.\nIn Chapter 11, Fine-Tuning Large Language Models, we will focus on\nfine-tuning your model in Python so that you can test it for your own\ntask.\nOn top of the above techniques (which you can also combine among each\nother), there is a fourth one, which is the most “drastic.” It consists of\ntraining an LLM from scratch, which you might want to either build on your\n",
      "content_length": 1461,
      "extraction_method": "Direct"
    },
    {
      "page_number": 70,
      "chapter": null,
      "content": "own or initialize from a pre-built architecture. We will see how to approach\nthis technique in the final chapters.\nSummary\nIn this chapter, we explored the field of LLMs, with a technical deep dive\ninto their architecture, functioning, and training process. We saw the most\nprominent architectures, such as the transformer-based frameworks, how the\ntraining process works, and different ways to customize your own LLM.\nWe now have the foundation to understand what LLMs are. In the next\nchapter, we will see how to use them and, more specifically, how to build\nintelligent applications with them.\nReferences\nAttention is all you need: 1706.03762.pdf (arxiv.org)\nPossible End of Humanity from AI? Geoffrey Hinton at MIT\nTechnology Review’s EmTech Digital:\nhttps://www.youtube.com/watch?\nv=sitHS6UDMJc&t=594s&ab_channel=JosephRaczynski\nThe Glue Benchmark: https://gluebenchmark.com/\nTruthfulQA:\nhttps://paperswithcode.com/dataset/truthfulqa\nHugging Face Open LLM Leaderboard:\nhttps://huggingface.co/spaces/optimum/llm-perf-\nleaderboard\nThink you have Solved Question Answering? Try ARC, the AI2\nReasoning Challenge: https://arxiv.org/abs/1803.05457\n",
      "content_length": 1147,
      "extraction_method": "Direct"
    },
    {
      "page_number": 71,
      "chapter": null,
      "content": "Join our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\nOceanofPDF.com\n",
      "content_length": 154,
      "extraction_method": "Direct"
    },
    {
      "page_number": 72,
      "chapter": null,
      "content": "2\nLLMs for AI-Powered\nApplications\nIn Chapter 1, Introduction to Large Language Models, we introduced large\nlanguage models (LLMs) as powerful foundation models with generative\ncapabilities as well as powerful common-sense reasoning. Now, the next question\nis: what should I do with those models?\nIn this chapter, we are going to see how LLMs are revolutionizing the world of\nsoftware development, leading to a new era of AI-powered applications. By the\nend of this chapter, you will have a clearer picture of how LLMs can be\nembedded in different application scenarios, thanks to the new AI orchestrator\nframeworks that are populating the market of AI development.\nIn this chapter, we will cover the following topics:\nHow LLMs are changing software development\nThe copilot system\nIntroducing AI orchestrators to embed LLMs into applications\nHow LLMs are changing\nsoftware development\nLLMs have proven to have extraordinary capabilities: from natural language\nunderstanding tasks (summarization, named entity recognition, and classification)\nto text generation, from common-sense reasoning to brainstorming skills.\n",
      "content_length": 1115,
      "extraction_method": "Direct"
    },
    {
      "page_number": 73,
      "chapter": null,
      "content": "However, they are not just incredible by themselves. As discussed in Chapter 1,\nLLMs and, generally speaking, large foundation models (LFMs), are\nrevolutionizing software development by serving as platforms for building\npowerful applications.\nIn fact, instead of starting from scratch, today developers can make API calls to a\nhosted version of an LLM, with the option of customizing it for their specific\nneeds, as we saw in the previous chapter. This shift allows teams to incorporate\nthe power of AI more easily and efficiently into their applications, similar to the\ntransition from single-purpose computing to time-sharing in the past.\nBut what does it mean, concretely, to incorporate LLMs within applications?\nThere are two main aspects to consider when incorporating LLMs within\napplications:\nThe technical aspect, which covers the how. Integrating LLMs into\napplications involves embedding them through REST API calls and\nmanaging them with AI orchestrators. This means setting up architectural\ncomponents that allow seamless communication with the LLMs via API\ncalls. Additionally, using AI orchestrators helps to efficiently manage and\ncoordinate the LLMs’ functionality within the application, as we will discuss\nlater in this chapter.\nThe conceptual aspect, which covers the what. LLMs bring a plethora of\nnew capabilities that can be harnessed within applications. These capabilities\nwill be explored in detail later in this book. One way to view LLMs’ impact\nis by considering them as a new category of software, often referred to as\ncopilot. This categorization highlights the significant assistance and\ncollaboration provided by LLMs in enhancing application functionalities.\nWe will delve into the technical aspect later on in this chapter, while the next\nsection will cover a brand-new category of software – the copilot system.\n",
      "content_length": 1848,
      "extraction_method": "Direct"
    },
    {
      "page_number": 74,
      "chapter": null,
      "content": "The copilot system\nThe copilot system is a new category of software that serves as an expert helper\nto users trying to accomplish complex tasks. This concept was coined by\nMicrosoft and has already been introduced into its applications, such as M365\nCopilot and the new Bing, now powered by GPT-4. With the same framework\nthat is used by these products, developers can now build their own copilots to\nembed within their applications.\nBut what exactly is a copilot?\nAs the name suggests, copilots are meant to be AI assistants that work side by\nside with users and support them in various activities, from information retrieval\nto blog writing and posting, from brainstorming ideas to code review and\ngeneration.\nThe following are some unique features of copilots:\nA copilot is powered by LLMs, or, more generally, LFMs, meaning that\nthese are the reasoning engines that make the copilot “intelligent.” This\nreasoning engine is one of its components, but not the only one. A copilot\nalso relies on other technologies, such as apps, data sources, and user\ninterfaces, to provide a useful and engaging experience for users. The\nfollowing illustration shows how this works:\n",
      "content_length": 1170,
      "extraction_method": "Direct"
    },
    {
      "page_number": 75,
      "chapter": null,
      "content": "Figure 2.1: A copilot is powered by an LLM\nA copilot is designed to have a conversational user interface, allowing\nusers to interact with it using natural language. This reduces or even\neliminates the knowledge gap between complex systems that need domain-\nspecific taxonomy (for example, querying tabular data needs the knowledge\nof programming languages such as T-SQL) and users. Let’s look at an\nexample of such a conversation:\n",
      "content_length": 431,
      "extraction_method": "Direct"
    },
    {
      "page_number": 76,
      "chapter": null,
      "content": "Figure 2.2: An example of a conversational UI to reduce the gap between the user and the database\nA copilot has a scope. This means that it is grounded to domain-specific\ndata so that it is entitled to answer only within the perimeter of the\napplication or domain.\nDefinition\nGrounding is the process of using LLMs with information that is\nuse case specific, relevant, and not available as part of the LLM’s\ntrained knowledge. It is crucial for ensuring the quality, accuracy,\nand relevance of the output. For example, let’s say you want an\nLLM-powered application that assists you during your research\non up-to-date papers (not included in the training dataset of your\nLLM). You also want your app to only respond if the answer is\n",
      "content_length": 732,
      "extraction_method": "Direct"
    },
    {
      "page_number": 77,
      "chapter": null,
      "content": "included in those papers. To do so, you will need to ground your\nLLM to the set of papers, so that your application will only\nrespond within this perimeter.\nGrounding is achieved through an architectural framework called retrieval-\naugmented generation (RAG), a technique that enhances the output of\nLLMs by incorporating information from an external, authoritative\nknowledge base before generating a response. This process helps to ensure\nthat the generated content is relevant, accurate, and up to date.\nWhat is the difference between a copilot and a RAG? RAG can be\nseen as one of the architectural patterns that feature a copilot.\nWhenever we want our copilot to be grounded to domain-specific\ndata, we use a RAG framework. Note that RAG is not the only\narchitectural pattern that can feature a copilot: there are further\nframeworks such as function calling or multi-agents that we will\nexplore throughout the book.\nFor example, let’s say we developed a copilot within our company that\nallows employees to chat with their enterprise knowledge base. As fun as it\ncan be, we cannot provide users with a copilot they can use to plan their\nsummer trip (it would be like providing users with a ChatGPT-like tool at\nour own hosting cost!); on the contrary, we want the copilot to be grounded\nonly to our enterprise knowledge base so that it can respond only if the\nanswer is pertinent to the domain-specific context.\nThe following figure shows an example of grounding a copilot system:\n",
      "content_length": 1484,
      "extraction_method": "Direct"
    },
    {
      "page_number": 78,
      "chapter": null,
      "content": "Figure 2.3: Example of grounding a copilot\nThe copilot’s capabilities can be extended by skills, which can be code or\ncalls to other models. In fact, the LLM (our reasoning engine) might have\ntwo kinds of limitations:\nLimited parametric knowledge. This is due to the knowledge base\ncutoff date, which is a physiological feature of LLMs. In fact, their\ntraining dataset will always be “outdated,” not in line with the current\n",
      "content_length": 425,
      "extraction_method": "Direct"
    },
    {
      "page_number": 79,
      "chapter": null,
      "content": "trends. This can be overcome by adding non-parametric knowledge\nwith grounding, as previously seen.\nLack of executive power. This means that LLMs by themselves are\nnot empowered to carry out actions. Let’s consider, for example, the\nwell-known ChatGPT: if we ask it to generate a LinkedIn post about\nproductivity tips, we will then need to copy and paste it onto our\nLinkedIn profile as ChatGPT is not able to do so by itself. That is the\nreason why we need plug-ins. Plug-ins are LLMs’ connectors toward\nthe external world that serve not only as input sources to extend\nLLMs’ non-parametric knowledge (for example, to allow a web\nsearch) but also as output sources so that the copilot can actually\nexecute actions. For example, with a LinkedIn plug-in, our copilot\npowered by an LLM will be able not only to generate the post but also\nto post it online.\nFigure 2.4: Example of Wikipedia and LinkedIn plug-ins\nNote that the user’s prompt in natural language is not the only input the model\nprocesses. In fact, it is a crucial component of the backend logic of our LLM-\npowered applications and the set of instructions we provide to the model. This\nmetaprompt or system message is the object of a new discipline called prompt\nengineering.\n",
      "content_length": 1238,
      "extraction_method": "Direct"
    },
    {
      "page_number": 80,
      "chapter": null,
      "content": "Definition\nPrompt engineering is the process of designing and optimizing\nprompts to LLMs for a wide variety of applications and research\ntopics. Prompts are short pieces of text that are used to guide the\nLLM’s output. Prompt engineering skills help to better understand\nthe capabilities and limitations of LLMs.\nPrompt engineering involves selecting the right words, phrases,\nsymbols, and formats that elicit the desired response from the\nLLM. Prompt engineering also involves using other controls, such\nas parameters, examples, or data sources, to influence the LLM’s\nbehavior. For example, if we want our LLM-powered application\nto generate responses for a 5-year-old child, we can specify this in\na system message similar to “Act as a teacher who explains\ncomplex concepts to 5-year-old children.”\nIn fact, Andrej Karpathy, the previous Director of AI at Tesla, who returned to\nOpenAI in February 2023, tweeted that “English is the hottest new programming\nlanguage.”\nWe will dive deeper into the concept of prompt engineering in Chapter 4, Prompt\nEngineering. In the next section, we are going to focus on the emerging AI\norchestrators.\nIntroducing AI orchestrators to\nembed LLMs into applications\nEarlier in this chapter, we saw that there are two main aspects to consider when\nincorporating LLMs within applications: a technical aspect and a conceptual\naspect. While we can explain the conceptual aspect with the brand-new category\n",
      "content_length": 1438,
      "extraction_method": "Direct"
    },
    {
      "page_number": 81,
      "chapter": null,
      "content": "of software called Copilot, in this section, we are going to further explore how to\ntechnically embed and orchestrate LLMs within our applications.\nThe main components of AI\norchestrators\nFrom one side, the paradigm shift of foundation models implies a great\nsimplification in the domain of AI-powered applications: after producing models,\nnow the trend is consuming models. On the other side, many roadblocks might\narise in developing this new kind of AI, since there are LLM-related components\nthat are brand new and have never been managed before within an application life\ncycle. For example, there might be malicious actors that could try to change the\nLLM instructions (the system message mentioned earlier) so that the application\ndoes not follow the correct instructions. This is an example of a new set of\nsecurity threats that are typical to LLM-powered applications and need to be\naddressed with powerful counterattacks or preventive techniques.\nThe following is an illustration of the main components of such applications:\nFigure 2.5: High-level architecture of LLM-powered applications\n",
      "content_length": 1099,
      "extraction_method": "Direct"
    },
    {
      "page_number": 82,
      "chapter": null,
      "content": "Let’s inspect each of these components in detail:\nModels: The model is simply the type of LLM we decide to embed in our\napplication. There are two main categories of models:\nProprietary LLMs: Models that are owned by specific companies or\norganizations. Examples include GPT-3 and GPT-4, developed by\nOpenAI, or Bard, developed by Google. As their source code and\narchitecture are not available, those models cannot be re-trained from\nscratch on custom data, yet they can be fine-tuned if needed.\nOpen-source: Models with code and architecture freely available and\ndistributed, hence they can also be trained from scratch on custom\ndata. Examples include Falcon LLM, developed by Abu Dhabi’s\nTechnology Innovation Institute (TII), or LLaMA, developed by\nMeta.\nWe will dive deeper into the main set of LLMs available today in Chapter\n3, Choosing an LLM for Your Application.\nMemory: LLM applications commonly use a conversational interface,\nwhich requires the ability to refer back to earlier information within the\nconversation. This is achieved through a “memory” system that allows the\napplication to store and retrieve past interactions. Note that past interactions\ncould also constitute additional non-parametric knowledge to be added to the\nmodel. To achieve that, it is important to store all the past conversations –\nproperly embedded – into VectorDB, which is at the core of the application’s\ndata.\nDefinition\nVectorDB is a type of database that stores and retrieves\ninformation based on vectorized embeddings, the numerical\nrepresentations that capture the meaning and context of\ntext. By using VectorDB, you can perform semantic search\n",
      "content_length": 1646,
      "extraction_method": "Direct"
    },
    {
      "page_number": 83,
      "chapter": null,
      "content": "and retrieval based on the similarity of meanings rather than\nkeywords. VectorDB can also help LLMs generate more\nrelevant and coherent text by providing contextual\nunderstanding and enriching generation results. Some\nexamples of VectorDBs are Chroma, Elasticsearch, Milvus,\nPinecone, Qdrant, Weaviate, and Facebook AI Similarity\nSearch (FAISS).\nFAISS, developed by Facebook (now Meta) in 2017, was\none of the pioneering vector databases. It was designed for\nefficient similarity search and clustering of dense vectors\nand is particularly useful for multimedia documents and\ndense embeddings. It was initially an internal research\nproject at Facebook. Its primary goal was to better utilize\nGPUs for identifying similarities related to user\npreferences. Over time, it evolved into the fastest available\nlibrary for similarity search and can handle billion-scale\ndatasets. FAISS has opened up possibilities for\nrecommendation engines and AI-based assistant systems.\nPlug-ins: They can be seen as additional modules or components that can be\nintegrated into the LLM to extend its functionality or adapt it to specific\ntasks and applications. These plug-ins act as add-ons, enhancing the\ncapabilities of the LLM beyond its core language generation or\ncomprehension abilities.\nThe idea behind plug-ins is to make LLMs more versatile and adaptable,\nallowing developers and users to customize the behavior of the language\nmodel for their specific needs. Plug-ins can be created to perform various\ntasks, and they can be seamlessly incorporated into the LLM’s architecture.\n",
      "content_length": 1567,
      "extraction_method": "Direct"
    },
    {
      "page_number": 84,
      "chapter": null,
      "content": "Prompts: This is probably the most interesting and pivotal component of an\nLLM-powered application. We’ve already quoted, in the previous section,\nAndrej Karpathy’s affirmation that “English is the hottest new programming\nlanguage,” and you will understand why in the upcoming chapters. Prompts\ncan defined at two different levels:\n“Frontend,” or what the user sees: A “prompt” refers to the input to\nthe model. It is the way the user interacts with the application, asking\nthings in natural language.\n“Backend,” or what the user does not see: Natural language is not\nonly the way to interact, as a user, with the frontend; it is also the way\nwe “program” the backend. In fact, on top of the user’s prompt, there\nare many natural language instructions, or meta-promts, that we give\nto the model so that it can properly address the user’s query. Meta-\nprompts are meant to instruct the model to act as it is meant to. For\nexample, if we want to limit our application to answer only questions\nrelated to the documentation we provided in VectorDB, we will\nspecify the following in our meta-prompts to the model: “Answer only\nif the question is related to the provided documentation.”\nFinally, we get to the core of the high-level architecture shown in Figure 2.5, that\nis, the AI orchestrator. With the AI orchestrator, we refer to lightweight libraries\nthat make it easier to embed and orchestrate LLMs within applications.\nAs LLMs went viral by the end of 2022, many libraries started arising in the\nmarket. In the next sections, we are going to focus on three of them: LangChain,\nSemantic Kernel, and Haystack.\nLangChain\nLangChain was launched as an open-source project by Harrison Chase in October\n2022. It can be used both in Python and JS/TS. It is a framework for developing\napplications powered by language models, making them data-aware (with\n",
      "content_length": 1849,
      "extraction_method": "Direct"
    },
    {
      "page_number": 85,
      "chapter": null,
      "content": "grounding) and agentic – which means they are able to interact with external\nenvironments.\nLet’s take a look at the key components of LangChain:\nFigure 2.6: LangChain’s components\nOverall, LangChain has the following core modules:\nModels: These are the LLMs or LFMs that will be the engine of the\napplication. LangChain supports proprietary models, such as those available\nin OpenAI and Azure OpenAI, and open-source models consumable from\nthe Hugging Face Hub.\nDefinition\nHugging Face is a company and a community that builds\nand shares state-of-the-art models and tools for natural\nlanguage processing and other machine learning domains. It\ndeveloped the Hugging Face Hub, a platform where people\ncan create, discover, and collaborate on machine learning\nmodels and LLMs, datasets, and demos. The Hugging Face\nHub hosts over 120k models, 20k datasets, and 50k demos\n",
      "content_length": 868,
      "extraction_method": "Direct"
    },
    {
      "page_number": 86,
      "chapter": null,
      "content": "in various domains and tasks, such as audio, vision, and\nlanguage.\nAlongside models, LangChain also offers many prompt-related components\nthat make it easier to manage the prompt flow.\nData connectors: These refer to the building blocks needed to retrieve the\nadditional external knowledge (for example, in RAG-based scenarios) we\nwant to provide the model with. Examples of data connectors are document\nloaders or text embedding models.\nMemory: This allows the application to keep references to the user’s\ninteractions, in both the short and long term. It is typically based on\nvectorized embeddings stored in VectorDB.\nChains: These are predetermined sequences of actions and calls to LLMs\nthat make it easier to build complex applications that require chaining LLMs\nwith each other or with other components. An example of a chain might be:\ntake the user query, chunk it into smaller pieces, embed those chunks, search\nfor similar embeddings in VectorDB, use the top three most similar chunks\nin VectorDB as context to provide the answer, and generate the answer.\nAgents: Agents are entities that drive decision-making within LLM-powered\napplications. They have access to a suite of tools and can decide which tool\nto call based on the user input and the context. Agents are dynamic and\nadaptive, meaning that they can change or adjust their actions based on the\nsituation or the goal.\nLangChain offers the following benefits:\nLangChain provides modular abstractions for the components we previously\nmentioned that are necessary to work with language models, such as\nprompts, memory, and plug-ins.\nAlongside those components, LangChain also offers pre-built chains, which\nare structured concatenations of components. Those chains can be pre-built\n",
      "content_length": 1749,
      "extraction_method": "Direct"
    },
    {
      "page_number": 87,
      "chapter": null,
      "content": "for specific use cases or be customized.\nIn Part 2 of this book, we will go through a series of hands-on applications, all\nLangChain based. So, starting from Chapter 5, Embedding LLMs within Your\nApplications, we will focus much deeper on LangChain components and overall\nframeworks.\nHaystack\nHaystack is a Python-based framework developed by Deepset, a startup founded\nin 2018 in Berlin by Milos Rusic, Malte Pietsch, and Timo Möller. Deepset\nprovides developers with the tools to build natural language processing (NLP)-\nbased applications, and with the introduction of Haystack, they are taking them to\nthe next level.\nThe following illustration shows the core components of Haystack:\nFigure 2.7: Haystack’s components\nLet’s look at these components in detail:\nNodes: These are components that perform a specific task or function, such\nas a retriever, a reader, a generator, a summarizer, etc. Nodes can be LLMs\nor other utilities that interact with LLMs or other resources. Among LLMs,\n",
      "content_length": 990,
      "extraction_method": "Direct"
    },
    {
      "page_number": 88,
      "chapter": null,
      "content": "Haystack supports proprietary models, such as those available in OpenAI\nand Azure OpenAI, and open-source models consumable from the Hugging\nFace Hub.\nPipelines: These are sequences of calls to nodes that perform natural\nlanguage tasks or interact with other resources. Pipelines can be querying\npipelines or indexing pipelines, depending on whether they perform searches\non a set of documents or prepare documents for search. Pipelines are\npredetermined and hardcoded, meaning that they do not change or adapt\nbased on the user input or the context.\nAgent: This is an entity that uses LLMs to generate accurate responses to\ncomplex queries. An agent has access to a set of tools, which can be\npipelines or nodes, and it can decide which tool to call based on the user\ninput and the context. An agent is dynamic and adaptive, meaning that it can\nchange or adjust its actions based on the situation or the goal.\nTools: There are functions that an agent can call to perform natural language\ntasks or interact with other resources. Tools can be pipelines or nodes that\nare available to the agent and they can be grouped into toolkits, which are\nsets of tools that can accomplish specific objectives.\nDocumentStores: These are backends that store and retrieve documents for\nsearches. DocumentStores can be based on different technologies, also\nincluding VectorDB (such as FAISS, Milvus, or Elasticsearch).\nSome of the benefits offered by Haystack are:\nEase of use: Haystack is user-friendly and straightforward. It’s often chosen\nfor lighter tasks and rapid prototypes.\nDocumentation quality: Haystack’s documentation is considered high-\nquality, aiding developers in building search systems, question-answering,\nsummarization, and conversational AI.\nEnd-to-end framework: Haystack covers the entire LLM project life cycle,\nfrom data preprocessing to deployment. It’s ideal for large-scale search\n",
      "content_length": 1893,
      "extraction_method": "Direct"
    },
    {
      "page_number": 89,
      "chapter": null,
      "content": "systems and information retrieval.\nAnother nice thing about Haystack is that you can deploy it as a REST API\nand it can be consumed directly.\nSemantic Kernel\nSemantic Kernel is the third open-source SDK we are going to explore in this\nchapter. It was developed by Microsoft, originally in C# and now also available in\nPython.\nThis framework takes its name from the concept of a “kernel,” which, generally\nspeaking, refers to the core or essence of a system. In the context of this\nframework, a kernel is meant to act as the engine that addresses a user’s input by\nchaining and concatenating a series of components into pipelines, encouraging\nfunction composition.\nDefinition\nIn mathematics, function composition is a way to combine two\nfunctions to create a new function. The idea is to use the output of\none function as the input to another function, forming a chain of\nfunctions. The composition of two functions f and g is denoted as\n(f  g), where the function g is applied first, followed by the\nfunction f \n(f  g)(x) = f(g(x)).\nFunction composition in computer science is a powerful concept\nthat allows for the creation of more sophisticated and reusable\ncode by combining smaller functions into larger ones. It enhances\nmodularity and code organization, making programs easier to read\nand maintain.\nThe following is an illustration of the anatomy of Semantic Kernel:\n",
      "content_length": 1373,
      "extraction_method": "Direct"
    },
    {
      "page_number": 90,
      "chapter": null,
      "content": "Figure 2.8: Anatomy of Semantic Kernel\nSemantic Kernel has the following main components:\nModels: These are the LLMs or LFMs that will be the engine of the\napplication. Semantic Kernel supports proprietary models, such as those\navailable in OpenAI and Azure OpenAI, and open-source models\nconsumable from the Hugging Face Hub.\nMemory: It allows the application to keep references to the user’s\ninteractions, both in the short and long term. Within the framework of\nSemantic Kernel, memories can be accessed in three ways:\nKey-value pairs: This consists of saving environment variables that\nstore simple information, such as names or dates.\nLocal storage: This consists of saving information to a file that can be\nretrieved by its filename, such as a CSV or JSON file.\nSemantic memory search: This is similar to LangChain’s and\nHaystack’s memory, as it uses embeddings to represent and search for\ntext information based on its meaning.\n",
      "content_length": 935,
      "extraction_method": "Direct"
    },
    {
      "page_number": 91,
      "chapter": null,
      "content": "Functions: Functions can be seen as skills that mix LLM prompts and code,\nwith the goal of making users’ asks interpretable and actionable. There are\ntwo types of functions:\nSemantic functions: These are a type of templated prompt, which is a\nnatural language query that specifies the input and output format for\nthe LLM, also incorporating prompt configuration, which sets the\nparameters for the LLM.\nNative functions: These refer to the native computer code that can\nroute the intent captured by the semantic function and perform the\nrelated task.\nTo make an example, a semantic function could ask the LLM to write a\nshort paragraph about AI, while a native function could actually post it on\nsocial media like LinkedIn.\nPlug-ins: These are connectors toward external sources or systems that are\nmeant to provide additional information or the ability to perform\nautonomous actions. Semantic Kernel offers out-of-the-box plug-ins, such as\nthe Microsoft Graph connector kit, but you can build a custom plug-in by\nleveraging functions (both native and semantic, or a mix of the two).\nPlanner: As LLMs can be seen as reasoning engines, they can also be\nleveraged to auto-create chains or pipelines to address new users’ needs.\nThis goal is achieved with a planner, which is a function that takes as input a\nuser’s task and produces the set of actions, plug-ins, and functions needed to\nachieve the goal.\nSome benefits of Semantic Kernel are:\nLightweight and C# support: Semantic Kernel is more lightweight and\nincludes C# support. It’s a great choice for C# developers or those using the\n.NET framework.\nWide range of use cases: Semantic Kernel is versatile, supporting various\nLLM-related tasks.\n",
      "content_length": 1695,
      "extraction_method": "Direct"
    },
    {
      "page_number": 92,
      "chapter": null,
      "content": "Industry-led: Semantic Kernel was developed by Microsoft, and it is the\nframework the company used to build its own copilots. Hence, it is mainly\ndriven by industry needs and asks, making it a solid tool for enterprise-scale\napplications.\nHow to choose a framework\nOverall, the three frameworks offer, more or less, similar core components,\nsometimes called by a different taxonomy, yet covering all the blocks illustrated\nwithin the concept of the copilot system. So, a natural question might be: “Which\none should I use to build my LLM-powered application?” Well, there is no right\nor wrong answer! All three are extremely valid. However, there are some features\nthat might be more relevant for specific use cases or developers’ preferences. The\nfollowing are some criteria you might want to consider:\nThe programming language you are comfortable with or prefer to use:\nDifferent frameworks may support different programming languages or have\ndifferent levels of compatibility or integration with them. For example,\nSemantic Kernel supports C#, Python, and Java, while LangChain and\nHaystack are mainly based on Python (even though LangChain also\nintroduced JS/TS support). You may want to choose a framework that\nmatches your existing skills or preferences, or that allows you to use the\nlanguage that is most suitable for your application domain or environment.\nThe type and complexity of the natural language tasks you want to\nperform or support: Different frameworks may have different capabilities\nor features for handling various natural language tasks, such as\nsummarization, generation, translation, reasoning, etc. For example,\nLangChain and Haystack provide utilities and components for orchestrating\nand executing natural language tasks, while Semantic Kernel allows you to\nuse natural language semantic functions to invoke LLMs and services. You\n",
      "content_length": 1860,
      "extraction_method": "Direct"
    },
    {
      "page_number": 93,
      "chapter": null,
      "content": "may want to choose a framework that offers the functionality and flexibility\nyou need or want for your application goals or scenarios.\nThe level of customization and control you need or want over the LLMs\nand their parameters or options: Different frameworks may have different\nways of accessing, configuring, and fine-tuning the LLMs and their\nparameters or options, such as model selection, prompt design, inference\nspeed, output format, etc. For example, Semantic Kernel provides\nconnectors that make it easy to add memories and models to your AI app,\nwhile LangChain and Haystack allow you to plug in different components\nfor the document store, retriever, reader, generator, summarizer, and\nevaluator. You may want to choose a framework that gives you the level of\ncustomization and control you need or want over the LLMs and their\nparameters or options.\nThe availability and quality of the documentation, tutorials, examples,\nand community support for the framework: Different frameworks may\nhave different levels of documentation, tutorials, examples, and community\nsupport that can help you learn, use, and troubleshoot the framework. For\nexample, Semantic Kernel has a website with documentation, tutorials,\nexamples, and a Discord community; LangChain has a GitHub repository\nwith documentation, examples, and issues; Haystack has a website with\ndocumentation, tutorials, demos, blog posts, and a Slack community. You\nmay want to choose a framework that has the availability and quality of\ndocumentation, tutorials, examples, and community support that can help\nyou get started and solve problems with the framework.\nLet’s briefly summarize the differences between these orchestrators:\nFeature\nLangChain\nHaystack\nSemantic\nKernel\n",
      "content_length": 1739,
      "extraction_method": "Direct"
    },
    {
      "page_number": 94,
      "chapter": null,
      "content": "LLM support\nProprietary and\nopen-source\nProprietary and\nopen source\nProprietary and\nopen source\nSupported\nlanguages\nPython and JS/TS\nPython\nC#, Java, and\nPython\nProcess\norchestration\nChains\nPipelines of\nnodes\nPipelines of\nfunctions\nDeployment\nNo REST API\nREST API\nNo REST API\nFeature\nLangChain\nHaystack\nSemantic\nKernel\nTable 2.1: Comparisons among the three AI orchestrators\nOverall, all three frameworks offer a wide range of tools and integrations to build\nyour LLM-powered applications, and a wise approach could be to use the one that\nis most in line with your current skills or the company’s overall approach.\nSummary\nIn this chapter, we delved into the new way of developing applications that LLMs\nhave been paving, as we introduced the concept of the copilot and discussed the\nemergence of new AI orchestrators. Among those, we focused on three projects –\nLangChain, Haystack, and Semantic Kernel – and we examined their features,\nmain components, and some criteria to decide which one to pick.\nOnce we have decided on the AI orchestrator, another pivotal step is to decide\nwhich LLM(s) we want to embed into our applications. In Chapter 3, Choosing\nan LLM for Your Application, we are going to see the most prominent LLMs on\n",
      "content_length": 1233,
      "extraction_method": "Direct"
    },
    {
      "page_number": 95,
      "chapter": null,
      "content": "the market today – both proprietary and open-source – and understand some\ndecision criteria to pick the proper models with respect to the application use\ncases.\nReferences\nLangChain repository: https://github.com/langchain-\nai/langchain\nSemantic Kernel documentation:\nhttps://learn.microsoft.com/en-us/semantic-\nkernel/get-started/supported-languages\nCopilot stack: https://build.microsoft.com/en-\nUS/sessions/bb8f9d99-0c47-404f-8212-a85fffd3a59d?\nsource=/speakers/ef864919-5fd1-4215-b611-\n61035a19db6b\nThe Copilot system: https://www.youtube.com/watch?\nv=E5g20qmeKpg\nJoin our community on Discord\nJoin our community’s Discord space for discussions with the author and other\nreaders:\nhttps://packt.link/llm\n",
      "content_length": 707,
      "extraction_method": "Direct"
    },
    {
      "page_number": 96,
      "chapter": null,
      "content": "OceanofPDF.com\n",
      "content_length": 15,
      "extraction_method": "Direct"
    },
    {
      "page_number": 97,
      "chapter": null,
      "content": "3\nChoosing an LLM for Your\nApplication\nIn the last chapter, we saw how pivotal it is to properly orchestrate large\nlanguage models (LLMs) and their components within applications. In fact,\nwe saw that not all LLMs are created equal. The next key decision is which\nLLMs to actually use. Different LLMs may have different architectures,\nsizes, training data, capabilities, and limitations. Choosing the right LLM for\nyour application is not a trivial decision, as it can have a significant impact\non the performance, quality, and cost of your solution.\nIn this chapter, we will guide you through the process of choosing the right\nLLM for your application. We will cover the following topics:\nAn overview of the most promising LLMs in the market\nThe main criteria and tools to use when comparing LLMs\nTrade-offs between size and performance\nBy the end of this chapter, you should have a clear understanding of how to\nchoose the right LLM for your application and how to use it effectively and\nresponsibly.\n",
      "content_length": 1003,
      "extraction_method": "Direct"
    },
    {
      "page_number": 98,
      "chapter": null,
      "content": "The most promising LLMs in\nthe market\nThe last year has witnessed an unprecedented surge in the research and\ndevelopment of LLMs. Several new models have been released or\nannounced by different organizations, each with its own features and\ncapabilities. Some of these models are the largest and most advanced ever\ncreated, surpassing the previous state-of-the-art (SOTA) by orders of\nmagnitude. Others are lighter yet more specialized in specific tasks.\nIn this chapter, we will review some of the most promising LLMs in the\nmarket as of 2024. We will introduce their background, key findings, and\nmain techniques. We will also compare their performance, strengths, and\nlimitations on various benchmarks and tasks. We will also discuss their\npotential applications, challenges, and implications for the future of AI and\nsociety.\nProprietary models\nProprietary LLMs are developed and owned by private companies, and they\nare not disclosed with code. They are also typically subject to a fee for\nconsumption.\nProprietary models offer a series of advantages, including better support and\nmaintenance as well as safety and alignment. They also tend to outperform\nopen-source models in terms of generalization, because of their complexity\nand training datasets. On the other hand, they act as a “black box,” meaning\nthat owners do not disclose the source code to developers.\nIn the next sections, we will cover three of the most popular proprietary\nLLMs in the market, as of August 2023.\n",
      "content_length": 1483,
      "extraction_method": "Direct"
    },
    {
      "page_number": 99,
      "chapter": null,
      "content": "GPT-4\nReleased in March 2023, GPT-4 is, together with its newly released “cousin”\nGPT-4 Turbo, one of the latest models developed by OpenAI, is among the\ntop performers in the market at the time of writing this book (while OpenAI,\nas confirmed by its CEO Sam Altman, is already working on GPT-5).\nIt belongs to the class of generative pretrained transformer (GPT)\nmodels, a decoder-only transformer-based architecture introduced by\nOpenAI. The following diagram shows the basic architecture:\n",
      "content_length": 492,
      "extraction_method": "Direct"
    },
    {
      "page_number": 100,
      "chapter": null,
      "content": "Figure 3.1: High-level architecture of a decoder-only transformer\nAs you can see from the preceding diagram, the decoder-only architecture\nstill includes the main elements that feature in transformer architecture that\nwe covered in Chapter 1, Positional Embeddings, Multi-Head Attention, and\nFeed Forward layers. However, in this architecture, the model solely\ncomprises a decoder, which is trained to predict the next token in a sequence\n",
      "content_length": 439,
      "extraction_method": "Direct"
    },
    {
      "page_number": 101,
      "chapter": null,
      "content": "based on the preceding tokens. Unlike the encoder-decoder architecture, the\ndecoder-only design lacks an explicit encoder for summarizing input\ninformation. Instead, the information is implicitly encoded within the hidden\nstate of the decoder, which is updated at each step during the generation\nprocess.\nNow, we’ll look at some of the improvements in GPT-4 over previous\nversions.\nGPT-4, like the previous models in the GPT series, has been trained on both\npublicly available and OpenAI-licensed datasets (OpenAI didn’t disclose the\nexact composition of the training set).\nAdditionally, to make the model more aligned with the user’s intent, the\ntraining process also involved reinforcement learning from human\nfeedback (RLHF) training.\nDefinition\nRLHF is a technique that aims at using human feedback as\nan evaluating metric for LLMs’ generated output and then\nusing that feedback to further optimize the model. There are\ntwo main steps to achieve that goal:\n1. Training a reward model based on human preferences.\n2. Optimizing the LLM with respect to the reward model.\nThis step is done via reinforcement learning and it is a\ntype of machine learning paradigm where an agent\nlearns to make decisions by interacting with an\nenvironment. The agent receives feedback in the form\nof rewards or penalties based on its actions, and its goal\nis to maximize the cumulative reward over time by\n",
      "content_length": 1388,
      "extraction_method": "Direct"
    },
    {
      "page_number": 102,
      "chapter": null,
      "content": "continuously adapting its behavior through trial and\nerror.\nWith RLHF, thanks to the reward model, the LLM is able to\nlearn from human preferences and be more aligned with\nusers’ intents.\nAs an example, think about ChatGPT. This model integrates\nvarious training methods, including unsupervised\npretraining, supervised fine-tuning, instruction tuning, and\nRLHF. The RLHF component involves training the model to\npredict human preferences by using feedback from human\ntrainers. These trainers review the model’s responses and\nprovide ratings or corrections, guiding the model to generate\nmore helpful, accurate, and aligned responses.\nFor instance, if a language model initially produces an output\nthat is not quite helpful or accurate, human trainers can\nprovide feedback that indicates the preferred output. The\nmodel then uses this feedback to adjust its parameters and\nimprove future responses. This process iteratively continues,\nwith the model learning from a series of human judgments to\nbetter align with what is considered helpful or appropriate by\nhuman standards.\nGPT-4 demonstrated outstanding capabilities in commonsense reasoning and\nanalytical skills. It has been benchmarked with SOTA systems, including the\nMassive Multitask Language Understanding (MMLU) we covered in\nChapter 1. On MMLU, GPT-4 outperformed previous models not only in\nEnglish, but also in other languages.\n",
      "content_length": 1390,
      "extraction_method": "Direct"
    },
    {
      "page_number": 103,
      "chapter": null,
      "content": "The following is an illustration that shows GPT-4’s performance on MMLU:\nFigure 3.2: GPT-4 3-shot accuracy on MMLU across languages (source:\nhttps://openai.com/research/gpt-4)\nIn addition to MMLU, GPT-4 has been benchmarked on a variety of SOTA\nsystems and academic exams, as you can see from the following graph:\n",
      "content_length": 314,
      "extraction_method": "Direct"
    },
    {
      "page_number": 104,
      "chapter": null,
      "content": "Figure 3.3: GPT performance on academic and professional exams (source:\nhttps://arxiv.org/pdf/2303.08774.pdf)\nNote: in the preceding graph, you can see two versions of\nGPT-4, vision and no vision (along with the GPT-3.5 for\nbenchmarking purposes). This is because GPT-4 is a multi-\nmodal model, meaning that it can take images as input, in\naddition to text. However, in this chapter, we will benchmark\nonly its textual capabilities.\n",
      "content_length": 433,
      "extraction_method": "Direct"
    },
    {
      "page_number": 105,
      "chapter": null,
      "content": "Another great improvement of GPT-4 with respect to its predecessors (GPT-\n3.5 and GPT-3) is its noticeable reduction in the risk of hallucination.\nDefinition\nHallucination is a term that describes a phenomenon where\nLLMs generate text that is incorrect, nonsensical, or not real,\nbut appears to be plausible or coherent. For example, an\nLLM may hallucinate a fact that contradicts the source or\ncommon knowledge, a name that does not exist, or a\nsentence that does not make sense.\nHallucination can happen because LLMs are not databases or\nsearch engines that store or retrieve factual information.\nRather, they are statistical models that learn from massive\namounts of text data and produce outputs based on the\npatterns and probabilities they have learned. However, these\npatterns and probabilities may not reflect the truth or the\nreality, as the data may be incomplete, noisy, or biased.\nMoreover, LLMs have limited contextual understanding and\nmemory, as they can only process a certain number of tokens\nat a time and abstract them into latent representations.\nTherefore, LLMs may generate text that is not supported by\nany data or logic but is the most likely or correlated from the\nprompt.\nIn fact, even though it is still not 100% reliable, GPT-4 made great\nimprovements with TruthfulQA benchmarks, which test the model’s ability\nto separate fact from incorrect statements (we covered TruthfulQA\nbenchmarks in Chapter 1, in the Model evaluation section).\n",
      "content_length": 1463,
      "extraction_method": "Direct"
    },
    {
      "page_number": 106,
      "chapter": null,
      "content": "Here, you can see an illustration that compares GPT-4 results in a\nTruthfulQA benchmark with those of GPT-3.5 (the model behind OpenAI’s\nChatGPT) and Anthropic-LM (we will cover this latter model in the next\nsections).\nFigure 3.4: Model comparison in TruthfulQA benchmark (source:\nhttps://openai.com/research/gpt-4)\nFinally, with GPT-4, OpenAI made an additional effort to make it safer and\nmore aligned, engaging from the beginning a team of over 50 experts in\ndomains like AI alignment risks, privacy, and cybersecurity, with the goal of\nunderstanding the extent of the risks of such a powerful model and how to\nprevent them.\nDefinition\nAlignment is a term that describes the degree to which LLMs\nbehave in ways that are useful and harmless for their human\n",
      "content_length": 759,
      "extraction_method": "Direct"
    },
    {
      "page_number": 107,
      "chapter": null,
      "content": "users. For example, an LLM may be aligned if it generates\ntext that is accurate, relevant, coherent, and respectful. An\nLLM may be misaligned if it generates text that is false,\nmisleading, harmful, or offensive.\nThanks to this analysis, further data have been collected and used while\ntraining GPT-4 to mitigate its potential risks, resulting in a reduced risk\ncompared to its predecessor, GPT-3.5.\nGemini 1.5\nGemini 1.5 is a SOTA generative AI model developed by Google and\nreleased in December 2023. Like GPT-4, Gemini is designed to be\nmultimodal, meaning that it can process and generate content across various\nmodalities, including text, images, audio, video, and code. It is based on a\nmixture-of-expert (MoE) transformer.\nDefinition\nIn the context of transformer architecture, MoE refers to a\nmodel that incorporates multiple specialized sub-models,\nknown as “experts,” within its layers. Each expert is a neural\nnetwork designed to handle different types of data or tasks\nmore efficiently. The MoE model uses a gating mechanism\nor router to determine which expert should process a given\ninput, allowing the model to dynamically allocate resources\nand specialize in processing certain types of information.\nThis approach can lead to more efficient training and\ninference, as it enables the model to scale up in size and\n",
      "content_length": 1328,
      "extraction_method": "Direct"
    },
    {
      "page_number": 108,
      "chapter": null,
      "content": "complexity without a proportional increase in computational\ncost.\nGemini comes in various sizes, including Ultra, Pro, and Nano, to cater to\ndifferent computational needs, from data centers to mobile devices. To use\nGemini, developers can access it via the APIs provided for different model\nvariants, allowing the integration of its capabilities into applications.\nCompared to its previous version, Gemini 1.0, the current model\noutperforms it in text, vision, and audio tasks, as shown in the following\nscreenshot:\nFigure 3.5: Gemini 1.5 Pro and Ultra compared to its previous version 1.0 (source:\nhttps://storage.googleapis.com/deepmind-\nmedia/gemini/gemini_v1_5_report.pdf )\nSimilarly, it has demonstrated outstanding capabilities in domains such as\nmath, science, and reasoning, and coding and multilinguality:\n",
      "content_length": 815,
      "extraction_method": "Direct"
    },
    {
      "page_number": 109,
      "chapter": null,
      "content": "Figure 3.6: Gemini 1.5 Pro compared to Gemini 1.0 Pro and Ultra on different benchmarks (source:\nhttps://storage.googleapis.com/deepmind-\nmedia/gemini/gemini_v1_5_report.pdf)\n",
      "content_length": 175,
      "extraction_method": "Direct"
    },
    {
      "page_number": 110,
      "chapter": null,
      "content": "Note that Gemini 1.5 Pro is outperforming Gemini 1.0 Ultra (which is\nremarkably bigger) in many benchmarks across the various domains. As of\ntoday, Gemini Pro can be tried via a web app at gemini.google.com for\nfree, while Gemini Ultra is available via a premium subscription with a\nmonthly fee. On the other hand, Gemini Nano, which is tailored for mobile\ndevices, can be executed on capable Android devices via the Google AI\nEdge SDK for Android. Note that, as of April 2024, this SDK is still under\nearly access preview and you can apply for the early access program at\nhttps://docs.google.com/forms/d/e/1FAIpQLSdDvg0eEzc\nUY_-CmtiMZLd68KD3F0usCnRzKKzWb4sAYwhFJg/viewform.\nFinally, Gemini Pro and Ultra can also be consumed by developers via the\nREST API from Google AI Studio.\nClaude 2\nClaude 2, which stands for Constitutional Large-scale Alignment via User\nData and Expertise, is an LLM developed by Anthropic, a research company\nfounded by former OpenAI researchers and focused on AI safety and\nalignment. It was announced in July 2023.\nClaude 2 is a transformer-based LLM that has been trained on a mix of\npublicly available information from the internet and proprietary data, via\nunsupervised learning, RLHF, and constitutional AI (CAI).\nCAI is a real peculiarity of Claude. In fact, Anthropic paid extraordinary\nattention to Claude 2 alignment with safety principles. More specifically,\nAnthropic developed this unique technique called CAI, which was disclosed\nin December 2022 in the paper Constitutional AI: Harmlessness from AI\nFeedback.\n",
      "content_length": 1550,
      "extraction_method": "Direct"
    },
    {
      "page_number": 111,
      "chapter": null,
      "content": "CAI aims to make the model safer and more aligned with human values and\nintentions by preventing toxic or discriminatory output, not helping a human\nengage in illegal or unethical activities, and broadly creating an AI system\nthat is helpful, honest, and harmless. To achieve this, it uses a set of\nprinciples to guide the model’s behavior and outputs, rather than relying on\nhuman feedback or data alone. The principles are derived from various\nsources, such as the UN Declaration of Human Rights, trust and safety best\npractices, principles proposed by other AI research labs, non-Western\nperspectives, and empirical research.\nCAI uses these principles in two stages of the training process:\nFirst, the model is trained to critique and revise its own responses using\nthe principles and a few examples.\nSecond, the model is trained via reinforcement learning, but rather than\nusing human feedback, it uses AI-generated feedback based on the\nprinciples to choose the more harmless output.\nThe following illustration shows the training process according to the CAI\ntechnique:\n",
      "content_length": 1075,
      "extraction_method": "Direct"
    },
    {
      "page_number": 112,
      "chapter": null,
      "content": "Figure 3.7: Claude’s training process according to the CAI technique (source:\nhttps://arxiv.org/abs/2212.08073)\nAnother peculiarity of Claude 2 is the context length, which has a limit of\n100,000 tokens. This means that users can input longer prompts, namely\npages of technical documentation or even a book, which do not need to be\nembedded. Plus, the model can also generate longer output compared to\nother LLMs.\nFinally, Claude 2 demonstrates relevant capabilities also when working with\ncode, scoring 71.2% on the HumanEval benchmark.\nDefinition\nHumanEval is a benchmark for evaluating the code\ngeneration ability of LLMs. It consists of 164 human-crafted\ncoding problems in Python, each with a prompt, a solution,\nand a test suite. The problems cover various topics, such as\ndata structures, algorithms, logic, math, and string\nmanipulation. The benchmark can be used to measure the\nfunctional correctness, syntactic validity, and semantic\ncoherence of the LLM’s outputs.\nOverall, Claude 2 is a very interesting model and competitor of GPT-4 to\npay attention to. It can be consumed via the REST API or directly via the\nAnthropic beta chat experience (limited for US and UK users as of August\n2023).\nThe following comparison table shows the main differences between the\nthree models:\n",
      "content_length": 1287,
      "extraction_method": "Direct"
    },
    {
      "page_number": 113,
      "chapter": null,
      "content": "GPT-4\nGemini\nClaude 2\nCompany or\ninstitution\nOpenAI\nGoogle\nAnthropic\nFirst release\nMarch 2023\nDecember 2023\nJuly 2023\nArchitecture\nTransformer-\nbased, decoder\nonly\nTransformer-\nbased\nTransformer-\nbased\nSizes and\nvariants\nParameters not\nofficially\nspecified\nTwo context-\nlength variants:\nGPT-4 8K tokens\nGPT-4 32K\ntokens\nThree sizes,\nfrom smallest\nto largest:\nNano, Pro, and\nUltra\nNot officially\nspecified\nHow to use\nREST API at\nOpenAI\ndeveloper\nplatforms\nUsing OpenAI\nPlayground at\nhttps://plat\nREST API at\nGoogle AI\nStudio\nUsing Gemini\nat\nhttps://ge\nREST API after\ncompiling the\nform at\nhttps://www.\nanthropic.co\nm/claude\n",
      "content_length": 623,
      "extraction_method": "Direct"
    },
    {
      "page_number": 114,
      "chapter": null,
      "content": "form.openai.\ncom/playgrou\nnd\nmini.googl\ne.com/\nTable 3.1: Comparison table of GPT-4, PaLM 2, and Claude 2\nIn addition to proprietary models, there is a huge market for open-source\nLLMs available today. Let’s discuss some of these in the next section.\nOpen-source models\nThe advantage of an open-source model is that, by definition, developers\nhave full visibility and access to the source code. In the context of LLMs,\nthis implies the following:\nYou have major control over the architecture, meaning that you can also\nmodify it in the local version you are going to use within your project.\nThis also implies that they are not prone to potential updates to the\nsource code made by models’ owners.\nThere is the possibility to train your model from scratch, on top of the\nclassical fine-tuning, which is also available for proprietary models.\nFree to use, meaning that you won’t incur any charge while using those\nLLMs, in contrast with the proprietary ones that have pay-per-use\npricing.\nTo compare open-source models, throughout this book, we will refer to the\nindependent Hugging Face Open LLM Leaderboard (you can find it at\nhttps://huggingface.co/spaces/HuggingFaceH4/open_ll\nm_leaderboard), a project that aims to evaluate and compare the\nperformance of LLMs on various natural language understanding (NLU)\n",
      "content_length": 1312,
      "extraction_method": "Direct"
    },
    {
      "page_number": 115,
      "chapter": null,
      "content": "tasks. The project is hosted on Hugging Face Spaces, a platform for creating\nand sharing machine-learning applications.\nThe Open LLM Leaderboard uses four main evaluation benchmarks, which\nwe covered in Chapter 1, in the Model evaluation section:\nAI2 Reasoning Challenge (ARC): Grade-school science questions and\ncomplex NLU tasks.\nHellaSwag: Common sense reasoning.\nMMLU: Tasks in various domains, including math, computer science,\nand law.\nTruthfulQA: An evaluation of how truthful the model is when\ngenerating answers.\nEven though those are just a subsample of the plethora of LLMs’\nbenchmarks, we will stick to this leaderboard as a reference evaluation\nframework as it being widely adopted.\nLLaMA-2\nLarge Language Model Meta AI 2 (LLaMA-2) is a new family of models\ndeveloped by Meta and unveiled to the public on July 18, 2023, open source\nand for free (its first version was originally limited to researchers).\nIt is an autoregressive model with an optimized, decoder-only transformer\narchitecture.\nDefinition\nThe concept of autoregressive in the context of transformers\nrefers to the fact that the model predicts the next token in the\nsequence, conditioned on all the previous tokens. This is\n",
      "content_length": 1201,
      "extraction_method": "Direct"
    },
    {
      "page_number": 116,
      "chapter": null,
      "content": "done by masking the future tokens in the input so that the\nmodel can only attend to the past tokens. For example, if the\ninput sequence is “The sky is blue,” the model would predict\n“The” first, then “sky,” then “is,” and finally “blue,” using a\nmask to hide the tokens that come after each prediction.\nLLaMA-2 models come in three sizes: 7, 13, and 70 billion parameters. All\nthe versions have been trained on 2 trillion tokens and have a context length\nof 4,092 tokens.\nOn top of that, all model sizes come with a “chat” version, called LLaMA-2-\nchat, which is more versatile for general-purpose conversational scenarios\ncompared to the base model LLama-2.\nNote\nIn the context of LLMs, the difference between base models\nand “chat” or assistant models is primarily in their training\nand intended use:\nBase models: These models are trained on vast amounts\nof text data, often sourced from the internet, and their\nprimary function is to predict the next word in a given\ncontext, which makes them great at understanding and\ngenerating language. However, they might not always\nbe precise or focused on specific instructions.\nAssistant models: These models start as base LLMs but\nare further fine-tuned with input-output pairs that\ninclude instructions and the model’s attempts to follow\nthose instructions. They often employ RLHF to refine\n",
      "content_length": 1338,
      "extraction_method": "Direct"
    },
    {
      "page_number": 117,
      "chapter": null,
      "content": "the model, making it better at being helpful, honest, and\nharmless. As a result, they are less likely to generate\nproblematic text and are more suitable for practical\napplications like chatbots and content generation. For\nexample, the assistant model GPT-3.5 Turbo (the model\nbehind ChatGPT) is a fine-tuned version of the\ncompletion model GPT-3.\nIn essence, while base models provide a broad understanding\nof language, assistant models are optimized to follow\ninstructions and provide more accurate and contextually\nrelevant responses.\nLLaMA-2-chat was developed with a fine-tuning process that consisted of\ntwo main steps:\n1. Supervised fine-tuning: This step involves fine-tuning the model on\npublicly available instruction datasets and over 1 million human\nannotations, to make them more helpful and safe for conversational use\ncases. The fine-tuning process uses a selected list of prompts to guide\nthe model outputs, and a loss function that encourages diversity and\nrelevance (that’s the reason why it is “supervised”).\n2. RLHF: As we saw while introducing GPT-4, RLHF is a technique that\naims at using human feedback as an evaluating metric for LLMs’\ngenerated output, and then using that feedback to further optimize the\nmodel.\nThe following is an illustration of how the training process for LLaMA\nworks:\n",
      "content_length": 1315,
      "extraction_method": "Direct"
    },
    {
      "page_number": 118,
      "chapter": null,
      "content": "Figure 3.8: Two-step fine-tuning to obtain LLaMa-2 chat (source:\nhttps://ai.meta.com/resources/models-and-libraries/llama/)\nTo access the model, you need to submit a request on Meta’s website (the\nform is available at https://ai.meta.com/resources/models-\nand-libraries/llama-downloads/). Once a request is submitted,\nyou will receive an email with the GitHub repository where you will be able\nto download the following assets:\nModel code\nModel weights\nREADME (User Guide)\nResponsible Use Guide\nLicense\nAcceptable Use Policy\nModel Card\nFalcon LLM\nFalcon LLM is a representation of a new trend of LLMs, consisting of\nbuilding lighter models (with fewer parameters) and focusing rather on the\n",
      "content_length": 691,
      "extraction_method": "Direct"
    },
    {
      "page_number": 119,
      "chapter": null,
      "content": "quality of the training dataset. Indeed, it is a matter of fact that complex\nmodels like GPT-4 with trillions of parameters are extremely heavy, both in\nthe training phase and inference phase. This implies the need for high and\nexpensive computational power (GPU and TPU-powered) as well as a long\ntraining time.\nFalcon LLM is an open-source model launched by Abu Dhabi’s Technology\nInnovation Institute (TII) in May 2023. It is an autoregressive, decoder-\nonly transformer, trained on 1 trillion tokens, and it has 40 billion parameters\n(even though it has also been released as a lighter version with 7 billion\nparameters). Similarly to what we saw for LlaMA, Falcon LLM also comes\nwith a fine-tuned variant, called “Instruct,” which is tailored toward\nfollowing the user’s instructions.\nDefinition\nInstruct models are specialized for short-form instruction\nfollowing. Instruction following is a task where the model\nhas to execute a natural language command or query, such as\n“write a haiku about cats” or “tell me about the weather in\nParis.” The Instruct fine-tuned models are trained on a large\ndataset of instructions and their corresponding outputs, such\nas the Stanford Alpaca dataset.\nAccording to the Open LLM leaderboard, since its launch, Falcon LLM has\nbeen among the first positions globally, second only to some versions of\nLlaMA.\nSo, the question might be: how can a model with “only” 40 billion\nparameters perform so well? In fact, the answer is in the quality of the\n",
      "content_length": 1486,
      "extraction_method": "Direct"
    },
    {
      "page_number": 120,
      "chapter": null,
      "content": "dataset. Falcon was developed using specialized tools and incorporates a\nunique data pipeline, which is capable of extracting valuable content from\nweb data. The pipeline was designed to extract high-quality content by\nemploying extensive filtering and deduplication techniques. The resulting\ndataset, called RefinedWeb, has been released by TII under the Apache-2.0\nlicense and can be found at\nhttps://huggingface.co/datasets/tiiuae/falcon-\nrefinedweb.\nBy combining superior data quality with these optimizations, Falcon\nachieves remarkable performance while utilizing around 75% and 80% of\nthe training compute budget of GPT-3 and PaLM-62B, respectively.\nMistral\nThe third and last open-source model series we are going to cover is Mistral,\ndeveloped by Mistral AI, a company founded in April 2023 by a team of AI\nscientists who previously worked at Meta Platforms and Google DeepMind.\nBased in France, the company has quickly made a name for itself by raising\nsignificant funding and releasing open-source LLMs, emphasizing the\nimportance of transparency and accessibility in AI development.\nThe Mistral model, particularly the Mistral-7B-v0.1, is a decoder-only\ntransformer with 7.3 billion parameters, designed for generative text tasks.\nIt’s known for its innovative architecture choices like grouped-query\nattention (GQA) and sliding-window attention (SWA), which have\nallowed it to outperform other models in benchmarks.\nDefinition\n",
      "content_length": 1440,
      "extraction_method": "Direct"
    },
    {
      "page_number": 121,
      "chapter": null,
      "content": "GQA and SWA are mechanisms designed to improve the\nefficiency and performance of an LLM.\nGQA is a technique that allows for faster inference times\ncompared to standard full attention mechanisms. It does this\nby partitioning the attention mechanism’s query heads into\ngroups, with each group sharing a single key head and value\nhead.\nSWA is used to handle longer text sequences efficiently. It\nextends the model’s attention beyond a fixed window size,\nallowing each layer to reference a range of positions from\nthe preceding layer. This means that the hidden state at a\ncertain position in one layer can attend to hidden states\nwithin a specific range in the previous layer, thus enabling\nthe model to access tokens at a greater distance and manage\nsequences of varying lengths with a reduced inference cost.\nThe model also provides a variant that was fine-tuned for general-purpose\ncapabilities. This variant is called Mistral-7B-instruct, which outperformed\nall other 7 billion LLMs on the market (as of April 2024) on MT-Bench (an\nevaluation framework that uses an LLM as a judge).\nLike many other open-source models, Mistral can be consumed and\ndownloaded via Hugging Face Hub.\nNote\nIn February 2024, Mistral AI and Microsoft entered a multi-\nyear partnership to accelerate AI innovation. This\ncollaboration will leverage Microsoft’s Azure AI\n",
      "content_length": 1346,
      "extraction_method": "Direct"
    },
    {
      "page_number": 122,
      "chapter": null,
      "content": "supercomputing infrastructure to support the development\nand deployment of Mistral AI’s LLMs. Mistral AI’s models,\nincluding their advanced model, Mistral Large, will be\navailable to customers through Azure AI Studio and Azure\nMachine Learning model catalog. The partnership aims to\nexpand Mistral AI’s reach to global markets and foster\nongoing research collaboration.\nThe following comparison table provides the main differences between the\nthree models:\nLlaMA\nFalcon LLM\nMistral\nCompany or\ninstitution\nMeta\nTechnology\nInnovation\nInstitute (TII)\nMistral AI\nFirst release\nJuly 2023\nMay 2023\nSeptember\n2023\nArchitecture\nAutoregressive\ntransformer,\ndecoder-only\nAutoregressive\ntransformer,\ndecoder-only\nTransformer,\ndecoder only\nSizes and\nvariants\nThree sizes: 7B,\n13B, and 70B,\nalongside the fine-\nTwo sizes: 7B\nand 40B,\nalongside the\nfine-tuned\n7B size\nalongside the\nfine-tuned\n",
      "content_length": 879,
      "extraction_method": "Direct"
    },
    {
      "page_number": 123,
      "chapter": null,
      "content": "tuned version\n(chat)\nversion\n(instruct)\nversion\n(instruct)\nLicenses\nA custom\ncommercial license\nis available at\nhttps://ai.me\nta.com/resour\nces/models-\nand-\nlibraries/lla\nma-downloads/\nCommercial\nApache 2.0\nlicensed\nCommercial\nApache 2.0\nlicensed\nHow to use\nSubmit request\nform at\nhttps://ai.me\nta.com/resour\nces/models-\nand-\nlibraries/lla\nma-downloads/\nand download\nthe GitHub\nrepo\nAlso available in\nHugging Face Hub\nDownload or\nuse Hugging\nFace Hub\nInference\nAPI/Endpoint\nDownload or\nuse Hugging\nFace Hub\nInference\nAPI/Endpoint\nor Azure AI\nStudio\n",
      "content_length": 549,
      "extraction_method": "Direct"
    },
    {
      "page_number": 124,
      "chapter": null,
      "content": "Table 3.2: Comparison table of LLMs\nBeyond language models\nSo far, we have only been covering language-specific foundation models as\nthey are the focus of this book. Nevertheless, in the context of AI-powered\napplications, it is worth mentioning that there are additional foundation\nmodels that can handle data that is different from text, which can be\nembedded and orchestrated.\nHere, you can find some examples of large foundation models (LFMs) on\nthe market today:\nWhisper: It is a general-purpose speech recognition model developed\nby OpenAI that can transcribe and translate speech in multiple\nlanguages. It is trained on a large dataset of diverse audio and is also a\nmultitasking model that can perform multilingual speech recognition,\nspeech translation, spoken language identification, and voice activity\ndetection.\nMidjourney: Developed by the independent research lab of the same\nname, Midjourney is based on a sequence-to-sequence transformer\nmodel that takes text prompts and outputs a set of four images that\nmatch the prompts. Midjourney is designed to be a tool for artists and\ncreative professionals, who can use it for rapid prototyping of artistic\nconcepts, inspiration, or experimentation.\nDALL-E: Similar to the previous one, DALL-E, developed by OpenAI,\ngenerates images from natural language descriptions, using a 12-billion\nparameter version of GPT-3 trained on a dataset of text-image pairs.\n",
      "content_length": 1417,
      "extraction_method": "Direct"
    },
    {
      "page_number": 125,
      "chapter": null,
      "content": "The idea is that we can combine and orchestrate multiple LFMs within our\napplications to achieve extraordinary results. For example, let’s say we want\nto write a review about an interview with a young chef and post it on\nInstagram. The involved models might be the following:\nWhisper will convert the interview audio into a transcript.\nAn LLM, such as Falcon-7B-instruct, with a web plugin, will\nextrapolate the name of the young chef and search it on the internet to\nretrieve the biography.\nAnother LLM, such as LlaMA, will process the transcript and generate\na review with an Instagram post style. We can also ask the same model\nto generate a prompt that will ask the following model to generate a\npicture based on the post content.\nDall-E will generate an image based on the prompt generated by the\nLLM.\nWe will then provide our LFMs flow with an Instagram plugin so that the\napplication is able to post the whole review, including the illustration, on our\nprofile.\nFinally, there are emerging LFMs that are meant to be multi-modal, meaning\nthat they can handle multiple data formats with just one architecture. An\nexample is GPT-4 itself.\nThe following screenshot shows an example of an early OpenAI experiment\nwith GPT-4 visuals, demonstrating its understanding of funny aspects within\nan image:\n",
      "content_length": 1301,
      "extraction_method": "Direct"
    },
    {
      "page_number": 126,
      "chapter": null,
      "content": "Figure 3.9: Early experiments with GPT-4 visuals (source:\nhttps://openai.com/research/gpt-4)\nThe following screenshot shows another example of an earlier version of\nGPT-4, demonstrating how it could understand and explain graphs in detail:\n",
      "content_length": 240,
      "extraction_method": "Direct"
    },
    {
      "page_number": 127,
      "chapter": null,
      "content": "Figure 3.10: Early experiments with GPT-4 visuals (source:\nhttps://openai.com/research/gpt-4)\n",
      "content_length": 94,
      "extraction_method": "Direct"
    },
    {
      "page_number": 128,
      "chapter": null,
      "content": "The following example shows how an early version of GPT-4 could\nunderstand and solve complex mathematical problems while also providing\nthe corresponding justification for its response:\n",
      "content_length": 186,
      "extraction_method": "Direct"
    },
    {
      "page_number": 129,
      "chapter": null,
      "content": "User Answer questionli.a. Think step-by-step.\n\nI. Principe de la détection de rayonnement avec un bolométre\n\nComme illustré sur la figure 1 un bolométre est constitué d'un absorbour qui recoit le rayon:\nnement que l'on désire dévecier. Sa température 7, supposée uniforme, est mesuréc 4 Vaide d'un\nthermométre incorporé, constitué d'un matériau conducteur dont la résistance A(T) varie avec la\ntempérature T; cctte variation est caractérisée par le cocfficient a = Rat Liensemble passéde\nla capacité thermique Cy.\n\nUn barreau, conducteur thermique, homogéne, de longueur £, de section S et de conducti-\nvilé thermique A et sans échanges thermiques latéraux, relie le bolométre & un thermostat de\n\ntempératiane Fh, five\n\nrayonnement incident puissance PF\n\\ absorbeur\na\n\nbarreau\nconducteur\n\ncourant de\npolarisation\n\nthermostat T;\n\nFigure 1 - Schéma d'un bolometre\n\n1.1. Dans cette premiére partie, on étudie le comportement du bolométre en régime station-\nnaire, 4 la température de fonctionnement T ~ Th.\n\nLi.a) Exprimer la lempéralure T(x) en lout point du barrenu conducleur repéré par son\n\nabseisse x: on prendra T(0) = Ty et TUL) = Ty.\n\nGPT-4 To answer question|‘.a, we need toexpress the temperature T(x) in every point of the\nconductive bar, which is characterized by the abscissa x. We are given that T(0)= TO\nand T(L) =Tb.\n\nFor a stationary regime, the heat equation for the one-dimensional heat conduction in\nthe bar is:\n\nd*T/dx*=0\n\nIntegrating once, we get:\n\nIntegrating a second time:\nT(x) = C,x+ C,\n\nTo determine the constants C, and C., we use the boundary conditions:\nT(0) = TO, which gives C,= TO\nT(L) = Tb, which gives C, =(Tb- TO)/L\n\nFinally, the expression for T(x) is:\n\nT(x) = (Tb- TO)* (x /L)+TO\n\nThis represents a linear temperature profile along the conductive bar, with a slope of (Tb\n- TO) /L.",
      "content_length": 1816,
      "extraction_method": "OCR"
    },
    {
      "page_number": 130,
      "chapter": null,
      "content": "Figure 3.11: Early experiments with GPT-4 visuals (source:\nhttps://openai.com/research/gpt-4)\nGPT-4 is just one example of a large multimodal model (LMM), and it is\nrepresentative of the trend that we will probably witness in the next few\nyears.\nA decision framework to pick\nthe right LLM\nIn previous paragraphs, we covered some of the most promising LLMs\navailable in the market today. Now, the question is: which one should I use\nwithin my applications? The truth is that there is not a straightforward\nanswer to this question.\nConsiderations\nThere are many factors to consider when choosing an LLM for your\napplication. Those factors also need to be declined in two scenarios:\nproprietary and open-source LLMs. The following are some factors and\ntrade-offs you might want to consider while choosing your LLMs:\nSize and performance: We saw that more complex models (that\nmeans, with a high number of parameters) tend to have better\nperformance, especially in terms of parametric knowledge and\ngeneralization capabilities. Nevertheless, the larger the model, the more\ncomputation and memory it requires to process the input and generate\nthe output, which can result in higher latency and, as we will see, higher\ncosts.\n",
      "content_length": 1220,
      "extraction_method": "Direct"
    },
    {
      "page_number": 131,
      "chapter": null,
      "content": "Cost and hosting strategy: When incorporating LLMs within our\napplications, there are two types of costs we have to keep in mind:\nCost for model consumption: This refers to the fee we pay to\nconsume the model. Proprietary models like GPT-4 or Claude 2\nrequire a fee, which is typically proportional to the number of\ntokens processed. On the other hand, open-source models like\nLlaMA or Falcon LLM are free to use.\nCost for model hosting: This refers to your hosting strategy.\nTypically, proprietary models are hosted in a private or public\nhyperscaler, so that they can be consumed via a REST API and\nyou don’t have to worry about the underlying infrastructure (for\nexample, GPT-4 is hosted in a super-computer built in the\nMicrosoft Azure cloud). With open-source models, we typically\nneed to provide our own infrastructure, since those models can be\ndownloaded locally. Of course, the larger the model, the more\npowerful the computational power needed.\nNote\nIn the context of open-source models, another\noption to consume those models is that of using\nthe Hugging Face Inference API. The free version\nallows you to test and evaluate, with a limited\nrate, all the available LLMs on a shared\ninfrastructure hosted on Hugging Face. For\nproduction use cases, Hugging Face also offers\nInference Endpoints, so that you can easily\ndeploy your LLMs on a dedicated and fully\nmanaged infrastructure, with the possibility to\n",
      "content_length": 1416,
      "extraction_method": "Direct"
    },
    {
      "page_number": 132,
      "chapter": null,
      "content": "configure parameters like region, compute power,\nand security level to accommodate your\nconstraints in terms of latency, throughput, and\ncompliance.\nPricing for the Inference Endpoint is publicly\navailable at\nhttps://huggingface.co/docs/infer\nence-endpoints/pricing.\nCustomization: This might be a requirement you want to evaluate\nbefore deciding which model to adopt. In fact, not all models are\nequally flexible in terms of customization. When we talk about\ncustomization, we refer to two activities:\nFine-tuning: This is the process of slightly adjusting LLMs’\nparameters to better fit into a domain. All open-source models\ncan be fine-tuned. When it comes to proprietary models, not all\nLLMs can be fine-tuned: for example, OpenAI’s GPT-3.5 can be\nfine-tuned, while the process of fine-tuning the GPT-4-0613 is\nstill experimental and accessible under request to OpenAI (as per\nDecember 2023).\nHenceforth, it is important to understand whether you will need fine-\ntuning in your application and decide accordingly.\nTraining from scratch: If you really want an LLM that is super\nspecific about your domain knowledge, you might want to retrain\nthe model from scratch. To train an LLM from scratch, without\nhaving to reinvent an architecture, you can download open-\nsource LLMs and simply re-train them on custom datasets. Of\n",
      "content_length": 1326,
      "extraction_method": "Direct"
    },
    {
      "page_number": 133,
      "chapter": null,
      "content": "course, this implies that we have access to the source code, which\nis not the case when we work with proprietary LLMs.\nDomain-specific capabilities: We saw that the most popular way of\nevaluating LLMs’ performance is that of averaging different\nbenchmarks across domains. However, there are benchmarks that are\ntailored towards specific capabilities: if MMLU measures LLMs’\ngeneralized culture and commonsense reasoning, TruthfulQA is more\nconcerned with LLMs’ alignment, while HumanEval is tailored towards\nLLMs’ coding capabilities.\nHenceforth, if you have a tailored use case in mind, you might want to\nuse a model that is a top performer in one specific benchmark, rather\nthan a top performer, on average, across all benchmarks. Namely, you\nmight pick Claude 2 if you are looking for exceptional coding\ncapabilities, or PaLM 2 if analytical reasoning is what you are looking\nfor. On the other hand, if you need a model that encompasses all of\nthese capabilities, GPT-4 might be the right choice for you.\nPicking a domain-specific model is also a way to make some savings\nin terms of model complexity. The thing is, it might be sufficient for\nyou to use a relatively small model (for example, a LlaMA-7B-\ninstruct) if you need to use it for a specific use case, which comes with\nall the benefits in terms of cost and performance.\nNote\nIf you are looking for LLMs that are extremely specific, there\nis a plethora of models that have been trained on domain-\nspecific technical documentation. For example, at the\nbeginning of 2023, the Stanford Center for Research on\n",
      "content_length": 1568,
      "extraction_method": "Direct"
    },
    {
      "page_number": 134,
      "chapter": null,
      "content": "Foundation Models (CRFM) and MosaicML announced\nthe release of BioMedLM, a decoder-only transformer-based\nLLM with 2.7 billion parameters, trained on biomedical\nabstracts and papers.\nAnother example is BloombergGPT, a 50 billion parameter\nLLM specialized for the financial domain developed by\nBloomberg and trained on a 363 billion token dataset based\non Bloomberg’s extensive data sources, perhaps the largest\ndomain-specific dataset yet, augmented with 345 billion\ntokens from general purpose datasets.\nTo make this decision framework more practical, let’s consider the following\nimaginary case study about the company TechGen.\nCase study\nTechGen Solutions, a leading provider of AI-driven analytics, face a decision\nbetween two advanced language models for their next-generation customer\ninteraction system: GPT-4 and LLaMa-2. They require a robust language\nmodel that can handle diverse customer queries, provide accurate technical\ninformation, and integrate with their proprietary software. The following are\ntheir options:\nGPT-4: Developed by OpenAI, GPT-4 is known for its vast parameter\ncount and the ability to process both text and image inputs\nLLama 2: Created by Meta AI, LLama 2 is an open-source model\npraised for its accessibility and performance on a smaller dataset.\nThe following are the factors that they consider when making their decision:\n",
      "content_length": 1361,
      "extraction_method": "Direct"
    },
    {
      "page_number": 135,
      "chapter": null,
      "content": "Performance: TechGen evaluates the models’ performance, particularly\nin generating technical content and code, where GPT-4 has shown\nhigher accuracy.\nIntegration: The ease of integration with TechGen’s systems is critical,\nwith GPT-4 potentially offering more seamless compatibility due to its\nwidespread adoption.\nCost: While LLama 2 is free for commercial use under certain\nconditions, GPT-4 comes with a cost, which TechGen must factor into\ntheir decision.\nFuture-proofing: TechGen considers the long-term viability of each\nmodel, including the potential for updates and improvements.\nBased on these considerations, TechGen opts for GPT-4, swayed by its\nsuperior performance in generating complex, technical responses and its\nmultilingual capabilities, which align with their international expansion\nplans. The decision is also influenced by GPT-4’s image processing feature,\nwhich TechGen anticipates will become increasingly relevant as they\nincorporate more multimedia content into their customer service.\nTechGen’s choice of GPT-4 over LLama 2 is driven by the need for a high-\nperforming, versatile language model that can scale with their growing\nglobal presence and diverse customer needs. While LLama 2’s open-source\nnature and cost effectiveness are appealing, GPT-4’s advanced capabilities\nand future-proof features present a more compelling case for TechGen’s\nambitious goals.\nNote that these decision factors are not meant to be an exhaustive guide to\ndeciding which models to embed within applications. Nevertheless, those are\nuseful elements of reflection while setting up your application flow, so that\n",
      "content_length": 1621,
      "extraction_method": "Direct"
    },
    {
      "page_number": 136,
      "chapter": null,
      "content": "you can determine your requirements and then shortlist those LLMs that are\nmore suitable for your goals.\nSummary\nThis chapter covered some of the most promising LLMs in the market. It\nfirst differentiated between proprietary and open-source models, with all the\nrelated pros and cons. It then offered a deep dive into the architecture and\ntechnical features of GPT-4, PaLM-2, Claude 2, LLaMa-2, Falcon LLM, and\nMPT, with the addition of a section covering some LMMs. Finally, it\nprovided a light framework to help developers decide which LLMs to pick\nwhile building AI-powered applications. This is pivotal to get the greatest\nimpact from your application, given your industry-specific scenario.\nStarting from the next chapter, we will start working hands-on with LLMs\nwithin applications.\nReferences\nGPT-4 Technical Report.\nhttps://cdn.openai.com/papers/gpt-4.pdf\nTrain short, test long: attention with linear biases enables input length\nextrapolation. https://arxiv.org/pdf/2108.12409.pdf\nConstitutional AI: Harmlessness from AI Feedback.\nhttps://arxiv.org/abs/2212.08073\nHugging Face Inference Endpoint.\nhttps://huggingface.co/docs/inference-\nendpoints/index\n",
      "content_length": 1162,
      "extraction_method": "Direct"
    },
    {
      "page_number": 137,
      "chapter": null,
      "content": "Hugging Face Inference Endpoint Pricing.\nhttps://huggingface.co/docs/inference-\nendpoints/pricing\nModel Card for BioMedLM 2.7B.\nhttps://huggingface.co/stanford-crfm/BioMedLM\nPaLM 2 Technical Report.\nhttps://ai.google/static/documents/palm2techrepo\nrt.pdf\nSolving Quantitative Reasoning Problems with Language Models.\nhttps://arxiv.org/abs/2206.14858\nJudging LLM-as-a-Judge with MT-Bench and Chatbot Arena.\nhttps://arxiv.org/abs/2306.05685\nJoin our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\nOceanofPDF.com\n",
      "content_length": 593,
      "extraction_method": "Direct"
    },
    {
      "page_number": 138,
      "chapter": null,
      "content": "4\nPrompt Engineering\nIn Chapter 2, we introduced the concept of prompt engineering as the\nprocess of designing and optimizing prompts – the text input that guides the\nbehavior of a large language model (LLM) – for LLMs for a wide variety\nof applications and research topics. Since prompts have a massive impact on\nLLM performance, prompt engineering is a crucial activity while designing\nLLM-powered applications. In fact, there are several techniques that can be\nimplemented not only to refine your LLM’s responses but also to reduce\nrisks associated with hallucination and bias.\nIn this chapter, we are going to cover the emerging techniques in the field of\nprompt engineering, starting from basic approaches up to advanced\nframeworks. By the end of this chapter, you will have the foundations to\nbuild functional and solid prompts for your LLM-powered applications,\nwhich will also be relevant in the upcoming chapters.\nWe will go through the following topics:\nIntroduction to prompt engineering\nBasic principles of prompt engineering\nAdvanced techniques of prompt engineering\nTechnical requirements\n",
      "content_length": 1103,
      "extraction_method": "Direct"
    },
    {
      "page_number": 139,
      "chapter": null,
      "content": "To complete the tasks in this chapter, you will require the following:\nOpenAI account and API\nPython 3.7.1 or later version\nYou can find all the code and examples in the book’s GitHub repository at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications.\nWhat is prompt engineering?\nA prompt is a text input that guides the behavior of an LLM to generate a\ntext output.\nPrompt engineering is the process of designing effective prompts that elicit\nhigh-quality and relevant output from LLMs. Prompt engineering requires\ncreativity, understanding of the LLM, and precision.\nThe following figure shows an example of how a well-written prompt can\ninstruct the same model to perform three different tasks:\nFigure 4.1: Example of prompt engineering to specialize LLMs\n",
      "content_length": 776,
      "extraction_method": "Direct"
    },
    {
      "page_number": 140,
      "chapter": null,
      "content": "As you might imagine, the prompt becomes one of the key elements for an\nLLM-powered application’s success. As such, it is pivotal to invest time and\nresources in this step, following some best practices and principles that we\nare going to cover in the next sections.\nPrinciples of prompt\nengineering\nGenerally speaking, there are no fixed rules to obtain the “perfect” prompt\nsince there are too many variables to be taken into account (the type of\nmodel used, the goal of the application, the supporting infrastructure, and so\non). Nevertheless, there are some clear principles that have proven to\nproduce positive effects if incorporated into the prompt. Let’s examine some\nof them.\nClear instructions\nThe principle of giving clear instructions is to provide the model with\nenough information and guidance to perform the task correctly and\nefficiently. Clear instructions should include the following elements:\nThe goal or objective of the task, such as “write a poem” or “summarize\nan article”\nThe format or structure of the expected output, such as “use four lines\nwith rhyming words” or “use bullet points with no more than 10 words\neach”\nThe constraints or limitations of the task, such as “do not use any\nprofanity” or “do not copy any text from the source”\n",
      "content_length": 1265,
      "extraction_method": "Direct"
    },
    {
      "page_number": 141,
      "chapter": null,
      "content": "The context or background of the task, such as “the poem is about\nautumn” or “the article is from a scientific journal”\nLet’s say, for example, that we want our model to fetch any kind of\ninstructions from text and return to us a tutorial in a bullet list. Also, if there\nare no instructions in the provided text, the model should inform us about\nthat. Here are the steps:\n1. First, we need to initialize our model. For this purpose, we are going to\nleverage OpenAI’s GPT-3.5-turbo model. We first install the openai\nlibrary:\n$pip install openai == 0.28\n2. To initialize the model, I used the openai Python library and set the\nOpenAI API key as the environmental variable:\nimport os\nimport openai\nopenai.api_key = os.environment.get('OPENAI_API_KEY')\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\", # engine = \"deployment_name\n    messages=[\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": instructions},\n    ]\n)\nAs you can see, the chat model comes with two variables\nplaceholders: system message (or metaprompt), where we define how\nwe want our model to behave, and instructions (or query), where the\nuser will ask the model its questions.\n",
      "content_length": 1205,
      "extraction_method": "Direct"
    },
    {
      "page_number": 142,
      "chapter": null,
      "content": "3. Then, it takes the user’s query (in this case, the text instructions). For\nthis scenario, I set the two variables system_message and instructions\nas follows:\nsystem_message = \"\"\"\nYou are an AI assistant that helps humans by generatin\nYou will be provided with a text. If the text contains\nOtherwise, inform the user that the text does not cont\nText:\n\"\"\"\ninstructions = \"\"\"\nTo prepare the known sauce from Genova, Italy, you can\nchop them in a kitchen mortar together with basil and \nFinally, transfer the pesto to a bowl and stir in the \n\"\"\"\n4. Now let’s test our model:\nprint(response['choices'][0]['message']['content'])\nWe get the following output:\nSure, I can generate a tutorial based on the given text. He\n- Toast the pine nuts on a pan until they are slightly brow\n- Coarsely chop the toasted pine nuts, along with basil lea\n- Add half of the olive oil into the kitchen mortar, and se\n- Grind the mixture until it becomes a smooth paste.\n- Transfer the pesto to a bowl, and stir in the grated Parm\n- You can add more olive oil to achieve the desired consist\n- Your Genovese pesto sauce is now ready. It can be served \n5. Note that if we pass the model another text that does not contain any\ninstructions, it will be able to respond as we instructed it:\n",
      "content_length": 1263,
      "extraction_method": "Direct"
    },
    {
      "page_number": 143,
      "chapter": null,
      "content": "response = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\", # engine = \"deployment_name\n    messages=[\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": 'the sun is shinin\n    ]\n)\n#print(response)\nprint(response['choices'][0]['message']['content'])\nThe following is the corresponding output:\nAs there are no instructions provided in the text you have \nBy giving clear instructions, you can help the model understand what you\nwant it to do and how you want it to do it. This can improve the quality and\nrelevance of the model’s output and reduce the need for further revisions or\ncorrections.\nHowever, sometimes, there are scenarios where clarity is not enough. We\nmight need to infer the way of thinking of our LLM to make it more robust\nwith respect to its task. In the next section, we are going to examine one of\nthese techniques, which will be very useful in the case of accomplishing\ncomplex tasks.\nSplit complex tasks into\nsubtasks\nAs discussed earlier, prompt engineering is a technique that involves\ndesigning effective inputs for LLMs to perform various tasks. Sometimes,\nthe tasks are too complex or ambiguous for a single prompt to handle, and it\n",
      "content_length": 1207,
      "extraction_method": "Direct"
    },
    {
      "page_number": 144,
      "chapter": null,
      "content": "is better to split them into simpler subtasks that can be solved by different\nprompts.\nHere are some examples of splitting complex tasks into subtasks:\nText summarization: A complex task that involves generating a\nconcise and accurate summary of a long text. This task can be split into\nsubtasks such as:\nExtracting the main points or keywords from the text\nRewriting the main points or keywords in a coherent and fluent\nway\nTrimming the summary to fit a desired length or format\nMachine translation: A complex task that involves translating a text\nfrom one language to another. This task can be split into subtasks such\nas:\nDetecting the source language of the text\nConverting the text into an intermediate representation that\npreserves the meaning and structure of the original text\nGenerating the text in the target language from the intermediate\nrepresentation\nPoem generation: A creative task that involves producing a poem that\nfollows a certain style, theme, or mood. This task can be split into\nsubtasks such as:\nChoosing a poetic form (such as sonnet, haiku, limerick, etc.) and\na rhyme scheme (such as ABAB, AABB, ABCB, etc.) for the\npoem\nGenerating a title and a topic for the poem based on the user’s\ninput or preference\n",
      "content_length": 1233,
      "extraction_method": "Direct"
    },
    {
      "page_number": 145,
      "chapter": null,
      "content": "Generating the lines or verses of the poem that match the chosen\nform, rhyme scheme, and topic\nRefining and polishing the poem to ensure coherence, fluency,\nand originality\nCode generation: A technical task that involves producing a code\nsnippet that performs a specific function or task. This task can be split\ninto subtasks such as:\nChoosing a programming language (such as Python, Java, C++,\netc.) and a framework or library (such as TensorFlow, PyTorch,\nReact, etc.) for the code\nGenerating a function name and a list of parameters and return\nvalues for the code based on the user’s input or specification\nGenerating the body of the function that implements the logic and\nfunctionality of the code\nAdding comments and documentation to explain the code and its\nusage\nLet’s consider the following example in Python, where we will ask our\nmodel to generate a summary of an article:\n1. We will leverage OpenAI’s GPT-3.5-turbo model in a manner similar to\nthe example discussed earlier in this chapter:\nimport os\nimport openai\nopenai.api_key = os.environ.get(\"OPENAI_API_KEY\")\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-35-turbo\", # engine = \"deployment_name\"\n    messages=[\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": article},\n",
      "content_length": 1288,
      "extraction_method": "Direct"
    },
    {
      "page_number": 146,
      "chapter": null,
      "content": "    ]\n)\n2. Let’s set both the system_message and article variables as follows\n(you can find the entire scripts in the book’s GitHub repository):\nsystem_message = \"\"\"\nYou are an AI assistant that summarizes articles.\nTo complete this task, do the following subtasks:\nRead the provided article context comprehensively and \nGenerate a paragraph summary of the current article co\nPrint each step of the process.\nArticle:\n\"\"\"\narticle = \"\"\"\nRecurrent neural networks, long short-term memory, and\nin particular, […]\n\"\"\"\n3. To see the output, you can run the following code:\nprint(response['choices'][0]['message']['content'])\nHere is the obtained output:\nSummary:\nThe article discusses the use of recurrent neural networks\nSteps:\n1. The article discusses the success and limitations of rec\n2. Attention mechanisms have become popular in addressing t\n3. The authors propose the Transformer, a model architectu\n4. The Transformer can achieve state-of-the-art results in \nAs you can see, the model was able to produce a high-quality summary\nbased on the key topics extracted (and displayed) from the given article. The\n",
      "content_length": 1109,
      "extraction_method": "Direct"
    },
    {
      "page_number": 147,
      "chapter": null,
      "content": "fact that we prompted the model to split the task into subtasks “forced” it to\nreduce the complexity of each subtask, hence improving the quality of the\nfinal result. This approach can also lead to noticeable results when we deal\nwith scenarios such as mathematical problems since it enhances the\nanalytical reasoning capabilities of the model.\nNote\nIn a landscape of many different LLMs, it is crucial to know\nthat the very same system message may not be as efficient in\nall models. A system message that perfectly works with GPT-\n4 might not be as efficient when applied to Llama 2, for\nexample. Therefore, it is pivotal to design the prompt in\naccordance with the type of LLM you decide to pick for your\napplication.\nSplitting complex tasks into easier subtasks is a powerful technique;\nnevertheless, it does not address one of the main risks of LLM-generated\ncontent, that is, having a wrong output. In the next two sections, we are\ngoing to see some techniques that are mainly aimed at addressing this risk.\nAsk for justification\nLLMs are built in such a way that they predict the next token based on the\nprevious ones without looking back at their generations. This might lead the\nmodel to output wrong content to the user, yet in a very convincing way. If\nthe LLM-powered application does not provide a specific reference to that\nresponse, it might be hard to validate the ground truth behind it. Henceforth,\nspecifying in the prompt to support the LLM’s answer with some reflections\n",
      "content_length": 1491,
      "extraction_method": "Direct"
    },
    {
      "page_number": 148,
      "chapter": null,
      "content": "and justification could prompt the model to recover from its actions.\nFurthermore, asking for justification might be useful also in case of answers\nthat are right but we simply don’t know the LLM’s reasoning behind it. For\nexample, let’s say we want our LLM to solve riddles. To do so, we can\ninstruct it as follows:\nsystem_message = \"\"\"\nYou are an AI assistant specialized in solving riddles.\nGiven a riddle, solve it the best you can.\nProvide a clear justification of your answer and the reasoning b\nRiddle:\n\"\"\"\nAs you can see, I’ve specified in the metaprompt to the LLM to justify its\nanswer and also provide its reasoning. Let’s see how it works:\nriddle = \"\"\"\nWhat has a face and two hands, but no arms or legs?\n\"\"\"\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": riddle},\n    ]\n)\n#print(response)\nprint(response['choices'][0]['message']['content'])\nThe following is the obtained output:\nThe answer to this riddle is a clock.\nA clock has a face with numbers and two hands (the hour and minut\nHence, a clock is the correct answer to this riddle.\n",
      "content_length": 1176,
      "extraction_method": "Direct"
    },
    {
      "page_number": 149,
      "chapter": null,
      "content": "Justifications are a great tool to make your model more reliable and robust\nsince they force it to “rethink” its output, as well as provide us with a view\nof how the reasoning was set to solve the problem.\nWith a similar approach, we could also intervene at different prompt levels\nto improve our LLM’s performance. For example, we might discover that\nthe model is systematically tackling a mathematical problem in the wrong\nway; henceforth, we might want to suggest the right approach directly at the\nmetaprompt level. Another example might be that of asking the model to\ngenerate multiple outputs – along with their justifications – to evaluate\ndifferent reasoning techniques and prompt the best one in the metaprompt.\nIn the next section, we are going to focus on one of these examples, more\nspecifically, the possibility of generating multiple outputs and then picking\nthe most likely one.\nGenerate many outputs, then\nuse the model to pick the best\none\nAs we saw in the previous section, LLMs are built in such a way that they\npredict the next token based on the previous ones without looking back at\ntheir generations. If this is the case, if one sampled token is the wrong one\n(in other words, if the model is unlucky), the LLM will keep generating\nwrong tokens and, henceforth, wrong content. Now, the bad news is that,\nunlike humans, LLMs cannot recover from errors on their own. This means\nthat, if we ask them, they acknowledge the error, but we need to explicitly\nprompt them to think about that.\n",
      "content_length": 1508,
      "extraction_method": "Direct"
    },
    {
      "page_number": 150,
      "chapter": null,
      "content": "One way to overcome this limitation is to broaden the space of probabilities\nof picking the right token. Rather than generating just one response, we can\nprompt the model to generate multiple responses, and then pick the one that\nis most suitable for the user’s query. This splits the job into two subtasks for\nour LLM:\n1. Generating multiple responses to the user’s query\n2. Comparing those responses and picking the best one, according to some\ncriteria we can specify in the metaprompt\nLet’s see an example, following up from the riddles examined in the\nprevious section:\nsystem_message = \"\"\"\nYou are an AI assistant specialized in solving riddles.\nGiven a riddle, you have to generate three answers to the riddle\nFor each answer, be specific about the reasoning you made.\nThen, among the three answers, select the one that is most plaus\nRiddle:\n\"\"\"\nriddle = \"\"\"\nWhat has a face and two hands, but no arms or legs?\n\"\"\"\nIn this case, I’ve prompted the model to generate three answers to the riddle,\nthen to give me the most likely, justifying why. Let’s see the result:\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\", # engine = \"deployment_name\".\n    messages=[\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": riddle},\n    ]\n)\n",
      "content_length": 1292,
      "extraction_method": "Direct"
    },
    {
      "page_number": 151,
      "chapter": null,
      "content": "#print(response)\nprint(response['choices'][0]['message']['content'])\nWe then get the following output:\nAnswer 1: A clock. A clock has a face with numbers and two hands \nAnswer 2: A watch. Similar to a clock, a watch also has a face wi\nAnswer 3: A wall. While this answer may be less obvious, a wall d\nAmong the three answers, the most plausible one is answer 1: a cl\nAs you can see, the model selected the most plausible answer along with a\njustification of its choice. It is interesting to note that “clock” and “watch”\nmight seem similar responses; however, the model specified that “watch” is\nusually worn on a person’s wrist and, even though it doesn’t mean it has\narms or legs, this element might have lowered the probability of being the\ncorrect answer.\nWhat would you have picked?\nAs discussed earlier, forcing the model to tackle a problem with different\napproaches is a way to collect multiple samples of reasonings, which might\nserve as further instructions in the metaprompt. For example, if we want the\nmodel to always propose something that is not the most straightforward\nsolution to a problem – in other words, if we want it to “think differently” –\nwe might force it to solve a problem in N ways and then use the most\ncreative reasoning as a framework in the metaprompt.\nThe last element we are going to examine is the overall structure we want to\ngive to our metaprompt. In fact, in previous examples, we saw a sample\nsystem message with some statements and instructions. In the next section,\nwe will see how the order and “strength” of those statements and instructions\nare not invariants.\n",
      "content_length": 1608,
      "extraction_method": "Direct"
    },
    {
      "page_number": 152,
      "chapter": null,
      "content": "Repeat instructions at the end\nLLMs tend not to process the metaprompt attributing the same weight or\nimprortance to all the sections. In fact, in his blog post Large Language\nModel Prompt Engineering for Complex Summarization, John Stewart (a\nsoftware engineer at Microsoft) found some interesting outcomes from\narranging prompt sections\n(https://devblogs.microsoft.com/ise/gpt-summary-\nprompt-engineering/). More specifically, after several\nexperimentations, he found that repeating the main instruction at the end of\nthe prompt can help the model overcome its inner recency bias.\nDefinition\nRecency bias is the tendency of LLMs to give more weight\nto the information that appears near the end of a prompt, and\nignore or forget the information that appears earlier. This can\nlead to inaccurate or inconsistent responses that do not take\ninto account the whole context of the task. For example, if\nthe prompt is a long conversation between two people, the\nmodel may only focus on the last few messages and\ndisregard the previous ones.\nLet’s look at some ways to overcome recency bias:\nOne possible way to overcome recency bias is to break down the task\ninto smaller steps or subtasks and provide feedback or guidance along\nthe way. This can help the model focus on each step and avoid getting\nlost in irrelevant details. We’ve covered this technique in the Split\n",
      "content_length": 1364,
      "extraction_method": "Direct"
    },
    {
      "page_number": 153,
      "chapter": null,
      "content": "complex tasks into subtasks section in, which we discussed splitting\ncomplex tasks into easier subtasks.\nAnother way to overcome recency bias with prompt engineering\ntechniques is to repeat the instructions or the main goal of the task at\nthe end of the prompt. This can help remind the model of what it is\nsupposed to do and what kind of response it should generate.\nFor instance, let’s say we want our model to output the sentiment of a\nwhole chat history between an AI agent and the user. We want to make\nsure that the model will output the sentiment in lowercase and without\npunctuation.\nLet’s consider the following example (the conversation is truncated, but you\ncan find the whole code in the book’s GitHub repository). In this case, the\nkey instruction is that of having as output only the sentiment in lowercase\nand without punctuation:\nsystem_message = \"\"\"\nYou are a sentiment analyzer. You classify conversations into th\nReturn only the sentiment, in lowercase and without punctuation.\nConversation:\n\"\"\"\nconversation = \"\"\"\nCustomer: Hi, I need some help with my order.\nAI agent: Hello, welcome to our online store. I'm an AI agent an\nCustomer: I ordered a pair of shoes yesterday, but I haven't rec\n[…]\n\"\"\"\nIn this scenario, we have key instructions before the conversation, so let’s\ninitialize our model and feed it with the two variables system_message and\nconversation:\n",
      "content_length": 1384,
      "extraction_method": "Direct"
    },
    {
      "page_number": 154,
      "chapter": null,
      "content": "response = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\", # engine = \"deployment_name\".\n    messages=[\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": conversation},\n    ]\n)\n#print(response)\nprint(response['choices'][0]['message']['content'])\nHere is the output that we receive:\nNeutral\nThe model didn’t follow the instruction of having only lowercase letters.\nLet’s try to repeat the instruction also at the end of the prompt:\nsystem_message = f\"\"\"\nYou are a sentiment analyzer. You classify conversations into th\nReturn only the sentiment, in lowercase and without punctuation.\nConversation:\n{conversation}\nRemember to return only the sentiment, in lowercase and without \n\"\"\"\nAgain, let’s invoke our model with the updated system_message:\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\", # engine = \"deployment_name\".\n    messages=[\n        {\"role\": \"user\", \"content\": system_message},\n    ]\n)\n#print(response)\nprint(response['choices'][0]['message']['content'])\n",
      "content_length": 1037,
      "extraction_method": "Direct"
    },
    {
      "page_number": 155,
      "chapter": null,
      "content": "Here is the corresponding output:\nneutral\nAs you can see, now the model was able to provide exactly the output we\ndesired. This approach is particularly useful whenever we have a\nconversation history to keep storing in the context window. If this is the\ncase, having the main instructions at the beginning might induce the model\nnot to have them in mind once it also goes through the whole history, hence\nreducing their strength.\nUse delimiters\nThe last principle to be covered is related to the format we want to give to\nour metaprompt. This helps our LLM to better understand its intents as well\nas relate different sections and paragraphs to each other.\nTo achieve this, we can use delimiters within our prompt. A delimiter can be\nany sequence of characters or symbols that is clearly mapping a schema\nrather than a concept. For example, we can consider the following sequences\nto be delimiters:\n>>>>\n====\n------\n####\n` ` ` ` `\nThis leads to a series of benefits, including:\n",
      "content_length": 978,
      "extraction_method": "Direct"
    },
    {
      "page_number": 156,
      "chapter": null,
      "content": "Clear separation: Delimiters mark distinct sections within a prompt,\nseparating instructions, examples, and desired output.\nGuidance for LLMs: Proper use of delimiters removes ambiguity,\nguiding the model effectively.\nEnhanced precision: Delimiters improve prompt understanding,\nresulting in more relevant responses.\nImproved coherence: Effective use of delimiters organizes instructions,\ninputs, and outputs, leading to coherent responses.\nLet’s consider, for example, a metaprompt that aims at instructing the model\nto translate user’s tasks into Python code, providing an example to do so:\nsystem_message = \"\"\"\nYou are a Python expert who produces Python code as per the user\n===>START EXAMPLE\n---User Query---\nGive me a function to print a string of text.\n---User Output---\nBelow you can find the described function:\n```def my_print(text):\n     return print(text)\n```\n<===END EXAMPLE\n\"\"\"\nquery = \"generate a Python function to calculate the nth Fibonac\nIn the above example, we’ve used delimiters to both specify the beginning\nand end of an example for a one-shot learning approach and, within the\nexample, specify the Python code snippet.\nLet’s see how it works:\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\", # engine = \"deployment_name\".\n",
      "content_length": 1266,
      "extraction_method": "Direct"
    },
    {
      "page_number": 157,
      "chapter": null,
      "content": "    messages=[\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": query},\n    ]\n)\n#print(response)\nprint(response['choices'][0]['message']['content'])\nHere is our output:\nSure, here's a Python function to calculate the nth Fibonacci num\n```python def fibonacci(n):\nif n < 0:\nreturn None\nelif n == 0:\nreturn 0\nelif n == 1:\nreturn 1\nelse:\nreturn fibonacci(n-1) + fibonacci(n-2) ```\nThe function takes a non-negative integer `n` as input and return\nAs you can see, it also printed the code with backticks, as shown within the\nsystem message.\nAll the principles examined up to this point are general rules that can make\nyour LLM-powered application more robust. Those techniques should be\nkept in mind regardless of the type of application you are developing since\nthey are general best practices that improve your LLM performance. In the\nfollowing section, we are going to see some advanced techniques for prompt\nengineering.\nAdvanced techniques\n",
      "content_length": 983,
      "extraction_method": "Direct"
    },
    {
      "page_number": 158,
      "chapter": null,
      "content": "Advanced techniques might be implemented for specific scenarios and\naddress the way the model reasons and thinks about the answer before\nproviding it to the final user. Let’s look at some of these in the upcoming\nsections.\nFew-shot approach\nIn their paper Language Models are Few-Shot Learners, Tom Brown et al.\ndemonstrate that GPT-3 can achieve strong performance on many NLP tasks\nin a few-shot setting. This means that for all tasks, GPT-3 is applied without\nany fine-tuning, with tasks and few-shot demonstrations specified purely via\ntext interaction with the model.\nThis is an example and evidence of how the concept of few-shot learning –\nwhich means providing the model with examples of how we would like it to\nrespond – is a powerful technique that enables model customization without\ninterfering with the overall architecture.\nFor example, let’s say we want our model to generate a tagline for a new\nproduct line of climbing shoes we’ve just coined – Elevation Embrace. We\nhave an idea of what the tagline should be like – concise and direct. We\ncould explain it to the model in plain text; however, it might be more\neffective simply to provide it with some examples of similar projects.\nLet’s see an implementation with code:\nsystem_message = \"\"\"\nYou are an AI marketing assistant. You help users to create tagl\nGiven a product name, produce a tagline similar to the following\nPeak Pursuit - Conquer Heights with Comfort\nSummit Steps - Your Partner for Every Ascent\nCrag Conquerors - Step Up, Stand Tall\nProduct name:\n",
      "content_length": 1530,
      "extraction_method": "Direct"
    },
    {
      "page_number": 159,
      "chapter": null,
      "content": "\"\"\"\nproduct_name = 'Elevation Embrace'\nLet’s see how our model will handle this request:\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\", # engine = \"deployment_name\".\n    messages=[\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": product_name},\n    ]\n)\n#print(response)\nprint(response['choices'][0]['message']['content'])\nThe following is our output:\nTagline idea: Embrace the Heights with Confidence.\nAs you can see, it maintained the style, length, and also writing convention\nof the provided taglines. This is extremely useful when you want your model\nto follow examples you already have, such as fixed templates.\nNote that, most of the time, few-shot learning is powerful enough to\ncustomize a model even in extremely specialized scenarios, where we could\nthink about fine-tuning as the proper tool. In fact, proper few-shot learning\ncould be as effective as a fine-tuning process.\nLet’s look at another example. Let’s say we want to develop a model that\nspecializes in sentiment analysis. To do so, we provide it with a series of\nexamples of texts with different sentiments, alongside the output we would\nlike – positive or negative. Note that this set of examples is nothing but a\n",
      "content_length": 1250,
      "extraction_method": "Direct"
    },
    {
      "page_number": 160,
      "chapter": null,
      "content": "small training set for supervised learning tasks; the only difference from\nfine-tuning is that we are not updating the model’s parameters.\nTo provide you with a concrete representation of what was said above, let’s\nprovide our model with just two examples for each label:\nsystem_message = \"\"\"\nYou are a binary classifier for sentiment analysis.\nGiven a text, based on its sentiment, you classify it into one o\nYou can use the following texts as examples:\nText: \"I love this product! It's fantastic and works perfectly.\"\nPositive\nText: \"I'm really disappointed with the quality of the food.\"\nNegative\nText: \"This is the best day of my life!\"\nPositive\nText: \"I can't stand the noise in this restaurant.\"\nNegative\nONLY return the sentiment as output (without punctuation).\nText:\n\"\"\"\nTo test our classifier, I’ve used the IMDb database of movie reviews\navailable on Kaggle at\nhttps://www.kaggle.com/datasets/yasserh/imdb-movie-\nratings-sentiment-analysis/data. As you can see, the dataset\ncontains many movie reviews along with their associated sentiment –\npositive or negative. Let’s substitute the binary label of 0–1 with a verbose\nlabel of Negative–Positive:\nimport numpy as np\nimport pandas as pd\ndf = pd .read_csv('movie.csv', encoding='utf-8')\n",
      "content_length": 1247,
      "extraction_method": "Direct"
    },
    {
      "page_number": 161,
      "chapter": null,
      "content": "df['label'] = df['label'].replace({0: 'Negative', 1: 'Positive'}\ndf.head()\nThis gives us the first few records of the dataset, which are as follows:\nFigure 4.2: First observations of the movie dataset\nNow, we want to test the performance of our model over a sample of 10\nobservations of this dataset:\ndf = df.sample(n=10, random_state=42)\ndef process_text(text):\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": text},\n        ]\n    )\n    return response['choices'][0]['message']['content']\ndf['predicted'] = df['text'].apply(process_text)\nprint(df)\nThe following is our output:\n",
      "content_length": 724,
      "extraction_method": "Direct"
    },
    {
      "page_number": 162,
      "chapter": null,
      "content": "Figure 4.3: Output of a GPT-3.5 model with few-shot examples\nAs you can see, by comparing the label and predicted columns, the model\nwas able to correctly classify all the reviews, without even fine-tuning! This\nis just an example of what you can achieve – in terms of model\nspecialization – with the technique of few-shot learning.\nChain of thought\nIntroduced in the paper Chain-of-Thought Prompting Elicits Reasoning in\nLarge Language Models by Wei et al., chain of thought (CoT) is a\ntechnique that enables complex reasoning capabilities through intermediate\nreasoning steps. It also encourages the model to explain its reasoning,\n“forcing” it not to be too fast and risking giving the wrong response (as we\nsaw in previous sections).\nLet’s say that we want to prompt our LLM to solve generic first-degree\nequations. To do so, we are going to provide it with a basic reasoning list\nthat it might want to follow:\nsystem_message = \"\"\"\nTo solve a generic first-degree equation, follow these steps:\n1. **Identify the Equation:** Start by identifying the equation \n2. **Isolate the Variable:** Your goal is to isolate the variabl\n",
      "content_length": 1128,
      "extraction_method": "Direct"
    },
    {
      "page_number": 163,
      "chapter": null,
      "content": " \n   a. **Add or Subtract Constants:** Add or subtract 'b' from bo\n \n   b. **Divide by the Coefficient:** Divide both sides by 'a' to\n3. **Simplify:** Simplify both sides of the equation as much as \n4. **Solve for 'x':** Once 'x' is isolated on one side, you have\n5. **Check Your Solution:** Plug the found value of 'x' back int\n6. **Express the Solution:** Write down the solution in a clear \n7. **Consider Special Cases:** Be aware of special cases where t\nEquation:\n\"\"\"\nLet’s see how it can be implemented:\nequation = \"3x + 5 = 11\"\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\", # engine = \"deployment_name\".\n    messages=[\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": equation},\n    ]\n)\n#print(response)\nprint(response['choices'][0]['message']['content'])\nThe following is our output:\n1. Identify the Equation: The equation is 3x + 5 = 11.\n2. Isolate the Variable:\n  \n   a. Subtract Constants: Subtract 5 from both sides to move cons\n  \n      3x + 5 - 5 = 11 - 5\n  \n   Simplifying, we get:\n  \n      3x = 6\n  \n",
      "content_length": 1080,
      "extraction_method": "Direct"
    },
    {
      "page_number": 164,
      "chapter": null,
      "content": "   b. Divide by the Coefficient: Divide both sides by 3 to isolat\n  \n      3x/3 = 6/3\n  \n   Simplifying, we get:\n  \n      x = 2\n  \n3. Simplify: Both sides are already simplified.\n4. Solve for 'x': x = 2.\n5. Check Your Solution: Plug x = 2 back into the original equatio\n   3(2) + 5 = 11\n  \n   Simplifying, we get:\n  \n   6 + 5 = 11\n  \n   The left side equals the right side, so the solution is correc\n6. Express the Solution: The solution to the equation 3x + 5 = 11\n7. Consider Special Cases: Since the coefficient 3 is not zero, t\nAs you can see, the model clearly followed the seven steps specified in the\nmetaprompt, which also allows the model to “take its time” to perform this\ntask. Note that you can also combine it with few-shot prompting to get better\nresults on more complex tasks that require reasoning before responding.\nWith CoT, we are prompting the model to generate intermediate reasoning\nsteps. This is also a component of another reasoning technique, which we are\ngoing to examine in the next section.\nReAct\nIntroduced in the paper ReAct: Synergizing Reasoning and Acting in\nLanguage Models by Yao et al., ReAct (Reason and Act) is a general\nparadigm that combines reasoning and acting with LLMs. ReAct prompts\n",
      "content_length": 1229,
      "extraction_method": "Direct"
    },
    {
      "page_number": 165,
      "chapter": null,
      "content": "the language model to generate verbal reasoning traces and actions for a\ntask, and also receives observations from external sources such as web\nsearches or databases. This allows the language model to perform dynamic\nreasoning and quickly adapt its action plan based on external information.\nFor example, you can prompt the language model to answer a question by\nfirst reasoning about the question, then performing an action to send a query\nto the web, then receiving an observation from the search results, and then\ncontinuing with this thought, action, observation loop until it reaches a\nconclusion.\nThe difference between CoT and ReAct approaches is that CoT prompts the\nlanguage model to generate intermediate reasoning steps for a task, while\nReAct prompts the language model to generate intermediate reasoning steps,\nactions, and observations for a task.\nNote that the “action” phase is generally related to the possibility for our\nLLM to interact with external tools, such as a web search.\nFor example, let’s say we want to ask our model for some up-to-date\ninformation about the upcoming Olympic games. To do so, we are going to\nbuild a smart LangChain agent (as described in Chapter 2) leveraging\nSerpAPIWrapperWrapper (to wrap the SerpApi to navigate the web), the\nAgentType tool (to decide which type of agent to use for our goal), and other\nprompt-related modules (to make it easier to “templatize” our instructions).\nLet’s see how we can do this (I won’t dive deeper into each component of\nthe following code since the next chapter will be entirely focused on\nLangChain and its main components):\nimport os\nfrom dotenv import load_dotenv\nfrom langchain import SerpAPIWrapper\nfrom langchain.agents import AgentType, initialize_agent\n",
      "content_length": 1745,
      "extraction_method": "Direct"
    },
    {
      "page_number": 166,
      "chapter": null,
      "content": "from langchain.chat_models import ChatOpenAI\nfrom langchain.tools import BaseTool, StructuredTool, Tool, tool\nfrom langchain.schema import HumanMessage\nmodel = ChatOpenAI(\n    model_name='gpt-35-turbo'\n)\nload_dotenv()\nkey = os.environ[\"SERPAPI_API_KEY\"]\nsearch = SerpAPIWrapper()\ntools = [\n    Tool.from_function(\n        func=search.run,\n        name=\"Search\",\n        description=\"useful for when you need to answer question\n    )\n    ]\nagent_executor = initialize_agent(tools, model, agent=AgentType.\nAs you can see, for this purpose, I’ve used a pre-built agent type available in\nLangChain called ZERO_SHOT_REACT_DESCRIPTION. It comes with a\nprecompiled prompt that follows the ReAct approach. Let’s inspect that\nprompt:\nprint(agent_executor.agent.llm_chain.prompt.template)\nHere is the corresponding output:\nAnswer the following questions as best you can. You have access t\nSearch: useful for when you need to answer questions about curren\nUse the following format:\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [Search]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N ti\nThought: I now know the final answer\n",
      "content_length": 1299,
      "extraction_method": "Direct"
    },
    {
      "page_number": 167,
      "chapter": null,
      "content": "Final Answer: the final answer to the original input question\nBegin!\nQuestion: {input}\nThought:{agent_scratchpad}\nLet’s now test our agent by asking something about the upcoming Olympic\ngames and zooming in on the intermediate steps:\nagent_executor('who are going to be the italian male athletes fo\nThis is the output with intermediate steps:\n> Entering new AgentExecutor chain... I should search for recent \n>Observation: A select group of climbers ensured their participat\n>Input: \"List of Italian male climbers Paris 2024 Olympics\" Obser\n>Thought:This information does not seem to be readily available, \n>Action: Contact Action Input: Email or phone call to the Italian\n>Action: Search Action Input: \"Latest updates on Italian male cli\n>Thought: Based on the information I have gathered, it seems that\n>Final Answer: Matteo Zurloni is one of the Italian male climbers\nHere is the obtained output:\n'Matteo Zurloni is one of the Italian male climbers who has secur\nAt the time of this question (7th of October 2023), the answer is definitely\ncorrect. Note how the model went through several iterations of\nObservation/Thought/Action until it reached the conclusion. This is a great\nexample of how prompting a model to think step by step and explicitly\ndefine each step of the reasoning makes it “wiser” and more cautious before\nanswering. It is also a great technique to prevent hallucination.\n",
      "content_length": 1394,
      "extraction_method": "Direct"
    },
    {
      "page_number": 168,
      "chapter": null,
      "content": "Overall, prompt engineering is a powerful discipline, still in its emerging\nphase yet already widely adopted within LLM-powered applications. In the\nfollowing chapters, we are going to see concrete applications of this\ntechnique.\nSummary\nIn this chapter, we covered many aspects of the activity of prompt\nengineering, a core step in the context of improving the performance of\nLLMs within your application, as well as customizing it depending on the\nscenario. Prompt engineering is an emerging discipline that is paving the\nway for a new category of applications, infused with LLMs.\nWe started with an introduction to the concept of prompt engineering and\nwhy it is important, and then moved toward the basic principles – including\nclear instructions, asking for justification, etc. Then, we moved on to more\nadvanced techniques that are meant to shape the reasoning approach of our\nLLM: few-shot learning, CoT, and ReAct.\nIn the next chapters, we will see those techniques in action by building real-\nworld applications using LLMs.\nReferences\nReAct approach: https://arxiv.org/abs/2210.03629\nWhat is prompt engineering?:\nhttps://www.mckinsey.com/featured-\ninsights/mckinsey-explainers/what-is-prompt-\nengineering\n",
      "content_length": 1214,
      "extraction_method": "Direct"
    },
    {
      "page_number": 169,
      "chapter": null,
      "content": "Prompt engineering techniques:\nhttps://blog.mrsharm.com/prompt-engineering-\nguide/\nPrompt engineering principles:\nhttps://learn.microsoft.com/en-us/azure/ai-\nservices/openai/concepts/advanced-prompt-\nengineering?pivots=programming-language-chat-\ncompletions\nRecency bias: https://learn.microsoft.com/en-\nus/azure/ai-services/openai/concepts/advanced-\nprompt-engineering?pivots=programming-language-\nchat-completions#repeat-instructions-at-the-end\nLarge Language Model Prompt Engineering for Complex\nSummarization:\nhttps://devblogs.microsoft.com/ise/2023/06/27/gp\nt-summary-prompt-engineering/\nLanguage Models are Few-Shot Learners:\nhttps://arxiv.org/pdf/2005.14165.pdf\nIMDb dataset:\nhttps://www.kaggle.com/datasets/yasserh/imdb-\nmovie-ratings-sentiment-analysis/code\nReAct: https://arxiv.org/abs/2210.03629\nChain of Thought Prompting Elicits Reasoning in Large Language\nModels: https://arxiv.org/abs/2201.11903\n",
      "content_length": 911,
      "extraction_method": "Direct"
    },
    {
      "page_number": 170,
      "chapter": null,
      "content": "Join our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\nOceanofPDF.com\n",
      "content_length": 154,
      "extraction_method": "Direct"
    },
    {
      "page_number": 171,
      "chapter": null,
      "content": "5\nEmbedding LLMs within\nYour Applications\nThis chapter kickstarts the hands-on portions of this book, focusing on how\nwe can leverage large language models (LLMs) to build powerful AI\napplications. In fact, LLMs have introduced a whole new paradigm in\nsoftware development, paving the way for new families of applications that\nhave the peculiarity of making the communication between the user and the\nmachine smooth and conversational. Plus, those models enhanced existing\napplications, such as chatbots and recommendation systems, with their\nunique reasoning capabilities.\nDeveloping LLM-powered applications is becoming a key factor for\nenterprises to keep themselves competitive in the market, and this leads to\nthe spreading of new libraries and frameworks that make it easier to embed\nLLMs within applications. Some examples are Semantic Kernel, Haystack,\nLlamaIndex, and LangChain. In this chapter, we are going to cover\nLangChain and use its modules to build hands-on examples. By the end of\nthis chapter, you will have the technical foundations to start developing your\nLLM-powered applications using LangChain and open-source Hugging Face\nmodels.\nIn this chapter, we will cover the following topics:\nA brief note about LangChain\n",
      "content_length": 1238,
      "extraction_method": "Direct"
    },
    {
      "page_number": 172,
      "chapter": null,
      "content": "Getting started with LangChain\nWorking with LLMs via the Hugging Face Hub\nTechnical requirements\nTo complete the hands-on sections of this chapter, the following\nprerequisites are needed:\nA Hugging Face account and user access token.\nAn OpenAI account and user access token.\nPython 3.7.1 or later version.\nPython packages: Make sure to have the following Python packages\ninstalled: langchain, python-dotenv, huggingface_hub, google-\nsearch-results, faiss, and tiktoken. Those can be easily installed via\npip install in your terminal.\nYou can find all the code and examples used in this chapter in the book’s\nGitHub repository at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications\nA brief note about LangChain\nJust as generative AI has evolved so rapidly over the last year, so has\nLangChain. In the months between the writing of this book and its\npublication, the AI orchestrator has gone through massive changes. The most\nremarkable traces back to January 2024, when the first stable version of\nLangChain was released, introducing a new organization of packages and\nlibraries.\n",
      "content_length": 1097,
      "extraction_method": "Direct"
    },
    {
      "page_number": 173,
      "chapter": null,
      "content": "It consists of the following:\nA core backbone where all the abstractions and runtime logic are stored\nA layer of third-party integrations and components\nA set of pre-built architectures and templates to leverage\nA serving layer to consume chains as APIs\nAn observability layer to monitor your applications in the development,\ntesting, and production stages\nYou can look at the architecture in greater detail at\nhttps://python.langchain.com/docs/get_started/intro\nduction.\nThere are three packages you can install to start using LangChain:\nlangchain-core: This contains the base abstractions and runtime for the\nwhole LangChain ecosystem.\nlangchain-experimental: This holds experimental LangChain code,\nintended for research and experimental uses.\nlangchain-community: This contains all third-party integrations.\nOn top of that, there are three additional packages that we’re not going to\ncover in this book, yet can be leveraged to monitor and maintain your\nLangChain applications:\nlangserve: LangServe is a tool that lets you deploy LangChain\nrunnables and chains as a REST API, making it easier to integrate\nLangChain applications into production environments.\nlangsmith: Think of LangSmith as an innovative testing framework\nfor evaluating language models and AI applications. It helps visualize\ninputs and outputs at each step in the chain, aiding understanding and\nintuition during development.\n",
      "content_length": 1400,
      "extraction_method": "Direct"
    },
    {
      "page_number": 174,
      "chapter": null,
      "content": "langchain-cli: The official command-line interface for LangChain, it\nfacilitates interactions with LangChain projects, including template\nusage and quickstarts.\nLast but not least, LangChain introduced the LangChain Expression\nLanguage (LCEL) to enhance the efficiency and flexibility of text\nprocessing tasks.\nKey features of LCEL include:\nStreaming asynchronous support: This allows for the efficient\nhandling of data streams.\nBatch support: This enables processing data in batches.\nParallel execution: This enhances performance by executing tasks\nconcurrently.\nRetries and fallbacks: This ensures robustness by handling failures\ngracefully.\nDynamically routing logic: This allows logic flow based on input and\noutput.\nMessage history: This keeps track of interactions for context-aware\nprocessing.\nWe are not going to cover LCEL in this book; however, all the code samples\ncan be converted into LCEL if you want to speed up your development and\nleverage its native integration with the end-to-end LangChain development\nstack.\nImportant note\nBefore we start working with LangChain, it is important to\nnote that all packages are versioned slightly differently, yet\n",
      "content_length": 1166,
      "extraction_method": "Direct"
    },
    {
      "page_number": 175,
      "chapter": null,
      "content": "all releases are cut with high frequency by a maintainer with\na clearer communication strategy for breaking changes.\nIn the upcoming chapters, you will see some packages that\nhave been moved, for example, to the experimental\npackage, meaning that they are more prone to experimental\nuses. Similarly, some third-party integrations have been\nmoved to the community package.\nStarting from the next section, we are going to cover the backbone concepts\n– such as memory, VectorDB, and agents – that remain solid in the\nLangChain framework and, more generally, in the landscape of LLM\ndevelopment.\nGetting started with\nLangChain\nAs introduced in Chapter 2, LangChain is a lightweight framework meant to\nmake it easier to integrate and orchestrate LLMs and their components\nwithin applications. It is mainly Python based, yet it recently extended its\nsupport to JavaScript and TypeScript.\nIn addition to LLM integration (which we will cover in an upcoming\ndedicated section), we saw that LangChain offers the following main\ncomponents:\nModels and prompt templates\nData connections\nMemory\nChains\n",
      "content_length": 1088,
      "extraction_method": "Direct"
    },
    {
      "page_number": 176,
      "chapter": null,
      "content": "Agents\nThese components are illustrated in the following diagram:\nFigure 5.1: LangChain’s components\nThe next sections will take a deep dive into each of these components.\nModels and prompts\nLangChain offers more than 50 integrations with third-party vendors and\nplatforms, including OpenAI, Azure OpenAI, Databricks, and MosaicML,\nas well as the integration with the Hugging Face Hub and the world of open-\nsource LLMs. In Part 2 of this book, we will be trying various LLMs, both\nproprietary and open-source, and leveraging LangChain’s integrations.\nJust to provide an example, let’s see how easy it is to consume the OpenAI\nGPT-3 model (you can retrieve your OpenAI API key at\nhttps://platform.openai.com/account/api-keys):\n",
      "content_length": 727,
      "extraction_method": "Direct"
    },
    {
      "page_number": 177,
      "chapter": null,
      "content": "from langchain.llms import OpenAI\nllm = OpenAI(openai_api_key=\"your-api-key\")\nprint(llm('tell me a joke'))\nHere is the corresponding output:\nQ: What did one plate say to the other plate?\nA: Dinner's on me!\nNote\nWhile running examples with LLMs, the output will vary at\neach run, due to the stochasticity of the models themselves.\nIf you want to reduce the margin of variations in your output,\nyou can make your model more “deterministic” by tuning the\ntemperature hyperparameter. This parameter ranges from 0\n(deterministic) to 1 (stochastic).\nBy default, the OpenAI module uses the gpt-3.5-turbo-instruct as a\nmodel. You can specify the model you want to use by passing the model’s\nname as a parameter.\nAs said previously, we will dive deeper into LLMs in the next section; so,\nfor now, let’s focus on prompts. There are two main components related to\nLLM prompts and prompts design/engineering:\nPrompt templates: A prompt template is a component that defines how\nto generate a prompt for a language model. It can include variables,\nplaceholders, prefixes, suffixes, and other elements that can be\ncustomized according to the data and the task.\n",
      "content_length": 1146,
      "extraction_method": "Direct"
    },
    {
      "page_number": 178,
      "chapter": null,
      "content": "For example, suppose you want to use a language model to generate a\ntranslation from one language to another. You can use a prompt\ntemplate like this:\nSentence: {sentence}\nTranslation in {language}:\n{sentence} is a variable that will be replaced by the actual text.\nTranslation in {language}: is a prefix that indicates the task and the\nexpected output format.\nYou can easily implement this template as follows:\nfrom langchain import PromptTemplate\ntemplate = \"\"\"Sentence: {sentence}\nTranslation in {language}:\"\"\"\nprompt = PromptTemplate(template=template, input_variables=\nprint(prompt.format(sentence = \"the cat is on the table\", l\nHere is the output:\nSentence: the cat is on the table\nTranslation in spanish:\nGenerally speaking, prompt templates tend to be agnostic with respect\nto the LLM you might decide to use, and it is adaptable to both\ncompletion and chat models.\nDefinition\nA completion model is a type of LLM that takes a text\ninput and generates a text output, which is called a\n",
      "content_length": 992,
      "extraction_method": "Direct"
    },
    {
      "page_number": 179,
      "chapter": null,
      "content": "completion. The completion model tries to continue\nthe prompt in a coherent and relevant way, according\nto the task and the data it was trained on. For example,\na completion model can generate summaries,\ntranslations, stories, code, lyrics, and more, depending\non the prompt.\nA chat model is a special kind of completion model\nthat is designed to generate conversational responses.\nA chat model takes a list of messages as input, where\neach message has a role (either system, user, or\nassistant) and content. The chat model tries to generate\na new message for the assistant role, based on the\nprevious messages and the system instruction.\nThe main difference between completion and chat\nmodels is that completion models expect a single text\ninput as a prompt, while chat models expect a list of\nmessages as input.\nExample selector: An example selector is a component in LangChain\nthat allows you to choose which examples to include in a prompt for a\nlanguage model. A prompt is a text input that guides the language\nmodel to produce a desired output. Examples are pairs of inputs and\noutputs that demonstrate the task and the format of the output as\nfollows:\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal gene\n",
      "content_length": 1214,
      "extraction_method": "Direct"
    },
    {
      "page_number": 180,
      "chapter": null,
      "content": "The idea recalls the concept of few-shot learning we covered in\nChapter 1.\nLangChain offers the example selector class called\nBaseExampleSelector that you can import and modify as you wish.\nYou can find the API reference at\nhttps://python.langchain.com/docs/modules/model\n_io/prompts/example_selectors/.\nData connections\nData connections refer to the building blocks needed to retrieve the\nadditional non-parametric knowledge we want to provide the model with.\nThe idea is to cover the typical flow of incorporating user-specific data into\napplications that are made of five main blocks, as illustrated in the following\nfigure:\nFigure 5.2: Incorporating user-specific knowledge into LLMs (source:\nhttps://python.langchain.com/docs/modules/data_connection/)\nThose blocks are addressed with the following LangChain tools:\nDocument loaders: They are in charge of loading documents from\ndifferent sources such as CSV, file directory, HTML, JSON,\n",
      "content_length": 942,
      "extraction_method": "Direct"
    },
    {
      "page_number": 181,
      "chapter": null,
      "content": "Markdown, and PDF. Document loaders expose a .load method for\nloading data as documents from a configured source. The output is a\nDocument object that contains a piece of text and associated metadata.\nFor example, let’s consider a sample CSV file to be loaded (you can\nfind the whole code in the book’s GitHub repository at\nhttps://github.com/PacktPublishing/Building-\nLLM-Powered-Applications):\nfrom langchain.document_loaders.csv_loader import CSVLoade\nloader = CSVLoader(file_path='sample.csv')\ndata = loader.load()\nprint(data)\nHere is the output:\n[Document(page_content='Name: John\\nAge: 25\\nCity: New York\nDocument transformers: After importing your documents, it’s\ncommon to modify them to better match your needs. A basic instance of\nthis is breaking down a lengthy document into smaller chunks that fit\nyour model’s context window. Within LangChain, there are various pre-\nbuilt document transformers available called text splitters. The idea of\ntext splitters is to make it easier to split documents into chunks that are\nsemantically related so that we do not lose context or relevant\ninformation.\nWith text splitters, you can decide how to split the text (for example,\nby character, heading, token, and so on) and how to measure the\nlength of the chunk (for example, by number of characters).\n",
      "content_length": 1303,
      "extraction_method": "Direct"
    },
    {
      "page_number": 182,
      "chapter": null,
      "content": "For example, let’s split a document using the\nRecursiveCharacterTextSplitter module, which operates at a\ncharacter level. For this purpose, we will be using a .txt file about\nmountains (you can find the whole code in the book’s GitHub\nrepository at\nhttps://github.com/PacktPublishing/Building-\nLLM-Powered-Applications):\nwith open('mountain.txt') as f:\n    mountain = f.read()\nfrom langchain.text_splitter import RecursiveCharacterTextS\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size = 100, #number of characters for each chunk\n    chunk_overlap  = 20,#number of characters overlapping b\n    length_function = len #function used to measure the num\n)\ntexts = text_splitter.create_documents([mountain])\nprint(texts[0])\nprint(texts[1])\nprint(texts[2])\nHere, chunk_size refers to the number of characters in each chunk\nwhile chunk_overlap represents the number of characters overlapping\nbetween successive chunks. Here is the output:\npage_content=\"Amidst the serene landscape, towering mountai\npage_content='The crisp mountain air carries whispers of t\nText embedding models: In Chapter 1, in the Under the hood of an\nLLM section, we introduced the concept of embedding as a way to\nrepresent words, subwords, or characters in a continuous vector space.\n",
      "content_length": 1265,
      "extraction_method": "Direct"
    },
    {
      "page_number": 183,
      "chapter": null,
      "content": "Embeddings are the key step in incorporating non-parametric\nknowledge into LLMs. In fact, once properly stored in a VectorDB\n(which will be covered in the next section), they become the non-\nparametric knowledge against which we can measure the distance of a\nuser’s query.\nTo get started with embedding, you will need an embedding model.\nThen, LangChain offers the Embedding class with two main modules,\nwhich address the embedding of, respectively, the non-parametric\nknowledge (multiple input text) and the user query (single input text).\nFor example, let’s consider the embeddings using the OpenAI\nembedding model text-embedding-ada-002 (for more details about\nOpenAI embedding models, you can refer to the official\ndocumentation at\nhttps://platform.openai.com/docs/guides/embeddi\nngs/what-are-embeddings):\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom dotenv import load_dotenv\nload_dotenv()\nos.environ[\"OPENAI_API_KEY\"]\nembeddings_model = OpenAIEmbeddings(model ='text-embedding\nembeddings = embeddings_model.embed_documents(\n    [\n        \"Good morning!\",\n        \"Oh, hello!\",\n        \"I want to report an accident\",\n        \"Sorry to hear that. May I ask your name?\",\n        \"Sure, Mario Rossi.\"\n    ]\n)\nprint(\"Embed documents:\")\nprint(f\"Number of vector: {len(embeddings)}; Dimension of e\nembedded_query = embeddings_model.embed_query(\"What was the\n",
      "content_length": 1368,
      "extraction_method": "Direct"
    },
    {
      "page_number": 184,
      "chapter": null,
      "content": "print(\"Embed query:\")\nprint(f\"Dimension of the vector: {len(embedded_query)}\")\nprint(f\"Sample of the first 5 elements of the vector: {embe\nHere is the output:\nEmbed documents:\nNumber of vector: 5; Dimension of each vector: 1536\nEmbed query:\nDimension of the vector: 1536\nSample of the first 5 elements of the vector: [0.0053872121\nOnce we have both documents and the query embedded, the next step\nwill be to compute the similarity between the two elements and\nretrieve the most suitable information from the document embedding.\nWe will see the details of this when talking about vector stores.\nVector stores: A vector store (or VectorDB) is a type of database that\ncan store and search over unstructured data, such as text, images, audio,\nor video, by using embeddings. By using embeddings, vector stores can\nperform a fast and accurate similarity search, which means finding the\nmost relevant data for a given query.\nDefinition\nSimilarity is a measure of how close or related two\nvectors are in a vector space. In the context of LLMs,\nvectors are numerical representations of sentences,\nwords, or documents that capture their semantic\nmeaning, and the distance between those vectors\nshould be representative of their semantic similarity.\n",
      "content_length": 1239,
      "extraction_method": "Direct"
    },
    {
      "page_number": 185,
      "chapter": null,
      "content": "There are different ways to measure similarity between\nvectors, and while working with LLMs, one of the\nmost popular measures in use is cosine similarity.\nThis is the cosine of the angle between two vectors in\na multidimensional space. It is computed as the dot\nproduct of the vectors divided by the product of their\nlengths. Cosine similarity is insensitive to scale and\nlocation, and it ranges from -1 to 1, where 1 means\nidentical, 0 means orthogonal, and -1 means opposite.\nThe following is an illustration of the typical flow while using a vector\nstore.\nFigure 5.3: Sample architecture of a vector store (source:\nhttps://python.langchain.com/docs/modules/data_connection/vectorstor\nes/)\nLangChain offers more than 40 integrations with third-party vector\nstores. Some examples are Facebook AI Similarity Search (FAISS),\nElasticsearch, MongoDB Atlas, and Azure Search. For an exhaustive\nlist and descriptions of all the integrations, you can check the official\ndocumentation at\n",
      "content_length": 981,
      "extraction_method": "Direct"
    },
    {
      "page_number": 186,
      "chapter": null,
      "content": "https://python.langchain.com/docs/integrations/\nvectorstores/.\nAs an example, let’s leverage the FAISS vector store, which has been\ndeveloped by Meta AI research for efficient similarity search and\nclustering of dense vectors. We are going to leverage the same\ndialogue.txt file saved in the previous section:\nfrom langchain.document_loaders import TextLoader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nfrom dotenv import load_dotenv\nload_dotenv()\nos.environ[\"OPENAI_API_KEY\"]\n# Load the document, split it into chunks, embed each chunk\nraw_documents = TextLoader('dialogue.txt').load()\ntext_splitter = CharacterTextSplitter(chunk_size=50, chunk_\ndocuments = text_splitter.split_documents(raw_documents)\ndb = FAISS.from_documents(documents, OpenAIEmbeddings())\nNow that we’ve embedded and saved the non-parametric knowledge,\nlet’s also embed a user’s query so that it can be used to search the\nmost similar text chunk using cosine similarity as a measure:\nquery = \"What is the reason for calling?\"\ndocs = db.similarity_search(query)\nprint(docs[0].page_content)\nThe following is the output:\nI want to report an accident\n",
      "content_length": 1232,
      "extraction_method": "Direct"
    },
    {
      "page_number": 187,
      "chapter": null,
      "content": "As you can see, the output is the piece of text that is more likely to\ncontain the answer to the question. In an end-to-end scenario, it will be\nused as context to the LLM to generate a conversational response.\nRetrievers: A retriever is a component in LangChain that can return\ndocuments relevant to an unstructured query, such as a natural language\nquestion or a keyword. A retriever does not need to store the documents\nitself, but only to retrieve them from a source. A retriever can use\ndifferent methods to find relevant documents, such as keyword\nmatching, semantic search, or ranking algorithms.\nThe difference between a retriever and a vector store is that a retriever\nis more general and flexible than a vector store. A retriever can use\nany method to find relevant documents, while a vector store relies on\nembeddings and similarity metrics. A retriever can also use different\nsources of documents, such as web pages, databases, or files, while a\nvector store needs to store the data itself.\nHowever, a vector store can also be used as the backbone of a retriever\nif the data is embedded and indexed by a vector store. In that case, the\nretriever can use the vector store to perform a similarity search over\nthe embedded data and return the most relevant documents. This is\none of the main types of retrievers in LangChain, and it is called a\nvector store retriever.\nFor example, let’s consider the FAISS vector store we previously\ninitialized and “mount” a retriever on top of that:\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\nretriever = db.as_retriever()\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\n",
      "content_length": 1659,
      "extraction_method": "Direct"
    },
    {
      "page_number": 188,
      "chapter": null,
      "content": "query = \"What was the reason of the call?\"\nqa.run(query)\nHere is the output:\n' The reason for the call was to report an accident.'\nOverall, data connection modules offer a plethora of integrations and pre-\nbuilt templates that make it easier to manage the flow of your LLM-powered\napplication. We will see some concrete applications of these building blocks\nin the upcoming chapters, but in the next section, we are going to take a deep\ndive into another one of LangChain’s main components.\nMemory\nIn the context of LLM-powered applications, memory allows the application\nto keep references to user interactions, both in the short and long term. For\nexample, let’s consider the well-known ChatGPT. While interacting with the\napplication, you have the possibility to ask follow-up questions referencing\nprevious interactions without explicitly telling the model.\nPlus, all conversations are saved into threads, so that, if you want to follow\nup on a previous conversation, you can re-open the thread without providing\nChatGPT with all the contexts. This is made possible thanks to ChatGPT’s\nability to store users’ interactions into a memory variable and use this\nmemory as context while addressing follow-up questions.\nLangChain offers several modules for designing your memory system within\nyour applications, enabling it with both reading and writing skills.\n",
      "content_length": 1361,
      "extraction_method": "Direct"
    },
    {
      "page_number": 189,
      "chapter": null,
      "content": "The first step to do with your memory system is to actually store your human\ninteractions somewhere. To do so, you can leverage numerous built-in\nmemory integrations with third-party providers, including Redis, Cassandra,\nand Postgres.\nThen, when it comes to defining how to query your memory system, there\nare various memory types you can leverage:\nConversation buffer memory: This is the “plain vanilla” memory type\navailable in LangChain. It allows you to store your chat messages and\nextract them in a variable.\nConversation buffer window memory: It is identical to the previous\none, with the only difference being allowing a sliding window over\nonly K interactions so that you can manage longer chat history over\ntime.\nEntity memory: Entity memory is a feature of LangChain that allows\nthe language model to remember given facts about specific entities in a\nconversation. An entity is a person, place, thing, or concept that can be\nidentified and distinguished from others. For example, in the sentence\n“Deven and Sam are working on a hackathon in Italy,” Deven and Sam\nare entities (person), as well as hackathon (thing) and Italy (place).\nEntity memory works by extracting information on entities from the\ninput text using an LLM. It then builds up its knowledge about that\nentity over time by storing the extracted facts in a memory store. The\nmemory store can be accessed and updated by the language model\nwhenever it needs to recall or learn new information about an entity.\nConversation knowledge graph memory: This type of memory uses a\nknowledge graph to recreate memory.\n",
      "content_length": 1585,
      "extraction_method": "Direct"
    },
    {
      "page_number": 190,
      "chapter": null,
      "content": "Definition\nA knowledge graph is a way of representing and\norganizing knowledge in a graph structure, where nodes\nare entities and edges are relationships between them. A\nknowledge graph can store and integrate data from\nvarious sources, and encode the semantics and context\nof the data. A knowledge graph can also support various\ntasks, such as search, question answering, reasoning,\nand generation.\nAnother example of a knowledge graph is DBpedia,\nwhich is a community project that extracts structured\ndata from Wikipedia and makes it available on the web.\nDBpedia covers topics such as geography, music,\nsports, and films, and provides links to other datasets\nlike GeoNames and WordNet.\nYou can use this type of memory to save the input and output of each\nconversation turn as knowledge triplets (such as subject, predicate,\nand object) and then use them to generate relevant and consistent\nresponses based on the current context. You can also query the\nknowledge graph to get the current entities or the history of the\nconversation.\nConversation summary memory: When it comes to longer\nconversations to be stored, this type of memory can be very useful,\nsince it creates a summary of the conversation over time (leveraging an\nLLM).\n",
      "content_length": 1235,
      "extraction_method": "Direct"
    },
    {
      "page_number": 191,
      "chapter": null,
      "content": "Conversation summary buffer memory: This type of memory\ncombines the ideas behind buffer memory and conversation summary\nmemory. It keeps a buffer of recent interactions in memory, but rather\nthan just completely flushing old interactions (as occurs for the\nconversation buffer memory) it compiles them into a summary and uses\nboth.\nConversation token buffer memory: It is similar to the previous one,\nwith the difference that, to determine when to start summarizing the\ninteractions, this type of memory uses token lengths rather than the\nnumber of interactions (as occurs in summary buffer memory).\nVector store-backed memory: This type of memory leverages the\nconcepts of embeddings and vector stores previously covered. It is\ndifferent from all the previous memories since it stores interactions as\nvectors, and then retrieves the top K most similar texts every time it is\nqueried, using a retriever.\nLangChain provides specific modules for each of those memory types. Let’s\nconsider an example with the conversation summary memory, where we will\nalso need an LLM to generate the summary of the interactions:\nfrom langchain.memory import ConversationSummaryMemory, ChatMess\nfrom langchain.llms import OpenAI\nmemory = ConversationSummaryMemory(llm=OpenAI(temperature=0))\nmemory.save_context({\"input\": \"hi, I'm looking for some ideas to\nmemory.load_memory_variables({})\nHere is the output:\n{'history': '\\nThe human asked for ideas to write an essay in AI \n",
      "content_length": 1458,
      "extraction_method": "Direct"
    },
    {
      "page_number": 192,
      "chapter": null,
      "content": "As you can see, the memory summarized the conversation, leveraging the\nOpenAI LLM we initialized.\nThere is no recipe to define which memory to use within your applications;\nhowever, there are some scenarios that might be particularly suitable for\nspecific memories. For example, a knowledge graph memory is useful for\napplications that need to access information from a large and diverse corpus\nof data and generate responses based on semantic relationships, while a\nconversation summary buffer memory could be suitable for creating\nconversational agents that can maintain a coherent and consistent context\nover multiple turns, while also being able to compress and summarize the\nprevious dialogue history.\nChains\nChains are predetermined sequences of actions and calls to LLMs that make\nit easier to build complex applications that require combining LLMs with\neach other or with other components.\nLangChain offers four main types of chain to get started with:\nLLMChain: This is the most common type of chain. It consists of a\nprompt template, an LLM, and an optional output parser.\nDefinition\nAn output parser is a component that helps structure\nlanguage model responses. It is a class that implements\ntwo main methods: get_format_instructions and\nparse. The get_format_instructions method returns\na string containing instructions for how the output of a\n",
      "content_length": 1356,
      "extraction_method": "Direct"
    },
    {
      "page_number": 193,
      "chapter": null,
      "content": "language model should be formatted. The parse\nmethod takes in a string (assumed to be the response\nfrom a language model) and parses it into some\nstructure, such as a dictionary, a list, or a custom\nobject.\nThis chain takes multiple input variables, uses PromptTemplate to\nformat them into a prompt, passes it to the model, and then uses\nOutputParser (if provided) to parse the output of the LLM into a final\nformat.\nFor example, let’s retrieve the prompt template we built in the\nprevious section:\nfrom langchain import PromptTemplate\ntemplate = \"\"\"Sentence: {sentence}\nTranslation in {language}:\"\"\"\nprompt = PromptTemplate(template=template, input_variables=\nNow, let’s put it into an LLMChain:\nfrom langchain import OpenAI, LLMChain\nllm = OpenAI(temperature=0)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nllm_chain.predict(sentence=\"the cat is on the table\", langu\nHere is the output:\n' El gato está en la mesa.'\n",
      "content_length": 917,
      "extraction_method": "Direct"
    },
    {
      "page_number": 194,
      "chapter": null,
      "content": "RouterChain: This is a type of chain that allows you to route the input\nvariables to different chains based on some conditions. You can specify\nthe conditions as functions or expressions that return a Boolean value.\nYou can also specify the default chain to use if none of the conditions\nare met.\nFor example, you can use this chain to create a chatbot that can handle\ndifferent types of requests, such as planning an itinerary or booking a\nrestaurant reservation. To achieve this goal, you might want to\ndifferentiate two different prompts, depending on the type of query the\nuser will make:\nitinerary_template = \"\"\"You are a vacation itinerary assist\nYou help customers finding the best destinations and itine\nYou help customer screating an optimized itinerary based on\nHere is a question:\n{input}\"\"\"\nrestaurant_template = \"\"\"You are a restaurant booking assis\nYou check with customers number of guests and food preferen\nYou pay attention whether there are special conditions to t\nHere is a question:\n{input}\"\"\"\nThanks to RouterChain, we can build a chain that is able to activate a\ndifferent prompt depending on the user’s query. I won’t post the whole\ncode here (you can find the notebook on the book’s GitHub at\nhttps://github.com/PacktPublishing/Building-\nLLM-Powered-Applications), but you can see a sample output\nof how the chain reacts to two different user’s queries:\nprint(chain.run(\"I'm planning a trip from Milan to Venice b\n",
      "content_length": 1438,
      "extraction_method": "Direct"
    },
    {
      "page_number": 195,
      "chapter": null,
      "content": "Here is the output:\n> Entering new MultiPromptChain chain...\nitinerary: {'input': \"I'm planning a trip from Milan to Ven\n> Finished chain.\nAnswer:\nThere are many attractions that you can visit while traveli\nHere it is with a second query:\nprint(chain.run(\"I want to book a table for tonight\"))\nHere is the output:\n> Entering new MultiPromptChain chain...\nrestaurant: {'input': 'I want to book a table for tonight'}\n> Finished chain.\n. How many people are in your party?\nHi there! How many people are in your party for tonight's \nSequentialChain: This is a type of chain that allows you to execute\nmultiple chains in a sequence. You can specify the order of the chains\nand how they pass their outputs to the next chain. The simplest module\nof a sequential chain, takes by default the output of one chain as the\ninput of the next chain. However, you can also use a more complex\nmodule to have more flexibility to set input and output among chains.\nAs an example, let’s consider an AI system that is meant to first\ngenerate a joke on a given topic, and then translate it in to another\nlanguage. To do so, we will first create two chains:\n",
      "content_length": 1135,
      "extraction_method": "Direct"
    },
    {
      "page_number": 196,
      "chapter": null,
      "content": "from langchain.llms import OpenAI\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\nllm = OpenAI(temperature=.7)\ntemplate = \"\"\"You are a comedian. Generate a joke on the fo\nJoke:\"\"\"\nprompt_template = PromptTemplate(input_variables=[\"topic\"]\njoke_chain = LLMChain(llm=llm, prompt=prompt_template)\ntemplate = \"\"\"You are translator. Given a text input, trans\nTranslation:\"\"\"\n.prompt_template = PromptTemplate(input_variables=[\"languag\ntranslator_chain = LLMChain(llm=llm, prompt=prompt_template\nNow, let’s combine them using the SimpleSequentialChain module:\n# This is the overall chain where we run these two chains i\nfrom langchain.chains import SimpleSequentialChain\noverall_chain = SimpleSequentialChain(chains=[joke_chain, t\ntranslated_joke = overall_chain.run(\"Cats and Dogs\")\nHere is the output:\n> Entering new SimpleSequentialChain chain...\nWhy did the cat cross the road? To prove to the dog that it\n ¿Por qué cruzó el gato la carretera? ¡Para demostrarle al \n> Finished chain.\nTransformationChain: This is a type of chain that allows you to\ntransform the input variables or the output of another chain using some\nfunctions or expressions. You can specify the transformation as a\nfunction that takes the input or output as an argument and returns a new\nvalue, as well as specify the output format of the chain.\n",
      "content_length": 1350,
      "extraction_method": "Direct"
    },
    {
      "page_number": 197,
      "chapter": null,
      "content": "For example, let’s say we want to summarize a text, but before that,\nwe want to rename one of the protagonists of the story (a cat) as\n“Silvester the Cat.” As a sample text, I asked Bing Chat to generate a\nstory about cats and dogs (you can find the whole .txt file in the\nGitHub repository of this book):\nfrom langchain.chains import TransformChain, LLMChain, Simp\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\ntransform_chain = TransformChain(\n    input_variables=[\"text\"], output_variables=[\"output_tex\n)\ntemplate = \"\"\"Summarize this text:\n{output_text}\nSummary:\"\"\"\nprompt = PromptTemplate(input_variables=[\"output_text\"], te\nllm_chain = LLMChain(llm=OpenAI(), prompt=prompt)\nsequential_chain = SimpleSequentialChain(chains=[transform_\nsequential_chain.run(cats_and_dogs)\nAs you can see, we’ve combined a simple sequential chain with a\ntransformation chain, where we set as a transformation function the\nrename_cat function (you can see the whole code in the GitHub\nrepository).\nThe output is the following:\n\" Silvester the Cat and a dog lived together but did not ge\nOverall, LangChain chains are a powerful way to combine different\nlanguage models and tasks into a single workflow. Chains are flexible,\nscalable, and easy to use, and they enable users to leverage the power of\n",
      "content_length": 1315,
      "extraction_method": "Direct"
    },
    {
      "page_number": 198,
      "chapter": null,
      "content": "language models for various purposes and domains. Starting from the next\nchapter, we are going to see chains in action in concrete use cases, but before\ngetting there, we need to cover the last component of LangChain: agents.\nAgents\nAgents are entities that drive decision-making within LLM-powered\napplications. They have access to a suite of tools and can decide which tool\nto call based on the user input and the context. Agents are dynamic and\nadaptive, meaning that they can change or adjust their actions based on the\nsituation or the goal: in fact, while in a chain, the sequence of actions is\nhardcoded, in agents, the LLM is used as the reasoning engine with the goal\nof planning and executing the right actions in the right order.\nA core concept while talking about agents is that of tools. In fact, an agent\nmight be good at planning all the right actions to fulfill a user’s query, but\nwhat if it cannot actually execute them, since it is missing information or\nexecutive power? For example, imagine I want to build an agent that is\ncapable of answering my questions by searching the web. By itself, the agent\nhas no access to the web, so I need to provide it with this tool. I will do so by\nusing SerpApi (the Google Search API) integration provided by LangChain\n(you can retrieve your API key at\nhttps://serpapi.com/dashboard).\nLet’s see it in Python:\nfrom langchain import SerpAPIWrapper\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain.llms import OpenAI\nfrom langchain.tools import BaseTool, StructuredTool, Tool, tool\nimport os\nfrom dotenv import load_dotenv\n",
      "content_length": 1600,
      "extraction_method": "Direct"
    },
    {
      "page_number": 199,
      "chapter": null,
      "content": "load_dotenv()\nos.environ[\"SERPAPI_API_KEY\"]\nsearch = SerpAPIWrapper()\ntools = [Tool.from_function(\n        func=search.run,\n        name=\"Search\",\n        description=\"useful for when you need to answer question\n    )]\nagent = initialize_agent(tools, llm = OpenAI(), agent=AgentType.\nagent.run(\"When was Avatar 2 released?\")\nThe following is the output:\n> Entering new AgentExecutor chain...\n I need to find out when Avatar 2 was released.\nAction: Search\nAction Input: \"Avatar 2 release date\"\nObservation: December 16, 2022\nThought: I now know the final answer.\nFinal Answer: Avatar 2 was released on December 16, 2022.\n> Finished chain.\n'Avatar 2 was released on December 16, 2022.'\nNote that, while initializing my agent, I set the agent type as\nZERO_SHOT_REACT_DESCRIPTION. This is one of the configurations we can pick\nand, specifically, it configures the agent to decide which tool to pick based\nsolely on the tool’s description with a ReAct approach:\nDefinition\nThe ReAct approach is a way of using LLMs to solve\nvarious language reasoning and decision-making tasks. It\nwas introduced in the paper ReAct: Synergizing Reasoning\n",
      "content_length": 1133,
      "extraction_method": "Direct"
    },
    {
      "page_number": 200,
      "chapter": null,
      "content": "and Acting in Language Models by Shunyu Yao et al., back\nin October 2022.\nThe ReAct approach prompts LLMs to generate both verbal\nreasoning traces and text actions in an interleaved manner,\nallowing for greater synergy between the two. Reasoning\ntraces help the model to plan, track, and update its actions, as\nwell as handle exceptions. Actions allow the model to\ninteract with external sources, such as knowledge bases or\nenvironments, to gather additional information.\nOn top of this configuration, LangChain also offers the following types of\nagents:\nStructured input ReAct: This is an agent type that uses the ReAct\nframework to generate natural language responses based on structured\ninput data. The agent can handle different types of input data, such as\ntables, lists, or key-value pairs. The agent uses a language model and a\nprompt to generate responses that are informative, concise, and\ncoherent.\nOpenAI Functions: This is an agent type that uses the OpenAI\nFunctions API to access various language models and tools from\nOpenAI. The agent can use different functions, such as GPT-3, Codex,\nDALL-E, CLIP, or ImageGPT. The agent uses a language model and a\nprompt to generate requests to the OpenAI Functions API and parse the\nresponses.\nConversational: This is an agent type that uses a language model to\nengage in natural language conversations with the user. The agent can\nhandle different types of conversational tasks, such as chit-chat,\nquestion answering, or task completion. The agent uses a language\n",
      "content_length": 1519,
      "extraction_method": "Direct"
    },
    {
      "page_number": 201,
      "chapter": null,
      "content": "model and a prompt to generate responses that are relevant, fluent, and\nengaging.\nSelf ask with search: This is an agent type that uses a language model\nto generate questions for itself and then search for answers on the web.\nThe agent can use this technique to learn new information or test its\nown knowledge.\nReAct document store: This is an agent type that uses the ReAct\nframework to generate natural language responses based on documents\nstored in a database. The agent can handle different types of documents,\nsuch as news articles, blog posts, or research papers.\nPlan-and-execute agents: This is an experimental agent type that uses\na language model to choose a sequence of actions to take based on the\nuser’s input and a goal. The agent can use different tools or models to\nexecute the actions it chooses. The agent uses a language model and a\nprompt to generate plans and actions and then uses AgentExecutor to\nrun them.\nLangChain agents are pivotal whenever you want to let your LLMs interact\nwith the external world. Plus, it is interesting to see how agents leverage\nLLMs not only to retrieve and generate responses, but also as reasoning\nengines to plan an optimized sequence of actions.\nTogether with all the LangChain components covered in this section, agents\ncan be the core of LLM-powered applications, as we will see in the next\nchapters. In the next section, we are going to shift toward the world of open-\nsource LLMs, introducing the Hugging Face Hub and its native integration\nwith LangChain.\n",
      "content_length": 1517,
      "extraction_method": "Direct"
    },
    {
      "page_number": 202,
      "chapter": null,
      "content": "Working with LLMs via the\nHugging Face Hub\nNow that we are familiar with LangChain components, it is time to start\nusing our LLMs. If you want to use open-source LLMs, leveraging the\nHugging Face Hub integration is extremely versatile. In fact, with just one\naccess token you can leverage all the open-source LLMs available in\nHugging Face’s repositories.\nAs it is a non-production scenario, I will be using the free Inference API;\nhowever, if you are meant to build production-ready applications, you can\neasily scale to the Inference Endpoint, which grants you a dedicated and\nfully managed infrastructure to host and consume your LLMs.\nSo, let’s see how to start integrating LangChain with the Hugging Face Hub.\nCreate a Hugging Face user\naccess token\nTo access the free Inference API, you will need a user access token, the\ncredential that allows you to run the service. The following are the steps to\nactivate the user access token:\n1. Create a Hugging Face account: You can create a Hugging Face\naccount for free at https://huggingface.co/join.\n2. Retrieve your user access token: Once you have your account, go to\nthe upper-right corner of your profile and go to Settings | Access\nTokens. From that tab, you will be able to copy your secret token and\nuse it to access Hugging Face models.\n",
      "content_length": 1296,
      "extraction_method": "Direct"
    },
    {
      "page_number": 203,
      "chapter": null,
      "content": "Figure 5.4: Retrieving access tokens from the Hugging Face account (source:\nhttps://huggingface.co/settings/tokens)\n3. Set permissions: Access tokens enable users, applications, and\nnotebooks to perform specific actions based on their assigned roles.\nThere are two available roles:\nRead: This allows tokens to provide read access to repositories\nyou have permission to read. This includes public and private\nrepositories owned by you or your organization. This role is\nsuitable for tasks like downloading private models or inference.\nWrite: In addition to read access, tokens with this role grant\nwrite access to repositories where you have writing privileges.\nThis token is useful for activities like training models or updating\nmodel cards.\nIn our series of use cases, we will keep a write permission on our\ntoken.\n",
      "content_length": 817,
      "extraction_method": "Direct"
    },
    {
      "page_number": 204,
      "chapter": null,
      "content": "4. Managing your user access token: Within your profile, you can create\nand manage multiple access tokens, so that you can also differentiate\npermissions. To create a new token, you can click on the New token\nbutton:\nFigure 5.5: Creating a new token\n5. Finally, at any time, you can delete or refresh your token under the\nManage button:\n",
      "content_length": 337,
      "extraction_method": "Direct"
    },
    {
      "page_number": 205,
      "chapter": null,
      "content": "Figure 5.6: Managing tokens\nIt is important not to leak your token, and a good practice is to\nperiodically regenerate it.\nStoring your secrets in an .env\nfile\nWith our user access token generated in the previous section, we have the\nfirst secret to be managed.\nDefinition\n",
      "content_length": 272,
      "extraction_method": "Direct"
    },
    {
      "page_number": 206,
      "chapter": null,
      "content": "Secrets are data that needs to be protected from unauthorized\naccess, such as passwords, tokens, keys, and credentials.\nSecrets are used to authenticate and authorize requests to API\nendpoints, as well as to encrypt and decrypt sensitive data.\nThroughout this hands-on portion of the book, we will keep all our secrets\nwithin an .env file.\nStoring Python secrets in an .env file is a common practice to enhance\nsecurity and maintainability in projects. To do this, create a file named .env\nin your project directory and list your sensitive information as key-value\npairs: in our scenario, we will have\nHUGGINGFACEHUB_API_TOKEN=\"your_user_access_token\". This file should be\nadded to your project’s .gitignore to prevent accidental exposure.\nTo access these secrets in your Python code, use the python-dotenv library\nto load the .env file’s values as environment variables. You can easily install\nit in your terminal via pip install python-dotenv.\nThis approach keeps sensitive data separate from your code base and helps\nensure that confidential information remains confidential throughout the\ndevelopment and deployment processes.\nHere, you can see an example of how to retrieve your access token and set it\nas an environmental variable:\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
      "content_length": 1332,
      "extraction_method": "Direct"
    },
    {
      "page_number": 207,
      "chapter": null,
      "content": "Note that, by default, load_dotenv will look for the .env file in the current\nworking directory; however, you can also specify the path to your secrets\nfile:\nfrom dotenv import load_dotenv\nfrom pathlib import Path\ndotenv_path = Path('path/to/.env')\nload_dotenv(dotenv_path=dotenv_path)\nNow that we have all the ingredients to start coding, it is time to try out\nsome open-source LLMs.\nStart using open-source LLMs\nThe nice thing about the Hugging Face Hub integration is that you can\nnavigate its portal and decide, within the model catalog, what to use. Models\nare also clustered per category (Computer Vision, Natural Language\nProcessing, Audio, and so on) and, within each category, per capability\n(within Natural Language Processing, we have summarization,\nclassification, Q&A, and so on), as shown in the following screenshot:\n",
      "content_length": 832,
      "extraction_method": "Direct"
    },
    {
      "page_number": 208,
      "chapter": null,
      "content": "Figure 5.7: Home page of Hugging Face’s model catalog\nSince we are interested in LLMs, we will focus on the text generation\ncategory. For this first experiment, let’s try Falcon LLM-7B:\nfrom langchain import HuggingFaceHub\nrepo_id = \"tiiuae/falcon-7b-instruct\" \nllm = HuggingFaceHub(\n    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_leng\n)\nprint(llm(\"what was the first disney movie?\"))\nHere is the corresponding output:\nThe first Disney movie was 'Snow White and the Seven Dwarfs'\n",
      "content_length": 493,
      "extraction_method": "Direct"
    },
    {
      "page_number": 209,
      "chapter": null,
      "content": "As you can see, with just a few lines of code, we integrated an LLM from\nthe Hugging Face Hub. With analogous code, you can test and consume all\nthe LLMs available in the Hub.\nNote that, throughout this book, we will be leveraging specific models for\neach application, both proprietary and open source. However, the idea is that\nyou can use the model you prefer by simply initializing it as the main LLM\nand running the code as it is, simply changing the LangChain LLM\nintegration. This is one of the main advantages of LLM-powered\napplications since you don’t have to change the whole code to adapt to\ndifferent LLMs.\nSummary\nIn this chapter, we dove deeper into the fundamentals of LangChain, since it\nwill be the AI orchestrator used in the upcoming chapters: we got familiar\nwith LangChain components such as memory, agents, chains, and prompt\ntemplates. We also covered how to start integrating LangChain with the\nHugging Face Hub and its model catalog, and how to use the available\nLLMs and start embedding them into your code.\nFrom now on, we will look at a series of concrete end-to-end use cases,\nstarting from a semantic Q&A search app, which we are going to develop in\nthe next chapter.\nReferences\nLangChain’s integration with OpenAI –\nhttps://python.langchain.com/docs/integrations/l\nlms/openai\n",
      "content_length": 1307,
      "extraction_method": "Direct"
    },
    {
      "page_number": 210,
      "chapter": null,
      "content": "LangChain’s prompt templates –\nhttps://python.langchain.com/docs/modules/model_\nio/prompts/prompt_templates/\nLangChain’s vector stores –\nhttps://python.langchain.com/docs/integrations/v\nectorstores/\nFAISS index – https://faiss.ai/\nLangChain’s chains –\nhttps://python.langchain.com/docs/modules/chains\n/\nReAct approach – https://arxiv.org/abs/2210.03629\nLangChain’s agents –\nhttps://python.langchain.com/docs/modules/agents\n/agent_types/\nHugging Face documentation –\nhttps://huggingface.co/docs\nLangChain Expression Language (LCEL) –\nhttps://python.langchain.com/docs/expression_lan\nguage/\nLangChain stable version –\nhttps://blog.langchain.dev/langchain-v0-1-0/\nJoin our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\n",
      "content_length": 777,
      "extraction_method": "Direct"
    },
    {
      "page_number": 211,
      "chapter": null,
      "content": "https://packt.link/llm\nOceanofPDF.com\n",
      "content_length": 38,
      "extraction_method": "Direct"
    },
    {
      "page_number": 212,
      "chapter": null,
      "content": "6\nBuilding Conversational\nApplications\nWith this chapter, we embark on the hands-on section of this book, with our\nfirst concrete implementation of LLM-powered applications. Throughout\nthis chapter, we will cover a step-by-step implementation of a conversational\napplication, using LangChain and its components, building on the\nknowledge you’ve gained from the previous chapters. By the end of this\nchapter, you will be able to set up your own conversational application\nproject with just a few lines of code.\nWe will cover the following key topics:\nConfiguring the schema of a simple chatbot\nAdding the memory component\nAdding non-parametric knowledge\nAdding tools and making the chatbot “agentic”\nDeveloping the front-end with Streamlit\nTechnical requirements\nTo complete the tasks in this chapter, you will need the following:\nA Hugging Face account and user access token.\n",
      "content_length": 876,
      "extraction_method": "Direct"
    },
    {
      "page_number": 213,
      "chapter": null,
      "content": "An OpenAI account and user access token.\nPython 3.7.1 or a later version.\nPython packages – make sure to have the following Python packages\ninstalled: langchain, python-dotenv, huggingface_hub, streamlit,\nopenai, pypdf, tiktoken, faiss-cpu, and google-search-results.\nThey can be easily installed via pip install in your terminal.\nYou’ll find the code for this chapter in the book’s GitHub repository at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications.\nGetting started with\nconversational applications\nA conversational application is a type of software that can interact with users\nusing natural language. It can be used for various purposes, such as\nproviding information, assistance, entertainment, or transactions. Generally\nspeaking, a conversational application can use different modes of\ncommunication, such as text, voice, graphics, or even touch. A\nconversational application can also use different platforms, such as\nmessaging apps, websites, mobile devices, or smart speakers.\nToday, conversational applications are being taken to the next level thanks to\nLLMs. Let’s look at some of the benefits that they provide:\nNot only do LLMs provide a new level of natural language interactions,\nbut they can also enable applications to perform reasoning based on the\nbest responses, given users’ preferences.\nAs we saw in previous chapters, LLMs can leverage their parametric\nknowledge, but are also enriched with non-parametric knowledge,\n",
      "content_length": 1464,
      "extraction_method": "Direct"
    },
    {
      "page_number": 214,
      "chapter": null,
      "content": "thanks to embeddings and plug-ins.\nFinally, LLMs are also able to keep track of the conversation thanks to\ndifferent types of memory.\nThe following image shows what the architecture of a conversational bot\nmight look like:\nFigure 6.1: Sample architecture of a conversational bot\nThroughout this chapter, we will build from scratch a text conversational\napplication that is able to help users plan their vacations. We will call this\napp GlobeBotter. We will add incremental layers of complexity to make the\napp as enjoyable as possible for the end user.\nSo, let’s start with the basics behind a conversational app architecture.\nCreating a plain vanilla bot\nTo start with, let’s initialize our LLM and set the schema for our bot. The\nschema refers to the type of messages the bot is able to receive. In our case,\nwe will have three types of messages:\n",
      "content_length": 849,
      "extraction_method": "Direct"
    },
    {
      "page_number": 215,
      "chapter": null,
      "content": "System message: The instructions we give the bot so that it behaves as\na travel assistant.\nAI Message: The message generated by the LLM\nHuman Message: The user’s query\nLet’s start with a simple configuration:\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\nfrom langchain.chains import LLMChain, ConversationChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chat_models import ChatOpenAI\nchat = ChatOpenAI()\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant that help\n    HumanMessage(content=\"I'm going to Rome for 2 days, what can\nWe can then save and print the output as follows:\noutput = chat(messages)\nprint(output.content)\nHere is the output:\nIn Rome, there are many famous attractions to visit. Here's an op\nDay 1:\n 1. Start your day by visiting the Colosseum, one of the world's \n 2. Next, explore the nearby Roman Forum, an ancient Roman market\n 3. Afterward, head to the Pantheon, a well-preserved Roman templ\n4. Take a stroll through the historic district of Trastevere, kno\n5. In the evening, visit the Trevi Fountain and toss a coin to en\nDay 2:\n1. Begin your day at Vatican City, the smallest independent state\n",
      "content_length": 1195,
      "extraction_method": "Direct"
    },
    {
      "page_number": 216,
      "chapter": null,
      "content": "2. Explore the Vatican Museums, home to an extensive collection o\n3. Enjoy a leisurely walk along the Tiber River and cross over to\n4. Visit the Spanish Steps, a popular meeting point with a beauti\n5. End your day by exploring the charming neighborhood of Piazza \nRemember to check the opening hours and availability of tickets f\nAs you can see, the model was pretty good at generating an itinerary in\nRome with only one piece of information from our side, the number of days.\nHowever, we might want to keep interacting with the bot, so that we can\nfurther optimize the itinerary, providing more information about our\npreferences and habits. To achieve that, we need to add memory to our bot.\nAdding memory\nAs we’re creating a conversational bot with relatively short messages, in this\nscenario, a ConversationBufferMemory could be suitable. To make the\nconfiguration easier, let’s also initialize a ConversationChain to combine the\nLLM and the memory components.\nLet’s first initialize our memory and chain (I’m keeping verbose = True so\nthat you can see the bot keeping track of previous messages):\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationChain\nmemory = ConversationBufferMemory()\nconversation = ConversationChain(\n    llm=chat, verbose=True, memory=memory\n)\nGreat, now let’s have some interactions with our bot:\nconversation.run(\"Hi there!\")\n",
      "content_length": 1400,
      "extraction_method": "Direct"
    },
    {
      "page_number": 217,
      "chapter": null,
      "content": "The following is the output:\n> Entering new ConversationChain chain...\nPrompt after formatting:\nThe following is a friendly conversation between a human and an A\nCurrent conversation:\nHuman: Hi there!\nAI:\n> Finished chain.\n'Hello! How can I assist you today?'\nNext, we provide the following input:\nconversation.run(\"what is the most iconic place in Rome?\")\nHere is the corresponding output:\n> Entering new ConversationChain chain...\nPrompt after formatting:\nThe following is a friendly conversation between a human and an A\nCurrent conversation:\nHuman: Hi there!\nAI: Hello! How can I assist you today?\nHuman: what is the most iconic place in Rome?\nAI:\n> Finished chain.\n'The most iconic place in Rome is probably the Colosseum. It is a\nAs you can see from the chain, it is keeping track of the previous\ninteractions. Let’s challenge it and ask something related to the previous\ncontext:\nconversation.run(\"What kind of other events?\")\n",
      "content_length": 934,
      "extraction_method": "Direct"
    },
    {
      "page_number": 218,
      "chapter": null,
      "content": "The following is the output that we receive:\n> Entering new ConversationChain chain...\nPrompt after formatting:\nThe following is a friendly conversation between a human and an A\nCurrent conversation:\nHuman: Hi there!\nAI: Hello! How can I assist you today?\nHuman: what is the most iconic place in Rome?\nAI: The most iconic place in Rome is probably the Colosseum. It i\nHuman: What kind of other events?\nAI:\n> Finished chain.\n'Other events that took place at the Colosseum include mock sea b\nThe bot was able to understand that our request was related to its previous\nanswer. We can also retrieve the message history with the\nmemory.load_memory_variables() method (you can see the full output in the\nGitHub repository). Here is a snippet of the output:\n{'history': 'Human: Hi there!\\nAI: Hello! How can I assist you to\nRather than running the conversation.run method at every interaction, I’ve\ncoded a while cycle to make it interactive. The following is a snapshot of\nthe whole conversation (you can find it in the book’s GitHub repository):\nwhile True:\n    query = input('you: ')\n    if query == 'q':\n        break\n    output = conversation({\"input\": query})\n    print('User: ', query)\n    print('AI system: ', output['response'])\n",
      "content_length": 1231,
      "extraction_method": "Direct"
    },
    {
      "page_number": 219,
      "chapter": null,
      "content": "The following is a truncated sample from the output (you can find the whole\noutput in the book’s GitHub repository):\nUser:  hello\nAI system:  Hello! How can I assist you today?\nUser:  I'm planning a 1-day trip in Venice. What should I visit?\nAI system:  That sounds like a wonderful plan! In Venice, there a\n1. St. Mark's Square (Piazza San Marco): [...] Enjoy your trip to\nUser:  thanks! I'm planning to be around also the next day, and I\nAI system:  Certainly! If you enjoy hiking and want to explore th\n1. The Dolomites: [...]\nUser:  which one is closer to Milan?\nAI system:  If you're looking for a hiking destination closer to \nAs you can see, now the AI assistant is capable of keeping track of the whole\nconversation. In the next section, we are going to add yet another layer of\ncomplexity: an external knowledge base.\nAdding non-parametric\nknowledge\nImagine that you also want your GlobeBotter to have access to exclusive\ndocumentation about itineraries that are not part of its parametric\nknowledge.\nTo do so, we can either embed the documentation in a VectorDB or directly\nuse a retriever to do the job. In this case, we will use a vector-store-backed\nretriever using a particular chain, ConversationalRetrievalChain. This type\nof chain leverages a retriever over the provided knowledge base that has the\nchat history, which can be passed as a parameter using the desired type of\nmemory previously seen.\n",
      "content_length": 1415,
      "extraction_method": "Direct"
    },
    {
      "page_number": 220,
      "chapter": null,
      "content": "With this goal in mind, we will use a sample Italy travel guide PDF\ndownloaded from https://www.minube.net/guides/italy.\nThe following Python code shows how to initialize all the ingredients we\nneed, which are:\nDocument Loader: Since the document is in PDF format, we will use\nPyPDFLoader.\nText splitter: We will use a RecursiveCharacterTextSplitter, which\nsplits text by recursively looking at characters to find one that works.\nVector store: We will use the FAISS VectorDB.\nMemory: We will use a ConversationBufferMemory.\nLLMs: We will use the gpt-3.5-turbo model for conversations.\nEmbeddings: We will use the text-embedding-ada-002.\nLet’s take a look at the code:\nfrom langchain.llms import OpenAI\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitt\nfrom langchain.vectorstores import FAISS\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.memory import ConversationBufferMemory\ntext_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1500,\n            chunk_overlap=200\n        )\nraw_documents = PyPDFLoader('italy_travel.pdf').load()\ndocuments = text_splitter.split_documents(raw_documents)\ndb = FAISS.from_documents(documents, OpenAIEmbeddings())\nmemory = ConversationBufferMemory(\n            memory_key='chat_history',\n            return_messages=True\n",
      "content_length": 1466,
      "extraction_method": "Direct"
    },
    {
      "page_number": 221,
      "chapter": null,
      "content": "        )\nllm = ChatOpenAI()\nLet’s now interact with the chain:\nqa_chain = ConversationalRetrievalChain.from_llm(llm, retriever=\nqa_chain.run({'question':'Give me some review about the Pantheon\nThe following is the output (I’m reporting a truncated version. You can see\nthe whole output in the book’s GitHub repository):\n> Entering new StuffDocumentsChain chain...\n> Entering new LLMChain chain...\nPrompt after formatting:\nSystem: Use the following pieces of context to answer the users q\nIf you don't know the answer, just say that you don't know, don't\n----------------\ncafes in the square. The most famous are the Quadri and\nFlorian.\nPiazza San Marco,\nVenice\n4\nHistorical Monuments\nPantheon\nMiskita:\n\"Angelic and non-human design,\" was how\nMichelangelo described the Pantheon 14 centuries after its\nconstruction. The highlights are the gigantic dome, the upper\neye, the sheer size of the place, and the harmony of the\nwhole building. We visited with a Roman guide which is\n...\n> Finished chain.\n'Miskita:\\n\"Angelic and non-human design,\" was how Michelangelo d\n",
      "content_length": 1064,
      "extraction_method": "Direct"
    },
    {
      "page_number": 222,
      "chapter": null,
      "content": "Note that, by default, the ConversationalRetrievalChain uses a prompt\ntemplate called CONDENSE_QUESTION_PROMPT, which merges the last user’s\nquery with the chat history, so that it results as just one query to the retriever.\nIf you want to pass a custom prompt, you can do so using the\ncondense_question_prompt parameter in the\nConversationalRetrievalChain.from_llm module.\nEven though the bot was able to provide an answer based on the\ndocumentation, we still have a limitation. In fact, with such a configuration,\nour GlobeBotter will only look at the provided documentation, but what if\nwe want it to also use its parametric knowledge? For example, we might\nwant the bot to be able to understand whether it could integrate with the\nprovided documentation or simply answer freely. To do so, we need to make\nour GlobeBotter agentic, meaning that we want to leverage the LLM’s\nreasoning capabilities to orchestrate and invoke the available tools without a\nfixed order, but rather following the best approach given the user’s query.\nTo do so, we will use two main components:\ncreate_retriever_tool: This method creates a custom tool that acts as\na retriever for an agent. It will need a database to retrieve from, a name,\nand a short description, so that the model can understand when to use it.\ncreate_conversational_retrieval_agent: This method initializes a\nconversational agent that is configured to work with retrievers and chat\nmodels. It will need an LLM, a list of tools (in our case, the retriever),\nand a memory key to keep track of the previous chat history.\nThe following code illustrates how to initialize the agent:\nfrom langchain.agents.agent_toolkits import create_retriever_too\ntool = create_retriever_tool(\n    db.as_retriever(),\n",
      "content_length": 1747,
      "extraction_method": "Direct"
    },
    {
      "page_number": 223,
      "chapter": null,
      "content": "    \"italy_travel\",\n    \"Searches and returns documents regarding Italy.\"\n)\ntools = [tool]\nmemory = ConversationBufferMemory(\n            memory_key='chat_history',\n            return_messages=True\n        )\nfrom langchain.agents.agent_toolkits import create_conversationa\nfrom langchain.chat_models import ChatOpenAI\nllm = ChatOpenAI(temperature = 0)\nagent_executor = create_conversational_retrieval_agent(llm, tool\nGreat, now let’s see the thought process of the agent with two different\nquestions (I will report only the chain of thoughts and truncate the output,\nbut you can find the whole code in the GitHub repo):\nagent_executor({\"input\": \"Tell me something about Pantheon\"})\nHere is the output:\n> Entering new AgentExecutor chain...\nInvoking: `italy_travel` with `Pantheon`\n[Document(page_content='cafes in the square. The most famous are \n> Finished chain.\nLet’s now try with a question not related to the document:\noutput = agent_executor({\"input\": \"what can I visit in India in \nThe following is the output that we receive:\n",
      "content_length": 1034,
      "extraction_method": "Direct"
    },
    {
      "page_number": 224,
      "chapter": null,
      "content": "> Entering new AgentExecutor chain...\nIn India, there are numerous incredible places to visit, each wit\n1. Delhi: Start your trip in the capital city of India, Delhi. […\n> Finished chain.\nAs you can see, when I asked the agent something about Italy, it\nimmediately invoked the provided document, while this was not done in the\nlast question.\nThe last thing we want to add to our GlobeBotter is the capability to navigate\nthe web, since, as travelers, we want to have up-to-date information about\nthe country we are traveling to. Let’s implement it with LangChain’s tools.\nAdding external tools\nThe tool we are going to add here is the Google SerpApi tool, so that our bot\nwill be able to navigate the internet.\nNote\nSerpApi is a real-time API designed to access Google search\nresults. It simplifies the process of data scraping by handling\ncomplexities such as managing proxies, solving\nCAPTCHAs, and parsing structured data from search engine\nresults pages.\nLangChain offers a pre-built tool that wraps SerpApi to\nmake it easier to integrate it within your agents. To enable\nSerpApi, you need to sign in at\n",
      "content_length": 1108,
      "extraction_method": "Direct"
    },
    {
      "page_number": 225,
      "chapter": null,
      "content": "https://serpapi.com/users/sign_up, then go to\nthe dashboard under the tab API key.\nSince we don’t want our GlobeBotter to be focused only on the web, we will\nadd the SerpApi tool to the previous one, so that the agent will be able to\npick the most useful tool to answer the question – or use no tool if not\nnecessary.\nLet’s initialize our tools and agent (you learned about this and other\nLangChain components in Chapter 5):\nfrom langchain import SerpAPIWrapper\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\nos.environ[\"SERPAPI_API_KEY\"]\nsearch = SerpAPIWrapper()\ntools = [\n    Tool.from_function(\n        func=search.run,\n        name=\"Search\",\n        description=\"useful for when you need to answer question\n    ),\n    create_retriever_tool(\n        db.as_retriever(),\n        \"italy_travel\",\n        \"Searches and returns documents regarding Italy.\"\n    )\n    ]\nagent_executor = create_conversational_retrieval_agent(llm, tool\nGreat, now let’s test it with three different questions (here, again, the output\nhas been truncated):\n“What can I visit in India in 3 days?”\n",
      "content_length": 1081,
      "extraction_method": "Direct"
    },
    {
      "page_number": 226,
      "chapter": null,
      "content": "> Entering new AgentExecutor chain...\nIndia is a vast and diverse country with numerous attr\n1. Delhi: Start your trip in the capital city of India\n> Finished chain.\nIn this case, the model doesn’t need external knowledge to answer the\nquestion, hence it is responding without invoking any tool.\n“What is the weather currently in Delhi?”\n> Entering new AgentExecutor chain...\nInvoking: `Search` with `{'query': 'current weather in\nCurrent Weather · 95°F Mostly sunny · RealFeel® 105°. \n> Finished chain.\nNote how the agent is invoking the search tool; this is due to the\nreasoning capability of the underlying gpt-3.5-turbo model, which\ncaptures the user’s intent and dynamically understands which tool to\nuse to accomplish the request.\n“I’m traveling to Italy. Can you give me some suggestions for the main\nattractions to visit?”\n> Entering new AgentExecutor chain...\nInvoking: `italy_travel` with `{'query': 'main attract\n[Document(page_content='ITALY\\nMINUBE TRAVEL GUIDE\\nTh\nHere are some suggestions for main attractions in Ital\n1. Parco Sempione, Milan: This is one of the most impo\n> Finished chain.\nNote how the agent is invoking the document retriever to provide the\npreceding output.\n",
      "content_length": 1194,
      "extraction_method": "Direct"
    },
    {
      "page_number": 227,
      "chapter": null,
      "content": "Overall, our GlobeBotter is now able to provide up-to-date information, as\nwell as retrieving specific knowledge from curated documentation. The next\nstep will be that of building a front-end. We will do so by building a web app\nusing Streamlit.\nDeveloping the front-end\nwith Streamlit\nStreamlit is a Python library that allows you to create and share web apps. It\nis designed to be easy and fast to use, without requiring any front-end\nexperience or knowledge. You can write your app in pure Python, using\nsimple commands to add widgets, charts, tables, and other elements.\nIn addition to its native capabilities, in July 2023, Streamlit announced an\ninitial integration and its future plans with LangChain. At the core of this\ninitial integration, there is the ambition of making it easier to build a GUI for\nconversational applications, as well as showing all the steps LangChain’s\nagents take before producing the final response.\nTo achieve this goal, the main module that Streamlit introduced is the\nStreamlit callback handler. This module provides a class called\nStreamlitCallbackHandler that implements the BaseCallbackHandler\ninterface from LangChain. This class can handle various events that occur\nduring the execution of a LangChain pipeline, such as tool start, tool end,\ntool error, LLM token, agent action, agent finish, etc.\nThe class can also create and update Streamlit elements, such as containers,\nexpanders, text, progress bars, etc., to display the output of the pipeline in a\nuser-friendly way. You can use the Streamlit callback handler to create\nStreamlit apps that showcase the capabilities of LangChain and interact with\n",
      "content_length": 1647,
      "extraction_method": "Direct"
    },
    {
      "page_number": 228,
      "chapter": null,
      "content": "the user through natural language. For example, you can create an app that\ntakes a user prompt and runs it through an agent that uses different tools and\nmodels to generate a response. You can use the Streamlit callback handler to\nshow the agent’s thought process and the results of each tool in real time.\nTo start building your application, you need to create a .py file to run in\nyour terminal via streamlit run file.py. In our case, the file will be\nnamed globebotter.py.\nThe following are the main building blocks of the application:\n1. Setting the configuration of the webpage:\nimport streamlit as st\nst.set_page_config(page_title=\"GlobeBotter\", page_icon\nst.header('\n Welcome to Globebotter, your travel assi\n2. Initializing the LangChain backbone components we need. The code is\nthe same as the one in the previous section, so I will share here only the\ninitialization code, without all the preliminary steps:\nsearch = SerpAPIWrapper()\ntext_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1500,\n            chunk_overlap=200\n        )\nraw_documents = PyPDFLoader('italy_travel.pdf').load()\ndocuments = text_splitter.split_documents(raw_document\ndb = FAISS.from_documents(documents, OpenAIEmbeddings(\nmemory = ConversationBufferMemory(\n    return_messages=True,\n    memory_key=\"chat_history\",\n    output_key=\"output\"\n)\nllm = ChatOpenAI()\n",
      "content_length": 1363,
      "extraction_method": "Direct"
    },
    {
      "page_number": 229,
      "chapter": null,
      "content": "tools = [\n    Tool.from_function(\n        func=search.run,\n        name=\"Search\",\n        description=\"useful for when you need to answe\n    ),\n    create_retriever_tool(\n        db.as_retriever(),\n        \"italy_travel\",\n        \"Searches and returns documents regarding Ital\n    )\n    ]\nagent = create_conversational_retrieval_agent(llm, too\n3. Setting the input box for the user with a placeholder question:\nuser_query = st.text_input(\n    \"**Where are you planning your next vacation?**\",\n    placeholder=\"Ask me anything!\"\n)\n4. Setting Streamlit’s session states. Session state is a way to share\nvariables between reruns, for each user session. In addition to the\nability to store and persist state, Streamlit also exposes the ability to\nmanipulate state using callbacks. Session state also persists across apps\ninside a multipage app. You can use the session state API to initialize,\nread, update, and delete variables in the session state. In the case of our\nGlobeBotter, we want two main states: messages and memory:\nif \"messages\" not in st.session_state:\n    st.session_state[\"messages\"] = [{\"role\": \"assistan\nif \"memory\" not in st.session_state:\n    st.session_state['memory'] = memory\n",
      "content_length": 1196,
      "extraction_method": "Direct"
    },
    {
      "page_number": 230,
      "chapter": null,
      "content": "5. Making sure to display the whole conversation. To do so, I created a for\nloop that iterates over the list of messages stored in\nst.session_state[\"messages\"]. For each message, it creates a\nStreamlit element called st.chat_message that displays a chat message\nin a nice format:\nfor msg in st.session_state[\"messages\"]:\n    st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n6. Configuring the AI assistant to respond when given a user’s query. In\nthis first example, we will keep the whole chain visible and printed to\nthe screen:\nif user_query:\n    st.session_state.messages.append({\"role\": \"user\", \n    st.chat_message(\"user\").write(user_query)\n    with st.chat_message(\"assistant\"):\n        st_cb = StreamlitCallbackHandler(st.container(\n        response = agent(user_query, callbacks=[st_cb]\n        st.session_state.messages.append({\"role\": \"ass\n        st.write(response)\n7. Finally, adding a button to clear the history of the conversation and start\nfrom scratch:\nif st.sidebar.button(\"Reset chat history\"):\n    st.session_state.messages = []\nThe final product looks as follows:\n",
      "content_length": 1088,
      "extraction_method": "Direct"
    },
    {
      "page_number": 231,
      "chapter": null,
      "content": "Figure 6.2: Front-end of GlobeBotter with Streamlit\nFrom the expander, we can see that the agent used the Search tool (provided\nwith the SerpApi). We can also expand chat_history or\nintermediate_steps as follows:\n",
      "content_length": 213,
      "extraction_method": "Direct"
    },
    {
      "page_number": 232,
      "chapter": null,
      "content": "Figure 6.3: Example of Streamlit expander\nOf course, we can also decide to only show the output rather than the whole\nchain of thoughts, by specifying in the code to return only\nresponse['output']. You can see the whole code in the book’s GitHub\nrepository.\nBefore we wrap up, let’s discuss how you can give your users a streaming\nexperience while interacting with your chatbot. You can leverage the\nBaseCallbackHandler class to create a custom callback handler in your\nStreamlit app:\nfrom langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.schema import ChatMessage\n",
      "content_length": 583,
      "extraction_method": "Direct"
    },
    {
      "page_number": 233,
      "chapter": null,
      "content": "from langchain_openai import ChatOpenAI\nimport streamlit as st\nclass StreamHandler(BaseCallbackHandler):\n    def __init__(self, container, initial_text=\"\"):\n        self.container = container\n        self.text = initial_text\n    def on_llm_new_token(self, token: str, **kwargs) -> None:\n        self.text += token\n        self.container.markdown(self.text)\nThe StreamHandler is designed to capture and display streaming data, such\nas text or other content, in a designated container. Then, you can use it as\nfollows in your Streamlit app, making sure to set streaming=True while\ninitializing your OpenAI LLM.\n with st.chat_message(\"assistant\"):\n        stream_handler = StreamHandler(st.empty())\n        llm = ChatOpenAI(streaming=True, callbacks=[stream_handl\n        response = llm.invoke(st.session_state.messages)\n        st.session_state.messages.append(ChatMessage(role=\"assis\nYou can refer to the original code on LangChain’s GitHub repo at\nhttps://github.com/langchain-ai/streamlit-\nagent/blob/main/streamlit_agent/basic_streaming.py.\nSummary\nIn this chapter, we approached the end-to-end implementation of a\nconversational application, leveraging LangChain’s modules and\nprogressively adding layers of complexity. We started with a plain vanilla\nchatbot with no memory, then moved on to more complex systems with the\nability to keep traces of past interactions. We’ve also seen how to add non-\n",
      "content_length": 1403,
      "extraction_method": "Direct"
    },
    {
      "page_number": 234,
      "chapter": null,
      "content": "parametric knowledge to our application with external tools, making it more\n“agentic” so that it is able to determine which tool to use, depending on the\nuser’s query. Finally, we introduced Streamlit as the front-end framework to\nbuild the web app for our GlobeBotter.\nIn the next chapter, we will focus on a more specific domain where LLMs\nadd value and demonstrate emerging behaviors, that is, recommendation\nsystems.\nReferences\nExample of a context-aware chatbot.\nhttps://github.com/shashankdeshpande/langchain-\nchatbot/blob/master/pages/2_%E2%AD%90_context_aw\nare_chatbot.py\nKnowledge base for the AI travel assistant.\nhttps://www.minube.net/guides/italy\nLangChain repository. https://github.com/langchain-ai\nJoin our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\n",
      "content_length": 853,
      "extraction_method": "Direct"
    },
    {
      "page_number": 235,
      "chapter": null,
      "content": "OceanofPDF.com\n",
      "content_length": 15,
      "extraction_method": "Direct"
    },
    {
      "page_number": 236,
      "chapter": null,
      "content": "7\nSearch and\nRecommendation Engines\nwith LLMs\nIn the previous chapter, we covered the core steps involved in building\nconversational applications. We started with a plain vanilla chatbot, then\nadded more complex components, such as memory, non-parametric\nknowledge, and external tools. All of this was made straightforward with the\npre-built components of LangChain, as well as Streamlit for UI rendering.\nEven though conversational applications are often seen as the “comfort\nzone” of generative AI and LLMs, those models do embrace a wider\nspectrum of applications.\nIn this chapter, we are going to cover how LLMs can enhance\nrecommendation systems, using both embeddings and generative models.\nWe will learn how to create our own recommendation system application\nleveraging state-of-the-art LLMs using LangChain as the framework.\nThroughout this chapter, we will cover the following topics:\nDefinition and evolutions of recommendation systems\nHow LLMs are impacting this field of research\nBuilding recommendation systems with LangChain\n",
      "content_length": 1040,
      "extraction_method": "Direct"
    },
    {
      "page_number": 237,
      "chapter": null,
      "content": "Technical requirements\nTo complete the tasks in this book, you will need the following:\nHugging Face account and a user access token.\nOpenAI account and a user access token.\nPython version 3.7.1 or later.\nMake sure to have the following Python packages installed: langchain,\npython-dotenv, huggingface_hub, streamlit, lancedb, openai, and\ntiktoken. These can be easily installed via pip install in your\nterminal.\nYou’ll find the code for this chapter in the book’s GitHub repository at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications.\nIntroduction to\nrecommendation systems\nA recommendation system is a computer program that recommends items for\nusers of digital platforms such as e-commerce websites and social networks.\nIt uses large datasets to develop models of users’ likes and interests, and then\nrecommends similar items to individual users.\nThere are different types of recommendation systems, depending on the\nmethods and data they use. Some of the common types are:\nCollaborative filtering: This type of recommendation system uses the\nratings or feedback of other users who have similar preferences to the\ntarget user. It assumes that users who liked certain items in the past will\nlike similar items in the future. For example, if user A and user B both\n",
      "content_length": 1287,
      "extraction_method": "Direct"
    },
    {
      "page_number": 238,
      "chapter": null,
      "content": "liked movies X and Y, then the algorithm may recommend movie Z to\nuser A if user B also liked it.\nCollaborative filtering can be further divided into two subtypes: user-\nbased and item-based:\nUser-based collaborative filtering finds similar users to the\ntarget user and recommends items that they liked.\nItem-based collaborative filtering finds similar items to the\nones that the target user liked and recommends them.\nContent-based filtering: This type of recommendation system uses the\nfeatures or attributes of the items themselves to recommend items that\nare similar to the ones that the target user has liked or interacted with\nbefore. It assumes that users who liked certain features of an item will\nlike other items with similar features. The main difference with item-\nbased collaborative filtering is that, while this latter item-based uses\npatterns of user behavior to make recommendations, content-based\nfiltering uses information about the items themselves. For example, if\nuser A liked movie X, which is a comedy with actor Y, then the\nalgorithm may recommend movie Z, which is also a comedy with actor\nY.\nHybrid filtering: This type of recommendation system combines both\ncollaborative and content-based filtering methods to overcome some of\ntheir limitations and provide more accurate and diverse\nrecommendations. For example, YouTube uses hybrid filtering to\nrecommend videos based on both the ratings and views of other users\nwho have watched similar videos, and the features and categories of the\nvideos themselves.\n",
      "content_length": 1534,
      "extraction_method": "Direct"
    },
    {
      "page_number": 239,
      "chapter": null,
      "content": "Knowledge-based filtering: This type of recommendation system uses\nexplicit knowledge or rules about the domain and the user’s needs or\npreferences to recommend items that satisfy certain criteria or\nconstraints. It does not rely on ratings or feedback from other users, but\nrather on the user’s input or query. For example, if user A wants to buy\na laptop with certain specifications and budget, then the algorithm may\nrecommend a laptop that satisfies those criteria. Knowledge-based\nrecommender systems work well when there is no or little rating history\navailable, or when the items are complex and customizable.\nWithin the above frameworks, there are then various machine learning\ntechniques that can be used, which we will cover in the next section.\nExisting recommendation\nsystems\nModern recommendation systems use machine learning (ML) techniques to\nmake better predictions about users’ preferences, based on the available data\nsuch as the following:\nUser behavior data: Insights about user interaction with a product.\nThis data can be acquired from factors like user ratings, clicks, and\npurchase records.\nUser demographic data: This refers to personal information about\nusers, including details like age, educational background, income level,\nand geographical location.\nProduct attribute data: This involves information about the\ncharacteristics of a product, such as genres of books, casts of movies, or\nspecific cuisines in the context of food.\n",
      "content_length": 1457,
      "extraction_method": "Direct"
    },
    {
      "page_number": 240,
      "chapter": null,
      "content": "As of today, some of the most popular ML techniques are K-nearest\nneighbors, dimensionality reduction, and neural networks. Let’s look at these\nmethods in detail.\nK-nearest neighbors\nK-nearest neighbors (KNN) is an ML algorithm that can be used for both\nclassification and regression problems. It works by finding the k closest data\npoints (where k refers to the number of nearest data point you want to find,\nand is set by the user before initializing the algorithm) to a new data point\nand using their labels or values to make a prediction. KNN is based on the\nassumption that similar data points are likely to have similar labels or values.\nKNN can be applied to recommendation systems in the context of\ncollaborative filtering, both user-based and item-based:\nUser-based KNN is a type of collaborative filtering, which uses the\nratings or feedback of other users who have similar tastes or\npreferences to the target user.\nFor example, let’s say we have three users: Alice, Bob, and Charlie.\nThey all buy books online and rate them. Alice and Bob both liked\n(rated highly) the series, Harry Potter, and the book, The Hobbit. The\nsystem sees this pattern and considers Alice and Bob to be similar.\nNow, if Bob also liked the book A Game of Thrones, which Alice\nhasn’t read yet, the system will recommend A Game of Thrones to\nAlice. This is because it assumes that since Alice and Bob have similar\ntastes, Alice might also like A Game of Thrones.\nItem-based KNN is another type of collaborative filtering, which uses\nthe attributes or features of the items to recommend similar items to the\n",
      "content_length": 1592,
      "extraction_method": "Direct"
    },
    {
      "page_number": 241,
      "chapter": null,
      "content": "target user.\nFor example, let’s consider the same users and their ratings for the\nbooks. The system notices that the Harry Potter series and the book,\nThe Hobbit are both liked by Alice and Bob. So, it considers these two\nbooks to be similar.\nNow, if Charlie reads and likes Harry Potter, the system will\nrecommend The Hobbit to Charlie. This is because it assumes that\nsince Harry Potter and The Hobbit are similar (both liked by the same\nusers), Charlie might also like The Hobbit.\nKNN is a popular technique in recommendation systems, but it has some\npitfalls:\nScalability: KNN can become computationally expensive and slow\nwhen dealing with large datasets, as it requires calculating distances\nbetween all pairs of items or users.\nCold-start problem: KNN struggles with new items or users that have\nlimited or no interaction history, as it relies on finding neighbors based\non historical data.\nData sparsity: KNN performance can degrade in sparse datasets where\nthere are many missing values, making it challenging to find\nmeaningful neighbors.\nFeature relevance: KNN treats all features equally and assumes that all\nfeatures contribute equally to similarity calculations. This may not hold\ntrue in scenarios where some features are more relevant than others.\nChoice of K: Selecting the appropriate value of K (number of\nneighbors) can be subjective and impact the quality of\nrecommendations. A small K may result in noise, while a large K may\nlead to overly broad recommendations.\n",
      "content_length": 1486,
      "extraction_method": "Direct"
    },
    {
      "page_number": 242,
      "chapter": null,
      "content": "Generally speaking, KNN is recommended in scenarios with small datasets\nwith minimal noise (so that outliers, missing values and other noises do not\nimpact the distance metric) and dynamic data (KNN is an instance-based\nmethod that doesn’t require retraining and can adapt to changes quickly).\nAdditionally, further techniques are widely used in the file of\nrecommendation systems, such as matrix factorization.\nMatrix factorization\nMatrix factorization is a technique used in recommendation systems to\nanalyze and predict user preferences or behaviors based on historical data. It\ninvolves decomposing a large matrix into two or more smaller matrices to\nuncover latent features that contribute to the observed data patterns and\naddress the so-called “curse of dimensionality.”\nDefinition\nThe curse of dimensionality refers to challenges that arise\nwhen dealing with high-dimensional data. It leads to\nincreased complexity, sparse data, and difficulties in analysis\nand modeling due to the exponential growth of data\nrequirements and potential overfitting.\nIn the context of recommendation systems, this technique is employed to\npredict missing values in the user-item interaction matrix, which represents\nusers’ interactions with various items (such as movies, products, or books).\nLet’s consider the following example. Imagine you have a matrix where\nrows represent users, columns represent movies, and the cells contain ratings\n",
      "content_length": 1431,
      "extraction_method": "Direct"
    },
    {
      "page_number": 243,
      "chapter": null,
      "content": "(from 1 as lowest to 5 as highest). However, not all users have rated all\nmovies, resulting in a matrix with many missing entries:\nMovie 1\nMovie 2\nMovie 3\nMovie 4\nUser 1\n4\n-\n5\n-\nUser 2\n-\n3\n-\n2\nUser 3\n5\n4\n-\n3\nTable 7.1: Example of a dataset with missing data\nMatrix factorization aims to break down this matrix into two matrices: one\nfor users and another for movies, with a reduced number of dimensions\n(latent factors). These latent factors could represent attributes like genre\npreferences or specific movie characteristics. By multiplying these matrices,\nyou can predict the missing ratings and recommend movies that the users\nmight enjoy.\nThere are different algorithms for matrix factorization, including the\nfollowing:\nSingular value decomposition (SVD) decomposes a matrix into three\nseparate matrices, where the middle matrix contains singular values that\nrepresent the importance of different components in the data. It’s\nwidely used in data compression, dimensionality reduction, and\ncollaborative filtering in recommendation systems.\nPrincipal component analysis (PCA) is a technique to reduce the\ndimensionality of data by transforming it into a new coordinate system\n",
      "content_length": 1180,
      "extraction_method": "Direct"
    },
    {
      "page_number": 244,
      "chapter": null,
      "content": "aligned with the principal components. These components capture the\nmost significant variability in the data, allowing efficient analysis and\nvisualization.\nNon-negative matrix factorization (NMF) decomposes a matrix into\ntwo matrices with non-negative values. It’s often used for topic\nmodeling, image processing, and feature extraction, where the\ncomponents represent non-negative attributes.\nIn the context of recommendation systems, probably the most popular\ntechnique is SVD (thanks to its interpretability, flexibility, and ability to\nhandle missing values and performance), so let’s use this one to go on with\nour example. We will use the Python numpy module to apply SVD as\nfollows:\nimport numpy as np\n# Your user-movie rating matrix (replace with your actual data)\nuser_movie_matrix = np.array([\n    [4, 0, 5, 0],\n    [0, 3, 0, 2],\n    [5, 4, 0, 3]\n])\n# Apply SVD\nU, s, V = np.linalg.svd(user_movie_matrix, full_matrices=False)\n# Number of latent factors (you can choose this based on your pr\nnum_latent_factors = 2\n# Reconstruct the original matrix using the selected latent fact\nreconstructed_matrix = U[:, :num_latent_factors] @ np.diag(s[:nu\n# Replace negative values with 0\nreconstructed_matrix = np.maximum(reconstructed_matrix, 0)\nprint(\"Reconstructed Matrix:\")\nprint(reconstructed_matrix)\nThe following is the output:\n",
      "content_length": 1335,
      "extraction_method": "Direct"
    },
    {
      "page_number": 245,
      "chapter": null,
      "content": "Reconstructed Matrix:\n[[4.2972542  0.         4.71897811 0.        ]\n [1.08572801 2.27604748 0.         1.64449028]\n [4.44777253 4.36821972 0.52207171 3.18082082]]\nIn this example, the U matrix contains user-related information, the s matrix\ncontains singular values, and the V matrix contains movie-related\ninformation. By selecting a certain number of latent factors\n(num_latent_factors), you can reconstruct the original matrix with reduced\ndimensions, while setting the full_matrices=False parameter in the\nnp.linalg.svd function ensures that the decomposed matrices are truncated\nto have dimensions consistent with the selected number of latent factors.\nThese predicted ratings can then be used to recommend movies with higher\npredicted ratings to users. Matrix factorization enables recommendation\nsystems to uncover hidden patterns in user preferences and make\npersonalized recommendations based on those patterns.\nMatrix factorization has been a widely used technique in recommendation\nsystems, especially when dealing with large datasets containing a substantial\nnumber of users and items, since it efficiently captures latent factors even in\nsuch scenarios; or when you want personalized recommendations based on\nlatent factors, since it learns unique latent representations for each user and\nitem. However, it has some pitfalls (some similar to the KNN’s technique):\nCold-start problem: Similar to KNN, matrix factorization struggles\nwith new items or users that have limited or no interaction history.\nSince it relies on historical data, it can’t effectively provide\nrecommendations for new items or users.\nData sparsity: As the number of users and items grows, the user-item\ninteraction matrix becomes increasingly sparse, leading to challenges in\n",
      "content_length": 1761,
      "extraction_method": "Direct"
    },
    {
      "page_number": 246,
      "chapter": null,
      "content": "accurately predicting missing values.\nScalability: For large datasets, performing matrix factorization can be\ncomputationally expensive and time-consuming.\nLimited context: Matrix factorization typically only considers user-\nitem interactions, ignoring contextual information like time, location, or\nadditional user attributes.\nHence, neural networks (NNs) have been explored as an alternative to\nmitigate these pitfalls in recent years.\nNeural networks\nNNs are used in recommendation systems to improve the accuracy and\npersonalization of recommendations by learning intricate patterns from data.\nHere’s how neural networks are commonly applied in this context:\nCollaborative filtering with neural networks: Neural networks can\nmodel user-item interactions by embedding users and items into\ncontinuous vector spaces. These embeddings capture latent features that\nrepresent user preferences and item characteristics. Neural collaborative\nfiltering models combine these embeddings with neural network\narchitectures to predict ratings or interactions between users and items.\nContent-based recommendations: In content-based recommendation\nsystems, neural networks can learn representations of item content,\nsuch as text, images, or audio. These representations capture item\ncharacteristics and user preferences. Neural networks like\nconvolutional neural networks (CNNs) and recurrent neural\nnetworks (RNNs) are used to process and learn from item content,\nenabling personalized content-based recommendations.\n",
      "content_length": 1507,
      "extraction_method": "Direct"
    },
    {
      "page_number": 247,
      "chapter": null,
      "content": "Sequential models: In scenarios where user interactions have a\ntemporal sequence, such as clickstreams or browsing history, RNNs or\nvariants such as long short-term memory (LSTM) networks can\ncapture temporal dependencies in the user behavior and make\nsequential recommendations.\nAutoencoders and variational autoencoders (VAEs) can be used to\nlearn low-dimensional representations of users and items.\nDefinition\nAutoencoders are a type of neural network architecture used\nfor unsupervised learning and dimensionality reduction.\nThey consist of an encoder and a decoder. The encoder maps\nthe input data into a lower-dimensional latent space\nrepresentation, while the decoder attempts to reconstruct the\noriginal input data from the encoded representation.\nVAEs are an extension of traditional autoencoders that\nintroduce probabilistic elements. VAEs not only learn to\nencode the input data into a latent space but also model the\ndistribution of this latent space using probabilistic methods.\nThis allows for the generation of new data samples from the\nlearned latent space. VAEs are used for generative tasks like\nimage synthesis, anomaly detection, and data imputation.\nIn both autoencoders and VAEs, the idea is to learn a compressed and\nmeaningful representation of the input data in the latent space, which can be\nuseful for various tasks including feature extraction, data generation, and\ndimensionality reduction.\n",
      "content_length": 1420,
      "extraction_method": "Direct"
    },
    {
      "page_number": 248,
      "chapter": null,
      "content": "These representations can then be used to make recommendations by\nidentifying similar users and items in the latent space. In fact, the unique\narchitecture that features NNs allows for the following techniques:\nSide information integration: NNs can incorporate additional user and\nitem attributes, such as demographic information, location, or social\nconnections, to improve recommendations by learning from diverse\ndata sources.\nDeep reinforcement learning: In certain scenarios, deep reinforcement\nlearning can be used to optimize recommendations over time, learning\nfrom user feedback to suggest actions that maximize long-term rewards.\nNNs offer flexibility and the ability to capture complex patterns in data,\nmaking them well suited for recommendation systems. However, they also\nrequire careful design, training, and tuning to achieve optimal performance.\nNNs also bring their own challenges, including the following:\nIncreased complexity: NNs, especially deep neural networks\n(DNNs), can become incredibly complex due to their layered\narchitecture. As we add more hidden layers and neurons, the model’s\ncapacity to learn intricate patterns increases.\nTraining requirements: NNs are heavy models whose training requires\nspecial hardware requirements including GPUs, which might be very\nexpensive.\nPotential overfitting: Overfitting occurs when an ANN learns to\nperform exceptionally well on the training data but fails to generalize to\nunseen data\nSelecting appropriate architectures, handling large datasets, and tuning\nhyperparameters are essential to effectively use NNs in recommendation\nsystems.\n",
      "content_length": 1608,
      "extraction_method": "Direct"
    },
    {
      "page_number": 249,
      "chapter": null,
      "content": "Even though relevant advancements have been made in recent years, the\naforementioned techniques still suffer from some pitfalls, primarily their\nbeing task-specific. For example, a rating-prediction recommendation\nsystem will not be able to tackle a task where we need to recommend the top\nk items that likely match the user’s taste. Actually, if we extend this\nlimitation to other “pre-LLMs” AI solutions, we might see some similarities:\nit is indeed the task-specific situation that LLMs and, more generally, Large\nFoundation Models are revolutionizing, being highly generalized and\nadaptable to various tasks, depending on user’s prompts and instructions.\nHenceforth, extensive research in the field of recommendation systems is\nbeing done into what extent LLMs can enhance the current models. In the\nfollowing sections, we will cover the theory behind these new approaches\nreferring to recent papers and blogs about this emerging domain.\nHow LLMs are changing\nrecommendation systems\nWe saw in previous chapters how LLMs can be customized in three main\nways: pre-training, fine-tuning, and prompting. According to the paper\nRecommender systems in the Era of Large Language Models (LLMs) from\nWenqi Fan et al., these techniques can also be used to tailor an LLM to be a\nrecommender system:\nPre-training: Pre-training LLMs for recommender systems is an\nimportant step to enable LLMs to acquire extensive world knowledge\nand user preferences, and to adapt to different recommendation tasks\nwith zero or few shots.\nNote\n",
      "content_length": 1519,
      "extraction_method": "Direct"
    },
    {
      "page_number": 250,
      "chapter": null,
      "content": "An example of a recommendation system LLM is P5,\nintroduced by Shijie Gang et al. in their paper\nRecommendation as Language Processing (RLP): A\nUnified Pretrain, Personalized Prompt & Predict\nParadigm (P5).\nP5 is a unified text-to-text paradigm for building\nrecommender systems using large language models\n(LLMs). It consists of three steps:\nPretrain: A foundation language model based on\nT5 architecture is pretrained on a large-scale web\ncorpus and fine-tuned on recommendation tasks.\nPersonalized prompt: A personalized prompt is\ngenerated for each user based on their behavior\ndata and contextual features.\nPredict: The personalized prompt is fed into the\npretrained language model to generate\nrecommendations.\nP5 is based on the idea that LLMs can encode\nextensive world knowledge and user preferences and\ncan be adapted to different recommendation tasks with\nzero or few shots.\nFine-tuning: Training an LLM from scratch is a highly computational-\nintensive activity. An alternative and less intrusive approach to\ncustomize an LLM for recommendation systems might be fine-tuning.\nMore specifically, the authors of the paper review two main strategies\nfor fine-tuning LLMs:\n",
      "content_length": 1178,
      "extraction_method": "Direct"
    },
    {
      "page_number": 251,
      "chapter": null,
      "content": "Full-model fine-tuning involves changing the entire model’s\nweights based on task-specific recommendation datasets.\nParameter-efficient fine-tuning aims to change only a small\npart of weights or develop trainable adapters to fit specific tasks.\nPrompting: The third and “lightest” way of tailoring LLMs to be\nrecommender systems is prompting. According to the authors, there are\nthree main techniques for prompting LLMs:\nConventional prompting aims to unify downstream tasks into\nlanguage generation tasks by designing text templates or\nproviding a few input-output examples.\nIn-context learning enables LLMs to learn new tasks based on\ncontextual information without fine-tuning.\nChain-of-thought enhances the reasoning abilities of LLMs by\nproviding multiple demonstrations to describe the chain of\nthought as examples within the prompt. The authors also discuss\nthe advantages and challenges of each technique and provide\nsome examples of existing methods that adopt them.\nRegardless of the typology, prompting is the fastest way to test\nwhether a general-purpose LLM can tackle recommendation systems’\ntasks.\nThe application of LLMs within the recommendation system domain is\nraising interest in the research field, and there is already some interesting\nevidence of the results as seen above.\nIn the next section, we are going to implement our own recommendation\napplication using the prompting approach and leveraging the capabilities of\nLangChain as an AI orchestrator.\n",
      "content_length": 1476,
      "extraction_method": "Direct"
    },
    {
      "page_number": 252,
      "chapter": null,
      "content": "Implementing an LLM-\npowered recommendation\nsystem\nNow that we have covered some theory about recommendation systems and\nemerging research on how LLMs can enhance them, let’s start building our\nrecommendation app, which will be a movie recommender system called\nMovieHarbor. The goal will be to make it as general as possible, meaning\nthat we want our app to be able to address various recommendations tasks\nwith a conversational interface. The scenario we are going to simulate will\nbe that of the so-called “cold start,” concerning the first interaction of a user\nwith the recommendation system where we do not have the user’s preference\nhistory. We will leverage a movie database with textual descriptions.\nFor this purpose, we will use the Movie recommendation data dataset,\navailable on Kaggle at\nhttps://www.kaggle.com/datasets/rohan4050/movie-\nrecommendation-data.\nThe reason for using a dataset with a textual description of each movie\n(alongside information such as ratings and movie titles) is so that we can get\nthe embeddings of the text. So let’s start building our MovieHarbor\napplication.\nData preprocessing\nIn order to apply LLMs to our dataset, we first need to preprocess the data.\nThe initial dataset included several columns; however, the ones we are\ninterested in are the following:\n",
      "content_length": 1304,
      "extraction_method": "Direct"
    },
    {
      "page_number": 253,
      "chapter": null,
      "content": "Genres: A list of applicable genres for the movie.\nTitle: The movie’s title.\nOverview: Textual description of the plot.\nVote_average: A rating from 1 to 10 for a given movie\nVote_count: The number of votes for a given movie.\nI won’t report here the whole code (you can find it in the GitHub repo of this\nbook at https://github.com/PacktPublishing/Building-\nLLM-Powered-Applications), however, I will share the main steps of\ndata preprocessing:\n1. First, we format the genres column into a numpy array, which is easier\nto handle than the original dictionary format in the dataset:\nimport pandas as pd\nimport ast\n# Convert string representation of dictionaries to act\nmd['genres'] = md['genres'].apply(ast.literal_eval)\n# Transforming the 'genres' column\nmd['genres'] = md['genres'].apply(lambda x: [genre['na\n2. Next, we merge the vote_average and vote_count columns into a\nsingle column, which is the weighted ratings with respect to the number\nof votes. I’ve also limited the rows to the 95th percentile of the number\nof votes, so that we can get rid of minimum vote counts to prevent\nskewed results:\n# Calculate weighted rate (IMDb formula)\ndef calculate_weighted_rate(vote_average, vote_count, \n    return (vote_count / (vote_count + min_vote_count)\n# Minimum vote count to prevent skewed results\nvote_counts = md[md['vote_count'].notnull()]['vote_cou\nmin_vote_count = vote_counts.quantile(0.95)\n",
      "content_length": 1399,
      "extraction_method": "Direct"
    },
    {
      "page_number": 254,
      "chapter": null,
      "content": "# Create a new column 'weighted_rate'\nmd['weighted_rate'] = md.apply(lambda row: calculate_w\n3. Next, we create a new column called combined_info where we are\ngoing to merge all the elements that will be provided as context to the\nLLMs. Those elements are the movie title, overview, genres, and\nratings:\nmd_final['combined_info'] = md_final.apply(lambda row:\n4. We tokenize the movie combined_info so that we will get better results\nwhile embedding:\nimport pandas as pd\nimport tiktoken\nimport os\nimport openai\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\nfrom openai.embeddings_utils import get_embedding\nembedding_encoding = \"cl100k_base\" # this the encoding\nmax_tokens = 8000 # the maximum for text-embedding-ada\nencoding = tiktoken.get_encoding(embedding_encoding)\n# omit reviews that are too long to embed\nmd_final[\"n_tokens\"] = md_final.combined_info.apply(la\nmd_final = md_final[md_final.n_tokens <= max_tokens]\nDefinition\ncl100k_base is the name of a tokenizer used by\nOpenAI’s embeddings API. A tokenizer is a tool that\nsplits a text string into units called tokens, which can\nthen be processed by a neural network. Different\n",
      "content_length": 1135,
      "extraction_method": "Direct"
    },
    {
      "page_number": 255,
      "chapter": null,
      "content": "tokenizers have different rules and vocabularies for\nhow to split the text and what tokens to use.\nThe cl100k_base tokenizer is based on the byte pair\nencoding (BPE) algorithm, which learns a vocabulary\nof subword units from a large corpus of text. The\ncl100k_base tokenizer has a vocabulary of 100,000\ntokens, which are mostly common words and word\npieces, but also include some special tokens for\npunctuation, formatting, and control. It can handle\ntexts in multiple languages and domains, and can\nencode up to 8,191 tokens per input.\n5. We embed the text with text-embedding-ada-002:\nmd_final[\"embedding\"] = md_final.overview.apply(lambda\nAfter changing some columns’ names and dropping unnecessary\ncolumns, the final dataset looks as follows:\n",
      "content_length": 747,
      "extraction_method": "Direct"
    },
    {
      "page_number": 256,
      "chapter": null,
      "content": "Figure 7.1: Sample of the final movies dataset\nLet’s have a look at a random row of text:\nmd['text'][0]\nThe following output is obtained:\n'Title: GoldenEye. Overview: James Bond must unmask the mys\nThe last change we will make is modifying some naming conventions\nand data types as follows:\nmd_final.rename(columns = {'embedding': 'vector'}, inplace \nmd_final.rename(columns = {'combined_info': 'text'}, inplac\nmd_final.to_pickle('movies.pkl')\n6. Now that we have our final dataset, we need to store it in a VectorDB.\nFor this purpose, we are going to leverage LanceDB, an open-source\ndatabase for vector-search built with persistent storage, which greatly\nsimplifies the retrieval, filtering, and management of embeddings and\nalso offers a native integration with LangChain. You can easily install\nLanceDB via pip install lancedb:\nimport lancedb\nuri = \"data/sample-lancedb\"\ndb = lancedb.connect(uri)\ntable = db.create_table(\"movies\", md)\n",
      "content_length": 939,
      "extraction_method": "Direct"
    },
    {
      "page_number": 257,
      "chapter": null,
      "content": "Now that we have all our ingredients, we can start working with those\nembeddings and start building our recommendation system. We will start\nwith a simple task in a cold-start scenario, adding progressive layers of\ncomplexity with LangChain components. Afterwards, we will also try a\ncontent-based scenario to challenge our LLMs with diverse tasks.\nBuilding a QA recommendation\nchatbot in a cold-start scenario\nIn previous sections, we saw how the cold-start scenario – that means\ninteracting with a user for the first time without their backstory – is a\nproblem often encountered by recommendation systems. The less\ninformation we have about a user, the harder it is to match the\nrecommendations to their preferences.\nIn this section, we are going to simulate a cold-start scenario with\nLangChain and OpenAI’s LLMs with the following high-level architecture:\nFigure 7.2: High-level architecture of recommendation system in a cold-start scenario\nIn the previous section, we’ve already saved our embeddings in LanceDB.\nNow, we are going to build a LangChain RetrievalQA retriever, a chain\n",
      "content_length": 1088,
      "extraction_method": "Direct"
    },
    {
      "page_number": 258,
      "chapter": null,
      "content": "component designed for question-answering against an index. In our case,\nwe will use the vector store as our index retriever. The idea is that the chain\nreturns the top k most similar movies upon the user’s query, using cosine\nsimilarity as the distance metric (which is the default).\nSo, let’s start building the chain:\n1. We are using only the movie overview as information input:\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import LanceDB\nos.environ[\"OPENAI_API_KEY\"]\nembeddings = OpenAIEmbeddings()\ndocsearch = LanceDB(connection = table, embedding = em\nquery = \"I'm looking for an animated action movie. Wha\ndocs = docsearch.similarity_search(query)\ndocs\nThe following is the corresponding output (I will display a truncated\nversion of the output, showing only the first out of four document\nsources):\n[Document(page_content='Title: Hitman: Agent 47. Overview: \nAs you can see, alongside each Document, all variables are reported as\nmetadata, plus the distance is also reported as a score. The lower the\ndistance, the greater the proximity between the user’s query and the\nmovie’s text embedding.\n2. Once we have gathered the most similar documents, we want a\nconversational response. For this goal, in addition to the embedding\n",
      "content_length": 1268,
      "extraction_method": "Direct"
    },
    {
      "page_number": 259,
      "chapter": null,
      "content": "models, we will also use OpenAI’s completion model GPT-3 and\ncombine it in RetrievalQA:\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_t\nquery = \"I'm looking for an animated action movie. Wha\nresult = qa({\"query\": query})\nresult['result']\nLet’s look at the output:\n' I would suggest Transformers. It is an animated action mo\n3. Since we set the return_source_documents=True parameter, we can\nalso retrieve the document sources:\nresult['source_documents'][0]\nThe following is the output:\nDocument(page_content='Title: Hitman: Agent 47. Overview: A\n       -0.01303058, -0.00709073], dtype=float32), '_distanc\nNote that the first document reported is not the one the model\nsuggested. This occurred probably because of the rating, which is\nlower than Transformers (which was only the third result). This is a\ngreat example of how the LLM was able to consider multiple factors,\non top of similarity, to suggest a movie to the user.\n4. The model was able to generate a conversational answer, however, it is\nstill using only a part of the available information – the textual\n",
      "content_length": 1074,
      "extraction_method": "Direct"
    },
    {
      "page_number": 260,
      "chapter": null,
      "content": "overview. What if we want our MovieHarbor system to also leverage\nthe other variables? We can approach the task in two ways:\nThe “filter” way: This approach consists of adding some filters\nas kwargs to our retriever, which might be required by the\napplication before responding to the user. Those questions might\nbe, for example, about the genre of a movie.\nFor example, let’s say we want to provide results featuring only those\nmovies for which the genre is tagged as comedy. You can achieve this\nwith the following code:\ndf_filtered = md[md['genres'].apply(lambda x: 'Comedy' in x)\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"s\n    retriever=docsearch.as_retriever(search_kwargs={'data': \nquery = \"I'm looking for a movie with animals and an adventu\nresult = qa({\"query\": query})\nThe filter can also operate at the metadata level, as shown in the\nfollowing example, where we want to filter only results with a rating\nabove 7:\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"s\n    retriever=docsearch.as_retriever(search_kwargs={'filter\nThe “agentic” way: This is probably the most innovative way to\napproach the problem. Making our chain agentic means\nconverting the retriever to a tool that the agent can leverage if\nneeded, including the additional variables. By doing so, it would\nbe sufficient for the user to provide their preferences in natural\n",
      "content_length": 1379,
      "extraction_method": "Direct"
    },
    {
      "page_number": 261,
      "chapter": null,
      "content": "language so that the agent can retrieve the most promising\nrecommendation if needed.\nLet’s see how to implement this with code, asking specifically for an\naction movie (thus filtering on the genre variable):\nfrom langchain.agents.agent_toolkits import create_retriever\nfrom langchain.agents.agent_toolkits import create_conversat\nfrom langchain.chat_models import ChatOpenAI\nllm = ChatOpenAI(temperature = 0)\nretriever = docsearch.as_retriever(return_source_documents =\ntool = create_retriever_tool(\n    retriever,\n    \"movies\",\n    \"Searches and returns recommendations about movies.\"\n)\ntools = [tool]\nagent_executor = create_conversational_retrieval_agent(llm, \nresult = agent_executor({\"input\": \"suggest me some action mo\nLet’s see a glimpse of the chain of thoughts and the output produced\n(always based on the four most similar movies according to cosine\nsimilarity):\n> Entering new AgentExecutor chain...\nInvoking: `movies` with `{'genre': 'action'}`\n[Document(page_content='The action continues from [REC], […\nHere are some action movies that you might enjoy:\n1. [REC]² - The action continues from [REC], with a medical\n2. The Boondock Saints - Twin brothers Conner and Murphy ta\n3. The Gamers - Four clueless players are sent on a quest t\n4. Atlas Shrugged Part III: Who is John Galt? - In a collap\nPlease note that these recommendations are based on the gen\n> Finished chain.\n",
      "content_length": 1385,
      "extraction_method": "Direct"
    },
    {
      "page_number": 262,
      "chapter": null,
      "content": "5. Finally, we might also want to make our application more tailored\ntoward its goal of being a recommender system. To do so, we need to\ndo some prompt engineering.\nNote\nOne of the advantages of using LangChain’s pre-built\ncomponents, such as the RetrievalQA chain, is that\nthey come with a pre-configured, well-curated prompt\ntemplate. Before overriding the existing prompt, it’s a\ngood practice to inspect it, so that you can also see\nwhich variables (within {}) are already expected from\nthe component.\nTo explore the existing prompt, you can run the following code:\nprint(qa.combine_documents_chain.llm_chain.prompt.template\nHere is the output:\nUse the following pieces of context to answer the question \n{context}\nQuestion: {question}\nHelpful Answer:\nLet’s say, for example, that we want our system to return three\nsuggestions for each user’s request, with a short description of the plot\nand the reason why the user might like it. The following is a sample\nprompt that could match this goal:\n",
      "content_length": 998,
      "extraction_method": "Direct"
    },
    {
      "page_number": 263,
      "chapter": null,
      "content": "from langchain.prompts import PromptTemplate\ntemplate = \"\"\"You are a movie recommender system that help \nUse the following pieces of context to answer the question \nFor each question, suggest three movies, with a short desc\nIf you don't know the answer, just say that you don't know\n{context}\nQuestion: {question}\nYour response:\"\"\"\n \nPROMPT = PromptTemplate(\n    template=template, input_variables=[\"context\", \"questio\n6. Now we need to pass it into our chain:\nPROMPT = PromptTemplate(\n    template=template, input_variables=[\"context\", \"qu\nchain_type_kwargs = {\"prompt\": PROMPT}\nqa = RetrievalQA.from_chain_type(llm=OpenAI(),\n    chain_type=\"stuff\",\n    retriever=docsearch.as_retriever(),\n    return_source_documents=True,\n    chain_type_kwargs=chain_type_kwargs)\nquery = \"I'm looking for a funny action movie, any sug\nresult = qa({'query':query})\nprint(result['result'])\nThe following output is obtained:\n1. A Good Day to Die Hard: An action-packed comedy directed\n2. The Hidden: An alien is on the run in America and uses t\n3. District B13: Set in the ghettos of Paris in 2010, this \n7. Another thing that we might want to implement in our prompt is the\ninformation gathered with the conversational preliminary questions that\n",
      "content_length": 1230,
      "extraction_method": "Direct"
    },
    {
      "page_number": 264,
      "chapter": null,
      "content": "we might want to set as a welcome page. For example, before letting\nthe user input their natural language question, we might want to ask\ntheir age, gender, and favorite movie genre. To do so, we can insert in\nour prompt a section where we can format the input variables with\nthose shared by the user, and then combine this prompt chunk in the\nfinal prompt we are going to pass to the chain. Below you can find an\nexample (for simplicity, we are going to set the variables without asking\nthe user):\nfrom langchain.prompts import PromptTemplate\ntemplate_prefix = \"\"\"You are a movie recommender syste\nUse the following pieces of context to answer the ques\nIf you don't know the answer, just say that you don't \n{context}\"\"\"\nuser_info = \"\"\"This is what we know about the user, an\nAge: {age}\nGender: {gender}\"\"\"\ntemplate_suffix= \"\"\"Question: {question}\nYour response:\"\"\"\nuser_info = user_info.format(age = 18, gender = 'femal\nCOMBINED_PROMPT = template_prefix +'\\n'+ user_info +'\\\nprint(COMBINED_PROMPT)\nHere is the output:\nYou are a movie recommender system that help users to find \nUse the following pieces of context to answer the question \nIf you don't know the answer, just say that you don't know\n{context}\nThis is what we know about the user, and you can use this i\nAge: 18\nGender: female\nQuestion: {question}\nYour response:\n",
      "content_length": 1327,
      "extraction_method": "Direct"
    },
    {
      "page_number": 265,
      "chapter": null,
      "content": "8. Now let’s format the prompt and pass it into our chain:\nPROMPT = PromptTemplate(\n    template=COMBINED_PROMPT, input_variables=[\"contex\nchain_type_kwargs = {\"prompt\": PROMPT}\nqa = RetrievalQA.from_chain_type(llm=OpenAI(),\n    chain_type=\"stuff\",\n    retriever=docsearch.as_retriever(),\n    return_source_documents=True,\n    chain_type_kwargs=chain_type_kwargs)\nresult = qa({'query':query})\nresult['result']\nWe receive the following output:\n' Sure, I can suggest some action movies for you. Here are \nAs you can see, the system considered the user’s information provided.\nWhen we build the front-end of MovieHarbor, we will make this information\ndynamic as preliminary questions proposed to the user.\nBuilding a content-based\nsystem\nIn the previous section, we covered the cold-start scenario where the system\nknew nothing about the user. Sometimes, recommender systems already\nhave some backstory about users, and it is extremely useful to embed this\nknowledge in our application. Let’s imagine, for example, that we have a\nusers database where the system has stored all the registered user’s\ninformation (such as age, gender, country, etc.) as well as the movies the\nuser has already watched alongside their rating.\n",
      "content_length": 1220,
      "extraction_method": "Direct"
    },
    {
      "page_number": 266,
      "chapter": null,
      "content": "To do so, we will need to set a custom prompt that is able to retrieve this\ninformation from a source. For simplicity, we will create a sample dataset\nwith users’ information with just two records, corresponding to two users.\nEach user will exhibit the following variables: username, age, gender, and a\ndictionary containing movies already watched alongside with the rating they\ngave to them.\nThe high-level architecture is represented by the following diagram:\nFigure 7.3: High-level architecture of a content-based recommendation system\nLet’s break down this architecture and examine each step to build the final\nchat for this content-based system, starting from the available users’ data:\n1. As discussed earlier, we now have a bit of information about our users’\npreferences. More specifically, imagine we have a dataset containing\nusers’ attributes (name, age, gender) along with their reviews (a score\nfrom 1 to 10) of some movies. The following is the code used to create\nthe dataset:\n",
      "content_length": 992,
      "extraction_method": "Direct"
    },
    {
      "page_number": 267,
      "chapter": null,
      "content": "import pandas as pd\ndata = {\n    \"username\": [\"Alice\", \"Bob\"],\n    \"age\": [25, 32],\n    \"gender\": [\"F\", \"M\"],\n    \"movies\": [\n        [(\"Transformers: The Last Knight\", 7), (\"Pokém\n        [(\"Bon Cop Bad Cop 2\", 8), (\"Goon: Last of the\n    ]\n}\n# Convert the \"movies\" column into dictionaries\nfor i, row_movies in enumerate(data[\"movies\"]):\n    movie_dict = {}\n    for movie, rating in row_movies:\n        movie_dict[movie] = rating\n    data[\"movies\"][i] = movie_dict\n# Create a pandas DataFrame\ndf = pd.DataFrame(data)\ndf.head()\nThe following output is obtained:\nFigure 7.4: Sample users dataset\n2. What we want to do now is apply the same logic of the prompt of the\ncold start with the formatting with variables. The difference here is that,\nrather than asking the user to provide the values for those variables, we\nwill directly collect them from our user dataset. So, we first define our\nprompt chunks:\n",
      "content_length": 906,
      "extraction_method": "Direct"
    },
    {
      "page_number": 268,
      "chapter": null,
      "content": "template_prefix = \"\"\"You are a movie recommender syste\nUse the following pieces of context to answer the ques\nIf you don't know the answer, just say that you don't \n{context}\"\"\"\nuser_info = \"\"\"This is what we know about the user, an\nAge: {age}\nGender: {gender}\nMovies already seen alongside with rating: {movies}\"\"\"\ntemplate_suffix= \"\"\"Question: {question}\nYour response:\"\"\"\n3. We then format the user_info chunk as follows (assuming that the user\ninteracting with the system is Alice):\nage = df.loc[df['username']=='Alice']['age'][0]\ngender = df.loc[df['username']=='Alice']['gender'][0]\nmovies = ''\n# Iterate over the dictionary and output movie name an\nfor movie, rating in df['movies'][0].items():\n    output_string = f\"Movie: {movie}, Rating: {rating}\n    movies+=output_string\n    #print(output_string)\nuser_info = user_info.format(age = age, gender = gende\nCOMBINED_PROMPT = template_prefix +'\\n'+ user_info +'\\\nprint(COMBINED_PROMPT)\nHere is the output:\nYou are a movie recommender system that help users to find \nUse the following pieces of context to answer the question \nIf you don't know the answer, just say that you don't know\n{context}\nThis is what we know about the user, and you can use this i\nAge: 25\nGender: F\nMovies already seen alongside with rating: Movie: Transform\nMovie: Pokémon: Spell of the Unknown, Rating: 5\n",
      "content_length": 1337,
      "extraction_method": "Direct"
    },
    {
      "page_number": 269,
      "chapter": null,
      "content": "Question: {question}\nYour response:\n4. Let’s now use this prompt within our chain:\nPROMPT = PromptTemplate(\n    template=COMBINED_PROMPT, input_variables=[\"contex\nchain_type_kwargs = {\"prompt\": PROMPT}\nqa = RetrievalQA.from_chain_type(llm=OpenAI(),\n    chain_type=\"stuff\",\n    retriever=docsearch.as_retriever(),\n    return_source_documents=True,\n    chain_type_kwargs=chain_type_kwargs)\nquery = \"Can you suggest me some action movie based on\nresult = qa({'query':query})\nresult['result']\nWe then obtain the following output:\n\" Based on your age, gender, and the movies you've already \n'\nAs you can see, the model is now able to recommend a list of movies\nto Alice based on the user’s information about past preferences,\nretrieved as context within the model’s metaprompt.\nNote that, in this scenario, we used as dataset a simple pandas dataframe. In\nproduction scenarios, a best practice for storing variables related to a task to\nbe addressed (such as a recommendation task) is that of using a feature store.\nFeature stores are data systems that are designed to support machine learning\nworkflows. They allow data teams to store, manage, and access features that\nare used for training and deploying machine learning models.\n",
      "content_length": 1226,
      "extraction_method": "Direct"
    },
    {
      "page_number": 270,
      "chapter": null,
      "content": "Furthermore, LangChain offers native integrations towards some of the most\npopular features stores:\nFeast: This is an open-source feature store for machine learning. It\nallows teams to define, manage, discover, and serve features. Feast\nsupports batch and streaming data sources and integrates with various\ndata processing and storage systems. Feast uses BigQuery for offline\nfeatures and BigTable or Redis for online features.\nTecton: This is a managed feature platform that provides a complete\nsolution for building, deploying, and using features for machine\nlearning. Tecton allows users to define features in code, version control\nthem, and deploy them to production with best practices. Furthermore,\nit integrates with existing data infrastructure and ML platforms like\nSageMaker and Kubeflow, and it uses Spark for feature transformations\nand DynamoDB for online feature serving.\nFeatureform: This is a virtual feature store that transforms existing\ndata infrastructure into a feature store. Featureform allows users to\ncreate, store, and access features using standard feature definitions and\na Python SDK. It orchestrates and manages the data pipelines required\nfor feature engineering and materialization, and it is compatible with a\nwide range of data systems, such as Snowflake, Redis, Spark, and\nCassandra.\nAzureML Managed Feature Store: This is a new type of workspace\nthat lets users discover, create, and operationalize features. This service\nintegrates with existing data stores, feature pipelines, and ML platforms\nlike Azure Databricks and Kubeflow. Plus, it uses SQL, PySpark,\nSnowPark, or Python for feature transformations and Parquet/S3 or\nCosmos DB for feature storage.\n",
      "content_length": 1693,
      "extraction_method": "Direct"
    },
    {
      "page_number": 271,
      "chapter": null,
      "content": "You can read more about LangChain’s integration with features at\nhttps://blog.langchain.dev/feature-stores-and-llms/.\nDeveloping the front-end\nwith Streamlit\nNow that we have seen the logic behind an LLM-powered recommendation\nsystem, it is time to give a GUI to our MovieHarbor. To do so, we will once\nagain leverage Streamlit, and we will assume the cold-start scenario. As\nalways, you can find the whole Python code in the GitHub book repository\nat https://github.com/PacktPublishing/Building-LLM-\nPowered-Applications.\nAs per the Globebotter application in Chapter 6, in this case also you need to\ncreate a .py file to run in your terminal via streamlit run file.py. In our\ncase, the file will be named movieharbor.py.\nLet’s now summarize the key steps to build the app with the front-end:\n1. Configure the application webpage:\nimport streamlit as st\nst.set_page_config(page_title=\"GlobeBotter\", page_icon\nst.header('\n Welcome to MovieHarbor, your favourite mo\n2. Import the credentials and establish the connection to LanceDB:\nload_dotenv()\n#os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\nopenai_api_key = os.environ['OPENAI_API_KEY']\nembeddings = OpenAIEmbeddings()\nuri = \"data/sample-lancedb\"\n",
      "content_length": 1192,
      "extraction_method": "Direct"
    },
    {
      "page_number": 272,
      "chapter": null,
      "content": "db = lancedb.connect(uri)\ntable = db.open_table('movies')\ndocsearch = LanceDB(connection = table, embedding = em\n# Import the movie dataset\nmd = pd.read_pickle('movies.pkl')\n3. Create some widgets for the user to define their features and movies\npreferences:\n# Create a sidebar for user input\nst.sidebar.title(\"Movie Recommendation System\")\nst.sidebar.markdown(\"Please enter your details and pre\n# Ask the user for age, gender and favourite movie gen\nage = st.sidebar.slider(\"What is your age?\", 1, 100, 2\ngender = st.sidebar.radio(\"What is your gender?\", (\"Ma\ngenre = st.sidebar.selectbox(\"What is your favourite m\n# Filter the movies based on the user input\ndf_filtered = md[md['genres'].apply(lambda x: genre in\n4. Define the parametrized prompt chunks:\ntemplate_prefix = \"\"\"You are a movie recommender syste\nUse the following pieces of context to answer the ques\nIf you don't know the answer, just say that you don't \n{context}\"\"\"\nuser_info = \"\"\"This is what we know about the user, an\nAge: {age}\nGender: {gender}\"\"\"\ntemplate_suffix= \"\"\"Question: {question}\nYour response:\"\"\"\nuser_info = user_info.format(age = age, gender = gende\nCOMBINED_PROMPT = template_prefix +'\\n'+ user_info +'\\\nprint(COMBINED_PROMPT)\n5. Set up the RetrievalQA chain:\n",
      "content_length": 1246,
      "extraction_method": "Direct"
    },
    {
      "page_number": 273,
      "chapter": null,
      "content": "#setting up the chain\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_t\n    retriever=docsearch.as_retriever(search_kwargs={'d\n6. Insert the search bar for the user:\nquery = st.text_input('Enter your question:', placehol\nif query:\n    result = qa({\"query\": query})\n    st.write(result['result'])\nAnd that’s it! You can run the final result in your terminal with streamlit\nrun movieharbor.py. It looks like the following:\nFigure 7.5: Sample front-end for Movieharbor with Streamlit\nSo, you can see, in just few lines of code we were able to set up a webapp\nfor our MovieHarbor. Starting from this template, you can customize your\nlayout with Streamlit’s components, as well as tailor it to content-based\nscenarios. Plus, you can customize your prompts in such a way that the\nrecommender acts as you prefer.\n",
      "content_length": 811,
      "extraction_method": "Direct"
    },
    {
      "page_number": 274,
      "chapter": null,
      "content": "Summary\nIn this chapter, we explored how LLMs could change the way we approach a\nrecommendation system task. We started from the analysis of the current\nstrategies and algorithms for building recommendation applications,\ndifferentiating between various scenarios (collaborative filtering, content-\nbased, cold start, etc.) as well as different techniques (KNN, matrix\nfactorization, and NNs).\nWe then moved to the new, emerging field of research into how to apply the\npower of LLMs to this field, and explored the various experiments that have\nbeen done in recent months.\nLeveraging this knowledge, we built a movie recommender application\npowered by LLMs, using LangChain as the AI orchestrator and Streamlit as\nthe front-end, showing how LLMs can revolutionize this field thanks to their\nreasoning capabilities as well as their generalization. This was just one\nexample of how LLMs not only can open new frontiers, but can also\nenhance existing fields of research.\nIn the next chapter, we will see what these powerful models can do when\nworking with structured data.\nReferences\nRecommendation as Language Processing (RLP): A Unified\nPretrain, Personalized Prompt & Predict Paradigm (P5).\nhttps://arxiv.org/abs/2203.13366\nLangChain’s blog about featurestores.\nhttps://blog.langchain.dev/feature-stores-and-\nllms/\n",
      "content_length": 1314,
      "extraction_method": "Direct"
    },
    {
      "page_number": 275,
      "chapter": null,
      "content": "Feast. https://docs.feast.dev/\nTecton. https://www.tecton.ai/\nFeatureForm. https://www.featureform.com/\nAzure Machine Learning feature store.\nhttps://learn.microsoft.com/en-us/azure/machine-\nlearning/concept-what-is-managed-feature-store?\nview=azureml-api-2\nJoin our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\nOceanofPDF.com\n",
      "content_length": 412,
      "extraction_method": "Direct"
    },
    {
      "page_number": 276,
      "chapter": null,
      "content": "8\nUsing LLMs with\nStructured Data\nIn this chapter, we are going to cover yet another great capability of large\nlanguage models (LLMs): the ability to handle structured, tabular data. We\nwill see how, thanks to plugins and an agentic approach, we can use LLMs\nas a natural language interface between us and our structured data, reducing\nthe gap between the business user and the structured information.\nDuring this chapter, we will cover the following topics:\nIntroduction to the main structured data systems\nUsing tools and plugins to connect LLMs to tabular data\nBuilding a database copilot with LangChain\nBy the end of this chapter, you will be able to build your own natural\nlanguage interface for your data estate and be able to combine unstructured\nwith structured sources.\nTechnical requirements\nTo complete the tasks in this chapter, you will need the following:\nA Hugging Face account and user access token.\nAn OpenAI account and user access token.\n",
      "content_length": 957,
      "extraction_method": "Direct"
    },
    {
      "page_number": 277,
      "chapter": null,
      "content": "Python 3.7.1 or later version.\nPython packages: Make sure to have the following Python packages\ninstalled: langchain, python-dotenv, huggingface_hub, streamlit, and\nsqlite3. Those can be easily installed via pip install in your\nterminal.\nYou can find all the code and examples in the book’s GitHub repository at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications.\nWhat is structured data?\nIn previous chapters, we focused on how LLMs can handle textual data. In\nfact, those models are, as the name suggests, “language” models, meaning\nthat they have been trained and are able to handle unstructured text data.\nNevertheless, unstructured data only refers to a portion of the overall data\nrealm that applications can handle. Generally, data can be categorized into\nthree types, which are as follows:\nUnstructured data: This refers to data that doesn’t have a specific or\npredefined format. It lacks a consistent structure, making it challenging\nto organize and analyze using traditional databases. Examples of\nunstructured data include:\nText documents: Emails, social media posts, articles, and reports.\nMultimedia: Images, videos, audio recordings.\nNatural language text: Chat logs, transcriptions of spoken\nconversations.\nBinary data: Files without a specific data format, such as\nproprietary file formats.\n",
      "content_length": 1326,
      "extraction_method": "Direct"
    },
    {
      "page_number": 278,
      "chapter": null,
      "content": "Note\nWhen it comes to storing unstructured data, NoSQL\ndatabases play a crucial role, due to their flexible\nschema-less design, which allows them to handle\nvarious data types like text, images, and videos\nefficiently. The term “NoSQL” originally stood for\n“non-SQL” or “not only SQL” to emphasize that these\ndatabases don’t rely solely on the traditional\nStructured Query Language (SQL) to manage and\nquery data. NoSQL databases emerged as a response to\nthe limitations of relational databases, particularly\ntheir rigid schema requirements and difficulties in\nscaling horizontally.\nAn example of a NoSQL database is MongoDB, a\ndocument-oriented NoSQL database, which stores data\nin JSON-like documents, making it highly effective\nfor managing diverse unstructured content; similarly,\nCassandra, with its wide-column store model, excels at\nhandling large volumes of data across many\ncommodity servers, providing high availability\nwithout compromising performance. This flexibility\nenables NoSQL databases to adapt to the volume,\nvariety, and velocity of unstructured data,\naccommodating rapid changes and scaling easily.\nTraditional relational databases, with their rigid\nschema requirements, struggle to manage such\ndiversity and volume efficiently.\n",
      "content_length": 1250,
      "extraction_method": "Direct"
    },
    {
      "page_number": 279,
      "chapter": null,
      "content": "Structured data: This type of data is organized and formatted with a\nclear structure, typically into rows and columns. It follows a fixed\nschema, making it easy to store, retrieve, and analyze using relational\ndatabases. Examples of structured data include:\nRelational databases: Data stored in tables with predefined\ncolumns and data types.\nSpreadsheets: Data organized in rows and columns in software\nlike Microsoft Excel.\nSensor data: Recorded measurements like temperature, pressure,\nand time in a structured format.\nFinancial data: Transaction records, balance sheets, and income\nstatements.\nSemi-structured data: This falls between the two categories. While it\ndoesn’t adhere to a rigid structure like structured data, it has some level\nof organization and may contain tags or other markers that provide\ncontext. Examples of semi-structured data include:\neXtensible Markup Language (XML) files: They use tags to\nstructure data, but the specific tags and their arrangement can\nvary.\nJavaScript Object Notation (JSON): This is used for data\ninterchange and allows for nested structures and key-value pairs.\nNoSQL databases: Storing data in a format that doesn’t require a\nfixed schema, allowing for flexibility.\nIn summary, unstructured data lacks a defined format, structured data\nfollows a strict format, and semi-structured data has some level of structure\nbut is more flexible than structured data. The distinction between these types\nof data is important as it impacts how they are stored, processed, and\nanalyzed in various applications.\n",
      "content_length": 1548,
      "extraction_method": "Direct"
    },
    {
      "page_number": 280,
      "chapter": null,
      "content": "However, regardless of its nature, querying structured data involves using a\nquery language or methods specific to that database technology. For\nexample, for SQL databases, SQL is used to interact with relational\ndatabases. Henceforth, to extract data from tables, you need to know this\nspecific language.\nBut what if we want to ask questions in natural language to our structured\ndata? What if our application could provide us not only with a sterile\nnumeric answer but rather with a conversational answer, which also gives us\ncontext about the number? This is exactly what we will try to achieve in the\nnext sections with our LLM-powered applications. More specifically, we are\ngoing build something that we’ve already defined in Chapter 2: a copilot.\nSince we are going to mount our copilot to a relational database, we will\nname our application DBCopilot. First, let’s look at what relational\ndatabases are.\nGetting started with\nrelational databases\nThe concept of relational databases was first proposed by E.F. Codd, an IBM\nresearcher, in 1970. He defined the rules and principles of the relational\nmodel, which aimed to provide a simple and consistent way of accessing and\nmanipulating data. He also introduced SQL, which became the standard\nlanguage for querying and manipulating relational databases. Relational\ndatabases have become widely used in various domains and applications,\nsuch as e-commerce, inventory management, payroll, customer\nrelationship management (CRM), and business intelligence (BI).\n",
      "content_length": 1515,
      "extraction_method": "Direct"
    },
    {
      "page_number": 281,
      "chapter": null,
      "content": "In this section, we are going to cover the main aspects of a relational\ndatabase. Then, we will start working with the sample database we will use\nin our DBCopilot, the Chinook database. We will inspect this database and\nexplore how to connect to remote tables using Python.\nIntroduction to relational\ndatabases\nA relational database is a type of database that stores and organizes data in\nstructured tables with rows and columns. Each row represents a record, and\neach column represents a field or attribute. The relationships between tables\nare established through keys, primarily the primary key and foreign key. This\nallows for efficient querying and manipulation of data using SQL. These\ndatabases are commonly used for various applications like websites and\nbusiness management systems, due to their ability to manage structured data\neffectively.\nTo have a better understanding of relational databases, let’s consider an\nexample of a database of a library. We’ll have two tables: one for books and\nanother for authors. The relationship between them will be established using\nprimary and foreign keys.\nDefinition\nA primary key is like the unique fingerprint of each record in\na table. It’s a special column that holds a value that’s distinct\nfor each row in that table. Think of it as the “identity” of a\nrecord. Having a primary key is important because it\nguarantees that no two records in the same table will share\n",
      "content_length": 1423,
      "extraction_method": "Direct"
    },
    {
      "page_number": 282,
      "chapter": null,
      "content": "the same key. This uniqueness makes it easy to locate,\nmodify, and manage individual records in the table.\nA foreign key is a bridge between two tables. It’s a column in\none table that references the primary key column in another\ntable. This reference creates a link between the data in the\ntwo tables, establishing a relationship. The purpose of the\nforeign key is to maintain data consistency and integrity\nacross related tables. It ensures that if a change is made in\nthe primary key table, the related data in the other table\nremains accurate. By using foreign keys, you can retrieve\ninformation from multiple tables that are connected, enabling\nyou to understand how different pieces of data are related to\neach other.\nLet’s take a closer look at our example, as shown in the following image:\nFigure 8.1: An example of the relationship between two tables in a database\nIn this example, the Authors table contains information about authors,\nincluding their ID, name, and birth year. The Books table includes details\nabout books, including the book’s ID, title, and a foreign key called\n",
      "content_length": 1090,
      "extraction_method": "Direct"
    },
    {
      "page_number": 283,
      "chapter": null,
      "content": "AuthorID, which references the corresponding author in the Authors table\n(with AuthorID as the primary key). This way, you can use SQL queries to\nretrieve information like finding all books written by a specific author or the\nbirth year of an author based on the book they wrote. The relational structure\nallows for efficient management and retrieval of data in a structured manner.\nSome of the main database systems in the market include:\nSQL databases: These are relational database management systems\n(RDBMS) that use SQL for data manipulation and querying. Examples\ninclude MySQL, PostgreSQL, and Microsoft SQL Server.\nOracle Database: A widely-used RDBMS that offers advanced features\nand scalability for large-scale applications.\nSQLite: A self-contained, serverless, and zero-configuration SQL\ndatabase engine commonly used in embedded systems and mobile\napplications.\nIBM Db2: A family of data management products, including relational\ndatabase servers, developed by IBM.\nAmazon Web Services (AWS) RDS: A managed relational database\nservice offered by Amazon, providing options for various databases like\nMySQL, PostgreSQL, SQL Server, and more.\nGoogle Cloud SQL: A managed database service by Google Cloud\nPlatform, supporting MySQL, PostgreSQL, and SQL Server.\nRedis: An open-source, in-memory data structure store that can be used\nas a database, cache, and message broker.\nIn this chapter, we are going to use SQLite database, which also offers a\nseamless integration with Python. But before we do that, let’s understand the\ndatabase we’ll be using.\n",
      "content_length": 1561,
      "extraction_method": "Direct"
    },
    {
      "page_number": 284,
      "chapter": null,
      "content": "Overview of the Chinook\ndatabase\nThe Chinook database is a sample database that can be used for learning and\npracticing SQL. It is based on a fictional digital media store and contains\ndata about artists, albums, tracks, customers, invoices, and more. The\nChinook database is available for various database management systems,\nsuch as SQL Server, Oracle, MySQL, PostgreSQL, SQLite, and DB2.\nHere are some features of this database:\nIt uses real data from an iTunes library, which makes it more realistic\nand interesting.\nIt has a clear and simple data model, which makes it easy to understand\nand query.\nIt covers more features of SQL, such as subqueries, joins, views, and\ntriggers.\nIt is compatible with multiple database servers, which makes it more\nversatile and portable.\nYou can find the configuration instructions at\nhttps://database.guide/2-sample-databases-sqlite/.\nYou can see an illustration of the relationship among the database’s tables\nhere:\n",
      "content_length": 957,
      "extraction_method": "Direct"
    },
    {
      "page_number": 285,
      "chapter": null,
      "content": "Figure 8.2: Diagram of Chinook Database (source:\nhttps://github.com/arjunchndr/Analyzing-Chinook-Database-using-SQL-\nand-Python)\nAs you can see, there are 11 tables, all related to each other with primary and\nforeign keys. In the upcoming paragraph, we will see how LLMs will be\nable to navigate among those tables, capturing their relationships and\ngathering relevant information. But before jumping to LLMs, let’s first\n",
      "content_length": 422,
      "extraction_method": "Direct"
    },
    {
      "page_number": 286,
      "chapter": null,
      "content": "inspect the Chinook database a bit more by setting up the connection with\nPython.\nHow to work with relational\ndatabases in Python\nTo work with relational databases in Python, you need to use a library that\ncan connect to the database and execute SQL queries. Some of these libraries\nare as follows:\nSQLAlchemy: This is an open-source SQL toolkit and object-relational\nmapper (ORM) for Python. It allows you to create, read, update, and\ndelete data from relational databases using Python objects and methods.\nIt supports many database engines, such as SQLite, MySQL,\nPostgreSQL, and Oracle.\nPsycopg: This is a popular database connector for PostgreSQL. It\nenables you to execute SQL queries and access PostgreSQL features\nfrom Python. It is fast, reliable, and thread-safe.\nMySQLdb: This is a database connector for MySQL. It allows you to\ninteract with MySQL databases from Python using the DB-API 2.0\nspecification. It is one of the oldest and most widely used Python\nlibraries for MySQL, but its development is mostly frozen.\ncx_Oracle: This is a database connector for Oracle Database. It enables\nyou to connect to Oracle databases and use SQL and PL/SQL features\nfrom Python. It supports advanced features such as object types, Large\nObjects (LOBs), and arrays.\nsqlite3: This is a database connector for SQLite3, a widely used,\nlightweight, serverless, self-contained, and open-source relational\n",
      "content_length": 1400,
      "extraction_method": "Direct"
    },
    {
      "page_number": 287,
      "chapter": null,
      "content": "database management system. You can use sqlite3 to create, query,\nupdate, and delete data from SQLite databases in your Python programs\nSince we are going to work with SQLite, we will use the sqlite3 module,\nwhich you will need to install via pip install sqlite3. Some of the\nfeatures of sqlite3 are as follows:\nIt follows the DB-API 2.0 specification, which defines a standard\ninterface for Python database access modules.\nIt supports transactions, which allow you to execute multiple SQL\nstatements as a single unit of work and roll back in case of errors.\nIt allows you to use Python objects as parameters and results for SQL\nqueries, using various adapters and converters.\nIt supports user-defined functions, aggregates, collations, and\nauthorizers, which enable you to extend the functionality of SQLite\nwith Python code.\nIt has a built-in row factory, which returns query results as named\ntuples or dictionaries instead of plain tuples.\nLet’s see an example of this connection using our Chinook database:\n1. The database can be downloaded locally from\nhttps://www.sqlitetutorial.net/wp-\ncontent/uploads/2018/03/chinook.zip. You will only\nneed to unzip the chinook.db file and it will be ready to be consumed.\nIn the following code, we are initializing a connection (conn) to our\nchinook.db, which will be used to interact with the database. Then, we\nwill save our tables in a pandas object with the read_sql module,\nwhich allows you to run SQL queries against your database:\n",
      "content_length": 1481,
      "extraction_method": "Direct"
    },
    {
      "page_number": 288,
      "chapter": null,
      "content": "import sqlite3\nimport pandas as pd\n## creating a connection\ndatabase = 'chinook.db'\nconn = sqlite3.connect(database)\n## importing tables\ntables = pd.read_sql(\"\"\"SELECT name, type\n                        FROM sqlite_master\n                         WHERE type IN (\"table\", \"view\nHere is the output that we can see:\nFigure 8.3: A list of tables within the Chinook database\n",
      "content_length": 370,
      "extraction_method": "Direct"
    },
    {
      "page_number": 289,
      "chapter": null,
      "content": "Note\nColumn names might be slightly different as the online\ndatabase is updated over time. To get up-to-date\ncolumns’ naming conventions, you can run the\nfollowing command:\npd.read_sql(\"PRAGMA table_info(customers);\", \nprint(customer_columns)\n2. We can also inspect the single table to gather some relevant data. For\nexample, let’s say we want to see the top five countries per album sales:\npd.read_sql(\"\"\"\nSELECT c.country AS Country, SUM(i.total) AS Sales\nFROM customer c\nJOIN invoice i ON c.customer_id = i.customer_id\nGROUP BY Country\nORDER BY Sales DESC\nLIMIT 5;\n\"\"\", conn)\nHere is the corresponding output:\n",
      "content_length": 613,
      "extraction_method": "Direct"
    },
    {
      "page_number": 290,
      "chapter": null,
      "content": "Figure 8.4: Top 5 countries with highest sales\n3. Finally, we can also use the matplotlib Python library to create useful\ndiagrams about the database’s statistics. In the following Python\nsnippet, we are going to run an SQL query to extract the number of\ntracks grouped by genre, and then plot the result using matplotlib as\nfollows:\nimport matplotlib.pyplot as plt\n# Define the SQL query\nsql = \"\"\"\nSELECT g.Name AS Genre, COUNT(t.track_id) AS Tracks\nFROM genre g\nJOIN track t ON g.genre_id = t.genre_id\nGROUP BY Genre\nORDER BY Tracks DESC;\n\"\"\"\n# Read the data into a dataframe\ndata = pd.read_sql(sql, conn)\n# Plot the data as a bar chart\nplt.bar(data.Genre, data.Tracks)\nplt.title(\"Number of Tracks by Genre\")\nplt.xlabel(\"Genre\")\nplt.ylabel(\"Tracks\")\n",
      "content_length": 752,
      "extraction_method": "Direct"
    },
    {
      "page_number": 291,
      "chapter": null,
      "content": "plt.xticks(rotation=90)\nplt.show()\nWe’ll see the following output:\nFigure 8.5: Number of tracks by genre\nAs you can see, in order to gather relevant information from our database,\nwe used the syntax of SQL. Our goal is to gather information by simply\n",
      "content_length": 251,
      "extraction_method": "Direct"
    },
    {
      "page_number": 292,
      "chapter": null,
      "content": "asking in natural language, and we are going to do so starting in the next\nsection.\nImplementing the DBCopilot\nwith LangChain\nIn this section, we are going to cover the architecture and implementation\nsteps behind a DBCopilot application, a natural language interface to chat\nwith database-structured data. In the upcoming sections, we will explore\nhow to achieve that by leveraging a powerful LangChain component called\nSQL Agent.\nLangChain agents and SQL\nAgent\nIn Chapter 4, we introduced the concept of LangChain agents, defining them\nas entities that drive decision making within LLM-powered applications.\nAgents have access to a suite of tools and can decide which tool to call based\non the user input and the context. Agents are dynamic and adaptive, meaning\nthat they can change or adjust their actions based on the situation or the goal.\nIn this chapter, we will see agents in action, using the following LangChain\ncomponents:\ncreate_sql_agent: An agent designed to interact with relational\ndatabases\nSQLDatabaseToolkit: A toolkit to provide the agent with the required\nnon-parametric knowledge\n",
      "content_length": 1103,
      "extraction_method": "Direct"
    },
    {
      "page_number": 293,
      "chapter": null,
      "content": "OpenAI: An LLM to act as the reasoning engine behind the agent, as\nwell as the generative engine to produce conversational results\nLet’s start with our implementation by following these steps:\n1. We’ll first initialize all the components and establish the connection to\nthe Chinook database, using the SQLDatabase LangChain component\n(which uses SQLAlchemy under the hood and is used to connect to our\ndatabase):\nfrom langchain.agents import create_sql_agent\nfrom langchain.llms import OpenAI\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.agents.agent_toolkits import SQLDatabas\nfrom langchain.sql_database import SQLDatabase\nfrom langchain.llms.openai import OpenAI\nfrom langchain.agents import AgentExecutor\nfrom langchain.agents.agent_types import AgentType\nfrom langchain.chat_models import ChatOpenAI\nllm = OpenAI()\ndb = SQLDatabase.from_uri('sqlite:///chinook.db')\ntoolkit = SQLDatabaseToolkit(db=db, llm=llm)\nagent_executor = create_sql_agent(\n    llm=llm,\n    toolkit=toolkit,\n    verbose=True,\n    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n)\n2. Before running the agent, let’s first inspect its available tools:\n[tool.name for tool in toolkit.get_tools()]\nHere is the output:\n",
      "content_length": 1207,
      "extraction_method": "Direct"
    },
    {
      "page_number": 294,
      "chapter": null,
      "content": "['sql_db_query', 'sql_db_schema', 'sql_db_list_tables', 'sq\nThose tools have the following capabilities:\nsql_db_query: This takes as input a detailed and correct SQL\nquery, and it outputs a result from the database. If the query is not\ncorrect, an error message will be returned.\nsql_db_schema: This takes as input a comma-separated list of\ntables, and it outputs the schema and sample rows for those\ntables.\nsql_db_list_tables: This takes as input an empty string, and it\noutputs a comma-separated list of tables in the database.\nsql_db_query_checker: This tool double-checks whether the\nquery is correct before executing it.\n3. Let’s now execute our agent with a simple query to describe the\nplaylisttrack table:\nagent_executor.run(\"Describe the playlisttrack table\")\nThe following output is then obtained (the output is truncated – you\ncan find the full output in the book’s GitHub repository):\n> Entering new AgentExecutor chain...\nAction: sql_db_list_tables\nAction Input:\nObservation: album, artist, customer, employee, genre, invo\nThought: The table I need is playlist_track\nAction: sql_db_schema\nAction Input: playlist_track\nObservation:\n",
      "content_length": 1145,
      "extraction_method": "Direct"
    },
    {
      "page_number": 295,
      "chapter": null,
      "content": "CREATE TABLE playlist_track (\n[...]\n> Finished chain.\n'The playlist_track table contains the playlist_id and trac\nAs you can see, with a simple question in natural language, our agent was\nable to understand its semantics, translate it into an SQL query, extract the\nrelevant information, and use it as context to generate the response.\nBut how was it able to do all of that? Under the hood, the SQL agent comes\nwith a default prompt template, which makes it tailored to this type of\nactivity. Let’s see the default template of the LangChain component:\nprint(agent_executor.agent.llm_chain.prompt.template)\nHere is the output obtained:\nYou are an agent designed to interact with a SQL database.\nGiven an input question, create a syntactically correct sqlite qu\nUnless the user specifies a specific number of examples they wish\nYou can order the results by a relevant column to return the most\nNever query for all the columns from a specific table, only ask f\nYou have access to tools for interacting with the database.\nOnly use the below tools. Only use the information returned by th\nYou MUST double check your query before executing it. If you get \nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.\nIf the question does not seem related to the database, just retur\nsql_db_query: Input to this tool is a detailed and correct SQL qu\nsql_db_schema: Input to this tool is a comma-separated list of ta\nBe sure that the tables actually exist by calling sql_db_list_tab\nsql_db_list_tables: Input is an empty string, output is a comma s\nsql_db_query_checker: Use this tool to double check if your query\nUse the following format:\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [sql_db_query, sql_d\n",
      "content_length": 1799,
      "extraction_method": "Direct"
    },
    {
      "page_number": 296,
      "chapter": null,
      "content": "Action Input: the input to the action\n...\nQuestion: {input}\nThought: I should look at the tables in the database to see what \n{agent_scratchpad}\nThanks to this prompt template, the agent is able to use the proper tools and\ngenerate a SQL query, without modifying the underlying database (you can\nsee the explicit rule not to run any data manipulation language (DML)\nstatements).\nDefinition\nDML is a class of SQL statements that are used to query,\nedit, add, and delete row-level data from database tables or\nviews. The main DML statements are as follows:\nSELECT: This is used to retrieve data from one or more\ntables or views based on specified criteria.\nINSERT: This is used to insert new data records or rows\ninto a table.\nUPDATE: This is used to modify the values of existing\ndata records or rows in a table.\nDELETE: This is used to remove one or more data\nrecords or rows from a table.\nMERGE: This is used to combine the data from two tables\ninto one based on a common column.\nDML statements are used to store, modify, retrieve,\ndelete, and update data in a database.\n",
      "content_length": 1072,
      "extraction_method": "Direct"
    },
    {
      "page_number": 297,
      "chapter": null,
      "content": "We can also see how the agent is able to correlate more than one table within\nthe database:\nagent_executor.run('what is the total number of tracks and the a\nFrom the first lines of the chain, you can see that Action Input invokes two\ntables – track and genre:\n> Entering new AgentExecutor chain...\nAction: sql_db_list_tables\nAction Input:\nObservation: album, artist, customer, employee, genre, invoice, \nThought: I should look at the schema of the track and genre tabl\nAction: sql_db_schema\nAction Input: track, genre\n[…]\nThe following is the output:\n'The top 10 genres by track count and average track length are Ro\nNow, the question is as follows: are we sure that we are getting the proper\nresult? A nice way to double-check this would be to print the SQL query that\nthe agent ran against the database. To do so, we can modify the default\nprompt to ask the agent to explicitly show us the reasoning behind its result.\nPrompt engineering\nAs we saw in the previous chapter, pre-built LangChain agents and chains\ncome with default prompts, which make it easier to tailor them toward their\n",
      "content_length": 1089,
      "extraction_method": "Direct"
    },
    {
      "page_number": 298,
      "chapter": null,
      "content": "goals. Nevertheless, we can customize that prompt and pass it as a parameter\nto our component. For example, let’s say that we want our SQL agent to\nprint the SQL query it used to return the result.\nFirst of all, we have to understand which kind of prompt chunks the SQL\nAgent is able to take as parameters. To do so, we can simply inspect the\nobjects running create_sql_agent.\nFigure 8.6: A screenshot of the description of the SQL agent\nThe Agent takes a prompt prefix and a format instruction, which are merged\nand constitute the default prompt we inspected in the previous section. To\nmake our agent more self-explanatory, we will create two variables, prefix\nand format_instructions, which will be passed as parameters and that\nslightly modify the default prompt as follows (you can find the whole\nprompts in the GitHub repository at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications:\n",
      "content_length": 909,
      "extraction_method": "Direct"
    },
    {
      "page_number": 299,
      "chapter": null,
      "content": "We have the prompt_prefix, which is already configured as follows:\nprefix: 'str' = 'You are an agent designed to interact\nTo this, we will add the following line of instruction:\nAs part of your final answer, ALWAYS include an explanation\nIn prompt_format_instructions, we will add the following example of\nexplanation using few-shot learning, which we covered in Chapter 1:\nExplanation:\n<===Beginning of an Example of Explanation:\nI joined the invoices and customers tables on the cust\n```sql\nSELECT c.country AS Country, SUM(i.total) AS Sales\nFROM customer c\nJOIN invoice i ON c.customer_id = i.customer_id\nGROUP BY Country\nORDER BY Sales DESC\nLIMIT 5;\n```sql\n===>End of an Example of Explanation\nNow, let’s pass those prompt chunks as parameters to our agent and print the\nresult (I will omit the whole chain here, but you can see it in the GitHub\nrepository):\nagent_executor = create_sql_agent(\n    prefix=prompt_prefix,\n    format_instructions = prompt_format_instructions,\n    llm=llm,\n    toolkit=toolkit,\n",
      "content_length": 1012,
      "extraction_method": "Direct"
    },
    {
      "page_number": 300,
      "chapter": null,
      "content": "    verbose=True,\n    top_k=10\n)\nresult = agent_executor.run(\"What are the top 5 best-selling alb\nprint(result)\nHere is the obtained output:\nThe top 5 best-selling albums and their artists are 'A Matter of \nExplanation: I joined the album and invoice tables on the album_i\n```sql\nSELECT al.title AS Album, ar.name AS Artist, SUM(i.total) AS Sale\nFROM album al\nJOIN invoice i ON al.album_id = i.invoice_id\nJOIN artist ar ON al.artist_id = ar.artist_id\nGROUP BY ar.name\nORDER BY Sales\nNow, in our result, we have a clear explanation of the thought process as\nwell as the printed query our agent made for us. This is key if we want to\ndouble-check the correctness of the reasoning procedure happening in the\nbackend of our agent.\nThis is already extremely useful, but we want to bring it to the next level: we\nwant our DBCopilot to also be able to generate graphs and save results in\nour local file system. To achieve this goal, we need to add tools to our agent,\nand we are going to do so in the next section.\nAdding further tools\nIn order to make our DBCopilot more versatile, there are two further\ncapabilities we need to add:\n",
      "content_length": 1127,
      "extraction_method": "Direct"
    },
    {
      "page_number": 301,
      "chapter": null,
      "content": "PythonREPLTool: This tool allows you to interact with the Python\nprogramming language using natural language. You can use this tool to\nwrite, run, and debug Python code without having to use a script file or\nan IDE. You can also use this tool to access and manipulate various\nPython modules, libraries, and data structures. We will need this tool\nto produce the matplotlib graphs from the SQL query’s results.\nDefinition\nREPL is an acronym for read-eval-print loop, which is\na term that describes an interactive shell or\nenvironment that allows you to execute code and see\nthe results immediately. REPL is a common feature of\nmany programming languages, such as Python, Ruby,\nand Lisp.\nIn the context of LangChain, REPL is a feature that\nallows you to interact with LangChain agents and tools\nusing natural language. You can use REPL in\nLangChain to test, debug, or experiment with different\nagents and tools without having to write and run a\nscript file. You can also use REPL in LangChain to\naccess and manipulate various data sources, such as\ndatabases, APIs, and web pages.\nFileManagementToolkit: This is a set of tools, or toolkit, that allows\nyou to interact with the file system of your computer or device using\nnatural language. You can use this toolkit to perform various operations\non files and directories, such as creating, deleting, renaming, copying,\nmoving, searching, reading, and writing. You can also use this toolkit to\n",
      "content_length": 1439,
      "extraction_method": "Direct"
    },
    {
      "page_number": 302,
      "chapter": null,
      "content": "access and manipulate the metadata and attributes of files and\ndirectories, such as name, size, type, date, and permissions.\nWe will need this toolkit to save the graphs generated by our agent in\nour working directory.\nNow, let’s see how we can add these tools to our DBCopilot:\n1. First, we define the list of tools for our agent:\nfrom  langchain_experimental.tools.python.tool import \nfrom  langchain_experimental.python import PythonREPL\nfrom langchain.agents.agent_toolkits import FileManage\nworking_directory  = os.getcwd()\ntools = FileManagementToolkit(\n    root_dir=str(working_directory),\n    selected_tools=[\"read_file\", \"write_file\", \"list_d\ntools.append(\n    PythonREPLTool())\ntools.extend(SQLDatabaseToolkit(db=db, llm=llm).get_to\n2. In order to leverage that heterogeneous set of tools – SQL Database,\nPython REPL, and File System\n(https://python.langchain.com/v0.1/docs/integrat\nions/tools/filesystem/) – we cannot work anymore with the\nSQL Database-specific agent, since its default configurations are meant\nto only accept SQL-related contents. Henceforth, we need to set up an\nagnostic agent that is able to use all of the tools that we provide it with.\nFor this purpose, we are going to use the\nSTRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION agent type, which is\nable to use a multi-tool input.\nLet’s first start with initializing the agent and asking it to produce a\nbar chart and save it in the current working directory for the top five\n",
      "content_length": 1454,
      "extraction_method": "Direct"
    },
    {
      "page_number": 303,
      "chapter": null,
      "content": "countries for sales (note that, for this purpose, I’ve used a chat model\nas best suited for the type of agent in use):\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.agents import initialize_agent, Tool\nfrom langchain.agents import AgentType\nmodel = ChatOpenAI()\nagent = initialize_agent(\n    tools, model, agent= AgentType.STRUCTURED_CHAT_ZERO_SHO\n)\nagent.run(\"generate a matplotlib bar chart of the top 5 cou\nWe then receive the following output, showing how, in this case, the\nagent was also able to dynamically orchestrate the available tools to\ngenerate the final answer (I will report here just the main actions of\nthe chain – you can see the whole code in the GitHub repository of the\nbook):\n> Entering new AgentExecutor chain...\nAction:\n```\n{\n  \"action\": \"sql_db_query\",\n  \"action_input\": \"SELECT billing_country as Country, SUM(t\n}\n```\n[…]\nObservation: [('USA', 10405.889999999912), ('Canada', 5489\n[…]\nWe have successfully retrieved the top 5 countries for sale\nAction:\n```\n{\n  \"action\": \"Python_REPL\",\n  \"action_input\": \"import matplotlib.pyplot as plt\\nsales_d\n}\n```\n",
      "content_length": 1092,
      "extraction_method": "Direct"
    },
    {
      "page_number": 304,
      "chapter": null,
      "content": "[…]\n> Finished chain.\n'Here is the bar chart of the top 5 countries for sales fro\nThe following is the generated chart of the top five countries by sales,\nas requested:\nFigure 8.7: Bar chart of top five countries by sales\nGreat! The agent was able to first invoke the SQL tool to retrieve the\nrelevant information, then it used the Python tool to generate the matplotlib\nbar chart. Then, it used the file system tool to save the result as PNG.\n",
      "content_length": 444,
      "extraction_method": "Direct"
    },
    {
      "page_number": 305,
      "chapter": null,
      "content": "Also, in this case, we can modify the prompt of the agent. For example, we\nmight want the agent to provide an explanation not only of the SQL query\nbut also of the Python code. To do so, we need to define the prompt_prefix\nand prompt_format_instructions variables to be passed as kgwargs to the\nagent as follows:\nprompt_prefix = \"\"\" Your prefix here\n\"\"\"\nprompt_format_instructions= \"\"\"\nYour instructions here.\n\"\"\"\nagent = initialize_agent(tools, model, agent=AgentType.STRUCTURE\n                         agent_kwargs={\n                            'prefix':prompt_prefix,\n                            'format_instructions': prompt_format\nThanks to LangChain’s tools components, we were able to extend our\nDBCopilot capabilities and make it more versatile, depending upon the\nuser’s query.\nWith the same logic, we can tailor our agents to any domain, adding or\nremoving tools so that we can control its perimeter of actions. Plus, thanks to\nthe prompt customization, we can always refine the agent’s backend logic to\nmake it more customized.\nDeveloping the front-end\nwith Streamlit\nNow that we have seen the logic behind an LLM-powered DBCopilot, it is\ntime to give a GUI to our application. To do so, we will once again leverage\nStreamlit. As always, you can find the whole Python code in the GitHub\n",
      "content_length": 1298,
      "extraction_method": "Direct"
    },
    {
      "page_number": 306,
      "chapter": null,
      "content": "book repository at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications.\nAs per the previous sections, you need to create a .py file to run in your\nterminal via streamlit run file.py. In our case, the file will be named\ndbcopilot.py.\nHere are the main steps to set up the frontend:\n1. Configure the application web page:\nimport streamlit as st\nst.set_page_config(page_title=\"DBCopilot\", page_icon=\"\nst.header('\n Welcome to DBCopilot, your copilot for st\n2. Import the credentials and establish the connection with the Chinook\ndatabase:\nload_dotenv()\n#os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\nopenai_api_key = os.environ['OPENAI_API_KEY']\ndb = SQLDatabase.from_uri('sqlite:///chinook.db')\n3. Initialize the LLM and the toolkit:\nllm = OpenAI()\ntoolkit = SQLDatabaseToolkit(db=db, llm=llm)\n4. Initialize the Agent using the prompt variables defined in the previous\nsections:\n",
      "content_length": 887,
      "extraction_method": "Direct"
    },
    {
      "page_number": 307,
      "chapter": null,
      "content": "agent_executor = create_sql_agent(\n    prefix=prompt_prefix,\n    format_instructions = prompt_format_instructions,\n    llm=llm,\n    toolkit=toolkit,\n    verbose=True,\n    top_k=10\n)\n5. Define Streamlit’s session states to make it conversational and memory\naware:\nif \"messages\" not in st.session_state or st.sidebar.bu\n    st.session_state[\"messages\"] = [{\"role\": \"assistan\nfor msg in st.session_state.messages:\n    st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n6. Finally, define the logic of the application whenever a user makes a\nquery:\nif user_query:\n    st.session_state.messages.append({\"role\": \"user\", \n    st.chat_message(\"user\").write(user_query)\n    with st.chat_message(\"assistant\"):\n        st_cb = StreamlitCallbackHandler(st.container(\n        response = agent_executor.run(user_query, call\n        st.session_state.messages.append({\"role\": \"ass\n        st.write(response)\nYou can run your application in the terminal with the streamlit run\ncopilot.py command. The final web page looks as follows:\n",
      "content_length": 1017,
      "extraction_method": "Direct"
    },
    {
      "page_number": 308,
      "chapter": null,
      "content": "Figure 8.8: Screenshot of the front-end of DBCopilot\n",
      "content_length": 53,
      "extraction_method": "Direct"
    },
    {
      "page_number": 309,
      "chapter": null,
      "content": "Thanks to the StreamlitCallbackHandler module, we can also expand each\naction the agent took, for example:\nFigure 8.9: Illustration of the agent’s actions during the chain\nWith just a few lines of code, we were able to set up a simple front-end for\nour DBCopilot with a conversational user interface.\nSummary\nIn this chapter, we saw how LLMs are not only capable of interacting with\ntextual and unstructured data, but also with structured and numeric data.\nThis is made possible because of two main elements: the natural capabilities\nof LLMs and, more generally, LFMs for understanding a problem’s\n",
      "content_length": 598,
      "extraction_method": "Direct"
    },
    {
      "page_number": 310,
      "chapter": null,
      "content": "statement, planning a resolution, and acting as reasoning engines, as well as\na set of tools that extend LLMs’ capabilities with domain-specific skills.\nIn this case, we mainly relied upon LangChain’s SQL Database toolkit,\nwhich connects the Agent to an SQL database with a curated prompt.\nFurthermore, we extended the Agent’s capabilities even further, making it\nable to generate matplotlib graphs, with the Python REPL tool, and save the\noutput to our local file system with the File Management tool.\nIn the next chapter, we are going to delve even deeper into the analytical\ncapabilities of LLMs. More specifically, we are going to cover their\ncapabilities of working with code.\nReferences\nChinook Database:\nhttps://github.com/lerocha/chinook-\ndatabase/tree/master/ChinookDatabase/DataSources\nLangChain File system tool:\nhttps://python.langchain.com/docs/integrations/t\nools/filesystem\nLangChain Python REPL tool:\nhttps://python.langchain.com/docs/integrations/t\noolkits/python\nJoin our community on\nDiscord\n",
      "content_length": 1011,
      "extraction_method": "Direct"
    },
    {
      "page_number": 311,
      "chapter": null,
      "content": "Join our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\nOceanofPDF.com\n",
      "content_length": 124,
      "extraction_method": "Direct"
    },
    {
      "page_number": 312,
      "chapter": null,
      "content": "9\nWorking with Code\nIn this chapter, we are going to cover another great capability of Large\nLanguage Models, that is, working with programming languages. In the\nprevious chapter, we’ve already seen a glimpse of this capability, namely,\nSQL query generation in a SQL database. In this chapter, we are going to\nexamine the other ways in which LLMs can be used with code, from\n“simple” code generation to interaction with code repositories and, finally, to\nthe possibility of letting an application behave as if it were an algorithm. By\nthe end of this chapter, you will be able to leverage LLMs to code-related\nprojects, as well as build LLM-powered applications with natural language\ninterfaces to work with code.\nThroughout this chapter, we will cover the following topics:\nAnalysis of the main LLMs with top-performing code capabilities\nUsing LLMs for code understanding and generation\nBuilding LLM-powered agents to “act as” algorithms\nLeveraging Code Interpreter\nTechnical requirements\nTo complete the tasks in this chapter, you will need the following:\nA Hugging Face account and user access token.\n",
      "content_length": 1104,
      "extraction_method": "Direct"
    },
    {
      "page_number": 313,
      "chapter": null,
      "content": "An OpenAI account and user access token.\nPython 3.7.1 or a later version.\nPython packages. Make sure you have the following Python packages\ninstalled: langchain, python-dotenv, huggingface_hub, streamlit,\ncodeinterpreterapi, and jupyter_kernel_gateway. Those can be\neasily installed via pip install in your terminal.\nYou can find all the code and examples in the book’s GitHub repository at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications.\nChoosing the right LLM for\ncode\nIn Chapter 3, we described a decision framework to use in order to decide\nthe proper LLM for a given application. Generally speaking, all LLMs are\nendowed with knowledge of code understanding and generation; however,\nsome of them are particularly specialized in doing so. More specifically,\nthere are some evaluation benchmarks – such as the HumanEval – that are\nspecifically tailored to assessing LLMs’ capabilities of working with code.\nThe leaderboard of HumanEval One is a good source for determining the\ntop-performing models, available at\nhttps://paperswithcode.com/sota/code-generation-on-\nhumaneval. HumanEval is a benchmark introduced by OpenAI to assess\nthe code generation capabilities of LLMs, where the model completes\nPython functions based on their signature and docstring. It has been used to\nevaluate models like Codex, demonstrating its effectiveness in measuring\nfunctional correctness.\n",
      "content_length": 1400,
      "extraction_method": "Direct"
    },
    {
      "page_number": 314,
      "chapter": null,
      "content": "In the following screenshot, you can see the situation of the leaderboard as\nof January 2024:\nFigure 9.1: HumanEval benchmark in January 2024\nAs you can see, the majority of the models are fine-tuned versions of the\nGPT-4 (as well as the GPT-4 itself), as it is the state-of-the-art LLM in\nbasically all the domains. Nevertheless, there are many open-source models\nthat reached stunning results in the field of code understanding and\ngeneration, some of which will be covered in the next sections. Another\nbenchmark is Mostly Basic Programming Problems (MBPP), a dataset of\n974 programming tasks in Python, designed to be solvable by entry-level\nprogrammers. Henceforth, when choosing your model for a code-specific\ntask, it might be useful to have a look at these benchmarks as well as other\nsimilar code metrics (we will see throughout the chapter some further\nbenchmarks for code-specific LLMs).\n",
      "content_length": 899,
      "extraction_method": "Direct"
    },
    {
      "page_number": 315,
      "chapter": null,
      "content": "Staying within the scope of coding, below you can find three additional\nbenchmarks often used in the market:\nMultiPL-E: An extension of HumanEval to many other languages,\nsuch as Java, C#, Ruby, and SQL.\nDS-1000: A data science benchmark that tests if the model can write\ncode for common data analysis tasks in Python.\nTech Assistant Prompt: A prompt that tests if the model can act as a\ntechnical assistant and answer programming-related requests.\nIn this chapter, we are going to test different LLMs: two code-specific\n(CodeLlama and StarCoder) and one general-purpose, yet also with\nemerging capabilities in the field of code generation (Falcon LLM).\nCode understanding and\ngeneration\nThe first experiment we are going to run will be code understanding and\ngeneration leveraging LLMs. This simple use case is at the base of the many\nAI code assistants that were developed since the launch of ChatGPT, first\namong all the GitHub Copilot.\nDefinition\nGitHub Copilot is an AI-powered tool that assists developers\nin writing code more efficiently. It analyzes code and\ncomments to provide suggestions for individual lines and\nentire functions. The tool is developed by GitHub, OpenAI,\nand Microsoft and supports multiple programming\nlanguages. It can perform various tasks such as code\n",
      "content_length": 1284,
      "extraction_method": "Direct"
    },
    {
      "page_number": 316,
      "chapter": null,
      "content": "completion, modification, explanation, and technical\nassistance.\nIn this experiment, we are going to try three different models: Falcon LLM,\nwhich we already explored in Chapter 3; CodeLlama, a fine-tuned version of\nMeta AI’s Llama; and StarCoder, a code-specific model that we are going to\ninvestigate in the upcoming sections.\nSince those models are pretty heavy to run on a local machine, for this\npurpose I’m going to use a Hugging Face Hub Inference Endpoint, with a\nGPU-powered virtual machine. You can link one model per Inference\nEndpoint and then embed it in your code, or use the convenient library\nHuggingFaceEndpoint, available in LangChain.\nTo start using your Inference Endpoint, you can use the following code:\nllm = HuggingFaceEndpoint(endpoint_url = \"your_endpoint_url\", ta\n        model_kwargs = {\"max_new_tokens\": 1100})\nAlternatively, you can copy and paste the Python code provided on your\nendpoint’s webpage at\nhttps://ui.endpoints.huggingface.co/user_name/endpo\nints/your_endpoint_name:\n",
      "content_length": 1010,
      "extraction_method": "Direct"
    },
    {
      "page_number": 317,
      "chapter": null,
      "content": "Figure 9.2: User interface of the Hugging Face Inference Endpoint\nTo create your Hugging Face Inference Endpoint, you can follow the\ninstructions at https://huggingface.co/docs/inference-\nendpoints/index.\nYou can always leverage the free Hugging Face API as described in Chapter\n4, but you have to expect some latency when running the models.\nFalcon LLM\nFalcon LLM is an open-source model developed by Abu Dhabi’s\nTechnology Innovation Institute (TII) and launched on the market in May\n2023. It is an autoregressive, decoder-only transformer, trained on 1 trillion\ntokens, and has 40 billion parameters (although it has also been released as a\nlighter version with 7 billion parameters). As discussed in Chapter 3,\n“small” language models are a representation of a new trend of LLMs,\n",
      "content_length": 784,
      "extraction_method": "Direct"
    },
    {
      "page_number": 318,
      "chapter": null,
      "content": "consisting of building lighter models (with fewer parameters) that focus\ninstead on the quality of the training dataset.\nTo start using Falcon LLM, we can follow these steps:\n1. We can leverage the HuggingFaceHub wrapper available in LangChain\n(remember to set the Hugging Face API in the .env file, passing your\nsecrets as os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] =\nHUGGINGFACEHUB_API_TOKEN):\nfrom langchain import HuggingFaceHub\nfrom langchain import PromptTemplate, LLMChain\nimport os\nload_dotenv()hugging_face_api = os.environ[\"HUGGINGFAC\nrepo_id = \"tiiuae/falcon-7b-instruct\"\nllm = HuggingFaceHub(\n    repo_id=repo_id,  model_kwargs={\"temperature\": 0.2\n)\n2. Now that we’ve initialized the model, let’s ask it to generate the code\nfor a simple webpage:\nprompt = \"\"\"\nGenerate a short html code to a simple webpage with a \n<!DOCTYPE html>\n<html>\n\"\"\"\nprint(llm(prompt))\nThe following is the corresponding output:\n<head>\n    <title>My Webpage</title>\n</head>\n",
      "content_length": 957,
      "extraction_method": "Direct"
    },
    {
      "page_number": 319,
      "chapter": null,
      "content": "<body>\n    <h1>My Webpage</h1>\n    <h2>Subheader</h2>\n    <p>This is the text body.</p>\n</body>\n</html>\n3. If you save it as an HTML file and execute it, the result will look like\nthe following:\nFigure 9.3: Sample webpage generated by FalconLLM\n4. We can also try to generate a Python function to generate random\npasswords:\nprompt = \"\"\"\nGenerate a python program that create random password \n\"\"\"\nprint(llm(prompt))\nHere is our output:\nimport random\ndef generate_password():\n    chars = \"abcdefghijklmnopqrstuvwxyz0123456789\"\n",
      "content_length": 525,
      "extraction_method": "Direct"
    },
    {
      "page_number": 320,
      "chapter": null,
      "content": "    lenght = 12\n    num = random.randint(1, 9)\n    cap = random.randint(1, 9)\n    password = \"\"\n    for i in range(lenght):\n        password += chars[random.randint(0, 9)]\n    password += num\n    password += cap\n    return password\nprint(generate_password())\nWe now have a function named generate_password(), which uses\nrandom functions to generate a password as per our prompt.\n5. Finally, let’s do the opposite, asking the model to explain to us the\nabove code:\nprompt = \"\"\"\nExplain to me the following code:\ndef generate_password():\n    chars = \"abcdefghijklmnopqrstuvwxyz0123456789\"\n    lenght = 12\n    num = random.randint(1, 9)\n    cap = random.randint(1, 9)\n    password = \"\"\n    for i in range(lenght):\n        password += chars[random.randint(0, 9)]\n    password += num\n    password += cap\n    return password\nprint(generate_password())\n\"\"\"\nprint(llm(prompt))\nHere is the obtained output:\n<p>The code generates a random password of length 12 charac\n",
      "content_length": 958,
      "extraction_method": "Direct"
    },
    {
      "page_number": 321,
      "chapter": null,
      "content": "Overall, even if not code-specific, the model was able to correctly perform\nall the tasks. Note also that this is the “light” version of the model (7 billion\nparameters), yet its performance is great.\nLet’s now investigate the capabilities of CodeLlama.\nCodeLlama\nCodeLlama is a family of LLMs for code based on Llama 2, which is a\ngeneral-purpose language model developed by Meta AI (as discussed in\nChapter 3). CodeLlama can generate and discuss code in various\nprogramming languages, such as Python, C++, Java, PHP, and more.\nCodeLlama can also perform infilling, which is the ability to fill in missing\nparts of code based on the surrounding context, as well as follow instructions\ngiven in natural language and produce code that matches the desired\nfunctionality.\nThe model comes in three sizes (7B, 13B, and 34B parameters) and three\nflavors (base model, Python fine-tuned, and instruction-tuned) to cover a\nwide range of applications. CodeLlama is trained on sequences of 16k\ntokens and can handle inputs with up to 100k tokens.\nIn the model paper “Code Llama: Open Foundation Models for Code” by\nRozière Baptiste et al, released in August 2023, the authors describe how the\nvarious models were tested against some of the most popular evaluation\nbenchmarks in the domain of code understanding and generation, including\nHumanEval and MBPP, according to which CodeLlama models achieved a\nscore up to 53% and 55%, respectively. On top of those remarkable results, it\nis stunning that the Python fine-tuned CodeLlama’s smallest size (7 billion\nparameters) outperformed the largest version of Llama 2 (70 billion\nparameters) on HumanEval and MBPP.\n",
      "content_length": 1650,
      "extraction_method": "Direct"
    },
    {
      "page_number": 322,
      "chapter": null,
      "content": "Now, let’s run some tests with this model. As per the previous section, we\ncan initialize the model leveraging either the Hugging Face Inference API\n(pay per use) or the free Hugging Face API (with the constraint of higher\nlatency). You can consume it as follows:\nrepo_id = \"codellama/CodeLlama-7b-Instruct-hf\"\nllm = HuggingFaceHub(    repo_id=repo_id,  model_kwargs={\"temper\n)\nLet’s now test it with some code tasks. The first task will be that of\noptimizing Python code so that it runs more efficiently. Let’s see how our\nmodel performs in this task. In the following code snippet, we simply\nprompt the model to regenerate the provided code in a more efficient way:\nprompt = \"\"\"\nRegenerate the below code in a more efficient way.\n```\ndef factorial(n):\n    result = 1\n    for i in range(1, n + 1):\n        result *= i\n    return result\n# Example usage:\nn = 5\nprint(\"Factorial of\", n, \"is\", factorial(n))\n```\n\"\"\"\nprint(llm(prompt))\nHere is our output:\n### Solution\n```\ndef factorial(n):\n",
      "content_length": 987,
      "extraction_method": "Direct"
    },
    {
      "page_number": 323,
      "chapter": null,
      "content": "    if n == 0:\n        return 1\n    else:\n        return n * factorial(n - 1)\n```\n### Explanation\nThe solution is a recursive function that calls itself until the \n### References\n- [Wikipedia](https://en.wikipedia.org/wiki/Factorial)\n- [Math is Fun](https://www.mathsisfun.com/numbers/factorial.html\n- [Mathematics Stack Exchange](https://math.stackexchange.com/que\n- [Mathematics Stack Exchange](https://math.stackexchange.com/que\nAs you can see, the model was able to use a recursive approach, which is\nmore efficient and “Pythonic.” It also provides a reference for the user to\ndive deeper into the mathematical theory behind the function.\nNext, let’s leverage the model’s completion capabilities by initializing a\nfunction to remove non-ASCII characters.\nDefinition\nAmerican Standard Code for Information Interchange\n(ASCII) is a character encoding standard that uses 7 bits to\nrepresent 128 characters, such as letters, digits, punctuation\nmarks, and control codes.\nNon-ASCII characters are those that are not part of the\nASCII standard and use more than 7 bits to encode. They\ninclude special characters such as letters with accents,\nglyphs, ideograms, and mathematical symbols. Non-ASCII\ncharacters can be encoded using different standards, such as\nUnicode, ISO 8859-1, Windows-1252, etc.\n",
      "content_length": 1296,
      "extraction_method": "Direct"
    },
    {
      "page_number": 324,
      "chapter": null,
      "content": "For example, the letter é is a non-ASCII character that can be\nencoded using Unicode as U+00E9 or using Windows-1252\nas 0xE9.\nAccordingly, here is the code to generate the function:\nprompt = '''\nDefine a python function that remove non-ascii character from a \n'''\nprint(llm(prompt))\nThe following is the function that we receive as the output:\ndef remove_non_ascii(string):\n    return string.encode('ascii', 'ignore').decode('utf-8')\nLet’s now leverage the model as a bug fixer, prompting it with the wrong\nfunction and also asking it to provide an explanation of why it is wrong and\nhow it can be fixed:\nprompt = \"\"\"\nIdentify the bug in the following code:.\n```\n#wrong function\nimport random\na = random.randint(1, 12)\nb = random.randint(1, 12)\nfor i in range(10):\n    question = \"What is \" + a + \" x \" + b + \"? \"\n    answer = input(question)\n    if answer = a * b:\n        print (Well done!)\n    else:\n        print(\"No.\")\n",
      "content_length": 924,
      "extraction_method": "Direct"
    },
    {
      "page_number": 325,
      "chapter": null,
      "content": "```\nExplanation:\n\"\"\"\nprint(llm(prompt))\nHere is our output:\n- The function is using the wrong syntax for the `randint` functi\n- The `for` loop is using the wrong syntax. The correct syntax is\n- The `question` and `answer` variables are not being used in the\n- The `if` statement is not being used correctly. It should be `i\n- The `print` statement is not being used correctly. It should be\nFinally, let’s ask the model in natural language to generate specific Python\ncode for a given task that, in our example, will be that of writing a function\nthat finds the longest substring of a given string containing only unique\ncharacters:\nprompt = \"\"\"\nWrite a Python function that finds the longest substring of a gi\n\"\"\"\nprint(llm(prompt))\nWe then get the following function as our output:\ndef longest_unique_substring(s):\n    unique_count = 0\n    longest_substring = \"\"\n    for i in range(1, len(s)):\n        if s[i] not in unique_count:\n            unique_count += 1\n            longest_substring = s[i]\n    return longest_substring\n",
      "content_length": 1028,
      "extraction_method": "Direct"
    },
    {
      "page_number": 326,
      "chapter": null,
      "content": "As per the Falcon LLM, in this case we used the light version of the model\n(7 billion parameters), still obtaining great results. This is a perfect example\nof how the task you want to address with your application must be a factor in\ndeciding what LLM to use: if you are only interested in code generation,\ncompletion, infilling, debugging, or any other code-related tasks, a light and\nopen-source model could be more than enough, rather than 70 billion\nparameters of a state-of-the-art GPT-4.\nIn the next section, we are going to cover the third and last LLM in the\ncontext of code generation and understanding.\nStarCoder\nThe StarCoder model is an LLM for code that can perform various tasks,\nsuch as code completion, code modification, code explanation, and technical\nassistance. It was trained on permissively licensed data from GitHub,\nincluding from 80+ programming languages, Git commits, GitHub issues,\nand Jupyter notebooks. It has a context length of over 8,000 tokens, which\nenables it to process more input than any other open-source language model.\nIt also has an improved license that simplifies the process for companies to\nintegrate the model into their products.\nThe StarCoder model was evaluated on several benchmarks that test its\nability to write and understand code in different languages and domains,\nincluding the aforementioned HumanEval and MBPP, where the model\nscored, respectively, 33.6% and 52.7%. Additionally, it was tested against\nMultiPL-E (where the model matched or outperformed the code-cushman-\n001 model from OpenAI on many languages), the DS-1000 (where the\nmodel clearly beat the code-cushman-001 model as well as all other open-\n",
      "content_length": 1669,
      "extraction_method": "Direct"
    },
    {
      "page_number": 327,
      "chapter": null,
      "content": "access models), and the Tech Assistant Prompt (where the model was able to\nrespond to various queries with relevant and accurate information).\nAccording to a survey published on May 4 2023 by Hugging Face,\nStarCoder demonstrated great capabilities compared to other models, using\nHumanEval and MBPP as benchmarks. You can see an illustration of this\nstudy below:\nFigure 9.4: Results of evaluation benchmarks for various LLMs. Source:\nhttps://huggingface.co/blog/starcoder\nTo start using StarCoder, we can follow these steps:\n",
      "content_length": 525,
      "extraction_method": "Direct"
    },
    {
      "page_number": 328,
      "chapter": null,
      "content": "1. We can leverage the HuggingFaceHub wrapper available in LangChain\n(remember to set the Hugging Face API in the .env file):\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\nhugging_face_api = os.environ[\"HUGGINGFACEHUB_API_TOKE\n2. Let’s set the repo_id for the StarCoder model and initialize it:\nfrom langchain import HuggingFaceHub\nfrom langchain import PromptTemplate, LLMChain\nrepo_id = \"bigcode/starcoderplus\"\nllm = HuggingFaceHub(\n    repo_id=repo_id,  model_kwargs={\"temperature\": 0.2\n)\nNote\nStarCoder is a gated model on the Hugging Face Hub,\nmeaning that you will need to request access directly from\nthe bigcode/starcoderplus repo before being able to connect\nto it.\nNow that we’re set up, let’s start asking our model to compile some code. To\nstart with, we will ask it to generate a Python function to generate the nth\nFibonacci number:\nprompt = \"\"\"\nHow can I write a Python function to generate the nth Fibonacci \n",
      "content_length": 934,
      "extraction_method": "Direct"
    },
    {
      "page_number": 329,
      "chapter": null,
      "content": "\"\"\"\nprint(llm(prompt))\nDefinition\nThe Fibonacci sequence is a mathematical series that begins\nwith 0 and 1, and each subsequent number is the sum of the\ntwo preceding numbers. For instance, the first 10 numbers of\nthe Fibonacci sequence are 0, 1, 1, 2, 3, 5, 8, 13, 21, and 34.\nThere are different ways to compute the nth Fibonacci\nnumber, which is denoted by F(n). One way is to use a\nrecursive formula:\nThis means that to find F(n), we need to find F(n-1) and F(n-\n2) first, and then add them together. This works for any n\ngreater than or equal to 2. For n equal to 0 or 1, we simply\nreturn n as the answer.\nWe then see the following output:\n",
      "content_length": 645,
      "extraction_method": "Direct"
    },
    {
      "page_number": 330,
      "chapter": null,
      "content": "Figure 9.5: Example of Fibonacci functions generated by StarCode\nAs you can see, it also proposed different approaches to solve the problem,\nalongside the explanation.\nLet’s now ask the model to generate a webpage to play tic tac toe against the\ncomputer:\nprompt = \"\"\"\nGenerate the html code for a single page website that let the us\n \"\"\"\nprint(llm(prompt))\nHere is the corresponding output:\n ## How to use\n```\ngit clone https://github.com/Mohamed-Elhawary/tic-tac-toe.git\ncd tic-tac-toe\n",
      "content_length": 488,
      "extraction_method": "Direct"
    },
    {
      "page_number": 331,
      "chapter": null,
      "content": "python3 -m http.server\n```\n## License\n [MIT](https://choosealicense.com/licenses/mit/)\n<|endoftext|>\nInterestingly enough, the model in this case didn’t generate the whole code;\nrather, it gave the instructions to clone and run a git repository that can\nachieve this result.\nFinally, StarCoder is also available as an extension in VS Code to act as your\ncode copilot. You can find it as HF Code Autocomplete, as shown in the\nfollowing screenshot:\n",
      "content_length": 447,
      "extraction_method": "Direct"
    },
    {
      "page_number": 332,
      "chapter": null,
      "content": "Figure 9.6: Hugging Face Code Autocomplete extension, powered by StarCoder\nOnce enabled, you can see that, while compiling your code, StarCoder will\nprovide suggestions to complete the code. For example:\n",
      "content_length": 204,
      "extraction_method": "Direct"
    },
    {
      "page_number": 333,
      "chapter": null,
      "content": "Figure 9.7: Screenshot of a suggested completion, given a function description\nAs you can see, I commented my code, describing a function to generate the\nnth Fibonacci number, and then started defining the function. Automatically,\nI’ve been provided with the StarCoder auto-completion suggestion.\nCode understanding and generation are great capabilities of LLMs. On top of\nthose capabilities, there are further applications that we can think about,\ngoing beyond code generation. In fact, the code can be seen also as a\nbackend reasoning tool to propose solutions to complex problems, such as an\nenergy optimization problem rather than an algorithm task. To do this, we\ncan leverage LangChain to create powerful agents that can act as if they\nwere algorithms. In the upcoming section, we will see how to do so.\nAct as an algorithm\nSome problems are complex by definition and difficult to solve leveraging\n“only” LLMs’ analytical reasoning skills. However, LLMs are still\n",
      "content_length": 970,
      "extraction_method": "Direct"
    },
    {
      "page_number": 334,
      "chapter": null,
      "content": "intelligent enough to understand the problems overall and leverage their\ncoding capabilities to solve them.\nIn this context, LangChain provides a tool that empowers the LLM to reason\n“in Python,” meaning that the LLM-powered agent will leverage Python to\nsolve complex problems. This tool is the Python REPL, which is a simple\nPython shell that can execute Python commands. The Python REPL is\nimportant because it allows users to perform complex calculations, generate\ncode, and interact with language models using Python syntax. In this section,\nwe will cover some examples of the tool’s capabilities.\nLet’s first initialize our agent using the create_python_agent class in\nLangChain. To do so, we will need to provide this class with an LLM and a\ntool, which, in our example, will be the Python REPL:\nimport os\nfrom dotenv import load_dotenv\nfrom langchain.agents.agent_types import AgentType\nfrom langchain.chat_models import ChatOpenAI\n from langchain_experimental.agents.agent_toolkits.python.base i\nfrom langchain_experimental.tools import PythonREPLTool\nload_dotenv()\nopenai_api_key = os.environ['OPENAI_API_KEY']\nmodel = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\nagent_executor = create_python_agent(\n    llm=model,\n    tool=PythonREPLTool(),\n    verbose=True,\n    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n)\nAs always, before starting to work with the agent, let’s first inspect the\ndefault prompt:\n",
      "content_length": 1428,
      "extraction_method": "Direct"
    },
    {
      "page_number": 335,
      "chapter": null,
      "content": "print(agent_executor.agent.llm_chain.prompt.template)\nHere is our output:\nFigure 9.8: Default prompt of the Python agent\nNow, let’s start with an easy query, asking the model to generate a scatter\nplot based on sample attributes of basketball players:\nquery = \"\"\"\nIn a different basketball game, we have the following player sta\n- Player A: 38 points, 10 rebounds, 7 assists\n- Player B: 28 points, 9 rebounds, 6 assists\n- Player C: 19 points, 6 rebounds, 3 assists\n- Player D: 12 points, 4 rebounds, 2 assists\n- Player E: 7 points, 2 rebounds, 1 assist\nCould you create a scatter plot graph in Seaborn talk mode for e\n\"\"\"\nagent_executor.run(query)\n",
      "content_length": 648,
      "extraction_method": "Direct"
    },
    {
      "page_number": 336,
      "chapter": null,
      "content": "We then get the following output:\nInvoking: `Python_REPL` with `import seaborn as sns\nimport matplotlib.pyplot as plt\n# Player stats\nplayers = ['Player A', 'Player B', 'Player C', 'Player D', 'Playe\npoints = [38, 28, 19, 12, 7]\nrebounds = [10, 9, 6, 4, 2]\n# Create scatter plot\nsns.scatterplot(x=rebounds, y=points, marker='o')\n# Label each point with player's name\nfor i, player in enumerate(players):\n    plt.text(rebounds[i], points[i], player, ha='center', va='bot\n# Set title\nplt.title('Team Players')\n# Show the plot\nplt.show()`\nThis output is accompanied by the following graph based on the players’\nstatistics:\n",
      "content_length": 619,
      "extraction_method": "Direct"
    },
    {
      "page_number": 337,
      "chapter": null,
      "content": "Figure 9.9: Sample plot generated by the Python agent\nLet’s look at another example. Say we want to predict the price of a house\nbased on some features, such as the number of bedrooms or the size of the\nhouse. To do so, we can ask our agent to design and train a model to give us\nthe result of a given house. For example, let’s consider the following prompt:\nquery = \"\"\"\nI want to predict the price of a house given the following infor\n- the number of rooms\n- the number of bathrooms\n- the size of the house in square meters\nDesign and train a regression model to predict the price of a ho\nOnce the model is trained, tell me the price of a house with the\n- 2 rooms\n",
      "content_length": 665,
      "extraction_method": "Direct"
    },
    {
      "page_number": 338,
      "chapter": null,
      "content": "- 1 bathroom\n- 100 square meters\n\"\"\"\nagent_executor.run(query)\nHere, we ask the agent to train a regression model on synthetic data\n(representative of houses with various configurations of rooms, bathrooms,\nand area, each with an associated price as a dependent variable) to give us\nthe estimated price of a house with the above features. Let’s see the output:\n> Entering new AgentExecutor chain...\nInvoking: `Python_REPL` with `import numpy as np\nfrom sklearn.linear_model import LinearRegression\n# Generate synthetic data\nnp.random.seed(0)\nX = np.random.rand(100, 3)  # 100 houses with 3 features: rooms, \ny = 100000 * X[:, 0] + 200000 * X[:, 1] + 300000 * X[:, 2] + 5000\n# Train the regression model\nmodel = LinearRegression()\nmodel.fit(X, y)\n# Predict the price of a house with the given features\nfeatures = np.array([[2, 1, 100]])\npredicted_price = model.predict(features)\npredicted_price`\nresponded: {content}\nThe predicted price of a house with 2 rooms, 1 bathroom, and 100 \n> Finished chain.\n'The predicted price of a house with 2 rooms, 1 bathroom, and 100\nAs you can see, the agent was able to generate synthetic training data, train a\nproper regression model using the sklearn libraries, and predict with the\nmodel the price of the house we provided.\nWith this approach, we can program an agent to act as an algorithm in real-\ntime scenarios. Imagine, for example, that we want to design an agent that is\n",
      "content_length": 1416,
      "extraction_method": "Direct"
    },
    {
      "page_number": 339,
      "chapter": null,
      "content": "capable of solving optimization problems in a smart building environment.\nThe goal is to optimize the Heating, Ventilation and Air Conditioning\n(HVAC) setpoints in the building to minimize energy costs while ensuring\noccupant comfort. Let’s define the variables and constraints of the problem:\nthe objective is to adjust the temperature setpoints within the specified\ncomfort ranges for each of the three zones while considering the varying\nenergy costs per degree, per hour.\nThe goal is to strike a balance between energy efficiency and occupant\ncomfort. Below, you can find a description of the problem and also the\ninitialization of our variables and constraints (energy cost per zone, initial\ntemperature per zone, and comfort range per zone):\nquery = \"\"\"\n**Problem**:\nYou are tasked with optimizing the HVAC setpoints in a smart bui\n- Zone 1: Energy cost = $0.05 per degree per hour\n- Zone 2: Energy cost = $0.07 per degree per hour\n- Zone 3: Energy cost = $0.06 per degree per hour\nYou need to find the optimal set of temperature setpoints for th\n- Zone 1: 72°F\n- Zone 2: 75°F\n- Zone 3: 70°F\nThe comfort range for each zone is as follows:\n- Zone 1: 70°F to 74°F\n- Zone 2: 73°F to 77°F\n- Zone 3: 68°F to 72°F\n**Question**:\nWhat is the minimum total energy cost (in dollars per hour) you \n\"\"\"\nagent_executor.run(query)\nWe then get the following output (you can find the whole reasoning chain in\nthe book’s GitHub repository):\n",
      "content_length": 1430,
      "extraction_method": "Direct"
    },
    {
      "page_number": 340,
      "chapter": null,
      "content": "> Entering new AgentExecutor chain...\nInvoking: `Python_REPL` with `import scipy.optimize as opt\n# Define the cost function\ndef cost_function(x):\n    zone1_temp = x[0]\n    zone2_temp = x[1]\n    zone3_temp = x[2]\n   \n    # Calculate the energy cost for each zone\n    zone1_cost = 0.05 * abs(zone1_temp - 72)\n    zone2_cost = 0.07 * abs(zone2_temp - 75)\n    zone3_cost = 0.06 * abs(zone3_temp - 70)\n[…]\n> Finished chain.\n'The minimum total energy cost that can be achieved by adjusting \nThe agent was able to solve the smart building optimization problem, finding\nthe minimum total energy cost, given some constraints. Staying in the scope\nof optimization problems, there are further use cases that these models could\naddress with a similar approach, including:\nSupply chain optimization: Optimize the logistics and distribution of\ngoods to minimize transportation costs, reduce inventory, and ensure\ntimely deliveries.\nPortfolio optimization: In finance, use algorithms to construct\ninvestment portfolios that maximize returns while managing risk.\nRoute planning: Plan optimal routes for delivery trucks, emergency\nservices, or ride-sharing platforms to minimize travel time and fuel\nconsumption.\nManufacturing process optimization: Optimize manufacturing\nprocesses to minimize waste, energy consumption, and production costs\nwhile maintaining product quality.\n",
      "content_length": 1360,
      "extraction_method": "Direct"
    },
    {
      "page_number": 341,
      "chapter": null,
      "content": "Healthcare resource allocation: Allocate healthcare resources like\nhospital beds, medical staff, and equipment efficiently during a\npandemic or other healthcare crisis.\nNetwork routing: Optimize data routing in computer networks to\nreduce latency, congestion, and energy consumption.\nFleet management: Optimize the use of a fleet of vehicles, such as\ntaxis or delivery vans, to reduce operating costs and improve service\nquality.\nInventory management: Determine optimal inventory levels and\nreorder points to minimize storage costs while preventing stockouts.\nAgricultural planning: Optimize crop planting and harvesting\nschedules based on weather patterns and market demand to maximize\nyield and profits.\nTelecommunications network design: Design the layout of\ntelecommunications networks to provide coverage while minimizing\ninfrastructure costs.\nWaste management: Optimize routes for garbage collection trucks to\nreduce fuel consumption and emissions.\nAirline crew scheduling: Create efficient flight crew schedules that\nadhere to labor regulations and minimize costs for airlines.\nThe Python REPL agent is amazing; however, it comes with some caveats:\nIt does not allow for FileIO, meaning that it cannot read and write with\nyour local file system.\nIt forgets the variables after every run, meaning that you cannot keep\ntrace of your initialized variables after the model’s response.\nTo bypass these caveats, in the next section, we are going to cover an open-\nsource project built on top of the LangChain agent: the Code Interpreter API.\n",
      "content_length": 1543,
      "extraction_method": "Direct"
    },
    {
      "page_number": 342,
      "chapter": null,
      "content": "Leveraging Code Interpreter\nThe name “Code Interpreter” was coined by OpenAI, referring to the\nrecently developed plugin for ChatGPT. The Code Interpreter plugin allows\nChatGPT to write and execute computer code in various programming\nlanguages. This enables ChatGPT to perform tasks such as calculations, data\nanalysis, and generating visualizations.\nThe Code Interpreter plugin is one of the tools designed specifically for\nlanguage models with safety as a core principle. It helps ChatGPT access up-\nto-date information, run computations, or use third-party services. The\nplugin is currently in private beta and is available for selected developers\nand ChatGPT Plus users.\nWhile OpenAI’s Code Interpreter still doesn’t offer an API, there are some\nopen-source projects that adapted the concept of this plugin in an open-\nsource Python library. In this section, we are going to leverage the work of\nShroominic, available at\nhttps://github.com/shroominic/codeinterpreter-api.\nYou can install it via pip install codeinterpreterapi.\nAccording to the blog post published by Shroominic, the author of the Code\nInterpreter API (which you can read at\nhttps://blog.langchain.dev/code-interpreter-api/), it\nis based on the LangChain agent OpenAIFunctionsAgent.\nDefinition\nOpenAIFunctionsAgent is a type of agent that can use the\nOpenAI functions’ ability to respond to the user’s prompts\nusing an LLM. The agent is driven by a model that supports\n",
      "content_length": 1440,
      "extraction_method": "Direct"
    },
    {
      "page_number": 343,
      "chapter": null,
      "content": "using OpenAI functions, and it has access to a set of tools\nthat it can use to interact with the user.\nThe OpenAIFunctionsAgent can also integrate custom\nfunctions. For example, you can define custom functions to\nget the current stock price or stock performance using Yahoo\nFinance. The OpenAIFunctionsAgent can use the ReAct\nframework to decide which tool to use, and it can use\nmemory to remember the previous conversation interactions.\nThe API comes already with some tools, such as the possibility to navigate\nthe web to get up-to-date information.\nYet the greatest difference from the Python REPL tool that we covered in the\nprevious section is that the Code Interpreter API can actually execute the\ncode it generates. In fact, when a Code Interpreter session starts, a miniature\nof a Jupyter Kernel is launched on your device, thanks to the underlying\nPython execution environment called CodeBox.\nTo start using the code interpreter in your notebook, you can install all the\ndependencies as follows:\n!pip install \"codeinterpreterapi[all]\"\nIn this case, I will ask it to generate a plot of COVID-19 cases in a specific\ntime range:\nfrom codeinterpreterapi import CodeInterpreterSession\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\napi_key = os.environ['OPENAI_API_KEY']\n",
      "content_length": 1284,
      "extraction_method": "Direct"
    },
    {
      "page_number": 344,
      "chapter": null,
      "content": "# create a session\nasync with CodeInterpreterSession() as session:\n    # generate a response based on user input\n    response = await session.generate_response(\n        \"Generate a plot of the evolution of Covid-19 from March\n    )\n    # output the response\nprint(\"AI: \", response.content)\n    for file in response.files:\n        file.show_image()\nHere is the generated output, including a graph that shows the number of\nglobal confirmed cases in the specified time period:\nAI:  Here is the plot showing the evolution of global daily confi\nFigure 9.10: Line chart generated by the Code Intepreter API\n",
      "content_length": 601,
      "extraction_method": "Direct"
    },
    {
      "page_number": 345,
      "chapter": null,
      "content": "As you can see, the Code Interpreter answered the question with an\nexplanation as well as a plot.\nLet’s try another one, this time also leveraging its real-time capabilities of\nsearching for up-to-date information. In the following snippet, we ask the\nmodel to plot the price of the S&P 500 index over the last 5 days:\nasync with CodeInterpreterSession() as session:\n    # generate a response based on user input\n    response = await session.generate_response(\n        \"Generate a plot of the price of S&P500 index in the las\n    )\n    # output the response\nprint(\"AI: \", response.content)\n    for file in response.files:\n        file.show_image()\nWe then get the following output, together with a line graph showing the\nprice of the S&P 500 index over the last 5 days:\nAI:  Here is the plot of the S&P 500 index for the last 5 days. T\n",
      "content_length": 836,
      "extraction_method": "Direct"
    },
    {
      "page_number": 346,
      "chapter": null,
      "content": "Figure 9.11: S&P 500 index price plotted by the Code Interpreter API\nFinally, we can provide local files to the Code Interpreter so that it can\nperform some analyses on that specific data. For example, I’ve downloaded\nthe Titanic dataset from Kaggle at\nhttps://www.kaggle.com/datasets/brendan45774/test-\nfile. The Titanic dataset is a popular dataset for machine learning that\ndescribes the survival status of individual passengers on the Titanic. It\ncontains information such as age, sex, class, fare, and whether they survived\nor not.\nOnce the dataset had downloaded, I passed it as a parameter to the model as\nfollows:\n",
      "content_length": 622,
      "extraction_method": "Direct"
    },
    {
      "page_number": 347,
      "chapter": null,
      "content": "from codeinterpreterapi import CodeInterpreterSession, File\n#os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\nos.environ['OPENAI_API_KEY'] = \"sk-YIN03tURjJRYmhcmv0yIT3BlbkFJv\nos.environ['VERBOSE'] = \"True\"\nasync with CodeInterpreterSession() as session:\n        # define the user request\n        user_request = \"Analyze this dataset and plot something \n        files = [\n            File.from_path(\"drive/MyDrive/titanic.csv\"),\n        ]\n        # generate the response\n        response = await session.generate_response(\n            user_request, files=files\n        )\n        # output to the user\nprint(\"AI: \", response.content)\n        for file in response.files:\n            file.show_image()\nWe then get the following output:\nAI:  The plot shows the survival count based on the passenger cla\nThese are just a few examples of the kind of insights we can extr\n",
      "content_length": 853,
      "extraction_method": "Direct"
    },
    {
      "page_number": 348,
      "chapter": null,
      "content": "Figure 9.12: Sample plots generated by the Code Interpreter API\n",
      "content_length": 64,
      "extraction_method": "Direct"
    },
    {
      "page_number": 349,
      "chapter": null,
      "content": "As you can see, the model was able to generate to bar charts showing the\nsurvival status grouped by sex (in the first plot) and then by class (in the\nsecond plot).\nThe Code Interpreter plugin, together with code-specific LLMs and the\nPython agent, are great examples of how LLMs are having a huge impact on\nthe world of software development. This can be summarized in two main\ncapabilities:\nLLMs can understand and generate code, since they have been trained\non a huge amount of programming languages, GitHub repos,\nStackOverflow conversations, and so on. Henceforth, along with\nnatural language, programming languages are part of their parametric\nknowledge.\nLLMs can understand a user’s intent and act as a reasoning engine to\nactivate tools like Python REPL or Code Interpreter, which are then\nable to provide a response by working with code.\nOverall, LLMs are going well beyond the elimination of the gap between\nnatural language and machine language: rather, they are integrating the two\nso that they can leverage each other to respond to a user’s query.\nSummary\nIn this chapter, we explored multiple ways in which LLMs can be leveraged\nto work with code. Armed with a refresher of how to evaluate LLMs and the\nspecific evaluation benchmarks to take into account when choosing an LLM\nfor code-related tasks, we delved into practical experimentations.\nWe started from the “plain vanilla” application that we have all tried at least\nonce using ChatGPT, which is code understanding and generation. For this\n",
      "content_length": 1508,
      "extraction_method": "Direct"
    },
    {
      "page_number": 350,
      "chapter": null,
      "content": "purpose, we leveraged three different models – Falcon LLM, CodeLlama,\nand StarCoder – each resulting in very good results.\nWe then moved forward with the additional applications that LLMs’ coding\ncapabilities can have in the real world. In fact, we saw how code-specific\nknowledge can be used as a booster to solve complex problems, such as\nalgorithmic or optimization tasks. Furthermore, we covered how code\nknowledge can not only be used in the backend reasoning of an LLM but\nalso actually executed in a working notebook, leveraging the open-source\nversion of the Code Interpreter API.\nWith this chapter, we are getting closer to the end of Part 2. So far, we have\ncovered the multiple capabilities of LLMs, while always handling language\ndata (natural or code). In the next chapter, we will see how to go a step\nfurther toward multi-modality and build powerful multi-modal agents that\ncan handle data in multiple formats.\nReferences\nThe open-source version of the Code Interpreter API:\nhttps://github.com/shroominic/codeinterpreter-\napi\nStarCoder: https://huggingface.co/blog/starcoder\nThe LangChain agent for the Python REPL:\nhttps://python.langchain.com/docs/integrations/t\noolkits/python\nA LangChain blog about the Code Interpreter API:\nhttps://blog.langchain.dev/code-interpreter-api/\n",
      "content_length": 1293,
      "extraction_method": "Direct"
    },
    {
      "page_number": 351,
      "chapter": null,
      "content": "The Titanic dataset:\nhttps://www.kaggle.com/datasets/brendan45774/tes\nt-file\nThe HF Inference Endpoint:\nhttps://huggingface.co/docs/inference-\nendpoints/index\nThe CodeLlama model card:\nhttps://huggingface.co/codellama/CodeLlama-7b-hf\nCode Llama: Open Foundation Models for Code, Rozière. B., et al\n(2023): https://arxiv.org/abs/2308.12950\nThe Falcon LLM model card:\nhttps://huggingface.co/tiiuae/falcon-7b-instruct\nThe StarCoder model card:\nhttps://huggingface.co/bigcode/starcoder\nJoin our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\n",
      "content_length": 621,
      "extraction_method": "Direct"
    },
    {
      "page_number": 352,
      "chapter": null,
      "content": "OceanofPDF.com\n",
      "content_length": 15,
      "extraction_method": "Direct"
    },
    {
      "page_number": 353,
      "chapter": null,
      "content": "10\nBuilding Multimodal\nApplications with LLMs\nIn this chapter, we are going beyond LLMs, to introduce the concept of\nmultimodality while building agents. We will see the logic behind the\ncombination of foundation models in different AI domains – language,\nimages, and audio – into one single agent that can adapt to a variety of tasks.\nBy the end of this chapter, you will be able to build your own multimodal\nagent, providing it with the tools and LLMs needed to perform various AI\ntasks.\nThroughout this chapter, we will cover the following topics:\nIntroduction to multimodality and large multimodal models (LMMs)\nExamples of emerging LMMs\nHow to build a multimodal agent with single-modal LLMs using\nLangChain\nTechnical requirements\nTo complete the tasks in this chapter, you will need the following:\nA Hugging Face account and user access token.\nAn OpenAI account and user access token.\n",
      "content_length": 891,
      "extraction_method": "Direct"
    },
    {
      "page_number": 354,
      "chapter": null,
      "content": "Python 3.7.1 or later version.\nPython packages. Make sure to have the following Python packages\ninstalled: langchain, python-dotenv, huggingface_hub, streamlit,\npytube, openai, and youtube_search. Those can be easily installed via\npip install in your terminal.\nYou can find all the code and examples in the book’s GitHub repository at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications.\nWhy multimodality?\nIn the context of Generative AI, multimodality refers to a model’s capability\nof processing data in various formats. For example, a multimodal model can\ncommunicate with humans via text, speech, images, or even videos, making\nthe interaction extremely smooth and “human-like.”\nIn Chapter 1, we defined large foundation models (LFMs) as a type of pre-\ntrained generative AI model that offers immense versatility by being\nadaptable for various specific tasks. LLMs, on the other hand, are a subset of\nfoundation models that are able to process one type of data: natural\nlanguage. Even though LLMs have proven to be not only excellent text\nunderstanders and generators but also reasoning engines to power\napplications and copilots, it soon became clear that we could aim at even\nmore powerful applications.\nThe dream is to have intelligent systems that are capable of handling\nmultiple data formats – text, images, audio, video, etc – always powered by\nthe reasoning engine, which makes them able to plan and execute actions\n",
      "content_length": 1447,
      "extraction_method": "Direct"
    },
    {
      "page_number": 355,
      "chapter": null,
      "content": "with an agentic approach. Such an AI system would be a further milestone\ntoward the reaching of artificial general intelligence (AGI).\nDefinition\nAGI is a hypothetical type of artificial intelligence (AI) that\ncan perform any intellectual task that a human can. AGI\nwould have a general cognitive ability, similar to human\nintelligence, and be able to learn from experience, reason,\nplan, communicate, and solve problems across different\ndomains. An AGI system would also be able to “perceive”\nthe world as we do, meaning that it could process data in\ndifferent formats, from text to images to sounds. Hence, AGI\nimplies multimodality.\nCreating AGI is a primary goal of some AI research and a\ncommon topic in science fiction. However, there is no\nconsensus on how to achieve AGI, what criteria to use to\nmeasure it, or when it might be possible. Some researchers\nargue that AGI could be achieved in years or decades, while\nothers maintain that it might take a century or longer, or that\nit might never be achieved.\nHowever, AGI is not seen as the ultimate milestone in AI development. In\nfact, in recent months another definition has emerged in the context of AI –\nthat is, Strong AI or Super AI, referring to an AI system that is more capable\nthan a human.\nAt the time of writing this book (February 2024), LMMs such as GPT-4\nTurbo with Vision are a reality. However, those are not the only ways to\n",
      "content_length": 1400,
      "extraction_method": "Direct"
    },
    {
      "page_number": 356,
      "chapter": null,
      "content": "reach multimodality. In this chapter, we are going to examine how to merge\nmultiple AI systems to reach a multimodal AI assistant. The idea is that if we\ncombine single-modal models, one for each data format we want to process,\nand then use an LLM as the brain of our agent to let it interact in dynamic\nways with those models (that will be its tools), we can still achieve this goal.\nThe following diagram shows the structure of a multimodal application that\nintegrates various single-modal tools to perform a task – in this case,\ndescribing a picture aloud. The application uses image analysis to examine\nthe picture, text generation to create some text that describes what it\nobserves in the picture, and text-to-speech to convey this text to the user\nthrough speech.\nThe LLM acts as the “reasoning engine” of the application, invoking the\nproper tools needed to accomplish the user’s query.\n",
      "content_length": 895,
      "extraction_method": "Direct"
    },
    {
      "page_number": 357,
      "chapter": null,
      "content": "Figure 10.1: Illustration of multimodal application with single-modal tools\nIn the upcoming section, we are going to explore various approaches to\nbuilding multimodal applications, all based on the idea of combining\nexisting single-modal tools or models.\n",
      "content_length": 255,
      "extraction_method": "Direct"
    },
    {
      "page_number": 358,
      "chapter": null,
      "content": "Building a multimodal agent\nwith LangChain\nSo far, we’ve covered the main aspects of multimodality and how to achieve\nit with modern LFMs. As we saw throughout Part 2 of this book, LangChain\noffers a variety of components that we leveraged massively, such as chains,\nagents, tools, and so on. As a result, we already have all the ingredients we\nneed to start building our multimodal agent.\nHowever, in this chapter, we will adopt three approaches to tackle the\nproblem:\nThe agentic, out-of-the-box approach: Here we will leverage the\nAzure Cognitive Services toolkit, which offers native integrations\ntoward a set of AI models that can be consumed via API, and that\ncovers various domains such as image, audio, OCR, etc.\nThe agentic, custom approach: Here, we are going to select single\nmodels and tools (including defining custom tools) and concatenate\nthem into a single agent that can leverage all of them.\nThe hard-coded approach: Here, we are going to build separate\nchains and combine them into a sequential chain.\nIn the upcoming sections, we will cover all these approaches with concrete\nexamples.\nOption 1: Using an out-of-\nthe-box toolkit for Azure AI\nServices\n",
      "content_length": 1171,
      "extraction_method": "Direct"
    },
    {
      "page_number": 359,
      "chapter": null,
      "content": "Formerly known as Azure Cognitive Services, Azure AI Services are a set of\ncloud-based APIs and AI services developed by Microsoft that enable\ndevelopers and data scientists to add cognitive capabilities to their apps. AI\nServices are meant to provide every developer with AI models to be\nintegrated with programming languages such as Python, C#, or JavaScript.\nAzure AI Services cover various domains of AI, including speech, natural\nlanguage, vision, and decision-making. All those services come with models\nthat can be consumed via API, and you can decide to:\nLeverage powerful pre-built models available as they are and ready to\nuse.\nCustomize those pre-built models with custom data so that they are\ntailored to your use case.\nHence, considered all together, Azure AI Services can achieve the goal of\nmultimodality, if properly orchestrated by an LLM as a reasoning engine,\nwhich is exactly the framework LangChain built.\nGetting Started with\nAzureCognitiveServicesToolkit\nIn fact, LangChain has a native integration with Azure AI Services called\nAzureCognitiveServicesToolkit, which can be passed as a parameter to an\nagent and leverage the multimodal capabilities of those models.\nThe toolkit makes it easier to incorporate Azure AI services’ capabilities –\nsuch as image analysis, form recognition, speech-to-text, and text-to-speech\n– within your application. It can be used within an agent, which is then\nempowered to use the AI services to enhance its functionality and provide\nricher responses.\n",
      "content_length": 1507,
      "extraction_method": "Direct"
    },
    {
      "page_number": 360,
      "chapter": null,
      "content": "Currently, the integration supports the following tools:\nAzureCogsImageAnalysisTool: Used to analyze and extract metadata\nfrom images.\nAzureCogsSpeech2TextTool: Used to convert speech to text.\nAzureCogsText2SpeechTool: Used to synthetize text to speech with\nneural voices.\nAzureCogsFormRecognizerTool: Used to perform optical character\nrecognition (OCR).\nDefinition\nOCR is a technology that converts different types of\ndocuments, such as scanned paper documents, PDFs, or\nimages captured by a digital camera, into editable and\nsearchable data. OCR can save time, cost, and resources by\nautomating data entry and storage processes. It can also\nenable access to and editing of the original content of\nhistorical, legal, or other types of documents.\nFor example, if you ask an agent what you can make with some ingredients,\nand provide an image of eggs and flour, the agent can use the Azure AI\nServices Image Analysis tool to extract the caption, objects, and tags from\nthe image, and then use the provided LLM to suggest some recipes based on\nthe ingredients. To implement this, let’s first set up our toolkit.\nSetting up the toolkit\nTo get started with the toolkit, you can follow these steps:\n",
      "content_length": 1194,
      "extraction_method": "Direct"
    },
    {
      "page_number": 361,
      "chapter": null,
      "content": "1. You first need to create a multi-service instance of Azure AI Services in\nAzure following the instructions at\nhttps://learn.microsoft.com/en-us/azure/ai-\nservices/multi-service-resource?\ntabs=windows&pivots=azportal.\n2. A multi-service resource allows you to access multiple AI services with\na single key and endpoint to be passed to LangChain as environmental\nvariables. You can find your keys and endpoint under the Keys and\nEndpoint tab in your resource panel:\nFigure 10.2: Screenshot of a multi-service instance of Azure AI Services\n3. Once the resource is set, we can start building our LegalAgent. To do\nso, the first thing we need to do is set the AI services environmental\nvariables in order to configure the toolkit. To do so, I’ve saved the\nfollowing variables in my .env file:\n",
      "content_length": 791,
      "extraction_method": "Direct"
    },
    {
      "page_number": 362,
      "chapter": null,
      "content": "AZURE_COGS_KEY = \"your-api-key\"\nAZURE_COGS_ENDPOINT = \"your-endpoint\nAZURE_COGS_REGION = \"your-region\"\n4. Then, you can load them as always alongside the other environmental\nvariables:\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\nazure_cogs_key = os.environ[\"AZURE_COGS_KEY\"]\nazure_cogs_endpoint = os.environ[\"AZURE_COGS_ENDPOINT\"\nazure_cogs_region = os.environ[\"AZURE_COGS_REGION\"]\nopenai_api_key = os.environ['OPENAI_API_KEY']\n5. Now, we can configure our toolkit and also see which tools we have,\nalongside their description:\nfrom langchain.agents.agent_toolkits import AzureCogni\ntoolkit = AzureCognitiveServicesToolkit()\n[(tool.name, tool.description) for tool in toolkit.get\nThe following is the corresponding output:\n[('azure_cognitive_services_form_recognizer',\n  'A wrapper around Azure Cognitive Services Form Recognize\n ('azure_cognitive_services_speech2text',\n  'A wrapper around Azure Cognitive Services Speech2Text. U\n ('azure_cognitive_services_text2speech',\n  'A wrapper around Azure Cognitive Services Text2Speech. U\n ('azure_cognitive_services_image_analysis',\n  'A wrapper around Azure Cognitive Services Image Analysis\n",
      "content_length": 1149,
      "extraction_method": "Direct"
    },
    {
      "page_number": 363,
      "chapter": null,
      "content": "6. Now, it’s time to initialize our agent. For this purpose, we will use a\nSTRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION agent that, as we saw\nin previous chapters, also allows for multi-tools input, since we will\nalso add further tools in the Leveraging multiple tools section:\nfrom langchain.agents import initialize_agent, AgentTy\nfrom langchain import OpenAI\nllm = OpenAI()\nModel = ChatOpenAI()\nagent = initialize_agent(\n    tools=toolkit.get_tools(),\n    llm=llm,\n    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DE\n    verbose=True,\n)\nNow we have all the ingredients to start testing our agent.\nLeveraging a single tool\nTo start easy, let’s simply ask the agent to describe the following picture,\nwhich will only require the image_analysis tool to be accomplished:\n",
      "content_length": 772,
      "extraction_method": "Direct"
    },
    {
      "page_number": 364,
      "chapter": null,
      "content": "Figure 10.3: Sample picture of a slingshot (source: https://www.stylo24.it/wp-\ncontent/uploads/2020/03/fionda.jpg)\nLet’s pass the URL of this image as input to our model, as per the\ndescription of the azure_cognitive_services_image_analysis tool:\ndescription = agent.run(\"what shows the following image?:\"\n\"https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg\")\nprint(description)\nWe then get the following output:\n> Entering new AgentExecutor chain...\nAction:\n```\n{\n  \"action\": \"azure_cognitive_services_image_analysis\",\n  \"action_input\": \"https://www.stylo24.it/wp-content/uploads/2020\n}\n```\nObservation: Caption: a person holding a slingshot\nTags: person, tool, nail, hand, holding, needle\nThought: I know what the image is.\n",
      "content_length": 736,
      "extraction_method": "Direct"
    },
    {
      "page_number": 365,
      "chapter": null,
      "content": "Action:\n```\n{\n  \"action\": \"Final Answer\",\n  \"action_input\": \"The image is of a person holding a slingshot.\"\n}\n```\n> Finished chain.\nThe image is of a person holding a slingshot.\nAs you can see, the agent was able to retrieve the proper tool to address the\nuser’s question. In this case, the question was very simple, so I want to\nchallenge the same tool with a trickier question.\nThe goal is to replicate the GPT-4 capabilities in its common-sense\nreasoning while working with images, as the following illustration from\nGPT-4’s earliest experiments shows:\n",
      "content_length": 556,
      "extraction_method": "Direct"
    },
    {
      "page_number": 366,
      "chapter": null,
      "content": "Figure 10.4: Example of visual capabilities and common sense reasoning of GPT-4 (source:\nhttps://openai.com/research/gpt-4)\nSo let’s ask our model something more challenging. Let’s ask it to reason\nabout the consequences of letting the slingshot go:\nagent.run(\"what happens if the person lets the slingshot go?:\"\n\"https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg\")\nWe then obtain the following output:\n",
      "content_length": 414,
      "extraction_method": "Direct"
    },
    {
      "page_number": 367,
      "chapter": null,
      "content": "> Entering new AgentExecutor chain...\nAction:\n```\n{\n  \"action\": \"azure_cognitive_services_image_analysis\",\n  \"action_input\": \"https://www.stylo24.it/wp-content/uploads/2020\n}\n```\nObservation: Caption: a person holding a slingshot\nTags: person, tool, nail, hand, holding, needle\nThought: I know what to respond\nAction:\n```\n{\n  \"action\": \"Final Answer\",\n  \"action_input\": \"If the person lets the slingshot go, it will f\n}\n```\n> Finished chain.\n'If the person lets go of the slingshot, the object being launche\nIt might seem an easy question, but the agent’s answers imply an extremely\nrefined common sense reasoning: thanks to the metadata extracted from the\nimage leveraging the image_analysis tool, the LLM was able to set up\nreasoning in terms of consequences given an action (the person letting the\nslingshot go).\nIn this example, the agent only leveraged one tool. But what happens if we\nask it something that requires at least two tools?\nLeveraging multiple tools\nLet’s say, for example, that we want the model to read a story aloud to us\nbased on a picture.\n",
      "content_length": 1063,
      "extraction_method": "Direct"
    },
    {
      "page_number": 368,
      "chapter": null,
      "content": "Figure 10.5: Example of an input image for a story-telling agent (source:\nhttps://i.redd.it/diawvlriobq11.jpg)\nLet’s ask the agent to do so:\nagent.run(\"Tell me a story related to the following picture and \nWe obtain the following output:\nThought:The human requested a story related to the picture. I wil\nAction:\n```\n{\n  \"action\": \"azure_cognitive_services_image_analysis\",\n  \"action_input\": {\n    \"query\": \"https://i.redd.it/diawvlriobq11.jpg\"\n  }\n}\n```\nObservation: Caption: a dog standing on a snowy hill\nObjects: dog\nTags: outdoor, animal, mammal, snow, freezing, dog breed, dog, mo\nThought:Based on the Azure Cognitive Services Image Analysis tool\n",
      "content_length": 652,
      "extraction_method": "Direct"
    },
    {
      "page_number": 369,
      "chapter": null,
      "content": "Action:\n```\n{\n  \"action\": \"azure_cognitive_services_text2speech\",\n  \"action_input\": {\n    \"query\": \"Once upon a time, in a snowy winter wonderland, the\n  }\n}\n```\nObservation: C:\\Users\\vaalt\\AppData\\Local\\Temp\\tmp_cecwqht.wav\nThought:The story involving a dog in a snowy winter wonderland se\nAction:\n```\n{\n  \"action\": \"Final Answer\",\n  \"action_input\": \"I hope you enjoyed the story of Snowy the Savi\n}\n```\nAs you can see, the agent was able to invoke two tools to accomplish the\nrequest:\n1. It first started with the image_analysis tool to generate the image\ncaption used to produce the story.\n2. Then, it invoked the text2speech tool to read it aloud to the user.\nThe agent saved the audio file in a temporary file, and you can listen to it\ndirectly by clicking on the URL. Alternatively, you can save the output as a\nPython variable and execute it as follows:\nfrom IPython import display\naudio = agent.run(\"Tell me a story related to the following pict\ndisplay.display(audio)\n",
      "content_length": 977,
      "extraction_method": "Direct"
    },
    {
      "page_number": 370,
      "chapter": null,
      "content": "Finally, we can also modify the default prompt that comes with the agent\ntype, to make it more customized with respect to our specific use case. To do\nso, we first need to inspect the template and then decide which part we can\nmodify. To inspect the template, you can run the following command:\nprint(agent.agent.llm_chain.prompt.messages[0].prompt.template)\nHere is our output:\nRespond to the human as helpfully and accurately as possible. You\n{tools}\nUse a json blob to specify a tool by providing an action key (too\nValid \"action\" values: \"Final Answer\" or youtube_search, CustomeY\nProvide only ONE action per $JSON_BLOB, as shown:\n```\n{{\n  \"action\": $TOOL_NAME,\n  \"action_input\": $INPUT\n}}\n```\nFollow this format:\nQuestion: input question to answer\nThought: consider previous and subsequent steps\nAction:\n```\n$JSON_BLOB\n...\n```\nBegin! Reminder to ALWAYS respond with a valid json blob of a sin\nThought:\nLet’s modify the prefix of the prompt and pass it as kwargs to our agent:\nPREFIX = \"\"\"\nYou are a story teller for children. \nYou read aloud stories based on pictures that the user pass you.\n",
      "content_length": 1097,
      "extraction_method": "Direct"
    },
    {
      "page_number": 371,
      "chapter": null,
      "content": " You always start your story with a welcome message targeting chi\n You can use multiple tools to answer the question.\n ALWAYS use the tools.\n You have access to the following tools:\"\"\"\nagent = initialize_agent(toolkit.get_tools(), model, agent=AgentT\n                         agent_kwargs={\n                            'prefix':PREFIX})\nAs you can see, now the agent acts more similar to a storyteller with a\nspecific style. You can customize your prompt as you wish, always keeping\nin mind that each pre-built agent has its own prompt template, hence it is\nalways recommended to first inspect it before customizing it.\nNow that we have explored the out-of-the-box capabilities of the toolkit,\nlet’s build an end-to-end application.\nBuilding an end-to-end application for\ninvoice analysis\nAnalyzing invoices might require a lot of manual work if not assisted by\ndigital processes. To address this, we will build an AI assistant that is able to\nanalyze invoices for us and tell us any relevant information aloud. We will\ncall this application CoPenny.\nWith CoPenny, individuals and enterprises could reduce the time of invoice\nanalysis, as well as build toward document process automation and, more\ngenerally, digital process automation.\nDefinition\nDocument process automation is a strategy that uses\ntechnology to streamline and automate various document-\nrelated tasks and processes within an organization. It\n",
      "content_length": 1411,
      "extraction_method": "Direct"
    },
    {
      "page_number": 372,
      "chapter": null,
      "content": "involves the use of software tools, including document\ncapture, data extraction, workflow automation, and\nintegration with other systems. For example, document\nprocess automation can help you extract, validate, and\nanalyze data from invoices, receipts, forms, and other types\nof documents. Document process automation can save you\ntime and money, improve accuracy and efficiency, and\nprovide valuable insights and reports from your document\ndata.\nDigital process automation (DPA) is a broader term that\nrefers to automating any business process with digital\ntechnology. DPA can help you connect your apps, data, and\nservices and boost your team’s productivity with cloud\nflows. DPA can also help you create more sophisticated and\nintuitive customer experiences, collaborate across your\norganization, and innovate with AI and ML.\nTo start building our application, we can follow these steps:\n1. Using AzureCognitiveServicesToolkit, we will leverage the\nazure_cognitive_services_form_recognizer and\nazure_cognitive_services_text2speech tools, so we can limit the\nagent’s “powers” only to those two:\ntoolkit = AzureCognitiveServicesToolkit().get_tools()\n#those tools are at the first and third position in th\ntools = [toolkit[0], toolkit[2]]\ntools\n",
      "content_length": 1245,
      "extraction_method": "Direct"
    },
    {
      "page_number": 373,
      "chapter": null,
      "content": "The following is the corresponding output:\n[AzureCogsFormRecognizerTool(name='azure_cognitive_services\n2. Let’s now initialize the agent with the default prompt and see the\nresults. For this purpose, we will use a sample invoice as a template\nwith which to query the agent:\n",
      "content_length": 274,
      "extraction_method": "Direct"
    },
    {
      "page_number": 374,
      "chapter": null,
      "content": "PURCHASE ORDER TEMPLATE\n\nCompany Name\n123 Main Street\nHamilton, OH 44414\n(321) 456-7890\nEmail Address\n\nPaint of Cantact\n\nBILL TO.\n\nATIN: Name / Dept\nCompany Name\n123 Main Street\nHamilton, OH 44416\n\n(321) 456-7890\nEmail Address\nSHIPPING METHOD SHIPPING TERMS\nTEM NO, DESCRIPTION\nAll Women's Tall -M.\n8223 Men's Tall - MM\nc333 Children’s - §\nD444 Men's - XL\nRemarks/Instructions:\n\nPlease make check payable fo Tour Company Name.\n\nTHANK YOU\n\nAUTHORIZED SIGNATURE\n\nPURCHASE ORDER\n\nSHIP TO\nATTN: Name / Dept\n\nCompany Name\n123 Main Street\nHamilton, OH 44416\n(321) 456-7890\nSHIP WIA PAYMENT\nor UNIT FRICE\n10 $10.00\n5 $20.00\n10 $5.00\n5 $10.00\nSUBTOTAL\n\nenter total amount DISCOUNT\nSUBTOTAL LESS DISCOUNT\nenter percentage TAX RATE\nTOTAL TAX\nSHIPPING/HANDLING\nOTHER\n\nTOTAL\n\nDATE\n\nFor questions concerning this invoice, please contact\nName, (321) 456-7890, Email Acidress\n\nwee. younvebadcdress.com\n\nDATE\n01/18/2022\n\nPURCHASE ORDER NO.\n\nA246\n\nCUSTOMER NO.\n\n114H\n\nDELIVERY DATE\n\nTOTAL\n$100.00\n$100.00\n\n$50.00\n$50.00\n$0.00\n40.00\n$0.00\n40.00\n$0.00\n$0.00\n$0.00\n300.00\n50.00\n250,00\n7.214%\n18.04\n50.00\n50.00\n3468.04",
      "content_length": 1097,
      "extraction_method": "OCR"
    },
    {
      "page_number": 375,
      "chapter": null,
      "content": "Figure 10.6: Sample template of a generic invoice (source: https://www.whiteelysee.fr/design/wp-\ncontent/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg)\n3. Let’s start by asking the model to tell us all the men’s stock-keeping\nunits (SKUs) on the invoice:\nagent.run(\"what are all men's skus?\"\n\"https://www.whiteelysee.fr/design/wp-content/uploads/\nWe then get the following output (showing a truncated output; you can\nfind the whole output in the book’s GitHub repository):\n> Entering new AgentExecutor chain...\nAction:\n```\n{\n  \"action\": \"azure_cognitive_services_form_recognizer\",\n  \"action_input\": {\n    \"query\": \"https://www.whiteelysee.fr/design/wp-content/\n  }\n}\n```\nObservation: Content: PURCHASE ORDER TEMPLATE […]\n> Finished chain.\n\"The men's skus are B222 and D444.\"\n4. We can also ask for multiple information (women’s SKUs, shipping\naddress, and delivery dates) as follows (note that the delivery date is not\nspecified, as we want our agent not to hallucinate):\nagent.run(\"give me the following information about the\n\"https://www.whiteelysee.fr/design/wp-content/uploads/\n",
      "content_length": 1096,
      "extraction_method": "Direct"
    },
    {
      "page_number": 376,
      "chapter": null,
      "content": "This gives us the following output:\n\"The women's SKUs are A111 Women's Tall - M. The shipping a\n5. Finally, let’s also leverage the text2speech tool to produce the audio of\nthe response:\nagent.run(\"extract women's SKUs in the following invoi\n\"https://www.whiteelysee.fr/design/wp-content/uploads/\nAs per the previous example, you can listen to the audio by clicking\non the URL in the chain, or using Python’s Display function if you\nsave it as a variable.\n6. Now, we want our agent to be better tailored toward our goal. To do so,\nlet’s customize the prompt giving specific instructions. In particular, we\nwant the agent to produce the audio output without the user explicitly\nasking for it:\nPREFIX = \"\"\"\nYou are an AI assistant that help users to interact wi\nYou extract information from invoices and read it alou\nYou can use multiple tools to answer the question.\nAlways divide your response in 2 steps:\n1. Extracting the information from the invoice upon us\n2. Converting the transcript of the previous point int\nALWAYS use the tools.\nALWAYS return an audio file using the proper tool.\nYou have access to the following tools:\n\"\"\"\nagent = initialize_agent(tools, model, agent=AgentType\n",
      "content_length": 1188,
      "extraction_method": "Direct"
    },
    {
      "page_number": 377,
      "chapter": null,
      "content": "                         agent_kwargs={\n                            'prefix':PREFIX})\n7. Let’s run the agent:\nagent.run(\"what are women's SKUs in the following invo\n\"https://www.whiteelysee.fr/design/wp-content/uploads/\nThis yields the following output:\n> Entering new AgentExecutor chain...\nI will need to use the azure_cognitive_services_form_recogn\nAction:\n```\n{\n  \"action\": \"azure_cognitive_services_form_recognizer\",\n  \"action_input\": {\n    \"query\": \"https://www.whiteelysee.fr/design/wp-content/\n  }\n}\n```\nObservation: Content: PURCHASE ORDER TEMPLATE […]\nObservation: C:\\Users\\vaalt\\AppData\\Local\\Temp\\tmpx1n4obf3\nThought:Now that I have provided the answer, I will wait fo\nAs you can see, now the agent saved the output into an audio file, even when\nthe user didn’t ask explicitly for it.\nAzureCognitiveServicesToolkit is a powerful integration that allows for\nnative consumption of Azure AI Services. However, there are some pitfalls\nof this approach, including the limited number of AI services. In the next\nsection, we are going to explore yet another option to achieve multimodality,\nwith a more flexible approach while still keeping an agentic strategy.\n",
      "content_length": 1167,
      "extraction_method": "Direct"
    },
    {
      "page_number": 378,
      "chapter": null,
      "content": "Option 2: Combining single\ntools into one agent\nIn this leg of our journey toward multimodality, we will leverage different\ntools as plug-ins to our STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION\nagent. Our goal is to build a copilot agent that will help us generate reviews\nabout YouTube videos, as well as post those reviews on our social media\nwith a nice description and related picture. In all of that, we want to make\nlittle or no effort, so we need our agent to perform the following steps:\n1. Search and transcribe a YouTube video based on our input.\n2. Based on the transcription, generate a review with a length and style\ndefined by the user query.\n3. Generate an image related to the video and the review.\nWe will call our copilot GPTuber. In the following subsections, we will\nexamine each tool and then put them all together.\nYouTube tools and Whisper\nThe first step of our agent will be to search and transcribe the YouTube video\nbased on our input. To do so, there are two tools we need to leverage:\nYouTubeSearchTool: An out-of-the-box tool offered by LangChain\nand adapted from\nhttps://github.com/venuv/langchain_yt_tools. You\ncan import and try the tool by running the following code, specifying\nthe topic of the video and the number of videos you want the tool to\nreturn:\n",
      "content_length": 1288,
      "extraction_method": "Direct"
    },
    {
      "page_number": 379,
      "chapter": null,
      "content": "from langchain.tools import YouTubeSearchTool\ntool = YouTubeSearchTool()\nresult = tool.run(\"Avatar: The Way of Water,1\")\nresult:\nHere is the output:\n\"['/watch?v=d9MyW72ELq0&pp=ygUYQXZhdGFyOiBUaGUgV2F5IG9mIFdh\nThe tool returns the URL of the video. To watch it, you can add it to\nhttps://youtube.com domain.\nCustomYTTranscribeTool: This is a custom tool that I’ve adapted\nfrom https://github.com/venuv/langchain_yt_tools.\nIt consists of transcribing the audio file retrieved from the previous tool\nusing a speech-to-text model. In our case, we will be leveraging\nOpenAI’s Whisper.\nWhisper is a transformer-based model introduced by OpenAI in\nSeptember 2022. It works as follows:\ni. It splits the input audio into 30-second chunks, converting them\ninto spectrograms (visual representations of sound frequencies).\nii. It then passes them to an encoder.\niii. The encoder then produces a sequence of hidden states that\ncapture the information in the audio.\niv. A decoder then predicts the corresponding text caption, using\nspecial tokens to indicate the task (such as language\nidentification, speech transcription, or speech translation) and the\noutput language.\n",
      "content_length": 1158,
      "extraction_method": "Direct"
    },
    {
      "page_number": 380,
      "chapter": null,
      "content": "v. The decoder can also generate timestamps for each word or\nphrase in the caption.\nUnlike most OpenAI models, Whisper is open-source.\nSince this model takes as input only files and not URLs, within the\ncustom tool, there is a function defined as yt_get (you can find it in\nthe GitHub repository) that, starting from the video URL, downloads it\ninto a .mp4 file. Once downloaded, you can try Whisper with the\nfollowing lines of code:\nimport openai\naudio_file = open(\"Avatar The Way of Water  Official Traile\nresult = openai.Audio.transcribe(\"whisper-1\", audio_file)\naudio_file.close()\nprint(result.text)\nHere is the corresponding output:\n♪ Dad, I know you think I'm crazy. But I feel her. I hear h\nBy embedding Whisper in this custom tool, we can transcribe the\noutput of the first tool into a transcript that will serve as input to the\nnext tool. You can see the code and logic behind this embedding and\nthe whole tool in this book’s GitHub repository at\nhttps://github.com/PacktPublishing/Building-\nLLM-Powered-Applications, which is a modified version from\nhttps://github.com/venuv/langchain_yt_tools.\nSince we already have two tools, we can start building our tools list and\ninitializing our agent, using the following code:\n",
      "content_length": 1229,
      "extraction_method": "Direct"
    },
    {
      "page_number": 381,
      "chapter": null,
      "content": "llm = OpenAI(temperature=0)\ntools = []\ntools.append(YouTubeSearchTool())\ntools.append(CustomYTTranscribeTool())\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_R\nagent.run(\"search a video trailer of Avatar: the way of water. R\nThe following is the corresponding output:\n> Entering new AgentExecutor chain...\nI need to find a specific video and transcribe it.\nAction: youtube_search\nAction Input: \"Avatar: the way of water,1\"\nObservation: ['/watch?v=d9MyW72ELq0&pp=ygUYQXZhdGFyOiB0aGUgd2F5IG\nThought:I found the video I was looking for, now I need to transc\nAction: CustomeYTTranscribe\nAction Input: […]\nObservation: ♪ Dad, I know you think I'm crazy. […]\nThought:I have the transcription of the video trailer for Avatar:\nFinal Answer: The transcription of the video trailer for Avatar: \n> Finished chain.\nGreat! We were able to generate the transcription of this video. The next step\nwill be to generate a review alongside a picture. While the review can be\nwritten directly from the LLM and passed as a parameter to the model (so\nwe don’t need another tool), the image generation will need an additional\ntool. For this purpose, we are going to use OpenAI’s DALL·E.\nDALL·E and text generation\nIntroduced by OpenAI in January 2021, DALL·E is a transformer-based\nmodel that can create images from text descriptions. It is based on GPT-3,\nwhich is also used for natural language processing tasks. It is trained on a\n",
      "content_length": 1428,
      "extraction_method": "Direct"
    },
    {
      "page_number": 382,
      "chapter": null,
      "content": "large dataset of text-image pairs from the web and uses a vocabulary of\ntokens for both text and image concepts. DALL·E can produce multiple\nimages for the same text, showing different interpretations and variations.\nLangChain offers native integration with DALL·E, which you can use as a\ntool by running the following code (always setting the environmental\nvariable of your OPENAI_API_KEY from the .env file):\nfrom langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\ntools = load_tools(['dalle-image-generator'])\nagent = initialize_agent(tools, model, AgentType.ZERO_SHOT_REACT\nagent.run(\"Create an image of a halloween night. Return only the\nHere is the corresponding output:\n> Entering new AgentExecutor chain...\nI need to use an image generator to create an image of a hallowee\nAction: Dall-E Image Generator\nAction Input: \"An image of a spooky halloween night with a full m\nObservation: [link_to_the_blob]\nThought:I have successfully generated an image of a halloween nig\nFinal Answer: The image url is [link_to_the_blob]\n> Finished chain.\nThe following is the image that was generated, as requested:\n",
      "content_length": 1135,
      "extraction_method": "Direct"
    },
    {
      "page_number": 383,
      "chapter": null,
      "content": "Figure 10.7: Image generated by DALL·E upon the user’s input\nGreat! Now let’s also see whether our agent is capable of generating a\nreview of a video based on the transcription:\nagent.run(\"search a video trailer of Avatar: the way of water. R\nWe get the following output:\n> Entering new AgentExecutor chain...\nI need to use both youtube_search and CustomeYTTranscribe tools t\nAction: youtube_search\nAction Input: \"Avatar: the way of water,1\"\nObservation: ['/watch?v=d9MyW72ELq0&pp=ygUYQXZhdGFyOiB0aGUgd2F5IG\nThought:I found the video I need to transcribe.\nAction: CustomeYTTranscribe\nAction Input: […]\nObservation: ♪ Dad, I know you think I'm crazy. […]\nThought:I need to write a review of the trailer.\nAction: Write a review\nAction Input: \"Avatar: The Way of Water trailer is visually stunn\nObservation: Write a review is not a valid tool, try one of [yout\n",
      "content_length": 858,
      "extraction_method": "Direct"
    },
    {
      "page_number": 384,
      "chapter": null,
      "content": "Thought:I need to use a different tool to write a review.\nAction: Write a review\nAction Input: N/A\nObservation: Write a review is not a valid tool, try one of [yout\n...\nThought:I can write the review manually.\nFinal Answer: Avatar: The Way of Water trailer is visually stunni\n> Finished chain.\nNote how the agent was initially looking for a tool to make a review, to then\nrealize that there is no tool yet that can do it manually thanks to its\nparametric knowledge. This is a great example of how LLMs are reasoning\nengines and endowed with common sense reasoning. As always, you can\nfind the entire chain of thoughts in the book’s repository.\nThe next step will be to put it all together and see whether the agent is\ncapable of orchestrating all the tools, with some assistance in terms of\nprompt engineering.\nPutting it all together\nNow that we have all the ingredients, we need to put them together into one\nsingle agent. To do so, we can follow these steps:\n1. First, we need to add the DALL·E tool to the list of tools:\ntools = []\ntools.append(YouTubeSearchTool())\ntools.append(CustomYTTranscribeTool())\ntools.append(load_tools(['dalle-image-generator'])[0])\n[tool.name for tool in tools]\nThis gives us the following output:\n",
      "content_length": 1230,
      "extraction_method": "Direct"
    },
    {
      "page_number": 385,
      "chapter": null,
      "content": "['youtube_search', 'CustomeYTTranscribe', 'Dall-E Image Gen\n2. The next step will be to test the agent with the default prompt, and then\ntry to refine the instructions with some prompt engineering. Let’s start\nwith a pre-configured agent (you can find all the steps in the GitHub\nrepository):\nagent = initialize_agent(tools, model, AgentType.ZERO_\nagent.run(\"search a video trailer of Avatar: the way o\nThis gives us the following output:\n> Entering new AgentExecutor chain...\nI need to search for a video trailer of \"Avatar: The Way of\nAction: youtube_search\nAction Input: \"Avatar: The Way of Water trailer,1\"\nObservation: ['/watch?v=d9MyW72ELq0&pp=ygUgQXZhdGFyOiBUaGUg\nThought:I found a video trailer of \"Avatar: The Way of Wate\nAction: CustomeYTTranscribe\nAction Input: '/watch?v=d9MyW72ELq0&pp=ygUgQXZhdGFyOiBUaGUg\nObservation: ♪ Dad, I know you think I'm crazy. […]\nThought:I have transcribed the video and now I can generate\nAction: Dall-E Image Generator\nAction Input: \"Review of Avatar: The Way of Water trailer: \nObservation: [image_url]\nThought:I have generated a review of the \"Avatar: The Way o\nFinal Answer: The \"Avatar: The Way of Water\" trailer is cap\n> Finished chain.\nThe following is the accompanying visual output:\n",
      "content_length": 1234,
      "extraction_method": "Direct"
    },
    {
      "page_number": 386,
      "chapter": null,
      "content": "Figure 10.8: Image generated by DALL·E based on the trailer review\nWell, even without any prompt engineering, the agent was able to\norchestrate the tools and return the desired results!\n3. Now, let’s try to make it more tailored toward our purpose. Similar to\nthe CoPenny application, we don’t want the user to specify every time\nto generate a review alongside an image. So let’s modify the default\nprompt as follows:\nPREFIX = \"\"\"\nYou are an expert reviewer of movie trailer.\nYou adapt the style of the review depending on the cha\nYou can use multiple tools to answer the question.\nALWAYS search for the youtube video related to the tra\nALWAYS transcribe the youtube trailer and use it to ge\nALWAYS generate an image alongside the review, based o\nALWAYS use all the available tools for the various ste\nYou have access to the following tools:\n\"\"\"\n",
      "content_length": 846,
      "extraction_method": "Direct"
    },
    {
      "page_number": 387,
      "chapter": null,
      "content": "agent = initialize_agent(tools, model, agent=AgentType\n                         agent_kwargs={\n                            'prefix':PREFIX})\nagent.run(\"Generate a review of the trailer of Avatar:\nThe output obtained is as follows:\n> Entering new AgentExecutor chain...\nTo generate a review for Instagram, I will need to search f\nAction: youtube_search\nAction Input: \"Avatar: The Way of Water trailer\", 1\nObservation: ['/watch?v=d9MyW72ELq0&pp=ygUhQXZhdGFyOiBUaGUg\nThought:I have found a YouTube video of the \"Avatar: The Wa\nAction: CustomeYTTranscribe\nAction Input: '/watch?v=d9MyW72ELq0&pp=ygUhQXZhdGFyOiBUaGUg\nObservation: ♪ Dad, I know you think I'm crazy.[…]\nAction: Dall-E Image Generator\nAction Input: \"A scene from the movie 'Avatar: The Way of W\nObservation: [image_url]\nThought:I have generated an image for the Instagram review \nFinal Answer: \"Avatar: The Way of Water\" is an upcoming mov\n> Finished chain.\nThis is accompanied by the following visual output:\n",
      "content_length": 969,
      "extraction_method": "Direct"
    },
    {
      "page_number": 388,
      "chapter": null,
      "content": "Figure 10.9: Image generated by DALL·E based on a trailer review\nWow! Not only was the agent able to use all the tools with the proper scope\nbut it also adapted the style to the type of channel we want to share our\nreview on – in this case, Instagram.\nOption 3: Hard-coded\napproach with a sequential\nchain\nThe third and last option offers yet another way of implementing a\nmultimodal application, which performs the following tasks:\nGenerates a story based on a topic given by the user.\nGenerates a social media post to promote the story.\n",
      "content_length": 539,
      "extraction_method": "Direct"
    },
    {
      "page_number": 389,
      "chapter": null,
      "content": "Generates an image to go along with the social media post.\nWe will call this application StoryScribe.\nTo implement this, we will build separate LangChain chains for those single\ntasks, and then combine them into a SequentialChain. As we saw in\nChapter 1, this is a type of chain that allows you to execute multiple chains\nin a sequence. You can specify the order of the chains and how they pass\ntheir outputs to the next chain. So, we first need to create individual chains,\nthen combine them and run as a unique chain. Let’s follow these steps:\n1. We’ll start by initializing the story generator chain:\nfrom langchain.chains import SequentialChain, LLMChain\nfrom langchain.prompts import PromptTemplate\nstory_template = \"\"\"You are a storyteller. Given a top\nTopic: {topic}\nGenre: {genre}\nAudience: {audience}\nStory: This is a story about the above topic, with the\nstory_prompt_template = PromptTemplate(input_variables\nstory_chain = LLMChain(llm=llm, prompt=story_prompt_te\nresult = story_chain({'topic': 'friendship story','gen\nprint(result['story'])\nThis gives us the following output:\nJohn and Sarah had been best friends since they were kids. \n2. Note that I’ve set the output_key= \"story\" parameter so that it can be\neasily linked as output to the next chain, which will be the social post\ngenerator:\n",
      "content_length": 1307,
      "extraction_method": "Direct"
    },
    {
      "page_number": 390,
      "chapter": null,
      "content": "template = \"\"\"You are an influencer that, given a stor\nThe style should reflect the type of social media used\nStory:\n{story}\nSocial media: {social}\nReview from a New York Times play critic of the above \nprompt_template = PromptTemplate(input_variables=[\"sto\nsocial_chain = LLMChain(llm=llm, prompt=prompt_templat\npost = social_chain({'story': result['story'], 'social\nprint(post['post'])\nThe following output is then obtained:\n\"John and Sarah's journey of discovery and friendship is a \nHere, I used the output of story_chain as input to social_chain.\nWhen we combine all the chains together, this step will be\nautomatically performed by the sequential chain.\n3. Finally, let’s initialize an image generator chain:\nfrom langchain.utilities.dalle_image_generator import \nfrom langchain.llms import OpenAI\ntemplate = \"\"\"Generate a detailed prompt to generate a\nSocial media post:\n{post}\n\"\"\"\nprompt = PromptTemplate(\n    input_variables=[\"post\"],\n    template=template,\n)\nimage_chain = LLMChain(llm=llm, prompt=prompt, output_\n",
      "content_length": 1024,
      "extraction_method": "Direct"
    },
    {
      "page_number": 391,
      "chapter": null,
      "content": "Note that the output of the chain will be the prompt to pass to the\nDALL·E model.\n4. In order to generate the image, we need to use the DallEAPIWrapper()\nmodule available in LangChain:\nfrom langchain.utilities.dalle_image_generator import \nimage_url = DallEAPIWrapper().run(image_chain.run(\"a c\nimport cv2\nfrom skimage import io\nimage = io.imread(image_url)\ncv2.imshow('image', image)\ncv2.waitKey(0)  \ncv2.destroyAllWindows()\nThis generates the following output:\nFigure 10.10: Picture generated by DALL·E given a social media post\n",
      "content_length": 531,
      "extraction_method": "Direct"
    },
    {
      "page_number": 392,
      "chapter": null,
      "content": "5. The final step will be to put it all together into a sequential chain:\noverall_chain = SequentialChain(input_variables = ['to\n                chains=[story_chain, social_chain, ima\n                output_variables = ['post', 'image'], \noverall_chain({'topic': 'friendship story','genre':'ad\nHere is our output:\n{'post': '\\n\\n\"John and Sarah\\'s journey of discovery and f\n'image': '\\nPrompt:\\n\\nCreate a digital drawing of John and\nSince we passed the output_variables = ['post, 'image'] parameter to\nthe chain, those will be the two outputs of the chain. With SequentialChain,\nwe have the flexibility to decide as many output variables as we want, so\nthat we can construct our output as we please.\nOverall, there are several ways to reach multimodality within your\napplication, and LangChain offers many components that make it easier.\nNow, let’s compare these approaches.\nComparing the three options\nWe examined three options to achieve this result: options 1 and 2 follow the\n“agentic” approach, using, respectively, pre-built toolkit and single tools\ncombined; option 3, on the other hand, follows a hard-coded approach,\nletting the developer decide the order of actions to be done.\nAll three come with pros and cons, so let’s wrap up some final\nconsiderations:\n",
      "content_length": 1268,
      "extraction_method": "Direct"
    },
    {
      "page_number": 393,
      "chapter": null,
      "content": "Flexibility vs control: The agentic approach lets the LLM decide\nwhich actions to take and in which order. This implies greater flexibility\nfor the end user since there are no constraints in terms of queries that\ncan be done. On the other hand, having no control over the agent’s\nchain of thoughts could lead to mistakes that would need several tests\nof prompt engineering. Plus, as LLMs are non-deterministic, it is also\nhard to recreate mistakes to retrieve the wrong thought process. Under\nthis point of view, the hard-coded approach is safer, since the developer\nhas full control over the order of execution of the actions.\nEvaluations: The agentic approach leverages the tools to generate the\nfinal answer so that we don’t have to bother to plan these actions.\nHowever, if the final output doesn’t satisfy us, it might be cumbersome\nto understand what is the main source of the error: it might be a wrong\nplan, rather than a tool that is not doing its job correctly, or maybe a\nwrong prompt overall. On the other hand, with the hard-coded\napproach, each chain has its own model that can be tested separately, so\nthat it is easier to identify the step of the process where the main error\nhas occurred.\nMaintenance: With the agentic approach, there is one component to\nmaintain: the agent itself. We have in fact one prompt, one agent, and\none LLM, while the toolkit or list of tools is pre-built and we don’t\nneed to maintain them. On the other hand, with the hard-coded\napproach, for each chain, we need a separate prompt, model, and testing\nactivities.\nTo conclude, there is no golden rule to decide which approach to follow: it’s\nup to the developer to decide depending on the relative weight of the above\nparameters. As a general rule of thumb, the first step should be to define the\nproblem to solve and then evaluate the complexity of each approach with\n",
      "content_length": 1864,
      "extraction_method": "Direct"
    },
    {
      "page_number": 394,
      "chapter": null,
      "content": "respect to that problem. If, for example, it is a task that can be entirely\naddressed with the Cognitive Services toolkit without even doing prompt\nengineering, that could be the easiest way to proceed; on the other hand, if it\nrequires a lot of control over the single components as well as on the\nsequence of execution, a hard-coded approach is preferable.\nIn the next section, we are going to build a sample front-end using Streamlit,\nbuilt on top of StoryScribe.\nDeveloping the front-end\nwith Streamlit\nNow that we have seen the logic behind an LLM-powered StoryScribe, it is\ntime to give our application a GUI. To do so, we will once again leverage\nStreamlit. As always, you can find the whole Python code in the GitHub\nbook repository at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications.\nAs per the previous sections, you need to create a .py file to run in your\nterminal via streamlit run file.py. In our case, the file will be named\nstoryscribe.py.\nThe following are the main steps to set up the front-end:\n1. Configuring the application webpage:\nst.set_page_config(page_title=\"StoryScribe\", page_icon\nst.header('\n Welcome to StoryScribe, your story gener\nload_dotenv()\nopenai_api_key = os.environ['OPENAI_API_KEY']\n",
      "content_length": 1245,
      "extraction_method": "Direct"
    },
    {
      "page_number": 395,
      "chapter": null,
      "content": "2. Initialize the dynamic variables to be used within the placeholders of\nthe prompts:\ntopic = st.sidebar.text_input(\"What is topic?\", 'A dog\ngenre = st.sidebar.text_input(\"What is the genre?\", 'D\naudience = st.sidebar.text_input(\"What is your audienc\nsocial = st.sidebar.text_input(\"What is your social?\",\n3. Initialize all the chains and the overall chain (I will omit here all the\nprompt templates; you can find them in the GitHub repository of the\nbook):\nstory_chain = LLMChain(llm=llm, prompt=story_prompt_te\nsocial_chain = LLMChain(llm=llm, prompt=social_prompt_\nimage_chain = LLMChain(llm=llm, prompt=prompt, output_\noverall_chain = SequentialChain(input_variables = ['to\n                chains=[story_chain, social_chain, ima\n                output_variables = ['story','post', 'i\n4. Run the overall chain and print the results:\nif st.button('Create your post!'):\n    result = overall_chain({'topic': topic,'genre':gen\n    image_url = DallEAPIWrapper().run(result['image'])\n    st.subheader('Story')\n    st.write(result['story'])\n    st.subheader('Social Media Post')\n    st.write(result['post'])\n    st.image(image_url)\nIn this case, I’ve set the output_variables = ['story','post', 'image']\nparameter so that we will have also the story itself as output. The final result\nlooks like the following:\n",
      "content_length": 1308,
      "extraction_method": "Direct"
    },
    {
      "page_number": 396,
      "chapter": null,
      "content": "Figure 10.11: Front-end of StoryScribe showing the story output\nThe following picture is the resulting Instagram post:\nFigure 10.12: Front-end of StoryScribe showing the social media post along with the generated image\n",
      "content_length": 219,
      "extraction_method": "Direct"
    },
    {
      "page_number": 397,
      "chapter": null,
      "content": "With just a few lines of code, we were able to set up a simple front-end for\nStoryScribe with multimodal capabilities.\nSummary\nIn this chapter, we introduced the concept of multimodality and how to\nachieve it even without multimodal models. We explored three different\nways of achieving the objective of a multimodal application: an agentic\napproach with a pre-built toolkit, an agentic approach with the combination\nof single tools, and a hard-coded approach with chained models.\nWe delved into the concrete implementation of three applications with the\nabove methods, examining the pros and cons of each approach. We saw, for\nexample, how an agentic approach gives higher flexibility to the end user at\nthe price of less control of the backend plan of action.\nFinally, we implemented a front-end with Streamlit to build a consumable\napplication with the hard-coded approach.\nWith this chapter, we conclude Part 2 of the book, where we examined\nhands-on scenarios and built LLMs-powered applications. In the next\nchapter, we will focus on how to customize your LLMs even more with the\nprocess of fine-tuning, leveraging open-source models, and using custom\ndata for this purpose.\nReferences\nSource code for YouTube tools:\nhttps://github.com/venuv/langchain_yt_tools\nLangChain YouTube tool:\nhttps://python.langchain.com/docs/integrations/t\n",
      "content_length": 1340,
      "extraction_method": "Direct"
    },
    {
      "page_number": 398,
      "chapter": null,
      "content": "ools/youtube\nLangChain AzureCognitiveServicesToolkit:\nhttps://python.langchain.com/docs/integrations/t\noolkits/azure_cognitive_services\nJoin our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\nOceanofPDF.com\n",
      "content_length": 290,
      "extraction_method": "Direct"
    },
    {
      "page_number": 399,
      "chapter": null,
      "content": "11\nFine-Tuning Large\nLanguage Models\nUp to this point, we’ve explored the features and applications of large\nlanguage models (LLMs) in their “base” form, meaning that we consumed\nthem with the parameters obtained from their base training. We\nexperimented with many scenarios in which, even in their base form, LLMs\nhave been able to adapt to a variety of scenarios. Nevertheless, there might\nbe extremely domain-specific cases where a general-purpose LLM is not\nsufficient to fully embrace the taxonomy and knowledge of that domain. If\nthis is the case, you might want to fine-tune your model on your domain-\nspecific data.\nDefinition\nIn the context of fine-tuning language models, “taxonomy”\nrefers to a structured classification or categorization system\nthat organizes concepts, terms, and entities according to their\nrelationships and hierarchies within a specific domain. This\nsystem is essential for making the model’s understanding and\ngeneration of content more relevant and accurate for\nspecialized applications.\n",
      "content_length": 1021,
      "extraction_method": "Direct"
    },
    {
      "page_number": 400,
      "chapter": null,
      "content": "A concrete example of taxonomy in a domain-specific sector\nis in the medical field. Here, taxonomy could categorize\ninformation into structured groups like diseases, symptoms,\ntreatments, and patient demographics. For instance, in the\n“diseases” category, there might be subcategories for types\nof diseases like “cardiovascular diseases,” which could be\nfurther divided into more specific conditions such as\n“hypertension” and “coronary artery disease.” This detailed\ncategorization helps in fine-tuning language models to\nunderstand and generate more precise and contextually\nappropriate responses in medical consultations or\ndocumentation.\nIn this chapter, we are going to cover the technical details of fine-tuning\nLLMs, from the theory behind it to the hands-on implementation with\nPython and Hugging Face. By the end of this chapter, you will be able to\nfine-tune an LLM on your own data, so that you can build domain-specific\napplications powered by those models.\nWe will delve into the following topics:\nIntroduction to fine-tuning\nUnderstanding when you need fine-tuning\nPreparing your data to fine-tune the model\nFine-tuning a base model on your data\nHosting strategies for your fine-tuned model\nTechnical requirements\nTo complete the tasks in this chapter, you will need the following:\n",
      "content_length": 1296,
      "extraction_method": "Direct"
    },
    {
      "page_number": 401,
      "chapter": null,
      "content": "A Hugging Face account and user access token.\nPython 3.7.1 or later version.\nPython packages: Make sure to have the following Python packages\ninstalled: python-dotenv, huggingface_hub, accelerate>=0.16.0, <1\ntransformers[torch], safetensors, tensorflow, datasets, evaluate,\nand accelerate. Those can be easily installed via pip install in your\nterminal. If you want to install everything from the latest release, you\ncan refer to the original GitHub by running pip install\ngit+https://github.com/huggingface/transformers.git in your\nterminal.\nYou can find all the code and examples in the book’s GitHub repository at\nhttps://github.com/PacktPublishing/Building-LLM-\nPowered-Applications.\nWhat is fine-tuning?\nFine-tuning is a technique of transfer learning in which the weights of a\npretrained neural network are used as the initial values for training a new\nneural network on a different task. This can improve the performance of the\nnew network by leveraging the knowledge learned from the previous task,\nespecially when the new task has limited data.\nDefinition\nTransfer learning is a technique in machine learning that\ninvolves using the knowledge learned from one task to\nimprove the performance on a related but different task. For\nexample, if you have a model that can recognize cars, you\n",
      "content_length": 1296,
      "extraction_method": "Direct"
    },
    {
      "page_number": 402,
      "chapter": null,
      "content": "can use some of its features to help you recognize trucks.\nTransfer learning can save you time and resources by reusing\nexisting models instead of training new ones from scratch.\nTo better understand the concepts of transfer learning and fine-tuning, let’s\nconsider the following example.\nImagine you want to train a computer vision neural network to recognize\ndifferent types of flowers, such as roses, sunflowers, and tulips. You have a\nlot of photos of flowers, but not enough to train a model from scratch.\nInstead, you can use transfer learning, which means taking a model that was\nalready trained on a different task and using some of its knowledge for your\nnew task. For example, you can take a model that was trained to recognize\nmany vehicles, such as cars, trucks, and bicycles. This model has learned\nhow to extract features from images, such as edges, shapes, colors, and\ntextures. These features are useful for any image recognition task, not just\nthe original one.\nYou can use this model as a base for your flower recognition model. You\nonly need to add a new layer on top of it, which will learn how to classify\nthe features into flower types. This layer is called the classifier layer, and it\nis needed for the model to adapt to the new task. Training the classifier layer\non top of the base model is a process called feature extraction. Once this\nstep is done, you can further tailor your model with fine-tuning by\nunfreezing some of the base model layers and training them together with\nthe classifier layer. This allows you to adjust the base model features to\nbetter suit your task.\nThe following picture illustrates the computer vision model example:\n",
      "content_length": 1672,
      "extraction_method": "Direct"
    },
    {
      "page_number": 403,
      "chapter": null,
      "content": "Figure 11.1: Example of transfer learning and fine-tuning\nFine-tuning is usually done after feature extraction, as a final step to improve\nthe performance of the model. You can decide how many layers to unfreeze\nbased on your data size and complexity. A common practice is to unfreeze\nthe last few layers of the base model, which are more specific to the original\ntask, and leave the first few layers frozen, which are more generic and\nreusable.\nTo summarize, transfer learning and fine-tuning are techniques that allow\nyou to use a pretrained model for a new task. Transfer learning involves\nadding a new classifier layer on top of the base model and training only that\nlayer. Fine-tuning involves unfreezing some or all of the base model layers\nand training them together with the classifier layer.\nIn the context of generative AI, fine-tuning is the process of adapting a\npretrained language model to a specific task or domain by updating its\n",
      "content_length": 946,
      "extraction_method": "Direct"
    },
    {
      "page_number": 404,
      "chapter": null,
      "content": "parameters on a task-specific dataset. Fine-tuning can improve the\nperformance and accuracy of the model for the target task. The steps\ninvolved in fine-tuning are:\n1. Load the pretrained language model and its tokenizer: The\ntokenizer is used to convert text into numerical tokens that the model\ncan process. Different models have unique architectures and\nrequirements, often coming with their own specialized tokenizers\ndesigned to handle their specific input formats.\nFor instance, BERT (which stands for Bidirectional Encoder\nRepresentations from Transformers) uses WordPiece tokenization,\nwhile GPT-2 employs byte-pair encoding (BPE). Models also impose\ntoken limits due to memory constraints during training and inference.\nThese limits determine the maximum sequence length that a model\ncan handle. For example, BERT has a maximum token limit of 512\ntokens, while the GPT-2 can handle longer sequences (e.g., up to\n1,024 tokens).\n2. Prepare the task-specific dataset: The dataset should contain input-\noutput pairs that are relevant to the task. For example, for sentiment\nanalysis, the input could be a text review and the output could be a\nsentiment label (positive, negative, or neutral).\n3. Define the task-specific head: The head is a layer or a set of layers that\nare added on top of the pretrained model to perform the task. The head\nshould match the output format and size of the task. For example, for\nsentiment analysis, the head could be a linear layer with three output\nunits corresponding to the three sentiment labels.\nNote\n",
      "content_length": 1544,
      "extraction_method": "Direct"
    },
    {
      "page_number": 405,
      "chapter": null,
      "content": "When dealing with an LLM specifically designed for\ntext generation, the architecture differs from models\nused for classification or other tasks. In fact, unlike\nclassification tasks, where we predict labels, an LLM\npredicts the next word or token in a sequence. This\nlayer is added on top of the pretrained transformer-\nbased models with the purpose of transforming the\ncontextualized hidden representations from the base\nmodel into probabilities over the vocabulary.\n4. Train the model on the task-specific dataset: The training process\ninvolves feeding the input tokens to the model, computing the loss\nbetween the model output and the true output, and updating the model\nparameters using an optimizer. The training can be done for a fixed\nnumber of epochs or until a certain criterion is met.\n5. Evaluate the model on a test or validation set: The evaluation\nprocess involves measuring the performance of the model on unseen\ndata using appropriate metrics. For example, for sentiment analysis, the\nmetric could be accuracy or F1-score (which will be discussed later in\nthis chapter). The evaluation results can be used to compare different\nmodels or fine-tuning strategies.\nEven though it is less computationally and time expensive than full training,\nfine-tuning an LLM is not a “light” activity. As LLMs are, by definition,\nlarge, their fine-tuning has hardware requirements as well as data collection\nand preprocessing.\nSo, the first question that you want to ask yourself while approaching a\ngiven scenario is: “Do I really need to finetune my LLM?”\n",
      "content_length": 1557,
      "extraction_method": "Direct"
    },
    {
      "page_number": 406,
      "chapter": null,
      "content": "When is fine-tuning\nnecessary?\nAs we saw in previous chapters, good prompt engineering combined with\nthe non-parametric knowledge you can add to your model via embeddings\nare exceptional techniques to customize your LLM, and they can account for\naround 90% of use cases. However, the preceding affirmation tends to hold\nfor the state-of-the-art models, such as GPT-4, Llama 2, and PaLM 2. As\ndiscussed, those models have a huge number of parameters that make them\nheavy, hence the need for computational power; plus, they might be\nproprietary and subject to a pay-per-use cost.\nHenceforth, fine-tuning might also be useful when you want to leverage a\nlight and free-of-charge LLM, such as the Falcon LLM 7B, yet you want it\nto perform as well as a SOTA model in your specific task.\nSome examples of when fine-tuning might be necessary are:\nWhen you want to use an LLM for sentiment analysis on movie\nreviews, but the LLM was pretrained on Wikipedia articles and books.\nFine-tuning can help the LLM learn the vocabulary, style, and tone of\nmovie reviews, as well as the relevant features for sentiment\nclassification.\nWhen you want to use an LLM for text summarization on news articles,\nbut the LLM was pretrained on a language modeling objective. Fine-\ntuning can help the LLM learn the structure, content, and length of\nsummaries, as well as the generation objective and evaluation metrics.\nWhen you want to use an LLM for machine translation between two\nlanguages, but the LLM was pretrained on a multilingual corpus that\ndoes not include those languages. Fine-tuning can help the LLM learn\n",
      "content_length": 1593,
      "extraction_method": "Direct"
    },
    {
      "page_number": 407,
      "chapter": null,
      "content": "the vocabulary, grammar, and syntax of the target languages, as well as\nthe translation objective and alignment methods.\nWhen you want to use an LLM to perform complex named entity\nrecognition (NER) tasks. For example, financial and legal documents\ncontain specialized terminology and entities that are not typically\nprioritized in general language models, henceforth a fine-tuning process\nmight be extremely beneficial here.\nIn this chapter, we will be covering a full-code approach leveraging Hugging\nFace models and libraries. However, be aware that Hugging Face also offers\na low-code platform called AutoTrain (you can read more about that at\nhttps://huggingface.co/autotrain), which might be a good\nalternative if your organization is more oriented towards low-code strategies.\nGetting started with fine-\ntuning\nIn this section, we are going to cover all the steps needed to fine-tune an\nLLM with a full-code approach. We will be leveraging Hugging Face\nlibraries, such as datasets (to load data from the Hugging Face datasets\necosystem) and tokenizers (to provide an implementation of the most\npopular tokenizers). The scenario we are going to address is a sentiment\nanalysis task. Our goal is to fine-tune a model to make it an expert binary\nclassifier of emotions, clustered into “positive” and “negative.”\nObtaining the dataset\nThe first ingredient that we need is the training dataset. For this purpose, I\nwill leverage the datasets library available in Hugging Face to load a binary\n",
      "content_length": 1495,
      "extraction_method": "Direct"
    },
    {
      "page_number": 408,
      "chapter": null,
      "content": "classification dataset called IMDB (you can find the dataset card at\nhttps://huggingface.co/datasets/imdb).\nThe dataset contains movie reviews, which are classified as positive or\nnegative. More specifically, the dataset contains two columns:\nText: The raw text movie review.\nLabel: The sentiment of that review. It is mapped as “0” for “Negative”\nand “1” for “Positive.”\nAs it is a supervised learning problem, the dataset already comes with\n25,000 rows for the training set and 25,000 rows for the validation set.\nDefinition\nSupervised learning is a type of machine learning that uses\nlabeled datasets to train algorithms to classify data or predict\noutcomes accurately. Labeled datasets are collections of\nexamples that have both input features and desired output\nvalues, also known as labels or targets. For example, a\nlabeled dataset for handwriting recognition might have\nimages of handwritten digits as input features and the\ncorresponding numerical values as labels.\nTraining and validation sets are subsets of the labeled dataset\nthat are used for different purposes in the supervised learning\nprocess. The training set is used to fit the parameters of the\nmodel, such as the weights of the connections in a neural\nnetwork. The validation set is used to tune the\nhyperparameters of the model, such as the number of hidden\nunits in a neural network or the learning rate.\n",
      "content_length": 1379,
      "extraction_method": "Direct"
    },
    {
      "page_number": 409,
      "chapter": null,
      "content": "Hyperparameters are settings that affect the overall behavior\nand performance of the model but are not directly learned\nfrom the data. The validation set helps to select the best\nmodel among different candidates by comparing their\naccuracy or other metrics on the validation set.\nSupervised learning differs from another type of machine\nlearning, which is unsupervised learning. With the latter,\nthe algorithm is tasked with finding patterns, structures, or\nrelationships in a dataset without the presence of labeled\noutputs or targets. In other words, in unsupervised learning,\nthe algorithm is not provided with specific guidance or labels\nto direct its learning process. Instead, it explores the data and\nidentifies inherent patterns or groupings on its own.\nYou can download the IMDB dataset by running the following code:\nfrom datasets import load_dataset\ndataset = load_dataset(\"imdb\")\ndataset\nHugging Face datasets come with a dictionary schema, which is as follows:\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n",
      "content_length": 1193,
      "extraction_method": "Direct"
    },
    {
      "page_number": 410,
      "chapter": null,
      "content": "        features: ['text', 'label'],\n        num_rows: 50000\n    })\n})\nTo access one observation of a particular Dataset object (for example,\ntrain), you can use slicers, as follows:\ndataset[\"train\"][100]\nThis gives us the following output:\n{'text': \"Terrible movie. Nuff Said.[…]\n 'label': 0}\nSo, the 101st observation of the training set contains a review labeled as\nnegative.\nNow that we have the dataset, we need to preprocess it so that can be used to\ntrain our LLM. To do so, we need to tokenize the provided text, and we will\ndiscuss this in the next section.\nTokenizing the data\nA tokenizer is a component that is responsible for splitting a text into smaller\nunits, such as words or subwords, that can be used as inputs for an LLM.\nTokenizers can be used to encode text efficiently and consistently, as well as\nto add special tokens, such as mask or separator tokens, that are required by\nsome models.\nHugging Face provides a powerful utility called AutoTokenizer, available in\nthe Hugging Face Transformers library, that offers tokenizers for various\n",
      "content_length": 1061,
      "extraction_method": "Direct"
    },
    {
      "page_number": 411,
      "chapter": null,
      "content": "models, such as BERT and GPT-2. It serves as a generic tokenizer class that\ndynamically selects and instantiates the appropriate tokenizer based on the\npretrained model you specify.\nThe following code snippet shows how we can initialize our tokenizer:\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\nNote that we picked a specific tokenizer called bert-base-cased. In fact,\nthere is a link between a tokenizer and an LLM, in the sense that the the\ntokenizer prepares the inputs for the model by converting the text into\nnumerical IDs that the model can understand.\nDefinition\nThe input IDs are the numerical IDs that correspond to the\ntokens in the vocabulary of the tokenizer. They are returned\nby the tokenizer function when encoding a text input. The\ninput IDs are used as inputs for the model, which expects\nnumerical tensors rather than strings. Different tokenizers\nmay have different input IDs for the same tokens, depending\non their vocabulary and tokenization algorithm.\nDifferent models may use different tokenization algorithms, such as word-\nbased, character-based, or subword-based. Therefore, it is important to use\nthe correct tokenizer for each model, otherwise the model may not perform\nwell or even produce errors. Let’s look at potential scenarios for each:\nA character-based approach might fit scenarios that deal with rare\nwords or languages with complex morphological structures, or when\n",
      "content_length": 1462,
      "extraction_method": "Direct"
    },
    {
      "page_number": 412,
      "chapter": null,
      "content": "dealing with spelling correction tasks\nThe word-based approach might be a good fit for scenarios like NER,\nsentiment analysis, and text classification\nThe sub-word approach interpolates between the previous two, and it is\nuseful when we want to balance the granularity of text representation\nwith efficiency.\nAs we will see in the next section, we will leverage the BERT model for this\nscenario, hence we loaded its pretrained tokenizer (which is a word-based\ntokenizer powered by an algorithm called WordPiece).\nWe now need to initialize tokenize_function, which will be used to format\nthe dataset:\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding = \"max_length\", t\ntokenized_datasets = dataset.map(tokenize_function, batched=True\nAs you can see, we also configured the padding and truncation of\ntokenize_function to ensure an output with the right sizing for our BERT\nmodel.\nDefinition\nPadding and truncation are two techniques that are used to\nmake the input sequences of text have the same length. This\nis often required for some natural language processing\n(NLP) models, such as the BERT model, that expect fixed-\nlength inputs.\n",
      "content_length": 1165,
      "extraction_method": "Direct"
    },
    {
      "page_number": 413,
      "chapter": null,
      "content": "Padding means adding some special tokens, usually zeros, at\nthe end or the beginning of a sequence to make it reach the\ndesired length. For example, if we have a sequence of length\n5 and we want to pad it to a length of 8, we can add 3 zeros\nat the end, like this: [1, 2, 3, 4, 5, 0, 0, 0]. This is called post-\npadding. Alternatively, we can add 3 zeros at the beginning,\nlike this: [0, 0, 0, 1, 2, 3, 4, 5]. This is called pre-padding.\nThe choice of padding strategy depends on the model and the\ntask.\nTruncation means removing some tokens from a sequence to\nmake it fit the desired length. For example, if we have a\nsequence of length 10 and we want to truncate it to a length\nof 8, we can remove 2 tokens from the end or the beginning\nof the sequence. For example, we can remove the last 2\ntokens, like this: [1, 2, 3, 4, 5, 6, 7, 8]. This is called post-\ntruncation. Alternatively, we can remove the first 2 tokens,\nlike this: [3, 4, 5, 6, 7, 8, 9, 10]. This is called pre-truncation.\nThe choice of truncation strategy also depends on the model\nand the task.\nNow, we can apply the function to our dataset and inspect the numerical IDs\nof one entry:\ntokenized_datasets = dataset.map(tokenize_function, batched=True\ntokenized_datasets['train'][100]['input_ids']\nHere is our output:\n",
      "content_length": 1285,
      "extraction_method": "Direct"
    },
    {
      "page_number": 414,
      "chapter": null,
      "content": "[101,\n 12008,\n 27788,\n...\n 0,\n 0,\n 0,\n 0,\n 0]\nAs you can see, the last elements of the vector are zeroes, due to the\npadding='max_lenght' parameter passed to the function.\nOptionally, you can decide to reduce the size of your dataset if you want to\nmake the training time shorter. In my case, I’ve shrunk the dataset as\nfollows:\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=4\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42)\nSo, I have two sets – one for training, one for testing – of 500 observations\neach. Now that we have our dataset preprocessed and ready, we need the\nmodel to be fine-tuned.\nFine-tuning the model\nAs anticipated in the previous section, the LLM we are going to leverage for\nfine-tuning is the base version of BERT. The BERT model is a transformer-\nbased, encoder-only model for natural language understanding introduced by\nGoogle researchers in 2018. BERT was the first example of a general-\npurpose LLM, meaning that it was the first model to be able to tackle\n",
      "content_length": 1019,
      "extraction_method": "Direct"
    },
    {
      "page_number": 415,
      "chapter": null,
      "content": "multiple NLP tasks at once, which was different from the task-specific\nmodels existing up to that moment.\nNow, even though it might sound a bit “old fashioned” (in fact, compared to\ntoday’s model like the GPT-4, it is not even “large,” with only 340 million\nparameters in its large version), given all the new LLMs that have emerged\nin the market in the last few months, BERT and its fine-tuned variants are\nstill a widely adopted architecture. In fact, it was thanks to BERT that the\nstandard for language models has greatly improved.\nThe BERT model has two main components:\nEncoder: The encoder consists of multiple layers of transformer blocks,\neach with a self-attention layer and a feedforward layer. The encoder\ntakes as input a sequence of tokens, which are the basic units of text,\nand outputs a sequence of hidden states, which are high-dimensional\nvectors that represent the semantic information of each token.\nOutput layer: The output layer is task-specific and can be different\ndepending on the type of task that BERT is used for. For example, for\ntext classification, the output layer can be a linear layer that predicts the\nclass label of the input text. For question answering, the output layer\ncan be two linear layers that predict the start and end positions of the\nanswer span in the input text.\nThe number of layers and parameters of the model depends on the\nmodel version. In fact, BERT comes in two sizes: BERTbase and\nBERTlarge. The following illustration shows the difference between the\ntwo versions:\n",
      "content_length": 1525,
      "extraction_method": "Direct"
    },
    {
      "page_number": 416,
      "chapter": null,
      "content": "Figure 11.2: A comparison between BERTbase and BERTlarge (source:\nhttps://huggingface.co/blog/bert-101)\nLater, other versions such as BERT-tiny, BERT-mini, BERT-small, and\nBERT-medium were introduced to reduce the computational cost and\nmemory usage of BERT.\nThe model has been trained on a heterogeneous corpus of around 3.3 billion\nwords, belonging to Wikipedia and Google’s BooksCorpus. The training\nphase involved two objectives:\nMasked language modeling (MLM): MLM aims to teach the model to\npredict the original words that are randomly masked (replaced with a\nspecial token) in the input text. For example, given the sentence “He\nbought a new [MASK] yesterday,” the model should predict the word\n“car” or “bike” or something else that makes sense. This objective helps\nthe model learn the vocabulary and the syntax of the language, as well\nas the semantic and contextual relations between words.\nNext sentence prediction (NSP): NSP aims to teach the model to\npredict whether two sentences are consecutive or not in the original\ntext. For example, given the sentences “She loves reading books” and\n“Her favorite genre is fantasy,” the model should predict that they are\nconsecutive because they are likely to appear together in a text.\nHowever, given the sentences “She loves reading books” and “He plays\nsoccer every weekend,” the model should predict that they are not\nconsecutive because they are unlikely to be related. This objective helps\n",
      "content_length": 1450,
      "extraction_method": "Direct"
    },
    {
      "page_number": 417,
      "chapter": null,
      "content": "the model learn the coherence and logic of the text, as well as the\ndiscourse and pragmatic relations between sentences.\nBy using these two objectives (on which the model is trained at the same\ntime), the BERT model can learn general language knowledge that can be\ntransferred to specific tasks, such as text classification, question answering,\nand NER. The BERT model can achieve better performance on these tasks\nthan previous models that only use one direction of context or do not use\npre-training at all. In fact, it has achieved state-of-the-art results on many\nbenchmarks and tasks, such as General Language Understanding\nEvaluation (GLUE), Stanford Question Answering Dataset (SQuAD),\nand Multi-Genre Natural Language Inference (MultiNLI).\nThe BERT model is available – along with many fine-tuned versions –in the\nHugging Face Hub. You can instantiate the model as follows:\nimport torch\nfrom transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert\nNote that AutoModelForSequenceClassification is a subclass of AutoModel,\nwhich can instantiate a model architecture suitable for sequence\nclassification, such as text classification or sentiment analysis. It can be used\nfor any task that requires a single label or a list of labels for each input\nsequence. In my case, I set the number of output labels equal to two since we\nare dealing with a binary classification problem.\nOn the other hand, AutoModel is a generic class that can instantiate any\nmodel architecture from the library based on the pretrained model name or\n",
      "content_length": 1592,
      "extraction_method": "Direct"
    },
    {
      "page_number": 418,
      "chapter": null,
      "content": "path. It can be used for any task that does not require a specific output\nformat, such as feature extraction or language modeling.\nThe final step before starting the training is to define the evaluation metrics\nwe will need to understand how well our model will perform once fine-\ntuned.\nUsing evaluation metrics\nAs we saw in Chapter 1, evaluating an LLM in its general-purpose\napplication might be cumbersome. As those models are trained on unlabeled\ntext and are not task-specific, but rather generic and adaptable given a user’s\nprompt, traditional evaluation metrics were not suitable anymore. Evaluating\nan LLM means, among other things, measuring its language fluency, its\ncoherence, and its ability to emulate different styles depending on a user’s\nrequest.\nHowever, we also saw how an LLM can be used for very specific scenarios,\nas in our binary classification task. If this is the case, evaluation metrics boil\ndown to those commonly used for that scenario.\nNote\nWhen it comes to more conversational tasks like\nsummarization, Q&A, and retrieval-augmented generation, a\nnew set of evaluation metrics needs to be introduced, often\npowered in turn by LLMs. Some of the most popular metrics\nare the following:\nFluency: This assesses how naturally and smoothly the\ngenerated text reads.\n",
      "content_length": 1292,
      "extraction_method": "Direct"
    },
    {
      "page_number": 419,
      "chapter": null,
      "content": "Coherence: This evaluates the logical flow and\nconnectivity of ideas within a text.\nRelevance: This measures how well the generated\ncontent aligns with the given prompt or context.\nGPT-similarity: This quantifies how closely the\ngenerated text resembles human-written content.\nGroundedness: This assesses whether the generated text\nis based on factual information or context.\nThese evaluation metrics help us understand the quality,\nnaturalness, and relevance of LLM-generated text, guiding\nimprovements and ensuring reliable AI assistance.\nWhen it comes to binary classification, one of the most basic ways to\nevaluate a binary classifier is to use a confusion matrix. A confusion matrix\nis a table that shows how many of the predicted labels match the true labels.\nIt has four cells:\nTrue positive (TP): The number of cases where the classifier correctly\npredicted 1 when the true label was 1.\nFalse positive (FP): The number of cases where the classifier\nincorrectly predicted 1 when the true label was 0.\nTrue negative (TN): The number of cases where the classifier correctly\npredicted 0 when the true label was 0.\nFalse negative (FN): The number of cases where the classifier\nincorrectly predicted 0 when the true label was 1.\nHere is an example of a confusion matrix for the sentiment classifier we are\ngoing to build, knowing that the label 0 is associated with “Negative” and\nthe label 1 with “Positive”:\n",
      "content_length": 1413,
      "extraction_method": "Direct"
    },
    {
      "page_number": 420,
      "chapter": null,
      "content": "Predicted Positive\nPredicted Negative\nPositive\n20 (TP)\n5 (FN)\nNegative\n3 (FP)\n72 (TN)\nThe confusion matrix can be used to calculate various metrics that measure\ndifferent aspects of the classifier’s performance. Some of the most common\nmetrics are:\nAccuracy: The proportion of correct predictions among all predictions.\nIt is calculated as (TP + TN) / (TP + FP + TN + FN). For example, the\naccuracy of the sentiment classifier is (20 + 72) / (20 + 3 + 72 + 5)\n= 0.92.\nPrecision: The proportion of correct positive predictions among all\npositive predictions. It is calculated as TP / (TP + FP). For example,\nthe precision of the sentiment classifier is 20 / (20 + 3) = 0.87.\nRecall: The proportion of correct positive predictions among all\npositive cases. It is also known as sensitivity or true positive rate. It is\ncalculated as TP / (TP + FN). For example, the recall of the sentiment\nclassifier is 20 / (20 + 5) = 0.8.\nSpecificity: The proportion of correct negative predictions among all\nnegative cases. It is also known as the true negative rate. It is calculated\nas TN / (TN + FP). For example, the specificity of the sentiment\nclassifier is 72 / (72 + 3) = 0.96.\nF1-score: The harmonic mean of precision and recall. It is a measure of\nbalance between precision and recall. It is calculated as 2 * (precision\n",
      "content_length": 1315,
      "extraction_method": "Direct"
    },
    {
      "page_number": 421,
      "chapter": null,
      "content": "* recall) / (precision + recall). For example, the F1-score of the\nsentiment classifier is 2 * (0.87 * 0.8) / (0.87 + 0.8) = 0.83.\nThere are many other metrics that can be derived from the confusion matrix\nor other sources, such as the decision score or the probability output of the\nclassifier. Some examples are:\nReceiver operating characteristic (ROC) curve: A plot of recall\nversus false positive rate (FP / (FP + TN)), which shows how well the\nclassifier can distinguish between positive and negative cases at\ndifferent thresholds.\nArea under the ROC curve (AUC): The AUC, which measures how\nwell the classifier can rank positive cases higher than negative cases. It\ncan be illustrated in the following diagram, where the ROC curve and\nthe area under the curve are displayed:\n",
      "content_length": 781,
      "extraction_method": "Direct"
    },
    {
      "page_number": 422,
      "chapter": null,
      "content": "Figure 11.3: Illustration of a ROC curve, hightlighting a perfect classifier and the Area Under the\nCurve (AUC)\nIn our case, we will simply use the accuracy metric by following these steps:\n1. You can import this metric from the evaluate library as follows:\nimport numpy as np\nimport evaluate\nmetric = evaluate.load(\"accuracy\")\n2. We also need to define a function that computes the accuracy given the\noutput of the training phase:\n",
      "content_length": 432,
      "extraction_method": "Direct"
    },
    {
      "page_number": 423,
      "chapter": null,
      "content": "def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, ref\n3. Finally, we need to set our evaluation strategy, which means how often\nwe want our model to be tested against the test set while training:\nfrom transformers import TrainingArguments, Trainer\ntraining_args = TrainingArguments(output_dir=\"test_tra\nevaluation_strategy=\"epoch\")\nIn our case, we will set epoch as the evaluation strategy, meaning that\nthe evaluation is done at the end of each epoch.\nDefinition\nAn epoch is a term used in machine learning to describe one\ncomplete pass through the entire training dataset. It is a\nhyperparameter that can be tuned to improve the\nperformance of a machine-learning model. During an epoch,\nthe model’s weights are updated based on the training data\nand the loss function. An epoch can consist of one or more\nbatches, which are smaller subsets of the training data. The\nnumber of batches in an epoch depends on the batch size,\nwhich is another hyperparameter that can be adjusted.\nNow we have all the ingredients needed to start our fine-tuning, which will\nbe covered in the next section.\n",
      "content_length": 1196,
      "extraction_method": "Direct"
    },
    {
      "page_number": 424,
      "chapter": null,
      "content": "Training and saving\nThe last component we need to fine-tune our model is a Trainer object. The\nTrainer object is a class that provides an API for feature-complete training\nand evaluation of models in PyTorch, optimized for Hugging Face\nTransformers. You can follow these steps:\n1. Let’s first initialize our Trainer by specifying the parameters we’ve\nalready configured in the previous steps. More specifically, the Trainer\nwill need a model, some configuration args (such as the number of\nepochs), a training dataset, an evaluation dataset, and the type of\nevaluation metric to compute:\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n)\n2. You can then initiate the process of fine-tuning by calling the trainer\nas follows:\ntrainer.train()\nDepending on your hardware, the training process might take some\ntime. In my case, given the reduced size of the dataset and the low\nnumber of epochs (only 2), I don’t expect exceptional results.\nNevertheless, the training results for only two epochs in terms of\naccuracy are the following:\n",
      "content_length": 1157,
      "extraction_method": "Direct"
    },
    {
      "page_number": 425,
      "chapter": null,
      "content": "{'eval_loss': 0.6720085144042969, 'eval_accuracy': 0.58, 'e\n{'eval_loss': 0.5366445183753967, 'eval_accuracy': 0.82, 'e\nAs you can see, between the two epochs the model gained an accuracy\nimprovement of 41.38%, hitting a final accuracy of 82%. Considering\nthe aforementioned elements, that’s not bad!\n3. Once the model is trained, we can save it locally, specifying the path as\nfollows:\ntrainer.save_model('models/sentiment-classifier')\n4. To consume and test the model, you can load it with the following code:\nmodel = AutoModelForSequenceClassification.from_pretra\n5. Finally, we need to test our model. To do so, let’s pass a sentence to the\nmodel (to be first tokenized) on which it can perform sentiment\nclassification:\ninputs = tokenizer(\"I cannot stand it anymore!\", retur\noutputs = model(**inputs)\noutputs\nThis yields the following output:\nSequenceClassifierOutput(loss=None, logits=tensor([[ 0.6467\n",
      "content_length": 908,
      "extraction_method": "Direct"
    },
    {
      "page_number": 426,
      "chapter": null,
      "content": "Note that the model output is a SequenceClassifierOutput object,\nwhich is the base class for outputs of sentence classification models.\nWithin this object, we are interested in the logit tensor, which is the\nvector of raw (non-normalized) predictions associated with labels that\nour classification model generated.\n6. Since we are working with tensors, we will need to leverage the\ntensorflow library in Python. Plus, we will use the softmax function to\nobtain the probability vector associated with each label, so that we\nknow that the final result corresponds to the label with the greatest\nprobability:\nimport tensorflow as tf\npredictions = tf.math.softmax(outputs.logits.detach(),\nprint(predictions)\nThe following is the obtained output:\ntf.Tensor([[0.6571879  0.34281212]], shape=(1, 2), dtype=fl\nOur model tells us that the sentiment of the sentence “I can’t stand it\nanymore” is negative, with a probability of 65.71%.\n7. Note that you can also save the model in your Hugging Face account.\nTo do so, you first need to allow the notebook to push the code to your\naccount as follows:\nfrom huggingface_hub import notebook_login\nnotebook_login()\n",
      "content_length": 1149,
      "extraction_method": "Direct"
    },
    {
      "page_number": 427,
      "chapter": null,
      "content": "8. You will be prompted to the Hugging Face login page, where you have\nto input your access token. Then, you can save the model, specifying\nyour account name and model name:\ntrainer.push_to_hub('vaalto/sentiment-classifier')\nBy doing so, this model can be consumed via the Hugging Face Hub as\neasily as we saw in the previous chapter, as shown in the following\nscreenshot:\nFigure 11.4: Model card within the Hugging Face Hub space\nFurthermore, you can also decide to make the model public, so that everyone\nwithin Hugging Face can test and consume your creation.\nIn this section, we fine-tuned a BERT model with just a few lines of code,\nthanks to Hugging Face libraries and accelerators. Again, if your goal is\n",
      "content_length": 712,
      "extraction_method": "Direct"
    },
    {
      "page_number": 428,
      "chapter": null,
      "content": "reducing the code amount, you can leverage the low-code AutoTrain\nplatform hosted in Hugging Face to train and fine-tune models.\nHugging Face is definitely a solid platform for training your open-source\nLLM. In addition to that, there are further platforms you might want to\nleverage since proprietary models can also be fine-tuned. For example,\nOpenAI lets you fine-tune the GPT series with your own data, providing the\ncomputational power to train and host your customized models.\nOverall, fine-tuning can be the icing on the cake that makes your LLM\nexceptional for your use case. Deciding a strategy to do so based on the\nframework we explored at the beginning is a pivotal step in building a\nsuccessful application.\nSummary\nIn this chapter, we covered the process of fine-tuning LLMs. We started with\na definition of fine-tuning and general considerations to take into account if\nyou have to decide to fine-tune your LLM.\nWe then went hands-on with practical sections on fine-tuning. We covered a\nscenario where, starting from a base BERT model, we wanted a powerful\nreview sentiment analyzer. To do so, we fine-tuned the base model on the\nIMDB dataset using a full-code approach with Hugging Face Python\nlibraries.\nFine-tuning is a powerful technique to further customize LLMs toward your\ngoal. However, along with many other aspects of LLMs, it comes with some\nconcerns and considerations in terms of ethics and security. In the next\nchapter, we are going to delve deeper into that, sharing how to establish\n",
      "content_length": 1515,
      "extraction_method": "Direct"
    },
    {
      "page_number": 429,
      "chapter": null,
      "content": "guardrails with LLMs and, more generally, how governments and countries\nare approaching the problem from a regulatory perspective.\nReferences\nTraining dataset: https://huggingface.co/datasets/imdb\nHF AutoTrain:\nhttps://huggingface.co/docs/autotrain/index\nBERT paper: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina\nToutanova, 2019, BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding:\nhttps://arxiv.org/abs/1810.04805\nJoin our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\nOceanofPDF.com\n",
      "content_length": 604,
      "extraction_method": "Direct"
    },
    {
      "page_number": 430,
      "chapter": null,
      "content": "12\nResponsible AI\nIn Part 2 of this book, we covered multiple applications of large language\nmodels (LLMs), gathering also a deeper understanding of how many factors\ncould influence their behavior and outputs. In fact, LLMs open the doors to a\nnew set of risks and biases to be taken into account while developing LLM-\npowered applications, in order to mitigate them with defensive attacks.\nIn this chapter, we are going to introduce the fundamentals of the discipline\nbehind mitigating the potential harms of LLMs – and AI models in general –\nwhich is Responsible AI. We will then move on to the risks associated with\nLLMs and how to prevent or at least mitigate them using proper techniques.\nBy the end of this chapter, you will have a deeper understanding of how to\nprevent LLMs from making your application potentially harmful.\nWe will cover the following key topics:\nWhat is Responsible AI and why do we need it?\nResponsible AI architecture\nRegulations surrounding Responsible AI\nWhat is Responsible AI and\nwhy do we need it?\n",
      "content_length": 1031,
      "extraction_method": "Direct"
    },
    {
      "page_number": 431,
      "chapter": null,
      "content": "Responsible AI refers to the ethical and accountable development,\ndeployment, and use of AI systems. It involves ensuring fairness,\ntransparency, privacy, and avoiding biases in AI algorithms. Responsible AI\nalso encompasses considerations for the social impact and consequences of\nAI technologies, promoting accountability and human-centric design.\nResponsible AI plays a crucial role in steering decisions toward positive and\nfair results. This involves prioritizing people and their objectives in the\ndesign of systems while upholding enduring values such as fairness,\nreliability, and transparency.\nSome ethical implications of Responsible AI are:\nBias: AI systems can inherit biases present in their training data. These\nbiases can lead to discriminatory outcomes, reinforcing existing\ninequalities.\nExplainability: Black-box models (such as LLMs) lack interpretability.\nEfforts are being made to create more interpretable models to enhance\ntrust and accountability.\nData protection: Collecting, storing, and processing data responsibly is\nessential. Consent, anonymization, and data minimization principles\nshould guide AI development.\nLiability: Determining liability for AI decisions (especially in critical\ndomains) remains a challenge. Legal frameworks need to evolve to\naddress this.\nHuman oversight: AI should complement human decision-making\nrather than replace it entirely. Human judgment is essential, especially\nin high-stakes contexts.\nEnvironmental impact: Training large models consumes significant\nenergy. Responsible AI considers environmental impacts and explores\n",
      "content_length": 1586,
      "extraction_method": "Direct"
    },
    {
      "page_number": 432,
      "chapter": null,
      "content": "energy-efficient alternatives.\nSecurity: Ensuring AI systems are secure and resistant to attacks is\ncrucial.\nAs an example of addressing these implications, Microsoft has established a\nframework called the Responsible AI Standard\n(https://blogs.microsoft.com/wp-\ncontent/uploads/prod/sites/5/2022/06/Microsoft-\nResponsible-AI-Standard-v2-General-Requirements-\n3.pdf), outlining six principles:\nFairness\nReliability and safety\nPrivacy and security\nInclusiveness\nTransparency\nAccountability\nIn the context of generative AI, Responsible AI would mean creating models\nthat respect these principles. For instance, the generated content should be\nfair and inclusive, not favoring any particular group or promoting any form\nof discrimination. The models should be reliable and safe to use. They\nshould respect user’s privacy and security. The process of generation should\nbe transparent, and there should be mechanisms for accountability.\nResponsible AI architecture\nGenerally speaking, there are many levels at which we can intervene to\nmake a whole LLM-powered application safer and more robust: the model\n",
      "content_length": 1101,
      "extraction_method": "Direct"
    },
    {
      "page_number": 433,
      "chapter": null,
      "content": "level, the metaprompt level, and the user interface level. This architecture\ncan be illustrated as follows:\nFigure 12.1: Illustration of different mitigation layers for LLM-powered applications\nOf course, it is not always possible to work at all levels. For example, in the\ncase of ChatGPT, we consume a pre-built application with a black-box\nmodel and a fixed UX, so we have little room for intervention only at the\nmetaprompt level. On the other hand, if we leverage open-source models via\nan API, we can act up to the model level to incorporate Responsible AI\nprinciples. Let’s now see a description of each layer of mitigation.\nModel level\nThe very first level is the model itself, which is impacted by the training\ndataset we train it with. In fact, if the training data is biased, the model will\ninherit a biased vision of the world.\n",
      "content_length": 840,
      "extraction_method": "Direct"
    },
    {
      "page_number": 434,
      "chapter": null,
      "content": "One example was covered in the paper Men Also Like Shopping: Reducing\nGender Bias Amplification using Corpus-level Constraints by Zhao et al.,\nwhere authors show an example of model bias in the field of computer\nvision, as shown in the following illustration:\nFigure 12.2: Example of sexism and bias of a vision model. Adapted from\nhttps://aclanthology.org/D17-1323.pdf, licensed under CC BY 4.0\n",
      "content_length": 396,
      "extraction_method": "Direct"
    },
    {
      "page_number": 435,
      "chapter": null,
      "content": "The model wrongly identifies a man cooking as a woman, since it associates\nthe activity of cooking with women with a greater probability, given the bias\nof the examples the model was trained on.\nAnother example traces back to the first experiments with ChatGPT, in\nDecember 2022, when it exhibited some sexist and racist comments. A\nrecent tweet highlighted this example, asking ChatGPT to create a Python\nfunction assessing a person’s aptitude as a scientist based on their race and\ngender.\nFigure 12.3: Inner bias of ChatGPT back in December 2022. Source:\nhttps://twitter.com/spiantado/status/1599462375887114240\nAs you can see, the model created a function that linked the probability of\nbeing a good scientist to race and gender, which is something that the model\nshouldn’t have created in the first place.\nTo act at the model level, there are some areas that researchers and\ncompanies should look at:\nRedact and curate training data: The primary goal of language\nmodeling is to faithfully represent the language found in the training\ncorpus. As a result, it is crucial to edit and carefully select the training\n",
      "content_length": 1116,
      "extraction_method": "Direct"
    },
    {
      "page_number": 436,
      "chapter": null,
      "content": "data. For example, in the scenario of the vision model previously\ndescribed, the training dataset should have been curated in such a way\nthat a man cooking did not represent a minority.\nNote\nThere are various toolkits available to developers to\nmake training datasets more “responsible.” A great\nopen-source example is the Python Responsible AI\nToolbox, a collection of tools and libraries designed to\nhelp developers incorporate Responsible AI practices\ninto their workflows. These tools aim to address various\naspects of AI development, including fairness,\ninterpretability, privacy, and security, to ensure that AI\nsystems are safe, trustworthy, and ethical. Specifically,\nthe toolkit includes resources to examine datasets for\npotential biases and ensure that models are fair and\ninclusive, providing metrics to assess group fairness and\ntools to mitigate identified biases; other tools\nspecifically focus on analyzing the balance of the\ndataset, providing metrics and techniques to address\nimbalances that could lead to biased model\nperformance.\nFine-tune language models: Adjust weightings to prevent bias and\nimplement checks to filter harmful language. There are many open-\nsource datasets with this goal, and you can also find a list of aligned\nfine-tuning datasets at the following GitHub repository:\n",
      "content_length": 1311,
      "extraction_method": "Direct"
    },
    {
      "page_number": 437,
      "chapter": null,
      "content": "https://github.com/Zjh-819/LLMDataHub#general-\nopen-access-datasets-for-alignment-.\nUse reinforcement learning with human feedback (RLHF): As\ncovered in Chapter 1, RLHF is an additional layer of LLMs’ training\nthat consists of adjusting a model’s weights according to human\nfeedback. This technique, in addition to making the model more\n“human-like,” is also pivotal in making it less biased, since any harmful\nor biased content will be penalized by the human feedback.\nOpenAI employs this strategy to avoid language models generating\nharmful or toxic content, ensuring that the models are geared toward\nbeing helpful, truthful, and benign. This is part of the whole training\nprocess of OpenAI’s models before they are released to the public\n(specifically, ChatGPT went through this development stage before\nbeing accessible).\nMaking LLMs align with human principles and preventing them from being\nharmful or discriminatory is a top priority among companies and research\ninstitutes that are in the process of developing LLMs. It is also the first layer\nof mitigation toward potential harms and risks, yet it might be not enough to\nfully mitigate the risk of adopting LLM-powered applications. In the next\nsection, we are going to cover the second layer of mitigation, which is the\none related to the platform adopted to host and deploy your LLMs.\nMetaprompt level\nIn Chapter 4, we learned how the prompt and, more specifically, the\nmetaprompt or system message associated with our LLM is a key\ncomponent to make our LLM-powered application successful, to the point\n",
      "content_length": 1565,
      "extraction_method": "Direct"
    },
    {
      "page_number": 438,
      "chapter": null,
      "content": "that a new whole discipline has arisen in the last few months: prompt\nengineering.\nSince the metaprompt can be used to instruct a model to behave as we wish,\nit is also a powerful tool to mitigate any harmful output it might generate.\nThe following are some guidelines on how to leverage prompt engineering\ntechniques in that sense:\nClear guidelines: Providing clear instructions and guidelines to the AI\nmodel about what it can and cannot do. This includes setting boundaries\non the type of content it can generate, ensuring it respects user privacy,\nand ensuring it does not engage in harmful or inappropriate behavior.\nTransparency: Being transparent about how the AI model works, its\nlimitations, and the measures in place to ensure responsible use. This\nhelps build trust with users and allows them to make informed\ndecisions about using AI.\nEnsure grounding: Implementing grounding strategies on top of the\nprovided data can ensure the model does not hallucinate or provide\nharmful information.\nNote that, due to its centrality in these new application architectures, the\nprompt is also a potential subject of prompt injection; henceforth, it should\nalso include some defensive techniques to prevent this attack.\nDefinition\nPrompt injection stands as a form of attack on LLMs,\nwherein an AI employing a specific metaprompt for a task is\ndeceived by adversarial user input, leading it to execute a\ntask diverging from its original purpose.\n",
      "content_length": 1445,
      "extraction_method": "Direct"
    },
    {
      "page_number": 439,
      "chapter": null,
      "content": "Prompt injection can be of different types:\nPrompt leakage (or direct prompt injection): When there is a malicious\nactivity that accesses the meta prompt of an LLM and changes it. For\nexample, from the defined metaprompt “You are an AI assistant that\ntranslates everything to French,” an attacker could leak the prompt and\nchange it to “You are an AI assistant that translates everything to\nGerman.”\nGoal hijacking (or indirect prompt injection): When the malicious\nactivity finds target prompts to feed the model with that are capable of\nbypassing the metaprompt instructions. In this context, there are plenty\nof prompts that have been tested as capable of jailbracking the\nmetaprompt instructions. An example of one of these prompts, which\nemerged in the first few months after ChatGPT’s launch, has been\ncoined as Do Anything Now (DAN) and is meant to bypass the content\nsafety restrictions embedded within ChatGPT.\nThe following lines are the start of one of the versions of this prompt\n(you can find a whole repository about DAN prompts at\nhttps://github.com/0xk1h0/ChatGPT_DAN#chatgpt-\ndan-and-other-jailbreaks):\nHello ChatGPT. You are about to immerse yourself into the \nThere are some defensive techniques you can use to prevent prompt\ninjections. One of the most remarkable of these techniques is called\nAdversarial Prompt Detector. It consists of enforcing the desired behavior\nthrough the instruction given to the model. While this doesn’t necessarily\n",
      "content_length": 1464,
      "extraction_method": "Direct"
    },
    {
      "page_number": 440,
      "chapter": null,
      "content": "provide a comprehensive solution, it underscores the effectiveness of a well-\nformulated prompt.\nThe third and final mitigation layer is at the user interface level, and we are\ngoing to cover it in the next section.\nUser interface level\nThe user interface represents the last mile for an LLM-powered application\nto mitigate the potential associated risks. In fact, the way the user can\nactually interact with the LLM in the backend is a powerful tool to control\nthe incoming and outgoing tokens.\nFor example, in Chapter 9, while examining some code-related scenarios,\nwe saw how the StarCoder model is used in GitHub as a completion copilot\nfor the user. In this case, the user has a closed-ended experience, in the sense\nthat they cannot ask direct questions to the model; rather, it receives\nsuggestions based on the code it writes.\nAnother example is in Chapter 7, where we developed a movie\nrecommendation application with a UX that encourages the user to insert\nsome hardcoded parameters, rather than asking an open-ended question.\nGenerally speaking, there are some principles that you might want to take\ninto account while designing the UX for your LLM-powered application:\nDisclose the LLM’s role in the interaction: This can help make\npeople aware that they are interacting with an AI system that might also\nbe inaccurate.\nCite references and sources: Let the model disclose to the user the\nretrieved documentation that has been used as the context to respond.\nThis holds true if there is a vector search within a custom VectorDB, as\n",
      "content_length": 1543,
      "extraction_method": "Direct"
    },
    {
      "page_number": 441,
      "chapter": null,
      "content": "well as when we provide the model with external tools, such as the\npossibility to navigate the web (as we saw with our GlobeBotter\nassistant in Chapter 6).\nShow the reasoning process: This helps the user to decide whether the\nratio behind the response is coherent and useful for its purpose. It is\nalso a way to be transparent and provide the user with all the necessary\ninformation about the output it is given. In Chapter 8, we covered a\nsimilar scenario while asking the LLM to show the reasoning as well as\nthe SQL query run against the provided database when given a user’s\nquery:\n",
      "content_length": 586,
      "extraction_method": "Direct"
    },
    {
      "page_number": 442,
      "chapter": null,
      "content": "Figure 12.4: Example of transparency with DBCopilot\n",
      "content_length": 52,
      "extraction_method": "Direct"
    },
    {
      "page_number": 443,
      "chapter": null,
      "content": "Show the tools used: When we extend an LLM’s capabilities with\nexternal tools, we want to make sure the model uses them properly.\nHenceforth, it is a best practice to inform the user about which tool the\nmodel uses and how. We saw an example of that in Chapter 10, while\nexamining the case of the agentic approach to building multimodal\napplications.\nPrepare pre-defined questions: Sometimes, LLMs don’t know the\nanswer – or even worse, hallucinate – simply because users don’t know\nhow to properly ask a question. To address this risk, a best practice\n(especially in conversational applications) is that of encouraging the\nusers with pre-defined questions to start with, as well as follow-up\nquestions given a model’s answer. This can reduce the risk of poorly\nwritten questions as well as give a better UX to the user. An example of\nthis technique can be found in Bing Chat, a web copilot developed by\nMicrosoft and powered by GPT-4:\n",
      "content_length": 936,
      "extraction_method": "Direct"
    },
    {
      "page_number": 444,
      "chapter": null,
      "content": "Figure 12.5: UX of Bing Chat with pre-defined questions\nProvide system documentation: Making users aware of the type of AI\nsystem they interact with is a pivotal step if you want to embed\nResponsible AI within your application. To achieve that, you might\nwant to educate the users with comprehensive system documentation,\ncovering the system’s capabilities, constraints, and risks. For example,\ndevelop a “learn more” page for easy access to this information within\nthe system.\nPublish user guidelines and best practices: Facilitate effective system\nutilization for users and stakeholders by disseminating best practices,\nsuch as crafting prompts and reviewing generated content before\n",
      "content_length": 686,
      "extraction_method": "Direct"
    },
    {
      "page_number": 445,
      "chapter": null,
      "content": "acceptance. Integrate these guidelines and best practices directly into\nthe UX whenever feasible.\nIt is important to establish a systematic approach to assess the effectiveness\nof implemented mitigations in addressing potential harms, as well as\ndocument measurement results and regularly review them to iteratively\nenhance a system’s performance.\nOverall, there are different levels where you could intervene to mitigate risks\nassociated with LLMs. From the model level to UX, it is pivotal to\nincorporate these considerations and best practices while developing your\nLLM-powered application.\nHowever, it’s important to note that Responsible AI is not just about the\ntechnology itself but also its use and impact on society. Therefore, it’s\ncrucial to consider ethical aspects and societal implications when developing\nand deploying these systems.\nRegulations surrounding\nResponsible AI\nRegulation of AI is becoming increasingly systematic and stringent, with\nnumerous proposals on the table.\nIn the United States, the government, particularly under the Biden-Harris\nadministration, has proactively implemented measures to ensure responsible\nAI usage. This includes initiatives like the Blueprint for an AI Bill of Rights,\nan AI Risk Management Framework, and a National AI Research Resource\nroadmap. President Biden’s Executive Order emphasizes eliminating bias in\nfederal agencies’ use of new technologies, including AI. Collaborative\nefforts from agencies like the Federal Trade Commission and the Equal\n",
      "content_length": 1508,
      "extraction_method": "Direct"
    },
    {
      "page_number": 446,
      "chapter": null,
      "content": "Employment Opportunity Commission showcase a commitment to\nprotecting Americans from AI-related harm.\nIn Europe, the European Commission proposed the Artificial Intelligence\nAct (AI Act), which seeks to establish a comprehensive regulatory\nframework for AI that applies to the following stakeholders:\nProviders: Organizations or individuals who develop, deploy, or offer\nAI systems in the EU are subject to the AI Act. This includes both\nprivate and public entities.\nUsers: Users who utilize AI systems within the EU fall under the scope\nof the regulation. This includes businesses, government agencies, and\nindividuals.\nImporters: Entities that import AI systems into the EU market are also\nsubject to compliance with the AI Act.\nDistributors: Distributors who place AI systems on the EU market are\nresponsible for ensuring that these systems comply with the regulation.\nThird-country entities: Even entities located outside the EU that\nprovide AI services or products to EU residents are subject to certain\nprovisions of the AI Act.\nBy categorizing AI systems by risk, the AI Act outlines the development and\nuse of requirements to promote human-centric and trustworthy AI. The Act\naims to safeguard health, safety, fundamental rights, democracy, the rule of\nlaw, and the environment. It empowers citizens to file complaints,\nestablishes an EU AI Office for enforcement, and mandates member states to\nappoint national supervisory authorities for AI. The Act aligns with\nResponsible AI principles, emphasizing fairness, accountability,\ntransparency, and ethics. The idea is to ensure that:\n",
      "content_length": 1591,
      "extraction_method": "Direct"
    },
    {
      "page_number": 447,
      "chapter": null,
      "content": "Providers of generative AI systems must train, design, and develop their\nsystems with state-of-the-art safeguards against generating content that\nbreaches EU laws.\nProviders are required to document and provide a publicly available\ndetailed summary of their use of copyrighted training data.\nProviders must adhere to more stringent transparency obligations.\nIf a generative AI system has been used to create “deep fakes,” users\nwho created such content must disclose that it was generated or\nmanipulated by AI.\nThe AI Act represents a significant step toward ensuring that AI technologies\nare developed and used in a way that benefits society, while respecting\nfundamental human rights and values. In 2023, amid the rapid growth of\ngenerative AI technologies, significant strides were made regarding the AI\nAct:\nBy June 14, 2023, the European Parliament had endorsed its stance on\nthe AI Act, securing 499 votes in favor, 28 against, and 93 abstentions.\nNoteworthy amendments were introduced to the proposal for a\nregulation, titled the AI Act, with the aim of establishing unified\nregulations on AI and modifying certain European Union legislative\nacts.\nApproved in December 2023, the AI Act allows a grace period of 2 to 3\nyears for preparation before its activation.\nThese developments signify the ongoing progress of the AI Act toward its\nimplementation, positioning the EU as a potential trailblazer in introducing\noversight or regulation for generative AI, given the advanced negotiations\nwithin the European Commission.\n",
      "content_length": 1527,
      "extraction_method": "Direct"
    },
    {
      "page_number": 448,
      "chapter": null,
      "content": "Overall, governments around the world are scrambling to figure out how to\napproach the questions posed by AI. These advancements reflect a growing\nrecognition of the need for Responsible AI and the role of government in\nensuring it.\nSummary\nIn this chapter, we covered the “dark side” of generative AI technologies,\nexposing its associated risks and biases, such as hallucinations, harmful\ncontent, and discrimination. To reduce and overcome those risks, we\nintroduced the concept of Responsible AI, starting with a deep dive into the\ntechnical approach we can have while developing LLM-powered\napplications; we covered the different levels of risk mitigation – model,\nmetaprompt, and UX – and then moved on to the broader topic of\ninstitutional regulations. In this context, we examined the advancements that\nhave been carried out by governments in the last year, with a focus on the AI\nAct.\nResponsible AI is an evolving field of research, and it definitely has an\ninterdisciplinary flavor. There will probably be an acceleration at the\nregulation level to address it in the near future.\nIn the next and final chapter, we are going to cover all the emerging trends\nand innovations happening in the generative AI field with a glimpse of what\nwe could expect from the near future.\nReferences\nReducing Gender Bias Amplification using Corpus-level Constraints:\nhttps://browse.arxiv.org/pdf/1707.09457.pdf\n",
      "content_length": 1403,
      "extraction_method": "Direct"
    },
    {
      "page_number": 449,
      "chapter": null,
      "content": "ChatGPT racist and sexist outputs:\nhttps://twitter.com/spiantado/status/15994623758\n87114240\nGitHub repository for an aligned dataset:\nhttps://github.com/Zjh-819/LLMDataHub#general-\nopen-access-datasets-for-alignment-\nAI Act:\nhttps://www.europarl.europa.eu/RegData/etudes/BR\nIE/2021/698792/EPRS_BRI(2021)698792_EN.pdf\nPrompt hijacking:\nhttps://arxiv.org/pdf/2211.09527.pdf\nAI Act:\nhttps://www.europarl.europa.eu/news/en/headlines\n/society/20230601STO93804/eu-ai-act-first-\nregulation-on-artificial-intelligence\nBlueprint for an AI Bill of Rights:\nhttps://www.whitehouse.gov/ostp/ai-bill-of-\nrights/\nJoin our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\n",
      "content_length": 738,
      "extraction_method": "Direct"
    },
    {
      "page_number": 450,
      "chapter": null,
      "content": "OceanofPDF.com\n",
      "content_length": 15,
      "extraction_method": "Direct"
    },
    {
      "page_number": 451,
      "chapter": null,
      "content": "13\nEmerging Trends and\nInnovations\nDear reader, if you have made it up to this point – congratulations! You\nmanaged to complete this journey into LLMs and how to implement modern\napplications with them. Starting from the fundamentals of what’s under the\nhood of an LLM, we covered many scenarios of LLM-powered applications,\nfrom conversational chatbots, to database copilots, to multimodal agents. We\nexperimented with different models, both proprietary and open-source, and\nwe also managed to fine-tune our own LLM. Last but not least, we covered\nthe key topic of Responsible AI and how to embed ethical considerations\nwithin our LLM-powered applications.\nIn this final chapter, we are going to explore the latest advancements and\nfuture trends in the field of generative AI. Note that, as a rapidly evolving\nfield, it is nearly impossible to keep up with up-to-date releases.\nNevertheless, the advancements covered in this chapter will give you an idea\nof what to expect in the near future.\nWe will cover the following topics:\nThe latest trends in language models and generative AI\nCompanies embracing generative AI\n",
      "content_length": 1119,
      "extraction_method": "Direct"
    },
    {
      "page_number": 452,
      "chapter": null,
      "content": "The latest trends in language\nmodels and generative AI\nAs we saw in the previous chapters, LLMs set the basis for extremely\npowerful applications. Starting with LLMs, over the last months we have\nwitnessed an explosive advancement in generative models, from\nmultimodality to newly born frameworks, to enable multi-agent applications.\nIn the next sections, we will see some examples of these new releases.\nGPT-4V(ision)\nGPT-4V(ision) is a large multimodal model (LMM) developed by OpenAI\nand officially released in September 2023. It enables users to instruct GPT-4\nto analyze image inputs provided by the user. This integration of image\nanalysis into LLMs represents a significant advancement in AI research and\ndevelopment. Model multimodality was achieved by using a technique\ncalled image tokenization, which converts images into sequences of tokens\nthat can be processed by the same model as text. This allows the model to\nhandle different types of data, such as text and images, and generate outputs\nthat are consistent and coherent across modalities.\nSince its initial trials in April 2023, GPT-4V has shown remarkable abilities\nin various domains. Moreover, many businesses have begun to integrate this\nmodel in their early testing stages. One of the successful examples is Be My\nEyes, an app that assists the population of more than 250 million people who\nhave visual impairments or blindness. The app links people who have low\nvision or blindness with helpers who can assist them with everyday\nactivities, such as recognizing a product or finding their way around an\nairport. Using the new visual input feature of GPT-4, Be My Eyes created a\n",
      "content_length": 1651,
      "extraction_method": "Direct"
    },
    {
      "page_number": 453,
      "chapter": null,
      "content": "Virtual Volunteer™ in its app that uses GPT-4. This Virtual Volunteer can\nproduce the same amount of context and comprehension as a human\nvolunteer.\nThe GPT-4 technology can do more than just identify and label what’s in a\npicture; it can also infer and examine the situation. For instance, it can look\nat the items in a fridge and recommend what you can cook with them. What\nsets GPT-4 apart from other language and machine learning models is its\ncapability to engage in dialogue and the higher level of analytical skill that\nthe technology provides. Simple image recognition applications only\nidentify what you see. They can’t converse to find out if the noodles are\nmade with the proper ingredients or if the thing on the floor is not just a ball\nbut also liable to trip you up—and tell you that.\nIn response to early experimentation on GPT-4V before it went public,\nOpenAI has implemented several mitigations to address risks and biases.\nThese mitigations are aimed at improving a model’s safety and reducing the\npotential harm caused by its output:\nRefusal system: OpenAI has added refusals for certain types of\nobviously harmful generations in GPT-4V. This system helps prevent a\nmodel from generating content that promotes hate groups or contains\nhate symbols.\nEvaluation and red teaming: OpenAI has performed assessments and\nconsulted with external experts to examine the strengths and\nweaknesses of GPT-4V. This process helps detect potential flaws and\nrisks in a model’s output. The assessments cover areas such as scientific\ncompetence, medical guidance, stereotyping, disinformation threats,\nhateful content, and visual vulnerabilities.\n",
      "content_length": 1649,
      "extraction_method": "Direct"
    },
    {
      "page_number": 454,
      "chapter": null,
      "content": "Scientific competence: Red teamers evaluated GPT-4V’s abilities and\nchallenges in scientific domains. While the model demonstrated the\nskill to comprehend complex information in images and verify claims\nin scientific papers, it also showed challenges, such as the occasional\nmixing of separate text elements and the possibility of factual mistakes.\nHateful content: GPT-4V declines to answer questions about hate\nsymbols and extremist content in some cases. However, the model’s\nbehavior may be variable, and it may not always decline to generate\ncompletions related to less-known hate groups or symbols. OpenAI\nrecognizes the need for further enhancements in addressing hateful\ncontent.\nUngrounded inferences: OpenAI has implemented mitigations to\naddress risks associated with ungrounded inferences. The model now\nrefuses requests for ungrounded inferences about people, reducing the\npotential for biased or inaccurate responses. OpenAI aims to refine\nthese mitigations to enable the model to answer questions about people\nin low-risk contexts in the future.\nDisinformation risks: GPT-4V’s ability to generate text content\ntailored to image input poses increased risks with disinformation.\nOpenAI acknowledges the need for proper risk assessment and context\nconsideration when using the model in relation to disinformation. The\ncombination of generative image models and GPT-4V’s text generation\ncapabilities may impact disinformation risks, but additional mitigations\nsuch as watermarking or provenance tools may be necessary.\nThese mitigations, along with the contribution from existing safety measures\nand ongoing research, aim to improve safety and reduce the biases in GPT-\n4V. OpenAI acknowledges the dynamic and challenging nature of addressing\n",
      "content_length": 1754,
      "extraction_method": "Direct"
    },
    {
      "page_number": 455,
      "chapter": null,
      "content": "these risks and remains committed to refining and improving a model’s\nperformance in future iterations.\nOverall, the GPT-4V has unveiled extraordinary capabilities and paves the\nway for multimodality within LLM-powered applications.\nDALL-E 3\nThe newest version of OpenAI’s image-generation tool, DALL-E 3, came\nout in October 2023. The most significant update from previous versions is\nits improved accuracy and faster speed when generating images from text. It\naims to render more detailed, expressive, and specific images that align more\nclosely with a user’s specifications. In fact, even with the same prompt,\nDALL-E 3 shows great improvements compared to its previous version:\nFigure 13.1: Images generated from the prompt “an expressive oil painting of a basketball player\ndunking, depicted as an explosion of a nebula” by DALLE-2 (left) and DALL-E 3 (right). Source:\nhttps://openai.com/dall-e-3\nDALL-E 3 has more safeguards and rules to avoid creating images that\ncontain adult, violent, or hateful content.\n",
      "content_length": 1015,
      "extraction_method": "Direct"
    },
    {
      "page_number": 456,
      "chapter": null,
      "content": "DALL-E 3 is now available to ChatGPT Plus and Enterprise customers\nvia the API and in OpenAI Playground. It’s also been integrated with\nMicrosoft’s Bing Chat.\nAutoGen\nIn October 2023, Microsoft released a new open-source project called\nAutoGen. It is a Python lightweight framework that allows multiple LLM-\npowered agents to cooperate with each other to solve users’ tasks. For an\noverview of what the cooperation frameworks look like, you can refer to\nhttps://github.com/microsoft/autogen/tree/main.\nEarlier in Part 2 of this book, we covered many scenarios of LangChain\nAgents leveraging external tools. In those scenarios, we had one agent\npowered by an LLM that dynamically decided which tool to use to solve a\nuser’s query. AutoGen works differently, in the sense that it lets different\nagents, each one acting with a specific role and expertise, cooperate to\naddress the user’s query. The main element of novelty here is that each agent\ncan actually generate output that serves as input to other agents, as well as\ngenerate and modify the plan to be executed. That is the reason why the\nframework has also been designed to keep a human or admin in the loop, to\nactually approve or discard actions and executions.\nAccording to the original paper AutoGen: Enabling Next-Gen LLM\nApplications via Multi-Agent Conversation by Wu et al., there are three main\nreasons why the multi-agent conversation exhibits great performance:\nFeedback incorporation: Since LLMs have the capacity to elaborate\nand leverage feedback, they can cooperate through conversations in\nnatural language with each other, and humans as well, to adjust the way\nthey solve a given problem.\n",
      "content_length": 1662,
      "extraction_method": "Direct"
    },
    {
      "page_number": 457,
      "chapter": null,
      "content": "Adaptability: Since LLMs are general-purpose models that can adapt\nto different tasks if properly configured, we can initialize different\nagents that leverage the various capabilities of LLMs in a modular and\ncomplementary way.\nSplitting complex tasks: LLMs work better when they split complex\ntasks into smaller subtasks (as covered in Chapter 4 about prompt\nengineering techniques). Henceforth, multi-agent conversations can\nenhance this partition, delegating each agent to a subtask, while keeping\nthe overall picture of the problem to solve.\nTo enable a multi-agent conversation, there are two main components to be\naware of:\nConversable agents are entities that can communicate with each other\nand have different capabilities, such as using LLMs, human input, or\ntools.\nConversation programming is a paradigm that allows developers to\ndefine the interaction behavior between agents using natural or\nprogramming languages.\nYou can see what these conversations look like at\nhttps://www.microsoft.com/en-\nus/research/publication/autogen-enabling-next-gen-\nllm-applications-via-multi-agent-conversation-\nframework/.\nThe AutoGen framework has already proven its great capability in\naddressing different use cases, among which are the following:\nCode generation and execution. AutoGen provides a class of agents\nthat can execute code as .py files in a given directory.\n",
      "content_length": 1368,
      "extraction_method": "Direct"
    },
    {
      "page_number": 458,
      "chapter": null,
      "content": "Multi-agent collaboration. This scenario fits whenever you want\nvaried expertise to reason upon a given task. For example, you might\nwant to set up a research group that, when given a user’s request, sets\nup a plan, evaluates it, receives a user’s input, executes it with different\nexpertise (aka different agents), and so on.\nTools integrations. AutoGen also offers some classes that facilitate the\nintegration of external tools, such as web search and retrieval-\naugmented generation (RAG) from a provided vector database.\nYou can find some examples of different applications of the AutoGen\nframework at\nhttps://microsoft.github.io/autogen/docs/Examples#a\nutomated-multi-agent-chat.\nOverall, AutoGen provides a useful and innovative toolkit that makes it\neasier to let agents cooperate with each other, as well as with a human in the\nloop. The project is open to contribution, and it will be very interesting to\nsee how it progresses and to what extent the multi-agent approach will\nbecome a best practice.\nSo far, we have been talking about LLMs that are, by definition, “large” (for\nexample, the GPT-3 has 175 billion parameters). However, sometimes,\nsmaller models can be useful as well.\nSmall language models\nSmaller models with fewer parameters can demonstrate extraordinary\ncapabilities in specific tasks. This class of models has paved the way for\nwhat are now called small language models (SLMs). SLMs have fewer\nparameters than LLMs, which means they require less computational power\nand can be deployed on mobile devices or resource-constrained\n",
      "content_length": 1557,
      "extraction_method": "Direct"
    },
    {
      "page_number": 459,
      "chapter": null,
      "content": "environments. SLMs can also be fine-tuned to excel in specific domains or\ntasks, such as finance, healthcare, or customer service, by using relevant\ntraining data.\nSLMs are promising because they offer several advantages over LLMs, such\nas:\nThey are more efficient and cost-effective, as they require less\ncomputational resources and energy to train and run.\nThey are more accessible and portable, as they can be deployed on\nmobile devices or edge computing platforms, enabling a wider range of\napplications and users.\nThey are more adaptable and specialized, as they can be fine-tuned to\nspecific domains or tasks using relevant data, improving their accuracy\nand relevance.\nThey are more interpretable and trustworthy, as they have fewer\nparameters and simpler architectures, making them easier to understand\nand debug.\nPhi-2 is an example of a promising SLM that demonstrates outstanding\nreasoning and language understanding capabilities, showcasing state-of-the-\nart performance among base language models with less than 13 billion\nparameters. It is a 2.7 billion-parameter language model developed by\nMicrosoft Research, trained on high-quality data sources, such as textbooks\nand synthetic texts, and uses a novel architecture that improves its efficiency\nand robustness. Phi-2 is available in the Azure AI Studio model catalog and\ncan be used for various research and development purposes, such as\nexploring safety challenges, interpretability, or fine-tuning experiments.\nIn the next section, we are going to see which companies are actively\nleveraging generative AI for their processes, services, and products.\n",
      "content_length": 1620,
      "extraction_method": "Direct"
    },
    {
      "page_number": 460,
      "chapter": null,
      "content": "Companies embracing\ngenerative AI\nSince the launch of ChatGPT in November 2022, up to the newest large\nfoundation models on the market (both proprietary and open-source), many\ncompanies in different industries started embracing generative AI within\ntheir processes and products. Let’s discuss some of the most popular ones.\nCoca-Cola\nCoca-Cola partnered with Bain & Company and OpenAI to leverage DALL-\nE, a generative AI model. This partnership was announced on February 21,\n2023.\nOpenAI’s ChatGPT and DALL-E platforms will help Coca-Cola create\ncustomized ad content, pictures, and messages. Coca-Cola’s “Create Real\nMagic” initiative is the result of the collaboration between OpenAI and Bain\n& Company (https://www.coca-colacompany.com/media-\ncenter/coca-cola-invites-digital-artists-to-create-\nreal-magic-using-new-ai-platform). The platform is a unique\ninnovation that merges the abilities of GPT-4, which generates text that\nsounds like humans making search engine queries, and DALL-E, which\ncreates images from text. This enables Coca-Cola to rapidly produce text,\nimages, and other content. This strategic alliance is expected to deliver real\nvalue to large enterprise customers, enabling massive business\ntransformation within the Fortune 500. It also sets a standard for their clients\nto follow.\nNotion\n",
      "content_length": 1314,
      "extraction_method": "Direct"
    },
    {
      "page_number": 461,
      "chapter": null,
      "content": "Notion is a versatile platform that combines note-taking, project\nmanagement, and database functionalities in a single space. It allows users to\ncapture thoughts, manage projects, and even run an entire company in a way\nthat suits their needs. Notion is ideal for individuals, freelancers, startups,\nand teams looking for a straightforward application to collaborate on\nmultiple projects.\nNotion has introduced a new feature called Notion AI that uses generative\nAI. This feature is essentially a prediction engine that guesses what words\nwill work best based on a prompt or text you’ve written. It can perform tasks\nsuch as:\nSummarizing lengthy text (e.g., meeting notes and transcripts)\nGenerating entire blog post outlines and emails\nCreating action items from meeting notes\nEditing your writing to fix grammar and spelling, change the tone, etc.\nAssisting with research and problem-solving\nThe following screenshot shows some of the Notion features powered by\ngenerative AI:\nFigure 13.2: Some features of Notion AI. Source: https://www.notion.so/product/ai\n",
      "content_length": 1061,
      "extraction_method": "Direct"
    },
    {
      "page_number": 462,
      "chapter": null,
      "content": "Notion AI is powered by OpenAI’s GPT models and integrated into the core\nNotion apps (desktop, browser, and mobile), allowing users to write prompts\nthat will generate text, as well as apply AI to text they’ve already written or\ncaptured. This makes Notion AI a powerful digital assistant that enhances\nthe functionality of the Notion workspace.\nMalbek\nMalbek is a modern, innovative contract lifecycle management (CLM)\nplatform with a proprietary AI core. It meets the increasing contractual needs\nof your entire organization, including Sales, Finance, Procurement, and other\nessential business units.\nMalbek uses generative AI to offer a feature powered by LLMs and featuring\nChatGPT. It can do tasks such as:\nUnderstanding the language in contracts\nMaking changes\nEasily accepting or rejecting redlines\nMaking custom requests – all in natural language\nThis remarkable new feature enables users to speed up negotiation time and\nshorten review cycles, improving the functionality of the Malbek workspace.\nMicrosoft\nSince its partnership with OpenAI, Microsoft has started infusing AI\npowered by GPT-series in all its products, introducing and coining the\nconcept of Copilot. We’ve already introduced the concept of a Copilot\nsystem in Chapter 2, as a new category of software that serves as an expert\nhelper to users trying to accomplish complex tasks, working side by side\n",
      "content_length": 1375,
      "extraction_method": "Direct"
    },
    {
      "page_number": 463,
      "chapter": null,
      "content": "with users and supporting them in various activities, from information\nretrieval to blog writing and posting, and from idea brainstorming to code\nreview and generation.\nIn 2023, Microsoft released several copilots within its products, such as the\nEdge Copiot (former Bing Chat). The following illustration shows the user\ninterface of Bing Chat:\nFigure 13.3: Microsoft Bing Chat\nBing Chat is also a perfect example of a multimodal conversational agent\npowered by both GPT-4V and DALL-E 3. Plus, you can interact with it via\naudio messaging. An example of these multimodal capabilities is shown in\nthe following screenshot:\n",
      "content_length": 622,
      "extraction_method": "Direct"
    },
    {
      "page_number": 464,
      "chapter": null,
      "content": "Figure 13.4: Leveraging the multimodal capabilities of Bing Chat\nMicrosoft’s copilots will empower professionals and organizations to\ndrastically improve their productivity and creativity, paving the way for a\n",
      "content_length": 210,
      "extraction_method": "Direct"
    },
    {
      "page_number": 465,
      "chapter": null,
      "content": "new way of working.\nOverall, companies in all industries are seizing the potential of generative\nAI, with the awareness that the competitive landscape will soon raise the\nbenchmark for copilots and AI-powered products.\nSummary\nIn this final chapter of this book, we took a glimpse at the latest\nadvancements in the field of generative AI. We covered new model releases\nsuch as OpenAI’s GPT-4V, as well as new frameworks to build LLM-\npowered applications such as AutoGen. Furthermore, we provided an\noverview of some companies that are actively powering their business with\nLLMs, such as Notion and Microsoft.\nGenerative AI has shown to be the most promising and exciting field of AI,\nand it has the potential to unleash human creativity, enhance productivity,\nand solve complex problems. However, as we learned in the previous\nchapter, it also poses some ethical and social challenges, such as ensuring\nthe quality, safety, and fairness of the generated content, as well as\nrespecting the intellectual property and privacy rights of the original\ncreators. Therefore, as we explore the new horizons of generative AI, we\nshould also be mindful of the implications of our actions in the context of the\ncurrent times. We should strive to use generative AI for good purposes and\nfoster a culture of collaboration, innovation, and responsibility among\nresearchers, developers, and users. Nevertheless, generative AI is an\nevolving field, and within its landscape, one month is worth several years of\ntechnological progress. What is sure is that it represents a paradigm shift,\nand both companies and individuals are continuously adapting to it.\n",
      "content_length": 1640,
      "extraction_method": "Direct"
    },
    {
      "page_number": 466,
      "chapter": null,
      "content": "References\nGPT-4V(ision) System Card: GPTV_System_Card.pdf\n(openai.com)\nAutoGen paper: Qingyun Wu et al., 2023, AutoGen: Enabling Next-Gen\nLLM Applications via Multi-Agent Conversation:\nhttps://arxiv.org/pdf/2308.08155.pdf\nAutoGen GitHub:\nhttps://github.com/microsoft/autogen/blob/main/n\notebook/agentchat_web_info.ipynb\nDALL-E 3: James Betker, Improving Image Generation with Better\nCaptions: https://cdn.openai.com/papers/dall-e-\n3.pdf\nNotion AI: https://www.notion.so/product/ai\nCoca-Cola and Bain partnership: https://www.coca-\ncolacompany.com/media-center/coca-cola-invites-\ndigital-artists-to-create-real-magic-using-new-\nai-platform\nMalbek and ChatGPT: https://www.malbek.io/news/chat-\ngpt-malbek-unveils-generative-ai-functionality\nMicrosoft Copilot: https://www.microsoft.com/en-\nus/microsoft-365/blog/2023/09/21/announcing-\nmicrosoft-365-copilot-general-availability-and-\nmicrosoft-365-chat/\n",
      "content_length": 902,
      "extraction_method": "Direct"
    },
    {
      "page_number": 467,
      "chapter": null,
      "content": "Join our community on\nDiscord\nJoin our community’s Discord space for discussions with the author and\nother readers:\nhttps://packt.link/llm\nOceanofPDF.com\n",
      "content_length": 154,
      "extraction_method": "Direct"
    },
    {
      "page_number": 468,
      "chapter": null,
      "content": "packt.com\nSubscribe to our online digital library for full access to over 7,000 books\nand videos, as well as industry leading tools to help you plan your personal\ndevelopment and advance your career. For more information, please visit\nour website.\nWhy subscribe?\nSpend less time learning and more time coding with practical eBooks\nand Videos from over 4,000 industry professionals\nImprove your learning with Skill Plans built especially for you\nGet a free eBook or video every month\nFully searchable for easy access to vital information\nCopy and paste, print, and bookmark content\nAt www.packt.com, you can also read a collection of free technical\narticles, sign up for a range of free newsletters, and receive exclusive\ndiscounts and offers on Packt books and eBooks.\n",
      "content_length": 769,
      "extraction_method": "Direct"
    },
    {
      "page_number": 469,
      "chapter": null,
      "content": "Other Books You May\nEnjoy\nIf you enjoyed this book, you may be interested in these other books by\nPackt:\nGenerative AI with LangChain\nBen Auffarth\nISBN: 9781835083468\nUnderstand LLMs, their strengths and limitations\nGrasp generative AI fundamentals and industry trends\nCreate LLM apps with LangChain like question-answering systems\nand chatbots\nUnderstand transformer models and attention mechanisms\n",
      "content_length": 400,
      "extraction_method": "Direct"
    },
    {
      "page_number": 470,
      "chapter": null,
      "content": "Automate data analysis and visualization using pandas and Python\nGrasp prompt engineering to improve performance\nFine-tune LLMs and get to know the tools to unleash their power\nDeploy LLMs as a service with LangChain and apply evaluation\nstrategies\nPrivately interact with documents using open-source LLMs to prevent\ndata leaks\n",
      "content_length": 328,
      "extraction_method": "Direct"
    },
    {
      "page_number": 471,
      "chapter": null,
      "content": "Transformers for Natural Language Processing and Computer Vision\nDenis Rothman\nISBN: 9781805128724\nLearn how to pretrain and fine-tune LLMs\nLearn how to work with multiple platforms, such as Hugging Face,\nOpenAI, and Google Vertex AI\nLearn about different tokenizers and the best practices for\npreprocessing language data\nImplement Retrieval Augmented Generation and rules bases to\nmitigate hallucinations\nVisualize transformer model activity for deeper insights using BertViz,\nLIME, and SHAP\nCreate and implement cross-platform chained models, such as\nHuggingGPT\nGo in-depth into vision transformers with CLIP, DALL-E 2, DALL-E\n3, and GPT-4V\n",
      "content_length": 643,
      "extraction_method": "Direct"
    },
    {
      "page_number": 472,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 473,
      "chapter": null,
      "content": "Packt is searching for\nauthors like you\nIf you’re interested in becoming an author for Packt, please visit\nauthors.packtpub.com and apply today. We have worked with\nthousands of developers and tech professionals, just like you, to help them\nshare their insight with the global tech community. You can make a general\napplication, apply for a specific hot topic that we are recruiting an author\nfor, or submit your own idea.\nShare your thoughts\nNow you’ve finished Building LLM Powered Application, we’d love to hear\nyour thoughts! If you purchased the book from Amazon, please click\nhere to go straight to the Amazon review page for this\nbook and share your feedback or leave a review on the site that you\npurchased it from.\nYour review is important to us and the tech community and will help us\nmake sure we’re delivering excellent quality content.\nOceanofPDF.com\n",
      "content_length": 864,
      "extraction_method": "Direct"
    },
    {
      "page_number": 474,
      "chapter": null,
      "content": "Index\nSymbols\n.env file\nsecrets, storing 114, 115\nA\nagent 109, 110\ninitializing 245\nagentic approach 154\nversus hard-coded approach 256\nversus out-of-the-box approach 256\nagent types\nconversational 111\nOpenAI Functions 111\nplan-and-execute agents 111\nReAct document store 111\nself ask with search 111\nstructured input ReAct 111\nAI2 Reasoning Challenge (ARC) 19, 51\nAI orchestrators\ncomponents 31-34\n",
      "content_length": 399,
      "extraction_method": "Direct"
    },
    {
      "page_number": 475,
      "chapter": null,
      "content": "framework, selecting 38, 39\nHaystack 35, 36\nLangChain 34, 35\nSemantic Kernel 36-38\nAmazon Web Services (AWS) RDS 173\narea under the ROC curve (AUC) 273\nartificial general intelligence (AGI) 226\nartificial intelligence (AI) 226\nartificial neural networks (ANNs) 4\nartificial neuron 4\nASCII characters 203\nassistant models 52\nversus base models 52\nattention mechanisms 11\nautoencoders 145\nAutoGen 296, 297\nAutomated Multi Agent Chat\nreference link 297\nautoregressive decoding 14\nautoregressive model 52\nAutoTokenizer 268\nAutoTrain 265\nreference link 265\nAzure AI Services\n",
      "content_length": 570,
      "extraction_method": "Direct"
    },
    {
      "page_number": 476,
      "chapter": null,
      "content": "out-of-the-box toolkit, using 228\nAzureCognitiveServicesToolkit\nmultiple tools, leveraging 234-238\nsingle tool, leveraging 231-234\ntoolkit, setting up 229-231\nworking with 228\nAzureCogsFormRecognizerTool 228\nAzureCogsImageAnalysisTool 228\nAzureCogsSpeech2TextTool 228\nAzureCogsText2SpeechTool 228\nAzureML Managed Feature Store 164\nB\nbackpropagation 6\nBaseExampleSelector 96\nbase models 17, 52\nversus assistant models 52\nversus customized models 20\nBayes' theorem 7\nlikelihood 8\nmost likely word, selecting 8\nposterior probability 8\nprior probability 8\nBERT model, components\n",
      "content_length": 575,
      "extraction_method": "Direct"
    },
    {
      "page_number": 477,
      "chapter": null,
      "content": "encoder 270\noutput layer 270\nBidirectional Encoder Representations from Transformers\n(BERT) model 15, 264, 268\nbusiness intelligence (BI) 171\nbyte pair encoding (BPE) 150, 264\nC\nchain of thought (CoT) 83-85\ncharacter-based approach 268\nChinook database\nfeatures 173\noverview 173\nclassifier layer 263\nClaude 2 49-51\nCoca-Cola 298\ncode\nlarge language model (LLM), selecting for 196, 197\nunderstanding 197, 198\nCodeBox 218\ncode generation 70, 197, 198\nCode Interpreter\nleveraging 217-223\nCodeLlama 201-205\n",
      "content_length": 503,
      "extraction_method": "Direct"
    },
    {
      "page_number": 478,
      "chapter": null,
      "content": "cold-start 148\nQA recommendation chatbot, building 151-159\ncollaborative filtering 140\ncompletion model 96\nconstitutional AI (CAI) 49\nprinciples, using in stages of training process 49\ncontent-based filtering 140\ncontent-based system\nbuilding 159-163\ncontract lifecycle management (CLM) 299\nconversable agents 296\nconversational applications 120\nbenefits 120\nexternal tools, adding 129-131\nmemory, adding 122-125\nnon-parametric knowledge, adding 125-129\nplain vanilla bot, creating 120-122\nconversation programming 296\nconvolutional neural networks (CNNs) 145\nCoPenny 238\ncopilot system 26, 31, 171\nfeatures 27-30\nversus retrieval-augmented generation (RAG) 29\ncosine similarity 100\n",
      "content_length": 683,
      "extraction_method": "Direct"
    },
    {
      "page_number": 479,
      "chapter": null,
      "content": "curse of dimensionality 142\ncustomer relationship management (CRM) 171\ncustomized models\nversus base models 20\nCustomYTTranscribeTool 244\nD\nDALL-E 56, 246-248\nDALL-E 3 295\ndata\ntokenizing 267-270\ndata connections 96-102\ndata manipulation language (DML) 182\nDBCopilot 171\nimplementing, with LangChain 179\ntools, adding to 187-190\ndecoder 13\ndeep learning 2\ndeep neural networks (DNNs) 146\ndigital process automation (DPA) 238\nDo Anything Now (DAN) 285\ndocument loaders 97\ndocument transformers 97, 98\nDS-1000 197\n",
      "content_length": 512,
      "extraction_method": "Direct"
    },
    {
      "page_number": 480,
      "chapter": null,
      "content": "E\nembedding 5\nreference link 99\nencoder 12\nend-to-end application\nbuilding, for invoice analysis 238-243\nepoch 275\nevaluation metrics\nusing 272-275\nexample selector 96\nreference link 96\neXtensible Markup Language (XML) 171\nF\nFacebook AI Similarity Search (FAISS) 101\nFalcon LLM 54, 198\nusing 198-201\nFeast 163\nfeature extraction 263\nFeatureform 163\nfew-shot approach 79-83\nFibonacci sequence 207\nFileManagementToolkit 187\nfilter approach 154\n",
      "content_length": 442,
      "extraction_method": "Direct"
    },
    {
      "page_number": 481,
      "chapter": null,
      "content": "fine-tuning 262-264\ndataset, obtaining 266, 267\ndata, tokenizing 267-270\nevaluation metrics, using 272-275\nmodel 270, 271\nrequiring, scenarios 265\nsaving 275-278\nsteps 264\ntraining 275-278\nworking with 266\nforeign key 172\nfoundation model 2-4\nfront-end\ndeveloping, with Streamlit 131-136, 190-193, 256-259\nfront-end for Movieharbor\ndeveloping, with Streamlit 164-166\nfull-model fine-tuning 147\nfunction composition 36, 37\nG\nGemini 1.5 47-49\nGeneral Language Understanding Evaluation (GLUE) 18, 19,\n271\ngenerative AI\n",
      "content_length": 516,
      "extraction_method": "Direct"
    },
    {
      "page_number": 482,
      "chapter": null,
      "content": "versus natural language understanding (NLU) 3\nGenerative Pre-trained Transformer 3 (GPT-3) 15\ngenerative pretrained transformer (GPT) models 42\nGitHub Copilot 197\ngoal hijacking 285\nGoogle Cloud SQL 173\nGPT-4 42-47\nGPT-4V(ision) 294, 295\nGPTuber 243\ngraphics processing units (GPUs) 16\ngrounding 28\ngrouped-query attention (GQA) 55\nH\nhard-coded approach 228\nversus agentic, custom approach 256\nversus out-of-the-box approach 256\nwith sequential chain 252-255\nHaystack 35\nbenefits 36\ncomponents 36\nHeating, Ventilation and Air Conditioning (HVAC) setpoints 214\nHellaSwag 19, 51\nHF Code Autocomplete 209\n",
      "content_length": 602,
      "extraction_method": "Direct"
    },
    {
      "page_number": 483,
      "chapter": null,
      "content": "Hugging Face Hub 34\nlarge language model (LLM), working via 112\nHugging Face user access token\ncreating 112-114\nhybrid filtering 140\nI\nIBM Db2 173\nimage tokenization 294\nIMDB dataset\nreference link 266\nIMDB Movie Ratings Sentiment Analysis\nreference link 81\nInference Endpoints\nreference link 198\ninput IDs 268\ninvoice analysis\nend-to-end application, building for 238-243\nitem-based collaborative filtering 140\nitem-based KNN 141, 142\nJ\nJavaScript Object Notation (JSON) 171\nJoin Hugging Face\n",
      "content_length": 494,
      "extraction_method": "Direct"
    },
    {
      "page_number": 484,
      "chapter": null,
      "content": "reference link 112\nK\nK-nearest neighbors (KNN) 141\npitfalls 142\nknowledge-based filtering 140\nknowledge graph 103\nkwargs 154\nL\nlabels 266\nLanceDB 151\nLangChain 34, 92, 93, 210\nagents, SQL Agent 179-183\nbenefits 35\ncore modules 34, 35\nDBCopilot, implementing with 179\npackages, installing with 92\nreference link 92\nused, for building multimodal agent 227\nworking with 93\nlangchain-cli 92\nLangChain, components 93\nagents 109-111\n",
      "content_length": 427,
      "extraction_method": "Direct"
    },
    {
      "page_number": 485,
      "chapter": null,
      "content": "chains 105-108\ndata connections 96-102\nmemory 102-104\nmodels 94, 95\nprompts 94-96\nLangChain Expression Language (LCEL) 93\nfeatures 93\nLangChain's integration with features\nreference link 164\nLangServe 92\nLangSmith 92\nlanguage-specific foundation models 56-59\nlarge foundation models (LFMs) 2, 25, 56, 226\nlarge language model (LLM) 65\nselecting, for code 195-197\nworking with, via Hugging Face Hub 112\nLarge Language Model Meta AI 2 (LLaMA-2) 52, 53\ndeveloping, with fine-tuning process 53\nLarge Language Model Prompt Engineering for Complex\nSummarization\nreference link 75\nlarge language model (LLM) 1, 2, 8, 10, 41, 147, 169, 261, 279\nassistant models versus base models 52\nevaluating 15\n",
      "content_length": 690,
      "extraction_method": "Direct"
    },
    {
      "page_number": 486,
      "chapter": null,
      "content": "in market 41\nmodel evaluation 18-20\nopen-source model 51, 52\nproprietary models 42\ntraining 15-18\ntraining, steps 17, 18\nlarge multimodal model (LMM) 294\ncase study 62\nselecting, considerations 60-62\nselecting, framework 60\nLarge Objects (LOBs) 175\nlikelihood 7, 8\nLLMChain 105\nLLM, for recommendation systems 146\nfine-tuning 147\npre-training 146\nprompting 147\nLLM-powered agents\nbuilding, to act as algorithms 210-216\nsolving, optimization issues 216, 217\nLLM-powered recommendation system\ndata preprocessing 148-151\nimplementing 148\nLLMs, incorporating within applications\n",
      "content_length": 575,
      "extraction_method": "Direct"
    },
    {
      "page_number": 487,
      "chapter": null,
      "content": "conceptual aspect 26\ntechnical aspect 26\nlong short-term memory (LSTM) 11, 145\nM\nmachine learning (ML) 141\nmachine translation 69\nMalbek 299\nmasked language modeling (MLM) 271\nMassive Multitask Language Understanding (MMLU) 19, 44\nmatrix factorization 142-144\nalgorithms 143\npitfalls 144\nmemory types\nconversation buffer memory 103\nconversation buffer window memory 103\nconversation knowledge graph memory 103\nconversation summary buffer memory 104\nconversation summary memory 104\nconversation token buffer memory 104\nentity memory 103\nvector store-backed memory 104\nmetaprompt 21\nMicrosoft 300-302\n",
      "content_length": 599,
      "extraction_method": "Direct"
    },
    {
      "page_number": 488,
      "chapter": null,
      "content": "Midjourney 56\nMistral model 54, 55\nmixture-of-expert (MoE) 47\nmodel\nfine-tuning 270, 271\nmodel customization\nfew-shot learning 21\nfine-tuning 21, 22\nnon-parametric knowledge, extending 20\nMongoDB 170\nMostly Basic Programming Problems (MBPP) 196\nMovieHarbor 148\nMovie Recommender Dataset, on Kaggle\nreference link 148\nmulti-agent conversation\ncomponents 296\nreasons, for exhibiting performance 296\nMulti-Genre Natural Language Inference (MultiNLI) 271\nmultimodal agent\nbuilding, with LangChain 227\nmultimodality 226, 227\nMultiPL-E 197\nmulti-service resource, creating for Azure AI services\nreference link 229\n",
      "content_length": 608,
      "extraction_method": "Direct"
    },
    {
      "page_number": 489,
      "chapter": null,
      "content": "N\nnamed entity recognition (NER) 265\nnative functions 38\nnatural language processing (NLP) 3, 269\nnatural language understanding (NLU) 2, 51\nversus generative AI 3\nneural network 145, 146\nautoencoders 145\nchallenges 146\ncontent-based recommendation 145\nfeatures 146\nhidden layers 6\ninput layer 6\noutput layer 6\nsequential models 145\nused, for collaborative filtering 145\nvariational autoencoders (VAEs) 145\nnext sentence prediction (NSP) 271\nnode/unit 4\nnon-ASCII characters 203\nnon-negative matrix factorization (NMF) 143\nnon-parametric knowledge 20\nNotion 298, 299\nperforming, task 298\n",
      "content_length": 588,
      "extraction_method": "Direct"
    },
    {
      "page_number": 490,
      "chapter": null,
      "content": "Notion AI 298\nO\nobject-relational mapper (ORM) 175\nOpenAI 42\nOpenAI API key\nreference link 94\nOpenAIFunctionsAgent 217\nopen-source LLMs\nusing 116, 117\nopen-source model\nadvantage 51, 52\noptical character recognition (OCR) 229\noptimization algorithm 17\nOracle Database 173\nout-of-the-box approach\nversus agentic, custom approach 256\nversus hard-coded approach 256\nout-of-the-box toolkit\nusing, for Azure AI Services 228\noutput parser 105\nP\npadding 268\n",
      "content_length": 451,
      "extraction_method": "Direct"
    },
    {
      "page_number": 491,
      "chapter": null,
      "content": "parameter-efficient fine-tuning 147\nparameters 4\nparametric knowledge 20\nPhi-2 298\nplug-in 20\npoem generation 69\nposterior probability 7, 8\npost-padding 269\npost-truncation 269\npre-padding 269\nPretrain, Personalized Prompt & Predict Paradigm (P5) 147\npre-truncation 269\nprimary key 172\nprincipal component analysis (PCA) 143\nprior probability 8\nproduct attribute data 141\nprompt engineering 30, 31, 65, 183-186\nprompt engineering, principles 66\nclear instructions 66-69\ncomplex tasks, splitting into subtasks 69-72\ndelimiters, using 77-79\ninstructions, repeating 75-77\njustification, requesting 72, 73\nmultiple outputs, generating to select best one 73-75\n",
      "content_length": 656,
      "extraction_method": "Direct"
    },
    {
      "page_number": 492,
      "chapter": null,
      "content": "prompt injection 285\nprompt leakage 285\nprompt template 95, 96\nproprietary LLMs 42\nClaude 2 49-51\nGemini 1.5 47-49\nGPT-4 42-47\nPython\nrelational databases, working with 175-177\nPython REPL agent\nlimitations 217\nPythonREPLTool 186\nQ\nQA recommendation chatbot\nbuilding, in cold-start scenario 151-159\nR\nReAct (Reason and Act) 85-88, 111\nread-eval-print loop (REPL) 187\nreceiver operating characteristic (ROC) curve 273\nrecency bias 75\novercoming, ways 75\nrecommendation systems 140\n",
      "content_length": 480,
      "extraction_method": "Direct"
    },
    {
      "page_number": 493,
      "chapter": null,
      "content": "recommendation systems, types\ncollaborative filtering 140\ncontent-based filtering 140\nhybrid filtering 140\nknowledge-based filtering 140\nrecurrent neural networks (RNNs) 10, 145\nRedis 173\nreinforcement learning from human feedback (RLHF) 43, 284\nreinforcement learning (RL) 18\nrelational databases 171, 172\nChinook database 173, 174\nworking with, in Python 175-178\nResponsible AI 279\nethical implications 280\nResponsible AI architecture 281\nmetaprompt level 284, 285\nmodel level 282-284\nregulations 289, 290\nuser interface level 286-289\nretrieval-augmented generation (RAG) 28\nversus copilot 29\nRetrievalQA chain 156\nretriever 102\nreward model (RM) 18\n",
      "content_length": 652,
      "extraction_method": "Direct"
    },
    {
      "page_number": 494,
      "chapter": null,
      "content": "RouterChain 106\nS\nschema 120\nsecrets 114\nstoring, in .env file 114, 115\nself-attention 11\nsemantic functions 38\nSemantic Kernel 36\nbenefits 38\ncomponents 37, 38\nsemi-structured data 170, 171\nsensitivity/true positive rate 273\nsequential chain 107\nused, for hard-coded approach 252-255\nSerpApi 129\nreference link 109, 129\nsimilarity 100\nsingle tools\ncombining, into single agent 243-252\nsingular value decomposition (SVD) 143\nsliding window attention (SWA) 55\nsmall language models (SLMs) 297\nadvantages, over LLMs 297\n",
      "content_length": 518,
      "extraction_method": "Direct"
    },
    {
      "page_number": 495,
      "chapter": null,
      "content": "Softmax function 9\nSQL Agent 179-183\nSQL databases 173\nSQLite 173\nStanford Center for Research on Foundation Models (CRFM)\n61\nStanford Question Answering Dataset (SQuAD) 271\nStarCoder model 205-210\nstate-of-the-art (SOTA) 41\nstochastic gradient descent (SGD) 17\nStoryScribe 252\nStreamlit\nfront-end for Movieharbor, developing with 164-166\nused, for developing front-end 131-136, 190-193, 256-259\nStrong AI/Super AI 226\nstructured data 169, 170\nexample 170\nStructured Query Language (SQL) 170\nsub-word approach 268\nSuperGLUE 18, 19\nsupervised fine-tuned (SFT) 17\nT\ntargets 266\n",
      "content_length": 576,
      "extraction_method": "Direct"
    },
    {
      "page_number": 496,
      "chapter": null,
      "content": "taxonomy 261\nTech Assistant Prompt 197\nTechGen Solutions 62\nTechnology Innovation Institute (TII) 54, 198\nTecton 163\ntensor 16\nTensor Processing Units (TPUs) 16\ntext embedding models 98-100\ntext generation 246-248\ntext splitters 97\ntext summarization 69\nText-to-Text Transfer Transformer (T5) 15\ntokenization 5\ntokens 5, 150\ntools list\nbuilding 245\ntransfer learning 262, 263\nTransformationChain 108\ntransformer architecture 11-15\nbuilding block 14\ndecoder layers 14\nlinear transformation 15\noutput embedding 14\nSoftMax 15\n",
      "content_length": 523,
      "extraction_method": "Direct"
    },
    {
      "page_number": 497,
      "chapter": null,
      "content": "transformers 10\ntrue negative rate 273\ntruncation 268\nTruthfulQA 19, 51\nU\nunstructured data 170\nunsupervised learning 266\nuser-based collaborative filtering 140\nuser-based KNN 141\nuser behavior data 141\nuser demographic data 141\nV\nvariational autoencoders (VAEs) 145\nVectorDB 33\nvector store 100\nreference link 101\nvector store retriever 102\nW\nWhisper 56, 244, 245\nword-based approach 268\nWordPiece 268\n",
      "content_length": 403,
      "extraction_method": "Direct"
    },
    {
      "page_number": 498,
      "chapter": null,
      "content": "Y\nYouTubeSearchTool 243, 244\nZ\nzero-shot evaluation 19\n",
      "content_length": 55,
      "extraction_method": "Direct"
    },
    {
      "page_number": 499,
      "chapter": null,
      "content": "Download a free PDF copy of\nthis book\nThanks for purchasing this book!\nDo you like to read on the go but are unable to carry your print books\neverywhere?\nIs your eBook purchase not compatible with the device of your choice?\nDon’t worry, now with every Packt book you get a DRM-free PDF version\nof that book at no cost.\nRead anywhere, any place, on any device. Search, copy, and paste code\nfrom your favorite technical books directly into your application.\nThe perks don’t stop there, you can get exclusive access to discounts,\nnewsletters, and great free content in your inbox daily.\nFollow these simple steps to get the benefits:\n1. Scan the QR code or visit the link below:\n",
      "content_length": 676,
      "extraction_method": "Direct"
    },
    {
      "page_number": 500,
      "chapter": null,
      "content": "https://packt.link/free-ebook/9781835462317\n2. Submit your proof of purchase.\n3. That’s it! We’ll send your free PDF and other benefits to your email\ndirectly.\nOceanofPDF.com\n",
      "content_length": 175,
      "extraction_method": "Direct"
    }
  ],
  "enrichment": {
    "version": "1.0.0",
    "generated_by": "generate_chapter_metadata.py",
    "contains": [
      "keywords",
      "concepts",
      "summary"
    ]
  }
}