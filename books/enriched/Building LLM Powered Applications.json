{
  "metadata": {
    "title": "Building LLM Powered Applications",
    "source_file": "Building LLM Powered Applications_metadata.json"
  },
  "chapters": [
    {
      "chapter_number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "summary": "Valentina Alto <packt\nCopyright © 2024 Packt Publishing\nauthor, nor Packt Publishing or its dealers and distributors, will be held\nPackt Publishing has endeavored to provide trademark information about all\nHowever, Packt Publishing cannot guarantee the accuracy of\nCopy Editor: Safis Editing\nPublished by Packt Publishing Ltd. Grosvenor House\nValentina Alto is an AI enthusiast, tech author, and runner.\nwhere she currently works as an AI specialist.\nlearning and AI since the outset of her academic journey, Valentina has\nShe also authored her first book with Packt, titled Modern\nGenerative AI with ChatGPT and OpenAI Models.\ncollaborates with large enterprises, aiming to integrate AI into their\nAlexandru Vesa has over a decade of expertise as an AI engineer and is\ncurrently serving as the CEO at Cube Digital, an AI software development\nfirm he leads with a vision inspired by the transformative potential of AI\nenvironments and shaping AI products in both multinational corporations\nBuilding Your Production-Ready AI Replica, hosted on the Substack\nLouis Owen is a data scientist/AI engineer hailing from Indonesia.\nCurrently contributing to NLP solutions at Yellow.ai, a leading CX\nLouis has also written a book with Packt, titled\nHyperparameter Tuning with Python, and published several papers in the AI\nJoin our community’s Discord space for discussions with the author and",
      "keywords": [
        "Building LLM Powered",
        "Packt Publishing",
        "EXPERT INSIGHT Building",
        "models MEE LETT",
        "LLM Powered",
        "Building LLM",
        "Alto Building LLM",
        "MEE LETT",
        "EXPERT INSIGHT",
        "Vig CLEP LDR",
        "LLM Powered Applications",
        "Valentina Alto",
        "Publishing",
        "book",
        "MEE large language"
      ],
      "concepts": [
        "editors",
        "current",
        "skill",
        "development",
        "products",
        "production",
        "llm",
        "louis",
        "models",
        "engineer"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 55,
          "title": "",
          "score": 0.795,
          "base_score": 0.645,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 54,
          "title": "",
          "score": 0.743,
          "base_score": 0.593,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 53,
          "title": "",
          "score": 0.67,
          "base_score": 0.52,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 52,
          "title": "",
          "score": 0.656,
          "base_score": 0.506,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 14,
          "title": "",
          "score": 0.563,
          "base_score": 0.413,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "packt",
          "publishing",
          "packt publishing",
          "ai",
          "alto"
        ],
        "semantic": [],
        "merged": [
          "packt",
          "publishing",
          "packt publishing",
          "ai",
          "alto"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2937772258458218,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.495310+00:00"
      }
    },
    {
      "chapter_number": 2,
      "title": "Segment 2 (pages 9-16)",
      "start_page": 9,
      "end_page": 16,
      "summary": "1. Introduction to Large Language Models\nWhat are large foundation models and LLMs?\nAI paradigm shift – an introduction to foundation models\nTraining and evaluating LLMs\nModel evaluation\nHow to customize your model\n2. LLMs for AI-Powered Applications\nHow LLMs are changing software development\nIntroducing AI orchestrators to embed LLMs into applications\n3. Choosing an LLM for Your Application\nProprietary models\nBeyond language models\nGenerate many outputs, then use the model to pick the best\n5. Embedding LLMs within Your Applications\nGetting started with LangChain\nModels and prompts\nWorking with LLMs via the Hugging Face Hub\nStart using open-source LLMs\nGetting started with conversational applications\n7. Search and Recommendation Engines with LLMs\nHow LLMs are changing recommendation systems\n8. Using LLMs with Structured Data\nAdding further tools\nChoosing the right LLM for code\nBuilding Multimodal Applications with LLMs\nBuilding a multimodal agent with LangChain\nOption 3: Hard-coded approach with a sequential chain\nFine-Tuning Large Language Models\nGetting started with fine-tuning\nFine-tuning the model\nResponsible AI\nWhat is Responsible AI and why do we need it?\nResponsible AI architecture\nModel level\nThe latest trends in language models and generative AI\nSmall language models\nWith this book, we embark upon an exploration of large language models\ncutting-edge technologies to practical applications that LLMs offer,\nusing generative AI solutions.\npowerful applications powered by LLMs, leveraging new AI orchestrators\ntools to get the most out of LLMs in both your daily tasks and your",
      "keywords": [
        "LLMs",
        "Summary",
        "Models",
        "Technical requirements",
        "References",
        "Language Models",
        "LLM",
        "Applications",
        "Technical",
        "requirements",
        "Large Language Models",
        "Language",
        "front-end with Streamlit",
        "started",
        "LangChain"
      ],
      "concepts": [
        "models",
        "llms",
        "summary",
        "references",
        "llm",
        "applications",
        "application",
        "recommendation",
        "generate",
        "generation"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 57,
          "title": "",
          "score": 0.848,
          "base_score": 0.698,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 42,
          "title": "",
          "score": 0.8,
          "base_score": 0.65,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 5,
          "title": "",
          "score": 0.799,
          "base_score": 0.649,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 3,
          "title": "",
          "score": 0.764,
          "base_score": 0.764,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 56,
          "title": "",
          "score": 0.756,
          "base_score": 0.606,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "llms",
          "models",
          "language models",
          "applications",
          "ai"
        ],
        "semantic": [],
        "merged": [
          "llms",
          "models",
          "language models",
          "applications",
          "ai"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4354029125174292,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.495396+00:00"
      }
    },
    {
      "chapter_number": 3,
      "title": "Segment 3 (pages 17-24)",
      "start_page": 17,
      "end_page": 24,
      "summary": "comprehensive understanding of the transformative power of LLMs and\nleverage this book to understand how to architect and deploy LLMs as\nbook to evaluate if and how LLMs could be used within their apps and\ncan learn how LLMs are applied in practice from this book.\nChapter 1, Introduction to Large Language Models, provides an\nbase LLMs with fine-tuned LLMs. By the end of this chapter, you will have\nChapter 2, LLMs for AI-Powered Applications, explores how LLMs are\nAI-powered applications.\npicture of how LLMs can be embedded in different application scenarios,\nChapter 3, Choosing an LLM for Your Application, highlights how different\nLLMs may have different architectures, sizes, training data, capabilities,\nthe end of this chapter, you will have the foundations to build functional\nand solid prompts for your LLM-powered applications, which will also be\nChapter 5, Embedding LLMs within Your Applications, discusses a new set\nthe advent of developing applications with LLMs. To make it easier to\norchestrate LLMs and their related components in an application flow,\nLLM-powered applications using LangChain and open-source Hugging\nChapter 6, Building Conversational Applications, allows us to embark on\nLLM-powered applications.\nChapter 7, Search and Recommendation Engines with LLMs, explores how\nrecommendation application and leverage state-of-the-art LLMs using\nChapter 8, Using LLMs with Structured Data, covers a great capability of\nChapter 9, Working with Code, covers another great capability of LLMs:\n“simple” code understanding and generation to the building of applications\nwill be able to build LLM-powered applications for your coding projects, as\nwell as build LLM-powered applications with natural language interfaces to\nChapter 10, Building Multimodal Applications with LLMs, goes beyond\nLLMs, introducing the concept of multi-modality while building agents.\nLLMs needed to perform various AI tasks.\nChapter 11, Fine-Tuning Large Language Models, covers the technical\nchapter, you will be able to fine-tune an LLM on your own data so that you\ncan build domain-specific applications powered by that LLM.\nbehind the mitigation of the potential harms of LLMs – and AI models in\ndeveloping LLM-powered applications.\nthis chapter, you will have a deeper understanding of how to prevent LLMs\nThis book aims to provide a solid theoretical foundation of what LLMs are,\nin LLMs and their applications in NLP.",
      "keywords": [
        "LLMs",
        "applications",
        "LLM",
        "book",
        "LLM-powered applications",
        "end",
        "Data",
        "build",
        "models",
        "LangChain",
        "language",
        "code",
        "build LLM-powered applications",
        "Large Language Models",
        "LLM-powered"
      ],
      "concepts": [
        "llms",
        "models",
        "llm",
        "code",
        "coding",
        "language",
        "data",
        "book",
        "developers",
        "building"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 21,
          "title": "",
          "score": 0.888,
          "base_score": 0.738,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 12,
          "title": "",
          "score": 0.826,
          "base_score": 0.676,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 11,
          "title": "",
          "score": 0.814,
          "base_score": 0.664,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 4,
          "title": "",
          "score": 0.766,
          "base_score": 0.616,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 2,
          "title": "",
          "score": 0.764,
          "base_score": 0.764,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "llms",
          "applications",
          "chapter",
          "powered applications",
          "powered"
        ],
        "semantic": [],
        "merged": [
          "llms",
          "applications",
          "chapter",
          "powered applications",
          "powered"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.39114480138831126,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.495466+00:00"
      }
    },
    {
      "chapter_number": 4,
      "title": "Segment 4 (pages 25-33)",
      "start_page": 25,
      "end_page": 33,
      "summary": "The code bundle for the book is hosted on GitHub at\nThere are a number of text conventions used throughout this book.\nGeneral feedback: Email feedback@packtpub.com and mention the book’s\nof this book, please email us at questions@packtpub.com.\nIf you have found a mistake in this book, we\nbook, please visit http://authors.packtpub.com.\nAmazon review page for this book and share your feedback.\nthis book\nThanks for purchasing this book!\nDo you like to read on the go but are unable to carry your print books\nDon’t worry, now with every Packt book you get a DRM-free PDF version\nof that book at no cost.\nfrom your favorite technical books directly into your application.\nLanguage Models\nDear reader, welcome to Building Large Language Model Applications!\nthis book, we will explore the fascinating world of a new era of application\ndevelopments, where large language models (LLMs) are the main\nlike content based on user requests made in natural language.\nwe learned to acknowledge the power of generative AI and its core models:\nLLMs. However, LLMs are more than language generators.\nIn this book, we will see the theory and practice of how to build LLM-\nThe book will start with Part 1, where we\nmarket right now, and the emerging frameworks for LLMs-powered\nimplement many applications using various LLMs, addressing different\nThis chapter provides an introduction and deep dive into LLMs, a\npowerful set of deep learning neural networks that feature the domain of\nLLMs in the hands-on part of this book, where we will see in practice how to\nembed LLMs within your applications.\nmodels and LLMs?\nLLMs are deep-learning-based models that use many parameters to learn\nLLMs belong to a wider set of models that feature the AI subfield of\ngenerative AI: large foundation models (LFMs).\nmodels",
      "keywords": [
        "LLMs",
        "book",
        "Models",
        "Applications",
        "hosted on GitHub",
        "code",
        "Language",
        "content",
        "Language Model Applications",
        "Large",
        "PDF",
        "large foundation models",
        "Language Models",
        "text",
        "foundation models"
      ],
      "concepts": [
        "model",
        "llms",
        "content",
        "text",
        "learned",
        "book",
        "free",
        "code",
        "application",
        "applications"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 3,
          "title": "",
          "score": 0.766,
          "base_score": 0.616,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 21,
          "title": "",
          "score": 0.67,
          "base_score": 0.52,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 37,
          "title": "",
          "score": 0.664,
          "base_score": 0.514,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 11,
          "title": "",
          "score": 0.655,
          "base_score": 0.505,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 2,
          "title": "",
          "score": 0.627,
          "base_score": 0.627,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "book",
          "llms",
          "models",
          "packtpub com",
          "packtpub"
        ],
        "semantic": [],
        "merged": [
          "book",
          "llms",
          "models",
          "packtpub com",
          "packtpub"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.40493753082291833,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.495528+00:00"
      }
    },
    {
      "chapter_number": 5,
      "title": "Segment 5 (pages 34-41)",
      "start_page": 34,
      "end_page": 41,
      "summary": "A foundation model refers to a type of pre-trained generative AI model that\nThese models undergo extensive training on vast and diverse datasets,\nThis initial pre-training phase equips the models\ndifferentiates generative AI models from standard natural language\nThe difference between generative AI and NLU algorithms is\nGenerative AI can be used for tasks\nFoundation models are designed with transfer learning in mind, meaning\ndata, eliminating the need for training separate models for individual tasks.\nadvantages, as foundation models, with their diverse training datasets, can\nFigure 1.1: From task-specific models to general models\ndata, we refer to the output LFM as an LLM, due to its focus on text\nWe can then say that an LLM is a type of foundation model specifically\nmany others, are trained on vast amounts of text data and can generate\nNevertheless, LLMs aren’t limited to performing text-related tasks.\nLLMs are a particular type of artificial neural networks (ANNs):\nThose weights represent the parameters of the model that\nANNs are, by definition, mathematical models that work with numerical\ndata.\nas model input:\nprocessed by machine learning models.\nThese embeddings are learned during the training of the\nlanguage model and capture semantic relationships between tokens.\nThe numerical representation allows the model to perform\nrelationship allows LLMs to process and understand textual data in a\ndata.\nthe input data.\nThese layers process the input data through a\nrelevant results depending on the task the neural network is designed\nbased on the training data and the desired outputs.\ntrain neural networks.",
      "keywords": [
        "models",
        "data",
        "tasks",
        "NLU algorithms",
        "Foundation models",
        "natural language",
        "NLU",
        "LLMs",
        "training",
        "neural network",
        "text",
        "language",
        "neural",
        "foundation",
        "generative"
      ],
      "concepts": [
        "models",
        "data",
        "training",
        "tasks",
        "generative",
        "generation",
        "generate",
        "layers",
        "tokenization",
        "text"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 2,
          "title": "",
          "score": 0.799,
          "base_score": 0.649,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 57,
          "title": "",
          "score": 0.73,
          "base_score": 0.58,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 56,
          "title": "",
          "score": 0.724,
          "base_score": 0.574,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 42,
          "title": "",
          "score": 0.717,
          "base_score": 0.567,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 49,
          "title": "",
          "score": 0.684,
          "base_score": 0.534,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "neural",
          "models",
          "foundation",
          "data",
          "training"
        ],
        "semantic": [],
        "merged": [
          "neural",
          "models",
          "foundation",
          "data",
          "training"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3586670437669504,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.495586+00:00"
      }
    },
    {
      "chapter_number": 6,
      "title": "Segment 6 (pages 42-50)",
      "start_page": 42,
      "end_page": 50,
      "summary": "network learn from data and make accurate predictions.\nconditional probability of A given B as:\nP(B|A) = probability of B occurring given A, also\nP(A|B) = probability of A occurring, given B; also\nknown as the posterior probability of A, given B.\nBayes’ theorem relates the conditional probability of an event based on new\ncontext of LLMs, we are saying that such a model functions by predicting\nthe next most likely word, given the previous words prompted by the user.\ndive deeper into the process of training an LLM in the next sections).\non the training text corpus, the model will be able to identify, given a user’s\ngenerate multiple candidate words, so we need a method to evaluate which\nto select the most likely word given the context.\nof each candidate word being the next word in the context, based on the\nlanguage model’s knowledge learned during training.\neach candidate word, based on the language model’s knowledge of the\nfrequency of these words in the training data.\ncandidate word fits the context “The cat is on the....” This is the\nprobability of observing the context given each candidate word.\nLLM calculates this based on the training data and how often each word\ncat is on the roof”) are the likelihoods for each candidate word given\nPosterior probability (P(A|B)): Using Bayes’ theorem, we can\ncalculate the posterior probability for each candidate word based on the\nprobabilities for each candidate word, we choose the word with the\nhighest posterior probability as the most likely next word to complete\nThe LLM uses Bayes’ theorem and the probabilities learned during training\nFigure 1.7: Predicting the next most likely word in an LLM\noutput vector suitable for representing probabilities of\nparticular ANNs’ architectural framework, introduced in recent years and the\ntransformers-based\nIn the following sections, we will explore the evolution of generative AI\nmodel architecture, from early developments to state-of-the-art transformers.\nWe will start by covering the first generative AI models that paved the way\nThe very first popular generative AI ANN architectures trace back to the 80s\nsuitable for tasks like language modeling, machine translation, and text\nweights based on the gradient of the loss function.\nRNNs during training when the gradients become\nThese architectures were popular and effective for various generative tasks,\ngoing to see how a transformers-based architecture overcomes the above\nlimitations and is at the core of modern generative AI LLMs. Introducing the transformer\nThe transformer architecture is a deep learning model introduced in the\nsequence while generating the output.\nbetween words in the data.\nSince transformers use attention on the same sequence that is currently being\nattention scores between the elements in the input sequence and are the three",
      "keywords": [
        "word",
        "probability",
        "candidate word",
        "Bayes’ theorem",
        "LLM",
        "training",
        "LLMs",
        "Bayes’",
        "context",
        "based",
        "data",
        "posterior probability",
        "generative",
        "candidate",
        "input"
      ],
      "concepts": [
        "attention",
        "probability",
        "probabilities",
        "word",
        "architecture",
        "architectural",
        "models",
        "context",
        "transformer",
        "function"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 5,
          "title": "",
          "score": 0.619,
          "base_score": 0.469,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 30,
          "title": "",
          "score": 0.608,
          "base_score": 0.458,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 49,
          "title": "",
          "score": 0.59,
          "base_score": 0.44,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 2,
          "title": "",
          "score": 0.571,
          "base_score": 0.421,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 48,
          "title": "",
          "score": 0.526,
          "base_score": 0.376,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "word",
          "candidate",
          "probability",
          "candidate word",
          "given"
        ],
        "semantic": [],
        "merged": [
          "word",
          "candidate",
          "probability",
          "candidate word",
          "given"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.27270888420489514,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.495638+00:00"
      }
    },
    {
      "chapter_number": 7,
      "title": "Segment 7 (pages 51-58)",
      "start_page": 51,
      "end_page": 58,
      "summary": "attention layer represents the input values in a transformed, context-aware\nThe encoder takes the input sequence and produces a sequence of\nThe decoder takes the output sequence (shifted right by one position)\nposition in the decoder layer is to prevent the model from\nis because the model is trained to generate the output\nsequence given the input sequence, and the output sequence\nBy shifting the output sequence\nright, the model only sees the previous tokens as input and\nlearns to predict the next token based on the input sequence\nand the previous output tokens.\nInput embedding: These are the vector representations of tokenized\nPositional encoding: As the transformer does not have an inherent\nencodings provide information about the positions of words in the input\nsequence, allowing the model to understand the order of tokens.\ntransformer model to attend to different parts of the input data in\nIt adds the output of a layer to the original input and then\nnormalized output of attention layers into a suitable representation for\nThe decoding part of the transformer starts with a similar process as the\nencoding part, where the target sequence (output sequence) undergoes input\nposition, the model tries to predict the token that comes after the\noutput sequences from a model that predicts each\nand then using the model’s prediction as the next input\nThe output sequence is then the\nDecoder layers: Similarly to the encoder block, here, we also have\nnon-linear transformation to the output vector.\ntransformation (Softmax) conveys the output vector into a probability\nOther models use only the decoder part, such as GPT-3 (Generative Pre-\ntrained Transformer 3), which is designed for natural language generation\nFinally, there are models that use both the encoder and the decoder parts,\ncharacterized also by what the model knows, depending on its training\nTraining and evaluating LLMs\ndiversity of the output text depend largely on two factors: the training dataset\ntext corpus for an LLM can be, let’s consider OpenAI’s GPT-3 training\nattention mechanism, allows the model to efficiently process long sequences",
      "keywords": [
        "output sequence",
        "output",
        "sequence",
        "input",
        "input sequence",
        "model",
        "transformer",
        "token",
        "LLM",
        "LLMs",
        "previous output tokens",
        "training",
        "LLM architecture",
        "transformer architecture",
        "layer"
      ],
      "concepts": [
        "token",
        "text",
        "trained",
        "transformation",
        "transformations",
        "sequence",
        "generate",
        "generating",
        "generation",
        "output"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 48,
          "title": "",
          "score": 0.622,
          "base_score": 0.472,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 5,
          "title": "",
          "score": 0.615,
          "base_score": 0.465,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 49,
          "title": "",
          "score": 0.613,
          "base_score": 0.463,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 30,
          "title": "",
          "score": 0.6,
          "base_score": 0.45,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 2,
          "title": "",
          "score": 0.544,
          "base_score": 0.394,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "sequence",
          "output sequence",
          "output",
          "input",
          "input sequence"
        ],
        "semantic": [],
        "merged": [
          "sequence",
          "output sequence",
          "output",
          "input",
          "input sequence"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.26843804839406354,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.495687+00:00"
      }
    },
    {
      "chapter_number": 8,
      "title": "Segment 8 (pages 59-66)",
      "start_page": 59,
      "end_page": 66,
      "summary": "while consuming less power, enabling faster model training\nThe training process involves numerous iterations over the dataset, fine-\ntuning the model’s parameters using optimization algorithms\nThrough this process, transformer-based language models\nformatting the data for training.\n3. Model architecture: This is the process of designing the structure and\n4. Model initialization: This is the process of assigning initial values to\nusing pre-trained weights from another model.\n5. Model pre-training: This is the process of updating the weights and\nThe model training may take several\nthe best set of weights for the model that minimizes\ntraining data.\nThe output of the pre-training phase is the so-called base model.\n6. Fine-tuning: The base model is trained in a supervised way with a\nnecessary to make the base model more in line with AI assistants, such\ntuned (SFT) model.\nconsists of iteratively optimizing the SFT model (by updating some of\nLLM trained incorporating human preferences).\nlearning that focuses on training computers to make optimal\ngiven model.\nOnce we have a trained model, the next and final step is evaluating its\nModel evaluation\nEvaluating traditional AI models was, in some ways, pretty intuitive.\nSo we train our\nmodel on a training dataset with a set of labeled images and, once the model\nAs those models are\ntrained on unlabeled text and are not task-specific, but rather generic and\nGeneral Language Understanding Evaluation (GLUE) and\nthe GLUE benchmark, the better the LLM is at generalizing across\nthan GLUE, such as natural language inference, question answering,\nleaderboard that ranks models based on their average score across all\nthe GLUE benchmark, as it covers more complex tasks and\nphenomena, requires models to handle multiple domains and formats,\nbenchmark measures the knowledge of an LLM using zero-shot and\nevaluating a language model without any labeled data\nmodel can perform a new task by using natural\nIt is the probability that a trained model will\nlabeled training data.\nTruthfulQA: This benchmark evaluates a language model’s accuracy\nmodels that can perform complex NLU tasks.\nlanguage understanding among various domains and tasks.\nuse cases, you still have a margin to customize those models and make them\nBase models versus\ncustomized models\nAs we saw in the previous section, training an LLM requires great\nLuckily, pre-trained LLMs are generalized enough to be applicable to\nmight want to customize your model.\nHow to customize your model\nThere are three main ways to customize your model:",
      "keywords": [
        "model",
        "LLM",
        "LLMs",
        "benchmark",
        "language model",
        "language",
        "data",
        "training",
        "GLUE benchmark",
        "tasks",
        "GLUE",
        "process",
        "model training",
        "evaluation",
        "base model"
      ],
      "concepts": [
        "model",
        "human",
        "humanities",
        "training",
        "evaluating",
        "evaluation",
        "evaluate",
        "tasks",
        "data",
        "question"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 9,
          "title": "",
          "score": 0.737,
          "base_score": 0.587,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 47,
          "title": "",
          "score": 0.54,
          "base_score": 0.54,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 15,
          "title": "",
          "score": 0.531,
          "base_score": 0.531,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 2,
          "title": "",
          "score": 0.527,
          "base_score": 0.527,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 56,
          "title": "",
          "score": 0.523,
          "base_score": 0.523,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "model",
          "training",
          "glue",
          "benchmark",
          "process"
        ],
        "semantic": [],
        "merged": [
          "model",
          "training",
          "glue",
          "benchmark",
          "process"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3264090389175531,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.495743+00:00"
      }
    },
    {
      "chapter_number": 9,
      "title": "Segment 9 (pages 67-74)",
      "start_page": 67,
      "end_page": 74,
      "summary": "Extending non-parametric knowledge: This allows the model to\nLLMs exhibit two types of knowledge: parametric and\nFew-shot learning: In this type of model customization, the LLM is\nThe model\nused to improve the performance of LLMs on new\nspecific datasets to customize the foundation models for particular\nparameters of the pre-trained model are altered and optimized toward\nThis is done by training the model on a smaller\nlabeled dataset that is specific to the new task.\nfine-tuning is to leverage the knowledge learned from the pre-trained\nmodel and fine-tune it to the new task, rather than training a model\na pre-trained model with general-purpose weights or parameters.\nOnce the training is done, you will have a customized model that is\nThe nice thing about fine-tuning is that you can make pre-built models\nAt the same time, the model keeps its generative\nIn Chapter 11, Fine-Tuning Large Language Models, we will focus on\nfine-tuning your model in Python so that you can test it for your own\nIn this chapter, we explored the field of LLMs, with a technical deep dive\ntraining process works, and different ways to customize your own LLM.\nLLMs for AI-Powered\nIn Chapter 1, Introduction to Large Language Models, we introduced large\nlanguage models (LLMs) as powerful foundation models with generative\nis: what should I do with those models?\nIn this chapter, we are going to see how LLMs are revolutionizing the world of\nsoftware development, leading to a new era of AI-powered applications.\nend of this chapter, you will have a clearer picture of how LLMs can be\nembedded in different application scenarios, thanks to the new AI orchestrator\nHow LLMs are changing software development\nIntroducing AI orchestrators to embed LLMs into applications\nLLMs and, generally speaking, large foundation models (LFMs), are\npowerful applications.\nBut what does it mean, concretely, to incorporate LLMs within applications?\ncoordinate the LLMs’ functionality within the application, as we will discuss\ncollaboration provided by LLMs in enhancing application functionalities.\nsection will cover a brand-new category of software – the copilot system.\nA copilot is powered by LLMs, or, more generally, LFMs, meaning that",
      "keywords": [
        "LLMs",
        "model",
        "Large Language Models",
        "applications",
        "knowledge",
        "LLM",
        "copilot",
        "Language Models",
        "training",
        "task",
        "non-parametric knowledge",
        "Extending non-parametric knowledge",
        "fine-tuning",
        "foundation models",
        "pre-trained model"
      ],
      "concepts": [
        "llms",
        "applications",
        "application",
        "model",
        "tasks",
        "training",
        "chapters",
        "development",
        "llm",
        "aspects"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 8,
          "title": "",
          "score": 0.737,
          "base_score": 0.587,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 47,
          "title": "",
          "score": 0.645,
          "base_score": 0.645,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 37,
          "title": "",
          "score": 0.547,
          "base_score": 0.547,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 2,
          "title": "",
          "score": 0.545,
          "base_score": 0.545,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 15,
          "title": "",
          "score": 0.492,
          "base_score": 0.492,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "llms",
          "model",
          "models",
          "knowledge",
          "trained model"
        ],
        "semantic": [],
        "merged": [
          "llms",
          "model",
          "models",
          "knowledge",
          "trained model"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.36002518842478615,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.495802+00:00"
      }
    },
    {
      "chapter_number": 10,
      "title": "Segment 10 (pages 75-82)",
      "start_page": 75,
      "end_page": 82,
      "summary": "Figure 2.1: A copilot is powered by an LLM\nA copilot is designed to have a conversational user interface, allowing\nspecific taxonomy (for example, querying tabular data needs the knowledge\nFigure 2.2: An example of a conversational UI to reduce the gap between the user and the database\napplication or domain.\nGrounding is the process of using LLMs with information that is\nLLM-powered application that assists you during your research\nLLM).\nLLM to the set of papers, so that your application will only\nWhenever we want our copilot to be grounded to domain-specific\nFor example, let’s say we developed a copilot within our company that\nThe following figure shows an example of grounding a copilot system:\nFigure 2.3: Example of grounding a copilot\nLLMs’ non-parametric knowledge (for example, to allow a web\nFor example, with a LinkedIn plug-in, our copilot\npowered by an LLM will be able not only to generate the post but also\nNote that the user’s prompt in natural language is not the only input the model\npowered applications and the set of instructions we provide to the model.\nprompts to LLMs for a wide variety of applications and research\nLLM.\nas parameters, examples, or data sources, to influence the LLM’s\nFor example, if we want our LLM-powered application\nembed LLMs into applications\nincorporating LLMs within applications: a technical aspect and a conceptual\ntechnically embed and orchestrate LLMs within our applications.\nsimplification in the domain of AI-powered applications: after producing models,\narise in developing this new kind of AI, since there are LLM-related components\nLLM instructions (the system message mentioned earlier) so that the application\nsecurity threats that are typical to LLM-powered applications and need to be\nFigure 2.5: High-level architecture of LLM-powered applications\napplication.\nExamples include Falcon LLM, developed by Abu Dhabi’s\n3, Choosing an LLM for Your Application.\nMemory: LLM applications commonly use a conversational interface,",
      "keywords": [
        "LLM",
        "copilot",
        "LLMs",
        "application",
        "knowledge",
        "prompt engineering",
        "models",
        "prompt",
        "RAG",
        "knowledge base",
        "LLM applications",
        "engineering",
        "data",
        "users",
        "LLM applications commonly"
      ],
      "concepts": [
        "llms",
        "prompt",
        "models",
        "specific",
        "llm",
        "application",
        "applications",
        "examples",
        "knowledge",
        "called"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 3,
          "title": "",
          "score": 0.715,
          "base_score": 0.565,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 11,
          "title": "",
          "score": 0.601,
          "base_score": 0.451,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 21,
          "title": "",
          "score": 0.598,
          "base_score": 0.448,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 4,
          "title": "",
          "score": 0.591,
          "base_score": 0.441,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 37,
          "title": "",
          "score": 0.569,
          "base_score": 0.419,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "copilot",
          "applications",
          "llm",
          "application",
          "powered"
        ],
        "semantic": [],
        "merged": [
          "copilot",
          "applications",
          "llm",
          "application",
          "powered"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3127909040399898,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.495874+00:00"
      }
    },
    {
      "chapter_number": 11,
      "title": "Segment 11 (pages 83-93)",
      "start_page": 83,
      "end_page": 93,
      "summary": "allowing developers and users to customize the behavior of the language\n“Backend,” or what the user does not see: Natural language is not\nthat make it easier to embed and orchestrate LLMs within applications.\nSemantic Kernel, and Haystack.\napplications powered by language models, making them data-aware (with\nFigure 2.6: LangChain’s components\nLangChain supports proprietary models, such as those available\nmodels and LLMs, datasets, and demos.\nAlongside models, LangChain also offers many prompt-related components\nwant to provide the model with.\nMemory: This allows the application to keep references to the user’s\nthat make it easier to build complex applications that require chaining LLMs\nto call based on the user input and the context.\nLangChain provides modular abstractions for the components we previously\nAlongside those components, LangChain also offers pre-built chains, which\nLangChain based.\nApplications, we will focus much deeper on LangChain components and overall\nHaystack is a Python-based framework developed by Deepset, a startup founded\nprovides developers with the tools to build natural language processing (NLP)-\nbased applications, and with the introduction of Haystack, they are taking them to\nNodes: These are components that perform a specific task or function, such\nHaystack supports proprietary models, such as those available in OpenAI\nbased on the user input or the context.\npipelines or nodes, and it can decide which tool to call based on the user\nTools: There are functions that an agent can call to perform natural language\nSemantic Kernel\nframework, a kernel is meant to act as the engine that addresses a user’s input by\nfunctions.\nSemantic Kernel has the following main components:\nSemantic Kernel supports proprietary models, such as those\nconsumable from the Hugging Face Hub. Memory: It allows the application to keep references to the user’s\nSemantic Kernel, memories can be accessed in three ways:\nSemantic memory search: This is similar to LangChain’s and\nHaystack’s memory, as it uses embeddings to represent and search for\nFunctions: Functions can be seen as skills that mix LLM prompts and code,\nSemantic functions: These are a type of templated prompt, which is a\nnatural language query that specifies the input and output format for\nroute the intent captured by the semantic function and perform the\nTo make an example, a semantic function could ask the LLM to write a\nSemantic Kernel offers out-of-the-box plug-ins, such as\nleveraging functions (both native and semantic, or a mix of the two).\nuser’s task and produces the set of actions, plug-ins, and functions needed to\nLightweight and C# support: Semantic Kernel is more lightweight and\nWide range of use cases: Semantic Kernel is versatile, supporting various\nIndustry-led: Semantic Kernel was developed by Microsoft, and it is the\nOverall, the three frameworks offer, more or less, similar core components,\none should I use to build my LLM-powered application?” Well, there is no right\nDifferent frameworks may support different programming languages or have\nSemantic Kernel supports C#, Python, and Java, while LangChain and\nHaystack are mainly based on Python (even though LangChain also\nThe type and complexity of the natural language tasks you want to\nperform or support: Different frameworks may have different capabilities\nor features for handling various natural language tasks, such as\nLangChain and Haystack provide utilities and components for orchestrating\nand executing natural language tasks, while Semantic Kernel allows you to\nuse natural language semantic functions to invoke LLMs and services.\nmay want to choose a framework that offers the functionality and flexibility\nFor example, Semantic Kernel provides\nwhile LangChain and Haystack allow you to plug in different components\nand community support for the framework: Different frameworks may\nhave different levels of documentation, tutorials, examples, and community\nsupport that can help you learn, use, and troubleshoot the framework.\nexample, Semantic Kernel has a website with documentation, tutorials,\ndocumentation, tutorials, examples, and community support that can help",
      "keywords": [
        "Semantic Kernel",
        "Kernel",
        "Semantic",
        "Haystack",
        "LLMs",
        "natural language",
        "Semantic Kernel supports",
        "language",
        "natural language tasks",
        "Hugging Face Hub",
        "LangChain",
        "models",
        "components",
        "framework",
        "Hugging Face"
      ],
      "concepts": [
        "prompts",
        "model",
        "based",
        "langchain",
        "functionality",
        "function",
        "functions",
        "semantic",
        "documents",
        "document"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 21,
          "title": "",
          "score": 0.843,
          "base_score": 0.693,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 3,
          "title": "",
          "score": 0.814,
          "base_score": 0.664,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 12,
          "title": "",
          "score": 0.724,
          "base_score": 0.574,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 2,
          "title": "",
          "score": 0.679,
          "base_score": 0.679,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 33,
          "title": "",
          "score": 0.676,
          "base_score": 0.526,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "semantic",
          "kernel",
          "semantic kernel",
          "haystack",
          "components"
        ],
        "semantic": [],
        "merged": [
          "semantic",
          "kernel",
          "semantic kernel",
          "haystack",
          "components"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.41351225030069627,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.495939+00:00"
      }
    },
    {
      "chapter_number": 12,
      "title": "Segment 12 (pages 94-102)",
      "start_page": 94,
      "end_page": 102,
      "summary": "your LLM-powered applications, and a wise approach could be to use the one that\nIn this chapter, we delved into the new way of developing applications that LLMs\nLangChain, Haystack, and Semantic Kernel – and we examined their features,\nan LLM for Your Application, we are going to see the most prominent LLMs on\ndecision criteria to pick the proper models with respect to the application use\nhttps://learn.microsoft.com/en-us/semantic-\nlanguage models (LLMs) and their components within applications.\nDifferent LLMs may have different architectures,\nLLM for your application.\nThe main criteria and tools to use when comparing LLMs\nchoose the right LLM for your application and how to use it effectively and\ndevelopment of LLMs. Several new models have been released or\nSome of these models are the largest and most advanced ever\nIn this chapter, we will review some of the most promising LLMs in the\nProprietary models\nProprietary LLMs are developed and owned by private companies, and they\nProprietary models offer a series of advantages, including better support and\nopen-source models in terms of generalization, because of their complexity\nGPT-4 Turbo, one of the latest models developed by OpenAI, is among the\nmodels, a decoder-only transformer-based architecture introduced by\nstill includes the main elements that feature in transformer architecture that\nHowever, in this architecture, the model solely\nGPT-4, like the previous models in the GPT series, has been trained on both\nAdditionally, to make the model more aligned with the user’s intent, the\ntraining process also involved reinforcement learning from human\nfeedback (RLHF) training.\nusing that feedback to further optimize the model.\n1. Training a reward model based on human preferences.\n2. Optimizing the LLM with respect to the reward model.\nWith RLHF, thanks to the reward model, the LLM is able to\nlearn from human preferences and be more aligned with\nThis model integrates\nThe RLHF component involves training the model to\nThese trainers review the model’s responses and\nprovide ratings or corrections, guiding the model to generate\nFor instance, if a language model initially produces an output\nmodel then uses this feedback to adjust its parameters and\nwith the model learning from a series of human judgments to\nOn MMLU, GPT-4 outperformed previous models not only in",
      "keywords": [
        "LLMs",
        "model",
        "LLM",
        "REST API",
        "human",
        "promising LLMs",
        "Proprietary",
        "training",
        "Application",
        "architecture",
        "RLHF",
        "reward model",
        "feedback",
        "Proprietary models",
        "market"
      ],
      "concepts": [
        "models",
        "training",
        "architectures",
        "llms",
        "langchain",
        "proprietary",
        "main",
        "learning",
        "language",
        "source"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 3,
          "title": "",
          "score": 0.826,
          "base_score": 0.676,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 37,
          "title": "",
          "score": 0.73,
          "base_score": 0.58,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 11,
          "title": "",
          "score": 0.724,
          "base_score": 0.574,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 21,
          "title": "",
          "score": 0.649,
          "base_score": 0.499,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 4,
          "title": "",
          "score": 0.625,
          "base_score": 0.475,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "model",
          "reward",
          "reward model",
          "rlhf",
          "models"
        ],
        "semantic": [],
        "merged": [
          "model",
          "reward",
          "reward model",
          "rlhf",
          "models"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.32259585130890467,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.495995+00:00"
      }
    },
    {
      "chapter_number": 13,
      "title": "Segment 13 (pages 103-110)",
      "start_page": 103,
      "end_page": 110,
      "summary": "https://openai.com/research/gpt-4)\nIn addition to MMLU, GPT-4 has been benchmarked on a variety of SOTA\nFigure 3.3: GPT performance on academic and professional exams (source:\nLLMs generate text that is incorrect, nonsensical, or not real,\namounts of text data and produce outputs based on the\nTherefore, LLMs may generate text that is not supported by\nimprovements with TruthfulQA benchmarks, which test the model’s ability\nbenchmarks in Chapter 1, in the Model evaluation section).\nTruthfulQA benchmark with those of GPT-3.5 (the model behind OpenAI’s\nFigure 3.4: Model comparison in TruthfulQA benchmark (source:\nhttps://openai.com/research/gpt-4)\nFinally, with GPT-4, OpenAI made an additional effort to make it safer and\ndomains like AI alignment risks, privacy, and cybersecurity, with the goal of\nFor example, an LLM may be aligned if it generates\nGemini 1.5 is a SOTA generative AI model developed by Google and\nLike GPT-4, Gemini is designed to be\nThe MoE model uses a gating mechanism\nGemini, developers can access it via the APIs provided for different model\nCompared to its previous version, Gemini 1.0, the current model\nFigure 3.5: Gemini 1.5 Pro and Ultra compared to its previous version 1.0 (source:\nFigure 3.6: Gemini 1.5 Pro compared to Gemini 1.0 Pro and Ultra on different benchmarks (source:\nNote that Gemini 1.5 Pro is outperforming Gemini 1.0 Ultra (which is\nFinally, Gemini Pro and Ultra can also be consumed by developers via the\nClaude 2, which stands for Constitutional Large-scale Alignment via User\nData and Expertise, is an LLM developed by Anthropic, a research company\nClaude 2 is a transformer-based LLM that has been trained on a mix of",
      "keywords": [
        "Gemini",
        "model",
        "Pro",
        "MMLU",
        "Gemini Pro",
        "text",
        "Ultra",
        "LLMs",
        "LLM",
        "data",
        "source",
        "Claude",
        "performance on MMLU",
        "Pro and Ultra",
        "alignment"
      ],
      "concepts": [
        "model",
        "gemini",
        "risk",
        "anthropic",
        "generate",
        "generative",
        "aligned",
        "different",
        "access",
        "source"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 14,
          "title": "",
          "score": 0.72,
          "base_score": 0.57,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 53,
          "title": "",
          "score": 0.711,
          "base_score": 0.561,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 15,
          "title": "",
          "score": 0.617,
          "base_score": 0.467,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 54,
          "title": "",
          "score": 0.599,
          "base_score": 0.449,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 16,
          "title": "",
          "score": 0.59,
          "base_score": 0.44,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "gemini",
          "pro",
          "ultra",
          "gemini pro",
          "pro ultra"
        ],
        "semantic": [],
        "merged": [
          "gemini",
          "pro",
          "ultra",
          "gemini pro",
          "pro ultra"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.28640842455652815,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.496289+00:00"
      }
    },
    {
      "chapter_number": 14,
      "title": "Segment 14 (pages 111-118)",
      "start_page": 111,
      "end_page": 118,
      "summary": "CAI aims to make the model safer and more aligned with human values and\nprinciples to guide the model’s behavior and outputs, rather than relying on\nFirst, the model is trained to critique and revise its own responses using\nSecond, the model is trained via reinforcement learning, but rather than\nusing human feedback, it uses AI-generated feedback based on the\nPlus, the model can also generate longer output compared to\nOverall, Claude 2 is a very interesting model and competitor of GPT-4 to\nthree models:\nIn addition to proprietary models, there is a huge market for open-source\nOpen-source models\nThe advantage of an open-source model is that, by definition, developers\nsource code made by models’ owners.\nThere is the possibility to train your model from scratch, on top of the\nclassical fine-tuning, which is also available for proprietary models.\nTo compare open-source models, throughout this book, we will refer to the\nThe Open LLM Leaderboard uses four main evaluation benchmarks, which\nwe covered in Chapter 1, in the Model evaluation section:\nTruthfulQA: An evaluation of how truthful the model is when\nLarge Language Model Meta AI 2 (LLaMA-2) is a new family of models\nIt is an autoregressive model with an optimized, decoder-only transformer\nrefers to the fact that the model predicts the next token in the\nmodel can only attend to the past tokens.\ninput sequence is “The sky is blue,” the model would predict\nLLaMA-2 models come in three sizes: 7, 13, and 70 billion parameters.\nOn top of that, all model sizes come with a “chat” version, called LLaMA-2-\ncompared to the base model LLama-2.\nIn the context of LLMs, the difference between base models\nand “chat” or assistant models is primarily in their training\nBase models: These models are trained on vast amounts\nAssistant models: These models start as base LLMs but\ninclude instructions and the model’s attempts to follow\nthe model, making it better at being helpful, honest, and\nexample, the assistant model GPT-3.5 Turbo (the model\ncompletion model GPT-3.\nIn essence, while base models provide a broad understanding\nof language, assistant models are optimized to follow\n1. Supervised fine-tuning: This step involves fine-tuning the model on\nthe model outputs, and a loss function that encourages diversity and\nmodel.\nhttps://ai.meta.com/resources/models-and-libraries/llama/)\nTo access the model, you need to submit a request on Meta’s website (the\nform is available at https://ai.meta.com/resources/models-\nModel code\nModel weights\nModel Card",
      "keywords": [
        "model",
        "LLMs",
        "REST API",
        "human",
        "Claude",
        "human feedback",
        "CAI",
        "tokens",
        "principles",
        "Open LLM Leaderboard",
        "base models",
        "LLM",
        "assistant models",
        "context",
        "CAI technique"
      ],
      "concepts": [
        "model",
        "uses",
        "llama",
        "generate",
        "generation",
        "generating",
        "output",
        "human",
        "tokens",
        "evaluating"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 13,
          "title": "",
          "score": 0.72,
          "base_score": 0.57,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 16,
          "title": "",
          "score": 0.706,
          "base_score": 0.556,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 15,
          "title": "",
          "score": 0.703,
          "base_score": 0.553,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 53,
          "title": "",
          "score": 0.693,
          "base_score": 0.543,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 51,
          "title": "",
          "score": 0.639,
          "base_score": 0.489,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "model",
          "models",
          "llama",
          "base",
          "assistant models"
        ],
        "semantic": [],
        "merged": [
          "model",
          "models",
          "llama",
          "base",
          "assistant models"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.33679154866636996,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.496348+00:00"
      }
    },
    {
      "chapter_number": 15,
      "title": "Segment 15 (pages 119-127)",
      "start_page": 119,
      "end_page": 127,
      "summary": "quality of the training dataset.\nmodels like GPT-4 with trillions of parameters are extremely heavy, both in\nFalcon LLM is an open-source model launched by Abu Dhabi’s Technology\nonly transformer, trained on 1 trillion tokens, and it has 40 billion parameters\nInstruct models are specialized for short-form instruction\nInstruction following is a task where the model\nParis.” The Instruct fine-tuned models are trained on a large\nSo, the question might be: how can a model with “only” 40 billion\ndataset, called RefinedWeb, has been released by TII under the Apache-2.0\nthe training compute budget of GPT-3 and PaLM-62B, respectively.\nThe third and last open-source model series we are going to cover is Mistral,\ndeveloped by Mistral AI, a company founded in April 2023 by a team of AI\nThe Mistral model, particularly the Mistral-7B-v0.1, is a decoder-only\ntransformer with 7.3 billion parameters, designed for generative text tasks.\nallowed it to outperform other models in benchmarks.\nextends the model’s attention beyond a fixed window size,\nthe model to access tokens at a greater distance and manage\nThe model also provides a variant that was fine-tuned for general-purpose\nThis variant is called Mistral-7B-instruct, which outperformed\nLike many other open-source models, Mistral can be consumed and\ndownloaded via Hugging Face Hub. Note\nIn February 2024, Mistral AI and Microsoft entered a multi-\nyear partnership to accelerate AI innovation.\nand deployment of Mistral AI’s LLMs. Mistral AI’s models,\nincluding their advanced model, Mistral Large, will be\navailable to customers through Azure AI Studio and Azure\nMachine Learning model catalog.\nexpand Mistral AI’s reach to global markets and foster\nthree models:\nFalcon LLM\nMistral AI\ncommercial license\nces/models-\nces/models-\nBeyond language models\nSo far, we have only been covering language-specific foundation models as\nmodels that can handle data that is different from text, which can be\nHere, you can find some examples of large foundation models (LFMs) on\nWhisper: It is a general-purpose speech recognition model developed\nmultitasking model that can perform multilingual speech recognition,\nmodel that takes text prompts and outputs a set of four images that\nparameter version of GPT-3 trained on a dataset of text-image pairs.\nThe involved models might be the following:\nAn LLM, such as Falcon-7B-instruct, with a web plugin, will\nWe can also ask the same model\nto generate a prompt that will ask the following model to generate a",
      "keywords": [
        "Falcon LLM",
        "model",
        "Mistral",
        "LLM",
        "Face Hub",
        "Falcon",
        "Hugging Face Hub",
        "LLMs",
        "dataset",
        "billion parameters",
        "Instruct",
        "training",
        "version",
        "Mistral model",
        "Azure"
      ],
      "concepts": [
        "model",
        "mistral",
        "language",
        "data",
        "falcon",
        "instruct",
        "instructions",
        "instruction",
        "inference",
        "dataset"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 14,
          "title": "",
          "score": 0.703,
          "base_score": 0.553,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 53,
          "title": "",
          "score": 0.643,
          "base_score": 0.493,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 16,
          "title": "",
          "score": 0.631,
          "base_score": 0.481,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 13,
          "title": "",
          "score": 0.617,
          "base_score": 0.467,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 54,
          "title": "",
          "score": 0.554,
          "base_score": 0.404,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "mistral",
          "mistral ai",
          "model",
          "models",
          "instruct"
        ],
        "semantic": [],
        "merged": [
          "mistral",
          "mistral ai",
          "model",
          "models",
          "instruct"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3166127212645504,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.496421+00:00"
      }
    },
    {
      "chapter_number": 16,
      "title": "Segment 16 (pages 128-135)",
      "start_page": 128,
      "end_page": 135,
      "summary": "Comme illustré sur la figure 1 un bolométre est constitué d'un absorbour qui recoit le rayon:\nGPT-4 To answer question|‘.a, we need toexpress the temperature T(x) in every point of the\nT(x) = (Tb- TO)* (x /L)+TO\nGPT-4 is just one example of a large multimodal model (LMM), and it is\nproprietary and open-source LLMs. The following are some factors and\nSize and performance: We saw that more complex models (that\nNevertheless, the larger the model, the more\nCost for model consumption: This refers to the fee we pay to\nconsume the model.\nProprietary models like GPT-4 or Claude 2\nOn the other hand, open-source models like\nCost for model hosting: This refers to your hosting strategy.\nTypically, proprietary models are hosted in a private or public\nWith open-source models, we typically\nneed to provide our own infrastructure, since those models can be\nOf course, the larger the model, the more\nIn the context of open-source models, another\noption to consume those models is that of using\nbefore deciding which model to adopt.\nIn fact, not all models are\nAll open-source models\nWhen it comes to proprietary models, not all\nLLMs can be fine-tuned: for example, OpenAI’s GPT-3.5 can be\nthe model from scratch.\nsource LLMs and simply re-train them on custom datasets.\nis not the case when we work with proprietary LLMs. Domain-specific capabilities: We saw that the most popular way of\nuse a model that is a top performer in one specific benchmark, rather\nOn the other hand, if you need a model that encompasses all of\nPicking a domain-specific model is also a way to make some savings\nin terms of model complexity.\nyou to use a relatively small model (for example, a LlaMA-7B-\nis a plethora of models that have been trained on domain-\nbetween two advanced language models for their next-generation customer\nmodel that can handle diverse customer queries, provide accurate technical\nLLama 2: Created by Meta AI, LLama 2 is an open-source model\nPerformance: TechGen evaluates the models’ performance, particularly\nconditions, GPT-4 comes with a cost, which TechGen must factor into\nBased on these considerations, TechGen opts for GPT-4, swayed by its\nThe decision is also influenced by GPT-4’s image processing feature,\nTechGen’s choice of GPT-4 over LLama 2 is driven by the need for a high-\nperforming, versatile language model that can scale with their growing\ndeciding which models to embed within applications.",
      "keywords": [
        "User Answer questionli.a",
        "model",
        "LLMs",
        "User Answer",
        "open-source models",
        "LLM",
        "TechGen",
        "Proprietary models",
        "performance",
        "Hugging Face",
        "Cost",
        "capabilities",
        "solve complex mathematical",
        "complex mathematical problems",
        "open-source"
      ],
      "concepts": [
        "model",
        "llms",
        "performance",
        "performer",
        "parameters",
        "specific",
        "technical",
        "custom",
        "source",
        "useful"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 14,
          "title": "",
          "score": 0.706,
          "base_score": 0.556,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 15,
          "title": "",
          "score": 0.631,
          "base_score": 0.481,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 13,
          "title": "",
          "score": 0.59,
          "base_score": 0.44,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 53,
          "title": "",
          "score": 0.581,
          "base_score": 0.431,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 12,
          "title": "",
          "score": 0.447,
          "base_score": 0.447,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "models",
          "techgen",
          "model",
          "open source",
          "source"
        ],
        "semantic": [],
        "merged": [
          "models",
          "techgen",
          "model",
          "open source",
          "source"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.24211343994404538,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.496471+00:00"
      }
    },
    {
      "chapter_number": 17,
      "title": "Segment 17 (pages 136-143)",
      "start_page": 136,
      "end_page": 143,
      "summary": "https://cdn.openai.com/papers/gpt-4.pdf\nPrompt Engineering\nIn Chapter 2, we introduced the concept of prompt engineering as the\nprocess of designing and optimizing prompts – the text input that guides the\nbehavior of a large language model (LLM) – for LLMs for a wide variety\nLLM performance, prompt engineering is a crucial activity while designing\nLLM-powered applications.\nprompt engineering, starting from basic approaches up to advanced\nbuild functional and solid prompts for your LLM-powered applications,\nIntroduction to prompt engineering\nBasic principles of prompt engineering\nAdvanced techniques of prompt engineering\nTo complete the tasks in this chapter, you will require the following:\nWhat is prompt engineering?\nA prompt is a text input that guides the behavior of an LLM to generate a\nPrompt engineering is the process of designing effective prompts that elicit\nhigh-quality and relevant output from LLMs. Prompt engineering requires\ninstruct the same model to perform three different tasks:\nFigure 4.1: Example of prompt engineering to specialize LLMs\nPrinciples of prompt\nmodel used, the goal of the application, the supporting infrastructure, and so\nThe principle of giving clear instructions is to provide the model with\nare no instructions in the provided text, the model should inform us about\nleverage OpenAI’s GPT-3.5-turbo model.\n2. To initialize the model, I used the openai Python library and set the\nmodel=\"gpt-3.5-turbo\", # engine = \"deployment_name\nwe want our model to behave, and instructions (or query), where the\n3. Then, it takes the user’s query (in this case, the text instructions).\nthis scenario, I set the two variables system_message and instructions\nText:\n5. Note that if we pass the model another text that does not contain any\nmodel=\"gpt-3.5-turbo\", # engine = \"deployment_name\nAs there are no instructions provided in the text you have \nBy giving clear instructions, you can help the model understand what you\nAs discussed earlier, prompt engineering is a technique that involves\nthe tasks are too complex or ambiguous for a single prompt to handle, and it",
      "keywords": [
        "Prompt Engineering",
        "Prompt",
        "Model",
        "Engineering",
        "text",
        "instructions",
        "LLMs",
        "LLM",
        "Face Inference Endpoint",
        "OpenAI",
        "message",
        "Hugging Face Inference",
        "tasks",
        "Clear instructions",
        "system"
      ],
      "concepts": [
        "prompt",
        "models",
        "openai",
        "engineering",
        "engine",
        "llm",
        "output",
        "text",
        "follows",
        "tasks"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 21,
          "title": "",
          "score": 0.639,
          "base_score": 0.639,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 22,
          "title": "",
          "score": 0.586,
          "base_score": 0.586,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 3,
          "title": "",
          "score": 0.581,
          "base_score": 0.581,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 19,
          "title": "",
          "score": 0.553,
          "base_score": 0.553,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 10,
          "title": "",
          "score": 0.507,
          "base_score": 0.507,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "engineering",
          "prompt engineering",
          "prompt",
          "instructions",
          "model"
        ],
        "semantic": [],
        "merged": [
          "engineering",
          "prompt engineering",
          "prompt",
          "instructions",
          "model"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.34022032179157474,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:11.496529+00:00"
      }
    },
    {
      "chapter_number": 18,
      "title": "Segment 18 (pages 144-151)",
      "start_page": 144,
      "end_page": 151,
      "summary": "Here are some examples of splitting complex tasks into subtasks:\nText summarization: A complex task that involves generating a\nGenerating the text in the target language from the intermediate\nPoem generation: A creative task that involves producing a poem that\nGenerating a title and a topic for the poem based on the user’s\nCode generation: A technical task that involves producing a code\nLet’s consider the following example in Python, where we will ask our\nmodel to generate a summary of an article:\n1. We will leverage OpenAI’s GPT-3.5-turbo model in a manner similar to\n2. Let’s set both the system_message and article variables as follows\nGenerate a paragraph summary of the current article co\nprint(response['choices'][0]['message']['content'])\nfact that we prompted the model to split the task into subtasks “forced” it to\nall models.\nnevertheless, it does not address one of the main risks of LLM-generated\nmodel to output wrong content to the user, yet in a very convincing way.\nspecifying in the prompt to support the LLM’s answer with some reflections\nand justification could prompt the model to recover from its actions.\nFurthermore, asking for justification might be useful also in case of answers\nexample, let’s say we want our LLM to solve riddles.\nProvide a clear justification of your answer and the reasoning b\nanswer and also provide its reasoning.\nprint(response['choices'][0]['message']['content'])\nThe answer to this riddle is a clock.\nHence, a clock is the correct answer to this riddle.\nAnother example might be that of asking the model to\ngenerate multiple outputs – along with their justifications – to evaluate\ndifferent reasoning techniques and prompt the best one in the metaprompt.\nspecifically, the possibility of generating multiple outputs and then picking\nGenerate many outputs, then\nuse the model to pick the best\ntheir generations.\n(in other words, if the model is unlucky), the LLM will keep generating\nRather than generating just one response, we can\nprompt the model to generate multiple responses, and then pick the one that\n1. Generating multiple responses to the user’s query\nLet’s see an example, following up from the riddles examined in the\nGiven a riddle, you have to generate three answers to the riddle\nFor each answer, be specific about the reasoning you made.\nIn this case, I’ve prompted the model to generate three answers to the riddle,\nprint(response['choices'][0]['message']['content'])\nAnswer 1: A clock.\nAs you can see, the model selected the most plausible answer along with a\nmight seem similar responses; however, the model specified that “watch” is\nAs discussed earlier, forcing the model to tackle a problem with different",
      "keywords": [
        "model",
        "answer",
        "message",
        "task",
        "system",
        "riddle",
        "LLM",
        "content",
        "subtasks",
        "generating",
        "article",
        "response",
        "Text",
        "system message",
        "message content"
      ],
      "concepts": [
        "article",
        "generating",
        "generation",
        "generate",
        "generations",
        "riddles",
        "response",
        "messages",
        "reasoning",
        "model"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 19,
          "title": "",
          "score": 0.784,
          "base_score": 0.634,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 20,
          "title": "",
          "score": 0.672,
          "base_score": 0.522,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 53,
          "title": "",
          "score": 0.511,
          "base_score": 0.511,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 22,
          "title": "",
          "score": 0.507,
          "base_score": 0.507,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 17,
          "title": "",
          "score": 0.491,
          "base_score": 0.491,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "generating",
          "answer",
          "riddle",
          "message",
          "article"
        ],
        "semantic": [],
        "merged": [
          "generating",
          "answer",
          "riddle",
          "message",
          "article"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3549251955175428,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.496590+00:00"
      }
    },
    {
      "chapter_number": 19,
      "title": "Segment 19 (pages 152-159)",
      "start_page": 152,
      "end_page": 159,
      "summary": "Model Prompt Engineering for Complex Summarization, John Stewart (a\nthe prompt can help the model overcome its inner recency bias.\nFor example, if\nmodel may only focus on the last few messages and\nThis can help the model focus on each step and avoid getting\nAnother way to overcome recency bias with prompt engineering\nThis can help remind the model of what it is\nFor instance, let’s say we want our model to output the sentiment of a\nsure that the model will output the sentiment in lowercase and without\nLet’s consider the following example (the conversation is truncated, but you\nkey instruction is that of having as output only the sentiment in lowercase\nIn this scenario, we have key instructions before the conversation, so let’s\nmodel=\"gpt-3.5-turbo\", # engine = \"deployment_name\".\n{\"role\": \"user\", \"content\": conversation},\nThe model didn’t follow the instruction of having only lowercase letters.\nLet’s try to repeat the instruction also at the end of the prompt:\nAgain, let’s invoke our model with the updated system_message:\nmodel=\"gpt-3.5-turbo\", # engine = \"deployment_name\".\n{\"role\": \"user\", \"content\": system_message},\nAs you can see, now the model was able to provide exactly the output we\ncase, having the main instructions at the beginning might induce the model\nseparating instructions, examples, and desired output.\nLet’s consider, for example, a metaprompt that aims at instructing the model\nto translate user’s tasks into Python code, providing an example to do so:\n---User Output---\n<===END EXAMPLE\nIn the above example, we’ve used delimiters to both specify the beginning\nand end of an example for a one-shot learning approach and, within the\nexample, specify the Python code snippet.\nmodel=\"gpt-3.5-turbo\", # engine = \"deployment_name\".\nfollowing section, we are going to see some advanced techniques for prompt\ntext interaction with the model.\nThis is an example and evidence of how the concept of few-shot learning –\nwhich means providing the model with examples of how we would like it to\nrespond – is a powerful technique that enables model customization without\nFor example, let’s say we want our model to generate a tagline for a new\nLet’s see how our model will handle this request:\nmodel=\"gpt-3.5-turbo\", # engine = \"deployment_name\".\n{\"role\": \"user\", \"content\": product_name},\nThis is extremely useful when you want your model\nLet’s look at another example.\nLet’s say we want to develop a model that\nexamples of texts with different sentiments, alongside the output we would",
      "keywords": [
        "Model",
        "message",
        "Prompt",
        "system",
        "response",
        "output",
        "user",
        "content",
        "conversation",
        "Python",
        "message content",
        "sentiment",
        "delimiters",
        "end",
        "recency bias"
      ],
      "concepts": [
        "responses",
        "messages",
        "let",
        "model",
        "user",
        "output",
        "prompt",
        "conversation",
        "conversations",
        "instructions"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 18,
          "title": "",
          "score": 0.784,
          "base_score": 0.634,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 20,
          "title": "",
          "score": 0.629,
          "base_score": 0.479,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 17,
          "title": "",
          "score": 0.553,
          "base_score": 0.553,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 22,
          "title": "",
          "score": 0.49,
          "base_score": 0.49,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 26,
          "title": "",
          "score": 0.452,
          "base_score": 0.452,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "model",
          "example",
          "let",
          "user content",
          "deployment_name"
        ],
        "semantic": [],
        "merged": [
          "model",
          "example",
          "let",
          "user content",
          "deployment_name"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3059833241756088,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.496643+00:00"
      }
    },
    {
      "chapter_number": 20,
      "title": "Segment 20 (pages 160-167)",
      "start_page": 160,
      "end_page": 167,
      "summary": "provide our model with just two examples for each label:\nYou can use the following texts as examples:\nNow, we want to test the performance of our model over a sample of 10\nFigure 4.3: Output of a GPT-3.5 model with few-shot examples\nis just an example of what you can achieve – in terms of model\nIntroduced in the paper Chain-of-Thought Prompting Elicits Reasoning in\nLarge Language Models by Wei et al., chain of thought (CoT) is a\nreasoning steps.\nIt also encourages the model to explain its reasoning,\nLet’s say that we want to prompt our LLM to solve generic first-degree\nTo solve a generic first-degree equation, follow these steps:\n3. **Simplify:** Simplify both sides of the equation as much as \nequation = \"3x + 5 = 11\"\nprint(response['choices'][0]['message']['content'])\n6. Express the Solution: The solution to the equation 3x + 5 = 11\nAs you can see, the model clearly followed the seven steps specified in the\nWith CoT, we are prompting the model to generate intermediate reasoning\nLanguage Models by Yao et al., ReAct (Reason and Act) is a general\nparadigm that combines reasoning and acting with LLMs. ReAct prompts\nthe language model to generate verbal reasoning traces and actions for a\nreasoning and quickly adapt its action plan based on external information.\nFor example, you can prompt the language model to answer a question by\nfirst reasoning about the question, then performing an action to send a query\nto the web, then receiving an observation from the search results, and then\ncontinuing with this thought, action, observation loop until it reaches a\nlanguage model to generate intermediate reasoning steps for a task, while\nReAct prompts the language model to generate intermediate reasoning steps,\nactions, and observations for a task.\nFor example, let’s say we want to ask our model for some up-to-date\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain.chat_models import ChatOpenAI\nmodel = ChatOpenAI(\nagent_executor = initialize_agent(tools, model, agent=AgentType.\nprecompiled prompt that follows the ReAct approach.\nprint(agent_executor.agent.llm_chain.prompt.template)\nAnswer the following questions as best you can.\nSearch: useful for when you need to answer questions about curren\n(this Thought/Action/Action Input/Observation can repeat N ti\n>Action: Search Action Input: \"Latest updates on Italian male cli\nObservation/Thought/Action until it reached the conclusion.\nexample of how prompting a model to think step by step and explicitly",
      "keywords": [
        "model",
        "action",
        "Reasoning",
        "Action Input",
        "language model",
        "text",
        "thought",
        "equation",
        "input",
        "small training set",
        "intermediate reasoning steps",
        "answer",
        "agent",
        "reasoning steps",
        "Solution"
      ],
      "concepts": [
        "reasoning",
        "thought",
        "text",
        "prompting",
        "searches",
        "search",
        "actions",
        "model",
        "steps",
        "react"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 18,
          "title": "",
          "score": 0.672,
          "base_score": 0.522,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 19,
          "title": "",
          "score": 0.629,
          "base_score": 0.479,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 17,
          "title": "",
          "score": 0.495,
          "base_score": 0.495,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 22,
          "title": "",
          "score": 0.48,
          "base_score": 0.48,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 4,
          "title": "",
          "score": 0.437,
          "base_score": 0.437,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "reasoning",
          "action",
          "steps",
          "thought",
          "equation"
        ],
        "semantic": [],
        "merged": [
          "reasoning",
          "action",
          "steps",
          "thought",
          "equation"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3177833177038711,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.496698+00:00"
      }
    },
    {
      "chapter_number": 21,
      "title": "Segment 21 (pages 168-175)",
      "start_page": 168,
      "end_page": 175,
      "summary": "following chapters, we are going to see concrete applications of this\nway for a new category of applications, infused with LLMs. We started with an introduction to the concept of prompt engineering and\nWhat is prompt engineering?:\nPrompt engineering techniques:\nhttps://blog.mrsharm.com/prompt-engineering-\nPrompt engineering principles:\nprompt-engineering?pivots=programming-language-\nLarge Language Model Prompt Engineering for Complex\nt-summary-prompt-engineering/\nYour Applications\nwe can leverage large language models (LLMs) to build powerful AI\napplications.\nDeveloping LLM-powered applications is becoming a key factor for\nLLMs within applications.\nLlamaIndex, and LangChain.\nLangChain and use its modules to build hands-on examples.\nLLM-powered applications using LangChain and open-source Hugging Face\nA brief note about LangChain\nGetting started with LangChain\ninstalled: langchain, python-dotenv, huggingface_hub, google-\nA brief note about LangChain\nLangChain.\nLangChain was released, introducing a new organization of packages and\nhttps://python.langchain.com/docs/get_started/intro\nThere are three packages you can install to start using LangChain:\nwhole LangChain ecosystem.\nlangchain-community: This contains all third-party integrations.\nLangChain applications:\nLangChain applications into production environments.\nfor evaluating language models and AI applications.\nleverage its native integration with the end-to-end LangChain development\nBefore we start working with LangChain, it is important to\nLangChain framework and, more generally, in the landscape of LLM\nLangChain\nAs introduced in Chapter 2, LangChain is a lightweight framework meant to\nwithin applications.",
      "keywords": [
        "prompt engineering",
        "LangChain",
        "applications",
        "engineering",
        "prompt",
        "Model Prompt Engineering",
        "LLMs",
        "Language Model Prompt",
        "Language Models",
        "Prompt engineering techniques",
        "Models",
        "Language",
        "Hugging Face",
        "packages",
        "Prompt engineering principles"
      ],
      "concepts": [
        "langchain",
        "prompting",
        "python",
        "development",
        "packages",
        "language",
        "applications",
        "application",
        "model",
        "llms"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 3,
          "title": "",
          "score": 0.888,
          "base_score": 0.738,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 11,
          "title": "",
          "score": 0.843,
          "base_score": 0.693,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 37,
          "title": "",
          "score": 0.692,
          "base_score": 0.542,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 4,
          "title": "",
          "score": 0.67,
          "base_score": 0.52,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 12,
          "title": "",
          "score": 0.649,
          "base_score": 0.499,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "engineering",
          "langchain",
          "prompt engineering",
          "prompt",
          "applications"
        ],
        "semantic": [],
        "merged": [
          "engineering",
          "langchain",
          "prompt engineering",
          "prompt",
          "applications"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3736862803169403,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.496757+00:00"
      }
    },
    {
      "chapter_number": 22,
      "title": "Segment 22 (pages 176-183)",
      "start_page": 176,
      "end_page": 183,
      "summary": "Models and prompts\nGPT-3 model (you can retrieve your OpenAI API key at\nfrom langchain.llms import OpenAI\nmodel.\nto generate a prompt for a language model.\nFor example, suppose you want to use a language model to generate a\ncompletion and chat models.\nA completion model is a type of LLM that takes a text\ninput and generates a text output, which is called a\nThe completion model tries to continue\na completion model can generate summaries,\nA chat model takes a list of messages as input, where\nThe chat model tries to generate\nmodels is that completion models expect a single text\ninput as a prompt, while chat models expect a list of\nlanguage model.\nA prompt is a text input that guides the language\nmodel to produce a desired output.\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal gene\nhttps://python.langchain.com/docs/modules/model\nadditional non-parametric knowledge we want to provide the model with.\nhttps://python.langchain.com/docs/modules/data_connection/)\nloading data as documents from a configured source.\nfrom langchain.document_loaders.csv_loader import CSVLoade\ntext splitters is to make it easier to split documents into chunks that are\nlength of the chunk (for example, by number of characters).\nFor example, let’s split a document using the\nfrom langchain.text_splitter import RecursiveCharacterTextS\ntexts = text_splitter.create_documents([mountain])\nText embedding models: In Chapter 1, in the Under the hood of an\nTo get started with embedding, you will need an embedding model.\nThen, LangChain offers the Embedding class with two main modules,\nFor example, let’s consider the embeddings using the OpenAI\nembedding model text-embedding-ada-002 (for more details about\nOpenAI embedding models, you can refer to the official\nfrom langchain.embeddings import OpenAIEmbeddings\nembeddings_model = OpenAIEmbeddings(model ='text-embedding\nembeddings = embeddings_model.embed_documents(\nembedded_query = embeddings_model.embed_query(\"What was the",
      "keywords": [
        "model",
        "text",
        "Prompt",
        "OpenAI",
        "completion model",
        "language model",
        "output",
        "embedding",
        "Document",
        "completion",
        "LLMs",
        "llm",
        "OpenAI embedding model",
        "LangChain",
        "Text embedding models"
      ],
      "concepts": [
        "document",
        "documentation",
        "models",
        "text",
        "langchain",
        "prompts",
        "openai",
        "embedding",
        "importing",
        "output"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 17,
          "title": "",
          "score": 0.586,
          "base_score": 0.586,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 21,
          "title": "",
          "score": 0.546,
          "base_score": 0.546,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 13,
          "title": "",
          "score": 0.51,
          "base_score": 0.51,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 40,
          "title": "",
          "score": 0.51,
          "base_score": 0.51,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 24,
          "title": "",
          "score": 0.509,
          "base_score": 0.509,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "embedding",
          "completion",
          "model",
          "text",
          "langchain"
        ],
        "semantic": [],
        "merged": [
          "embedding",
          "completion",
          "model",
          "text",
          "langchain"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.384184133643768,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:11.496834+00:00"
      }
    },
    {
      "chapter_number": 23,
      "title": "Segment 23 (pages 184-192)",
      "start_page": 184,
      "end_page": 192,
      "summary": "Vector stores: A vector store (or VectorDB) is a type of database that\nBy using embeddings, vector stores can\nAs an example, let’s leverage the FAISS vector store, which has been\nA retriever does not need to store the documents\nThe difference between a retriever and a vector store is that a retriever\nany method to find relevant documents, while a vector store relies on\nvector store needs to store the data itself.\nHowever, a vector store can also be used as the backbone of a retriever\nif the data is embedded and indexed by a vector store.\nretriever can use the vector store to perform a similarity search over\nvector store retriever.\nFor example, let’s consider the FAISS vector store we previously\nMemory\nIn the context of LLM-powered applications, memory allows the application\nability to store users’ interactions into a memory variable and use this\nmemory as context while addressing follow-up questions.\nLangChain offers several modules for designing your memory system within\nThe first step to do with your memory system is to actually store your human\nThen, when it comes to defining how to query your memory system, there\nare various memory types you can leverage:\nConversation buffer memory: This is the “plain vanilla” memory type\nConversation buffer window memory: It is identical to the previous\nEntity memory: Entity memory is a feature of LangChain that allows\nentity over time by storing the extracted facts in a memory store.\nmemory store can be accessed and updated by the language model\nConversation knowledge graph memory: This type of memory uses a\nknowledge graph to recreate memory.\nknowledge graph can store and integrate data from\nlike GeoNames and WordNet. You can use this type of memory to save the input and output of each\nConversation summary memory: When it comes to longer\nconversations to be stored, this type of memory can be very useful,\nConversation summary buffer memory: This type of memory\ncombines the ideas behind buffer memory and conversation summary\nmemory.\nconversation buffer memory) it compiles them into a summary and uses\nConversation token buffer memory: It is similar to the previous one,\ninteractions, this type of memory uses token lengths rather than the\nnumber of interactions (as occurs in summary buffer memory).\nVector store-backed memory: This type of memory leverages the\nconcepts of embeddings and vector stores previously covered.\ndifferent from all the previous memories since it stores interactions as\nvectors, and then retrieves the top K most similar texts every time it is\nLangChain provides specific modules for each of those memory types.\nconsider an example with the conversation summary memory, where we will\nfrom langchain.memory import ConversationSummaryMemory, ChatMess\nAs you can see, the memory summarized the conversation, leveraging the\nThere is no recipe to define which memory to use within your applications;\nFor example, a knowledge graph memory is useful for\nconversation summary buffer memory could be suitable for creating",
      "keywords": [
        "vector store",
        "Memory",
        "vector",
        "store",
        "FAISS vector store",
        "conversation",
        "buffer memory",
        "documents",
        "retriever",
        "similarity",
        "vector store retriever",
        "Conversation summary memory",
        "query",
        "type",
        "knowledge"
      ],
      "concepts": [
        "memory",
        "memories",
        "conversational",
        "conversations",
        "langchain",
        "vector",
        "stores",
        "storing",
        "similarity",
        "documents"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 33,
          "title": "",
          "score": 0.623,
          "base_score": 0.473,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 11,
          "title": "",
          "score": 0.603,
          "base_score": 0.453,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 3,
          "title": "",
          "score": 0.501,
          "base_score": 0.351,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 12,
          "title": "",
          "score": 0.48,
          "base_score": 0.33,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 4,
          "title": "",
          "score": 0.455,
          "base_score": 0.305,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "memory",
          "store",
          "vector",
          "vector store",
          "conversation"
        ],
        "semantic": [],
        "merged": [
          "memory",
          "store",
          "vector",
          "vector store",
          "conversation"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.22921827310840154,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.496886+00:00"
      }
    },
    {
      "chapter_number": 24,
      "title": "Segment 24 (pages 193-201)",
      "start_page": 193,
      "end_page": 201,
      "summary": "This chain takes multiple input variables, uses PromptTemplate to\nprompt = PromptTemplate(template=template, input_variables=\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nllm_chain.predict(sentence=\"the cat is on the table\", langu\nRouterChain: This is a type of chain that allows you to route the input\nvariables to different chains based on some conditions.\nYou can also specify the default chain to use if none of the conditions\nFor example, you can use this chain to create a chatbot that can handle\nof how the chain reacts to two different user’s queries:\nprint(chain.run(\"I want to book a table for tonight\"))\nSequentialChain: This is a type of chain that allows you to execute\nand how they pass their outputs to the next chain.\nof a sequential chain, takes by default the output of one chain as the\ninput of the next chain.\nmodule to have more flexibility to set input and output among chains.\nfrom langchain.chains import LLMChain\nprompt_template = PromptTemplate(input_variables=[\"topic\"]\njoke_chain = LLMChain(llm=llm, prompt=prompt_template)\n.prompt_template = PromptTemplate(input_variables=[\"languag\ntranslator_chain = LLMChain(llm=llm, prompt=prompt_template\nfrom langchain.chains import SimpleSequentialChain\ntranslated_joke = overall_chain.run(\"Cats and Dogs\")\nTransformationChain: This is a type of chain that allows you to\ntransform the input variables or the output of another chain using some\nvalue, as well as specify the output format of the chain.\nfrom langchain.chains import TransformChain, LLMChain, Simp\nprompt = PromptTemplate(input_variables=[\"output_text\"], te\nllm_chain = LLMChain(llm=OpenAI(), prompt=prompt)\nsequential_chain.run(cats_and_dogs)\nOverall, LangChain chains are a powerful way to combine different\nchapter, we are going to see chains in action in concrete use cases, but before\nAgents\nsituation or the goal: in fact, while in a chain, the sequence of actions is\nBy itself, the agent\nfrom langchain.agents import AgentType, initialize_agent\nStructured input ReAct: This is an agent type that uses the ReAct\nThe agent can handle different types of input data, such as\nThe agent uses a language model and a\nOpenAI Functions: This is an agent type that uses the OpenAI\nFunctions API to access various language models and tools from\nThe agent can use different functions, such as GPT-3, Codex,\nThe agent uses a language model and a\nConversational: This is an agent type that uses a language model to\nThe agent uses a language\nSelf ask with search: This is an agent type that uses a language model\nReAct document store: This is an agent type that uses the ReAct\nThe agent can handle different types of documents,\nPlan-and-execute agents: This is an experimental agent type that uses\na language model to choose a sequence of actions to take based on the\nThe agent can use different tools or models to\nThe agent uses a language model and a\nprompt to generate plans and actions and then uses AgentExecutor to\nLangChain agents are pivotal whenever you want to let your LLMs interact",
      "keywords": [
        "chain",
        "agent",
        "input",
        "language model",
        "language",
        "prompt",
        "agent type",
        "output",
        "LLM",
        "model",
        "Finished chain",
        "template",
        "type",
        "OpenAI",
        "generate"
      ],
      "concepts": [
        "chain",
        "input",
        "agents",
        "prompt",
        "reacts",
        "uses",
        "custom",
        "language",
        "action",
        "searching"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 27,
          "title": "",
          "score": 0.646,
          "base_score": 0.496,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 46,
          "title": "",
          "score": 0.6,
          "base_score": 0.6,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 21,
          "title": "",
          "score": 0.513,
          "base_score": 0.513,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 22,
          "title": "",
          "score": 0.509,
          "base_score": 0.509,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 11,
          "title": "",
          "score": 0.494,
          "base_score": 0.494,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "chain",
          "agent",
          "uses language",
          "agent type",
          "uses"
        ],
        "semantic": [],
        "merged": [
          "chain",
          "agent",
          "uses language",
          "agent type",
          "uses"
        ]
      },
      "topic_id": 6,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3181162513649519,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.496940+00:00"
      }
    },
    {
      "chapter_number": 25,
      "title": "Segment 25 (pages 202-210)",
      "start_page": 202,
      "end_page": 210,
      "summary": "access token you can leverage all the open-source LLMs available in\nfully managed infrastructure to host and consume your LLMs. So, let’s see how to start integrating LangChain with the Hugging Face Hub. Create a Hugging Face user\naccess token\nTo access the free Inference API, you will need a user access token, the\nactivate the user access token:\n2. Retrieve your user access token: Once you have your account, go to\nTokens.\nuse it to access Hugging Face models.\nFigure 5.4: Retrieving access tokens from the Hugging Face account (source:\n3. Set permissions: Access tokens enable users, applications, and\nRead: This allows tokens to provide read access to repositories\nWrite: In addition to read access, tokens with this role grant\nThis token is useful for activities like training models or updating\ntoken.\n4. Managing your user access token: Within your profile, you can create\nand manage multiple access tokens, so that you can also differentiate\nWith our user access token generated in the previous section, we have the\naccess, such as passwords, tokens, keys, and credentials.\nHUGGINGFACEHUB_API_TOKEN=\"your_user_access_token\".\nTo access these secrets in your Python code, use the python-dotenv library\nHere, you can see an example of how to retrieve your access token and set it\nand running the code as it is, simply changing the LangChain LLM\nHugging Face Hub and its model catalog, and how to use the available\nLangChain’s integration with OpenAI –\nhttps://python.langchain.com/docs/integrations/l\nLangChain’s prompt templates –\nhttps://python.langchain.com/docs/modules/model_\nhttps://python.langchain.com/docs/integrations/v\nhttps://python.langchain.com/docs/modules/chains\nLangChain’s agents –\nhttps://python.langchain.com/docs/modules/agents\nhttps://python.langchain.com/docs/expression_lan",
      "keywords": [
        "Hugging Face Hub",
        "Hugging Face",
        "Hugging Face account",
        "Face Hub",
        "access Hugging Face",
        "user access token",
        "Face",
        "access token",
        "Hugging",
        "access",
        "Hugging Face user",
        "token",
        "Hugging Face models",
        "Face Hub integration",
        "user access"
      ],
      "concepts": [
        "langchain",
        "tokens",
        "models",
        "llm",
        "managed",
        "manage",
        "llms",
        "python",
        "important",
        "integration"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 21,
          "title": "",
          "score": 0.578,
          "base_score": 0.428,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 11,
          "title": "",
          "score": 0.569,
          "base_score": 0.419,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 3,
          "title": "",
          "score": 0.565,
          "base_score": 0.415,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 4,
          "title": "",
          "score": 0.468,
          "base_score": 0.318,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 48,
          "title": "",
          "score": 0.449,
          "base_score": 0.449,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "access",
          "access token",
          "token",
          "face",
          "user access"
        ],
        "semantic": [],
        "merged": [
          "access",
          "access token",
          "token",
          "face",
          "user access"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2390452830803094,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.496998+00:00"
      }
    },
    {
      "chapter_number": 26,
      "title": "Segment 26 (pages 211-218)",
      "start_page": 211,
      "end_page": 218,
      "summary": "Building Conversational\nthis chapter, we will cover a step-by-step implementation of a conversational\nchapter, you will be able to set up your own conversational application\nconversational applications\nA conversational application is a type of software that can interact with users\nspeaking, a conversational application can use different modes of\nconversational application can also use different platforms, such as\nToday, conversational applications are being taken to the next level thanks to\nFinally, LLMs are also able to keep track of the conversation thanks to\nThe following image shows what the architecture of a conversational bot\nFigure 6.1: Sample architecture of a conversational bot\nThroughout this chapter, we will build from scratch a text conversational\nSo, let’s start with the basics behind a conversational app architecture.\nTo start with, let’s initialize our LLM and set the schema for our bot.\nschema refers to the type of messages the bot is able to receive.\nHuman Message: The user’s query\noutput = chat(messages)\nHere is the output:\nAs we’re creating a conversational bot with relatively short messages, in this\nLet’s first initialize our memory and chain (I’m keeping verbose = True so\nthat you can see the bot keeping track of previous messages):\nfrom langchain.memory import ConversationBufferMemory\nconversation = ConversationChain(\nconversation.run(\"Hi there!\")\nThe following is the output:\nThe following is a friendly conversation between a human and an A\nCurrent conversation:\nconversation.run(\"what is the most iconic place in Rome?\")\nThe following is a friendly conversation between a human and an A\nCurrent conversation:\nconversation.run(\"What kind of other events?\")\nThe following is a friendly conversation between a human and an A\nCurrent conversation:\nRather than running the conversation.run method at every interaction, I’ve\nthe whole conversation (you can find it in the book’s GitHub repository):\noutput = conversation({\"input\": query})",
      "keywords": [
        "conversational application",
        "Conversational",
        "bot",
        "Human",
        "Rome",
        "conversational bot",
        "memory",
        "output",
        "chain",
        "place in Rome",
        "application",
        "messages",
        "ConversationChain",
        "Finished chain",
        "iconic place"
      ],
      "concepts": [
        "conversational",
        "messaging",
        "messages",
        "roman",
        "output",
        "memory",
        "user",
        "different",
        "adding",
        "python"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 3,
          "title": "",
          "score": 0.594,
          "base_score": 0.594,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 2,
          "title": "",
          "score": 0.541,
          "base_score": 0.541,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 11,
          "title": "",
          "score": 0.527,
          "base_score": 0.527,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 40,
          "title": "",
          "score": 0.523,
          "base_score": 0.523,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 28,
          "title": "",
          "score": 0.521,
          "base_score": 0.521,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "conversational",
          "conversation",
          "bot",
          "conversational bot",
          "conversational application"
        ],
        "semantic": [],
        "merged": [
          "conversational",
          "conversation",
          "bot",
          "conversational bot",
          "conversational application"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.36312710458082376,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:11.497057+00:00"
      }
    },
    {
      "chapter_number": 27,
      "title": "Segment 27 (pages 219-226)",
      "start_page": 219,
      "end_page": 226,
      "summary": "User:  I'm planning a 1-day trip in Venice.\nuse a retriever to do the job.\nof chain leverages a retriever over the provided knowledge base that has the\nWith this goal in mind, we will use a sample Italy travel guide PDF\nDocument Loader: Since the document is in PDF format, we will use\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.text_splitter import RecursiveCharacterTextSplitt\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.memory import ConversationBufferMemory\nraw_documents = PyPDFLoader('italy_travel.pdf').load()\nqa_chain = ConversationalRetrievalChain.from_llm(llm, retriever=\nSystem: Use the following pieces of context to answer the users q\nwe want it to also use its parametric knowledge?\nour GlobeBotter agentic, meaning that we want to leverage the LLM’s\ncreate_retriever_tool: This method creates a custom tool that acts as\na retriever for an agent.\ncreate_conversational_retrieval_agent: This method initializes a\nconversational agent that is configured to work with retrievers and chat\nIt will need an LLM, a list of tools (in our case, the retriever),\nfrom langchain.agents.agent_toolkits import create_retriever_too\ntool = create_retriever_tool(\n\"Searches and returns documents regarding Italy.\"\nfrom langchain.agents.agent_toolkits import create_conversationa\nfrom langchain.chat_models import ChatOpenAI\nagent_executor = create_conversational_retrieval_agent(llm, tool\nquestions (I will report only the chain of thoughts and truncate the output,\nInvoking: `italy_travel` with `Pantheon`\nLet’s now try with a question not related to the document:\noutput = agent_executor({\"input\": \"what can I visit in India in \nLet’s implement it with LangChain’s tools.\nLangChain offers a pre-built tool that wraps SerpApi to\nadd the SerpApi tool to the previous one, so that the agent will be able to\npick the most useful tool to answer the question – or use no tool if not\nLet’s initialize our tools and agent (you learned about this and other\ntools = [\ndescription=\"useful for when you need to answer question\ncreate_retriever_tool(\n\"Searches and returns documents regarding Italy.\"\nagent_executor = create_conversational_retrieval_agent(llm, tool\nquestion, hence it is responding without invoking any tool.\nNote how the agent is invoking the search tool; this is due to the\nInvoking: `italy_travel` with `{'query': 'main attract\n[Document(page_content='ITALY\\nMINUBE TRAVEL GUIDE\\nTh\nNote how the agent is invoking the document retriever to provide the",
      "keywords": [
        "chain",
        "tool",
        "Finished chain",
        "agent",
        "retriever",
        "Italy",
        "Entering new AgentExecutor",
        "Entering",
        "documents",
        "output",
        "llm",
        "Document",
        "Finished",
        "question",
        "AgentExecutor chain"
      ],
      "concepts": [
        "chain",
        "tools",
        "documentation",
        "document",
        "user",
        "searches",
        "search",
        "entering",
        "serpapi",
        "memory"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 24,
          "title": "",
          "score": 0.646,
          "base_score": 0.496,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 40,
          "title": "",
          "score": 0.487,
          "base_score": 0.487,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 26,
          "title": "",
          "score": 0.481,
          "base_score": 0.481,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 4,
          "title": "",
          "score": 0.457,
          "base_score": 0.457,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 31,
          "title": "",
          "score": 0.455,
          "base_score": 0.455,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "tool",
          "retriever",
          "italy",
          "document",
          "langchain"
        ],
        "semantic": [],
        "merged": [
          "tool",
          "retriever",
          "italy",
          "document",
          "langchain"
        ]
      },
      "topic_id": 6,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.31960004123578667,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.497113+00:00"
      }
    },
    {
      "chapter_number": 28,
      "title": "Segment 28 (pages 227-234)",
      "start_page": 227,
      "end_page": 234,
      "summary": "using Streamlit.\nwith Streamlit\nStreamlit is a Python library that allows you to create and share web apps.\nduring the execution of a LangChain pipeline, such as tool start, tool end,\nThe class can also create and update Streamlit elements, such as containers,\nYou can use the Streamlit callback handler to create\nStreamlit apps that showcase the capabilities of LangChain and interact with\ntakes a user prompt and runs it through an agent that uses different tools and\nYou can use the Streamlit callback handler to\nimport streamlit as st\nuser_query = st.text_input(\n4. Setting Streamlit’s session states.\nSession state also persists across apps\nYou can use the session state API to initialize,\nGlobeBotter, we want two main states: messages and memory:\nif \"messages\" not in st.session_state:\nst.session_state[\"messages\"] = [{\"role\": \"assistan\nif \"memory\" not in st.session_state:\nst.session_state['memory'] = memory\nst.session_state[\"messages\"].\nStreamlit element called st.chat_message that displays a chat message\nfor msg in st.session_state[\"messages\"]:\nst.session_state.messages.append({\"role\": \"user\", \nst.chat_message(\"user\").write(user_query)\nwith st.chat_message(\"assistant\"):\nresponse = agent(user_query, callbacks=[st_cb]\nst.session_state.messages.append({\"role\": \"ass\nst.session_state.messages = []\nFigure 6.2: Front-end of GlobeBotter with Streamlit\nStreamlit app:\nimport streamlit as st\nfollows in your Streamlit app, making sure to set streaming=True while\nwith st.chat_message(\"assistant\"):\nresponse = llm.invoke(st.session_state.messages)\nst.session_state.messages.append(ChatMessage(role=\"assis\nhttps://github.com/langchain-ai/streamlit-\nagent/blob/main/streamlit_agent/basic_streaming.py.",
      "keywords": [
        "Streamlit",
        "Streamlit callback handler",
        "st.session",
        "state",
        "Streamlit callback",
        "user",
        "LangChain",
        "Streamlit app",
        "agent",
        "create",
        "app",
        "Session state",
        "messages",
        "import streamlit",
        "tool"
      ],
      "concepts": [
        "langchain",
        "tool",
        "callback",
        "agents",
        "llm",
        "role",
        "states",
        "search",
        "searches",
        "streaming"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 26,
          "title": "",
          "score": 0.521,
          "base_score": 0.521,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 11,
          "title": "",
          "score": 0.511,
          "base_score": 0.511,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 21,
          "title": "",
          "score": 0.491,
          "base_score": 0.491,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 24,
          "title": "",
          "score": 0.477,
          "base_score": 0.477,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 40,
          "title": "",
          "score": 0.448,
          "base_score": 0.448,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "streamlit",
          "st",
          "session_state",
          "st session_state",
          "messages"
        ],
        "semantic": [],
        "merged": [
          "streamlit",
          "st",
          "session_state",
          "st session_state",
          "messages"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2931586776627022,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:11.497163+00:00"
      }
    },
    {
      "chapter_number": 29,
      "title": "Segment 29 (pages 235-246)",
      "start_page": 235,
      "end_page": 246,
      "summary": "recommendation systems\nIt uses large datasets to develop models of users’ likes and interests, and then\nrecommends similar items to individual users.\nCollaborative filtering: This type of recommendation system uses the\nratings or feedback of other users who have similar preferences to the\nIt assumes that users who liked certain items in the past will\nUser-based collaborative filtering finds similar users to the\ntarget user and recommends items that they liked.\nones that the target user liked and recommends them.\nContent-based filtering: This type of recommendation system uses the\nfeatures or attributes of the items themselves to recommend items that\nare similar to the ones that the target user has liked or interacted with\nIt assumes that users who liked certain features of an item will\nlike other items with similar features.\npatterns of user behavior to make recommendations, content-based\nuser A liked movie X, which is a comedy with actor Y, then the\nrecommendations.\nrecommend videos based on both the ratings and views of other users\nKnowledge-based filtering: This type of recommendation system uses\nModern recommendation systems use machine learning (ML) techniques to\nmake better predictions about users’ preferences, based on the available data\nThis data can be acquired from factors like user ratings, clicks, and\nUser demographic data: This refers to personal information about\nKNN can be applied to recommendation systems in the context of\ncollaborative filtering, both user-based and item-based:\nUser-based KNN is a type of collaborative filtering, which uses the\nratings or feedback of other users who have similar tastes or\nItem-based KNN is another type of collaborative filtering, which uses\nthe attributes or features of the items to recommend similar items to the\nFor example, let’s consider the same users and their ratings for the\nusers), Charlie might also like The Hobbit.\nKNN is a popular technique in recommendation systems, but it has some\nbetween all pairs of items or users.\nCold-start problem: KNN struggles with new items or users that have\nrecommendations.\nrecommendation systems, such as matrix factorization.\nMatrix factorization is a technique used in recommendation systems to\nanalyze and predict user preferences or behaviors based on historical data.\npredict missing values in the user-item interaction matrix, which represents\nusers’ interactions with various items (such as movies, products, or books).\nHowever, not all users have rated all\nUser 1\nUser 2\nUser 3\nyou can predict the missing ratings and recommend movies that the users\ncollaborative filtering in recommendation systems.\n# Your user-movie rating matrix (replace with your actual data)\nuser_movie_matrix = np.array([\nU, s, V = np.linalg.svd(user_movie_matrix, full_matrices=False)\nIn this example, the U matrix contains user-related information, the s matrix\npredicted ratings to users.\nMatrix factorization enables recommendation\nMatrix factorization has been a widely used technique in recommendation\nnumber of users and items, since it efficiently captures latent factors even in\nwith new items or users that have limited or no interaction history.\nrecommendations for new items or users.\nData sparsity: As the number of users and items grows, the user-item\nLimited context: Matrix factorization typically only considers user-\nmodel user-item interactions by embedding users and items into\nrepresent user preferences and item characteristics.\narchitectures to predict ratings or interactions between users and items.",
      "keywords": [
        "recommendation systems",
        "user",
        "matrix",
        "Recommendation",
        "items",
        "data",
        "KNN",
        "systems",
        "matrix factorization",
        "similar",
        "Collaborative filtering",
        "filtering",
        "target user",
        "neural networks",
        "latent"
      ],
      "concepts": [
        "user",
        "data",
        "recommendation",
        "recommendations",
        "matrix",
        "likes",
        "similar",
        "based",
        "movies",
        "item"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 30,
          "title": "",
          "score": 0.848,
          "base_score": 0.698,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 58,
          "title": "",
          "score": 0.689,
          "base_score": 0.539,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 2,
          "title": "",
          "score": 0.543,
          "base_score": 0.393,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 6,
          "title": "",
          "score": 0.476,
          "base_score": 0.326,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 32,
          "title": "",
          "score": 0.434,
          "base_score": 0.434,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "items",
          "users",
          "matrix",
          "recommendation",
          "user"
        ],
        "semantic": [],
        "merged": [
          "items",
          "users",
          "matrix",
          "recommendation",
          "user"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.20741037770477916,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.497435+00:00"
      }
    },
    {
      "chapter_number": 30,
      "title": "Segment 30 (pages 247-255)",
      "start_page": 247,
      "end_page": 255,
      "summary": "sequential recommendations.\nlearn low-dimensional representations of users and items.\noriginal input data from the encoded representation.\nencode the input data into a latent space but also model the\nmeaningful representation of the input data in the latent space, which can be\nuseful for various tasks including feature extraction, data generation, and\nThese representations can then be used to make recommendations by\nconnections, to improve recommendations by learning from diverse\nlearning can be used to optimize recommendations over time, learning\nmaking them well suited for recommendation systems.\nFor example, a rating-prediction recommendation\nsystem will not be able to tackle a task where we need to recommend the top\nit is indeed the task-specific situation that LLMs and, more generally, Large\nadaptable to various tasks, depending on user’s prompts and instructions.\nHenceforth, extensive research in the field of recommendation systems is\nrecommendation systems\nRecommender systems in the Era of Large Language Models (LLMs) from\nrecommender system:\nPre-training: Pre-training LLMs for recommender systems is an\nand user preferences, and to adapt to different recommendation tasks\nAn example of a recommendation system LLM is P5,\nRecommendation as Language Processing (RLP): A\nrecommender systems using large language models\ncorpus and fine-tuned on recommendation tasks.\nrecommendations.\nP5 is based on the idea that LLMs can encode\ncan be adapted to different recommendation tasks with\ncustomize an LLM for recommendation systems might be fine-tuning.\nfor fine-tuning LLMs:\nweights based on task-specific recommendation datasets.\nrecommender systems is prompting.\nthree main techniques for prompting LLMs:\nIn-context learning enables LLMs to learn new tasks based on\nwhether a general-purpose LLM can tackle recommendation systems’\nThe application of LLMs within the recommendation system domain is\nIn the next section, we are going to implement our own recommendation\npowered recommendation\nNow that we have covered some theory about recommendation systems and\nrecommendation app, which will be a movie recommender system called\nthat we want our app to be able to address various recommendations tasks\nwith the recommendation system where we do not have the user’s preference\nFor this purpose, we will use the Movie recommendation data dataset,\nrecommendation-data.\nIn order to apply LLMs to our dataset, we first need to preprocess the data.\nVote_average: A rating from 1 to 10 for a given movie\nLLMs. Those elements are the movie title, overview, genres, and\n4. We tokenize the movie combined_info so that we will get better results\nmax_tokens = 8000 # the maximum for text-embedding-ada",
      "keywords": [
        "LLMs",
        "recommendation",
        "recommendation systems",
        "data",
        "Vote",
        "recommendation system LLM",
        "systems",
        "tasks",
        "user",
        "latent space",
        "movie",
        "LLM",
        "recommendation tasks",
        "input data",
        "make sequential recommendations"
      ],
      "concepts": [
        "recommendations",
        "recommendation",
        "data",
        "dataset",
        "learn",
        "tasks",
        "llms",
        "encoded",
        "encode",
        "important"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 29,
          "title": "",
          "score": 0.848,
          "base_score": 0.698,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 2,
          "title": "",
          "score": 0.741,
          "base_score": 0.591,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 58,
          "title": "",
          "score": 0.709,
          "base_score": 0.559,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 49,
          "title": "",
          "score": 0.643,
          "base_score": 0.493,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 6,
          "title": "",
          "score": 0.608,
          "base_score": 0.458,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "recommendation",
          "systems",
          "recommendations",
          "recommendation systems",
          "recommender"
        ],
        "semantic": [],
        "merged": [
          "recommendation",
          "systems",
          "recommendations",
          "recommendation systems",
          "recommender"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.32628397734704156,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.497494+00:00"
      }
    },
    {
      "chapter_number": 31,
      "title": "Segment 31 (pages 256-264)",
      "start_page": 256,
      "end_page": 264,
      "summary": "embeddings and start building our recommendation system.\nNow, we are going to build a LangChain RetrievalQA retriever, a chain\nreturns the top k most similar movies upon the user’s query, using cosine\nSo, let’s start building the chain:\n1. We are using only the movie overview as information input:\nquery = \"I'm looking for an animated action movie.\nmovie’s text embedding.\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_t\nquery = \"I'm looking for an animated action movie.\nalso retrieve the document sources:\nresult['source_documents'][0]\non top of similarity, to suggest a movie to the user.\nbe, for example, about the genre of a movie.\nFor example, let’s say we want to provide results featuring only those\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"s\nfollowing example, where we want to filter only results with a rating\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"s\nretriever=docsearch.as_retriever(search_kwargs={'filter\naction movie (thus filtering on the genre variable):\nfrom langchain.agents.agent_toolkits import create_retriever\nretriever = docsearch.as_retriever(return_source_documents =\n\"movies\",\n\"Searches and returns recommendations about movies.\"\nagent_executor = create_conversational_retrieval_agent(llm, \nresult = agent_executor({\"input\": \"suggest me some action mo\nInvoking: `movies` with `{'genre': 'action'}`\nHere are some action movies that you might enjoy:\nprint(qa.combine_documents_chain.llm_chain.prompt.template\nUse the following pieces of context to answer the question \nfrom langchain.prompts import PromptTemplate\ntemplate = \"\"\"You are a movie recommender system that help \nUse the following pieces of context to answer the question \nFor each question, suggest three movies, with a short desc\nchain_type_kwargs = {\"prompt\": PROMPT}\nqa = RetrievalQA.from_chain_type(llm=OpenAI(),\nquery = \"I'm looking for a funny action movie, any sug\nthe user input their natural language question, we might want to ask\ntheir age, gender, and favorite movie genre.\nour prompt a section where we can format the input variables with\nthose shared by the user, and then combine this prompt chunk in the\nfinal prompt we are going to pass to the chain.\nthe user):\nfrom langchain.prompts import PromptTemplate\ntemplate_prefix = \"\"\"You are a movie recommender syste\nCOMBINED_PROMPT = template_prefix +'\\n'+ user_info +'\\\nYou are a movie recommender system that help users to find \nUse the following pieces of context to answer the question ",
      "keywords": [
        "chain",
        "user",
        "prompt",
        "movie",
        "retriever",
        "question",
        "query",
        "action movie",
        "action",
        "context",
        "result",
        "type",
        "document",
        "answer",
        "output"
      ],
      "concepts": [
        "movies",
        "prompt",
        "result",
        "chain",
        "embedding",
        "retrieval",
        "retriever",
        "retrieve",
        "action",
        "lancedb"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 32,
          "title": "",
          "score": 0.52,
          "base_score": 0.52,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 58,
          "title": "",
          "score": 0.473,
          "base_score": 0.473,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 27,
          "title": "",
          "score": 0.455,
          "base_score": 0.455,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 45,
          "title": "",
          "score": 0.433,
          "base_score": 0.433,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 46,
          "title": "",
          "score": 0.431,
          "base_score": 0.431,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "movie",
          "action",
          "movies",
          "action movie",
          "retrievalqa"
        ],
        "semantic": [],
        "merged": [
          "movie",
          "action",
          "movies",
          "action movie",
          "retrievalqa"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2590585555389615,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:11.497541+00:00"
      }
    },
    {
      "chapter_number": 32,
      "title": "Segment 32 (pages 265-272)",
      "start_page": 265,
      "end_page": 272,
      "summary": "information (such as age, gender, country, etc.) as well as the movies the\nEach user will exhibit the following variables: username, age, gender, and a\nchat for this content-based system, starting from the available users’ data:\nusers’ attributes (name, age, gender) along with their reviews (a score\nfrom 1 to 10) of some movies.\n\"movies\": [\nfor i, row_movies in enumerate(data[\"movies\"]):\nmovie_dict = {}\ndata[\"movies\"][i] = movie_dict\nFigure 7.4: Sample users dataset\n3. We then format the user_info chunk as follows (assuming that the user\nmovies = ''\n# Iterate over the dictionary and output movie name an\nfor movie, rating in df['movies'][0].items():\noutput_string = f\"Movie: {movie}, Rating: {rating}\nmovies+=output_string\nuser_info = user_info.format(age = age, gender = gende\nCOMBINED_PROMPT = template_prefix +'\\n'+ user_info +'\\\nYou are a movie recommender system that help users to find \nThis is what we know about the user, and you can use this i\nMovies already seen alongside with rating: Movie: Transform\n4. Let’s now use this prompt within our chain:\n\" Based on your age, gender, and the movies you've already \nto Alice based on the user’s information about past preferences,\nbe addressed (such as a recommendation task) is that of using a feature store.\nFeature stores are data systems that are designed to support machine learning\nThey allow data teams to store, manage, and access features that\npopular features stores:\nFeast: This is an open-source feature store for machine learning.\nTecton allows users to define features in code, version control\nFeatureform: This is a virtual feature store that transforms existing\ndata infrastructure into a feature store.\nAzureML Managed Feature Store: This is a new type of workspace\nthat lets users discover, create, and operationalize features.\nintegrates with existing data stores, feature pipelines, and ML platforms\n# Import the movie dataset\n3. Create some widgets for the user to define their features and movies\n# Create a sidebar for user input\nst.sidebar.title(\"Movie Recommendation System\")\n# Ask the user for age, gender and favourite movie gen\n# Filter the movies based on the user input\nuser_info = user_info.format(age = age, gender = gende\nCOMBINED_PROMPT = template_prefix +'\\n'+ user_info +'\\",
      "keywords": [
        "user",
        "prompt",
        "movies",
        "age",
        "feature",
        "gender",
        "data",
        "chain",
        "system",
        "rating",
        "feature store",
        "COMBINED",
        "type",
        "create",
        "Question"
      ],
      "concepts": [
        "movies",
        "feature",
        "data",
        "user",
        "useful",
        "uses",
        "gender",
        "sidebar",
        "stored",
        "store"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 31,
          "title": "",
          "score": 0.52,
          "base_score": 0.52,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 29,
          "title": "",
          "score": 0.434,
          "base_score": 0.434,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 30,
          "title": "",
          "score": 0.377,
          "base_score": 0.377,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 24,
          "title": "",
          "score": 0.368,
          "base_score": 0.368,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 45,
          "title": "",
          "score": 0.363,
          "base_score": 0.363,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "movies",
          "age",
          "movie",
          "gender",
          "age gender"
        ],
        "semantic": [],
        "merged": [
          "movies",
          "age",
          "movie",
          "gender",
          "age gender"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.20238167766874102,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:11.497585+00:00"
      }
    },
    {
      "chapter_number": 33,
      "title": "Segment 33 (pages 273-282)",
      "start_page": 273,
      "end_page": 282,
      "summary": "working with structured data.\nStructured Data\nlanguage models (LLMs): the ability to handle structured, tabular data.\nas a natural language interface between us and our structured data, reducing\nIntroduction to the main structured data systems\nUsing tools and plugins to connect LLMs to tabular data\nlanguage interface for your data estate and be able to combine unstructured\nWhat is structured data?\nIn previous chapters, we focused on how LLMs can handle textual data.\nthat they have been trained and are able to handle unstructured text data.\nUnstructured data: This refers to data that doesn’t have a specific or\nunstructured data include:\nWhen it comes to storing unstructured data, NoSQL\nvarious data types like text, images, and videos\nStructured Query Language (SQL) to manage and\nquery data.\ndocument-oriented NoSQL database, which stores data\nStructured data: This type of data is organized and formatted with a\ndatabases.\nExamples of structured data include:\nRelational databases: Data stored in tables with predefined\ncolumns and data types.\nSpreadsheets: Data organized in rows and columns in software\nSemi-structured data: This falls between the two categories.\ndoesn’t adhere to a rigid structure like structured data, it has some level\nExamples of semi-structured data include:\nstructure data, but the specific tags and their arrangement can\nNoSQL databases: Storing data in a format that doesn’t require a\nIn summary, unstructured data lacks a defined format, structured data\nfollows a strict format, and semi-structured data has some level of structure\nbut is more flexible than structured data.\nHowever, regardless of its nature, querying structured data involves using a\nexample, for SQL databases, SQL is used to interact with relational\ndatabases.\nHenceforth, to extract data from tables, you need to know this\ndata?\ndatabases are.\nrelational databases\nmanipulating data.\nlanguage for querying and manipulating relational databases.\ndatabase.\ndatabases\nA relational database is a type of database that stores and organizes data in\nstructured tables with rows and columns.\nallows for efficient querying and manipulation of data using SQL.\nbusiness management systems, due to their ability to manage structured data\none table that references the primary key column in another\nforeign key is to maintain data consistency and integrity\nacross related tables.\nthe primary key table, the related data in the other table\nyou to understand how different pieces of data are related to\nFigure 8.1: An example of the relationship between two tables in a database",
      "keywords": [
        "data",
        "structured data",
        "relational databases",
        "databases",
        "structured",
        "relational",
        "Language",
        "unstructured data",
        "LLMs",
        "tables",
        "key",
        "chain",
        "unstructured",
        "SQL",
        "streamlit"
      ],
      "concepts": [
        "data",
        "structured",
        "structures",
        "database",
        "tables",
        "language",
        "manage",
        "managing",
        "key",
        "keys"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 11,
          "title": "",
          "score": 0.676,
          "base_score": 0.526,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 23,
          "title": "",
          "score": 0.623,
          "base_score": 0.473,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 12,
          "title": "",
          "score": 0.569,
          "base_score": 0.419,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 3,
          "title": "",
          "score": 0.568,
          "base_score": 0.418,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 37,
          "title": "",
          "score": 0.562,
          "base_score": 0.412,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data",
          "structured",
          "structured data",
          "databases",
          "unstructured"
        ],
        "semantic": [],
        "merged": [
          "data",
          "structured",
          "structured data",
          "databases",
          "unstructured"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.28349101901193297,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.497635+00:00"
      }
    },
    {
      "chapter_number": 34,
      "title": "Segment 34 (pages 283-291)",
      "start_page": 283,
      "end_page": 291,
      "summary": "SQL databases: These are relational database management systems\n(RDBMS) that use SQL for data manipulation and querying.\nGoogle Cloud SQL: A managed database service by Google Cloud\nIn this chapter, we are going to use SQLite database, which also offers a\ndatabase we’ll be using.\ndatabase\nsuch as SQL Server, Oracle, MySQL, PostgreSQL, SQLite, and DB2.\nHere are some features of this database:\nhttps://database.guide/2-sample-databases-sqlite/.\nYou can see an illustration of the relationship among the database’s tables\nFigure 8.2: Diagram of Chinook Database (source:\nhttps://github.com/arjunchndr/Analyzing-Chinook-Database-using-SQL-\ndatabases in Python\nTo work with relational databases in Python, you need to use a library that\ncan connect to the database and execute SQL queries.\ndelete data from relational databases using Python objects and methods.\nIt supports many database engines, such as SQLite, MySQL,\ninteract with MySQL databases from Python using the DB-API 2.0\nyou to connect to Oracle databases and use SQL and PL/SQL features\nsqlite3: This is a database connector for SQLite3, a widely used,\ndatabase management system.\nupdate, and delete data from SQLite databases in your Python programs\ninterface for Python database access modules.\nIt allows you to use Python objects as parameters and results for SQL\nLet’s see an example of this connection using our Chinook database:\nchinook.db, which will be used to interact with the database.\nwhich allows you to run SQL queries against your database:\ndatabase = 'chinook.db'\nconn = sqlite3.connect(database)\ntables = pd.read_sql(\"\"\"SELECT name, type\nFigure 8.3: A list of tables within the Chinook database\npd.read_sql(\"PRAGMA table_info(customers);\", \ndata = pd.read_sql(sql, conn)\nplt.bar(data.Genre, data.Tracks)\nAs you can see, in order to gather relevant information from our database,",
      "keywords": [
        "SQL",
        "database",
        "Chinook database",
        "Python",
        "SQL Server",
        "Microsoft SQL Server",
        "SQL queries",
        "data",
        "Chinook",
        "relational database",
        "database servers",
        "Oracle",
        "tracks",
        "genre",
        "relational"
      ],
      "concepts": [
        "databases",
        "data",
        "python",
        "tracks",
        "related",
        "querying",
        "mysql",
        "object",
        "genre",
        "management"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 33,
          "title": "",
          "score": 0.482,
          "base_score": 0.482,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 35,
          "title": "",
          "score": 0.449,
          "base_score": 0.449,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 36,
          "title": "",
          "score": 0.381,
          "base_score": 0.381,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 11,
          "title": "",
          "score": 0.362,
          "base_score": 0.362,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 58,
          "title": "",
          "score": 0.315,
          "base_score": 0.315,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "database",
          "sql",
          "chinook",
          "databases",
          "sqlite"
        ],
        "semantic": [],
        "merged": [
          "database",
          "sql",
          "chinook",
          "databases",
          "sqlite"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.19337007700676165,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:11.497677+00:00"
      }
    },
    {
      "chapter_number": 35,
      "title": "Segment 35 (pages 292-299)",
      "start_page": 292,
      "end_page": 299,
      "summary": "SQL Agent.\nLangChain agents and SQL\nAgent\nIn this chapter, we will see agents in action, using the following LangChain\ncreate_sql_agent: An agent designed to interact with relational\nfrom langchain.agents import create_sql_agent\nfrom langchain.agents.agent_toolkits import SQLDatabas\nfrom langchain.sql_database import SQLDatabase\nfrom langchain.agents import AgentExecutor\nagent_executor = create_sql_agent(\n['sql_db_query', 'sql_db_schema', 'sql_db_list_tables', 'sq\nsql_db_query: This takes as input a detailed and correct SQL\nsql_db_schema: This takes as input a comma-separated list of\nsql_db_list_tables: This takes as input an empty string, and it\nsql_db_query_checker: This tool double-checks whether the\n3. Let’s now execute our agent with a simple query to describe the\nagent_executor.run(\"Describe the playlisttrack table\")\nAction: sql_db_list_tables\nUnder the hood, the SQL agent comes\nprint(agent_executor.agent.llm_chain.prompt.template)\nYou are an agent designed to interact with a SQL database.\nsql_db_query: Input to this tool is a detailed and correct SQL qu\nsql_db_schema: Input to this tool is a comma-separated list of ta\nBe sure that the tables actually exist by calling sql_db_list_tab\nsql_db_list_tables: Input is an empty string, output is a comma s\nsql_db_query_checker: Use this tool to double check if your query\nAction: the action to take, should be one of [sql_db_query, sql_d\nThanks to this prompt template, the agent is able to use the proper tools and\ngenerate a SQL query, without modifying the underlying database (you can\nWe can also see how the agent is able to correlate more than one table within\nAction: sql_db_list_tables\nthe agent ran against the database.\nprompt to ask the agent to explicitly show us the reasoning behind its result.\nFor example, let’s say that we want our SQL agent to\nprint the SQL query it used to return the result.\nobjects running create_sql_agent.\nFigure 8.6: A screenshot of the description of the SQL agent\nThe Agent takes a prompt prefix and a format instruction, which are merged\nNow, let’s pass those prompt chunks as parameters to our agent and print the\nagent_executor = create_sql_agent(",
      "keywords": [
        "SQL",
        "Agent",
        "SQL Agent",
        "input",
        "Action Input",
        "SQL query",
        "query",
        "tables",
        "action",
        "database",
        "prompt",
        "LangChain agents",
        "list",
        "LLM",
        "LangChain"
      ],
      "concepts": [
        "tables",
        "agent",
        "input",
        "database",
        "data",
        "follows",
        "actions",
        "prompt",
        "langchain",
        "llm"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 36,
          "title": "",
          "score": 0.704,
          "base_score": 0.554,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 40,
          "title": "",
          "score": 0.694,
          "base_score": 0.544,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 45,
          "title": "",
          "score": 0.477,
          "base_score": 0.327,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 24,
          "title": "",
          "score": 0.462,
          "base_score": 0.462,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 34,
          "title": "",
          "score": 0.449,
          "base_score": 0.449,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "sql",
          "agent",
          "sql agent",
          "sql_db_list_tables",
          "action"
        ],
        "semantic": [],
        "merged": [
          "sql",
          "agent",
          "sql agent",
          "sql_db_list_tables",
          "action"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2299523654398656,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.497724+00:00"
      }
    },
    {
      "chapter_number": 36,
      "title": "Segment 36 (pages 300-307)",
      "start_page": 300,
      "end_page": 307,
      "summary": "result = agent_executor.run(\"What are the top 5 best-selling alb\nwell as the printed query our agent made for us.\nTo achieve this goal, we need to add tools to our agent,\nPythonREPLTool: This tool allows you to interact with the Python\nYou can use this tool to\nwrite, run, and debug Python code without having to use a script file or\nYou can also use this tool to access and manipulate various\nWe will need this tool\nallows you to interact with LangChain agents and tools\nagents and tools without having to write and run a\nFileManagementToolkit: This is a set of tools, or toolkit, that allows\nWe will need this toolkit to save the graphs generated by our agent in\nNow, let’s see how we can add these tools to our DBCopilot:\n1. First, we define the list of tools for our agent:\nfrom  langchain_experimental.tools.python.tool import \nfrom langchain.agents.agent_toolkits import FileManage\nselected_tools=[\"read_file\", \"write_file\", \"list_d\n2. In order to leverage that heterogeneous set of tools – SQL Database,\nPython REPL, and File System\nSQL Database-specific agent, since its default configurations are meant\nagnostic agent that is able to use all of the tools that we provide it with.\nable to use a multi-tool input.\nas best suited for the type of agent in use):\nfrom langchain.agents import initialize_agent, Tool\nfrom langchain.agents import AgentType\ntools, model, agent= AgentType.STRUCTURED_CHAT_ZERO_SHO\nagent.run(\"generate a matplotlib bar chart of the top 5 cou\nagent was also able to dynamically orchestrate the available tools to\nThe agent was able to first invoke the SQL tool to retrieve the\nrelevant information, then it used the Python tool to generate the matplotlib\nThen, it used the file system tool to save the result as PNG.\nAlso, in this case, we can modify the prompt of the agent.\nmight want the agent to provide an explanation not only of the SQL query\nagent as follows:\nagent = initialize_agent(tools, model, agent=AgentType.STRUCTURE\nThanks to LangChain’s tools components, we were able to extend our\nthe prompt customization, we can always refine the agent’s backend logic to\n4. Initialize the Agent using the prompt variables defined in the previous\nagent_executor = create_sql_agent(\nst.chat_message(\"user\").write(user_query)\nresponse = agent_executor.run(user_query, call",
      "keywords": [
        "agent",
        "tools",
        "Python",
        "REPL",
        "file",
        "sql",
        "prompt",
        "top",
        "toolkit",
        "query",
        "file system",
        "best-selling alb print",
        "llm",
        "SQL Database-specific agent",
        "Python code"
      ],
      "concepts": [
        "tools",
        "agent",
        "python",
        "run",
        "actions",
        "result",
        "working",
        "role",
        "databases",
        "printed"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 35,
          "title": "",
          "score": 0.704,
          "base_score": 0.554,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 40,
          "title": "",
          "score": 0.654,
          "base_score": 0.504,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 41,
          "title": "",
          "score": 0.615,
          "base_score": 0.465,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 45,
          "title": "",
          "score": 0.587,
          "base_score": 0.437,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 44,
          "title": "",
          "score": 0.487,
          "base_score": 0.337,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "agent",
          "tools",
          "tool",
          "file",
          "sql"
        ],
        "semantic": [],
        "merged": [
          "agent",
          "tools",
          "tool",
          "file",
          "sql"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2078348995209003,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.497780+00:00"
      }
    },
    {
      "chapter_number": 37,
      "title": "Segment 37 (pages 308-315)",
      "start_page": 308,
      "end_page": 315,
      "summary": "a set of tools that extend LLMs’ capabilities with domain-specific skills.\ncapabilities of LLMs. More specifically, we are going to cover their\ncapabilities of working with code.\nJoin our community’s Discord space for discussions with the author and\nWorking with Code\nexamine the other ways in which LLMs can be used with code, from\nthe end of this chapter, you will be able to leverage LLMs to code-related\ninterfaces to work with code.\nAnalysis of the main LLMs with top-performing code capabilities\nUsing LLMs for code understanding and generation\nYou can find all the code and examples in the book’s GitHub repository at\ncode\nendowed with knowledge of code understanding and generation; however,\nspecifically tailored to assessing LLMs’ capabilities of working with code.\nhttps://paperswithcode.com/sota/code-generation-on-\nthe code generation capabilities of LLMs, where the model completes\nHenceforth, when choosing your model for a code-specific\nsimilar code metrics (we will see throughout the chapter some further\nbenchmarks for code-specific LLMs).\ncode for common data analysis tasks in Python.\nIn this chapter, we are going to test different LLMs: two code-specific\nemerging capabilities in the field of code generation (Falcon LLM).\nCode understanding and\nIt analyzes code and\nIt can perform various tasks such as code",
      "keywords": [
        "code",
        "LLMs",
        "Python",
        "code understanding",
        "Python REPL tool",
        "SQL Database",
        "capabilities",
        "generation",
        "code generation",
        "SQL",
        "Python REPL",
        "tool",
        "Database",
        "agent",
        "StreamlitCallbackHandler module"
      ],
      "concepts": [
        "code",
        "coding",
        "python",
        "llms",
        "programming",
        "specifically",
        "models",
        "capable",
        "capabilities",
        "capability"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 12,
          "title": "",
          "score": 0.73,
          "base_score": 0.58,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 3,
          "title": "",
          "score": 0.707,
          "base_score": 0.557,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 21,
          "title": "",
          "score": 0.692,
          "base_score": 0.542,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 4,
          "title": "",
          "score": 0.664,
          "base_score": 0.514,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 11,
          "title": "",
          "score": 0.662,
          "base_score": 0.512,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "code",
          "capabilities",
          "llms",
          "generation",
          "llms code"
        ],
        "semantic": [],
        "merged": [
          "code",
          "capabilities",
          "llms",
          "generation",
          "llms code"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.349783110036078,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.498092+00:00"
      }
    },
    {
      "chapter_number": 38,
      "title": "Segment 38 (pages 316-325)",
      "start_page": 316,
      "end_page": 325,
      "summary": "In this experiment, we are going to try three different models: Falcon LLM,\nMeta AI’s Llama; and StarCoder, a code-specific model that we are going to\nEndpoint and then embed it in your code, or use the convenient library\nTo start using your Inference Endpoint, you can use the following code:\nTo create your Hugging Face Inference Endpoint, you can follow the\n2. Now that we’ve initialized the model, let’s ask it to generate the code\nGenerate a short html code to a simple webpage with a \n4. We can also try to generate a Python function to generate random\nGenerate a python program that create random password \npassword += chars[random.randint(0, 9)]\nreturn password\nprint(generate_password())\nWe now have a function named generate_password(), which uses\nrandom functions to generate a password as per our prompt.\n5. Finally, let’s do the opposite, asking the model to explain to us the\nabove code:\nExplain to me the following code:\npassword += chars[random.randint(0, 9)]\nreturn password\nprint(generate_password())\n<p>The code generates a random password of length 12 charac\nOverall, even if not code-specific, the model was able to correctly perform\nCodeLlama is a family of LLMs for code based on Llama 2, which is a\nCodeLlama can generate and discuss code in various\nparts of code based on the surrounding context, as well as follow instructions\nThe model comes in three sizes (7B, 13B, and 34B parameters) and three\nflavors (base model, Python fine-tuned, and instruction-tuned) to cover a\nHumanEval and MBPP, according to which CodeLlama models achieved a\nNow, let’s run some tests with this model.\ncan initialize the model leveraging either the Hugging Face Inference API\nllm = HuggingFaceHub(    repo_id=repo_id,  model_kwargs={\"temper\nLet’s now test it with some code tasks.\noptimizing Python code so that it runs more efficiently.\nmodel performs in this task.\nIn the following code snippet, we simply\nprompt the model to regenerate the provided code in a more efficient way:\nfor i in range(1, n + 1):\nNext, let’s leverage the model’s completion capabilities by initializing a\nfunction to remove non-ASCII characters.\nAccordingly, here is the code to generate the function:\nDefine a python function that remove non-ascii character from a \nThe following is the function that we receive as the output:\nLet’s now leverage the model as a bug fixer, prompting it with the wrong\nIdentify the bug in the following code:.\nFinally, let’s ask the model in natural language to generate specific Python\ncode for a given task that, in our example, will be that of writing a function\nWrite a Python function that finds the longest substring of a gi\nWe then get the following function as our output:",
      "keywords": [
        "Hugging Face Inference",
        "Hugging Face API",
        "Hugging Face",
        "Face Inference Endpoint",
        "code",
        "model",
        "Inference Endpoint",
        "Face Inference API",
        "Face Inference",
        "Hugging Face Hub",
        "Face Hub Inference",
        "password",
        "LLM",
        "Face API",
        "Hub Inference Endpoint"
      ],
      "concepts": [
        "codes",
        "models",
        "prompt",
        "random",
        "function",
        "functions",
        "functionality",
        "python",
        "pythonic",
        "html"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 39,
          "title": "",
          "score": 0.81,
          "base_score": 0.66,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 4,
          "title": "",
          "score": 0.447,
          "base_score": 0.447,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 40,
          "title": "",
          "score": 0.442,
          "base_score": 0.442,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 37,
          "title": "",
          "score": 0.44,
          "base_score": 0.44,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 17,
          "title": "",
          "score": 0.417,
          "base_score": 0.417,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "code",
          "password",
          "inference",
          "face",
          "random"
        ],
        "semantic": [],
        "merged": [
          "code",
          "password",
          "inference",
          "face",
          "random"
        ]
      },
      "topic_id": 7,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30099502517732674,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.498151+00:00"
      }
    },
    {
      "chapter_number": 39,
      "title": "Segment 39 (pages 326-333)",
      "start_page": 326,
      "end_page": 333,
      "summary": "deciding what LLM to use: if you are only interested in code generation,\ncontext of code generation and understanding.\nThe StarCoder model is an LLM for code that can perform various tasks,\nThe StarCoder model was evaluated on several benchmarks that test its\nMultiPL-E (where the model matched or outperformed the code-cushman-\nmodel clearly beat the code-cushman-001 model as well as all other open-\naccess models), and the Tech Assistant Prompt (where the model was able to\nStarCoder demonstrated great capabilities compared to other models, using\n2. Let’s set the repo_id for the StarCoder model and initialize it:\nStarCoder is a gated model on the Hugging Face Hub,\nNow that we’re set up, let’s start asking our model to compile some code.\nHow can I write a Python function to generate the nth Fibonacci \nFigure 9.5: Example of Fibonacci functions generated by StarCode\nLet’s now ask the model to generate a webpage to play tic tac toe against the\nGenerate the html code for a single page website that let the us\nInterestingly enough, the model in this case didn’t generate the whole code;\nFinally, StarCoder is also available as an extension in VS Code to act as your\nFigure 9.6: Hugging Face Code Autocomplete extension, powered by StarCoder\nOnce enabled, you can see that, while compiling your code, StarCoder will\nprovide suggestions to complete the code.\nAs you can see, I commented my code, describing a function to generate the\nnth Fibonacci number, and then started defining the function.\nCode understanding and generation are great capabilities of LLMs. On top of\ngoing beyond code generation.",
      "keywords": [
        "model",
        "code",
        "Falcon LLM",
        "Hugging Face",
        "StarCoder",
        "StarCoder model",
        "Fibonacci",
        "Hugging Face Code",
        "LLM",
        "billion parameters",
        "Fibonacci number",
        "Hugging Face API",
        "nth Fibonacci",
        "Hugging",
        "Face"
      ],
      "concepts": [
        "code",
        "model",
        "git",
        "licensed",
        "license",
        "completion",
        "complete",
        "reasoning",
        "function",
        "prompt"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 38,
          "title": "",
          "score": 0.81,
          "base_score": 0.66,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 37,
          "title": "",
          "score": 0.525,
          "base_score": 0.525,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 4,
          "title": "",
          "score": 0.487,
          "base_score": 0.487,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 50,
          "title": "",
          "score": 0.445,
          "base_score": 0.445,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 40,
          "title": "",
          "score": 0.43,
          "base_score": 0.43,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "starcoder",
          "code",
          "fibonacci",
          "starcoder model",
          "model"
        ],
        "semantic": [],
        "merged": [
          "starcoder",
          "code",
          "fibonacci",
          "starcoder model",
          "model"
        ]
      },
      "topic_id": 7,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.31800140076565725,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.498207+00:00"
      }
    },
    {
      "chapter_number": 40,
      "title": "Segment 40 (pages 334-341)",
      "start_page": 334,
      "end_page": 341,
      "summary": "“in Python,” meaning that the LLM-powered agent will leverage Python to\nLet’s first initialize our agent using the create_python_agent class in\ntool, which, in our example, will be the Python REPL:\nfrom langchain.agents.agent_types import AgentType\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain_experimental.agents.agent_toolkits.python.base i\nagent_executor = create_python_agent(\nFigure 9.8: Default prompt of the Python agent\nNow, let’s start with an easy query, asking the model to generate a scatter\n- Player A: 38 points, 10 rebounds, 7 assists\n- Player B: 28 points, 9 rebounds, 6 assists\n- Player C: 19 points, 6 rebounds, 3 assists\n- Player D: 12 points, 4 rebounds, 2 assists\n- Player E: 7 points, 2 rebounds, 1 assist\nInvoking: `Python_REPL` with `import seaborn as sns\nFigure 9.9: Sample plot generated by the Python agent\nTo do so, we can ask our agent to design and train a model to give us\nI want to predict the price of a house given the following infor\nDesign and train a regression model to predict the price of a ho\nOnce the model is trained, tell me the price of a house with the\nHere, we ask the agent to train a regression model on synthetic data\nInvoking: `Python_REPL` with `import numpy as np\n# Predict the price of a house with the given features\npredicted_price = model.predict(features)\nThe predicted price of a house with 2 rooms, 1 bathroom, and 100 \n'The predicted price of a house with 2 rooms, 1 bathroom, and 100\nmodel the price of the house we provided.\ncapable of solving optimization problems in a smart building environment.\n(HVAC) setpoints in the building to minimize energy costs while ensuring\ninitialization of our variables and constraints (energy cost per zone, initial\n- Zone 1: Energy cost = $0.05 per degree per hour\n- Zone 2: Energy cost = $0.07 per degree per hour\n- Zone 3: Energy cost = $0.06 per degree per hour\nInvoking: `Python_REPL` with `import scipy.optimize as opt\n# Calculate the energy cost for each zone\nThe agent was able to solve the smart building optimization problem, finding\nthe minimum total energy cost, given some constraints.\nof optimization problems, there are further use cases that these models could\nprocesses to minimize waste, energy consumption, and production costs\nThe Python REPL agent is amazing; however, it comes with some caveats:",
      "keywords": [
        "Python REPL",
        "Python",
        "Python REPL agent",
        "player",
        "energy cost",
        "agent",
        "zone",
        "model",
        "energy",
        "Python agent",
        "cost",
        "price",
        "house",
        "REPL",
        "total energy cost"
      ],
      "concepts": [
        "important",
        "players",
        "optimization",
        "optimize",
        "optimal",
        "python",
        "cost",
        "zones",
        "models",
        "agent"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 35,
          "title": "",
          "score": 0.694,
          "base_score": 0.544,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 45,
          "title": "",
          "score": 0.658,
          "base_score": 0.508,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 36,
          "title": "",
          "score": 0.654,
          "base_score": 0.504,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 41,
          "title": "",
          "score": 0.62,
          "base_score": 0.47,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 43,
          "title": "",
          "score": 0.524,
          "base_score": 0.374,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "energy",
          "energy cost",
          "price",
          "cost",
          "price house"
        ],
        "semantic": [],
        "merged": [
          "energy",
          "energy cost",
          "price",
          "cost",
          "price house"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3547038034060258,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.498295+00:00"
      }
    },
    {
      "chapter_number": 41,
      "title": "Segment 41 (pages 342-350)",
      "start_page": 342,
      "end_page": 350,
      "summary": "Leveraging Code Interpreter\nThe Code Interpreter plugin allows\nChatGPT to write and execute computer code in various programming\nThe Code Interpreter plugin is one of the tools designed specifically for\nWhile OpenAI’s Code Interpreter still doesn’t offer an API, there are some\nhttps://blog.langchain.dev/code-interpreter-api/), it\nprevious section is that the Code Interpreter API can actually execute the\ncode it generates.\nIn fact, when a Code Interpreter session starts, a miniature\nPython execution environment called CodeBox. To start using the code interpreter in your notebook, you can install all the\nIn this case, I will ask it to generate a plot of COVID-19 cases in a specific\n# generate a response based on user input\nFigure 9.10: Line chart generated by the Code Intepreter API\nAs you can see, the Code Interpreter answered the question with an\nmodel to plot the price of the S&P 500 index over the last 5 days:\n# generate a response based on user input\n\"Generate a plot of the price of S&P500 index in the las\nFigure 9.11: S&P 500 index price plotted by the Code Interpreter API\nFinally, we can provide local files to the Code Interpreter so that it can\nuser_request = \"Analyze this dataset and plot something \nFigure 9.12: Sample plots generated by the Code Interpreter API\nThe Code Interpreter plugin, together with code-specific LLMs and the\nLLMs can understand and generate code, since they have been trained\nactivate tools like Python REPL or Code Interpreter, which are then\nable to provide a response by working with code.\nto work with code.\nonce using ChatGPT, which is code understanding and generation.\nWe then moved forward with the additional applications that LLMs’ coding\nIn fact, we saw how code-specific\nversion of the Code Interpreter API.\ncovered the multiple capabilities of LLMs, while always handling language\ndata (natural or code).\nThe open-source version of the Code Interpreter API:\nA LangChain blog about the Code Interpreter API:\nhttps://blog.langchain.dev/code-interpreter-api/",
      "keywords": [
        "Code Interpreter API",
        "Code Interpreter",
        "Code Interpreter plugin",
        "Interpreter API",
        "Code",
        "Interpreter",
        "API",
        "Code Interpreter session",
        "Leveraging Code Interpreter",
        "Interpreter plugin",
        "Code Interpreter answered",
        "response",
        "plot",
        "user",
        "LLMs"
      ],
      "concepts": [
        "code",
        "coding",
        "python",
        "file",
        "generating",
        "generates",
        "generation",
        "languages",
        "output",
        "api"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 40,
          "title": "",
          "score": 0.62,
          "base_score": 0.47,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 36,
          "title": "",
          "score": 0.615,
          "base_score": 0.465,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 45,
          "title": "",
          "score": 0.553,
          "base_score": 0.403,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 37,
          "title": "",
          "score": 0.438,
          "base_score": 0.438,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 43,
          "title": "",
          "score": 0.438,
          "base_score": 0.288,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "interpreter",
          "code interpreter",
          "code",
          "interpreter api",
          "api"
        ],
        "semantic": [],
        "merged": [
          "interpreter",
          "code interpreter",
          "code",
          "interpreter api",
          "api"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.27638975388953996,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.498360+00:00"
      }
    },
    {
      "chapter_number": 42,
      "title": "Segment 42 (pages 351-359)",
      "start_page": 351,
      "end_page": 359,
      "summary": "Join our community’s Discord space for discussions with the author and\nBuilding Multimodal\nmultimodality while building agents.\ncombination of foundation models in different AI domains – language,\nimages, and audio – into one single agent that can adapt to a variety of tasks.\nBy the end of this chapter, you will be able to build your own multimodal\nagent, providing it with the tools and LLMs needed to perform various AI\nIntroduction to multimodality and large multimodal models (LMMs)\nHow to build a multimodal agent with single-modal LLMs using\nWhy multimodality?\nIn the context of Generative AI, multimodality refers to a model’s capability\nFor example, a multimodal model can\ncommunicate with humans via text, speech, images, or even videos, making\ntrained generative AI model that offers immense versatility by being\nfoundation models that are able to process one type of data: natural\nCreating AGI is a primary goal of some AI research and a\nmultiple AI systems to reach a multimodal AI assistant.\ncombine single-modal models, one for each data format we want to process,\nways with those models (that will be its tools), we can still achieve this goal.\nFigure 10.1: Illustration of multimodal application with single-modal tools\nbuilding multimodal applications, all based on the idea of combining\nexisting single-modal tools or models.\nBuilding a multimodal agent\nneed to start building our multimodal agent.\ntoward a set of AI models that can be consumed via API, and that\nFormerly known as Azure Cognitive Services, Azure AI Services are a set of\nAI\nServices are meant to provide every developer with AI models to be\nAzure AI Services cover various domains of AI, including speech, natural\nHence, considered all together, Azure AI Services can achieve the goal of\nIn fact, LangChain has a native integration with Azure AI Services called\nagent and leverage the multimodal capabilities of those models.\nThe toolkit makes it easier to incorporate Azure AI services’ capabilities –\nempowered to use the AI services to enhance its functionality and provide",
      "keywords": [
        "Titanic dataset",
        "Models",
        "Services",
        "AGI",
        "Multimodal",
        "Azure Cognitive Services",
        "Azure",
        "Foundation Models",
        "multimodal agent",
        "agent",
        "Inference Endpoint",
        "Falcon LLM model",
        "tools",
        "LLMs",
        "model card"
      ],
      "concepts": [
        "model",
        "langchain",
        "services",
        "python",
        "text",
        "agents",
        "building",
        "llm",
        "cognitive",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 2,
          "title": "",
          "score": 0.8,
          "base_score": 0.65,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 57,
          "title": "",
          "score": 0.74,
          "base_score": 0.59,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 5,
          "title": "",
          "score": 0.717,
          "base_score": 0.567,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 53,
          "title": "",
          "score": 0.638,
          "base_score": 0.638,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 56,
          "title": "",
          "score": 0.573,
          "base_score": 0.423,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "multimodal",
          "ai",
          "services",
          "azure",
          "ai services"
        ],
        "semantic": [],
        "merged": [
          "multimodal",
          "ai",
          "services",
          "azure",
          "ai services"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.34386617294380145,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.498428+00:00"
      }
    },
    {
      "chapter_number": 43,
      "title": "Segment 43 (pages 360-367)",
      "start_page": 360,
      "end_page": 367,
      "summary": "and provide an image of eggs and flour, the agent can use the Azure AI\nServices Image Analysis tool to extract the caption, objects, and tags from\n1. You first need to create a multi-service instance of Azure AI Services in\nFigure 10.2: Screenshot of a multi-service instance of Azure AI Services\n5. Now, we can configure our toolkit and also see which tools we have,\nfrom langchain.agents.agent_toolkits import AzureCogni\n[(tool.name, tool.description) for tool in toolkit.get\n[('azure_cognitive_services_form_recognizer',\n'A wrapper around Azure Cognitive Services Form Recognize\n('azure_cognitive_services_speech2text',\n'A wrapper around Azure Cognitive Services Speech2Text.\n('azure_cognitive_services_text2speech',\n'A wrapper around Azure Cognitive Services Text2Speech.\n('azure_cognitive_services_image_analysis',\n'A wrapper around Azure Cognitive Services Image Analysis\nTo start easy, let’s simply ask the agent to describe the following picture,\nwhich will only require the image_analysis tool to be accomplished:\nLet’s pass the URL of this image as input to our model, as per the\ndescription of the azure_cognitive_services_image_analysis tool:\ndescription = agent.run(\"what shows the following image?:\"\n\"action\": \"azure_cognitive_services_image_analysis\",\n\"action_input\": \"The image is of a person holding a slingshot.\"\nThe image is of a person holding a slingshot.\nagent.run(\"what happens if the person lets the slingshot go?:\"\n\"action\": \"azure_cognitive_services_image_analysis\",\n\"action_input\": \"If the person lets the slingshot go, it will f\nimage leveraging the image_analysis tool, the LLM was able to set up\nIn this example, the agent only leveraged one tool.",
      "keywords": [
        "Azure Cognitive Services",
        "Azure",
        "Services",
        "Azure Cognitive",
        "image",
        "Cognitive Services",
        "Services Image Analysis",
        "COGS",
        "cognitive",
        "Action",
        "agent",
        "tool",
        "Cognitive Services Image",
        "slingshot",
        "person"
      ],
      "concepts": [
        "tools",
        "agent",
        "services",
        "follow",
        "images",
        "data",
        "text",
        "let",
        "letting",
        "lets"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 44,
          "title": "",
          "score": 0.807,
          "base_score": 0.657,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 45,
          "title": "",
          "score": 0.581,
          "base_score": 0.431,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 40,
          "title": "",
          "score": 0.524,
          "base_score": 0.374,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 36,
          "title": "",
          "score": 0.442,
          "base_score": 0.292,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 41,
          "title": "",
          "score": 0.438,
          "base_score": 0.288,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "services",
          "azure",
          "cognitive",
          "image",
          "cognitive services"
        ],
        "semantic": [],
        "merged": [
          "services",
          "azure",
          "cognitive",
          "image",
          "cognitive services"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.26351038636857516,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.498480+00:00"
      }
    },
    {
      "chapter_number": 44,
      "title": "Segment 44 (pages 368-377)",
      "start_page": 368,
      "end_page": 377,
      "summary": "Figure 10.5: Example of an input image for a story-telling agent (source:\nLet’s ask the agent to do so:\nagent.run(\"Tell me a story related to the following picture and \n\"action\": \"azure_cognitive_services_image_analysis\",\nThought:Based on the Azure Cognitive Services Image Analysis tool\n\"action\": \"azure_cognitive_services_text2speech\",\nAs you can see, the agent was able to invoke two tools to accomplish the\nThe agent saved the audio file in a temporary file, and you can listen to it\naudio = agent.run(\"Tell me a story related to the following pict\nFinally, we can also modify the default prompt that comes with the agent\nprint(agent.agent.llm_chain.prompt.messages[0].prompt.template)\n{tools}\nUse a json blob to specify a tool by providing an action key (too\n\"action\": $TOOL_NAME,\nLet’s modify the prefix of the prompt and pass it as kwargs to our agent:\nYou can use multiple tools to answer the question.\nALWAYS use the tools.\nYou have access to the following tools:\"\"\"\nagent = initialize_agent(toolkit.get_tools(), model, agent=AgentT\nagent_kwargs={\nin mind that each pre-built agent has its own prompt template, hence it is\nanalysis, as well as build toward document process automation and, more\nDocument process automation is a strategy that uses\ninvolves the use of software tools, including document\nDocument process automation can save you\nTo start building our application, we can follow these steps:\nazure_cognitive_services_text2speech tools, so we can limit the\ntoolkit = AzureCognitiveServicesToolkit().get_tools()\ntools\n2. Let’s now initialize the agent with the default prompt and see the\nwith which to query the agent:\nFigure 10.6: Sample template of a generic invoice (source: https://www.whiteelysee.fr/design/wp-\nagent.run(\"what are all men's skus?\"\n\"action\": \"azure_cognitive_services_form_recognizer\",\nagent.run(\"give me the following information about the\n5. Finally, let’s also leverage the text2speech tool to produce the audio of\nagent.run(\"extract women's SKUs in the following invoi\nwant the agent to produce the audio output without the user explicitly\nYou can use multiple tools to answer the question.\nALWAYS use the tools.\nYou have access to the following tools:\nagent = initialize_agent(tools, model, agent=AgentType\nagent_kwargs={\n7. Let’s run the agent:\nagent.run(\"what are women's SKUs in the following invo\nI will need to use the azure_cognitive_services_form_recogn\n\"action\": \"azure_cognitive_services_form_recognizer\",\nAs you can see, now the agent saved the output into an audio file, even when",
      "keywords": [
        "agent",
        "Action",
        "tools",
        "PURCHASE ORDER TEMPLATE",
        "PURCHASE ORDER",
        "services",
        "azure",
        "output",
        "input",
        "template",
        "story",
        "prefix",
        "ORDER TEMPLATE",
        "cognitive",
        "process automation"
      ],
      "concepts": [
        "tool",
        "action",
        "shipping",
        "ship",
        "output",
        "follows",
        "thought",
        "template",
        "dog",
        "entering"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 43,
          "title": "",
          "score": 0.807,
          "base_score": 0.657,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 45,
          "title": "",
          "score": 0.643,
          "base_score": 0.493,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 36,
          "title": "",
          "score": 0.487,
          "base_score": 0.337,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 40,
          "title": "",
          "score": 0.478,
          "base_score": 0.328,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 35,
          "title": "",
          "score": 0.423,
          "base_score": 0.273,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "agent",
          "tools",
          "action",
          "agent run",
          "audio"
        ],
        "semantic": [],
        "merged": [
          "agent",
          "tools",
          "action",
          "agent run",
          "audio"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.21829019254344784,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.498526+00:00"
      }
    },
    {
      "chapter_number": 45,
      "title": "Segment 45 (pages 378-387)",
      "start_page": 378,
      "end_page": 387,
      "summary": "tools into one agent\nOur goal is to build a copilot agent that will help us generate reviews\n1. Search and transcribe a YouTube video based on our input.\n3. Generate an image related to the video and the review.\nYouTube tools and Whisper\nThe first step of our agent will be to search and transcribe the YouTube video\nTo do so, there are two tools we need to leverage:\ncan import and try the tool by running the following code, specifying\ntool = YouTubeSearchTool()\nresult = tool.run(\"Avatar: The Way of Water,1\")\nThe tool returns the URL of the video.\noutput of the first tool into a transcript that will serve as input to the\nnext tool.\ntools = []\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_R\nagent.run(\"search a video trailer of Avatar: the way of water.\nAction Input: \"Avatar: the way of water,1\"\nThought:I have the transcription of the video trailer for Avatar:\nwe don’t need another tool), the image generation will need an additional\ntool.\ntool by running the following code (always setting the environmental\nfrom langchain.agents import load_tools\ntools = load_tools(['dalle-image-generator'])\nagent = initialize_agent(tools, model, AgentType.ZERO_SHOT_REACT\nThe following is the image that was generated, as requested:\nFigure 10.7: Image generated by DALL·E upon the user’s input\nagent.run(\"search a video trailer of Avatar: the way of water.\nI need to use both youtube_search and CustomeYTTranscribe tools t\nAction Input: \"Avatar: The Way of Water trailer is visually stunn\nObservation: Write a review is not a valid tool, try one of [yout\nThought:I need to use a different tool to write a review.\nObservation: Write a review is not a valid tool, try one of [yout\nNote how the agent was initially looking for a tool to make a review, to then\ntools = []\ntools.append(load_tools(['dalle-image-generator'])[0])\nagent = initialize_agent(tools, model, AgentType.ZERO_\nagent.run(\"search a video trailer of Avatar: the way o\nI need to search for a video trailer of \"Avatar: The Way of\nAction Input: \"Avatar: The Way of Water trailer,1\"\nThought:I found a video trailer of \"Avatar: The Way of Wate\nThought:I have transcribed the video and now I can generate\nAction Input: \"Review of Avatar: The Way of Water trailer: \nThought:I have generated a review of the \"Avatar: The Way o\nFigure 10.8: Image generated by DALL·E based on the trailer review\nto generate a review alongside an image.\nALWAYS generate an image alongside the review, based o\nYou have access to the following tools:\nagent = initialize_agent(tools, model, agent=AgentType\nagent.run(\"Generate a review of the trailer of Avatar:\nAction Input: \"Avatar: The Way of Water trailer\", 1\nThought:I have generated an image for the Instagram review ",
      "keywords": [
        "Action Input",
        "Avatar",
        "Action",
        "input",
        "tools",
        "image",
        "agent",
        "review",
        "video",
        "video trailer",
        "trailer",
        "Combining single tools",
        "Water",
        "Observation",
        "Search"
      ],
      "concepts": [
        "tools",
        "agent",
        "action",
        "image",
        "observation",
        "generation",
        "generator",
        "generated",
        "review",
        "output"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 40,
          "title": "",
          "score": 0.658,
          "base_score": 0.508,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 44,
          "title": "",
          "score": 0.643,
          "base_score": 0.493,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 36,
          "title": "",
          "score": 0.587,
          "base_score": 0.437,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 43,
          "title": "",
          "score": 0.581,
          "base_score": 0.431,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 41,
          "title": "",
          "score": 0.553,
          "base_score": 0.403,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "avatar",
          "trailer",
          "video",
          "avatar way",
          "review"
        ],
        "semantic": [],
        "merged": [
          "avatar",
          "trailer",
          "video",
          "avatar way",
          "review"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2821928580923246,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.498575+00:00"
      }
    },
    {
      "chapter_number": 46,
      "title": "Segment 46 (pages 388-395)",
      "start_page": 388,
      "end_page": 395,
      "summary": "chain\nGenerates a social media post to promote the story.\nGenerates an image to go along with the social media post.\ntheir outputs to the next chain.\n1. We’ll start by initializing the story generator chain:\nfrom langchain.chains import SequentialChain, LLMChain\nstory_prompt_template = PromptTemplate(input_variables\nstory_chain = LLMChain(llm=llm, prompt=story_prompt_te\nresult = story_chain({'topic': 'friendship story','gen\neasily linked as output to the next chain, which will be the social post\nsocial_chain = LLMChain(llm=llm, prompt=prompt_templat\npost = social_chain({'story': result['story'], 'social\nHere, I used the output of story_chain as input to social_chain.\n3. Finally, let’s initialize an image generator chain:\nimage_chain = LLMChain(llm=llm, prompt=prompt, output_\nNote that the output of the chain will be the prompt to pass to the\nThis generates the following output:\n5. The final step will be to put it all together into a sequential chain:\noverall_chain = SequentialChain(input_variables = ['to\nchains=[story_chain, social_chain, ima\noutput_variables = ['post', 'image'], \noverall_chain({'topic': 'friendship story','genre':'ad\nSince we passed the output_variables = ['post, 'image'] parameter to\nthe chain, those will be the two outputs of the chain.\ncombined; option 3, on the other hand, follows a hard-coded approach,\nFlexibility vs control: The agentic approach lets the LLM decide\nEvaluations: The agentic approach leverages the tools to generate the\napproach, each chain has its own model that can be tested separately, so\napproach, for each chain, we need a separate prompt, model, and testing\nstory_chain = LLMChain(llm=llm, prompt=story_prompt_te\nsocial_chain = LLMChain(llm=llm, prompt=social_prompt_\nimage_chain = LLMChain(llm=llm, prompt=prompt, output_\noverall_chain = SequentialChain(input_variables = ['to\nchains=[story_chain, social_chain, ima\noutput_variables = ['story','post', 'i\n4. Run the overall chain and print the results:\nresult = overall_chain({'topic': topic,'genre':gen\nIn this case, I’ve set the output_variables = ['story','post', 'image']\nparameter so that we will have also the story itself as output.",
      "keywords": [
        "chain",
        "story",
        "prompt",
        "social",
        "Image",
        "social media post",
        "output",
        "post",
        "social media",
        "llm",
        "approach",
        "media post",
        "variables",
        "topic",
        "input"
      ],
      "concepts": [
        "prompts",
        "story",
        "post",
        "chain",
        "results",
        "outputs",
        "finally",
        "final",
        "social",
        "approach"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 24,
          "title": "",
          "score": 0.6,
          "base_score": 0.6,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 22,
          "title": "",
          "score": 0.438,
          "base_score": 0.438,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 31,
          "title": "",
          "score": 0.431,
          "base_score": 0.431,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 39,
          "title": "",
          "score": 0.413,
          "base_score": 0.413,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 45,
          "title": "",
          "score": 0.399,
          "base_score": 0.399,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "post",
          "story",
          "chain",
          "social",
          "llmchain"
        ],
        "semantic": [],
        "merged": [
          "post",
          "story",
          "chain",
          "social",
          "llmchain"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.25465450009145263,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:11.498622+00:00"
      }
    },
    {
      "chapter_number": 47,
      "title": "Segment 47 (pages 396-406)",
      "start_page": 396,
      "end_page": 406,
      "summary": "of single tools, and a hard-coded approach with chained models.\nprocess of fine-tuning, leveraging open-source models, and using custom\nLanguage Models\nthis is the case, you might want to fine-tune your model on your domain-\nIn the context of fine-tuning language models, “taxonomy”\ncategorization helps in fine-tuning language models to\nIn this chapter, we are going to cover the technical details of fine-tuning\nfine-tune an LLM on your own data, so that you can build domain-specific\napplications powered by those models.\nPreparing your data to fine-tune the model\nFine-tuning a base model on your data\nHosting strategies for your fine-tuned model\nWhat is fine-tuning?\nFine-tuning is a technique of transfer learning in which the weights of a\nexample, if you have a model that can recognize cars, you\nTo better understand the concepts of transfer learning and fine-tuning, let’s\nInstead, you can use transfer learning, which means taking a model that was\nFor example, you can take a model that was trained to recognize\nThis model has learned\nYou can use this model as a base for your flower recognition model.\nis needed for the model to adapt to the new task.\non top of the base model is a process called feature extraction.\nstep is done, you can further tailor your model with fine-tuning by\nunfreezing some of the base model layers and training them together with\nThis allows you to adjust the base model features to\nThe following picture illustrates the computer vision model example:\nFigure 11.1: Example of transfer learning and fine-tuning\nthe performance of the model.\nthe last few layers of the base model, which are more specific to the original\nTo summarize, transfer learning and fine-tuning are techniques that allow\nyou to use a pretrained model for a new task.\nadding a new classifier layer on top of the base model and training only that\nFine-tuning involves unfreezing some or all of the base model layers\npretrained language model to a specific task or domain by updating its\nperformance and accuracy of the model for the target task.\n1. Load the pretrained language model and its tokenizer: The\nare added on top of the pretrained model to perform the task.\ntext generation, the architecture differs from models\nbased models with the purpose of transforming the\n4. Train the model on the task-specific dataset: The training process\ninvolves feeding the input tokens to the model, computing the loss\nmodels or fine-tuning strategies.\nfine-tuning an LLM is not a “light” activity.\nWhen is fine-tuning\ndiscussed, those models have a huge number of parameters that make them\nHenceforth, fine-tuning might also be useful when you want to leverage a\nto perform as well as a SOTA model in your specific task.\nSome examples of when fine-tuning might be necessary are:\nFine-tuning can help the LLM learn the vocabulary, style, and tone of\nbut the LLM was pretrained on a language modeling objective.\nFine-tuning can help the LLM learn",
      "keywords": [
        "model",
        "LLM",
        "fine-tuning",
        "base model",
        "task",
        "fine-tuning language models",
        "base model layers",
        "Language Models",
        "base",
        "layer",
        "LLMs",
        "pretrained language model",
        "transfer learning",
        "training",
        "model layers"
      ],
      "concepts": [
        "models",
        "specific",
        "tuning",
        "token",
        "llm",
        "fine",
        "tasks",
        "data",
        "involves",
        "involved"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 9,
          "title": "",
          "score": 0.645,
          "base_score": 0.645,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 2,
          "title": "",
          "score": 0.588,
          "base_score": 0.588,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 8,
          "title": "",
          "score": 0.54,
          "base_score": 0.54,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 3,
          "title": "",
          "score": 0.506,
          "base_score": 0.506,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 12,
          "title": "",
          "score": 0.505,
          "base_score": 0.505,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "fine",
          "fine tuning",
          "tuning",
          "model",
          "base model"
        ],
        "semantic": [],
        "merged": [
          "fine",
          "fine tuning",
          "tuning",
          "model",
          "base model"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.33584193771484727,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:11.498677+00:00"
      }
    },
    {
      "chapter_number": 48,
      "title": "Segment 48 (pages 407-414)",
      "start_page": 407,
      "end_page": 414,
      "summary": "libraries, such as datasets (to load data from the Hugging Face datasets\nThe first ingredient that we need is the training dataset.\nwill leverage the datasets library available in Hugging Face to load a binary\nlabeled datasets to train algorithms to classify data or predict\nLabeled datasets are collections of\nlabeled dataset for handwriting recognition might have\nTraining and validation sets are subsets of the labeled dataset\ndataset\ntrain: Dataset({\nunsupervised: Dataset({\ndataset[\"train\"][100]\nTo do so, we need to tokenize the provided text, and we will\nTokenizing the data\nTokenizers can be used to encode text efficiently and consistently, as well as\nsome models.\nthe Hugging Face Transformers library, that offers tokenizers for various\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\nNote that we picked a specific tokenizer called bert-base-cased.\ntokenizer prepares the inputs for the model by converting the text into\nby the tokenizer function when encoding a text input.\nDifferent tokenizers\nmay have different input IDs for the same tokens, depending\non their vocabulary and tokenization algorithm.\nDifferent models may use different tokenization algorithms, such as word-\nthe correct tokenizer for each model, otherwise the model may not perform\nscenario, hence we loaded its pretrained tokenizer (which is a word-based\ntokenizer powered by an algorithm called WordPiece).\nthe dataset:\ndef tokenize_function(examples):\nreturn tokenizer(examples[\"text\"], padding = \"max_length\", t\ntokenized_datasets = dataset.map(tokenize_function, batched=True\ntokenize_function to ensure an output with the right sizing for our BERT\nmodel.\nPadding means adding some special tokens, usually zeros, at\nTruncation means removing some tokens from a sequence to\ntokens, like this: [1, 2, 3, 4, 5, 6, 7, 8].\nAlternatively, we can remove the first 2 tokens,\ntokenized_datasets = dataset.map(tokenize_function, batched=True\ntokenized_datasets['train'][100]['input_ids']\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=4\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42)\nmodel to be fine-tuned.\nFine-tuning the model",
      "keywords": [
        "Hugging Face datasets",
        "Hugging Face",
        "dataset",
        "model",
        "BERT model",
        "Text",
        "tokenizer",
        "Face datasets",
        "Face",
        "Hugging",
        "Hugging Face Transformers",
        "LLM",
        "BERT",
        "learning",
        "leveraging Hugging Face"
      ],
      "concepts": [
        "grammar",
        "vocabulary",
        "errors",
        "smaller",
        "obtaining",
        "negative",
        "size",
        "select",
        "network",
        "neural"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 49,
          "title": "",
          "score": 0.71,
          "base_score": 0.56,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 50,
          "title": "",
          "score": 0.684,
          "base_score": 0.534,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 7,
          "title": "",
          "score": 0.622,
          "base_score": 0.472,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 56,
          "title": "",
          "score": 0.601,
          "base_score": 0.451,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 5,
          "title": "",
          "score": 0.583,
          "base_score": 0.433,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "tokenizer",
          "dataset",
          "face",
          "tokenized_datasets",
          "datasets"
        ],
        "semantic": [],
        "merged": [
          "tokenizer",
          "dataset",
          "face",
          "tokenized_datasets",
          "datasets"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.31650359175605064,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.498733+00:00"
      }
    },
    {
      "chapter_number": 49,
      "title": "Segment 49 (pages 415-422)",
      "start_page": 415,
      "end_page": 422,
      "summary": "standard for language models has greatly improved.\nThe BERT model has two main components:\nOutput layer: The output layer is task-specific and can be different\ntext classification, the output layer can be a linear layer that predicts the\ncan be two linear layers that predict the start and end positions of the\nThe number of layers and parameters of the model depends on the\nmodel version.\nbought a new [MASK] yesterday,” the model should predict the word\nNext sentence prediction (NSP): NSP aims to teach the model to\n“Her favorite genre is fantasy,” the model should predict that they are\nsoccer every weekend,” the model should predict that they are not\nthe model learn the coherence and logic of the text, as well as the\nBy using these two objectives (on which the model is trained at the same\ntime), the BERT model can learn general language knowledge that can be\ntransferred to specific tasks, such as text classification, question answering,\nThe BERT model can achieve better performance on these tasks\nThe BERT model is available – along with many fine-tuned versions –in the\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert\nwhich can instantiate a model architecture suitable for sequence\nUsing evaluation metrics\nAs those models are trained on unlabeled\ntext and are not task-specific, but rather generic and adaptable given a user’s\nIf this is the case, evaluation metrics boil\nevaluate a binary classifier is to use a confusion matrix.\nTrue positive (TP): The number of cases where the classifier correctly\npredicted 1 when the true label was 1.\nFalse positive (FP): The number of cases where the classifier\nTrue negative (TN): The number of cases where the classifier correctly\nFalse negative (FN): The number of cases where the classifier\nHere is an example of a confusion matrix for the sentiment classifier we are\nPredicted Positive\nPredicted Negative\nThe confusion matrix can be used to calculate various metrics that measure\nPrecision: The proportion of correct positive predictions among all\npositive predictions.\nRecall: The proportion of correct positive predictions among all\nSpecificity: The proportion of correct negative predictions among all\nclassifier can distinguish between positive and negative cases at\nwell the classifier can rank positive cases higher than negative cases.\n1. You can import this metric from the evaluate library as follows:",
      "keywords": [
        "model",
        "BERT model",
        "BERT",
        "text",
        "classifier",
        "multiple NLP tasks",
        "positive",
        "NLP tasks",
        "sentiment classifier",
        "Output layer",
        "negative",
        "metrics",
        "evaluation metrics",
        "multiple NLP",
        "cases"
      ],
      "concepts": [
        "model",
        "text",
        "evaluation",
        "evaluating",
        "evaluates",
        "language",
        "metrics",
        "positions",
        "positive",
        "masked"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 56,
          "title": "",
          "score": 0.776,
          "base_score": 0.626,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 48,
          "title": "",
          "score": 0.71,
          "base_score": 0.56,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 5,
          "title": "",
          "score": 0.684,
          "base_score": 0.534,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 2,
          "title": "",
          "score": 0.667,
          "base_score": 0.517,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 50,
          "title": "",
          "score": 0.654,
          "base_score": 0.504,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "positive",
          "classifier",
          "negative",
          "cases",
          "bert"
        ],
        "semantic": [],
        "merged": [
          "positive",
          "classifier",
          "negative",
          "cases",
          "bert"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.36072823426208234,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.498793+00:00"
      }
    },
    {
      "chapter_number": 50,
      "title": "Segment 50 (pages 423-432)",
      "start_page": 423,
      "end_page": 432,
      "summary": "we want our model to be tested against the test set while training:\nthe model’s weights are updated based on the training data\nThe last component we need to fine-tune our model is a Trainer object.\nand evaluation of models in PyTorch, optimized for Hugging Face\nwill need a model, some configuration args (such as the number of\nepochs), a training dataset, an evaluation dataset, and the type of\nmodel=model,\ntrainer.train()\nNevertheless, the training results for only two epochs in terms of\nAs you can see, between the two epochs the model gained an accuracy\n3. Once the model is trained, we can save it locally, specifying the path as\ntrainer.save_model('models/sentiment-classifier')\n4. To consume and test the model, you can load it with the following code:\n5. Finally, we need to test our model.\nmodel (to be first tokenized) on which it can perform sentiment\noutputs = model(**inputs)\nNote that the model output is a SequenceClassifierOutput object,\nwhich is the base class for outputs of sentence classification models.\nour classification model generated.\n7. Note that you can also save the model in your Hugging Face account.\nThen, you can save the model, specifying\nyour account name and model name:\nBy doing so, this model can be consumed via the Hugging Face Hub as\nFigure 11.4: Model card within the Hugging Face Hub space\nIn this section, we fine-tuned a BERT model with just a few lines of code,\nplatform hosted in Hugging Face to train and fine-tune models.\nleverage since proprietary models can also be fine-tuned.\ncomputational power to train and host your customized models.\nIn this chapter, we covered the process of fine-tuning LLMs. We started with\na definition of fine-tuning and general considerations to take into account if\nscenario where, starting from a base BERT model, we wanted a powerful\nTo do so, we fine-tuned the base model on the\nResponsible AI\nmodels (LLMs), gathering also a deeper understanding of how many factors\nbehind mitigating the potential harms of LLMs – and AI models in general –\nwhich is Responsible AI.\nWhat is Responsible AI and why do we need it?\nWhat is Responsible AI and\nResponsible AI refers to the ethical and accountable development,\nResponsible AI\nSome ethical implications of Responsible AI are:\nBias: AI systems can inherit biases present in their training data.\nEnvironmental impact: Training large models consumes significant\nResponsible AI considers environmental impacts and explores\nIn the context of generative AI, Responsible AI would mean creating models\nThe models should be reliable and safe to use.\nmake a whole LLM-powered application safer and more robust: the model",
      "keywords": [
        "Hugging Face",
        "model",
        "Hugging Face Hub",
        "Responsible",
        "training",
        "Face",
        "Hugging",
        "Hugging Face Python",
        "LLMs",
        "Hugging Face account",
        "dataset",
        "epoch",
        "training dataset",
        "eval",
        "Hugging Face login"
      ],
      "concepts": [
        "model",
        "training",
        "responsible",
        "responsibly",
        "outputs",
        "llms",
        "application",
        "applications",
        "sentiment",
        "predictions"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 48,
          "title": "",
          "score": 0.684,
          "base_score": 0.534,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 49,
          "title": "",
          "score": 0.654,
          "base_score": 0.504,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 56,
          "title": "",
          "score": 0.54,
          "base_score": 0.39,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 5,
          "title": "",
          "score": 0.46,
          "base_score": 0.31,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 8,
          "title": "",
          "score": 0.46,
          "base_score": 0.46,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "responsible",
          "responsible ai",
          "model",
          "face",
          "hugging"
        ],
        "semantic": [],
        "merged": [
          "responsible",
          "responsible ai",
          "model",
          "face",
          "hugging"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2866115085344272,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.498865+00:00"
      }
    },
    {
      "chapter_number": 51,
      "title": "Segment 51 (pages 433-440)",
      "start_page": 433,
      "end_page": 440,
      "summary": "Figure 12.1: Illustration of different mitigation layers for LLM-powered applications\nOn the other hand, if we leverage open-source models via\nan API, we can act up to the model level to incorporate Responsible AI\nModel level\nThe very first level is the model itself, which is impacted by the training\nIn fact, if the training data is biased, the model will\nwhere authors show an example of model bias in the field of computer\nFigure 12.2: Example of sexism and bias of a vision model.\nof the examples the model was trained on.\nAs you can see, the model created a function that linked the probability of\nbeing a good scientist to race and gender, which is something that the model\nTo act at the model level, there are some areas that researchers and\nmodeling is to faithfully represent the language found in the training\nFor example, in the scenario of the vision model previously\nopen-source example is the Python Responsible AI\npotential biases and ensure that models are fair and\nimbalances that could lead to biased model\nFine-tune language models: Adjust weightings to prevent bias and\nthat consists of adjusting a model’s weights according to human\nThis technique, in addition to making the model more\nharmful or toxic content, ensuring that the models are geared toward\nIn Chapter 4, we learned how the prompt and, more specifically, the\nSince the metaprompt can be used to instruct a model to behave as we wish,\nmodel about what it can and cannot do.\nTransparency: Being transparent about how the AI model works, its\nprovided data can ensure the model does not hallucinate or provide\nwherein an AI employing a specific metaprompt for a task is\nactivity that accesses the meta prompt of an LLM and changes it.\nexample, from the defined metaprompt “You are an AI assistant that\nactivity finds target prompts to feed the model with that are capable of\nAn example of one of these prompts, which\nThere are some defensive techniques you can use to prevent prompt\nthrough the instruction given to the model.\nThe third and final mitigation layer is at the user interface level, and we are\nThe user interface represents the last mile for an LLM-powered application\nCite references and sources: Let the model disclose to the user the",
      "keywords": [
        "model",
        "prompt",
        "model level",
        "level",
        "user interface level",
        "user",
        "prompt injection",
        "metaprompt level",
        "training",
        "metaprompt",
        "ChatGPT",
        "interface level",
        "Bias",
        "harmful",
        "vision model"
      ],
      "concepts": [
        "prompt",
        "model",
        "applications",
        "application",
        "biased",
        "providing",
        "provide",
        "user",
        "harmful",
        "specifically"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 53,
          "title": "",
          "score": 0.711,
          "base_score": 0.561,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 14,
          "title": "",
          "score": 0.639,
          "base_score": 0.489,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 54,
          "title": "",
          "score": 0.519,
          "base_score": 0.369,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 15,
          "title": "",
          "score": 0.511,
          "base_score": 0.361,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 13,
          "title": "",
          "score": 0.501,
          "base_score": 0.351,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "level",
          "model",
          "metaprompt",
          "model level",
          "bias"
        ],
        "semantic": [],
        "merged": [
          "level",
          "model",
          "metaprompt",
          "model level",
          "bias"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.28436227716754425,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.498920+00:00"
      }
    },
    {
      "chapter_number": 52,
      "title": "Segment 52 (pages 441-448)",
      "start_page": 441,
      "end_page": 448,
      "summary": "Provide system documentation: Making users aware of the type of AI\nResponsible AI within your application.\nHowever, it’s important to note that Responsible AI is not just about the\nResponsible AI\nRegulation of AI is becoming increasingly systematic and stringent, with\nAI usage.\nfederal agencies’ use of new technologies, including AI.\nAct (AI Act), which seeks to establish a comprehensive regulatory\nAI systems in the EU are subject to the AI Act. This includes both\nUsers: Users who utilize AI systems within the EU fall under the scope\nImporters: Entities that import AI systems into the EU market are also\nsubject to compliance with the AI Act. Distributors: Distributors who place AI systems on the EU market are\nprovide AI services or products to EU residents are subject to certain\nprovisions of the AI Act. By categorizing AI systems by risk, the AI Act outlines the development and\nestablishes an EU AI Office for enforcement, and mandates member states to\nResponsible AI principles, emphasizing fairness, accountability,\nProviders of generative AI systems must train, design, and develop their\nIf a generative AI system has been used to create “deep fakes,” users\nmanipulated by AI.\nThe AI Act represents a significant step toward ensuring that AI technologies\ngenerative AI technologies, significant strides were made regarding the AI\nregulation, titled the AI Act, with the aim of establishing unified\nregulations on AI and modifying certain European Union legislative\nThese developments signify the ongoing progress of the AI Act toward its\noversight or regulation for generative AI, given the advanced negotiations\napproach the questions posed by AI.\nrecognition of the need for Responsible AI and the role of government in\nIn this chapter, we covered the “dark side” of generative AI technologies,\nintroduced the concept of Responsible AI, starting with a deep dive into the\nhave been carried out by governments in the last year, with a focus on the AI\nAct. Responsible AI is an evolving field of research, and it definitely has an",
      "keywords": [
        "Act",
        "users",
        "systems",
        "Responsible",
        "model",
        "questions",
        "possibility to navigate",
        "GlobeBotter assistant",
        "European Commission",
        "pre-defined questions",
        "Regulation",
        "Show the reasoning",
        "Bing Chat",
        "risk",
        "generative"
      ],
      "concepts": [
        "act",
        "acts",
        "provide",
        "provided",
        "regulations",
        "regulation",
        "commission",
        "developed",
        "developments",
        "user"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 1,
          "title": "",
          "score": 0.656,
          "base_score": 0.506,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 54,
          "title": "",
          "score": 0.573,
          "base_score": 0.423,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 53,
          "title": "",
          "score": 0.496,
          "base_score": 0.346,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 51,
          "title": "",
          "score": 0.471,
          "base_score": 0.321,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 55,
          "title": "",
          "score": 0.464,
          "base_score": 0.314,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ai",
          "ai act",
          "act",
          "responsible",
          "eu"
        ],
        "semantic": [],
        "merged": [
          "ai",
          "ai act",
          "act",
          "responsible",
          "eu"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.21111843870942265,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.498965+00:00"
      }
    },
    {
      "chapter_number": 53,
      "title": "Segment 53 (pages 449-458)",
      "start_page": 449,
      "end_page": 458,
      "summary": "Join our community’s Discord space for discussions with the author and\nhood of an LLM, we covered many scenarios of LLM-powered applications,\nexperimented with different models, both proprietary and open-source, and\nfuture trends in the field of generative AI.\nThe latest trends in language models and generative AI\nmodels and generative AI\nwitnessed an explosive advancement in generative models, from\nmultimodality to newly born frameworks, to enable multi-agent applications.\nGPT-4V(ision) is a large multimodal model (LMM) developed by OpenAI\nthat can be processed by the same model as text.\nThis allows the model to\nhandle different types of data, such as text and images, and generate outputs\nsets GPT-4 apart from other language and machine learning models is its\nOpenAI has implemented several mitigations to address risks and biases.\nThese mitigations are aimed at improving a model’s safety and reducing the\nobviously harmful generations in GPT-4V.\nmodel from generating content that promotes hate groups or contains\nrisks in a model’s output.\nScientific competence: Red teamers evaluated GPT-4V’s abilities and\nWhile the model demonstrated the\nHateful content: GPT-4V declines to answer questions about hate\nHowever, the model’s\nThe model now\nthese mitigations to enable the model to answer questions about people\nDisinformation risks: GPT-4V’s ability to generate text content\ncombination of generative image models and GPT-4V’s text generation\nthese risks and remains committed to refining and improving a model’s\nOverall, the GPT-4V has unveiled extraordinary capabilities and paves the\nway for multimodality within LLM-powered applications.\nThe newest version of OpenAI’s image-generation tool, DALL-E 3, came\nhttps://openai.com/dall-e-3\npowered agents to cooperate with each other to solve users’ tasks.\nhttps://github.com/microsoft/autogen/tree/main.\nAgents leveraging external tools.\nIn those scenarios, we had one agent\ncan actually generate output that serves as input to other agents, as well as\nApplications via Multi-Agent Conversation by Wu et al., there are three main\nreasons why the multi-agent conversation exhibits great performance:\nAdaptability: Since LLMs are general-purpose models that can adapt\nagents that leverage the various capabilities of LLMs in a modular and\nHenceforth, multi-agent conversations can\nTo enable a multi-agent conversation, there are two main components to be\nand have different capabilities, such as using LLMs, human input, or\nllm-applications-via-multi-agent-conversation-\nAutoGen provides a class of agents\nYou can find some examples of different applications of the AutoGen\nhttps://microsoft.github.io/autogen/docs/Examples#a\nsmaller models can be useful as well.\nSmall language models\nThis class of models has paved the way for\nwhat are now called small language models (SLMs).",
      "keywords": [
        "model",
        "LLMs",
        "agents",
        "OpenAI",
        "language models",
        "applications",
        "AutoGen",
        "images",
        "LLM",
        "generative image models",
        "risks",
        "content",
        "multi-agent",
        "racist and sexist",
        "generative"
      ],
      "concepts": [
        "generative",
        "generate",
        "generation",
        "images",
        "different",
        "openai",
        "agents",
        "models",
        "risks",
        "hate"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 54,
          "title": "",
          "score": 0.819,
          "base_score": 0.669,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 51,
          "title": "",
          "score": 0.711,
          "base_score": 0.561,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 13,
          "title": "",
          "score": 0.711,
          "base_score": 0.561,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 14,
          "title": "",
          "score": 0.693,
          "base_score": 0.543,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 1,
          "title": "",
          "score": 0.67,
          "base_score": 0.52,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "gpt 4v",
          "4v",
          "multi agent",
          "multi",
          "autogen"
        ],
        "semantic": [],
        "merged": [
          "gpt 4v",
          "4v",
          "multi agent",
          "multi",
          "autogen"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3662453290820188,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.499023+00:00"
      }
    },
    {
      "chapter_number": 54,
      "title": "Segment 54 (pages 459-466)",
      "start_page": 459,
      "end_page": 466,
      "summary": "leveraging generative AI for their processes, services, and products.\ngenerative AI\ncompanies in different industries started embracing generative AI within\nCoca-Cola partnered with Bain & Company and OpenAI to leverage DALL-\nE, a generative AI model.\nOpenAI’s ChatGPT and DALL-E platforms will help Coca-Cola create\n& Company (https://www.coca-colacompany.com/media-\nreal-magic-using-new-ai-platform).\ninnovation that merges the abilities of GPT-4, which generates text that\nNotion has introduced a new feature called Notion AI that uses generative\nAI.\nThe following screenshot shows some of the Notion features powered by\ngenerative AI:\nFigure 13.2: Some features of Notion AI.\nSource: https://www.notion.so/product/ai\nNotion AI is powered by OpenAI’s GPT models and integrated into the core\nthat will generate text, as well as apply AI to text they’ve already written or\nThis makes Notion AI a powerful digital assistant that enhances\nplatform with a proprietary AI core.\nMalbek uses generative AI to offer a feature powered by LLMs and featuring\nThis remarkable new feature enables users to speed up negotiation time and\nSince its partnership with OpenAI, Microsoft has started infusing AI\nIn 2023, Microsoft released several copilots within its products, such as the\nFigure 13.3: Microsoft Bing Chat\nbenchmark for copilots and AI-powered products.\nadvancements in the field of generative AI.\nLLMs, such as Notion and Microsoft.\nGenerative AI has shown to be the most promising and exciting field of AI,\nTherefore, as we explore the new horizons of generative AI, we\nWe should strive to use generative AI for good purposes and\nNevertheless, generative AI is an\nDALL-E 3: James Betker, Improving Image Generation with Better\nNotion AI: https://www.notion.so/product/ai\nai-platform\ngpt-malbek-unveils-generative-ai-functionality\nMicrosoft Copilot: https://www.microsoft.com/en-",
      "keywords": [
        "Notion",
        "generative",
        "Bing Chat",
        "Microsoft",
        "Microsoft Bing Chat",
        "text",
        "users",
        "Chat",
        "Bing",
        "Notion features powered",
        "Coca-Cola",
        "OpenAI",
        "powered",
        "companies",
        "Malbek"
      ],
      "concepts": [
        "notion",
        "generative",
        "generate",
        "generation",
        "making",
        "makes",
        "coca",
        "models",
        "powering",
        "customer"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 53,
          "title": "",
          "score": 0.819,
          "base_score": 0.669,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 1,
          "title": "",
          "score": 0.743,
          "base_score": 0.593,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 13,
          "title": "",
          "score": 0.599,
          "base_score": 0.449,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 14,
          "title": "",
          "score": 0.578,
          "base_score": 0.428,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 52,
          "title": "",
          "score": 0.573,
          "base_score": 0.423,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "notion",
          "ai",
          "generative ai",
          "generative",
          "microsoft"
        ],
        "semantic": [],
        "merged": [
          "notion",
          "ai",
          "generative ai",
          "generative",
          "microsoft"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3240312146624549,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.499077+00:00"
      }
    },
    {
      "chapter_number": 55,
      "title": "Segment 55 (pages 467-474)",
      "start_page": 467,
      "end_page": 474,
      "summary": "Join our community’s Discord space for discussions with the author and\npackt.com\nSubscribe to our online digital library for full access to over 7,000 books\nAt www.packt.com, you can also read a collection of free technical\ndiscounts and offers on Packt books and eBooks.\nOther Books You May\nIf you enjoyed this book, you may be interested in these other books by\nPackt:\nGenerative AI with LangChain\nGrasp generative AI fundamentals and industry trends\nLearn how to pretrain and fine-tune LLMs\nPackt is searching for\nIf you’re interested in becoming an author for Packt, please visit\nIf you purchased the book from Amazon, please click\nbook and share your feedback or leave a review on the site that you\nYour review is important to us and the tech community and will help us\nmake sure we’re delivering excellent quality content.\nagent 109, 110\nagentic approach 154",
      "keywords": [
        "LLMs",
        "books",
        "community",
        "Discord",
        "Packt books",
        "Join",
        "Join our community",
        "Fine-tune LLMs",
        "industry",
        "Discord space",
        "free",
        "LLM Powered Application",
        "Building LLM Powered",
        "author",
        "Learn"
      ],
      "concepts": [
        "data",
        "llm",
        "agent",
        "transformers",
        "language",
        "llms",
        "grasp",
        "react",
        "free",
        "generative"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 1,
          "title": "",
          "score": 0.795,
          "base_score": 0.645,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 59,
          "title": "",
          "score": 0.592,
          "base_score": 0.442,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 4,
          "title": "",
          "score": 0.569,
          "base_score": 0.569,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 54,
          "title": "",
          "score": 0.549,
          "base_score": 0.399,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 53,
          "title": "",
          "score": 0.507,
          "base_score": 0.357,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "packt",
          "books",
          "community",
          "discord",
          "author"
        ],
        "semantic": [],
        "merged": [
          "packt",
          "books",
          "community",
          "discord",
          "author"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2824522236021003,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.499240+00:00"
      }
    },
    {
      "chapter_number": 56,
      "title": "Segment 56 (pages 475-482)",
      "start_page": 475,
      "end_page": 482,
      "summary": "artificial intelligence (AI) 226\nartificial neural networks (ANNs) 4\nassistant models 52\nversus base models 52\nautoregressive model 52\nsingle tool, leveraging 231-234\nbase models 17, 52\nversus assistant models 52\nversus customized models 20\nBERT model, components\n(BERT) model 15, 264, 268\ncharacter-based approach 268\ncode\nlarge language model (LLM), selecting for 196, 197\ncode generation 70, 197, 198\ncompletion model 96\nbuilding 159-163\nexternal tools, adding 129-131\ncustomized models\nversus base models 20\ndata\ntools, adding to 187-190\ndeep neural networks (DNNs) 146\ndata, tokenizing 267-270\nmodel 270, 271\nfoundation model 2-4\nfull-model fine-tuning 147\nGeneral Language Understanding Evaluation (GLUE) 18, 19,\ngenerative AI\nversus natural language understanding (NLU) 3\nGenerative Pre-trained Transformer 3 (GPT-3) 15\ngenerative pretrained transformer (GPT) models 42\nhard-coded approach 228\nversus agentic, custom approach 256\nversus out-of-the-box approach 256",
      "keywords": [
        "reference link",
        "models",
        "versus",
        "language",
        "link",
        "artificial",
        "approach",
        "reference",
        "Web Services",
        "Amazon Web Services",
        "code",
        "adding",
        "Chat reference link",
        "Multi Agent Chat",
        "data"
      ],
      "concepts": [
        "models",
        "code",
        "language",
        "encoding",
        "generation",
        "generative",
        "artificial",
        "feature",
        "tool",
        "customized"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 57,
          "title": "",
          "score": 0.856,
          "base_score": 0.706,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 49,
          "title": "",
          "score": 0.776,
          "base_score": 0.626,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 2,
          "title": "",
          "score": 0.756,
          "base_score": 0.606,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 5,
          "title": "",
          "score": 0.724,
          "base_score": 0.574,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 58,
          "title": "",
          "score": 0.722,
          "base_score": 0.572,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "versus",
          "52",
          "approach",
          "52 versus",
          "models"
        ],
        "semantic": [],
        "merged": [
          "versus",
          "52",
          "approach",
          "52 versus",
          "models"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.373983406978436,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.499302+00:00"
      }
    },
    {
      "chapter_number": 57,
      "title": "Segment 57 (pages 483-490)",
      "start_page": 483,
      "end_page": 490,
      "summary": "large language model (LLM), working via 112\nknowledge-based filtering 140\nLangChain 34, 92, 93, 210\nused, for building multimodal agent 227\nLangChain, components 93\nmemory 102-104\nmodels 94, 95\nlanguage-specific foundation models 56-59\nlarge foundation models (LFMs) 2, 25, 56, 226\nlarge language model (LLM) 65\nLarge Language Model Meta AI 2 (LLaMA-2) 52, 53\nLarge Language Model Prompt Engineering for Complex\nlarge language model (LLM) 1, 2, 8, 10, 41, 147, 169, 261, 279\nassistant models versus base models 52\nmodel evaluation 18-20\nopen-source model 51, 52\nproprietary models 42\nlarge multimodal model (LMM) 294\nmachine learning (ML) 141\nmasked language modeling (MLM) 271\nconversation buffer memory 103\nconversation buffer window memory 103\nconversation knowledge graph memory 103\nconversation summary buffer memory 104\nconversation summary memory 104\nconversation token buffer memory 104\nMistral model 54, 55\nmodel\nmodel customization\nmultimodal agent\nbuilding, with LangChain 227\ninput layer 6\noutput layer 6\nsequential models 145\nopen-source model",
      "keywords": [
        "reference link",
        "Hugging Face Hub",
        "Hugging Face",
        "link",
        "language",
        "large language model",
        "language model",
        "model",
        "Face reference link",
        "large language",
        "reference",
        "Face Hub",
        "Hugging Face reference",
        "memory",
        "Face"
      ],
      "concepts": [
        "models",
        "memory",
        "agents",
        "knowledge",
        "llm",
        "langchain",
        "non",
        "object",
        "token",
        "filtering"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 56,
          "title": "",
          "score": 0.856,
          "base_score": 0.706,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 2,
          "title": "",
          "score": 0.848,
          "base_score": 0.698,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 42,
          "title": "",
          "score": 0.74,
          "base_score": 0.59,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 5,
          "title": "",
          "score": 0.73,
          "base_score": 0.58,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 58,
          "title": "",
          "score": 0.681,
          "base_score": 0.531,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "memory",
          "conversation",
          "large",
          "model",
          "large language"
        ],
        "semantic": [],
        "merged": [
          "memory",
          "conversation",
          "large",
          "model",
          "large language"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.39210015643146656,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.499362+00:00"
      }
    },
    {
      "chapter_number": 58,
      "title": "Segment 58 (pages 491-498)",
      "start_page": 491,
      "end_page": 498,
      "summary": "parameter-efficient fine-tuning 147\npost-truncation 269\npre-truncation 269\nprompt engineering 30, 31, 65, 183-186\nPython\nrelational databases, working with 175-177\nPython REPL agent\nrecommendation systems 140\nrecommendation systems, types\ncontent-based filtering 140\nknowledge-based filtering 140\nworking with, in Python 175-178\nmodel level 282-284\nretrieval-augmented generation (RAG) 28\nsemi-structured data 170, 171\nSoftmax function 9\nSQL databases 173\nstructured data 169, 170\ntext embedding models 98-100\ntext generation 246-248\nText-to-Text Transfer Transformer (T5) 15\nbuilding 245\ntransformer architecture 11-15\ntransformers 10\ntruncation 268\nuser-based collaborative filtering 140\nuser-based KNN 141\nuser behavior data 141\nuser demographic data 141\nvector store retriever 102\nword-based approach 268",
      "keywords": [
        "parameter-efficient fine-tuning",
        "parametric knowledge",
        "Predict Paradigm",
        "Prompt",
        "data",
        "filtering",
        "SQL",
        "parameters",
        "prompt engineering",
        "databases",
        "learning",
        "Python",
        "text",
        "engineering",
        "parameter-efficient"
      ],
      "concepts": [
        "model",
        "data",
        "prompt",
        "user",
        "filtering",
        "transformer",
        "approach",
        "generation",
        "retrieval",
        "retriever"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 56,
          "title": "",
          "score": 0.722,
          "base_score": 0.572,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 30,
          "title": "",
          "score": 0.709,
          "base_score": 0.559,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 29,
          "title": "",
          "score": 0.689,
          "base_score": 0.539,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 57,
          "title": "",
          "score": 0.681,
          "base_score": 0.531,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 2,
          "title": "",
          "score": 0.573,
          "base_score": 0.423,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "140",
          "filtering",
          "parameter efficient",
          "filtering 140",
          "truncation"
        ],
        "semantic": [],
        "merged": [
          "140",
          "filtering",
          "parameter efficient",
          "filtering 140",
          "truncation"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3129084914782983,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.499417+00:00"
      }
    },
    {
      "chapter_number": 59,
      "title": "Segment 59 (pages 499-500)",
      "start_page": 499,
      "end_page": 500,
      "summary": "this book\nThanks for purchasing this book!\nDon’t worry, now with every Packt book you get a DRM-free PDF version\nof that book at no cost.",
      "keywords": [
        "free PDF",
        "PDF",
        "book",
        "free PDF copy",
        "Download",
        "Packt book",
        "free",
        "PDF copy",
        "DRM-free PDF",
        "PDF version",
        "Download a free",
        "copy",
        "read",
        "n’t",
        "DRM-free PDF version"
      ],
      "concepts": [
        "directly",
        "book",
        "email",
        "thanks",
        "link",
        "free",
        "content",
        "great",
        "version",
        "submit"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 55,
          "title": "",
          "score": 0.592,
          "base_score": 0.442,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 4,
          "title": "",
          "score": 0.497,
          "base_score": 0.497,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 1,
          "title": "",
          "score": 0.303,
          "base_score": 0.153,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "pdf",
          "free",
          "free pdf",
          "book",
          "drm"
        ],
        "semantic": [],
        "merged": [
          "pdf",
          "free",
          "free pdf",
          "book",
          "drm"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.13669099793290962,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:11.499457+00:00"
      }
    }
  ],
  "total_chapters": 59,
  "enrichment_provenance": {
    "taxonomy_id": "none",
    "taxonomy_version": "none",
    "taxonomy_path": "none",
    "taxonomy_checksum": "sha256:none",
    "source_metadata_file": "Building LLM Powered Applications_metadata.json",
    "enrichment_date": "2025-12-17T23:01:11.506932+00:00",
    "enrichment_method": "msep",
    "model_version": "ai-agents-msep-v1",
    "processing_time_ms": 5379.343336000602,
    "total_similar_chapters": 293
  }
}