{
  "metadata": {
    "title": "Game Engine Architecture",
    "author": "Jason Gregory",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 853,
    "conversion_date": "2025-11-27T15:35:15.230302",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Game_Engine_Architecture.pdf"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-18)",
      "start_page": 1,
      "end_page": 18,
      "detection_method": "synthetic",
      "content": "Game Engine Architecture\n\nJason Gregory\n\nForeword by Jeff Lander and Matt Whiting\n\nGame Engine Architecture\n\n\nGame Engine Architecture\nJason Gregory\nA K Peters, Ltd.\nWellesley, Massachusetts\n\n\nA K Peters/CRC Press\nTaylor & Francis Group\n6000 Broken Sound Parkway NW, Suite 300\nBoca Raton, FL 33487-2742\n© 2009 by Taylor and Francis Group, LLC\nA K Peters/CRC Press is an imprint of Taylor & Francis Group, an Informa business\nNo claim to original U.S. Government works\nPrinted in the United States of America on acid-free paper\n10 9 8 7 6 5 4 3 2 1\nInternational Standard Book Number-13: 978-1-4398-6526-2 (Ebook-PDF)\nThis book contains information obtained from authentic and highly regarded sources. Reasonable efforts have been made to publish reliable data and information, but the author \nand publisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders \nof all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been \nacknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or utilized in any form by any electronic, mechanical, or other means, \nnow known or hereafter invented, including photocopying, microfilming, and recording, or in any information storage or retrieval system, without written permission from the \npublishers.\nFor permission to photocopy or use material electronically from this work, please access www.copyright.com (http://www.copyright.com/) or contact the Copyright Clearance \nCenter, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 978-750-8400. CCC is a not-for-profit organization that provides licenses and registration for a variety of users. For \norganizations that have been granted a photocopy license by the CCC, a separate system of payment has been arranged.\nTrademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used only for identification and explanation without intent to infringe.\nVisit the Taylor & Francis Web site at\nhttp://www.taylorandfrancis.com\nand the A K Peters Web site at\nhttp://www.akpeters.com \n\n\nDedicated to\nTrina, Evan and Quinn Gregory,\nin memory of our heros,\nJoyce Osterhus and Kenneth Gregory.\n\n\nvii\nContents\n \nForeword \nxiii\n \nPreface \nxvii\nI \nFoundations \n1\n1 \nIntroduction \n3\n1.1  \nStructure of a Typical Game Team \n5\n1.2 \nWhat Is a Game? \n8\n1.3 \nWhat Is a Game Engine? \n11\n1.4 \nEngine Differences Across Genres \n13\n1.5 \nGame Engine Survey \n25\n1.6 \nRuntime Engine Architecture \n28\n1.7 \n Tools and the Asset Pipeline \n49\n2 \nTools of the Trade \n57\n2.1 \n Version Control \n57\n2.2 \nMicrosoft Visual Studio \n66\n2.3 \nProﬁ ling Tools \n85\n\n\nviii \nContents\n2.4 \nMemory Leak and Corruption Detection \n87\n2.5 \nOther Tools \n88\n3 \nFundamentals of Software \n \nEngineering for Games \n91\n3.1 \nC++ Review and Best Practices \n91\n3.2 \nData, Code, and Memory in C/C++ \n98\n3.3 \nCatching and Handling Errors \n128\n4 \n3D Math for Games \n137\n4.1 \nSolving 3D Problems in 2D \n137\n4.2 \nPoints and Vectors \n138\n4.3 \nMatrices \n151\n4.4 \nQuaternions \n169\n4.5 \nComparison of Rotational Representations \n177\n4.6 \nOther Useful Mathematical Objects \n181\n4.7 \nHardware-Accelerated SIMD Math \n185\n4.8 \nRandom Number Generation \n192\nII \nLow-Level Engine Systems \n195\n5 \nEngine Support Systems \n197\n5.1 \nSubsystem Start-Up and Shut-Down \n197\n5.2 \nMemory Management \n205\n5.3 \nContainers \n223\n5.4 \nStrings \n242\n5.5 \nEngine Conﬁ guration \n252\n6 \nResources and the File System \n261\n6.1 \nFile System \n262\n6.2 \nThe Resource Manager \n272\n7 \nThe Game Loop and Real-Time Simulation \n303\n7.1 \nThe Rendering Loop \n303\n7.2 \nThe Game Loop \n304\n\n\nix \nContents\n7.3 \nGame Loop Architectural Styles \n307\n7.4 \nAbstract Timelines \n310\n7.5 \nMeasuring and Dealing with Time \n312\n7.6 \nMultiprocessor Game Loops \n324\n7.7 \nNetworked Multiplayer Game Loops \n333\n8 \nHuman Interface Devices (HID) \n339\n8.1 \nTypes of Human Interface Devices \n339\n8.2 \nInterfacing with a HID \n341\n8.3 \nTypes of Inputs \n343\n8.4 \nTypes of Outputs \n348\n8.5 \nGame Engine HID Systems \n349\n8.6 \nHuman Interface Devices in Practice \n366\n9 \nTools for Debugging and Development \n367\n9.1 \nLogging and Tracing \n367\n9.2 \nDebug Drawing Facilities \n372\n9.3 \nIn-Game Menus \n379\n9.4 \nIn-Game Console \n382\n9.5 \nDebug Cameras and Pausing the Game \n383\n9.6 \nCheats \n384\n9.7 \nScreen Shots and Movie Capture \n384\n9.8 \nIn-Game Proﬁ ling \n385\nIII Graphics and Motion \n397\n10 The Rendering Engine \n399\n10.1 \nFoundations of Depth-Buffered \n \nTriangle Rasterization \n400\n10.2 \nThe Rendering Pipeline \n444\n10.3 \nAdvanced Lighting and Global Illumination \n469\n10.4 \nVisual Effects and Overlays \n481\n11 \nAnimation Systems \n491\n11.1 \nTypes of Character Animation \n491\n11.2 \nSkeletons \n496\n\n\nx \nContents\n11.3 \nPoses \n499\n11.4 \nClips \n504\n11.5 \nSkinning and Matrix Palette Generation \n518\n11.6 \nAnimation Blending \n523\n11.7 \nPost-Processing \n542\n11.8 \nCompression Techniques \n545\n11.9 \nAnimation System Architecture \n552\n11.10 \nThe Animation Pipeline \n553\n11.11 \nAction State Machines \n568\n11.12 \nAnimation Controllers \n593\n12 \nCollision and Rigid Body Dynamics \n595\n12.1 \nDo You Want Physics in Your Game? \n596\n12.2 \nCollision/Physics Middleware \n601\n12.3 \nThe Collision Detection System \n603\n12.4 \nRigid Body Dynamics \n630\n12.5 \nIntegrating a Physics Engine into Your Game \n666\n12.6 \nA Look Ahead: Advanced Physics Features \n684\nIV Gameplay \n687\n13 \nIntroduction to Gameplay Systems \n689\n13.1 \nAnatomy of a Game World \n690\n13.2 \nImplementing Dynamic Elements: Game Objects \n695\n13.3 \nData-Driven Game Engines \n698\n13.4 \nThe Game World Editor \n699\n14 \nRuntime Gameplay Foundation Systems \n711\n14.1 \nComponents of the Gameplay \n \nFoundation System \n711\n14.2 \nRuntime Object Model Architectures \n715\n14.3 \nWorld Chunk Data Formats \n734\n14.4 \nLoading and Streaming Game Worlds \n741\n14.5 \nObject References and World Queries \n750\n14.6 \nUpdating Game Objects in Real Time \n757\n\n\nxi \nContents\n14.7 \nEvents and Message-Passing \n773\n14.8 \nScripting \n794\n14.9 \nHigh-Level Game Flow \n817\nV Conclusion \n819\n15 \nYou Mean There’s More? \n821\n15.1 \nSome Engine Systems We Didn’t Cover \n821\n15.2 \nGameplay Systems \n823\n \nReferences \n827\n \nIndex \n831\n\n\nxiii\nForeword\nT\nhe very ﬁ rst video game was built entirely out of hardware, but rapid ad-\nvancements in microprocessors have changed all that. These days, video \ngames are played on versatile PCs and specialized video game consoles that \nuse soft ware to make it possible to oﬀ er a tremendous variety of gaming ex-\nperiences. It’s been 50 years since those ﬁ rst primitive games, but the industry \nis still considered by many to be immature. It may be young, but when you \ntake a closer look, you will ﬁ nd that things have been developing rapidly. \nVideo games are now a multibillion-dollar industry covering a wide range of \ndemographics.\nVideo games come in all shapes and sizes, falling into categories or \n“genres” covering everything from solitaire to massively multiplayer online \nrole-playing games, and these games are played on virtually anything with a \nmicrochip in it. These days, you can get games for your PC, your cell phone, \nas well as a number of diﬀ erent specialized gaming consoles—both handheld \nand those that connect to your home TV. These specialized home consoles \ntend to represent the cutt ing edge of gaming technology, and the patt ern of \nthese platforms being released in cycles has come to be called console “gen-\nerations.” The powerhouses of this latest generation are Microsoft ’s Xbox 360 \nand Sony’s PLAYSTATION 3, but the ever-present PC should never be over-\nlooked, and the extremely popular Nintendo Wii represents something new \nthis time around. \n\n\nxiv \nForeword\nThe recent explosion of downloadable and casual games has added even \nmore complexity to the diverse world of commercial video games. Even so, \nbig games are still big business. The incredible computing power available \non today’s complicated platforms has made room for increased complexity in \nthe soft ware. Naturally, all this advanced soft ware has to be created by some-\none, and that has driven up the size of development teams—not to mention \ndevelopment costs. As the industry matures, we’re always looking for bett er, \nmore eﬃ  cient ways to build our products, and development teams have be-\ngun compensating for the increased complexity by taking advantage of things \nlike reusable soft ware and middleware.\nWith so many diﬀ erent styles of game on such a wide array of platforms, \nthere cannot be any single ideal soft ware solution. However, certain patt erns \nhave developed, and there is a vast menu of potential solutions out there. The \nproblem today is choosing the right solution to ﬁ t the needs of the particular \nproject. Going deeper, a development team must consider all the diﬀ erent as-\npects of a project and how they ﬁ t together. It is rare to ﬁ nd any one soft ware \npackage that perfectly suits every aspect of a new game design.\nThose of us who are now veterans of the industry found ourselves pio-\nneering unknown territory. Few programmers of our generation have Com-\nputer Science degrees (Matt ’s is in Aeronautical Engineering, and Jason’s is \nin Systems Design Engineering), but these days many colleges are starting to \nprograms and degrees in video games. The students and developers of today \nneed a good place to turn to for solid game-development information. For \npure high-end graphics, there are a lot of sources of very good information \nfrom research to practical jewels of knowledge. However, these sources are \noft en not directly applicable to production game environments or suﬀ er from \nnot having actual production-quality implementations. For the rest of game \ndevelopment, there are so-called beginner books that so gloss over the details \nand act as if they invented everything without giving references that they are \njust not useful or oft en even accurate. Then there are high-end specialty books \nfor various niches like physics, collision, AI, etc. But these can be needlessly \nobtuse or too high level to be understood by all, or the piecemeal approach just \ndoesn’t all ﬁ t together. Many are even so directly tied to a particular piece of \ntechnology as to become rapidly dated as the hardware and soft ware change.\nThen there is the Internet, which is an excellent supplementary tool for \nknowledge gathering. However, broken links, widely inaccurate data, and \nvariable-to-poor quality oft en make it not useful at all unless you know ex-\nactly what you are aft er.\nEnter Jason Gregory, himself an industry veteran with experience at \nNaughty Dog—one of the most highly regarded video game studios in the \n\n\nxv \nForeword\nworld. While teaching a course in game programming at USC, Jason found \nhimself facing a shortage of textbooks covering the fundamentals of video-\ngame architecture. Luckily for the rest of us, he has taken it upon himself to \nﬁ ll that gap.\nWhat Jason has done is pull together production-quality knowledge actu-\nally used in shipped game projects and bring together the entire game-devel-\nopment picture. His experience has allowed him to bring together not only \nthe ideas and techniques but also actual code samples and implementation \nexamples to show you how the pieces come together to actually make a game. \nThe references and citations make it a great jumping-oﬀ  point to dig deeper \ninto any particular aspect of the process. The concepts and techniques are the \nactual ones we use to create games, and while the examples are oft en ground-\ned in a technology, they extend way beyond any particular engine or API.\nThis is the kind of book we wanted when we were gett ing started, and we \nthink it will prove very instructive to people just starting out as well as those \nwith experience who would like some exposure to the larger context.\nJeﬀ  Lander\nMatt hew Whiting\n",
      "page_number": 1,
      "chapter_number": 1,
      "summary": "This chapter covers segment 1 (pages 1-18). Key topics include game, gaming, and engine.",
      "keywords": [
        "Game Engine Architecture",
        "Game",
        "Game Engine",
        "Engine",
        "Engine Architecture",
        "Game Engine HID",
        "video games",
        "Whiting Game Engine",
        "Game Loop",
        "Engine HID Systems",
        "Game Engine Survey",
        "Engine Systems",
        "Animation System Architecture",
        "Francis Group",
        "Engine Architecture Jason"
      ],
      "concepts": [
        "game",
        "gaming",
        "engine",
        "engineering",
        "systems",
        "animation",
        "contents",
        "video",
        "world",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 3",
          "chapter": 11,
          "title": "Segment 11 (pages 111-118)",
          "relevance_score": 0.74,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 47,
          "title": "Segment 47 (pages 448-460)",
          "relevance_score": 0.7,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 26,
          "title": "Segment 26 (pages 243-252)",
          "relevance_score": 0.69,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.67,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 12,
          "title": "Segment 12 (pages 101-108)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 19-41)",
      "start_page": 19,
      "end_page": 41,
      "detection_method": "synthetic",
      "content": "xvii\nPreface\nW\nelcome to Game Engine Architecture. This book aims to present a com-\nplete discussion of the major components that make up a typical com-\nmercial game engine. Game programming is an immense topic, so we have a \nlot of ground to cover. Nevertheless, I trust you’ll ﬁ nd that the depth of our \ndiscussions is suﬃ  cient to give you a solid understanding of both the theory \nand the common practices employed within each of the engineering disci-\nplines we’ll cover. That said, this book is really just the beginning of a fasci-\nnating and potentially life-long journey. A wealth of information is available \non all aspects of game technology, and this text serves both as a foundation-\nlaying device and as a jumping-oﬀ  point for further learning.\nOur focus in this book will be on game engine technologies and architec-\nture. This means we’ll cover both the theory underlying the various subsys-\ntems that comprise a commercial game engine and also the data structures, \nalgorithms, and soft ware interfaces that are typically used to implement them. \nThe line between the game engine and the game is rather blurry. We’ll fo-\ncus primarily on the engine itself, including a host of low-level foundation \nsystems, the rendering engine, the collision system, the physics simulation, \ncharacter animation, and an in-depth discussion of what I call the gameplay \nfoundation layer. This layer includes the game’s object model, world editor, \nevent system, and scripting system. We’ll also touch on some aspects of game-\n\n\nxviii \nPreface\nplay programming, including player mechanics, cameras, and AI. However, \nby necessity, the scope of these discussions will be limited mainly to the ways \nin which gameplay systems interface with the engine.\nThis book is intended to be used as a course text for a two- or three-course \ncollege-level series in intermediate game programming. Of course, it can also \nbe used by amateur soft ware engineers, hobbyists, self-taught game program-\nmers, and existing members of the game industry alike. Junior engineers can \nuse this text to solidify their understanding of game mathematics, engine ar-\nchitecture, and game technology. And some senior engineers who have de-\nvoted their careers to one particular specialty may beneﬁ t from the bigger \npicture presented in these pages, as well.\nTo get the most out of this book, you should have a working knowledge \nof basic object-oriented programming concepts and at least some experience \nprogramming in C++. Although a host of new and exciting languages are be-\nginning to take hold within the game industry, industrial-strength 3D game \nengines are still writt en primarily in C or C++, and any serious game pro-\ngrammer needs to know C++. We’ll review the basic tenets of object-oriented \nprogramming in Chapter 3, and you will no doubt pick up a few new C++ \ntricks as you read this book, but a solid foundation in the C++ language is best \nobtained from [39], [31], and [32]. If your C++ is a bit rusty, I recommend you \nrefer to these or similar books to refresh your knowledge as you read this text. \nIf you have no prior C++ experience, you may want to consider reading at least \nthe ﬁ rst few chapters of [39], or working through a few C++ tutorials online, \nbefore diving into this book.\nThe best way to learn computer programming of any kind is to actually \nwrite some code. As you read through this book, I strongly encourage you to \nselect a few topic areas that are of particular interest to you and come up with \nsome projects for yourself in those areas. For example, if you ﬁ nd character \nanimation interesting, you could start by installing Ogre3D and exploring its \nskinned animation demo. Then you could try to implement some of the anima-\ntion blending techniques described in this book, using Ogre. Next you might \ndecide to implement a simple joypad-controlled animated character that can \nrun around on a ﬂ at plane. Once you have something relatively simple work-\ning, expand upon it! Then move on to another area of game technology. Rinse \nand repeat. It doesn’t particularly matt er what the projects are, as long as \nyou’re practicing the art of game programming, not just reading about it.\nGame technology is a living, breathing thing that can never be entirely \ncaptured within the pages of a book. As such, additional resources, errata, \nupdates, sample code, and project ideas will be posted from time to time on \nthis book’s website at htt p://gameenginebook.com.\n\n\nxix \nPreface\nAcknowledgments\nNo book is created in a vacuum, and this one is certainly no exception. This \nbook would not have been possible without the help of my family, friends, \nand colleagues in the game industry, and I’d like to extend warm thanks to \neveryone who helped me to bring this project to fruition.\nOf course, the ones most impacted by a project like this one are invariably \nthe author’s family. So I’d like to start by oﬀ ering a special thank-you to my \nwife Trina, who has been a pillar of strength during this diﬃ  cult time, tak-\ning care of our two boys Evan (age 5) and Quinn (age 3) day aft er day (and \nnight aft er night!) while I holed myself up to get yet another chapter under \nmy belt, forgoing her own plans to accommodate my schedule, doing my \nchores as well as her own (more oft en than I’d like to admit), and always giv-\ning me kind words of encouragement when I needed them the most. I’d also \nlike to thank my eldest son Evan for being patient as he endured the absence \nof his favorite video game playing partner, and his younger brother Quinn \nfor always welcoming me home aft er a long day’s work with huge hugs and \nendless smiles.\nI would also like to extend special thanks to my editors, Matt  Whiting and \nJeﬀ  Lander. Their insightful, targeted, and timely feedback was always right \non the money, and their vast experience in the game industry has helped to \ngive me conﬁ dence that the information presented in these pages is as accu-\nrate and up-to-date as humanly possible. Matt  and Jeﬀ  were both a pleasure \nto work with, and I am honored to have had the opportunity to collaborate \nwith such consummate professionals on this project. I’d like to thank Jeﬀ  in \nparticular for putt ing me in touch with Alice Peters and helping me to get this \nproject oﬀ  the ground in the ﬁ rst place.\nA number of my colleagues at Naughty Dog also contributed to this \nbook, either by providing feedback or by helping me with the structure \nand topic content of one of the chapters. I’d like to thank Marshall Robin \nand Carlos Gonzalez-Ochoa for their guidance and tutelage as I wrote the \nrendering chapter, and Pål-Kristian Engstad for his excellent and insightful \nfeedback on the text and content of that chapter. I’d also like to thank Chris-\ntian Gyrling for his feedback on various sections of the book, including the \nchapter on animation (which is one of his many specialties). My thanks also \ngo to the entire Naughty Dog engineering team for creating all of the in-\ncredible game engine systems that I highlight in this book. Special thanks \ngo to Keith Schaeﬀ er of Electronic Arts for providing me with much of the \nraw content regarding the impact of physics on a game, found in Section \n12.1. I’d also like to thank Paul Keet of Electronic Arts and Steve Ranck, the \n\n\nxx \nPreface\nlead engineer on the Hydro Thunder project at Midway San Diego, for their \nmentorship and guidance over the years. While they did not contribute to \nthe book directly, their inﬂ uences are echoed on virtually every page in one \nway or another.\nThis book arose out of the notes I developed for a course called ITP-485: \nProgramming Game Engines, which I have been teaching under the auspices \nof the Information Technology Program at the University of Southern Cali-\nfornia for approximately three years now. I would like to thank Dr. Anthony \nBorquez, the director of the ITP department at the time, for hiring me to de-\nvelop the ITP-485 course curriculum in the ﬁ rst place. I’d also like to extend \nwarm thanks to Ashish Soni, the current ITP director, for his continued sup-\nport and encouragement as ITP-485 continues to evolve.\nMy extended family and friends also deserve thanks, in part for their un-\nwavering encouragement, and in part for entertaining my wife and our two \nboys on so many occasions while I was working. I’d like to thank my sister- and \nbrother-in-law, Tracy Lee and Doug Provins, my cousin-in-law Matt  Glenn, \nand all of our incredible friends, including: Kim and Drew Clark, Sherilyn \nand Jim Kritzer, Anne and Michael Scherer, and Kim and Mike Warner. My \nfather Kenneth Gregory wrote a book on investing in the stock market when \nI was a teenager, and in doing so he inspired me to write a book. For this and \nso much more, I am eternally grateful to him.  I’d also like to thank my mother \nErica Gregory, in part for her insistence that I embark on this project, and in \npart for spending countless hours with me when I was a child, beating the art \nof writing into my cranium—I owe my writing skills (not to mention my work \nethic… and my rather twisted sense of humor…) entirely to her!\nLast but certainly not least, I’d like to thank Alice Peters and Kevin Jack-\nson-Mead, as well as the entire A K Peters staﬀ , for their Herculean eﬀ orts \nin publishing this book. Alice and Kevin have both been a pleasure to work \nwith, and I truly appreciate both their willingness to bend over backwards to \nget this book out the door under very tight time constraints, and their inﬁ nite \npatience with me as a new author.\nJason Gregory\nApril 2009\n\n\nI\nFoundations\n\n\n1\nIntroduction\nW\nhen I got my ﬁ rst game console in 1979—a way-cool Intellivision sys-\ntem by Matt el—the term “game engine” did not exist. Back then, video \nand arcade games were considered by most adults to be nothing more than \ntoys, and the soft ware that made them tick was highly specialized to both \nthe game in question and the hardware on which it ran. Today, games are \na multi-billion-dollar mainstream industry rivaling Hollywood in size and \npopularity. And the soft ware that drives these now-ubiquitous three-dimen-\nsional  worlds—game engines like id Soft ware’s Quake and Doom engines, Epic \nGames’ Unreal Engine 3 and Valve’s Source engine—have become fully fea-\ntured reusable soft ware development kit s that can be licensed and used to \nbuild almost any game imaginable.\nWhile game engines vary widely in the details of their architecture and \nimplementation, recognizable coarse-grained patt erns are emerging across \nboth publicly licensed game engines and their proprietary in-house counter-\nparts. Virtually all game engines contain a familiar set of core components, in-\ncluding the rendering engine, the collision and physics engine, the animation \nsystem, the audio system, the game world object model, the artiﬁ cial intelli-\ngence system, and so on. Within each of these components, a relatively small \nnumber of semi-standard design alternatives are also beginning to emerge.\nThere are a great many books that cover individual game engine subsys-\ntems, such as three-dimensional graphics, in exhaustive detail. Other books \n3\n\n\n4 \n1. Introduction\ncobble together valuable tips and tricks across a wide variety of game technol-\nogy areas. However, I have been unable to ﬁ nd a book that provides its reader \nwith a reasonably complete picture of the entire gamut of components that \nmake up a modern game engine. The goal of this book, then, is to take the \nreader on a guided hands-on tour of the vast and complex landscape of game \nengine architecture.\nIn this book you will learn\nz how real industrial-strength production game engines are architected;\nz how game development teams are organized and work in the real \nworld;\nz which major subsystems and design patt erns appear again and again in \nvirtually every game engine;\nz the typical requirements for each major subsystem;\nz which subsystems are genre- or game-agnostic, and which ones are typ-\nically designed explicitly for a speciﬁ c genre or game;\nz where the engine normally ends and the game begins.\nWe’ll also get a first-hand glimpse into the inner workings of some popu-\nlar game engines, such as Quake and Unreal , and some well-known mid-\ndleware packages, such as the Havok Physics library, the OGRE rendering \nengine, and Rad Game Tools’ Granny 3D animation and geometry man-\nagement toolkit.\nBefore we get started, we’ll review some techniques and tools for large-\nscale soft ware engineering in a game engine context, including\nz the diﬀ erence between logical and physical soft ware architecture;\nz conﬁ guration management, revision control, and build systems;\nz some tips and tricks for dealing with one of the common development \nenvironments for C and C++, Microsoft  Visual Studio.\nIn this book I assume that you have a solid understanding of C++ (the \nlanguage of choice among most modern game developers) and that you un-\nderstand basic soft ware engineering principles. I also assume you have some \nexposure to linear algebra, three-dimensional vector and matrix math, and \ntrigonometry (although we’ll review the core concepts in Chapter 4). Ideally \nyou should have some prior exposure to the basic concepts of real-time and \nevent-driven programming. But never fear—I will review these topics brieﬂ y, \nand I’ll also point you in the right direction if you feel you need to hone your \nskills further before we embark.\n\n\n5 \n1.1. Structure of a Typical Game Team\n1.1. \nStructure of a Typical Game Team\nBefore we delve into the structure of a typical game engine, let’s ﬁ rst take a \nbrief look at the structure of a typical game development team. Game  stu-\ndios are usually composed of ﬁ ve basic disciplines: engineers, artists, game \ndesigners, producers, and other management and support staﬀ  (marketing, \nlegal, information technology/technical support, administrative, etc.). Each \ndiscipline can be divided into various subdisciplines. We’ll take a brief look \nat each below.\n1.1.1. \nEngineers\nThe  engineers design and implement the soft ware that makes the game, and \nthe tools, work. Engineers are oft en categorized into two basic groups: runtime \nprogrammers (who work on the engine and the game itself) and tools program-\nmers (who work on the oﬀ -line tools that allow the rest of the development \nteam to work eﬀ ectively). On both sides of the runtime/tools line, engineers \nhave various specialties. Some engineers focus their careers on a single engine \nsystem, such as rendering, artiﬁ cial intelligence, audio, or collision and phys-\nics. Some focus on gameplay programming and scripting, while others prefer \nto work at the systems level and not get too involved in how the game actu-\nally plays. Some engineers are generalists—jacks of all trades who can jump \naround and tackle whatever problems might arise during development.\nSenior engineers are sometimes asked to take on a technical leadership \nrole. Lead engineers usually still design and write code, but they also help to \nmanage the team’s schedule, make decisions regarding the overall technical \ndirection of the project, and sometimes also directly manage people from a \nhuman resources perspective.\nSome companies also have one or more technical directors (TD), whose \njob it is to oversee one or more projects from a high level, ensuring that the \nteams are aware of potential technical challenges, upcoming industry devel-\nopments, new technologies, and so on. The highest engineering-related posi-\ntion at a game studio is the chief technical oﬃ  cer (CTO), if the studio has one. \nThe CTO’s job is to serve as a sort of technical director for the entire studio, as \nwell as serving a key executive role in the company.\n1.1.2. \nArtists\nAs we say in the game industry, “content is king.” The  artists produce all of \nthe visual and audio content in the game, and the quality of their work can \nliterally make or break a game. Artists come in all sorts of ﬂ avors:\n\n\n6 \n1. Introduction\nz Concept artists produce sketches and paintings that provide the team \nwith a vision of what the ﬁ nal game will look like. They start their work \nearly in the concept phase of development, but usually continue to pro-\nvide visual direction throughout a project’s life cycle. It is common for \nscreen shots taken from a shipping game to bear an uncanny resem-\nblance to the concept art.\nz 3D modelers produce the three-dimensional geometry for everything \nin the virtual game world. This discipline is typically divided into \ntwo subdisciplines: foreground modelers and background model-\ners. The former create objects, characters, vehicles, weapons, and the \nother objects that populate the game world, while the latt er build \nthe world’s static background geometry (terrain, buildings, bridges, \netc.).\nz Texture artists create the two-dimensional images known as textures, \nwhich are applied to the surfaces of 3D models in order to provide de-\ntail and realism.\nz Lighting artists lay out all of the light sources in the game world, both \nstatic and dynamic, and work with color, intensity, and light direction to \nmaximize the artfulness and emotional impact of each scene.\nz Animators imbue the characters and objects in the game with motion. \nThe animators serve quite literally as actors in a game production, \njust as they do in a CG ﬁ lm production. However, a game animator \nmust have a unique set of skills in order to produce animations that \nmesh seamlessly with the technological underpinnings of the game \nengine.\nz  Motion capture actors are oft en used to provide a rough set of motion \ndata, which are then cleaned up and tweaked by the animators before \nbeing integrated into the game.\nz  Sound designers work closely with the engineers in order to produce and \nmix the sound eﬀ ects and music in the game.\nz  Voice actors provide the voices of the characters in many games.\nz Many games have one or more  composers, who compose an original \nscore for the game.\nAs with engineers, senior artists are oft en called upon to be team lead-\ners. Some game teams have one or more art directors—very senior artists who \nmanage the look of the entire game and ensure consistency across the work of \nall team members.\n\n\n7 \n1.1.3. \nGame Designers\nThe  game designers’ job is to design the interactive portion of the player’s \nexperience, typically known as gameplay. Diﬀ erent kinds of designers work \nat diﬀ erent levels of detail. Some (usually senior) game designers work at the \nmacro level, determining the story arc, the overall sequence of chapters or lev-\nels, and the high-level goals and objectives of the player. Other designers work \non individual levels or geographical areas within the virtual game world, lay-\ning out the static background geometry, determining where and when en-\nemies will emerge, placing supplies like weapons and health packs, designing \npuzzle elements, and so on. Still other designers operate at a highly technical \nlevel, working closely with gameplay engineers and/or writing code (oft en in \na high-level scripting language). Some game designers are ex-engineers, who \ndecided they wanted to play a more active role in determining how the game \nwill play.\nSome game teams employ one or more  writers. A game writer’s job can \nrange from collaborating with the senior game designers to construct the story \narc of the entire game, to writing individual lines of dialogue.\nAs with other disciplines, some senior designers play management roles. \nMany game teams have a game director, whose job it is to oversee all aspects \nof a game’s design, help manage schedules, and ensure that the work of indi-\nvidual designers is consistent across the entire product. Senior designers also \nsometimes evolve into producers.\n1.1.4. \nProducers\nThe role of  producer is deﬁ ned diﬀ erently by diﬀ erent studios. In some game \ncompanies, the producer’s job is to manage the schedule and serve as a hu-\nman resources manager. In other companies, producers serve in a senior game \ndesign capacity. Still other studios ask their producers to serve as liaisons be-\ntween the development team and the business unit of the company (ﬁ nance, \nlegal, marketing, etc.). Some smaller studios don’t have producers at all. For \nexample, at Naughty Dog, literally everyone in the company, including the \ntwo co-presidents, play a direct role in constructing the game; team man-\nagement and business duties are shared between the senior members of the \nstudio.\n1.1.5. \nOther Staff\nThe team of people who directly construct the game is typically supported by \na crucial team of support staﬀ . This includes the studio’s executive manage-\n1.1. Structure of a Typical Game Team\n\n\n8 \n1. Introduction\nment team, the marketing department (or a team that liaises with an external \nmarketing group), administrative staﬀ , and the IT department, whose job is \nto purchase, install, and conﬁ gure hardware and soft ware for the team and to \nprovide technical support.\n1.1.6. \nPublishers and Studios\nThe marketing, manufacture, and distribution of a game title are usually \nhandled by a publisher, not by the game studio itself. A  publisher is typically \na large corporation, like Electronic Arts, THQ, Vivendi, Sony, Nintendo, etc. \nMany game studios are not aﬃ  liated with a particular publisher. They sell \neach game that they produce to whichever publisher strikes the best deal with \nthem. Other studios work exclusively with a single publisher, either via a long-\nterm publishing contract, or as a fully owned subsidiary of the publishing \ncompany. For example, THQ’s game studios are independently managed, but \nthey are owned and ultimately controlled by THQ. Electronic Arts takes this \nrelationship one step further, by directly managing its studios.  First-party de-\nvelopers are game studios owned directly by the console manufacturers (Sony, \nNintendo, and Microsoft ). For example, Naughty Dog is a ﬁ rst-party Sony \ndeveloper. These studios produce games exclusively for the gaming hardware \nmanufactured by their parent company.\n1.2. \nWhat Is a Game?\nWe probably all have a prett y good intuitive notion of what a  game is. The \ngeneral term “game” encompasses board games like chess and Monopoly, card \ngames like poker and blackjack, casino games like roulett e and slot machines, \nmilitary war games, computer games, various kinds of play among children, \nand the list goes on. In academia we sometimes speak of “game theory,” in \nwhich multiple agents select strategies and tactics in order to maximize their \ngains within the framework of a well-deﬁ ned set of game rules. When used \nin the context of  console or computer-based entertainment, the word “game” \nusually conjures images of a three-dimensional virtual world featuring a hu-\nmanoid, animal, or vehicle as the main character under player control. (Or for \nthe old geezers among us, perhaps it brings to mind images of two-dimen-\nsional classics like Pong, Pac-Man, or Donkey Kong.)  In his excellent book, A \nTheory of Fun for Game Design, Raph Koster deﬁ nes a “game” to be an inter-\nactive experience that provides the player with an increasingly challenging \nsequence of patt erns which he or she learns and eventually masters [26]. Ko-\nster’s assertion is that the activities of learning and mastering are at the heart \n\n\n9 \n1.2. What Is a Game?\nof what we call “fun,” just as a joke becomes funny at the moment we “get it” \nby recognizing the patt ern.\nFor the purposes of this book, we’ll focus on the subset of games that \ncomprise two- and three-dimensional virtual worlds with a small number of \nplayers (between one and 16 or thereabouts). Much of what we’ll learn can \nalso be applied to Flash games on the Internet, pure puzzle games like Tetris, \nor massively multiplayer online games (MMOG). But our primary focus will \nbe on game engines capable of producing ﬁ rst-person shooters, third-person \naction/platform games, racing games, ﬁ ghting games, and the like.\n1.2.1. \nVideo Games as Soft Real-Time Simulations\nMost two- and three-dimensional video games are examples of what comput-\ner scientists would call soft  real-time interactive agent-based computer  simulations. \nLet’s break this phrase down in order to bett er understand what it means.\nIn most video games, some subset of the real world—or an imaginary \nworld—is  modeled mathematically so that it can be manipulated by a com-\nputer. The model is an approximation to and a simpliﬁ cation of reality (even \nif it’s an imaginary reality), because it is clearly impractical to include every \ndetail down to the level of atoms or quarks. Hence, the mathematical model \nis a simulation of the real or imagined  game world. Approximation and sim-\npliﬁ cation are two of the game developer’s most powerful tools. When used \nskillfully, even a greatly simpliﬁ ed model can sometimes be almost indistin-\nguishable from reality—and a lot more fun.\nAn agent-based simulation is one in which a number of distinct entities \nknown as “agents” interact. This ﬁ ts the description of most three-dimen-\ntsional computer games very well, where the agents are vehicles, characters, \nﬁ reballs, power dots, and so on. Given the agent-based nature of most games, \nit should come as no surprise that most games nowadays are implemented in \nan object-oriented, or at least loosely object-based, programming language.\nAll interactive video games are temporal  simulations, meaning that the vir-\ntual game world model is dynamic—the state of the game world changes over \ntime as the game’s events and story unfold. A video game must also respond \nto unpredictable inputs from its human player(s)—thus interactive temporal \nsimulations. Finally, most video games present their stories and respond to \nplayer input in real-time , making them interactive real-time  simulations. One \nnotable exception is in the category of turn-based games like computerized \nchess or non-real-time strategy games. But even these types of games usually \nprovide the user with some form of real-time graphical user interface . So for \nthe purposes of this book, we’ll assume that all video games have at least some \nreal-time constraints.\n\n\n10 \n1. Introduction\nAt the core of every real-time system is the concept of a  deadline. An obvi-\nous example in video games is the requirement that the screen be updated \nat least 24 times per second in order to provide the illusion of motion. (Most \ngames render the screen at 30 or 60  frames per second because these are mul-\ntiples of an NTSC monitor’s refresh rate.)  Of course, there are many other \nkinds of deadlines in video games as well. A physics simulation may need \nto be updated 120 times per second in order to remain stable. A character’s \nartiﬁ cial intelligence system may need to “think” at least once every second to \nprevent the appearance of stupidity. The audio library may need to be called \nat least once every 1/60 second in order to keep the audio buﬀ ers ﬁ lled and \nprevent audible glitches.\nA “soft ” real-time system is one in which missed deadlines are not cata-\nstrophic. Hence all video games are  soft  real-time systems—if the frame rate \ndies, the human player generally doesn’t!  Contrast this with a  hard real-time \nsystem, in which a missed deadline could mean severe injury to or even the \ndeath of a human operator. The avionics system in a helicopter or the control-\nrod system in a nuclear power plant are examples of hard real-time systems.\nMathematical models can be  analytic or numerical. For example, the ana-\nlytic (closed-form) mathematical model of a rigid body falling under the inﬂ u-\nence of constant acceleration due to gravity is typically writt en as follows:\n \ny(t) = ½ g t2 + v0 t + y0 . \n(1.1)\nAn analytic model can be evaluated for any value of its independent variables, \nsuch as the time t in the above equation, given only the initial conditions v0\nand y0 and the constant g. Such models are very convenient when they can be \nfound. However many problems in mathematics have no closed-form solu-\ntion. And in video games, where the user’s input is unpredictable, we cannot \nhope to model the entire game analytically.\nA  numerical model of the same rigid body under gravity might be\n \ny(t + Δt) = F(y(t), y˙ (t), ÿ(t), …) . \n(1.2)\nThat is, the height of the rigid body at some future time (t + Δt) can be found as \na function of the height and its ﬁ rst and second time derivatives at the current \ntime t. Numerical simulations are typically implemented by running calcula-\ntions repeatedly, in order to determine the state of the system at each discrete \ntime step. Games work in the same way. A main “game loop” runs repeatedly, \nand during each iteration of the loop, various game systems such as artiﬁ cial \nintelligence, game logic, physics simulations, and so on are given a chance to \ncalculate or update their state for the next discrete time step. The results are \nthen “rendered” by displaying graphics, emitt ing sound, and possibly pro-\nducing other outputs such as force feedback on the joypad.\n\n\n11 \n1.3. \nWhat Is a Game Engine?\nThe term “ game engine” arose in the mid-1990s in reference to ﬁ rst-person \nshooter (FPS) games like the insanely popular Doom by id Soft ware.  Doom was \narchitected with a reasonably well-deﬁ ned separation between its core soft -\nware components (such as the three-dimensional graphics rendering system, \nthe collision detection system, or the audio system) and the art assets, game \nworlds, and rules of play that comprised the player’s gaming experience. The \nvalue of this separation became evident as developers began licensing games \nand re-tooling them into new products by creating new art, world layouts, \nweapons, characters, vehicles, and game rules with only minimal changes to \nthe “engine” soft ware. This marked the birth of the “mod community ”—a \ngroup of individual gamers and small independent studios that built new \ngames by modifying existing games, using free toolkits provided by the origi-\nnal developers. Towards the end of the 1990s, some games like  Quake III Arena \nand  Unreal were designed with reuse and “ modding” in mind. Engines were \nmade highly customizable via scripting languages like id’s  Quake C, and en-\ngine licensing began to be a viable secondary revenue stream for the develop-\ners who created them. Today, game developers can license a game engine and \nreuse signiﬁ cant portions of its key soft ware components in order to build \ngames. While this practice still involves considerable investment in custom \nsoft ware engineering, it can be much more economical than developing all of \nthe core engine components in-house.\nThe  line between a game and its engine is oft en blurry. Some engines \nmake a reasonably clear distinction, while others make almost no att empt \nto separate the two. In one game, the rendering code might “know” speciﬁ -\ncally how to draw an orc. In another game, the rendering engine might pro-\nvide general-purpose material and shading facilities, and “orc-ness” might \nbe deﬁ ned entirely in data. No studio makes a perfectly clear separation \nbetween the game and the engine, which is understandable considering that \nthe deﬁ nitions of these two components oft en shift  as the game’s design so-\nlidiﬁ es.\nArguably a  data-driven architecture is what differentiates a game en-\ngine from a piece of software that is a game but not an engine. When a \ngame contains hard-coded logic or game rules, or employs special-case \ncode to render specific types of game objects, it becomes difficult or im-\npossible to reuse that software to make a different game. We should prob-\nably reserve the term “game engine” for software that is extensible and \ncan be used as the foundation for many different games without major \nmodification.\n1.3. What Is a Game Engine?\n\n\n12 \n1. Introduction\nClearly this is not a black-and-white distinction. We can think of a gamut \nof reusability onto which every engine falls. Figure 1.1 takes a stab at the loca-\ntions of some well-known games/engines along this  gamut.\nOne would think that a game engine could be something akin to Apple \nQuickTime or Microsoft  Windows Media Player—a general-purpose piece of \nsoft ware capable of playing virtually any game content imaginable. However \nthis ideal has not yet been achieved (and may never be). Most game engines \nare carefully craft ed and ﬁ ne-tuned to run a particular game on a particular \nhardware platform. And even the most general-purpose multiplatform en-\ngines are really only suitable for building games in one particular genre, such \nas ﬁ rst-person shooters or racing games. It’s safe to say that the more general-\npurpose a game engine or middleware component is, the less optimal it is for \nrunning a particular game on a particular platform.\nThis phenomenon occurs because designing any eﬃ  cient piece of soft -\nware invariably entails making trade-oﬀ s, and those trade-oﬀ s are based on \nassumptions about how the soft ware will be used and/or about the target \nhardware on which it will run. For example, a rendering engine that was de-\nsigned to handle intimate indoor environments probably won’t be very good \nat rendering vast outdoor environments. The indoor engine might use a BSP \ntree or portal system to ensure that no geometry is drawn that is being oc-\ncluded by walls or objects that are closer to the camera. The outdoor engine, \non the other hand, might use a less-exact occlusion mechanism, or none at all, \nbut it probably makes aggressive use of level-of-detail (LOD) techniques to \nensure that distant objects are rendered with a minimum number of triangles, \nwhile using high resolution triangle meshes for geometry that is close to the \ncamera.\nThe advent of ever-faster computer hardware and specialized graphics \ncards, along with ever-more-eﬃ  cient rendering algorithms and data struc-\ntures, is beginning to soft en the diﬀ erences between the graphics engines of \ndiﬀ erent genres. It is now possible to use a ﬁ rst-person shooter engine to build \na real-time strategy game, for example. However, the trade-oﬀ  between gener-\nCan be “modded” to \nbuild any game in a \nspecific genre\nCan be used to build any \ngame imaginable\nCannot be used to build \nmore than one game\nCan be customized to \nmake very similar games\nQuake III\nEngine\nUnreal\nEngine \n3\nHydro Thunder \nEngine\nProbably \nimpossible\nPacMan\nFigure 1.1.  Game engine reusability gamut.\n\n\n13 \n1.4. Engine Differnces Across Genres\nality and optimality still exists. A game can always be made more impressive \nby ﬁ ne-tuning the engine to the speciﬁ c requirements and constraints of a \nparticular game and/or hardware platform.\n1.4. \nEngine Differences Across Genres\nGame engines are typically somewhat  genre speciﬁ c. An engine designed \nfor a two-person ﬁ ghting game in a boxing ring will be very diﬀ erent from a \nmassively multiplayer online game (MMOG) engine or a ﬁ rst-person shooter \n(FPS) engine or a real-time strategy (RTS) engine. However, there is also a \ngreat deal of overlap—all 3D games, regardless of genre, require some form \nof low-level user input from the joypad, keyboard, and/or mouse, some form \nof 3D mesh rendering, some form of heads-up display (HUD) including text \nrendering in a variety of fonts, a powerful audio system, and the list goes on. \nSo while the Unreal Engine, for example, was designed for ﬁ rst-person shoot-\ner games, it has been used successfully to construct games in a number of \nother genres as well, including the wildly popular third-person shooter Gears \nof War by Epic Games; the character-based action-adventure game Grimm, by \nAmerican McGee’s Shanghai-based development studio, Spicy Horse; and \nSpeed Star, a futuristic racing game by South Korea-based Acro Games.\nLet’s take a look at some of the most common game genres and explore \nsome examples of the technology requirements particular to each.\n1.4.1. \nFirst-Person Shooters (FPS)\nThe  ﬁ rst-person shooter (FPS) genre is typiﬁ ed by games like Quake , Unreal \nTournament, Half-Life, Counter-Strike, and Call of Duty (see Figure 1.2). These \ngames have historically involved relatively slow on-foot roaming of a poten-\ntially large but primarily corridor-based world. However, modern ﬁ rst-person \nshooters can take place in a wide variety of virtual environments including \nvast open outdoor areas and conﬁ ned indoor areas. Modern FPS traversal me-\nchanics can include on-foot locomotion, rail-conﬁ ned or free-roaming ground \nvehicles, hovercraft , boats, and aircraft . For an overview of this genre, see \nhtt p://en.wikipedia.org/wiki/First-person_shooter.\nFirst-person games are typically some of the most technologically chal-\nlenging to build, probably rivaled in complexity only by third-person shooter/\naction/platformer games and massively multiplayer games. This is because \nﬁ rst-person shooters aim to provide their players with the illusion of being \nimmersed in a detailed, hyperrealistic world. It is not surprising that many of \nthe game industry’s big technological innovations arose out of the games in \nthis genre.\n\n\n14 \n1. Introduction\nFirst-person shooters typically focus on technologies, such as\nz eﬃ  cient rendering of large 3D virtual worlds;\nz a responsive camera control/aiming mechanic;\nz high-ﬁ delity animations of the player’s virtual arms and weapons;\nz a wide range of powerful hand-held weaponry;\nz a forgiving player character motion and collision model, which oft en \ngives these games a “ﬂ oaty” feel;\nz high-ﬁ delity animations and artiﬁ cial intelligence for the non-player \ncharacters (the player’s enemies and allies);\nz small-scale online multiplayer capabilities (typically supporting up to \n64 simultaneous players), and the ubiquitous “death match” gameplay \nmode.\nThe rendering technology employed by ﬁ rst-person shooters is almost \nalways highly optimized and carefully tuned to the particular type of envi-\nFigure 1.2.  Call of Duty 2 (Xbox 360/PLAYSTATION 3).\n\n\n15 \nronment being rendered. For example, indoor “dungeon crawl” games oft en \nemploy binary space partitioning (BSP) trees or portal -based rendering sys-\ntems. Outdoor FPS games use other kinds of rendering optimizations such as \nocclusion culling , or an oﬄ  ine sectorization of the game world with manual \nor automated speciﬁ cation of which target sectors are visible from each source \nsector.\nOf course, immersing a player in a hyperrealistic game world requires \nmuch more than just optimized high-quality graphics technology. The charac-\nter animations, audio and music, rigid-body physics, in-game cinematics, and \nmyriad other technologies must all be cutt ing-edge in a ﬁ rst-person shooter. \nSo this genre has some of the most stringent and broad technology require-\nments in the industry.\n1.4.2. Platformers and Other Third-Person Games\n“ Platformer” is the term applied to third-person character-based action games \nwhere jumping from platform to platform is the primary gameplay mechanic. \nTypical games from the 2D era include Space Panic, Donkey Kong, Pitfall!, and \n1.4. Engine Differnces Across Genres\nFigure 1.3.  Jak & Daxter: The Precursor Legacy.\n\n\n16 \n1. Introduction\nSuper Mario Brothers. The 3D era includes platformers like Super Mario 64, Crash \nBandicoot, Rayman 2, Sonic the Hedgehog, the Jak and Daxter series (Figure 1.3), \nthe Ratchet & Clank series, and more recently Super Mario Galaxy. See htt p://\nen.wikipedia.org/wiki/Platformer for an in-depth discussion of this genre.\nIn terms of their technological requirements, platformers can usually be \nlumped together with third-person shooters and third-person action/adven-\nture games, like Ghost Recon, Gears of War (Figure 1.4), and Uncharted: Drake’s \nFortune.\nThird-person character-based games have a lot in common with ﬁ rst-per-\nson shooters, but a great deal more emphasis is placed on the main character’s \nabilities and locomotion modes. In addition, high-ﬁ delity full-body character \nanimations are required for the player’s avatar, as opposed to the somewhat \nless-taxing animation requirements of the “ﬂ oating arms” in a typical FPS \ngame. It’s important to note here that almost all ﬁ rst-person shooters have an \nonline multiplayer component, so a full-body player avatar must be rendered \nin addition to the ﬁ rst-person arms. However the ﬁ delity of these FPS player \navatars is usually not comparable to the ﬁ delity of the non-player characters \nFigure 1.4.  Gears of War.\n\n\n17 \nin these same games; nor can it be compared to the ﬁ delity of the player avatar \nin a third-person game.\nIn a platformer, the main character is oft en cartoon-like and not particu-\nlarly realistic or high-resolution. However, third-person shooters oft en feature \na highly realistic humanoid player character. In both cases, the player charac-\nter typically has a very rich set of actions and animations.\nSome of the technologies speciﬁ cally focused on by games in this genre \ninclude\nz moving platforms, ladders, ropes, trellises, and other interesting loco-\nmotion modes;\nz puzzle-like environmental elements;\nz a third-person “follow camera ” which stays focused on the player char-\nacter and whose rotation is typically controlled by the human player via \nthe right joypad stick (on a console) or the mouse (on a PC—note that \nwhile there are a number of popular third-person shooters on PC, the \nplatformer genre exists almost exclusively on consoles);\nz a complex camera collision system for ensuring that the view point \nnever “clips” through background geometry or dynamic foreground \nobjects.\n1.4.3. \nFighting Games\n Fighting games are typically two-player games involving humanoid char-\nacters pummeling each other in a ring of some sort. The genre is typiﬁ ed \nby games like Soul Calibur and Tekken (see Figure 1.5). The Wikipedia page \nhtt p://en.wikipedia.org/wiki/Fighting_game provides an overview of this \ngenre.\nTraditionally games in the ﬁ ghting genre have focused their technology \neﬀ orts on\nz a rich set of ﬁ ghting animations;\nz accurate hit detection;\nz a user input system capable of detecting complex butt on and joystick \ncombinations;\nz crowds, but otherwise relatively static backgrounds.\nSince the 3D world in these games is small and the camera is centered \non the action at all times, historically these games have had litt le or no need \nfor world subdivision or occlusion culling . They would likewise not be ex-\npected to employ advanced three-dimensional audio propagation models, for \nexample.\n1.4. Engine Differnces Across Genres\n\n\n18 \n1. Introduction\nState-of-the-art ﬁ ghting games like EA’s Fight Night Round 3 (Figure 1.6) \nhave upped the technological ante with features like\nz high-deﬁ nition character graphics, including realistic skin shaders with \nsubsurface scatt ering and sweat eﬀ ects;\nz high-ﬁ delity character animations; \nz physics-based cloth and hair simulations for the characters.\nIt’s important to note that some ﬁ ghting games like Heavenly Sword take \nplace in a large-scale virtual world, not a conﬁ ned arena. In fact, many people \nconsider this to be a separate genre, sometimes called a  brawler. This kind of \nﬁ ghting game can have technical requirements more akin to those of a ﬁ rst-\nperson shooter or real-time strategy game.\nFigure 1.5.  Tekken 3 (PlayStation).\n\n\n19 \n1.4.4. Racing Games\nThe racing  genre encompasses all games whose primary task is driving a car \nor other vehicle on some kind of track. The genre has many subcategories. \nSimulation-focused racing games (“sims”) aim to provide a driving experi-\nence that is as realistic as possible (e.g., Gran Turismo). Arcade racers favor \nover-the-top fun over realism (e.g., San Francisco Rush, Cruisin’ USA, Hydro \nThunder). A relatively new subgenre explores the subculture of street racing \nwith tricked out consumer vehicles (e.g., Need for Speed, Juiced). Kart racing is \na subcategory in which popular characters from platformer games or cartoon \ncharacters from TV are re-cast as the drivers of whacky vehicles (e.g., Mario \nKart, Jak X, Freaky Flyers). “Racing” games need not always involve time-based \ncompetition. Some kart racing games, for example, oﬀ er modes in which play-\ners shoot at one another, collect loot, or engage in a variety of other timed \nand untimed tasks. For a discussion of this genre, see htt p://en.wikipedia.org/\nwiki/Racing_game.\n1.4. Engine Differnces Across Genres\nFigure 1.6.   Fight Night Round 3 (PLAYSTATION 3).\n",
      "page_number": 19,
      "chapter_number": 2,
      "summary": "This chapter covers segment 2 (pages 19-41). Key topics include game, gaming, and engine. Game programming is an immense topic, so we have a \nlot of ground to cover.",
      "keywords": [
        "Game",
        "Game Engine",
        "Engine",
        "game world",
        "Video Games",
        "Typical Game Team",
        "Game Engine Architecture",
        "game engine systems",
        "Typical Game",
        "book",
        "soft ware",
        "typical game engine",
        "Programming Game Engines",
        "racing games",
        "game designers"
      ],
      "concepts": [
        "game",
        "gaming",
        "engine",
        "engineering",
        "time",
        "timed",
        "based",
        "design",
        "ers",
        "worlds"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 7",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.75,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 11,
          "title": "Segment 11 (pages 111-118)",
          "relevance_score": 0.71,
          "method": "sentence_transformers"
        },
        {
          "book": "A Philosophy of Software Design",
          "chapter": 2,
          "title": "Segment 2 (pages 10-17)",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 47,
          "title": "Segment 47 (pages 448-460)",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 25,
          "title": "Segment 25 (pages 238-245)",
          "relevance_score": 0.67,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 42-63)",
      "start_page": 42,
      "end_page": 63,
      "detection_method": "synthetic",
      "content": "20 \n1. Introduction\nA racing game is oft en very linear, much like older FPS games. However, \ntravel speed is generally much faster than in a FPS. Therefore more focus is \nplaced on very long corridor-based tracks, or looped tracks, sometimes with \nvarious alternate routes and secret short-cuts. Racing games usually focus all \ntheir graphic detail on the vehicles, track, and immediate surroundings. How-\never, kart racers also devote signiﬁ cant rendering and animation bandwidth \nto the characters driving the vehicles. Figure 1.7 shows a screen shot from the \nlatest installment in the well-known Gran Turismo racing game series, Gran \nTurismo 5.\nSome of the technological properties of a typical racing game include the \nfollowing techniques. \nz Various “tricks” are used when rendering distant background elements, \nsuch as employing two-dimensional cards for trees, hills, and mountains.\nz The track is oft en broken down into relatively simple two-dimension-\nal regions called “sectors.” These data structures are used to optimize \nrendering and visibility determination, to aid in artiﬁ cial intelligence \nand path ﬁ nding for non-human-controlled vehicles, and to solve many \nother technical problems.\nFigure 1.7.  Gran Turismo 5 (PLAYSTATION 3).\n\n\n21 \nz The camera typically follows behind the vehicle for a third-person per-\nspective, or is sometimes situated inside the cockpit ﬁ rst-person style.\nz When the track involves tunnels and other “tight” spaces, a good deal \nof eﬀ ort is oft en put into ensuring that the camera does not collide with \nbackground geometry.\n1.4.5. Real-Time Strategy (RTS)\nThe modern  real-time strategy (RTS) genre was arguably deﬁ ned by Dune II: \nThe Building of a Dynasty (1992). Other games in this genre include Warcraft , \nCommand & Conquer, Age of Empires, and Starcraft . In this genre, the player \ndeploys the batt le units in his or her arsenal strategically across a large play-\ning ﬁ eld in an att empt to overwhelm his or her opponent. The game world is \ntypically displayed at an oblique top-down viewing angle. For a discussion of \nthis genre, see htt p://en.wikipedia.org/wiki/Real-time_strategy.\nThe RTS player is usually prevented from signiﬁ cantly changing the \nviewing angle in order to see across large distances. This restriction permits \nFigure 1.8.  Age of Empires.\n1.4. Engine Differnces Across Genres\n\n\n22 \n1. Introduction\ndevelopers to employ various optimizations in the rendering engine of an RTS \ngame.\nOlder games in the genre employed a grid-based (cell-based) world con-\nstruction, and an orthographic projection was used to greatly simplify the ren-\nderer. For example, Figure 1.8 shows a screen shot from the classic RTS Age \nof Empires.\nModern RTS games sometimes use perspective projection and a true 3D \nworld, but they may still employ a grid layout system to ensure that units and \nbackground elements, such as buildings, align with one another properly. A \npopular example, Command & Conquer 3, is shown in Figure 1.9.\nSome other common practices in RTS games include the following tech-\nniques.\nFigure 1.9.  Command & Conquer 3.\n\n\n23 \nz Each unit is relatively low-res, so that the game can support large num-\nbers of them on-screen at once.\nz Height-ﬁ eld terrain is usually the canvas upon which the game is de-\nsigned and played.\nz The player is oft en allowed to build new structures on the terrain in ad-\ndition to deploying his or her forces.\nz User interaction is typically via single-click and area-based selection of \nunits, plus menus or toolbars containing commands, equipment, unit \ntypes, building types, etc.\n1.4.6. Massively Multiplayer Online Games (MMOG)\nThe  massively multiplayer online game (MMOG) genre is typiﬁ ed by games \nlike Neverwinter Nights, EverQuest, World of Warcraft , and Star Wars Galaxies, to \nname a few. An MMOG is deﬁ ned as any game that supports huge numbers of \nsimultaneous players (from thousands to hundreds of thousands), usually all \n1.4. Engine Differnces Across Genres\nFigure 1.10.  World of Warcraft.\n\n\n24 \n1. Introduction\nplaying in one very large, persistent virtual world (i.e., a world whose internal \nstate persists for very long periods of time, far beyond that of any one player’s \ngameplay session). Otherwise, the gameplay experience of an MMOG is oft en \nsimilar to that of their small-scale multiplayer counterparts. Subcategories of \nthis genre include MMO role-playing games (MMORPG), MMO real-time \nstrategy games (MMORTS), and MMO ﬁ rst-person shooters (MMOFPS). For a \ndiscussion of this genre, see htt p://en.wikipedia.org/wiki/MMOG. Figure 1.10 \nshows a screen shot from the hugely popular MMORPG World of Warcraft .\nAt the heart of all MMOGs is a very powerful batt ery of servers. These \nservers maintain the authoritative state of the game world, manage users sign-\ning in and out of the game, provide inter-user chat or voice-over-IP (VoIP) \nservices, etc. Almost all MMOGs require users to pay some kind of regular \nsubscription fee in order to play, and they may oﬀ er micro-transactions within \nthe game world or out-of-game as well. Hence, perhaps the most important \nrole of the central server is to handle the billing and micro-transactions which \nserve as the game developer’s primary source of revenue.\nGraphics ﬁ delity in an MMOG is almost always lower than its non-mas-\nsively multiplayer counterparts, as a result of the huge world sizes and ex-\ntremely large numbers of users supported by these kinds of games.\n1.4.7. \nOther Genres\nThere are of course many  other game genres which we won’t cover in depth \nhere. Some examples include\nz sports, with subgenres for each major sport (football, baseball, soccer, \ngolf, etc.);\nz role-playing games (RPG);\nz God games, like Populus and Black & White;\nz environmental/social simulation games, like SimCity or The Sims;\nz puzzle games like Tetris;\nz conversions of non-electronic games, like chess, card games, go, etc.;\nz web-based games, such as those oﬀ ered at Electronic Arts’ Pogo site;\nz and the list goes on.\nWe have seen that each game genre has its own particular technologi-\ncal requirements. This explains why game engines have traditionally diﬀ ered \nquite a bit from genre to genre. However, there is also a great deal of tech-\nnological overlap between genres, especially within the context of a single \nhardware platform. With the advent of more and more powerful hardware, \n\n\n25 \ndiﬀ erences between genres that arose because of optimization concerns are \nbeginning to evaporate. So it is becoming increasingly possible to reuse the \nsame engine technology across disparate genres, and even across disparate \nhardware platforms.\n1.5. \nGame Engine Survey\n1.5.1. \nThe Quake Family of Engines\nThe ﬁ rst 3D ﬁ rst-person shooter (FPS) game is generally accepted to be  Castle \nWolfenstein 3D (1992). Writt en by id Soft ware of Texas for the PC platform, this \ngame led the game industry in a new and exciting direction. Id Soft ware went \non to create Doom,  Quake , Quake II, and Quake III. All of these engines are very \nsimilar in architecture, and I will refer to them as the Quake family of engines. \nQuake technology has been used to create many other games and even other \nengines. For example, the lineage of Medal of Honor for the PC platform goes \nsomething like this:\nz Quake I II (Id);\nz Sin (Ritual);\nz F.A.K.K. 2 (Ritual);\nz Medal of Honor: Allied Assault (2015 & Dreamworks Interactive);\nz Medal of Honor: Paciﬁ c Assault (Electronic Arts, Los Angeles).\nMany other games based on Quake technology follow equally circuitous paths \nthrough many diﬀ erent games and studios. In fact, Valve’s Source engine (used \nto create the Half-Life games) also has distant roots in Quake technology.\nThe Quake and Quake II source code is freely available, and the original \nQuake engines are reasonably well architected and “clean” (although they \nare of course a bit outdated and writt en entirely in C). These code bases serve \nas great examples of how industrial-strength game engines are built. The full \nsource code to Quake and Quake II is available on id’s website at htt p://www.\nidsoft ware.com/business/techdownloads.\nIf you own the Quake and/or Quake II games, you can actually build the \ncode using Microsoft  Visual Studio and run the game under the debugger \nusing the real game assets from the disk. This can be incredibly instructive. \nYou can set break points, run the game, and then analyze how the engine \nactually works by stepping through the code. I highly recommend down-\nloading one or both of these engines and analyzing the source code in this \nmanner. \n1.5. Game Engine Survey\n\n\n26 \n1. Introduction\n1.5.2. \nThe Unreal Family of Engines\nEpic Games Inc. burst onto the FPS scene in 1998 with its legendary game  Un-\nreal . Since then, the Unreal Engine has become a major competitor to Quake \ntechnology in the FPS space. Unreal Engine 2 (UE2) is the basis for Unreal \nTournament 2004 (UT2004) and has been used for countless “mods,” university \nprojects, and commercial games. Unreal Engine 3 (UE3) is the next evolution-\nary step, boasting some of the best tools and richest engine feature sets in \nthe industry, including a convenient and powerful graphical user interface for \ncreating shaders and a graphical user interface for game logic programming \ncalled Kismet. Many games are being developed with UE3 lately, including of \ncourse Epic’s popular Gears of War.\nThe Unreal Engine has become known for its extensive feature set and \ncohesive, easy-to-use tools. The Unreal Engine is not perfect, and most devel-\nopers modify it in various ways to run their game optimally on a particular \nhardware platform. However, Unreal is an incredibly powerful prototyping \ntool and commercial game development platform, and it can be used to build \nvirtually any 3D ﬁ rst-person or third-person game (not to mention games in \nother genres as well).\nThe  Unreal Developer Network (UDN) provides a rich set of documenta-\ntion and other information about the various versions of the Unreal Engine \n(see htt p://udn.epicgames.com). Some of the documentation on Unreal Engine \n2 is freely available, and “mods” can be constructed by anyone who owns a \ncopy of UT2004. However, access to the balance of the UE2 docs and all of the \nUE3 docs are restricted to licensees of the engine. Unfortunately, licenses are \nextremely expensive, and hence out of reach for all independent game devel-\nopers and most small studios as well. But there are plenty of other useful web-\nsites and wikis on Unreal. One popular one is htt p://www.beyondunreal.com.\n1.5.3. \nThe Half Life Source Engine\n Source is the game engine that drives the smash hit  Half-Life 2 and its sequels \nHL2: Episode One, HL2: Episode Two, Team Fortress 2, and Portal (shipped to-\ngether under the title The Orange Box). Source is a high-quality engine, rivaling \nUnreal Engine 3 in terms of graphics capabilities and tool set.\n1.5.4. Microsoft’s XNA Game Studio\nMicrosoft ’s  XNA Game Studio is an easy-to-use and highly accessible game \ndevelopment platform aimed at encouraging players to create their own \ngames and share them with the online gaming community, much as YouTube \nencourages the creation and sharing of home-made videos.\n\n\n27 \nXNA is based on Microsoft ’s C# language and the Common Language \nRuntime (CLR). The primary development environment is Visual Studio or \nits free counterpart, Visual Studio Express. Everything from source code \nto game art assets are managed within Visual Studio. With XNA, develop-\ners can create games for the PC platform and Microsoft ’s Xbox 360 console. \nAft er paying a modest fee, XNA games can be uploaded to the  Xbox Live \nnetwork and shared with friends. By providing excellent tools at essentially \nzero cost, Microsoft  has brilliantly opened the ﬂ oodgates for the average \nperson to create new games. XNA clearly has a bright and fascinating future \nahead of it.\n1.5.5. \nOther Commercial Engines\nThere are lots of other commercial game engines out there. Although indie \ndevelopers may not have the budget to purchase an engine, many of these \nproducts have great online documentation and/or wikis that can serve as a \ngreat source of information about game engines and game programming in \ngeneral. For example, check out the  C4 Engine by Terathon Soft ware (htt p://\nwww.terathon.com), a company founded by Eric Lengyel in 2001. Docu-\nmentation for the C4 Engine can be found on Terathon’s website, with ad-\nditional details on the C4 Engine wiki (htt p://www.terathon.com/wiki/index.\nphp?title=Main_Page).\n1.5.6. Proprietary in-House Engines\nMany companies build and maintain proprietary in-house game engines. \nElectronic Arts built many of its RTS games on a proprietary engine called \nSAGE, developed at Westwood Studios. Naughty Dog’s Crash Bandicoot, Jak \nand Daxter series, and most recently Uncharted: Drake’s Fortune franchises were \neach built on in-house engines custom-tailored to the PlayStation, PlayStation \n2, and PLAYSTATION 3 platforms, respectively. And of course, most commer-\ncially licensed game engines like Quake , Source, or the Unreal Engine started \nout as proprietary in-house engines.\n1.5.7. \nOpen Source Engines\nOpen source 3D game engines are engines built by amateur and professional \ngame developers and provided online for free. The term “open source” typi-\ncally implies that source code is freely available and that a somewhat open de-\nvelopment model is employed, meaning almost anyone can contribute code. Li-\ncensing, if it exists at all, is oft en provided under the Gnu Public License (GPL) \nor Lesser Gnu Public License (LGPL). The former permits code to be freely used \n1.5. Game Engine Survey\n\n\n28 \n1. Introduction\nby anyone, as long as their code is also freely available; the latt er allows the \ncode to be used even in proprietary for-proﬁ t applications. Lots of other free \nand semi-free licensing schemes are also available for open source projects.\nThere are a staggering number of open source engines available on the \nweb. Some are quite good, some are mediocre, and some are just plain aw-\nful!  The list of game engines provided online at htt p://cg.cs.tu-berlin.de/~ki/\nengines.html will give you a feel for the sheer number of engines that are out \nthere.\nOGRE 3D is a well-architected, easy-to-learn, and easy-to-use 3D render-\ning engine. It boasts a fully featured 3D renderer including advanced lighting \nand shadows , a good skeletal character animation system, a two-dimensional \noverlay system for heads-up display s and graphical user interface s, and a \npost-processing system for full-screen eﬀ ects like bloom . OGRE is, by its au-\nthors’ own admission, not a full game engine, but it does provide many of the \nfoundational components required by prett y much any game engine. \nSome other well-known open source engines are listed here.\nz  Panda3D is a script-based engine. The engine’s primary interface is the \nPython custom scripting language. It is designed to make prototyping \n3D games and virtual worlds convenient and fast.\nz  Yake is a relatively new fully featured game engine built on top of \nOGRE .\nz  Crystal Space is a game engine with an extensible modular architecture.\nz  Torque and  Irrlicht are also well-known and widely used engines.\n1.6. \nRuntime Engine Architecture\nA game engine generally consists of a tool suite and a  runtime component. \nWe’ll explore the architecture of the runtime piece ﬁ rst and then get into tools \narchitecture in the following section.\nFigure 1.11 shows all of the major runtime components that make up a \ntypical 3D game engine. Yeah, it’s big!  And this diagram doesn’t even account \nfor all the tools. Game engines are deﬁ nitely large soft ware systems.\nLike all soft ware systems, game engines are built in layers. Normally up-\nper layers depend on lower layers, but not vice versa. When a lower layer \ndepends upon a higher layer, we call this a circular dependency.  Dependency \ncycles are to be avoided in any soft ware system, because they lead to un-\ndesirable  coupling between systems, make the soft ware untestable, and in-\nhibit  code reuse. This is especially true for a large-scale system like a game \nengine.\n\n\n29 \n1.6. Runtime Engine Architecture\nGameplay Foundations\nEvent/Messaging \nSystem\nDynamic Game \nObject Model\nScripting System\nWorld Loading / \nStreaming\nStatic World \nElements\nReal-Time Agent-\nBased Simulation\nHigh-Level Game Flow System/FSM\nSkeletal Animation\nAnimation \nDecompression\nInverse \nKinematics (IK)\nGame-Specific \nPost-Processing\nSub-skeletal \nAnimation\nLERP and \nAdditive Blending\nAnimation \nPlayback\nAnimation State \nTree & Layers\nProfiling & Debugging\nMemory & \nPerformance Stats\nIn-Game Menus \nor Console\nRecording & \nPlayback\nHierarchical \nObject Attachment\n3rd Party SDKs\nHavok, PhysX, \nODE etc.\nDirectX, OpenGL, \nlibgcm, Edge, etc.\nBoost++\nSTL / STLPort\netc.\nKynapse\nEuphoria\nGranny, Havok \nAnimation, etc.\nOS\nDrivers\nHardware (PC, XBOX360, PS3, etc.)\nPlatform Independence Layer\nAtomic Data \nTypes\nPlatform Detection\nCollections and \nIterators\nThreading Library\nHi-Res Timer\nFile System\nNetwork Transport \nLayer (UDP/TCP)\nGraphics \nWrappers\nPhysics/Coll. \nWrapper\nCore Systems\nModule Start-Up \nand Shut-Down\nParsers (CSV, \nXML, etc.)\nAssertions\nUnit Testing\nMath Library\nStrings and \nHashed String Ids\nDebug Printing \nand Logging\nMemory Allocation\nEngine Config\n(INI files etc.)\nProfiling / Stats \nGathering\nObject Handles / \nUnique Ids\nRTTI / Reflection \n& Serialization\nCurves & \nSurfaces Library\nRandom Number \nGenerator\nLocalization \nServices\nAsynchronous\nFile I/O\nMovie Player\nMemory Card I/O \n(Older Consoles)\nResources (Game Assets)\nResource Manager\nTexture \nResource\nMaterial \nResource\n3D Model \nResource\nFont \nResource\nCollision \nResource\nPhysics \nParameters\nGame \nWorld/Map\netc.\nSkeleton \nResource\nHuman Interface \nDevices (HID)\nPhysical Device\nI/O\nGame-Specific \nInterface\nAudio\nAudio Playback / \nManagement\nDSP/Effects\n3D Audio Model\nOnline Multiplayer\nMatch-Making & \nGame Mgmt.\nGame State \nReplication\nObject Authority \nPolicy\nScene Graph / Culling Optimizations\nLOD System\nOcclusion & PVS\nSpatial Subdivision \n(BSP Tree, kd-Tree, …)\nVisual Effects\nParticle & Decal \nSystems\nPost Effects\nHDR Lighting\nPRT Lighting, \nSubsurf. Scatter\nEnvironment \nMapping\nLight Mapping & \nDynamic Shadows\nFront End\nHeads-Up Display \n(HUD)\nFull-Motion Video \n(FMV)\nIn-Game Menus\nIn-Game GUI\nWrappers / Attract \nMode\nIn-Game Cinematics \n(IGC)\nCollision & Physics\nShapes/\nCollidables\nRigid Bodies\nPhantoms\nRay/Shape \nCasting (Queries)\nForces & \nConstraints\nPhysics/Collision \nWorld\nRagdoll \nPhysics\nGAME-SPECIFIC SUBSYSTEMS\nGame-Specific Rendering\nTerrain Rendering\nWater Simulation \n& Rendering\netc.\nPlayer Mechanics\nCollision Manifold\nMovement\nState Machine & \nAnimation\nGame Cameras\nPlayer-Follow \nCamera\nDebug Fly-\nThrough Cam\nFixed Cameras\nScripted/Animated \nCameras\nAI\nSight Traces & \nPerception\nPath Finding (A*)\nGoals & Decision-\nMaking\nActions\n(Engine Interface)\nCamera-Relative \nControls (HID)\nWeapons\nPower-Ups\netc.\nVehicles\nPuzzles\nLow-Level Renderer\nPrimitive \nSubmission\nViewports & \nVirtual Screens\nMaterials & \nShaders\nTexture and \nSurface Mgmt.\nGraphics Device Interface\nStatic & Dynamic \nLighting\nCameras\nText & Fonts\nDebug Drawing\n(Lines etc.)\nSkeletal Mesh \nRendering\nFigure 1.11.  Runtime game engine architecture.\n\n\n30 \n1. Introduction\nWhat follows is a brief overview of the components shown in the diagram \nin Figure 1.11. The rest of this book will be spent investigating each of these \ncomponents in a great deal more depth and learning how these components \nare usually integrated into a functional whole.\n1.6.1. \nTarget Hardware\nThe  target hardware layer, shown in isolation in Figure 1.12, represents the \ncomputer system or console on which the game will run. Typical platforms \ninclude Microsoft  Windows- and Linux-based PCs, the Apple iPhone and \nMacintosh, Microsoft ’s Xbox and Xbox 360, Sony’s PlayStation, PlayStation 2, \nPlayStation Portable (PSP), and PLAYSTATION 3, and Nintendo’s DS, Game-\nCube, and Wii. Most of the topics in this book are platform-agnostic, but we’ll \nalso touch on some of the design considerations peculiar to PC or console \ndevelopment, where the distinctions are relevant.\nHardware (PC, XBOX360, PS3, etc.)\nFigure 1.12.  Hardware layer.\nDrivers\nFigure 1.13.  Device driver layer.\n1.6.2. Device Drivers\nAs depicted in Figure 1.13, device drivers are low-level soft ware components \nprovided by the operating system or hardware vendor. Drivers manage hard-\nware resources and shield the operating system and upper engine layers from \nthe details of communicating with the myriad variants of hardware devices \navailable.\n1.6.3. Operating System\nOn a PC, the  operating system (OS) is running all the time. It orchestrates the \nexecution of multiple programs on a single computer, one of which is your \ngame. The OS layer is shown in Figure 1.14. Operating systems like Microsoft  \nWindows employ a time-sliced approach to sharing the hardware with mul-\ntiple running programs, known as pre-emptive multitasking . This means that \na PC game can never assume it has full control of the hardware—it must “play \nnice” with other programs in the system.\n\n\n31 \n1.6. Runtime Engine Architecture\nOS\nFigure 1.14.  Operating system layer.\n3rd Party SDKs\nHavok, PhysX, \nODE etc.\nDirectX, OpenGL, \nlibgcm, Edge, etc.\nBoost++\nSTL / STLPort\netc.\nKynapse\nEuphoria\nGranny, Havok \nAnimation, etc.\nFigure 1.15.  Third-party SDK layer.\nOn a console, the operating system is oft en just a thin library layer that is \ncompiled directly into your game executable. On a console, the game typically \n“owns” the entire machine. However, with the introduction of the Xbox 360 \nand PLAYSTATION 3, this is no longer strictly the case. The operating sys-\ntem on these consoles can interrupt the execution of your game, or take over \ncertain system resources, in order to display online messages, or to allow the \nplayer to pause the game and bring up the PS3’s Xross Media Bar or the Xbox \n360’s dashboard, for example. So the gap between console and PC develop-\nment is gradually closing (for bett er or for worse).\n1.6.4. Third-Party SDKs and Middleware\nMost game engines leverage a number of third-party  soft ware development \nkit s (SDKs) and middleware, as shown in Figure 1.15. The functional or class-\nbased interface provided by an SDK is oft en called an  application program-\nming interface (API). We will look at a few examples.\n1.6.4.1. \nData Structures and Algorithms\nLike any soft ware system, games depend heavily on collection data structures \nand algorithms to manipulate them. Here are a few examples of third-party \nlibraries which provide these kinds of services.\nz STL. The C++  standard template library provides a wealth of code and \nalgorithms for managing data structures, strings, and stream-based \nI/O.\nz  STLport . This is a portable, optimized implementation of STL.\nz  Boost . Boost is a powerful data structures and algorithms library, \ndesigned in the style of STL. (The online documentation for Boost is \nalso a great place to learn a great deal about computer science!)\nz  Loki . Loki is a powerful generic programming template library which is \nexceedingly good at making your brain hurt!\n\n\n32 \n1. Introduction\nGame developers are divided on the question of whether to use template \nlibraries like STL in their game engines. Some believe that the memory alloca-\ntion patt erns of STL, which are not conducive to high-performance program-\nming and tend to lead to memory fragmentation (see Section 5.2.1.4), make \nSTL unusable in a game. Others feel that the power and convenience of STL \noutweigh its problems, and that most of the problems can in fact be worked \naround anyway. My personal belief is that STL is all right for use on a PC, be-\ncause its advanced virtual memory system renders the need for careful mem-\nory allocation a bit less crucial (although one must still be very careful). On a \nconsole, with limited or no virtual memory facilities and exorbitant cache miss \ncosts, you’re probably bett er oﬀ  writing custom data structures that have pre-\ndictable and/or limited memory allocation patt erns. (And you certainly won’t \ngo far wrong doing the same on a PC game project either.)\n1.6.4.2. Graphics\nMost game rendering engines are built on top of a hardware interface library, \nsuch as the following:\nz  Glide is the 3D graphics SDK for the old Voodoo graphics cards. This \nSDK was popular prior to the era of hardware transform and lighting \n(hardware T&L) which began with DirectX 8.\nz  OpenGL is a widely used portable 3D graphics SDK.\nz  DirectX is Microsoft ’s 3D graphics SDK and primary rival to OpenGL .\nz  libgcm is a low-level direct interface to the PLAYSTATION 3’s RSX graph-\nics hardware, which was provided by Sony as a more eﬃ  cient alterna-\ntive to OpenGL.\nz  Edge is a powerful and highly-eﬃ  cient rendering and animation engine \nproduced by Naughty Dog and Sony for the  PLAYSTATION 3 and used \nby a number of ﬁ rst- and third-party game studios.\n1.6.4.3. Collision and Physics\nCollision detection and  rigid body dynamics (known simply as “physics” \nin the game development community) are provided by the following well-\nknown SDKs.\nz  Havok is a popular industrial-strength physics and collision engine.\nz  PhysX is another popular industrial-strength physics and collision en-\ngine, available for free download from NVIDIA.\nz  Open Dynamics Engine (ODE) is a well-known open source physics/col-\nlision p ackage.\n\n\n33 \n1.6.4.4. Character Animation\nA number of commercial animation packages exist, including but certainly \nnot limited to the following.\nz  Granny . Rad Game Tools’ popular Granny toolkit includes robust 3D \nmodel and animation exporters for all the major 3D modeling and ani-\nmation packages like Maya, 3D Studio MAX, etc., a runtime library for \nreading and manipulating the exported model and animation data, and \na powerful runtime animation system. In my opinion, the Granny SDK \nhas the best-designed and most logical animation API of any I’ve seen, \ncommercial or proprietary, especially its excellent handling of time.\nz  Havok Animation . The line between physics and animation is becoming \nincreasingly blurred as characters become more and more realistic. The \ncompany that makes the popular Havok physics SDK decided to create \na complimentary animation SDK, which makes bridging the physics-\nanimation gap much easier than it ever has been.\nz  Edge. The Edge  library produced for the PS3 by the ICE team at Naughty \nDog, the Tools and Technology group of Sony Computer Entertainment \nAmerica, and Sony’s Advanced Technology Group in Europe includes \na powerful and eﬃ  cient animation engine and an eﬃ  cient geometry-\nprocessing engine for rendering.\n1.6.4.5. Artiﬁ cial Intelligence\nz  Kynapse . Until recently, artiﬁ cial intelligence (AI) was handled in a cus-\ntom manner for each game. However, a company called Kynogon has \nproduced a middleware SDK called Kynapse. This SDK provides low-\nlevel AI building blocks such as path ﬁ nding, static and dynamic object \navoidance, identiﬁ cation of vulnerabilities within a space (e.g., an open \nwindow from which an ambush could come), and a reasonably good \ninterface between AI and animation.\n1.6.4.6. Biomechanical Character Models\nz  Endorphin and  Euphoria .  These are animation packages that produce \ncharacter motion using advanced  biomechanical models of realistic hu-\nman movement.\nAs we mentioned above, the line between character animation and phys-\nics is beginning to blur. Packages like Havok Animation try to marry physics \nand animation in a traditional manner, with a human animator providing the \nmajority of the motion through a tool like Maya and with physics augmenting \nthat motion at runtime. But recently a ﬁ rm called Natural Motion Ltd. has pro-\n1.6. Runtime Engine Architecture\n\n\n34 \n1. Introduction\nduced a product that att empts to redeﬁ ne how character motion is handled in \ngames and other forms of digital media.\nIts ﬁ rst product, Endorphin , is a Maya plug-in that permits animators \nto run full biomechanical simulations on characters and export the resulting \nanimations as if they had been hand-animated. The biomechanical model ac-\ncounts for center of gravity, the character’s weight distribution, and detailed \nknowledge of how a real human balances and moves under the inﬂ uence of \ngravity and other forces.\nIts second product, Euphoria , is a real-time version of Endorphin intend-\ned to produce physically and biomechanically accurate character motion at \nruntime under the inﬂ uence of unpredictable forces.\n1.6.5. Platform Independence Layer\nMost game engines are required to be capable of running on more than one \nhardware platform. Companies like Electronic Arts and Activision/Blizzard, \nfor example, always target their games at a wide variety of platforms, because \nit exposes their games to the largest possible market. Typically, the only game \nstudios that do not target at least two diﬀ erent platforms per game are ﬁ rst-\nparty studios, like Sony’s Naughty Dog and Insomniac studios. Therefore, \nmost game engines are architected with a  platform independence layer, like \nthe one shown in Figure 1.16. This layer sits atop the hardware, drivers, oper-\nating system, and other third-party soft ware and shields the rest of the engine \nfrom the majority of knowledge of the underlying platform.\nBy wrapping or replacing the most commonly used standard C library \nfunctions, operating system calls, and other foundational application pro-\ngramming interfaces (APIs), the platform independence layer ensures consis-\ntent behavior across all hardware platforms. This is necessary because there is \na good deal of variation across platforms, even among “standardized” librar-\nies like the standard C library.\nPlatform Independence Layer\nAtomic Data \nTypes\nPlatform Detection\nCollections and \nIterators\nThreading Library\nHi-Res Timer\nFile System\nNetwork Transport \nLayer (UDP/TCP)\nGraphics \nWrappers\nPhysics/Coll. \nWrapper\nFigure 1.16.  Platform independence layer.\n1.6.6. Core Systems\nEvery game engine, and really every large, complex C++ soft ware application, \nrequires a grab bag of useful soft ware utilities. We’ll categorize these under \nthe label “core systems.” A typical core systems layer is shown in Figure 1.17. \nHere are a few examples of the facilities the core layer usually provides.\n\n\n35 \nz  Assertions are lines of error-checking code that are inserted to catch logi-\ncal mistakes and violations of the programmer’s original assumptions. \nAssertion checks are usually stripped out of the ﬁ nal production build \nof the game.\nz  Memory management. Virtually every game engine implements its own \ncustom memory allocation system(s) to ensure high-speed allocations \nand deallocations and to limit the negative eﬀ ects of memory fragmen-\ntation (see Section 5.2.1.4).\nz  Math library. Games are by their nature highly mathematics-intensive. As \nsuch, every game engine has at least one, if not many, math libraries. These \nlibraries provide facilities for vector and matrix math, quaternion rota-\ntions, trigonometry, geometric operations with lines, rays, spheres, frusta, \netc., spline manipulation, numerical integration, solving systems of equa-\ntions, and whatever other facilities the game programmers require.\nz Custom data structures and algorithms. Unless an engine’s designers de-\ncided to rely entirely on a third-party package such as STL, a suite of \ntools for managing fundamental data structures (linked lists, dynamic \narrays, binary trees, hash maps, etc.) and algorithms (search, sort, etc.) \nis usually required. These are oft en hand-coded to minimize or elimi-\nnate dynamic memory allocation and to ensure optimal runtime perfor-\nmance on the target platform(s).\nA detailed discussion of the most common core engine systems can be \nfound in Part II.\n1.6.7. \nResource Manager\nPresent in every game engine in some form, the  resource manager provides \na uniﬁ ed interface (or suite of interfaces) for accessing any and all types of \ngame assets and other engine input data. Some engines do this in a highly \ncentralized and consistent manner (e.g., Unreal ’s packages, OGRE 3D ’s Re-\nsourceManager class). Other engines take an ad hoc approach, oft en leaving \nit up to the game programmer to directly access raw ﬁ les on disk or within \ncompressed archives such as Quake ’s PAK ﬁ les. A typical resource manager \nlayer is depicted in Figure 1.18.\n1.6. Runtime Engine Architecture\nCore Systems\nModule Start-Up \nand Shut-Down\nParsers (CSV, \nXML, etc.)\nAssertions\nUnit Testing\nMath Library\nStrings and \nHashed String Ids\nDebug Printing \nand Logging\nMemory Allocation\nEngine Config\n(INI files etc.)\nProfiling / Stats \nGathering\nObject Handles / \nUnique Ids\nRTTI / Reflection \n& Serialization\nCurves & \nSurfaces Library\nRandom Number \nGenerator\nLocalization \nServices\nAsynchronous\nFile I/O\nMovie Player\nMemory Card I/O \n(Older Consoles)\nFigure 1.17.  Core engine systems.\n\n\n36 \n1. Introduction\nLow-Level Renderer\nPrimitive \nSubmission\nViewports & \nVirtual Screens\nMaterials & \nShaders\nTexture and \nSurface Mgmt.\nGraphics Device Interface\nStatic & Dynamic \nLighting\nCameras\nText & Fonts\nDebug Drawing\n(Lines etc.)\nSkeletal Mesh \nRendering\nFigure 1.19.  Low-level rendering engine.\nResources (Game Assets)\nResource Manager\nTexture \nResource\nMaterial \nResource\n3D Model \nResource\nFont \nResource\nCollision \nResource\nPhysics \nParameters\nGame \nWorld/Map\netc.\nSkeleton \nResource\nFigure 1.18.  Resource manager.\n1.6.8. Rendering Engine\nThe  rendering engine is one of the largest and most complex components of \nany game engine. Renderers can be architected in many diﬀ erent ways. There \nis no one accepted way to do it, although as we’ll see, most modern rendering \nengines share some fundamental design philosophies, driven in large part by \nthe design of the 3D graphics hardware upon which they depend.\nOne common and eﬀ ective approach to rendering engine design is to em-\nploy a layered architecture as follows.\n1.6.8.1. \nLow-Level Renderer\nThe  low-level renderer , shown in Figure 1.19, encompasses all of the raw ren-\ndering facilities of the engine. At this level, the design is focused on rendering \na collection of geometric  primitives as quickly and richly as possible, without \nmuch regard for which portions of a scene may be visible. This component is \nbroken into various subcomponents, which are discussed below.\nGraphics Device Interface\nGraphics SDKs, such as DirectX and OpenGL, require a reasonable amount of \ncode to be writt en just to enumerate the available graphics devices, initialize \nthem, set up render surfaces (back-buﬀ er, stencil buﬀ er etc.), and so on. This \n\n\n37 \nis typically handled by a component that I’ll call the  graphics device interface \n(although every engine uses its own terminology).\nFor a PC game engine, you also need code to integrate your renderer with \nthe Windows message loop. You typically write a “ message pump ” that ser-\nvices Windows messages when they are pending and otherwise runs your \nrender loop over and over as fast as it can. This ties the game’s keyboard poll-\ning loop to the renderer’s screen update loop. This coupling is undesirable, \nbut with some eﬀ ort it is possible to minimize the dependencies. We’ll explore \nthis topic in more depth later.\nOther Renderer Components\nThe other components in the low-level renderer cooperate in order to collect \nsubmissions of geometric primitives (sometimes called  render packet s), such as \nmeshes, line lists, point lists, particles , terrain patches, text strings, and what-\never else you want to draw, and render them as quickly as possible.\nThe low-level renderer usually provides a viewport abstraction with an \nassociated camera -to-world matrix and 3D projection parameters, such as ﬁ eld \nof view and the location of the near and far clip plane s. The low-level renderer \nalso manages the state of the graphics hardware and the game’s shaders via \nits material system and its dynamic lighting system. Each submitt ed primitive \nis associated with a material and is aﬀ ected by n dynamic lights. The mate-\nrial describes the texture (s) used by the primitive, what device state sett ings \nneed to be in force, and which vertex and pixel shader to use when rendering \nthe primitive. The lights determine how dynamic lighting calculations will \nbe applied to the primitive. Lighting and shading is a complex topic, which \nis covered in depth in many excellent books on computer graphics, including \n[14], [42], and [1].\n1.6.8.2. Scene Graph/Culling Optimizations\nThe low-level renderer draws all of the geometry submitt ed to it, without \nmuch regard for whether or not that geometry is actually visible (other than \nback-face culling and clipping triangles to the camera frustum). A higher-level \ncomponent is usually needed in order to limit the number of primitives sub-\nmitt ed for rendering, based on some form of visibility determination. This \nlayer is shown in Figure 1.20.\nFor very small game worlds, a simple frustum  cull (i.e., removing objects \nthat the camera cannot “see”) is probably all that is required. For larger game \nworlds, a more advanced spatial subdivision data structure might be used to \nimprove rendering eﬃ  ciency, by allowing the  potentially visible set (PVS) \nof objects to be determined very quickly. Spatial subdivisions can take many \n1.6. Runtime Engine Architecture\n\n\n38 \n1. Introduction\nforms, including a  binary space partitioning (BSP) tree, a  quadtree , an  octree , \na  kd-tree , or a  sphere hierarchy . A spatial subdivision is sometimes called a \n scene graph, although technically the latt er is a particular kind of data struc-\nture and does not subsume the former.  Portals or  occlusion culling methods \nmight also be applied in this layer of the rendering engine.\nIdeally, the low-level renderer should be completely agnostic to the type \nof spatial subdivision or scene graph being used. This permits diﬀ erent game \nteams to reuse the primitive submission code, but craft  a PVS determination \nsystem that is speciﬁ c to the needs of each team’s game. The design of the \nOGRE 3D open source rendering engine (htt p://www.ogre3d.org) is a great \nexample of this principle in action. OGRE provides a plug-and-play scene \ngraph architecture. Game developers can either select from a number of pre-\nimplemented scene graph designs, or they can provide a custom scene graph \nimplementation.\n1.6.8.3. Visual Effects\nModern game engines support a wide range of  visual eﬀ ects , as shown in \nFigure 1.21, including\nz  particle system s (for smoke, ﬁ re, water splashes, etc.);\nz  decal systems (for bullet holes, foot prints, etc.);\nz light mapping and environment mapping; \nz dynamic shadows;\nz full-screen  post eﬀ ects , applied aft er the 3D scene has been rendered to \nan oﬀ screen buﬀ er.\nScene Graph / Culling Optimizations\nLOD System\nOcclusion & PVS\nSpatial Subdivision \n(BSP Tree, kd-Tree, …)\nFigure 1.20.  A typical scene graph/spatial subdivision layer, for culling optimization.\nVisual Effects\nParticle & Decal \nSystems\nPost Effects\nHDR Lighting\nPRT Lighting, \nSubsurf. Scatter\nEnvironment \nMapping\nLight Mapping & \nDynamic Shadows\nFigure 1.21.  Visual effects.\n\n\n39 \nSome examples of full-screen post eﬀ ects include\nz high dynamic range (HDR) lighting and bloom ;\nz full-screen anti-aliasing (FSAA);\nz color correction and color-shift  eﬀ ects, including bleach bypass , satura-\ntion and de-saturation eﬀ ects, etc.\nIt is common for a game engine to have an eﬀ ects system component that \nmanages the specialized rendering needs of particles, decals, and other vi-\nsual eﬀ ects . The particle and decal systems are usually distinct components \nof the rendering engine and act as inputs to the low-level renderer . On the \nother hand, light mapping , environment mapping, and shadows are usually \nhandled internally within the rendering engine proper. Full-screen post ef-\nfects are either implemented as an integral part of the renderer or as a separate \ncomponent that operates on the renderer’s output buﬀ ers.\n1.6.8.4. Front End\nMost games employ some kind of 2D graphics  overlaid on the 3D scene for \nvarious purposes. These include\nz the game’s heads-up display (HUD);\nz in-game menus, a console, and/or other development tools, which may or \nmay not be shipped with the ﬁ nal product;\nz possibly an in-game  graphical user interface (GUI), allowing the player to \nmanipulate his or her character’s inventory, conﬁ gure units for batt le, or \nperform other complex in-game tasks.\nThis layer is shown in Figure 1.22. Two-dimensional graphics like these are \nusually implemented by drawing textured quads (pairs of triangles) with an \northographic projection . Or they may be rendered in full 3D, with the quads \nbill-boarded so they always face the camera .\nWe’ve also included the  full-motion video (FMV) system in this layer. This \nsystem is responsible for playing full-screen movies that have been recorded \n1.6. Runtime Engine Architecture\nFront End\nHeads-Up Display \n(HUD)\nFull-Motion Video \n(FMV)\nIn-Game Menus\nIn-Game GUI\nWrappers / Attract \nMode\nIn-Game Cinematics \n(IGC)\nFigure 1.22.  Front end graphics.\n\n\n40 \n1. Introduction\nearlier (either rendered with the game’s rendering engine or using another \nrendering package).\nA related system is the  in-game cinematics (IGC) system. This component \ntypically allows cinematic sequences to be choreographed within the game it-\nself, in full 3D. For example, as the player walks through a city, a conversation \nbetween two key characters might be implemented as an in-game cinematic. \nIGCs may or may not include the player character(s). They may be done as a \ndeliberate cut-away during which the player has no control, or they may be \nsubtly integrated into the game without the human player even realizing that \nan IGC is taking place.\n1.6.9. Proﬁ ling and Debugging Tools\nGames are real-time systems and, as such, game engineers oft en need to proﬁ le \nthe performance of their games in order to optimize performance. In addition, \nmemory resources are usually scarce, so developers make heavy use of mem-\nory analysis tools as well. The  proﬁ ling and  debugging layer, shown in Figure \n1.23, encompasses these tools and also includes in-game debugging facilities, \nsuch as  debug drawing, an  in-game menu system or console, and the ability to \n record and play back gameplay for testing and debugging purposes.\nThere are plenty of good general-purpose soft ware proﬁ ling tools avail-\nable, including\nz Intel’s VTune,\nz IBM’s Quantify and Purify (part of the PurifyPlus tool suite),\nz Compuware’s Bounds Checker.\nHowever, most game engines also incorporate a suite of custom proﬁ ling \nand debugging tools. For example, they might include one or more of the fol-\nlowing:\nz a mechanism for manually instrumenting the code, so that speciﬁ c sec-\ntions of code can be timed;\nz a facility for displaying the proﬁ ling statistics on-screen while the game \nis running;\nz a facility for dumping performance stats to a text ﬁ le or to an Excel \nspreadsheet;\nz a facility for determining how much memory is being used by the en-\ngine, and by each subsystem, including various on-screen displays;\nz the ability to dump memory usage, high-water mark, and leakage stats \nwhen the game terminates and/or during gameplay;\nProfiling & Debugging\nMemory & \nPerformance Stats\nIn-Game Menus \nor Console\nRecording & \nPlayback\nFigure 1.23.  Proﬁ l-\ning and debugging \ntools.\n\n\n41 \n1.6. Runtime Engine Architecture\nz tools that allow debug print statements to be peppered throughout the \ncode, along with an ability to turn on or oﬀ  diﬀ erent categories of debug \noutput and control the level of verbosity of the output;\nz the ability to record game events and then play them back. This is tough \nto get right, but when done properly it can be a very valuable tool for \ntracking down bugs.\n1.6.10. Collision and Physics\n Collision detection is important for every game. Without it, objects would in-\nterpenetrate, and it would be impossible to interact with the virtual world \nin any reasonable way. Some games also include a realistic or semi-realistic \ndynamics simulation . We call this the “physics system” in the game industry, \nalthough the term  rigid body dynamics is really more appropriate, because we \nare usually only concerned with the motion (kinematics) of rigid bodies and \nthe forces and torques (dynamics) that cause this motion to occur. This layer \nis depicted in Figure 1.24.\nCollision and physics are usually quite tightly coupled. This is because \nwhen collisions are detected, they are almost always resolved as part of the \nphysics integration and constraint satisfaction logic. Nowadays, very few \ngame companies write their own collision /physics engine. Instead, a third-\nparty SDK is typically integrated into the engine.\nz  Havok is the gold standard in the industry today. It is feature-rich and \nperforms well across the boards.\nz  PhysX by NVIDIA is another excellent collision and dynamics engine. \nIt was integrated into Unreal Engine 3 and is also available for free as \na standalone product for PC game development. PhysX was originally \ndesigned as the interface to Ageia’s new physics accelerator chip. The \nCollision & Physics\nShapes/\nCollidables\nRigid Bodies\nPhantoms\nRay/Shape \nCasting (Queries)\nForces & \nConstraints\nPhysics/Collision \nWorld\nRagdoll \nPhysics\nFigure 1.24.  Collision and physics subsystem.\n",
      "page_number": 42,
      "chapter_number": 3,
      "summary": "This chapter covers segment 3 (pages 42-63). Key topics include game, gaming, and engine.",
      "keywords": [
        "Game Engine",
        "game",
        "Engine",
        "Unreal Engine",
        "Game Engine Survey",
        "Runtime Engine Architecture",
        "rendering engine",
        "Engine Architecture",
        "system",
        "game rendering engines",
        "game engine architecture",
        "Runtime Engine",
        "game world",
        "Runtime game engine",
        "Open Source Engines"
      ],
      "concepts": [
        "game",
        "gaming",
        "engine",
        "rendering",
        "render",
        "animation",
        "animator",
        "animations",
        "layers",
        "layered"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 8",
          "chapter": 47,
          "title": "Segment 47 (pages 448-460)",
          "relevance_score": 0.69,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 11,
          "title": "Segment 11 (pages 111-118)",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 26,
          "title": "Segment 26 (pages 243-252)",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 64,
          "title": "Segment 64 (pages 617-624)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "makinggames",
          "chapter": 35,
          "title": "Segment 35 (pages 308-315)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 64-82)",
      "start_page": 64,
      "end_page": 82,
      "detection_method": "synthetic",
      "content": "42 \n1. Introduction\nSDK is now owned and distributed by NVIDIA, and the company is \nadapting PhysX to run on its latest GPUs.\nOpen source physics and collision engines are also available. Perhaps the \nbest-known of these is the  Open Dynamics Engine (ODE). For more informa-\ntion, see htt p://www.ode.org. I-Collide, V-Collide, and RAPID are other popu-\nlar non-commercial collision detection engines. All three were developed at the \nUniversity of North Carolina (UNC). For more information, see htt p://www.\ncs.unc.edu/~geom/I_COLLIDE/index.html, htt p://www.cs.unc.edu/~geom/V_\nCOLLIDE/index.html, and htt p://www.cs.unc.edu/~geom/OBB/OBBT.html.\n1.6.11. Animation\nAny game that has organic or semi-organic characters (humans, animals, car-\ntoon characters, or even robots) needs an  animation system. There are ﬁ ve \nbasic types of animation used in games:\nz sprite/texture an imation,\nz rigid body hierarchy animation,\nz skeletal animation,\nz vertex animation, and\nz morph targets.\nSkeletal animation permits a detailed 3D character mesh to be posed by \nan animator using a relatively simple system of bones. As the bones move, the \nvertices of the 3D mesh move with them. Although morph targets and vertex \nanimation are used in some engines, skeletal animation is the most prevalent \nanimation method in games today; as such, it will be our primary focus in this \nbook. A typical skeletal animation system is shown in Figure 1.25.\nSkeletal Animation\nAnimation \nDecompression\nInverse \nKinematics (IK)\nGame-Specific \nPost-Processing\nSub-skeletal \nAnimation\nLERP and \nAdditive Blending\nAnimation \nPlayback\nAnimation State \nTree & Layers\nFigure 1.25.  Skeletal animation subsystem.\n\n\n43 \nYou’ll notice in Figure 1.11 that Skeletal Mesh Rendering is a component \nthat bridges the gap between the renderer and the animation system. There \nis a tight cooperation happening here, but the interface is very well deﬁ ned. \nThe animation system produces a pose for every bone in the skeleton, and \nthen these poses are passed to the rendering engine as a palett e of matrices. \nThe renderer transforms each vertex by the matrix or matrices in the palett e, \nin order to generate a ﬁ nal blended vertex position. This process is known as \nskinning.\nThere is also a tight coupling between the animation and physics systems, \nwhen rag dolls are employed. A rag doll is a limp (oft en dead) animated char-\nacter, whose bodily motion is simulated by the physics system. The physics \nsystem determines the positions and orientations of the various parts of the \nbody by treating them as a constrained system of rigid bodies. The animation \nsystem calculates the palett e of matrices required by the rendering engine in \norder to draw the character on-screen.\n1.6.12. Human Interface Devices (HID)\nEvery game needs to process input from the player, obtained from various \nhuman interface device s (HIDs) including\nz the keyboard and mouse,\nz a joypad, or\nz other specialized game controllers, like steering wheels, ﬁ shing rods, \ndance pads, the WiiMote, etc.\nWe sometimes call this component the player I/O component, because we \nmay also provide output to the player through the HID , such as force feed-\nback /rumble on a joypad or the audio produced by the WiiMote. A typical \nHID layer is shown in Figure 1.26.\nThe HID engine component is sometimes architected to divorce the \nlow-level details of the game controller(s) on a particular hardware platform \nfrom the high-level game controls. It massages the raw data coming from the \nhardware, introducing a dead zone around the center point of each joypad \nstick, de-bouncing butt on-press inputs, detecting butt on-down and butt on-\nup events, interpreting and smoothing accelerometer inputs (e.g., from the \nPLAYSTATION 3 Sixaxis controller), and more. It oft en provides a mecha-\nnism allowing the player to customize the mapping between physical controls \nand logical game functions. It sometimes also includes a system for detecting \nchords (multiple butt ons pressed together), sequences (butt ons pressed in se-\nquence within a certain time limit), and gestures (sequences of inputs from the \nbutt ons, sticks, accelerometers, etc.).\n1.6. Runtime Engine Architecture\nHuman Interface \nDevices (HID)\nPhysical Device\nI/O\nGame-Specific \nInterface\nFigure 1.26.  The \nplayer \ninput/out-\nput system, also \nknown as the hu-\nman interface de-\nvice (HID) layer.\n\n\n44 \n1. Introduction\n1.6.13. Audio\nAudio is just as important as graphics in any game engine. Unfortunately, \n audio oft en gets less att ention than rendering, physics, animation, AI, and \ngameplay. Case in point: Programmers oft en develop their code with their \nspeakers turned oﬀ ! (In fact, I’ve known quite a few game programmers \nwho didn’t even have speakers or headphones.) Nonetheless, no great game \nis complete without a stunning audio engine. The audio layer is depicted in \nFigure 1.27.\nAudio engines vary greatly in sophistication. Quake ’s and Unreal ’s au-\ndio engines are prett y basic, and game teams usually augment them with \ncustom functionality or replace them with an in-house solution. For DirectX \nplatforms (PC and Xbox 360), Microsoft  provides an excellent audio tool suite \ncalled  XACT . Electronic Arts has developed an advanced, high-powered au-\ndio engine internally called SoundR!OT. In conjunction with ﬁ rst-party stu-\ndios like Naughty Dog, Sony Computer Entertainment America (SCEA) pro-\nvides a powerful 3D audio engine called  Scream, which has been used on \na number of PS3 titles including Naughty Dog’s Uncharted: Drake’s Fortune. \nHowever, even if a game team uses a pre-existing audio engine, every game \nrequires a great deal of custom soft ware development, integration work, ﬁ ne-\ntuning, and att ention to detail in order to produce high-quality audio in the \nﬁ nal product.\n1.6.14. Online Multiplayer/Networking\nMany games permit multiple human players to play within a single virtual \nworld.  Multiplayer games come in at least four basic ﬂ avors.\nz Single-screen multiplayer. Two or more human interface devices (joypads, \nkeyboards, mice, etc.) are connected to a single arcade machine, PC, or \nconsole. Multiple player characters inhabit a single virtual world, and a \nsingle camera keeps all player characters in frame simultaneously. Ex-\namples of this style of multiplayer gaming include Smash Brothers, Lego \nStar Wars, and Gauntlet.\nz  Split-screen multiplayer. Multiple player characters inhabit a single vir-\ntual world, with multiple HIDs att ached to a single game machine, but \neach with its own camera, and the screen is divided into sections so that \neach player can view his or her character.\nz  Networked multiplayer. Multiple computers or consoles are networked \ntogether, with each machine hosting one of the players.\nz Massively multiplayer online games (MMOG). Literally hundreds of \nthousands of users can be playing simultaneously within a giant, per-\nAudio\nAudio Playback / \nManagement\nDSP/Effects\n3D Audio Model\nFigure 1.27.  Audio \nsubsystem.\n\n\n45 \nsistent, online virtual world hosted by a powerful batt ery of central \nservers.\nThe multiplayer networking layer is shown in Figure 1.28.\nMultiplayer games are quite similar in many ways to their single-player \ncounterparts. However, support for multiple players can have a profound \nimpact on the design of certain game engine components. The game world \nobject model, renderer, human input device system, player control system, \nand animation systems are all aﬀ ected. Retroﬁ tt ing multiplayer features into \na pre-existing single-player engine is certainly not impossible, although it can \nbe a daunting task. Still, many game teams have done it successfully. That \nsaid, it is usually bett er to design multiplayer features from day one, if you \nhave that luxury.\nIt is interesting to note that going the other way—converting a multi-\nplayer game into a single-player game—is typically trivial. In fact, many game \nengines treat single-player mode as a special case of a multiplayer game, in \nwhich there happens to be only one player. The Quake engine is well known \nfor its client-on-top-of-server mode, in which a single executable, running on a \nsingle PC, acts both as the client and the server in single-player campaigns.\n1.6.15. Gameplay Foundation Systems\nThe term  gameplay refers to the action that takes place in the game, the rules \nthat govern the virtual world in which the game takes place, the abilities of \nthe player character(s) (known as player mechanics) and of the other characters \nand objects in the world, and the goals and objectives of the player(s). Game-\nplay is typically implemented either in the native language in which the rest \nof the engine is writt en, or in a high-level scripting language—or sometimes \nboth. To bridge the gap between the gameplay code and the low-level engine \nsystems that we’ve discussed thus far, most game engines introduce a layer \n1.6. Runtime Engine Architecture\nGameplay Foundations\nEvent/Messaging \nSystem\nDynamic Game \nObject Model\nScripting System\nWorld Loading / \nStreaming\nStatic World \nElements\nReal-Time Agent-\nBased Simulation\nHigh-Level Game Flow System/FSM\nHierarchical \nObject Attachment\nFigure 1.29.  Gameplay foundation systems.\nOnline Multiplayer\nMatch-Making & \nGame Mgmt.\nGame State \nReplication\nObject Authority \nPolicy\nFigure 1.28.  On-\nline \nmultiplayer \nsubsystem.\n\n\n46 \n1. Introduction\nthat I’ll call the  gameplay foundations layer (for lack of a standardized name). \nShown in Figure 1.29, this layer provides a suite of core facilities, upon which \ngame-speciﬁ c logic can be implemented conveniently.\n1.6.15.1. Game Worlds and Object Models\nThe gameplay foundations layer introduces the notion of a  game world, con-\ntaining both static and dynamic elements. The contents of the world are usu-\nally modeled in an object-oriented manner (oft en, but not always, using an \nobject-oriented programming language). In this book, the collection of object \ntypes that make up a game is called the  game object model. The game object \nmodel provides a real-time simulation of a heterogeneous collection of objects \nin the virtual game world.\nTypical types of game objects include\nz static background geometry, like buildings, roads, terrain (oft en a spe-\ncial case), etc.;\nz dynamic rigid bodies, such as rocks, soda cans, chairs, etc.;\nz player characters (PC);\nz non-player characters (NPC);\nz weapons;\nz projectiles;\nz vehicles;\nz lights (which may be present in the dynamic scene at run time, or only \nused for static lighting oﬄ  ine);\nz cameras;\nz and the list goes on.\nThe game world model is intimately tied to a  soft ware object model, and \nthis model can end up pervading the entire engine. The term soft ware object \nmodel refers to the set of language features, policies, and conventions used to \nimplement a piece of object-oriented soft ware. In the context of game engines, \nthe soft ware object model answers questions, such as:\nz Is your game engine designed in an object-oriented manner?\nz What language will you use? C? C++? Java? OCaml?\nz How will the static class hierarchy be organized? One giant monolithic \nhierarchy? Lots of loosely coupled components?\nz Will you use templates and policy-based design, or traditional polymor-\nphism?\nz How are objects referenced? Straight old pointers? Smart pointers? \nHandles?\n\n\n47 \nz How will objects be uniquely identiﬁ ed? By address in memory only? \nBy name? By a global unique identiﬁ er (GUID)?\nz How are the lifetimes of game objects managed?\nz How are the states of the game objects simulated over time?\nWe’ll explore soft ware object models and game object models in consider-\nable depth in Section 14.2.\n1.6.15.2.  Event System\nGame objects invariably need to communicate with one another. This can be \naccomplished in all sorts of ways. For example, the object sending the message \nmight simply call a member function of the receiver object. An  event-driven \narchitecture, much like what one would ﬁ nd in a typical graphical user inter-\nface, is also a common approach to inter-object communication. In an event-\ndriven system, the sender creates a litt le data structure called an event or mes-\nsage, containing the message’s type and any argument data that are to be sent. \nThe event is passed to the receiver object by calling its  event handler function. \nEvents can also be stored in a queue for handling at some future time.\n1.6.15.3. Scripting System\nMany game engines employ a  scripting language in order to make develop-\nment of game-speciﬁ c gameplay rules and content easier and more rapid. \nWithout a scripting language, you must recompile and relink your game ex-\necutable every time a change is made to the logic or data structures used in the \nengine. But when a scripting language is integrated into your engine, changes \nto game logic and data can be made by modifying and reloading the script \ncode. Some engines allow script to be reloaded while the game continues to \nrun. Other engines require the game to be shut down prior to script recompi-\nlation. But either way, the turn-around time is still much faster than it would \nbe if you had to recompile and relink the game’s executable.\n1.6.15.4. Artiﬁ cial Intelligence Foundations\nTraditionally,  artiﬁ cial intelligence (AI) has fallen squarely into the realm of \ngame-speciﬁ c soft ware—it was usually not considered part of the game en-\ngine per se. More recently, however, game companies have recognized pat-\nterns that arise in almost every AI system, and these foundations are slowly \nstarting to fall under the purview of the engine proper.\nA company called Kynogon has developed a commercial AI engine called \n Kynapse , which acts as an “AI foundation layer” upon which game-speciﬁ c \nAI logic can be quite easily developed. Kynapse provides a powerful suite of \nfeatures, including\n1.6. Runtime Engine Architecture\n\n\n48 \n1. Introduction\nz a network of path nodes or roaming volumes, that deﬁ nes areas or paths \nwhere AI characters are free to move without fear of colliding with static \nworld geometry;\nz simpliﬁ ed collision information around the edges of each free-roaming \narea;\nz knowledge of the entrances and exits from a region, and from where in \neach region an enemy might be able to see and/or ambush you;\nz a path-ﬁ nding engine based on the well-known A* algorithm;\nz hooks into the collision system and world model, for line-of-sight (LOS) \ntraces and other perceptions;\nz a custom world model which tells the AI system where all the entities of \ninterest (friends, enemies, obstacles) are, permits dynamic avoidance of \nmoving objects, and so on.\nKynapse also provides an architecture for the AI decision layer, including \nthe concept of brains (one per character), agents (each of which is responsible \nfor executing a speciﬁ c task, such as moving from point to point, ﬁ ring on an \nenemy, searching for enemies, etc.), and actions (responsible for allowing the \ncharacter to perform a fundamental movement, which oft en results in playing \nanimations on the character’s skeleton).\n1.6.16. Game-Speciﬁ c Subsystems\nOn top of the gameplay foundation layer and the other low-level engine com-\nponents,  gameplay programmers and designers cooperate to implement the \nfeatures of the game itself. Gameplay systems are usually numerous, highly \nvaried, and speciﬁ c to the game being developed. As shown in Figure 1.30, \nthese systems include, but are certainly not limited to the mechanics of the \nplayer character, various in-game camera systems, artiﬁ cial intelligence for \nthe control of non-player characters (NPCs), weapon systems, vehicles, and \nGAME-SPECIFIC SUBSYSTEMS\nGame-Specific Rendering\nTerrain Rendering\nWater Simulation \n& Rendering\netc.\nPlayer Mechanics\nCollision Manifold\nMovement\nState Machine & \nAnimation\nGame Cameras\nPlayer-Follow \nCamera\nDebug Fly-\nThrough Cam\nFixed Cameras\nScripted/Animated \nCameras\nAI\nSight Traces & \nPerception\nPath Finding (A*)\nGoals & Decision-\nMaking\nActions\n(Engine Interface)\nCamera-Relative \nControls (HID)\nWeapons\nPower-Ups\netc.\nVehicles\nPuzzles\nFigure 1.30.  Game-speciﬁ c subsystems.\n\n\n49 \n1.7. Tools and the Asset Pipeline\nthe list goes on. If a clear line could be drawn between the engine and the \ngame, it would lie between the game-speciﬁ c subsystems and the gameplay \nfoundations layer. Practically speaking, this line is never perfectly distinct. \nAt least some game-speciﬁ c knowledge invariably seeps down through the \ngameplay foundations layer and sometimes even extends into the core of the \nengine itself.\n1.7. \n Tools and the Asset Pipeline\nAny game engine must be fed a great deal of data, in the form of  game assets, \nconﬁ guration ﬁ les, scripts, and so on. Figure 1.31 depicts some of the types of \ngame assets typically found in modern game engines. The thicker dark-grey \narrows show how data ﬂ ows from the tools used to create the original  source \nassets all the way through to the game engine itself. The thinner light-grey ar-\nrows show how the various types of assets refer to or use other assets.\n1.7.1. \nDigital Content Creation Tools\nGames are multimedia applications by nature. A game engine’s input data \ncomes in a wide variety of forms, from 3D mesh data to texture bitmaps to \nanimation data to audio ﬁ les. All of this source data must be created and ma-\nnipulated by artists. The tools that the artists use are called digital content cre-\nation (DCC) applications.\nA DCC application is usually targeted at the creation of one particular \ntype of data—although some tools can produce multiple data types. For ex-\nample, Autodesk’s Maya and 3ds Max are prevalent in the creation of both \n3D meshes and animation data. Adobe’s Photoshop and its ilk are aimed at \ncreating and editing bitmaps (textures). SoundForge is a popular tool for cre-\nating audio clips. Some types of game data cannot be created using an oﬀ -\nthe-shelf DCC app. For example, most game engines provide a custom editor \nfor laying out game worlds. Still, some engines do make use of pre-existing \ntools for  game world layout. I’ve seen game teams use 3ds Max or Maya as a \nworld layout tool, with or without custom plug-ins to aid the user. Ask most \ngame developers, and they’ll tell you they can remember a time when they \nlaid out terrain height ﬁ elds using a simple bitmap editor, or typed world \nlayouts directly into a text ﬁ le by hand. Tools don’t have to be prett y—game \nteams will use whatever tools are available and get the job done. That said, \ntools must be relatively easy to use, and they absolutely must be reliable, if a \ngame team is going to be able to develop a highly polished product in a timely \nmanner.\n\n\n50 \n1. Introduction\nDigital Content Creation (DCC) Tools\nGame World\nGame \nObject\nMesh\nSkeletal Hierarchy \nExporter\nSkel. \nHierarchy\nAnimation \nExporter\nAnimation \nCurves\nTGA\nTexture\nDXT Compression\nDXT \nTexture\nWorld Editor\nGame Object \nDefinition Tool\nMaterial\nGame Obj. \nTemplate\nAnimation \nSet\nAnimation Tree \nEditor\nAnimation \nTree\nGame \nObject\nGame \nObject\nAsset \nConditioning \nPipeline\nGAME\nWAV\nsound\nAudio Manager \nTool\nSound \nBank\nMesh Exporter\nPhotoshop\nPhotoshop\nSound Forge or Audio Tool\nSound Forge or Audio Tool\nGame \nObject\nMaya, 3DSMAX, etc.\nMaya, 3DSMAX, etc.\nCustom Material \nPlug-In\nHoudini/Other Particle Tool\nHoudini/Other Particle Tool\nParticle \nSystem\nParticle Exporter\nFigure 1.31.  Tools and the asset pipeline.\n1.7.2. \n Asset Conditioning Pipeline\nThe data formats used by digital content creation (DCC) applications are rare-\nly suitable for direct use in-game. There are two primary reasons for this.\n1. The DCC app’s in-memory model of the data is usually much more \ncomplex than what the game engine requires. For example, Maya stores \na directed acyclic graph (DAG) of scene nodes, with a complex web \nof interconnections. It stores a history of all the edits that have been \nperformed on the ﬁ le. It represents the position, orientation, and scale \nof every object in the scene as a full hierarchy of 3D transformations, \ndecomposed into translation, rotation, scale, and shear components. A \n\n\n51 \ngame engine typically only needs a tiny fraction of this information in \norder to render the model in-game.\n2. The DCC application’s ﬁ le format is oft en too slow to read at run time, \nand in some cases it is a closed proprietary format.\nTherefore, the data produced by a DCC app is usually exported to a more ac-\ncessible standardized format, or a custom ﬁ le format, for use in-game.\nOnce data has been exported from the DCC app, it oft en must be fur-\nther processed before being sent to the game engine. And if a game studio \nis shipping its game on more than one platform, the intermediate ﬁ les might \nbe processed diﬀ erently for each target platform. For example, 3D mesh data \nmight be exported to an intermediate format, such as XML or a simple binary \nformat. Then it might be processed to combine meshes that use the same ma-\nterial, or split up meshes that are too large for the engine to digest. The mesh \ndata might then be organized and packed into a memory image suitable for \nloading on a speciﬁ c hardware platform.\nThe pipeline from DCC app to game engine is sometimes called the asset \nconditioning pipeline . Every game engine has this in some form.\n1.7.3. \n3D Model/Mesh Data\nThe visible geometry you see in a game is typically made up of two kinds of \ndata.\n1.7.3.1. \nBrush Geometry\n Brush geometry is deﬁ ned as a collection of convex hulls, each of which is de-\nﬁ ned by multiple planes. Brushes are typically created and edited directly in \nthe game world editor. This is what some would call an “old school” approach \nto creating renderable geometry, but it is still used.\nPros:\nz fast and easy to create;\nz accessible to game designers—oft en used to “block out” a game level for \nprototyping purposes;\nz can serve both as collision volumes and as renderable geometry.\nCons:\nz low-resolution – diﬃ  cult to create complex shapes;\nz cannot support articulated objects or animated characters.\n1.7.3.2. \n3D Models (Meshes)\nFor detailed scene elements,  3D models (also referred to as  meshes) are superior \nto brush geometry. A mesh is a complex shape composed of triangles and ver-\n1.7. Tools and the Asset Pipeline\n\n\n52 \n1. Introduction\ntices. (A mesh might also be constructed from quads or higher-order subdivi-\nsion surfaces. But on today’s graphics hardware, which is almost exclusively \ngeared toward rendering rasterized triangles, all shapes must eventually be \ntranslated into triangles prior to rendering.)  A mesh typically has one or more \nmaterials applied to it, in order to deﬁ ne visual surface properties (color, re-\nﬂ ectivity, bumpiness, diﬀ use texture , etc.). In this book, I will use the term \n“mesh ” to refer to a single renderable shape, and “model” to refer to a com-\nposite object that may contain multiple meshes, plus animation data and other \nmetadata for use by the game.\nMeshes are typically created in a 3D modeling package such as  3ds Max, \n Maya, or  Soft Image. A relatively new tool called  ZBrush allows ultra high-\nresolution meshes to be built in a very intuitive way and then down-converted \ninto a lower-resolution model with normal maps to approximate the high-\nfrequency detail.\nExporters must be writt en to extract the data from the digital content \ncreation (DCC) tool (Maya, Max, etc.) and store it on disk in a form that \nis digestible by the engine. The DCC apps provide a host of standard or \nsemi-standard export formats, although none are perfectly suited for game \ndevelopment (with the possible exception of COLLADA). Therefore, game \nteams oft en create custom ﬁ le formats and custom exporters to go with \nthem.\n1.7.4. \nSkeletal Animation Data\nA  skeletal mesh is a special kind of mesh that is bound to a skeletal hierarchy \nfor the purposes of articulated animation. Such a mesh is sometimes called a \nskin, because it forms the skin that surrounds the invisible underlying skel-\neton. Each vertex of a skeletal mesh contains a list of indices indicating to \nwhich joint(s) in the skeleton it is bound. A vertex usually also includes a \nset of joint weights, specifying the amount of inﬂ uence each joint has on the \nvertex.\nIn order to render a skeletal mesh , the game engine requires three distinct \nkinds of data.\n1. the mesh itself,\n2. the skeletal hierarchy (joint names, parent-child relationships and the \nbase pose the skeleton was in when it was originally bound to the mesh ), \nand\n3. one or more animation clips, which specify how the joints should move \nover time.\n\n\n53 \nThe mesh and skeleton are oft en exported from the DCC application as a sin-\ngle data ﬁ le. However, if multiple meshes are bound to a single skeleton, then \nit is bett er to export the skeleton as a distinct ﬁ le. The animations are usually \nexported individually, allowing only those animations which are in use to be \nloaded into memory at any given time. However, some game engines allow \na bank of animations to be exported as a single ﬁ le, and some even lump the \nmesh, skeleton, and animations into one monolithic ﬁ le.\nAn unoptimized skeletal animation is deﬁ ned by a stream of 4 × 3 matrix \nsamples, taken at a frequency of at least 30 frames per second, for each of the \njoints in a skeleton (of which there are oft en 100 or more). Thus animation data \nis inherently memory-intensive. For this reason, animation data is almost al-\nways stored in a highly compressed format. Compression schemes vary from \nengine to engine, and some are proprietary. There is no one standardized for-\nmat for game-ready animation data.\n1.7.5. \nAudio Data\nAudio clips are usually exported from  Sound Forge or some other audio pro-\nduction tool in a variety of formats and at a number of diﬀ erent data sam-\npling rates. Audio ﬁ les may be in mono, stereo, 5.1, 7.1, or other multichannel \nconﬁ gurations. Wave ﬁ les (.wav) are common, but other ﬁ le formats such as \nPlayStation ADPCM ﬁ les (.vag and .xvag) are also commonplace. Audio clips \nare oft en organized into banks for the purposes of organization, easy loading \ninto the engine, and streaming.\n1.7.6. \nParticle Systems Data\nModern games make use of complex particle eﬀ ects. These are authored by \nartists who specialize in the creation of visual eﬀ ects . Third-party tools, such \nas  Houdini, permit ﬁ lm-quality eﬀ ects to be authored; however, most game \nengines are not capable of rendering the full gamut of eﬀ ects that can be cre-\nated with Houdini. For this reason, many game companies create a custom \nparticle eﬀ ect editing tool, which exposes only the eﬀ ects that the engine actu-\nally supports. A custom tool might also let the artist see the eﬀ ect exactly as it \nwill appear in-game.\n1.7.7. \nGame World Data and the World Editor\nThe  game world is where everything in a game engine comes together. To my \nknowledge, there are no commercially available  game world editors (i.e., the \ngame world equivalent of Maya or Max). However, a number of commercially \navailable game engines provide good world editors.\n1.7. Tools and the Asset Pipeline\n\n\n54 \n1. Introduction\nz Some variant of the  Radiant game editor is used by most game engines \nbased on Quake technology;\nz The Half-Life 2 Source engine provides a world editor called  Hammer;\nz  UnrealEd is the Unreal Engine’s world editor. This powerful tool also \nserves as the asset manager for all data types that the engine can con-\nsume.\nWriting a good world editor is diﬃ  cult, but it is an extremely important \npart of any good game engine.\n1.7.8. Some Approaches to Tool Architecture\nA game engine’s tool suite may be architected in any number of ways. Some \ntools might be standalone pieces of soft ware, as shown in Figure 1.32. Some \ntools may be built on top of some of the lower layers used by the runtime en-\ngine, as Figure 1.33 illustrates. Some tools might be built into the game itself. \nFor example, Quake - and Unreal -based games both boast an in-game console \nthat permits developers and “modders” to type debugging and conﬁ guration \ncommands while running the game.\nAs an interesting and unique example, Unreal ’s world editor and asset \nmanager, UnrealEd , is built right into the runtime game engine. To run the \neditor, you run your game with a command-line argument of “editor.” This \nunique architectural style is depicted in Figure 1.34. It permits the tools to \nhave total access to the full range of data structures used by the engine and \nOS\nDrivers\nHardware (PC, XBOX360, PS3, etc.)\n3rd Party SDKs\nPlatform Independence Layer\nCore Systems\nRun-Time Engine\nTools and World Builder\nFigure 1.32.  Standalone tools architecture.\n\n\n55 \navoids a common problem of having to have two representations of every \ndata structure – one for the runtime engine and one for the tools. It also means \nthat running the game from within the editor is very fast (because the game \nis actually already running). Live in-game editing, a feature that is normally \nvery tricky to implement, can be developed relatively easily when the editor is \na part of the game. However, an in-engine editor design like this does have its \nshare of problems. For example, when the engine is crashing, the tools become \nunusable as well. Hence a tight coupling between engine and asset creation \ntools can tend to slow down production.\nOS\nDrivers\nHardware (PC, XBOX360, PS3, etc.)\n3rd Party SDKs\nPlatform Independence Layer\nCore Systems\nRun-Time Engine\nTools and World Builder\nFigure 1.33.  Tools built on a framework shared with the game.\nOS\nDrivers\nHardware (PC, XBOX360, PS3, etc.)\n3\nrd Party SDKs\nPlatform Independence Layer\nCore Systems\nRun-Time Engine\nOther Tools\nWorld Builder\nFigure 1.34.  UnrealEngine’s tool architecture.\n1.7. Tools and the Asset Pipeline\n\n\n57\n2\n \nTools of the Trade\nB\nefore we embark on our journey across the fascinating landscape of game \nengine architecture, it is important that we equip ourselves with some ba-\nsic tools and provisions. In the next two chapters, we will review the soft ware \nengineering concepts and practices that we will need during our voyage.  In \nChapter 2, we’ll explore the tools used by the majority of professional game \nengineers.  Then in Chapter 3, we’ll round out our preparations by reviewing \nsome key topics in the realms of object-oriented programming, design pat-\nterns, and large-scale C++ programming.\nGame development is one of the most demanding and broad areas of soft -\nware engineering, so believe me, we’ll want to be well equipped if we are to \nsafely navigate the sometimes-treacherous terrain we’ll be covering. For some \nreaders, the contents of this chapter and the next will be very familiar. How-\never, I encourage you not to skip these chapters entirely. I hope that they will \nserve as a pleasant refresher; and who knows—you might even pick up a new \ntrick or two.\n2.1. \n Version Control\nA version control system is a tool that permits multiple users to work on a \ngroup of ﬁ les collectively. It maintains a history of each ﬁ le, so that changes \n\n\n58 \n2. Tools of the Trade\ncan be tracked and reverted if necessary. It permits multiple users to modify \nﬁ les—even the same ﬁ le—simultaneously, without everyone stomping on \neach other’s work. Version control gets its name from its ability to track the \nversion history of ﬁ les. It is sometimes called source control, because it is pri-\nmarily used by computer programmers to manage their source code. Howev-\ner, version control can be used for other kinds of ﬁ les as well. Version control \nsystems are usually best at managing text ﬁ les, for reasons we will discover \nbelow. However, many game studios use a single version control system to \nmanage both source code ﬁ les (which are text) and game assets like textures, \n3D meshes, animations, and audio ﬁ les (which are usually binary).\n2.1.1. \nWhy Use Version Control?\nVersion control is crucial whenever soft ware is developed by a team of mul-\ntiple engineers. Version control\nz provides a central repository from which engineers can share source \ncode;\nz keeps a history of the changes made to each source ﬁ le;\nz provides mechanisms allowing speciﬁ c versions of the code base to be \ntagged and later retrieved;\nz permits versions of the code to be branched oﬀ  from the main develop-\nment line, a feature oft en used to produce demos or make patches to \nolder versions of the soft ware.\nA source control system can be useful even on a single-engineer project. Al-\nthough its multiuser capabilities won’t be relevant, its other abilities, such \nas maintaining a history of changes, tagging versions, creating branches for \ndemos and patches, tracking bugs, etc., are still invaluable.\n2.1.2. \nCommon Version Control Systems\nHere are the most common source control systems you’ll probably encounter \nduring your career as a game engineer.\nz  SCCS and  RCS. The Source Code Control System (SCCS) and the Revi-\nsion Control System (RCS) are two of the oldest version control systems. \nBoth employ a command-line interface. They are prevalent primarily on \nUNIX platforms.\nz  CVS. The Concurrent Version System (CVS) is a heavy-duty profession-\nal-grade command-line-based source control system, originally built on \ntop of RCS (but now implemented as a standalone tool). CVS is preva-\n\n\n59 \n2.1. Version Control\nlent on UNIX systems but is also available on other development plat-\nforms such as Microsoft  Windows. It is open source and licensed under \nthe Gnu General Public License (GPL). CVSNT (also known as WinCVS) \nis a native Windows implementation that is based on, and compatible \nwith, CVS.\nz  Subversion. Subversion is an open source version control system aimed \nat replacing and improving upon CVS. Because it is open source and \nhence free, it is a great choice for individual projects, student projects, \nand small studios.\nz  Git. This is an open source revision control system that has been \nused for many venerable projects, including the Linux kernel. In the \ngit development model, the programmer makes changes to ﬁ les and \ncommits the changes to a branch. The programmer can then merge \nhis changes into any other code branch quickly and easily, because git \n“knows” how to rewind a sequence of diﬀ s and reapply them onto \na new base revision—a process git calls rebasing. The net result is a \nrevision control system that is highly eﬃ  cient and fast when dealing \nwith multiple code branches. More information on git can be found at \nhtt p://git-scm.com/.\nz  Perforce. Perforce is a professional-grade source control system, with \nboth text-based and GUI interfaces. One of Perforce’s claims to fame is \nits concept of change lists. A change list is a collection of source ﬁ les that \nhave been modiﬁ ed as a logical unit. Change lists are checked into the \nrepository atomically – either the entire change list is submitt ed, or none \nof it is. Perforce is used by many game companies, including Naughty \nDog and Electronic Arts.\nz NxN Alienbrain.  Alienbrain is a powerful and feature-rich source control \nsystem designed explicitly for the game industry. Its biggest claim to \nfame is its support for very large databases containing both text source \ncode ﬁ les and binary game art assets, with a customizable user interface \nthat can be targeted at speciﬁ c disciplines such as artists, producers, or \nprogrammers.\nz ClearCase.  ClearCase is professional-grade source control system aimed \nat very large-scale soft ware projects. It is powerful and employs a \nunique user interface that extends the functionality of Windows Explor-\ner. I haven’t seen ClearCase used much in the game industry, perhaps \nbecause it is one of the more expensive version control systems.\nz Microsoft  Visual SourceSafe.  SourceSafe is a light-weight source control \npackage that has been used successfully on some game projects.\n\n\n60 \n2. Tools of the Trade\n2.1.3. \nOverview of Subversion and TortoiseSVN\nI have chosen to highlight Subversion in this book for a few reasons. First oﬀ , \nit’s free, which is always nice. It works well and is reliable, in my experience. \nA Subversion central  repository is quite easy to set up; and as we’ll see, there \nare already a number of free repository servers out there, if you don’t want to \ngo to the trouble of sett ing one up yourself. There are also a number of good \nWindows and Mac Subversion clients, such as the freely available Tortois-\neSVN for Windows. So while Subversion may not be the best choice for a large \ncommercial project (I personally prefer Perforce for that purpose), I ﬁ nd it \nperfectly suited to small personal and educational projects. Let’s take a look at \nhow to set up and use Subversion on a Microsoft  Windows PC development \nplatform. As we do so, we’ll review core concepts that apply to virtually any \nversion control system.\nSubversion, like most other version control systems, employs a client-\nserver architecture. The server manages a central repository, in which a ver-\nsion-controlled directory hierarchy is stored. Clients connect to the server and \nrequest operations, such as checking out the latest version of the directory \ntree, committ ing new changes to one or more ﬁ les, tagging revisions, branch-\ning the repository, and so on. We won’t discuss sett ing up a server here; we’ll \nassume you have a server, and instead we will focus on sett ing up and using \nthe client. You can learn how to set up a Subversion server by reading Chap-\nter 6 of [37]. However you probably will never need to do so, because you \ncan always ﬁ nd free Subversion servers. For example, Google provides free \nSubversion code hosting at htt p://code.google.com/.\n2.1.4. Setting up a Code Repository on Google\nThe easiest way to get started with Subversion is to visit htt p://code.google.\ncom/ and set up a free Subversion repository. Create a Google user name and \npassword if you don’t already have one, then navigate to Project Hosting un-\nder Developer Resources (see Figure 2.1). Click “Create a new project,” then \nenter a suitable unique project name, like “mygoogleusername-code.” You can \nenter a summary and/or description if you like, and even provide tags so that \nother users all over the world can search for and ﬁ nd your repository. Click \nthe “Create Project” butt on and you’re oﬀ  to the races.\nOnce you’ve created your repository, you can administer it on the  Google \nCode website. You can add and remove users, control options, and perform a \nwealth of advanced tasks. But all you really need to do next is set up a Subver-\nsion client and start using your repository.\n",
      "page_number": 64,
      "chapter_number": 4,
      "summary": "This chapter covers segment 4 (pages 64-82). Key topics include game, engines, and engineering. Although morph targets and vertex \nanimation are used in some engines, skeletal animation is the most prevalent \nanimation method in games today; as such, it will be our primary focus in this \nbook.",
      "keywords": [
        "game",
        "game engine",
        "Engine",
        "game world",
        "control system",
        "Game Object",
        "Version control systems",
        "source control system",
        "Game Object Model",
        "system",
        "Tools",
        "Animation",
        "world",
        "Version Control",
        "data"
      ],
      "concepts": [
        "game",
        "engines",
        "engineering",
        "animation",
        "animals",
        "animator",
        "animated",
        "animations",
        "tool",
        "object"
      ],
      "similar_chapters": [
        {
          "book": "makinggames",
          "chapter": 6,
          "title": "Segment 6 (pages 43-50)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 13,
          "title": "Segment 13 (pages 121-128)",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 48,
          "title": "Segment 48 (pages 462-474)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 33,
          "title": "Segment 33 (pages 314-325)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 46,
          "title": "Segment 46 (pages 443-452)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 83-102)",
      "start_page": 83,
      "end_page": 102,
      "detection_method": "synthetic",
      "content": "61 \n2.1.5. \nInstalling  TortoiseSVN\nTortoiseSVN is a popular front-end for Subversion. It extends the functionality \nof the Microsoft  Windows Explorer via a convenient right-click menu and over-\nlay icons to show you the status of your version-controlled ﬁ les and folders.\nTo get TortoiseSVN, visit htt p://tortoisesvn.tigris.org/. Download the lat-\nest version from the download page. Install it by double-clicking the .msi ﬁ le \nthat you’ve downloaded and following the installation wizard’s instructions.\nOnce TortoiseSVN is installed, you can go to any folder in Windows Ex-\nplorer and right-click—TortoiseSVN’s menu extensions should now be vis-\nible. To connect to an existing code repository (such as one you created on \nGoogle Code), create a folder on your local hard disk and then right-click \nand select “SVN Checkout….”  The dialog shown in Figure 2.2 will appear. \nIn the “URL of repository” ﬁ eld, enter your repository’s URL. If you are using \nGoogle Code, it should be htt ps://myprojectname.googlecode.com/svn/trunk, \nwhere myprojectname is whatever you named your project when you ﬁ rst cre-\nated it (e.g., “mygoogleusername-code”).\nIf you forget the URL of your repository, just log in to htt p://code.google.\ncom/, go to “Project Hosting” as before, sign in by clicking the “Sign in” link \nin the upper right-hand corner of the screen, and then click the Sett ings link, \nalso found in the upper right-hand corner of the screen. Click the “My Proﬁ le” \ntab, and you should see your project listed there. Your project’s URL is htt ps://\nmyprojectname.googlecode.com/svn/trunk, where myprojectname is whatever \nname you see listed on the “My Proﬁ le” tab.\nYou should now see the dialog shown in Figure 2.3. The user name \nshould be your Google login name. The password is not your Google login \n2.1. Version Control\nFigure 2.1.  Google Code home page, Project Hosting link.\n\n\n62 \n2. Tools of the Trade\npassword—it is an automatically generated password that can be obtained by \nsigning in to your account on Goggle’s “Project Hosting” page and clicking \non the “Sett ings” link. (See above for details.)  Checking the “Save authenti-\ncation” option on this dialog allows you to use your repository without ever \nhaving to log in again.  Only select this option if you are working on your own \npersonal machine—never on a machine that is shared by many users.\n Once you’ve authenticated your user name, TortoiseSVN will download \n(“check out”) the entire contents of your repository to your local disk. If you \njust set up your repository, this will be … nothing! The folder you created \nwill still be empty.  But now it is connected to your Subversion repository on \nGoogle (or wherever your server is located). If you refresh your Windows \nExplorer window (hit F5), you should now see a litt le green and white check-\nmark on your folder. This icon indicates that the folder is connected to a Sub-\nversion repository via TortoiseSVN and that the local copy of the repository \nis up-to-date.\n2.1.6. File Versions, Updating, and Committing\nAs we’ve seen, one of the key purposes of any source control system like Sub-\nversion is to allow multiple programmers to work on a single soft ware code \nbase by maintaining a central repository or “master” version of all the source \ncode on a server. The server maintains a version history for each ﬁ le, as shown \nin Figure 2.4. This feature is crucial to large-scale multiprogrammer soft ware \ndevelopment. For example, if someone makes a mistake and checks in code \nthat “breaks the build,” you can easily go back in time to undo those changes \n(and check the log to see who the culprit was!).  You can also grab a snapshot \nof the code as it existed at any point in time, allowing you to work with, dem-\nonstrate, or patch  previous versions of the soft ware.\nFigure 2.2.  TortoiseSVN initial check-out dialog.\nFigure 2.3.  TortoiseSVN user authentication dialog.\n\n\n63 \n Each programmer gets a local copy of the code on his or her machine. In \nthe case of TortoiseSVN, you obtain your initial working copy by “ checking \nout” the repository, as described above. Periodically you should update your \nlocal copy to reﬂ ect any changes that may have been made by other program-\nmers.  You do this by right-clicking on a folder and selecting “SVN Update” \nfrom the pop-up menu.\nYou can work on your local copy of the code base without aﬀ ecting the \nother programmers on the team (Figure 2.5). When you are ready to share \nyour changes with everyone else, you commit your changes to the repository \n(also known as  submitt ing or  checking in). You do this by right-clicking on the \nfolder you want to commit and selecting “ SVN Commit…” from the pop-up \n2.1. Version Control\nFigure 2.6.  TortoiseSVN Commit dialog.\nFoo.cpp (version 1)\nFoo.cpp (version 2)\nFoo.cpp (version 3)\nFoo.cpp (version 4)\nBar.cpp (version 1)\nBar.cpp (version 2)\nBar.cpp (version 3)\nFigure 2.4.  File version histories.\nFoo.cpp (version 4)\nFoo.cpp (local edits)\nFigure 2.5.  Editing the local copy of a ver-\nsion-controlled ﬁ le.\n\n\n64 \n2. Tools of the Trade\nmenu. You will get a dialog like the one shown in Figure 2.6, asking you to \nconﬁ rm the changes.\nDuring a commit operation, Subversion generates a  diﬀ  between your lo-\ncal version of each ﬁ le and the latest version of that same ﬁ le in the repository. \nThe term “diﬀ ” means diﬀ erence, and it is typically produced by performing \na line-by-line comparison of the two versions of the ﬁ le. You can double-click \non any ﬁ le in the TortoiseSVN Commit dialog (Figure 2.6) to see the diﬀ s be-\ntween your version and the latest version on the server (i.e., the changes you \nmade). Files that have changed (i.e., any ﬁ les that “have diﬀ s”) are committ ed. \nThis replaces the latest version in the repository with your local version, add-\ning a new entry to the ﬁ le’s version history. Any ﬁ les that have not changed \n(i.e., your local copy is identical to the latest version in the repository) are \nignored by default during a commit. An example commit operation is shown \nin Figure 2.7.\nIf you created any new ﬁ les prior to the commit, they will be listed as \n“non-versioned” in the Commit dialog. You can check the litt le check boxes \nbeside them in order to add them to the repository. Any ﬁ les that you deleted \nlocally will likewise show up as “missing”—if you check their check boxes, \nthey will be deleted from the repository. You can also type a comment in the \nCommit dialog. This comment is added to the repository’s history log, so that \nyou and others on your team will know why these ﬁ les were checked in.\n2.1.7. \nMultiple Check-Out, Branching, and Merging\nSome version control systems require  exclusive check-out. This means that you \nmust ﬁ rst indicate your intentions to modify a ﬁ le by checking it out and lock-\ning it.  The ﬁ le(s) that are checked out to you are writable on your local disk \nand cannot be checked out by anyone else. All other ﬁ les in the repository are \nread-only on your local disk. Once you’re done editing the ﬁ le, you can check \nit in, which releases the lock and commits the changes to the repository for ev-\neryone else to see. The process of exclusively locking ﬁ les for editing ensures \nthat no two people can edit the same ﬁ le simultaneously.\nSubversion, CVS, Perforce, and many other high-quality version control \nsystems also permit  multiple check-out.; i.e., you can be editing a ﬁ le while \nsomeone else is editing that same ﬁ le. Whichever user’s changes are commit-\nted ﬁ rst become the latest version of the ﬁ le in the repository. Any subsequent \ncommits by other users require that programmer to merge his or her changes \nwith the changes made by the programmer(s) who committ ed previously.\nBecause more than one set of changes (diﬀ s) have been made to the same \nﬁ le, the version control system must  merge the changes in order to produce a \nﬁ nal version of the ﬁ le. This is oft en not a big deal, and in fact many conﬂ icts \nFoo.cpp (version 5)\nFoo.cpp (version 4)\nFigure 2.7.  Com-\nmitting local edits \nto the repository.\n\n\n65 \ncan be resolved automatically by the version control system. For example, if \nyou changed function f() and another programmer changed function g(), \nthen your edits would have been to a diﬀ erent range of lines in the ﬁ le than \nthose of the other programmer.  In this case, the merge between your changes \nand his or her changes will usually resolve automatically without any con-\nﬂ icts. However, if you were both making changes to the same function f(), \nthen the second programmer to commit his or her changes will need to do a \n three-way merge (see Figure 2.8).\nFor three-way merges to work, the version control server has to be smart \nenough to keep track of which version of each ﬁ le you currently have on your \nlocal disk. That way, when you merge the ﬁ les, the system will know which ver-\nsion is the base version (the common ancestor, such as version 4 in Figure 2.8).\nSubversion permits multiple check-out, and in fact it doesn’t require you \nto check out ﬁ les explicitly at all. You simply start editing the ﬁ les locally—all \nﬁ les are writable on your local disk at all times. (By the way, this is one reason \nthat Subversion doesn’t scale well to large projects, in my opinion. To deter-\nmine which ﬁ les you have changed, Subversion must search the entire tree of \nsource ﬁ les, which can be slow. Version control systems like Perforce, which \nexplicitly keep track of which ﬁ les you have modiﬁ ed, are usually easier to \nwork with when dealing with large amounts of code. But for small projects, \nSubversion’s approach works just ﬁ ne.)\n2.1. Version Control\nFoo.cpp (joe_b)\nFoo.cpp (suzie_q)\njoe_b and suzie_q both \nstart editing Foo.cpp at \nthe same time\nFoo.cpp (version 4)\nFoo.cpp (joe_b)\nFoo.cpp (version 5)\nsuzie_q commits her \nchanges first\njoe_b must now do a 3-way \nmerge, which involves 2 sets\nof diffs:\nFoo.cpp (version 6)\nFoo.cpp (joe_b)\nFoo.cpp (version 5)\nFoo.cpp (version 4)\nFoo.cpp (version 4)\nversion 4 to his local version\nversion 4 to version 5\nFigure 2.8.  Three-way merge due to local edits by two different users.\n\n\n66 \n2. Tools of the Trade\nWhen you perform a commit operation by right-clicking on any folder \nand selecting “SVN Commit…” from the pop-up menu, you may be prompt-\ned to merge your changes with changes made by someone else. But if no one \nhas changed the ﬁ le since you last updated your local copy, then your changes \nwill be committ ed without any further action on your part. This is a very con-\nvenient feature, but it can also be dangerous. It’s a good idea to always check \nyour commits carefully to be sure you aren’t committ ing any ﬁ les that you \ndidn’t intend to modify. When TortoiseSVN displays its Commit Files dialog, \nyou can double-click on an individual ﬁ le in order to see the diﬀ s you made \nprior to hitt ing the “OK” butt on.\n2.1.8.  Deleting Files\nWhen a ﬁ le is deleted from the repository, it’s not really gone. The ﬁ le still ex-\nists in the repository, but its latest version is simply marked “deleted” so that \nusers will no longer see the ﬁ le in their local directory trees. You can still see \nand access previous versions of a deleted ﬁ le by right-clicking on the folder in \nwhich the ﬁ le was contained and selecting “Show log” from the TortoiseSVN \nmenu.\nYou can undelete a deleted ﬁ le by updating your local directory to the \nversion immediately before the version in which the ﬁ le was marked deleted. \nThen simply commit the ﬁ le again. This replaces the latest deleted version of \nthe ﬁ le with the version just prior to the deletion, eﬀ ectively undeleting the \nﬁ le.\n2.2. Microsoft Visual Studio\nCompiled languages, such as C++, require a  compiler and  linker in order to \ntransform source code into an executable program. There are many com-\npilers/linkers available for C++, but for the Microsoft  Windows platform \nthe most commonly used package is probably  Microsoft  Visual Studio. The \nfully featured Professional Edition of the product can be purchased at any \nstore that sells Windows soft ware. And Visual Studio Express, its lighter-\nweight cousin, is available for free download at htt p://www.microsoft .com/\nexpress/download/. Documentation on Visual Studio is available online at the \nMicrosoft  Developer’s Network (MSDN) site (htt p://msdn.microsoft .com/en-\nus/library/52f3sw5c.aspx). \nVisual Studio is more than just a compiler and linker. It is an  integrated \ndevelopment environment (IDE), including a slick and fully featured text editor \nfor source code and a powerful source-level and machine-level debugger. In \n\n\n67 \nthis book, our primary focus is the Windows platform, so we’ll investigate \nVisual Studio in some depth. Much of what you learn below will be applicable \nto other compilers, linkers, and debuggers, so even if you’re not planning on \never using Visual Studio, I suggest you skim this section for useful tips on us-\ning compilers, linkers, and debuggers in general.\n2.2.1. \nSource Files, Headers, and Translation Units\nA program writt en in C++ is comprised of  source ﬁ les. These typically have a .c, \n.cc, .cxx, or .cpp extension, and they contain the bulk of your program’s source \ncode.  Source ﬁ les are technically known as  translation units, because the com-\npiler translates one source ﬁ le at a time from C++ into machine code.\nA special kind of source ﬁ le, known as a  header ﬁ le, is oft en used in order to \nshare information, such as type declarations and function prototypes, between \ntranslation units. Header ﬁ les are not seen by the compiler. Instead, the C++ \n preprocessor replaces each  #include statement with the contents of the corre-\nsponding header ﬁ le prior to sending the translation unit to the compiler. This \nis a subtle but very important distinction to make. Header ﬁ les exist as distinct \nﬁ les from the point of view of the programmer—but thanks to the preproces-\nsor’s header ﬁ le expansion, all the compiler ever sees are translation units.\n2.2.2. Libraries, Executables, and Dynamic Link Libraries\nWhen a translation unit is compiled, the resulting machine code is placed in \nan  object ﬁ le (ﬁ les with a .obj extension under Windows, or .o under UNIX-\nbased operating systems). The machine code in an object ﬁ le is\nz relocatable, meaning that the memory addresses at which the code re-\nsides have not yet been determined, and\nz unlinked, meaning that any external references to functions and global \ndata that are deﬁ ned outside the translation unit have not yet been re-\nsolved.\nObject ﬁ les can be collected into groups called libraries. A  library is simply \nan archive, much like a Zip or tar ﬁ le, containing zero or more object ﬁ les. Li-\nbraries exist merely as a convenience, permitt ing a large number of object ﬁ les \nto be collected into a single easy-to-use ﬁ le.\nObject ﬁ les and libraries are linked into an  executable by the linker. The \nexecutable ﬁ le contains fully resolved machine code that can be loaded and \nrun by the operating system . The linker’s jobs are\nz to calculate the ﬁ nal relative addresses of all the machine code, as it will \nappear in memory when the program is run, and\n2.2. Microsoft Visual Studio\n\n\n68 \n2. Tools of the Trade\nz to ensure that all external references to functions and global data made \nby each translation unit (object ﬁ le) are properly resolved.\nIt’s important to remember that the machine code in an executable ﬁ le is still \nrelocatable, meaning that the addresses of all instructions and data in the ﬁ le \nare still relative to an arbitrary base address, not absolute. The ﬁ nal absolute \nbase address of the program is not known until the program is actually loaded \ninto memory, just prior to running it.\nA  dynamic link library (DLL) is a special kind of library that acts like a \nhybrid between a regular static library and an executable. The DLL acts like \na library, because it contains functions that can be called by any number of \ndiﬀ erent executables. However, a DLL also acts like an executable, because it \ncan be loaded by the operating system independently, and it contains some \nstart-up and shut-down code that runs much the way the main() function in \na C++ executable does.\nThe executables that use a DLL contain partially linked machine code. Most \nof the function and data references are fully resolved within the ﬁ nal execut-\nable, but any references to external functions or data that exist in a DLL re-\nmain unlinked. When the executable is run, the operating system resolves the \naddresses of all unlinked functions by locating the appropriate DLLs, load-\ning them into memory if they are not already loaded, and patching in the \nnecessary memory addresses. Dynamically linked libraries are a very useful \noperating system feature, because individual DLLs can be updated without \nchanging the executable(s) that use them.\n2.2.3. Projects and Solutions\nNow that we understand the diﬀ erence between libraries, executables, and \ndynamic link libraries (DLLs), let’s see how to create them. In Visual Studio, \na project is a collection of source ﬁ les which, when compiled, produce a library, \nan executable, or a DLL. Projects are stored in project ﬁ les with a .vcproj ex-\ntension. In Visual Studio .NET 2003 (version 7), Visual Studio 2005 (version 8), \nand Visual Studio 2008 (version 9), .vcproj ﬁ les are in XML format, so they are \nreasonably easy for a human to read and even edit by hand if necessary.\nAll versions of Visual Studio since version 7 (Visual Studio 2003) employ \n solution ﬁ les (ﬁ les with a .sln extension) as a means of containing and manag-\ning collections of projects. A solution is a collection of dependent and/or in-\ndependent projects intended to build one or more libraries, executables and/\nor DLLs. In the Visual Studio graphical user interface , the Solution Explorer is \nusually displayed along the right or left  side of the main window, as shown \nin Figure 2.9.\n\n\n69 \nThe Solution Explorer is a tree view. The solution itself is at the root, with \nthe projects as its immediate children. Source ﬁ les and headers are shown as \nchildren of each project. A project can contain any number of user-deﬁ ned \nfolders, nested to any depth. Folders are for organizational purposes only and \nhave nothing to do with the folder structure in which the ﬁ les may reside \non-disk. However it is common practice to mimic the on-disk folder structure \nwhen sett ing up a project’s folders.\n2.2.4. Build Conﬁ gurations\nThe C/C++ preprocessor, compiler, and linker oﬀ er a wide variety of options \nto control how your code will be built. These options are normally speciﬁ ed \non the command line when the compiler is run.  For example, a typical com-\nmand to build a single translation unit with the Microsoft  compiler might look \nlike this:\nC:\\> cl /c foo.cpp /Fo foo.obj /Wall /Od /Zi\nThis tells the compiler/linker to compile but not link (/c) the translation unit \nnamed foo.cpp, output the result to an object ﬁ le named foo.obj (/Fo foo.obj), \nturn on all  warnings (/Wall), turn oﬀ  all  optimizations (/Od), and generate \ndebugging information (/Zi).\nModern compilers provide so many options that it would be impracti-\ncal and error prone to specify all of them every time you build your code. \nThat’s where  build conﬁ gurations come in. A build conﬁ guration is really just \na collection of preprocessor, compiler, and linker options associated with a \nparticular project in your solution. You can deﬁ ne any number of build con-\n2.2. Microsoft Visual Studio\nFigure 2.9.  The VisualStudio Solution Explorer window.\n\n\n70 \n2. Tools of the Trade\nﬁ gurations, name them whatever you want, and conﬁ gure the preprocessor, \ncompiler, and linker options diﬀ erently in each conﬁ guration. By default, the \nsame options are applied to every translation unit in the project, although you \ncan override the global project sett ings on an individual translation unit basis. \n(I recommend avoiding this if at all possible, because it becomes diﬃ  cult to \ntell which .cpp ﬁ les have custom sett ings and which do not.)\nMost projects have at least two build conﬁ gurations, typically called \n“Debug” and “Release.” The release build is for the ﬁ nal shipping soft ware, \nwhile the debug build is for development purposes. A debug build runs more \nslowly than a release build, but it provides the programmer with invaluable \ninformation for developing and debugging the program.\n2.2.4.1. Common Build Options\nThis section lists some of the most common options you’ll want to control via \nbuild conﬁ gurations for a game engine project.\nPreprocessor Settings\nThe C++ preprocessor handles the expansion of #included ﬁ les and the deﬁ -\nnition and substitution of #defined macros. One extremely powerful feature \nof all modern C++ preprocessors is the ability to deﬁ ne preprocessor macros \nvia command-line options (and hence via build conﬁ gurations). Macros de-\nﬁ ned in this way act as though they had been writt en into your source code \nwith a #define statement. For most compilers, the command line option for \nthis is –D or /D, and any number of these directives can be used.\nThis feature allows you to communicate various build options to your \ncode, without having to modify the source code itself.  As a ubiquitous exam-\nple, the symbol _DEBUG is always deﬁ ned for a debug build, while in release \nbuilds the symbol  NDEBUG is deﬁ ned instead. The source code can check these \nﬂ ags and in eﬀ ect “know” whether it is being built in debug or release mode. \nThis is known as conditional compilation. For example:\nvoid f()\n{\n#ifdef _DEBUG\n \nprintf(“Calling function f()\\n”);\n#endif\n \n// ...\n}\nThe compiler is also free to introduce “magic” preprocessor macros into \nyour code, based on its knowledge of the compilation environment and target \nplatform. For example, the macro  __cplusplus is deﬁ ned by most C/C++ \n\n\n71 \ncompilers when compiling a C++ ﬁ le. This allows code to be writt en that auto-\nmatically adapts to being compiled for C or C++.\nAs another example, every compiler identiﬁ es itself to the source code \nvia a “magic” preprocessor macro. When compiling code under the Microsoft  \ncompiler, the macro  _MSC_VER is deﬁ ned; when compiling under the  GNU \ncompiler (gcc), the macro  _GNUC_ is deﬁ ned instead, and so on for the oth-\ner compilers. The target platform on which the code will be run is likewise \nidentiﬁ ed via macros. For example, when building for a 32-bit Windows \nmachine, the symbol  _WIN32 is always deﬁ ned. These key features permit \ncross-platform code to be writt en, because they allow your code to “know” \nwhat compiler is compiling it and on which target platform it is destined to \nbe run.\nCompiler Settings\nOne of the most common compiler options controls whether or not the com-\npiler should include  debugging information with the object ﬁ les it produces. \nThis information is used by debuggers to step through your code, display the \nvalues of variables, and so on. Debugging information makes your executa-\nbles larger on disk and also opens the door for hackers to reverse-engineer \nyour code. So, it is always stripped from the ﬁ nal shipping version of your \nexecutable. However, during development, debugging information is invalu-\nable and should always be included in your builds.\nThe compiler can also be told whether or not to expand inline functions. \nWhen  inline function expansion is turned oﬀ , every inline function appears \nonly once in memory, at a distinct address. This makes the task of tracing \nthrough the code in the debugger much simpler, but obviously comes at the \nexpense of the execution speed improvements normally achieved by inlin-\ning.\nInline function expansion is but one example of generalized code trans-\nformations known as  optimizations. The aggressiveness with which the com-\npiler att empts to optimize your code, and the kinds of optimizations its uses, \ncan be controlled via compiler options. Optimizations have a tendency to re-\norder the statements in your code, and they also cause variables to be stripped \nout of the code altogether, or moved around, and can cause CPU registers to \nbe reused for new purposes later in the same function. Optimized code usu-\nally confuses most debuggers, causing them to “lie” to you in various ways, \nand making it diﬃ  cult or impossible to see what’s really going on. As a result, \nall optimizations are usually turned oﬀ  in a debug build. This permits every \nvariable and every line of code to be scrutinized as it was originally coded. \nBut, of course, such code will run much more slowly than its fully optimized \ncounterpart.\n2.2. Microsoft Visual Studio\n\n\n72 \n2. Tools of the Trade\nLinker Settings\nThe linker also exposes a number of options. You can control what type of \noutput ﬁ le to produce—an executable or a DLL. You can also specify which \nexternal libraries should be linked into your executable, and which directory \npaths to search in order to ﬁ nd them. A common practice is to link with de-\nbug libraries when building a debug executable and with optimized libraries \nwhen building in release mode.\nLinker options also control things like stack size, the preferred base ad-\ndress of your program in memory, what type of machine the code will run on \n(for machine-speciﬁ c optimizations), and a host of other minutia with which \nwe will not concern ourselves here.\n2.2.4.2. Typical Build Conﬁ gurations\nGame projects oft en have more than just two build conﬁ gurations. Here are a \nfew of the common conﬁ gurations I’ve seen used in game development.\nz  Debug. A debug build is a very slow version of your program, with all \noptimizations turned oﬀ , all function inlining disabled, and full debug-\nging information included. This build is used when testing brand new \ncode and also to debug all but the most trivial problems that arise dur-\ning development.\nz Release. A  release build is a faster version of your program, but with \ndebugging information and assertions still turned on. (See Section \n3.3.3.3 for a discussion of assertions.) This allows you to see your game \nrunning at a speed representative of the ﬁ nal product, but still gives you \nsome opportunity to debug problems.\nz Production. A production conﬁ guration is intended for building the ﬁ nal \ngame that you will ship to your customers. It is sometimes called a “Fi-\nnal” build or “Disk” build. Unlike a release build, all debugging informa-\ntion is stripped out of a  production build, all assertions are usually turned \noﬀ , and optimizations are cranked all the way up. A production build is \nvery tricky to debug, but it is the fastest and leanest of all build types.\nz Tools. Some game studios utilize code libraries that are shared between \noﬄ  ine tools and the game itself. In this scenario, it oft en makes sense \nto deﬁ ne a  “Tools” build, which can be used to conditionally compile \nshared code for use by the tools.  The tools build usually deﬁ nes a pre-\nprocessor macro (e.g., TOOLS_BUILD) that informs the code that it is be-\ning built for use in a tool. For example, one of your tools might require \ncertain C++ classes to expose editing functions that are not needed by \nthe game. These functions could be wrapped in an #ifdef TOOLS_\n\n\n73 \nBUILD directive. Since you usually want both debug and release ver-\nsions of your tools, you will probably ﬁ nd yourself creating two tools \nbuilds, named something like “ToolsDebug” and “ToolsRelease.”\nHybrid Builds\nA  hybrid build is a build conﬁ guration in which the majority of the translation \nunits are built in release mode, but a small subset of them is built in debug \nmode. This permits the segment of code that is currently under scrutiny to be \neasily debugged, while the rest of the code continues to run at full speed.\nWith a text-based build system like  make, it is quite easy to set up a hybrid \nbuild which permits users to specify the use of debug mode on a per-transla-\ntion-unit basis. In a nutshell, we deﬁ ne a make variable called something like \n$HYBRID_SOURCES, which lists the names of all translation units (.cpp ﬁ les) \nthat should be compiled in debug mode for our hybrid build. We set up build \nrules for compiling both debug and release versions of every translation unit, \nand arrange for the resulting object ﬁ les (.obj/.o) to be placed into two diﬀ er-\nent folders, one for debug and one for release. The ﬁ nal link rule is set up to \nlink with the debug versions of the object ﬁ les listed in $HYBRID_SOURCES\nand with the release versions of all other object ﬁ les. If we’ve set it up properly, \nmake’s dependency rules will take care of the rest.\nUnfortunately, this is not so easy to do in Visual Studio, because its build \nconﬁ gurations are designed to be applied on a per-project basis, not per trans-\nlation unit. The crux of the problem is that we cannot easily deﬁ ne a list of \nthe translation units that we want to build in debug mode. However, if your \nsource code is already organized into libraries, you can set up a “Hybrid” \nbuild conﬁ guration at the solution level, which picks and chooses between \ndebug and release builds on a per-project (and hence per-library) basis. This \nisn’t as ﬂ exible as having control on a per-translation-unit basis, but it does \nwork reasonably well if your libraries are suﬃ  ciently granular.\nBuild Conﬁ gurations and Testability\nThe more build conﬁ gurations your project supports, the more diﬃ  cult test-\ning becomes. Although the diﬀ erences between the various conﬁ gurations \nmay be slight, there’s a ﬁ nite probability that a critical bug may exist in one \nof them but not in the others. Therefore, each build conﬁ guration must be \ntested equally thoroughly. Most game studios do not formally test their debug \nbuilds, because the debug conﬁ guration is primarily intended for internal use \nduring initial development of a feature and for the debugging of problems \nfound in one of the other conﬁ gurations. However, if your testers spend most \nof their time testing your release conﬁ guration, then you cannot simply make \na production build of your game the night before Gold Master and expect it \n2.2. Microsoft Visual Studio\n\n\n74 \n2. Tools of the Trade\nto have an identical bug proﬁ le to that of the release build. Practically speak-\ning, the test team must test both your release and production builds equally \nthroughout alpha and beta, to ensure that there aren’t any nasty surprises \nlurking in your production build. In terms of  testability, there is a clear advan-\ntage to keeping your build conﬁ gurations to a minimum, and in fact some stu-\ndios have no production build for this reason—they simply ship their release \nbuild once it has been thoroughly tested.\n2.2.4.3.  Project Conﬁ guration Tutorial\nRight-clicking on any project in the Solution Explorer and selecting “Proper-\nties…” from the menu brings up the project’s “Property Pages” dialog. The \ntree view on the left  shows various categories of sett ings. Of these, the three \nwe will use most are\nz Conﬁ guration Properties/General,\nz Conﬁ guration Properties/Debugging,\nz Conﬁ guration Properties/C++,\nz Conﬁ guration Properties/Linker.\nConﬁ gurations Drop-Down Combo Box\nNotice the drop-down combo box labeled “Conﬁ guration:” at the top-left  cor-\nner of the window. All of the properties displayed on these property pages ap-\nply separately to each build conﬁ guration. If you set a property for the debug \nconﬁ guration, this does not necessarily mean that the same sett ing exists for \nthe release conﬁ guration.\nIf you click on the combo box to drop down the list, you’ll ﬁ nd that you \ncan select a single conﬁ guration or multiple conﬁ gurations, including “All \nconﬁ gurations.” As a rule of thumb, try to do most of your build conﬁ guration \nediting with “All conﬁ gurations” selected. That way, you won’t have to make \nthe same edits multiple times, once for each conﬁ guration—and you don’t risk \nsett ing things up incorrectly in one of the conﬁ gurations by accident. How-\never, be aware that some sett ings need to be diﬀ erent between the debug and \nrelease conﬁ gurations. For example, function inlining and code optimization \nsett ings should of course be diﬀ erent between debug and release builds.\nGeneral Tab\nOn the General tab, shown in Figure 2.10, the most useful ﬁ elds are the fol-\nlowing.\nz Output directory. This deﬁ nes where the ﬁ nal product(s) of the build will \ngo—namely the executable, library, or DLL that the compiler/linker ul-\ntimately outputs.\n\n\n75 \nz Intermediate directory. This deﬁ nes where intermediate ﬁ les, primarily \nobject ﬁ les (.obj extension), are placed during a build. Intermediate ﬁ les \nare never shipped with your ﬁ nal program—they are only required \nduring the process of building your executable, library, or DLL. Hence, \nit is a good idea to place intermediate ﬁ les in a diﬀ erent directory than \nthe ﬁ nal products (.exe, .lib or .dll ﬁ les).\nNote that VisualStudio provides a macro facility which may be used \nwhen specifying directories and other sett ings in the “Project Property Pages” \ndialog. A macro is essentially a named variable that contains a global value and \nthat can be referred to in your project conﬁ guration sett ings.\nMacros are invoked by writing the name of the macro enclosed in paren-\ntheses and preﬁ xed with a dollar sign (e.g., $(ConfigurationName)). Some \ncommonly used macros are listed below.\nz $(TargetFileName). The name of the ﬁ nal executable, library, or DLL \nﬁ le being built by this project.\nz $(TargetPath). The full path of the folder containing the ﬁ nal execut-\nable, library, or DLL.\nz $(ConfigurationName). The name of the build conﬁ g, typically “De-\nbug” or “Release.”\n2.2. Microsoft Visual Studio\nFigure 2.10.  Visual Studio project property pages—General page.\n\n\n76 \n2. Tools of the Trade\nz $(OutDir). The value of the “Output Directory” ﬁ eld speciﬁ ed in this \ndialog.\nz $(IntDir). The value of the “Intermediate Directory” ﬁ eld in this \ndialog.\nz $(VCInstallDir). The directory in which Visual Studio’s standard C \nlibrary is currently installed.\nThe beneﬁ t of using macros instead of hard-wiring your conﬁ guration \nsett ings is that a simple change of the global macro’s value will automatically \naﬀ ect all conﬁ guration sett ings in which the macro is used. Also, some macros \nlike $(ConfigurationName) automatically change their values depending \non the build conﬁ guration, so using them can permit you to use identical set-\ntings across all your conﬁ gurations.\nTo see a complete list of all available macros, click in either the “Output \nDirectory” ﬁ eld or the “Intermediate Directory” ﬁ eld on the “General” tab, \nclick the litt le arrow to the right of the text ﬁ eld, select “Edit…” and then click \nthe “Macros” butt on in the dialog that comes up.\nDebugging Tab\nThe “Debugging” tab is where the name and location of the executable to \ndebug is speciﬁ ed. On this page, you can also specify the command-line \nargument(s) that should be passed to the program when it runs.  We’ll discuss \ndebugging your program in more depth below.\nC/C++ Tab\nThe C/C++ tab controls compile-time language sett ings—things that aﬀ ect \nhow your source ﬁ les will be compiled into object ﬁ les (.obj extension). The \nsett ings on this page do not aﬀ ect how your object ﬁ les are linked into a ﬁ nal \nexecutable or DLL.\nYou are encouraged to explore the various subpages of the C/C++ tab to \nsee what kinds of sett ings are available. Some of the most commonly used set-\ntings include the following.\nz General Tab/Include Directories. This ﬁ eld lists the on-disk directories that \nwill be searched when looking for #included header ﬁ les.\nImportant: It is always best to specify these directories using relative \npaths and/or with Visual Studio macros like $(OutDir) or $(IntDir). \nThat way, if you move your build tree to a diﬀ erent location on disk or to \nanother computer with a diﬀ erent root folder, everything will continue \nto work properly.\nz General Tab/Debug Information. This ﬁ eld controls whether or not debug \ninformation is generated. Typically both debug and release conﬁ gura-\n\n\n77 \ntions include debugging information so that you can track down prob-\nlems during development of your game. The ﬁ nal production build will \nhave all the debug info stripped out to prevent hacking.\nz Preprocessor Tab/Preprocessor Deﬁ nitions. This very handy ﬁ eld lists any \nnumber of C/C++ preprocessor symbols that should be deﬁ ned in the \ncode when it is compiled. See Preprocessor Sett ings in Section 2.2.4.1 for a \ndiscussion of preprocessor-deﬁ ned symbols.\nLinker Tab\nThe “Linker” tab lists properties that aﬀ ect how your object code ﬁ les will be \nlinked into an executable or DLL. Again, you are encouraged to explore the \nvarious subpages. Some commonly used sett ings follow.\nz General Tab/Output File. This sett ing lists the name and location of the \nﬁ nal product of the build, usually an executable or DLL.\nz General Tab/Additional Library Directories. Much like the C/C++ Include \nDirectories ﬁ eld, this ﬁ eld lists zero or more directories that will be \nsearched when looking for libraries and object ﬁ les to link into the ﬁ nal \nexecutable.\nz Input Tab/Additional Dependencies. This ﬁ eld lists external libraries that you \nwant linked into your executable or DLL. For example, the Ogre libraries \nwould be listed here if you are building an Ogre-enabled application.\nNote that Visual Studio employs various “magic spells” to specify librar-\nies that should be linked into an executable. For example, a special #pragma\ninstruction in your source code can be used to instruct the linker to automati-\ncally link with a particular library.  For this reason, you may not see all of the \nlibraries you’re actually linking to in the “Additional Dependencies” ﬁ eld. (In \nfact, that’s why they are called additional dependencies.) You may have noticed, \nfor example, that Direct X applications do not list all of the DirectX libraries \nmanually in their “Additional Dependencies” ﬁ eld. Now you know why.\n2.2.4.4. Creating New .vcproj Files\nWith so many preprocessor, compiler, and linker options, all of which must \nbe set properly, creating a new project may seem like an impossibly daunting \ntask. I usually take one of the following two approaches when creating a new \nVisual Studio project.\nUse a Wizard\nVisual Studio provides a wide variety of wizards to create new projects of \nvarious kinds. If you can ﬁ nd a wizard that does what you want, this is the \neasiest way to create a new project.\n2.2. Microsoft Visual Studio\n\n\n78 \n2. Tools of the Trade\nCopy an Existing Project\nIf I am creating a project that is similar to an existing project that I know al-\nready works, I’ll oft en just copy that .vcproj ﬁ le and then modify it as neces-\nsary. In Visual Studio 2005, this is very easy. You simply copy the .vcproj ﬁ le \non disk, then add the newly copied project to your solution by right-clicking \nthe solution in the Solution Explorer and selecting “Add…” and “Existing \nproject…” from the pop-up menus.\nOne caveat when copying project ﬁ les is that the name of the project is \nstored inside the .vcproj ﬁ le itself. So when you load up the new project for the \nﬁ rst time in Visual Studio 2005, it will still have the original name. To rectify \nthis, you can simply select the project in the Solution Explorer window, and \nhit F2 to rename it appropriately.\nAnother problem arises when the name of the executable, library, or DLL \nthat the project creates is speciﬁ ed explicitly in the .vcproj ﬁ le. For example, \nthe executable might be speciﬁ ed as “C:\\MyGame\\bin\\MyGame.exe” or \n“$(OutDir)\\MyGame.exe.” In this case, you’ll need to open the .vcproj ﬁ le \nand do a global search-and-replace of the executable, library, or DLL name \nand/or its directory path. This is not too diﬃ  cult. Project ﬁ les are XML, so you \ncan rename your copied .vcproj ﬁ le to have an “.xml” extension and then open \nit in Visual Studio (or any other XML or raw text editor). One elegant solution \nto this problem is to use Visual Studio’s macro system when specifying all out-\nput ﬁ les in your project. For example, if you specify the output executable as \n“$(OutDir)\\$(ProjectName).exe”, then the project’s name will automati-\ncally be reﬂ ected in the name of the output executable ﬁ le.\nI should mention that using a text editor to manipulate .vcproj ﬁ les is not \nalways to be avoided.  In fact, the practice is quite common, at least in my ex-\nperience. For example, let’s say you decided to move the folder containing all \nof your graphics header ﬁ les to a new path on disk. Rather than manually open \neach project in turn, open the Project Property Pages window, navigate to the \nC/C++ tab, and ﬁ nally update the include path manually, it’s much easier and \nless error-prone to edit the ﬁ les as XML text and do a search-and-replace. You \ncan even do a “Replace in ﬁ les” operation in Visual Studio for mass edits.\n2.2.5. Debugging Your Code\nOne of the most important skills any programmer can learn is how to eﬀ ec-\ntively debug code. This section provides some useful debugging tips and \ntricks. Some are applicable to any debugger and some are speciﬁ c to Microsoft  \nVisual Studio. However, you can usually ﬁ nd an equivalent to Visual Studio’s \ndebugging features in other debuggers, so this section should prove useful \neven if you don’t use Visual Studio to debug your code.\n\n\n79 \n2.2.5.1. The Start-Up Project\nA Visual Studio solution can contain more than one project. Some of these \nprojects build executables, while others build libraries or DLLs. It’s possible \nto have more than one project that builds an executable in a single solution. \nHowever, you cannot debug more than one program at a time. For this reason, \nVisual Studio provides a sett ing known as the “Start-Up Project.”  This is the \nproject that is considered “current” for the purposes of the debugger.\nThe start-up project is highlighted in bold in the Solution Explorer. \nHitt ing F5 to run your program in the debugger will run the .exe built by the \nstart-up project (if the start-up project builds an executable).\n2.2.5.2. Break Points\nBreak points are the bread and butt er of code debugging. A break point in-\nstructs the program to stop at a particular line in your source code so that you \ncan inspect what’s going on.\nIn Visual Studio, select a line and hit F9 to toggle a break point. When you \nrun your program and the line of code containing the break point is about to \nbe executed, the debugger will stop the program. We say that the break point \nhas been “hit.” A litt le arrow will show you which line of code the CPU’s pro-\ngram counter is currently on. This is shown in Figure 2.11.\n2.2. Microsoft Visual Studio\nFigure 2.11.  Setting a break point in Visual Studio.\n2.2.5.3. Stepping through Your Code\nOnce a break point has been hit, you can  single-step your code by hitt ing the \nF10 key. The yellow program-counter arrow moves to show you the lines as \nthey execute. Hitt ing F11 steps into a function call (i.e., the next line of code \nyou’ll see is the ﬁ rst line of the called function), while F10 steps over that func-\n\n\n80 \n2. Tools of the Trade\ntion call (i.e., the debugger calls the function at full speed and then breaks \nagain on the line right aft er the call).\n2.2.5.4. The  Call Stack\nThe call stack window, shown in Figure 2.12, shows you the stack of functions \nthat have been called at any given moment during the execution of your code. \nTo display the call stack (if it is not already visible), go to the “Debug” menu \non the main menu bar, select “Windows,” and then “Call Stack.”\nOnce a break point has been hit (or the program is manually paused), you \ncan move up and down the call stack by double-clicking on entries in the “Call \nStack” window. This is very useful for inspecting the chain of function calls \nthat were made between main() and the current line of code. For example, \nyou might trace back to the root cause of a bug in a parent function which has \nmanifested itself in a deeply nested child function.\nFigure 2.12.  The call stack window.\n2.2.5.5. The  Watch Window\nAs you step through your code and move up and down the call stack, you will \nwant to be able to inspect the values of the variables in your program. This \nis what watch windows are for. To open a watch window, go to the “Debug” \nmenu, select “Windows…,” then select “Watch…,” and ﬁ nally select one of \n“Watch 1” through “Watch 4.” (Visual Studio allows you to open up to four \nwatch windows simultaneously.) Once a watch window is open, you can type \nthe names of variables into the window or drag expressions in directly from \nyour source code.\nAs you can see in Figure 2.13, variables with simple data types are shown \nwith their values listed immediately to the right of their names. Complex \ndata types are shown as litt le tree views that can be easily expanded to “drill \n",
      "page_number": 83,
      "chapter_number": 5,
      "summary": "This chapter covers segment 5 (pages 83-102). Key topics include versions, project, and code. It extends the functionality \nof the Microsoft  Windows Explorer via a convenient right-click menu and over-\nlay icons to show you the status of your version-controlled ﬁ les and folders.",
      "keywords": [
        "Visual Studio",
        "Microsoft Visual Studio",
        "Build Conﬁ gurations",
        "Visual Studio project",
        "Build Conﬁ",
        "code",
        "conﬁ guration",
        "Studio",
        "Visual",
        "build",
        "version",
        "les",
        "conﬁ",
        "project",
        "Microsoft Visual"
      ],
      "concepts": [
        "versions",
        "project",
        "code",
        "coded",
        "build",
        "debugging",
        "debug",
        "compiled",
        "compile",
        "libraries"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents In Action",
          "chapter": 24,
          "title": "Segment 24 (pages 208-215)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "AntiPatterns",
          "chapter": 9,
          "title": "Segment 9 (pages 74-81)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 38,
          "title": "Segment 38 (pages 345-352)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "Microservice APIs Using Python Flask FastAPI",
          "chapter": 4,
          "title": "Segment 4 (pages 25-34)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 103-120)",
      "start_page": 103,
      "end_page": 120,
      "detection_method": "synthetic",
      "content": "81 \ndown” into virtually any nested structure. The base class of a class is always \nshown as the ﬁ rst child of an instance of a derived class. This allows you to \ninspect not only the class’ data members, but also the data members of its base \nclass(es).\nYou can type virtually any valid C/C++ expression into the watch window, \nand Visual Studio will evaluate that expression and att empt to display the \nresulting value. For example, you could type “5+3” and Visual Studio will \ndisplay “8.” You can cast variables from one type to another by using C or C++ \ncasting syntax. For example, typing “(float)myIntegerVariable * 0.5f” \nin the watch window will display the value of myIntegerVariable divided \nby two, as a ﬂ oating-point value.\nYou can even call functions in your program from within the watch window. \nVisual Studio re-evaluates the expressions typed into the watch window(s) \nautomatically, so if you invoke a function in the watch window, it will be \ncalled every time you hit a break point or single-step your code. This allows \nyou to leverage the functionality of your program in order to save yourself \nwork when trying to interpret the data that you’re inspecting in the debug-\nger. For example, let’s say that your game engine provides a function called \nquatToAngleDeg() which converts a quaternion to an angle of rotation in \ndegrees. You can call this function in the watch window in order to easily in-\nspect the rotation angle of any quaternion from within the debugger.\nYou can also use various suﬃ  xes on the expressions in the watch window \nin order to change the way Visual Studio displays the data, as shown in Fig-\nure 2.14.\nz The “,d” suﬃ  x forces values to be displayed in decimal notation.\nz The “,x” suﬃ  x forces values to be displayed in hexadecimal notation.\n2.2. Microsoft Visual Studio\nFigure 2.13.  Visual Studio’s watch window.\n\n\n82 \n2. Tools of the Trade\nz The “,n” suﬃ  x (where n is any positive integer) forces Visual Studio to \ntreat the value as an array with n elements. This allows you to expand \narray data that is referenced through a pointer.\nBe careful when expanding very large data structures in the watch window, be-\ncause it can sometimes slow the debugger down to the point of being unusable.\n2.2.5.6.  Data Break Points\nRegular break points trip when the CPU’s program counter hits a particular \nmachine instruction or line of code. However, another incredibly useful fea-\nture of modern debuggers is the ability to set a break point that trips when-\never a speciﬁ c memory address is writt en to (i.e., changed). These are called \ndata break points, because they are triggered by changes to data, or sometimes \nhardware break points, because they are implemented via a special feature of the \nCPU’s hardware—namely the ability to raise an interrupt when a predeﬁ ned \nmemory address is writt en to.\nHere’s how data break points are typically used. Let’s say you are tracking \ndown a bug that manifests itself as a zero (0.0f) value mysteriously appear-\ning inside a member variable of a particular object called m_angle that should \nalways contain a nonzero angle. You have no idea which function might be \nwriting that zero into your variable. However, you do know the address of the \nvariable. (You can just type “&object.m_angle” into the watch window to \nﬁ nd its address.) To track down the culprit, you can set a data break point on \nthe address of object.m_angle, and then simply let the program run. When \nthe value changes, the debugger will stop automatically. You can then inspect \nthe call stack to catch the oﬀ ending function red-handed.\nTo set a data break point in Visual Studio, take the following steps.\nz Bring up the “Breakpoints” window found on the “Debug” menu under \n“Windows” and then “Breakpoints” (Figure 2.15).\nz Select the “New” drop-down butt on in the upper-left  corner of the win-\ndow.\nFigure 2.14.  Comma sufﬁ xes in the Visual Studio watch window.\n\n\n83 \nz Select “New Data Breakpoint.”\nz Type in the raw address or an address-valued expression, such as \n“&myVariable” (Figure 2.16).\nThe “Byte count” ﬁ eld should almost always contain the value 4. This is \nbecause 32-bit Pentium CPUs can really only inspect 4-byte (32-bit) values na-\ntively. Specifying any other data size requires the debugger to do some trickery \nwhich tends to slow your program’s execution to a crawl (if it works at all).\n2.2.5.7. Conditional Break Points\nYou’ll also notice in the “Break Points” window that you can set conditions \nand hit counts on any type break point—data break points or regular line-of-\ncode break points.\nA  conditional break point causes the debugger to evaluate the C/C++ expres-\nsion you provide every time the break point is hit. If the expression is true, the \ndebugger stops your program and gives you a chance to see what’s going on. \nIf the expression is false, the break point is ignored and the program contin-\nues. This is very useful for sett ing break points that only trip when a function \nis called on a particular instance of a class. For example, let’s say you have \na game level with 20 tanks on-screen, and you want to stop your program \nFigure 2.16.  Deﬁ ning a data break point.\nFigure 2.15.  The Visual Studio break points window.\n2.2. Microsoft Visual Studio\n\n\n84 \n2. Tools of the Trade\nwhen the third tank, whose memory address you know to be 0x12345678, \nis running. By sett ing the break point’s condition express to something like \n“(unsigned)this == 0x12345678”, you can restrict the break point only to \nthe class instance whose memory address (this pointer) is 0x12345678.\nSpecifying a  hit count for a break point causes the debugger to decrement \na counter every time the break point is hit, and only actually stop the program \nwhen that counter reaches zero. This is really useful for situations where your \nbreak point is inside a loop, and you need to inspect what’s happening during \nthe 376th iteration of the loop (e.g., the 376th element in an array). You can’t \nvery well sit there and hit the F5 key 375 times! But you can let the hit count \nfeature of Visual Studio do it for you.\nOne note of caution: Conditional break points cause the debugger to eval-\nuate the conditional expression every time the break point is hit, so they can \nbog down the performance of the debugger and your game.\n2.2.5.8.  Debugging Optimized Builds\nI mentioned above that it can be very tricky to debug problems using a release \nbuild, due primarily to the way the compiler optimizes the code. Ideally, every \nprogrammer would prefer to do all of his or her debugging in a debug build. \nHowever, this is oft en not possible. Sometimes a bug occurs so rarely that \nyou’ll jump at any chance to debug the problem, even if it occurs in a release \nbuild on someone else’s machine. Other bugs only occur in your release build, \nbut magically disappear whenever you run the debug build. These dreaded \nrelease-only bugs are sometimes caused by uninitialized variables, because vari-\nables and dynamically allocated memory blocks are oft en set to zero in debug \nmode, but are left  containing garbage in a release build. Other common causes \nof release-only bugs include code that has been accidentally omitt ed from the \nrelease build (e.g., when important code is erroneously placed inside an asser-\ntion statement), data structures whose size or data member packing changes \nbetween debug and release builds, bugs that are only triggered by inlining or \ncompiler-introduced optimizations, and (in rare cases) bugs in the compiler’s \noptimizer itself, causing it to emit incorrect code in a fully optimized build.\nClearly, it behooves every programmer to be capable of debugging prob-\nlems in a release build, unpleasant as it may seem. The best ways to reduce the \npain of debugging optimized code is to practice doing it and to expand your \nskill set in this area whenever you have the opportunity. Here are a few tips.\nz Learn to read and step through  disassembly in the debugger. In a release build, \nthe debugger oft en has trouble keeping track of which line of source \ncode is currently being executed. Thanks to instruction reordering, \nyou’ll oft en see the program counter jump around erratically within the \n\n\n85 \nfunction when viewed in source code mode. However, things become \nsane again when you work with the code in disassembly mode (i.e., step \nthrough the assembly language instructions individually). Every C/C++ \nprogrammer should be at least a litt le bit familiar with the architecture \nand assembly language of their target CPU(s). That way, even if the de-\nbugger is confused, you won’t be.\nz Use registers to deduce variables’ values or addresses. The debugger will \nsometimes be unable to display the value of a variable or the contents of \nan object in a release build. However, if the program counter is not too \nfar away from the initial use of the variable, there’s a good chance its ad-\ndress or value is still stored in one of the CPU’s registers. If you can trace \nback through the disassembly to where the variable is ﬁ rst loaded into \na register, you can oft en discover its value or its address by inspecting \nthat register. Use the register window, or type the name of the register \ninto a watch window, to see its contents.\nz Inspect variables and object contents by address. Given the address of a vari-\nable or data structure, you can usually see its contents by casting the \naddress to the appropriate type in a watch window. For example, if we \nknow that an instance of the Foo class resides at address 0x1378A0C0, we \ncan type “(Foo*)0x1378A0C0” in a watch window, and the debugger \nwill interpret that memory address as if it were a pointer to a Foo object.\nz Leverage static and global variables. Even in an optimized build, the de-\nbugger can usually inspect global and static variables. If you cannot de-\nduce the address of a variable or object, keep your eye open for a static \nor global that might contain its address, either directly or indirectly. For \nexample, if we want to ﬁ nd the address of an internal object within the \nphysics system, we might discover that it is in fact stored in a member \nvariable of the global PhysicsWorld object.\nz Modify the code. If you can reproduce a release-only bug relatively eas-\nily, consider modifying the source code to help you debug the problem. \nAdd print statements so you can see what’s going on. Introduce a global \nvariable to make it easier to inspect a problematic variable or object in \nthe debugger. Add code to detect a problem condition or to isolate a \nparticular instance of a class.\n2.3. Proﬁ ling Tools\nGames are typically high-performance real-time programs. As such, game en-\ngine programmers are always looking for ways to speed up their code. There \n2.3. Proﬁ ling Tools\n\n\n86 \n2. Tools of the Trade\nis a well-known, albeit rather unscientiﬁ c, rule of thumb known as the  Pareto \nprinciple (see htt p://en.wikipedia.org/wiki/Pareto_principle). It is also known \nas the  80-20 rule, because it states that in many situations, 80% of the eﬀ ects \nof some event come from only 20% of the possible causes. In computer sci-\nence, we oft en use a variant of this principle known as the 90-10 rule, which \nstates that 90% of the wall clock time spent running any piece of soft ware is \naccounted for by only 10% of the code. In other words, if you optimize 10% of \nyour code, you can potentially realize 90% of all the gains in execution speed \nyou’ll ever realize.\nSo, how do you know which 10% of your code to optimize? For that, you \nneed a  proﬁ ler.  A proﬁ ler is a tool that measures the execution time of your \ncode. It can tell you how much time is spent in each function. You can then di-\nrect your optimizations toward only those functions that account for the lion’s \nshare of the execution time.\nSome proﬁ lers also tell you how many times each function is called. This \nis an important dimension to understand. A function can eat up time for two \nreasons: (a) it takes a long time to execute on its own, or (b) it is called fre-\nquently. For example, a function that runs an A* algorithm to compute the \noptimal paths through the game world might only be called a few times each \nframe, but the function itself may take a signiﬁ cant amount of time to run.  On \nthe other hand, a function that computes the dot product may only take a few \ncycles to execute, but if you call it hundreds of thousands of times per frame, \nit might drag down your game’s frame rate.\nEven more information can be obtained, if you use the right proﬁ ler. Some \nproﬁ lers report the call graph, meaning that for any given function, you can \nsee which functions called it (these are known as parent functions) and which \nfunctions it called (these are known as child functions or descendants). You can \neven see what percentage of the function’s time was spent calling each of its \ndescendants and the percentage of the overall running time accounted for by \neach individual function.\nProﬁ lers fall into two broad categories.\n1.  Statistical proﬁ lers. This kind of proﬁ ler is designed to be unobtrusive, \nmeaning that the target code runs at almost the same speed, wheth-\ner or not proﬁ ling is enabled. These proﬁ lers work by sampling the \nCPU’s program counter register periodically and noting which func-\ntion is currently running. The number of samples taken within each \nfunction yields an approximate percentage of the total running time \nthat is eaten up by that function. Intel’s VTune is the gold standard in \nstatistical proﬁ lers for Windows machines employing Intel Pentium \nprocessors, and it is now also available for Linux.  See htt p://www.\n\n\n87 \nintel.com/cd/soft ware/products/ asmo-na /eng /vtune /239144.htm \nfor \ndetails.\n2.  Instrumenting proﬁ lers. This kind of proﬁ ler is aimed at providing the \nmost accurate and comprehensive timing data possible, but at the ex-\npense of real-time execution of the target program—when proﬁ ling is \nturned on, the target program usually slows to a crawl. These proﬁ lers \nwork by preprocessing your executable and inserting special prologue \nand epilogue code into every function. The prologue and epilogue code \ncalls into a proﬁ ling library, which in turn inspects the program’s call \nstack and records all sorts of details, including which parent function \ncalled the function in question and how many times that parent has \ncalled the child. This kind of proﬁ ler can even be set up to monitor every \nline of code in your source program, allowing it to report how long each \nline is taking to execute. The results are stunningly accurate and com-\nprehensive, but turning on proﬁ ling can make a game virtually unplay-\nable. IBM’s Rational Quantify, available as part of the Rational Purify \nPlus tool suite, is an excellent instrumenting proﬁ ler.  See htt p://www.\nibm.com/developerworks/rational/library/957.html for an introduction \nto proﬁ ling with Quantify.\nMicrosoft  has also published a proﬁ ler that is a hybrid between the two \napproaches.  It is called LOP, which stands for low-overhead proﬁ ler. It uses \na statistical approach, sampling the state of the processor periodically, which \nmeans it has a low impact on the speed of the program’s execution. However, \nwith each sample it analyzes the call stack, thereby determining the chain of \nparent functions that resulted in each sample. This allows LOP to provide \ninformation normally not available with a statistical proﬁ ler, such as the dis-\ntribution of calls across parent functions.\n2.3.1. \nList of Proﬁ lers\nThere are a great many proﬁ ling tools available. See htt p://en.wikipedia.org/\nwiki/List_of_performance_analysis_tool for a reasonably comprehensive list.\n2.4. Memory Leak and Corruption Detection\nTwo other problems that plague C and C++ programmers are  memory leaks \nand  memory corruption. A memory leak occurs when memory is allocated \nbut never freed. This wastes memory and eventually leads to a potentially \nfatal out-of-memory condition. Memory corruption occurs when the program \ninadvertently writes data to the wrong memory location, overwriting the im-\n2.4. Memory Leak and Corruption Detection\n\n\n88 \n2. Tools of the Trade\nportant data that was there—while simultaneously failing to update the mem-\nory location where that data should have been writt en. Blame for both of these \nproblems falls squarely on the language feature known as the pointer.\nA pointer is a powerful tool. It can be an agent of good when used prop-\nerly—but it can also be all-too-easily transformed into an agent of evil. If a \npointer points to memory that has been freed, or if it is accidentally assigned \na nonzero integer or ﬂ oating-point value, it becomes a dangerous tool for cor-\nrupting memory, because data writt en through it can quite literally end up \nanywhere. Likewise, when pointers are used to keep track of allocated mem-\nory, it is all too easy to forget to free the memory when it is no longer needed. \nThis leads to memory leaks.\nClearly good coding practices are one approach to avoiding pointer-re-\nlated memory problems. And it is certainly possible to write solid code that \nessentially never corrupts or leaks memory. Nonetheless, having a tool to help \nyou detect potential memory corruption and leak problems certainly can’t \nhurt. Thankfully, many such tools exist.\nMy personal favorite is IBM’s  Rational Purify, which comes as part of the \nPurify Plus tool kit. Purify instruments your code prior to running it, in order \nto hook into all pointer dereferences and all memory allocations and dealloca-\ntions made by your code. When you run your code under Purify, you get a \nlive report of the problems—real and potential—encountered by your code. \nAnd when the program exits, you get a detailed memory leak report. Each \nproblem is linked directly to the source code that caused the problem, making \ntracking down and ﬁ xing these kinds of problems relatively easy. You can ﬁ nd \nmore information on Purify at htt p://www-306.ibm.com/soft ware/awdtools\n/purify.\nAnother popular tool is Bounds Checker by CompuWare. It is similar \nto Purify in purpose and functionality. You can ﬁ nd more information on \nBounds Checker at htt p://www.compuware.com/products/devpartner/visualc\n.htm.\n2.5. Other Tools\nThere are a number of other commonly used tools in a game programmer’s \ntoolkit. We won’t cover them in any depth here, but the following list will \nmake you aware of their existence and point you in the right direction if you \nwant to learn more.\nz Diﬀ erence tools. A diﬀ erence tool, or diﬀ  tool, is a program that com-\npares two versions of a text ﬁ le and determines what has changed be-\n\n\n89 \ntween them. (See htt p://en.wikipedia.org/wiki/Diﬀ  for a discussion of \ndiﬀ  tools.) Diﬀ s are usually calculated on a line-by-line basis, although \nmodern diﬀ  tools can also show you a range of characters on a changed \nline that have been modiﬁ ed. Most version control systems come with \na diﬀ  tool. Some programmers like a particular diﬀ  tool and conﬁ gure \ntheir version control soft ware to use the tool of their choice. Popular \ntools include ExamDiﬀ  (htt p://www.prestosoft .com/edp_examdiﬀ .asp), \nAraxisMerge (htt p://www.araxis.com), WinDiﬀ  (available in the Op-\ntions Packs for most Windows versions and available from many inde-\npendent websites as well), and the GNU diﬀ  tools package (htt p://www.\ngnu.org/soft ware/diﬀ utils/diﬀ utils.html).\nz  Three-way merge tools. When two people edit the same ﬁ le, two inde-\npendent sets of diﬀ s are generated. A tool that can merge two sets of \ndiﬀ s into a ﬁ nal version of the ﬁ le that contains both person’s changes \nis called a three-way merge tool. The name “three-way” refers to the \nfact that three versions of the ﬁ le are involved: the original, user A’s \nversion, and user B’s version. (See htt p://en.wikipedia.org/wiki/3-way_\nmerge#Three-way_merge for a discussion of two-way and three-way \nmerge technologies.) Many merge tools come with an associated diﬀ  \ntool. Some popular merge tools include AraxisMerge (htt p://www.arax-\nis.com) and WinMerge (htt p://winmerge.org). Perforce also comes with \nan excellent three-way merge tool (htt p://www.perforce.com/perforce/\nproducts/merge.html).\nz Hex editors. A  hex editor is a program used for inspecting and modify-\ning the contents of binary ﬁ les. The data are usually displayed as in-\ntegers in hexadecimal format, hence the name. Most good hex editors \ncan display data as integers from one byte to 16 bytes each, in 32- and \n64-bit ﬂ oating point format and as ASCII text. Hex editors are particu-\nlarly useful when tracking down problems with binary ﬁ le formats or \nwhen reverse-engineering an unknown binary format—both of which \nare relatively common endeavors in game engine development circles. \nThere are quite literally a million diﬀ erent hex editors out there; I’ve \nhad good luck with HexEdit by Expert Commercial Soft ware (htt p://\nwww.expertcomsoft .com/index.html), but your mileage may vary.\nAs a game engine programmer you will undoubtedly come across other \ntools that make your life easier, but I hope this chapter has covered the main \ntools you’ll use on a day-to-day basis.\n2.5. Other Tools\n\n\n91\n3\nFundamentals of Software \nEngineering for Games\nI\nn this chapter, we’ll brieﬂ y review the basic concepts of object-oriented pro-\ngramming and then delve into some advanced topics which should prove \ninvaluable in any soft ware engineering endeavor (and especially when creat-\ning games). As with Chapter 2, I hope you will not to skip this chapter en-\ntirely; it’s important that we all embark on our journey with the same set of \ntools and supplies.\n3.1. \nC++ Review and Best Practices\n3.1.1. \nBrief Review of Object-Oriented Programming\nMuch of what we’ll discuss in this book assumes you have a solid understand-\ning of the principles of object-oriented design. If you’re a bit rusty, the follow-\ning section should serve as a pleasant and quick review. If you have no idea \nwhat I’m talking about in this section, I recommend you pick up a book or two \non  object-oriented programming (e.g., [5]) and C++ in particular (e.g., [39] and \n[31]) before continuing.\n3.1.1.1. \nClasses and Objects\nA  class is a collection of att ributes (data) and behaviors (code) which together \nform a useful, meaningful whole. A class is a speciﬁ cation describing how in-\n\n\n92 \n3. Fundamentals of Software Engineering for Games\ndividual instances of the class, known as objects, should be constructed. For \nexample, your pet Rover is an instance of the class “dog.” Thus there is a one-\nto-many relationship between a class  and its instances.\n3.1.1.2. \n Encapsulation\nEncapsulation means that an object presents only a limited interface to the out-\nside world; the object’s internal state and implementation details are kept hid-\nden. Encapsulation simpliﬁ es life for the user of the class, because he or she \nneed only understand the class’ limited interface, not the potentially intricate \ndetails of its implementation. It also allows the programmer who wrote the \nclass to ensure that its instances are always in a logically consistent state.\n3.1.1.3. \n Inheritance\nInheritance allows new classes to be deﬁ ned as extensions to pre-existing class-\nes. The new class modiﬁ es or extends the data, interface, and/or behavior of \nthe existing class. If class Child extends class Parent, we say that Child in-\nherits from or is derived from Parent. In this relationship, the class Parent is \nknown as the base class or superclass, and the class Child is the derived class \nor subclass. Clearly, inheritance leads to hierarchical (tree-structured) relation-\nships between classes.\nInheritance creates an “is-a” relationship between classes. For example, \na circle is a type of shape. So if we were writing a 2D drawing application, \nit would probably make sense to derive our Circle class from a base class \ncalled Shape.\nWe can draw diagrams of class hierarchies using the conventions deﬁ ned \nby the  Uniﬁ ed Modeling Language (UML). In this notation, a rectangle repre-\nsents a class, and an arrow with a hollow triangular head represents inheritance. \nThe inheritance arrow points from child class to parent. See Figure 3.1 for an ex-\nample of a simple class hierarchy represented as a UML  static class diagram.\nShape\nCircle\nRectangle\nTriangle\nFigure 3.1.  UML static class diagram depicting a simple class hierarchy.\n Multiple In heritance\nSome languages support multiple inheritance (MI), meaning that a class can \nhave more than one parent class. In theory MI can be quite elegant, but in \n\n\n93 \n3.1. C++ Review and Best Practices\npractice this kind of design usually gives rise to a lot of confusion and techni-\ncal diﬃ  culties (see htt p://en.wikipedia.org/wiki/Multiple_inheritance). This is \nbecause multiple inheritance transforms a simple tree of classes into a poten-\ntially complex graph. A class graph can have all sorts of problems that never \nplague a simple tree—for example, the  deadly diamond (htt p://en.wikipedia.\norg/wiki/Diamond_problem), in which a derived class ends up containing two \ncopies of a grandparent base class (see Figure 3.2). In C++,  virtual inheritance al-\nlows one to avoid this doubling of the grandparent’s data.\nMost C++ soft ware developers avoid multiple inheritance completely or \nonly permit it in a limited form. A common rule of thumb is to allow only \nsimple, parentless classes to be multiply inherited into an otherwise strictly \nsingle-inheritance hierarchy. Such classes are sometimes called mix-in classes \nClassA\nClassB\nClassC\nClassD\nClassA\nClassA\nClassB\nClassB’s \nmemory layout:\nClassA’s \nmemory layout:\nClassA\nClassC\nClassC’s \nmemory layout:\nClassA\nClassB\nClassD’s \nmemory layout:\nClassA\nClassC\nClassD\nFigure 3.2.  “Deadly diamond” in a multiple inheritance hierarchy.\n+Draw()\nShape\n+Draw()\nCircle\n+Draw()\nRectangle\n+Draw()\nTriangle\n+Animate()\nAnimator\nAnimator is a hypothetical mix-in\nclass that adds animation\nfunctionality to whatever class it\nis inherited by.\nFigure 3.3.  Example of a mix-in class.\n\n\n94 \n3. Fundamentals of Software Engineering for Games\nbecause they can be used to introduce new functionality at arbitrary points in a \nclass tree. See Figure 3.3 for a somewhat contrived example of a  mix-in class.\n3.1.1.4. \n Polymorphism\nPolymorphism is a language feature that allows a collection of objects of diﬀ er-\nent types to be manipulated through a single common interface. The common \ninterface makes a heterogeneous collection of objects appear to be homoge-\nneous, from the point of view of the code using the interface.\nFor example, a 2D painting program might be given a list of various \nshapes to draw on-screen. One way to draw this heterogeneous collection of \nshapes is to use a switch statement to perform diﬀ erent drawing commands \nfor each distinct type of shape.\nvoid drawShapes(std::list<Shape*> shapes)\n{\n \nstd::list<Shape*>::iterator pShape = shapes.begin();\n \nstd::list<Shape*>::iterator pEnd = shapes.end();\n \nfor ( ; pShape != pEnd; ++pShape)\n {\nswitch (pShape->mType)\n  {\n  case \nCIRCLE:\n \n \n \n// draw shape as a circle\n   break;\n  case \nRECTANGLE:\n \n \n \n// draw shape as a rectangle\n   break;\n  case \nTRIANGLE:\n \n \n \n// draw shape as a triangle\n   break;\n  //...\n  }\n }\n}\nThe problem with this approach is that the drawShapes() function needs \nto “know” about all of the kinds of shapes that can be drawn. This is ﬁ ne in a \nsimple example, but as our code grows in size and complexity, it can become \ndiﬃ  cult to add new types of shapes to the system. Whenever a new shape \ntype is added, one must ﬁ nd every place in the code base where knowledge \nof the set of shape types is embedded—like this switch statement—and add a \ncase to handle the new type.\nThe solution is to insulate the majority of our code from any knowledge of \nthe types of objects with which it might be dealing. To accomplish this, we can \n\n\n95 \ndeﬁ ne classes for each of the types of shapes we wish to support. All of these \nclasses would inherit from the common base class Shape. A virtual function—\nthe C++ language’s primary polymorphism mechanism—would be deﬁ ned \ncalled Draw(), and each distinct shape class would implement this function \nin a diﬀ erent way. Without “knowing” what speciﬁ c types of shapes it has \nbeen given, the drawing function can now simply call each shape’s Draw()\nfunction in turn.\nstruct Shape\n{\nvirtual void Draw() = 0; \n// pure virtual function\n};\nstruct Circle : public Shape\n{\nvirtual void Draw()\n {\n \n \n// draw shape as a circle\n }\n};\nstruct Rectangle : public Shape\n{\nvirtual void Draw()\n {\n \n \n// draw shape as a rectangle\n }\n};\nstruct Triangle : public Shape\n{\nvoid Draw()\n {\n \n \n// draw shape as a triangle\n }\n};\nvoid drawShapes(std::list<Shape*> shapes)\n{\n \nstd::list<Shape*>::iterator pShape = shapes.begin();\n \nstd::list<Shape*>::iterator pEnd = shapes.end();\n \nfor ( ; pShape != pEnd; ++pShape)\n {\n  pShape->Draw();\n }\n}\n3.1. C++ Review and Best Practices\n\n\n96 \n3. Fundamentals of Software Engineering for Games\n3.1.1.5. \n Composition and  Aggregation\nComposition is the practice of using a group of interacting objects to accomplish \na high-level task. Composition creates a “has-a” or “uses-a” relationship be-\ntween classes. (Technically speaking, the “has-a” relationship is called com-\nposition, while the “uses-a” relationship is called aggregation.) For example, a \nspace ship has an engine, which in turn has a fuel tank. Composition/aggrega-\ntion usually results in the individual classes being simpler and more focused. \nInexperienced object-oriented programmers oft en rely too heavily on inheri-\ntance and tend to underutilize aggregation and composition.\nAs an example, imagine that we are designing a graphical user interface \nfor our game’s front end. We have a class Window that represents any rectan-\ngular GUI element. We also have a class called Rectangle that encapsulates \nthe mathematical concept of a rectangle. A naïve programmer might derive \nthe Window class from the Rectangle class (using an “is-a” relationship). But \nin a more ﬂ exible and well-encapsulated design, the Window class would refer \nto or contain a Rectangle (employing a “has-a” or “uses-a” relationship). This \nmakes both classes simpler and more focused and allows the classes to be \nmore easily tested, debugged, and reused.\n3.1.1.6. \nDesign Patterns\nWhen the same type of problem arises over and over, and many diﬀ erent pro-\ngrammers employ a very similar solution to that problem, we say that a  design \npatt ern has arisen. In object-oriented programming, a number of common de-\nsign patt erns have been identiﬁ ed and described by various authors. The most \nwell-known book on this topic is probably the “Gang of Four” book [17].\nHere are a few examples of common general-purpose design patt erns.\nz Singleton. This patt ern ensures that a particular class has only one in-\nstance (the singleton instance) and provides a global point of access to it.\nz Iterator. An iterator provides an eﬃ  cient means of accessing the indi-\nvidual elements of a collection, without exposing the collection’s under-\nlying implementation. The iterator “knows” the implementation details \nof the collection, so that its users don’t have to.\nz Abstract factory. An abstract factory provides an interface for creating \nfamilies of related or dependent classes without specifying their con-\ncrete classes.\nThe game industry has its own set of design patt erns, for addressing \nproblems in every realm from rendering to collision to animation to audio. \nIn a sense, this book is all about the high-level design patt erns prevalent in \nmodern 3D game engine design.\n\n\n97 \n3.1.2. \n Coding Standards: Why and How Much?\nDiscussions of coding conventions among engineers can oft en lead to heated \n“religious” debates. I do not wish to spark any such debate here, but I will go \nso far as to suggest that following at least some minimal coding standards is a \ngood idea. Coding standards exist for two primary reasons.\n1. Some standards make the code more readable, understandable, and \nmaintainable.\n2. Other conventions help to prevent programmers from shooting them-\nselves in the foot. For example, a coding standard might encourage the \nprogrammer to use only a smaller, more testable, and less error-prone \nsubset of the whole language. The C++ language is rife with possibili-\nties for abuse, so this kind of coding standard is particularly important \nwhen using C++.\nIn my opinion, the most important things to achieve in your coding con-\nventions are the following.\nz Interfaces are king. Keep your  interfaces (.h ﬁ les) clean, simple, minimal, \neasy to understand, and well-commented.\nz Good names encourage understanding and avoid confusion. Stick to intuitive \nnames that map directly to the purpose of the class, function, or vari-\nable in question. Spend time up-front identifying a good name. Avoid \na naming scheme that requires programmers to use a look-up table in \norder to decipher the meaning of your code. Remember that high-level \nprogramming languages like C++ are intended for humans to read. (If \nyou disagree, just ask yourself why you don’t write all your soft ware \ndirectly in machine language.)\nz Don’t clutt er the  global namespace. Use C++ namespaces or a common \nnaming preﬁ x to ensure that your symbols don’t collide with symbols \nin other libraries. (But be careful not to overuse namespaces, or nest \nthem too deeply.) Name #defined symbols with extra care; remember \nthat C++ preprocessor macros are really just text substitutions, so they \ncut across all C/C++ scope and namespace boundaries.\nz Follow C++ best practices. Books like the Eﬀ ective C++ series by Scott  Mey-\ners [31, 32], Meyers’ Eﬀ ective STL [33], and Large-Scale C++ Soft ware De-\nsign by John Lakos [27] provide excellent guidelines that will help keep \nyou out of trouble.\nz Be consistent. The rule I try to use is as follows: If you’re writing a body \nof code from scratch, feel free to invent any convention you like—then \nstick to it. When editing pre-existing code, try to follow whatever con-\nventions have already been established.\n3.1. C++ Review and Best Practices\n\n\n98 \n3. Fundamentals of Software Engineering for Games\nz Make errors stick out. Joel Spolsky wrote an excellent article on coding \nconventions, which can be found at htt p://www.joelonsoft ware.com /\narticles /Wrong.html. Joel suggests that the “cleanest” code is not neces-\nsarily code that looks neat and tidy on a superﬁ cial level, but rather the \ncode that is writt en in a way that makes common programming errors \neasier to see. Joel’s articles are always fun and educational, and I highly \nrecommend this one.\n3.2. Data, Code, and Memory in C/C++\n3.2.1. \nNumeric Representations\nNumbers are at the heart of everything that we do in game engine development \n(and soft ware development in general). Every soft ware engineer should under-\nstand how numbers are represented and stored by a computer. This section will \nprovide you with the basics you’ll need throughout the rest of the book.\n3.2.1.1. \nNumeric Bases\nPeople think most naturally in base ten, also known as  decimal notation. In this \nnotation, ten distinct digits are used (0 through 9), and each digit from right \nto left  represents the next highest power of 10. For example, the number 7803 \n= (7×103) + (8×102) + (0×101) + (3×100) = 7000 + 800 + 0 + 3.\nIn computer science, mathematical quantities such as integers and real-\nvalued numbers need to be stored in the computer’s memory. And as we know, \ncomputers store numbers in  binary format, meaning that only the two digits 0 \nand 1 are available. We call this a base-two representation, because each digit \nfrom right to left  represents the next highest power of 2. Computer scientists \nsometimes use a preﬁ x of “0b” to represent binary numbers. For example, the \nbinary number 0b1101 is equivalent to decimal 13, because 0b1101 = (1×23) + \n(1×22) + (0×21) + (1×20) = 8 + 4 + 0 + 1 = 13.\nAnother common notation popular in computing circles is  hexadecimal, or \nbase 16. In this notation, the 10 digits 0 through 9 and the six lett ers A through \nF are used; the lett ers A through F replace the decimal values 10 through 15, \nrespectively. A preﬁ x of “0x” is used to denote hex numbers in the C and C++ \nprogramming languages. This notation is popular because computers gener-\nally store data in groups of 8 bits known as bytes, and since a single hexadeci-\nmal digit represents 4 bits exactly, a pair of hex digits represents a byte. For \nexample, the value 0xFF = 0b11111111 = 255 is the largest number that can be \nstored in 8 bits (1 byte). Each digit in a hexadecimal number, from right to left , \nrepresents the next power of 16. So, for example, 0xB052 = (11×163) + (0×162) + \n(5×161) + (2×160) = (11×4096) + (0×256) + (5×16) + (2×1) = 45,138.\n",
      "page_number": 103,
      "chapter_number": 6,
      "summary": "For example, let’s say that your game engine provides a function called \nquatToAngleDeg() which converts a quaternion to an angle of rotation in \ndegrees Key topics include classes, code, and coding.",
      "keywords": [
        "Visual Studio",
        "break point",
        "Data Break Points",
        "code",
        "watch window",
        "shape",
        "data",
        "Data Break",
        "draw shape",
        "Visual Studio break",
        "function",
        "memory",
        "Studio break points",
        "Proﬁ",
        "Tools"
      ],
      "concepts": [
        "classes",
        "code",
        "coding",
        "data",
        "memory",
        "tools",
        "program",
        "programming",
        "inheritance",
        "inherited"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 7,
          "title": "Segment 7 (pages 69-84)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "More Effective C++",
          "chapter": 14,
          "title": "Segment 14 (pages 133-140)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 59,
          "title": "Segment 59 (pages 567-580)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 121-141)",
      "start_page": 121,
      "end_page": 141,
      "detection_method": "synthetic",
      "content": "99 \n3.2. Data, Code, and Memory in C/C++\n3.2.1.2. Signed and Unsigned Integers\nIn computer science, we use both  signed and  unsigned integers. Of course, \nthe term “unsigned integer” is actually a bit of a misnomer—in mathematics, \nthe whole numbers or natural numbers range from 0 (or 1) up to positive inﬁ nity, \nwhile the integers range from negative inﬁ nity to positive inﬁ nity. Neverthe-\nless, we’ll use computer science lingo in this book and stick with the terms \n“signed integer” and “unsigned integer.”\nMost modern personal computers and game consoles work most easily \nwith integers that are 32 bits or 64 bits wide (although 8- and 16-bit integers \nare also used a great deal in game programming as well). To represent a 32-\nbit unsigned integer, we simply encode the value using binary notation (see \nabove). The range of possible values for a 32-bit unsigned integer is 0x00000000 \n(0) to 0xFFFFFFFF (4,294,967,295).\nTo represent a signed integer in 32 bits, we need a way to diﬀ erentiate be-\ntween positive and negative vales. One simple approach would be to reserve \nthe most signiﬁ cant bit as a sign bit—when this bit is zero the value is positive, \nand when it is one the value is negative. This gives us 31 bits to represent the \nmagnitude of the value, eﬀ ectively cutt ing the range of possible magnitudes \nin half (but allowing both positive and negative forms of each distinct magni-\ntude, including zero).\nMost microprocessors use a slightly more eﬃ  cient technique for encod-\ning negative integers, called  two’s complement notation. This notation has only \none representation for the value zero, as opposed to the two representations \npossible with simple sign bit (positive zero and negative zero). In 32-bit two’s \ncomplement notation, the value 0xFFFFFFFF is interpreted to mean –1, and \nnegative values count down from there. Any value with the most signiﬁ cant \nbit set is considered negative. So values from 0x00000000 (0) to 0x7FFFFFFF \n(2,147,483,647) represent positive integers, and 0x80000000 (–2,147,483,648) to \n0xFFFFFFFF (–1) represent negative integers.\n3.2.1.3.  Fixed-Point Notation\nIntegers are great for representing whole numbers, but to represent fractions \nand irrational numbers we need a diﬀ erent format that expresses the concept \nof a decimal point.\nOne early approach taken by computer scientists was to use ﬁ xed-point \nnotation. In this notation, one arbitrarily chooses how many bits will be used \nto represent the whole part of the number, and the rest of the bits are used \nto represent the fractional part. As we move from left  to right (i.e., from the \nmost signiﬁ cant bit to the least signiﬁ cant bit), the magnitude bits represent \ndecreasing powers of two (…, 16, 8, 4, 2, 1), while the fractional bits represent \n\n\n100 \n3. Fundamentals of Software Engineering for Games\ndecreasing inverse powers of two (1/2 , 1/4 , 1/8 , 1/16 , …). For example, to store the \nnumber –173.25 in 32-bit ﬁ xed-point notation, with one sign bit, 16 bits for the \nmagnitude and 15 bits for the fraction, we ﬁ rst convert the sign, the whole part \nand fractional part into their binary equivalents individually (negative = 0b1, \n173 = 0b0000000010101101, and 0.25 = 1/4 = 0b010000000000000). Then we pack \nthose values together into a 32-bit integer. The ﬁ nal result is 0x8056A000. This \nis illustrated in Figure 3.4.\nThe problem with ﬁ xed-point notation is that it constrains both the range \nof magnitudes that can be represented and the amount of precision we can \nachieve in the fractional part. Consider a 32-bit ﬁ xed-point value with 16 bits \nfor the magnitude, 15 bits for the fraction, and a sign bit. This format can only \nrepresent magnitudes up to ±65,535, which isn’t particularly large. To over-\ncome this problem, we employ a ﬂ oating-point representation.\n3.2.1.4. Floating-Point Notation\nIn  ﬂ oating-point notation, the position of the decimal place is arbitrary and is \nspeciﬁ ed with the help of an exponent. A ﬂ oating-point number is broken into \nthree parts: the mantissa, which contains the relevant digits of the number on \nboth sides of the decimal point, the exponent, which indicates where in that \nstring of digits the decimal point lies, and a sign bit, which of course indicates \nwhether the value is positive or negative. There are all sorts of diﬀ erent ways \nto lay out these three components in memory, but the most common standard \nis IEEE-754. It states that a 32-bit ﬂ oating-point number will be represented \nwith the sign in the most signiﬁ cant bit, followed by 8 bits of exponent, and \nﬁ nally 23 bits of mantissa.\nThe value v represented by a sign bit s, an exponent e and a mantissa m is \nv = s × 2(e – 127) × (1 + m).\nThe sign bit s has the value +1 or –1. The exponent e is biased by 127 so \nthat negative exponents can be easily represented. The mantissa begins with \nan implicit 1 that is not actually stored in memory, and the rest of the bits are \ninterpreted as inverse powers of two. Hence the value represented is really 1 \n+ m, where m is the fractional value stored in the mantissa.\n31\n15\n0\nmagnitude (16 bits)\nfraction (15 bits)\n1\n= –173.25\nsign\n0x80\n0x56\n0xA0\n0x00\n1 0\n0 0 0\n0 0\n0 0 1\n0 1\n0 1 1\n0 1\n0 1 0\n0 0\n0 0 0\n0 0\n0 0 0\n0 0\nFigure 3.4.  Fixed-point notation with 16-bit magnitude and 16-bit fraction.\n\n\n101 \nFor example, the bit patt ern shown in Figure 3.5 represents the value \n0.15625, because s = 0 (indicating a positive number), e = 0b01111100 = 124, \nand m = 0b0100… = 0×2–1 + 1×2–2 = ¼. Therefore,\n \nv = s × 2(e – 127) × (1 + m)\n= (+1) × 2(124 – 127) × (1 + 1/4)\n    \n= 2–3 × 5/4 \n \n \n \n             (3.1)\n    \n= 1/8 × 5/4\n    \n= 0.125 × 1.25 = 0.15625.\nThe Trade-Off between Magnitude and Precision\nThe  precision of a ﬂ oating-point number increases as the magnitude decreases, \nand vice versa. This is because there are a ﬁ xed number of bits in the mantissa, \nand these bits must be shared between the whole part and the fractional part \nof the number. If a large percentage of the bits are spent representing a large \nmagnitude, then a small percentage of bits are available to provide fractional \nprecision. In physics the term  signiﬁ cant digits is typically used to describe this \nconcept (htt p://en.wikipedia.org/wiki/Signiﬁ cant_digits).\nTo understand the trade-oﬀ  between magnitude and precision, let’s look \nat the largest possible ﬂ oating-point value, FLT_MAX ≈ 3.403×1038, whose rep-\nresentation in 32-bit IEEE ﬂ oating-point format is 0x7F7FFFFF. Let’s break this \ndown:\nz The largest absolute value that we can represent with a 23-bit mantissa \nis 0x00FFFFFF in hexadecimal, or 24 consecutive binary ones—that’s 23 \nones in the mantissa, plus the implicit leading one.\nz An exponent of 255 has a special meaning in the IEEE-754 format—it is \nused for values like not-a-number (NaN) and inﬁ nity—so it cannot be \nused for regular numbers. Hence the maximum 8-bit exponent is actu-\nally 254, which translates into 127 aft er subtracting the implicit bias of \n127.\nSo  FLT_MAX is 0x00FFFFFF×2127 = 0xFFFFFF00000000000000000000000000. In \nother words, our 24 binary ones were shift ed up by 127 bit positions, leav-\ning 127 – 23 = 104 binary zeros (or 104/4 = 26 hexadecimal zeros) aft er the \n3.2. Data, Code, and Memory in C/C++\n0\n31\n23\n0\nexponent (8 bits)\n0\n1 1 1\n1 1\n0 0 0\n1 0\n0 0 0\n0 0\n0 0 0\n0 0\n0 0 0\n0 0\n0 0 0\n0 0\nmantissa (23 bits)\nsign\n= 0.15625\nFigure 3.5.  IEEE-754 32-bit ﬂ oating-point format.\n\n\n102 \n3. Fundamentals of Software Engineering for Games\nleast signiﬁ cant digit of the mantissa. Those trailing zeros don’t correspond \nto any actual bits in our 32-bit ﬂ oating-point value—they just appear out of \nthin air because of the exponent. If we were to subtract a small number (where \n“small” means any number composed of fewer than 26 hexadecimal digits) \nfrom FLT_MAX, the result would still be FLT_MAX, because those 26 least sig-\nniﬁ cant hexadecimal digits don’t really exist!\nThe opposite eﬀ ect occurs for ﬂ oating-point values whose magnitudes \nare much less than one. In this case, the exponent is large but negative, and \nthe signiﬁ cant digits are shift ed in the opposite direction. We trade the ability \nto represent large magnitudes for high precision. In summary, we always have \nthe same number of signiﬁ cant digits (or really signiﬁ cant bits) in our ﬂ oating-\npoint numbers, and the exponent can be used to shift  those signiﬁ cant bits into \nhigher or lower ranges of magnitude.\nAnother subtlety to notice is that there is a ﬁ nite gap between zero and the \nsmallest nonzero value we can represent with any ﬂ oating-point notation. The \nsmallest nonzero magnitude we can represent is FLT_MIN = 2–126 ≈ 1.175×10–38, \nwhich has a binary representation of 0x00800000 (i.e., the exponent is 0x01, \nor –126 aft er subtracting the bias, and the mantissa is all zeros, except for the \nimplicit leading one). There is no way to represent a nonzero magnitude that \nis smaller than 1.175×10–38, because the next smallest valid value is zero. Put \nanother way, the real number line is quantized when using a ﬂ oating-point \nrepresentation.\nFor a particular ﬂ oating-point representation, the machine epsilon is de-\nﬁ ned to be the smallest ﬂ oating-point value ε  that satisﬁ es the equation, 1 + \nε  ≠ 1. For an IEEE-754 ﬂ oating-point number, with its 23 bits of precision, the \nvalue of ε is 2–23, which is approximately 1.192×10–7. The most signiﬁ cant digit \nof ε falls just inside the range of signiﬁ cant digits in the value 1.0, so adding \nany value smaller than ε to 1.0 has no eﬀ ect. In other words, any new bits con-\ntributed adding a value smaller than ε will get “chopped oﬀ ” when we try to \nﬁ t the sum into a mantissa with only 23 bits.\nThe concepts of limited precision and the machine epsilon have real im-\npacts on game soft ware. For example, let’s say we use a ﬂ oating-point vari-\nable to track absolute game time in seconds. How long can we run our game \nbefore the magnitude of our clock variable gets so large that adding 1/30th of \na second to it no longer changes its value? The answer is roughly 12.9 days. \nThat’s longer than most games will be left  running, so we can probably get \naway with using a 32-bit ﬂ oating-point clock measured in seconds in a game. \nBut clearly it’s important to understand the limitations of the ﬂ oating-point \nformat, so that we can predict potential problems and take steps to avoid them \nwhen necessary.\n\n\n103 \nIEEE Floating-Point Bit Tricks\nSee [7], Section 2.1, for a few really useful IEEE ﬂ oating-point “bit tricks” that \ncan make ﬂ oating-point calculations lightning-fast.\n3.2.1.5. Atomic Data Types\nAs you know, C and C++ provide a number of  atomic data types. The C and \nC++ standards provide guidelines on the relative sizes and signedness of these \ndata types, but each compiler is free to deﬁ ne the types slightly diﬀ erently in \norder to provide maximum performance on the target hardware.\nz  char. A char is usually 8 bits and is generally large enough to hold an \nASCII or UTF-8 character (see Section 5.4.4.1). Some compilers deﬁ ne \nchar to be signed, while others use unsigned chars by default.\nz int, short, long. An int is supposed to hold a signed integer value \nthat is the most eﬃ  cient size for the target platform; it is generally de-\nﬁ ned to be 32 bits wide on Pentium class PCs. A short is intended to \nbe smaller than an int and is 16 bits on many machines. A long is as \nlarge as or larger than an int and may be 32 or 64 bits, depending on \nthe hardware.\nz float. On most modern compilers, a float is a 32-bit IEEE-754 ﬂ oat-\ning-point value.\nz double. A double is a double-precision (i.e., 64-bit) IEEE-754 ﬂ oating-\npoint value.\nz bool. A bool is a true/false value. The size of a bool varies widely across \ndiﬀ erent compilers and hardware architectures. It is never implemented \nas a single bit, but some compilers deﬁ ne it to be 8 bits while others use \na full 32 bits.\nCompiler-Speciﬁ c Sized Types\nThe standard C/C++ atomic data types were designed to be portable and \ntherefore nonspeciﬁ c. However, in many soft ware engineering endeavors, in-\ncluding game engine programming, it is oft en important to know exactly how \nwide a particular variable is. The Visual Studio C/C++ compiler deﬁ nes the fol-\nlowing extended keywords for declaring variables that are an explicit number \nof bits wide: __int8,  __int16, __int32, and __int64.\nSIMD Types\nThe CPUs on many modern computers and game consoles have a special-\nized type of  arithmetic logic unit (ALU) referred to as a vector processor or \nvector unit. A vector processor supports a form of parallel processing known \nas single instruction, multiple data (SIMD), in which a mathematical operation \n3.2. Data, Code, and Memory in C/C++\n\n\n104 \n3. Fundamentals of Software Engineering for Games\nis performed on multiple quantities in parallel, using a single machine in-\nstruction. In order to be processed by the vector unit, two or more quanti-\nties are packed into a 64- or 128-bit CPU register. In game programming, \nthe most commonly used SIMD register format packs four 32-bit IEEE-754 \nﬂ oating-point quantities into a 128-bit SIMD register. This format allows us to \nperform calculations such as vector dot products and matrix multiplications \nmuch more eﬃ  ciently than would be possible with a SISD (single instruction, \nsingle data) ALU.\nEach microprocessor has a diﬀ erent name for its SIMD instruction set, \nand the compilers that target those microprocessors use a custom syntax to \ndeclare SIMD variables. For example, on a Pentium class CPU, the SIMD in-\nstruction set is known as SSE (streaming  SIMD extensions), and the Microsoft  \nVisual Studio compiler provides the built-in data type  __m128 to represent a \nfour-ﬂ oat SIMD quantity.  The PowerPC class of CPUs used on the PLAYSTA-\nTION 3 and Xbox 360 calls its SIMD instruction set  Altivec, and the Gnu C++ \ncompiler uses the syntax vector float to declare a packed four-ﬂ oat SIMD \nvariable. We’ll discuss how SIMD programming works in more detail in Sec-\ntion 4.7.\nPortable Sized Types\nMost other compilers have their own “sized” data types, with similar seman-\ntics but slightly diﬀ erent syntax. Because of these diﬀ erences between compil-\ners, most game engines achieve source code portability by deﬁ ning their own \ncustom atomic data types. For example, at Naughty Dog we use the following \natomic types:\nz  F32 is a 32-bit IEEE-754 ﬂ oating-point value.\nz U8, I8, U16, I16,  U32,  I32, U64, and I64 are unsigned and signed 8-, \n16-, 32-, and 64-bit integers, respectively.\nz  U32F and  I32F are “fast” unsigned and signed 32-bit values, respec-\ntively. Each of these data types acts as though it contains a 32-bit value, \nbut it actually occupies 64 bits in memory. This permits the PS3’s cen-\ntral PowerPC-based processor (called the PPU) to read and write these \nvariables directly into its 64-bit registers, providing a signiﬁ cant speed \nboost over reading and writing 32-bit variables.\nz  VF32 represents a packed four-ﬂ oat SIMD value.\nOGRE ’s Atomic Data Types\nOGRE deﬁ nes a number of atomic types of its own. Ogre::uint8, Ogre::\nuint16 and Ogre::uint32 are the basic unsigned sized integral types.\n\n\n105 \nOgre ::Real deﬁ nes a real ﬂ oating-point value. It is usually deﬁ ned to \nbe 32 bits wide (equivalent to a float), but it can be redeﬁ ned globally to be \n64 bits wide (like a double) by deﬁ ning the preprocessor macro OGRE_DOU-\nBLE_PRECISION to 1. This ability to change the meaning of Ogre::Real is \ngenerally only used if one’s game has a particular requirement for double-\nprecision math, which is rare. Graphics chips (GPUs) always perform their \nmath with 32-bit or 16-bit ﬂ oats, the CPU/FPU is also usually faster when \nworking in single-precision, and SIMD vector instructions operate on 128-bit \nregisters that contain four 32-bit ﬂ oats each. Hence most games tend to stick \nto single-precision ﬂ oating-point math.\nThe data types Ogre ::uchar, Ogre::ushort, Ogre::uint and \nOgre::ulong are just shorthand notations for C/C++’s unsigned char, un-\nsigned short, and unsigned long, respectively. As such, they are no more \nor less useful than their native C/C++ counterparts.\nThe types Ogre ::Radian and Ogre::Degree are particularly interest-\ning. These classes are wrappers around a simple Ogre::Real value. The pri-\nmary role of these types is to permit the angular units of hard-coded literal \nconstants to be documented and to provide automatic conversion between \nthe two unit systems. In addition, the type Ogre::Angle represents an angle \nin the current “default” angle unit. The programmer can deﬁ ne whether the \ndefault will be radians or degrees when the OGRE application ﬁ rst starts \nup.\nPerhaps surprisingly, OGRE does not provide a number of sized atomic \ndata types that are commonplace in other game engines. For example, it de-\nﬁ nes no signed 8-, 16-, or 64-bit integral types. If you are writing a game en-\ngine on top of OGRE, you will probably ﬁ nd yourself deﬁ ning these types \nmanually at some point.\n3.2.1.6. Multi-Byte Values and Endianness\nValues that are larger than eight bits (one byte) wide are called  multi-byte quan-\ntities. They’re commonplace on any soft ware project that makes use of integers \nand ﬂ oating-point values that are 16 bits or wider. For example, the integer \nvalue 4660 = 0x1234 is represented by the two bytes 0x12 and 0x34. We call \n0x12 the most signiﬁ cant byte (MSB) and 0x34 the least signiﬁ cant byte (LSB). \nIn a 32-bit value, such as 0xABCD1234, the MSB is 0xAB and the LSB is 0x34. \nThe same concepts apply to 64-bit integers and to 32- and 64-bit ﬂ oating-point \nvalues as well.\nMulti-byte integers can be stored into memory in one of two ways, and \ndiﬀ erent microprocessors may diﬀ er in their choice of storage method (see \nFigure 3.6).\n3.2. Data, Code, and Memory in C/C++\n\n\n106 \n3. Fundamentals of Software Engineering for Games\nz Litt le-endian. If a microprocessor stores the least signiﬁ cant byte (LSB) of \na multi-byte value at a lower memory address than the most signiﬁ cant \nbyte (MSB), we say that the processor is  litt le-endian. On a litt le-endian \nmachine, the number 0xABCD1234 would be stored in memory using \nthe consecutive bytes 0x34, 0x12, 0xCD, 0xAB.\nz Big-endian. If a microprocessor stores the most signiﬁ cant byte (MSB) of \na multi-byte value at a lower memory address than the least signiﬁ cant \nbyte (LSB), we say that the processor is  big-endian. On a big-endian ma-\nchine, the number 0xABCD1234 would be stored in memory using the \nbytes 0xAB, 0xCD, 0x12, 0x34.\nMost programmers don’t need to think much about  endianness. How-\never, when you’re a game programmer, endianness can become a bit of a thorn \nin your side. This is because games are usually developed on a PC or Linux ma-\nchine running an Intel Pentium processor (which is litt le-endian), but run on \na console such as the Wii, Xbox 360, or PLAYSTATION 3—all three of which \nutilize a variant of the PowerPC processor (which can be conﬁ gured to use \neither endianness, but is big-endian by default). Now imagine what happens \nwhen you generate a data ﬁ le for consumption by your game engine on an \nIntel processor and then try to load that data ﬁ le into your engine running on \na PowerPC processor. Any multi-byte value that you wrote out into that data \nﬁ le will be stored in litt le-endian format. But when the game engine reads the \nﬁ le, it expects all of its data to be in big-endian format. The result? You’ll write \n0xABCD1234, but you’ll read 0x3412CDAB, and that’s clearly not what you \nintended!\nThere are at least two solutions to this problem.\n1. You could write all your data ﬁ les as text and store all multi-byte num-\nbers as sequences of decimal digits, one character (one byte) per digit. \nThis would be an ineﬃ  cient use of disk space, but it would work.\nU32 value = 0xABCD1234;\nU8* pBytes = (U8*)&value;\nBig-endian\n0xAB\n0xCD\npBytes + 0x0\n0x12\n0x34\npBytes + 0x1\npBytes + 0x2\npBytes + 0x3\nLittle-endian\n0x34\n0x12\n0xCD\n0xAB\npBytes + 0x0\npBytes + 0x1\npBytes + 0x2\npBytes + 0x3\nFigure 3.6.  Big- and little-endian representations of the value 0xABCD1234.\n\n\n107 \n2. You can have your tools endian-swap the data prior to writing it into a \nbinary data ﬁ le. In eﬀ ect, you make sure that the data ﬁ le uses the endi-\nanness of the target microprocessor (the game console), even if the tools \nare running on a machine that uses the opposite endianness.\nInteger Endian-Swapping\n Endian-swapping an integer is not conceptually diﬃ  cult. You simply start at \nthe most signiﬁ cant byte of the value and swap it with the least signiﬁ cant \nbyte; you continue this process until you reach the half-way point in the value. \nFor example, 0xA7891023 would become 0x231089A7.\nThe only tricky part is knowing which bytes to swap. Let’s say you’re writ-\ning the contents of a C struct or C++ class from memory out to a ﬁ le. To \nproperly endian-swap this data, you need to keep track of the locations and \nsizes of each data member in the struct and swap each one appropriately \nbased on its size. For example, the structure\nstruct Example\n{\n    U32   m_a;\n    U16   m_b;\n    U32   m_c;\n};\nmight be writt en out to a data ﬁ le as follows:\nvoid writeExampleStruct(Example& ex, Stream& stream)\n{\n    stream.writeU32(swapU32(ex.m_a));\n    stream.writeU16(swapU16(ex.m_b));\n    stream.writeU32(swapU32(ex.m_c));\n}\nand the swap functions might be deﬁ ned like this:\ninline U16 swapU16(U16 value)\n{\n    return ((value & 0x00FF) << 8)\n         | ((value & 0xFF00) >> 8);\n}\ninline U32 swapU32(U32 value)\n{\n return \n   ((value & 0x000000FF) << 24)\n         | ((value & 0x0000FF00) << 8)\n         | ((value & 0x00FF0000) >> 8)\n         | ((value & 0xFF000000) >> 24);\n}\n3.2. Data, Code, and Memory in C/C++\n\n\n108 \n3. Fundamentals of Software Engineering for Games\nYou cannot simply cast the Example object into an array of bytes and \nblindly swap the bytes using a single general-purpose function. We need to \nknow both which data members to swap and how wide each member is; and each \ndata member must be swapped individually.\nFloating-Point Endian-Swapping\nLet’s take a brief look at how ﬂ oating-point endian-swapping diﬀ ers from in-\nteger endian-swapping. As we’ve seen, an IEEE-754 ﬂ oating-point value has \na detailed internal structure involving some bits for the mantissa, some bits \nfor the exponent, and a sign bit. However, you can endian-swap it just as if it \nwere an integer, because bytes are bytes. You can reinterpret ﬂ oats as integers \nby using C++’s reinterpret_cast operator on a pointer to the ﬂ oat; this is \nknown as type punning. But punning can lead to optimization bugs when  strict \naliasing is enabled. (See htt p://cocoawithlove.com/2008/04/using-pointers-to-\nrecast-in-c-is-bad.html for an excellent description of this problem.) One con-\nvenient approach is to use a union, as follows:\nunion U32F32\n{\n    U32    m_asU32;\n    F32    m_asF32;\n};\ninline F32 swapF32(F32 value)\n{\n    U32F32 u;\n    u.m_asF32 = value;\n       // endian-swap as integer\n  u.m_asU32 \n= swapU32(u.m_asU32);\n  return \nu.m_asF32;\n}\n3.2.2. Declarations, Deﬁ nitions, and Linkage\n3.2.2.1. Translation Units Revisited\nAs we saw in Chapter 2, a C or C++ program is comprised of translation units. \nThe compiler translates one .cpp ﬁ le at a time, and for each one it generates \nan output ﬁ le called an object ﬁ le (.o or .obj). A .cpp ﬁ le is the smallest unit of \ntranslation operated on by the compiler; hence, the name “ translation unit.” \nAn object ﬁ le contains not only the compiled machine code for all of the func-\ntions deﬁ ned in the .cpp ﬁ le, but also all of its global and static variables. In ad-\ndition, an object ﬁ le may contain  unresolved references to functions and global \nvariables deﬁ ned in other .cpp ﬁ les.\n\n\n109 \n3.2. Data, Code, and Memory in C/C++\n???\nUnresolved Reference\n???\nMultiply-Defined Symbol\n???\nfoo.cpp\nU32 gGlobalA ;\nU32 gGlobalB ;\nvoid f ()\n{\n    // ...\n    gGlobalC = 5.3f;\n    gGlobalD = -2;\n    // ...\n}\nextern U 32 gGlobalC ;\nbar.cpp\nF32 gGlobalC ;\nvoid g ()\n{\n    // ...\n    U32 a = gGlobalA ;\n    // ...\n    f();\n    // ...\n    gGlobalB = 0;\n}\nextern U 32 gGlobalA ;\nextern U 32 gGlobalB ;\nextern void f ();\nspam.cpp\nU32 gGlobalA ;\nvoid h()\n{\n    // ...\n}\nFigure 3.9.  The two most common linker errors.\nfoo.cpp\nU32 gGlobalA ;\nU32 gGlobalB ;\nvoid f()\n{\n    // ...\n    gGlobalC = 5.3f;\n    // ...\n}\nextern U 32 gGlobalC ;\nbar.cpp\nF32 gGlobalC ;\nvoid g ()\n{\n    // ...\n    U32 a = gGlobalA ;\n    // ...\n    f();\n    // ...\n    gGlobalB = 0;\n}\nextern U 32 gGlobalA ;\nextern U 32 gGlobalB ;\nextern void f ();\nFigure 3.7.  Unresolved external references in two translation units.\nfoo.cpp\nU32 gGlobalA ;\nU32 gGlobalB ;\nvoid f ()\n{\n    // ...\n    gGlobalC = 5.3f;\n    // ...\n}\nextern U 32 gGlobalC ;\nbar.cpp\nF32 gGlobalC ;\nvoid g()\n{\n    // ...\n    U32 a = gGlobalA ;\n    // ...\n    f();\n    // ...\n    gGlobalB = 0;\n}\nextern U 32 gGlobalA ;\nextern U 32 gGlobalB ;\nextern void f ();\nFigure 3.8.  Fully resolved external references after successful linking.\n\n\n110 \n3. Fundamentals of Software Engineering for Games\nThe compiler only operates on one translation unit at a time, so whenever \nit encounters a reference to an external global variable or function, it must \n“go on faith” and assume that the entity in question really exists, as shown \nin Figure 3.7. It is the linker’s job to combine all of the object ﬁ les into a ﬁ nal \nexecutable image. In doing so, the linker reads all of the object ﬁ les and at-\ntempts to resolve all of the unresolved cross-references between them. If it is \nsuccessful, an executable image is generated containing all of the functions, \nglobal variables, and static variables, with all cross-translation-unit references \nproperly resolved. This is depicted in Figure 3.8.\nThe linker’s primary job is to resolve external references, and in this ca-\npacity it can generate only two kinds of errors:\n1. The target of an extern reference might not be found, in which case the \nlinker generates an  “unresolved symbol” error.\n2. The linker might ﬁ nd more than one variable or function with the same \nname, in which case it generates a  “multiply deﬁ ned symbol” error.\nThese two situations are shown in Figure 3.9.\n3.2.2.2. Declaration versus Deﬁ nition\nIn the C and C++ languages, variables and functions must be declared and de-\nﬁ ned before they can be used. It is important to understand the diﬀ erence be-\ntween a declaration and a deﬁ nition in C and C++.\nz A  declaration is a description of a data object or function. It provides the \ncompiler with the name of the entity and its  data type or function signature \n(i.e., return type and argument type(s)).\nz A  deﬁ nition, on the other hand, describes a unique region of memory in \nthe program. This memory might contain a variable, an instance of a \nstruct or class, or the machine code of a function.\nIn other words, a declaration is a reference to an entity, while a deﬁ nition is the \nentity itself. A deﬁ nition is always a declaration, but the reverse is not always \nthe case—it is possible to write a pure declaration in C and C++ that is not a \ndeﬁ nition.\nFunctions are deﬁ ned by writing the body of the function immediately af-\nter the signature, enclosed in curly braces:\nfoo.cpp\n// definition of the max() function\nint max(int a, int b) \n{\n\n\n111 \n \nreturn (a > b) ? a : b;\n}\n// definition of the min() function\nint min(int a, int b) \n{\n \nreturn (a <= b) ? a : b;\n}\nA pure declaration can be provided for a function so that it can be used in \nother translation units (or later in the same translation unit). This is done by \nwriting a function signature followed by a semicolon, with an optional preﬁ x \nof extern:\nfoo.h\nextern int max(int a, int b); // a function declaration\nint min(int a, int b);  \n     // also a declaration (the\n         \n    // ‘extern’ is optional/ \n \n         // \nassumed)\nVariables and instances of classes and structs are deﬁ ned by writing the \ndata type followed by the name of the variable or instance, and an optional \narray speciﬁ er in square brackets:\nfoo.cpp\n// All of these are variable definitions:\nU32 gGlobalInteger = 5;\nF32 gGlobalFloatArray[16];\nMyClass gGlobalInstance;\nA global variable deﬁ ned in one translation unit can optionally be declared for \nuse in other translation units by using the extern keyword:\nfoo.h\n// These are all pure declarations:\nextern U32 gGlobalInteger; \nextern F32 gGlobalFloatArray[16];\nextern MyClass gGlobalInstance;\nMultiplicity of Declarations and Deﬁ nitions\nNot surprisingly, any particular data object or function in a C/C++ program \ncan have multiple identical declarations, but each can have only one deﬁ ni-\ntion. If two or more identical deﬁ nitions exist in a single translation unit, \nthe compiler will notice that multiple entities have the same name and \nﬂ ag an error. If two or more identical deﬁ nitions exist in diﬀ erent transla-\n3.2. Data, Code, and Memory in C/C++\n\n\n112 \n3. Fundamentals of Software Engineering for Games\ntion units, the compiler will not be able to identify the problem, because \nit operates on one translation unit at a time. But in this case, the linker \nwill give us a “multiply deﬁ ned symbol” error when it tries to resolve the \ncross-references.\nDeﬁ nitions in Header Files and Inlining\nIt is usually dangerous to place deﬁ nitions in header ﬁ les. The reason for this \nshould be prett y obvious: If a header ﬁ le containing a deﬁ nition is  #included \ninto more than one .cpp ﬁ le, it’s a sure-ﬁ re way of generating a “multiply de-\nﬁ ned symbol” linker error.\n Inline function deﬁ nitions are an exception to this rule, because each in-\nvocation of an inline function gives rise to a brand new copy of that function’s \nmachine code, embedded directly into the calling function. In fact, inline func-\ntion deﬁ nitions must be placed in header ﬁ les if they are to be used in more \nthan one translation unit. Note that it is not suﬃ  cient to tag a function declara-\ntion with the inline keyword in a .h ﬁ le and then place the body of that func-\ntion in a .cpp ﬁ le. The compiler must be able to “see” the body of the function \nin order to inline it. For example:\nfoo.h\n// This function definition will be inlined properly.\ninline int max(int a, int b)\n{\n \nreturn (a > b) ? a : b;\n}\n// This declaration cannot be inlined because the \n// compiler cannot “see” the body of the function.\ninline int min(int a, int b);\nfoo.cpp\n// The body of min() is effectively “hidden” from the\n// compiler, and so it can ONLY be inlined within \n// foo.cpp.\nint min(int a, int b)\n{\n \nreturn (a <= b) ? a : b;\n}\nThe inline keyword is really just a hint to the compiler. It does a cost/\nbeneﬁ t analysis of each inline function, weighing the size of the function’s \ncode versus the potential performance beneﬁ ts of inling it, and the compiler \ngets the ﬁ nal say as to whether the function will really be inlined or not. Some \ncompilers provide syntax like __forceinline, allowing the programmer \n\n\n113 \nto bypass the compiler’s cost/beneﬁ t analysis and control function inlining \ndirectly.\n3.2.2.3.  Linkage\nEvery deﬁ nition in C and C++ has a property known as linkage. A deﬁ nition \nwith  external linkage is visible to and can be referenced by translation units \nother than the one in which it appears. A deﬁ nition with  internal linkage can \nonly be “seen” inside the translation unit in which it appears and thus cannot \nbe referenced by other translation units. We call this property linkage because \nit dictates whether or not the linker is permitt ed to cross-reference the entity \nin question. So, in a sense, linkage is the translation unit’s equivalent of the \n public: and  private: keywords in C++ class deﬁ nitions.\nBy default, deﬁ nitions have external linkage. The  static keyword is \nused to change a deﬁ nition’s linkage to internal. Note that two or more identi-\ncal static deﬁ nitions in two or more diﬀ erent .cpp ﬁ les are considered to be \ndistinct entities by the linker (just as if they had been given diﬀ erent names), \nso they will not generate a “multiply deﬁ ned symbol” error. Here are some \nexamples:\nfoo.cpp\n// This variable can be used by other .cpp files \n//  (external linkage).\nU32 gExternalVariable; \n// This variable is only usable within foo.cpp (internal\n// linkage).\nstatic U32 gInternalVariable;\n// This function can be called from other .cpp files \n//  (external linkage).\nvoid externalFunction()\n{\n \n// ...\n}\n// This function can only be called from within foo.cpp\n// (internal linkage).\nstatic void internalFunction()\n{\n \n// ...\n}\nbar.cpp\n// This declaration grants access to foo.cpp’s variable.\nextern U32 gExternalVariable;\n3.2. Data, Code, and Memory in C/C++\n\n\n114 \n3. Fundamentals of Software Engineering for Games\n// This ‘gInternalVariable’ is distinct from the one \n// defined in foo.cpp – no error. We could just as \n// well have named it gInternalVariableForBarCpp – the \n// net effect is the same.\nstatic U32 gInternalVariable;\n// This function is distinct from foo.cpp’s \n// version – no error. It acts as if we had named it \n// internalFunctionForBarCpp().\nstatic void internalFunction()\n{\n \n// ...\n}\n// ERROR – multiply defined symbol!\nvoid externalFunction()\n{\n \n// ...\n}\nTechnically speaking, declarations don’t have a linkage property at all, be-\ncause they do not allocate any storage in the executable image; therefore, there \nis no question as to whether or not the linker should be permitt ed to cross-\nreference that storage. A declaration is merely a reference to an entity deﬁ ned \nelsewhere. However, it is sometimes convenient to speak about declarations \nas having internal linkage, because a declaration only applies to the transla-\ntion unit in which it appears. If we allow ourselves to loosen our terminology \nin this manner, then declarations always have internal linkage—there is no \nway to cross-reference a single declaration in multiple .cpp ﬁ les. (If we put a \ndeclaration in a header ﬁ le, then multiple .cpp ﬁ les can “see” that declaration, \nbut they are in eﬀ ect each gett ing a distinct copy of the declaration, and each \ncopy has internal linkage within that translation unit.)\nThis leads us to the real reason why inline function deﬁ nitions are permit-\nted in header ﬁ les: It is because inline functions have internal linkage by de-\nfault, just as if they had been declared static. If multiple .cpp ﬁ les #include\na header containing an inline function deﬁ nition, each translation unit gets a \nprivate copy of that function’s body, and no “multiply deﬁ ned symbol” errors \nare generated. The linker sees each copy as a distinct entity.\n3.2.3. C/C++ Memory Layout\nA program writt en in C or C++ stores its data in a number of diﬀ erent places in \nmemory. In order to understand how storage is allocated and how the various \n\n\n115 \ntypes of C/C++ variables work, we need to understand the memory layout of \na C/C++ program.\n3.2.3.1. Executable Image\nWhen a C/C++ program is built, the linker creates an executable ﬁ le. Most UN-\nIX-like operating system s, including many game consoles, employ a popular \nexecutable ﬁ le format called the  executable and linking format (ELF). Executable \nﬁ les on those systems therefore have an .elf extension. The Windows execut-\nable format is similar to the ELF format; executables under Windows have \nan .exe extension. Whatever its format, the executable ﬁ le always contains a \npartial image of the program as it will exist in memory when it runs. I say a \n“partial” image because the program generally allocates memory at runtime \nin addition to the memory laid out in its executable image.\nThe  executable image is divided into contiguous blocks called  segments \nor sections. Every operating system lays things out a litt le diﬀ erently, and the \nlayout may also diﬀ er slightly from executable to executable on the same op-\nerating system. But the image is usually comprised of at least the following \nfour segments:\n1. Text segment. Sometimes called the code segment, this block contains  ex-\necutable machine code for all functions deﬁ ned by the program.\n2. Data segment. This segment contains all initialized global and static vari-\nables. The memory needed for each global variable is laid out exactly \nas it will appear when the program is run, and the proper initial values \nare all ﬁ lled in. So when the executable ﬁ le is loaded into memory, the \ninitialized  global and  static variables are ready to go.\n3. BSS  segment. “BSS” is an outdated name which stands for “block started \nby symbol.” This segment contains all of the uninitialized global and stat-\nic variables deﬁ ned by the program. The C and C++ languages explicitly \ndeﬁ ne the initial value of any uninitialized global or static variable to be \nzero. But rather than storing a potentially very large block of zeros in \nthe BSS section, the linker simply stores a count of how many zero bytes \nare required to account for all of the uninitialized globals and statics in \nthe segment. When the executable is loaded into memory, the operating \nsystem reserves the requested number of bytes for the BSS section and \nﬁ lls it with zeros prior to calling the program’s entry point (e.g. main()\nor WinMain()).\n4. Read-only data segment. Sometimes called the rodata segment, this seg-\nment contains any read-only (constant) global data deﬁ ned by the pro-\ngram. For example, all ﬂ oating-point constants (e.g., const float kPi \n3.2. Data, Code, and Memory in C/C++\n\n\n116 \n3. Fundamentals of Software Engineering for Games\n= 3.141592f;) and all global object instances that have been declared \nwith the const keyword (e.g., const Foo gReadOnlyFoo; ) reside in \nthis segment. Note that integer constants (e.g., const int kMaxMon-\nsters = 255; ) are oft en used as manifest constants by the compiler, \nmeaning that they are inserted directly into the machine code wherever \nthey are used. Such constants occupy storage in the text segment, but \nthey are not present in the read-only data segment.\nGlobal variables, i.e., variables deﬁ ned at ﬁ le  scope outside any function or \nclass declaration, are stored in either the data or BSS segments, depending on \nwhether or not they have been initialized. The following global will be stored \nin the data segment, because it has been initialized:\nfoo.cpp\nF32 gInitializedGlobal = -2.0f;\nand the following global will be allocated and initialized to zero by the operat-\ning system , based on the speciﬁ cations given in the BSS segment, because it \nhas not been initialized by the programmer:\nfoo.cpp\nF32 gUninitializedGlobal;\nWe’ve seen that the static keyword can be used to give a global vari-\nable or function deﬁ nition internal linkage, meaning that it will be “hidden” \nfrom other translation units. The static keyword can also be used to declare \na global variable within a function. A function-static variable is lexically scoped \nto the function in which it is declared (i.e., the variable’s name can only be \n“seen” inside the function). It is initialized the ﬁ rst time the function is called \n(rather than before main() is called as with ﬁ le-scope statics). But in terms of \nmemory layout in the executable image, a function-static variable acts identi-\ncally to a ﬁ le-static global variable—it is stored in either the data or BSS seg-\nment based on whether or not it has been initialized.\nvoid readHitchhikersGuide(U32 book)\n{\n \nstatic U32 sBooksInTheTrilogy = 5; \n// data segment\n static \nU32 sBooksRead;     // \nBSS segment\n \n// ...\n}\n3.2.3.2. Program Stack\nWhen an executable program is loaded into memory and run, the operating \nsystem reserves an area of memory for the program  stack. Whenever a function \nis called, a contiguous area of stack memory is pushed onto the stack—we call \nthis block of memory a  stack frame. If function a() calls another function b(), \n\n\n117 \na new stack frame for b() is pushed on top of a()’s frame. When b() returns, \nits stack frame is popped, and execution continues wherever a() left  oﬀ .\nA stack frame stores three kinds of data:\n1. It stores the  return address of the calling function, so that execution may \ncontinue in the calling function when the called function returns.\n2. The contents of all relevant CPU  registers are saved in the stack frame. \nThis allows the new function to use the registers in any way it sees ﬁ t, \nwithout fear of overwriting data needed by the calling function. Upon \nreturn to the calling function, the state of the registers is restored so that \nexecution of the calling function may resume. The return value of the \ncalled function, if any, is usually left  in a speciﬁ c register so that the call-\ning function can retrieve it, but the other registers are restored to their \noriginal values.\n3. The stack frame also contains all  local variables declared by the func-\ntion; these are also known as  automatic variables. This allows each dis-\ntinct function invocation to maintain its own private copy of every local \nvariable, even when a function calls itself recursively. (In practice, some \nlocal variables are actually allocated to CPU registers rather than being \nstored in the stack frame but, for the most part, such variables operate as \nif they were allocated within the function’s stack frame.) For example:\n3.2. Data, Code, and Memory in C/C++\na()’s\nstack\nframe\nsaved CPU registers\nreturn address\naLocalsA1[5]\nlocalA2\na()’s\nstack\nframe\nsaved CPU registers\nreturn address\naLocalsA1[5]\nlocalA2\na()’s\nstack\nframe\nsaved CPU registers\nreturn address\naLocalsA1[5]\nlocalA2\nb()’s\nstack\nframe\nsaved CPU registers\nreturn address\nlocalB1\nlocalB2\nb()’s\nstack\nframe\nsaved CPU registers\nreturn address\nlocalB1\nlocalB2\nsaved CPU registers\nreturn address\nlocalC1\nc()’s\nstack\nframe\nfunction a() is called\nfunction b() is called\nfunction c() is called\nFigure 3.10.  Stack frames.\n\n\n118 \n3. Fundamentals of Software Engineering for Games\n \nvoid someFunction()\n {\n  U32 \nanInteger;\n  // \n...\n }\nPushing and popping stack frames is usually implemented by adjusting \nthe value of a single register in the CPU, known as the stack pointer. Figure \n3.10 illustrates what happens when the functions shown below are executed.\nvoid c()\n{\n \nU32 localC1;\n \n// ...\n}\nF32 b()\n{\n \nF32 localB1;\n \nI32 localB2;\n \n// ... \n \nc(); \n// call function c()\n \n// ...\n \nreturn localB1;\n}\nvoid a()\n{\n \nU32 aLocalsA1[5];\n \n// ...\n \nF32 localA2 = b();  \n// call function b()\n \n// ...\n}\nWhen a function containing automatic variables returns, its stack frame \nis abandoned and all automatic variables in the function should be treated as \nif they no longer exist. Technically, the memory occupied by those variables \nis still there in the abandoned stack frame—but that memory will very likely \nbe overwritt en as soon as another function is called. A common error involves \nreturning the address of a local variable, like this:\n\n\n119 \nU32* getMeaningOfLife()\n{\n \nU32 anInteger = 42;\n \nreturn &anInteger;\n}\nYou might get away with this if you use the returned pointer immediately and \ndon’t call any other functions in the interim. But more oft en than not, this kind \nof code will crash—in ways that can be diﬃ  cult to debug.\n3.2.3.3. Dynamic Allocation Heap\nThus far, we’ve seen that a program’s data can be stored as global or static \nvariables or as local variables. The globals and statics are allocated within the \nexecutable image, as deﬁ ned by the data and BSS segments of the executable \nﬁ le. The locals are allocated on the program stack. Both of these kinds of stor-\nage are statically deﬁ ned, meaning that the size and layout of the memory \nis known when the program is compiled and linked. However, a program’s \nmemory requirements are oft en not fully known at compile time. A program \nusually needs to allocate additional memory dynamically.\nTo allow for  dynamic allocation, the operating system maintains a block \nof memory that can be allocated by a running program by calling malloc()\nand later returned to the pool for use by other programs by calling free(). \nThis memory block is known as  heap memory, or the free store. When we al-\nlocate memory dynamically, we sometimes say that this memory resides on \nthe heap.\nIn C++, the global  new and  delete operators are used to allocate and free \nmemory to and from the heap. Be wary, however—individual classes may \noverload these operators to allocate memory in custom ways, and even the \nglobal new and delete operators can be overloaded, so you cannot simply as-\nsume that new is always allocating from the heap.\nWe will discuss dynamic memory allocation in more depth in Chap-\nter 6. For additional information, see htt p://en.wikipedia.org/wiki/Dynamic_\nmemory_allocation.\n3.2.4. Member Variables\nC structs and C++  classes allow  variables to be grouped into logical units. \nIt’s important to remember that a class or struct declaration allocates no \nmemory. It is merely a description of the layout of the data—a cookie cutt er \nwhich can be used to stamp out  instances of that struct or class later on. \nFor example:\n3.2. Data, Code, and Memory in C/C++\n",
      "page_number": 121,
      "chapter_number": 7,
      "summary": "This chapter covers segment 7 (pages 121-141). Key topics include value, bits. Covers function. To represent a 32-\nbit unsigned integer, we simply encode the value using binary notation (see \nabove).",
      "keywords": [
        "function",
        "Memory",
        "deﬁ",
        "Data",
        "bits",
        "deﬁ ned",
        "Data Types",
        "Deﬁ nition",
        "bit",
        "Software Engineering",
        "Atomic Data Types",
        "variables",
        "oating-point",
        "variables deﬁ ned",
        "Types"
      ],
      "concepts": [
        "value",
        "bit",
        "bits",
        "functions",
        "function",
        "data",
        "variable",
        "point",
        "numbers",
        "memory"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 9,
          "title": "Segment 9 (pages 158-179)",
          "relevance_score": 0.79,
          "method": "sentence_transformers"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 5,
          "title": "Segment 5 (pages 79-99)",
          "relevance_score": 0.72,
          "method": "sentence_transformers"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 4,
          "title": "Segment 4 (pages 60-78)",
          "relevance_score": 0.7,
          "method": "sentence_transformers"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 8,
          "title": "Segment 8 (pages 140-157)",
          "relevance_score": 0.7,
          "method": "sentence_transformers"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 6,
          "title": "Segment 6 (pages 100-121)",
          "relevance_score": 0.69,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 142-159)",
      "start_page": 142,
      "end_page": 159,
      "detection_method": "synthetic",
      "content": "120 \n3. Fundamentals of Software Engineering for Games\nstruct Foo \n \n// struct declaration\n{\n U32 \n mUnsignedValue;\n F32 \n mFloatValue;\n \nbool mBooleanValue;\n};\nOnce a struct or class has been declared, it can be allocated (deﬁ ned) in \nany of the ways that an atomic data type can be allocated, for example,\nz as an automatic variable, on the program stack;\n \nvoid someFunction()\n {\n  Foo \nlocalFoo;\n  // \n...\n }\nz as a global, ﬁ le-static or function-static;\n \nFoo gFoo;\n \nstatic Foo sFoo;\n \nvoid someFunction()\n {\n \n \nstatic Foo sLocalFoo;\n  // \n...\n }\nz dynamically allocated from the heap. In this case, the pointer or refer-\nence variable used to hold the address of the data can itself be allocated \nas an automatic, global, static, or even dynamically.\n \nFoo* gpFoo = NULL; // global pointer to a Foo\n \nvoid someFunction()\n {\n \n \n// allocate a Foo instance from the heap\n \n \ngpFoo = new Foo;\n  // \n...\n \n \n// allocate another Foo, assign to local  \n \n \n \n  // \npointer\n \n \nFoo* pAnotherFoo = new Foo;\n  // \n...\n \n \n// allocate a POINTER to a Foo from the heap\n \n \nFoo** ppFoo = new Foo*;\n \n \n(*ppFoo) = pAnotherFoo;\n }\n\n\n121 \n3.2.4.1. Class-Static Members\nAs we’ve seen, the  static keyword has many diﬀ erent meanings depending \non context:\nz When used at ﬁ le scope, static means “restrict the visibility of this \nvariable or function so it can only be seen inside this .cpp ﬁ le.”\nz When used at function scope, static means “this variable is a global, \nnot an automatic, but it can only be seen inside this function.”\nz When used inside a struct or  class declaration, static means “this \nvariable is not a regular member variable, but instead acts just like a \nglobal.”\nNotice that when static is used inside a class declaration, it does not \ncontrol the visibility of the variable (as it does when used at ﬁ le scope)—\nrather, it diﬀ erentiates between regular per-instance member variables \nand per-class variables that act like globals. The visibility of a class-static \nvariable is determined by the use of public:, protected: or private:\nkeywords in the class declaration. Class-static variables are automatically \nincluded within the namespace of the class or struct in which they are \ndeclared. So the name of the class or struct must be used to disambigu-\nate the variable whenever it is used outside that class or struct (e.g., \nFoo::sVarName).\nLike an extern declaration for a regular global variable, the declaration \nof a class-static variable within a class allocates no memory. The memory for \nthe class-static variable must be deﬁ ned in a .cpp ﬁ le. For example:\nfoo.h\nclass Foo\n{\npublic:\n \nstatic F32 sClassStatic; \n \n// allocates no   \n \n \n          // \nmemory!\n};\nfoo.cpp\nF32 Foo::sClassStatic = -1.0f; \n// define memory and  \n \n          // \ninit\n3.2.5. Object Layout in Memory\nIt’s useful to be able to visualize the memory layout of your classes and \nstructs. This is usually prett y straightforward—we can simply draw a box \nfor the struct or class, with horizontal lines separating data members. An \n3.2. Data, Code, and Memory in C/C++\n\n\n122 \n3. Fundamentals of Software Engineering for Games\nexample of such a diagram for the struct Foo listed below is shown in Fig-\nure 3.11.\nstruct Foo\n{\n U32 \n  mUnsignedValue;\n F32 \n  mFloatValue;\n I32 \n  mSignedValue;\n};\nThe sizes of the data members are important and should be represented \nin your diagrams. This is easily done by using the width of each data member \nto indicate its size in bits—i.e., a 32-bit integer should be roughly four times \nthe width of an 8-bit integer (see Figure 3.12).\nstruct Bar\n{\n U32 \n  mUnsignedValue;\n F32 \n  mFloatValue;\n \nbool   mBooleanValue; // diagram assumes this is 8 bits\n};\n3.2.5.1. Alignment and Packing\nAs we start to think more carefully about the layout of our structs and classes \nin memory, we may start to wonder what happens when small data members \nare interspersed with larger members. For example:\nstruct InefficientPacking\n{\n U32 \n \nmU1; \n// 32 bits\n F32 \n \nmF2; \n// 32 bits\n \nU8  \nmB3; \n// 8 bits\n I32 \n \nmI4; \n// 32 bits\n \nbool \nmB5; \n// 8 bits\n \nchar* mP6; \n// 32 bits\n};\nYou might imagine that the compiler simply  packs the data members into \nmemory as tightly as it can. However, this is not usually the case. Instead, \nthe compiler will typically leave “holes” in the layout, as depicted in Fig-\nure 3.13. (Some compilers can be requested not to leave these holes by us-\ning a preprocessor directive like #pragma pack , or via command-line op-\ntions; but the default behavior is to space out the members as shown in Fig-\nure 3.13.)\nmU1\nmF2\nmB3\nmI4\nmB5\nmP6\n+0x0\n+0x4\n+0x8\n+0xC\n+0x10\n+0x14\nFigure 3.13.  Inefﬁ cient \nstruct packing due to \nmixed data member \nsizes.\nmUnsignedValue\nmFloatValue\nmSignedValue\n+0x0\n+0x4\n+0x8\nFigure 3.11.  Memory \nlayout of a simple \nstruct.\nmUnsignedValue\nmFloatValue\nmBooleanValue\n+0x0\n+0x4\n+0x8\nFigure 3.12.  A memory \nlayout using width to \nindicate member sizes.\n\n\n123 \nWhy does the compiler leave these “holes?” The reason lies in the fact that \nevery data type has a natural  alignment which must be respected in order to \npermit the CPU to read and write memory eﬀ ectively. The alignment of a data \nobject refers to whether its address in memory is a multiple of its size (which is \ngenerally a power of two):\nz An object with one-byte alignment resides at any memory address.\nz An object with two-byte alignment resides only at even addresses (i.e., \naddresses whose least signiﬁ cant nibble is 0x0, 0x2, 0x4, 0x8, 0xA, 0xC, \nor 0xE).\nz An object with four-byte alignment resides only at addresses that are a \nmultiple of four (i.e., addresses whose least signiﬁ cant nibble is 0x0, 0x4, \n0x8, or 0xC).\nz A 16-byte aligned object resides only at addresses that are a multiple of \n16 (i.e., addresses whose least signiﬁ cant nibble is 0x0).\nAlignment is important because many modern processors can actually \nonly read and write properly aligned blocks of data. For example, if a program \nrequests that a 32-bit (four-byte) integer be read from address 0x6A341174, the \nmemory controller will load the data happily because the address is four-byte \naligned (in this case, its least signiﬁ cant nibble is 0x4). However, if a request is \nmade to load a 32-bit integer from address 0x6A341173, the memory control-\nler now has to read two four-byte blocks: the one at 0x6A341170 and the one \nat 0x6A341174. It must then mask and shift  the two parts of the 32-bit integer \nand logically OR them together into the destination register on the CPU. This \nis shown in Figure 3.14.\nSome microprocessors don’t even go this far. If you request a read or write \nof unaligned data, you might just get garbage. Or your program might just \ncrash altogether! (The PlayStation 2 is a notable example of this kind of intol-\nerance for unaligned data.)\nDiﬀ erent data types have diﬀ erent alignment requirements. A good rule \nof thumb is that a data type should be aligned to a boundary equal to the \nwidth of the data type in bytes. For example, 32-bit values generally have a \nfour-byte alignment requirement, 16-bit values should be two-byte aligned, \nand 8-bit values can be stored at any address (one-byte aligned). On CPUs that \nsupport SIMD vector math, the SIMD registers each contain four 32-bit ﬂ oats, \nfor a total of 128 bits or 16 bytes. And as you would guess, a four-ﬂ oat SIMD \nvector typically has a 16-byte alignment requirement.\nThis brings us back to those “holes” in the layout of struct Ineffi-\ncientPacking shown in Figure 3.13. When smaller data types like 8-bit bools \nare interspersed with larger types like 32-bit integers or floats in a structure \n3.2. Data, Code, and Memory in C/C++\n\n\n124 \n3. Fundamentals of Software Engineering for Games\nor class, the compiler introduces padding (holes) in order to ensure that every-\nthing is properly aligned. It’s a good idea to think about alignment and pack-\ning when declaring your data structures. By simply rearranging the members \nof struct InefficientPacking from the example above, we can eliminate \nsome of the wasted padding space, as shown below and in Figure 3.15:\n \nstruct MoreEfficientPacking\n {\n  U32 \n \nmU1; \n// 32 bits (4-byte aligned)\n  F32 \n \nmF2; \n// 32 bits (4-byte aligned)\n  I32 \n \nmI4; \n// 32 bits (4-byte aligned)\n \n \nchar* mP6; \n// 32 bits (4-byte aligned)\n \n \nU8  \nmB3; \n// 8 bits (1-byte aligned)\n \n \nbool \nmB5; \n// 8 bits (1-byte aligned)\n };\nYou’ll notice in Figure 3.15 that the size of the structure as a whole is \nnow 20 bytes, not 18 bytes as we might expect, because it has been padded \nby two bytes at the end. This padding is added by the compiler to ensure \nproper alignment of the structure in an array context. That is, if an array of \nthese structs is deﬁ ned and the ﬁ rst element of the array is aligned, then the \npadding at the end guarantees that all subsequent elements will also be aligned \nproperly.\nThe alignment of a structure as a whole is equal to the largest alignment \nrequirement among its members. In the example above, the largest mem-\nber alignment is four-byte, so the structure as a whole should be four-byte \nCPU\nalignedValue\n0x6A341170\n0x6A341174\n0x6A341178\nregister\n-alignedValue\n0x6A341170\n0x6A341174\n0x6A341178\nun-\n-alignedValue\nun-\nshift\nshift\n-alignedValue\nun-\nAligned read from\n0x6A341174\nUnaligned read from\n0x6A341173\nCPU\nregister\nFigure 3.14.  Aligned and unaligned reads of a 32-bit integer.\n(pad)\nmU1\nmF2\nmB3\nmI4\nmB5\nmP6\n+0x0\n+0x4\n+0x8\n+0xC\n+0x10\nFigure 3.15.  More ef-\nﬁ cient \npacking \nby \ngrouping small mem-\nbers together.\n\n\n125 \naligned. I usually like to add explicit padding to the end of my structs, to make \nthe wasted space visible and explicit, like this:\n \nstruct BestPacking\n {\n \n U32  \nmU1; \n \n// 32 bits (4-byte aligned)\n \n F32  \nmF2; \n \n// 32 bits (4-byte aligned)\n \n I32  \nmI4; \n \n// 32 bits (4-byte aligned)\n \n char* \nmP6; \n \n// 32 bits (4-byte aligned)\n \n U8 \n \nmB3; \n \n// 8 bits (1-byte aligned)\n \n bool     mB5; \n \n// 8 bits (1-byte aligned)\nU8\n_pad[2]; // explicit padding\n };\n3.2.5.2. Memory Layout of C++ Classes\nTwo things make C++ classes a litt le diﬀ erent from C structures in terms of \nmemory layout:  inheritance and virtual functions. \nWhen class B inherits from class A, B’s data members simply appear im-\nmediately aft er A’s in memory, as shown in Figure 3.16. Each new derived \nclass simply tacks its data members on at the end, although alignment re-\nquirements may introduce padding between the classes. (Multiple inheritance \ndoes some whacky things, like including multiple copies of a single base class \nin the memory layout of a derived class. We won’t cover the details here, be-\ncause game programmers usually prefer to avoid  multiple inheritance alto-\ngether anyway.)\nIf a class contains or inherits one or more virtual functions, then four ad-\nditional bytes (or however many bytes a pointer occupies on the target hard-\nware) are added to the class layout, typically at the very beginning of the \nclass’ layout. These four bytes are collectively called the virtual table pointer \nor vpointer, because they contain a pointer to a data structure known as the \nvirtual function table or vtable. The vtable for a particular class contains pointers \nto all the virtual functions that it declares or inherits. Each concrete class has \nits own virtual table, and every instance of that class has a pointer to it, stored \nin its vpointer.\nThe virtual function table is at the heart of  polymorphism, because it al-\nlows code to be writt en that is ignorant of the speciﬁ c concrete classes it is deal-\ning with. Returning to the ubiquitous example of a Shape base class with de-\nrived classes for Circle, Rectangle, and Triangle, let’s imagine that Shape\ndeﬁ nes a virtual function called Draw(). The derived classes all override \nthis function, providing distinct implementations named Circle::Draw(), \nRectangle::Draw(), and Triangle::Draw(). The virtual table for any \nclass derived from Shape will contain an entry for the Draw() function, but \nthat entry will point to diﬀ erent function implementations, depending on the \n3.2. Data, Code, and Memory in C/C++\nA\nB\n+0x0\n+sizeof(A)\nFigure 3.16.  Effect of \ninheritance on class \nlayout.\n\n\n126 \n3. Fundamentals of Software Engineering for Games\nconcrete class. Circle’s vtable will contain a pointer to Circle::Draw(), \nwhile Rectangle’s virtual table will point to Rectangle::Draw(), and Tri-\nangle’s vtable will point to Triangle::Draw(). Given an arbitrary point-\ner to a Shape (Shape* pShape), the code can simply dereference the vtable \npointer, look up the Draw() function’s entry in the vtable, and call it. The \nresult will be to call Circle::Draw() when pShape points to an instance \nof Circle, Rectangle::Draw() when pShape points to a Rectangle, and \nTriangle::Draw() when pShape points to a Triangle.\nThese ideas are illustrated by the following code excerpt. Notice that the \nbase class Shape deﬁ nes two virtual functions, SetId() and Draw(), the lat-\nter of which is declared to be pure virtual. (This means that Shape provides \nno default implementation of the Draw() function, and derived classes must \noverride it if they want to be instantiable.) Class Circle derives from Shape, \nadds some data members and functions to manage its center and radius, and \noverrides the Draw()function; this is depicted in Figure 3.17. Class Triangle\nalso derives from Shape. It adds an array of Vector3 objects to store its three \nvertices and adds some functions to get and set the individual vertices. Class \nTriangle overrides Draw() as we’d expect, and for illustrative purposes it \nalso overrides SetId(). The memory image generated by the Triangle class \nis shown in Figure 3.18.\nclass Shape\n{\npublic:\nvirtual void  SetId(int id) { m_id = id; }\n \nint  \n \n \nGetId() const { return m_id; }\nvirtual void  Draw() = 0; // pure virtual – no impl.\nprivate:\n int \n \n  m_id;\n};\nShape::m_id\nCircle::m_center\nCircle::m_radius\nvtable pointer\npointer to SetId()\npointer to Draw ()\n+0x00\n+0x04\n+0x08\n+0x14\npShape1\nInstance of Circle\nCircle’s Virtual Table\nCircle::Draw()\n{\n    // code to draw a Circle\n}\nShape::SetId(int id )\n{\n    m_id = id;\n}\nFigure 3.17.  pShape1 points to an instance of class Circle.\n\n\n127 \nclass Circle : public Shape\n{\npublic:\n    void   \nSetCenter(const Vector3& c) { m_center=c; }\n \nVector3  GetCenter() const { return m_center; }\n \nvoid   \nSetRadius(float r) { m_radius = r; }\n \nfloat   \nGetRadius() const { return m_radius; }\nvirtual void Draw()\n {\n \n \n// code to draw a circle\n }\nprivate:\n Vector3 \n  m_center;\n float \n  m_radius;\n};\nclass Triangle : public Shape\n{\npublic:\n \nvoid \n \nSetVertex(int i, const Vector3& v);\n \nVector3 \nGetVertex(int i) const { return m_vtx[i]; }\nvirtual void Draw()\n {\n \n \n// code to draw a triangle\n }\nvirtual void SetId(int id)\n {\n  Shape::SetId(id);\nFigure 3.18.  pShape2 points to an instance of class Triangle.\nShape::m_id\nTriangle::m_vtx[0]\nTriangle::m_vtx[1]\nvtable pointer\npointer to SetId()\npointer to Draw ()\n+0x00\n+0x04\n+0x08\n+0x14\npShape2\nInstance of Triangle\nTriangle’s Virtual Table\nTriangle ::Draw()\n{\n    // code to draw a Triangle\n}\nTriangle ::SetId(int id)\n{\n    Shape ::SetId(id);\n    // do additional work\n    // specific to Triangles\n}\nTriangle::m_vtx[2]\n+0x20\n3.2. Data, Code, and Memory in C/C++\n\n\n128 \n3. Fundamentals of Software Engineering for Games\n \n \n// do additional work specific to Triangles...\n }\nprivate:\n Vector3 \n  m_vtx[3];\n};\n// -----------------------------\nvoid main(int, char**)\n{\nShape* pShape1 = new Circle;\n \nShape* pShape2 = new Triangle;\n \n// ...\n pShape1->Draw();\n pShape2->Draw();\n \n// ...\n}\n3.3. Catching and Handling Errors\nThere are a number of ways to catch and handle error conditions in a game \nengine. As a game programmer, it’s important to understand these diﬀ erent \nmechanisms, their pros and cons, and when to use each one.\n3.3.1. \nTypes of Errors\nIn any soft ware project there are two basic kinds of  error conditions: user er-\nrors and programmer errors. A user error occurs when the user of the program \ndoes something incorrect, such as typing an invalid input, att empting to open \na ﬁ le that does not exist, etc. A programmer error is the result of a bug in the \ncode itself. Although it may be triggered by something the user has done, the \nessence of a programmer error is that the problem could have been avoided if \nthe programmer had not made a mistake, and the user has a reasonable expec-\ntation that the program should have handled the situation gracefully.\nOf course, the deﬁ nition of “user” changes depending on context. In the \ncontext of a game project, user errors can be roughly divided into two catego-\nries: errors caused by the person playing the game and errors caused by the \npeople who are making the game during development. It is important to keep \ntrack of which type of user is aﬀ ected by a particular error and handle the er-\nror appropriately.\n\n\n129 \nThere’s actually a third kind of user—the other programmers on your \nteam. (And if you are writing a piece of game middleware soft ware, like \nHavok or OpenGL, this third category extends to other programmers all over \nthe world who are using your library.) This is where the line between user er-\nrors and programmer errors gets blurry. Let’s imagine that programmer A writes \na function f(), and programmer B tries to call it. If B calls f() with invalid \narguments (e.g., a NULL pointer, or an out-of-range array index), then this \ncould be seen as a user error by programmer A, but it would be a program-\nmer error from B’s point of view. (Of course, one can also argue that program-\nmer A should have anticipated the passing of invalid arguments and should \nhave handled them gracefully, so the problem really is a programmer error, \non A’s part.) The key thing to remember here is that the line between user and \nprogrammer can shift  depending on context—it is rarely a black-and-white \ndistinction.\n3.3.2. Handling Errors\nWhen handling errors, the requirements diﬀ er signiﬁ cantly between the two \ntypes. It is best to handle user errors as gracefully as possible, displaying some \nhelpful information to the user and then allowing him or her to continue \nworking—or in the case of a game, to continue playing. Programmer errors, \non the other hand, should not be handled with a graceful “inform and contin-\nue” policy. Instead, it is usually best to halt the program and provide detailed \nlow-level debugging information, so that a programmer can quickly identify \nand ﬁ x the problem. In an ideal world, all programmer errors would be caught \nand ﬁ xed before the soft ware ships to the public.\n3.3.2.1. Handling Player Errors\nWhen the “user” is the person playing your game, errors should obviously be \nhandled within the context of gameplay. For example, if the player att empts to \nreload a weapon when no ammo is available, an audio cue and/or an anima-\ntion can indicate this problem to the player without taking him or her “out of \nthe game.” \n3.3.2.2. Handling Developer Errors\nWhen the “user” is someone who is making the game, such as an artist, ani-\nmator or game designer, errors may be caused by an invalid asset of some sort. \nFor example, an animation might be associated with the wrong skeleton, or a \ntexture might be the wrong size, or an audio ﬁ le might have been sampled at \nan unsupported sample rate. For these kinds of developer errors, there are two \ncompeting camps of thought.\n3.3. Catching and Handling Errors\n\n\n130 \n3. Fundamentals of Software Engineering for Games\nOn the one hand, it seems important to prevent bad game assets from \npersisting for too long. A game typically contains literally thousands of assets, \nand a problem asset might get “lost,” in which case one risks the possibility of \nthe bad asset surviving all the way into the ﬁ nal shipping game. If one takes \nthis point of view to an extreme, then the best way to handle bad game assets \nis to prevent the entire game from running whenever even a single problem-\natic asset is encountered. This is certainly a strong incentive for the developer \nwho created the invalid asset to remove or ﬁ x it immediately.\nOn the other hand, game development is a messy and iterative process, \nand generating “perfect” assets the ﬁ rst time around is rare indeed. By this \nline of thought, a game engine should be robust to almost any kind of problem \nimaginable, so that work can continue even in the face of totally invalid game \nasset data. But this too is not ideal, because the game engine would become \nbloated with error-catching and error-handling code that won’t be needed \nonce the development pace sett les down and the game ships. And the prob-\nability of shipping the product with “bad” assets becomes too high.\nIn my experience, the best approach is to ﬁ nd a middle ground between \nthese two extremes. When a developer error occurs, I like to make the error \nobvious and then allow the team to continue to work in the presence of the \nproblem. It is extremely costly to prevent all the other developers on the team \nfrom working, just because one developer tried to add an invalid asset to the \ngame. A game studio pays its employees well, and when multiple team mem-\nbers experience downtime, the costs are multiplied by the number of people \nwho are prevented from working. Of course, we should only handle errors in \nthis way when it is practical to do so, without spending inordinate amounts of \nengineering time, or bloating the code.\nAs an example, let’s suppose that a particular mesh cannot be loaded. In \nmy view, it’s best to draw a big red box in the game world at the places that \nmesh would have been located, perhaps with a text string hovering over each \none that reads, “Mesh blah-dee-blah failed to load.” This is superior to printing \nan easy-to-miss message to an error log. And it’s far bett er than just crashing \nthe game, because then no one will be able to work until that one mesh refer-\nence has been repaired. Of course, for particularly egregious problems it’s ﬁ ne \nto just spew an error message and crash. There’s no silver bullet for all kinds \nof problems, and your judgment about what type of error handling approach \nto apply to a given situation will improve with experience.\n3.3.2.3. Handling Programmer Errors\nThe best way to detect and handle programmer errors (a.k.a. bugs) is oft en \nto embed error-checking code into your source code and arrange for failed \n\n\n131 \nerror checks to halt the program. Such a mechanism is known as an assertion \nsystem; we’ll investigate assertions in detail in Section 3.3.3.3. Of course, as we \nsaid above, one programmer’s user error is another programmer’s bug; hence, \nassertions are not always the right way to handle every programmer error. \nMaking a judicious choice between an assertion and a more graceful error \nhandling technique is a skill that one develops over time.\n3.3.3. Implementation of Error Detection and Handling\nWe’ve looked at some philosophical approaches to handling errors. Now let’s \nturn our att ention to the choices we have as programmers when it comes to \nimplementing error detection and handling code.\n3.3.3.1. \nError Return Codes\nA common approach to handling errors is to return some kind of failure code \nfrom the function in which the problem is ﬁ rst detected. This could be a Bool-\nean value indicating success or failure or it could be an “impossible” value, \none that is outside the range of normally returned results. For example, a \nfunction that returns a positive integer or ﬂ oating-point value could return a \nnegative value to indicate that an error occurred. Even bett er than a Boolean or \nan “impossible” return value, the function could be designed to return an enu-\nmerated value to indicate success or failure. This clearly separates the error \ncode from the output(s) of the function, and the exact nature of the problem \ncan be indicated on failure (e.g., enum Error { kSuccess, kAssetNot-\nFound, kInvalidRange, ... };).\nThe calling function should intercept error return codes and act appro-\npriately. It might handle the error immediately. Or it might work around the \nproblem, complete its own execution, and then pass the error code on to what-\never function called it.\n3.3.3.2. Exceptions\nError return codes are a simple and reliable way to communicate and respond \nto error conditions. However, error return codes have their drawbacks. Per-\nhaps the biggest problem with error return codes is that the function that \ndetects an error may be totally unrelated to the function that is capable of \nhandling the problem. In the worst-case scenario, a function that is 40 calls \ndeep in the call stack might detect a problem that can only be handled by \nthe top-level game loop, or by main(). In this scenario, every one of the 40 \nfunctions on the call stack would need to be writt en so that it can pass an \nappropriate error code all the way back up to the top-level error-handling \nfunction.\n3.3. Catching and Handling Errors\n\n\n132 \n3. Fundamentals of Software Engineering for Games\nOne way to solve this problem is to throw an exception.  Structured excep-\ntion handling (SEH) is a very powerful feature of C++. It allows the function \nthat detects a problem to communicate the error to the rest of the code with-\nout knowing anything about which function might handle the error. When an \nexception is thrown, relevant information about the error is placed into a data \nobject of the programmer’s choice known as an exception object. The call stack \nis then automatically unwound, in search of a calling function that wrapped \nits call in a try-catch block. If a try-catch block is found, the exception object \nis matched against all possible catch blocks and if a match is found, the cor-\nresponding catch block’s code is executed. The destructors of any automatic \nvariables are called as needed during the stack unwinding.\nThe ability to separate error detection from error handling in such a clean \nway is certainly att ractive, and exception handling is an excellent choice for \nsome soft ware projects. However, SEH adds a lot of overhead to the program. \nEvery stack frame must be augmented to contain additional information re-\nquired by the stack unwinding process. Also, the stack unwind is usually very \nslow—on the order of two to three times more expensive than simply return-\ning from the function. Also, if even one function in your program (or a library \nthat your program links with) uses SEH, your entire program must use SEH. \nThe compiler can’t know which functions might be above you on the call stack \nwhen you throw an exception.\nTherefore, there’s a prett y strong argument for turning oﬀ  structured ex-\nception handling in your game engine altogether. This is the approach em-\nployed at Naughty Dog and also on most of the projects I’ve worked on at \nElectronic Arts and Midway. Console game engines should probably never \nuse SEH, because of a console’s limited memory and processing bandwidth. \nHowever, a game engine that is intended to be run on a personal computer \nmight be able to use SEH without any problems.\nThere are many interesting articles on this topic on the web. Here are links \nto a few of them:\nz htt p://www.joelonsoft ware.com/items/2003/10/13.html\nz htt p://www.nedbatchelder.com/text/exceptions-vs-status.html\nz htt p://www.joelonsoft ware.com/items/2003/10/15.html\n3.3.3.3. Assertions\nAn assertion is a line of code that checks an expression. If the expression evalu-\nates to true, nothing happens. But if the expression evaluates to false, the pro-\ngram is stopped, a message is printed, and the debugger is invoked if possible. \nSteve Maguire provides an in-depth discussion of assertions in his must-read \nbook, Writing Solid Code [30].\n\n\n133 \nAssertions check a programmer’s assumptions. They act like land mines \nfor bugs. They check the code when it is ﬁ rst writt en to ensure that it is func-\ntioning properly. They also ensure that the original assumptions continue to \nhold for long periods of time, even when the code around them is constantly \nchanging and evolving. For example, if a programmer changes code that \nused to work, but accidentally violates its original assumptions, they’ll hit \nthe land mine. This immediately informs the programmer of the problem \nand permits him or her to rectify the situation with minimum fuss. Without \nassertions , bugs have a tendency to “hide out” and manifest themselves later \nin ways that are diﬃ  cult and time-consuming to track down. But with as-\nsertions embedded in the code, bugs announce themselves the moment they \nare introduced—which is usually the best moment to ﬁ x the problem, while \nthe code changes that caused the problem are fresh in the programmer’s \nmind.\nAssertions are implemented as a #define macro, which means that the \nassertion checks can be stripped out of the code if desired, by simply changing \nthe #define. The cost of the assertion checks can usually be tolerated during \ndevelopment, but stripping out the assertions prior to shipping the game can \nbuy back that litt le bit of crucial performance if necessary.\nAssertion Implementation\nAssertions are usually implemented via a combination of a #defined macro \nthat evaluates to an if/else clause, a function that is called when the asser-\ntion fails (the expression evaluates to false), and a bit of assembly code that \nhalts the program and breaks into the debugger when one is att ached. Here’s \na typical implementation:\n#if ASSERTIONS_ENABLED\n \n// define some inline assembly that causes a break     \n \n// into the debugger – this will be different on each  \n \n// target CPU\n \n#define debugBreak() asm { int 3 }\n \n// check the expression and fail if it is false\n \n#define ASSERT(expr) \\\n \n \nif (expr) { } \\\n  else \n\\\n  { \n\\\n   reportAssertionFailure(#expr, \n\\\n          \n__FILE__, \\\n          \n__LINE__); \\\n   debugBreak(); \n\\\n  }\n3.3. Catching and Handling Errors\n\n\n134 \n3. Fundamentals of Software Engineering for Games\n#else\n \n#define ASSERT(expr) \n \n// evaluates to nothing\n#endif\nLet’s break down this deﬁ nition so we can see how it works:\nz The outer #if/#else/#endif is used to strip assertions from the \ncode base. When ASSERTIONS_ENABLED is nonzero, the ASSERT()\nmacro is deﬁ ned in its fully glory, and all assertion checks in the code \nwill be included in the program. But when assertions are turned oﬀ , \nASSERT(expr) evaluates to nothing, and all instances of it in the code \nare eﬀ ectively removed.\nz The debugBreak() macro evaluates to whatever assembly-language \ninstructions are required in order to cause the program to halt and the \ndebugger to take charge (if one is connected). This diﬀ ers from CPU to \nCPU, but it is usually a single assembly instruction.\nz The ASSERT() macro itself is deﬁ ned using a full if/else statement (as \nopposed to a lone if). This is done so that the macro can be used in any \ncontext, even within other unbracketed if/else statements.\nHere’s an example of what would happen if ASSERT() were deﬁ ned \nusing a solitary if:\n \n#define ASSERT(expr)  if (!(expr)) debugBreak()\n \nvoid f()\n {\n \n \nif (a < 5)\n   ASSERT(a \n>= 0);\n  else\n   doSomething(a);\n }\nThis expands to the following incorrect code:\n \nvoid f()\n {\n \n \nif (a < 5)\n \n \n \nif (!(a >= 0))\n    debugBreak();\n \n \n \nelse \n// Oops! Bound to the wrong if()!\n    doSomething(a);\n }\nz The else clause of an ASSERT() macro does two things. It displays \nsome kind of message to the programmer indicating what went wrong, \n\n\n135 \nand then it breaks into the debugger. Notice the use of #expr as the ﬁ rst \nargument to the message display function. The pound (#) preprocessor \noperator causes the expression expr to be turned into a string, thereby \nallowing it to be printed out as part of the assertion failure message.\nz Notice also the use of __FILE__ and __LINE__. These compiler-deﬁ ned \nmacros magically contain the .cpp ﬁ le name and line number of the line \nof code on which they appear. By passing them into our message dis-\nplay function, we can print the exact location of the problem.\nI highly recommend the use of assertions in your code. However, it’s im-\nportant to be aware of their performance cost. You may want to consider de-\nﬁ ning two kinds of assertion macros. The regular ASSERT() macro can be left  \nactive in all builds, so that errors are easily caught even when not running \nin debug mode. A second assertion macro, perhaps called SLOW_ASSERT(), \ncould be activated only in debug builds. This macro could then be used in \nplaces where the cost of assertion checking is too high to permit inclusion \nin release builds. Obviously SLOW_ASSERT() is of lower utility, because it is \nstripped out of the version of the game that your testers play every day. But at \nleast these assertions become active when programmers are debugging their \ncode.\nIt’s also extremely important to use assertions properly. They should be \nused to catch bugs in the program itself—never to catch user errors. Also, as-\nsertions should always cause the entire game to halt when they fail. It’s usu-\nally a bad idea to allow assertions to be skipped by testers, artists, designers, \nand other non-engineers. (This is a bit like the boy who cried wolf: if assertions \ncan be skipped, then they cease to have any signiﬁ cance, rendering them inef-\nfective.) In other words, assertions should only be used to catch fatal errors. If \nit’s OK to continue past an assertion, then it’s probably bett er to notify the user \nof the error in some other way, such as with an on-screen message, or some \nugly bright-orange 3D graphics. For a great discussion on the proper usage of \nassertions, see htt p://www.wholesalealgorithms.com/blog9.\n3.3. Catching and Handling Errors\n\n\n137\n4\n3D Math for Games\nA game is a mathematical model of a virtual world simulated in real-time on \na computer of some kind. Therefore, mathematics pervades everything we do \nin the game industry. Game programmers make use of virtually all branches \nof mathematics, from trigonometry to algebra to statistics to calculus. How-\never, by far the most prevalent kind of mathematics you’ll be doing as a game \nprogrammer is 3D vector and matrix math (i.e., 3D linear algebra ).\nEven this one branch of mathematics is very broad and very deep, so we \ncannot hope to cover it in any great depth in a single chapter. Instead, I will \natt empt to provide an overview of the mathematical tools needed by a typical \ngame programmer. Along the way, I’ll oﬀ er some tips and tricks which should \nhelp you keep all of the rather confusing concepts and rules straight in your \nhead. For an excellent in-depth coverage of 3D math for games, I highly rec-\nommend Eric Lengyel’s book on the topic [28].\n4.1. \nSolving 3D Problems in 2D\nMany of the mathematical operations we’re going to learn about in the follow-\ning chapter work equally well in 2D and 3D. This is very good news, because \nit means you can sometimes solve a 3D vector problem by thinking and draw-\ning pictures in 2D (which is considerably easier to do!) Sadly, this equivalence \n",
      "page_number": 142,
      "chapter_number": 8,
      "summary": "This chapter covers segment 8 (pages 142-159). Key topics include errors, game, and assertion. Covers function. }\nz as a global, ﬁ le-static or function-static;\n \nFoo gFoo;\n \nstatic Foo sFoo;\n \nvoid someFunction()\n {\n \n \nstatic Foo sLocalFoo;\n  //.",
      "keywords": [
        "error",
        "draw",
        "Error Return Codes",
        "game",
        "Code",
        "Handling Errors",
        "Foo",
        "programmer errors",
        "function",
        "memory",
        "data",
        "Triangle",
        "virtual void Draw",
        "Games struct Foo",
        "programmer"
      ],
      "concepts": [
        "errors",
        "game",
        "assertion",
        "assertions",
        "assert",
        "classes",
        "handling",
        "code",
        "data",
        "alignment"
      ],
      "similar_chapters": [
        {
          "book": "makinggames",
          "chapter": 8,
          "title": "Segment 8 (pages 60-68)",
          "relevance_score": 0.73,
          "method": "sentence_transformers"
        },
        {
          "book": "makinggames",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.67,
          "method": "sentence_transformers"
        },
        {
          "book": "C++ Templates_ The Complete Guide",
          "chapter": 26,
          "title": "Segment 26 (pages 815-849)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 16,
          "title": "Segment 16 (pages 308-330)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 54,
          "title": "Segment 54 (pages 1084-1102)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 160-181)",
      "start_page": 160,
      "end_page": 181,
      "detection_method": "synthetic",
      "content": "138 \n4. 3D Math for Games\nFigure 4.1. A point rep-\nresented in Cartesian \ncoordinates.\nPy\nx\nz\ny\nPz\nPx\nP\nPh\nP\nh\nr\nPr\nθ\nPθ\nFigure \n4.2. \nA \npoint represent-\ned in cylindrical \ncoordinates.\nbetween 2D and 3D does not hold all the time. Some operations, like the cross \nproduct, are only deﬁ ned in 3D, and some problems only make sense when all \nthree dimensions are considered. Nonetheless, it almost never hurts to start by \nthinking about a simpliﬁ ed two-dimensional version of the problem at hand. \nOnce you understand the solution in 2D, you can think about how the prob-\nlem extends into three dimensions. In some cases, you’ll happily discover that \nyour 2D result works in 3D as well. In others, you’ll be able to ﬁ nd a coor-\ndinate system in which the problem really is two-dimensional. In this book, \nwe’ll employ two-dimensional diagrams wherever the distinction between 2D \nand 3D is not relevant.\n4.2. Points and Vectors\nThe majority of modern 3D games are made up of three-dimensional objects \nin a virtual world. A game engine needs to keep track of the positions, orien-\ntations, and scales of all these objects, animate them in the game world, and \ntransform them into screen space so they can be rendered on screen. In games, \n3D objects are almost always made up of triangles, the vertices of which are \nrepresented by points. So before we learn how to represent whole objects in \na game engine, let’s ﬁ rst take a look the point and its closely related cousin, \nthe vector.\n4.2.1. Points and Cartesian Coordinates\nTechnically speaking, a point is a location in n-dimensional space. (In games, \nn is usually equal to 2 or 3.) The Cartesian coordinate system is by far the \nmost common coordinate system employed by game programmers. It uses \ntwo or three mutually perpendicular axes to specify a position in 2D or 3D \nspace. So a point P is represented by a pair or triple of real numbers, (Px , Py) \nor (Px , Py , Pz).\nOf course, the Cartesian coordinate system is not our only choice. Some \nother common systems include:\nz Cylindrical coordinates . This system employs a vertical “height” axis h, a \nradial axis r emanating out from the vertical, and a yaw angle theta (θ). \nIn cylindrical coordinates, a point P is represented by the triple of num-\nbers (Ph , Pr , Pθ). This is illustrated in Figure 4.2.\nz Spherical coordinates . This system employs a pitch angle phi (φ), a yaw \nangle theta (θ), and a radial measurement r. Points are therefore rep-\nresented by the triple of numbers (Pr , Pφ , Pθ). This is illustrated in Fig-\nure 4.3.\n\n\n139 \n4.2. Points and Vectors\nFigure \n4.3. A point \nrepresented in spherical \ncoordinates.\nr\nθ\nφ\nPr\nP\nPθ\nPφ\nRight-Handed\nx\nz\ny\nLeft-Handed\nx\ny\nz\nFigure 4.4. Left- and right-handed Cartesian coordinate systems.\nCartesian coordinates are by far the most widely used coordinate system \nin game programming. However, always remember to select the coordinate \nsystem that best maps to the problem at hand. For example, in the game Crank \nthe Weasel by Midway Home Entertainment, the main character Crank runs \naround an art-deco city picking up loot. I wanted to make the items of loot \nswirl around Crank’s body in a spiral, gett ing closer and closer to him until \nthey disappeared. I represented the position of the loot in cylindrical coor-\ndinates relative to the Crank character’s current position. To implement the \nspiral animation, I simply gave the loot a constant angular speed in θ, a small \nconstant linear speed inward along its radial axis r, and a very slight constant \nlinear speed upward along the h-axis so the loot would gradually rise up to \nthe level of Crank’s pants pockets. This extremely simple animation looked \ngreat, and it was much easier to model using cylindrical coordinates than it \nwould have been using a Cartesian system.\n4.2.2. Left-Handed vs. Right-Handed Coordinate Systems\nIn three-dimensional Cartesian coordinates, we have two choices when ar-\nranging our three mutually perpendicular axes: right-handed (RH) and left -\nhanded (LH). In a right-handed coordinate system, when you curl the ﬁ ngers \nof your right hand around the z-axis with the thumb pointing toward positive \nz coordinates, your ﬁ ngers point from the x-axis toward the y-axis. In a left -\nhanded coordinate system the same thing is true using your left  hand.\nThe only diﬀ erence between a left -handed coordinate system and a right-\nhanded coordinate system is the direction in which one of the three axes is \npointing. For example, if the y-axis points upward and x points to the right, \nthen z comes toward us (out of the page) in a right-handed system, and away \nfrom us (into the page) in a left -handed system. Left - and right-handed Carte-\nsian coordinate systems are depicted in Figure 4.4.\n\n\n140 \n4. 3D Math for Games\nIt is easy to convert from LH to RH coordinates and vice-versa. We sim-\nply ﬂ ip the direction of any one axis, leaving the other two axes alone. It’s \nimportant to remember that the rules of mathematics do not change between \nLH and RH coordinate systems. Only our interpretation of the numbers—our \nmental image of how the numbers map into 3D space—changes. Left -handed \nand right-handed conventions apply to visualization only, not to the underly-\ning mathematics. (Actually, handedness does matt er when dealing with cross \nproducts in physical simulations, but we can safely ignore these subtleties \nfor the majority of our game programming tasks. For more information, see \nhtt p://en.wikipedia.org/wiki/Pseudovector .)\nThe mapping between the numerical representation and the visual repre-\nsentation is entirely up to us as mathematicians and programmers. We could \nchoose to have the y-axis pointing up, with z forward and x to the left  (RH) \nor right (LH). Or we could choose to have the z-axis point up. Or the x-axis \ncould point up instead—or down. All that matt ers is that we decide upon a \nmapping, and then stick with it consistently.\nThat being said, some conventions do tend to work bett er than others for \ncertain applications. For example, 3D graphics programmers typically work \nwith a left -handed coordinate system, with the y-axis pointing up, x to the \nright and positive z pointing away from the viewer (i.e., in the direction the \nvirtual camera is pointing). When 3D graphics are rendered onto a 2D screen \nusing this particular coordinate system, increasing z-coordinates correspond \nto increasing depth into the scene (i.e., increasing distance away from the vir-\ntual camera). As we will see in subsequent chapters, this is exactly what is \nrequired when using a z-buﬀ ering scheme for depth occlusion.\n4.2.3. Vectors\nA vector is a quantity that has both a magnitude and a direction in n-dimensional \nspace. A vector can be visualized as a directed line segment extending from a \npoint called the tail to a point called the head. Contrast this to a scalar (i.e., an \nordinary real-valued number), which represents a magnitude but has no di-\nrection. Usually scalars are writt en in italics (e.g., v) while vectors are writt en \nin boldface (e.g., v).\nA 3D vector can be represented by a triple of scalars (x, y, z), just as a point \ncan be. The distinction between points and vectors is actually quite subtle. \nTechnically, a vector is just an oﬀ set relative to some known point. A vector \ncan be moved anywhere in 3D space—as long as its magnitude and direction \ndon’t change, it is the same vector.\nA vector can be used to represent a point, provided that we ﬁ x the tail of \nthe vector to the origin of our coordinate system. Such a vector is sometimes \n\n\n141 \n4.2. Points and Vectors\ncalled a position vector or radius vector. For our purposes, we can interpret any \ntriple of scalars as either a point or a vector, provided that we remember that \na position vector is constrained such that its tail remains at the origin of the \nchosen coordinate system. This implies that points and vectors are treated in \nsubtly diﬀ erent ways mathematically. One might say that points are absolute, \nwhile vectors are relative.\nThe vast majority of game programmers use the term “vector” to refer \nboth to points (position vectors) and to vectors in the strict linear algebra sense \n(purely directional vectors). Most 3D math libraries also use the term “vector” \nin this way. In this book, we’ll use the term “direction vector ” or just “direc-\ntion” when the distinction is important. Be careful to always keep the diﬀ er-\nence between points and directions clear in your mind (even if your math \nlibrary doesn’t). As we’ll see in Section 4.3.6.1, directions need to be treated \ndiﬀ erently from  points when converting them into homogeneous coordinates \nfor manipulation with 4 × 4 matrices, so gett ing the two types of vector mixed \nup can and will lead to bugs in your code.\n4.2.3.1. Cartesian Basis Vectors\nIt is oft en useful to deﬁ ne three orthogonal unit vectors (i.e., vectors that are mu-\ntually perpendicular and each with a length equal to one), corresponding to \nthe three principal Cartesian axes. The unit vector along the x-axis is typically \ncalled i, the y-axis unit vector is called j, and the z-axis unit vector is called k. \nThe vectors i, j, and k are sometimes called Cartesian basis ve ctors .\nAny point or vector can be expressed as a sum of scalars (real numbers) \nmultiplied by these unit basis vectors. For example,\n \n(5, 3, –2) = 5i + 3j – 2k. \n4.2.4. Vector Operations\nMost of the mathematical operations that you can perform on scalars can be \napplied to vectors as well. There are also some new operations that apply only \nto vectors.\n4.2.4.1. Multiplication by a Scalar\nMultiplication of a vector a by a scalar s is accomplished by multiplying the \nindividual components of a by s:\n \nsa = ( sax , say , saz ). \nMultiplication by a scalar has the eﬀ ect of scaling the magnitude of the \nvector, while leaving its direction unchanged, as shown in Figure 4.5. Multi-\nplication by –1 ﬂ ips the direction of the vector (the head becomes the tail and \nvice-versa).\n\n\n142 \n4. 3D Math for Games\na + b\n–b\nb\na\na – b\nFigure 4.6. Vector addition and subtraction.\nThe scale factor can be diﬀ erent along each axis. We call this nonuniform \nscale , and it can be represented as the component-wise product of a scaling vector \ns and the vector in question, which we’ll denote with the ⊗ operator. Techni-\ncally speaking, this special kind of product between two vectors is known as \nthe Hadamard product . It is rarely used in the game industry—in fact, nonuni-\nform scaling is one of its only commonplace uses in games:\n \n \n(4.1)\nAs we’ll see in Section 4.3.7.3, a scaling vector s is really just a compact way to \nrepresent a 3 × 3 diagonal scaling matrix S. So another way to write Equation \n(4.1) is as follows:\n \n \n4.2.4.2. Addition and Subtraction\nThe addition of two vectors a and b is deﬁ ned as the vector whose components \nare the sums of the components of a and b. This can be visualized by placing the \nhead of vector a onto the tail of vector b—the sum is then the vector from the \ntail of a to the head of b:\n \na + b = [ (ax + bx), (ay + by), (az + bz) ]. \nVector subtraction a – b is nothing more than addition of a and –b (i.e., the \nresult of scaling b by –1, which ﬂ ips it around). This corresponds to the vector \nv\n2v\nv\nFigure 4.5. Multiplication of a vector by the scalar 2.\n \n(\n,\n,\n).\nx x\ny\ny\nz z\ns a\ns a\ns a\n⊗=\ns\na\n \n0\n0\n[\n] 0\n0\n[\n].\n0\n0\nx\nx\ny\nz\ny\nx x\ny y\nz z\nz\ns\na\na\na\ns\ns a\ns a\ns a\ns\n⎡\n⎤\n⎢\n⎥\n=\n=\n⎢\n⎥\n⎢\n⎥\n⎣\n⎦\naS\n\n\n143 \n4.2. Points and Vectors\na\nax\nay\n|a|\nFigure 4.7. Magnitude of a vector (shown in 2D for ease of illustration).\nwhose components are the diﬀ erence between the components of a and the \ncomponents of b:\n \na – b = [ (ax – bx), (ay – by), (az – bz) ]. \nVector addition and subtraction are depicted in Figure 4.6.\nAdding and Subtracting Points and Directions\n You can add and subtract direction vectors freely. However, technically speak-\ning, points cannot be added to one another—you can only add a direction \nvector to a point, the result of which is another point. Likewise, you can take \nthe diﬀ erence between two points, resulting in a direction vector. These opera-\ntions are summarized below:\nz direction + direction = direction\nz direction – direction = direction\nz point + direction = point\nz point – point = direction\nz point + point = nonsense (don’t do it!)\n4.2.4.3. Magnitude\nThe magnitude of a vector is a scalar representing the length of the vector as \nit would be measured in 2D or 3D space. It is denoted by placing vertical bars \naround the vector’s boldface symbol. We can use the Pythagorean theorem to \ncalculate a vector’s magnitude, as shown in Figure 4.7:\n2\n2\n2 .\nx\ny\nz\na\na\na\n=\n+\n+\na\n4.2.4.4. Vector Operations in Action\nBelieve it or not, we can already solve all sorts of real-world game problems \ngiven just the vector operations we’ve learned thus far. When trying to solve a \nproblem, we can use operations like addition, subtraction, scaling, and mag-\nnitude to generate new data out of the things we already know. For example, \n\n\n144 \n4. 3D Math for Games\nif we have the current position vector of an A.I. character P1, and a vector v \nrepresenting her current velocity , we can ﬁ nd her position on the next frame \nP2 by scaling the velocity vector by the frame time interval Δt, and then adding \nit to the current position. As shown in Figure 4.8, the resulting vector equation \nis P2 = P1 + (Δt)v. (This is known as explicit Euler integration —it’s actually only \nvalid when the velocity is constant, but you get the idea.)\nAs another example, let’s say we have two spheres, and we want to know \nwhether they intersect. Given that we know the center points of the two \nspheres, C1 and C2, we can ﬁ nd a direction vector between them by simply \nsubtracting the points, d = C2 – C1. The magnitude of this vector d = |d| de-\ntermines how far apart the spheres’ centers are. If this distance is less than the \nsum of the spheres’ radii, they are intersecting; otherwise they’re not. This is \nshown in Figure 4.9.\nSquare roots are expensive to calculate on most computers, so game \nprogrammers should always use the squared magnitude whenever it is valid to do \nso:\n \n \nUsing the squared magnitude is valid when comparing the relative lengths of \ntwo vectors (“is vector a longer than vector b?”), or when comparing a vector’s \nmagnitude to some other (squared) scalar quantity. So in our sphere-sphere \nintersection test, we should calculate d2 =       and compare this to the squared \nsum of the radii, (r1 + r2)2 for maximum speed. When writing high-perfor-\nmance soft ware, never take a square root when you don’t have to!\nvΔt\n1\nP\n2P\ny\nx\nFigure 4.8. Simple vec-\ntor addition can be used \nto ﬁ nd a character’s po-\nsition in the next frame, \ngiven her position and \nvelocity ln the current \nframe.\n1\nC\n2\nC\ny\nx\n1r\n2r\nd\n2\nC  –\n1\nC\nFigure 4.9.  A sphere-sphere intersection test involves only vector subtraction, vector mag-\nnitude, and ﬂ oating-point comparison operations.\n(\n) \n2\n2\n2\n2 .\nx\ny\nz\na\na\na\n=\n+\n+\na\n4.2.4.5. Normalization and Unit Vectors\nA unit vector is a vector with a magnitude (length) of one. Unit vectors are very \nuseful in 3D mathematics and game programming, for reasons we’ll see below.\n2\nd\n\n\n145 \n4.2. Points and Vectors\nGiven an arbitrary vector v of length v =     , we can convert it to a unit \nvector u that points in the same direction as v, but has unit length. To do this, \nwe simply multiply v by the reciprocal of its magnitude. We call this normal-\nization :\n \n \n4.2.4.6. Normal Vectors\nA vector is said to be normal to a surface if it is perpendicular to that surface. \nNormal vectors are highly useful in games and computer graphics. For ex-\nample, a plane can be deﬁ ned by a point and a normal vector. And in 3D \ngraphics, lighting calculations make heavy use of normal vectors to deﬁ ne \nthe direction of surfaces relative to the direction of the light rays impinging \nupon them.\nNormal vectors are usually of unit length, but they do not need to be. Be \ncareful not to confuse the term “normalization” with the term “normal vec-\ntor.” A normalized vector is any vector of unit length. A normal vector is any \nvector that is perpendicular to a surface, whether or not it is of unit length.\n4.2.4.7. Dot Product and Projection\nVectors can be multiplied, but unlike scalars there are a number of diﬀ erent \nkinds of vector multiplication. In game programming, we most oft en work \nwith the following two kinds of multiplication:\nz the dot product (a.k.a. scalar product or inner product), and\nz the cross product (a.k.a. vector product or outer product).\nThe dot product of two vectors yields a scalar; it is deﬁ ned by adding the \nproducts of the individual components of the two vectors:\n \n (a scalar). \nThe dot product can also be writt en as the product of the magnitudes of the \ntwo vectors and the cosine of the angle between them:\n \n \nThe dot product is commutative (i.e., the order of the two vectors can be \nreversed) and distributive over addition:\n \n \n \n \n1 .\nv\n=\n=\nv\nu\nv\nv\nx x\ny y\nz z\na b\na b\na b\nd\n⋅=\n+\n+\n=\na b\nv\n \n \n \ncos( ).\n⋅=\nθ\na b\na b\n ;\n⋅= ⋅\na b\nb a\n \n(\n)\n.\n⋅\n+\n= ⋅+ ⋅\na\nb\nc\na b\na c\n\n\n146 \n4. 3D Math for Games\nAnd the dot product combines with scalar multiplication as follows:\n \n \nVector Projection\nIf u is a unit vector (     = 1), then the dot product (a ⋅ u) represents the length \nof the projection of vector a onto the inﬁ nite line deﬁ ned by the direction of \nu, as shown in Figure 4.10. This projection concept works equally well in 2D \nor 3D and is highly useful for solving a wide variety of three-dimensional \nproblems.\nFigure 4.10. Vector projection using the dot product.\n \n(\n).\ns\ns\ns\n⋅= ⋅\n=\n⋅\na b\na\nb\na b\nu\nMagnitude as a Dot Product\nThe squared magnitude of a vector can be found by taking the dot product of \nthat vector with itself. Its magnitude is then easily found by taking the square \nroot:\n \n \nThis works because the cosine of zero degrees is 1, so all that is left  is\n \nDot Product Tests\nDot products are great for testing if two vectors are collinear or perpendicular, \nor whether they point in roughly the same or roughly opposite directions. For \nany two arbitrary vectors a and b, game programmers oft en use the following \ntests, as shown in Figure 4.11:\nz Collinear. (a ⋅ b) =         = ab (i.e., the angle between them is exactly 0 \ndegrees—this dot product equals +1 when a and b are unit vectors).\nz Collinear but opposite.  (a ⋅ b) = –ab (i.e., the angle between them is 180 \ndegrees—this dot product equals –1 when a and b are unit vectors).\n2\n;\n.\n= ⋅\n=\n⋅\na\na a\na\na a\n \n2 .\n=\na a\na\n \na b\n\n\n147 \n4.2. Points and Vectors\nz Perpendicular. (a ⋅ b) = 0 (i.e., the angle between them is 90 degrees).\nz Same direction.  (a ⋅ b) > 0 (i.e., the angle between them is less than 90 \ndegrees).\nz Opposite directions.  (a ⋅ b) < 0 (i.e., the angle between them is greater than \n90 degrees).\nSome Other Applications of the Dot Product\nDot products can be used for all sorts of things in game programming. For ex-\nample, let’s say we want to ﬁ nd out whether an enemy is in front of the player \ncharacter or behind him. We can ﬁ nd a vector from the player’s position P to \nthe enemy’s position E by simple vector subtraction (v = E – P). Let’s assume \nwe have a vector f pointing in the direction that the player is facing . (As we’ll \nsee in Section 4.3.10.3, the vector f can be extracted directly from the player’s \nmodel-to-world matrix .) The dot product d = v ⋅ f can be used to test whether \nthe enemy is in front of or behind the player—it will be positive when the \nenemy is in front and negative when the enemy is behind.\n(a · b) = ab\n(a · b) = –ab\n(a · b) = 0\n(a · b) > 0\n(a · b) < 0\na\nb\na\nb\na\nb\na\nb\nb\na\nFigure 4.11. Some common dot product tests.\nFigure 4.12. The dot product can be used to ﬁ nd the height of a point above or below a \nplane.\n\n\n148 \n4. 3D Math for Games\nThe dot product can also be used to ﬁ nd the height of a point above or \nbelow a plane (which might be useful when writing a moon-landing game for \nexample). We can deﬁ ne a plane with two vector quantities: a point Q lying \nanywhere on the plane, and a unit vector n that is perpendicular (i.e., normal) \nto the plane. To ﬁ nd the height h of a point P above the plane, we ﬁ rst calculate \na vector from any point on the plane (Q will do nicely) to the point in ques-\ntion P. So we have v = P – Q. The dot product of vector v with the unit-length \nnormal vector n is just the projection of v onto the line deﬁ ned by n. But that \nis exactly the height we’re looking for. Therefore, h = v ⋅ n = (P – Q) ⋅ n. This \nis illustrated in Figure 4.12.\n4.2.4.8. Cross Product\nThe cross product (also known as the outer product or vector product) of two vec-\ntors yields another vector that is perpendicular to the two vectors being multi-\nplied, as shown in Figure 4.13. The cross product operation is only deﬁ ned in \nthree dimensions:\n \n \nMagnitude of the Cross Product\nThe magnitude of the cross product vector is the product of the magnitudes of \nthe two vectors and the sine of the angle between them. (This is similar to the \ndeﬁ nition of the dot product, but it replaces the cosine with the sine.)\n \n \nThe magnitude of the cross product                is equal to the area of the par-\nallelogram whose sides are a and b, as shown in Figure 4.14. Since a triangle \nis one-half of a parallelogram, the area of a triangle whose vertices are speci-\nﬁ ed by the position vectors V1 , V2 , and V3 can be calculated as one-half of the \nmagnitude of the cross product of any two of its sides:\n \n \na × b\na\nb\nFigure 4.13. The cross \nproduct of vectors a \nand b (right-handed).\nV2\nV1\nV3\na = (V2 – V1)\nb = (V3 – V1)\n|a × b|\nFigure 4.14. Area of a parallelogram expressed as the magnitude of a cross product.\n[(\n),\n(\n),\n(\n)]\n(\n)\n(\n)\n(\n) .\ny z\nz y\nz x\nx z\nx y\ny x\ny z\nz y\nz x\nx z\nx y\ny x\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\n=\n−\n=\n−\na\nb\ni\n−\n−\nj\nk\n+\n−\n+\n−\n×\n1\ntriangle\n2\n1\n3\n1\n2 (\n)\n(\n) .\nA\n=\n−\n×\n−\nV\nV\nV\nV\n \n \n \nsin( ).\n×\n=\nθ\na\nb\na b\n×\na\nb\n\n\n149 \nDirection of the Cross Product\nWhen using a right-handed coordinate system, you can use the right-hand rule \nto determine the direction of the cross product. Simply cup your ﬁ ngers such \nthat they point in the direction you’d rotate vector a to move it on top of vector \nb, and the cross product (a × b) will be in the direction of your thumb.\nNote that the cross product is deﬁ ned by the left -hand rule when using \na left -handed coordinate system. This means that the direction of the cross \nproduct changes depending on the choice of coordinate system. This might \nseem odd at ﬁ rst, but remember that the handedness of a coordinate system \ndoes not aﬀ ect the mathematical calculations we carry out—it only changes \nour visualization of what the numbers look like in 3D space. When converting \nfrom a RH system to a LH system or vice-versa, the numerical representations \nof all the points and vectors stay the same, but one axis ﬂ ips. Our visualization \nof everything is therefore mirrored along that ﬂ ipped axis. So if a cross prod-\nuct just happens to align with the axis we’re ﬂ ipping (e.g., the z-axis), it needs \nto ﬂ ip when the axis ﬂ ips. If it didn’t, the mathematical deﬁ nition of the cross \nproduct itself would have to be changed so that the z-coordinate of the cross \nproduct comes out negative in the new coordinate system. I wouldn’t lose too \nmuch sleep over all of this. Just remember: when visualizing a cross product, \nuse the right-hand rule in a right-handed coordinate system and the left -hand \nrule in a left -handed coordinate system.\nProperties of the Cross Product\nThe cross product is not commutative (i.e., order matt ers):\n \na × b ≠ b × a. \nHowever, it is anti-commutative :\n \na × b = – b × a. \nThe cross product is distributive over addition:\n \na × (b + c) = (a × b) + (a × c). \nAnd it combines with scalar multiplication as follows:\n \n(sa) × b = a × (sb) = s(a × b). \nThe Cartesian basis vectors are related by cross products as follows:\n \n \n \n \n \n \n \n(\n)\n(\n)\n,\n(\n)\n(\n)\n,\n(\n)\n(\n)\n.\n×\n=−×\n=\n×\n=−\n× =\n× =−\n×\n=\nj\nk\nk\nj\ni\nk\ni\ni\nk\nj\ni\nj\nj\ni\nk\n4.2. Points and Vectors\n\n\n150 \n4. 3D Math for Games\nThese three cross products deﬁ ne the direction of positive rotations about the \nCartesian axes. The positive rotations go from x to y (about z), from y to z \n(about x) and from z to x (about y). Notice how the rotation about the y-axis \n“reversed” alphabetically, in that it goes from z to x (not from x to z). As we’ll \nsee below, this gives us a hint as to why the matrix for rotation about the y-axis \nlooks inverted when compared to the matrices for rotation about the x- and \nz-axes.\nThe Cross Product in Action\nThe cross product has a number of applications in games. One of its most \ncommon uses is for ﬁ nding a vector that is perpendicular to two other vectors. \nAs we’ll see in Section 4.3.10.2, if we know an object’s local unit basis vectors, \n(ilocal , jlocal , and klocal), we can easily ﬁ nd a matrix representing the object’s \norientation. Let’s assume that all we know is the object’s klocal vector—i.e., the \ndirection in which the object is facing. If we assume that the object has no roll \nabout klocal , then we can ﬁ nd ilocal by taking the cross product between klocal\n(which we already know) and the world-space up vector jworld (which equals \n[0  1  0]). We do so as follows: ilocal = normalize(jworld × klocal). We can then ﬁ nd \njlocal by simply crossing ilocal and klocal as follows: jlocal = klocal × ilocal.\nA very similar technique can be used to ﬁ nd a unit vector normal to the \nsurface of a triangle or some other plane. Given three points on the plane P1 ,\nP2 , and P3 , the normal vector is just n = normalize[(P2 – P1) × (P3 – P1)].\nCross products are also used in physics simulations. When a force is ap-\nplied to an object, it will give rise to rotational motion if and only if it is ap-\nplied oﬀ -center. This rotational force is known as a torque , and it is calculated \nas follows. Given a force F, and a vector r from the center of mass to the point \nat which the force is applied, the torque N = r × F.\n4.2.5. Linear Interpolation of Points and Vectors\nIn games, we oft en need to ﬁ nd a vector that is midway between two known \nvectors. For example, if we want to smoothly animate an object from point A \nto point B over the course of two seconds at 30 frames per second, we would \nneed to ﬁ nd 60 intermediate positions between A and B.\nA linear interpolation is a simple mathematical operation that ﬁ nds an in-\ntermediate point between two known points. The name of this operation is \noft en shortened to LERP. The operation is deﬁ ned as follows, where β ranges \nfrom 0 to 1 inclusive:\n \n \n \n(\n, , )\n(1\n)\n[(1\n)\n,\n(1\n)\n,\n(1\n)\n].\nx\nx\ny\ny\nz\nz\nA\nB\nA\nB\nA\nB\n=\nβ =\n−β\n+ β\n=\n−β\n+ β\n−β\n+ β\n−β\n+ β\nL\nLERP A B\nA\nB\n\n\n151 \n4.3. Matrices\nGeometrically, L = LERP(A, B, β) is the position vector of a point that lies \nβ percent of the way along the line segment from point A to point B, as shown \nin Figure 4.15. Mathematically, the LERP function is just a weighted average of \nthe two input vectors, with weights (1 – β) and β, respectively. Notice that the \nweights always add to 1, which is a general requirement for any weighted \naverage.\n4.3. Matrices\nA matrix is a rectangular array of  m × n scalars. Matrices are a convenient way \nof representing linear transformations such as translation, rotation, and scale. \nA matrix M is usually writt en as a grid of scalars Mrc enclosed in square \nbrackets, where the subscripts r and c represent the row and column indices \nof the entry, respectively. For example, if M is a 3 × 3 matrix, it could be writ-\nten as follows:\n \n \nWe can think of the rows and/or columns of a 3 × 3 matrix as 3D vectors. \nWhen all of the row and column vectors of a 3 × 3 matrix are of unit magni-\ntude, we call it a special orthogonal matrix. This is also known as an isotropic \nmatrix, or an orthonormal matrix. Such matrices represent pure rotations.\nUnder certain constraints, a 4 × 4 matrix can represent arbitrary 3D trans-\nformations , including translations , rotations , and changes in scale . These are \ncalled transformation matrices , and they are the kinds of matrices that will be \nmost useful to us as game engineers. The transformations represented by a \nmatrix are applied to a point or vector via matrix multiplication. We’ll inves-\ntigate how this works below.\nAn aﬃ  ne matrix is a 4 × 4 transformation matrix that preserves parallelism \nof lines and relative distance ratios, but not necessarily absolute lengths and \nangles. An aﬃ  ne matrix is any combination of the following operations: rota-\ntion, translation, scale and/or shear.\n4.3.1. \nMatrix Multiplication\n The product P of two matrices A and B is writt en P = AB. If A and B are \ntransformation matrices, then the product P is another transformation matrix \nthat performs both of the original transformations. For example, if A is a scale \nmatrix and B is a rotation, the matrix P would both scale and rotate the points \nA\nL = LERP(A, B, 0.4)\nB\nβ = 0\nβ = 1\nβ = 0.4\nFigure 4.15. Linear in-\nterpolation (LERP) be-\ntween points A and B, \nwith β = 0.4.\n \n11\n12\n13\n21\n22\n23\n31\n32\n33\n.\nM\nM\nM\nM\nM\nM\nM\nM\nM\n⎡\n⎤\n⎢\n⎥\n=⎢\n⎥\n⎢\n⎥\n⎣\n⎦\nM\n\n\n152 \n4. 3D Math for Games\nor vectors to which it is applied. This is particularly useful in game program-\nming, because we can precalculate a single matrix that performs a whole se-\nquence of transformations and then apply all of those transformations to a \nlarge number of vectors eﬃ  ciently.\nTo calculate a matrix product, we simply take dot products between the \nrows of the nA × mA matrix A and the columns of the nB × mB matrix B. Each dot \nproduct becomes one component of the resulting matrix P. The two matrices \ncan be multiplied as long as the inner dimensions are equal (i.e., mA = nB). For \nexample, if A and B are 3 × 3 matrices, then\n \nP = AB, \n \n         \nMatrix multiplication is not commutative. In other words, the order in \nwhich matrix multiplication is done matt ers:\n \n \nAB ≠ BA . \nWe’ll see exactly why this matt ers in Section 4.3.2.\nMatrix multiplication is oft en called concatenation, because the product \nof n transformation matrices is a matrix that concatenates, or chains together, \nthe original sequence of transformations in the order the matrices were mul-\ntiplied.\n4.3.2. Representing Points and Vectors as Matrices\nPoints and vectors can be represented as row matrices (1 × n) or column matrices \n(n × 1), where n is the dimension of the space we’re working with (usually 2 or \n3). For example, the vector v = (3, 4, –1) can be writt en either as\n \n \nor as\n \n \nThe choice between column and row vectors is a completely arbitrary \none, but it does aﬀ ect the order in which matrix multiplications are writt en. \nThis happens because when multiplying matrices, the inner dimensions of the \ntwo matrices must be equal, so:\n11\nrow1\ncol1\n21\nrow2\ncol1\n31\nrow3\ncol1\n;\n;\n;\nP\nP\nP\n=\n⋅\n=\n⋅\n=\n⋅\nA\nB\nA\nB\nA\nB\n12\nrow1\ncol2\n22\nrow2\ncol2\n32\nrow3\ncol2\n;\n;\n;\nP\nP\nP\n=\n⋅\n=\n⋅\n=\n⋅\nA\nB\nA\nB\nA\nB\n13\nrow1\ncol3\n23\nrow2\ncol3\n33\nrow3\ncol3\n;\n;\n.\nP\nP\nP\n=\n⋅\n=\n⋅\n=\n⋅\nA\nB\nA\nB\nA\nB\n \n1\n[3\n4\n1],\n=\n−\nv\nT\n2\n1\n3\n4\n.\n1\n⎡\n⎤\n⎢\n⎥\n=\n=\n⎢\n⎥\n⎢\n⎥\n−\n⎣\n⎦\nv\nv\n\n\n153 \n4.3. Matrices\nz to multiply a 1 × n row vector by an n × n matrix, the vector must appear \nto the left  of the matrix (                             ), whereas\nz to multiply an n × n matrix by an n × 1 column vector, the vector must \nappear to the right of the matrix (                             ).\nIf multiple transformation matrices A, B, and C are applied in order to a \nvector v, the transformations “read” from left  to right when using row vectors, \nbut from right to left  when using column vectors. The easiest way to remember \nthis is to realize that the matrix closest to the vector is applied ﬁ rst. This is il-\nlustrated by the parentheses below:\n \nv’ = ( ( ( vA ) B ) C )    Row vectors: read left -to-right; \n \nv’ = ( C ( B ( Av ) ) )    Column vectors: read right-to-left . \nIn this book we’ll adopt the row vector convention, because the left -to-right \norder of transformations is most intuitive to read for English-speaking people. \nThat said, be very careful to check which convention is used by your game \nengine, and by other books, papers, or web pages you may read. You can \nusually tell by seeing whether vector-matrix multiplications are writt en with \nthe vector on the left  (for row vectors) or the right (for column vectors) of the \nmatrix. When using column vectors, you’ll need to transpose all the matrices \nshown in this book.\n4.3.3. The Identity Matrix\nThe identity matrix is a matrix that, when multiplied by any other matrix, \nyields the very same matrix. It is usually represented by the symbol I. The \nidentity matrix is always a square matrix with 1’s along the diagonal and 0’s \neverywhere else:\n \n \n \nAI = IA ≡ A . \n4.3.4. Matrix Inversion\nThe inverse of a matrix A is another matrix (denoted A–1) that undoes the eﬀ ects \nof matrix A. So, for example, if A rotates objects by 37 degrees about the z-axis, \nthen A–1 will rotate by –37 degrees about the z-axis. Likewise, if A scales objects \nto be twice their original size, then A–1 scales objects to be half-sized. When a ma-\ntrix is multiplied by its own inverse, the result is always the identity matrix, so\n \n1\n1\nn\nn\nn n\n×\n×\n×\n′\n=\nv\nv\nM\n \n1\n1\nn\nn n\nn\n×\n×\n×\n′\n=\nv\nM\nv\n \n3 3\n1\n0\n0\n0\n1\n0 ;\n0\n0\n1\n×\n⎡\n⎤\n⎢\n⎥\n=⎢\n⎥\n⎢\n⎥\n⎣\n⎦\nI\n\n\n154 \n4. 3D Math for Games\n  \n \n         Not all matrices have inverses. However, all aﬃ  ne matri-\nces (combinations of pure rotations, translations, scales, and shears) do have \ninverses. Gaussian elimination or LU decomposition can be used to ﬁ nd the \ninverse, if one exists.\nSince we’ll be dealing with matrix multiplication a lot, it’s important to \nnote here that the inverse of a sequence of concatenated matrices can be writt en \nas the reverse concatenation of the individual matrices’ inverses. For example,\n \n(ABC)–1 = C–1 B–1 A–1. \n4.3.5. Transposition\nThe transpose of a matrix M is denoted MT. It is obtained by reﬂ ecting the en-\ntries of the original matrix across its diagonal. In other words, the rows of the \noriginal matrix become the columns of the transposed matrix, and vice-versa:\n \n \nThe transpose is useful for a number of reasons. For one thing, the inverse \nof an orthonormal (pure rotation) matrix is exactly equal to its transpose—\nwhich is good news, because it’s much cheaper to transpose a matrix than it is \nto ﬁ nd its inverse in general. Transposition can also be important when mov-\ning data from one math library to another, because some libraries use column \nvectors while others expect row vectors. The matrices used by a row-vector-\nbased library will be transposed relative to those used by a library that employs \nthe column vector convention.\nAs with the inverse, the transpose of a sequence of concatenated matrices \ncan be rewritt en as the reverse concatenation of the individual matrices’ trans-\nposes. For example,\n \n(ABC)T = CT   BT  AT. \nThis will prove useful when we consider how to apply transformation matri-\nces to points and vectors.\n4.3.6. Homogeneous Coordinates\n You may recall from high-school algebra that a 2 × 2 matrix can represent a \nrotation in two dimensions. To rotate a vector r through an angle of φ degrees \n(where positive rotations are counter-clockwise), we can write\n \n \n1\n1\n(\n)\n(\n)\n.\n−\n−\n≡\n≡\nA A\nA\nA\nI\n \n \ncos\nsin\n[\n]\n[\n]\n.\nsin\ncos\nx\ny\nx\ny\nr\nr\nr\nr\nφ\nφ\n⎡\n⎤\n′\n′ =\n⎢\n⎥\n−\nφ\nφ\n⎣\n⎦\n \nT\n.\na\nb\nc\na\nd\ng\nd\ne\nf\ng\nh\ni\nc\nf     i\n⎡\n⎤\n⎡\n⎤\n⎢\n⎥\n⎢\n⎥\n=\n⎢\n⎥\n⎢\n⎥\n⎢\n⎥\n⎢\n⎥\n⎣\n⎦\n⎣\n⎦\ne\nh\nb\n\n\n155 \n4.3. Matrices\nIt’s probably no surprise that rotations in three dimensions can be represented \nby a 3 × 3 matrix. The two-dimensional example above is really just a three-\ndimensional rotation about the z-axis, so we can write\n \n \nThe question naturally arises: Can a 3 × 3 matrix be used to represent \ntranslations? Sadly, the answer is no. The result of translating a point r by a \ntranslation t requires adding the components of t to the components of r in-\ndividually:\n \n \nMatrix multiplication involves multiplication and addition of matrix ele-\nments, so the idea of using multiplication for translation seems promising. \nBut, unfortunately, there is no way to arrange the components of t within a 3 × \n3 matrix such that the result of multiplying it with the column vector r yields \nsums like (rx + tx).\nThe good news is that we can obtain sums like this if we use a 4 × 4 matrix. \nWhat would such a matrix look like? Well, we know that we don’t want any \nrotational eﬀ ects, so the upper 3 × 3 should contain an identity matrix. If we \narrange the components of t across the bott om-most row of the matrix and \nset the fourth element of the r vector (usually called w) equal to 1, then taking \nthe dot product of the vector r with column 1 of the matrix will yield (1 × rx) + \n(0 × ry) + (0 × rz) + (tx × 1) = (rx + tx), which is exactly what we want. If the bott om \nright-hand corner of the matrix contains a 1 and the rest of the fourth column \ncontains zeros, then the resulting vector will also have a 1 in its w component. \nHere’s what the ﬁ nal 4 × 4 translation matrix looks like:\n \n \nWhen a point or vector is extended from three dimensions to four in this \nmanner, we say that it has been writt en in homogeneous coordinates. A point in \nhomogeneous coordinates always has w = 1. Most of the 3D matrix math done \nby game engines is performed using 4 × 4 matrices with four-element points \nand vectors writt en in homogeneous coordinates.\n \n  cos\nsin\n0\n[\n]\n[\n] \nsin\ncos\n0 .\n0\n0\n1\nx\ny\nz\nx\ny\nz\nr\nr\nr\nr\nr\nr\n⎡\n⎤\nφ\nφ\n⎢\n⎥\n′\n′\n′ =\n−\nφ\nφ\n⎢\n⎥\n⎢\n⎥\n⎣\n⎦\n \n[(\n)\n(\n)\n(\n)].\nx\nx\ny\ny\nz\nz\nr\nt\nr\nt\nr\nt\n+ =\n+\n+\n+\nr\nt\n \n1\n0\n0\n0\n0\n1\n0\n0\n[\n1] 0\n0\n1\n0\n1\n       \n[(\n)\n(\n)\n(\n)\n1].\nx\ny\nz\nx\ny\nz\nx\nx\ny\ny\nz\nz\nr\nr\nr\nt\nr\nt\nr\nt\nr\nt\n⎡\n⎤\n⎢\n⎥\n⎢\n⎥\n+ =\n⎢\n⎥\n⎢\n⎥\n⎣\n⎦\n=\n+\n+\n+\nr\nt\nt\nt\n\n\n156 \n4. 3D Math for Games\n4.3.6.1. Transforming Direction Vectors\nMathematically, points (position vectors) and direction vectors are treated in \nsubtly diﬀ erent ways. When transforming a point by a matrix, the translation, \nrotation, and scale of the matrix are all applied to the point. But when trans-\nforming a direction by a matrix, the translational eﬀ ects of the matrix are ig-\nnored. This is because direction vectors have no translation per se—applying \na translation to a direction would alter its magnitude, which is usually not \nwhat we want.\nIn homogeneous coordinates, we achieve this by deﬁ ning points to have \ntheir w components equal to one, while direction vectors have their w com-\nponents equal to zero. In the example below, notice how the w = 0 component \nof the vector v multiplies with the t vector in the matrix, thereby eliminating \ntranslation in the ﬁ nal result:\n \n \nTechnically, a point in homogeneous (four-dimensional) coordinates can \nbe converted into non-homogeneous (three-dimensional) coordinates by di-\nviding the x, y, and z components by the w component:\n \n \nThis sheds some light on why we set a point’s w component to one and a vec-\ntor’s w component to zero. Dividing by w = 1 has no eﬀ ect on the coordinates \nof a point, but dividing a pure direction vector’s components by w = 0 would \nyield inﬁ nity. A point at inﬁ nity in 4D can be rotated but not translated, be-\ncause no matt er what translation we try to apply, the point will remain at in-\nﬁ nity. So in eﬀ ect, a pure direction vector in three-dimensional space acts like \na point at inﬁ nity in four-dimensional homogeneous space.\n4.3.7. Atomic Transformation Matrices\nAny aﬃ  ne transformation matrix can be created by simply concatenating a \nsequence of 4 × 4 matrices representing pure translations, pure rotations, pure \nscale operations, and/or pure shears. These atomic transformation building \nblocks are presented below. (We’ll omit shear from these discussions, as it \ntends to be used only rarely in games.)\nNotice that all aﬃ  ne 4 × 4 transformation matrices can be partitioned into \nfour components:\n \n \n \n[\n0] \n[(\n0 )\n0]\n[\n0].\n1\n⎡\n⎤=\n+\n=\n⎢\n⎥\n⎣\n⎦\nU\n0\nv\nvU\nt\nvU\nt\n \n[\n]\n.\ny\nx\nz\nx\ny\nz\nw\nw\nw\nw\n⎡\n⎤\n≡⎢\n⎥\n⎣\n⎦\n \n3 3\n3 1\n1 3\n.\n1\n×\n×\n×\n⎡\n⎤\n⎢\n⎥\n⎣\n⎦\nU\n0\nt\n\n\n157 \n4.3. Matrices\nz the upper 3 × 3 matrix U, which represents the rotation and/or scale,\nz a 1 × 3 translation vector t,\nz a 3 × 1 vector of zeros 0 = [ 0 0 0 ]T, and\nz a scalar 1 in the bott om-right corner of the matrix.\nWhen a point is multiplied by a matrix that has been partitioned like this, the \nresult is as follows:\n \n \n4.3.7.1. \nTranslation\n The following matrix translates a point by the vector t:\n \n \nor in partitioned shorthand:\n \n \nTo invert a pure translation matrix, simply negate the vector t (i.e., negate tx ,\nty , and tz).\n4.3.7.2. Rotation\n All 4  × 4 pure rotation matrices have the form:\n \n \nThe t vector is zero and the upper 3 × 3 matrix R contains cosines and sines of \nthe rotation angle, measured in radians.\nThe following matrix represents rotation about the x-axis by an angle φ:\n  \n \n \n3 3\n3 1\n 1 3\n 1 3\n1 3\n[\n1]\n[\n1] \n[(\n)\n1].\n1\n×\n×\n×\n×\n×\n⎡\n⎤\n′\n=\n=\n+\n⎢\n⎥\n⎣\n⎦\nU\n0\nr\nr\nrU\nt\nt\n \n1\n0\n0\n0\n0\n1\n0\n0\n[\n1] 0\n0\n1\n0\n1\n       \n[(\n)\n(\n)\n(\n)\n1],\nx\ny\nz\nx\ny\nz\nx\nx\ny\ny\nz\nz\nr\nr\nr\nt\nr\nt\nr\nt\nr\nt\n⎡\n⎤\n⎢\n⎥\n⎢\n⎥\n+ =\n⎢\n⎥\n⎢\n⎥\n⎣\n⎦\n=\n+\n+\n+\nr\nt\nt\nt\n \n[\n1] \n[(\n)\n1].\n1\n⎡\n⎤=\n+\n⎢\n⎥\n⎣\n⎦\nI\n0\nr\nr\nt\nt\n \n[\n1] \n[\n1].\n1\n⎡\n⎤=\n⎢\n⎥\n⎣\n⎦\nR\n0\nr\nrR\n0\n \n1\n0\n0\n0\n0\n  cos\nsin\n0\nrotate ( , )\n[\n1] \n.\n0\nsin\ncos\n0\n0\n0\n0\n1\nx\nx\ny\nz\nr\nr\nr\n⎡\n⎤\n⎢\n⎥\nφ\nφ\n⎢\n⎥\nφ =\n⎢\n⎥\n−\nφ\nφ\n⎢\n⎥\n⎣\n⎦\nr\n\n\n158 \n4. 3D Math for Games\nThe matrix below represents rotation about the y-axis by an angle θ. Notice \nthat this one is transposed relative to the other two—the positive and negative \nsine terms have been reﬂ ected across the diagonal:\n \n \nThis matrix represents rotation about the z-axis by an angle γ:\n \n \nHere are a few observations about these matrices:\nz The 1 within the upper 3 × 3 always appears on the axis we’re rotating \nabout, while the sine and cosine terms are oﬀ -axis.\nz Positive rotations go from x to y (about z), from y to z (about x), and from \nz to x (about y). The z to x rotation “wraps around,” which is why the \nrotation matrix about the y-axis is transposed relative to the other two. \n(Use the right-hand or left -hand rule to remember this.)\nz The inverse of a pure rotation is just its transpose. This works because \ninverting a rotation is equivalent to rotating by the negative angle. You \nmay recall that cos(–θ) = cos(θ) while sin(–θ) = –sin(θ), so negating the \nangle causes the two sine terms to eﬀ ectively switch places, while the \ncosine terms stay put.\n4.3.7.3. Scale\n The following matrix scales the point r by a factor of sx along the x-axis, sy\nalong the y-axis, and sz along the z-axis:\n  \n \n \ncos\n0\nsin\n0\n0\n1\n0\n0\nrotate ( , )\n[\n1] \n.\nsin\n0\n  cos\n0\n0\n0\n0\n1\ny\nx\ny\nz\nr\nr\nr\nθ\n−\nθ\n⎡\n⎤\n⎢\n⎥\n⎢\n⎥\nθ =\n⎢\n⎥\nθ\nθ\n⎢\n⎥\n⎣\n⎦\nr\n \n  cos\nsin\n0\n0\nsin\ncos\n0\n0\nrotate ( , )\n[\n1] \n.\n0\n0\n1\n0\n0\n0\n0\n1\nz\nx\ny\nz\nr\nr\nr\nγ\nγ\n⎡\n⎤\n⎢\n⎥\n−\nγ\nγ\n⎢\n⎥\nγ =\n⎢\n⎥\n⎢\n⎥\n⎣\n⎦\nr\n \n \n \n0\n0\n0\n0\n0\n0\n[\n1] 0\n0\n0\n0\n0\n0\n1\n    \n[\n1].\nx\ny\nx\ny\nz\nz\nx x\ny y\nz z\ns\ns\nr\nr\nr\ns\ns r\ns r\ns r\n⎡\n⎤\n⎢\n⎥\n⎢\n⎥\n=\n⎢\n⎥\n⎢\n⎥\n⎣\n⎦\n=\nrS\n\n\n159 \n4.3. Matrices\nor in partitioned shorthand:\n \n \nHere are some observations about this kind of matrix:\nz To invert a scaling matrix, simply substitute sx , sy , and sz with their re-\nciprocals (i.e., 1/sx , 1/sy , and 1/sz).\nz When the scale factor along all three axes is the same (sx = sy = sz), we call \nthis uniform scale. Spheres remain spheres under uniform scale, whereas \nunder nonuniform scale they become ellipsoids. To keep the mathemat-\nics of bounding sphere checks simple and fast, many game engines im-\npose the restriction that only uniform scale may be applied to render-\nable geometry or collision primitives.\nz When a uniform scale matrix Su and a rotation matrix R are concat-\nenated, the order of multiplication is unimportant (i.e., SuR = RSu). This \nonly works for uniform scale!\n4.3.8. 4 × 3 Matrices\n The rightmost column of an aﬃ  ne 4 × 4 matrix always contains the vector \n[ 0  0  0  1 ]T. As such, game programmers oft en omit the fourth column to \nsave memory. You’ll encounter 4 × 3 aﬃ  ne matrices frequently in game math \nlibraries.\n4.3.9. Coordinate Spaces\nWe’ve seen how to apply transformations to points and direction vectors us-\ning 4 × 4 matrices. We can extend this idea to rigid objects by realizing that \nsuch an object can be thought of as an inﬁ nite collection of points. Applying \na transformation to a rigid object is like applying that same transformation to \nevery point within the object. For example, in computer graphics an object is \nusually represented by a mesh of triangles, each of which has three vertices \nrepresented by points. In this case, the object can be transformed by applying \na transformation matrix to all of its vertices in turn.\nWe said above that a point is a vector whose tail is ﬁ xed to the origin of \nsome coordinate system. This is another way of saying that a point (position \nvector) is always expressed relative to a set of coordinate axes. The triplet of \nnumbers representing a point changes numerically whenever we select a new \nset of coordinate axes. In Figure 4.16, we see a point P represented by two \ndiﬀ erent position vectors—the vector PA gives the position of P relative to the \n \n \n3 3\n3 3\n[\n1] \n[\n1].\n1\n×\n×\n⎡\n⎤=\n⎢\n⎥\n⎣\n⎦\nS\n0\nr\nrS\n0\n",
      "page_number": 160,
      "chapter_number": 9,
      "summary": "This chapter covers segment 9 (pages 160-181). Key topics include vectors, matrix, and point. Points and Vectors\nThe majority of modern 3D games are made up of three-dimensional objects \nin a virtual world.",
      "keywords": [
        "vector",
        "matrix",
        "cross product",
        "point",
        "product",
        "Dot Product",
        "coordinate system",
        "direction vector",
        "direction",
        "unit vector",
        "matrices",
        "coordinate",
        "cross product vector",
        "game",
        "column vectors"
      ],
      "concepts": [
        "vectors",
        "matrix",
        "point",
        "matrices",
        "coordinates",
        "product",
        "rotate",
        "rotations",
        "rotation",
        "rotated"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 1",
          "chapter": 30,
          "title": "Segment 30 (pages 279-290)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 31,
          "title": "Segment 31 (pages 291-298)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 61,
          "title": "Segment 61 (pages 589-596)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 20,
          "title": "Segment 20 (pages 181-193)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 26,
          "title": "Segment 26 (pages 243-252)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 182-202)",
      "start_page": 182,
      "end_page": 202,
      "detection_method": "synthetic",
      "content": "160 \n4. 3D Math for Games\n“A” axes, while the vector PB gives the position of that same point relative to a \ndiﬀ erent set of axes “B.”\nIn physics, a set of coordinate axes represents a frame of reference, so \nwe sometimes refer to a set of axes as a coordinate frame (or just a frame). \nPeople in the game industry also use the term coordinate space (or simply \nspace) to refer to a set of coordinate axes. In the following sections, we’ll look \nat a few of the most common coordinate spaces used in games and computer \ngraphics.\n4.3.9.1. Model Space\nWhen a triangle mesh is created in a tool such as Maya or 3DStudioMAX, the \npositions of the triangles’ vertices are speciﬁ ed relative to a Cartesian coordi-\nnate system which we call model space (also known as object space or local space). \nThe model space origin is usually placed at a central location within the object, \nsuch as at its center of mass, or on the ground between the feet of a humanoid \nor animal character.\nMost game objects have an inherent directionality. For example, an air-\nplane has a nose, a tail ﬁ n, and wings that correspond to the front, up, and \nleft /right directions. The model space axes are usually aligned to these natural \ndirections on the model, and they’re given intuitive names to indicate their \ndirectionality as illustrated in Figure 4.17. \nz Front. This name is given to the axis that points in the direction that the \nobject naturally travels or faces. In this book, we’ll use the symbol F to \nrefer to a unit basis vector along the front axis.\nz Up. This name is given to the axis that points towards the top of the \nobject. The unit basis vector along this axis will be denoted U.\nz Left  or right. The name “left ” or “right” is given to the axis that points \ntoward the left  or right side of the object. Which name is chosen de-\npends on whether your game engine uses left -handed or right-handed \nxA\nyA\nxB\nyB\nPA = (2, 3)\nPB = (1, 5)\nFigure 4.16. Position vectors for the point P relative to different coordinate axes.\n\n\n161 \n4.3. Matrices\ncoordinates. The unit basis vector along this axis will be denoted L or R, \nas appropriate.\nThe mapping between the (front, up, left ) labels and the (x, y, z) axes is com-\npletely arbitrary. A common choice when working with right-handed axes is \nto assign the label front to the positive z-axis, the label left  to the positive x-axis, \nand the label up to the positive y-axis (or in terms of unit basis vectors, F = k, \nL = i, and U = j). However, it’s equally common for +x to be front and +z to be \nright (F = i, R = k, U = j). I’ve also worked with engines in which the z-axis is \noriented vertically. The only real requirement is that you stick to one conven-\ntion consistently throughout your engine.\nAs an example of how intuitive axis names can reduce confusion, consid-\ner Euler angles (pitch, yaw, roll), which are oft en used to describe an aircraft ’s \norientation. It’s not possible to deﬁ ne pitch, yaw, and roll angles in terms of \nthe (i, j, k) basis vectors because their orientation is arbitrary. However, we can \ndeﬁ ne pitch, yaw, and roll in terms of the (L, U, F) basis vectors, because their \norientations are clearly deﬁ ned. Speciﬁ cally,\nz pitch is rotation about L or R,\nz yaw is rotation about U, and\nz roll is rotation about F.\n4.3.9.2. World Space\nWorld space is a ﬁ xed coordinate space, in which the positions, orientations, \nand scales of all objects in the game world are expressed. This coordinate \nspace ties all the individual objects together into a cohesive virtual world.\nThe location of the world-space origin is arbitrary, but it is oft en placed \nnear the center of the playable game space to minimize the reduction in ﬂ oat-\ning-point precision that can occur when (x, y, z) coordinates grow very large. \nLikewise, the orientation of the x-, y-, and z-axes is arbitrary, although most \nle\u0002\nfront\nup\nFigure 4.17. One possible choice of the model-space front, left and up axis basis vectors for \nan airplane.\n\n\n162 \n4. 3D Math for Games\nof the engines I’ve encountered use either a y-up or a z-up convention. The \ny-up convention was probably an extension of the two-dimensional conven-\ntion found in most mathematics textbooks, where the y-axis is shown going \nup and the x-axis going to the right. The z-up convention is also common, be-\ncause it allows a top-down orthographic view of the game world to look like \na traditional two-dimensional xy-plot.\nAs an example, let’s say that our aircraft ’s left  wingtip is at (5, 0, 0) in mod-\nel space. (In our game, front vectors correspond to the positive z-axis in model \nspace with y up, as shown in Figure 4.17.) Now imagine that the jet is facing \ndown the positive x-axis in world space, with its model-space origin at some \narbitrary location, such as (–25, 50, 8). Because the F vector of the airplane, \nwhich corresponds to +z in model space, is facing down the +x-axis in world \nspace, we know that the jet has been rotated by 90 degrees about the world \ny-axis. So if the aircraft  were sitt ing at the world space origin, its left  wingtip \nwould be at (0, 0, –5) in world space. But because the aircraft ’s origin has been \ntranslated to (–25, 50, 8), the ﬁ nal position of the jet’s left  wingtip in model \nspace is (–25, 50, [8 – 5]) = (–25, 50, 3). This is illustrated in Figure 4.18.\nWe could of course populate our friendly skies with more than one Lear \njet. In that case, all of their left  wingtips would have coordinates of (5, 0, 0) \nin model space. But in world space, the left  wingtips would have all sorts of \ninteresting coordinates, depending on the orientation and translation of each \naircraft .\n4.3.9.3. View Space\nView space (also known as camera space) is a coordinate frame ﬁ xed to the cam-\nera. The view space origin is placed at the focal point of the camera. Again, \nany axis orientation scheme is possible. However, a y-up convention with z \nAirport\nz W\nxW\nxM\nz M\n(5,0,0)M\n(–25,50,3)W\n(–25,50,8)W\nAircraft:\nLeft \nWingtip:\nFigure 4.18.  A lear jet whose left wingtip is at (5, 0, 0) in model space. If the jet is rotated by 90 \ndegrees about the world-space y-axis, and its model-space origin translated to (–25, 50, 8) in \nworld space, then its left wingtip would end up at (–25, 50, 3) when expressed in world space \ncoordinates.\n\n\n163 \n4.3. Matrices\nincreasing in the direction the camera is facing (left -handed) is typical because \nit allows z coordinates to represent depths into the screen . Other engines and \nAPIs, such as OpenGL , deﬁ ne view space to be right-handed, in which case the \ncamera faces towards negative z, and z coordinates represent negative depths.\n4.3.10. Change of Basis\nIn games and computer graphics, it is oft en quite useful to convert an object’s \nposition, orientation, and scale from one coordinate system into another. We \ncall this operation a change of basis .\n4.3.10.1. Coordinate Space Hierarchies\nCoordinate frames are relative. That is, if you want to quantify the position, \norientation, and scale of a set of axes in three-dimensional space, you must \nspecify these quantities relative to some other set of axes (otherwise the num-\nbers would have no meaning). This implies that coordinate spaces form a hi-\nerarchy—every coordinate space is a child of some other coordinate space, and \nthe other space acts as its parent. World space has no parent; it is at the root \nof the coordinate-space tree, and all other coordinate systems are ultimately \nspeciﬁ ed relative to it, either as direct children or more-distant relatives.\n4.3.10.2. Building a Change of Basis Matrix\nThe matrix that transforms points and directions from any child coordinate \nsystem C to its parent coordinate system P can be writt en \nC\nP\n→\nM\n (pronounced \n“C to P”). The subscript indicates that this matrix transforms points and direc-\ntions from child space to parent space. Any child-space position vector PC can \nbe transformed into a parent-space position vector PP as follows:\nLeft-Handed\nx\nz\ny\nRight-Handed\nz\nx\ny\nVirtual \nScreen\nVirtual \nScreen\nFigure 4.19. Left- and right-handed examples of view space, also known as camera space.\n\n\n164 \n4. 3D Math for Games\n \n \n \nIn this equation,\nz iC is the unit basis vector along the child space x-axis, expressed in par-\nent space coordinates;\nz jC is the unit basis vector along the child space y-axis, in parent space;\nz kC is the unit basis vector along the child space z-axis, in parent space;\nz tC is the translation of the child coordinate system relative to parent \nspace.\nThis result should not be too surprising. The tC vector is just the transla-\ntion of the child space axes relative to parent space, so if the rest of the ma-\ntrix were identity, the point (0, 0, 0) in child space would become tC in parent \nspace, just as we’d expect. The iC , jC , and kC unit vectors form the upper 3 × 3 \nof the matrix, which is a pure rotation matrix because these vectors are of unit \nlength. We can see this more clearly by considering a simple example, such as \na situation in which child space is rotated by an angle γ about the z-axis, with \nno translation. The matrix for such a rotation is given by \n \n \n(4.2)\nBut in Figure 4.20, we can see that the coordinates of the iC and jC vectors, \nexpressed in parent space, are iC = [ cos γ  sin γ  0 ] and jC = [ –sin γ  cos γ  0 ]. \nWhen we plug these vectors into our formula for \nC\nP\n→\nM\n, with kC = [ 0  0  1 ], it \nexactly matches the matrix rotatez(r, γ) from Equation (4.2).\nScaling the Child Axes\nScaling of the child coordinate system is accomplished by simply scaling the \nunit basis vectors appropriately. For example, if child space is scaled up by a \n \n  cos\nsin\n0\n0\nsin\ncos\n0\n0\nrotate ( , )\n[\n1] \n.\n0\n0\n1\n0\n0\n0\n0\n1\nz\nx\ny\nz\nr\nr\nr\nγ\nγ\n⎡\n⎤\n⎢\n⎥\n−\nγ\nγ\n⎢\n⎥\nγ =\n⎢\n⎥\n⎢\n⎥\n⎣\n⎦\nr\n \n \nC\nP\n;\n0\n0\n0\n0\n0\n0\n.\n0\n1\nP\nC\nC\nP\nC\nC\nC\nC\nCx\nCy\nCz\nCx\nCy\nCz\nCx\nCy\nCz\nCx\nCy\nCz\ni\ni\ni\nj\nj\nj\nk\nk\nk\nt\n→\n→\n=\n⎡\n⎤\n⎢\n⎥\n⎢\n⎥\n=⎢\n⎥\n⎢\n⎥\n⎣\n⎦\n⎡\n⎤\n⎢\n⎥\n⎢\n⎥\n=⎢\n⎥\n⎢\n⎥\n⎣\n⎦\nP\nP M\ni\nj\nM\nk\nt\nt\nt\n\n\n165 \n4.3. Matrices\nfactor of two, then the basis vectors iC , jC , and kC will be of length 2 instead \nof unit length.\n4.3.10.3. Extracting Unit Basis Vectors from a Matrix\n The fact that we can build a change of basis matrix out of a translation and \nthree Cartesian basis vectors gives us another powerful tool: Given any aﬃ  ne \n4 × 4 transformation matrix, we can go in the other direction and extract the \nchild-space basis vectors iC , jC , and kC from it by simply isolating the appropri-\nate rows of the matrix (or columns if your math library uses column vectors).\nThis can be incredibly useful. Let’s say we are given a vehicle’s model-\nto-world transform as an aﬃ  ne 4 × 4 matrix (a very common representation). \nThis is really just a change of basis matrix, transforming points in model space \ninto their equivalents in world space. Let’s further assume that in our game, \nthe positive z-axis always points in the direction that an object is facing. So, to \nﬁ nd a unit vector representing the vehicle’s facing direction, we can simply ex-\ntract kC directly from the model-to-world matrix (by grabbing its third row). \nThis vector will already be normalized and ready to go.\n4.3.10.4. Transforming Coordinate Systems versus Vectors\nWe’ve said that the matrix \nC\nP\n→\nM\n transforms points and directions from child \nspace into parent space. Recall that the fourth row of \nC\nP\n→\nM\n contains tC , the \ntranslation of the child coordinate axes relative to the world space axes. There-\nfore, another way to visualize the matrix \nC\nP\n→\nM\n is to imagine it taking the \nparent coordinate axes and transforming them into the child axes. This is the \nreverse of what happens to points and direction vectors. In other words, if a \nmatrix transforms vectors from child space to parent space, then it also trans-\nforms coordinate axes from parent space to child space. This makes sense when \nyou think about it—moving a point 20 units to the right with the coordinate \naxes ﬁ xed is the same as moving the coordinate axes 20 units to the left  with \nthe point ﬁ xed. This concept is illustrated in Figure 4.21.\nx\ny\ncos(γ)\nsin(γ)\n–sin(γ)\ncos(γ)\nγ\nγ\niC\njC\nFigure 4.20. Change of basis when child axes are rotated by an angle γ relative to parent.\n\n\n166 \n4. 3D Math for Games\nOf course, this is just another point of potential confusion. If you’re think-\ning in terms of coordinate axes, then transformations go in one direction, but \nif you’re thinking in terms of points and vectors, they go in the other direction! \nAs with many confusing things in life, your best bet is probably to choose \na single “canonical” way of thinking about things and stick with it. For ex-\nample, in this book we’ve chosen the following conventions:\nz Transformations apply to vectors (not coordinate axes).\nz Vectors are writt en as rows (not columns).\nTaken together, these two conventions allow us to read sequences of ma-\ntrix multiplications from left  to right and have them make sense (e.g., \n \n \n \nD\nA\nA\nB\nB\nC\nC\nD\n→\n→\n→\n=\nP\nP M\nM\nM\n). Obviously if you start thinking about the coordi-\nnate axes moving around rather than the points and vectors, you either have \nto read the transforms from right to left , or ﬂ ip one of these two conventions \naround. It doesn’t really matt er what conventions you choose as long as you \nﬁ nd them easy to remember and work with.\nThat said, it’s important to note that certain problems are easier to think \nabout in terms of vectors being transformed, while others are easier to work \nwith when you imagine the coordinate axes moving around. Once you get good \nat thinking about 3D vector and matrix math, you’ll ﬁ nd it prett y easy to ﬂ ip \nback and forth between conventions as needed to suit the problem at hand.\n4.3.11. Transforming Normal Vectors\n A normal vector is a special kind of vector, because in addition to (usually!) be-\ning of unit length, it carries with it the additional requirement that it should \nalways remain perpendicular to whatever surface or plane it is associated with. \nSpecial care must be taken when transforming a normal vector, to ensure that \nboth its length and perpendicularity properties are maintained.\nx\ny\nx'\ny'\ny\nx\nP'\nP\nP\nFigure 4.21. Two ways to interpret a transformation matrix. On the left, the point moves \nagainst a ﬁ xed set of axes. On the right, the axes move in the opposite direction while the \npoint remains ﬁ xed.\n\n\n167 \nIn general, if a point or (non-normal) vector can be rotated from space A to \nspace B via the 3  ×  3 marix \nA\nB\n→\nM\n, then a normal vector n will be transformed \nfrom space A to space B via the inverse transpose of that matrix, \n1\nT\nA\nB\n(\n)\n−\n→\nM\n. We \nwill not prove or derive this result here (see [28], Section 3.5 for an excellent \nderivation). However, we will observe that if the matrix \nA\nB\n→\nM\n contains only \nuniform scale and no shear, then the angles between all surfaces and vectors in \nspace B will be the same as they were in space A. In this case, the matrix \nA\nB\n→\nM\nwill actually work just ﬁ ne for any vector, normal or non-normal. However, \nif \nA\nB\n→\nM\n contains nonuniform scale or shear (i.e., is non-orthogonal), then the \nangles between surfaces and vectors are not preserved when moving from \nspace A to space B. A vector that was normal to a surface in space A will not \nnecessarily be perpendicular to that surface in space B. The inverse transpose \noperation accounts for this distortion, bringing normal vectors back into per-\npendicularity with their surfaces even when the transformation involves non-\nuniform scale or shear.\n4.3.12. Storing Matrices in Memory\n In the C and C++ languages, a two-dimensional array is oft en used to store a \nmatrix. Recall that in C/C++ two-dimensional array syntax, the ﬁ rst subscript \nis the row and the second is the column, and that the column index varies fast-\nest as you move through memory sequentially.\nfloat m[4][4]; // [row][col], col varies fastest\n// \"flatten\" the array to demonstrate ordering\nfloat* pm = &m[0][0]; \nASSERT( &pm[0] == &m[0][0] );\nASSERT( &pm[1] == &m[0][1] );\nASSERT( &pm[2] == &m[0][2] );\n// etc.\nWe have two choices when storing a matrix in a two-dimensional C/C++ \narray. We can either\nstore the vectors (\n1. \niC , jC , kC , tC) contiguously in memory (i.e., each row \ncontains a single vector), or\nstore the vectors \n2. \nstrided in memory (i.e., each column contains one vector).\nThe beneﬁ t of approach (1) is that we can address any one of the four vec-\ntors by simply indexing into the matrix and interpreting the four contiguous \nvalues we ﬁ nd there as a 4-element vector. This layout also has the beneﬁ t of \nmatching up exactly with row vector matrix equations (which is another reason \nwhy I’ve selected row vector notation for this book). Approach (2) is some-\ntimes necessary when doing fast matrix-vector multiplies using a vector-en-\n4.3. Matrices\n\n\n168 \n4. 3D Math for Games\nabled (SIMD) microprocessor, as we’ll see later in this chapter. In most game \nengines I’ve personally encountered, matrices are stored using approach (1), \nwith the vectors in the rows of the two-dimensional C/C++ array. This is shown \nbelow:\nfloat M[4][4];\nM[0][0]=ix;  M[0][1]=iy;  M[0][2]=iz;  M[0][3]=0.0f;\nM[1][0]=jx;  M[1][1]=jy;  M[1][2]=jz;  M[1][3]=0.0f;\nM[2][0]=kx;  M[2][1]=ky;  M[2][2]=kz;  M[2][3]=0.0f;\nM[3][0]=tx;  M[3][1]=ty;  M[3][2]=tz;  M[3][3]=1.0f;\nThe matrix M looks like this when viewed in a debugger:\nM[][]\n [0]\n  [0] \nix\n  [1] \niy\n  [2] \niz\n  [3] \n0.0000\n [1]\n  [0] \njx\n  [1] \njy\n  [2] \njz\n  [3] \n0.0000\n [2]\n  [0] \nkx\n  [1] \nky\n  [2] \nkz\n  [3] \n0.0000\n [3]\n  [0] \ntx\n  [1] \nty\n  [2] \ntz\n  [3] \n1.0000\nOne easy way to determine which layout your engine uses is to ﬁ nd a \nfunction that builds a 4 × 4 translation matrix. (Every good 3D math library \nprovides such a function.) You can then inspect the source code to see where \nthe elements of the t vector are being stored. If you don’t have access to the \nsource code of your math library (which is prett y rare in the game industry), \nyou can always call the function with an easy-to-recognize translation like \n(4, 3, 2), and then inspect the resulting matrix. If row 3 contains the values 4.0, \n3.0, 2.0, 1.0, then the vectors are in the rows, otherwise the vectors are in \nthe columns.\n\n\n169 \n4.4. Quaternions\n4.4. Quaternions\n We’ve seen that a 3  ×  3 matrix can be used to represent an arbitrary rotation in \nthree dimensions. However, a matrix is not always an ideal representation of \na rotation, for a number of reasons:\nWe need nine ﬂ oating-point values to represent a rotation, which seems \n1. \nexcessive considering that we only have three degrees of freedom—\npitch, yaw, and roll.\nRotating a vector requires a vector-matrix multiplication, which involves \n2. \nthree dot products, or a total of nine multiplications and six additions. \nWe would like to ﬁ nd a rotational representation that is less expensive \nto calculate, if possible.\nIn games and computer graphics, it’s oft en important to be able to ﬁ nd \n3. \nrotations that are some percentage of the way between two known rota-\ntions. For example, if we are to smoothly animate a camera from some \nstarting orientation A to some ﬁ nal orientation B over the course of a \nfew seconds, we need to be able to ﬁ nd lots of intermediate rotations be-\ntween A and B over the course of the animation. It turns out to be diﬃ  -\ncult to do this when the A and B orientations are expressed as matrices.\nThankfully, there is a rotational representation that overcomes these three \nproblems. It is a mathematical object known as a quaternion. A quaternion \nlooks a lot like a four-dimensional vector, but it behaves quite diﬀ erently. \nWe usually write quaternions using non-italic, non-boldface type, like this: \nq = [ qx  qy  qz  qw ].\nQuaternions were developed by Sir William Rowan Hamilton in 1843 as \nan extension to the complex numbers. They were ﬁ rst used to solve prob-\nlems in the area of mechanics. Technically speaking, a quaternion obeys a \nset of rules known as a four-dimensional normed division algebra over the real \nnumbers. Thankfully, we won’t need to understand the details of these rather \nesoteric algebraic rules. For our purposes, it will suﬃ  ce to know that the unit-\nlength quaternions (i.e., all quaternions obeying the constraint qx2 + qy2 + qz2 + \nqw2 = 1) represent three-dimensional rotations.\nThere are a lot of great papers, web pages, and presentations on quater-\nnions available on the web, for further reading. Here’s one of my favorites: \nhtt p://graphics.ucsd.edu/courses/cse169_w05/CSE169_04.ppt.\n4.4.1. Unit Quaternions as 3D Rotations\nA unit quaternion can be visualized as a three-dimensional vector plus a \nfourth scalar coordinate. The vector part qV is the unit axis of rotation, scaled \n\n\n170 \n4. 3D Math for Games\nby the sine of the half-angle of the rotation. The scalar part qS is the cosine of \nthe half-angle. So the unit quaternion q can be writt en as follows:\n \n \nwhere a is a unit vector along the axis of rotation, and θ is the angle of rota-\ntion. The direction of the rotation follows the right-hand rule , so if your thumb \npoints in the direction of a, positive rotations will be in the direction of your \ncurved ﬁ ngers.\nOf course, we can also write q as a simple four-element vector:\n \n \nA unit quaternion is very much like an axis+angle representation of a ro-\ntation (i.e., a four-element vector of the form [ a  θ ]). However, quaternions \nare more convenient mathematically than their axis+angle counterparts, as we \nshall see below.\n4.4.2. Quaternion Operations\nQuaternions support some of the familiar operations from vector algebra, \nsuch as magnitude and vector addition. However, we must remember that the \nsum of two unit quaternions does not represent a 3D rotation, because such a \nquaternion would not be of unit length. As a result, you won’t see any quater-\nnion sums in a game engine, unless they are scaled in some way to preserve \nthe unit length requirement.\n4.4.2.1. Quaternion Multiplication\n One of the most important operations we will perform on quaternions is that \nof multiplication. Given two quaternions p and q representing two rotations P \nand Q, respectively, the product pq represents the composite rotation (i.e., ro-\ntation Q followed by rotation P). There are actually quite a few diﬀ erent kinds \nof quaternion multiplication, but we’ll restrict this discussion to the variety \nused in conjunction with 3D rotations, namely the Grassman product. Using \nthis deﬁ nition, the product pq is deﬁ ned as follows:\n \npq\n(\n)\n(\n) .\nS\nV\nS\nV\nV\nV\nS S\nV\nV\np\nq\np q\n⎡\n⎤\n=\n+\n+\n×\n−\n⋅\n⎣\n⎦\nq\np\np\nq\np\nq\n \n2\n2\nq\n[\n]\n[ sin\ncos ],\nV\nS\nq\nθ\nθ\n=\n=\nq\na\n \n \n \n2\n2\n2\n2\n \nq\n[\n], where\nsin ,\nsin ,\nsin ,\n \ncos .\n \nx\ny\nz\nw\nx\nVx\nx\ny\nVy\ny\nz\nVz\nz\nw\nS\nq\nq\nq\nq\nq\nq\na\nq\nq\na\nq\nq\na\nq\nq\nθ\nθ\nθ\nθ\n=\n=\n=\n=\n=\n=\n=\n=\n=\n\n\n171 \n4.4. Quaternions\nNotice how the Grassman product is deﬁ ned in terms of a vector part, which \nends up in the x, y, and z components of the resultant quaternion, and a scalar \npart, which ends up in the w component.\n4.4.2.2. Conjugate and Inverse\n The inverse of a quaternion q is denoted q–1 and is deﬁ ned as a quaternion \nwhich, when multiplied by the original, yields the scalar 1 (i.e., qq–1 = 0i + 0j \n+ 0k + 1). The quaternion [ 0  0  0  1 ] represents a zero rotation (which makes \nsense since sin(0) = 0 for the ﬁ rst three components, and cos(0) = 1 for the last \ncomponent).\nIn order to calculate the inverse of a quaternion, we must ﬁ rst deﬁ ne a \nquantity known as the conjugate. This is usually denoted q* and it is deﬁ ned \nas follows:\n \n \nIn other words, we negate the vector part but leave the scalar part unchaged.\nGiven this deﬁ nition of the quaternion conjugate, the inverse quaternion \nq–1 is deﬁ ned as follows:\n \n \nOur quaternions are always of unit length (i.e., |q| = 1), because they represent \n3D rotations. So, for our purposes, the inverse and the conjugate are identical:\n \n \nThis fact is incredibly useful, because it means we can always avoid doing \nthe (relatively expensive) division by the squared magnitude when inverting \na quaternion, as long as we know a priori that the quaternion is normalized. \nThis also means that inverting a quaternion is generally much faster than in-\nverting a 3  ×  3 matrix—a fact that you may be able to leverage in some situa-\ntions when optimizing your engine.\nConjugate and Inverse of a Product\n The conjugate of a quaternion product (pq) is equal to the reverse product of \nthe conjugates of the individual quaternions:\n \n \nLikewise the inverse of a quaternion product is equal to the reverse product of \nthe inverses of the individual quaternions:\n \n \n(4.3)\n \nq*\n[\n].\nV\nS\nq\n= −q\n \n \n1\n2\nq*\nq\n.\nq\n−=\n \n \n1\nq\nq*\n[\n]     when    q\n1.\nV\nS\nq\n−=\n= −\n=\nq\n(pq)*\nq* p* .\n=\n1\n1\n1\n(pq)\nq\np\n.\n−\n−\n−\n=\n\n\n172 \n4. 3D Math for Games\nThis is analogous to the reversal that occurs when transposing or inverting \nmatrix products.\n4.4.3. Rotating Vectors with Quaternions\nHow can we apply a quaternion rotation to a vector ? The ﬁ rst step is to rewrite \nthe vector in quaternion form . A vector is a sum involving the unit basis vectors \ni, j, and k. A quaternion is a sum involving i, j, and k, but with a fourth scalar \nterm as well. So it makes sense that a vector can be writt en as a quaternion \nwith its scalar term qS equal to zero. Given the vector v, we can write a cor-\nresponding quaternion v = [ v  0 ] = [ vx  vy  vz  0 ].\nIn order to rotate a vector v by a quaternion q, we pre-multiply the vec-\ntor (writt en in its quaternion form v) by q and then post-multiply it by the \ninverse quaternion, q–1. Therefore, the rotated vector v’ can be found as fol-\nlows:\n \n \nThis is equivalent to using the quaternion conjugate, because our quaternions \nare always unit length:\n \n \n(4.4)\nThe rotated vector v’ is obtained by simply extracting it from its quaternion \nform v’.\nQuaternion multiplication can be useful in all sorts of situations in real \ngames. For example, let’s say that we want to ﬁ nd a unit vector describing the \ndirection in which an aircraft  is ﬂ ying. We’ll further assume that in our game, \nthe positive z-axis always points toward the front of an object by convention. \nSo the forward unit vector of any object in model space is always FM  ≡ [ 0  0  1 ] \nby deﬁ nition. To transform this vector into world space, we can simply take \nour aircraft ’s orientation quaternion q and use it with Equation (4.4) to rotate \nour model-space vector FM into its world space equivalent FW (aft er converting \nthese vectors into quaternion form, of course):\n \n \n4.4.3.1. Quaternion Concatenation\n Rotations can be concatenated in exactly the same way that matrix-based trans-\nformations can, by multiplying the quaternions together. For example, consid-\ner three distinct rotations, represented by the quaternions q1 , q2 , and q3 , with \nmatrix equivalents R1 , R2 , and R3. We want to apply rotation 1 ﬁ rst, followed \nby rotation 2 and ﬁ nally rotation 3. The composite rotation matrix Rnet can be \nfound and applied to a vector v as follows:\nv\nrotate(q, )\nqvq* .\n′=\n=\nv\n \n1\n1\nF\nqF q\nq [0\n0\n1\n0] q\n.\nW\nM\n−\n−\n=\n=\n1\nv\nrotate(q, )\nqvq\n.\n−\n′=\n=\nv\n\n\n173 \n4.4. Quaternions\n \n \nLikewise, the composite rotation quaternion qnet can be found and applied to \nvector v (in its quaternion form, v) as follows:\n \n \nNotice how the quaternion product must be performed in an order opposite \nto that in which the rotations are applied (q3q2q1). This is because quaternion \nrotations always multiply on both sides of the vector, with the uninverted \nquaternions on the left  and the inverted quaternions on the right. As we saw \nin Equation (4.3), the inverse of a quaternion product is the reverse product of \nthe individual inverses, so the uninverted quaternions read right-to-left  while \nthe inverted quaternions read left -to-right.\n4.4.4. Quaternion-Matrix Equivalence\n We can convert any 3D rotation freely between a 3  ×  3 matrix representation \nR and a quaternion representation q. If we let q = [ qV  qS ] = [ qVx  qVy  qVz  qS ] = \n[ x  y  z  w ], then we can ﬁ nd R as follows:\n \n \n Likewise, given R we can ﬁ nd q as follows (where q[0] = qVx , q[1] = qVy , \nq[2] = qVz , and q[3] = qS). This code assumes that we are using row vectors \nin C/C++ (i.e., that the rows of matrix R[row][col] correspond to the rows \nof the matrix R shown above). The code was adapted from a Gamasutra article \nby Nick Bobic, published on July 5, 1998, which is available here: htt p://www.\ngamasutra.com/view/feature/3278/rotating_objects_using_quaternions.php. \nFor a discussion of some even faster methods for converting a matrix to a \nquaternion, leveraging various assumptions about the nature of the matrix, \nsee htt p://www.euclideanspace.com/maths/geometry/rotations/conversions/\nmatrixToQuaternion/index.htm.\nvoid matrixToQuaternion(\n \nconst float R[3][3], \n \nfloat   \n  q[/*4*/])\n{\n \nfloat trace = R[0][0] + R[1][1] + R[2][2];\nnet\n1\n2\n3\n1\n2\n3\nnet\n;\n.\n=\n′=\n=\nR\nR R R\nv\nvR R R\nvR\nnet\n3\n2\n1\n1\n1\n1\n1\n3\n2\n1\n1\n2\n3\nnet\nnet\nq\nq q q ;\nv\nq q q vq\nq\nq\nq\nvq\n.\n−\n−\n−\n−\n=\n′=\n=\n \n2\n2\n2\n2\n2\n2\n1\n2\n2\n2\n2\n2\n2\n2\n2\n1\n2\n2\n2\n2\n.\n2\n2\n2\n2\n1\n2\n2\ny\nz\nxy\nzw\nxz\nyw\nxy\nzw\nx\nz\nyz\nxw\nxz\nyw\nyz\nxw\nx\ny\n⎡\n⎤\n−\n−\n+\n−\n⎢\n⎥\n=\n−\n−\n−\n+\n⎢\n⎥\n⎢\n⎥\n+\n−\n−\n−\n⎣\n⎦\nR\n\n\n174 \n4. 3D Math for Games\n \n// check the diagonal\n \nif (trace > 0.0f)\n {\n \n  \nfloat s = sqrt(trace + 1.0f);\n \n  \nq[3] = s * 0.5f;\n \n  \nfloat t = 0.5f / s;\n \n  \nq[0] = (R[2][1] - R[1][2]) * t;\n \n  \nq[1] = (R[0][2] - R[2][0]) * t;\n \n  \nq[2] = (R[1][0] - R[0][1]) * t;\n }\n else\n {\n \n  \n// diagonal is negative\n     \nint i = 0;\n \n  \nif (R[1][1] > R[0][0]) i = 1;\n \n  \nif (R[2][2] > R[i][i]) i = 2;\n \n  \nstatic const int NEXT[3] = {1, 2, 0};\n \n  \nj = NEXT[i];\n \n  \nk = NEXT[j];\n \n  \nfloat s = sqrt ((R[i][i] \n  \n \n    \n     \n      -  (R[j][j] \n+ R[k][k])) \n  \n \n    \n        + \n1.0f);\n \n  \nq[i] = s * 0.5f;\n \n  float t;\n \n  \nif (s != 0.0)  \nt = 0.5f / s;\n  \n \nelse     t \n= s;\n \n  \nq[3] = (R[k][j] - R[j][k]) * t;\n \n  \nq[j] = (R[j][i] + R[i][j]) * t;\n \n  \nq[k] = (R[k][i] + R[i][k]) * t;\n }\n}\n4.4.5. Rotational Linear Interpolation\n Rotational interpolation has many applications in the animation, dynamics \nand camera systems of a game engine. With the help of quaternions, rotations \ncan be easily interpolated just as vectors and points can.\nThe easiest and least computationally intensive approach is to perform \na four-dimensional vector LERP on the quaternions you wish to interpolate. \nGiven two quaternions qA and qB representing rotations A and B, we can \nﬁ nd an intermediate rotation qLERP that is β percent of the way from A to B as \nfollows:\n\n\n175 \nNotice that the resultant interpolated quaternion had to be renormalized. This \nis necessary because the LERP operation does not preserve a vector’s length \nin general.\nGeometrically, qLERP = LERP(qA , qB , β) is the quaternion whose orientation \nlies  β percent of the way from orientation A to orientation B, as shown (in two \ndimensions for clarity) in Figure 4.22. Mathematically, the LERP operation re-\nsults in a weighed average of the two quaternions, with weights (1 – β) and β \n(notice that (1 – β) + β = 1).\n4.4.5.1. Spherical Linear Interpolation\n The problem with the LERP operation is that it does not take account of the \nfact that quaternions are really points on a four-dimensional hypersphere. A \nLERP eﬀ ectively interpolates along a chord of the hypersphere, rather than \nalong the surface of the hypersphere itself. This leads to rotation animations \nthat do not have a constant angular speed when the parameter β is changing \nat a constant rate. The rotation will appear slower at the end points and faster \nin the middle of the animation.\n \nLERP\n(1\n)q\nq\nq\nLERP(q ,q , )\n(1\n)q\nq\n(1\n)\n(1\n)\nnormalize\n.\n(1\n)\n(1\n)\nA\nB\nA\nB\nA\nB\nT\nAx\nBx\nAy\nBy\nAz\nBz\nAw\nBw\nq\nq\nq\nq\nq\nq\nq\nq\n−β\n+ β\n=\nβ =\n−β\n+ β\n⎛\n⎞\n−β\n+ β\n⎡\n⎤\n⎜\n⎟\n⎢\n⎥\n⎜\n⎟\n⎢\n⎥\n−β\n+ β\n⎜\n⎟\n⎢\n⎥\n=\n⎜\n⎟\n⎢\n⎥\n−β\n+ β\n⎜\n⎟\n⎢\n⎥\n⎜\n⎟\n⎢\n⎥\n⎜\n⎟\n−β\n+ β\n⎣\n⎦\n⎝\n⎠\nqA (β = 0)\nqLERP = LERP(qA, qB, 0.4)\nqB (β = 1)\nFigure 4.22. Linear interpolation (LERP) between quaternions qA and qB.\n4.4. Quaternions\n\n\n176 \n4. 3D Math for Games\nTo solve this problem, we can use a variant of the LERP operation known \nas spherical linear interpolation, or SLERP for short. The SLERP operation uses \nsines and cosines to interpolate along a great circle of the 4D hypersphere, \nrather than along a chord, as shown in Figure 4.23. This results in a constant \nangular speed when β varies at a constant rate.\nThe formula for SLERP is similar to the LERP formula, but the weights \n(1 – β) and β are replaced with weights wp and wq involving sines of the angle \nbetween the two quaternions.\nwhere\nThe cosine of the angle between any two unit-length quaternions can \nbe found by taking their four-dimensional dot product. Once we know \ncos(θ), we can calculate the angle θ and the various sines we need quite \neasily:\n \n \nqA (β = 0)\nqLERP = LERP(qA, qB, 0.4)\nqB (β = 1)\nqSLERP = SLERP(qA, qB, 0.4)\n0.4 along chord\n0.4 along arc\nFigure 4.23. Spherical linear interpolation along a great circle arc of a 4D hypersphere.\nSLERP(p,q, )\np\nq,\np\nq\nw\nw\nβ =\n+\nsin((1\n) ) ,\nsin( )\nsin(\n).\nsin( )\np\nq\nw\nw\n−β θ\n=\nθ\nβθ\n=\nθ\n1\ncos( )\np q\n;\ncos\n(p q).\nx x\ny\ny\nz\nz\nw w\np q\np q\np q\np q\n−\nθ =    =\n+\n+\n+\nθ=\n⋅\n⋅\n\n\n177 \n4.4.5.2. To SLERP or Not to SLERP (That’s Still the Question)\nThe jury is still out on whether or not to use SLERP in a game engine. Jonathan \nBlow wrote a great article positing that SLERP is too expensive, and LERP’s \nquality is not really that bad—therefore, he suggests, we should understand \nSLERP but avoid it in our game engines (see htt p://number-none.com/prod-\nuct/Understanding%20Slerp,%20Then%20Not%20Using%20It/index.html). \nOn the other hand, some of my colleagues at Naughty Dog have found that \na good SLERP implementation performs nearly as well as LERP. (For exam-\nple, on the PS3’s SPUs, Naughty Dog’s Ice team’s implementation of SLERP \ntakes 20 cycles per joint, while its LERP implementation takes 16.25 cycles per \njoint.) Therefore, I’d personally recommend that you proﬁ le your SLERP and \nLERP implementations before making any decisions. If the performance hit \nfor SLERP isn’t unacceptable, I say go for it, because it may result in slightly \nbett er-looking animations. But if your SLERP is slow (and you cannot speed \nit up, or you just don’t have the time to do so), then LERP is usually good \nenough for most purposes.\n4.5. Comparison of Rotational Representations\nWe’ve seen that rotations can be represented in quite a few diﬀ erent ways. \nThis section summarizes the most common rotational representations and \noutlines their pros and cons. No one representation is ideal in all situations. \nUsing the information in this section, you should be able to select the best \nrepresentation for a particular application.\n4.5.1. Euler Angles\n We brieﬂ y explored Euler angles in Section 4.3.9.1. A rotation represented via \nEuler angles consists of three scalar values: yaw, pitch, and roll. These quanti-\nties are sometimes represented by a 3D vector [ θY  θP  θR ].\nThe beneﬁ ts of this representation are its simplicity, its small size (three \nﬂ oating-point numbers), and its intuitive nature—yaw, pitch, and roll are easy \nto visualize. You can also easily interpolate simple rotations about a single axis. \nFor example, it’s trivial to ﬁ nd intermediate rotations between two distinct yaw \nangles by linearly interpolating the scalar θY. However, Euler angles cannot be \ninterpolated easily when the rotation is about an arbitrarily-oriented axis.\nIn addition, Euler angles are prone to a condition known as gimbal lock . \nThis occurs when a 90-degree rotation causes one of the three principal axes \nto “collapse” onto another principal axis. For example, if you rotate by 90 \ndegrees about the x-axis, the y-axis collapses onto the z-axis. This prevents \n4.5. Comparison of Rotational Representations\n\n\n178 \n4. 3D Math for Games\nany further rotations about the original y-axis, because rotations about y and z \nhave eﬀ ectively become equivalent.\nAnother problem with Euler angles is that the order in which the rotations \nare performed around each axis matt ers. The order could be PYR, YPR, RYP, \nand so on, and each ordering may produce a diﬀ erent composite rotation. \nNo one standard rotation order exists for Euler angles across all disciplines \n(although certain disciplines do follow speciﬁ c conventions). So the rotation \nangles [ θY  θP  θR ] do not uniquely deﬁ ne a particular rotation—you need to \nknow the rotation order to interpret these numbers properly.\nA ﬁ nal problem with Euler angles is that they depend upon the mapping \nfrom the x-, y -, and z-axes onto the natural front, left /right, and up directions for \nthe object being rotated. For example, yaw is always deﬁ ned as rotation about \nthe up axis, but without additional information we cannot tell whether this \ncorresponds to a rotation about x, y, or z.\n4.5.2. 3 × 3 M atrices\n A 3 × 3 matrix is a convenient and eﬀ ective rotational representation for a \nnumber of reasons. It does not suﬀ er from gimbal lock , and it can represent \narbitrary rotations uniquely. Rotations can be applied to points and vectors in \na straightforward manner via matrix multiplication (i.e., a series of dot prod-\nucts). Most CPUs and all GPUs now have built-in support for hardware-accel-\nerated dot products and matrix multiplication. Rotations can also be reversed \nby ﬁ nding an inverse matrix, which for a pure rotation matrix is the same \nthing as ﬁ nding the transpose—a trivial operation. And 4 × 4 matrices oﬀ er a \nway to represent arbitrary aﬃ  ne transformations—rotations, translations, and \nscaling—in a totally consistent way.\nHowever, rotation matrices are not particularly intuitive. Looking at a big \ntable of numbers doesn’t help one picture the corresponding transformation \nin three-dimensional space. Also, rotation matrices are not easily interpolated. \nFinally, a rotation matrix takes up a lot of storage (nine ﬂ oating-point num-\nbers) relative to Euler angles.\n4.5.3. Axis + Angle\n We can represent rotations as a unit vector deﬁ ning the axis of rotation plus a \nscalar for the angle of rotation. This is known as an axis+angle representation, \nand it is sometimes denoted by the four-dimensional vector [ a  θ ] , where a \nis the axis of rotation and θ the angle in radians. In a right-handed coordinate \nsystem, the direction of a positive rotation is deﬁ ned by the right-hand rule, \nwhile in a left -handed system we use the left -hand rule instead.\n\n\n179 \nThe beneﬁ ts of the axis+angle representation are that it is reasonably intu-\nitive and also compact (only requires four ﬂ oating-point numbers, as opposed \nto the nine required for a 3 × 3 matrix).\nOne important limitation of the axis+angle representation is that rota-\ntions cannot be easily interpolated. Also, rotations in this format cannot be \napplied to points and vectors in a straightforward way—one needs to convert \nthe axis+angle representation into a matrix or quaternion ﬁ rst.\n4.5.4. Quaternions\n As we’ve seen, a unit-length quaternion can represent 3D rotations in a man-\nner analogous to the axis+angle representation. The primary diﬀ erence be-\ntween the two representations is that a quaternion’s axis of rotation is scaled \nby the sine of the half angle of rotation, and instead of storing the angle in the \nfourth component of the vector, we store the cosine of the half angle.\nThe quaternion formulation provides two immense beneﬁ ts over the \naxis+angle representation. First, it permits rotations to be concatenated and \napplied directly to points and vectors via quaternion multiplication. Second, \nit permits rotations to be easily interpolated via simple LERP or SLERP op-\nerations. Its small size (four ﬂ oating-point numbers) is also a beneﬁ t over the \nmatrix formulation.\n4.5.5. SQT Transformations\n By itself, a quaternion can only represent a rotation, whereas a 4  ×  4 matrix \ncan represent an arbitrary aﬃ  ne transformation (rotation, translation, and \nscale). When a quaternion is combined with a translation vector and a scale \nfactor (either a scalar for uniform scaling or a vector for nonuniform scaling), \nthen we have a viable alternative to the 4  ×  4 matrix representation of aﬃ  ne \ntransformations. We sometimes call this an SQT transform, because it contains \na scale factor, a quaternion for rotation, and a translation vector.\nor \nSQT transforms are widely used in computer animation because of their \nsmaller size (eight ﬂ oats for uniform scale, or ten ﬂ oats for nonuniform scale, \nas opposed to the 12 ﬂ oating-point numbers needed for a 4  ×  3 matrix) and \ntheir ability to be easily interpolated. The translation vector and scale factor \nare interpolated via LERP, and the quaternion can be interpolated with either \nLERP or SLERP.\nSQT\n[\nq\n]   (uniform scale ),\ns\ns\n=\nt\nSQT\n[\nq\n]   (non-uniform scale vector ).\n= s\nt\ns\n4.5. Comparison of Rotational Representations\n\n\n180 \n4. 3D Math for Games\n4.5.6. Dual Quaternions\n Complete transformations involving rotation, translation, and scale can be \nrepresented using a mathematical object known as a dual quaternion. A dual \nquaternion is like an ordinary quaternion, except that its four components are \ndual numbers instead of regular real-valued numbers. A dual number can be \nwritt en as the sum of a non-dual part and a dual part as follows: \n0\nˆ\n.\na\na\naε\n=\n+ε\nHere ε is a magical number called the dual unit, deﬁ ned as \n2\n0.\nε =\n (This is \nanalogous to the imaginary number \n1\ni= − used when writing a complex \nnumber as the sum of a real and an imaginary part: \n.\nc\na\nib\n= +\n)\nBecause each dual number can be represented by two real numbers \n(the non-dual and dual parts), a dual quaternion can be represented by an \neight-element vector. It can also be represented as the sum of two ordinary \nquaternions, where the second one is multiplied by the dual unit, as follows: \n \n \nA full discussion of dual numbers and dual quaternions is beyond our \nscope here. However, a number of excellent articles on them exist online and \nin the literature. I recommend starting with htt ps://www.cs.tcd.ie/publica-\ntions/tech-reports/reports.06/TCD-CS-2006-46.pdf.\n4.5.7. Rotations and Degrees of Freedom\nThe term “degrees of freedom ” (or DOF for short) refers to the number of mu-\ntually-independent ways in which an object’s physical state (position and ori-\nentation) can change. You may have encountered the phrase “six degrees of \nfreedom” in ﬁ elds such as mechanics, robotics, and aeronautics. This refers \nto the fact that a three-dimensional object (whose motion is not artiﬁ cially \nconstrained) has three degrees of freedom in its translation (along the x-, y-, \nand z-axes) and three degrees of freedom in its rotation (about the x-, y-, and \nz-axes), for a total of six degrees of freedom.\nThe DOF concept will help us to understand how diﬀ erent rotational rep-\nresentations can employ diﬀ erent numbers of ﬂ oating-point parameters, yet \nall specify rotations with only three degrees of freedom. For example, Euler \nangles require three ﬂ oats, but axis+angle and quaternion representations use \nfour ﬂ oats, and a 3  ×  3 matrix takes up nine ﬂ oats. How can these representa-\ntions all describe 3-DOF rotations?\nThe answer lies in constraints . All 3D rotational representations employ \nthree or more ﬂ oating-point parameters, but some representations also have \none or more constraints on those parameters. The constraints indicate that the \nparameters are not independent—a change to one parameter induces changes \nto the other parameters in order to maintain the validity of the constraint(s). \n0\nˆq\nq\nq .\nε\n=\n+ε\n",
      "page_number": 182,
      "chapter_number": 10,
      "summary": "3D Math for Games\n“A” axes, while the vector PB gives the position of that same point relative to a \ndiﬀ erent set of axes “B.”\nIn physics, a set of coordinate axes represents a frame of reference, so \nwe sometimes refer to a set of axes as a coordinate frame (or just a frame) Key topics include rotation, rotated, and rotate.",
      "keywords": [
        "space",
        "vector",
        "quaternion",
        "rotation",
        "unit basis vector",
        "Matrix",
        "World Space",
        "basis vectors",
        "rotations",
        "child space",
        "coordinate space",
        "coordinate",
        "Model Space",
        "LERP",
        "parent space"
      ],
      "concepts": [
        "rotation",
        "rotated",
        "rotate",
        "rotations",
        "vector",
        "quaternions",
        "space",
        "matrix",
        "coordinate",
        "point"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 1",
          "chapter": 39,
          "title": "Segment 39 (pages 371-380)",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 22,
          "title": "Segment 22 (pages 207-219)",
          "relevance_score": 0.49,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 12,
          "title": "Segment 12 (pages 108-115)",
          "relevance_score": 0.48,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 22,
          "title": "Segment 22 (pages 195-204)",
          "relevance_score": 0.42,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 23,
          "title": "Segment 23 (pages 205-212)",
          "relevance_score": 0.41,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 203-224)",
      "start_page": 203,
      "end_page": 224,
      "detection_method": "synthetic",
      "content": "181 \nIf we subtract the number of constraints from the number of ﬂ oating-point \nparameters, we arrive at the number of degrees of freedom—and this number \nshould always be three for a 3D rotation:\n \nNDOF = Nparameters – Nconstraints. \n(4.5)\nThe following list shows Equation (4.5) in action for each of the rotational \nrepresentations we’ve encountered in this book.\nz Euler Angles. 3 parameters – 0 constraints = 3 DOF.\nz Axis+Angle. 4 parameters – 1 constraint = 3 DOF.\n \nConstraint: Axis is constrained to be unit length.\nz Quaternion. 4 parameters – 1 constraint = 3 DOF.\n \nConstraint: Quaternion is constrained to be unit length.\nz 3  ×  3 Matrix. 9 parameters – 6 constraints = 3 DOF.\n \nConstraints: All three rows and all three columns must be of unit length \n(when treated as three-element vectors).\n4.6. Other Useful Mathematical Objects\nAs game engineers, we will encounter a host of other mathematical objects, \nin addition to points, vectors, matrices and quaternions. This section brieﬂ y \noutlines the most common of these.\n4.6.1. \nLines, Rays, and Line Segments\nAn inﬁ nite line can be represented by a point P0 plus a unit vector u in the \ndirection of the line. A parametric equation of a line traces out every possible \npoint P along the line by starting at the initial point P0 and moving an arbi-\ntrary distance t along the direction of the unit vector u. The inﬁ nitely large set \nof points P becomes a vector function of the scalar parameter t:\n \nP(t) = P0 + t u,  where   –∞ < t < +∞. \n(4.73)\nThis is depicted in Figure 4.24.\nt = 0\nt = 1\nt = 2\nt = 3\nt = –1\nu\nP0\nFigure 4.24. Parametric equation of a line.\n4.6. Other Useful Mathematical Objects\n\n\n182 \n4. 3D Math for Games\nx\nz\ny\nC\nr\nFigure 4.27. Point-radius representation of a sphere.\nA ray is a line that extends to inﬁ nity in only one direction. This is easily \nexpressed as P(t) with the constraint t ≥ 0, as shown in Figure 4.25.\nA line segment is bounded at both ends by P0 and P1. It too can be repre-\nsented by P(t), in either one of the following two ways (where L = P1 – P0 and \nL = |L| is the length of the line segment):\n \n1. P(t) = P0 + tu,  where 0 ≤ t ≤ L, or\n \n2. P(t) = P0 + tL, where 0 ≤ t ≤ 1.\nThe latt er format, depicted in Figure 4.26, is particularly convenient because \nthe parameter t is normalized; in other words, t always goes from zero to one, \nno matt er which particular line segment we are dealing with. This means we \ndo not have to store the constraint L in a separate ﬂ oating-point parameter; it \nis already encoded in the vector L = Lu (which we have to store anyway).\nt = 0\nt = 1\nt = 2\nt = 3\nu\nP0\nFigure 4.25. Parametric equation of a ray.\nt = 0\nt = 1\nL = P1 – P0\nP0\nP1\nt = 0.5\nFigure 4.26. Parametric equation of a line segment, with normalized parameter t.\n4.6.2. Spheres\nSpheres are ubiquitous in game engine programming. A sphere is typically \ndeﬁ ned as a center point C plus a radius r, as shown in Figure 4.27. This packs \n\n\n183 \nnicely into a four-element vector, [ Cx Cy Cz r ]. As we’ll see below when we dis-\ncuss SIMD vector processing, there are distinct beneﬁ ts to being able to pack \ndata into a vector containing four 32-bit ﬂ oats (i.e., a 128-bit package).\n4.6.3. Planes\n A plane is a 2D surface in 3D space. As you may recall from high school alge-\nbra, the equation of a plane is oft en writt en as follows:\n \nAx + By + Cz + D = 0. \nThis equation is satisﬁ ed only for the locus of points P = [ x  y  z ] that lie on \nthe plane.\nPlanes can be represented by a point P0 and a unit vector n that is normal \nto the plane. This is sometimes called point-normal form , as depicted in Fig-\nure 4.28.\nIt’s interesting to note that when the parameters A, B, and C from the tra-\nditional plane equation are interpreted as a 3D vector, that vector lies in the di-\nrection of the plane normal. If the vector [ A  B  C ] is normalized to unit length, \nthen the normalized sub-vector [ a  b  c ] = n, and the normalized parameter \n2\n2\n2\nd\nD\nA\nB\nC\n=\n+\n+\n is just the distance from the plane to the origin . The sign \nof d is positive if the plane’s normal vector (n) is pointing toward the origin \n(i.e., the origin is on the “front” side of the plane) and negative if the normal \nis pointing away from the origin (i.e., the origin is “behind” the plane). In \nfact, the normalized equation ax + by + cz + d = 0 is just another way of writing \n(n x P) = –  d, which means that when any point P on the plane is projected onto \nthe plane normal n, the length of that projection will be –  d.\nA plane can actually be packed into a four-element vector, much like a \nsphere can. To do so, we observe that to describe a plane uniquely, we need \nonly the normal vector n = [ a  b  c ] and the distance from the origin d. The \nfour-element vector L = [ n  d ] = [ a  b  c  d ] is a compact and convenient way \nto represent and store a plane in memory. Note that when P is writt en in ho-\nmogeneous coordinates with w = 1, the equation (L x P) = 0 is yet another way \nof writing (n x P) = –  d. (These equations are satisﬁ ed for all points P that lie \non the plane L.)\nPlanes deﬁ ned in four-element vector form can be easily transformed \nfrom one coordinate space to another. Given a matrix \nA\nB\n→\nM\n that transforms \npoints and (non-normal) vectors from space A to space B, we already know \nthat to transform a normal vector such as the plane’s n vector, we need to use \nthe inverse transpose of that matrix, \n1\nT\nA\nB\n(\n)\n−\n→\nM\n. So it shouldn’t be a big surprise \nto learn that applying the inverse transpose of a matrix to a four-element plane \nvector L will, in fact, correctly transform that plane from space A to space B. \nP0\nn\nFigure 4.28. A plane \nin point-normal form.\n4.6. Other Useful Mathematical Objects\n\n\n184 \n4. 3D Math for Games\nWe won’t derive or prove this result any further here, but a thorough explana-\ntion of why this litt le “trick” works is provided in Section 4.2.3 of [28].\n4.6.4. Axis-Aligned Bounding Boxes (AABB)\nAn axis-aligned bounding box (AABB) is a 3D cuboid whose six rectangular \nfaces are aligned with a particular coordinate frame’s mutually orthogonal \naxes. As such, an AABB can be represented by a six-element vector containing \nthe minimum and maximum coordinates along each of the 3 principal axes, \n[ xmin , xmax , ymin , ymax , zmin , zmax ], or two points Pmin and Pmax.\nThis simple representation allows for a particularly convenient and in-\nexpensive method of testing whether a point P is inside or outside any given \nAABB. We simply test if all of the following conditions are true:\n        Px ≥ xmin  and  Px ≤ xmax  and\n \n        Py ≥ ymin  and  Py ≤ ymax  and \nPz ≥ zmin   and  Pz ≤ zmax.\nBecause intersection tests are so speedy, AABBs are oft en used as an “early \nout” collision check; if the AABBs of two objects do not intersect, then there is \nno need to do a more detailed (and more expensive) collision test.\n4.6.5. Oriented Bounding Boxes (OBB)\nAn oriented bounding box (OBB) is a cuboid that has been oriented so \nas to align in some logical way with the object it bounds. Usually an OBB \naligns with the local-space axes of the object. Hence it acts like an AABB \nin local space, although it may not necessarily align with the world space \naxes.\nVarious techniques exist for testing whether or not a point lies within \nan OBB, but one common approach is to transform the point into the OBB’s \n“aligned” coordinate system and then use an AABB intersection test as pre-\nsented above.\n4.6.6. Frusta\nAs shown in Figure 4.29, a frustum is a group of six planes that deﬁ ne a trun-\ncated pyramid shape. Frusta are commonplace in 3D rendering because they \nconveniently deﬁ ne the viewable region of the 3D world when rendered via a \nperspective projection from the point of view of a virtual camera. Four of the \nplanes bound the edges of the screen space, while the other two planes repre-\nsent the the near and far clipping planes (i.e., they deﬁ ne the minimum and \nmaximum z coordinates possible for any visible point).\nNear\nFar\nLeft\nRight\nTop\nBottom\nFigure 4.29. A frustum.\n\n\n185 \n4.7. Hardware-Accelerated SIMD Math\nOne convenient representation of a frustum is as an array of six planes, \neach of which is represented in point-normal form (i.e., one point and one \nnormal vector per plane).\nTesting whether a point lies inside a frustum is a bit involved, but the basic \nidea is to use dot products to determine whether the point lies on the front or \nback side of each plane. If it lies inside all six planes, it is inside the frustum.\nA helpful trick is to transform the world-space point being tested, by \napplying the camera’s perspective projection to it. This takes the point from \nworld space into a space known as homogeneous clip space. In this space, the \nfrustum is just an axis-aligned cuboid (AABB). This permits much simpler in/\nout tests to be performed.\n4.6.7. Convex Polyhedral Regions\nA convex polyhedral region is deﬁ ned by an arbitrary set of planes, all with nor-\nmals pointing inward (or outward). The test for whether a point lies inside \nor outside the volume deﬁ ned by the planes is relatively straightforward; it \nis similar to a frustum test, but with possibly more planes. Convex regions \nare very useful for implementing arbitrarily-shaped trigger regions in games. \nMany engines employ this technique; for example, the Quake engine’s ubiqui-\ntous brushes are just volumes bounded by planes in exactly this way.\n4.7. Hardware-Accelerated SIMD Math\nSIMD stands for “single instruction multiple data .” This refers to the ability of \nmost modern microprocessors to perform a single mathematical operation on \nmultiple data items in parallel, using a single machine instruction. For exam-\nple, the CPU might multiply four pairs of ﬂ oating-point numbers in parallel \nwith a single instruction. SIMD is widely used in game engine math libraries, \nbecause it permits common vector operations such as dot products and matrix \nmultiplication to be performed extremely rapidly.\nIntel ﬁ rst introduced MMX instructions with their Pentium line of CPUs \nin 1994. These instructions permitt ed SIMD calculations to be performed on \n8-, 16-, and 32-bit integers packed into special 64-bit MMX registers. Intel fol-\nlowed this up with various revisions of an extended instruction set called \nStreaming SIMD Extensions, or SSE, the ﬁ rst version of which appeared in the \nPentium III processor. The SSE instruction set utilizes 128-bit registers that can \ncontain integer or IEEE ﬂ oating-point data.\nThe SSE mode most commonly used by game engines is called packed 32-\nbit ﬂ oating-point mode. In this mode, four 32-bit float values are packed into \n\n\n186 \n4. 3D Math for Games\na single 128-bit register; four operations such as additions or multiplications \nare performed in parallel on four pairs of ﬂ oats using a single instruction. This \nis just what the doctor ordered when multiplying a four-element vector by a \n4  ×  4 matrix!\n4.7.1.1. \nSSE Registers\nIn packed 32-bit ﬂ oating-point mode, each 128-bit SSE register contains four \n32-bit ﬂ oats. The individual ﬂ oats within an SSE register are conveniently re-\nferred to as [ x  y  z  w ], just as they would be when doing vector/matrix math \nin homogeneous coordinates on paper (see Figure 4.30). To see how the SSE \nregisters work, here’s an example of a SIMD instruction:\n \naddps xmm0, xmm1\nThe addps instruction adds the four ﬂ oats in the 128-bit XMM0 register with \nthe four ﬂ oats in the XMM1 register, and stores the four results back into \nXMM0. Put another way:\n \nxmm0.x = xmm0.x + xmm1.x;\n \nxmm0.y = xmm0.y + xmm1.y; \n \nxmm0.z = xmm0.z + xmm1.z;\n \n  xmm0.w = xmm0.w + xmm1.w.\nThe four ﬂ oating-point values stored in an SSE register can be extracted \nto or loaded from memory or registers individually, but such operations tend \nto be comparatively slow. Moving data between the x87 FPU registers and the \nSSE registers is particularly bad, because the CPU has to wait for either the x87 \nor the SSE unit to spit out its pending calculations. This stalls out the CPU’s \nentire instruction execution pipeline and results in a lot of wasted cycles. In a \nnutshell, code that mixes regular float mathematics with SSE mathematics \nshould be avoided like the plague.\nTo minimize the costs of going back and forth between memory, x87 FPU \nregisters, and SSE registers, most SIMD math libraries do their best to leave \ndata in the SSE registers for as long as possible. This means that even scalar \nvalues are left  in SSE registers, rather than transferring them out to float\nvariables. For example, a dot product between two vectors produces a scalar \nresult, but if we leave that result in an SSE register it can be used later in other \nx\ny\nz\nw\n32 bits\n32 bits\n32 bits\n32 bits\nFigure 4.30. The four components of an SSE register in 32-bit ﬂ oating-point mode.\n\n\n187 \n4.7. Hardware-Accelerated SIMD Math\nvector calculations without incurring a transfer cost. Scalars are represented \nby duplicating the single ﬂ oating-point value across all four “slots” in an SSE \nregister. So to store the scalar s in an SSE register, we’d set x = y = z = w = s.\n4.7.1.2. The __m128 Data Type\nUsing one of these magic SSE 128-bit values in C or C++ is quite easy. The \nMicrosoft  Visual Studio compiler provides a predeﬁ ned data type called \n__m128. This data type can be used to declare global variables, automatic vari-\nables, and even class and structure members. In many cases, variables of this \ntype will be stored in RAM. But when used in calculations, __m128 values are \nmanipulated directly in the CPU’s SSE registers. In fact, declaring automatic \nvariables and function arguments to be of type __m128 oft en results in the \ncompiler storing those values directly in SSE registers, rather than keeping \nthem in RAM on the program stack.\nAlignment of __m128 Variables\n When an __m128 variable is stored in RAM, it is the programmer’s responsi-\nbility to ensure that the variable is aligned to a 16-byte address boundary. This \nmeans that the hexadecimal address of an __m128 variable must always end \nin the nibble 0x0. The compiler will automatically pad structures and classes \nso that if the entire struct or class is aligned to a 16-byte boundary, all of the \n__m128 data members within it will be properly aligned as well. If you de-\nclare an automatic or global struct/class containing one or more __m128s, the \ncompiler will align the object for you. However, it is still your responsibility \nto align dynamically allocated data structures (i.e., data allocated with new or \nmalloc()); the compiler can’t help you there.\n4.7.1.3. \nCoding with SSE Intrinsics\nSSE mathematics can be done in raw assembly language, or via inline assem-\nbly in C or C++. However, writing code like this is not only non-portable, it’s \nalso a big pain in the butt . To make life easier, modern compilers provide \nintrinsics —special commands that look and behave like regular C functions, \nbut are really boiled down to inline assembly code by the compiler. Many in-\ntrinsics translate into a single assembly language instruction, although some \nare macros that translate into a sequence of instructions.\nIn order to use the __m128 data type and SSE intrinsics, your .cpp ﬁ le \nmust #include <xmmintrin.h>.\nAs an example, let’s take another look at the addps assembly language \ninstruction. This instruction can be invoked in C/C++ using the intrinsic _mm\n_add_ps(). Here’s a side-by-side comparison of what the code would look \nlike with and without the use of the intrinsic.\n\n\n188 \n4. 3D Math for Games\n__m128 addWithAssembly(\n   __m128 a,\n   __m128 b)\n{\n   __m128 r;\n   __asm\n   {\n      movaps xmm0, \n \n xmmword ptr [a]\n      movaps xmm1, \n \n xmmword ptr [b]\n      addps  xmm0, xmm1\n      movaps xmmword ptr [r],\n \n xmm0\n   }\n   return r;\n}\n__m128 addWithIntrinsics(\n   __m128 a,\n   __m128 b)\n{\n   __m128 r = \n_mm_add_ps(a, b);\n   return r;\n}\nIn the assembly language version, we have to use the __asm keyword to \ninvoke inline assembly instructions, and we must create the linkage between \nthe input parameters a and b and the SSE registers xmm0 and xmm1 manually, \nvia movaps instructions. On the other hand, the version using intrinsics is \nmuch more intuitive and clear, and the code is smaller. There’s no inline as-\nsembly, and the SSE instruction looks just like a regular function call.\nIf you’d like to experiment with these example functions, they can be in-\nvoked via the following test bed main() function. Notice the use of another \nintrinsic, _mm_load_ps(), which loads values from an in-memory array of \nfloats into an __m128 variable (i.e., into an SSE register). Also notice that \nwe are forcing our four global float arrays to be 16-byte aligned via the \n__declspec(align(16)) directive—if we omit these directives, the pro-\ngram will crash.\n#include <xmmintrin.h>\n// ... function definitions from above ...\n__declspec(align(16)) float A[]={2.0f,-1.0f,3.0f,4.0f};\n__declspec(align(16)) float B[]={-1.0f,3.0f,4.0f,2.0f};\n__declspec(align(16)) float C[]={0.0f,0.0f,0.0f,0.0f};\n__declspec(align(16)) float D[]={0.0f,0.0f,0.0f,0.0f};\nint main(int argc, char* argv[])\n{\n \n// load a and b from floating-point data arrays above\n \n__m128 a = _mm_load_ps(&A[0]);\n \n__m128 b = _mm_load_ps(&B[0]);\n\n\n189 \n \n// test the two functions\n \n__m128 c = addWithAssembly(a, b);\n \n__m128 d = addWithIntrinsics(a, b);\n \n// store the original values back to check that they\n \n// weren’t overwritten\n_mm_store_ps(&A[0], a);\n_mm_store_ps(&B[0], b);\n \n// store results into float arrays so we can print  \n \n \n// them\n_mm_store_ps(&C[0], c);\n_mm_store_ps(&D[0], d);\n \n// inspect the results\n \nprintf(“%g %g %g %g\\n”, A[0], A[1], A[2], A[3]);\n \nprintf(“%g %g %g %g\\n”, B[0], B[1], B[2], B[3]);\n \nprintf(“%g %g %g %g\\n”, C[0], C[1], C[2], C[3]);\n \nprintf(“%g %g %g %g\\n”, D[0], D[1], D[2], D[3]);\n \nreturn 0;\n}\n4.7.1.4. Vector-Matrix Multiplication with SSE\n Let’s take a look at how vector-matrix multiplication might be implemented \nusing SSE instructions. We want to multiply the 1 × 4 vector v with the 4 × 4 \nmatrix M to generate a result vector r.\n \n \nThe multiplication involves taking the dot product of the row vector v \nwith the columns of matrix M. So to do this calculation using SSE instructions, \nwe might ﬁ rst try storing v in an SSE register (__m128), and storing each of \nthe columns of M in SSE registers as well. Then we could calculate all of the \nproducts vkMĳ  in parallel using only four mulps instructions, like this:\n11\n12\n13\n14\n21\n22\n23\n24\n31\n32\n33\n34\n41\n42\n43\n44\n11\n12\n13\n14\n21\n22\n23\n24\n31\n32\n33\n34\n41\n42\n43\n44\n;\n[\n]\n[\n] \n(\n(\n(\n(\n)\n)\n)\n)\nx\ny\nz\nw\nx\ny\nz\nw\nx\nx\nx\nx\ny\ny\ny\ny\nz\nz\nz\nz\nw\nw\nw\nw\nM\nM\nM\nM\nM\nM\nM\nM\nr\nr\nr\nr\nv\nv\nv\nv\nM\nM\nM\nM\nM\nM\nM\nM\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\n=\n⎡\n⎤\n⎢\n⎥\n⎢\n⎥\n=\n⎢\n⎥\n⎢\n⎥\n⎣\n⎦\n⎡\n⎤\n⎢\n⎥\n+\n+\n+\n+\n⎢\n⎥\n=⎢\n⎥\n+\n+\n+\n+\n⎢\n⎥\n+\n+\n+\n+\n⎣\n⎦\nr\nvM\n .\n4.7. Hardware-Accelerated SIMD Math\n\n\n190 \n4. 3D Math for Games\n__m128 mulVectorMatrixAttempt1(__m128 v,\n   __m128 Mcol1, __m128 Mcol2, \n   __m128 Mcol3, __m128 Mcol4)\n{\n \n__m128 vMcol1 = _mm_mul_ps(v, Mcol1);\n \n__m128 vMcol2 = _mm_mul_ps(v, Mcol2);\n \n__m128 vMcol3 = _mm_mul_ps(v, Mcol3);\n \n__m128 vMcol4 = _mm_mul_ps(v, Mcol4);\n \n// ... then what?\n}\nThe above code would yield the following intermediate results:\n vMcol1\n = [ vxM11  vyM21  vzM31  vwM41 ];\n \nvMcol2 = [ vxM12  vyM22  vzM32  vwM42 ]; \n \nvMcol3 = [ vxM13  vyM23  vzM33  vwM43 ];\n \nvMcol4 = [ vxM14  vyM24  vzM34  vwM44 ].\nBut the problem with doing it this way is that we now have to add “across \nthe registers” in order to generate the results we need. For example, rx = \n(vxM11 + vyM21 + vzM31 + vwM41), so we’d need to add the four components of \nvMcol1 together. Adding across a register like this is diﬃ  cult and ineﬃ  cient, \nand moreover it leaves the four components of the result in four separate SSE \nregisters, which would need to be combined into the single result vector r. We \ncan do bett er.\nThe “trick” here is to multiply with the rows of M, not its columns. \nThat way, we’ll have results that we can add in parallel, and the ﬁ nal sums \nwill end up in the four components of a single SSE register representing \nthe output vector r. However, we don’t want to multiply v as-is with the \nrows of M—we want to multiply vx with all of row 1, vy with all of row 2, \nvz with all of row 3, and vw with all of row 4. To do this, we need to replicate \na single component of v, such as vx, across a register to yield a vector like \n[ vx  vx  vx  vx ]. Then we can multiply the replicated component vectors by the \nappropriate rows of M.\nThankfully there’s a powerful SSE instruction which can replicate values \nlike this. It is called shufps, and it’s wrapped by the intrinsic _mm_shuffle_\nps(). This beast is a bit complicated to understand, because it’s a general-\npurpose instruction that can shuﬄ  e the components of an SSE register around \nin arbitrary ways. However, for our purposes we need only know that the \nfollowing macros replicate the x, y, z or w components of a vector across an \nentire register:\n#define SHUFFLE_PARAM(x, y, z, w) \\\n   ((x) | ((y) << 2) | ((z) << 4) | ((w) << 6))\n\n\n191 \n#define\n_mm_replicate_x_ps(v) \\\n   _mm_shuffle_ps((v), (v), SHUFFLE_PARAM(0, 0, 0, 0))\n#define\n_mm_replicate_y_ps(v) \\\n   _mm_shuffle_ps((v), (v), SHUFFLE_PARAM(1, 1, 1, 1))\n#define\n_mm_replicate_z_ps(v) \\\n   _mm_shuffle_ps((v), (v), SHUFFLE_PARAM(2, 2, 2, 2))\n#define\n_mm_replicate_w_ps(v) \\\n   _mm_shuffle_ps((v), (v), SHUFFLE_PARAM(3, 3, 3, 3))\nGiven these convenient macros, we can write our vector-matrix multipli-\ncation function as follows:\n__m128 mulVectorMatrixAttempt2(__m128 v,\n   __m128 Mrow1, __m128 Mrow2, \n   __m128 Mrow3, __m128 Mrow4)\n{\n \n__m128 xMrow1 = _mm_mul_ps(_mm_replicate_x_ps(v),  \n \n   Mrow1);\n \n__m128 yMrow2 = _mm_mul_ps(_mm_replicate_y_ps(v),  \n \nMrow2);\n \n__m128 zMrow3 = _mm_mul_ps(_mm_replicate_z_ps(v),  \n \nMrow3);\n \n__m128 wMrow4 = _mm_mul_ps(_mm_replicate_w_ps(v),  \n \n Mrow4);\n \n__m128 result = _mm_add_ps(xMrow1, yMrow2);\n \nresult        = _mm_add_ps(result, zMrow3);\n \nresult        = _mm_add_ps(result, wMrow4);\n \nreturn result;\n}\nThis code produces the following intermediate vectors:\n xMrow1\n = [ vxM11  vxM12  vxM13  vxM14 ];\n \nyMrow2 = [ vyM21  vyM22  vyM23  vyM24 ]; \n                                   zMrow3 = [ vzM31  vzM32  vzM33  vzM34 ];\n \n    wMrow4 = [ vwM41  vwM42  vwM43  vwM44 ].\nAdding these four vectors in parallel produces our result r:\n \n \n4.7. Hardware-Accelerated SIMD Math\n \n11\n12\n13\n14\n21\n22\n23\n24\n31\n32\n33\n34\n41\n42\n43\n44\n(\n(\n(\n(\n.\n)\n)\n)\n)\nx\nx\nx\nx\ny\ny\ny\ny\nz\nz\nz\nz\nw\nw\nw\nw\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\n⎡\n⎤\n⎢\n⎥\n+\n+\n+\n+\n⎢\n⎥\n=⎢\n⎥\n+\n+\n+\n+\n⎢\n⎥\n+\n+\n+\n+\n⎣\n⎦\nr\n\n\n192 \n4. 3D Math for Games\nOn some CPUs, the code shown above can be optimized even further by \nusing a rather handy multiply-and-add instruction, usually denoted madd. This \ninstruction multiplies its ﬁ rst two arguments and then adds the result to its \nthird argument. Unfortunately SSE doesn’t support a madd instruction, but we \ncan fake it reasonably well with a macro like this:\n#define _mm_madd_ps(a, b, c) \\\n \n_mm_add_ps(_mm_mul_ps((a), (b)), (c))\n__m128 mulVectorMatrixFinal(__m128 v,\n   __m128 Mrow1, __m128 Mrow2, \n   __m128 Mrow3, __m128 Mrow4)\n{\n \n__m128 result;\n \nresult = _mm_mul_ps (_mm_replicate_x_ps(v), Mrow1);\n \nresult = _mm_madd_ps(_mm_replicate_y_ps(v), Mrow2,  \n \n result);\n \nresult = _mm_madd_ps(_mm_replicate_z_ps(v), Mrow3,  \n \n result);\n \nresult = _mm_madd_ps(_mm_replicate_w_ps(v), Mrow4,  \n \n result);\n \nreturn result;\n}\nWe can of course perform matrix-matrix multiplication using a similar \napproach. Check out htt p://msdn.microsoft .com for a full listing of the SSE \nintrinsics for the Microsoft  Visual Studio compiler.\n4.8. Random Number Generation\nRandom numbers are ubiquitous in game engines, so it behooves us to have \na brief look at the two most common random number generators, the linear \ncongruential generator and the Mersenne Twister. It’s important to realize that \nrandom number generators are just very complicated but totally deterministic \npre-deﬁ ned sequences of numbers. For this reason, we call the sequences they \nproduce pseudo-random. What diﬀ erentiates a good generator from a bad one \nis how long the sequence of numbers is before it repeats (its period), and how \nwell the sequences hold up under various well-known randomness tests.\n4.8.1. Linear Congruential Generators\nLinear congruential generators are a very fast and simple way to generate a \nsequence of pseudo-random numbers. Depending on the platform, this algo-\nrithm is sometimes used in the standard C library’s rand() function. How-\n\n\n193 \never, your mileage may vary, so don’t count on rand() being based on any \nparticular algorithm. If you want to be sure, you’ll be bett er oﬀ  implementing \nyour own random number generator.\nThe linear congruential algorithm is explained in detail in the book Nu-\nmerical Recipes in C, so I won’t go into the details of it here.\nWhat I will say is that this random number generator does not produce \nparticularly high-quality pseudo-random sequences. Given the same initial \nseed value, the sequence is always exactly the same. The numbers produced \ndo not meet many of the criteria widely accepted as desirable, such as a long \nperiod, low- and high-order bits that have similarly-long periods, and absence \nof sequential or spatial correlation between the generated values.\n4.8.2. Mersenne Twister\nThe Mersenne Twister pseudo-random number generator algorithm was de-\nsigned speciﬁ cally to improve upon the various problems of the linear con-\ngruential algorithm. Wikipedia provides the following description of the ben-\neﬁ ts of the algorithm:\nIt was designed to have a colossal period of 2\n1. \n19937 − 1 (the creators of the \nalgorithm proved this property). In practice, there is litt le reason to use \nlarger ones, as most applications do not require 219937 unique combina-\ntions (219937 ≈ 4.3 × 106001).\nIt has a very high order of dimensional equidistribution (see linear \n2. \ncongruential generator). Note that this means, by default, that there is \nnegligible serial correlation between successive values in the output se-\nquence.\nIt passes numerous tests for statistical randomness, including the strin-\n3. \ngent Diehard tests.\nIt is fast.\n4. \nVarious implementations of the Twister are available on the web, includ-\ning a particularly cool one that uses SIMD vector instructions for an extra \nspeed boost, called SFMT (SIMD-oriented fast Mersenne Twister). SFMT can \nbe downloaded from htt p://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/\nSFMT/index.html.\n4.8.3. Mother-of-All and Xorshift\nIn 1994, George Marsaglia, a computer scientist and mathematician best known \nfor developing the Diehard batt ery of tests of randomness (htt p://www.stat.\nfsu.edu/pub/diehard), published a pseudo-random number generation algo-\n4.8. Random Number Generation\n\n\n194 \n4. 3D Math for Games\nrithm that is much simpler to implement and runs faster than the Mersenne \nTwister algorithm. He claimed that it could produce a sequence of 32-bit pseu-\ndo-random numbers with a period of non-repetition of 2250. It passed all of the \nDiehard tests and still stands today as one of the best pseudo-random number \ngenerators for high-speed applications. He called his algorithm the Mother of \nAll Pseudo-Random Number Generators , because it seemed to him to be the only \nrandom number generator one would ever need.\nLater, Marsaglia published another generator called Xorshift  , which is be-\ntween Mersenee and Mother-of-All in terms of randomness, but runs slightly \nfaster than Mother.\nYou can read about George Marsaglia at htt p://en.wikipedia.org/wiki/\nGeorge_Marsaglia, and about the Mother-of-All generator at ft p://ft p.forth.\norg/pub/C/mother.c and at htt p://www.agner.org/random. You can down-\nload a PDF of George’s paper on Xorshift  at htt p://www.jstatsoft .org/v08/i14/\npaper.\n\n\nPart II\nLow-Level \nEngine Systems\n\n\n5\nEngine Support Systems\nE\nvery game engine requires some low-level support systems that manage \nmundane but crucial tasks, such as starting up and shutt ing down the en-\ngine, conﬁ guring engine and game features, managing the engine’s memory \nusage, handling access to ﬁ le system(s), providing access to the wide range of \nheterogeneous asset types used by the game (meshes, textures, animations, \naudio, etc.), and providing debugging tools for use by the game development \nteam. This chapter will focus on the lowest-level support systems found in \nmost game engines. In the chapters that follow, we will explore some of the \nlarger core systems, including resource management, human interface devic-\nes, and in-game debugging tools.\n5.1. \nSubsystem Start-Up and Shut-Down\n A game engine is a complex piece of soft ware consisting of many interacting \nsubsystems. When the engine ﬁ rst starts up, each subsystem must be conﬁ g-\nured and initialized in a speciﬁ c order. Interdependencies between subsys-\ntems implicitly deﬁ ne the order in which they must be started—i.e., if sub-\nsystem B depends on subsystem A, then A will need to be started up before B \ncan be initialized. Shut-down typically occurs in the reverse order, so B would \nshut down ﬁ rst, followed by A.\n197\n\n\n198 \n5. Engine Support Systems\n5.1.1. \nC++ Static Initialization Order (or Lack Thereof)\n Since the programming language used in most modern game engines is C++, \nwe should brieﬂ y consider whether C++’s native start-up and shut-down se-\nmantics can be leveraged in order to start up and shut down our engine’s sub-\nsystems. In C++, global and static objects are constructed before the program’s \nentry point (main(), or WinMain() under Windows) is called. However, \nthese constructors are called in a totally unpredictable order. The destructors \nof global and static class instances are called aft er main() (or WinMain()) \nreturns, and once again they are called in an unpredictable order. Clearly this \nbehavior is not desirable for initializing and shutt ing down the subsystems \nof a game engine, or indeed any soft ware system that has interdependencies \nbetween its global objects.\nThis is somewhat unfortunate, because a common design patt ern for im-\nplementing major subsystems such as the ones that make up a game engine \nis to deﬁ ne a singleton class (oft en called a manager ) for each subsystem. If C++ \ngave us more control over the order in which global and static class instances \nwere constructed and destroyed, we could deﬁ ne our singleton instances as \nglobals, without the need for dynamic memory allocation. For example, we \ncould write: \nclass RenderManager\n{\npublic:\n RenderManager()\n {\n \n \n// start up the manager...\n }\n ~RenderManager()\n {\n \n \n// shut down the manager...\n }\n \n// ...\n};\n// singleton instance\nstatic RenderManager gRenderManager;\nAlas, with no way to directly control construction and destruction order, this \napproach won’t work.\n5.1.1.1. \nConstruct On Demand\n There is one C++ “trick” we can leverage here. A static variable that is declared \nwithin a function will not be constructed before main() is called, but rather \n\n\n199 \n5.1. Subsystem Start-Up and Shut-Down\non the ﬁ rst invocation of that function. So if our global singleton is function-\nstatic, we can control the order of construction for our global singletons.\nclass RenderManager\n{\npublic:\n \n// Get the one and only instance.\nstatic RenderManager& get()\n {\n \n \n// This function-static will be constructed on the\n \n \n// first call to this function.\nstatic RenderManager sSingleton;\n  return \nsSingleton;\n }\n RenderManager()\n {\n \n \n// Start up other managers we depend on, by  \n \n \n \n \n// calling their get() functions first...\nVideoManager::get();\nTextureManager::get();\n \n \n// Now start up the render manager.\n  // \n...\n }\n ~RenderManager()\n {\n \n \n// Shut down the manager.\n  // \n...\n }\n};\nYou’ll ﬁ nd that many soft ware engineering textbooks suggest this de-\nsign, or a variant that involves dynamic allocation of the singleton as shown \nbelow.\nstatic RenderManager& get()\n {\n  static \nRenderManager* gpSingleton = NULL;\n \n \nif (gpSingleton == NULL)\n  {\n   gpSingleton \n= new RenderManager;\n  }\n  ASSERT(gpSingleton);\n  return \n*gpSingleton;\n }\n\n\n200 \n5. Engine Support Systems\nUnfortunately, this still gives us no way to control destruction order. It \nis possible that C++ will destroy one of the managers upon which the \nRenderManager depends for its shut-down procedure, prior to the \nRenderManager’s destructor being called. In addition, it’s diﬃ  cult to predict \nexactly when the RenderManager singleton will be constructed, because the \nconstruction will happen on the ﬁ rst call to RenderManager::get()—and \nwho knows when that might be? Moreover, the programmers using the class \nmay not be expecting an innocuous-looking get() function to do something \nexpensive, like allocating and initializing a heavy-weight singleton. This is an \nunpredictable and dangerous design. Therefore we are prompted to resort to \na more direct approach that gives us greater control.\n5.1.2. \nA Simple Approach That Works\n Let’s presume that we want to stick with the idea of singleton managers for \nour subsystems. In this case, the simplest “brute-force” approach is to deﬁ ne \nexplicit start-up and shut-down functions for each singleton manager class. \nThese functions take the place of the constructor and destructor, and in fact \nwe should arrange for the constructor and destructor to do absolutely nothing. \nThat way, the start-up and shut-down functions can be explicitly called in the \nrequired order from within main() (or from some over-arching singleton object \nthat manages the engine as a whole). For example:\nclass RenderManager\n{\npublic:\n RenderManager()\n {\n// do nothing\n }\n ~RenderManager()\n {\n// do nothing\n }\n void \nstartUp()\n {\n \n \n// start up the manager...\n }\n void \nshutDown()\n {\n \n \n// shut down the manager...\n }\n\n\n201 \n \n// ...\n};\nclass PhysicsManager    { /* similar... */ };\nclass AnimationManager  { /* similar... */ };\nclass MemoryManager     { /* similar... */ };\nclass FileSystemManager { /* similar... */ };\n// ...\nRenderManager           gRenderManager;\nPhysicsManager          gPhysicsManager;\nAnimationManager        gAnimationManager;\nTextureManager          gTextureManager;\nVideoManager            gVideoManager;\nMemoryManager           gMemoryManager;\nFileSystemManager       gFileSystemManager;\n// ...\nint main(int argc, const char* argv)\n{\n \n// Start up engine systems in the correct order.\n gMemoryManager.\nstartUp();\n gFileSystemManager.\nstartUp();\n gVideoManager.\nstartUp();\n gTextureManager.\nstartUp();\n gRenderManager.\nstartUp();\n gAnimationManager.\nstartUp();\n gPhysicsManager.\nstartUp();\n \n// ...\n \n// Run the game.\n gSimulationManager.\nrun();\n \n// Shut everything down, in reverse order.\n \n// ...\n gPhysicsManager.\nshutDown();\n gAnimationManager.\nshutDown();\n gRenderManager.\nshutDown();\n gFileSystemManager.\nshutDown();\n gMemoryManager.\nshutDown();\n \nreturn 0;\n}\n5.1. Subsystem Start-Up and Shut-Down\n\n\n202 \n5. Engine Support Systems\nThere are “more elegant” ways to accomplish this. For example, you could \nhave each manager register itself into a global priority queue and then walk \nthis queue to start up all the managers in the proper order. You could deﬁ ne \nthe manger-to-manager dependency graph by having each manager explicitly \nlist the other managers upon which it depends and then write some code to \ncalculate the optimal start-up order given their interdependencies. You could \nuse the construct-on-demand approach outlined above. In my experience, the \nbrute-force approach always wins out, because:\nIt’s simple and easy to implement.\n• \nIt’s explicit. You can see and understand the start-up order immediately \n• \nby just looking at the code.\nIt’s easy to debug and maintain. If something isn’t starting early enough, \n• \nor is starting too early, you can just move one line of code.\nOne minor disadvantage to the brute-force manual start-up and shut-\ndown method is that you might accidentally shut things down in an order \nthat isn’t strictly the reverse of the start-up order. But I wouldn’t lose any sleep \nover it. As long as you can start up and shut down your engine’s subsystems \nsuccessfully, you’re golden.\n5.1.3. \nSome Examples from Real Engines\nLet’s take a brief look at some examples of engine start-up and shut-down \ntaken from real game engines.\n5.1.3.1. \nOgre3D\nOgre3D is by its authors’ admission a rendering engine, not a game engine \nper se. But by necessity it provides many of the low-level features found in \nfull-ﬂ edged game engines, including a simple and elegant start-up and shut-\ndown mechanism. Everything in Ogre is controlled by the singleton object \nOgre::Root. It contains pointers to every other subsystem in Ogre and man-\nages their creation and destruction. This makes it very easy for a programmer \nto start up Ogre—just new an instance of Ogre::Root and you’re done.\nHere are a few excerpts from the Ogre source code so we can see what \nit’s doing:\nOgreRoot.h\nclass _OgreExport Root : public Singleton<Root>\n{\n // \n<some code omitted...>\n \n// Singletons\n \nLogManager* mLogManager;\n",
      "page_number": 203,
      "chapter_number": 11,
      "summary": "Other Useful Mathematical Objects\nAs game engineers, we will encounter a host of other mathematical objects, \nin addition to points, vectors, matrices and quaternions Key topics include vectors, points, and instruction.",
      "keywords": [
        "SSE",
        "SSE Registers",
        "vector",
        "SIMD Math",
        "Hardware-Accelerated SIMD Math",
        "plane",
        "game engine",
        "engine",
        "SSE instruction",
        "game",
        "result",
        "Math",
        "register",
        "Engine Support Systems",
        "order"
      ],
      "concepts": [
        "vectors",
        "points",
        "instruction",
        "instructions",
        "engineers",
        "engineering",
        "ordered",
        "order",
        "bit",
        "bits"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "More Effective C++",
          "chapter": 32,
          "title": "Segment 32 (pages 320-329)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 63,
          "title": "Segment 63 (pages 609-616)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 8,
          "title": "Segment 8 (pages 62-70)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 4,
          "title": "Segment 4 (pages 60-78)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 225-246)",
      "start_page": 225,
      "end_page": 246,
      "detection_method": "synthetic",
      "content": "203 \n \nControllerManager* mControllerManager;\n \nSceneManagerEnumerator* mSceneManagerEnum;\n \nSceneManager* mCurrentSceneManager;\n \nDynLibManager* mDynLibManager;\n \nArchiveManager* mArchiveManager;\n \nMaterialManager* mMaterialManager;\n \nMeshManager* mMeshManager;\n \nParticleSystemManager* mParticleManager;\n \nSkeletonManager* mSkeletonManager;\n \nOverlayElementFactory* mPanelFactory;\n \nOverlayElementFactory* mBorderPanelFactory;\n \nOverlayElementFactory* mTextAreaFactory;\n \nOverlayManager* mOverlayManager;\n \nFontManager* mFontManager;\n \nArchiveFactory *mZipArchiveFactory;\n \nArchiveFactory *mFileSystemArchiveFactory;\n \nResourceGroupManager* mResourceGroupManager;\n \nResourceBackgroundQueue* mResourceBackgroundQueue;\n \nShadowTextureManager* mShadowTextureManager;\n // \netc.\n};\nOgreRoot.cpp\nRoot::Root(const String& pluginFileName,\n \n \n \n const String& configFileName, \n \n \n \n const String& logFileName): \n \n \n  mLogManager(0), \n \n \n  mCurrentFrame(0),\n \n \n  mFrameSmoothingTime(0.0f),\n \n \n  mNextMovableObjectTypeFlag(1),\n \n \n  mIsInitialised(false)\n{\n \n// superclass will do singleton checking\n \nString msg;\n \n// Init\n \nmActiveRenderer = 0;\n mVersion\n  = \nStringConverter::toString(OGRE_VERSION_MAJOR)\n  + \n\".\"\n  + \nStringConverter::toString(OGRE_VERSION_MINOR) \n  + \n\".\"\n  + \nStringConverter::toString(OGRE_VERSION_PATCH)\n \n \n+ OGRE_VERSION_SUFFIX + \" \"\n \n \n+ \"(\" + OGRE_VERSION_NAME + \")\";\n \nmConfigFileName = configFileName;\n \n// Create log manager and default log file if there   \n \n// is no log manager yet\n5.1. Subsystem Start-Up and Shut-Down\n\n\n204 \n5. Engine Support Systems\n \nif(LogManager::getSingletonPtr() == 0)\n {\n \n \nmLogManager = new LogManager();\n \n \nmLogManager->createLog(logFileName, true, true);\n }\n \n// Dynamic library manager\n \nmDynLibManager = new DynLibManager();\n \nmArchiveManager = new ArchiveManager();\n \n// ResourceGroupManager\n \nmResourceGroupManager = new ResourceGroupManager();\n \n// ResourceBackgroundQueue\n mResourceBackgroundQueue \n \n \n= new ResourceBackgroundQueue();\n // \nand so on...\nOgre provides a templated Ogre::Singleton base class from which all of its \nsingleton (manager) classes derive. If you look at its implementation, you’ll \nsee that Ogre::Singleton does not use deferred construction, but instead \nrelies on Ogre::Root to explicitly new each singleton. As we discussed above, \nthis is done to ensure that the singletons are created and destroyed in a well-\ndeﬁ ned order.\n5.1.3.2. Naughty Dog’s Uncharted: Drake’s Fortune\nThe Uncharted: Drake’s Fortune engine created by Naughty Dog Inc. uses a \nsimilar explicit technique for starting up its subsystems. You’ll notice by look-\ning at the following code that engine start-up is not always a simple sequence \nof allocating singleton instances. A wide range of operating system services, \nthird party libraries, and so on must all be started up during engine initial-\nization. Also, dynamic memory allocation is avoided wherever possible, so \nmany of the singletons are statically-allocated objects (e.g., g_fileSystem, \ng_languageMgr, etc.) It’s not always prett y, but it gets the job done.\nErr BigInit()\n{\n init_exception_handler();\n \nU8* pPhysicsHeap = new(kAllocGlobal, kAlign16)  \n \n \n  U8[ALLOCATION_GLOBAL_PHYS_HEAP]; \n PhysicsAllocatorInit(pPhysicsHeap, \n  ALLOCATION_GLOBAL_PHYS_HEAP);\n g_textDb.Init();\n g_textSubDb.Init();\n\n\n205 \n5.2. Memory Management\n g_spuMgr.Init();\n g_drawScript.InitPlatform();\n PlatformUpdate();\n \nthread_t init_thr;\n \nthread_create(&init_thr, threadInit, 0, 30,  \n \n \n \n \n \n64*1024, 0, \"Init\");\n \nchar masterConfigFileName[256];\n snprintf(masterConfigFileName, \n \n       \n  sizeof(masterConfigFileName), \n  MASTER_CFG_PATH);\n {\n \n \nErr err = ReadConfigFromFile(\n   masterConfigFileName);\n  if \n(err.Failed())\n  {\n \n \n \nMsgErr(\"Config file not found (%s).\\n\",  \n \n \n    masterConfigFileName);\n  }\n }\n \nmemset(&g_discInfo, 0, sizeof(BootDiscInfo));\n \nint err1 = GetBootDiscInfo(&g_discInfo);\n \nMsg(\"GetBootDiscInfo() : 0x%x\\n\", err1);\n \nif(err1 == BOOTDISCINFO_RET_OK)\n {\n \n \nprintf(\"titleId       : [%s]\\n\", \n   g_discInfo.titleId);\n \n \nprintf(\"parentalLevel : [%d]\\n\", \n   g_discInfo.parentalLevel);\n }\n g_fileSystem.Init(g_gameInfo.m_onDisc);\n g_languageMgr.Init();\n \nif (g_shouldQuit) return Err::kOK;\n // \nand so on...\n5.2. Memory Management\n As game developers, we are always trying to make our code run more quickly. \nThe performance of any piece of soft ware is dictated not only by the algo-\nrithms it employs, or the eﬃ  ciency with which those algorithms are coded, \n\n\n206 \n5. Engine Support Systems\nbut also by how the program utilizes memory (RAM). Memory aﬀ ects perfor-\nmance in two ways:\n \n1. Dynamic memory allocation via malloc() or C++’s global operator new\nis a very slow operation. We can improve the performance of our code \nby either avoiding dynamic allocation altogether or by making use of \ncustom memory allocators that greatly reduce allocation costs.\nOn modern CPUs, the performance of a piece of soft ware is oft en \n2. \ndominated by its memory access patt erns . As we’ll see, data that is located \nin small, contiguous blocks of memory can be operated on much more \neﬃ  ciently by the CPU than if that same data were to be spread out across \na wide range of memory addresses. Even the most eﬃ  cient algorithm, \ncoded with the utmost care, can be brought to its knees if the data upon \nwhich it operates is not laid out eﬃ  ciently in memory.\nIn this section, we’ll learn how to optimize our code’s memory utilization \nalong these two axes.\n5.2.1. \nOptimizing Dynamic Memory Allocation\n Dynamic memory allocation via malloc() and free() or C++’s global new\nand delete operators—also known as heap allocation—is typically very slow. \nThe high cost can be att ributed to two main factors. First, a heap allocator is \na general-purpose facility, so it must be writt en to handle any allocation size, \nfrom one byte to one gigabyte. This requires a lot of management overhead, \nmaking the malloc() and free() functions inherently slow. Second, on most \noperating systems a call to malloc() or free() must ﬁ rst context-switch from \nuser mode into kernel mode, process the request, and then context-switch \nback to the program. These context switches can be extraordinarily expensive. \nOne rule of thumb oft en followed in game development is:\nKeep heap allocations to a minimum, and never allocate from the \nheap within a tight loop.\nOf course, no game engine can entirely avoid dynamic memory alloca-\ntion, so most game engines implement one or more custom allocators. A \ncustom allocator can have bett er performance characteristics than the oper-\nating system’s heap allocator for two reasons. First, a custom allocator can \nsatisfy requests from a preallocated memory block (itself allocated using \nmalloc() or new, or declared as a global variable). This allows it to run in \nuser mode and entirely avoid the cost of context-switching into the operat-\n\n\n207 \ning system. Second, by making various assumptions about its usage pat-\nterns, a custom allocator can be much more eﬃ  cient than a general-purpose \nheap allocator.\nIn the following sections, we’ll take a look at some common kinds of cus-\ntom allocators. For additional information on this topic, see Christian Gyr-\nling’s excellent blog post, htt p://www.swedishcoding.com/2008/08/31/are-we-\nout-of-memory.\n5.2.1.1. \nStack-Based Allocators\n Many games allocate memory in a stack-like fashion. Whenever a new game \nlevel is loaded, memory is allocated for it. Once the level has been loaded, \nlitt le or no dynamic memory allocation takes place. At the conclusion of \nthe level, its data is unloaded and all of its memory can be freed. It makes \na lot of sense to use a stack-like data structure for these kinds of memory \nallocations.\nA stack allocator is very easy to implement. We simply allocate a large con-\ntiguous block of memory using malloc() or global new, or by declaring a \nglobal array of bytes (in which case the memory is eﬀ ectively allocated out of \nthe executable’s BSS segment). A pointer to the top of the stack is maintained. \nAll memory addresses below this pointer are considered to be in use, and all \naddresses above it are considered to be free. The top pointer is initialized to \nthe lowest memory address in the stack. Each allocation request simply moves \nthe pointer up by the requested number of bytes. The most-recently allocated \nblock can be freed by simply moving the top pointer back down by the size \nof the block.\nIt is important to realize that with a stack allocator, memory cannot be \nfreed in an arbitrary order. All frees must be performed in an order oppo-\nsite to that in which they were allocated. One simple way to enforce these \nrestrictions is to disallow individual blocks from being freed at all. Instead, \nwe can provide a function that rolls the stack top back to a previously-marked \nlocation, thereby freeing all blocks between the current top and the roll-back \npoint.\nIt’s important to always roll the top pointer back to a point that lies \nat the boundary between two allocated blocks, because otherwise new al-\nlocations would overwrite the tail end of the top-most block. To ensure \nthat this is done properly, a stack allocator oft en provides a function that \nreturns a marker representing the current top of the stack. The roll-back \nfunction then takes one of these markers as its argument. This is depicted \nin Figure 5.1. The interface of a stack allocator oft en looks something like \nthis.\n5.2. Memory Management\n\n\n208 \n5. Engine Support Systems\nclass StackAllocator\n{\npublic:\n \n// Stack marker: Represents the current top of the  \n \n \n// stack. You can only roll back to a marker, not to  \n \n// arbitrary locations within the stack.\n \ntypedef U32 Marker;\n \n// Constructs a stack allocator with the given total   \n \n// size.\n explicit \nStackAllocator(U32 stackSize_bytes);\n \n// Allocates a new block of the given size from stack  \n \n// top.\n void* \nalloc(U32 size_bytes);\n \n// Returns a marker to the current stack top.\n Marker \ngetMarker();\n \n// Rolls the stack back to a previous marker.\n void \nfreeToMarker(Marker marker);\n \n// Clears the entire stack (rolls the stack back to   \n \n// zero).\n void \nclear();\nObtain marker after allocating blocks A and B.\nA\nB\nAllocate additional blocks C , D and E.\nA\nB\nC\nD\nE\nFree back to marker.\nA\nB\nFigure 5.1. Stack allocation, and freeing back to a marker.\n\n\n209 \nprivate:\n \n// ...\n};\nDouble-Ended Stack Allocators\nA single memory block can actually contain two stack allocators—one which \nallocates up from the bott om of the block and one which allocates down from \nthe top of the block. A double-ended stack allocator is useful because it uses \nmemory more eﬃ  ciently by allowing a trade-oﬀ  to occur between the memory \nusage of the bott om stack and the memory usage of the top stack. In some situ-\nations, both stacks may use roughly the same amount of memory and meet in \nthe middle of the block. In other situations, one of the two stacks may eat up \na lot more memory than the other stack, but all allocation requests can still be \nsatisﬁ ed as long as the total amount of memory requested is not larger than \nthe block shared by the two stacks. This is depicted in Figure 5.2.\nIn Midway’s Hydro Thunder arcade game, all memory allocations are \nmade from a single large block of memory managed by a double-ended stack \nallocator. The bott om stack is used for loading and unloading levels (race \ntracks), while the top stack is used for temporary memory blocks that are al-\nlocated and freed every frame. This allocation scheme worked extremely well \nand ensured that Hydro Thunder never suﬀ ered from memory fragmentation \nproblems (see Section 5.2.1.4). Steve Ranck, Hydro Thunder’s lead engineer, de-\nscribes this allocation technique in depth in [6], Section 1.9.\nLower\nUpper\nFigure 5.2. A double-ended stack allocator.\n5.2. Memory Management\n5.2.1.2. Pool Allocators\n It’s quite common in game engine programming (and soft ware engineering in \ngeneral) to allocate lots of small blocks of memory, each of which are the same \nsize. For example, we might want to allocate and free matrices, or iterators, or \nlinks in a linked list, or renderable mesh instances. For this type of memory \nallocation patt ern, a pool allocator is oft en the perfect choice.\nA pool allocator works by preallocating a large block of memory whose \nsize is an exact multiple of the size of the elements that will be allocated. For \nexample, a pool of 4  ×  4 matrices would be an exact multiple of 64 bytes (16 el-\nements per matrix times four bytes per element). Each element within the pool \nis added to a linked list of free elements; when the pool is ﬁ rst initialized, the \nfree list contains all of the elements. Whenever an allocation request is made, \n\n\n210 \n5. Engine Support Systems\nwe simply grab the next free element oﬀ  the free list and return it. When an \nelement is freed, we simply tack it back onto the free list. Both allocations and \nfrees are O(1) operations, since each involves only a couple of pointer ma-\nnipulations, no matt er how many elements are currently free. (The notation \nO(1) is an example of big “O” notation. In this case it means that the execution \ntime of both allocations and frees are roughly constant and do not depend on \nthings like the number of elements currently in the pool. See Section 5.3.3 for \nan explanation of big “O” notation.)\nThe linked list of free elements can be a singly-linked list, meaning that \nwe need a single pointer (four bytes on most machines) for each free ele-\nment. Where should we obtain the memory for these pointers? Certainly \nthey could be stored in a separate preallocated memory block, occupying \n(sizeof(void*) * numElementsInPool) bytes. However, this is unduly \nwasteful. We need only realize that the blocks on the free list are, by deﬁ nition, \nfree memory blocks. So why not use the free blocks themselves to store the \nfree list’s “next” pointers? This litt le “trick” works as long as elementSize >=\nsizeof(void*).\nIf each element is smaller than a pointer, then we can use pool element in-\ndices instead of pointers to implement our linked list. For example, if our pool \ncontains 16-bit integers, then we can use 16-bit indices as the “next pointers” \nin our linked list. This works as long as the pool doesn’t contain more than 216\n= 65,536 elements.\n5.2.1.3. Aligned Allocations\nAs we saw in Section 3.2.5.1, every variable and data object has an alignment \nrequirement. An 8-bit integer variable can be aligned to any address, but a \n32-bit integer or ﬂ oating-point variable must be 4-byte aligned, meaning its \naddress can only end in the nibbles 0x0, 0x4, 0x8 or 0xC. A 128-bit SIMD vector \nvalue generally has a 16-byte alignment requirement, meaning that its mem-\nory address can end only in the nibble 0x0. On the PS3, memory blocks that \nare to be transferred to an SPU via the direct memory access (DMA) controller \nshould be 128-bit aligned for maximum DMA throughput, meaning they can \nonly end in the bytes 0x00 or 0x80.\nAll memory allocators must be capable of returning aligned memory \nblocks. This is relatively straightforward to implement. We simply allocate \na litt le bit more memory than was actually requested, adjust the address of \nthe memory block upward slightly so that it is aligned properly, and then re-\nturn the adjusted address. Because we allocated a bit more memory than was \nrequested, the returned block will still be large enough, even with the slight \nupward adjustment.\n\n\n211 \nIn most implementations, the number of additional bytes allocated is \nequal to the alignment. For example, if the request is for a 16-byte aligned \nmemory block, we would allocate 16 additional bytes. This allows for the \nworst-case address adjustment of 15 bytes, plus one extra byte so that we can \nuse the same calculations even if the original block is already aligned. This \nsimpliﬁ es and speeds up the code at the expense of one wasted byte per al-\nlocation. It’s also important because, as we’ll see below, we’ll need those extra \nbytes to store some additional information that will be used when the block \nis freed.\nWe determine the amount by which the block’s address must be adjusted \nby masking oﬀ  the least-signiﬁ cant bits of the original block’s memory ad-\ndress, subtracting this from the desired alignment, and using the result as \nthe adjustment oﬀ set. The alignment should always be a power of two (four-\nbyte and 16-byte alignments are typical), so to generate the mask we simply \nsubtract one from the alignment. For example, if the request is for a 16-byte \naligned block, then the mask would be (16 – 1) = 15 = 0x0000000F. Taking \nthe bitwise AND of this mask and any misaligned address will yield the \namount by which the address is misaligned. For example, if the originally-\nallocated block’s address is 0x50341233, ANDing this address with the mask \n0x0000000F yields 0x00000003, so the address is misaligned by three bytes. \nTo align the address, we add (alignment – misalignment) = (16 – 3) = 13 = \n0xD bytes to it. The ﬁ nal aligned address is therefore 0x50341233 + 0xD = \n0x50341240.\nHere’s one possible implementation of an aligned memory allocator:\n// Aligned allocation function. IMPORTANT: 'alignment'   \n// must be a power of 2 (typically 4 or 16).\nvoid* allocateAligned(U32 size_bytes, U32 alignment)\n{\n \n// Determine total amount of memory to allocate.\n \nU32 expandedSize_bytes = size_bytes + alignment;\n \n// Allocate an unaligned block & convert address to a  \n \n// U32.\n U32 \nrawAddress\n  = \n(U32)allocateUnaligned(expandedSize_bytes);\n \n// Calculate the adjustment by masking off the lower   \n \n// bits of the address, to determine how \"misaligned\"  \n \n// it is.\n \nU32 mask = (alignment – 1);\n \nU32 misalignment = (rawAddress & mask);\n U32 \nadjustment = alignment – misalignment;\n5.2. Memory Management\n\n\n212 \n5. Engine Support Systems\n \n// Calculate the adjusted address, and return as a  \n \n \n// pointer.\n \nU32 alignedAddress = rawAddress + adjustment;\n \nreturn (void*)alignedAddress;\n}\nWhen this block is later freed, the code will pass us the adjusted address, \nnot the original address we allocated. How, then, do we actually free the mem-\nory? We need some way to convert an adjusted address back into the original, \npossibly misaligned address.\nTo accomplish this, we simply store some meta-information in those \nextra bytes we allocated in order to align the data in the ﬁ rst place. The \nsmallest adjustment we might make is one byte. That’s enough room to \nstore the number of bytes by which the address was adjusted (since it will \nnever be more than 256). We always store this information in the byte im-\nmediately preceding the adjusted address (no matt er how many bytes of \nadjustment we actually added), so that it is trivial to ﬁ nd it again, given the \nadjusted address. Here’s how the modiﬁ ed allocateAligned() function \nwould look.\n// Aligned allocation function. IMPORTANT: ‘alignment’   \n// must be a power of 2 (typically 4 or 16).\nvoid* allocateAligned(U32 size_bytes, U32 alignment)\n{\n \n// Clients must call allocateUnaligned() and\n \n// freeUnaligned() if alignment == 1.\n \nASSERT(alignment > 1);\n \n// Determine total amount of memory to allocate.\n \nU32 expandedSize_bytes = size_bytes + alignment;\n \n// Allocate an unaligned block & convert address to a  \n \n// U32.\n \nU32 rawAddress\n  = \n(U32)allocateUnaligned(expandedSize_bytes);\n \n// Calculate the adjustment by masking off the lower   \n \n// bits of the address, to determine how “misaligned”  \n \n// it is.\n \nU32 mask = (alignment – 1);\n \nU32 misalignment = (rawAddress & mask);\n \nU32 adjustment = alignment – misalignment;\n \n// Calculate the adjusted address, and return as a  \n \n \n// pointer.\n \nU32 alignedAddress = rawAddress + adjustment;\n\n\n213 \n \n// Store the adjustment in the four bytes immediately\n \n// preceding the adjusted address that we’re   \n \n \n \n// returning.\n U32* \npAdjustment = (U32*)(alignedAddress – 4);\n*pAdjustment = adjustment;\n \nreturn (void*)alignedAddress;\n}\nAnd here’s how the corresponding freeAligned() function would be imple-\nmented.\nvoid freeAligned(void* p)\n{\n \nU32 alignedAddress = (U32)p;\n \nU8* pAdjustment = (U8*)(alignedAddress – 4);\n U32 \nadjustment = (U32)*pAdjustment;\n \nU32 rawAddress = alignedAddress – adjustment;\nfreeUnaligned((void*)rawAddress);\n}\n5.2.1.4. Single-Frame and Double-Buffered Memory Allocators\nVirtually all game engines allocate at least some temporary data during the \ngame loop. This data is either discarded at the end of each iteration of the loop \nor used on the next frame and then discarded. This allocation patt ern is so \ncommon that many engines support single- and double-buﬀ ered allocators.\nSingle-Frame Allocators\nA single-frame allocator is implemented by reserving a block of memory and \nmanaging it with a simple stack allocator as described above. At the beginning \nof each frame, the stack’s “top” pointer is cleared to the bott om of the memory \nblock. Allocations made during the frame grow toward the top of the block. \nRinse and repeat.\nStackAllocator g_singleFrameAllocator;\n// Main Game Loop\nwhile (true) \n{\n \n// Clear the single-frame allocator’s buffer every  \n \n \n// frame.\n g_singleFrameAllocator.\nclear();\n5.2. Memory Management\n\n\n214 \n5. Engine Support Systems\n \n// ...\n \n// Allocate from the single-frame buffer. We never \n \n// need to free this data! Just be sure to use it  \n \n \n// only this frame.\n \nvoid* p = g_singleFrameAllocator.alloc(nBytes);\n \n// ...\n}\nOne of the primary beneﬁ ts of a single-frame allocator is that allocated \nmemory needn’t ever be freed—we can rely on the fact that the allocator will \nbe cleared at the start of every frame. Single-frame allocators are also blind-\ningly fast. The one big negative is that using a single-frame allocator requires \na reasonable level of discipline on the part of the programmer. You need to \nrealize that a memory block allocated out of the single-frame buﬀ er will only \nbe valid during the current frame. Programmers must never cache a pointer to \na single-frame memory block across the frame boundary!\nDouble-Buffered Allocators\nA double-buﬀ ered allocator allows a block of memory allocated on frame i to \nbe used on frame (i + 1). To accomplish this, we create two single-frame stack \nallocators of equal size and then ping-pong between them every frame.\nclass DoubleBufferedAllocator\n{\n U32 \nm_curStack;\n StackAllocator \nm_stack[2];\npublic:\n void \nswapBuffers()\n {\n \n \nm_curStack = (U32)!m_curStack;\n }\n void \nclearCurrentBuffer()\n {\n  m_stack[m_curStack].\nclear();\n }\n void* \nalloc(U32 nBytes)\n {\n  return \nm_stack[m_curStack].alloc(nBytes);\n }\n \n// ...\n};\n\n\n215 \n// ...\nDoubleBufferedAllocator g_doubleBufAllocator;\n// Main Game Loop\nwhile (true) \n{\n //\nClear the single-frame allocator every frame as   \n// before.\ng_singleFrameAllocator.clear();\n \n// Swap the active and inactive buffers of the double\n \n// buffered allocator.\n g_doubleBufAllocator.\nswapBuffers();\n \n// Now clear the newly active buffer, leaving last  \n \n \n// frame’s buffer intact.\n g_doubleBufAllocator.\nclearCurrentBuffer();\n \n// ...\n \n// Allocate out of the current buffer, without  \n \n \n \n// disturbing last frame’s data. Only use this data   \n // \nthis frame or next frame. Again, this memory never\n \n// needs to be freed.\n \nvoid* p = g_doubleBufAllocator.alloc(nBytes);\n \n// ...\n}\nThis kind of allocator is extremely useful for caching the results of asyn-\nchronous processing on a multicore game console like the Xbox 360 or the \nPLAYSTATION 3. On frame i, we can kick oﬀ  an asynchronous job on one of \nthe PS3’s SPUs, handing it the address of a destination buﬀ er that has been \nallocated from our double-buﬀ ered allocator. The job runs and produces its \nresults some time before the end of frame i, storing them into the buﬀ er we \nprovided. On frame (i + 1), the buﬀ ers are swapped. The results of the job \nare now in the inactive buﬀ er, so they will not be overwritt en by any double-\nbuﬀ ered allocations that might be made during this frame. As long as we use \nthe results of the job before frame (i + 2), our data won’t be overwritt en.\n5.2.2. Memory Fragmentation\n Another problem with dynamic heap allocations is that memory can become \nfragmented over time. When a program ﬁ rst runs, its heap memory is entirely \nfree. When a block is allocated, a contiguous region of heap memory of the \n5.2. Memory Management\n\n\n216 \n5. Engine Support Systems\nappropriate size is marked as “in use,” and the remainder of the heap remains \nfree. When a block is freed, it is marked as such, and adjacent free blocks are \nmerged into a single, larger free block. Over time, as allocations and dealloca-\ntions of various sizes occur in random order, the heap memory begins to look \nlike a patchwork of free and used blocks. We can think of the free regions as \n“holes” in the fabric of used memory. When the number of holes becomes \nlarge, and/or the holes are all relatively small, we say the memory has become \nfragmented. This is illustrated in Figure 5.3.\nThe problem with memory fragmentation is that allocations may fail \neven when there are enough free bytes to satisfy the request. The crux of the \nproblem is that allocated memory blocks must always be contiguous. For ex-\nample, in order to satisfy a request of 128 kB, there must exist a free “hole” \nthat is 128 kB or larger. If there are 2 holes, each of which is 64 kB in size, then \nenough bytes are available but the allocation fails because they are not contigu-\nous bytes.\nfree\nAfter one allocation...\nAfter eight allocations...\nAfter eight allocations and three frees...\nAfter n allocations and m frees...\nfree\nused\nFigure 5.3.  Memory fragmentation.\n\n\n217 \nMemory fragmentation is not as much of a problem on operating sys-\ntems that support virtual memory . A virtual memory system maps discontigu-\nous blocks of physical memory known as pages into a virtual address space, in \nwhich the pages appear to the application to be contiguous. Stale pages can \nbe swapped to the hard disk when physical memory is in short supply and \nreloaded from disk when they are needed. For a detailed discussion of how \nvirtual memory works, see htt p://lyle.smu.edu/~kocan/7343/fall05/slides/\nchapter08.ppt. Most embedded systems cannot aﬀ ord to implement a virtual \nmemory system. While some modern consoles do technically support it, most \nconsole game engines still do not make use of virtual memory due to the in-\nherent performance overhead.\n5.2.2.1. Avoiding Fragmentation with Stack and Pool Allocators\n The detrimental eﬀ ects of memory fragmentation can be avoided by using \nstack and/or pool allocators.\nA stack allocator is impervious to fragmentation because allocations are \n• \nalways contiguous, and blocks must be freed in an order opposite to \nthat in which they were allocated. This is illustrated in Figure 5.4.\nA pool allocator is also free from fragmentation problems. Pools \n• \ndo be-\ncome fragmented, but the fragmentation never causes premature out-\nof-memory conditions as it does in a general-purpose heap. Pool alloca-\ntion requests can never fail due to a lack of a large enough contiguous \nfree block, because all of the blocks are exactly the same size. This is \nshown in Figure 5.5.\n5.2. Memory Management\nFigure 5.4.  A stack allocator is free from fragmentation problems.\nSingle free block, always contiguous\nAllocated blocks, always contiguous\ndeallocation\nallocation\nAllocated and free blocks all the same size\nFigure 5.5.  A pool allocator is not degraded by fragmentation.\n\n\n218 \n5. Engine Support Systems\n5.2.2.2. Defragmentation and Relocation\n When diﬀ erently-sized objects are being allocated and freed in a random or-\nder, neither a stack-based allocator nor a pool-based allocator can be used. In \nsuch cases, fragmentation can be avoided by periodically defragmenting the \nheap. Defragmentation involves coalescing all of the free “holes” in the heap \nby shift ing allocated blocks from higher memory addresses down to lower \naddresses (thereby shift ing the holes up to higher addresses). One simple al-\ngorithm is to search for the ﬁ rst “hole” and then take the allocated block im-\nmediately above the hole and shift  it down to the start of the hole. This has the \neﬀ ect of “bubbling up” the hole to a higher memory address. If this process is \nrepeated, eventually all the allocated blocks will occupy a contiguous region \nof memory at the low end of the heap’s address space, and all the holes will \nhave bubbled up into one big hole at the high end of the heap. This is illus-\ntrated in Figure 5.6.\nThe shift ing of memory blocks described above is not particularly tricky \nto implement. What is tricky is accounting for the fact that we’re moving al-\nlocated blocks of memory around. If anyone has a pointer into one of these al-\nlocated blocks, then moving the block will invalidate the pointer.\nThe solution to this problem is to patch any and all pointers into a shift ed \nmemory block so that they point to the correct new address aft er the shift . \nThis procedure is known as pointer relocation . Unfortunately, there is no gen-\neral-purpose way to ﬁ nd all the pointers that point into a particular region \nA\nB\nC\nD\nE\nA\nB\nC\nD\nE\nA\nB\nC\nD\nE\nA\nB\nC\nD\nE\nA\nB\nC\nD\nE\nFigure 5.6.  Defragmentation by shifting allocated blocks to lower addresses.\n\n\n219 \nof memory. So if we are going to support memory defragmentation in our \ngame engine, programmers must either carefully keep track of all the pointers \nmanually so they can be relocated, or pointers must be abandoned in favor of \nsomething inherently more amenable to relocation, such as smart pointers or \nhandles.\nA smart pointer is a small class that contains a pointer and acts like a \npointer for most intents and purposes. But because a smart pointer is a class, \nit can be coded to handle memory relocation properly. One approach is to \narrange for all smart pointers to add themselves to a global linked list. When-\never a block of memory is shift ed within the heap, the linked list of all smart \npointers can be scanned, and each pointer that points into the shift ed block of \nmemory can be adjusted appropriately.\nA handle is usually implemented as an index into a non-relocatable ta-\nble which itself contains the pointers. When an allocated block is shift ed in \nmemory, the handle table can be scanned and all relevant pointers found and \nupdated automatically. Because the handles are just indices into the pointer \ntable, their values never change no matt er how the memory blocks are shift ed, \nso the objects that use the handles are never aﬀ ected by memory relocation.\nAnother problem with relocation arises when certain memory blocks can-\nnot be relocated. For example, if you are using a third-party library that does \nnot use smart pointers or handles, it’s possible that any pointers into its data \nstructures will not be relocatable. The best way around this problem is usu-\nally to arrange for the library in question to allocate its memory from a special \nbuﬀ er outside of the relocatable memory area. The other option is to simply \naccept that some blocks will not be relocatable. If the number and size of the \nnon-relocatable blocks are both small, a relocation system will still perform \nquite well.\nIt is interesting to note that all of Naughty Dog’s engines have supported \ndefragmentation. Handles are used wherever possible to avoid the need to re-\nlocate pointers. However, in some cases raw pointers cannot be avoided. These \npointers are carefully tracked and relocated manually whenever a memory \nblock is shift ed due to defragmentation. A few of Naughty Dog’s game object \nclasses are not relocatable for various reasons. However, as mentioned above, \nthis doesn’t pose any practical problems, because the number of such objects \nis always very small, and their sizes are tiny when compared to the overall \nsize of the relocatable memory area.\nAmortizing Defragmentation Costs\nDefragmentation can be a slow operation because it involves copying memory \nblocks. However, we needn’t fully defragment the heap all at once. Instead, \nthe cost can be amortized over many frames. We can allow up to N allocated \n5.2. Memory Management\n\n\n220 \n5. Engine Support Systems\nblocks to be shift ed each frame, for some small value of N like 8 or 16. If our \ngame is running at 30 frames per second, then each frame lasts 1/30 of a sec-\nond (33 ms). So the heap can usually be completely defragmented in less than \none second without having any noticeable eﬀ ect on the game’s frame rate. As \nlong as allocations and deallocations aren’t happening at a faster rate than \nthe defragmentation shift s, the heap will remain mostly defragmented at all \ntimes.\nThis approach is only valid when the size of each block is relatively small, \nso that the time required to move a single block does not exceed the time al-\nlott ed to relocation each frame. If very large blocks need to be relocated, we \ncan oft en break them up into two or more subblocks, each of which can be \nrelocated independently. This hasn’t proved to be a problem in Naughty Dog’s \nengine, because relocation is only used for dynamic game objects, and they \nare never larger than a few kilobytes—and usually much smaller.\n5.2.3. Cache Coherency\n To understand why memory access patt erns aﬀ ect performance, we need \nﬁ rst to understand how modern processors read and write memory. Access-\ning main system RAM is always a slow operation, oft en taking thousands of \nprocessor cycles to complete. Contrast this with a register access on the CPU \nitself, which takes on the order of tens of cycles or sometimes even a single \ncycle. To reduce the average cost of reading and writing to main RAM, mod-\nern processors utilize a high-speed memory cache.\nA cache is a special type of memory that can be read from and writt en to \nby the CPU much more quickly than main RAM. The basic idea of memory \ncaching is to load a small chunk of memory into the high-speed cache when-\never a given region of main RAM is ﬁ rst read. Such a memory chunk is called \na cache line and is usually between 8 and 512 bytes, depending on the micro-\nprocessor architecture. On subsequent read operations, if the requested data \nalready exists in the cache, it is loaded from the cache directly into the CPU’s \nregisters—a much faster operation than reading from main RAM. Only if the \nrequired data is not already in the cache does main RAM have to be accessed. \nThis is called a cache miss . Whenever a cache miss occurs, the program is forced \nto wait for the cache line to be refreshed from main RAM.\nSimilar rules may apply when writing data to RAM. The simplest kind \nof cache is called a write-through cache ; in such a cache design, all writes to \nthe cache are simply mirrored to main RAM immediately. However, in a \nwrite-back (or copy-back ) cache design, data is ﬁ rst writt en into the cache and \nthe cache line is only ﬂ ushed out to main RAM under certain circumstances, \nsuch as when a dirty cache line needs to be evicted in order to read in a new \n\n\n221 \ncache line from main RAM, or when the program explicitly requests a ﬂ ush to \noccur.\nObviously cache misses cannot be totally avoided, since data has to move \nto and from main RAM eventually. However, the trick to high-performance \ncomputing is to arrange your data in RAM and code your algorithms in such \na way that the minimum number of cache misses occur. We’ll see exactly how \nto accomplish this below.\n5.2.3.1. Level 1 and Level 2 Caches\nWhen caching techniques were ﬁ rst developed, the cache memory was locat-\ned on the motherboard, constructed from a faster and more expensive type \nof memory module than main RAM in order to give it the required boost in \nspeed. However, cache memory was expensive, so the cache size was usually \nquite small—on the order of 16 kB. As caching techniques evolved, an even \nfaster type of cache memory was developed that was located on the CPU die \nitself. This gave rise to two distinct types of cache memory: an on-die level 1 \n(L1) cache and an on-motherboard level 2 (L2) cache. More recently, the L2 \ncache has also migrated onto the CPU die (see Figure 5.7).\nThe rules for moving data back and forth between main RAM are of course \ncomplicated by the presence of a level 2 cache. Now, instead of data hopping \nfrom RAM to cache to CPU and back again, it must make two hops—ﬁ rst from \nmain RAM to the L2 cache, and then from L2 cache to L1 cache. We won’t go \ninto the speciﬁ cs of these rules here. (They diﬀ er slightly from CPU to CPU \nanyway.) But suﬃ  ce it to say that RAM is slower than L2 cache memory, and \nL2 cache is slower than L1 cache. Hence L2 cache misses are usually more \nexpensive than L1 cache misses, all other things being equal.\n5.2. Memory Management\nCPU Die\nCPU\nL1\nCache\nL2\nCache\nMain RAM\nslower\nslowest\nfast\nFigure 5.7.  Level 1 and level 2 caches.\n\n\n222 \n5. Engine Support Systems\nA load-hit-store is a particularly bad kind of cache miss, prevalent on the \nPowerPC architectures found in the Xbox 360 and PLAYSTATION 3, in which \nthe CPU writes data to a memory address and then reads the data back before \nit has had a chance to make its way through the CPU’s instruction pipeline and \nout into the L1 cache. See htt p://assemblyrequired.crashworks.org/2008/07/08/\nload-hit-stores-and-the-_ _restrict-keyword for more details.\n5.2.3.2. Instruction Cache and Data Cache\nWhen writing high-performance code for a game engine or for any other per-\nformance-critical system, it is important to realize that both data and code are \ncached. The instruction cache (I-cache) is used to preload executable machine \ncode before it runs, while the data cache (D-cache) is used to speed up reading \nand writing of data to main RAM. Most processors separate the two caches \nphysically. Hence it is possible for a program to slow down because of an I-\ncache miss or because of a D-cache miss.\n5.2.3.3. Avoiding Cache Misses\nThe best way to avoid D-cache misses is to organize your data in contiguous \nblocks that are as small as possible and then access them sequentially. This \nyields the minimum number of cache misses. When the data is contiguous \n(i.e., you don’t “jump around” in memory a lot), a single cache miss will load \nthe maximum amount of relevant data in one go. When the data is small, it \nis more likely to ﬁ t into a single cache line (or at least a minimum number \nof cache lines). And when you access your data sequentially (i.e., you don’t \n“jump around” within the contiguous memory block), you achieve the mini-\nmum number of cache misses, since the CPU never has to reload a cache line \nfrom the same region of RAM.\nAvoiding I-cache misses follows the same basic principle as avoiding D-\ncache misses. However, the implementation requires a diﬀ erent approach. \nThe compiler and linker dictate how your code is laid out in memory, so you \nmight think you have litt le control over I-cache misses. However, most C/C++ \nlinkers follow some simple rules that you can leverage, once you know what \nthey are:\nThe machine code for a single function is almost always contiguous in \n• \nmemory. That is, the linker almost never splits a function up in order \nto intersperse another function in the middle. (Inline functions are the \nexception to this rule—more on this topic below.)\nFunctions are laid out in memory in the order they appear in the \n• \ntranslation unit’s source code (.cpp ﬁ le).\n\n\n223 \nTherefore, functions in a single translation unit are always contiguous \n• \nin memory. That is, the linker never splits up a complied translation unit \n(.obj ﬁ le) in order to intersperse code from some other translation unit.\nSo, following the same principles that apply to avoiding D-cache misses, \nwe should follow the rules of thumb listed below.\nKeep high-performance code \n• \nas small as possible, in terms of number of \nmachine language instructions. (The compiler and linker take care of \nkeeping our functions contiguous in memory.)\nAvoid calling functions\n• \n from within a performance-critical section of \ncode.\nIf you do have to call a function, place it as \n• \nclose as possible to the calling \nfunction—preferably immediately before or aft er the calling function \nand never in a diﬀ erent translation unit (because then you completely \nlose control over its proximity to the calling function).\nUse inline functions judiciously. Inlining a small function can be a big \n• \nperformance boost. However, too much inlining bloats the size of the \ncode, which can cause a performance-critical section of code to no \nlonger ﬁ t within the cache. Let’s say we write a tight loop that processes \na large amount of data—if the entire body of that loop doesn’t ﬁ t into \nthe cache, then we are signing up for two I-cache misses during every \niteration of the loop. In such a situation, it is probably best to rethink the \nalgorithm and/or implementation so that less code is required within \ncritical loops.\n5.3. Containers\nGame programmers employ a wide variety of collection-oriented data struc-\ntures, also known as containers or collections. The job of a container is always \nthe same—to house and manage zero or more data elements; however, the \ndetails of how they do this varies greatly, and each type of container has its \npros and cons. Common container data types include, but are certainly not \nlimited to, the following.\nArray\n• \n . An ordered, contiguous collection of elements accessed by index. \nThe length of the array is usually statically deﬁ ned at compile time. It \nmay be multidimensional. C and C++ support these natively (e.g., int\na[5]).\nDynamic array\n• \n . An array whose length can change dynamically at \nruntime (e.g., STL’s std::vector)\n5.3. Containers\n\n\n224 \n5. Engine Support Systems\nLinked list\n• \n . An ordered collection of elements not stored contiguously \nin memory but rather linked to one another via pointers (e.g., STL’s \nstd::list).\nStack\n• \n . A container that supports the last-in-ﬁ rst-out (LIFO) model \nfor adding and removing elements, also known as push/pop (e.g., \nstd::stack).\nQueue\n• \n . A container that supports the ﬁ rst-in-ﬁ rst-out (FIFO) model for \nadding and removing elements (e.g., std::queue).\nDeque\n• \n . A double-ended queue—supports eﬃ  cient insertion and removal \nat both ends of the array (e.g., std::deque).\nPriority queue\n• \n . A container that permits elements to be added in any or-\nder and then removed in an order deﬁ ned by some property of the ele-\nments themselves (i.e., their priority). It can be thought of as a list that \nstays sorted at all times. A priority queue is typically implemented as a \nbinary search tree (e.g., std::priority_queue).\nTree\n• \n . A container in which elements are grouped hierarchically. Each ele-\nment (node) has zero or one parent and zero or more children. A tree is \na special case of a DAG (see below).\nBinary search tree (BST)\n• \n  . A tree in which each node has at most two chil-\ndren, with an order property to keep the nodes sorted by some well-de-\nﬁ ned criteria. There are various kinds of binary search trees, including \nred-black trees, splay trees, SVL trees, etc.\nBinary heap\n• \n . A binary tree that maintains itself in sorted order, much like \na binary search tree, via two rules: the shape property, which speciﬁ es that \nthe tree must be fully ﬁ lled and that the last row of the tree is ﬁ lled from \nleft  to right; and the heap property, which states that every node is, by some \nuser-deﬁ ned criterion, “greater than” or “equal to” all of its children.\nDictionary\n• \n . A table of key-value pairs. A value can be “looked up” ef-\nﬁ ciently given the corresponding key. A dictionary is also known as a \nmap or hash table, although technically a hash table is just one possible \nimplementation of a dictionary (e.g., std::map, std::hash_map).\nSet\n• \n . A container that guarantees that all elements are unique according to \nsome criteria. A set acts like a dictionary with only keys, but no values.\nGraph\n• \n . A collection of nodes connected to one another by unidirectional \nor bidirectional pathways in an arbitrary patt ern.\nDirected acyclic graph (DAG)\n• \n . A collection of nodes with unidirectional \n(i.e., directed) interconnections, with no cycles (i.e., there is no non-empty \npath that starts and ends on the same node).\n",
      "page_number": 225,
      "chapter_number": 12,
      "summary": "Ogre provides a templated Ogre::Singleton base class from which all of its \nsingleton (manager) classes derive Key topics include allocating, allocators, and allocations.",
      "keywords": [
        "memory",
        "memory block",
        "cache",
        "Engine Support Systems",
        "block",
        "Memory Management",
        "dynamic memory allocation",
        "stack allocator",
        "main RAM",
        "stack",
        "allocator",
        "Support Systems",
        "cache memory",
        "memory allocation",
        "RAM"
      ],
      "concepts": [
        "allocating",
        "allocators",
        "allocations",
        "allocate",
        "block",
        "cache",
        "caching",
        "aligned",
        "alignments",
        "free"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 22,
          "title": "Segment 22 (pages 204-211)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 21,
          "title": "Segment 21 (pages 193-203)",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 6,
          "title": "Segment 6 (pages 48-56)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 2,
          "title": "Segment 2 (pages 11-18)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 43,
          "title": "Segment 43 (pages 858-879)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 247-268)",
      "start_page": 247,
      "end_page": 268,
      "detection_method": "synthetic",
      "content": "225 \n5.3.1. \nContainer Operations\nGame engines that make use of container classes inevitably make use of vari-\nous commonplace algorithms as well. Some examples include:\nInsert.\n• \n Add a new element to the container. The new element might be \nplaced at the beginning of the list, or the end, or in some other location; \nor the container might not have a notion of ordering at all.\nRemove.\n• \n Remove an element from the container; may require a ﬁ nd op-\neration (see below). However if an iterator is available that refers to the \ndesired element, it may be more eﬃ  cient to remove the element using \nthe iterator.\nSequential access (iteration).\n• \n Accessing each element of the container in \nsome “natural” predeﬁ ned order.\nRandom access.\n• \n Accessing elements in the container in an arbitrary or-\nder.\nFind.\n• \n Search a container for an element that meets a given criterion. \nThere are all sorts of variants on the ﬁ nd operation, including ﬁ nding \nin reverse, ﬁ nding multiple elements, etc. In addition, diﬀ erent types of \ndata structures and diﬀ erent situations call for diﬀ erent algorithms (see \nhtt p://en.wikipedia.org/wiki/Search_algorithm).\nSort\n• \n . Sort the contents of a container according to some given criteria. \nThere are many diﬀ erent sorting algorithms, including bubble sort, se-\nlection sort, insertion sort, quicksort, and so on. (See htt p://en.wikipedia.\norg/wiki/Sorting_algorithm for details.)\n5.3.2. Iterators\nAn iterator is a litt le class that “knows” how to eﬃ  ciently visit the elements \nin a particular kind of container. It acts like an array index or pointer—it \nrefers to one element in the container at a time, it can be advanced to the \nnext element, and it provides some sort of mechanism for testing whether \nor not all elements in the container have been visited. As an example, the \nﬁ rst of the following two code snippets iterates over a C-style array using a \npointer, while the second iterates over an STL linked list using almost identi-\ncal syntax.\nvoid processArray(int container[], int numElements)\n{\n \nint* pBegin = &container[0];\n \nint* pEnd = &container[numElements];\n5.3. Containers\n\n\n225 \n5.3.1. \nContainer Operations\nGame engines that make use of container classes inevitably make use of vari-\nous commonplace algorithms as well. Some examples include:\nInsert.\n• \n Add a new element to the container. The new element might be \nplaced at the beginning of the list, or the end, or in some other location; \nor the container might not have a notion of ordering at all.\nRemove.\n• \n Remove an element from the container; may require a ﬁ nd op-\neration (see below). However if an iterator is available that refers to the \ndesired element, it may be more eﬃ  cient to remove the element using \nthe iterator.\nSequential access (iteration).\n• \n Accessing each element of the container in \nsome “natural” predeﬁ ned order.\nRandom access.\n• \n Accessing elements in the container in an arbitrary or-\nder.\nFind.\n• \n Search a container for an element that meets a given criterion. \nThere are all sorts of variants on the ﬁ nd operation, including ﬁ nding \nin reverse, ﬁ nding multiple elements, etc. In addition, diﬀ erent types of \ndata structures and diﬀ erent situations call for diﬀ erent algorithms (see \nhtt p://en.wikipedia.org/wiki/Search_algorithm).\nSort\n• \n . Sort the contents of a container according to some given criteria. \nThere are many diﬀ erent sorting algorithms, including bubble sort, se-\nlection sort, insertion sort, quicksort, and so on. (See htt p://en.wikipedia.\norg/wiki/Sorting_algorithm for details.)\n5.3.2. Iterators\nAn iterator is a litt le class that “knows” how to eﬃ  ciently visit the elements \nin a particular kind of container. It acts like an array index or pointer—it \nrefers to one element in the container at a time, it can be advanced to the \nnext element, and it provides some sort of mechanism for testing whether \nor not all elements in the container have been visited. As an example, the \nﬁ rst of the following two code snippets iterates over a C-style array using a \npointer, while the second iterates over an STL linked list using almost identi-\ncal syntax.\nvoid processArray(int container[], int numElements)\n{\n \nint* pBegin = &container[0];\n \nint* pEnd = &container[numElements];\n5.3. Containers\n\n\n226 \n5. Engine Support Systems\n \nfor (int* p = pBegin; p != pEnd; ++p)\n {\n \n \nint element = *p;\n \n \n// process element...\n }\n}\nvoid processList(std::list<int>& container)\n{\n std::list<int>::\niterator pBegin = container.begin();\n std::list<int>::\niterator pEnd = container.end();\n std::list<inf>::\niterator p;\n \nfor (p = pBegin; p != pEnd; ++p)\n {\n \n \nint element = *p;\n \n \n// process element...\n }\n}\nThe key beneﬁ ts to using an iterator over att empting to access the con-\ntainer’s elements directly are:\nDirect access would break the container class’ encapsulation. An iterator, \n• \non the other hand, is typically a friend of the container class, and as such \nit can iterate eﬃ  ciently without exposing any implementation details \nto the outside world. (In fact, most good container classes hide their \ninternal details and cannot be iterated over without an iterator.)\nAn iterator can simplify the process of iterating. Most iterators act like \n• \narray indices or pointers, so a simple loop can be writt en in which the \niterator is incremented and compared against a terminating condition—\neven when the underlying data structure is arbitrarily complex. For \nexample, an iterator can make an in-order depth-ﬁ rst tree traversal look \nno more complex than a simple array iteration.\n5.3.2.1. Preincrement versus Postincrement\nNotice in the above example that we are using C++’s preincrement operator , \n++p, rather than the postincrement operator , p++. This is a subtle but some-\ntimes important optimization. The preincrement operator returns the value of \nthe operand aft er the increment has been performed, whereas postincrement \nreturns the previous, unincremented value. Hence preincrement can simply \nincrement the pointer or iterator in place and return a reference to it. Postin-\ncrement must cache the old value, then increment the pointer or iterator, and \nﬁ nally return the cached value. This isn’t a big deal for pointers or integer \n\n\n228 \n5. Engine Support Systems\ntion. If an algorithm executes a subalgorithm n times, and the subalgorithm is \nO(log n), then the resulting algorithm would be O(n log n).\nTo select an appropriate container class, we should look at the opera-\ntions that we expect to be most common, then select the container whose per-\nformance characteristics for those operations are most favorable. The most \ncommon orders you’ll encounter are listed here from fastest to slowest: O(1), \nO(log n), O(n), O(n log n), O(n2), O(nk) for k > 2.\nWe should also take the memory layout and usage characteristics \nof our containers into account. For example, an array (e.g., int a[5]  or\nstd::vector) stores its elements contiguously in memory and requires no \noverhead storage for anything other than the elements themselves. (Note that \na dynamic array does require a small ﬁ xed overhead.) On the other hand, a \nlinked list (e.g., std::list) wraps each element in a “link” data structure \nthat contains a pointer to the next element and possibly also a pointer to the \nprevious element, for a total of up to eight bytes of overhead per element. Also, \nthe elements in a linked list need not be contiguous in memory and oft en \naren’t. A contiguous block of memory is usually much more cache-friendly \nthan a set of disparate memory blocks. Hence, for high-speed algorithms, ar-\nrays are usually bett er than linked lists in terms of cache performance (unless \nthe nodes of the linked list are themselves allocated from a small, contiguous \nmemory block of memory, which is rare but not entirely unheard of). But a \nlinked list is bett er for situations in which speed of inserting and removing \nelements is of prime importance.\n5.3.4. Building Custom Container Classes\n Many game engines provide their own custom implementations of the com-\nmon container data structures. This practice is especially prevalent in console \ngame engines and games targeted at mobile phone and PDA platforms. The \nreasons for building these classes yourself include:\nTotal control.\n• \n You control the data structure’s memory requirements, the \nalgorithms used, when and how memory is allocated, etc.\nOpportunities for optimization.\n• \n You can optimize your data structures \nand algorithms to take advantage of hardware features speciﬁ c to the \nconsole(s) you are targeting; or ﬁ ne-tune them for a particular applica-\ntion within your engine.\nCustomizability.\n• \n You can provide custom algorithms not prevalent in \nthird-party libraries like STL (for example, searching for the n most-\nrelevant elements in a container, instead of just the single most-rele-\nvant).\n\n\n229 \nElimination of external dependencies.\n• \n Since you built the soft ware your-\nself, you are not beholden to any other company or team to maintain it. \nIf problems arise, they can be debugged and ﬁ xed immediately, rather \nthan waiting until the next release of the library (which might not be \nuntil aft er you have shipped your game!)\nWe cannot cover all possible data structures here, but let’s look at a few \ncommon ways in which game engine programmers tend to tackle contain-\ners.\n5.3.4.1. To Build or Not to Build\nWe will not discuss the details of how to implement all of these data types \nand algorithms here—a plethora of books and online resources are available \nfor that purpose. However, we will concern ourselves with the question of \nwhere to obtain implementations of the types and algorithms that you need. \nAs game engine designers, we have a number of choices:\nBuild the needed data structures manually.\n1. \nRely on third-party implementations. Some common choices include\n2. \nthe C++ standard template library (STL),\na. \na variant of STL, such as STLport,\nb. \nthe powerful and robust Boost libraries (htt p://www.boost.org).\nc. \nBoth STL and Boost are att ractive, because they provide a rich and power-\nful set of container classes covering prett y much every type of data structure \nimaginable. In addition, both of these packages provide a powerful suite of \ntemplate-based generic algorithms—implementations of common algorithms, \nsuch as ﬁ nding an element in a container, which can be applied to virtually \nany type of data object. However, third-party packages like these may not be \nappropriate for some kinds of game engines. And even if we decide to use a \nthird-party package, we must select between Boost and the various ﬂ avors of \nSTL, or another third-party library. So let’s take a moment to investigate some \nof the pros and cons of each approach.\nSTL\nThe beneﬁ ts of the standard template library include:\nSTL oﬀ ers a rich set of features.\n• \nReasonably robust implementations are available on a wide variety of \n• \nplatforms.\nSTL comes “standard” with virtually all C++ compilers.\n• \n5.3. Containers\n\n\n230 \n5. Engine Support Systems\nHowever, the STL also has numerous drawbacks, including:\nSTL has a steep learning curve. The documentation is now quite good, \n• \nbut the header ﬁ les are cryptic and diﬃ  cult to understand on most plat-\nforms.\nSTL is oft en slower than a data structure that has been craft ed speciﬁ -\n• \ncally for a particular problem.\nSTL also almost always eats up more memory than a custom-designed \n• \ndata structure.\nSTL does a lot of dynamic memory allocation, and it’s sometimes chal-\n• \nlenging to control its appetite for memory in a way that is suitable for \nhigh-performance, memory-limited console games.\nSTL’s implementation and behavior varies slightly from compiler to \n• \ncompiler, making its use in multiplatform engines more diﬃ  cult.\nAs long as the programmer is aware of the pitfalls of STL and uses it ju-\ndiciously, it can have a place in game engine programming. It is best suited \nto a game engine that will run on a personal computer platform, because the \nadvanced virtual memory systems on modern PCs make memory allocation \ncheaper, and the probability of running out of physical RAM is oft en negli-\ngible. On the other hand, STL is not generally well-suited for use on memory-\nlimited consoles that lack advanced CPUs and virtual memory. And code that \nuses STL may not port easily to other platforms. Here are some rules of thumb \nthat I use:\nFirst and foremost, be aware of the performance and memory character-\n• \nistics of the particular STL class you are using.\nTry to avoid heavier-weight STL classes in code that you believe will be \n• \na performance bott leneck.\nPrefer STL in situations where memory is not at a premium. For ex-\n• \nample, embedding a std::list inside a game object is OK, but em-\nbedding a std::list inside every vertex of a 3D mesh is probably not \na good idea. Adding every vertex of your 3D mesh to a std::list is \nprobably also not OK—the std::list class dynamically allocates a \nsmall “link” object for every element inserted into it, and that can result \nin a lot of tiny, fragmented memory allocations.\nIf your engine is to be multiplatform, I highly recommend \n• \nSTLport \n(htt p://www.stlport.org), an implementation of STL that was speciﬁ cally \ndesigned to be portable across a wide range of compilers and target \nplatforms, more eﬃ  cient, and more feature-rich than the original STL \nimplementations.\n\n\n231 \nThe Medal of Honor: Paciﬁ c Assault engine for the PC made heavy use of \nSTL, and while MOHPA did have its share of frame rate problems, the team \nwas able to work around the performance problems caused by STL (primarily \nby carefully limiting and controlling its use). Ogre3D, the popular object-ori-\nented rendering library that we use for some of the examples in this book, also \nmakes heavy use of STL. Your mileage may vary. Using STL on a game engine \nproject is certainly feasible, but it must be used with utmost care.\nBoost\nThe Boost project was started by members of the C++ Standards Committ ee \nLibrary Working Group, but it is now an open-source project with many con-\ntributors from across the globe. The aim of the project is to produce libraries \nthat extend and work together with STL, for both commercial and non-com-\nmercial use. Many of the Boost libraries have already been included in the \nC++ Standards Committ ee’s Library Technical Report (TR1), which is a step \ntoward becoming part of a future C++ standard. Here is a brief summary of \nwhat Boost brings to the table:\nBoost provides a lot of useful facilities not available in STL.\n• \nIn some cases, Boost provides alternatives to work around certain prob-\n• \nlems with STL’s design or implementation.\nBoost does a great job of handling some very complex problems, like \n• \nsmart pointers. (Bear in mind that smart pointers are complex beasts, \nand they can be performance hogs. Handles are usually preferable; see \nSection 14.5 for details.)\nThe Boost libraries’ documentation is usually excellent. Not only does \n• \nthe documentation explain what each library does and how to use it, but \nin most cases it also provides an excellent in-depth discussion of the de-\nsign decisions, constraints, and requirements that went into construct-\ning the library. As such, reading the Boost documentation is a great way \nto learn about the principles of soft ware design.\nIf you are already using STL, then Boost can serve as an excellent exten-\nsion and/or alterative to many of STL’s features. However, be aware of the \nfollowing caveats:\nMost of the core Boost classes are templates, so all that one needs in \n• \norder to use them is the appropriate set of header ﬁ les. However, some \nof the Boost libraries build into rather large .lib ﬁ les and may not be \nfeasible for use in very small-scale game projects.\nWhile the world-wide Boost community is an excellent support net-\n• \nwork, the Boost libraries come with no guarantees. If you encounter a \n5.3. Containers\n\n\n232 \n5. Engine Support Systems\nbug, it will ultimately be your team’s responsibility to work around it \nor ﬁ x it.\nBackward compatibility may not be supported.\n• \nThe Boost libraries are distributed under the Boost Soft ware License. \n• \nRead the license information (htt p://www.boost.org/more/license_info.\nhtml) carefully to be sure it is right for your engine.\nLoki\n There is a rather esoteric branch of C++ programming known as template meta-\nprogramming. The core idea is to use the compiler to do a lot of the work that \nwould otherwise have to be done at runtime by exploiting the template fea-\nture of C++ and in eﬀ ect “tricking” the compiler into doing things it wasn’t \noriginally designed to do. This can lead to some startlingly powerful and use-\nful programming tools.\nBy far the most well-known and probably most powerful template meta-\nprogramming library for C++ is Loki, a library designed and writt en by Andrei \nAlexandrescu (whose home page is at htt p://www.erdani.org). The library can \nbe obtained from SourceForge at htt p://loki-lib.sourceforge.net.\nLoki is extremely powerful; it is a fascinating body of code to study and \nlearn from. However, its two big weaknesses are of a practical nature: (a) its \ncode can be daunting to read and use, much less truly understand, and (b) \nsome of its components are dependent upon exploiting “side-eﬀ ect” behav-\niors of the compiler that require careful customization in order to be made \nto work on new compilers. So Loki can be somewhat tough to use, and it \nis not as portable as some of its “less-extreme” counterparts. Loki is not for \nthe faint of heart. That said, some of Loki’s concepts such as policy-based pro-\ngramming can be applied to any C++ project, even if you don’t use the Loki \nlibrary per se. I highly recommend that all soft ware engineers read Andrei’s \nground-breaking book, Modern C++ Design [2], from which the Loki library \nwas born.\n5.3.4.2. Dynamic Arrays and Chunky Allocation\n Fixed-size C-style arrays are used quite a lot in game programming, because \nthey require no memory allocation, are contiguous and hence cache-friendly, \nand support many common operations such as appending data and searching \nvery eﬃ  ciently.\nWhen the size of an array cannot be determined a priori, programmers \ntend to turn either to linked lists or dynamic arrays. If we wish to maintain the \nperformance and memory characteristics of ﬁ xed-length arrays, then the dy-\nnamic array is oft en the data structure of choice.\n\n\n233 \nThe easiest way to implement a dynamic array is to allocate an n-element \nbuﬀ er initially and then grow the list only if an att empt is made to add more \nthan n elements to it. This gives us the favorable characteristics of a ﬁ xed-\nsize array but with no upper bound. Growing is implemented by allocating \na new larger buﬀ er, copying the data from the original buﬀ er into the new \nbuﬀ er, and then freeing the original buﬀ er. The size of the buﬀ er is increased \nin some orderly manner, such as adding n to it on each grow, or doubling it \non each grow. Most of the implementations I’ve encountered never shrink the \narray, only grow it (with the notable exception of clearing the array to zero \nsize, which might or might not free the buﬀ er). Hence the size of the array be-\ncomes a sort of “high water mark .” The STL std::vector class works in this \nmanner.\nOf course, if you can establish a high water mark for your data, then you’re \nprobably bett er oﬀ  just allocating a single buﬀ er of that size when the engine \nstarts up. Growing a dynamic array can be incredibly costly due to realloca-\ntion and data copying costs. The impact of these things depends on the sizes \nof the buﬀ ers involved. Growing can also lead to fragmentation when dis-\ncarded buﬀ ers are freed. So, as with all data structures that allocate memory, \ncaution must be exercised when working with dynamic arrays. Dynamic ar-\nrays are probably best used during development, when you are as yet unsure \nof the buﬀ er sizes you’ll require. They can always be converted into ﬁ xed size \narrays once suitable memory budgets have been established.)\n5.3.4.3. Linked Lists\n If contiguous memory is not a primary concern, but the ability to insert and \nremove elements at random is paramount, then a linked list is usually the data \nstructure of choice. Linked lists are quite easy to implement, but they’re also \nquite easy to get wrong if you’re not careful. This section provides a few tips \nand tricks for creating robust linked lists.\nThe Basics of Linked Lists\nA linked list is a very simple data structure. Each element in the list has a \npointer to the next element, and, in a doubly-linked list , it also has a pointer to \nthe previous element. These two pointers are referred to as links. The list as a \nwhole is tracked using a special pair of pointers called the head and tail point-\ners. The head pointer points to the ﬁ rst element, while the tail pointer points \nto the last element.\nInserting a new element into a doubly-linked list involves adjusting the \nnext pointer of the previous element and the previous pointer of the next ele-\nment to both point at the new element and then sett ing the new element’s next \n5.3. Containers\n\n\n234 \n5. Engine Support Systems\nand previous pointers appropriately as well. There are four cases to handle \nwhen adding a node to a linked list:\nAdding the ﬁ rst element to a previously-empty list;\n• \nPrepending an element before the current head element;\n• \nAppending an element aft er the current tail element;\n• \nInserting an interior element.\n• \nThese cases are illustrated in Figure 5.8.\nRemoving an element involves the same kinds of operations in and \naround the node being removed. Again there are four cases: removing the \nhead element, removing the tail element, removing an interior element, and \nremoving the last element (emptying the list).\nThe Link Data Structure\nLinked list code isn’t particularly tough to write, but it can be error-prone. \nAs such, it’s usually a good idea to write a general-purpose linked list facility \nthat can be used to manage lists of any element type. To do this, we need to \nseparate the data structure that contains the links (i.e., the next and previ-\nous pointers) from the element data structure. The link data structure is typi-\ncally a simple struct or class, oft en called something like Link, Node, or \nLinkNode, and templated on the type of element to which it refers. It will usu-\nally look something like this.\nHead\nTail\nHead\nTail\nHead\nTail\nHead\nTail\nHead\nTail\nHead\nTail\nHead\nTail\nHead\nTail\nAdd First\nPrepend\n(Push Front)\nInsert\nAppend\n(Push Back)\nFigure 5.8. The four cases that must be handled when adding an element to a linked list: add \nﬁ rst, prepend, append, and insert.\n\n\n235 \ntemplate< typename ELEMENT >\nstruct Link\n{\n \nLink<ELEMENT>* m_pPrev;\n \nLink<ELEMENT>* m_pNext;\nELEMENT*   m_pElem;\n};\nExtrusive Lists\nAn extrusive list is a linked list in which the Link data structures are entirely \nseparate from the element data structures. Each Link contains a pointer to the \nelement, as shown in the example. Whenever an element is to be inserted into \na linked list, a link is allocated for it, and the pointers to the element and the \nnext and previous links are set up appropriately. When an element is removed \nfrom a linked list, its link can be freed.\nThe beneﬁ t of the extrusive design is that an element can reside in mul-\ntiple linked lists simultaneously—all we need is one link per list. The down \nside is that the Link objects must be dynamically allocated. Oft en a pool al-\nlocator (see Section 5.2.1.2) is used to allocate links, because they are always \nexactly the same size (viz., 12 bytes on a machine with 32-bit pointers). A pool \nallocator is an excellent choice due to its speed and its freedom from fragmen-\ntation problems.\nIntrusive Lists\nAn intrusive list is a linked list in which the Link data structure is embedded \nin the target element itself. The big beneﬁ t of this approach is that we no lon-\nger need to dynamically allocate the links—we get a link “for free” whenever \nwe allocate an element. For example, we might have:\nclass SomeElement\n{\nLink<SomeElement>\nm_link;\n \n// other members...\n};\nWe can also derive our element class from class Link. Using inheri-\ntance like this is virtually identical to embedding a Link as the ﬁ rst member \nof the class, but it has the additional beneﬁ t of allowing a pointer to a link \n(Link<SomeElement>*) to be down-cast into a pointer to the element itself \n(SomeElement*). This means we can eliminate the back-pointer to the ele-\nment that would otherwise have to be embedded within the Link. Here’s how \nsuch a design might be implemented in C++.\n5.3. Containers\n\n\n236 \n5. Engine Support Systems\ntemplate< typename ELEMENT >\nstruct Link\n{\n \nLink<ELEMENT>* m_pPrev;\n \nLink<ELEMENT>*  m_pNext;\n// No ELEMENT* pointer required, thanks to\n \n// inheritance.\n};\nclass SomeElement : public Link<SomeElement>\n{\n \n// other members...\n};\nThe big pitfall of the intrusive linked list design is that it prevents an ele-\nment from residing in more than one linked list at a time (because each ele-\nment has one and only one link). We can allow an element to be a member of \nN concurrent lists by providing it with N embedded link instances (in which \ncase we cannot use the inheritance method). However, the number N must \nbe ﬁ xed a priori, so this approach is still not quite as ﬂ exible as the extrusive \ndesign.\nThe choice between intrusive and extrusive linked lists depends on the \napplication and the constraints under which you are operating. If dynamic \nmemory allocation must be avoided at all costs, then an intrusive list is prob-\nably best. If you can aﬀ ord the overhead of pool allocation, then an extrusive \ndesign may be preferable. Sometimes only one of the two approaches will \nbe feasible. For example, if we wish to store instances of a class deﬁ ned by a \nthird-party library in a linked list and are unable or unwilling to modify that \nlibrary’s source code, then an extrusive list is the only option.\nHead and Tail Pointers: Circular Lists\n To fully implement a linked list, we need to provide a head and a tail pointer. \nThe simplest approach is to embed these pointers in their own data structure, \nperhaps called LinkedList, as follows.\ntemplate< typename ELEMENT >\nclass LinkedList\n{\n Link<ELEMENT>* \nm_pTail;\n Link<ELEMENT>* \nm_pHead;\n \n// member functions for manipulating the list...\n};\nYou may have noticed that there isn’t much diﬀ erence between a \nLinkedList and a Link—they both contain a pair of pointers to Link. As it \n\n\n237 \nturns out, there are some distinct beneﬁ ts to using an instance of class Link to \nmanage the head and tail of the list, like this:\ntemplate< typename ELEMENT >\nclass LinkedList\n{\n Link<ELEMENT> \nm_root; \n \n// contains head and tail\n \n// member functions for manipulating the list...\n};\nThe embedded m_root member is a Link, no diﬀ erent from any other Link in \nthe list (except that its m_pElement member will always be NULL). This allows \nus to make the linked list circular as shown in Figure 5.9. In other words, the \nm_pNext pointer of the last “real” node in the list points to m_root, as does \nthe m_pPrev pointer of the ﬁ rst “real” node in the list.\nThis design is preferable to the one involving two “loose” pointers for the \nhead and tail, because it simpliﬁ es the logic for inserting and removing ele-\nments. To see why this is the case, consider the code that would be required \nto remove an element from a linked list when “loose” head and tail pointers \nare being used.\nvoid LinkedList::remove(Link<ELEMENT>& link)\n{\n \nif (link.m_pNext)\n \n \nlink.m_pNext->m_pPrev = link.m_pPrev;\n else\n \n \n// Removing last element in the list.\nm_pTail = link.m_pPrev;\n \nif (link.m_pPrev)\n \n \nlink.m_pPrev->m_pNext = link.m_pNext;\n else\n \n \n// Removing first element in the list.\nm_pHead = link.m_pNext;\n5.3. Containers\nHead\nTail\nm_root\nFigure 5.9. When the head and tail pointers are stored in a link, the linked list can be made \ncircular, which simpliﬁ es the implementation and has some additional beneﬁ ts.\n\n\n238 \n5. Engine Support Systems\n \nlink.m_pPrev = link.m_pNext = NULL;\n}\nThe code is a bit simpler when we use the m_root design:\nvoid LinkedList::remove(Link<ELEMENT>& link)\n{\n \n// The link must currently be a member of the list.\n \nASSERT(link.m_pNext != NULL);\n \nASSERT(link.m_pPrev != NULL);\n \nlink.m_pNext->m_pPrev = link.m_pPrev;\n \nlink.m_pPrev->m_pNext = link.m_pNext;\n \n// Do this to indicate the link is no longer in any   \n \n// list.\n \nlink.m_pPrev = link.m_pNext = NULL;\n}\nThe example code shown above highlights an additional beneﬁ t of the \ncircularly linked list approach: A link’s m_pPrev and m_pNext pointers are \nnever null, unless the link is not a member of any list (i.e., the link is unused/\ninactive). This gives us a simple test for list membership.\nContrast this with the “loose” head/tail pointer design. In that case, the \nm_pPrev pointer of the ﬁ rst element in the list is always null, as is the m_pN-\next pointer of the last element. And if there is only one element in the list, that \nlink’s next and previous pointers will both be null. This makes it impossible to \nknow whether or not a given Link is a member of a list or not.\nSingly-Linked Lists\nA singly-linked list is one in which the elements have a next pointer, but no pre-\nvious pointer. (The list as a whole might have both a head and a tail pointer, or \nit might have only a head pointer.) Such a design is obviously a memory saver, \nbut the cost of this approach becomes evident when inserting or removing an \nelement from the list. We have no m_pPrev pointer, so we need to traverse the \nlist from the head in order to ﬁ nd the previous element, so that its m_pNext\npointer can be updated appropriately. Therefore, removal is an O(1) operation \nfor a doubly-linked list, but it’s an O(n) operation for a singly-linked list.\nThis inherent insertion and removal cost is oft en prohibitive, so most \nlinked lists are doubly linked. However, if you know for certain that you will \nonly ever add and remove elements from the head of the list (as when imple-\nmenting a stack), or if you always add to the head and remove from the tail (as \nwith a queue—and your list has both a head and a tail pointer), then you can \nget away with a singly-linked list and save yourself some memory.\n\n\n239 \n5.3.4.4. Dictionaries and Hash Tables\nA dictionary is a table of key-value pairs . A value in the dictionary can be \nlooked up quickly, given its key. The keys and values can be of any data type. \nThis kind of data structure is usually implemented either as a binary search \ntree or as a hash table.\nIn a binary tree implementation, the key-value pairs are stored in the \nnodes of the binary tree, and the tree is maintained in key-sorted order. Look-\ning up a value by key involves performing an O(log n) binary search.\nIn a hash table implementation, the values are stored in a ﬁ xed-size table, \nwhere each slot in the table represents one or more keys. To insert a key-value \npair into a hash table, the key is ﬁ rst converted into integer form via a pro-\ncess known as hashing (if it is not already an integer). Then an index into the \nhash table is calculated by taking the hashed key modulo the size of the table. \nFinally, the key-value pair is stored in the slot corresponding to that index. \nRecall that the modulo operator (% in C/C++) ﬁ nds the remainder of dividing \nthe integer key by the table size. So if the hash table has ﬁ ve slots, then a key of \n3 would be stored at index 3 (3 % 5 == 3), while a key of 6 would be stored \nat index 1 (6 % 5 == 1). Finding a key-value pair is an O(1) operation in the \nabsence of collisions.\nCollisions: Open and Closed Hash Tables\n Sometimes two or more keys end up occupying the same slot in the hash table. \nThis is known as a collision. There are two basic ways to resolve a collision, giv-\ning rise to two diﬀ erent kinds of hash tables:\nOpen\n• \n . In an open hash table (see Figure 5.10), collisions are resolved \nby simply storing more than one key-value pair at each index, usually \nin the form of a linked list. This approach is easy to implement and \nimposes no upper bound on the number of key-value pairs that can be \nstored. However, it does require memory to be allocated dynamically \nwhenever a new key-value pair is added to the table.\nClosed\n• \n . In a closed hash table (see Figure 5.11), collisions are resolved via \na process of probing until a vacant slot is found. (“Probing” means apply-\ning a well-deﬁ ned algorithm to search for a free slot.) This approach is \na bit more diﬃ  cult to implement, and it imposes an upper limit on the \nnumber of key-value pairs that can reside in the table (because each slot \ncan hold only one key-value pair). But the main beneﬁ t of this kind of \nhash table is that it uses up a ﬁ xed amount of memory and requires no dy-\nnamic memory allocation. Therefore it is oft en a good choice in a console \nengine.\n5.3. Containers\n\n\n240 \n5. Engine Support Systems\nHashing\nHashing is the process of turning a key of some arbitrary data type into an \ninteger, which can be used modulo the table size as an index into the table. \nMathematically, given a key k, we want to generate an integer hash value h us-\ning the hash function H, and then ﬁ nd the index i into the table as follows:\n \nh = H(k), \n \ni = h mod N,  \nwhere N is the number of slots in the table, and the symbol mod represents the \nmodulo operation, i.e., ﬁ nding the remainder of the quotient h/N.\nIf the keys are unique integers, the hash function can be the identity func-\ntion, H(k) = k. If the keys are unique 32-bit ﬂ oating-point numbers, a hash func-\ntion might simply re-interpret the bit patt ern of the 32-bit ﬂ oat as if it were a \n32-bit integer.\nU32 hashFloat(float f)\n{\n union\n {\n  float \nasFloat;\nSlot 0\nSlot 1\nSlot 2\nSlot 3\nSlot 4\n(55, apple)\n(0, orange)\n(26, grape)\n(33, plum)\nFigure 5.10. An open hash table.\n(55, apple)\n(0, orange)\ncollision!\n(33, plum)\n(55, apple)\n(26, grape)\n(33, plum)\n(0, orange)\n(26, grape)\nprobe to\nfind new \nslot\n0\n1\n2\n3\n4\n0\n1\n2\n3\n4\nFigure 5.11. A closed hash table.\n\n\n241 \n \n \nU32   asU32;\n \n} u;\n \nu.asFloat = f;\n \nreturn u.asU32;\n}\nIf the key is a string, we can employ a string hashing function, which combines \nthe ASCII or UTF codes of all the characters in the string into a single 32-bit \ninteger value.\nThe quality of the hashing function H(k) is crucial to the eﬃ  ciency of the \nhash table. A “good” hashing function is one that distributes the set of all valid \nkeys evenly across the table, thereby minimizing the likelihood of collisions. \nA hash function must also be reasonably quick to calculate and deterministic \nin the sense that it must produce the exact same output every time it is called \nwith an indentical input.\nStrings are probably the most prevalent type of key you’ll encounter, so \nit’s particularly helpful to know a “good” string hashing function. Here are a \nfew reasonably good ones:\nLOOKUP3 by Bob Jenkins (htt p://burtleburtle.net/bob/c/lookup3.c).\n• \nCyclic redundancy check functions, such as CRC-32 (htt p://en.wikipedia.\n• \norg/wiki/Cyclic_redundancy_check).\nMessage-digest algorithm 5 (MD5), a cryptographic hash which yields \n• \nexcellent results but is quite expensive to calculate (htt p://en.wikipedia.\norg/wiki/MD5).\nA number of other excellent alternatives can be found in an article by \n• \nPaul Hsieh available at htt p://www.azillionmonkeys.com/qed/hash.\nhtml.\nImplementing a Closed Hash Table\nIn a closed hash table, the key-value pairs are stored directly in the table, rath-\ner than in a linked list at each table entry. This approach allows the program-\nmer to deﬁ ne a priori the exact amount of memory that will be used by the \nhash table. A problem arises when we encounter a collision —two keys that end \nup wanting to be stored in the same slot in the table. To address this, we use a \nprocess known as probing.\nThe simplest approach is linear probing . Imagining that our hashing func-\ntion has yielded a table index of i, but that slot is already occupied, we simply \ntry slots (i + 1), (i + 2), and so on until an empty slot is found (wrapping around \nto the start of the table when i = N). Another variation on linear probing is to \nalternate searching forwards and backwards, (i + 1), (i – 1), (i + 2), (i – 2), and \n5.3. Containers\n\n\n242 \n5. Engine Support Systems\nso on, making sure to modulo the resulting indices into the valid range of the \ntable.\nLinear probing tends to cause key-value pairs to “clump up.” To avoid \nthese clusters, we can use an algorithm known as quadratic probing . We start at \nthe occupied table index i and use the sequence of probes ij = (i  ± j 2) for j = 1, 2, \n3, …. In other words, we try (i + 12), (i – 12), (i + 22), (i – 22), and so on, remem-\nbering to always modulo the resulting index into the valid range of the table.\nWhen using closed hashing, it is a good idea to make your table size a \nprime number. Using a prime table size in conjunction with quadratic probing \ntends to yield the best coverage of the available table slots with minimal clus-\ntering. See htt p://www.cs.utk.edu/~eĳ khout/594-LaTeX/handouts/hashing-\nslides.pdf for a good discussion of why prime hash table sizes are preferable.\n5.4. Strings\nStrings are ubiquitous in almost every soft ware project, and game engines are \nno exception. On the surface, the string may seem like a simple, fundamental \ndata type. But when you start using strings in your projects, you will quickly \ndiscover a wide range of design issues and constraints, all of which must be \ncarefully accounted for.\n5.4.1. The Problem with Strings\nThe most fundamental question is how strings should be stored and managed \nin your program. In C and C++, strings aren’t even an atomic type—they are \nimplemented as arrays of characters. The variable length of strings means we \neither have to hard-code limitations on the sizes of our strings, or we need to \ndynamically allocate our string buﬀ ers. C++ programmers oft en prefer to use \na string class, rather than deal directly with character arrays. But then, which \nstring class should we use? STL provides a reasonably good string class, but if \nyou’ve decided not to use STL you might be stuck writing your own.\nAnother big string-related problem is that of localization —the process of \nadapting your soft ware for release in other languages. This is also known as \ninternationalization, or I18N for short. Any string that you display to the user \nin English must be translated into whatever languages you plan to support. \n(Strings that are used internally to the program but are never displayed to the \nuser are exempt from localization, of course.) This not only involves making \nsure that you can represent all the character glyphs of all the languages you \nplan to support (via an appropriate set of fonts), but it also means ensuring \nthat your game can handle diﬀ erent text orientations. For example, Chinese \n\n\n243 \ntext is oriented vertically instead of horizontally, and some languages like He-\nbrew read right-to-left . Your game also needs to gracefully deal with the pos-\nsibility that a translated string will be either much longer, or much shorter, \nthan its English counterpart.\nFinally, it’s important to realize that strings are used internally within a \ngame engine for things like resource ﬁ le names and object ids. For example, \nwhen a game designer lays out a level, it’s highly convenient to permit him or \nher to identify the objects in the level using meaningful names, like “Player-\nCamera,” “enemy-tank-01,” or “explosionTrigger.”\nHow our engine deals with these internal strings oft en has pervasive ram-\niﬁ cations on the performance of the game. This is because strings are inherent-\nly expensive to work with at runtime. Comparing or copying ints or floats \ncan be accomplished via simple machine language instructions. On the other \nhand, comparing strings requires an O(n) scan of the character arrays using a \nfunction like strcmp() (where n is the length of the string). Copying a string \nrequires an O(n) memory copy, not to mention the possibility of having to \ndynamically allocate the memory for the copy. During one project I worked \non, we proﬁ led our game’s performance only to discover that strcmp() and \nstrcpy() were the top two most expensive functions! By eliminating unnec-\nessary string operations and using some of the techniques outlined in this \nsection, we were able to all but eliminate these functions from our proﬁ le, and \nincrease the game’s frame rate signiﬁ cantly. (I’ve heard similar stories from \ndevelopers at a number of diﬀ erent studios.)\n5.4.2. String Classes\nString classes can make working with strings much more convenient for the \nprogrammer. However, a string class can have hidden costs that are diﬃ  cult \nto see until the game is proﬁ led. For example, passing a string to a function \nusing a C-style character array is fast because the address of the ﬁ rst character \nis typically passed in a hardware register. On the other hand, passing a string \nobject might incur the overhead of one or more copy constructors, if the func-\ntion is not declared or used properly. Copying strings might involve dynamic \nmemory allocation, causing what looks like an innocuous function call to end \nup costing literally thousands of machine cycles.\nFor this reason, in game programming I generally like to avoid string \nclasses. However, if you feel a strong urge to use a string class, make sure you \npick or implement one that has acceptable runtime performance character-\nistics—and be sure all programmers that use it are aware of its costs. Know \nyour string class: Does it treat all string buﬀ ers as read-only? Does it utilize \nthe copy on write optimization? (See htt p://en.wikipedia.org/wiki/Copy-on-\n5.4. Strings\n\n\n244 \n5. Engine Support Systems\nwrite.) As a rule of thumb, always pass string objects by reference, never by \nvalue (as the latt er oft en incurs string-copying costs). Proﬁ le your code early \nand oft en to ensure that your string class isn’t becoming a major source of lost \nframe rate!\nOne situation in which a specialized string class does seem justiﬁ able \nto me is when storing and managing ﬁ le system paths . Here, a hypothetical \nPath class could add signiﬁ cant value over a raw C-style character array. For \nexample, it might provide functions for extracting the ﬁ lename, ﬁ le exten-\nsion or directory from the path. It might hide operating system diﬀ erences by \nautomatically converting Windows-style backslashes to UNIX-style forward \nslashes or some other operating system’s path separator. Writing a Path class \nthat provides this kind of functionality in a cross-platform way could be high-\nly valuable within a game engine context. (See Section 6.1.1.4 for more details \non this topic.)\n5.4.3. Unique Identiﬁ ers\n The objects in any virtual game world need to be uniquely identiﬁ ed in some \nway. For example, in Pac Man we might encounter game objects named “pac_\nman,” “blinky,” “pinky,” “inky,” and “clyde.” Unique object identiﬁ ers allow \ngame designers to keep track of the myriad objects that make up their game \nworlds and also permit those objects to be found and operated on at runtime \nby the engine. In addition, the assets from which our game objects are con-\nstructed—meshes, materials, textures, audio clips, animations, and so on—all \nneed unique identiﬁ ers as well.\nStrings seem like a natural choice for such identiﬁ ers. Assets are oft en \nstored in individual ﬁ les on disk, so they can usually be identiﬁ ed uniquely by \ntheir ﬁ le paths, which of course are strings. And game objects are created by \ngame designers, so it is natural for them to assign their objects understandable \nstring names, rather than have to remember integer object indices, or 64- or \n128-bit globally unique identiﬁ ers (GUIDs). However, the speed with which \ncomparisons between unique identiﬁ ers can be made is of paramount impor-\ntance in a game, so strcmp() simply doesn’t cut it. We need a way to have \nour cake and eat it too—a way to get all the descriptiveness and ﬂ exibility of a \nstring, but with the speed of an integer.\n5.4.3.1. Hashed String Ids\n One good solution is to hash our strings. As we’ve seen, a hash function maps \na string onto a semi-unique integer. String hash codes can be compared just \nlike any other integers, so comparisons are fast. If we store the actual strings \nin a hash table, then the original string can always be recovered from the hash \n\n\n245 \ncode. This is useful for debugging purposes and to permit hashed strings to \nbe displayed on-screen or in log ﬁ les. Game programmers sometimes use the \nterm string id to refer to such a hashed string. The Unreal engine uses the term \nname instead (implemented by class FName).\nAs with any hashing system, collisions are a possibility (i.e., two diﬀ erent \nstrings might end up with the same hash code). However, with a suitable hash \nfunction, we can all but guarantee that collisions will not occur for all rea-\nsonable input strings we might use in our game. Aft er all, a 32-bit hash code \nrepresents more than four billion possible values. So if our hash function does \na good job of distributing strings evenly throughout this very large range, we \nare unlikely to collide. At Naughty Dog, we used a variant of the CRC-32 al-\ngorithm to hash our strings, and we didn’t encounter a single collision in over \ntwo years of development on Uncharted: Drake’s Fortune.\n5.4.3.2. Some Implementation Ideas\nConceptually, it’s easy enough to run a hash function on your strings in order \nto generate string ids. Practically speaking, however, it’s important to con-\nsider when the hash will be calculated. Most game engines that use string \nids do the hashing at runtime. At Naughty Dog, we permit runtime hash-\ning of strings, but we also preprocess our source code using a simple utility \nthat searches for macros of the form SID(any-string) and translates each one \ndirectly into the appropriate hashed integer value. This permits string ids to \nbe used anywhere that an integer manifest constant can be used, including \nthe constant case labels of a switch statement. (The result of a function call \nthat generates a string id at runtime is not a constant, so it cannot be used as \na case label.)\nThe process of generating a string id from a string is sometimes called \ninterning the string, because in addition to hashing it, the string is typi-\ncally also added to a global string table. This allows the original string to \nbe recovered from the hash code later. You may also want your tools to be \ncapable of hashing strings into string ids. That way, when the tool generates \ndata for consumption by your engine, the strings will already have been \nhashed.\nThe main problem with interning a string is that it is a slow operation. \nThe hashing function must be run on the string, which can be an expensive \nproposition, especially when a large number of strings are being interned. \nIn addition, memory must be allocated for the string, and it must be copied \ninto the lookup table. As a result (if you are not generating string ids at \ncompile-time), it is usually best to intern each string only once and save oﬀ  \nthe result for later use. For example, it would be preferable to write code like \n5.4. Strings\n\n\n246 \n5. Engine Support Systems\nthis because the latt er implementation causes the strings to be unnecessarily \nre-interned every time the function f() is called.\nstatic StringId\nsid_foo = internString(“foo”);\nstatic StringId\nsid_bar = internString(“bar”);\n// ...\nvoid f(StringId id)\n{\n \nif (id == sid_foo)\n {\n \n \n// handle case of id == “foo”\n }\n \nelse if (id == sid_bar)\n {\n \n \n// handle case of id == “bar”\n }\n}\nThis approach is much less eﬃ  cient.\nvoid f(StringId id)\n{\n \nif (id == internString(“foo”))\n {\n \n \n// handle case of id == “foo”\n }\n \nelse if (id == internString(“bar”))\n {\n \n \n// handle case of id == “bar”\n }\n}\nHere’s one possible implementation of internString().\nstringid.h\ntypedef U32 StringId;\nextern StringId internString(const char* str);\nstringid.cpp\nstatic HashTable<StringId, const char*> gStringIdTable;\n",
      "page_number": 247,
      "chapter_number": 13,
      "summary": "This chapter covers segment 13 (pages 247-268). Key topics include string, strings, and link. Container Operations\nGame engines that make use of container classes inevitably make use of vari-\nous commonplace algorithms as well.",
      "keywords": [
        "element",
        "linked list",
        "list",
        "Engine Support Systems",
        "link",
        "STL",
        "string",
        "STL linked list",
        "hash table",
        "Container",
        "Strings",
        "Hash",
        "elements",
        "Support Systems",
        "Link Data Structure"
      ],
      "concepts": [
        "string",
        "strings",
        "link",
        "element",
        "elements",
        "list",
        "memory",
        "classes",
        "hash",
        "engine"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 3",
          "chapter": 55,
          "title": "Segment 55 (pages 524-531)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "More Effective C++",
          "chapter": 32,
          "title": "Segment 32 (pages 320-329)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 5,
          "title": "Segment 5 (pages 39-47)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 59,
          "title": "Segment 59 (pages 567-580)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective_Modern_C++",
          "chapter": 21,
          "title": "Segment 21 (pages 223-230)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 269-288)",
      "start_page": 269,
      "end_page": 288,
      "detection_method": "synthetic",
      "content": "247 \nStringId internString(const char* str)\n{\n \nStringId sid = hashCrc32(str);\n \nHashTable<StringId, const char*>::iterator it\n  = \ngStringIdTable.find(sid);\n \nif (it == gStringTable.end())\n {\n \n \n// This string has not yet been added to the   \n \n \n \n// table. Add it, being sure to copy it in case   \n \n   // the original was dynamically allocated and \n \n \n// might later be freed.\n \n \ngStringTable[sid] = strdup(str);\n }\n \nreturn sid;\n}\nAnother idea employed by the Unreal Engine is to wrap the string id and \na pointer to the corresponding C-style character array in a tiny class. In the \nUnreal Engine, this class is called FName.\nUsing Debug Memory for Strings\nWhen using string ids, the strings themselves are only kept around for human \nconsumption. When you ship your game, you almost certainly won’t need \nthe strings—the game itself should only ever use the ids. As such, it’s a good \nidea to store your string table in a region of memory that won’t exist in the \nretail game. For example, a PS3 development kit has 256 MB of retail memory, \nplus an additional 256 MB of “debug” memory that is not available on a retail \nunit. If we store our strings in debug memory, we needn’t worry about their \nimpact on the memory footprint of the ﬁ nal shipping game. (We just need to \nbe careful never to write production code that depends on the strings being \navailable!)\n5.4.4. Localization\n Localization of a game (or any soft ware project) is a big undertaking. It is a \ntask which is best handled by planning for it from day one and accounting for \nit at every step of development. However, this is not done as oft en as we all \nwould like. Here are some tips that should help you plan your game engine \nproject for localization. For an in-depth treatment of soft ware localization, \nsee [29].\n5.4. Strings\n\n\n248 \n5. Engine Support Systems\n5.4.4.1. Unicode\n The problem for most English-speaking soft ware developers is that they are \ntrained from birth (or thereabouts!) to think of strings as arrays of 8-bit ASCII \ncharacter codes (i.e., characters following the ANSI standard). ANSI strings \nwork great for a language with a simple alphabet, like English. But they just \ndon’t cut it for languages with complex alphabets containing a great many \nmore characters, sometimes totally diﬀ erent glyphs than English’s 26 lett ers. \nTo address the limitations of the ANSI standard, the Unicode character set \nsystem was devised.\nPlease set down this book right now and read the article entitled, “The \nAbsolute Minimum Every Soft ware Developer Absolutely, Positively Must \nKnow About Unicode and Character Sets (No Excuses!)” by Joel Spolsky. You \ncan ﬁ nd it here: htt p://www.joelonsoft ware.com/articles/Unicode.html. (Once \nyou’ve done that, please pick up the book again!)\nAs Joel describes in his article, Unicode is not a single standard but actu-\nally a family of related standards. You will need to select the speciﬁ c standard \nthat best suits your needs. The two most common choices I’ve seen used in \ngame engines are UTF-8 and UTF-16.\nUTF-8\n In UTF-8, the character codes are 8 bits each, but certain characters occupy \nmore than one byte. Hence the number of bytes occupied by a UTF-8 character \nstring is not necessarily the length of the string in characters. This is known as \na multibyte character set (MBCS), because each character may take one or more \nbytes of storage.\nOne of the big beneﬁ ts of the UTF-8 encoding is that it is backwards-com-\npatible with the ANSI encoding. This works because the ﬁ rst character of a \nmultibyte character sequence always has its most signiﬁ cant bit set (i.e., lies \nbetween 128 and 255, inclusive). Since the standard ANSI character codes are \nall less than 128, a plain old ANSI string is a valid and unambiguous UTF-8 \nstring as well.\nUTF-16\n The UTF-16 standard employs a simpler, albeit more expensive, approach. \nEach character takes up exactly 16 bits (whether it needs all of those bits or \nnot). As a result, dividing the number of bytes occupied by the string by two \nyields the number of characters. This is known as a wide character set (WCS), \nbecause each character is 16 bits wide instead of the 8 bits used by “regular” \nANSI chars.\n\n\n249 \nUnicode under Windows\nUnder Microsoft  Windows, the data type wchar_t is used to represent a single \n“wide ” UTF-16 character (WCS), while the char type is used both for ANSI \nstrings and for multibyte UTF-16 strings (MBCS). What’s more, Windows per-\nmits you to write code that is character set independent. To accomplish this, a \ndata type known as TCHAR is provided. The data type TCHAR is a typedef\nto char when building your application in ANSI mode and is a typedef to \nwchar_t when building your application in UTF-16 (WCS) mode. (For consis-\ntency, the type WCHAR is also provided as a synonym for wchar_t.)\nThroughout the Windows API, a preﬁ x or suﬃ  x of “w,” “wcs,” or “W” \nindicates wide (UTF-16) characters; a preﬁ x or suﬃ  x of “t,” “tcs,” or “T” \nindicates the current character type (which might be ANSI or might be UTF-\n16, depending on how your application was built); and no preﬁ x or suf-\nﬁ x indicates plain old ANSI. STL uses a similar convention—for example, \nstd::string is STL’s ANSI string class, while std::wstring is its wide \ncharacter equivalent.\nPrett y much every standard C library function that deals with strings has \nequivalent WCS and MBCS versions under Windows. Unfortunately, the API \ncalls don’t use the terms UTF-8 and UTF-16, and the names of the functions \naren’t always 100% consistent. This all leads to some confusion among pro-\ngrammers who aren’t in the know. (But you aren’t one of those programmers!) \nTable 5.1 lists some examples.\nWindows also provides functions for translating between ANSI character \nstrings, multibyte UTF-8 strings, and wide UTF-16 strings. For example, wcs-\ntombs() converts a wide UTF-16 string into a multibyte UTF-8 string.\nComplete documentation for these functions can be found on Microsoft ’s \nMSDN web site. Here’s a link to the documentation for strcmp() and its ilk, \nfrom which you can quite easily navigate to the other related string-manip-\nulation functions using the tree view on the left -hand side of the page, or via \nthe search bar: htt p://msdn2.microsoft .com/en-us/library/kk6xf663(VS.80).\naspx.\nANSI\nWCS\nMBCS\nstrcmp()\nwcscmp()\n_mbscmp()\nstrcpy()\nwcscpy()\n_mbscpy()\nstrlen()\nwcslen()\n_mbstrlen()\nTable 5.1. Variants of some common standard C library string functions for use with ANSI, \nwide and multibyte character sets.\n5.4. Strings\n\n\n250 \n5. Engine Support Systems\nUnicode on Consoles\nThe Xbox 360 soft ware development kit (XDK) uses WCS strings prett y much \nexclusively, for all strings—even for internal strings like ﬁ le paths. This is cer-\ntainly one valid approach to the localization problem, and it makes for very \nconsistent string handling throughout the XDK. However, the UTF-16 encod-\ning is a bit wasteful on memory, so diﬀ erent game engines may employ diﬀ er-\nent conventions. At Naughty Dog, we use 8-bit char strings throughout our \nengine, and we handle foreign languages via a UTF-8 encoding. The choice of \nencoding is not important, as long as you select one as early in the project as \npossible and stick with it consistently.\n5.4.4.2. Other Localization Concerns\nEven once you have adapted your soft ware to use Unicode characters, there \nare still a host of other localization problems to contend with. For one thing, \nstrings aren’t the only place where localization issues arise. Audio clips in-\ncluding recorded voices must be translated. Textures may have English words \npainted into them that require translation. Many symbols have diﬀ erent mean-\nings in diﬀ erent cultures. Even something as innocuous as a no-smoking sign \nmight be misinterpreted in another culture. In addition, some markets draw the \nboundaries between the various game-rating levels diﬀ erently. For example, in \nJapan a Teen-rated game is not permitt ed to show blood of any kind, whereas \nin North America small red blood spatt ers are considered acceptable.\nFor strings, there are other details to worry about as well. You will need \nto manage a database of all human-readable strings in your game, so that \nthey can all be reliably translated. The soft ware must display the proper lan-\nguage given the user’s installation sett ings. The formatt ing of the strings may \nbe totally diﬀ erent in diﬀ erent languages—for example, Chinese is writt en \nvertically, and Hebrew reads right-to-left . The lengths of the strings will vary \ngreatly from language to language. You’ll also need to decide whether to ship \na single DVD or Blu-ray disc that contains all languages or ship diﬀ erent discs \nfor particular territories.\nThe most crucial components in your localization system will be the cen-\ntral database of human-readable strings and an in-game system for looking \nup those strings by id. For example, let’s say you want a heads-up display \nthat lists the score of each player with “Player 1 Score:” and “Player 2 Score:” \nlabels and that also displays the text “Player 1 Wins” or “Player 2 Wins” at \nthe end of a round. These four strings would be stored in the localization \ndatabase under unique ids that are understandable to you, the developer of \nthe game. So our database might use the ids “p1score,” “p2score,” “p1wins,” \nand “p2wins,” respectively. Once our game’s strings have been translated into \n\n\n251 \nFrench, our database would look something like the simple example shown in \nTable 5.2. Additional columns can be added for each new language your game \nsupports.\nThe exact format of this database is up to you. It might be as simple as \na Microsoft  Excel worksheet that can be saved as a comma-separated values \n(CSV ) ﬁ le and parsed by the game engine or as complex as a full-ﬂ edged Or-\nacle database. The speciﬁ cs of the string database are largely unimportant to \nthe game engine, as long as it can read in the string ids and the correspond-\ning Unicode strings for whatever language(s) your game supports. (However, \nthe speciﬁ cs of the database may be very important from a practical point of \nview, depending upon the organizational structure of your game studio. A \nsmall studio with in-house translators can probably get away with an Excel \nspreadsheet located on a network drive. But a large studio with branch oﬃ  ces \nin Britain, Europe, South America, and Japan would probably ﬁ nd some kind \nof distributed database a great deal more amenable.)\nAt runtime, you’ll need to provide a simple function that returns the Uni-\ncode string in the “current” language, given the unique id of that string. The \nfunction might be declared like this:\nwchar_t getLocalizedString(const char* id);\nand it might be used like this:\nvoid drawScoreHud(const Vector3& score1Pos,\n \n \n \n \n         const \nVector3& score2Pos)\n{\n renderer.displayTextOrtho(\ngetLocalizedString(\"p1score\"),\n  score1Pos);\n renderer.displayTextOrtho(\ngetLocalizedString(\"p2score\"),\n  score2Pos);\n \n// ...\n }\nId\nEnglish\nFrench\np1score\n“Player 1 Score”\n“Grade Joueur 1”\np2score\n“Player 2 Score”\n“Grade Joueur 2”\np1wins\n“Player 1 wins!”\n“Joueur un gagne!”\np2wins\n“Player 2 wins!”\n“Joueur deux gagne!”\nTable 5.2. Example of a string database used for localization.\n5.4. Strings\n\n\n252 \n5. Engine Support Systems\nOf course, you’ll need some way to set the “current” language globally. This \nmight be done via a conﬁ guration sett ing which is ﬁ xed during the installa-\ntion of the game. Or you might allow users to change the current language on \nthe ﬂ y via an in-game menu. Either way, the sett ing is not diﬃ  cult to imple-\nment; it can be as simple as a global integer variable specifying the index of \nthe column in the string table from which to read (e.g., column one might be \nEnglish, column two French, column three Spanish, and so on).\nOnce you have this infrastructure in place, your programmers must re-\nmember to never display a raw string to the user. They must always use the id of \na string in the database and call the look-up function in order to retrieve the \nstring in question.\n5.5. Engine Conﬁ guration\n Game engines are complex beasts, and they invariably end up having a large \nnumber of conﬁ gurable options. Some of these options are exposed to the \nplayer via one or more options menus in-game. For example, a game might \nexpose options related to graphics quality, the volume of music and sound ef-\nfects, or controller conﬁ guration. Other options are created for the beneﬁ t of \nthe game development team only and are either hidden or stripped out of the \ngame completely before it ships. For example, the player character’s maximum \nwalk speed might be exposed as an option so that it can be ﬁ ne-tuned during \ndevelopment, but it might be changed to a hard-coded value prior to ship.\n5.5.1. \nLoading and Saving Options\n A conﬁ gurable option can be implemented trivially as a global variable or a \nmember variable of a singleton class. However, conﬁ gurable options are not \nparticularly useful unless their values can be conﬁ gured, stored on a hard \ndisk, memory card, or other storage medium and later retrieved by the game. \nThere are a number of simple ways to load and save conﬁ guration options:\nText conﬁ guration ﬁ les.\n• \n By far the most common method of saving and \nloading conﬁ guration options is by placing them into one or more text \nﬁ les. The format of these ﬁ les varies widely from engine to engine, but it \nis usually very simple. For example, Windows INI ﬁ les (which are used \nby the Ogre3D renderer) consist of ﬂ at lists of key-value pairs grouped \ninto logical sections.\n[SomeSection]\n Key1=Value1\n Key2=Value2\n\n\n253 \n[AnotherSection]\n Key3=Value3\n Key4=Value4\n Key5=Value5\nThe XML format is another common choice for conﬁ gurable game op-\ntions ﬁ les.\nCompressed binary ﬁ les.\n• \n Most modern consoles have hard disk drives in \nthem, but older consoles could not aﬀ ord this luxury. As a result, all \ngame consoles since the Super Nintendo Entertainment System (SNES) \nhave come equipped with proprietary removable memory cards that \npermit both reading and writing of data. Game options are sometimes \nstored on these cards, along with saved games. Compressed binary ﬁ les \nare the format of choice on a memory card, because the storage space \navailable on these cards is oft en very limited.\nThe Windows registry\n• \n . The Microsoft  Windows operating system pro-\nvides a global options database known as the registry. It is stored as a \ntree, where the interior nodes (known as registry keys) act like ﬁ le fold-\ners, and the leaf nodes store the individual options as key-value pairs. \nAny application, game or otherwise, can reserve an entire subtree (i.e., a \nregistry key) for its exclusive use, and then store any set of options with-\nin it. The Windows registry acts like a carefully-organized collection of \nINI ﬁ les, and in fact it was introduced into Windows as a replacement \nfor the ever-growing network of INI ﬁ les used by both the operating \nsystem and Windows applications.\nCommand line options\n• \n . The command line can be scanned for option set-\ntings. The engine might provide a mechanism for controlling any option \nin the game via the command line, or it might expose only a small sub-\nset of the game’s options here.\nEnvironment variables\n• \n . On personal computers running Windows, Linux, \nor MacOS, environment variables are sometimes used to store conﬁ gu-\nration options as well.\nOnline user proﬁ les.\n• \n With the advent of online gaming communities like \nXbox Live , each user can create a proﬁ le and use it to save achievements, \npurchased and unlockable game features, game options, and other in-\nformation. The data is stored on a central server and can be accessed by \nthe player wherever an Internet connection is available.\n5.5.2. Per-User Options\nMost game engines diﬀ erentiate between global options and per-user options . \nThis is necessary because most games allow each player to conﬁ gure the game \n5.5. Engine Conﬁ guration\n\n\n254 \n5. Engine Support Systems\nto his or her liking. It is also a useful concept during development of the game, \nbecause it allows each programmer, artist, and designer to customize his or \nher work environment without aﬀ ecting other team members.\nObviously care must be taken to store per-user options in such a way that \neach player “sees” only his or her options and not the options of other play-\ners on the same computer or console. In a console game, the user is typically \nallowed to save his or her progress, along with per-user options such as con-\ntroller preferences, in “slots” on a memory card or hard disk. These slots are \nusually implemented as ﬁ les on the media in question.\nOn a Windows machine, each user has a folder under C:\\Documents and \nSett ings containing information such as the user’s desktop, his or her My Doc-\numents folder, his or her Internet browsing history and temporary ﬁ les, and \nso on. A hidden subfolder named Application Data is used to store per-user \ninformation on a per-application basis; each application creates a folder un-\nder Application Data and can use it to store whatever per-user information it \nrequires.\nWindows games sometimes store per-user conﬁ guration data in the reg-\nistry. The registry is arranged as a tree, and one of the top-level children of \nthe root node, called HKEY_CURRENT_USER, stores sett ings for whichever user \nhappens to be logged on. Every user has his or her own subtree in the registry \n(stored under the top-level subtree HKEY_USERS), and HKEY_CURRENT_USER\nis really just an alias to the current user’s subtree. So games and other applica-\ntions can manage per-user conﬁ guration options by simply reading and writ-\ning them to keys under the HKEY_CURRENT_USER subtree.\n5.5.3. Conﬁ guration Management in Some Real Engines\nIn this section, we’ll take a brief look at how some real game engines manage \ntheir conﬁ guration options.\n5.5.3.1. Example: Quake’s CVARs\n The Quake family of engines uses a conﬁ guration management system known \nas console variables, or CVARs for short. A CVAR is just a ﬂ oating-point or \nstring global variable whose value can be inspected and modiﬁ ed from within \nQuake’s in-game console. The values of some CVARs can be saved to disk and \nlater reloaded by the engine.\nAt runtime, CVARs are stored in a global linked list. Each CVAR is a dy-\nnamically-allocated instance of struct cvar_t, which contains the variable’s \nname, its value as a string or ﬂ oat, a set of ﬂ ag bits, and a pointer to the next \nCVAR in the linked list of all CVARs. CVARs are accessed by calling Cvar_\nGet(), which creates the variable if it doesn’t already exist and modiﬁ ed by \n\n\n255 \ncalling Cvar_Set(). One of the bit ﬂ ags, CVAR_ARCHIVE, controls whether or \nnot the CVAR will be saved into a conﬁ guration ﬁ le called conﬁ g.cfg. If this ﬂ ag \nis set, the value of the CVAR will persist across multiple runs of the game.\n5.5.3.2. Example: Ogre3D\n The Ogre3D rendering engine uses a collection of text ﬁ les in Windows INI \nformat for its conﬁ guration options. By default, the options are stored in three \nﬁ les, each of which is located in the same folder as the executable program:\nplugins.cfg\n• \n contains options specifying which optional engine plug-ins \nare enabled and where to ﬁ nd them on disk.\nresources.cfg\n• \n contains a search path specifying where game assets (a.k.a. \nmedia, a.k.a. resources) can be found.\nogre.cfg\n• \n contains a rich set of options specifying which renderer (DirectX \nor OpenGL) to use and the preferred video mode, screen size, etc.\nOut of the box, Ogre provides no mechanism for storing per-user conﬁ gu-\nration options. However, the Ogre source code is freely available, so it would \nbe quite easy to change it to search for its conﬁ guration ﬁ les in the user’s C:\\\nDocuments and Sett ings folder instead of in the folder containing the execut-\nable. The Ogre::ConfigFile class makes it easy to write code that reads and \nwrites brand new conﬁ guration ﬁ les, as well.\n5.5.3.3. Example: Uncharted: Drake’s Fortune\n Naughty Dog’s Uncharted engine makes use of a number of conﬁ guration \nmechanisms.\nIn-Game Menu Settings\nThe Uncharted engine supports a powerful in-game menu system, allowing \ndevelopers to control global conﬁ guration options and invoke commands. \nThe data types of the conﬁ gurable options must be relatively simple (primar-\nily Boolean, integer, and ﬂ oating-point variables), but this limitation did not \nprevent the developers of Uncharted from creating literally hundreds of useful \nmenu-driven options.\nEach conﬁ guration option is implemented as a global variable. When the \nmenu option that controls an option is created, the address of the global vari-\nable is provided, and the menu item directly controls its value. As an exam-\nple, the following function creates a submenu item containing some options \nfor Uncharted’s rail vehicles (the vehicles used in the “Out of the Frying Pan” \njeep chase level). It deﬁ nes menu items controlling three global variables: two \nBooleans and one ﬂ oating-point value. The items are collected onto a menu, \n5.5. Engine Conﬁ guration\n\n\n256 \n5. Engine Support Systems\nand a special item is returned that will bring up the menu when selected. \nPresumably the code calling this function would add this item to the parent \nmenu that it is building.\nDMENU::ItemSubmenu * CreateRailVehicleMenu()\n{\n \nextern bool g_railVehicleDebugDraw2D;\n \nextern bool g_railVehicleDebugDrawCameraGoals;\n \nextern float g_railVehicleFlameProbability;\n \nDMENU::Menu * pMenu = new DMENU::Menu(\n  \"RailVehicle\");\n    pMenu->PushBackItem(\n  new \nDMENU::ItemBool(\"Draw 2D Spring Graphs\",\n   \nDMENU::ToggleBool,\n  &\ng_railVehicleDebugDraw2D));\n    pMenu->PushBackItem(\n  new \nDMENU::ItemBool(\"Draw Goals (Untracked)\",\n  DMENU::ToggleBool,\n &g_railVehicleDebugDrawCameraGoals));\n \nDMENU::ItemFloat * pItemFloat;\n \npItemFloat = new DMENU::ItemFloat(\n  \"FlameProbability\", \n  DMENU::\nEditFloat, 5, \"%5.2f\",\n  &\ng_railVehicleFlameProbability);\n \npItemFloat->SetRangeAndStep(0.0f, 1.0f, 0.1f, 0.01f);\n pMenu->PushBackItem(pItemFloat);\n \nDMENU::ItemSubmenu * pSubmenuItem;\n \npSubmenuItem = new DMENU::ItemSubmenu(\n  \"RailVehicle...\", \npMenu);\n \nreturn pSubmenuItem;\n}\nThe value of any option can be saved by simply marking it with the circle \nbutt on on the PS3 joypad when the corresponding menu item is selected. The \nmenu sett ings are saved in an INI-style text ﬁ le, allowing the saved global vari-\nables to retain the values across multiple runs of the game. The ability to con-\ntrol which options are saved on a per-menu-item basis is highly useful, because \nany option which is not saved will take on its programmer-speciﬁ ed default \nvalue. If a programmer changes a default, all users will “see” the new value, \nunless of course a user has saved a custom value for that particular option.\n\n\n257 \nCommand Line Arguments\nThe Uncharted engine scans the command line for a predeﬁ ned set of special \noptions. The name of the level to load can be speciﬁ ed, along with a number \nof other commonly-used arguments.\nScheme Data Deﬁ nitions\n The vast majority of engine and game conﬁ guration information in Uncharted \nis speciﬁ ed using a Lisp -like language called Scheme . Using a proprietary data \ncompiler, data structures deﬁ ned in the Scheme language are transformed \ninto binary ﬁ les that can be loaded by the engine. The data compiler also spits \nout header ﬁ les containing C struct declarations for every data type deﬁ ned \nin Scheme. These header ﬁ les allow the engine to properly interpret the data \ncontained in the loaded binary ﬁ les. The binary ﬁ les can even be recompiled \nand reloaded on the ﬂ y, allowing developers to alter the data in Scheme and \nsee the eﬀ ects of their changes immediately (as long as data members are not \nadded or removed, as that would require a recompile of the engine).\nThe following example illustrates the creation of a data structure specify-\ning the properties of an animation. It then exports three unique animations to \nthe game. You may have never read Scheme code before, but for this relatively \nsimple example it should be prett y self-explanatory. One oddity you’ll notice \nis that hyphens are permitt ed within Scheme symbols, so simple-animation\nis a single symbol (unlike in C/C++ where simple-animation would be the \nsubtraction of two variables, simple and animation).\nsimple-animation.scm\n;; Define a new data type called simple-animation.\n(deftype simple-animation ()\n (\n \n \n(name             string)\n \n \n(speed            float   :default 1.0)\n \n \n(fade-in-seconds  float   :default 0.25)\n \n \n(fade-out-seconds float   :default 0.25)\n )\n)\n;; Now define three instances of this data structure...\n(define-export anim-walk\n \n(new simple-animation\n  :name \n“walk”\n  :speed \n1.0\n )\n)\n5.5. Engine Conﬁ guration\n\n\n258 \n5. Engine Support Systems\n(define-export anim-walk-fast\n \n(new simple-animation\n  :name \n\"walk\"\n  :speed \n2.0\n )\n)\n(define-export anim-jump\n \n(new simple-animation\n  :name \n\"jump\"\n  :fade-in-seconds \n0.1\n  :fade-out-seconds \n0.1\n )\n)\nThis Scheme code would generate the following C/C++ header ﬁ le:\nsimple-animation.h\n// WARNING: This file was automatically generated from \n// Scheme. Do not hand-edit.\nstruct SimpleAnimation\n{\n \nconst char* \nm_name;\n float \n  m_speed;\n float \n  m_fadeInSeconds;\n float \n  m_fadeOutSeconds;\n};\nIn-game, the data can be read by calling the LookupSymbol() function, which \nis templated on the data type returned, as follows:\n#include \"simple-animation.h\"\nvoid someFunction()\n{\nSimpleAnimation* pWalkAnim\n  = \nLookupSymbol<SimpleAnimation*>(\"anim-walk\");\nSimpleAnimation* pFastWalkAnim\n  = \nLookupSymbol<SimpleAnimation*>(\n   \"\nanim-walk-fast\");\nSimpleAnimation* pJumpAnim\n  = \nLookupSymbol<SimpleAnimation*>(\"anim-jump\");\n \n// use the data here...\n}\n\n\n259 \nThis system gives the programmers a great deal of ﬂ exibility in deﬁ n-\ning all sorts of conﬁ guration data—from simple Boolean, ﬂ oating-point, and \nstring options all the way to complex, nested, interconnected data structures. \nIt is used to specify detailed animation trees, physics parameters, player me-\nchanics, and so on.\n5.5. Engine Conﬁ guration\n\n\n261\n6\nResources and \nthe File System\nG\names are by nature multimedia experiences. A game engine therefore \nneeds to be capable of loading and managing a wide variety of diﬀ erent \nkinds of media—texture bitmaps, 3D mesh data, animations, audio clips, col-\nlision and physics data, game world layouts, and the list goes on. Moreover, \nbecause memory is usually scarce, a game engine needs to ensure that only \none copy of each media ﬁ le is loaded into memory at any given time. For ex-\nample, if ﬁ ve meshes share the same texture, then we would like to have only \none copy of that texture in memory, not ﬁ ve. Most game engines employ some \nkind of resource manager (a.k.a. asset manager, a.k.a. media manager) to load and \nmanage the myriad resources that make up a modern 3D game.\nEvery resource manager makes heavy use of the ﬁ le system. On a per-\nsonal computer, the ﬁ le system is exposed to the programmer via a library \nof operating system calls. However, game engines oft en “wrap” the native \nﬁ le system API in an engine-speciﬁ c API, for two primary reasons. First, the \nengine might be cross-platform, in which case the game engine’s ﬁ le system \nAPI can shield the rest of the soft ware from diﬀ erences between diﬀ erent \ntarget hardware platforms. Second, the operating system’s ﬁ le system API \nmight not provide all the tools needed by a game engine. For example, many \nengines support ﬁ le streaming (i.e., the ability to load data “on the ﬂ y” while \nthe game is running), yet most operating systems don’t provide a streaming \nﬁ le system API out of the box. Console game engines also need to provide ac-\n\n\n262 \n6. Resources and the File System\ncess to a variety of removable and non-removable media, from memory sticks \nto optional hard drives to a DVD-ROM or Blu-ray ﬁ xed disk to network ﬁ le \nsystems (e.g., Xbox Live or the PlayStation Network , PSN). The diﬀ erences \nbetween various kinds of media can likewise be “hidden” behind a game \nengine’s ﬁ le system API.\nIn this chapter, we’ll ﬁ rst explore the kinds of ﬁ le system APIs found in \nmodern 3D game engines. Then we’ll see how a typical resource manager \nworks.\n6.1. \nFile System\nA game engine’s ﬁ le system API typically addresses the following areas of \nfunctionality:\nz manipulating ﬁ le names and paths,\nz opening, closing, reading and writing individual ﬁ les,\nz scanning the contents of a directory,\nz handling asynchronous ﬁ le I/O requests (for streaming).\nWe’ll take a brief look at each of these in the following sections.\n6.1.1. \nFile Names and Paths\nA path is a string describing the location of a ﬁ le or directory within a ﬁ le sys-\ntem hierarchy. Each operating system uses a slightly diﬀ erent path format, but \npaths have essentially the same structure on every operating system. A path \ngenerally takes the following form:\nvolume/directory1/ directory2/…/directoryN/ﬁ le-name\n \nor\nvolume/directory1/directory2/…/directory(N – 1)/directoryN\nIn other words, a path generally consists of an optional volume speciﬁ er fol-\nlowed by a sequence of path components separated by a reserved path separa-\ntor character such as the forward or backward slash (/ or \\). Each component \nnames a directory along the route from the root directory to the ﬁ le or direc-\ntory in question. If the path speciﬁ es the location of a ﬁ le, the last compo-\nnent in the path is the ﬁ le name; otherwise it names the target directory. The \nroot directory is usually indicated by a path consisting of the optional volume \nspeciﬁ er followed by a single path separator character (e.g., / on UNIX, or C:\\\non Windows).\n\n\n263 \n6.1. File System\n6.1.1.1. \nDifferences Across Operating Systems\nEach operating system introduces slight variations on this general path struc-\nture. Here are some of the key diﬀ erences between Microsoft  DOS , Microsoft  \nWindows , the UNIX family of operating systems, and Apple Macintosh OS:\nz UNIX uses a forward slash (/) as its path component separator, while \nDOS and older versions of Windows used a backslash (\\) as the path \nseparator. Recent versions of Windows allow either forward or back-\nward slashes to be used to separate path components, although some \napplications still fail to accept forward slashes.\nz Mac OS 8 and 9 use the colon (:) as the path separator character. Mac \nOS X is based on UNIX, so it supports UNIX’s forward slash notation.\nz UNIX and its variants don’t support volumes as separate directory hi-\nerarchies. The entire ﬁ le system is contained within a single monolithic \nhierarchy, and local disk drives, network drives, and other resources are \nmounted so that they appear to be subtrees within the main hierarchy. As \na result, UNIX paths never have a volume speciﬁ er.\nz On Microsoft  Windows, volumes can be speciﬁ ed in two ways. A local \ndisk drive is speciﬁ ed using a single lett er followed by a colon (e.g., the \nubiquitous C:). A remote network share can either be mounted so that \nit looks like a local disk, or it can be referenced via a volume speciﬁ er \nconsisting of two backslashes followed by the remote computer name \nand the name of a shared directory or resource on that machine (e.g., \n\\\\some-computer\\some-share). This double backslash notation is an \nexample of the Universal Naming Convention (UNC).\nz Under DOS and early versions of Windows, a ﬁ le name could be up to \neight characters in length, with a three-character extension which was \nseparated from the main ﬁ le name by a dot. The extension described \nthe ﬁ le’s type, for example .txt for a text ﬁ le or .exe for an executable \nﬁ le. In recent Windows implementations, ﬁ le names can contain any \nnumber of dots (as they can under UNIX), but the characters aft er the \nﬁ nal dot are still interpreted as the ﬁ le’s extension by many applications \nincluding the Windows Explorer.\nz Each operating system disallows certain characters in the names of ﬁ les \nand directories. For example, a colon cannot appear anywhere in a Win-\ndows or DOS path except as part of a drive lett er volume speciﬁ er. Some \noperating systems permit a subset of these reserved characters to ap-\npear in a path as long as the path is quoted in its entirety or the oﬀ end-\ning character is escaped by preceding it with a backslash or some other \n\n\n264 \n6. Resources and the File System\nreserved escape character. For example, ﬁ le and directory names may \ncontain spaces under Windows, but such a path must be surrounded by \ndouble quotes in certain contexts.\nz Both UNIX and Windows have the concept of a current working directory \nor CWD (also known as the present working directory or PWD). The CWD \ncan be set from a command shell via the cd (change directory) command \non both operating systems, and it can be queried by typing cd with \nno arguments under Windows or by executing the pwd command on \nUNIX. Under UNIX there is only one CWD. Under Windows, each vol-\nume has its own private CWD.\nz Operating systems that support multiple volumes, like Windows, also \nhave the concept of a current working volume. From a Windows com-\nmand shell, the current volume can be set by entering its drive lett er and \na colon followed by the Enter key (e.g., C:<Enter>).\nz Consoles oft en also employ a set of predeﬁ ned path preﬁ xes to repre-\nsent multiple volumes. For example, PLAYSTATION 3 uses the preﬁ x \n/dev_bdvd/ to refer to the Bluray disk drive, while /dev_hddx/ refers \nto one or more hard disks (where x is the index of the device). On a PS3 \ndevelopment kit, /app_home/ maps to a user-deﬁ ned path on whatever \nhost machine is being used for development. During development, the \ngame usually reads its assets from /app_home/ rather than from the \nBluray or the hard disk.\n6.1.1.2. \nAbsolute and Relative Paths\nAll paths are speciﬁ ed relative to some location within the ﬁ le system. When a \npath is speciﬁ ed relative to the root directory, we call it an absolute path . When \nit is relative to some other directory in the ﬁ le system hierarchy, we call it a \nrelative pa th .\nUnder both UNIX and Windows, absolute paths start with a path sepa-\nrator character (/ or \\), while relative paths have no leading path separator. \nOn Windows, both absolute and relative paths may have an optional volume \nspeciﬁ er—if the volume is omitt ed, then the path is assumed to refer to the \ncurrent working volume.\nThe following paths are all absolute:\nWindows\nz C:\\Windows\\System32\nz D:\\ (root directory on the D: volume)\nz \\ (root directory on the current working volume)\n\n\n265 \nz \\game\\assets\\animation\\walk.anim (current working volume)\nz \\\\joe-dell\\Shared_Files\\Images\\foo.jpg (network path)\nUNIX\nz /usr/local/bin/grep\nz /game/src/audio/effects.cpp\nz / (root directory)\nThe following paths are all relative:\nWindows\nz System32 (relative to CWD \\Windows on the current volume)\nz X:animation\\walk.anim (relative to CWD \\game\\assets on the X:\nvolume)\nUNIX\nz bin/grep (relative to CWD /usr/local)\nz src/audio/effects.cpp (relative to CWD /game)\n6.1.1.3. \nSearch Paths\nThe term path should not be confused with the term search path. A path is a \nstring representing the location of a single ﬁ le or directory within the ﬁ le \nsystem hierarchy. A search path is a string containing a list of paths, each sepa-\nrated by a special character such as a colon or semicolon, which is searched \nwhen looking for a ﬁ le. For example, when you run any program from a com-\nmand prompt, the operating system ﬁ nds the executable ﬁ le by searching \neach directory on the search path contained in the shell’s PATH environment \nvariable.\nSome game engines also use search paths to locate resource ﬁ les. For ex-\nample, the Ogre3D rendering engine uses a resource search path contained in \na text ﬁ le named resources.cfg. The ﬁ le provides a simple list of directories \nand Zip archives that should be searched in order when trying to ﬁ nd an as-\nset. That said, searching for assets at runtime is a time-consuming proposition. \nUsually there’s no reason our assets’ paths cannot be known a priori. Presum-\ning this is the case, we can avoid having to search for assets at all—which is \nclearly a superior approach.\n6.1.1.4. \nPath APIs\nClearly paths are much more complex than simple strings. There are many \nthings a programmer may need to do when dealing with paths, such as isolat-\ning the directory, ﬁ lename and extension, canonicalizing a path, converting \n6.1. File System\n\n\n266 \n6. Resources and the File System\nback and forth between absolute and relative paths, and so on. It can be ex-\ntremely helpful to have a feature-rich API to help with these tasks.\nMicrosoft  Windows provides an API for this purpose. It is implement-\ned by the dynamic link library shlwapi.dll, and exposed via the header \nﬁ le shlwapi.h. Complete documentation for this API is provided on the \nMicrosoft  Developer’s Network (MSDN) at the following URL: htt p://msdn2.\nmicrosoft .com/en-us/library/bb773559(VS.85).aspx.\nOf course, the shlwapi API is only available on Win32 platforms. Sony \nprovides a similar API for use on the PLAYSTATION 3. But when writing a \ncross-platform game engine, we cannot use platform-speciﬁ c APIs directly. A \ngame engine may not need all of the functions provided by an API like sh-\nlwapi anyway. For these reasons, game engines oft en implement a stripped-\ndown path-handling API that meets the engine’s particular needs and works \non every operating system targeted by the engine. Such an API can be imple-\nmented as a thin wrapper around the native API on each platform or it can be \nwritt en from scratch.\n6.1.2. Basic File I/O\nThe standard C library provides two APIs for opening, reading, and writing \nthe contents of ﬁ les—one buﬀ ered and the other unbuﬀ ered. Every ﬁ le I/O \nAPI requires data blocks known as buﬀ ers to serve as the source or destination \nof the bytes passing between the program and the ﬁ le on disk. We say a ﬁ le \nI/O API is buﬀ ered when the API manages the necessary input and output data \nbuﬀ ers for you. With an unbuﬀ ered API, it is the responsibility of the pro-\ngrammer using the API to allocate and manage the data buﬀ ers. The standard \nC library’s buﬀ ered ﬁ le I/O routines are sometimes referred to as the stream \nI/O API, because they provide an abstraction which makes disk ﬁ les look like \nstreams of bytes.\nThe standard C library functions for buﬀ ered and un-buﬀ ered ﬁ le I/O are \nlisted in Table 6.1.\nThe standard C library I/O functions are well-documented, so we will not \nrepeat detailed documentation for them here. For more information, please \nrefer to htt p://msdn2.microsoft .com/en-us/library/c565h7xx(VS.71).aspx for \nMicrosoft ’s implementation of the buﬀ ered (stream I/O) API, and to htt p://\nmsdn2.microsoft .com/en-us/library/40bbyw78(VS.71).aspx \nfor \nMicrosoft ’s \nimplementation of the unbuﬀ ered (low-level I/O) API.\nOn UNIX and its variants, the standard C library’s unbuﬀ ered I/O routes \nare native operating system calls. However, on Microsoft  Windows these rou-\ntines are merely wrappers around an even lower-level API. The Win32 func-\ntion CreateFile() creates or opens a ﬁ le for writing or reading, ReadFile()\n",
      "page_number": 269,
      "chapter_number": 14,
      "summary": "This chapter covers segment 14 (pages 269-288). Key topics include string, strings, and game. gStringTable[sid] = strdup(str);\n }\n \nreturn sid;\n}\nAnother idea employed by the Unreal Engine is to wrap the string id and \na pointer to the corresponding C-style character array in a tiny class.",
      "keywords": [
        "Engine Support Systems",
        "game",
        "Windows",
        "game engine",
        "conﬁ guration",
        "system",
        "Engine Conﬁ guration",
        "conﬁ guration options",
        "Engine",
        "system API",
        "API",
        "path",
        "options",
        "string",
        "Strings"
      ],
      "concepts": [
        "string",
        "strings",
        "game",
        "gaming",
        "paths",
        "windows",
        "options",
        "optional",
        "engine",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 3",
          "chapter": 10,
          "title": "Segment 10 (pages 102-110)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 9,
          "title": "Segment 9 (pages 93-101)",
          "relevance_score": 0.46,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 16,
          "title": "String and Text Handling",
          "relevance_score": 0.44,
          "method": "sentence_transformers"
        },
        {
          "book": "makinggames",
          "chapter": 34,
          "title": "Segment 34 (pages 300-307)",
          "relevance_score": 0.43,
          "method": "sentence_transformers"
        },
        {
          "book": "More Effective C++",
          "chapter": 23,
          "title": "Segment 23 (pages 224-233)",
          "relevance_score": 0.42,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 289-307)",
      "start_page": 289,
      "end_page": 307,
      "detection_method": "synthetic",
      "content": "267 \nand WriteFile() read and write data, respectively, and CloseFile() closes \nan open ﬁ le handle. The advantage to using low-level system calls as opposed \nto standard C library functions is that they expose all of the details of the na-\ntive ﬁ le system. For example, you can query and control the security att ributes \nof ﬁ les when using the Windows native API—something you cannot do with \nthe standard C library.\nSome game teams ﬁ nd it useful to manage their own buﬀ ers. For example, \nthe Red Alert 3 team at Electronic Arts observed that writing data into log ﬁ les \nwas causing signiﬁ cant performance degradation. They changed the logging \nsystem so that it accumulated its output into a memory buﬀ er, writing the \nbuﬀ er out to disk only when it was ﬁ lled. Then they moved the buﬀ er dump \nroutine out into a separate thread to avoid stalling the main game loop.\n6.1.2.1. \nTo Wrap or Not To Wrap\n A game engine can be writt en to use the standard C library’s ﬁ le I/O functions or \nthe operating system’s native API. However, many game engines wrap the ﬁ le \nI/O API in a library of custom I/O functions. There are at least three advantages \nto wrapping the operating system’s I/O API. First, the engine programmers \ncan guarantee identical behavior across all target platforms, even when native \nlibraries are inconsistent or buggy on a particular platform. Second, the API \ncan be simpliﬁ ed down to only those functions actually required by the engine, \nwhich keeps maintenance eﬀ orts to a minimum. Third, extended functionality \ncan be provided. For example, the engine’s custom wrapper API might be ca-\npable of dealing ﬁ les on a hard disk, a DVD-ROM or Blu-ray disk on a console, \n6.1. File System\nOperation\nBuﬀ ered API\nUnbuﬀ ered API\nOpen a ﬁ le\nfopen()\nopen()\nClose a ﬁ le\nfclose()\nclose()\nRead from a ﬁ le\nfread()\nread()\nWrite to a ﬁ le\nfwrite()\nwrite()\nSeek to an oﬀ set\nfseek()\nseek()\nReturn current oﬀ set\nftell()\ntell()\nRead a single line\nfgets()\nn/a\nWrite a single line\nfputs()\nn/a\nRead formatt ed string\nfscanf()\nn/a\nWrite formatt ed string\nfprintf()\nn/a\nQuery ﬁ le status\nfstat()\nstat()\nTable 6.1.  Buffered and unbuffered ﬁ le operations in the standard C library.\n\n\n268 \n6. Resources and the File System\nﬁ les on a network (e.g., remote ﬁ les managed by Xbox Live or PSN ), and also \nwith ﬁ les on memory sticks or other kinds of removable media.\n6.1.2.2. Synchronous File I/O\n Both of the standard C library’s ﬁ le I/O libraries are synchronous, meaning that \nthe program making the I/O request must wait until the data has been com-\npletely transferred to or from the media device before continuing. The fol-\nlowing code snippet demonstrates how the entire contents of a ﬁ le might be \nread into an in-memory buﬀ er using the synchronous I/O function fread(). \nNotice how the function syncReadFile() does not return until all the data \nhas been read into the buﬀ er provided.\nbool syncReadFile(const char* filePath,\n \nU8* buffer, size_t bufferSize, size_t& rBytesRead)\n{\n \nFILE* handle = fopen(filePath, \"rb\");\n \nif (handle)\n {\n// BLOCK here until all data has been read.\n \n \nsize_t bytesRead = fread(buffer, 1, bufferSize,   \n   handle);\n \n \nint err = ferror(handle); // get error if any\n  fclose(handle);\n \n \nif (0 == err)\n  {\n   rBytesRead \n= bytesRead;\n   return \ntrue;\n  }\n }\n \nreturn false;\n}\nvoid main(int argc, const char* argv[])\n{\n \nU8 testBuffer[512];\n \nsize_t bytesRead = 0;\n \nif (syncReadFile(\"C:\\\\testfile.bin\",\n \n \ntestBuffer, sizeof(testBuffer), bytesRead))\n {\n \n \nprintf(\"success: read %u bytes\\n\", bytesRead);\n// Contents of buffer can be used here...\n }\n}\n\n\n269 \n6.1.3. Asynchronous File I/O\n Streaming refers to the act of loading data in the background while the main \nprogram continues to run. Many games provide the player with a seamless, \nload-screen-free playing experience by streaming data for upcoming levels \nfrom the DVD-ROM, Blu-ray disk, or hard drive while the game is being \nplayed. Audio and texture data are probably the most commonly streamed \ntypes of data, but any type of data can be streamed, including geometry, level \nlayouts, and animation clips.\nIn order to support streaming, we must utilize an asynchronous ﬁ le I/O \nlibrary, i.e., one which permits the program to continue to run while its I/O re-\nquests are being satisﬁ ed. Some operating systems provide an asynchronous \nﬁ le I/O library out of the box. For example, the Windows Common Language \nRuntime (CLR, the virtual machine upon which languages like Visual BASIC, \nC#, managed C++ and J# are implemented) provides functions like System.\nIO.BeginRead() and System.IO.BeginWrite(). An asynchronous API \nknown as fios is available for the PLAYSTATION 3. If an asynchronous ﬁ le \nI/O library is not available for your target platform, it is possible to write one \nyourself. And even if you don’t have to write it from scratch, it’s probably a \ngood idea to wrap the system API for portability.\nThe following code snippet demonstrates how the entire contents of a ﬁ le \nmight be read into an in-memory buﬀ er using an asynchronous read opera-\ntion. Notice that the asyncReadFile() function returns immediately—the \ndata is not present in the buﬀ er until our callback function asyncReadCom-\nplete() has been called by the I/O library.\nAsyncRequestHandle g_hRequest; // handle to async I/O  \n \n          \n// request\nU8 g_asyncBuffer[512];         // input buffer\nstatic void asyncReadComplete(AsyncRequestHandle   \n \n \n hRequest);\nvoid main(int argc, const char* argv[])\n{\n \n// NOTE: This call to asyncOpen() might itself be an\n \n// asynchronous call, but we’ll ignore that detail  \n \n \n// here and just assume it’s a blocking function.\n \nAsyncFileHandle hFile = asyncOpen(\n  \"C:\\\\testfile.bin\");\n \nif (hFile)\n {\n6.1. File System\n\n\n270 \n6. Resources and the File System\n \n \n// This function requests an I/O read, then  \n \n \n \n \n// returns immediately (non-blocking).\n  g_hRequest \n= asyncReadFile(\n \n \n \nhFile,                  // file handle\n \n \n \ng_asyncBuffer,          // input buffer\n \n \n \nsizeof(g_asyncBuffer),  // size of buffer\nasyncReadComplete);     // callback function\n }\n \n// Now go on our merry way...\n \n// (This loop simulates doing real work while we wait  \n \n// for the I/O read to complete.)\n \nfor (;;)\n {\n  OutputDebugString(\"zzz...\\n\");\n  Sleep(50);\n }\n}\n// This function will be called when the data has been read.\nstatic void asyncReadComplete(\n \nAsyncRequestHandle hRequest)\n{\n \nif (hRequest == g_hRequest \n  && \nasyncWasSuccessful(hRequest))\n {\n \n \n// The data is now present in g_asyncBuffer[] and  \n \n \n// can be used. Query for the number of bytes   \n \n \n \n// actually read:\n \n \nsize_t bytes = asyncGetBytesReadOrWritten(\n   hRequest);\n  char \nmsg[256];\n \n \nsprintf(msg, \"async success, read %u bytes\\n\",  \n \n   bytes); \n  OutputDebugString(msg);\n }\n}\nMost asynchronous I/O libraries permit the main program to wait for an \nI/O operation to complete some time aft er the request was made. This can be \nuseful in situations where only a limited amount of work can be done before \nthe results of a pending I/O request are needed. This is illustrated in the fol-\nlowing code snippet.\nU8 g_asyncBuffer[512];         // input buffer\nvoid main(int argc, const char* argv[])\n{\n\n\n271 \n \nAsyncRequestHandle hRequest = ASYNC_INVALID_HANDLE;\n \nAsyncFileHandle hFile = asyncOpen(\n  \"C:\\\\testfile.bin\");\n \nif (hFile)\n {\n \n \n// This function requests an I/O read, then  \n \n \n \n \n// returns immediately (non-blocking).\n  hRequest \n= asyncReadFile(\n \n \n \nhFile,                  // file handle\n \n \n \ng_asyncBuffer,          // input buffer\n \n \n \nsizeof(g_asyncBuffer),  // size of buffer\nNULL);                  // no callback\n }\n \n// Now do some limited amount of work...\n \nfor (int i = 0; i < 10; ++i)\n {\n  OutputDebugString(\"zzz...\\n\");\n  Sleep(50);\n }\n \n// We can’t do anything further until we have that  \n \n \n// data, so wait for it here.\nasyncWait(hRequest);\n \nif (asyncWasSuccessful(hRequest))\n {\n \n \n// The data is now present in g_asyncBuffer[] and  \n \n \n// can be used. Query for the number of bytes   \n \n \n \n// actually read:\n \n \nsize_t bytes = asyncGetBytesReadOrWritten(\n   hRequest);\n  char \nmsg[256];\n \n \nsprintf(msg, \"async success, read %u bytes\\n\",  \n \n   bytes);\n  OutputDebugString(msg);\n }\n}\nSome asynchronous I/O libraries allow the programmer to ask for an esti-\nmate of how long a particular asynchronous operation will take to complete. \nSome APIs also allow you to set deadlines on a request (eﬀ ectively prioritizes \nthe request relative to other pending requests), and to specify what happens \nwhen a request misses its deadline (e.g., cancel the request, notify the pro-\ngram and keep trying, etc.)\n6.1. File System\n\n\n272 \n6. Resources and the File System\n6.1.3.1. \nPriorities\nIt’s important to remember that ﬁ le I/O is a real-time system, subject to dead-\nlines just like the rest of the game. Therefore, asynchronous I/O operations \noft en have varying priorities. For example, if we are streaming audio from \nthe hard disk or Bluray and playing it on the ﬂ y, loading the next buﬀ er-full \nof audio data is clearly higher priority than, say, loading a texture or a chunk \nof a game level. Asynchronous I/O systems must be capable of suspending \nlower-priority requests, so that higher-priority I/O requests have a chance to \ncomplete within their deadlines.\n6.1.3.2. How Asynchronous File I/O Works\nAsynchronous ﬁ le I/O works by handling I/O requests in a separate thread . \nThe main thread calls functions that simply place requests on a queue and \nthen return immediately. Meanwhile, the I/O thread picks up requests from \nthe queue and handles them sequentially using blocking I/O routines like \nread() or fread(). When a request is completed, a callback provided by \nthe main thread is called, thereby notifying it that the operation is done. If the \nmain thread chooses to wait for an I/O request to complete, this is handled via \na semaphore. (Each request has an associated semaphore, and the main thread \ncan put itself to sleep waiting for that semaphore to be signaled by the I/O \nthread upon completion of the request.)\nVirtually any synchronous operation you can imagine can be transformed \ninto an asynchronous operation by moving the code into a separate thread—\nor by running it on a physically separate processor, such as on one of the six \nsynergistic processing units (SPUs) on the PLAYSTATION 3. See Section 7.6 \nfor more details.\n6.2. The Resource Manager\n Every game is constructed from a wide variety of resources (sometimes called \nassets or media). Examples include meshes, materials, textures, shader pro-\ngrams, animations, audio clips, level layouts, collision primitives, physics pa-\nrameters, and the list goes on. A game’s resources must be managed, both in \nterms of the oﬄ  ine tools used to create them, and in terms of loading, unload-\ning, and manipulating them at runtime. Therefore every game engine has a \nresource manager of some kind.\nEvery resource manager is comprised of two distinct but integrated com-\nponents. One component manages the chain of oﬀ -line tools used to create the \nassets and transform them into their engine-ready form. The other component \n\n\n273 \nmanages the resources at runtime, ensuring that they are loaded into memory \nin advance of being needed by the game and making sure they are unloaded \nfrom memory when no longer needed.\nIn some engines, the resource manager is a cleanly-designed, uniﬁ ed, \ncentralized subsystem that manages all types of resources used by the game. \nIn other engines, the resource manager doesn’t exist as a single subsystem \nper se, but is rather spread across a disparate collection of subsystems, per-\nhaps writt en by diﬀ erent individuals at various times over the engine’s long \nand sometimes colorful history. But no matt er how it is implemented, a re-\nsource manager invariably takes on certain responsibilities and solves a well-\nunderstood set of problems. In this section, we’ll explore the functionality \nand some of the implementation details of a typical game engine resource \nmanager.\n6.2.1. Off-Line Resource Management and the Tool Chain\n6.2.1.1. \nRevision Control for Assets\n On a small game project, the game’s assets can be managed by keeping loose \nﬁ les sitt ing around on a shared network drive with an ad hoc directory struc-\nture. This approach is not feasible for a modern commercial 3D game, com-\nprised of a massive number and variety of assets. For such a project, the team \nrequires a more formalized way to track and manage its assets.\nSome game teams use a source code revision control system to manage \ntheir resources. Art source ﬁ les (Maya scenes, Photoshop .PSD ﬁ les, Illustrator \nﬁ les, etc.) are checked in to Perforce or a similar package by the artists. This \napproach works reasonably well, although some game teams build custom \nasset management tools to help ﬂ att en the learning curve for their artists. Such \ntools may be simple wrappers around a commercial revision control system, \nor they might be entirely custom.\nDealing with Data Size\nOne of the biggest problems in the revision control of art assets is the sheer \namount of data. Whereas C++ and script source code ﬁ les are small, relative \nto their impact on the project, art ﬁ les tend to be much, much larger. Because \nmany source control systems work by copying ﬁ les from the central reposito-\nry down to the user’s local machine, the sheer size of the asset ﬁ les can render \nthese packages almost entirely useless.\nI’ve seen a number of diﬀ erent solutions to this problem employed at \nvarious studios. Some studios turn to commercial revision control systems \nlike Alienbrain that have been speciﬁ cally designed to handle very large data \n6.2. The Resource Manager\n\n\n274 \n6. Resources and the File System\nsizes. Some teams simply “take their lumps” and allow their revision control \ntool to copy assets locally. This can work, as long as your disks are big enough \nand your network bandwidth suﬃ  cient, but it can also be ineﬃ  cient and slow \nthe team down. Some teams build elaborate systems on top of their revision \ncontrol tool to ensure that a particular end-user only gets local copies of the \nﬁ les he or she actually needs. In this model, the user either has no access to \nthe rest of the repository or can access it on a shared network drive when \nneeded.\nAt Naughty Dog we use a proprietary tool that makes use of UNIX symbol-\nic links to virtually eliminate data copying, while permitt ing each user to have \na complete local view of the asset repository. As long as a ﬁ le is not checked \nout for editing, it is a symlink to a master ﬁ le on a shared network drive. Sym-\nbolic links occupy very litt le space on the local disk, because it is nothing more \nthan a directory entry. When the user checks out a ﬁ le for editing, the symlink \nis removed, and a local copy of the ﬁ le replaces it. When the user is done edit-\ning and checks the ﬁ le in, the local copy becomes the new master copy, its revi-\nsion history is updated in a master database, and the local ﬁ le turns back into \na symlink. This systems works very well, but it requires the team to build their \nown revision control system from scratch; I am unaware of any commercial \ntool that works like this. Also, symbolic links are a UNIX feature—such a tool \ncould probably be built with Windows junctions (the Windows equivalent of \na symbolic link), but I haven’t seen anyone try it as yet.\n6.2.1.2. The Resource Database\n As we’ll explore in depth in the next section, most assets are not used in their \noriginal format by the game engine. They need to pass through some kind of \nasset conditioning pipeline, whose job it is to convert the assets into the binary \nformat needed by the engine. For every resource that passes through the asset \nconditioning pipeline, there is some amount of metadata that describes how \nthat resource should be processed. When compressing a texture bitmap, we \nneed to know what type of compression best suits that particular image. When \nexporting an animation, we need to know what range of frames in Maya \nshould be exported. When exporting character meshes out of a Maya scene \ncontaining multiple characters, we need to know which mesh corresponds to \nwhich character in the game.\nTo manage all of this metadata, we need some kind of database. If we are \nmaking a very small game, this database might be housed in the brains of the \ndevelopers themselves. I can hear them now: “Remember: the player’s anima-\ntions need to have the ‘ﬂ ip X’ ﬂ ag set, but the other characters must not have it \nset… or… rats… is it the other way around?”\n\n\n275 \nClearly for any game of respectable size, we simply cannot rely on the \nmemories of our developers in this manner. For one thing, the sheer volume of \nassets becomes overwhelming quite quickly. Processing individual resource \nﬁ les by hand is also far too time-consuming to be practical on a full-ﬂ edged \ncommercial game production. Therefore, every professional game team has \nsome kind of semi-automated resource pipeline, and the data that drives the \npipeline is stored in some kind of resource database.\nThe resource database takes on vastly diﬀ erent forms in diﬀ erent game \nengines. In one engine, the metadata describing how a resource should be \nbuilt might be embedded into the source assets themselves (e.g., it might be \nstored as so-called blind data within a Maya ﬁ le). In another engine, each \nsource resource ﬁ le might be accompanied by a small text ﬁ le that describes \nhow it should be processed. Still other engines encode their resource build-\ning metadata in a set of XML ﬁ les, perhaps wrapped in some kind of custom \ngraphical user interface. Some engines employ a true relational database, such \nas Microsoft  Access, MySQL, or conceivably even a heavy-weight database \nlike Oracle.\nWhatever its form, a resource database must provide the following basic \nfunctionality:\nz The ability to deal with multiple types of resources, ideally (but certainly \nnot necessarily) in a somewhat consistent manner.\nz The ability to create new resources.\nz The ability to delete resources.\nz The ability to inspect and modify existing resources.\nz The ability to move a resource’s source ﬁ le(s) from one location to an-\nother on-disk. (This is very helpful because artists and game designers \noft en need to rearrange assets to reﬂ ect changing project goals, re-think-\ning of game designs, feature additions and cuts, etc.)\nz The ability of a resource to cross-reference other resources (e.g., the ma-\nterial used by a mesh, or the collection of animations needed by level \n17). These cross-references typically drive both the resource building \nprocess and the loading process at runtime.\nz The ability to maintain referential integrity of all cross-references within \nthe database and to do so in the face of all common operations such as \ndeleting or moving resources around.\nz The ability to maintain a revision history, complete with a log of who \nmade each change and why.\nz It is also very helpful if the resource database supports searching or \nquerying in various ways. For example, a developer might want to \n6.2. The Resource Manager\n\n\n276 \n6. Resources and the File System\nknow in which levels a particular animation is used or which textures \nare referenced by a set of materials. Or they might simply be trying to \nﬁ nd a resource whose name momentarily escapes them.\nIt should be prett y obvious from looking at the above list that creating a \nreliable and robust resource database is no small task. When designed well \nand implemented properly, the resource database can quite literally make \nthe diﬀ erence between a team that ships a hit game and a team that spins its \nwheels for 18 months before being forced by management to abandon the \nproject (or worse). I know this to be true, because I’ve personally experienced \nboth.\n6.2.1.3. Some Successful Resource Database Designs\nEvery game team will have diﬀ erent requirements and make diﬀ erent deci-\nsions when designing their resource database. However, for what it’s worth, \nhere are some designs that have worked well in my own experience:\nUnreal Engine 3\n Unreal’s resource database is managed by their über-tool, UnrealEd . UnrealEd \nis responsible for literally everything, from resource metadata management to \nasset creation to level layout and more. UnrealEd has its drawbacks, but its \nsingle biggest beneﬁ t is that UnrealEd is a part of the game engine itself. This \npermits assets to be created and then immediately viewed in their full glory, \nexactly as they will appear in-game. The game can even be run from within \nUnrealEd, in order to visualize the assets in their natural surroundings and \nsee if and how they work in-game.\nAnother big beneﬁ t of UnrealEd is what I would call one-stop shopping. \nUnrealEd’s Generic Browser (depicted in Figure 6.1) allows a developer to \naccess literally every resource that is consumed by the engine. Having a sin-\ngle, uniﬁ ed, and reasonably-consistent interface for creating and managing \nall types of resources is a big win. This is especially true considering that the \nresource data in most other game engines is fragmented across countless in-\nconsistent and oft en cryptic tools. Just being able to ﬁ nd any resource easily in \nUnrealEd is a big plus.\nUnreal can be less error-prone than many other engines, because assets \nmust be explicitly imported into Unreal’s resource database. This allows re-\nsources to be checked for validity very early in the production process. In \nmost game engines, any old data can be thrown into the resource database, \nand you only know whether or not that data is valid when it is eventually \nbuilt—or sometimes not until it is actually loaded into the game at runtime. \nBut with Unreal, assets can be validated as soon as they are imported into \n\n\n277 \nUnrealEd. This means that the person who created the asset gets immediate \nfeedback as to whether his or her asset is conﬁ gured properly.\nOf course, Unreal’s approach has some serious drawbacks. For one thing, \nall resource data is stored in a small number of large package ﬁ les . These ﬁ les \nare binary, so they are not easily merged by a revision control package like \nCVS, Subversion, or Perforce. This presents some major problems when more \nthan one user wants to modify resources that reside in a single package. Even \nif the users are trying to modify diﬀ erent resources, only one user can lock the \npackage at a time, so the other has to wait. The severity of this problem can be \nreduced by dividing resources into relatively small, granular packages, but it \ncannot practically be eliminated.\nReferential integrity is quite good in UnrealEd, but there are still some \nproblems. When a resource is renamed or moved around, all references to it \nare maintained automatically using a dummy object that remaps the old re-\n6.2. The Resource Manager\nFigure 6.1.  UnrealEd’s Generic Browser.\n\n\n278 \n6. Resources and the File System\nsource to its new name/location. The problem with these dummy remapping \nobjects is that they hang around and accumulate and sometimes cause prob-\nlems, especially if a resource is deleted. Overall, Unreal’s referential integrity \nis quite good, but it is not perfect.\nDespite its problems, UnrealEd is by far the most user-friendly, well-in-\ntegrated, and streamlined asset creation toolkit, resource database, and asset- \nconditioning pipeline that I have ever worked with.\nNaughty Dog’s Uncharted: Drake’s Fortune Engine\n For Uncharted: Drake’s Fortune (UDF), Naughty Dog stored its resource \nmetadata in a MySQL database. A custom graphical user interface was writt en \nto manage the contents of the database. This tool allowed artists, game design-\ners, and programmers alike to create new resources, delete existing resources, \nand inspect and modify resources as well. This GUI was a crucial component \nof the system, because it allowed users to avoid having to learn the intricacies \nof interacting with a relational database via SQL.\nThe original MySQL database used on UDF did not provide a useful his-\ntory of the changes made to the database, nor did it provide a good way to roll \nback “bad” changes. It also did not support multiple users editing the same \nresource, and it was diﬃ  cult to administer. Naughty Dog has since moved \naway from MySQL in favor of an XML ﬁ le-based asset database, managed \nunder Perforce.\nBuilder, Naughty Dog’s resource database GUI, is depicted in Figure 6.2. \nThe window is broken into two main sections: a tree view showing all resourc-\nes in the game on the left  and a properties window on the right, allowing the \nresource(s) that are selected in the tree view to be viewed and edited. The re-\nsource tree contains folders for organizational purposes, so that the artists and \ngame designers can organize their resources in any way they see ﬁ t. Various \ntypes of resources can be created and managed within any folder, including \nactors and levels, and the various subresources that comprise them (primar-\nily meshes, skeletons, and animations). Animations can also be grouped into \npseudo-folders known as bundles. This allows large groups of animations to \nbe created and then managed as a unit, and prevents a lot of wasted time drag-\nging individual animations around in the tree view.\nThe asset conditioning pipeline on UDF consists of a set of resource ex-\nporters, compilers, and linkers that are run from the command line. The engine \nis capable of dealing with a wide variety of diﬀ erent kinds of data objects, but \nthese are packaged into one of two types of resource ﬁ les: actors and levels. An \nactor can contain skeletons, meshes, materials, textures, and/or animations. \nA level contains static background meshes, materials and textures, and also \nlevel-layout information. To build an actor, one simply types ba name-of-actor \n\n\n279 \non the command line; to build a level, one types bl name-of-level. These com-\nmand-line tools query the database to determine exactly how to build the actor \nor level in question. This includes information on how to export the assets \nfrom DCC tools like Maya, Photoshop etc., how to process the data, and how \nto package it into binary .pak ﬁ les that can be loaded by the game engine. This \nis much simpler than in many engines, where resources have to be exported \nmanually by the artists—a time-consuming, tedious, and error-prone task.\nThe beneﬁ ts of the resource pipeline design used by Naughty Dog in-\nclude:\nz Granular resources. Resources can be manipulated in terms of logical en-\ntities in the game—meshes, materials, skeletons, and animations. These \n6.2. The Resource Manager\nFigure 6.2. The front-end GUI for Naughty Dog’s off-line resource database, Builder.\n\n\n280 \n6. Resources and the File System\nresource types are granular enough that the team almost never has \nconﬂ icts in which two users want to edit the same resource simultane-\nously.\nz The necessary features (and no more). The Builder tool provides a powerful \nset of features that meet the needs of the team, but Naughty Dog didn’t \nwaste any resources creating features they didn’t need.\nz Obvious mapping to source ﬁ les. A user can very quickly determine which \nsource assets (native DCC ﬁ les, like Maya .ma ﬁ les or photoshop .psd \nﬁ les) make up a particular resource.\nz Easy to change how DCC data is exported and processed. Just click on the \nresource in question and twiddle its processing properties within the \nresource database GUI.\nz Easy to build assets. Just type ba or bl followed by the resource name on \nthe command line. The dependency system takes care of the rest.\nOf course, the UDF tool chain has some drawbacks as well, including:\nz Lack of visualization tools. The only way to preview an asset is to load \nit into the game or the model/animation viewer (which is really just a \nspecial mode of the game itself).\nz The tools aren’t fully integrated. Naughty Dog uses one tool to lay out \nlevels, another to manage the majority of resources in the resource data-\nbase, and a third to set up materials and shaders (this is not part of the \nresource database front-end). Building the assets is done on the com-\nmand line. It might be a bit more convenient if all of these functions \nwere to be integrated into a single tool. However, Naughty Dog has no \nplans to do this, because the beneﬁ t would probably not outweigh the \ncosts involved.\nOgre’s Resource Manager System\n Ogre3D is a rendering engine, not a full-ﬂ edged game engine. That said, Ogre \ndoes boast a reasonably complete and very well-designed runtime resource \nmanager. A simple, consistent interface is used to load virtually any kind of \nresource. And the system has been designed with extensibility in mind. Any \nprogrammer can quite easily implement a resource manager for a brand new \nkind of asset and integrate it easily into Ogre’s resource framework.\nOne of the drawbacks of Ogre’s resource manager is that it is a runtime- \nonly solution. Ogre lacks any kind of oﬀ -line resource database. Ogre does \nprovide some exporters which are capable of converting a Maya ﬁ le into a \nmesh that can be used by Ogre (complete with materials, shaders, a skeleton \nand optional animations). However, the exporter must be run manually from \n\n\n281 \nwithin Maya itself. Worse, all of the metadata describing how a particular \nMaya ﬁ le should be exported and processed must be entered by the user do-\ning the export.\nIn summary, Ogre’s runtime resource manager is powerful and well-de-\nsigned. But Ogre would beneﬁ t a great deal from an equally powerful and \nmodern resource database and asset conditioning pipeline on the tools side.\nMicrosoft’s XNA\n XNA is a game development toolkit by Microsoft , targeted at the PC and Xbox \n360 platforms. XNA’s resource management system is unique, in that it lever-\nages the project management and build systems of the Visual Studio IDE to \nmanage and build the assets in the game as well. XNA’s game development \ntool, Game Studio Express, is just a plug-in to Visual Studio Express. You can \nread more about Game Studio Express at htt p://msdn.microsoft .com/en-us/\nlibrary/bb203894.aspx.\n6.2.1.4. The Asset Conditioning Pipeline\nIn Section 1.7, we learned that resource data is typically created using ad-\nvanced digital content creation (DCC) tools like Maya, Z-Brush, Photoshop, or \nHoudini. However, the data formats used by these tools are usually not suit-\nable for direct consumption by a game engine. So the majority of resource data \nis passed through an asset conditioning pipeline (ACP) on its way to the game \nengine. The ACP is sometimes referred to as the resource conditioning pipeline \n(RCP), or simply the tool chain.\nEvery resource pipeline starts with a collection of source assets in native \nDCC formats (e.g., Maya .ma or .mb ﬁ les, Photoshop .psd ﬁ les, etc.)  These \nassets are typically passed through three processing stages on their way to the \ngame engine:\n \n1. Exporters . We need some way of gett ing the data out of the DCC’s na-\ntive format and into a format that we can manipulate. This is usually \naccomplished by writing a custom plug-in for the DCC in question. It \nis the plug-in’s job to export the data into some kind of intermediate ﬁ le \nformat that can be passed to later stages in the pipeline. Most DCC ap-\nplications provide a reasonably convenient mechanism for doing this. \nMaya actually provides three: a C++ SDK, a scripting language called \nMEL , and most recently a Python interface as well.\n \nIn cases where a DCC application provides no customization hooks, we \ncan always save the data in one of the DCC tool’s native formats. With \nany luck, one of these will be an open format, a reasonably-intuitive text \nformat, or some other format that we can reverse engineer. Presuming \n6.2. The Resource Manager\n\n\n282 \n6. Resources and the File System\nthis is the case, we can pass the ﬁ le directly to the next stage of the pipe-\nline.\n \n2. Resource compilers . We oft en have to “massage” the raw data exported \nfrom a DCC application in various ways in order to make it game-ready. \nFor example, we might need to rearrange a mesh’s triangles into strips, or \ncompress a texture bitmap, or calculate the arc lengths of the segments of \na Catmull-Rom spline. Not all types of resources need to be compiled—\nsome might be game-ready immediately upon being exported.\n \n3. Resource linkers . Multiple resource ﬁ les sometimes need to be combined \ninto a single useful package prior to being loaded by the game engine. \nThis mimics the process of linking together the object ﬁ les of a compiled \nC++ program into an executable ﬁ le, and so this process is sometimes \ncalled resource linking. For example, when building a complex compos-\nite resource like a 3D model, we might need to combine the data from \nmultiple exported mesh ﬁ les, multiple material ﬁ les, a skeleton ﬁ le, and \nmultiple animation ﬁ les into a single resource. Not all types of resources \nneed to be linked—some assets are game-ready aft er the export or com-\npile steps.\nResource Dependencies and Build Rules\n Much like compiling the source ﬁ les in a C or C++ project and then linking \nthem into an executable, the asset conditioning pipeline processes source as-\nsets (in the form of Maya geometry and animation ﬁ les, Photoshop PSD ﬁ les, \nraw audio clips, text ﬁ les, etc.), converts them into game-ready form, and then \nlinks them together into a cohesive whole for use by the engine. And just like \nthe source ﬁ les in a computer program, game assets oft en have interdepen-\ndencies. (For example, a mesh refers to one or more materials, which in turn \nrefer to various textures.) These interdependencies typically have an impact \non the order in which assets must be processed by the pipeline. (For example, \nwe might need to build a character’s skeleton before we can process any of \nthat character’s animations.) In addition, the dependencies between assets tell \nus which assets need to be rebuilt when a particular source asset changes.\nBuild dependencies revolve not only around changes to the assets them-\nselves, but also around changes to data formats. If the format of the ﬁ les used \nto store triangle meshes changes, for instance, all meshes in the entire game \nmay need to be reexported and/or rebuilt. Some game engines employ data \nformats that are robust to version changes. For example, an asset may contain \na version number, and the game engine may include code that “knows” how \nto load and make use of legacy assets. The downside of such a policy is that \nasset ﬁ les and engine code tend to become bulky. When data format changes \n\n\n283 \nare relatively rare, it may be bett er to just bite the bullet and reprocess all the \nﬁ les when format changes do occur.\nEvery asset conditioning pipeline requires a set of rules that describe the \ninterdependencies between the assets, and some kind of build tool that can \nuse this information to ensure that the proper assets are built, in the proper \norder, when a source asset is modiﬁ ed. Some game teams roll their own build \nsystem. Others use an established tool, such as make. Whatever solution is \nselected, teams should treat their build dependency system with utmost care. \nIf you don’t, changes to sources assets may not trigger the proper assets to \nbe rebuilt. The result can be inconsistent game assets, which may lead to vi-\nsual anomalies or even engine crashes. In my personal experience, I’ve wit-\nnessed countness hours wasted in tracking down problems that could have \nbeen avoided had the asset interdependencies been properly speciﬁ ed and the \nbuild system implemented to use them reliably.\n6.2.2. Runtime Resource Management\n Let us turn our att ention now to how the assets in our resource database are \nloaded, managed, and unloaded within the engine at runtime.\n6.2.2.1. Responsibilities of the Runtime Resource Manager\nA game engine’s runtime resource manager takes on a wide range of responsi-\nbilities, all related to its primary mandate of loading resources into memory:\nz Ensures that only one copy of each unique resource exists in memory at \nany given time.\nz Manages the lifetime of each resource loads needed resources and un-\nloads resources that are no longer needed.\nz Handles loading of composite resources. A composite resource is a resource \ncomprised of other resources. For example, a 3D model is a composite \nresource that consists of a mesh, one or more materials, one or more \ntextures, and optionally a skeleton and multiple skeletal animations.\nz Maintains referential integrity . This includes internal referential integrity \n(cross-references within a single resource) and external referential integ-\nrity (cross-references between resources). For example, a model refers to \nits mesh and skeleton; a mesh refers to its materials, which in turn refer \nto texture resources; animations refer to a skeleton, which ultimately \nties them to one or more models. When loading a composite resource, \nthe resource manager must ensure that all necessary subresources are \nloaded, and it must patch in all of the cross-references properly.\nz Manages the memory usage of loaded resources and ensures that re-\nsources are stored in the appropriate place(s) in memory.\n6.2. The Resource Manager\n\n\n284 \n6. Resources and the File System\nz Permits custom processing to be performed on a resource aft er it has been \nloaded, on a per-resource-type basis. This process is sometimes known \nas logging in or load-initializing the resource.\nz Usually (but not always) provides a single uniﬁ ed interface through \nwhich a wide variety of resource types can be managed. Ideally a re-\nsource manager is also easily extensible, so that it can handle new types \nof resources as they are needed by the game development team.\nz Handles streaming (i.e., asynchronous resource loading), if the engine \nsupports this feature.\n6.2.2.2. Resource File and Directory Organization\n In some game engines (typically PC engines), each individual resource is \nmanaged in a separate “loose” ﬁ le on-disk. These ﬁ les are typically con-\ntained within a tree of directories whose internal organization is designed \nprimarily for the convenience of the people creating the assets; the engine \ntypically doesn’t care where resource ﬁ les are located within the resource \ntree. Here’s a typical resource directory tree for a hypothetical game called \nSpace Evaders:\nSpaceEvaders\nRoot directory for entire game.\n  Resources\nRoot of all resources.\n    Characters\nNon-player character models and animations.\n      Pirate\nModels and animations for pirates.\n      Marine\nModels and animations for marines.\n      ...\n    Player\nPlayer character models and animations.\n    Weapons\nModels and animations for weapons.\n      Pistol\nModels and animations for the pistol.\n      Rifle\nModels and animations for the riﬂ e.\n      BFG\nModels and animations for the big... uh… gun.\n      ...\n    Levels\nBackground geometry and level layouts.\n      Level1\nFirst level’s resources.\n      Level2\nSecond level’s resources.\n      ...\n    Objects\nMiscellaneous 3D objects.\n      Crate\nThe ubiquitous breakable crate.\n      Barrel\nThe ubiquitous exploding barrel.\nOther engines package multiple resources together in a single ﬁ le, such as \na ZIP archive, or some other composite ﬁ le (perhaps of a proprietary format). \n\n\n285 \nThe primary beneﬁ t of this approach is improved load times. When loading \ndata from ﬁ les, the three biggest costs are seek times (i.e., moving the read head \nto the correct place on the physical media), the time required to open each \nindividual ﬁ le, and the time to read the data from the ﬁ le into memory. Of \nthese, the seek times and ﬁ le-open times can be non-trivial on many operating \nsystems. When a single large ﬁ le is used, all of these costs are minimized. A \nsingle ﬁ le can be organized sequentially on the disk, reducing seek times to \na minimum. And with only one ﬁ le to open, the cost of opening individual \nresource ﬁ les is eliminated.\nThe Ogre3D rendering engine’s resource manager permits resources to \nexist as loose ﬁ les on disk, or as virtual ﬁ les within a large ZIP archive. The \nprimary beneﬁ ts of the ZIP format are the following:\n \n1. It is an open format. The zlib and zziplib libraries used to read and \nwrite ZIP archives are freely available. The zlib SDK is totally free (see \nhtt p://www.zlib.net), while the zziplib SDK falls under the Lesser Gnu \nPublic License (LGPL) (see htt p://zziplib.sourceforge.net).\n \n2. The virtual ﬁ les within a ZIP archive “remember” their relative paths. This \nmeans that a ZIP archive “looks like” a raw ﬁ le system for most in-\ntents and purposes. The Ogre resource manager identiﬁ es all resources \nuniquely via strings that appear to be ﬁ le system paths. However, these \npaths sometimes identify virtual ﬁ les within a ZIP archive instead of \nloose ﬁ les on disk, and a game programmer needn’t be aware of the dif-\nference in most situations.\n \n3. ZIP archives may be compressed. This reduces the amount of disk space \noccupied by resources. But, more importantly, it again speeds up load \ntimes, as less data need be loaded into memory from the ﬁ xed disk. This \nis especially helpful when reading data from a DVD-ROM or Blu-ray \ndisk, as the data transfer rates of these devices are much slower than a \nhard disk drive. Hence the cost of decompressing the data aft er it has \nbeen loaded into memory is oft en more than oﬀ set by the time saved in \nloading less data from the device.\n \n4. ZIP archives are modular. Resources can be grouped together into a ZIP \nﬁ le and managed as a unit. One particularly elegant application of this \nidea is in product localization. All of the assets that need to be local-\nized (such as audio clips containing dialogue and textures that contain \nwords or region-speciﬁ c symbols) can be placed in a single ZIP ﬁ le, and \nthen diﬀ erent versions of this ZIP ﬁ le can be generated, one for each \nlanguage or region. To run the game for a particular region, the engine \nsimply loads the corresponding version of the ZIP archive.\n6.2. The Resource Manager\n",
      "page_number": 289,
      "chapter_number": 15,
      "summary": "This chapter covers segment 15 (pages 289-307). Key topics include resources, assets, and tools. The advantage to using low-level system calls as opposed \nto standard C library functions is that they expose all of the details of the na-\ntive ﬁ le system.",
      "keywords": [
        "Resource",
        "Resource Manager",
        "Resource Database",
        "game",
        "File System",
        "runtime resource manager",
        "game engine",
        "les",
        "data",
        "assets",
        "system",
        "File System resource",
        "resource data",
        "Resource Manager System",
        "engine"
      ],
      "concepts": [
        "resources",
        "assets",
        "tools",
        "data",
        "engine",
        "manage",
        "managed",
        "loading"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 1",
          "chapter": 9,
          "title": "Segment 9 (pages 78-85)",
          "relevance_score": 0.69,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 8,
          "title": "Segment 8 (pages 65-77)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 10,
          "title": "Segment 10 (pages 86-98)",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "More Effective C++",
          "chapter": 8,
          "title": "Segment 8 (pages 61-75)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 50,
          "title": "Segment 50 (pages 512-520)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 308-325)",
      "start_page": 308,
      "end_page": 325,
      "detection_method": "synthetic",
      "content": "286 \n6. Resources and the File System\nUnreal Engine 3 takes a similar approach, with a few important diﬀ er-\nences. In Unreal, all resources must be contained within large composite \nﬁ les known as packages (a.k.a. “pak ﬁ les”) . No loose disk ﬁ les are permitt ed. \nThe format of a package ﬁ le is proprietary. The Unreal Engine’s game editor, \nUnrealEd, allows developers to create and manage packages and the resourc-\nes they contain.\n6.2.2.3. Resource File Formats\n Each type of resource ﬁ le potentially has a diﬀ erent format. For example, a \nmesh ﬁ le is always stored in a diﬀ erent format than that of a texture bitmap. \nSome kinds of assets are stored in standardized, open formats. For example, \ntextures are typically stored as Targa ﬁ les (TGA), Portable Network Graph-\nics ﬁ les (PNG), Tagged Image File Format ﬁ les (TIFF), Joint Photographic Ex-\nperts Group ﬁ les (JPEG), or Windows Bitmap ﬁ les (BMP)—or in a standard-\nized compressed format such as DirectX’s S3 Texture Compression family of \nformats (S3TC, also known as DXTn or DXTC). Likewise, 3D mesh data is \noft en exported out of a modeling tool like Maya or Lightwave into a stan-\ndardized format such as OBJ or COLLADA for consumption by the game \nengine.\nSometimes a single ﬁ le format can be used to house many diﬀ erent types \nof assets. For example, the Granny SDK by Rad Game Tools (htt p://www.rad-\ngametools.com) implements a ﬂ exible open ﬁ le format that can be used to \nstore 3D mesh data, skeletal hierarchies, and skeletal animation data. (In fact \nthe Granny ﬁ le format can be easily repurposed to store virtually any kind of \ndata imaginable.)\nMany game engine programmers roll their own ﬁ le formats for various \nreasons. This might be necessary if no standardized format provides all of \nthe information needed by the engine. Also, many game engines endeavor to \ndo as much oﬀ -line processing as possible in order to minimize the amount \nof time needed to load and process resource data at runtime. If the data \nneeds to conform to a particular layout in memory, for example, a raw binary \nformat might be chosen so that the data can be laid out by an oﬀ -line tool \n(rather than att empting to format it at runtime aft er the resource has been \nloaded).\n6.2.2.4. Resource GUIDs\n Every resource in a game must have some kind of globally unique identiﬁ er \n(GUID). The most common choice of GUID is the resource’s ﬁ le system path \n(stored either as a string or a 32-bit hash). This kind of GUID is intuitive, be-\ncause it clearly maps each resource to a physical ﬁ le on-disk. And it’s guar-\n\n\n287 \nanteed to be unique across the entire game, because the operating system al-\nready guarantees that no two ﬁ les will have the same path.\nHowever, a ﬁ le system path is by no means the only choice for a resource \nGUID. Some engines use a less-intuitive type of GUID, such as a 128-bit hash \ncode, perhaps assigned by a tool that guarantees uniqueness. In other engines, \nusing a ﬁ le system path as a resource identiﬁ er is infeasible. For example, \nUnreal Engine 3 stores many resources in a single large ﬁ le known as a pack-\nage, so the path to the package ﬁ le does not uniquely identify any one re-\nsource. To overcome this problem, an Unreal package ﬁ le is organized into \na folder hierarchy containing individual resources. Unreal gives each indi-\nvidual resource within a package a unique name which looks much like a ﬁ le \nsystem path. So in Unreal, a resource GUID is formed by concatenating the \n(unique) name of the package ﬁ le with the in-package path of the resource \nin question. For example, the Gears of War resource GUID Locust_Boomer.\nPhysicalMaterials.  LocustBoomerLeather identiﬁ es a material called \nLocustBoomerLeather within the PhysicalMaterials folder of the \nLocust_Boomer package ﬁ le.\n6.2.2.5. The Resource Registry\nIn order to ensure that only one copy of each unique resource is loaded into \nmemory at any given time, most resource managers maintain some kind of \nregistry of loaded resources. The simplest implementation is a dictionary—i.e., \na collection of key-value pairs . The keys contain the unique ids of the resources, \nwhile the values are typically pointers to the resources in memory.\nWhenever a resource is loaded into memory, an entry for it is added to the \nresource registry dictionary, using its GUID as the key. Whenever a resource is \nunloaded, its registry entry is removed. When a resource is requested by the \ngame, the resource manager looks up the resource by its GUID within the re-\nsource registry. If the resource can be found, a pointer to it is simply returned. \nIf the resource cannot be found, it can either be loaded automatically or a \nfailure code can be returned.\nAt ﬁ rst blush, it might seem most intuitive to automatically load a re-\nquested resource if it cannot be found in the resource registry. And in fact, \nsome game engines do this. However, there are some serious problems with \nthis approach. Loading a resource is a slow operation, because it involves lo-\ncating and opening a ﬁ le on disk, reading a potentially large amount of data \ninto memory (from a potentially slow device like a DVD-ROM drive), and \nalso possibly performing post-load initialization of the resource data once it \nhas been loaded. If the request comes during active gameplay, the time it takes \nto load the resource might cause a very noticeable hitch in the game’s frame \n6.2. The Resource Manager\n\n\n288 \n6. Resources and the File System\nrate, or even a multi-second freeze. For this reason, engines tend to take one of \ntwo alternative approaches:\nResource loading might be disallowed completely during active game-\n1. \nplay. In this model, all of the resources for a game level are loaded en \nmasse just prior to gameplay, usually while the player watches a loading \nscreen or progress bar of some kind.\nResource loading might be done \n2. \nasynchronously (i.e., the data might be \nstreamed). In this model, while the player is engaged in level A, the re-\nsources for level B are being loaded in the background. This approach \nis preferable because it provides the player with a load-screen-free play \nexperience. However, it is considerably more diﬃ  cult to implement.\n6.2.2.6. Resource Lifetime\nThe lifetime of a resource is deﬁ ned as the time period between when it is ﬁ rst \nloaded into memory and when its memory is reclaimed for other purposes. \nOne of the resource manager’s jobs is to manage resource lifetimes—either \nautomatically, or by providing the necessary API functions to the game, so it \ncan manage resource lifetimes manually.\nEach resource has its own lifetime requirements:\nz Some resources must be loaded when the game ﬁ rst starts up and must \nstay resident in memory for the entire duration of the game. That is, \ntheir lifetimes are eﬀ ectively inﬁ nite. These are sometimes called load-\nand-stay-resident (LSR) resources. Typical examples include the player \ncharacter’s mesh, materials, textures and core animations, textures and \nfonts used on the heads-up display (HUD), and the resources for all of \nthe standard-issue weapons used throughout the game. Any resource \nthat is visible or audible to the player throughout the entire game (and \ncannot be loaded on the ﬂ y when needed) should be treated as an LSR \nresource.\nz Other resources have a lifetime that matches that of a particular game \nlevel. These resources must be in memory by the time the level is ﬁ rst \nseen by the player and can be dumped once the player has permanently \nleft  the level.\nz Some resources might have a lifetime that is shorter than the duration of \nthe level in which they are found. For example, the animations and au-\ndio clips that make up an in-game cut-scene (a mini-movie that advances \nthe story or provides the player with important information) might be \nloaded in advance of the player seeing the cut-scene and then dumped \nonce the cut-scene has played.\n\n\n289 \nz Some resources like background music, ambient sound eﬀ ects, or full-\nscreen movies are streamed “live” as they play. The lifetime of this kind \nof resource is diﬃ  cult to deﬁ ne, because each byte only persists in mem-\nory for a tiny fraction of a second, but the entire piece of music sounds \nlike it lasts for a long period of time. Such assets are typically loaded in \nchunks of a size that matches the underlying hardware’s requirements. \nFor example, a music track might be read in 4 kB chunks, because that \nmight be the buﬀ er size used by the low-level sound system. Only two \nchunks are ever present in memory at any given moment—the chunk \nthat is currently playing and the chunk immediately following it that is \nbeing loaded into memory.\nThe question of when to load a resource is usually answered quite easily, \nbased on knowledge of when the asset is ﬁ rst seen by the player. However, the \nquestion of when to unload a resource and reclaim its memory is not so eas-\nily answered. The problem is that many resources are shared across multiple \nlevels. We don’t want to unload a resource when level A is done, only to im-\nmediately reload it because level B needs the same resource.\nOne solution to this problem is to reference-count the resources. When-\never a new game level needs to be loaded, the list of all resources used by that \nlevel is traversed, and the reference count for each resource is incremented \nby one (but they are not loaded yet). Next, we traverse the resources of any \nunneeded levels and decrement their reference counts by one; any resource \nwhose reference count drops to zero is unloaded. Finally, we run through the \nlist of all resources whose reference count just went from zero to one and load \nthose assets into memory.\nFor example, imagine that level 1 uses resources A, B, and C, and that \nlevel 2 uses resources B, C, D, and E. (B and C are shared between both levels.) \nTable 6.2 shows the reference counts of these ﬁ ve resources as the player plays \nthrough levels 1 and 2. In this table, reference counts are shown in boldface \ntype to indicate that the corresponding resource actually exists in memory, \nwhile a grey background indicates that the resource is not in memory. A refer-\nence count in parentheses indicates that the corresponding resource data is \nbeing loaded or unloaded.\n6.2.2.7. Memory Management for Resources\nResource management is closely related to memory management , because we \nmust inevitably decide where the resources should end up in memory once \nthey have been loaded. The destination of every resource is not always the \nsame. For one thing, certain types of resources must reside in video RAM. \nTypical examples include textures, vertex buﬀ ers, index buﬀ ers, and shader \n6.2. The Resource Manager\n\n\n290 \n6. Resources and the File System\ncode. Most other resources can reside in main RAM, but diﬀ erent kinds of \nresources might need to reside within diﬀ erent address ranges. For example, a \nresource that is loaded and stays resident for the entire game (LSR resources) \nmight be loaded into one region of memory, while resources that are loaded \nand unloaded frequently might go somewhere else.\nThe design of a game engine’s memory allocation subsystem is usually \nclosely tied to that of its resource manager. Sometimes we will design the re-\nsource manager to take best advantage of the types of memory allocators we \nhave available; or vice-versa, we may design our memory allocators to suit the \nneeds of the resource manager.\nAs we saw in Section 5.2.1.4, one of the primary problems facing any re-\nsource management system is the need to avoid fragmenting memory as re-\nsources are loaded and unloaded. We’ll discuss a few of the more-common \nsolutions to this problem below.\nHeap-Based Resource Allocation\n One approach is to simply ignore memory fragmentation issues and use a \ngeneral-purpose heap allocator to allocate your resources (like the one imple-\nmented by malloc() in C, or the global new operator in C++). This works best \nif your game is only intended to run on personal computers, on operating \nsystems that support advanced virtual memory allocation. On such a system, \nphysical memory will become fragmented, but the operating system’s abil-\nity to map non-contiguous pages of physical RAM into a contiguous virtual \nmemory space helps to mitigate some of the eﬀ ects of fragmentation.\nIf your game is running on a console with limited physical RAM and only \na rudimentary virtual memory manager (or none whatsoever), then fragmen-\ntation will become a problem. In this case, one alternative is to defragment \nyour memory periodically. We saw how to do this in Section 5.2.2.2.\nEvent\nA\nB\nC\nD\nE\nInitial state\n0\n0\n0\n0\n0\nLevel 1 counts incremented\n1\n1\n1\n0\n0\nLevel 1 loads\n(1)\n(1)\n(1)\n0\n0\nLevel 1 plays\n1\n1\n1\n0\n0\nLevel 2 counts incremented\n1\n2\n2\n1\n1\nLevel 1 counts decremented\n0\n1\n1\n1\n1\nLevel 1 unloads, level 2 loads\n(0)\n1\n1\n(1)\n(1)\nLevel 2 plays\n0\n1\n1\n1\n1\nTable 6.2.  Resource usage as two levels load and unload.\n\n\n291 \nStack-Based Resource Allocation\n A stack allocator does not suﬀ er from fragmentation problems, because mem-\nory is allocated contiguously and freed in an order opposite to that in which it \nwas allocated. A stack allocator can be used to load resources if the following \ntwo conditions are met:\nz The game is linear and level-centric (i.e., the player watches a loading \nscreen, then plays a level, then watches another loading screen, then \nplays another level).\nz Each level ﬁ ts into memory in its entirety.\nPresuming that these requirements are satisﬁ ed, we can use a stack alloca-\ntor to load resources as follows: When the game ﬁ rst starts up, the load-and-\nstay-resident (LSR) resources are allocated ﬁ rst. The top of the stack is then \nmarked, so that we can free back to this position later. To load a level, we sim-\nply allocate its resources on the top of the stack. When the level is complete, \nwe can simply set the stack top back to the marker we took earlier, thereby \nfreeing all of the level’s resources in one fell swoop without disturbing the LSR \nresources. This process can be repeated for any number of levels, without ever \nfragmenting memory. Figure 6.3 illustrates how this is accomplished.\n6.2. The Resource Manager\nFigure 6.3.  Loading resources using a stack allocator.\n\n\n292 \n6. Resources and the File System\nA double-ended stack allocator can be used to augment this approach. \nTwo stacks are deﬁ ned within a single large memory block. One grows up \nfrom the bott om of the memory area, while the other grows down from the \ntop. As long as the two stacks never overlap, the stacks can trade memory re-\nsources back and forth naturally—something that wouldn’t be possible if each \nstack resided in its own ﬁ xed-size block.\nOn Hydro Thunder, Midway used a double-ended stack allocator. The low-\ner stack was used for persistent data loads, while the upper was used for tem-\nporary allocations that were freed every frame. Another way a double-ended \nstack allocator can be used is to ping-pong level loads. Such an approach was \nused at Bionic Games Inc. for one of their projects. The basic idea is to load a \ncompressed version of level B into the upper stack, while the currently-active \nlevel A resides (in uncompressed form) in the lower stack. To switch from \nlevel A to level B, we simply free level A’s resources (by clearing the lower \nstack) and then decompress level B from the upper stack into the lower stack. \nDecompression is generally much faster than loading data from disk, so this \napproach eﬀ ectively eliminates the load time that would otherwise be experi-\nenced by the player beween levels.\nPool-Based Resource Allocation\n Another resource allocation technique that is common in game engines that \nsupport streaming is to load resource data in equally-sized chunks. Because \nthe chunks are all the same size, they can be allocated using a pool allocator (see \nSection 5.2.1.2). When resources are later unloaded, the chunks can be freed \nwithout causing fragmentation.\nOf course, a chunk-based allocation approach requires that all resource \ndata be laid out in a manner that permits division into equally-sized chunks. \nWe cannot simply load an arbitrary resource ﬁ le in chunks, because the ﬁ le \nmight contain a contiguous data structure like an array or a very large struct\nthat is larger than a single chunk. For example, if the chunks that contain an \narray are not arranged sequentially in RAM, the continuity of the array will \nbe lost, and array indexing will cease to function properly. This means that all \nresource data must be designed with “chunkiness” in mind. Large contigu-\nous data structures must be avoided in favor of data structures that are either \nsmall enough to ﬁ t within a single chunk or do not require contiguous RAM \nto function properly (e.g., linked lists).\nEach chunk in the pool is typically associated with a particular game lev-\nel. (One simple way to do this is to give each level a linked list of its chunks.) \nThis allows the engine to manage the lifetimes of each chunk appropriately, \neven when multiple levels with diﬀ erent life spans are in memory concur-\n\n\n293 \nrently. For example, when level A is loaded, it might allocate and make use of \nN chunks. Later, level B might allocate an additional M chunks. When level \nA is eventually unloaded, its N chunks are returned to the free pool. If level \nB is still active, its M chunks need to remain in memory. By associating each \nchunk with a speciﬁ c level, the lifetimes of the chunks can be managed easily \nand eﬃ  ciently. This is illustrated in Figure 6.4.\nOne big trade-oﬀ  inherent in a “chunky”  resource allocation scheme is \nwasted space. Unless a resource ﬁ le’s size is an exact multiple of the chunk \nsize, the last chunk in a ﬁ le will not be fully utilized (see Figure 6.5). Choos-\ning a smaller chunk size can help to mitigate this problem, but the smaller the \nchunks, the more onerous the restrictions on the layout of the resource data. \n(As an extreme example, if a chunk size of one byte were selected, then no \ndata structure could be larger than a single byte—clearly an untenable situ-\nation.) A typical chunk size is on the order of a few kilobytes. For example \nat Naughty Dog, we use a chunky resource allocator as part of our resource \nstreaming system, and our chunks are 512 kB in size. You may also want to \nconsider selecting a chunk size that is a multiple of the operating system’s I/O \nbuﬀ er size to maximize eﬃ  ciency when loading individual chunks.\n6.2. The Resource Manager\nFile A\nChunk 1\nFile A\nChunk 2\nFile A\nChunk 3\nFile B\nChunk 1\nFile B\nChunk 2\nFile C\nChunk 1\nFile C\nChunk 2\nFile C\nChunk 3\nFile C\nChunk 4\nFile D\nChunk 1\nFile D\nChunk 2\nFile D\nChunk 3\nFile E\nChunk 1\nFile E\nChunk 2\nFile E\nChunk 3\nFile E\nChunk 4\nFile E\nChunk 5\nFile E\nChunk 6\nLevel X\n(files A, D)\nLevel Y\n(files B, C, E)\nFigure 6.4.  Chunky allocation of resources for levels A and B.\nFile size:\n1638 kB\nUnused:\n410 kB\nChunk 4\nChunk 1\nChunk 2\nChunk 3\nChunk size:\n512 kB each\nFigure 6.5.  The last chunk of a resource ﬁ le is often not fully utilized.\n\n\n294 \n6. Resources and the File System\nResource Chunk Allocators\n One way to limit the eﬀ ects of wasted chunk memory is to set up a special \nmemory allocator that can utilize the unused portions of chunks. As far as I’m \naware, there is no standardized name for this kind of allocator, but we will call \nit a resource chunk allocator for lack of a bett er name.\nA resource chunk allocator is not particularly diﬃ  cult to implement. We \nneed only maintain a linked list of all chunks that contain unused memory, \nalong with the locations and sizes of each free block. We can then allocate \nfrom these free blocks in any way we see ﬁ t. For example, we might manage \nthe linked list of free blocks using a general-purpose heap allocator. Or we \nmight map a small stack allocator onto each free block; whenever a request for \nmemory comes in, we could then scan the free blocks for one whose stack has \nenough free RAM, and then use that stack to satisfy the request.\nUnfortunately, there’s a rather grotesque-looking ﬂ y in our ointment here. \nIf we allocate memory in the unused regions of our resource chunks, what hap-\npens when those chunks are freed? We cannot free part of a chunk—it’s an all \nor nothing proposition. So any memory we allocate within an unused portion \nof a resource chunk will magically disappear when that resource is unloaded.\nA simple solution to this problem is to only use our free-chunk alloca-\ntor for memory requests whose lifetimes match the lifetime of the level with \nwhich a particular chunk is associated. In other words, we should only al-\nlocate memory out of level A’s chunks for data that is associated exclusively \nwith level A and only allocate from B’s chunks memory that is used exclu-\nsively by level B. This requires our resource chunk allocator to manage each \nlevel’s chunks separately. And it requires the users of the chunk allocator to \nspecify which level they are allocating for, so that the correct linked list of free \nblocks can be used to satisfy the request.\nThankfully, most game engines need to allocate memory dynamically \nwhen loading resources, over and above the memory required for the resource \nﬁ les themselves. So a resource chunk allocator can be a fruitful way to reclaim \nchunk memory that would otherwise have been wasted.\nSectioned Resource Files\n Another useful idea that is related to “chunky” resource ﬁ les is the concept \nof ﬁ le sections. A typical resource ﬁ le might contain between one and four sec-\ntions, each of which is divided into one or more chunks for the purposes of \npool allocation as described above. One section might contain data that is des-\ntined for main RAM, while another section might contain video RAM data. \nAnother section could contain temporary data that is needed during the load-\ning process but is discarded once the resource has been completely loaded. Yet \n\n\n295 \nanother section might contain debugging information. This debug data could \nbe loaded when running the game in debug mode, but not loaded at all in \nthe ﬁ nal production build of the game. The Granny SDK’s ﬁ le system (htt p://\nwww.radgametools.com) is an excellent example of how to implement ﬁ le \nsectioning in a simple and ﬂ exible manner.\n6.2.2.8. Composite Resources and Referential Integrity\n Usually a game’s resource database consists of multiple resource ﬁ les, each ﬁ le \ncontaining one or more data objects. These data objects can refer to and depend \nupon one another in arbitrary ways. For example, a mesh data structure might \ncontain a reference to its material, which in turn contains a list of references to \ntextures. Usually cross-references imply dependency (i.e., if resource A refers \nto resource B, then both A and B must be in memory in order for the resources \nto be functional in the game.) In general, a game’s resource database can be \nrepresented by a directed graph of interdependent data objects.\nCross-references between data objects can be internal (a reference between \ntwo objects within a single ﬁ le) or external (a reference to an object in a dif-\nferent ﬁ le). This distinction is important because internal and external cross-\nreferences are oft en implemented diﬀ erently. When visualizing a game’s re-\nsource database, we can draw dott ed lines surrounding individual resource \nﬁ les to make the internal/external distinction clear—any edge of the graph \nthat crosses a dott ed line ﬁ le boundary is an external reference, while edges \nthat do not cross ﬁ le boundaries are internal. This is illustrated in Fiure 6.6.\n6.2. The Resource Manager\nFigure 6.6.  Example of a resource database dependency graph.\n\n\n296 \n6. Resources and the File System\nWe sometimes use the term composite resource to describe a self-suﬃ  cient \ncluster of interdependent resources. For example, a model is a composite re-\nsource consisting of one or more triangle meshes, an optional skeleton, and an \noptional collection of animations. Each mesh is mapped with a material, and \neach material refers to one or more textures. To fully load a composite resource \nlike a 3D model into memory, all of its dependent resources must be loaded \nas well.\n6.2.2.9. Handling Cross-References between Resources\nOne of the more-challenging aspects of implementing a resource manager is \nmanaging the cross-references between resource objects and guaranteeing \nthat referential integrity is maintained. To understand how a resource man-\nager accomplishes this, let’s look at how cross-references are represented in \nmemory, and how they are represented on-disk.\nIn C++, a cross-reference between two data objects is usually implemented \nvia a pointer or a reference. For example, a mesh might contain the data mem-\nber Material* m_pMaterial (a pointer) or Material& m_material (a ref-\nerence) in order to refer to its material. However, pointers are just memory \naddresses—they lose their meaning when taken out of the context of the run-\nning application. In fact, memory addresses can and do change even between \nsubsequent runs of the same application. Clearly when storing data to a disk \nﬁ le, we cannot use pointers to describe inter-object dependencies.\nGUIDs As Cross-References\nOne good approach is to store each cross-reference as a string or hash code \ncontaining the unique id of the referenced object. This implies that every re-\nsource object that might be cross-referenced must have a globally unique identi-\nﬁ er or GUID.\nTo make this kind of cross-reference work, the runtime resource manager \nmaintains a global resource look-up table. Whenever a resource object is load-\ned into memory, a pointer to that object is stored in the table with its GUID as \nthe look-up key. Aft er all resource objects have been loaded into memory and \ntheir entries added to the table, we can make a pass over all of the objects and \nconvert all of their cross-references into pointers, by looking up the address \nof each cross-referenced object in the global resource look-up table via that \nobject’s GUID.\nPointer Fix-Up Tables\nAnother approach that is oft en used when storing data objects into a binary \nﬁ le is to convert the pointers into ﬁ le oﬀ sets. Consider a group of C structs or \nC++ objects that cross-reference each other via pointers. To store this group \n\n\n297 \nof objects into a binary ﬁ le, we need to visit each object once (and only once) \nin an arbitrary order and write each object’s memory image into the ﬁ le se-\nquentially. This has the eﬀ ect of serializing the objects into a contiguous image \nwithin the ﬁ le, even when their memory images are not contiguous in RAM. \nThis is shown in Figure 6.7.\nBecause the objects’ memory images are now contiguous within the ﬁ le, \nwe can determine the oﬀ set of each object’s image relative to the beginning of \nthe ﬁ le. During the process of writing the binary ﬁ le image, we locate every \npointer within every data object, convert each pointer into an oﬀ set, and store \nthose oﬀ sets into the ﬁ le in place of the pointers. We can simply overwrite the \npointers with their oﬀ sets, because the oﬀ sets never require more bits to store \nthan the original pointers. In eﬀ ect, an oﬀ set is the binary ﬁ le equivalent of a \npointer in memory. (Do be aware of the diﬀ erences between your development \nplatform and your target platform. If you write out a memory image on a 64-\nbit Windows machine, its pointers will all be 64 bits wide and the resulting ﬁ le \nwon’t be compatible with a 32-bit console.)\nOf course, we’ll need to convert the oﬀ sets back into pointers when the \nﬁ le is loaded into memory some time later. Such conversions are known as \npointer ﬁ x-ups . When the ﬁ le’s binary image is loaded, the objects contained \nin the image retain their contiguous layout. So it is trivial to convert an oﬀ set \ninto a pointer. We merely add the oﬀ set to the address of the ﬁ le image as a \nwhole. This is demonstrated by the code snippet below, and illustrated in \nFigure 6.8.\n6.2. The Resource Manager\nAddresses:\nOffsets:\nRAM\nBinary File\nObject 1\nObject 2\nObject 3\nObject 4\n0x0\n0x240\n0x4A0\n0x7F0\nObject 1\nObject 4\nObject 2\nObject 3\n0x2A080\n0x2D750\n0x2F110\n0x32EE0\nFigure 6.7. In-memory object images become contiguous when saved into a binary ﬁ le.\n\n\n298 \n6. Resources and the File System\nU8* ConvertOffsetToPointer(U32 objectOffset,\n                           U8* pAddressOfFileImage)\n{\n \nU8* pObject = pAddressOfFileImage + objectOffset;\n \nreturn pObject;\n}\nThe problem we encounter when trying to convert pointers into oﬀ sets, \nand vice-versa, is how to ﬁ nd all of the pointers that require conversion. This \nproblem is usually solved at the time the binary ﬁ le is writt en. The code that \nwrites out the images of the data objects has knowledge of the data types and \nclasses being writt en, so it has knowledge of the locations of all the pointers \nAddresses:\nOffsets:\nRAM\nBinary File\nObject 1\nObject 2\nObject 3\nObject 4\n0x0\n0x240\n0x4A0\n0x7F0\nObject 1\nObject 4\nObject 2\nObject 3\n0x2A080\n0x2D750\n0x2F110\n0x32EE0\n0x32EE0\n0x2F110\n0x2A080\n0x4A0\n0x240\n0x0\nPointers converted \nto offsets; locations \nof pointers stored in \nfix-up table.\nFix-Up Table\n0x200\n0x340\n0x810\nPointers to various \nobjects are present.\n3 pointers\nFigure 6.9.  A pointer ﬁ x-up table.\nAddresses:\nOffsets:\nRAM\nBinary File\nObject 1\nObject 2\nObject 3\nObject 4\n0x0\n0x240\n0x4A0\n0x7F0\nObject 1\nObject 4\nObject 2\nObject 3\n0x30100\n0x30340\n0x305A0\n0x308 F0\nFigure 6.8.  Contiguous resource ﬁ le image, after it has been loaded into RAM.\n\n\n299 \nwithin each object. The locations of the pointers are stored into a simple table \nknown as a pointer ﬁ x-up table. This table is writt en into the binary ﬁ le along \nwith the binary images of all the objects. Later, when the ﬁ le is loaded into \nRAM again, the table can be consulted in order to ﬁ nd and ﬁ x up every point-\ner. The table itself is just a list of oﬀ sets within the ﬁ le—each oﬀ set represents \na single pointer that requires ﬁ xing up. This is illustrated in Figure 6.9.\nStoring C++ Objects as Binary Images: Constructors\n One important step that is easy to overlook when loading C++ objects from a \nbinary ﬁ le is to ensure that the objects’ constructors are called. For example, \nif we load a binary image containing three objects—an instance of class A, an \ninstance of class B, and an instance of class C—then we must make sure that \nthe correct constructor is called on each of these three objects.\nThere are two common solutions to this problem. First, you can simply \ndecide not to support C++ objects in your binary ﬁ les at all. In other words, \nrestrict yourself to plain old data structures (PODS)—i.e., C structs and C++ \nstructs and classes that contain no virtual functions and trivial do-nothing con-\nstructors (See htt p://en.wikipedia.org/wiki/Plain_Old_Data_Structures for a \nmore complete discussion of PODS.)\nSecond, you can save oﬀ  a table containing the oﬀ sets of all non-PODS \nobjects in your binary image along with some indication of which class each \nobject is an instance of. Then, once the binary image has been loaded, you can \niterate through this table, visit each object, and call the appropriate construc-\ntor using placement new syntax (i.e., calling the constructor on a preallocated \nblock of memory). For example, given the oﬀ set to an object within the binary \nimage, we might write:\nvoid* pObject = ConvertOffsetToPointer(objectOffset);\n::new(pObject) ClassName; // placement-new syntax\nwhere ClassName is the class of which the object is an instance.\nHandling External References\nThe two approaches described above work very well when applied to resourc-\nes in which all of the cross-references are internal—i.e., they only reference \nobjects within a single resource ﬁ le. In this simple case, you can load the bi-\nnary image into memory and then apply the pointer ﬁ x-ups to resolve all the \ncross-references. But when cross-references reach out into other resource ﬁ les, \na slightly augmented approach is required.\nTo successfully represent an external cross-reference, we must specify not \nonly the oﬀ set or GUID of the data object in question, but also the path to the \nresource ﬁ le in which the referenced object resides.\n6.2. The Resource Manager\n\n\n300 \n6. Resources and the File System\nThe key to loading a multi-ﬁ le composite resource is to load all of the \ninterdependent ﬁ les ﬁ rst. This can be done by loading one resource ﬁ le and \nthen scanning through its table of cross-references and loading any externally-\nreferenced ﬁ les that have not already been loaded. As we load each data object \ninto RAM, we can add the object’s address to the master look-up table . Once \nall of the interdependent ﬁ les have been loaded and all of the objects are pres-\nent in RAM, we can make a ﬁ nal pass to ﬁ x up all of the pointers using the \nmaster look-up table to convert GUIDs or ﬁ le oﬀ sets into real addresses.\n6.2.2.10. Post-Load Initialization\nIdeally, each and every resource would be completely prepared by our oﬀ -line \ntools, so that it is ready for use the moment it has been loaded into memory. \nPractically speaking, this is not always possible. Many types of resources re-\nquire at least some “massaging” aft er having been loaded, in order to prepare \nthem for use by the engine. In this book, I will use the term post-load initializa-\ntion to refer to any processing of resource data aft er it has been loaded. Other \nengines may use diﬀ erent terminology. (For example, at Naughty Dog we call \nthis logging in a resource.) Most resource managers also support some kind of \ntear-down step prior to a resource’s memory being freed. (At Naughty Dog, \nwe call this logging out a resource.)\nPost-load initialization generally comes in one of two varieties:\nz In some cases, post-load initialization is an unavoidable step. For ex-\nample, the vertices and indices that describe a 3D mesh are loaded into \nmain RAM, but they almost always need to be transferred into video \nRAM. This can only be accomplished at runtime, by creating a Direct X \nvertex buﬀ er or index buﬀ er, locking it, copying or reading the data into \nthe buﬀ er, and then unlocking it.\nz In other cases, the processing done during post-load initialization is \navoidable (i.e., could be moved into the tools), but is done for conve-\nnience or expedience. For example, a programmer might want to add the \ncalculation of accurate arc lengths to our engine’s spline library. Rather \nthan spend the time to modify the tools to generate the arc length data, \nthe programmer might simply calculate it at runtime during post-load \ninitialization. Later, when the calculations are perfected, this code can \nbe moved into the tools, thereby avoiding the cost of doing the calcula-\ntions at runtime.\nClearly, each type of resource has its own unique requirements for post-\nload initialization and tear-down. So resource managers typically permit these \ntwo steps to be conﬁ gurable on a per-resource-type basis. In a non-object-ori-\n\n\n301 \nented language like C, we can envision a look-up table that maps each type of \nresource to a pair of function pointers, one for post-load initialization and one \nfor tear-down. In an object-oriented language like C++, life is even easier—we \ncan make use of polymorphism to permit each class to handle post-load ini-\ntialization and tear-down in a unique way.\nIn C++, post-load initialization could be implemented as a special con-\nstructor, and tear-down could be done in the class’ destructor. However, there \nare some problems with using constructors and destructors for this purpose. \n(For example, constructors cannot be virtual in C++, so it would be diﬃ  cult \nfor a derived class to modify or augment the post-load initialization of its base \nclass.) Many developers prefer to defer post-load initialization and tear-down \nto plain old virtual functions. For example, we might choose to use a pair of \nvirtual functions named something sensible like Init() and Destroy().\nPost-load initialization is closely related to a resource’s memory allocation \nstrategy, because new data is oft en generated by the initialization routine. In \nsome cases, the data generated by the post-load initialization step augments the \ndata loaded from the ﬁ le. (For example, if we are calculating the arc lengths \nof the segments of a Catmull-Rom spline curve aft er it has been loaded, we \nwould probably want to allocate some additional memory in which to store \nthe results.) In other cases, the data generated during post-load initialization \nreplaces the loaded data. (For example, we might allow mesh data in an older \nout-of-date format to be loaded and then automatically converted into the lat-\nest format for backwards compatibility reasons.) In this case, the loaded data \nmay need to be discarded, either partially or in its entirety, aft er the post-load \nstep has generated the new data.\nThe Hydro Thunder engine had a simple but powerful way of handling \nthis. It would permit resources to be loaded in one of two ways: (a) directly \ninto its ﬁ nal resting place in memory, or (b) into a temporary area of memory. \nIn the latt er case, the post-load initialization routine was responsible for copy-\ning the ﬁ nalized data into its ultimate destination; the temporary copy of the \nresource would be discarded aft er post-load initialization was complete. This \nwas very useful for loading resource ﬁ les that contained both relevant and \nirrelevant data. The relevant data would be copied into its ﬁ nal destination \nin memory, while the irrelevant data would be discarded. For example, mesh \ndata in an out-of-date format could be loaded into temporary memory and \nthen converted into the latest format by the post-load initialization routine, \nwithout having to waste any memory keeping the old-format data kicking \naround.\n6.2. The Resource Manager\n\n\n303\n7\nThe Game Loop and \nReal-Time Simulation\nG\names are real-time, dynamic, interactive computer simulations . As such, \ntime plays an incredibly important role in any electronic game. There are \nmany diﬀ erent kinds of time to deal with in a game engine—real time , game \ntime , the local timeline of an animation, the actual CPU cycles spent within \na particular function, and the list goes on. Every engine system might deﬁ ne \nand manipulate time diﬀ erently. We must have a solid understanding of all \nthe ways time can be used in a game. In this chapter, we’ll take a look at how \nreal-time, dynamic simulation soft ware works and explore the common ways \nin which time plays a role in such a simulation.\n7.1. \nThe Rendering Loop\nIn a graphical user interface (GUI), of the sort found on a Windows PC or a \nMacintosh, the majority of the screen’s contents are static. Only a small part \nof any one window is actively changing appearance at any given moment. \nBecause of this, graphical user interfaces have traditionally been drawn on-\nscreen via a technique known as rectangle invalidation , in which only the small \nportions of the screen whose contents have actually changed are re-drawn. \nOlder 2D video games used similar techniques to minimize the number of \npixels that needed to be drawn.\n",
      "page_number": 308,
      "chapter_number": 16,
      "summary": "This might be necessary if no standardized format provides all of \nthe information needed by the engine Key topics include resources, memory, and data.",
      "keywords": [
        "Resource",
        "resource manager",
        "memory",
        "resource chunk allocator",
        "resource data",
        "chunk",
        "data",
        "Resource Chunk",
        "object",
        "level",
        "File",
        "loaded",
        "game",
        "File System",
        "resource GUID"
      ],
      "concepts": [
        "resources",
        "memory",
        "data",
        "level",
        "chunks",
        "load",
        "game",
        "objects",
        "ers",
        "allocation"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 1",
          "chapter": 10,
          "title": "Segment 10 (pages 86-98)",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 21,
          "title": "Segment 21 (pages 193-203)",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 9,
          "title": "Segment 9 (pages 78-85)",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 15,
          "title": "Segment 15 (pages 135-143)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 326-347)",
      "start_page": 326,
      "end_page": 347,
      "detection_method": "synthetic",
      "content": "304 \n7. The Game Loop and Real-Time Simulation\nReal-time 3D computer graphics are implemented in an entirely diﬀ erent \nway. As the camera moves about in a 3D scene, the entire contents of the screen \nor window change continually, so the concept of invalid rectangles no longer \napplies. Instead, an illusion of motion and interactivity is produced in much \nthe same way that a movie produces it—by presenting the viewer with a se-\nries of still images in rapid succession.\nObviously, producing a rapid succession of still images on-screen requires \na loop. In a real-time rendering application, this is sometimes known as the \nrender loop . At its simplest, a rendering loop is structured as follows:\nwhile (!quit)\n{\n \n// Update the camera transform based on interactive   \n \n// inputs or by following a predefined path.\nupdateCamera();\n \n// Update positions, orientations and any other   \n \n \n// relevant visual state of any dynamic elements \n \n// in the scene.\nupdateSceneElements();\n \n// Render a still frame into an off-screen frame   \n \n \n// buffer known as the \"back buffer\".\nrenderScene();\n \n// Swap the back buffer with the front buffer, making  \n \n// the most-recently-rendered image visible \n \n// on-screen. (Or, in windowed mode, copy (blit) the   \n \n// back buffer’s contents to the front buffer.\nswapBuffers();\n}\n7.2. The Game Loop\n A game is composed of many interacting subsystems, including device I/O, \nrendering, animation, collision detection and resolution, optional rigid body \ndynamics simulation, multiplayer networking, audio, and the list goes on. \nMost game engine subsystems require periodic servicing while the game is \nrunning. However, the rate at which these subsystems need to be serviced var-\nies from subsystem to subsystem. Animation typically needs to be updated \nat a rate of 30 or 60 Hz, in synchronization with the rendering subsystem. \nHowever, a dynamics simulation may actually require more frequent updates \n\n\n305 \n7.2. The Game Loop\n(e.g., 120 Hz). Higher-level systems, like AI , might only need to be serviced \nonce or twice per second, and they needn’t necessarily be synchronized with \nthe rendering loop at all.\nThere are a number of ways to implement the periodic updating of our \ngame engine subsystems. We’ll explore some of the possible architectures in a \nmoment. But for the time being, let’s stick with the simplest way to update our \nengine’s subsystems—using a single loop to update everything. Such a loop \nis oft en called the game loop, because it is the master loop that services every \nsubsystem in the engine.\n7.2.1. \nA Simple Example: Pong\nPong is a well-known genre of table tennis video games that got its start in \n1958, in the form of an analog computer game called Tennis for Two, created \nby William A. Higinbotham at the Brookhaven National Laboratory and dis-\nplayed on an oscilloscope. The genre is best known by its later incarnations on \ndigital computers—the Magnavox Oddysey game Table Tennis and the Atari \narcade game Pong.\nIn pong, a ball bounces back and forth between two movable vertical pad-\ndles and two ﬁ xed horizontal walls. The human players control the positions \nof the paddles via control wheels. (Modern re-implementations allow control \nvia a joystick, the keyboard, or some other human interface device.) If the ball \npasses by a paddle without striking it, the other team wins the point and the \nball is reset for a new round of play.\nThe following pseudocode demonstrates what the game loop of a pong \ngame might look like at its core :\nvoid main() // Pong\n{\ninitGame();\nwhile (true) // game loop\n {\nreadHumanInterfaceDevices();\n  if \n(quitButtonPressed())\n  {\n \n \n \nbreak; // exit the game loop\n  }\nmovePaddles();\nmoveBall();\n\n\n306 \n7. The Game Loop and Real-Time Simulation\ncollideAndBounceBall();\n  if \n(ballImpactedSide(LEFT_PLAYER))\n  {\nincremenentScore(RIGHT_PLAYER);\nresetBall();\n  }\n \n \nelse if (ballImpactedSide(RIGHT_PLAYER))\n  {\nincrementScore(LEFT_PLAYER);\nresetBall();\n  }\nrenderPlayfield();\n }\n}\nClearly this example is somewhat contrived. The original pong games were \ncertainly not implemented by redrawing the entire screen at a rate of 30 \nframes per second. Back then, CPUs were so slow that they could barely mus-\nter the power to draw two lines for the paddles and a box for the ball in real \ntime. Specialized 2D sprite hardware was oft en used to draw moving objects \non-screen. However, we’re only interested in the concepts here, not the imple-\nmentation details of the original Pong.\nAs you can see, when the game ﬁ rst runs, it calls initGame() to do \nwhatever set-up might be required by the graphics system, human I/O de-\nvices, audio system, etc. Then the main game loop is entered. The statement \nwhile (true) tells us that the loop will continue forever, unless interrupted \ninternally. The ﬁ rst thing we do inside the loop is to read the human interface \ndevice(s). We check to see whether either human player pressed the “quit” \nbutt on—if so, we exit the game via a break statement. Next, the positions of \nthe paddles are adjusted slightly upward or downward in movePaddles(), \nbased on the current deﬂ ection of the control wheels, joysticks, or other I/O \ndevices. The function moveBall() adds the ball’s current velocity vector to its \nposition in order to ﬁ nd its new position next frame. In collideAndBounce-\nBall(), this position is then checked for collisions against both the ﬁ xed hori-\nzontal walls and the paddles. If collisions are detected, the ball’s position is re-\ncalculated to account for any bounce. We also note whether the ball impacted \neither the left  or right edge of the screen. This means that it missed one of the \npaddles, in which case we increment the other player’s score and reset the ball \nfor the next round. Finally, renderPlayﬁeld() draws the entire contents of \nthe screen.\n\n\n307 \n7.3. Game Loop Architectural Styles\n7.3. Game Loop Architectural Styles\nGame loops can be implemented in a number of diﬀ erent ways—but at their \ncore, they usually boil down to one or more simple loops, with various embel-\nlishments. We’ll explore a few of the more common architectures below.\n7.3.1. \nWindows Message Pumps\nOn a Windows platform, games need to service messages from the Windows \noperating system in addition to servicing the various subsystems in the game \nengine itself. Windows games therefore contain a chunk of code known as a \nmessage pump . The basic idea is to service Windows messages whenever they \narrive and to service the game engine only when no Windows messages are \npending. A message pump typically looks something like this:\nwhile (true)\n{\n \n// Service any and all pending Windows messages.\n \nMSG msg;\nwhile (PeekMessage(&msg, NULL, 0, 0) > 0)\n {\nTranslateMessage(&msg);\nDispatchMessage(&msg);\n }\n \n// No more Windows messages to process – run one   \n \n \n// iteration of our \"real\" game loop.\nRunOneIterationOfGameLoop();\n}\nOne of the side-eﬀ ects of implementing the game loop like this is that Win-\ndows messages take precedence over rendering and simulating the game. As \na result, the game will temporarily freeze whenever you resize or drag the \ngame’s window around on the desktop.\n7.3.2. Callback-Driven Frameworks\nMost game engine subsystems and third-party game middleware packages \nare structured as libraries . A library is a suite of functions and/or classes that \n\n\n308 \n7. The Game Loop and Real-Time Simulation\ncan be called in any way the application programmer sees ﬁ t. Libraries pro-\nvide maximum ﬂ exibility to the programmer. But libraries are sometimes dif-\nﬁ cult to use, because the programmer must understand how to properly use \nthe functions and classes they provide.\nIn contrast, some game engines and game middleware packages are \nstructured as frameworks . A framework is a partially-constructed applica-\ntion—the programmer completes the application by providing custom im-\nplementations of missing functionality within the framework (or overriding \nits default behavior). But he or she has litt le or no control over the overall \nﬂ ow of control within the application, because it is controlled by the frame-\nwork.\nIn a framework-based rendering engine or game engine, the main game \nloop has been writt en for us, but it is largely empty. The game programmer can \nwrite callback functions in order to “ﬁ ll in” the missing details. The Ogre3D \nrendering engine is an example of a library that has been wrapped in a frame-\nwork. At the lowest level, Ogre provides functions that can be called directly \nby a game engine programmer. However, Ogre also provides a framework that \nencapsulates knowledge of how to use the low-level Ogre library eﬀ ectively. If \nthe programmer chooses to use the Ogre framework, he or she derives a class \nfrom Ogre::FrameListener and overrides two virtual functions: frame-\nStarted() and frameEnded(). As you might guess, these functions are called \nbefore and aft er the main 3D scene has been rendered by Ogre, respectively. \nThe Ogre framework’s implementation of its internal game loop looks some-\nthing like the following pseudocode. (See Ogre::Root::renderOneFrame()\nin OgreRoot.cpp for the actual source code.)\nwhile (true)\n{\n \nfor (each frameListener)\n {\n  frameListener.\nframeStarted();\n }\nrenderCurrentScene();\n \nfor (each frameListener)\n {\n  frameListener.\nframeEnded();\n }\nfinalizeSceneAndSwapBuffers();\n}\n\n\n309 \nA particular game’s frame listener implementation might look something like \nthis.\nclass GameFrameListener : public Ogre::FrameListener\n{\npublic:\n \nvirtual void frameStarted(const FrameEvent& event)\n {\n \n  \n// Do things that must happen before the 3D scene  \n \n  \n// is rendered (i.e., service all game engine   \n \n \n  // subsystems).\n \n  pollJoypad(event);\n \n  updatePlayerControls(event);\n \n  updateDynamicsSimulation(event);\n \n  resolveCollisions(event);\n \n  updateCamera(event);\n \n  // etc.\n }\n \nvirtual void frameEnded(const FrameEvent& event)\n {\n \n  \n// Do things that must happen after the 3D scene \n \n  \n// has been rendered.\n \n  drawHud(event);\n \n  // etc.\n }\n};\n7.3.3. Event-Based Updating\nIn games, an event is any interesting change in the state of the game or its \nenvironment. Some examples include: the human player pressing a butt on \non the joypad, an explosion going oﬀ , an enemy character spott ing the player, \nand the list goes on. Most game engines have an event system, which permits \nvarious engine subsystems to register interest in particular kinds of events \nand to respond to those events when they occur (see Section 14.7 for details). \nA game’s event system is usually very similar to the event/messaging system \nunderlying virtually all graphical user interfaces (for example, Microsoft  Win-\ndows’ window messages, the event handling system in Java’s AWT, or the \nservices provided by C#’s delegate and event keywords).\nSome game engines leverage their event system in order to implement \nthe periodic servicing of some or all of their subsystems. For this to work, the \nevent system must permit events to be posted into the future—that is, to be \nqueued for later delivery. A game engine can then implement periodic updat-\n7.3. Game Loop Architectural Styles\n\n\n310 \n7. The Game Loop and Real-Time Simulation\ning by simply posting an event. In the event handler, the code can perform \nwhatever periodic servicing is required. It can then post a new event 1/30 or \n1/60 of a second into the future, thus continuing the periodic servicing for as \nlong as it is required.\n7.4. Abstract Timelines\nIn game programming, it can be extremely useful to think in terms of abstract \ntimelines . A timeline is a continuous, one-dimensional axis whose origin (t = 0) \ncan lie at any arbitrary location relative to other timelines in the system. A \ntimeline can be implemented via a simple clock variable that stores absolute \ntime values in either integer or ﬂ oating-point format.\n7.4.1. \nReal Time\n We can think of times measured directly via the CPU’s high-resolution timer \nregister (see Section 7.5.3) as lying on what we’ll call the real timeline. The ori-\ngin of this timeline is deﬁ ned to coincide with the moment the CPU was last \npowered on or reset. It measures times in units of CPU cycles (or some mul-\ntiple thereof), although these time values can be easily converted into units of \nseconds by multiplying them by the frequency of the high-resolution timer on \nthe current CPU.\n7.4.2. Game Time\n We needn’t limit ourselves to working with the real timeline exclusively. We \ncan deﬁ ne as many other timeline(s) as we need, in order to solve the prob-\nlems at hand. For example, we can deﬁ ne a game timeline that is technically \nindependent of real time. Under normal circumstances, game time coincides \nwith real time. If we wish to pause the game, we can simply stop updating \nthe game timeline temporarily. If we want our game to go into slow-motion, \nwe can update the game clock more slowly than the real-time clock. All sorts \nof eﬀ ects can be achieved by scaling and warping one timeline relative to an-\nother.\nPausing or slowing down the game clock is also a highly useful debug-\nging tool. To track down a visual anomaly, a developer can pause game time \nin order to freeze the action. Meanwhile, the rendering engine and debug ﬂ y-\nthrough camera can continue to run, as long as they are governed by a dif-\nferent clock (either the real-time clock, or a separate camera clock). This allows \nthe developer to ﬂ y the camera around the game world to inspect it from \nany angle desired. We can even support single-stepping the game clock, by \n\n\n311 \n7.4. Abstract Timelines\nadvancing the game clock by one target frame interval (e.g., 1/30 of a second) \neach time a “single-step” butt on is pressed on the joypad or keyboard while \nthe game is in a paused state.\nWhen using the approach described above, it’s important to realize that \nthe game loop is still running when the game is paused—only the game clock \nhas stopped. Single-stepping the game by adding 1/30 of a second to a paused \ngame clock is not the same thing as sett ing a break point in your main loop, \nand then hitt ing the F5 key repeatedly to run one iteration of the loop at a \ntime. Both kinds of single-stepping can be useful for tracking down diﬀ erent \nkinds of problems. We just need to keep the diﬀ erences between these ap-\nproaches in mind.\n7.4.3. Local and Global Timelines\nWe can envision all sorts of other timelines. For example, an animation clip or \naudio clip might have a local timeline, with its origin (t = 0) deﬁ ned to coincide \nwith the start of the clip. The local timeline measures how time progressed \nwhen the clip was originally authored or recorded. When the clip is played \nback in-game, we needn’t play it at the original rate. We might want to speed \nup an animation, or slow down an audio sample. We can even play an anima-\ntion backwards by running its local clock in reverse.\nAny one of these eﬀ ects can be visualized as a mapping between the lo-\ncal timeline and a global timeline, such as real time or game time. To play an \nanimation clip back at its originally-authored speed, we simply map the start \nof the animation’s local timeline (t = 0) onto the desired start time \nstart\n(\n)\nτ =τ\nalong the global timeline. This is shown in Figure 7.1.\nTo play an animation clip back at half speed, we can imagine scaling the \nlocal timeline to twice its original size prior to mapping it onto the global \ntimeline. To accomplish this, we simply keep track of a time scale factor or \nplayback rate R, in addition to the clip’s global start time \nstart.\nτ\n  This is illus-\ntrated in Figure 7.2. A clip can even be played in reverse, by using a negative \ntime scale (R < 0) as shown in Figure 7.3.\nClip A\nt = 0 sec\n5 sec\nτstart = 102 sec\nτ = 105 sec\n110 sec\nFigure 7.1. Playing an animation clip can be envisioned as mapping its local timeline onto the \nglobal game timeline.\n\n\n312 \n7. The Game Loop and Real-Time Simulation\n7.5. Measuring and Dealing with Time\nIn this section, we’ll investigate some of the subtle and not-so-subtle distinc-\ntions between diﬀ erent kinds of timelines and clocks and see how they are \nimplemented in real game engines.\n7.5.1. \nFrame Rate and Time Deltas\nThe frame rate of a real-time game describes how rapidly the sequence of still \n3D frames is presented to the viewer. The unit of Hertz (Hz), deﬁ ned as the \nnumber of cycles per second, can be used to describe the rate of any periodic \nprocess. In games and ﬁ lm, frame rate is typically measured in frames per sec-\nond (FPS), which is the same thing as Hertz for all intents and purposes. Films \ntraditionally run at 24 FPS. Games in North America and Japan are typically \nrendered at 30 or 60 FPS, because this is the natural refresh rate of the NTSC \ncolor television standard used in these regions. In Europe and most of the rest \nClip A\nτstart = 102 sec\nτ = 105 sec\nClip A\nR = 2\n(scale t by 1/R = 0.5)\nt = 0 sec\nt = 5 sec\nt = 0 sec\n5 sec\nFigure 7.2. Animation play-back speed can be controlled by simply scaling the local time line \nprior to mapping it onto the global time line.\nt = 5 sec\n0 sec\nτstart = 102 sec\nτ = 105 sec\n110 sec\n A pilC\nClip A\nt = 0 sec\n5 sec\nR = –1\n(ﬂip t)\nFigure 7.3.  Playing an animation in reverse is like mapping the clip to the global time line with \na time scale of R = –1.\n\n\n313 \nof the world, games update at 50 FPS, because this is the natural refresh rate \nof a PAL or SECAM color television signal.\nThe amount of time that elapses between frames is known as the frame \ntime, time delta , or delta time. This last term is commonplace because the dura-\ntion between frames is oft en represented mathematically by the symbol Δt. \n(Technically speaking, Δt should really be called the frame period, since it is \nthe inverse of the frame frequency: T = 1/f. But game programmers hardly ever \nuse the term “period” in this context.) If a game is being rendered at exactly \n30 FPS, then its delta time is 1/30 of a second, or 33.3 ms (milliseconds). At \n60 FPS, the delta time is half as big, 1/60 of a second or 16.6 ms. To really know \nhow much time has elapsed during one iteration of the game loop, we need to \nmeasure it. We’ll see how this is done below.\nWe should note here that milliseconds are a common unit of time mea-\nsurement in games. For example, we might say that animation is taking 4 ms, \nwhich implies that it occupies about 12% of the entire frame (4 / 33.3 ≈ 0.12). \nOther common units include seconds and machine cycles. We’ll discuss time \nunits and clock variables in more depth below.\n7.5.2. From Frame Rate to Speed\nLet’s imagine that we want to make a spaceship ﬂ y through our game world at \na constant speed of 40 meters per second (or in a 2D game, we might specify \nthis as 40 pixels per second!) One simple way to accomplish this is to multiply \nthe ship’s speed v (measured in meters per second) by the duration of one \nframe Δt (measured in seconds), yielding a change in position Δx = v Δt (which \nis measured in meters per frame). This position delta can then be added to the \nship’s current position x1 , in order to ﬁ nd its position next frame: x2 = x1 + Δx \n= x1 + v Δt.\nThis is actually a simple form of numerical integration known as the explicit \nEuler method (see Section 12.4.4). It works well as long as the speeds of our \nobjects are roughly constant. To handle variable speeds, we need to resort to \nsomewhat more-complex integration methods. But all numerical integration \ntechniques make use of the elapsed frame time Δt in one way or another. So \nit is safe to say that the perceived speeds of the objects in a game are dependent \nupon the frame duration, Δt. Hence a central problem in game programming \nis to determine a suitable value for Δt. In the sections that follow, we’ll discuss \nvarious ways of doing this.\n7.5.2.1. Old-School CPU-Dependent Games\nIn many early video games, no att empt was made to measure how much real \ntime had elapsed during the game loop. The programmers would essentially \n7.5. Measuring and Dealing with Time\n\n\n314 \n7. The Game Loop and Real-Time Simulation\nignore Δt altogether and instead specify the speeds of objects directly in terms \nof meters (or pixels, or some other distance unit) per frame. In other words, \nthey were, perhaps unwitt ingly, specifying object speeds in terms of Δx = v Δt, \ninstead of in terms of v.\nThe net eﬀ ect of this simplistic approach was that the perceived speeds of \nthe objects in these games were entirely dependent upon the frame rate that \nthe game was actually achieving on a particular piece of hardware. If this kind \nof game were to be run on a computer with a faster CPU than the machine for \nwhich it was originally writt en, the game would appear to be running in fast \nforward. For this reason, I’ll call these games CPU-dependent games .\nSome older PCs provided a “Turbo” butt on to support these kinds of \ngames. When the Turbo butt on was pressed, the PC would run at its fastest \nspeed, but CPU-dependent games would run in fast forward. When the Turbo \nbutt on was not pressed, the PC would mimic the processor speed of an older \ngeneration of PCs, allowing CPU-dependent games writt en for those PCs to \nrun properly.\n7.5.2.2. Updating Based on Elapsed Time\n To make our games CPU-independent, we must measure Δt in some way, rath-\ner than simply ignoring it. Doing this is quite straightforward. We simply read \nthe value of the CPU’s high resolution timer twice—once at the beginning of \nthe frame and once at the end. Then we subtract, producing an accurate mea-\nsure of Δt for the frame that has just passed. This delta is then made available \nto all engine subsystems that need it, either by passing it to every function that \nwe call from within the game loop or by storing it in a global variable or en-\ncapsulating it within a singleton class of some kind. (We’ll describe the CPU’s \nhigh resolution timer in more detail Section 7.5.3.)\nThe approach outlined above is used by many game engines. In fact, I am \ntempted to go out on a limb and say that most game engines use it. However, \nthere is one big problem with this technique: We are using the measured value \nof Δt taken during frame k as an estimate of the duration of the upcoming frame \n(k + 1). This isn’t necessarily very accurate. (As they say in investing, “past per-\nformance is not a guarantee of future results.”) Something might happen next \nframe that causes it to take much more time (or much less) than the current \nframe. We call such an event a frame-rate spike.\nUsing last frame’s delta as an estimate of the upcoming frame can have \nsome very real detrimental eﬀ ects. For example, if we’re not careful it can put \nthe game into a “viscious cycle” of poor frame times. Let’s assume that our \nphysics simulation is most stable when updated once every 33.3 ms (i.e., at \n30 Hz). If we get one bad frame, taking say 57 ms, then we might make the \n\n\n315 \nmistake of stepping the physics system twice on the next frame, presumably to \n“cover” the 57 ms that has passed. Those two steps take roughly twice as long \nto complete as a regular step, causing the next frame to be at least as bad as \nthis one was, and possibly worse. This only serves to exacerbate and prolong \nthe problem.\n7.5.2.3. Using a Running Average\nIt is true that game loops tend to have at least some frame-to-frame coher-\nency . If the camera is pointed down a hallway containing lots of expensive-to-\ndraw objects on one frame, there’s a good chance it will still be pointed down \nthat hallway on the next. Therefore, one reasonable approach is to average the \nframe-time measurements over a small number of frames and use that as the \nnext frame’s estimate of Δt . This allows the game to adapt to varying frame \nrate, while soft ening the eﬀ ects of momentary performance spikes. The longer \nthe averaging interval, the less responsive the game will be to varying frame \nrate, but spikes will have less of an impact as well.\n7.5.2.4. Governing the Frame Rate\nWe can avoid the inaccuracy of using last frame’s Δt as an estimate of this \nframe’s duration altogether, by ﬂ ipping the problem on its head. Rather than \ntrying to guess at what next frame’s duration will be, we can instead att empt \nto guarantee that every frame’s duration will be exactly 33.3 ms (or 16.6 ms if \nwe’re running at 60 FPS). To do this, we measure the duration of the current \nframe as before. If the measured duration is less than the ideal frame time, we \nsimply put the main thread to sleep until the target frame time has elapsed. \nIf the measured duration is more than the ideal frame time, we must “take \nour lumps” and wait for one more whole frame time to elapse. This is called \nframe-rate governing .\nClearly this approach only works when your game’s frame rate is reason-\nably close to your target frame rate on average. If your game is ping-ponging \nbetween 30 FPS and 15 FPS due to frequent “slow” frames, then the game’s \nquality can degrade signiﬁ cantly. As such, it’s still a good idea to design all \nengine systems so that they are capable of dealing with arbitrary frame dura-\ntions. During development, you can leave the engine in “variable frame rate” \nmode, and everything will work as expected. Later on, when the game is get-\nting closer to achieving its target frame rate consistently, we can switch on \nframe-rate governing and start to reap its beneﬁ ts.\nKeeping the frame rate consistent can be important for a number of rea-\nsons. Some engine systems, such as the numerical integrators used in a phys-\nics simulation, operate best when updated at a constant rate. A consistent \n7.5. Measuring and Dealing with Time\n\n\n316 \n7. The Game Loop and Real-Time Simulation\nframe rate also looks bett er, and as we’ll see in the next section, it can be used \nto avoid the tearing that can occur when the video buﬀ er is updated at a rate \nthat doesn’t match the refresh rate of the monitor.\nIn addition, when elapsed frame times are consistent, features like record \nand play back become a lot more reliable. As its name implies, the record and \nplay back feature allows a player’s gameplay experience to be recorded and \nlater played back in exactly the same way. This can be a fun game feature, and \nit’s also a valuable testing and debugging tool. For example, diﬃ  cult-to-ﬁ nd \nbugs can be reproduced by simply playing back a recorded game that dem-\nonstrates the bug.\nTo implement record and play back, we make note of every relevant event \nthat occurs during gameplay, saving each one in a list along with an accurate \ntime stamp. The list of events can then be replayed with exactly the same tim-\ning, using the same initial conditions, and an identical initial random seed. \nIn theory, doing this should produce a gameplay experience that is indis-\ntinguishable from the original playthrough. However, if the frame rate isn’t \nconsistent, things may not happen in exactly the same order. This can cause \n“drift ,” and prett y soon your AI characters are ﬂ anking when they should \nhave fallen back.\n7.5.2.5. The Vertical Blanking Interval\nA visual anomaly known as tearing occurs when the back buﬀ er is swapped \nwith the front buﬀ er while the electron gun in the CRT monitor is only part \nway through its scan. When tearing occurs, the top portion of the screen shows \nthe old image, while the bott om portion shows the new one. To avoid tearing, \nmany rendering engines wait for the vertical blanking interval of the monitor \n(the time during which the electron gun is being reset to the top-left  corner of \nthe screen) before swapping buﬀ ers.\nWaiting for the v-blank interval is another form of frame-rate governing . It \neﬀ ectively clamps the frame rate of the main game loop to a multiple of the \nscreen’s refresh rate. For example, on an NTSC monitor that refreshes at a rate \nof 60 Hz, the game’s real update rate is eﬀ ectively quantized to a multiple \nof 1/60 of a second. If more than 1/60 of a second elapses between frames, \nwe must wait until the next v-blank interval, which means waiting 2/60 of a \nsecond (30 FPS). If we miss two v-blanks, then we must wait a total of 3/60 of \na second (20 FPS), and so on. Also, be careful not to make assumptions about \nthe frame rate of your game, even when it is synchronized to the v-blank in-\nterval; remember that the PAL and SECAM standards are based around an \nupdate rate of 50 Hz, not 60 Hz.\n\n\n317 \n7.5.3. Measuring Real Time with a High-Resolution Timer\n We’ve talked a lot about measuring the amount of real “wall clock” time that \nelapses during each frame. In this section, we’ll investigate how such timing \nmeasurements are made in detail.\nMost operating systems provide a function for querying the system time, \nsuch as the standard C library function time(). However, such functions are \nnot suitable for measuring elapsed times in a real-time game, because they \ndo not provide suﬃ  cient resolution. For example, time() returns an integer \nrepresenting the number of seconds since midnight, January 1, 1970, so its reso-\nlution is one second—far too coarse, considering that a frame takes only tens \nof milliseconds to execute.\nAll modern CPUs contain a high-resolution timer , which is usually imple-\nmented as a hardware register that counts the number of CPU cycles (or some \nmultiple thereof) that have elapsed since the last time the processor was pow-\nered on or reset. This is the timer that we should use when measuring elapsed \ntime in a game, because its resolution is usually on the order of the duration \nof a few CPU cycles. For example, on a 3 GHz Pentium processor, the high-\nresolution timer increments once per CPU cycle, or 3 billion times per second. \nHence the resolution of the high-res timer is 1 / 3 billion = 3.33 × 10–10 seconds = \n0.333 ns (one-third of a nanosecond). This is more than enough resolution for \nall of our time-measurement needs in a game.\nDiﬀ erent microprocessors and diﬀ erent operating systems provide dif-\nferent ways to query the high-resolution timer. On a Pentium, a special instruc-\ntion called rdtsc (read time-stamp counter) can be used, although the Win32 \nAPI wraps this facility in a pair of functions: QueryPerformanceCounter()\nreads the 64-bit counter register and QueryPerformanceFrequency()\nreturns the number of counter increments per second for the current CPU. \nOn a PowerPC architecture, such as the chips found in the Xbox 360 and \nPLAYSTATION 3, the instruction mftb (move from time base register) can \nbe used to read the two 32-bit time base registers, while on other PowerPC \narchitectures, the instruction mfspr (move from special-purpose register) is \nused instead.\nA CPU’s high-resolution timer register is 64 bits wide on most processors, \nto ensure that it won’t wrap too oft en. The largest possible value of a 64-bit un-\nsigned integer is 0xFFFFFFFFFFFFFFFF ≈ 1.8 × 1019 clock ticks. So, on a 3 GHz \nPentium processor that updates its high-res timer once per CPU cycle, the \nregister’s value will wrap back to zero once every 195 years or so—deﬁ nitely \nnot a situation we need to lose too much sleep over. In contrast, a 32-bit integer \nclock will wrap aft er only about 1.4 seconds at 3 GHz.\n7.5. Measuring and Dealing with Time\n\n\n318 \n7. The Game Loop and Real-Time Simulation\n7.5.3.1. \nHigh-Resolution Clock Drift\n Be aware that even timing measurements taken via a high-resolution timer can \nbe inaccurate in certain circumstances. For example, on some multicore pro-\ncessors , the high-resolution timers are independent on each core, and they can \n(and do) drift  apart. If you try to compare absolute timer readings taken on dif-\nferent cores to one another, you might end up with some strange results—even \nnegative time deltas. Be sure to keep an eye out for these kinds of problems.\n7.5.4. Time Units and Clock Variables\nWhenever we measure or specify time durations in a game, we have two \nchoices to make:\nWhat \n1. \ntime units should be used? Do we want to store our times in \nseconds, or milliseconds, or machine cycles… or in some other unit?\nWhat \n2. \ndata type should be used to store time measurements? Should we \nemploy a 64-bit integer, or a 32-bit integer, or a 32-bit ﬂ oating point \nvariable ?\nThe answers to these questions depend on the intended purpose of a given \nmeasurement. This gives rise to two more questions: How much precision \ndo we need? And what range of magnitudes do we expect to be able to rep-\nresent?\n7.5.4.1. 64-Bit Integer Clocks\nWe’ve already seen that a 64-bit unsigned integer clock, measured in machine \ncycles, supports both an extremely high precision (a single cycle is 0.333 ns in \nduration on a 3 GHz CPU) and a broad range of magnitudes (a 64-bit clock \nwraps once roughly every 195 years at 3 GHz). So this is the most ﬂ exible time \nrepresentation, presuming you can aﬀ ord 64 bits worth of storage.\n7.5.4.2. 32-Bit Integer Clocks\nWhen measuring relatively short durations with high precision, we can turn \nto a 32-bit integer clock, measured in machine cycles. For eample, to proﬁ le \nthe performance of a block of code, we might do something like this:\n// Grab a time snapshot.\nU64 tBegin = readHiResTimer();\n// This is the block of code whose performance we wish \n// to measure.\ndoSomething();\ndoSomethingElse();\nnowReallyDoSomething();\n\n\n319 \n// Measure the duration.\nU64 tEnd = readHiResTimer();\nU32 dtCycles = static_cast<U32>(tEnd – tBegin);\n// Now use or cache the value of dtCycles...\nNotice that we still store the raw time measurements in 64-bit integer \nvariables. Only the time delta dt is stored in a 32-bit variable. This circum-\nvents potential problems with wrapping at the 32-bit boundary. For example, \nif tBegin == 0x12345678FFFFFFB7 and tEnd == 0x1234567900000039, \nthen we would measure a negative time delta if we were to truncate the indi-\nvidual time measurements to 32 bits each prior to subtracting them.\n7.5.4.3. 32-Bit Floating-Point Clocks\n Another common approach is to store relatively small time deltas in ﬂ oating- \npoint format, measured in units of seconds. To do this, we simply multiply a \nduration measured in CPU cycles by the CPU’s clock frequency, which is in \ncycles per second. For example:\n// Start off assuming an ideal frame time (30 FPS).\nF32 dtSeconds = 1.0f / 30.0f;\n// Prime the pump by reading the current time.\nU64 tBegin = readHiResTimer();\nwhile (true) // main game loop\n{\nrunOneIterationOfGameLoop(dtSeconds);\n \n// Read the current time again, and calculate the  \n \n \n// delta.\n U64 \ntEnd = readHiResTimer();\ndtSeconds = (F32)(tEnd – tBegin)\n \n          * (F32)getHiResTimerFrequency();\n \n// Use tEnd as the new tBegin for next frame.\ntBegin = tEnd;\n}\nNotice once again that we must be careful to subtract the two 64-bit time \nmeasurements before converting them into ﬂ oating point format. This ensures \nthat we don’t store too large a magnitude into a 32-bit ﬂ oating point variable.\n7.5.4.4. Limitations of Floating Point Clocks\nRecall that in a 32-bit IEEE ﬂ oat, the 23 bits of the mantissa are dynamically \ndistributed between the whole and fractional parts of the value, by way \n7.5. Measuring and Dealing with Time\n\n\n320 \n7. The Game Loop and Real-Time Simulation\nof the exponent (see Section 3.2.1.4). Small magnitudes require only a few \nbits, leaving plenty of bits of precision for the fraction. But once the magni-\ntude of our clock grows too large, its whole part eats up more bits, leaving \nfewer bits for the fraction. Eventually, even the least-signiﬁ cant bits of the \nwhole part become implicit zeros. This means that we must be cautious \nwhen storing long durations in a ﬂ oating-point clock variable. If we keep \ntrack of the amount of time that has elapsed since the game was started, a \nﬂ oating-point clock will eventually become inaccurate to the point of being \nunusable.\nFloating-point clocks are usually only used to store relatively short time \ndeltas, measuring at most a few minutes, and more oft en just a single frame \nor less. If an absolute-valued ﬂ oating-point clock is used in a game, you will \nneed to reset the clock to zero periodically, to avoid accumulation of large \nmagnitudes.\n7.5.4.5. Other Time Units\nSome game engines allow timing values to be speciﬁ ed in a game-deﬁ ned \nunit that is ﬁ ne-grained enough to permit a 32-bit integer format to be used, \nprecise enough to be useful for a wide range of applications within the en-\ngine, and yet large enough that the 32-bit clock won’t wrap too oft en. One \ncommon choice is a 1/300 second time unit. This works well because (a) it is \nﬁ ne-grained enough for many purposes, (b) it only wraps once every 165.7 \ndays, and (c) it is an even multiple of both NTSC and PAL refresh rates. A \n60 FPS frame would be 5 such units in duration, while a 50 FPS frame would \nbe 6 units in duration.\nObviously a 1/300 second time unit is not precise enough to handle subtle \neﬀ ects, like time-scaling an animation. (If we tried to slow a 30 FPS anima-\ntion down to less than 1/10 of its regular speed, we’d be out of precision!) So \nfor many purposes, it’s still best to use ﬂ oating-point time units, or machine \ncycles. But a 1/300 second time unit can be used eﬀ ectively for things like \nspecifying how much time should elapse between the shots of an automatic \nweapon, or how long an AI -controlled character should wait before starting \nhis patrol, or the amount of time the player can survive when standing in a \npool of acid.\n7.5.5. Dealing with Break Points\n When your game hits a break point, its loop stops running and the debug-\nger takes over. However, the CPU continues to run, and the real-time clock \ncontinues to accrue cycles. A large amount of wall clock time can pass while \nyou are inspecting your code at a break point. When you allow the program \n\n\n321 \nto continue, this can lead to a measured frame time many seconds, or even \nminutes or hours in duration!\nClearly if we allow such a huge delta-time to be passed to the subsystems \nin our engine, bad things will happen. If we are lucky, the game might con-\ntinue to function properly aft er lurching forward many seconds in a single \nframe. Worse, the game might just crash.\nA simple approach can be used to get around this problem. In the main \ngame loop, if we ever measure a frame time in excess of some predeﬁ ned up-\nper limit (e.g., 1/10 of a second), we can assume that we have just resumed ex-\necution aft er a break point, and we set the delta time artiﬁ cially to 1/30 or 1/60 \nof a second (or whatever the target frame rate is). In eﬀ ect, the game becomes \nframe-locked for one frame, in order to avoid a massive spike in the measured \nframe duration.\n// Start off assuming the ideal dt (30 FPS).\nF32 dt = 1.0f / 30.0f;\n// Prime the pump by reading the current time.\nU64 tBegin = readHiResTimer();\nwhile (true) // main game loop\n{\n updateSubsystemA(dt);\n updateSubsystemB(dt);\n \n// ...\n renderScene();\n swapBuffers();\n \n// Read the current time again, and calculate an   \n \n \n// estimate of next frame’s delta time.\n \nU64 tEnd = readHiResTimer();\n \ndt = (F32)(tEnd – tBegin) / (F32) \n   getHiResTimerFrequency();\n \n// If dt is too large, we must have resumed from a  \n \n \n// break point -- frame-lock to the target rate this   \n \n// frame.\n \nif (dt > 1.0f/10.0f)\n {\n \n \ndt = 1.0f/30.0f;\n }\n \n// Use tEnd as the new tBegin for next frame.\n \ntBegin = tEnd;\n}\n7.5. Measuring and Dealing with Time\n\n\n322 \n7. The Game Loop and Real-Time Simulation\n7.5.6. A Simple Clock Class\nSome game engines encapsulate their clock variables in a class. An engine \nmight have a few instances of this class—one to represent real “wall clock” \ntime, another to represent “game time” (which can be paused, slowed down \nor sped up relative to real time), another to track time for full-motion videos, \nand so on. A clock class is reasonably straightforward to implement. I’ll pres-\nent a simple implementation below, making note of a few common tips, tricks, \nand pitfalls in the process.\nA clock class typically contains a variable that tracks the absolute time \nthat has elapsed since the clock was created. As described above, it’s im-\nportant to select a suitable data type and time unit for this variable. In the \nfollowing example, we’ll store absolute times in the same way the CPU \ndoes—with a 64-bit unsigned integer, measured in machine cycles. There \nare other possible implementations, of course, but this is probably the sim-\nplest.\nA clock class can support some nift y features, like time-scaling. This can \nbe implemented by simply multiplying the measured time delta by an arbi-\ntrary scale factor prior to adding it to the clock’s running total. We can also \npause time by simply skipping its update while the clock is paused. Single-\nstepping a clock can be implemented by adding a ﬁ xed time interval to a \npaused clock in response to a butt on press on the joypad or keyboard. All of \nthis is demonstrated by the example Clock class shown below.\nclass Clock\n{\n U64 \nm_timeCycles;\n F32 \nm_timeScale;\n \nbool  \nm_isPaused;\nstatic F32 \ns_cyclesPerSecond;\n \nstatic inline U64 secondsToCycles(F32 timeSeconds) \n {\n \n \nreturn (U64)(timeSeconds * s_cyclesPerSecond);\n }\n \n// WARNING: Dangerous -- only use to convert small\n \n// durations into seconds.\n \nstatic inline F32 cyclesToSeconds(U64 timeCycles)\n {\n \n \nreturn (F32)timeCycles / s_cyclesPerSecond;\n }\n\n\n323 \npublic:\n \n// Call this when the game first starts up.\n \nstatic void init()\n {\n  s_cyclesPerSecond \n   = \n(F32)readHiResTimerFrequency();\n }\n \n// Construct a clock.\n explicit \nClock(F32 startTimeSeconds = 0.0f) :\n  m_timeCycles(\nsecondsToCycles(startTimeSeconds)),\n  m_timeScale(\n1.0f), // default to unscaled\n  m_isPaused(\nfalse)  // default to running\n {\n }\n \n// Return the current time in cycles. NOTE that we do  \n \n// not return absolute time measurements in floating   \n \n// point seconds, because a 32-bit float doesn’t have  \n \n// enough precision. See calcDeltaSeconds().\n U64 \ngetTimeCycles() const\n {\n  return \nm_timeCycles;\n }\n \n// Determine the difference between this clock’s  \n \n// absolute time and that of another clock, in \n \n// seconds. We only return time deltas as floating  \n \n \n// point seconds, due to the precision limitations of  \n \n// a 32-bit float.\n F32 \ncalcDeltaSeconds(const Clock& other)\n {\n \n \nU64 dt = m_timeCycles – other.m_timeCycles;\n  return \ncyclesToSeconds(dt);\n }\n \n// This function should be called once per frame,  \n \n   // with the real measured frame time delta in seconds.\n void \nupdate(F32 dtRealSeconds)\n {\n  if \n(!m_isPaused)\n  { \n  \n   U64 \ndtScaledCycles\n    \n= secondsToCycles(\n    dtRealSeconds \n* m_timeScale);\n   m_timeCycles \n+= dtScaledCycles;\n  }\n }\n7.5. Measuring and Dealing with Time\n\n\n324 \n7. The Game Loop and Real-Time Simulation\n void \nsetPaused(bool isPaused)\n {\n \n \nm_isPaused = isPaused;\n }\n bool \nisPaused() const\n {\n  return \nm_isPaused;\n }\n void \nsetTimeScale(F32 scale)\n {\n \n \nm_timeScale = scale;\n }\n F32 \ngetTimeScale() const\n {\n  return \nm_timeScale;\n }\n void \nsingleStep()\n {\n  if \n(m_isPaused)\n  {\n \n \n \n// Add one ideal frame interval; don’t forget   \n \n \n \n// to scale it by our current time scale!\n   U64 \ndtScaledCycles = secondsToCycles(\n    (\n1.0f/30.0f) * m_timeScale);\n   m_timeCycles \n+= dtScaledCycles;\n  }\n }\n};\n7.6. Multiprocessor Game Loops\nNow that we’ve investigated basic single-threaded game loops and learned \nsome of the ways in which time is commonly measured and manipulated \nin a game engine, let’s turn our att ention to some more complex kinds of \ngame loops. In this section, we’ll explore how game loops have evolved to \ntake advantage of modern multiprocessor hardware. In the following sec-\ntion, we’ll see how networked multiplayer games typically structure their \ngame loops.\nIn 2004, microprocessor manufacturers industry-wide encountered a \nproblem with heat dissipation that prevented them from producing faster \n\n\n325 \nCPUs. Moore’s Law , which predicts an approximate doubling in transistor \ncounts every 18 to 24 months, still holds true. But in 2004, its assumed cor-\nrelation with doubling processor speeds was shown to be no longer val-\nid. As a result, microprocessor manufacturers shift ed their focus toward \nmulticore CPUs. (For more information on this trend, see Microsoft ’s “The \nManycore Shift  Whitepaper,” available at htt p://www.microsoft post.com/\nmicrosoft -download/the-manycore-shift -white-paper, and “Multicore Erod-\ning Moore’s Law” by Dean Dauger, available at htt p://www.macresearch.\norg/multicore_eroding_moores_law.) The net eﬀ ect on the soft ware industry \nwas a major shift  toward parallel processing techniques. As a result, mod-\nern game engines running on multicore systems like the Xbox 360 and the \nPLAYSTATION 3 can no longer rely on a single main game loop to service \ntheir subsystems.\nThe shift  from single core to multicore has been painful. Multithreaded \nprogram design is a lot harder than single-threaded programming. Most \ngame companies took on the transformation gradually, by selecting a hand-\nful of engine subsystems for parallelization, and leaving the rest under the \ncontrol of the old, single-threaded main loop. By 2008, most game studios had \ncompleted the transformation for the most part and have embraced parallel-\nism to varying degrees within their engines.\nWe don’t have room here for a full treatise on parallel programming \narchitectures and techniques. (Refer to [20] for an in-depth discussion of \nthis topic.) However, we will take a brief look at some of the most common \nways in which game engines leverage multicore hardware. There are many \ndiﬀ erent soft ware architectures possible—but the goal of all of these archi-\ntectures is to maximize hardware utilization (i.e., to att empt to minimize \nthe amount of time during which any particular hardware thread, core or \nCPU is idle).\n7.6.1. \nMultiprocessor Game Console Architectures\nThe Xbox 360 and the PLAYSTATION 3 are both multiprocessor consoles. In \norder to have a meaningful discussion of parallel soft ware architectures, let’s \ntake a brief look at how these two consoles are structured internally.\n7.6.1.1. \nXbox 360\nThe Xbox 360 consists of three identical PowerPC processor cores. Each core \nhas a dedicated L1 instruction cache and L1 data cache, and the three cores \nshare a single L2 cache. The three cores and the GPU share a uniﬁ ed 512 MB \npool of RAM, which can be used for executable code, application data, tex-\ntures, video RAM—you name it. The Xbox 360 architecture is described in \n7.6. Multiprocessor Game Loops\n",
      "page_number": 326,
      "chapter_number": 17,
      "summary": "This chapter covers segment 17 (pages 326-347). Key topics include time, timing, and game.",
      "keywords": [
        "Game Loop",
        "Game",
        "time",
        "main game loop",
        "frame",
        "Game Loop Architectural",
        "frame time",
        "Loop",
        "Game Time",
        "Frame Rate",
        "game engine",
        "game clock",
        "clock",
        "game engine subsystems",
        "Multiprocessor Game Loops"
      ],
      "concepts": [
        "time",
        "timing",
        "game",
        "frame",
        "clock",
        "measures",
        "measuring",
        "measurements",
        "bit",
        "bits"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 12,
          "title": "Segment 12 (pages 101-108)",
          "relevance_score": 0.71,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 47,
          "title": "Segment 47 (pages 448-460)",
          "relevance_score": 0.7,
          "method": "sentence_transformers"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 26,
          "title": "Segment 26 (pages 243-252)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 33,
          "title": "Segment 33 (pages 309-318)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 348-366)",
      "start_page": 348,
      "end_page": 366,
      "detection_method": "synthetic",
      "content": "326 \n7. The Game Loop and Real-Time Simulation\nMain RAM\n(512 MB)\nPowerPC\nCore 0\nPowerPC\nCore 1\nPowerPC\nCore 2\nL1\nData\nL1\nInstr\nL1\nData\nL1\nInstr\nL1\nData\nL1\nInstr\nShared L2 Cache\nGPU\nFigure 7.4. A simpliﬁ ed view of the Xbox 360 hardware architecture.\na great deal more depth in the PowerPoint presentation entited “Xbox 360 \nSystem Architecture,” by Jeﬀ  Andrews and Nick Baker of the Xbox Semicon-\nductor Technology Group, available at htt p://www.hotchips.org/archives/\nhc17/3_Tue/HC17.S8/HC17.S8T4.pdf. However, the preceding extremely brief \noverview should suﬃ  ce for our purposes. Figure 7.4 shows the Xbox 360’s \narchitecture in highly simpliﬁ ed form.\n7.6.1.2. PLAYSTATION 3\nThe PLAYSTATION 3 hardware makes use of the Cell Broadband Engine \n(CBE) architecture (see Figure 7.5), developed jointly by Sony, Toshiba, and \nIBM. The PS3 takes a radically diﬀ erent approach to the one employed by the \nXbox 360. Instead of three identical processors, it contains a number of diﬀ er-\nent types of processors, each designed for speciﬁ c tasks. And instead of a uni-\nﬁ ed memory architecture, the PS3 divides its RAM into a number of blocks, \neach of which is designed for eﬃ  cient use by certain processing units in the \nsystem. The architecture is described in detail at htt p://www.blachford.info/\ncomputer/Cell/Cell1_v2.html, but the following overview and the diagram \nshown in Figure 7.5 should suﬃ  ce for our purposes.\nThe PS3’s main CPU is called the Power Processing Unit (PPU). It is a \nPowerPC processor, much like the ones found in the Xbox 360. In addition \nto this central processor, the PS3 has six coprocessors known as Synergistic \nProcessing Units (SPUs). These coprocessors are based around the PowerPC \ninstruction set, but they have been streamlined for maximum performance.\nThe GPU on the PS3 has a dedicated 256 MB of video RAM. The PPU has \naccess to 256 MB of system RAM. In addition, each SPU has a dedicated high-\nspeed 256 kB RAM area called its local store (LS). Local store memory performs \nabout as eﬃ  ciently as an L1 cache, making the SPUs blindingly fast.\n\n\n327 \nThe SPUs never read directly from main RAM. Instead, a direct memory \naccess (DMA) controller allows blocks of data to be copied back and forth \nbetween system RAM and the SPUs’ local stores. These data transfers happen \nin parallel, so both the PPU and SPUs can be doing useful calculations while \nthey wait for data to arrive.\n7.6.2. SIMD\nAs we saw in Section 4.7, most modern CPUs (including the Xbox 360’s three \nPowerPC processors, and the PS3’s PPU and SPUs) provide a class of instruc-\ntions known as single instruction, multiple data (SIMD). Such instructions can \nperform a particular operation on more than one piece of data simultaneously, \nand as such they represent a ﬁ ne-grained form of hardware parallelism. CPUs \nprovide a number of diﬀ erent SIMD instruction variants, but by far the most \ncommonly-used in games are instructions that operate on four 32-bit ﬂ oating-\npoint values in parallel, because they allow 3D vector and matrix math to be \nperformed four times more quickly than with their single instruction, single \ndata (SISD) counterparts.\nRetroﬁ tt ing existing 3D math code to leverage SIMD instructions can be \ntricky, although the task is much easier if a well-encapsulated 3D math li-\nbrary was used in the original code. For example, if a dot product is calcu-\nlated in long hand everywhere (e.g., ﬂoat d =  a.x * b.x + a.y * b.y \n+ a.z * b.z;), then a very large amount of code will need to be re-writt en. \nHowever, if dot products are calculated by calling a function (e.g., ﬂoat d = \nDot(a, b);), and if vectors are treated largely as black boxes throughout the \ncode base, then retroﬁ tt ing for SIMD can be accomplished by modifying the \n7.6. Multiprocessor Game Loops\nVideo RAM\n(256 MB)\nGPU\nSystem RAM\n(256 MB)\n...\nPPU\nL1\nData\nL1\nInstr\nL2 Cache\nSPU0\nLocal\nStore\n(256 kB)\nSPU1\nLocal\nStore\n(256 kB)\nSPU6\nLocal\nStore\n(256 kB)\nDMA \nController\nDMA Bus\nFigure 7.5. Simpliﬁ ed view of the PS3’s cell broadband architecture.\n\n\n328 \n7. The Game Loop and Real-Time Simulation\n3D math library, without having to modify much if any of the calling code \n(except perhaps to ensure alignment of vector data to 16-byte boundaries).\n7.6.3. Fork and Join\nAnother way to utilize multicore or multiprocessor hardware is to adapt di-\nvide-and-conquer algorithms for parallelism. This is oft en called the fork/join \napproach. The basic idea is to divide a unit of work into smaller subunits, dis-\ntribute these workloads onto multiple processing cores or hardware threads \n(fork), and then merge the results once all workloads have been completed \n(join). When applied to the game loop, the fork/join architecture results in a \nmain loop that looks very similar to its single-threaded counterpart, but with \nsome of the major phases of the update loop being parallelized. This architec-\nture is illustrated in Figure 7.6.\nLet’s take a look at a concrete example. Blending animations using linear \ninterpolation (LERP) is an operation that can be done on each joint indepen-\ndently of all other joints within a skeleton (see Section 11.5.2.2). We’ll assume \nthat we want to blend pairs of skeletal poses for ﬁ ve characters, each of which \nhas 100 joints, meaning that we need to process 500 pairs of joint poses.\nTo parallelize this task, we can divide the work into N batches, each con-\ntaining roughly 500/N joint-pose pairs, where N is selected based on the avail-\nMain \nThread\nHID\nUpdate Game \nObjects\nRagdoll Physics\nPost Animation \nGame Object Update\nFork\nJoin\nFork\nJoin\netc.\nPose \nBlending\nPose \nBlending\nPose \nBlending\nSimulate/\nIntegrate\nSimulate/\nIntegrate\nSimulate/\nIntegrate\nFigure 7.6. Fork and join used to parallelize selected CPU-intensive phases of the game loop.\n\n\n329 \nable processing resources. (On the Xbox 360, N should probably be 3 or 6, \nbecause the console has three cores with two hardware threads each. On a \nPS3, N might range anywhere from 1 to 6, depending on how many SPUs are \navailable.) We then “fork” (i.e., create) N threads, requesting each one to work \non a diﬀ erent group of pose pairs. The main thread can either continue doing \nsome useful but work that is independent of the animation blending task, or it \ncan go to sleep , waiting on a semaphore that will tell it when all of the worker \nthreads have completed their tasks. Finally, we “join” the individual resultant \njoint poses into a cohesive whole—in this case, by calculating the ﬁ nal global \npose of each of our ﬁ ve skeletons. (The global pose calculation needs access \nto the local poses of all the joints in each skeleton, so it doesn’t parallelize \nwell within a single skeleton. However, we could imagine forking again to \ncalculate the global pose, this time with each thread working on one or more \nwhole skeletons.)\nYou can ﬁ nd sample code illustrating how to fork and join worker \nthreads using Win32 system calls at htt p://msdn.microsoft .com/en-us/library/\nms682516(VS.85).aspx.\n7.6.4. One Thread per Subsystem\nYet another approach to multitasking is to assign particular engine subsys-\ntems to run in separate threads . A master thread controls and synchronizes \nthe operations of these secondary subsystem threads and also continues to \nhandle the lion’s share of the game’s high-level logic (the main game loop ). \nOn a hardware platform with multiple physical CPUs or hardware threads, \nthis design allows these threaded engine subsystems to execute in parallel. \nThis design is well suited to any engine subsystem that performs a relative-\nly isolated function repeatedly, such as a rendering engine, physics simula-\ntion, animation pipeline, or audio engine. The architecture is depicted in \nFigure 7.7.\nThreaded architectures are usually supported by some kind of thread \nlibrary on the target hardware system. On a personal computer running \nWindows, the Win32 thread API is usually used. On a UNIX-based system, \na library like pthreads might be the best choice. On the PLAYSTATION 3, a \nlibrary known as SPURS permits workloads to be run on the six synergistic \nprocessing units (SPUs). SPURS provides two primary ways to run code on \nthe SPUs—the task model and the job model . The task model can be used to \nsegregate engine subsystems into coarse-grained independent units of execu-\ntion that act very much like threads. We’ll discuss the SPURS job model in the \nnext section.\n7.6. Multiprocessor Game Loops\n\n\n330 \n7. The Game Loop and Real-Time Simulation\n7.6.5. Jobs\nOne problem with the multithreaded approach is that each thread represents \na relatively coarse-grained chunk of work (e.g., all animation tasks are in one \nthread, all collision and physics tasks in another). This can place restrictions \non how the various processors in the system can be utilized . If one of the \nsubsystem threads has not completed its work, the progress of other threads, \nincluding that of the main game loop, may be blocked.\nAnother way to take advantage of parallel hardware architecture is to \ndivide up the work that is done by the game engine into multiple small, rela-\ntively independent jobs . A job is best thought of as a pairing between a chunk \nof data and a bit of code that operates on that data. When a job is ready to be \nrun, it is placed on a queue, to be picked up and worked on by the next avail-\nable processing unit. This approach is supported on the PLAYSTATION 3 via \nthe SPURS job model. The main game loop runs on the PPU, and the six SPUs \nare used as job processors. Each job’s code and data are sent to an SPU’s local \nstore via a DMA transfer. The SPU processes the job, and then it DMAs its \nresults back to main RAM.\nFigure 7.7.  One thread per major engine subsystem.\n\n\n331 \nAs shown in Figure 7.8, the fact that jobs are relatively ﬁ ne-grained and \nindependent of one another helps to maximize processor utilization. It can \nalso reduce or eliminate some of the restrictions placed on the main thread \nin the one-thread-per-subsystem design. This architecture also scales up or \ndown naturally to hardware with any number of processing units (something \nthe one-thread-per-subsystem architecture does not do particularly well).\n7.6.6. Asynchronous Program Design\n When writing or retroﬁ tt ing a game engine to take advantage of multitasking \nhardware, programmers must be careful to design their code in an asynchro-\nnous manner. This means that the results of an operation will usually not be \navailable immediately aft er requesting it, as they would be in a synchronous \ndesign. For example, a game might request that a ray be cast into the world, in \norder to determine whether the player has line-of-sight to an enemy character. \nIn a synchronous design, the ray cast would be done immediately in response \nto the request, and when the ray casting function returned, the results would \nbe available, as shown below.\nFigure 7.8.  In a job architecture, work is broken down into ﬁ ne-grained chunks that can \nbe picked up by any available processor. This can help maximize processor utilization, while \nproviding the main game loop with improved ﬂ exibility.\n7.6. Multiprocessor Game Loops\n\n\n332 \n7. The Game Loop and Real-Time Simulation\nwhile (true) // main game loop\n{\n \n// ...\n \n// Cast a ray to see if the player has line of sight   \n \n// to the enemy.\n RayCastResult \nr = castRay(playerPos, enemyPos);\n \n// Now process the results...\n \nif (r.hitSomething() && isEnemy(r.getHitObject()))\n {\n \n \n// Player can see the enemy.\n  // \n...\n }\n \n// ...\n}\nIn an asynchronous design, a ray cast request would be made by calling \na function that simply sets up and enqueues a ray cast job, and then returns \nimmediately. The main thread can continue doing other unrelated work while \nthe job is being processed by another CPU or core. Later, once the job has been \ncompleted, the main thread can pick up the results of the ray cast query and \nprocess them:\nwhile (true) // main game loop\n{\n \n// ...\n \n// Cast a ray to see if the player has line of sight   \n \n// to the enemy.\n RayCastResult \nr;\nrequestRayCast(playerPos, enemyPos, &r);\n \n// Do other unrelated work while we wait for the   \n \n \n// other CPU to perform the ray cast for us.\n// ...\n \n// OK, we can’t do any more useful work. Wait for the\n \n// results of our ray cast job. If the job is   \n \n \n \n// complete, this function will return immediately.\n \n// Otherwise, the main thread will idle until they  \n \n \n// are ready...\nwaitForRayCastResults(&r);\n \n// Process results...\n \nif (r.hitSomething() && isEnemy(r.getHitObject()))\n {\n \n \n// Player can see the enemy.\n  // \n...\n }\n\n\n333 \n \n// ...\n}\nIn many instances, asynchronous code can kick oﬀ  a request on one frame, \nand pick up the results on the next. In this case, you may see code that looks \nlike this:\nRayCastResult r;\nbool rayJobPending = false;\nwhile (true) // main game loop\n{\n \n// ...\n \n// Wait for the results of last frame’s ray cast job.\n \nif (rayJobPending)\n {\nwaitForRayCastResults(&r);\n \n \n// Process results...\n  if \n(r.hitSomething() && isEnemy(r.getHitObject()))\n  {\n \n \n \n// Player can see the enemy.\n   // \n...\n  }\n }\n \n// Cast a new ray for next frame.\nrayJobPending = true;\nrequestRayCast(playerPos, enemyPos, &r);\n \n// Do other work...\n// ...\n}\n7.7. Networked Multiplayer Game Loops\nThe game loop of a networked multiplayer game is particularly interesting, \nso we’ll have a brief look at how such loops are structured. We don’t have \nroom here to go into the all of the details of how multiplayer games work. \n(Refer to [3] for an excellent in-depth discussion of the topic.) However, we’ll \nprovide a brief overview of the two most-common multiplayer architectures \nhere, and then look at how these architectures aﬀ ect the structure of the game \nloop.\n7.7. Networked Multiplayer Game Loops\n\n\n334 \n7. The Game Loop and Real-Time Simulation\n7.7.1. \nClient-Server\nIn the client-server model, the vast majority of the game’s logic runs on a single \nserver machine. Hence the server’s code closely resembles that of a non-net-\nworked single-player game. Multiple client machines can connect to the server \nin order to take part in the online game. The client is basically a “dumb” ren-\ndering engine that also reads human interface devices and controls the local \nplayer character, but otherwise simply renders whatever the server tells it to \nrender. Great pains are taken in the client code to ensure that the inputs of the \nlocal human player are immediately translated into the actions of the player’s \ncharacter on-screen. This avoids what would otherwise be an extremely an-\nnoying sense of delayed reaction on the part of the player character. But other \nthan this so-called player prediction code, the client is usually not much more \nthan a rendering and audio engine, combined with some networking code.\nThe server may be running on a dedicated machine, in which case we say \nit is running in dedicated server mode. However, the client and server needn’t \nbe on separate machines, and in fact it is quite typical for one of the client ma-\nchines to also be running the server. In fact, in many client-server multiplayer \ngames, the single-player game mode is really just a degenerate multiplayer \ngame, in which there is only one client, and both the client and server are run-\nning on the same machine. This is known as client-on-top-of-server mode.\nThe game loop of a client-server multiplayer game can be implemented \nin a number of diﬀ erent ways. Since the client and server are conceptually \nseparate entities, they could be implemented as entirely separate processes \n(i.e., separate applications). They could also be implemented as two separate \nthreads of execution, within a single process. However, both of these ap-\nproaches require quite a lot of overhead to permit the client and server to \ncommunicate locally, when being run in client-on-top-of-server mode. As a \nresult, a lot of multiplayer games run both client and server in a single thread, \nserviced by a single game loop.\nIt’s important to realize that the client and server code can be updated \nat diﬀ erent rates. For example, in Quake, the server runs at 20 FPS (50 ms per \nframe), while the client typically runs at 60 FPS (16.6 ms per frame). This is \nimplemented by running the main game loop at the faster of the two rates \n(60 FPS) and then servicing the server code once roughly every three frames. \nIn reality, the amount of time that has elapsed since the last server update is \ntracked, and when it reaches or exceeds 50 ms, a server frame is run and the \ntimer is reset. Such a game loop might look something like this:\nF32 dtReal = 1.0f/30.0f; // the real frame delta time\nF32 dtServer = 0.0f;     // the server’s delta time\n\n\n335 \nU64 tBegin = readHiResTimer();\nwhile (true) // main game loop\n{\n \n// Run the server at 50 ms intervals.\ndtServer += dtReal;\n \nif (dtServer >= 0.05f)  // 50 ms\n {\n \n  \nrunServerFrame(0.05f);\n \n  \ndtServer -= 0.05f;  // reset for next update\n }\n \n// Run the client at maximum frame rate.\nrunClientFrame(dtReal);\n \n// Read the current time, and calculate an estimate\n \n// of next frame’s real delta time.\n \nU64 tEnd = readHiResTimer();\ndtReal   = (F32)(tEnd – tBegin)\n \n  \n \n     / \n(F32)getHiResTimerFrequency();\n \n// Use tEnd as the new tBegin for next frame.\n \ntBegin = tEnd;\n}\n7.7.2. Peer-to-Peer\nIn the peer-to-peer multiplayer architecture, every machine in the online game \nacts somewhat like a server, and somewhat like a client. One and only one \nmachine has authority over each dynamic object in the game. So each machine \nacts like a server for those objects over which it has authority. For all other ob-\njects in the game world, the machine acts like a client, rendering the objects in \nwhatever state is provided to it by that object’s remote authority.\nThe structure of a peer-to-peer multiplayer game loop is much simpler \nthan a client-server game loop, in that at the top-most level, it looks very much \nlike a single-player game loop. However, the internal details of the code can be \na bit more confusing. In a client-server model, it is usually quite clear which \ncode is running on the server and which code is client-side. But in a peer-to-\npeer architecture, much of the code needs to be set up to handle two possible \ncases: one in which the local machine has authority over the state of an object \nin the game, and one in which the object is just a dumb proxy for a remote \nauthoritative representation. These two modes of operation are oft en imple-\nmented by having two kinds of game objects—a full-ﬂ edged “real” game ob-\n7.7. Networked Multiplayer Game Loops\n\n\n336 \n7. The Game Loop and Real-Time Simulation\nject, over which the local machine has authority and a “proxy ” version that \ncontains a minimal subset of the state of the remote object.\nPeer-to-peer architectures are made even more complex because author-\nity over an object sometimes needs to migrate from machine to machine. For \nexample, if one computer drops out of the game, all of the objects over which \nit had authority must be picked up by the other machines in the game. Like-\nwise, when a new machine joins the game, it should ideally take over author-\nity of some game objects from other machines, in order to balance the load. \nThe details are beyond the scope of this book. The key point here is that multi-\nplayer architectures can have profound eﬀ ects on the structure of a game’s \nmain loop.\n7.7.3. Case Study: Quake II\nThe following is an excerpt from the Quake II game loop . The source code for \nQuake, Quake II, and Quake 3 Arena is available on Id Soft ware’s website, htt p://\nwww.idsoft ware.com. As you can see, all of the elements we’ve discussed are \npresent, including the Windows message pump (in the Win32 version of the \ngame), calculation of the real frame delta time , ﬁ xed-time and time-scaled \nmodes of operation, and servicing of both server-side and client-side engine \nsystems.\nint WINAPI WinMain (HINSTANCE hInstance,\n      HINSTANCE \nhPrevInstance,\n      LPSTR \nlpCmdLine, int nCmdShow)\n{\n MSG \n   msg;\n int \n   time, \noldtime, newtime;\n char \n  *cddir;\n \nParseCommandLine (lpCmdLine);\n \nQcommon_Init (argc, argv);\n \noldtime = Sys_Milliseconds ();\n/* main window message loop */\n \nwhile (1)\n {\n \n \n// Windows message pump.\n \n \nwhile (PeekMessage (&msg, NULL, 0, 0, \n   PM_NOREMOVE))\n  {\n \n \n \nif (!GetMessage (&msg, NULL, 0, 0))\n    Com_Quit \n();\n   sys_msg_time \n= msg.time;\n\n\n337 \n   TranslateMessage \n(&msg);\n  DispatchMessage \n(&msg);\n  }\n \n \n// Measure real delta time in milliseconds.\n  do\n  {\n \n \n \nnewtime = Sys_Milliseconds ();\n \n \n \ntime = newtime - oldtime;\n \n \n} while (time < 1);\n \n \n// Run a frame of the game.\nQcommon_Frame (time);\n \n \noldtime = newtime;\n }\n \n// never gets here\n \nreturn TRUE;\n}\nvoid Qcommon_Frame (int msec)\n{\n \nchar *s;\n int \n \ntime_before, time_between, time_after;\n \n// [some details omitted...]\n \n// Handle fixed-time mode and time scaling.\n \nif (fixedtime->value)\n \n \nmsec = fixedtime->value;\n \nelse if (timescale->value)\n {\n \n \nmsec *= timescale->value;\n \n \nif (msec < 1)\n   msec \n= 1;\n }\n \n// Service the in-game console.\n do\n {\n \n \ns = Sys_ConsoleInput ();\n  if \n(s)\n   Cbuf_AddText \n(va(\"%s\\n\",s));\n \n} while (s);\n \nCbuf_Execute ();\n7.7. Networked Multiplayer Game Loops\n\n\n338 \n7. The Game Loop and Real-Time Simulation\n \n// Run a server frame.\nSV_Frame (msec);\n \n// Run a client frame.\nCL_Frame (msec);\n \n// [some details omitted...]\n}\n\n\n339\n8\nHuman Interface \nDevices (HID)\nG\names are interactive computer simulations, so the human player(s) need \nsome way of providing inputs to the game. All sorts of human interface \ndevices (HID) exist for gaming, including joysticks, joypads, keyboards and \nmice, track balls, the  Wii remote, and specialized input devices like steering \nwheels, ﬁ shing rods, dance pads, and even electric guitars. In this chapter, \nwe’ll investigate how game engines typically read, process, and utilize the \ninputs from human interface devices. We’ll also have a look at how outputs \nfrom these devices provide feedback to the human player.\n8.1. \nTypes of Human Interface Devices\nA wide range of human interface devices are available for gaming purposes. \nConsoles like the Xbox 360 and PS3 come equipped with joypad controllers, as \nshown in Figure 8.1. Nintendo’s Wii console is well known for its unique and \ninnovative WiiMote controller, shown in Figure 8.2. PC games are generally \neither controlled via a keyboard and the mouse, or via a joypad. (Microsoft  \ndesigned the Xbox 360 joypad so that it can be used both on the Xbox 360 and \non Windows/DirectX PC platforms.) As shown in Figure 8.3, arcade machines \nhave one or more built-in controllers, such as a joystick and various butt ons, or \na track ball, a steering wheel, etc. An arcade machine’s input device is usually \n\n\n340 \n8. Human Interface Devices (HID)\nFigure 8.1. Standard joypads for the Xbox 360 and PLAYSTATION 3 consoles.\nFigure 8.2. The innovative WiiMote for the Nintendo Wii.\nFigure 8.3. Various custom input devices for the arcade game Mortal Kombat II by Midway.\nFigure 8.4.  Many specialized input devices are available for use with consoles.\n\n\n341 \n8.2. Interfacing with a HID\nsomewhat customized to the game in question, although input hardware is \noft en re-used among arcade machines produced by the same manufacturer.\nOn console platforms, specialized input devices and adapters are usually \navailable, in addition to the “standard” input device such as the joypad. For \nexample, guitar and drum devices are available for the Guitar Hero series of \ngames, steering wheels can be purchased for driving games, and games like \nDance Dance Revolution use a special dance pad device. Some of these devices \nare shown in Figure 8.4.\nThe Nintendo WiiMote is one of the most ﬂ exible input devices on the \nmarket today. As such, it is oft en adapted to new purposes, rather than re-\nplaced with an entirely new device. For example, Mario Kart Wii comes with \na pastic steering wheel adapter into which the WiiMote can be inserted (see \nFigure 8.5).\n8.2. Interfacing with a HID\nAll human interface devices provide input to the game soft ware, and some \nalso allow the soft ware to provide feedback to the human player via various \nkinds of outputs as well. Game soft ware reads and writes HID inputs and \noutputs in various ways, depending on the speciﬁ c design of the device in \nquestion.\n8.2.1. Polling\nSome simple devices, like game pads and old-school joysticks, are read by \npolling the hardware periodically (usually once per iteration of the main game \nloop). This means explicitly querying the state of the device, either by read-\ning hardware registers directly, reading a memory-mapped I/O port, or via a \nhigher-level soft ware interface (which, in turn, reads the appropriate registers \nor memory-mapped I/O ports). Likewise, outputs might be sent to the HID by \nFigure 8.5. Steering wheel adapter for the Nintendo Wii.\n\n\n342 \n8. Human Interface Devices (HID)\nwriting to special registers or memory-mapped I/O addresses, or via a higher-\nlevel API that does our dirty work for us.\nMicrosoft ’s XInput API, for use with Xbox 360 game pads on both the \nXbox 360 and Windows PC platforms, is a good example of a simple polling \nmechanism. Every frame, the game calls the function XInputGetState(). \nThis function communicates with the hardware and/or drivers, reads the data \nin the appropriate way, and packages it all up for convenient use by the soft -\nware. It returns a pointer to an XINPUT_STATE struct, which in turn contains \nan embedded instance of a struct called XINPUT_GAMEPAD. This struct con-\ntains the current states of all of the controls (butt ons, thumb sticks, and trig-\ngers) on the device.\n8.2.2. Interrupts\nSome HIDs only send data to the game engine when the state of the controller \nchanges in some way. For example, a mouse spends a lot of its time just sitt ing \nstill on the mouse pad. There’s no reason to send a continuous stream of data \nbetween the mouse and the computer when the mouse isn’t moving—we need \nonly transmit information when it moves, or a butt on is pressed or released.\nThis kind of device usually communicates with the host computer via \nhardware interrupts . An interrupt is an electronic signal generated by the hard-\nware, which causes the CPU to temporarily suspend execution of the main \nprogram and run a small chunk of code called an interrupt service routine (ISR). \nInterrupts are used for all sorts of things, but in the case of a HID, the ISR code \nwill probably read the state of the device, store it oﬀ  for later processing, and \nthen relinquish the CPU back to the main program. The game engine can pick \nup the data the next time it is convenient to do so.\n8.2.3. Wireless Devices\nThe inputs and outputs of a Bluetooth device, like the WiiMote, the \nDualShock 3 and the Xbox 360 wireless controller, cannot be read and writ-\nten by simply accessing registers or memory-mapped I/O ports. Instead, the \nsoft ware must “talk” to the device via the Bluetooth protocol. The soft ware \ncan request the HID to send input data (such as the states of its butt ons) back \nto the host, or it can send output data (such as rumble sett ings or a stream of \naudio data) to the device. This communication is oft en handled by a thread \nseparate from the game engine’s main loop, or at least encapsulated behind a \nrelatively simple interface that can be called from the main loop. So from the \npoint of view of the game programmer, the state of a Bluetooth device can be \nmade to look prett y much indistinguishable from a traditional polled device.\n\n\n343 \n8.3. Types of Inputs\n8.3. Types of Inputs\nAlthough human interface devices for games vary widely in terms of form \nfactor and layout, most of the inputs they provide fall into one of a small num-\nber of categories. We’ll investigate each category in depth below.\n8.3.1. Digital Buttons\nAlmost every HID has at least a few digital butt ons . These are butt ons that can \nonly be in one of two states: pressed and not pressed. Game programmers oft en \nrefer to a pressed butt on as being down and a non-pressed butt on as being up.\nElectrical engineers speak of a circuit containing a switch as being closed \n(meaning electricity is ﬂ owing through the circuit) or open (no electricity is \nﬂ owing—the circuit has inﬁ nite resistance). Whether closed corresponds to \npressed or not pressed depends on the hardware. If the switch is normally open, \nthen when it is not pressed (up), the circuit is open, and when it is pressed \n(down), the circuit is closed. If the switch is normally closed, the reverse is true—\nthe act of pressing the butt on opens the circuit.\nIn soft ware, the state of a digital butt on (pressed or not pressed) is usually \nrepresented by a single bit. It’s common for 0 to represent not pressed (up) \nand 1 to represent pressed (down). But again, depending on the nature of the \ncircuitry, and the decisions made by the programmers who wrote the device \ndriver, the sense of these values might be reversed.\nIt is quite common for the states of all of the butt ons on a device to be \npacked into a single unsigned integer value. For example, in Microsoft ’s \nXInput API, the state of the Xbox 360 joypad is returned in a struct called \nXINPUT_GAMEPAD, shown below.\ntypedef struct _XINPUT_GAMEPAD {\n    WORD\nwButtons;\n    BYTE  bLeftTrigger;\n    BYTE  bRightTrigger;\n    SHORT sThumbLX;\n    SHORT sThumbLY;\n    SHORT sThumbRX;\n    SHORT sThumbRY;\n} XINPUT_GAMEPAD;\nThis struct contains a 16-bit unsigned integer (WORD) variable named \nwButtons that holds the state of all butt ons. The following masks deﬁ ne \n\n\n344 \n8. Human Interface Devices (HID)\nwhich physical butt on corresponds to each bit in the word. (Note that bits 10 \nand 11 are unused.)\n#define XINPUT_GAMEPAD_DPAD_UP          0x0001 // bit 0\n#define XINPUT_GAMEPAD_DPAD_DOWN        0x0002 // bit 1\n#define XINPUT_GAMEPAD_DPAD_LEFT        0x0004 // bit 2\n#define XINPUT_GAMEPAD_DPAD_RIGHT       0x0008 // bit 3\n#define XINPUT_GAMEPAD_START            0x0010 // bit 4\n#define XINPUT_GAMEPAD_BACK             0x0020 // bit 5\n#define XINPUT_GAMEPAD_LEFT_THUMB       0x0040 // bit 6\n#define XINPUT_GAMEPAD_RIGHT_THUMB      0x0080 // bit 7\n#define XINPUT_GAMEPAD_LEFT_SHOULDER    0x0100 // bit 8\n#define XINPUT_GAMEPAD_RIGHT_SHOULDER   0x0200 // bit 9\n#define XINPUT_GAMEPAD_A                0x1000 // bit 12\n#define XINPUT_GAMEPAD_B                0x2000 // bit 13\n#define XINPUT_GAMEPAD_X                0x4000 // bit 14\n#define XINPUT_GAMEPAD_Y                0x8000 // bit 15\nAn individual butt on’s state can be read by masking the wButtons word \nwith the appropriate bit mask via C/C++’s bitwise AND operator (&) and then \nchecking if the result is non-zero. For example, to determine if the A butt on is \npressed (down), we would write:\nbool IsButtonADown(const XINPUT_GAMEPAD& pad)\n{\n \n// Mask off all bits but bit 12 (the A button).\n \nreturn ((pad.wButtons & XINPUT_GAMEPAD_A) != 0);\n}\n8.3.2. Analog Axes and Buttons\nAn analog input is one that can take on a range of values (rather than just 0 \nor 1). These kinds of inputs are oft en used to represent the degree to which \na trigger is pressed, or the two-dimensional position of a joystick (which is \nrepresented using two analog inputs, one for the x-axis and one for the y-axis, \n",
      "page_number": 348,
      "chapter_number": 18,
      "summary": "This chapter covers segment 18 (pages 348-366). Key topics include game, gaming, and threads. The Game Loop and Real-Time Simulation\nMain RAM\n(512 MB)\nPowerPC\nCore 0\nPowerPC\nCore 1\nPowerPC\nCore 2\nL1\nData\nL1\nInstr\nL1\nData\nL1\nInstr\nL1\nData\nL1\nInstr\nShared L2 Cache\nGPU\nFigure 7.4.",
      "keywords": [
        "Game Loop",
        "main game loop",
        "Game",
        "Multiplayer Game Loops",
        "main game",
        "human interface devices",
        "Loop",
        "Multiplayer Game",
        "define XINPUT",
        "Multiprocessor Game Loops",
        "XInput",
        "interface devices",
        "game loop runs",
        "GAMEPAD",
        "Networked Multiplayer Game"
      ],
      "concepts": [
        "game",
        "gaming",
        "threads",
        "engine",
        "code",
        "times",
        "processing",
        "process",
        "processes",
        "devices"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 8",
          "chapter": 47,
          "title": "Segment 47 (pages 448-460)",
          "relevance_score": 0.7,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 39,
          "title": "Segment 39 (pages 789-807)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 41,
          "title": "Segment 41 (pages 830-852)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 53,
          "title": "Segment 53 (pages 511-519)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 367-384)",
      "start_page": 367,
      "end_page": 384,
      "detection_method": "synthetic",
      "content": "345 \nas shown in Figure 8.6). Because of this common usage, analog inputs are \nsometimes called analog axes , or just axes.\nOn some devices, certain butt ons are analog as well, meaning that the \ngame can actually detect how hard the player is pressing on them. However, \nthe signals produced by analog butt ons are usually too noisy to be particu-\nlarly usable. I have yet to see a game that uses analog butt on inputs eﬀ ectively \n(although some may very well exist!)\nStrictly speaking, analog inputs are not really analog by the time they \nmake it to the game engine. An analog input signal is usually digitized, mean-\ning it is quantized and represented using an integer in soft ware. For example, \nan analog input might range from –32,768 to 32,767 if represented by a 16-bit \nsigned integer. Sometimes analog inputs are converted to ﬂ oating-point—\nthe values might range from –1 to 1, for instance. But as we know from Sec-\ntion 3.2.1.3, ﬂ oating-point numbers are really just quantized digital values as \nwell.\nReviewing the deﬁ nition of XINPUT_GAMEPAD (repeated below), we can \nsee that Microsoft  chose to represent the deﬂ ections of the left  and right thumb \nsticks on the Xbox 360 gamepad using 16-bit signed integers (sThumbLX\nand sThumbLY for the left  stick and sThumbRX and sThumbRY for the right). \nHence, these values range from –32,768 (left  or down) to 32,767 (right or up). \nHowever, to represent the positions of the left  and right shoulder triggers, \nMicrosoft  chose to use 8-bit unsigned integers (bLeftTrigger and bRight-\nTrigger respectively). These input values range from 0 (not pressed) to 255 \n(fully pressed). Diﬀ erent game machines use diﬀ erent digital representions \nfor their analog axes.\ntypedef struct _XINPUT_GAMEPAD {\n    WORD  wButtons;\nx\ny\n(1, 1)\n(–1, –1)\n(0.1, 0.3)\nFigure 8.6.  Two analog inputs can be used to represent the x and y deﬂ ection of a joystick.\n8.3. Types of Inputs\n\n\n346 \n8. Human Interface Devices (HID)\n// 8-bit unsigned\n    BYTE\nbLeftTrigger;\n    BYTE\nbRightTrigger;\n// 16-bit signed\n    SHORT sThumbLX;\n    SHORT sThumbLY;\n    SHORT sThumbRX;\n    SHORT sThumbRY;\n} XINPUT_GAMEPAD;\n8.3.3. Relative Axes\nThe position of an analog butt on, trigger, joystick, or thumb stick is absolute, \nmeaning that there is a clear understanding of where zero lies. However, the \ninputs of some devices are relative . For these devices, there is no clear location \nat which the input value should be zero. Instead, a zero input indicates that \nthe position of the device has not changed, while non-zero values represent \na delta from the last time the input value was read. Examples include mice, \nmouse wheels, and track balls.\n8.3.4. Accelerometers\nThe PLAYSTATION 3’s Sixaxis and DualShock 3 joypads, and the Nintendo \nWiiMote , all contain acceleration sensors (accelerometers ). These devices can \ndetect acceleration along the three principle axes (x, y, and z), as shown in Fig-\nure 8.7. These are relative analog inputs, much like a mouse’s two-dimensional \naxes. When the controller is not accelerating these inputs are zero, but when \nthe controller is accelerating, they measure the acceleration up to ±3 g along \neach axis, quantized into three signed 8-bit integers, one for each of x, y, and z.\nx\ny\nz\nFigure 8.7. Accelerometer axes for the WiiMote.\n8.3.5. 3D Orientation with the WiiMote or Sixaxis\nSome Wii and PS3 games make use of the three accelerometers in the WiiMote \nor Sixaxis joypad to estimate the orientation of the controller in the player’s \n\n\n347 \nhand. For example, in Super Mario Galaxy, Mario hops onto a large ball and \nrolls it around with his feet. To control Mario in this mode, the WiiMote is held \nwith the IR sensor facing the ceiling. Tilting the WiiMote left , right, forward, \nor back causes the ball to accelerate in the corresponding direction.\nA trio of accelerometers can be used to detect the orientation of the \nWiiMote or Sixaxis joypad, because of the fact that we are playing these games \non the surface of the Earth where there is a constant downward acceleration \ndue to gravity of 1g (≈ 9.8 m/s2). If the controller is held perfectly level, with \nthe IR sensor pointing toward your TV set, the vertical (z) acceleration should \nbe approximately –1 g.\nIf the controller is held upright, with the IR sensor pointing toward the \nceiling, we would expect to see a 0 g acceleration on the z sensor, and +1 g \non the y sensor (because it is now experiencing the full gravitational eﬀ ect). \nHolding the WiiMote at a 45-degree angle should produce roughly sin(45°) = \ncos(45°) = 0.707 g on both the y and z inputs. Once we’ve calibrated the accel-\nerometer inputs to ﬁ nd the zero points along each axis, we can calculate pitch, \nyaw, and roll easily, using inverse sine and cosine operations.\nTwo caveats here: First, if the person holding the WiiMote is not hold-\ning it still, the accelerometer inputs will include this acceleration in their val-\nues, invalidating our math. Second, the z-axis of the accelerometer has been \ncalibrated to account for gravity, but the other two axes have not. This means \nthat the z-axis has less precision available for detecting orientation. Many Wii \ngames request that the user hold the WiiMote in a non-standard orientation, \nsuch as with the butt ons facing the player’s chest, or with the IR sensor point-\ning toward the ceiling. This maximizes the precision of the orientation read-\ning, by placing the x- or y-accelerometer axis in line with gravity, instead of the \ngravity-calibrated z- axis. For more information on this topic, see htt p://druid.\ncaughq.org/presentations/turbo/Wiimote-Hacking.pdf and htt p://www.wiili.\norg/index.php/Motion_analysis.\n8.3.6. Cameras\nThe WiiMote has a unique feature not found on any other standard console \nHID—an infrared (IR) sensor. This sensor is essentially a low-resolution cam-\nera that records a two-dimension infrared image of whatever the WiiMote is \npointed at. The Wii comes with a “sensor bar” that sits on top of your televi-\nsion set and contains two infrared light emitt ing diodes (LEDs). In the image \nrecorded by the IR camera, these LEDs appear as two bright dots on an oth-\nerwise dark background. Image processing soft ware in the WiiMote analyzes \nthe image and isolates the location and size of the two dots. (Actually, it can \ndetect and transmit the locations and sizes of up to four dots.) This position \n8.3. Types of Inputs\n\n\n348 \n8. Human Interface Devices (HID)\nand size information can be read by the console via a Bluetooth wireless con-\nnection.\nThe position and orientation of the line segment formed by the two dots \ncan be used to determine the pitch, yaw, and roll of the WiiMote (as long as it \nis being pointed toward the sensor bar). By looking at the separation between \nthe dots, soft ware can also determine how close or far away the WiiMote is \nfrom the TV. Some soft ware also makes use of the sizes of the dots. This is il-\nlustrated in Figure 8.8.\nAnother popular camera device is Sony’s EyeToy for the PlayStation line \nof consoles, shown in Figure 8.9. This device is basically a high quality color \ncamera, which can be used for a wide range of applications. It can be used \nfor simple video conferencing, like any web cam. It could also conceivably be \nused much like the WiiMote’s IR camera, for position, orientation, and depth \nsensing. The gamut of possibilities for these kinds of advanced input devices \nhas only begun to be tapped by the gaming community.\n8.4. Types of Outputs\nHuman interface devices are primarily used to transmit inputs from the play-\ner to the game soft ware. However, some HIDs can also provide feedback to \nthe human player via various kinds of outputs.\n8.4.1. Rumble\nGame pads like the PlayStation’s DualShock line of controllers and the Xbox \nand Xbox 360 controllers have a rumble feature. This allows the controller to \nvibrate in the player’s hands, simulating the turbulence or impacts that the \nFigure 8.8.  The Wii sensor bar houses two infrared LEDs which produce two bright spots on \nthe image recorded by the WiiMote’s IR camera.\nFigure 8.9. Sony’s Eye-\nToy for the PlaySta-\ntion3.\n\n\n349 \n8.4. Types of Outputs\ncharacter in the game world might be experiencing. Vibrations are usually \nproduced by one or more motors, each of which rotates a slightly unbalanced \nweight at various speeds. The game can turn these motors on and oﬀ , and con-\ntrol their speeds to produce diﬀ erent tactile eﬀ ects in the player’s hands.\n8.4.2. Force-Feedback\nForce feedback is a technique in which an actuator on the HID is driven by \na motor in order to slightly resist the motion the human operator is trying to \nimpart to it. It is common in arcade driving games, where the steering wheel \nresists the player’s att empt to turn it, simulating diﬃ  cult driving conditions or \ntight turns. As with rumble, the game soft ware can typically turn the motor(s) \non and oﬀ , and can also control the strength and direction of the forces ap-\nplied to the actuator.\n8.4.3. Audio\nAudio is usually a stand-alone engine system. However, some HIDs provide \noutputs that can be utilized by the audio system. For example, the WiiMote \ncontains a small, low-quality speaker. The Xbox 360 controller has a headset \njack and can be used just like any USB audio device for both output (speak-\ners) and input (microphone). One common use of USB headsets is for multi-\nplayer games, in which human players can communicate with one another via \na voice over IP (VOIP) connection.\n8.4.4. Other Inputs and Outputs\nHuman interface devices may of course support many other kinds of inputs \nand outputs. On some older consoles like the Sega Dreamcast, the memory card \nslots were located on the game pad. The Xbox 360 game pad, the Sixaxis and \nDualShock 3, and the WiiMote all have four LEDs which can be illuminated by \ngame soft ware if desired. And of course specialized devices like musical instru-\nments, dance pads, etc. have their own particular kinds of inputs and outputs.\nInnovation is actively taking place in the ﬁ eld of human interfaces. Some \nof the most interesting areas today are gestural interfaces and thought-con-\ntrolled devices. We can certainly expect more innovation from console and \nHID manufacturers in years to come.\n8.5. Game Engine HID Systems\nMost game engines don’t use “raw” HID inputs directly. The data is usually \nmassaged in various ways to ensure that the inputs coming from the HID \n\n\n350 \n8. Human Interface Devices (HID)\ntranslate into smooth, pleasing, intuitive behaviors in-game. In addition, most \nengines introduce at least one additional level of indirection between the HID \nand the game in order to abstract HID inputs in various ways. For example, a \nbutt on-mapping table might be used to translate raw butt on inputs into logi-\ncal game actions, so that human players can re-assign the butt ons’ functions \nas they see ﬁ t. In this section, we’ll outline the typical requirements of a game \nengine HID system and then explore each one in some depth.\n8.5.1. Typical Requirements\n A game engine’s HID system usually provides some or all of the following \nfeatures:\nz dead zones,\nz analog signal ﬁ ltering,\nz event detection (e.g., butt on up, butt on down),\nz detection of butt on sequences and multibutt on combinations (known as \nchords),\nz gesture detection,\nz management of multiple HIDs for multiple players,\nz multiplatform HID support,\nz controller input re-mapping,\nz context-sensitive inputs,\nz the ability to temporarily disable certain inputs.\n8.5.2. Dead Zone\n A joystick, thumb stick, shoulder trigger, or any other analog axis produces \ninput values that range between a predeﬁ ned minimum and maximum value, \nwhich we’ll call Imin and Imax. When the control is not being touched, we would \nexpect it to produce a steady and clear “undisturbed” value, which we’ll call \nI0. The undisturbed value is usually numerically equal to zero, and it either \nlies half-way between Imin and Imax for a centered, two-way control like a joy-\nstick axis, or it coincides with Imin for a one-way control like a trigger.\nUnfortunately, because HIDs are analog devices by nature, the voltage pro-\nduced by the device is noisy, and the actual inputs we observe may ﬂ uctuate \nslightly around I0. The most common solution to this problem is to introduce a \nsmall dead zone around I0. The dead zone might be deﬁ ned as [I0 – δ , I0 + δ] for \na joy stick, or [I0  , I0 + δ] for a trigger. Any input values that are within the dead \nzone are simply clamped to I0. The dead zone must be wide enough to account \n\n\n351 \nfor the noisiest inputs generated by an undisturbed control, but small enough \nnot to interfere with the player’s sense of the HID’s responsiveness.\n8.5.3. Analog Signal Filtering\n Signal noise is a problem even when the controls are not within their dead \nzones. This noise can sometimes cause the in-game behaviors controlled by \nthe HID to appear jerky or unnatural. For this reason, many games ﬁ lter the \nraw inputs coming from the HID. A noise signal is usually of a high-frequency, \nrelative to the signal produced by the human player. Therefore, one solution \nis to pass the raw input data through a simple low-pass ﬁ lter , prior to it being \nused by the game.\nA discrete ﬁ rst-order low-pass ﬁ lter can be implemented by combining \nthe current unﬁ ltered input value with last frame’s ﬁ ltered input. If we denote \nthe sequence of unﬁ ltered inputs by the time-varying function u(t) and the \nﬁ ltered inputs by f(t), where t denotes time, then we can write\n \n \n(8.1)\nwhere the parameter a is determined by the frame duration Δt and a ﬁ ltering \nconstant RC (which is just the product of the resistance and the capacitance in \na traditional analog RC low-pass ﬁ lter circuit):\n \n \n(8.2)\nThis can be implemented trivially in C or C++ as follows, where it is assumed \nthe calling code will keep track of last frame’s ﬁ ltered input for use on the \nsubsequent frame. For more information, see htt p://en.wikipedia.org/wiki/\nLow-pass_ﬁ lter.\nF32 lowPassFilter(F32 unfilteredInput,\n                  F32 lastFramesFilteredInput,\n                  F32 rc, F32 dt)\n{\n \nF32 a = dt / (rc + dt);\n \nreturn (1 – a) * lastFramesFilteredInput\n         + a * unfilteredInput;\n}\nAnother way to ﬁ lter HID input data is to calculate a simple moving av-\nerage . For example, if we wish to average the input data over a 3/30 second \n(3 frame) interval, we simply store the raw input values in a 3-element circular \n8.5. Game Engine HID Systems\n( )\n(1\n) (\n)\n( ),\nf\n=\n−\n−Δ +\nt\nf t\nt\nt\na\nau\n.\nt\na\nRC\nt\nΔ\n=\n+Δ\n\n\n352 \n8. Human Interface Devices (HID)\nbuﬀ er. The ﬁ ltered input value is then the sum of the values in this array at \nany moment, divided by 3. There are a few minor details to account for when \nimplementing such a ﬁ lter. For example, we need to properly handle the ﬁ rst \ntwo frames of input, during which the 3-element array has not yet been ﬁ lled \nwith valid data. However, the implementation is not particularly complicated. \nThe code below shows one way to properly implement an N-element moving \naverage.\ntemplate< typename TYPE, int SIZE >\nclass MovingAverage\n{\n TYPE \n m_samples[SIZE];\n TYPE \n m_sum;\n U32 \n  m_curSample;\n U32 \n  m_sampleCount;\npublic:\n \nMovingAverage() :\n \n  m_sum(static_cast<TYPE>(0)),\n \n  m_curSample(0),\n \n  m_sampleCount(0)\n {\n }\n void \naddSample(TYPE data)\n {\n \n  \nif (m_sampleCount == SIZE)\n \n  {\n \n  \n \nm_sum -= m_samples[m_curSample];\n \n  }\n \n  else\n \n  {\n  \n \n ++m_sampleCount;\n \n  }\n \n  \nm_samples[m_curSample] = data;\n \n  \nm_sum += data;\n \n  ++m_curSample;\n \n  \nif (m_curSample >= SIZE)\n \n  {\n \n  \n \nm_curSample = 0;\n \n  }\n \n }\n F32 \ngetCurrentAverage() const\n {\n\n\n353 \n \n  \nif (m_sampleCount != 0)\n \n  {\n  \n \n return \nstatic_cast<F32>(m_sum)\n \n  \n \n  / static_cast<F32>(m_sampleCount);\n \n  }\n \n  return 0.0f;\n }\n};\n8.5.4. Detecting Input Events\n The low-level HID interface typically provides the game with the current \nstates of the device’s various inputs. However, games are oft en interested \nin detecting events, such as changes in state, rather than just inspecting the \ncurrent state each frame. The most common HID events are probably butt on \ndown (pressed) and butt on up (released), but of course we can detect other \nkinds of events as well.\n8.5.4.1. Button Up and Button Down\nLet’s assume for the moment that our butt ons’ input bits are 0 when not pressed \nand 1 when pressed. The easiest way to detect a change in butt on state is to \nkeep track of the butt ons’ state bits as observed last frame and compare them \nto the state bits observed this frame. If they diﬀ er, we know an event occurred. \nThe current state of each butt on tells us whether the event is a butt on-up or a \nbutt on-down.\nWe can use simple bit-wise operators to detect butt on-down and but-\nton-up events. Given a 32-bit word buttonStates, containing the current \nstate bits of up to 32 butt ons, we want to generate two new 32-bit words: \none for butt on-down events which we’ll call buttonDowns and one for \nbutt on-up events which we’ll call buttonUps. In both cases, the bit corre-\nsponding to each butt on will be 0 if the event has not occurred this frame \nand 1 if it has. To implement this, we also need last frame’s butt on states, \nprevButtonStates.\nThe exclusive OR (XOR) operator produces a 0 if its two inputs are iden-\ntical and a 1 if they diﬀ er. So if we apply the XOR operator to the previous \nand current butt on state words, we’ll get 1’s only for butt ons whose states \nhave changed between last frame and this frame. To determine whether the \nevent is a butt on-up or a butt on-down, we need to look at the current state \nof each butt on. Any butt on whose state has changed that is currently down \ngenerates a butt on-down event, and vice-versa for butt on-up events. The fol-\nlowing code applies these ideas in order to generate our two butt on event \nwords:\n8.5. Game Engine HID Systems\n\n\n354 \n8. Human Interface Devices (HID)\nclass ButtonState\n{\n \nU32 m_buttonStates;  \n// current frame’s button   \n \n  \n       // \nstates\n \nU32 m_prevButtonStates; // previous frame’s states\n U32 \nm_buttonDowns;  \n// 1 = button pressed this  \n \n  \n       // \nframe\n U32 \nm_buttonUps;  \n \n// 1 = button released this   \n  \n       // \nframe\n void \nDetectButtonUpDownEvents()\n {\n \n  // Assuming that m_buttonStates and \n \n  // m_prevButtonStates are valid, generate \n \n  // m_buttonDowns and m_buttonUps.\n \n  // First determine which bits have changed via  \n \n \n  // XOR.\n \n  U32 buttonChanges = m_buttonStates \n  \n       \n   \n^ m_prevButtonStates;\n \n  // Now use AND to mask off only the bits that are  \n \n  // DOWN. \n \n  \nm_buttonDowns = buttonChanges & m_buttonStates;\n \n  // Use AND-NOT to mask off only the bits that are   \n \n  // UP.\n \n  \nm_buttonUps = buttonChanges & (~m_buttonStates);\n }\n \n// ...\n};\n8.5.4.2. Chords\nA chord is a group of butt ons that, when pressed at the same time, produce a \nunique behavior in the game. Here are a few examples:\nz Super Mario Galaxy’s start-up screen requires you to press the A and B \nbutt ons on the WiiMote together in order to start a new game.\nz Pressing the 1 and 2 butt ons on the WiiMote at the same time put it into \nBluetooth discovery mode (no matt er what game you’re playing).\nz The “grapple” move in many ﬁ ghting games is triggered by a two-but-\nton combination.\n\n\n355 \nz For development purposes, holding down both the left  and right trig-\ngers on the DualShock 3 in Uncharted: Drake’s Fortune allows the player \ncharacter to ﬂ y anywhere in the game world, with collisions turned oﬀ . \n(Sorry, this doesn’t work in the shipping game!) Many games have a \ncheat like this to make development easier. (It may or may not be trig-\ngered by a chord, of course.) It is called no-clip mode in the Quake engine, \nbecause the character’s collision volume is not clipped to the valid play-\nable area of the world. Other engines use diﬀ erent terminology.\nDetecting chords is quite simple in principle: We merely watch the states \nof two or more butt ons and only perform the requested operation when all of \nthem are down.\nThere are some subtleties to account for, however. For one thing, if the \nchord includes a butt on or butt ons that have other purposes in the game, we \nmust take care not to perform both the actions of the individual butt ons and \nthe action of chord when it is pressed. This is usually done by including a \ncheck that the other butt ons in the chord are not down when detecting the \nindividual butt on-presses.\nAnother ﬂ y in the ointment is that humans aren’t perfect, and they oft en \npress one or more of the butt ons in the chord slightly earlier than the rest. So our \nchord-detection code must be robust to the possibility that we’ll observe one or \nmore individual butt ons on frame i and the rest of the chord on frame i + 1 (or \neven multiple frames later). There are a number of ways to handle this:\nz You can design your butt on inputs such that a chord always does \nthe actions of the individual butt ons plus some additional action. For \nexample, if pressing L1 ﬁ res the primary weapon and L2 lobs a grenade, \nperhaps the L1  +  L2 chord could ﬁ re the primary weapon, lob a grenade, \nand send out an energy wave that doubles the damage done by these \nweapons. That way, whether or not the individual butt ons are detected \nbefore the chord or not, the behavior will be identical from the point of \nview of the player.\nz You can introduce a delay between when an individual butt on-down \nevent is seen and when it “counts” as a valid game event. During the \ndelay period (say 2 or 3 frames), if a chord is detected, then it takes \nprecedence over the individual butt on-down events. This gives the \nhuman player some leeway in performing the chord.\nz You can detect the chord when the butt ons are pressed, but wait to \ntrigger the eﬀ ect until the butt ons are released again.\nz You can begin the single-butt on move immediately and allow it to be \npreempted by the chord move.\n8.5. Game Engine HID Systems\n\n\n356 \n8. Human Interface Devices (HID)\n8.5.4.3. Sequences and Gesture Detection\n The idea of introducing a delay between when a butt on actually goes down \nand when it really “counts” as down is a special case of gesture detection. A \ngesture is a sequence of actions performed via a HID by the human player \nover a period of time. For example, in a ﬁ ghting game or brawler, we might \nwant to detect a sequence of butt on presses, such as A-B-A. We can extend this \nidea to non-butt on inputs as well. For example, A-B-A-Left -Right-Left , where \nthe latt er three actions are side-to-side motions of one of the thumb sticks on \nthe game pad. Usually a sequence or gesture is only considered to be valid if \nit is performed within some maximum time-frame. So a rapid A-B-A within a \nquarter of a second might “count,” but a slow A-B-A performed over a second \nor two might not.\nGesture detection is generally implemented by keeping a brief history of \nthe HID actions performed by the player. When the ﬁ rst component of the \ngesture is detected, it is stored in the history buﬀ er, along with a time stamp \nindicating when it occurred. As each subsequent component is detected, the \ntime between it and the previous component is checked. If it is within the \nallowable time window, it too is added to the history buﬀ er. If the entire se-\nquence is completed within the allott ed time (i.e., the history buﬀ er is ﬁ lled), \nan event is generated telling the rest of the game engine that the gesture \nhas occurred. However, if any non-valid intervening inputs are detected, or \nif any component of the gesture occurs outside of its valid time window, \nthe entire history buﬀ er is reset, and the player must start the gesture over \nagain.\nLet’s look at three concrete examples, so we can really understand how \nthis works.\nRapid Button Tapping\nMany games require the user to tap a butt on rapidly in order to perform an ac-\ntion. The frequency of the butt on presses may or may not translate into some \nquantity in the game, such as the speed with which the player character runs \nor performs some other action. The frequency is usually also used to deﬁ ne \nthe validity of the gesture—if the frequency drops below some minimum val-\nue, the gesture is no longer considered valid.\nWe can detect the frequency of a butt on press by simply keeping track of \nthe last time we saw a butt on-down event for the butt on in question. We’ll call \nthis Tlast  . The frequency f is then just the inverse of the time interval between \npresses (ΔT = Tcur – Tlast  and f = 1/ΔT). Every time we detect a new butt on-down \nevent, we calculate a new frequency f. To implement a minimum valid fre-\nquency, we simply check f against the minimum frequency fmin (or we can just \n\n\n357 \ncheck ΔT against the maximum period ΔTmax = 1/fmin directly). If this threshold \nis satisiﬁ ed, we update the value of Tlast   , and the gesture is considered to be \non-going. If the threshold is not satisﬁ ed, we simply don’t update Tlast  . The \ngesture will be considered invalid until a new pair of rapid-enough butt on-\ndown events occurs. This is illustrated by the following pseudocode:\nclass ButtonTapDetector\n{\n U32 \n \nm_buttonMask; // which button to observe (bit   \n        \n// mask)\n F32 \n \nm_dtMax;      // max allowed time between   \n \n        \n// presses\n F32 \n \nm_tLast;      // last button-down event, in  \n \n        \n// seconds\npublic:\n \n// Construct an object that detects rapid tapping of   \n \n// the given button (identified by an index).\nButtonTapDetector(U32 buttonId, F32 dtMax) :\n \n \nm_buttonMask(1U << buttonId),\n  m_dtMax(dtMax),\n \n \nm_tLast(CurrentTime() – dtMax) // start out  \n \n \n            \n// invalid\n {\n }\n \n// Call this at any time to query whether or not the\n \n// gesture is currently being performed.\n void \nIsGestureValid() const\n {\n \n \nF32 t = CurrentTime();\n \n \nF32 dt = t – m_tLast;\n \n \nreturn (dt < m_dtMax);\n }\n \n// Call this once per frame.\n void \nUpdate()\n {\n  if \n(ButtonsJustWentDown(m_buttonMask))\n  {\n   m_tLast \n= CurrentTime();\n  }\n }\n};\nIn the above code excerpt, we assume that each butt on is identiﬁ ed by a \nunique id. The id is really just an index, ranging from 0 to N – 1 (where N is \nthe number of butt ons on the HID in question). We convert the butt on id to a \n8.5. Game Engine HID Systems\n\n\n358 \n8. Human Interface Devices (HID)\nbit mask by shift ing an unsigned 1 bit to the left  by an amount equaling the \nbutt on’s index (1U << buttonId ). The function ButtonsJustWentDown()\nreturns a non-zero value if any one of the butt ons speciﬁ ed by the given bit \nmask just went down this frame. Here, we’re only checking for a single but-\nton-down event, but we can and will use this same function later to check for \nmultiple simultaneous butt on-down events.\nMultibutton Sequence\nLet’s say we want to detect the sequence A-B-A, performed within at most one \nsecond. We can detect this butt on sequence as follows: We maintain a variable \nthat tracks which butt on in the sequence we’re currently looking for. If we de-\nﬁ ne the sequence with an array of butt on ids (e.g., aButtons[3] = {A, B, \nA}), then our variable is just an index i into this array. It starts out initialized \nto the ﬁ rst butt on in the sequence, i = 0. We also maintain a start time for the \nentire sequence, Tstart  , much as we did in the rapid butt on-pressing example.\nThe logic goes like this: Whenever we see a butt on-down event that match-\nes the butt on we’re currently looking for, we check its time stamp against the \nstart time of the entire sequence, Tstart  . If it occurred within the valid time \nwindow, we advance the current butt on to the next butt on in the sequence; \nfor the ﬁ rst butt on in the sequence only (i = 0), we also update Tstart  . If we see \na butt on-down event that doesn’t match the next butt on in the sequence, or \nif the time delta has grown too large, we reset the butt on index i back to the \nbeginning of the sequence and set Tstart to some invalid value (such as 0). This \nis illustrated by the code below.\nclass ButtonSequenceDetector\n{\n \nU32* \nm_aButtonIds;  // sequence of buttons to watch for \n U32 \n m_buttonCount; \n// number of buttons in sequence\n F32 \n \nm_dtMax;       // max time for entire sequence\n U32 \n \nm_iButton; \n  // next button to watch for in seq.\n F32 \n \nm_tStart;      // start time of sequence, in   \n        \n    // \nseconds\npublic:\n \n// Construct an object that detects the given button\n \n// sequence. When the sequence is successfully   \n   \n \n// detected, the given event is broadcast, so the  \n \n// rest of the game can respond in an appropriate way.\nButtonSequenceDetector(U32* aButtonIds, \n        \n  U32 buttonCount,\n \n                       F32 dtMax, \n        \n  EventId eventIdToSend) :\n  m_aButtonIds(aButtonIds),\n  m_buttonCount(buttonCount),\n\n\n359 \n  m_dtMax(dtMax),\n \n \nm_eventId(eventIdToSend), // event to send when   \n          \n    // complete\n \n \nm_iButton(0),             // start of sequence\n \n \nm_tStart(0)               // initial value  \n          \n    // irrelevant\n {\n }\n \n// Call this once per frame.\n void \nUpdate()\n {\n \n \nASSERT(m_iButton < m_buttonCount);\n \n \n// Determine which button we’re expecting next, as\n \n \n// a bit mask (shift a 1 up to the correct bit  \n \n  // \nindex).\n  U32 \nbuttonMask = (1U << m_aButtonId[m_iButton]);\n \n \n// If any button OTHER than the expected button   \n \n \n// just went down, invalidate the sequence. (Use   \n \n \n// the bitwise NOT operator to check for all other  \n  // \nbuttons.)\n  if \n(ButtonsJustWentDown(~buttonMask))\n  {\n \n \n \nm_iButton = 0; // reset\n  }\n \n \n// Otherwise, if the expected button just went  \n \n \n// down, check dt and update our state appropriately.\n \n \nelse if (ButtonsJustWentDown(buttonMask))\n  {\n \n \n \nif (m_iButton == 0)\n   {\n \n \n \n \n// This is the first button in the   \n \n \n    // \nsequence.\n    m_tStart \n= CurrentTime();\n \n \n \n \n++m_iButton; // advance to next button\n   }\n   else\n   {\n \n \n \n \nF32 dt = CurrentTime() – m_tStart;\n    if \n(dt < m_dtMax)\n    {\n     // \nSequence is still valid.\n8.5. Game Engine HID Systems\n\n\n360 \n8. Human Interface Devices (HID)\n \n \n \n \n \n++m_iButton; // advance to next button\n     // \nIs the sequence complete?\n     if \n(m_iButton == m_buttonCount)\n     {\nBroadcastEvent(m_eventId);\n      m_iButton \n= 0; // reset\n     }\n    }\n    else\n    {\n     // \nSorry, not fast enough.\n     m_iButton \n= 0; // reset\n    }\n   }\n  }\n }\n};\nThumb Stick Rotation\nAs an example of a more-complex gesture, let’s see how we might detect when \nthe player is rotating the left  thumb stick in a clockwise circle. We can detect \nthis quite easily by dividing the two-dimensional range of possible stick po-\nsitions into quadrants, as shown in Figure 8.10. In a clockwise rotation, the \nstick passes through the upper-left  quadrant, then the upper-right, then the \nlower-right, and ﬁ nally the lower-left . We can treat each of these cases like a \nbutt on press and detect a full rotation with a slightly modiﬁ ed version of the \nsequence detection code shown above. We’ll leave this one as an exercise for \nthe reader. Try it!\nx\ny\nUL\nUR\nLL\nLR\nFigure 8.10.  Detecting circular rotations of the stick by dividing the 2D range of stick inputs \ninto quadrants.\n\n\n361 \n8.5.5. Managing Multiple HIDs for Multiple Players\n Most game machines allow two or more HIDs to be att ached for multiplayer \ngames. The engine must keep track of which devices are currently att ached \nand route each one’s inputs to the appropriate player in the game. This implies \nthat we need some way of mapping controllers to players. This might be as \nsimple as a one-to-one mapping between controller index and player index, \nor it might be something more sophisticated, such as assigning controllers to \nplayers at the time the user hits the Start butt on.\nEven in a single-player game with only one HID, the engine needs to be \nrobust to various exceptional conditions, such as the controller being acciden-\ntally unplugged or running out of batt eries. When a controller’s connection \nis lost, most games pause gameplay, display a message, and wait for the con-\ntroller to be reconnected. Some multiplayer games suspend or temporarily \nremove the avatar corresponding to a removed controller, but allow the other \nplayers to continue playing the game; the removed/suspended avatar might \nreactivate when the controller is reconnected.\nOn systems with batt ery-operated HIDs, the game or the operating sys-\ntem is responsible for detecting low-batt ery conditions. In response, the play-\ner is usually warned in some way, for example via an unobtrusive on-screen \nmessage and/or a sound eﬀ ect.\n8.5.6. Cross-Platform HID Systems\n Many game engines are cross-platform. One way to handle HID inputs and \noutputs in such an engine would be to sprinkle conditional compilation di-\nrectives all over the code, wherever interactions with the HID take place, as \nshown below. This is clearly not an ideal solution, but it does work.\n#if TARGET_XBOX360\n \nif (ButtonsJustWentDown(XB360_BUTTONMASK_A))\n#elif TARGET_PS3\n \nif (ButtonsJustWentDown(PS3_BUTTONMASK_TRIANGLE))\n#elif TARGET_WII\n \nif (ButtonsJustWentDown(WII_BUTTONMASK_A))\n#endif\n{\n \n// do something...\n}\nA bett er solution is to provide some kind of hardware abstraction layer, there-\nby insulating the game code from hardware-speciﬁ c details.\nIf we’re lucky, we can abstract most of the diﬀ erences beween the HIDs \non the diﬀ erent platforms by a judicious choice of abstract butt on and axis \n8.5. Game Engine HID Systems\n\n\n362 \n8. Human Interface Devices (HID)\nids. For example, if our game is to ship on Xbox 360 and PS3, the layout \nof the controls (butt ons, axes and triggers) on these two joypads are almost \nidentical. The controls have diﬀ erent ids on each platform, but we can come \nup with generic control ids that cover both types of joypad quite easily. For \nexample:\nenum AbstractControlIndex\n{\n \n// Start and back buttons\n AINDEX_START, \n \n \n   // Xbox 360 Start, PS3 Start\n \nAINDEX_BACK_PAUSE,    // Xbox 360 Back, PS3 Pause\n \n// Left D-pad\n AINDEX_LPAD_DOWN,\n AINDEX_LPAD_UP,\n AINDEX_LPAD_LEFT,\n AINDEX_LPAD_RIGHT,\n \n// Right \"pad\" of four buttons\n \nAINDEX_RPAD_DOWN, \n   // Xbox 360 A, PS3 X\n \nAINDEX_RPAD_UP,  \n   // Xbox 360 Y, PS3 Triangle\n \nAINDEX_RPAD_LEFT, \n   // Xbox 360 X, PS3 Square\n \nAINDEX_RPAD_RIGHT,    // Xbox 360 B, PS3 Circle\n \n// Left and right thumb stick buttons\n \nAINDEX_LSTICK_BUTTON,  // Xbox 360 LThumb, PS3 L3,\n        \n  // Xbox white\n \nAINDEX_RSTICK_BUTTON,  // Xbox 360 RThumb, PS3 R3,\n        \n  // Xbox black\n \n// Left and right shoulder buttons\n \nAINDEX_LSHOULDER, \n \n// Xbox 360 L shoulder, PS3 L1\n \nAINDEX_RSHOULDER, \n \n// Xbox 360 R shoulder, PS3 R1\n \n// Left thumb stick axes\n AINDEX_LSTICK_X,\n AINDEX_LSTICK_Y,\n \n// Right thumb stick axes\n AINDEX_RSTICK_X,\n AINDEX_RSTICK_Y,\n \n// Left and right trigger axes\n \nAINDEX_LTRIGGER,  \n   // Xbox 360 –Z, PS3 L2\n \nAINDEX_RTRIGGER,  \n   // Xbox 360 +Z, PS3 R2\n};\n",
      "page_number": 367,
      "chapter_number": 19,
      "summary": "This chapter covers segment 19 (pages 367-384). Key topics include game, gaming, and input.",
      "keywords": [
        "Game Engine HID",
        "Engine HID Systems",
        "butt",
        "HID",
        "Human Interface Devices",
        "butt ons",
        "Engine HID",
        "game",
        "HID Systems",
        "inputs",
        "game engine",
        "Interface Devices",
        "butt on-down event",
        "butt on-down",
        "HID inputs"
      ],
      "concepts": [
        "game",
        "gaming",
        "input",
        "hid",
        "bit",
        "bits",
        "butt",
        "detect",
        "button",
        "controller"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 3",
          "chapter": 11,
          "title": "Segment 11 (pages 111-118)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 26,
          "title": "Segment 26 (pages 243-252)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 25,
          "title": "Segment 25 (pages 227-237)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 12,
          "title": "Segment 12 (pages 101-108)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 385-402)",
      "start_page": 385,
      "end_page": 402,
      "detection_method": "synthetic",
      "content": "363 \nOur abstraction layer can translate between the raw control ids on the cur-\nrent target hardware into our abstract control indices . For example, whenever \nwe read the state of the butt ons into a 32-bit word, we can perform a bit-swiz-\nzling operation that rearranges the bits into the proper order to correspond to \nour abstract indices. Analog inputs can likewise be shuﬄ  ed around into the \nproper order.\nIn performing the mapping between physical and abstract controls, we’ll \nsometimes need to get a bit clever. For example, on the Xbox, the left  and right \ntriggers act as a single axis, producing negative values when the left  trigger is \npressed, zero when neither is trigger is pressed, and positive values when the \nright trigger is pressed. To match the behavior of the PlayStation’s DualShock \ncontroller, we might want to separate this axis into two distinct axes on the \nXbox, scaling the values appropriately so the range of valid values is the same \non all platforms.\nThis is certainly not the only way to handle HID I/O in a multiplatform \nengine. We might want to take a more functional approach, for example, by \nnaming our abstract controls according to their function in the game, rather \nthan their physical locations on the joypad. We might introduce higher-level \nfunctions that detect abstract gestures, with custom detection code on each \nplatform, or we might just bite the bullet and write platform-speciﬁ c versions \nof all of the game code that requires HID I/O. The possibilities are numerous, \nbut virtually all cross-platform game engines insulate the game from hard-\nware details in some manner.\n8.5.7. Input Re-Mapping\n Many games allow the player some degree of choice with regard to the func-\ntionality of the various controls on the physical HID. A common option is \nthe sense of the vertical axis of the right thumb stick for camera control in a \nconsole game. Some folks like to push forward on the stick to angle the camera \nup, while others like an inverted control scheme, where pulling back on the \nstick angles the camera up (much like an airplane control stick). Other games \nallow the player to select between two or more predeﬁ ned butt on mappings. \nSome PC games allow the user full control over the functions of individual \nkeys on the keyboard, the mouse butt ons, and the mouse wheel, plus a choice \nbetween various control schemes for the two mouse axes.\nTo implement this, we turn to a favorite saying of an old professor of \nmine, Professor Jay Black of the University of Waterloo, “Every problem in \ncomputer science can be solved with a level of indirection.” We assign each \nfunction in the game a unique id and then provide a simple table which maps \neach physical or abstract control index to a logical function in the game. When-\n8.5. Game Engine HID Systems\n\n\n364 \n8. Human Interface Devices (HID)\never the game wishes to determine whether a particular logical game function \nshould be activated, it looks up the corresponding abstract or physical control \nid in the table and then reads the state of that control. To change the mapping, \nwe can either swap out the entire table wholesale, or we can allow the user to \nedit individual entries in the table.\nWe’re glossing over a few details here. For one thing, diﬀ erent controls \nproduce diﬀ erent kinds of inputs. Analog axes may produce values ranging \nfrom –32,768 to 32,767, or from 0 to 255, or some other range. The states of \nall the digital butt ons on a HID are usually packed into a single machine \nword. Therefore, we must be careful to only permit control mappings that \nmake sense. We cannot use a butt on as the control for a logical game func-\ntion that requires an axis, for example. One way around this problem is to \nnormalize all of the inputs. For example, we could re-scale the inputs from \nall analog axes and butt ons into the range [0, 1]. This isn’t quite as helpful as \nyou might at ﬁ rst think, because some axes are inherently bidirectional (like \na joy stick) while others are unidirectional (like a trigger). But if we group \nour controls into a few classes, we can normalize the inputs within those \nclasses, and permit remapping only within compatible classes. A reason-\nable set of classes for a standard console joypad and their normalized input \nvalues might be:\nz Digital butt ons. States are packed into a 32-bit word, one bit per butt on.\nz Unidirectional absolute axes (e.g., triggers, analog butt ons). Produce ﬂ oat-\ning-point input values in the range [0, 1].\nz Bidirectional absolute axes (e.g., joy sticks). Produce ﬂ oating-point input \nvalues in the range [–1, 1].\nz Relative axes (e.g., mouse axes, wheels, track balls). Produce ﬂ oating-point \ninput values in the range [–1, 1] , where ±1 represents the maximum \nrelative oﬀ set possible within a single game frame (i.e., during a period \nof 1/30 or 1/60 of a second).\n8.5.8. Context-Sensitive Controls\n In many games, a single physical control can have diﬀ erent functions, depend-\ning on context. A simple example is the ubiquitous “use” butt on. If pressed \nwhile standing in front of a door, the “use” butt on might cause the character to \nopen the door. If it is pressed while standing near an object, it might cause the \nplayer character to pick up the object, and so on. Another common example is \na modal control scheme. When the player is walking around, the controls are \nused to navigate and control the camera. When the player is riding a vehicle, \nthe controls are used to steer the vehicle, and the camera controls might be \ndiﬀ erent as well.\n\n\n365 \nContext-sensitive controls are reasonably straightforward to imple-\nment via a state machine. Depending on what state we’re in, a particu-\nlar HID control may have a diﬀ erent purpose. The tricky part is deciding \nwhat state to be in. For example, when the context-sensitive “use” butt on \nis pressed, the player might be standing at a point equidistant between a \nweapon and a health pack, facing the center point between them. Which \nobject do we use in this case? Some games implement a priority system to \nbreak ties like this. Perhaps the weapon has a higher weight than the health \npack, so it would “win” in this example. Implementing context-sensitive \ncontrols isn’t rocket science, but it invariably requires lots of trial-and-error \nto get it feeling and behaving just right. Plan on lots of iteration and focus \ntesting!\nAnother related concept is that of control ownership. Certain controls on \nthe HID might be “owned” by diﬀ erent parts of the game. For example, some \ninputs are for player control, some for camera control, and still others are for \nuse by the game’s wrapper and menu system (pausing the game, etc.) Some \ngame engines introduce the concept of a logical device, which is composed of \nonly a subset of the inputs on the physical device. One logical device might \nbe used for player control, while another is used by the camera system, and \nanother by the menu system.\n8.5.9. Disabling Inputs\n In most games, it is sometimes necessary to disallow the player from control-\nling his or her character. For example, when the player character is involved in \nan in-game cinematic, we might want to disable all player controls temporar-\nily; or when the player is walking through a narrow doorway, we might want \nto temporarily disable free camera rotation.\nOne rather heavy-handed approach is to use a bit mask to disable indi-\nvidual controls on the input device itself. Whenever the control is read, the \ndisable mask is checked, and if the corresponding bit is set, a neutral or zero \nvalue is returned instead of the actual value read from the device. We must be \nparticularly cautious when disabling controls, however. If we forget to reset \nthe disable mask, the game can get itself into a state where the player looses \nall control forever, and must restart the game. It’s important to check our logic \ncarefully, and it’s also a good idea to put in some fail-safe mechanisms to en-\nsure that the disable mask is cleared at certain key times, such as whenever the \nplayer dies and re-spawns.\nDisabling a HID input masks it for all possible clients, which can \nbe overly limiting. A better approach is probably to put the logic for \ndisabling specific player actions or camera behaviors directly into the \n8.5. Game Engine HID Systems\n\n\n366 \n8. Human Interface Devices (HID)\nplayer or camera code itself. That way, if the camera decides to ignore \nthe deflection of the right thumb stick, for example, other game engine \nsystems still have the freedom to read the state of that stick for other \npurposes.\n8.6. Human Interface Devices in Practice\nCorrect and smooth handling of human interface devices is an important part \nof any good game. Conceptually speaking, HIDs may seem quite straightfor-\nward. However, there can be quite a few “gotchas” to deal with, including \nvariations between diﬀ erent physical input devices, proper implementation \nof low-pass ﬁ ltering, bug-free handling of control scheme mappings, achiev-\ning just the right “feel” in your joypad rumble, limitations imposed by console \nmanufacturers via their technical requirements checklists (TRCs), and the list \ngoes on. A game team should expect to devote a non-trivial amount of time \nand engineering bandwidth to a careful and complete implementation of the \nhuman interface device system. This is extremely important because the HID \nsystem forms the underpinnings of your game’s most precious resource—its \nplayer mechanics.\n\n\n367\n9\n \nTools for Debugging \nand Development\nD\neveloping game soft ware is a complex, intricate, math-intensive, and er-\nror-prone business. So it should be no surprise that virtually every pro-\nfessional game team builds a suite of tools for themselves, in order to make \nthe game development process easier and less error-prone. In this chapter, \nwe’ll take a look at the development and debugging tools most oft en found in \nprofessional-grade game engines.\n9.1. \nLogging and Tracing\n Remember when you wrote your ﬁ rst program in BASIC or Pascal? (OK, may-\nbe you don’t. If you’re signiﬁ cantly younger than me—and there’s a prett y \ngood chance of that—you probably wrote your ﬁ rst program in Java, or maybe \nPython or Lua.) In any case, you probably remember how you debugged your \nprograms back then. You know, back when you thought a debugger was one of \nthose glowing blue insect zapper things? You probably used print statements \nto dump out the internal state of your program. C/C++ programmers call this \nprintf debugging (aft er the standard C library function, printf()).\nIt turns out that printf debugging is still a perfectly valid thing to do—even \nif you know that a debugger isn’t a device for frying hapless insects at night. \nEspecially in real-time programming, it can be diﬃ  cult to trace certain kinds \n\n\n368 \n9. Tools for Debugging and Development\nof bugs using breakpoints and watch windows. Some bugs are timing-depen-\ndent; they only happen when the program is running at full speed. Other bugs \nare caused by a complex sequence of events too long and intricate to trace \nmanually one-by-one. In these situations, the most powerful debugging tool \nis oft en a sequence of print statements.\nEvery game platform has some kind of console or teletype (TTY) output \ndevice. Here are some examples:\nz In a console application writt en in C/C++, running under Linux or \nWin32, you can produce output in the console by printing to stdout or \nstderr via printf(), fprintf(), or STL’s iostream interface.\nz Unfortunately, printf() and iostream don’t work if your game is built \nas a windowed application under Win32, because there’s no console \nin which to display the output. However, if you’re running under the \nVisual Studio debugger, it provides a debug console to which you can \nprint via the Win32 function OutputDebugString().\nz On the PLAYSTATION 3, an application known as the Target Manager \nruns on your PC and allows you to launch programs on the console. \nThe Target Manager includes a set of TTY output windows to which \nmessages can be printed by the game engine.\nSo printing out information for debugging purposes is almost always as easy \nas adding calls to printf() throughout your code. However, most game en-\ngines go a bit farther than this. In the following sections, we’ll investigate the \nkinds of printing facilities most game engines provide.\n9.1.1. \nFormatted Output with OutputDebugString()\nThe Win32 function OutputDebugString() is great for printing debug-\nging information to Visual Studio’s Debug Output window. However, unlike \nprintf(), OutputDebugString() does not support formatt ed output—it can \nonly print raw strings in the form of char arrays. For this reason, most Windows \ngame engines wrap OutputDebugString() in a custom function, like this:\n#include <stdio.h>     // for va_list et al\n#ifndef WIN32_LEAN_AND_MEAN\n#define WIN32_LEAN_AND_MEAN 1\n#endif\n#include <windows.h>   // for OutputDebugString()\nint VDebugPrintF(const char* format, va_list argList)\n{\n \nconst U32 MAX_CHARS = 1023;\n \nstatic char s_buffer[MAX_CHARS + 1];\n\n\n369 \n9.1. Logging and Tracing\n \nint charsWritten\n \n \n= vsnprintf(s_buffer, MAX_CHARS, format, argList);\n \ns_buffer[MAX_CHARS] = ‘\\0’; // be sure to \n          \n  // NIL-terminate\n \n// Now that we have a formatted string, call the  \n \n// Win32 API.\nOutputDebugString(s_buffer);\n \nreturn charsWritten;\n}\nint DebugPrintF(const char* format, ...)\n{\n \nva_list argList;\nva_start(argList, format);\n \nint charsWritten = VDebugPrintF(format, argList);\nva_end(argList);\n \nreturn charsWritten;\n}\nNotice that two functions are implemented: DebugPrintF()takes a \nvariable-length argument list (speciﬁ ed via the ellipsis, …), while VDebug\nPrintF()takes a va_list argument. This is done so that programmers can \nbuild additional printing functions in terms of VDebugPrintF(). (It’s impos-\nsible to pass ellipses from one function to another, but it is possible to pass \nva_lists around.)\n9.1.2. Verbosity\n Once you’ve gone to the trouble of adding a bunch of print statements to your \ncode in strategically chosen locations, it’s nice to be able to leave them there, \nin case they’re needed again later. To permit this, most engines provide some \nkind of mechanism for controlling the level of verbosity via the command-line, \nor dynamically at runtime. When the verbosity level is at its minimum value \n(usually zero), only critical error messages are printed. When the verbosity is \nhigher, more of the print statements embedded in the code start to contribute \nto the output.\nThe simplest way to implement this is to store the current verbosity level \nin a global integer variable, perhaps called g_verbosity. We then provide a \nVerboseDebugPrintF() function whose ﬁ rst argument is the verbosity level \nat or above which the message will be printed. This function could be imple-\nmented as follows:\n\n\n370 \n9. Tools for Debugging and Development\nint g_verbosity = 0;\nvoid VerboseDebugPrintF(int verbosity,\n \nconst char* format, ...)\n{\n \n// Only print when the global verbosity level is\n \n// high enough.\n \nif (g_verbosity >= verbosity)\n {\n \n  va_list argList;\n \n  va_start(argList, format);\n \n  \nVDebugPrintF(format, argList);\n \n  va_end(argList);\n }\n}\n9.1.3. Channels\n It’s also extremely useful to be able to categorize your debug output into chan-\nnels. One channel might contain messages from the animation system, while \nanother might be used to print messages from the physics system, for exam-\nple.\nOn some platforms, like the PLAYSTATION 3, debug output can be di-\nrected to one of 14 distinct TTY windows. In addition, messages are mirrored \nto a special TTY window that contains the output from all of the other 14 \nwindows. This makes it very easy for a developer to focus in on only the mes-\nsages he or she wants to see. When working on an animation problem, one \ncan simply ﬂ ip to the animation TTY and ignore all the other output. When \nworking on a general problem of unknown origin, the “all” TTY can be con-\nsulted for clues.\nOther platforms like Windows provide only a single debug output con-\nsole. However, even on these systems it can be helpful to divide your output \ninto channels. The output from each channel might be assigned a diﬀ erent \ncolor. You might also implement ﬁ lters, which can be turned on and oﬀ  at \nruntime, and restrict output to only a speciﬁ ed channel or set of channels. \nIn this model, if a developer is debugging an animation-related problem, for \nexample, he or she can simply ﬁ lter out all of the channels except the anima-\ntion channel.\nA channel-based debug output system can be implemented quite easily \nby adding an additional channel argument to our debug printing function. \nChannels might be numbered, or bett er, assigned symbolic values via a C/C++ \nenum declaration. Or channels might be named using a string or hashed string \n\n\n371 \nid. The printing function can simply consult the list of active channels and \nonly print the message if the speciﬁ ed channel is among them.\nIf you don’t have more than 32 or 64 channels, it can be helpful to identify \nthe channels via a 32- or 64-bit mask. This makes implementing a channel \nﬁ lter as easy as specifying a single integer. When a bit in the mask is 1, the cor-\nresponding channel is active; when the bit is 0, the channel is muted.\n9.1.4. Mirroring Output to a File\n It’s a good idea to mirror all debug output to one or more log ﬁ les (e.g., one \nﬁ le per channel). This permits problems to be diagnosed aft er the fact. Ideally \nthe log ﬁ le(s) should contain all of the debug output, independent of the cur-\nrent verbosity level and active channels mask. This allows unexpected prob-\nlems to be caught and tracked down by simply inspecting the most-recent \nlog ﬁ les.\nYou may want to consider ﬂ ushing your log ﬁ le(s) aft er every call to your \ndebug output function to ensure that if the game crashes the log ﬁ le(s) won’t \nbe missing the last buﬀ er-full of output. The last data printed is usually the \nmost crucial to determine the cause of a crash, so we want to be sure that the \nlog ﬁ le always contains the most up-to-date output. Of course, ﬂ ushing the \noutput buﬀ er can be expensive. So you should only ﬂ ush buﬀ ers aft er every \ndebug output call if either (a) you are not doing a lot of logging, or (b) you \ndiscover that it is truly necessary on your particular platform. If ﬂ ushing is \ndeemed to be necessary, you can always provide an engine conﬁ guration op-\ntion to turn it on and oﬀ .\n9.1.5. Crash Reports\n Some game engines produce special text output and/or log ﬁ les when the \ngame crashes. In most operating systems, a top-level exception handler can \nbe installed that will catch most crashes. In this function, you could print out \nall sorts of useful information. You could even consider emailing the crash \nreport to the entire programming team. This can be incredibly enlightening \nfor the programmers: When they see just how oft en the art and design teams \nare crashing, they may discover a renewed sense of urgency in their debug-\nging tasks!\nHere are just a few examples of the kinds of information you can include \nin a crash report:\nz Current level(s) being played at the time of the crash.\nz World-space location of the player character when the crash occurred.\nz Animation/action state of the player when the game crashed.\n9.1. Logging and Tracing\n\n\n372 \n9. Tools for Debugging and Development\nz Gameplay script(s) that were running at the time of the crash. (This can \nbe especially helpful if the script is the cause of the crash!)\nz Stack trace. Most operating systems provide a mechanism for walking \nthe call stack (although they are nonstandard and highly platform \nspeciﬁ c). With such a facility, you can print out the symbolic names of \nall non-inline functions on the stack at the time the crash occurred.\nz State of all memory allocators in the engine (amount of memory free, \ndegree of fragmentation, etc.). This kind of data can be helpful when \nbugs are caused by low-memory conditions, for example.\nz Any other information you think might be relevant when tracking down \nthe cause of a crash.\n9.2. Debug Drawing Facilities\nModern interactive games are driven almost entirely by math. We use math \nto position and orient objects in the game world, move them around, test for \ncollisions, cast rays to determine lines of sight, and of course use matrix mul-\ntiplication to transform objects from object space to world space and even-\ntually into screen space for rendering. Almost all modern games are three-\ndimensional, but even in a two-dimensional game it can be very diﬃ  cult to \nmentally visualize the results of all these mathematical calculations. For this \nreason, most good game engines provide an API for drawing colored lines, \nsimple shapes, and 3D text. We call this a debug drawing facility, because the \nlines, shapes, and text that are drawn with it are intended for visualization \nduring development and debugging and are removed prior to shipping the \ngame.\nA debug drawing API can save you huge amounts of time. For example, \nif you are trying to ﬁ gure out why your projectiles are not hitt ing the enemy \ncharacters, which is easier? Deciphering a bunch of numbers in the debugger? \nOr drawing a line showing the trajectory of the projectile in three dimensions \nwithin your game? With a debug drawing API, logical and mathematical er-\nrors become immediately obvious. One might say that a picture is worth 1,000 \nminutes of debugging.\nHere are some examples of debug drawing in action within Naughty \nDog’s Uncharted: Drake’s Fortune engine. The following screen shots were all \ntaken within our play-test level, one of many special levels we use for testing \nout new features and debugging problems in the game.\nz Figure 9.1 shows how a single line can help developers understand \nwhether a target is within the line of sight of an enemy character. You’ll \n\n\n373 \nFigure 9.1.  Visualizing the line of sight from an NPC to the player.\n9.2. Debug Drawing Facilities\nalso notice some debug text rendered just above the head of the enemy, \nin this case showing weapon ranges, a damage multiplier, the distance \nto the target, and the character’s percentage chance of striking the tar-\nget. Being able to print out arbitrary information in three-dimensional \nspace is an incredibly useful feature.\nz Figure 9.2 shows how a wireframe sphere can be used to visualize the \ndynamically expanding blast radius of an explosion.\nz Figure 9.3 shows how spheres can be used to visualize the radii used \nby Drake when searching for ledges to hang from in the game. A red \nline shows the ledge he is currently hanging from. Notice that in this \ndiagram, white text is displayed in the upper left -hand corner of the \nscreen. In the Uncharted: Drake’s Fortune engine, we have the ability to \ndisplay text in two-dimensional screen space, as well as in full 3D. This \ncan be useful when you want the text to be displayed independently of \nthe current camera angle.\nz Figure 9.4 shows an AI character that has been placed in a special de-\nbugging mode. In this mode, the character’s brain is eﬀ ectively turned \n\n\n374 \n9. Tools for Debugging and Development\nFigure 9.2.  Visualizing the expanding blast sphere of an explosion.\nFigure 9.3.  Spheres and vectors used in Drake’s ledge hang and shimmy system.\n\n\n375 \noﬀ , and the developer is given full control over the character’s move-\nments and actions via a simple heads-up menu. The developer can paint \ntarget points in the game world by simply aiming the camera and can \nthen instruct the character to walk, run, or sprint to the speciﬁ ed points. \nThe user can also tell the character to enter or leave nearby cover, ﬁ re its \nweapon, and so on.\n9.2.1. Debug Drawing API\nA debug drawing API generally needs to satisfy the following requirements:\nz The API should be simple and easy to use.\nz It should support a useful set of primitives , including (but not limited \nto):\nlines,\n \n□\nspheres,\n \n□\npoints (usually represented as small crosses or spheres, because a \n \n□\nsingle pixel is very diﬃ  cult to see),\nFigure 9.4.  Manually controlling an NPC’s actions for debugging purposes.\n9.2. Debug Drawing Facilities\n\n\n376 \n9. Tools for Debugging and Development\ncoordinate axes (typically the \n \n□\nx-axis is drawn in red, y in green and \nz in blue),\nbounding boxes, and\n \n□\nformatt ed text.\n \n□\nz It should provide a good deal of ﬂ exibility in controlling how primitives \nare drawn, including:\ncolor,\n \n□\nline width,\n \n□\nsphere radii,\n \n□\nthe size of points, lengths of coordinate axes, and dimensions of oth-\n \n□\ner “canned” primitives.\nz It should be possible to draw primitives in world space (full 3D, using \nthe game camera’s perspective projection matrix) or in screen space (ei-\nther using an orthographic projection, or possibly a perspective projec-\ntion). World-space primitives are useful for annotating objects in the \n3D scene. Screen-space primitives are helpful for displaying debugging \ninformation in the form of a heads-up display that is independent of \ncamera position or orientation.\nz It should be possible to draw primitives with or without depth testing \nenabled.\nWhen depth testing is enabled, the primitives will be occluded by \n \n□\nreal objects in your scene. This makes their depth easy to visualize, \nbut it also means that the primitives may sometimes be diﬃ  cult to \nsee or totally hidden by the geometry of your scene.\nWith depth testing disabled, the primitives will “hover” over the real \n \n□\nobjects in the scene. This makes it harder to gauge their real depth, \nbut it also ensures that no primitive is ever hidden from view.\nz It should be possible to make calls to the drawing API from anywhere \nin your code. Most rendering engines require that geometry be submit-\nted for rendering during a speciﬁ c phase of the game loop, usually at \nthe end of each frame. So this requirement implies that the system must \nqueue up all incoming debug drawing requests, so that they may be \nsubmitt ed at the proper time later on.\nz Ideally, every debug primitive should have a lifetime associated with it. \nThe lifetime controls how long the primitive will remain on-screen aft er \nhaving been requested. If the code that is drawing the primitive is called \nevery frame, the lifetime can be one frame—the primitive will remain \non-screen because it will be refreshed every frame. However, if the code \n\n\n377 \nthat draws the primitive is called rarely or intermitt ently (e.g., a func-\ntion that calculates the initial velocity of a projectile), then you do not \nwant the primitive to ﬂ icker on-screen for just one frame and then dis-\nappear. In such situations the programmer should be able to give his or \nher debug primitives a longer lifetime, on the order of a few seconds.\nz It’s also important that the debug drawing system be capable of han-\ndling a large number of debug primitives eﬃ  ciently. When you’re draw-\ning debug information for 1,000 game objects, the number of primitives \ncan really add up, and you don’t want your game to be unusable when \ndebug drawing is turned on.\nThe debug drawing API in Naughty Dog’s Uncharted: Drake’s Fortune en-\ngine looks something like this:\nclass DebugDrawManager\n{\npublic:\n \n// Adds a line segment to the debug drawing queue.\n void \nAddLine(  \nconst Point& fromPosition,\n      const \nPoint& toPosition,\n      Color \ncolor,\n      float \nlineWidth = 1.0f,\n      float \nduration = 0.0f,\n      bool \ndepthEnabled = true);\n \n// Adds an axis-aligned cross (3 lines converging at   \n \n// a point) to the debug drawing queue.\n void \nAddCross( \nconst Point& position,\n      Color \ncolor,\n      float \nsize,\n      float \nduration = 0.0f,\n      bool \ndepthEnabled = true);\n \n// Adds a wireframe sphere to the debug drawing queue.\n void \nAddSphere( const Point& centerPosition,\n      float \nradius,\n      Color \ncolor,\n      float \nduration = 0.0f,\n      bool \ndepthEnabled = true);\n \n// Adds a circle to the debug drawing queue.\n void \nAddCircle( const Point& centerPosition,\n      const \nVector& planeNormal,\n      float \nradius,\n      Color \ncolor,\n      float \nduration = 0.0f,\n      bool \ndepthEnabled = true);\n9.2. Debug Drawing Facilities\n\n\n378 \n9. Tools for Debugging and Development\n \n// Adds a set of coordinate axes depicting the  \n \n \n \n// position and orientation of the given \n \n// transformation to the debug drawing queue.\n void \nAddAxes(  \nconst Transform& xfm,\n      Color \ncolor,\n      float \nsize,\n      float \nduration = 0.0f,\n      bool \ndepthEnabled = true);\n \n// Adds a wireframe triangle to the debug drawing  \n \n \n// queue.\n void \nAddTriangle( \nconst Point& vertex0,\n      const \nPoint& vertex1,\n      const \nPoint& vertex2,\n      Color \ncolor,\n      float \nlineWidth = 1.0f,\n      float \nduration = 0.0f,\n      bool \ndepthEnabled = true);\n \n// Adds an axis-aligned bounding box to the debug  \n \n \n// queue.\n void \nAddAABB( const Point& minCoords,\n     const \nPoint& maxCoords,\n     Color \ncolor,\n     float \nlineWidth = 1.0f,\n     float \nduration = 0.0f,\n     bool \ndepthEnabled = true);\n \n// Adds an oriented bounding box to the debug queue.\n void \nAddOBB( const Mat44& centerTransform,\n     const \nVector& scaleXYZ,\n     Color \ncolor,\n     float \nlineWidth = 1.0f,\n     float \nduration = 0.0f,\n     bool \ndepthEnabled = true);\n \n// Adds a text string to the debug drawing queue.\n void \nAddString( const Point& pos,\n      const \nchar* text,\n      Color \ncolor,\n      float \nduration = 0.0f,\n      bool \ndepthEnabled = true);\n};\n// This global debug drawing manager is configured for \n// drawing in full 3D with a perspective projection.\nextern DebugDrawManager g_debugDrawMgr;\n\n\n379 \n9.3. In-Game Menus\n// This global debug drawing manager draws its \n// primitives in 2D screen space. The (x,y) coordinates  \n// of a point specify a 2D location on-screen, and the \n// z coordinate contains a special code that indicates \n// whether the (x,y) coordidates are measured in absolute \n// pixels or in normalized coordinates that range from \n// 0.0 to 1.0. (The latter mode allows drawing to be \n// independent of the actual resolution of  the screen.)\nextern DebugDrawManager g_debugDrawMgr2D;\nHere’s an example of this API being used within game code:\nvoid Vehicle::Update()\n{\n \n// Do some calculations...\n \n// Debug-draw my velocity vector.\n \nPoint start = GetWorldSpacePosition();\n \nPoint end = start + GetVelocity();\ng_debugDrawMgr.AddLine(start, end, kColorRed);\n \n// Do some other calculations...\n \n// Debug-draw my name and number of passengers.\n {\n  char b\nuffer[128];\n \n \nsprintf(buffer, \"Vehicle %s: %d passengers\",\n   GetName(), GetN\numPassengers());\ng_debugDrawMgr.AddString(GetWorldSpacePosition(),\n \n \n \nbuffer, kColorWhite, 0.0f, false);\n }\n}\nYou’ll notice that the names of the drawing functions use the verb “add” \nrather than “draw.” This is because the debug primitives are typically not \ndrawn immediately when the drawing function is called. Instead, they are \nadded to a list of visual elements that will be drawn at a later time. Most high-\nspeed 3D rendering engines require that all visual elements be maintained in \na scene data structure so that they can be drawn eﬃ  ciently, usually at the end \nof the game loop. We’ll learn a lot more about how rendering engines work \nin Chapter 10.\n9.3. In-Game Menus\n Every game engine has a large number of conﬁ guration options and features. \nIn fact, each major subsystem, including rendering, animation, collision, \n\n\n380 \n9. Tools for Debugging and Development\nphysics, audio, networking, player mechanics, AI, and so on, exposes its own \nspecialized conﬁ guration options. It is highly useful to programmers, artists, \nand game designers alike to be able to conﬁ gure these options while the game \nis running, without having to change the source code, recompile and relink \nthe game executable, and then rerun the game. This can greatly reduce the \namount of time the game development team spends on debugging problems \nand sett ing up new levels or game mechanics.\nFigure 9.5.  Main development menu in Uncharted.\nFigure 9.6.  Rendering submenu.\n",
      "page_number": 385,
      "chapter_number": 20,
      "summary": "This chapter covers segment 20 (pages 385-402). Key topics include game, control, and controller. We might want to take a more functional approach, for example, by \nnaming our abstract controls according to their function in the game, rather \nthan their physical locations on the joypad.",
      "keywords": [
        "Debug Drawing",
        "game",
        "debug drawing API",
        "debug",
        "debug drawing queue",
        "Game Engine HID",
        "Debug Drawing Facilities",
        "Drawing",
        "Debug Output",
        "const Point",
        "game engines",
        "control",
        "output",
        "drawing API",
        "Debugging"
      ],
      "concepts": [
        "game",
        "control",
        "controller",
        "debugging",
        "debug",
        "point",
        "functional",
        "function",
        "functions",
        "engine"
      ],
      "similar_chapters": [
        {
          "book": "makinggames",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 26,
          "title": "Segment 26 (pages 243-252)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 14,
          "title": "Segment 14 (pages 116-123)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "makinggames",
          "chapter": 17,
          "title": "Segment 17 (pages 140-147)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 403-420)",
      "start_page": 403,
      "end_page": 420,
      "detection_method": "synthetic",
      "content": "381 \n9.4. In-Game Console\nOne simple and convenient way to permit this kind of thing is to provide \na system of in-game menus. Items on an in-game menu can do any number of \nthings, including (but certainly not limited to):\nz toggling global Boolean sett ings,\nz adjusting global integer and ﬂ oating-point values,\nz calling arbitrary functions, which can perform literally any task within \nthe engine,\nFigure 9.7.  Mesh options subsubmenu.\nFigure 9.8.  Background meshes turned off.\n\n\n382 \n9. Tools for Debugging and Development\nz bringing up submenus, allowing the menu system to be organized hier-\narchically for easy navigation.\nAn in-game menu should be easy and convenient to bring up, perhaps via \na simple butt on-press on the joypad. (Of course, you’ll want to choose a but-\nton combination that doesn’t occur during normal gameplay.) Bringing up the \nmenus usually pauses the game. This allows the developer to play the game \nuntil the moment just before a problem occurs, then pause the game by bring-\ning up the menus, adjust engine sett ings in order to visualize the problem \nmore clearly, and then un-pause the game to inspect the problem in depth.\nLet’s take a brief look at how the menu system works in the Uncharted: \nDrake’s Fortune engine, by Naughty Dog. Figure 9.5 shows the top-level menu. \nIt contains submenus for each major subsystem in the engine. In Figure 9.6, \nwe’ve drilled down one level into the Rendering… submenu. Since the render-\ning engine is a highly complex system, its menu contains many submenus con-\ntrolling various aspects of rendering. To control the way in which 3D meshes \nare rendered, we drill down further into the Mesh Options… submenu, shown \nin Figure 9.7. On this menu, we can turn oﬀ  rendering of all static background \nmeshes, leaving only the dynamic foreground meshes visible. This is shown \nin Figure 9.8.\n9.4. In-Game Console\n Some engines provide an in-game console, either in lieu of or in addition to an \nin-game menu system. An in-game console provides a command-line inter-\nface to the game engine’s features, much as a DOS command prompt provides \nusers with access to various features of the Windows operating system, or a \ncsh, tcsh, ksh or bash shell prompt provides users with access to the features \nof UNIX-like operating systems. Much like a menu system, the game engine \nconsole can provide commands allowing a developer to view and manipulate \nglobal engine sett ings, as well as running arbitrary commands.\nA console is somewhat less convenient than a menu system, especially for \nthose who aren’t very fast typists. However, a console can be much more pow-\nerful than a menu. Some in-game consoles provide only a rudimentary set \nof hard-coded commands, making them about as ﬂ exible as a menu system. \nBut others provide a rich interface to virtually every feature of the engine. A \nscreen shot of the in-game console in Quake 4 is shown in Figure 9.9.\nSome game engines provide a powerful scripting language that can be \nused by programmers and game designers to extend the functionality of the \nengine, or even build entirely new games. If the in-game console “speaks” \n\n\n383 \nthis same scripting language, then anything you can do in script can also be \ndone interactively via the console. We’ll explore scripting languages in depth \nin Section 14.8.\n9.5. Debug Cameras and Pausing the Game\nAn in-game menu or console system is best accompanied by two other crucial \nfeatures: (a) the ability to detach the camera from the player character and ﬂ y \nit around the game world in order to scrutinize any aspect of the scene, and \n(b) the ability to pause , un-pause and single-step the game (see Section 7.5.6). \nWhen the game is paused, it is important to still be able to control the camera. \nTo support this, we can simply keep the rendering engine and camera controls \nrunning, even when the game’s logical clock is paused.\nSlow motion mode is another incredibly useful feature for scrutinizing \nanimations, particle eﬀ ects, physics and collision behaviors, AI behaviors, \nand the list goes on. This feature is easy to implement. Presuming we’ve tak-\nen care to update all gameplay elements using a clock that is distinct from \nthe real-time clock, we can put the game into slo-mo by simply updating the \ngameplay clock at a rate that is slower than usual. This approach can also \nbe used to implement a fast-motion mode, which can be useful for moving \n9.6. Cheats\nFigure 9.9.  The in-game console in Quake 4, overlaid on top of the main game menu.\n\n\n384 \n9. Tools for Debugging and Development\nrapidly through time-consuming portions of gameplay in order to get to an \narea of interest.\n9.6. Cheats\nWhen developing or debugging a game, it’s important to allow the user to \nbreak the rules of the game in the name of expediency. Such features are aptly \nnamed cheats . For example, many engines allow you to “pick up” the player \ncharacter and ﬂ y him or her around in the game world, with collisions dis-\nabled so he or she can pass through all obstacles. This can be incredibly help-\nful for testing out gameplay. Rather than taking the time to actually play the \ngame in an att empt to get the player character into some desirable location, \nyou can simply pick him up, ﬂ y him over to where you want him to be, and \nthen drop him back into his regular gameplay mode.\nOther useful cheats include, but are certainly not limited to:\nz Invincible player. As a developer, you oft en don’t want to be bothered \nhaving to defend yourself from enemy characters, or worrying about \nfalling from too high a height, as you test out a feature or track down a \nbug.\nz Give player weapon. It’s oft en useful to be able to give the player any \nweapon in the game for testing purposes.\nz Inﬁ nite ammo. When you’re trying to kill bad guys to test out the weap-\non system or AI hit reactions, you don’t want to be scrounging for \nclips!\nz Select player mesh. If the player character has more than one “costume,” \nit can be useful to be able to select any of them for testing purposes.\nObviously this list could go on for pages. The sky’s the limit—you can add \nwhatever cheats you need in order to develop or debug the game. You might \neven want to expose some of your favorite cheats to the players of the ﬁ nal \nshipping game. Players can usually activate cheats by entering unpublished \ncheat codes on the joypad or keyboard, and/or by accomplishing certain objec-\ntives in the game.\n9.7. Screen Shots and Movie Capture\nAnother extremely useful facility is the ability to capture screen shots and \nwrite them to disk in a suitable image format such as Windows Bitmap ﬁ les \n\n\n385 \n(.bmp) or Targa (.tga). The details of how to capture a screen shot vary from \nplatform to platform, but they typically involve making a call to the graphics \nAPI that allows the contents of the frame buﬀ er to be transferred from video \nRAM to main RAM, where it can be scanned and converted into the image \nﬁ le format of your choice. The image ﬁ les are typically writt en to a predeﬁ ned \nfolder on disk and named using a date and time stamp to guarantee unique \nﬁ le names.\nYou may want to provide your users with various options controlling \nhow screen shots are to be captured. Some common examples include:\nz Whether or not to include debug lines and text in the screen shot.\nz Whether or not to include heads-up display (HUD) elements in the \nscreen shot.\nz The resolution at which to capture. Some engines allow high resolution \nscreen shots to be captured, perhaps by modifying the projection matrix \nso that separate screen shots can be taken of the four quadrants of the \nscreen at normal resolution and then combined into the ﬁ nal high-res \nimage.\nz Simple camera animations. For example, you could allow the user to \nmark the starting and ending positions and orientations of the camera. \nA sequence of screen shots could then be taken while gradually interpo-\nlating the camera from the start location to the ending location.\nSome engines also provide a full-ﬂ edged movie capture mode. Such a sys-\ntem captures a sequence of screen shots at the target frame rate of the game, \nwhich are typically processed oﬄ  ine to generate a movie ﬁ le in a suitable \nformat such as AVI or MP4.\nCapturing a screen shot is usually a relatively slow operation, due in \npart to the time required to transfer the frame buﬀ er data from video RAM \nto main RAM (an operation for which the graphics hardware is usually not \noptimized), and in larger part to the time required to write image ﬁ les to disk. \nIf you want to capture movies in real time (or at least close to real time), you’ll \nalmost certainly need to store the captured images to a buﬀ er in main RAM, \nonly writing them out to disk when the buﬀ er has been ﬁ lled (during which \nthe game will typically be frozen).\n9.8. In-Game Proﬁ ling\n Games are real-time systems, so achieving and maintaining a high frame rate \n(usually 30 FPS or 60 FPS) is important. Therefore, part of any game program-\n9.8. In-Game Proﬁ ling \n\n\n386 \n9. Tools for Debugging and Development\nmer’s job is ensuring that his or her code runs eﬃ  ciently and within budget. \nAs we saw when we discussed the 80-20 and 90-10 rules in Chapter 2, a large \npercentage of your code probably doesn’t need to be optimized. The only way \nto know which bits require optimization is to measure your game’s performance. \nWe discussed various third-party proﬁ ling tools in Chapter 2. However, these \ntools have various limitations and may not be available at all on a console. For \nFigure 9.11.  The Uncharted 2 engine also provides a proﬁ le hierarchy display that allows the \nuser to drill down into particular function calls in inspect their costs.\nFigure 9.10. The proﬁ le category display in the Uncharted 2: Among Theives engine shows \ncoarse timing ﬁ gures for various top-level engine systems.\n\n\n387 \nthis reason, and/or for convenience, many game engines provide an in-game \nproﬁ ling tool of some sort.\nTypically an in-game proﬁ ler permits the programmer to annotate blocks \nof code which should be timed and give them human-readable names. The \nproﬁ ler measures the execution time of each annotated block via the CPU’s \nhi-res timer, and stores the results in memory. A heads-up display is provided \nwhich shows up-to-date execution times for each code block (examples are \nshown in Figure 9.10, Figure 9.11, and Figure 9.12). The display oft en provides \nthe data in various forms, including raw numbers of cycles, execution times \nin micro-seconds, and percentages relative to the execution time of the entire \nframe.\n9.8.1. Hierarchical Proﬁ ling\n Computer programs writt en in an imperative language are inherently hierar-\nchical—a function calls other functions, which in turn call still more functions. \nFor example, let’s imagine that function a() calls functions b() and c(), and \nfunction b() in turn calls functions d(), e() and f(). The pseudocode for this \nis shown below.\nvoid a()\n{\n b();\n c();\n}\n9.8. In-Game Proﬁ ling \nFigure 9.12. The timeline mode in Uncharted 2 shows exactly when various operations are \nperformed across a single frame on the PS3’s SPUs, GPU and PPU.\n\n\n388 \n9. Tools for Debugging and Development\nvoid b()\n{\n d();\n e();\n f();\n}\nvoid c() { ... }\nvoid d() { ... }\nvoid e() { ... }\nvoid f() { ... }\nAssuming function a() is called directly from main(), this function call hier-\narchy is shown in Figure 9.13.\nWhen debugging a program, the call stack shows only a snapshot of this \ntree. Speciﬁ cally, it shows us the path from whichever function in the hierarchy \nis currently executing all the way to the root function in the tree. In C/C++, the \nroot function is usually main() or WinMain(), although technically this func-\ntion is called by a start-up function that is part of the standard C runtime library \n(CRT), so that function is the true root of the hierarchy. If we set a breakpoint in \nfunction e(), for example, the call stack would look something like this:\ne()\nÅ The currently-executing function.\nb()\na()\nmain()\n_crt_startup()\nÅ Root of the call hierarchy.\nThis call stack is depicted in Figure 9.14 as a pathway from function e() to the \nroot of the function call tree.\nf()\na()\nb()\nc()\nd()\ne()\nFigure 9.13.  A hy-\npothetical \nfunc-\ntion call hierar-\nchy.\nf()\na()\nb()\nc()\nd()\ne()\nmain()\n_crt_startup()\nFigure 9.14.  Call stack resulting from setting a break point in function e().\n\n\n389 \n9.8.1.1. \nMeasuring Execution Times Hierarchically\nIf we measure the execution time of a single function, the time we measure \nincludes the execution time of any the child functions called and all of their \ngrandchildren, great grandchildren, and so on as well. To properly interpret \nany proﬁ ling data we might collect, we must be sure to take the function call \nhierarchy into account.\nMany commercial proﬁ lers can automatically instrument every single \nfunction in your program. This permits them to measure both the inclusive \nand exclusive execution times of every function that is called during a proﬁ l-\ning session. As the name implies, inclusive times measure the execution time \nof the function including all of its children, while exclusive times measure \nonly the time spent in the function itself. (The exclusive time of a function \ncan be calculated by subtracting the inclusive times of all its immediate chil-\ndren from the inclusive time of the function in question.) In addition, some \nproﬁ lers record how many times each function is called. This is an impor-\ntant piece of information to have when optimizing a program, because it al-\nlows you to diﬀ erentiate between functions that eat up a lot of time internally \nand functions that eat up time because they are called a very large number of \ntimes.\nIn contrast, in-game proﬁ ling tools are not so sophisticated and usually \nrely on manual instrumentation of the code. If our game engine’s main loop \nis structured simply enough, we may be able to obtain valid data at a coarse \nlevel without thinking much about the function call hierarchy. For example, a \ntypical game loop might look roughly like this:\nwhile (!quitGame)\n{\n PollJoypad();\n UpdateGameObjects();\n UpdateAllAnimations();\n PostProcessJoints();\n DetectCollisions();\n RunPhysics();\n GenerateFinalAnimationPoses();\n UpdateCameras();\n RenderScene();\n UpdateAudio();\n}\nWe could proﬁ le this game at a very coarse level by measuring the execution \ntimes of each major phase of the game loop:\nwhile (!quitGame)\n{\n9.8. In-Game Proﬁ ling \n\n\n390 \n9. Tools for Debugging and Development\n {\n  PROFILE(\"Poll \nJoypad\");\n  PollJoypad();\n }\n {\n \n \nPROFILE(\"Game Object Update\");\n  UpdateGameObjects();\n }\n {\n  PROFILE(\"Animation\");\n  UpdateAllAnimations();\n }\n {\n  PROFILE(\"Joint \nPost-Processing\");\n  PostProcessJoints();\n }\n {\n  PROFILE(\"Collision\");\n  DetectCollisions();\n }\n {\n  PROFILE(\"Physics\");\n  RunPhysics();\n }\n {\n  PROFILE(\"Animation \nFinaling\");\n  GenerateFinalAnimationPoses();\n }\n {\n  PROFILE(\"Cameras\");\n  UpdateCameras();\n }\n {\n  PROFILE(\"Rendering\");\n  RenderScene();\n }\n {\n  PROFILE(\"Audio\");\n  UpdateAudio();\n }\n}\nThe PROFILE() macro shown above would probably be implemented as a \nclass whose constructor starts the timer and whose destructor stops the timer \nand records the execution time under the given name. Thus it only times the \ncode within its containing block, by nature of the way C++ automatically con-\nstructs and destroys objects as they go in and out of scope.\n\n\n391 \nstruct AutoProfile\n{\n \nAutoProfile(const char* name)\n {\n \n \nm_name = name;\n \n \nm_startTime = QueryPerformanceCounter();\n }\n ~AutoProfile()\n {\n \n \n__int64 endTime = QueryPerformanceCounter();\n \n \n__int64 elapsedTime = endTime – m_startTime;\n  g_profileManager.storeSample(m_name, \nelapsedTime);\n }\n \nconst char* m_name;\n __int64 \n  m_startTime;\n};\n#define PROFILE(name) AutoProfile p(name)\nThe problem with this simplistic approach is that it breaks down when \nused within deeper levels of function call nesting. For example, if we embed \nadditional PROFILE() annotations within the RenderScene() function, we \nneed to understand the function call hierarchy in order to properly interpret \nthose measurements.\nOne solution to this problem is to allow the programmer who is an-\nnotating the code to indicate the hierarchical interrelationships between \nproﬁ ling samples. For example, any PROFILE(...) samples taken with-\nin the RenderScene() function could be declared to be children of the \nPROFILE(\"Rendering\") sample. These relationships are usually set up sepa-\nrately from the annotations themselves, by predeclaring all of the sample bins. \nFor example, we might set up the in-game proﬁ ler during engine initialization \nas follows:\n// This code declares various profile sample \"bins\", \n// listing the name of the bin and the name of its \n// parent bin, if any.\nProfilerDeclareSampleBin(\"Rendering\", NULL);\n \nProfilerDeclareSampleBin(\"Visibility\", \"Rendering\");\n \nProfilerDeclareSampleBin(\"ShaderSetUp\", \"Rendering\");\n  ProfilerDeclareSampleBin(\"Materials\", \n\"Shaders\");\n \nProfilerDeclareSampleBin(\"SubmitGeo\", \"Rendering\");\nProfilerDeclareSampleBin(\"Audio\", NULL);\n ...\n9.8. In-Game Proﬁ ling \n\n\n392 \n9. Tools for Debugging and Development\nThis approach still has its problems. Speciﬁ cally, it works well when every \nfunction in the call hierarchy has only one parent, but it breaks down when \nwe try to proﬁ le a function that is called by more than one parent function. \nThe reason for this should be prett y obvious. We’re statically declaring our \nsample bins as if every function can only appear once in the function call hi-\nerarchy, but actually the same function can reappear many times in the tree, \neach time with a diﬀ erent parent. The result can be misleading data, because a \nfunction’s time will be included in one of the parent bins, but really should be \ndistributed across all of its parents’ bins. Most game engines don’t make an at-\ntempt to remedy this problem, since they are primarily interested in proﬁ ling \ncoarse-grained functions that are only called from one speciﬁ c location in the \nfunction call hierarchy. But this limitation is something to be aware of when \nproﬁ ling your code with a simple in-engine proﬁ le of the sort found in most \ngame engines.\nWe would also like to account for how many times a given function is \ncalled. In the example above, we know that each of the functions we proﬁ led \nare called exactly once per frame. But other functions, deeper in the func-\ntion call hierarchy, may be called more than once per frame. If we measure \nfunction x() to take 2 ms to execute, it’s important to know whether it takes \n2 ms to execute on its own, or whether it executes in 2 μs but was called 1000 \ntimes during the frame. Keeping track of the number of times a function is \ncalled per frame is quite simple—the proﬁ ling system can simply increment \na counter each time a sample is received and reset the counters at the start of \neach frame.\n9.8.2. Exporting to Excel\nSome game engines permit the data captured by the in-game proﬁ ler to be \ndumped to a text ﬁ le for subsequent analysis. I ﬁ nd that a comma-separat-\ned values (CSV ) format is best, because such ﬁ les can be loaded easily into \na Microsoft  Excel spreadsheet, where the data can be manipulated and ana-\nlyzed in myriad ways. I wrote such an exporter for the Medal of Honor: Paciﬁ c \nAssault engine. The columns corresponded to the various annotated blocks, \nand each row represented the proﬁ ling sample taken during one frame of \nthe game’s execution. The ﬁ rst column contained frame numbers and the sec-\nond actual game time measured in seconds. This allowed the team to graph \nhow the performance statistics varied over time and to determine how long \neach frame actually took to execute. By adding some simple formulae to the \nexported spreadsheet, we could calculate frame rates, execution time percent-\nages, and so on.\n\n\n393 \n9.9. In-Game Memory Stats and Leak Detection\nIn addition to runtime performance (i.e., frame rate), most game engines are \nalso constrained by the amount of memory available on the target hardware. \nPC games are least aﬀ ected by such constraints, because modern PCs have \nsophisticated virtual memory managers. But even PC games are constrained \nby the memory limitations of their so-called “min spec” machine—the least-\npowerful machine on which the game is guaranteed to run, as promised by \nthe publisher and stated on the game’s packaging.\n For this reason, most game engines implement custom memory-tracking \ntools. These tools allow the developers to see how much memory is being \nused by each engine subsystem and whether or not any memory is leaking \n(i.e., memory is allocated but never freed). It’s important to have this informa-\ntion, so that you can make informed decisions when trying to cut back the \nmemory usage of your game so that it will ﬁ t onto the console or type of PC \nyou are targeting.\nKeeping track of how much memory a game actually uses can be a sur-\nprisingly tricky job. You’d think you could simply wrap malloc()/free() or \nnew/delete in a pair of functions or macros that keep track of the amount of \nmemory that is allocated and freed. However, it’s never that simple for a few \nreasons:\n \n1. You oft en can’t control the allocation behavior of other people’s code. Unless \nyou write the operating system, drivers, and the game engine entire-\nly from scratch, there’s a good chance you’re going to end up linking \nyour game with at least some third-party libraries. Most good libraries \nprovide memory allocation hooks, so that you can replace their allocators \nwith your own. But some do not. It’s oft en diﬃ  cult to keep track of the \nmemory allocated by each and every third-party library you use in your \ngame engine—but it usually can be done if you’re thorough and selec-\ntive in your choice of third-party libraries.\n \n2. Memory comes in diﬀ erent ﬂ avors. For example, a PC has two kinds of \nRAM: main RAM and video RAM (the memory residing on your graph-\nics card, which is used primarily for geometry and texture data). Even \nif you manage to track all of the memory allocations and deallocations \noccurring within main RAM, it can be well neigh impossible to track \nvideo RAM usage. This is because graphics APIs like DirectX actually \nhide the details of how video RAM is being allocated and used from the \ndeveloper. On a console, life is a bit easier, only because you oft en end \nup having to write a video RAM manager yourself. This is more diﬃ  cult \n9.9. In-Game Memory Stats and Leak Detection \n\n\n394 \n9. Tools for Debugging and Development\nthan using DirectX, but at least you have complete knowledge of what’s \ngoing on.\n \n3. Allocators come in diﬀ erent ﬂ avors. Many games make use of specialized \nallocators for various purposes. For example, the Uncharted: Drake’s \nFortune engine has a global heap for general-purpose allocations, a spe-\ncial heap for managing the memory created by game objects as they \nspawn into the game world and are destroyed, a level-loading heap for \ndata that is streamed into memory during gameplay, a stack allocator \nfor single-frame allocations (the stack is cleared automatically every \nframe), an allocator for video RAM, and a debug memory heap used only \nfor allocations that will not be needed in the ﬁ nal shipping game. Each \nof these allocators grabs a large hunk of memory when the game starts \nup and then manages that memory block itself. If we were to track all \nthe calls to new and delete, we’d see one new for each of these six al-\nlocators and that’s all. To get any useful information, we really need \nto track all of the allocations within each of these allocators’ memory \nblocks.\nMost professional game teams expend a signiﬁ cant amount of eﬀ ort on \ncreating in-engine memory-tracking tools that provide accurate and detailed \ninformation. The resulting tools usually provide their output in a variety of \nforms. For example, the engine might produce a detailed dump of all memory \nallocations made by the game during a speciﬁ c period of time. The data might \ninclude high water marks for each memory allocator or each game system, \nindicating the maximum amount of physical RAM required by each. Some \nengines also provide heads-up displays of memory usage while the game is \nFigure 9.15.  Tabular memory statistics from the Uncharted 2: Among Thieves engine.\n\n\n395 \nrunning. This data might be tabular, as shown in Figure 9.15, or graphical as \nshown in Figure 9.16.\nIn addition, when low-memory or out-of-memory conditions arise, a good \nengine will provide this information in as helpful a way as possible. When PC \ngames are developed, the game team usually works on high-powered PCs \nwith more RAM than the min-spec machine being targeted. Likewise, console \ngames are developed on special development kits which have more memory \nthan a retail console. So in both cases, the game can continue to run even when \nit technically has run out of memory (i.e., would no longer ﬁ t on a retail con-\nsole or min-spec PC). When this kind of out-of-memory condition arises, the \ngame engine can display a message saying something like, “Out of memory—\nthis level will not run on a retail system.”\nThere are lots of other ways in which a game engine’s memory tracking \nsystem can aid developers in pinpointing problems as early and as conve-\nniently as possible. Here are just a few examples:\nz If a model fails to load, a bright red text string could be displayed in 3D \nhovering in the game world where that object would have been.\nz If a texture fails to load, the object could be drawn with an ugly pink \ntexture that is very obviously not part of the ﬁ nal game.\nz If an animation fails to load, the character could assume a special (pos-\nsibly humorous) pose that indicates a missing animation, and the name \nof the missing asset could hover over the character’s head.\nThe key to providing good memory analysis tools is (a) to provide accurate \ninformation, (b) to present the data in a way that is convenient and that makes \nproblems obvious, and (c) to provide contextual information to aid the team \nin tracking down the root cause of problems when they occur. \nFigure 9.16.  A graphical memory usage display, also from Uncharted 2.\n9.9. In-Game Memory Stats and Leak Detection \n\n\nPart III\nGraphics and Motion\n",
      "page_number": 403,
      "chapter_number": 21,
      "summary": "This chapter covers segment 21 (pages 403-420). Key topics include game, memory, and engine. Covers function.",
      "keywords": [
        "game",
        "function",
        "In-Game Proﬁ ling",
        "In-Game Proﬁ",
        "game engine",
        "memory",
        "Proﬁ ling",
        "Proﬁ",
        "time",
        "engine",
        "function call",
        "In-Game",
        "game engines provide",
        "PROFILE",
        "In-Game Console"
      ],
      "concepts": [
        "game",
        "memory",
        "engine",
        "profile",
        "functions",
        "functionality",
        "time",
        "timing",
        "timed",
        "frame"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 12,
          "title": "Segment 12 (pages 101-108)",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 47,
          "title": "Segment 47 (pages 448-460)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 56,
          "title": "Segment 56 (pages 544-551)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 26,
          "title": "Segment 26 (pages 243-252)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 421-440)",
      "start_page": 421,
      "end_page": 440,
      "detection_method": "synthetic",
      "content": "399\n10\nThe Rendering Engine\nW\nhen most people think about computer and video games, the ﬁ rst thing \nthat comes to mind is the stunning three-dimensional graphics. Real-\ntime 3D rendering is an exceptionally broad and profound topic, so there’s \nsimply no way to cover all of the details in a single chapter. Thankfully there \nare a great many excellent books and other resources available on this topic. \nIn fact, real-time 3D graphics is perhaps one of the best covered of all the tech-\nnologies that make up a game engine. The goal of this chapter, then, is to pro-\nvide you with a broad understanding of real-time rendering technology and \nto serve as a jumping-oﬀ  point for further learning. Aft er you’ve read through \nthese pages, you should ﬁ nd that reading other books on 3D graphics seems \nlike a journey through familiar territory. You might even be able to impress \nyour friends at parties (… or alienate them…)\nWe’ll begin by laying a solid foundation in the concepts, theory, and math-\nematics that underlie any real-time 3D rendering engine. Next, we’ll have \na look at the soft ware and hardware pipelines used to turn this theoretical \nframework into reality. We’ll discuss some of the most common optimization \ntechniques and see how they drive the structure of the tools pipeline and the \nruntime rendering API in most engines. We’ll end with a survey of some of the \nadvanced rendering techniques and lighting models in use by game engines \ntoday. Throughout this chapter, I’ll point you to some of my favorite books \n\n\n400 \n10. The Rendering Engine\nand other resources that should help you to gain an even deeper understand-\ning of the topics we’ll cover here.\n10.1. Foundations of Depth-Buffered \nTriangle Rasterization\nWhen you boil it down to its essence, rendering a three-dimensional scene \ninvolves the following basic steps:\nz A virtual scene is described, usually in terms of 3D surfaces represented \nin some mathematical form.\nz A virtual camera is positioned and oriented to produce the desired view \nof the scene. Typically the camera is modeled as an idealized focal point, \nwith an imaging surface hovering some small distance in front of it, \ncomposed of virtual light sensors corresponding to the picture elements \n(pixels ) of the target display device .\nz Various light sources are deﬁ ned. These sources provide all the light rays \nthat will interact with and reﬂ ect oﬀ  the objects in the environment and \neventually ﬁ nd their way onto the image-sensing surface of the virtual \ncamera.\nz The visual properties of the surfaces in the scene are described. This de-\nﬁ nes how light should interact with each surface.\nz For each pixel within the imaging rectangle, the rendering engine calcu-\nlates the color and intensity of the light ray(s) converging on the virtual \ncamera’s focal point through that pixel. This is known as solving the ren-\ndering equation (also called the shading equation).\nThis high-level rendering process is depicted in Figure 10.1.\nMany diﬀ erent technologies can be used to perform the basic render-\ning steps described above. The primary goal is usually photorealism , although \nsome games aim for a more stylized look (e.g., cartoon, charcoal sketch, wa-\ntercolor, and so on). As such, rendering engineers and artists usually att empt \nto describe the properties of their scenes as realistically as possible and to \nuse light transport models that match physical reality as closely as possible. \nWithin this context, the gamut of rendering technologies ranges from tech-\nniques designed for real-time performance at the expense of visual ﬁ delity, \nto those designed for photorealism but which are not intended to operate in \nreal time.\nReal-time rendering engines perform the steps listed above repeatedly, \ndisplaying rendered images at a rate of 30, 50, or 60 frames per second to \n\n\n401 \n10.1. Foundations of Depth-Buffered Triangle Rasterization\nprovide the illusion of motion. This means a real-time rendering engine has at \nmost 33.3 ms to generate each image (to achieve a frame rate of 30 FPS). Usu-\nally much less time is available, because bandwidth is also consumed by other \nengine systems like animation, AI, collision detection, physics simulation, au-\ndio, player mechanics, and other gameplay logic. Considering that ﬁ lm ren-\ndering engines oft en take anywhere from many minutes to many hours to \nrender a single frame, the quality of real-time computer graphics these days \nis truly astounding.\n10.1.1. Describing a Scene\nA real-world scene is composed of objects. Some objects are solid, like a brick, \nand some are amorphous, like a cloud of smoke, but every object occupies a \nvolume of 3D space. An object might be opaque (in which case light cannot \npass through its volume), transparent (in which case light passes through it \nwithout being scatt ered, so that we can see a reasonably clear image of what-\never is behind the object), or translucent (meaning that light can pass through \nthe object but is scatt ered in all directions in the process, yielding only a blur \nof colors that hint at the objects behind it).\nOpaque objects can be rendered by considering only their surfaces . We \ndon’t need to know what’s inside an opaque object in order to render it, be-\ncause light cannot penetrate its surface. When rendering a transparent or \ntranslucent object, we really should model how light is reﬂ ected, refracted, \nscatt ered, and absorbed as it passes through the object’s volume. This requires \nknowledge of the interior structure and properties of the object. However, \nmost game engines don’t go to all that trouble. They just render the surfaces \nVirtual \nScreen\n(Near Plane)\nxC\nzC\nyC\nRendered \nImage\nCamera \nFrustum\nCamera\nFigure 10.1. The high-level rendering approach used by virtually all 3D computer graphics \ntechnologies.\n\n\n402 \n10. The Rendering Engine\nof transparent and translucent objects in almost the same way opaque objects \nare rendered. A simple numeric opacity measure known as alpha is used to \ndescribe how opaque or transparent a surface is. This approach can lead to \nvarious visual anomalies (for example, surface features on the far side of the \nobject may be rendered incorrectly), but the approximation can be made to \nlook reasonably realistic in many cases. Even amorphous objects like clouds \nof smoke are oft en represented using particle eﬀ ects, which are typically com-\nposed of large numbers of semi-transparent rectangular cards. Therefore, it’s \nsafe to say that most game rendering engines are primarily concerned with \nrendering surfaces.\n10.1.1.1. Representations Used by High-End Rendering Packages\nTheoretically, a surface is a two-dimensional sheet comprised of an inﬁ nite \nnumber of points in three-dimensional space. However, such a description is \nclearly not practical. In order for a computer to process and render arbitrary \nsurfaces, we need a compact way to represent them numerically.\nSome surfaces can be described exactly in analytical form, using a para-\nmetric surface equation . For example, a sphere centered at the origin can be rep-\nresented by the equation x2 + y2 + z2 = r2. However, parametric equations aren’t \nparticularly useful for modeling arbitrary shapes.\nIn the ﬁ lm industry, surfaces are oft en represented by a collection of rect-\nangular patches each formed from a two-dimensional spline deﬁ ned by a small \nnumber of control points. Various kinds of splines are used, including Bézi-\ner surfaces (e.g., bicubic patches , which are third-order Béziers —see htt p://\nen.wikipedia.org/wiki/Bezier_surface for more information), nonuniform \nrational B-splines (NURBS—see htt p://en.wikipedia.org/wiki/Nurbs), Bézi-\ner triangles, and N-patches (also known as normal patches—see htt p://www.\ngamasutra.com/features/20020715/mollerhaines_01.htm for more details). \nModeling with patches is a bit like covering a statue with litt le rectangles of \ncloth or paper maché.\nHigh-end ﬁ lm rendering engines like Pixar’s RenderMan use subdivision \nsurfaces to deﬁ ne geometric shapes. Each surface is represented by a mesh of \ncontrol polygons (much like a spline), but the polygons can be subdivided \ninto smaller and smaller polygons using the Catmull-Clark algorithm. This \nsubdivision typically proceeds until the individual polygons are smaller than \na single pixel in size. The biggest beneﬁ t of this approach is that no matt er \nhow close the camera gets to the surface, it can always be subdivided further \nso that its silhouett e edges won’t look faceted. To learn more about subdivi-\nsion surfaces, check out the following great article on Gamasutra: htt p://www.\ngamasutra.com/features/20000411/sharp_pfv.htm.\n\n\n403 \n10.1.1.2. Triangle Meshes\n Game developers have traditionally modeled their surfaces using triangle \nmeshes. Triangles serve as a piece-wise linear approximation to a surface, \nmuch as a chain of connected line segments acts as a piece-wise approxima-\ntion to a function or curve (see Figure 10.2).\nTriangles are the polygon of choice for real-time rendering because they \nhave the following desirable properties:\nz The triangle is the simplest type of polygon. Any fewer than three vertices, \nand we wouldn’t have a surface at all.\nz A triangle is always planar. Any polygon with four or more vertices need \nnot have this property, because while the ﬁ rst three vertices deﬁ ne a \nplane, the fourth vertex might lie above or below that plane.\nz Triangles remain triangles under most kinds of transformations, including \naﬃ  ne transforms and perspective projections. At worst, a triangle viewed \nedge-on will degenerate into a line segment. At every other orientation, \nit remains triangular.\nz Virtually all commercial graphics-acceleration hardware is designed around \ntriangle rasterization. Starting with the earliest 3D graphics accelerators \nfor the PC, rendering hardware has been designed almost exclusively \naround triangle rasterization. This decision can be traced all the way \nback to the ﬁ rst soft ware rasterizers used in the earliest 3D games like \nCastle Wolfenstein 3D and Doom . Like it or not, triangle-based technolo-\ngies are entrenched in our industry and probably will be for years to \ncome.\nTessellation\nThe term tessellation describes a process of dividing a surface up into a collec-\ntion of discrete polygons (which are usually either quadrilaterals, also known \nas quads, or triangles). Triangulation is tessellation of a surface into triangles.\nOne problem with the kind of triangle mesh used in games is that its level \nof tessellation is ﬁ xed by the artist when he or she creates it. Fixed tessellation \nFigure 10.2. A mesh of \ntriangles is a linear ap-\nproximation to a sur-\nface, just as a series of \nconnected \nline \nseg-\nments can serve as a \nlinear approximation to \na function or curve.\nx\nf(x)\nFigure 10.3. Fixed tessellation can cause an object’s silhouette edges to look blocky, especially \nwhen the object is close to the camera.\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n\n\n404 \n10. The Rendering Engine\ncan cause an object’s silhouett e edges to look blocky, as shown in Figure 10.3; \nthis is especially noticeable when the object is close to the camera.\nIdeally, we’d like a solution that can arbitrarily increase tessellation as an \nobject gets closer to the virtual camera. In other words, we’d like to have a uni-\nform triangle-to-pixel density, no matt er how close or far away the object is. \nSubdivision surfaces can achieve this ideal—surfaces can be tessellated based \non distance from the camera, so that every triangle is less than one pixel in \nsize.\nGame developers oft en att empt to approximate this ideal of uniform tri-\nangle-to-pixel density by creating a chain of alternate versions of each triangle \nmesh, each known as a level of detail (LOD). The ﬁ rst LOD, oft en called LOD 0, \nrepresents the highest level of tessellation; it is used when the object is very \nclose to the camera. Subsequent LODs are tessellated at lower and lower reso-\nlutions (see Figure 10.4). As the object moves farther away from the camera, \nthe engine switches from LOD 0 to LOD 1 to LOD 2, and so on. This allows the \nrendering engine to spend the majority of its time transforming and lighting \nthe vertices of the objects that are closest to the camera (and therefore occupy \nthe largest number of pixels on-screen).\nSome game engines apply dynamic tessellation techniques to expansive \nmeshes like water or terrain. In this technique, the mesh is usually represented \nby a height ﬁ eld deﬁ ned on some kind of regular grid patt ern. The region of \nthe mesh that is closest to the camera is tessellated to the full resolution of \nthe grid. Regions that are farther away from the camera are tessellated using \nfewer and fewer grid points.\nProgressive meshes are another technique for dynamic tessellation and \nLODing. With this technique, a single high-resolution mesh is created for dis-\nplay when the object is very close to the camera. (This is essentially the LOD 0 \nFigure 10.4. A chain of LOD meshes, each with a ﬁ xed level of tessellation, can be used to \napproximate uniform triangle-to-pixel density. The leftmost torus is constructed from 5000 \ntriangles, the center torus from 450 triangles, and the rightmost torus from 200 triangles.\n\n\n405 \nmesh.) This mesh is automatically detessellated as the object gets farther away \nby collapsing certain edges. In eﬀ ect, this process automatically generates a \nsemi-continuous chain of LODs. See htt p://research.microsoft .com/en-us/um/\npeople/hoppe/pm.pdf for a detailed discussion of progressive mesh technol-\nogy.\n10.1.1.3. Constructing a Triangle Mesh\n Now that we understand what triangle meshes are and why they’re used, let’s \ntake a brief look at how they’re constructed.\nWinding Order\nA triangle is deﬁ ned by the position vectors of its three vertices, which we can \ndenote p1 , p2 , and p3. The edges of a triangle can be found by simply subtract-\ning the position vectors of adjacent vertices. For example,\n \ne12 = p2 – p1 ,\ne13 = p3 – p1 ,\n \ne23 = p3 – p2.\nThe normalized cross product of any two edges deﬁ nes a unit face normal N:\n \n12\n13\n12\n13\n.\n×\n=\n×\ne\ne\nN\ne\ne\n \nThese derivations are illustrated in Figure 10.5. To know the direction of the \nface normal (i.e., the sense of the edge cross product), we need to deﬁ ne which \nside of the triangle should be considered the front (i.e., the outside surface of \nan object) and which should be the back (i.e., its inside surface). This can be \ndeﬁ ned easily by specifying a winding order —clockwise (CW) or counterclock-\nwise (CCW).\nMost low-level graphics APIs give us a way to cull back-facing triangles \nbased on winding order. For example, if we set the cull mode parameter in Di-\nFigure 10.5. Deriving the edges and plane of a triangle from its vertices.\np1\np2\np3\ne12\nN\ne13\ne23\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n\n\n406 \n10. The Rendering Engine\nrect3D (D3DRS_CULL) to D3DCULLMODE_CW, then any triangle whose vertices \nwind in a clockwise fashion in screen space will be treated as a back-facing \ntriangle and will not be drawn.\nBack face culling is important because we generally don’t want to waste \ntime drawing triangles that aren’t going to be visible anyway. Also, rendering \nthe back faces of transparent objects can actually cause visual anomalies. The \nchoice of winding order is an arbitrary one, but of course it must be consistent \nacross all assets in the entire game. Inconsistent winding order is a common \nerror among junior 3D modelers.\nTriangle Lists\nThe easiest way to deﬁ ne a mesh is simply to list the vertices in groups of \nthree, each triple corresponding to a single triangle. This data structure is \nknown as a triangle list ; it is illustrated in Figure 10.6.\nFigure 10.6.  A triangle list.\nV0\nV1\nV2\nV3\nV4\nV5\nV6\nV7\n...\nV5\nV7\nV6\nV0\nV5\nV1\nV1\nV2\nV3\nV0\nV1\nV3\nIndexed Triangle Lists\nYou probably noticed that many of the vertices in the triangle list shown in \nFigure 10.6 were duplicated, oft en multiple times. As we’ll see in Section \n10.1.2.1, we oft en store quite a lot of metadata with each vertex, so repeating \nthis data in a triangle list wastes memory. It also wastes GPU bandwidth, be-\ncause a duplicated vertex will be transformed and lit multiple times.\nFor these reasons, most rendering engines make use of a more eﬃ  cient \ndata structure known as an indexed triangle list . The basic idea is to list the \nvertices once with no duplication and then to use light-weight vertex indi-\nces (usually occupying only 16 bits each) to deﬁ ne the triples of vertices that \nconstitute the triangles. The vertices are stored in an array known as a vertex \n\n\n407 \nbuﬀ er (DirectX) or vertex array (OpenGL). The indices are stored in a separate \nbuﬀ er known as an index buﬀ er or index array. This technique is shown in Fig-\nure 10.7.\nStrips and Fans\nSpecialized mesh data structures known as triangle strips and triangle fans are \nsometimes used for game rendering. Both of these data structures eliminate \nthe need for an index buﬀ er, while still reducing vertex duplication to some \ndegree. They accomplish this by predeﬁ ning the order in which vertices must \nappear and how they are combined to form triangles.\nV0\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nIndices\n0\n1\n3\n1\n2\n3\n0\n5\n1\n...\n5\n7\n6\nVertices\nV0\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nFigure 10.7.  An indexed triangle list.\nInterpreted \nas triangles:\n0\n1\n2\n1\n3\n2\n2\n3\n4\n3\n5\n4\nV0\nV1\nV2\nV3\nV4\nV5\nVertices\nV0\nV1\nV2\nV3\nV4\nV5\nFigure 10.8.  A triangle strip.\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n\n\n408 \n10. The Rendering Engine\nIn a strip, the ﬁ rst three vertices deﬁ ne the ﬁ rst triangle. Each subsequent \nvertex forms an entirely new triangle, along with its previous two neigh-\nbors. To keep the winding order of a triangle strip consistent, the previous \ntwo neighbor vertices swap places aft er each new triangle. A triangle strip is \nshown in Figure 10.8.\nIn a fan, the ﬁ rst three vertices deﬁ ne the ﬁ rst triangle and each subse-\nquent vertex deﬁ nes a new triangle with the previous vertex and the ﬁ rst ver-\ntex in the fan. This is illustrated in Figure 10.9.\nVertex Cache Optimization\n When a GPU processes an indexed triangle list, each triangle can refer to any \nvertex within the vertex buﬀ er. The vertices must be processed in the order \nthey appear within the triangles, because the integrity of each triangle must be \nmaintained for the rasterization stage. As vertices are processed by the vertex \nshader , they are cached for reuse. If a subsequent primitive refers to a vertex \nthat already resides in the cache, its processed att ributes are used instead of \nreprocessing the vertex.\nStrips and fans are used in part because they can potentially save memory \n(no index buﬀ er required) and in part because they tend to improve the cache \ncoherency of the memory accesses made by the GPU to video RAM. Even \nbett er, we can use an indexed strip or indexed fan to virtually eliminate vertex \nduplication (which can oft en save more memory than eliminating the index \nbuﬀ er), while still reaping the cache coherency beneﬁ ts of the strip or fan ver-\ntex ordering.\nIndexed triangle lists can also be cache-optimized without restricting \nourselves to strip or fan vertex ordering. A vertex cache optimizer is an oﬄ  ine \ngeometry processing tool that att empts to list the triangles in an order that \nFigure 10.9.  A triangle fan.\nInterpreted \nas triangles:\n0\n1\n2\n0\n2\n3\n0\n3\n4\nV0\nV1\nV2\nV3\nV4\nVertices\nV0\nV4\nV3\nV2\nV1\n\n\n409 \noptimizes vertex reuse within the cache. It generally takes into account factors \nsuch as the size of the vertex cache(s) present on a particular type of GPU and \nthe algorithms used by the GPU to decide when to cache vertices and when \nto discard them. For example, the vertex cache optimizer included in Sony’s \nEdge geometry processing library can achieve rendering throughput that is up \nto 4% bett er than what is possible with triangle stripping.\n10.1.1.4. Model Space\nThe position vectors of a triangle mesh’s vertices are usually speciﬁ ed relative \nto a convenient local coordinate system called model space , local space,  or object \nspace. The origin of model space is usually either in the center of the object or \nat some other convenient location, like on the ﬂ oor between the feet of a char-\nacter or on the ground at the horizontal centroid of the wheels of a vehicle.\nAs we learned in Section 4.3.9.1, the sense of the model space axes is ar-\nbitrary, but the axes typically align with the natural “front,” “left ” or “right,” \nand “up” directions on the model. For a litt le mathematical rigor, we can de-\nﬁ ne three unit vectors F, L (or R), and U and map them as desired onto the \nunit basis vectors i, j, and k (and hence to the x-, y-, and z-axes, respectively) \nin model space. For example, a common mapping is L = i, U = j, and F = k. \nThe mapping is completely arbitrary, but it’s important to be consistent for all \nmodels across the entire engine. Figure 10.10 shows one possible mapping of \nthe model space axes for an aircraft  model.\nL = i\nF = k\nU = j\nFigure 10.10.  One possible mapping of the model space axes.\n10.1.1.5. World Space and Mesh Instancing\nMany individual meshes are composed into a complete scene by position-\ning and orienting them within a common coordinate system known as world \nspace . Any one mesh might appear many times in a scene—examples include \na street lined with identical lamp posts, a faceless mob of soldiers, or a swarm \nof spiders att acking the player. We call each such object a mesh instance .\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n\n\n410 \n10. The Rendering Engine\nA mesh instance contains a reference to its shared mesh data and also \nincludes a transformation matrix that converts the mesh’s vertices from model \nspace to world space, within the context of that particular instance. This ma-\ntrix is called the model-to-world matrix, or sometimes just the world matrix. Us-\ning the notation from Section 4.3.10.2, this matrix can be writt en as follows:\n \n \n(\n)\n,\n1\nM\nW\nM\nW\nM\n→\n→\n⎡\n⎤\n=⎢\n⎥\n⎣\n⎦\nRS\n0\nM\nt\nwhere the upper 3 × 3 matrix (\n)M\nW\n→\nRS\n rotates and scales model-space ver-\ntices into world space, and \nM\nt\n is the translation of the model space axes ex-\npressed in world space. If we have the unit model space basis vectors \n,\nM\ni\n \n,\nM\nj\nand \n,\nM\nk\n expressed in world space coordinates, this matrix can also be writt en \nas follows:\n \n \n0\n0\n.\n0\n1\nM\nM\nM\nW\nM\nM\n→\n⎡\n⎤\n⎢\n⎥\n⎢\n⎥\n=⎢\n⎥\n⎢\n⎥\n⎢\n⎥\n⎣\n⎦\ni\nj\nM\nk\nt\n \nGiven a vertex expressed in model-space coordinates, the rendering en-\ngine calculates its world-space equivalent as follows:\n \n.\nW\nM\nM\nW\n→\n=\nv\nv M\nWe can think of the matrix MM→W as a description of the position and orienta-\ntion of the model space axes themselves, expressed in world space coordi-\nnates. Or we can think of it as a matrix that transforms vertices from model \nspace to world space.\nWhen rendering a mesh, the model-to-world matrix is also applied to the \nsurface normals of the mesh (see Section 10.1.2.1). Recall from Section 4.3.11, \nthat in order to transform normal vectors properly, we must multiply them \nby the inverse transpose of the model-to-world matrix. If our matrix does not \ncontain any scale or shear, we can transform our normal vectors correctly by \nsimply sett ing their w components to zero prior to multiplication by the mod-\nel-to-world matrix, as described in Section 4.3.6.1.\nSome meshes like buildings, terrain, and other background elements are \nentirely static and unique. The vertices of these meshes are oft en expressed in \nworld space, so their model-to-world matrices are identity and can be ignored.\n10.1.2. Describing the Visual Properties of a Surface\n In order to properly render and light a surface, we need a description of its \nvisual properties. Surface properties include geometric information, such as the \n\n\n411 \ndirection of the surface normal at various points on the surface. They also \nencompass a description of how light should interact with the surface. This \nincludes diﬀ use color, shininess/reﬂ ectivity, roughness or texture, degree of \nopacity or transparency, index of refraction, and other optical properties. Sur-\nface properties might also include a speciﬁ cation of how the surface should \nchange over time (e.g., how an animated character’s skin should track the \njoints of its skeleton or how the surface of a body of water should move).\nThe key to rendering photorealistic images is properly accounting for \nlight’s behavior as it interacts with the objects in the scene. Hence rendering \nengineers need to have a good understanding of how light works, how it is \ntransported through an environment, and how the virtual camera “senses” it \nand translates it into the colors stored in the pixels on-screen.\n10.1.2.1. Introduction to Light and Color\nLight is electromagnetic radiation; it acts like both a wave and a particle in \ndiﬀ erent situations. The color of light is determined by its intensity I and its \nwavelength λ  (or its frequency f, where f = 1/λ). The visible gamut ranges from \na wavelength of 740 nm (or a frequency of 430 THz) to a wavelength of 380 nm \n(750 THz). A beam of light may contain a single pure wavelength (i.e., the \ncolors of the rainbow, also known as the spectral colors ), or it may contain a \nmixture of various wavelengths. We can draw a graph showing how much of \neach frequency a given beam of light contains, called a spectral plot . White light \ncontains a litt le bit of all wavelengths, so its spectral plot would look roughly \nlike a box extending across the entire visible band. Pure green light contains \nonly one wavelength, so its spectral plot would look like a single inﬁ nitesi-\nmally narrow spike at about 570 THz.\nLight-Object Interactions\nLight can have many complex interactions with matt er . Its behavior is gov-\nerned in part by the medium through which it is traveling and in part by the \nshape and properties of the interfaces between diﬀ erent types of media (air-\nsolid, air-water, water-glass, etc.). Technically speaking, a surface is really just \nan interface between two diﬀ erent types of media.\nDespite all of its complexity, light can really only do four things:\nz It can be absorbed ;\nz It can be reﬂ ected ;\nz It can be transmitt ed through an object, usually being refracted (bent) in \nthe process;\nz It can be diﬀ racted when passing through very narrow openings.\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n\n\n412 \n10. The Rendering Engine\nMost photorealistic rendering engines account for the ﬁ rst three of these be-\nhaviors; diﬀ raction is not usually taken into account because its eﬀ ects are \nrarely noticeable in most scenes.\nOnly certain wavelengths may be absorbed by a surface, while others are \nreﬂ ected. This is what gives rise to our perception of the color of an object. \nFor example, when white light falls on a red object, all wavelengths except \nred are absorbed, hence the object appears red. The same perceptual eﬀ ect is \nachieved when red light is cast onto a white object—our eyes don’t know the \ndiﬀ erence.\nReﬂ ections can be diﬀ use , meaning that an incoming ray is scatt ered equal-\nly in all directions. Reﬂ ections can also be specular , meaning that an incident \nlight ray will reﬂ ect directly or be spread only into a narrow cone. Reﬂ ections \ncan also be anisotropic , meaning that the way in which light reﬂ ects from a sur-\nface changes depending on the angle at which the surface is viewed.\nWhen light is transmitt ed through a volume, it can be scatt ered (as is the \ncase for translucent objects), partially absorbed (as with colored glass), or re-\nfracted (as happens when light travels through a prism). The refraction an-\ngles can be diﬀ erent for diﬀ erent wavelengths, leading to spectral spreading. \nThis is why we see rainbows when light passes through raindrops and glass \nprisms. Light can also enter a semi-solid surface, bounce around, and then exit \nthe surface at a diﬀ erent point from the one at which it entered the surface. We \ncall this subsurface scatt ering , and it is one of the eﬀ ects that gives skin, wax, \nand marble their characteristic warm appearance.\nColor Spaces and Color Models\nA color model is a three-dimensional coordinate system that measures colors. \nA color space is a speciﬁ c standard for how numerical colors in a particular \ncolor model should be mapped onto the colors perceived by human beings in \nthe real world. Color models are typically three-dimensional because of the \nthree types of color sensors (cones) in our eyes, which are sensitive to diﬀ erent \nwavelengths of light.\nThe most commonly used color model in computer graphics is the RGB \nmodel. In this model, color space is represented by a unit cube, with the rela-\ntive intensities of red, green, and blue light measured along its axes. The red, \ngreen, and blue components are called color channels . In the canonical RGB \ncolor model, each channel ranges from zero to one. So the color (0, 0, 0) repre-\nsents black, while (1, 1, 1) represents white.\nWhen colors are stored in a bitmapped image , various color formats can \nbe employed. A color format is deﬁ ned in part by the number of bits per pixel \nit occupies and, more speciﬁ cally, the number of bits used to represent each \ncolor channel. The RGB888 format uses eight bits per channel, for a total of \n\n\n413 \n24 bits per pixel. In this format, each channel ranges from 0 to 255 rather than \nfrom zero to one. RGB565 uses ﬁ ve bits for red and blue and six for green, for \na total of 16 bits per pixel. A palett ed format might use eight bits per pixel to \nstore indices into a 256-element color palett e, each entry of which might be \nstored in RGB888 or some other suitable format. \nA number of other color models are also used in 3D rendering. We’ll see \nhow the log-LUV color model is used for high dynamic range (HDR) lighting \nin Section 10.3.1.5.\nOpacity and the Alpha Channel\nA fourth channel called alpha  is oft en tacked on to RGB color vectors. As men-\ntioned in Section 10.1.1, alpha measures the opacity of an object. When stored \nin an image pixel, alpha represents the opacity of the pixel.\nRGB color formats can be extended to include an alpha channel, in which \ncase they are referred to as RGBA or ARGB color formats. For example, \nRGBA8888 is a 32 bit-per-pixel format with eight bits each for red, green, blue, \nand alpha. RGBA5551 is a 16 bit-per-pixel format with one-bit alpha; in this \nformat, colors can either be fully opaque or fully transparent.\n10.1.2.2. Vertex Attributes\nThe simplest way to describe the visual properties of a surface is to specify \nthem at discrete points on the surface. The vertices of a mesh are a conve-\nnient place to store surface properties, in which case they are called vertex \natt ributes .\nA typical triangle mesh includes some or all of the following att ributes at \neach vertex. As rendering engineers, we are of course free to deﬁ ne any ad-\nditional att ributes that may be required in order to achieve a desired visual \neﬀ ect on-screen.\nz Position vector (pi = [ pix  piy  piz ]). This is the 3D position of the ith vertex in \nthe mesh. It is usually speciﬁ ed in a coordinate space local to the object, \nknown as model space.\nz Vertex normal (ni = [ nix  niy  niz ]). This vector deﬁ nes the unit surface nor-\nmal at the position of vertex i. It is used in per-vertex dynamic lighting \ncalculations.\nz Vertex tangent (ti = [ tix  tiy  tiz ]) and bitangent (bi = [ bix  biy  biz ]). These two \nunit vectors lie perpendicular to one another and to the vertex normal \nni. Together, the three vectors ni , ti , and bi deﬁ ne a set of coordinate axes \nknown as tangent space . This space is used for various per-pixel lighting \ncalculations, such as normal mapping and environment mapping. (The \n10.1. Foundations of Depth-Buffered Triangle Rasterization\n\n\n414 \n10. The Rendering Engine\nbitangent bi is sometimes confusingly called the binormal , even though \nit is not normal to the surface.)\nz Diﬀ use color (di = [ dRi  dGi  dBi  dAi ]). This four-element vector describes \nthe diﬀ use color of the surface, expressed in the RGB color space. It \ntypically also includes a speciﬁ cation of the opacity or alpha (A) of the \nsurface at the position of the vertex. This color may be calculated oﬀ -line \n(static lighting) or at runtime (dynamic lighting).\nz Specular color (si = [ sRi  sGi  sBi  sAi ]). This quantity describes the color of the \nspecular highlight that should appear when light reﬂ ects directly from a \nshiny surface onto the virtual camera’s imaging plane.\nz Texture coordinates (uĳ  = [ uĳ   vĳ  ]). Texture coordinates allow a two- (or \nsometimes three-) dimensional bitmap to be “shrink wrapped” onto the \nsurface of a mesh—a process known as texture mapping. A texture co-\nordinate (u, v) describes the location of a particular vertex within the \ntwo-dimensional normalized coordinate space of the texture. A triangle \ncan be mapped with more than one texture; hence it can have more than \none set of texture coordinates. We’ve denoted the distinct sets of texture \ncoordinates via the subscript j above.\nz Skinning weights (kĳ  , wĳ ). In skeletal animation, the vertices of a mesh are \natt ached to individual joints in an articulated skeleton. In this case, each \nvertex must specify to which joint it is att ached via an index, k. A vertex \ncan be inﬂ uenced by multiple joints, in which case the ﬁ nal vertex posi-\ntion becomes a weighted average of these inﬂ uences. Thus, the weight of \neach joint’s inﬂ uence is denoted by a weighting factor w. In general, a \nvertex i can have multiple joint inﬂ uences j, each denoted by the pair of \nnumbers [ kĳ   wĳ  ].\n10.1.2.3. Vertex Formats\n Vertex att ributes are typically stored within a data structure such as a C \nstruct or a C++ class. The layout of such a data structure is known as a ver-\ntex format. Diﬀ erent meshes require diﬀ erent combinations of att ributes and \nhence need diﬀ erent vertex formats. The following are some examples of com-\nmon vertex formats:\n// Simplest possible vertex – position only (useful for\n// shadow volume extrusion, silhouette edge detection\n// for cartoon rendering, z prepass, etc.)\nstruct Vertex1P\n{\n Vector3 \n m_p; \n // \nposition\n};\n\n\n415 \n// A typical vertex format with position, vertex normal \n// and one set of texture coordinates.\nstruct Vertex1P1N1UV\n{\n Vector3 \nm_p; \n // \nposition\n \nVector3 \nm_n; \n \n// vertex normal\n F32 \n \n \nm_uv[2]; // (u, v) texture coordinate\n};\n// A skinned vertex with position, diffuse and specular \n// colors and four weighted joint influences.\nstruct Vertex1P1D1S2UV4J\n{\n Vector3 \nm_p; \n // \nposition\n \nColor4  \nm_d; \n \n// diffuse color and translucency\n Color4 \n m_S; \n // \nspecular color\n F32 \n \n \nm_uv0[2]; // first set of tex coords\n F32 \n \n \nm_uv1[2]; // second set of tex coords\n \nU8  \n \nm_k[4]; \n// four joint indices, and...\n F32 \n \n \nm_w[3]; \n// three joint weights, for   \n \n       // \nskinning\n       // \n(fourth calc’d from other    \n       // \nthree)\n};\nClearly the number of possible permutations of vertex att ributes—and \nhence the number of distinct vertex formats—can grow to be extremely large. \n(In fact the number of formats is theoretically unbounded, if one were to per-\nmit any number of texture coordinates and/or joint weights.) Management \nof all these vertex formats is a common source of headaches for any graphics \nprogrammer.\nSome steps can be taken to reduce the number of vertex formats that an \nengine has to support. In practical graphics applications, many of the theo-\nretically possible vertex formats are simply not useful, or they cannot be \nhandled by the graphics hardware or the game’s shaders. Some game teams \nalso limit themselves to a subset of the useful/feasible vertex formats in or-\nder to keep things more manageable. For example, they might only allow \nzero, two, or four joint weights per vertex, or they might decide to support \nno more than two sets of texture coordinates per vertex. Modern GPUs are \ncapable of extracting a subset of att ributes from a vertex data structure, so \ngame teams can also choose to use a single “überformat” for all meshes and \nlet the hardware select the relevant att ributes based on the requirements of the \nshader.\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n\n\n416 \n10. The Rendering Engine\n10.1.2.4. Attribute Interpolation\n The att ributes at a triangle’s vertices are just a coarse, discretized approxima-\ntion to the visual properties of the surface as a whole. When rendering a tri-\nangle, what really matt ers are the visual properties at the interior points of the \ntriangle as “seen” through each pixel on-screen. In other words, we need to \nknow the values of the att ributes on a per-pixel basis, not a per-vertex basis.\nOne simple way to determine the per-pixel values of a mesh’s surface at-\ntributes is to linearly interpolate the per-vertex att ribute data. When applied to \nvertex colors, att ribute interpolation is known as Gouraud shading . An example \nof Gouraud shading applied to a triangle is shown in Figure 10.11, and its ef-\nfects on a simple triangle mesh are illustrated in Figure 10.12. Interpolation is \nroutinely applied to other kinds of vertex att ribute information as well, such \nas vertex normals, texture coordinates, and depth.\nFigure 10.11.  A Gouraud-shaded triangle with different shades of gray at the vertices.\nFigure 10.12.  Gouraud shading can make faceted objects appear to be smooth.\nVertex Normals and Smoothing\nAs we’ll see in Section 10.1.3, lighting is the process of calculating the color of \nan object at various points on its surface, based on the visual properties of the \nsurface and the properties of the light impinging upon it. The simplest way to \nlight a mesh is to calculate the color of the surface on a per-vertex basis. In other \nwords, we use the properties of the surface and the incoming light to calculate \nthe diﬀ use color of each vertex (di). These vertex colors are then interpolated \nacross the triangles of the mesh via Gouraud shading.\n\n\n417 \nIn order to determine how a ray of light will reﬂ ect from a point on a sur-\nface, most lighting models make use of a vector that is normal to the surface at \nthe point of the light ray’s impact. Since we’re performing lighting calculations \non a per-vertex basis, we can use the vertex normal ni for this purpose. There-\nfore, the directions of a mesh’s vertex normals can have a signiﬁ cant impact on \nthe ﬁ nal appearance of a mesh.\nAs an example, consider a tall, thin, four-sided box. If we want the box \nto appear to be sharp-edged, we can specify the vertex normals to be perpen-\ndicular to the faces of the box. As we light each triangle, we will encounter the \nsame normal vector at all three vertices, so the resulting lighting will appear \nﬂ at, and it will abruptly change at the corners of the box just as the vertex \nnormals do.\nWe can also make the same box mesh look a bit like a smooth cylinder by \nspecifying vertex normals that point radially outward from the box’s center \nline. In this case, the vertices of each triangle will have diﬀ erent vertex nor-\nmals, causing us to calculate diﬀ erent colors at each vertex. Gouraud shading \nwill smoothly interpolate these vertex colors, resulting in lighting that appears \nto vary smoothly across the surface. This eﬀ ect is illustrated in Figure 10.13.\nFigure 10.13. The directions of a mesh’s vertex normals can have a profound effect on the \ncolors calculated during per-vertex lighting calculations.\n10.1.2.5. Textures\n When triangles are relatively large, specifying surface properties on a per-ver-\ntex basis can be too coarse-grained. Linear att ribute interpolation isn’t always \nwhat we want, and it can lead to undesirable visual anomalies.\nAs an example, consider the problem of rendering the bright specular \nhighlight that can occur when light shines on a glossy object. If the mesh is \n10.1. Foundations of Depth-Buffered Triangle Rasterization\n\n\n418 \n10. The Rendering Engine\nhighly tessellated, per-vertex lighting combined with Gouraud shading can \nyield reasonably good results. However, when the triangles are too large, the \nerrors that arise from linearly interpolating the specular highlight can become \njarringly obvious, as shown in Figure 10.14.\nTo overcome the limitations of per-vertex surface att ributes, rendering en-\ngineers use bitmapped images known as texture maps. A texture oft en contains \ncolor information and is usually projected onto the triangles of a mesh. In this \ncase, it acts a bit like those silly fake tatt oos we used to apply to our arms when \nwe were kids. But a texture can contain other kinds of visual surface proper-\nties as well as colors. And a texture needn’t be projected onto a mesh—for \nexample, a texture might be used as a stand-alone data table. The individual \npicture elements of a texture are called texels to diﬀ erentiate them from the \npixels on the screen.\nThe dimensions of a texture bitmap are constrained to be powers of two \non some graphics hardware. Typical texture dimensions include 256 × 256, \n512 × 512, 1024 × 1024, and 2048 × 2048, although textures can be any size on \nmost hardware, provided the texture ﬁ ts into video memory. Some graph-\nics hardware imposes additional restrictions, such as requiring textures to be \nsquare, or lift s some restrictions, such as not constraining texture dimensions \nto be powers of two.\nTypes of Textures\nThe most common type of texture is known as a diﬀ use map , or albedo map . It \ndescribes the diﬀ use surface color at each texel on a surface and acts like a de-\ncal or paint job on the surface.\nOther types of textures are used in computer graphics as well, including \nnormal maps (which store unit normal vectors at each texel, encoded as RGB \nvalues), gloss maps (which encode how shiny a surface should be at each texel), \nFigure 10.14.  Linear interpolation of vertex attributes does not always yield an adequate \ndescription of the visual properties of a surface, especially when tessellation is low.\n",
      "page_number": 421,
      "chapter_number": 22,
      "summary": "The goal of this chapter, then, is to pro-\nvide you with a broad understanding of real-time rendering technology and \nto serve as a jumping-oﬀ  point for further learning Key topics include vertex, light, and color.",
      "keywords": [
        "Triangle",
        "Rendering Engine",
        "vertex",
        "Depth-Buffered Triangle Rasterization",
        "Triangle Rasterization",
        "Rendering",
        "surface",
        "color",
        "light",
        "triangle mesh",
        "mesh",
        "object",
        "Model Space",
        "space",
        "Depth-Buffered Triangle"
      ],
      "concepts": [
        "vertex",
        "light",
        "color",
        "surfaces",
        "rendering",
        "render",
        "triangle",
        "engine",
        "objects",
        "vectors"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 1",
          "chapter": 54,
          "title": "Segment 54 (pages 523-530)",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 43,
          "title": "Segment 43 (pages 424-431)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 48,
          "title": "Segment 48 (pages 469-476)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 36,
          "title": "Segment 36 (pages 347-355)",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 42,
          "title": "Segment 42 (pages 416-423)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 441-462)",
      "start_page": 441,
      "end_page": 462,
      "detection_method": "synthetic",
      "content": "419 \nenvironment maps (which contain a picture of the surrounding environment \nfor rendering reﬂ ections), and many others. See Section 10.3.1 for a discussion \nof how various types of textures can be used for image-based lighting and \nother eﬀ ects.\nWe can actually use texture maps to store any information that we happen \nto need in our lighting calculations. For example, a one-dimensional texture \ncould be used to store sampled values of a complex math function, a color-to-\ncolor mapping table, or any other kind of look-up table (LUT) .\nTexture Coordinates\n Let’s consider how to project a two-dimensional texture onto a mesh. To do \nthis, we deﬁ ne a two-dimensional coordinate system known as texture space . \nA texture coordinate is usually represented by a normalized pair of numbers \ndenoted (u, v). These coordinates always range from (0, 0) at the bott om left  \ncorner of the texture to (1, 1) at the top right. Using normalized coordinates \nlike this allows the same coordinate system to be used regardless of the di-\nmensions of the texture.\nTo map a triangle onto a 2D texture, we simply specify a pair of texture \ncoordinates (ui, vi) at each vertex i. This eﬀ ectively maps the triangle onto the \nimage plane in texture space. An example of texture mapping is depicted in \nFigure 10.15.\nFigure 10.15.  An example of texture mapping. The triangles are shown both in three-dimen-\nsional space and in texture space.\nTexture Addressing Modes\n Texture coordinates are permitt ed to extend beyond the [0, 1] range. The \ngraphics hardware can handle out-of-range texture coordinates in any one of \n10.1. Foundations of Depth-Buffered Triangle Rasterization\n\n\n420 \n10. The Rendering Engine\nthe following ways. These are known as texture addressing modes; which mode \nis used is under the control of the user.\nz Wrap. In this mode, the texture is repeated over and over in every direc-\ntion. All texture coordinates of the form (ju, kv) are equivalent to the \ncoordinate (u, v), where j and k are arbitrary integers.\nz Mirror. This mode acts like wrap mode, except that the texture is mir-\nrored about the v-axis for odd integer multiples of u, and about the u- \naxis for odd integer multiples of v.\nz Clamp. In this mode, the colors of the texels around the outer edge of the \ntexture are simply extended when texture coordinates fall outside the \nnormal range.\nz Border color. In this mode, an arbitrary user-speciﬁ ed color is used for the \nregion outside the [0, 1] texture coordinate range.\nThese texture addressing modes are depicted in Figure 10.16.\nFigure 10.16.  Texture addressing modes.\nTexture Formats\n Texture bitmaps can be stored on disk in virtually any image format provided \nyour game engine includes the code necessary to read it into memory. Com-\nmon formats include Targa (.tga), Portable Network Graphics (.png), Win-\ndows Bitmap (.bmp), and Tagged Image File Format (.tif). In memory, textures \nare usually represented as two-dimensional (strided) arrays of pixels using \n\n\n421 \nvarious color formats, including RGB888, RGBA8888, RGB565, RGBA5551, \nand so on.\nMost modern graphics cards and graphics APIs support compressed tex-\ntures . DirectX supports a family of compressed formats known as DXT or S3 \nTexture Compression (S3TC). We won’t cover the details here, but the basic \nidea is to break the texture into 2 × 2 blocks of pixels and use a small color pal-\nett e to store the colors for each block. You can read more about S3 compressed \ntexture formats at htt p://en.wikipedia.org/wiki/S3_Texture_Compression.\nCompressed textures have the obvious beneﬁ t of using less memory than \ntheir uncompressed counterparts. An additional unexpected plus is that they \nare faster to render with as well. S3 compressed textures achieve this speed-up \nbecause of more cache-friendly memory access patt erns—4 × 4 blocks of ad-\njacent pixels are stored in a single 64- or 128-bit machine word—and because \nmore of the texture can ﬁ t into the cache at once. Compressed textures do \nsuﬀ er from compression artifacts. While the anomalies are usually not notice-\nable, there are situations in which uncompressed textures must be used.\nTexel Density and Mipmapping\nImagine rendering a full-screen quad (a rectangle composed of two triangles) \nthat has been mapped with a texture whose resolution exactly matches that of \nthe screen. In this case, each texel maps exactly to a single pixel on-screen, and \nwe say that the texel density (ratio of texels to pixels) is one. When this same \nquad is viewed at a distance, its on-screen area becomes smaller. The resolu-\ntion of the texture hasn’t changed, so the quad’s texel density is now greater \nthan one (meaning that more than one texel is contributing to each pixel).\nClearly texel density is not a ﬁ xed quantity—it changes as a texture-\nmapped object moves relative to the camera. Texel density aﬀ ects the memory \nconsumption and the visual quality of a three-dimensional scene. When the \ntexel density is much less than one, the texels become signiﬁ cantly larger than \na pixel on-screen, and you can start to see the edges of the texels. This destroys \nthe illusion. When texel density is much greater than one, many texels contrib-\nute to a single pixel on-screen. This can cause a moiré banding patt ern , as shown \nin Figure 10.17. Worse, a pixel’s color can appear to swim and ﬂ icker as diﬀ er-\nent texels within the boundaries of the pixel dominate its color depending on \nsubtle changes in camera angle or position. Rendering a distant object with a \nvery high texel density can also be a waste of memory if the player can never \nget close to it. Aft er all, why keep such a high-res texture in memory if no one \nwill ever see all that detail?\nIdeally we’d like to maintain a texel density that is close to one at all times, \nfor both nearby and distant objects. This is impossible to achieve exactly, but \nit can be approximated via a technique called mipmapping . For each texture, \n10.1. Foundations of Depth-Buffered Triangle Rasterization\n\n\n422 \n10. The Rendering Engine\nwe create a sequence of lower-resolution bitmaps, each of which is one-half \nthe width and one-half the height of its predecessor. We call each of these \nimages a mipmap, or mip level. For example, a 64 × 64 texture would have the \nfollowing mip levels: 64 × 64, 32 × 32, 16 × 16, 8 × 8, 4 × 4, 2 × 2, and 1 × 1, as \nshown in Figure 10.18. Once we have mipmapped our textures, the graphics \nhardware selects the appropriate mip level based on a triangle’s distance away \nfrom the camera, in an att empt to maintain a texel density that is close to one. \nFor example, if a texture takes up an area of 40 × 40 on-screen, the 64 × 64 mip \nlevel might be selected; if that same texture takes up only a 10 × 10 area, the \n16 × 16 mip level might be used. As we’ll see below, trilinear ﬁ ltering allows \nthe hardware to sample two adjacent mip levels and blend the results. In this \ncase, a 10 × 10 area might be mapped by blending the 16 × 16 and 8 × 8 mip \nlevels together.\nFigure 10.17.  A texel density greater than one can lead to a moiré pattern.\nFigure 10.18.  Mip levels for a 64×64 texture.\nWorld Space Texel Density\nThe term “texel density ” can also be used to describe the ratio of texels to world \nspace area on a textured surface. For example, a two meter cube mapped with \na 256 × 256 texture would have a texel density of 2562/22 = 16,384. I will call this \nworld space texel density to diﬀ erentiate it from the screen space texel density \nwe’ve been discussing thus far.\n\n\n423 \nWorld-space texel density need not be close to one, and in fact the speciﬁ c \nvalue will usually be much greater than one and depends entirely upon your \nchoice of world units. Nonetheless, it is important for objects to be texture \nmapped with a reasonably consistent world space texel density. For example, \nwe would expect all six sides of a cube to occupy the same texture area. If this \nwere not the case, the texture on one side of the cube would have a lower-res-\nolution appearance than another side, which can be noticeable to the player. \nMany game studios provide their art teams with guidelines and in-engine \ntexel density visualization tools in an eﬀ ort to ensure that all objects in the \ngame have a reasonably consistent world space texel density.\nTexture Filtering\n When rendering a pixel of a textured triangle, the graphics hardware samples \nthe texture map by considering where the pixel center falls in texture space. \nThere is usually not a clean one-to-one mapping between texels and pixels, \nand pixel centers can fall at any place in texture space, including directly on \nthe boundary between two or more texels. Therefore, the graphics hardware \nusually has to sample more than one texel and blend the resulting colors to \narrive at the actual sampled texel color. We call this texture ﬁ ltering.\nMost graphics cards support the following kinds of texture ﬁ ltering:\nz Nearest neighbor . In this crude approach, the texel whose center is closest to \nthe pixel center is selected. When mipmapping is enabled, the mip level is \nselected whose resolution is nearest to but greater than the ideal theoreti-\ncal resolution needed to achieve a screen-space texel density of one.\nz Bilinear . In this approach, the four texels surrounding the pixel center \nare sampled, and the resulting color is a weighted average of their col-\nors (where the weights are based on the distances of the texel centers \nfrom the pixel center). When mipmapping is enabled, the nearest mip \nlevel is selected.\nz Trilinear . In this approach, bilinear ﬁ ltering is used on each of the two \nnearest mip levels (one higher-res than the ideal and the other lower-\nres), and these results are then linearly interpolated. This eliminates \nabrupt visual boundaries between mip levels on-screen.\nz Anisotropic . Both bilinear and trilinear ﬁ ltering sample 2 × 2 square \nblocks of texels. This is the right thing to do when the textured sur-\nface is being viewed head-on, but it’s incorrect when the surface is at an \noblique angle relative to the virtual screen plane. Anisotropic ﬁ ltering \nsamples texels within a trapezoidal region corresponding to the view \nangle, thereby increasing the quality of textured surfaces when viewed \nat an angle.\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n\n\n424 \n10. The Rendering Engine\n10.1.2.6. Materials\nA material is a complete description of the visual properties of a mesh. This \nincludes a speciﬁ cation of the textures that are mapped to its surface and also \nvarious higher-level properties, such as which shader programs to use when \nrendering the mesh, the input parameters to those shaders, and other parame-\nters that control the functionality of the graphics acceleration hardware itself.\nWhile technically part of the surface properties description, vertex att ri-\nbutes are not considered to be part of the material. However, they come along \nfor the ride with the mesh, so a mesh-material pair contains all the informa-\ntion we need to render the object. Mesh-material pairs are sometimes called \nrender packets, and the term “geometric primitive” is sometimes extended to \nencompass mesh-material pairs as well.\nA 3D model typically uses more than one material. For example, a mod-\nel of a human would have separate materials for the hair, skin, eyes, teeth, \nand various kinds of clothing. For this reason, a mesh is usually divided into \nsubmeshes , each mapped to a single material. The Ogre3D rendering engine \nimplements this design via its Ogre::SubMesh class.\n10.1.3. Lighting Basics\nLighting is at the heart of all CG rendering. Without good lighting, an other-\nwise beautifully modeled scene will look ﬂ at and artiﬁ cial. Likewise, even the \nFigure 10.19.  A variation on the classic “Cornell box” scene illustrating how realistic lighting \ncan make even the simplest scene appear photorealistic.\n\n\n425 \nsimplest of scenes can be made to look extremely realistic when it is lit accu-\nrately. The classic “Cornell box” scene, shown in Figure 10.19, is an excellent \nexample of this.\nThe following sequence of screen shots from Naughty Dog’s Uncharted: \nDrake’s Fortune is another good illustration of the importance of lighting. In \nFigure 10.20, the scene is rendered without textures. Figure 10.21 shows the \nsame scene with diﬀ use textures applied. The fully lit scene is shown in Fig-\nure 10.22. Notice the marked jump in realism when lighting is applied to the \nscene.\nFigure 10.20.  A scene from Uncharted: Drake’s Fortune rendered without textures.\nFigure 10.21.  The same UDF scene with only diffuse textures applied.\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n\n\n426 \n10. The Rendering Engine\nThe term shading is oft en used as a loose generalization of lighting plus \nother visual eﬀ ects. As such, “shading” encompasses procedural deformation \nof vertices to simulate the motion of a water surface, generation of hair curves \nor fur shells, tessellation of high-order surfaces, and prett y much any other \ncalculation that’s required to render a scene.\nIn the following sections, we’ll lay the foundations of lighting that we’ll \nneed in order to understand graphics hardware and the rendering pipeline. \nWe’ll return to the topic of lighting in Section 10.3, where we’ll survey some \nadvanced lighting and shading techniques.\n10.1.3.1. Local and Global Illumination Models\nRendering engines use various mathematical models of light-surface and light-\nvolume interactions called light transport models . The simplest models only ac-\ncount for direct lighting in which light is emitt ed, bounces oﬀ  a single object in \nthe scene, and then proceeds directly to the imaging plane of the virtual cam-\nera. Such simple models are called local illumination models , because only the \nlocal eﬀ ects of light on a single object are considered; objects do not aﬀ ect one \nanother’s appearance in a local lighting model. Not surprisingly, local models \nwere the ﬁ rst to be used in games, and they are still in use today—local light-\ning can produce surprisingly realistic results in some circumstances.\nTrue photorealism can only be achieved by accounting for indirect light-\ning , where light bounces multiple times oﬀ  many surfaces before reaching the \nvirtual camera. Lighting models that account for indirect lighting are called \nglobal illumination models . Some global illumination models are targeted at \nsimulating one speciﬁ c visual phenomenon, such as producing realistic shad-\nFigure 10.22.  The UDF scene with full lighting.\n\n\n427 \nows, modeling reﬂ ective surfaces, accounting for interreﬂ ection between ob-\njects (where the color of one object aﬀ ects the colors of surrounding objects), \nand modeling caustic eﬀ ects (the intense reﬂ ections from water or a shiny \nmetal surface). Other global illumination models att empt to provide a holis-\ntic account of a wide range of optical phenomena. Ray tracing and radiosity \nmethods are examples of such technologies.\nGlobal illumination is described completely by a mathematical formula-\ntion known as the rendering equation or shading equation. It was introduced in \n1986 by J. T. Kajiya as part of a seminal SIGGRAPH paper. In a sense, every \nrendering technique can be thought of as a full or partial solution to the ren-\ndering equation, although they diﬀ er in their fundamental approach to solv-\ning it and in the assumptions, simpliﬁ cations, and approximations they make. \nSee htt p://en.wikipedia.org/wiki/Rendering_equation, [8], [1], and virtually \nany other text on advanced rendering and lighting for more details on the \nrendering equation.\n10.1.3.2. The Phong Lighting Model\nThe most common local lighting model employed by game rendering engines \nis the Phong reﬂ ection model . It models the light reﬂ ected from a surface as a \nsum of three distinct terms:\nz The ambient term models the overall lighting level of the scene. It is a \ngross approximation of the amount of indirect bounced light present \nin the scene. Indirect bounces are what cause regions in shadow not to \nappear totally black.\nz The diﬀ use term accounts for light that is reﬂ ected uniformly in all direc-\ntions from each direct light source. This is a good approximation to the \nway in which real light bounces oﬀ  a matt e surface, such as a block of \nwood or a piece of cloth.\nz The specular term models the bright highlights we sometimes see when \nviewing a glossy surface. Specular highlights occur when the view-\ning angle is closely aligned with a path of direct reﬂ ection from a light \nsource.\nFigure 10.23 shows how the ambient, diﬀ use, and specular terms add together \nto produce the ﬁ nal intensity and color of a surface.\nTo calculate Phong reﬂ ection at a speciﬁ c point on a surface, we require \na number of input parameters. The Phong model is normally applied to all \nthree color channels (R, G and B) independently, so all of the color parameters \nin the following discussion are three-element vectors. The inputs to the Phong \nmodel are:\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n\n\n428 \n10. The Rendering Engine\nz the viewing direction vector V = [ Vx  Vy  Vz ], which extends from the \nreﬂ ection point to the virtual camera’s focal point (i.e., the negation of \nthe camera’s world-space “front” vector);\nz the \nambient \nlight \nintensity \nfor \nthe \nthree \ncolor \nchannels, \nA = [ AR  AG  AB ];\nz the surface normal N = [ Nx  Ny  Nz ] at the point the light ray impinges \non the surface;\nz the surface reﬂ ectance properties, which are\nthe ambient reﬂ ectivity\n \n□\n kA,\nthe diﬀ use reﬂ ectivity \n \n□\nkD,\nthe specular reﬂ ectivity \n \n□\nkS,\nand a specular “glossiness” exponent \n \n□\nα;\nz and, for each light source i, \nthe light’s color and intensity \n \n□\nCi = [ CRi  CGi  CBi ],\nthe direction vector \n \n□\nLi from the reﬂ ection point to the light source.\nIn the Phong model, the intensity I of light reﬂ ected from a point can be ex-\npressed with the following vector equation:\n \n(\n)\n(\n)\n,\nA\nD\ni\nS\ni\ni\ni\nk\nk\nk\nα\n⎡\n⎤\n=\n+\n⋅\n+\n⋅\n⎣\n⎦\n∑\nI\nA\nN L\nR\nV\nC\n \nwhere the sum is taken over all lights i aﬀ ecting the point in question. This can \nbe broken into three scalar equations, one for each color channel:\n \n(\n)\n(\n)\n,\n(\n)\n(\n)\n,\n(\n)\n(\n)\n.\nR\nA\nR\nD\ni\nS\ni\nRi\ni\nG\nA\nG\nD\ni\nS\ni\nGi\nB\nA\nB\nD\ni\nS\ni\nBi\nI\nk A\nk\nk\nC\nI\nk A\nk\nk\nC\nI\nk A\nk\nk\nC\nα\nα\nα\n⎡\n⎤\n=\n+\n⋅\n+\n⋅\n⎣\n⎦\n⎡\n⎤\n=\n+\n⋅\n+\n⋅\n⎣\n⎦\n⎡\n⎤\n=\n+\n⋅\n+\n⋅\n⎣\n⎦\n∑\n∑\n∑\nN L\nR\nV\nN L\nR\nV\nN L\nR\nV\ni\ni\n \nFigure 10.23. Ambient, diffuse and specular terms are summed to calculate Phong \nreﬂ ection.\n\n\n429 \nIn these equations, the vector Ri = [ Rxi  Ryi  Rzi ] is the reﬂ ection of the light ray’s \ndirection vector Li about the surface normal N.\nThe vector Ri can be easily calculated via a bit of vector math. Any vec-\ntor can be expressed as a sum of its tangential and normal components. For \nexample, we can break up the light direction vector L as follows:\n \n.\nT\nN\n=\n+\nL\nL\nL\n \nWe know that the dot product (N · L) represents the projection of L normal \nto the surface (a scalar quantity). So the normal component LN is just the unit \nnormal vector N scaled by this dot product:\n \n(\n) .\nN =\n⋅\nL\nN L N\nThe reﬂ ected vector R has the same normal component as L but the opposite \ntangential component (–LT). So we can ﬁ nd R as follows:\n \n \nThis equation can be used to ﬁ nd all of the Ri values corresponding to the light \ndirections Li.\nBlinn-Phong\nThe Blinn-Phong lighting model is a variation on Phong shading that calcu-\nlates specular reﬂ ection in a slightly diﬀ erent way. We deﬁ ne the vector H to \nbe the vector that lies halfway between the view vector V and the light direc-\ntion vector L. The Blinn-Phong specular component is then (N · H)a, as op-\nposed to Phong’s (R · V)α. The exponent a is slightly diﬀ erent than the Phong \nexponent α, but its value is chosen in order to closely match the equivalent \nPhong specular term.\nThe Blinn-Phong model oﬀ ers increased runtime eﬃ  ciency at the cost of \nsome accuracy, although it actually matches empirical results more closely \nthan Phong for some kinds of surfaces. The Blinn-Phong model was used \nalmost exclusively in early computer games and was hard-wired into the \nﬁ xed-function pipelines of early GPUs. See htt p://en.wikipedia.org/wiki/\nBlinn%E2%80%93Phong_shading_model for more details.\nBRDF Plots\nThe three terms in the Phong lighting model are special cases of a general local \nreﬂ ection model known as a bidirectional reﬂ ection distribution function (BRDF ). \n10.1. Foundations of Depth-Buffered Triangle Rasterization\n(\n)\n2\n;\n2(\n)\n.\nN\nT\nN\nN\nN\n=\n−\n=\n−\n−\n=\n−\n=\n⋅\n−\nR\nL\nL\nL\nL\nL\nL\nL\nR\nN L N\nL\n\n\n430 \n10. The Rendering Engine\nA BRDF calculates the ratio of the outgoing (reﬂ ected) radiance along a given \nviewing direction V to the incoming irradiance along the incident ray L.\nA BRDF can be visualized as a hemispherical plot, where the radial dis-\ntance from the origin represents the intensity of the light that would be seen if \nthe reﬂ ection point were viewed from that direction. The diﬀ use Phong reﬂ ec-\ntion term is kD(N · L). This term only accounts for the incoming illumination \nray L, not the viewing angle V. Hence the value of this term is the same for all \nviewing angles. If we were to plot this term as a function of the viewing angle \nin three dimensions, it would look like a hemisphere centered on the point at \nwhich we are calculating the Phong reﬂ ection. This is shown in two dimen-\nsions in Figure 10.24.\nThe specular term of the Phong model is kS(R · V)α. This term is dependent \non both the illumination direction L and the viewing direction V. It produces \na specular “hot spot” when the viewing angle aligns closely with the reﬂ ection \nR of the illumination direction L about the surface normal. However, its con-\ntribution falls oﬀ  very quickly as the viewing angle diverges from the reﬂ ected \nillumination direction. This is shown in two dimensions in Figure 10.25.\n10.1.3.3. Modeling Light Sources\n In addition to modeling the light’s interactions with surfaces, we need to de-\nscribe the sources of light in the scene. As with all things in real-time rendering, \nwe approximate real-world light sources using various simpliﬁ ed models.\nFigure 10.24.  The diffuse term of the Phong reﬂ ection model is dependent upon N • L, but is \nindependent of the viewing angle V.\nFigure 10.25.  The specular term of the Phong reﬂ ection model is at its maximum when the \nviewing angle V coincides with the reﬂ ected light direction R and drops off quickly as V di-\nverges from R.\n\n\n431 \nStatic Lighting\n The fastest lighting calculation is the one you don’t do at all. Lighting is there-\nfore performed oﬀ -line whenever possible. We can precalculate Phong reﬂ ec-\ntion at the vertices of a mesh and store the results as diﬀ use vertex color at-\ntributes. We can also precalculate lighting on a per pixel basis and store the \nresults in a kind of texture map known as a light map . At runtime, the light \nmap texture is projected onto the objects in the scene in order to determine the \nlight’s eﬀ ects on them.\nYou might wonder why we don’t just bake lighting information directly \ninto the diﬀ use textures in the scene. There are a few reasons for this. For one \nthing, diﬀ use texture maps are oft en tiled and/or repeated throughout a scene, \nso baking lighting into them wouldn’t be practical. Instead, a single light map \nis usually generated per light source and applied to any objects that fall within \nthat light’s area of inﬂ uence. This approach permits dynamic objects to move \npast a light source and be properly illuminated by it. It also means that our \nlight maps can be of a diﬀ erent (oft en lower) resolution than our diﬀ use tex-\nture maps. Finally, a “pure” light map usually compresses bett er than one that \nincludes diﬀ use color information.\nAmbient Lights\nAn ambient light corresponds to the ambient term in the Phong lighting model. \nThis term is independent of the viewing angle and has no speciﬁ c direction. \nAn ambient light is therefore represented by a single color, corresponding to \nthe A color term in the Phong equation (which is scaled by the surface’s ambi-\nent reﬂ ectivity kA at runtime). The intensity and color of ambient light may \nvary from region to region within the game world.\nDirectional Lights\nA directional light models a light source that is eﬀ ectively an inﬁ nite distance \naway from the surface being illuminated—like the sun. The rays emanating \nfrom a directional light are parallel, and the light itself does not have any \nparticular location in the game world. A directional light is therefore modeled \nas a light color C and a direction vector L. A directional light is depicted in \nFigure 10.26.\nPoint (Omni-Directional) Lights\nA point light (omni-directional light) has a distinct position in the game world \nand radiates uniformly in all directions. The intensity of the light is usually \nconsidered to fall oﬀ  with the square of the distance from the light source, \nand beyond a predeﬁ ned maximum radius its eﬀ ects are simply clamped to \nzero. A point light is modeled as a light position P, a source color/intensity C, \nFigure 10.26.  Model \nof a directional light \nsource.\nFigure 10.27.  Mod-\nel of a point light \nsource.\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n\n\n432 \n10. The Rendering Engine\nand a maximum radius rmax. The rendering engine only applies the eﬀ ects of a \npoint light to those surfaces that fall within is sphere of inﬂ uence (a signiﬁ cant \noptimization). Figure 10.27 illustrates a point light.\nSpot Lights\nA spot light acts like a point light whose rays are restricted to a cone-shaped \nregion, like a ﬂ ashlight. Usually two cones are speciﬁ ed with an inner and an \nouter angle. Within the inner cone, the light is considered to be at full inten-\nsity. The light intensity falls oﬀ  as the angle increases from the inner to the \nouter angle, and beyond the outer cone it is considered to be zero. Within \nboth cones, the light intensity also falls oﬀ  with radial distance. A spot light is \nmodeled as a position P, a source color C, a central direction vector L, a maxi-\nmum radius rmax , and inner and outer cone angles θmin and θmax. Figure 10.28 \nillustrates a spot light source.\nArea Lights\n All of the light sources we’ve discussed thus far radiate from an idealized \npoint, either at inﬁ nity or locally. A real light source almost always has a non-\nzero area—this is what gives rise to the umbra and penumbra in the shadows \nit casts.\nRather than trying to model area lights explicitly, CG engineers oft en use \nvarious “tricks” to account for their behavior. For example to simulate a pen-\numbra, we might cast multiple shadows and blend the results, or we might \nblur the edges of a sharp shadow in some manner.\nEmissive Objects\n Some surfaces in a scene are themselves light sources. Examples include ﬂ ash-\nlights, glowing crystal balls, ﬂ ames from a rocket engine, and so on. Glowing \nsurfaces can be modeled using an emissive texture map —a texture whose colors \nare always at full intensity, independent of the surrounding lighting environ-\nment. Such a texture could be used to deﬁ ne a neon sign, a car’s headlights, \nand so on.\nSome kinds of emissive objects are rendered by combining multiple tech-\nniques. For example, a ﬂ ashlight might be rendered using an emissive texture \nfor when you’re looking head-on into the beam, a colocated spot light that \ncasts light into the scene, a yellow translucent mesh to simulate the light cone, \nsome camera-facing transparent cards to simulate lens ﬂ are (or a bloom eﬀ ect \nif high dynamic range lighting is supported by the engine), and a projected \ntexture to produce the caustic eﬀ ect that a ﬂ ashlight has on the surfaces it il-\nluminates. The ﬂ ashlight in Luigi’s Mansion is a great example of this kind of \neﬀ ect combination, as shown in Figure 10.29.\nFigure 10.28.  Model \nof a spot light source.\n\n\n433 \n10.1.4. The Virtual Camera\nIn computer graphics, the virtual camera is much simpler than a real camera \nor the human eye. We treat the camera as an ideal focal point with a rectangu-\nlar virtual sensing surface called the imaging rectangle ﬂ oating some small dis-\ntance in front of it. The imaging rectangle consists of a grid of square or rect-\nangular virtual light sensors, each corresponding to a single pixel on-screen. \nRendering can be thought of as the process of determining what color and \nintensity of light would be recorded by each of these virtual sensors.\n10.1.4.1. View Space\nThe focal point of the virtual camera is the origin of a 3D coordinate system \nknown as view space or camera space. The camera usually “looks” down the \npositive or negative z-axis in view space, with y up and x to the left  or right. \nTypical left - and right-handed view space axes are illustrated in Figure 10.30.\nFigure 10.29.  The ﬂ ashlight in Luigi’s Mansion is composed of numerous visual effects, in-\ncluding a cone of translucent geometry for the beam, a dynamic spot light to cast light into \nthe scene, an emissive texture on the lens, and camera-facing cards for the lens ﬂ are.\nLeft-Handed\nRight-Handed\nVirtual \nScreen\nVirtual \nScreen\nFrustum\nFrustum\nxC\nzC\nyC\nxC\nzC\nyC\nFigure 10.30.  Left- and right-handed camera space axes.\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n\n\n434 \n10. The Rendering Engine\nThe camera’s position and orientation can be speciﬁ ed using a view-to-\nworld matrix , just as a mesh instance is located in the scene with its model-to-\nworld matrix. If we know the position vector and three unit basis vectors of \ncamera space, expressed in world-space coordinates, the view-to-world ma-\ntrix can be writt en as follows, in a manner analogous to that used to construct \na model-to-view matrix:\n \n \n0\n0\n.\n0\n1\nV\nV\nV\nW\nV\nV\n→\n⎡\n⎤\n⎢\n⎥\n⎢\n⎥\n=⎢\n⎥\n⎢\n⎥\n⎢\n⎥\n⎣\n⎦\ni\nj\nM\nk\nt\n \nWhen rendering a triangle mesh, its vertices are transformed ﬁ rst from \nmodel space to world space, and then from world space to view space. To \nperform this latt er transformation, we need the world-to-view matrix , which \nis the inverse of the view-to-world matrix. This matrix is sometimes called the \nview matrix:\n \n1\nview\n(\n)\n.\nW\nV\nV\nW\n−\n→\n→\n=\n=\nM\nM\nM\nBe careful here. The fact that the camera’s matrix is inverted relative to the \nmatrices of the objects in the scene is a common point of confusion and bugs \namong new game developers.\nThe world-to-view matrix is oft en concatenated to the model-to-world \nmatrix prior to rendering a particular mesh instance. This combined matrix is \ncalled the model-view matrix in OpenGL. We precalculate this matrix so that the \nrendering engine only needs to do a single matrix multiply when transform-\ning vertices from model space into view space:\n \n \nmodel view.\nM\nV\nM\nW\nW\nV\n→\n→\n→\n-\n=\n=\nM\nM\nM\nM\n \n10.1.4.2. Projections\nIn order to render a 3D scene onto a 2D image plane, we use a special kind \nof transformation known as a projection . The perspective projection is the most \ncommon projection in computer graphics, because it mimics the kinds of im-\nages produced by a typical camera. With this projection, objects appear small-\ner the farther away they are from the camera—an eﬀ ect known as perspective \nforeshortening .\nThe length-preserving orthographic projection is also used by some games, \nprimarily for rendering plan views (e.g., front, side, and top) of 3D models or \ngame levels for editing purposes, and for overlaying 2D graphics onto the \nscreen for heads-up displays (HUDs) and the like. Figure 10.31 illustrates how \na cube would look when rendered with these two types of projections.\n\n\n435 \n10.1.4.3. The View Volume and the Frustum\nThe region of space that the camera can “see” is known as the view volume . A \nview volume is deﬁ ned by six planes. The near plane corresponds to the virtual \nimage-sensing surface. The four side planes correspond to the edges of the \nvirtual screen. The far plane is used as a rendering optimization to ensure that \nextremely distant objects are not drawn. It also provides an upper limit for the \ndepths that will be stored in the depth buﬀ er (see Section 10.1.4.8).\nWhen rendering the scene with a perspective projection, the shape of the \nview volume is a truncated pyramid known as a frustum . When using an or-\nthographic projection, the view volume is a rectangular prism. Perspective \nand orthographic view volumes are illustrated in Figure 10.32 and Figure \n10.33, respectively.\nThe six planes of the view volume can be represented compactly using six \nfour-element vectors (nxi , nyi , nzi , di), where n = (nx , ny , nz) is the plane normal \nand d is its perpendicular distance from the origin. If we prefer the point-\nnormal plane representation, we can also describe the planes with six pairs of \nvectors (Qi, ni), where Q is the arbitrary point on the plane and n is the plane \nnormal. (In both cases, i is the index of the plane.)\nFigure 10.31.  A cube rendered using a perspective projection (on the left) and an ortho-\ngraphic projection (on the right).\nFar \nPlane\nyV\nNear \nPlane\nxV\nzV\n(r, b, n)\n(r, b, f)\n(r, t, f)\n(l, t, f)\n(l, b, n)\n(l, t, n)\n(l, b, f)\n(r, t, n)\nFigure 10.32.  A perspective view volume (frustum).\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n\n\n436 \n10. The Rendering Engine\n10.1.4.4. Projection and Homogeneous Clip Space\nBoth perspective and orthographic projections transform points in view space \ninto a coordinate space called homogeneous clip space . This three-dimensional \nspace is really just a warped version of view space. The purpose of clip space \nis to convert the camera-space view volume into a canonical view volume that \nis independent both of the kind of projection used to convert the 3D scene into \n2D screen space, and of the resolution and aspect ratio of the screen onto which \nthe scene is going to be rendered.\nIn clip space, the canonical view volume is a rectangular prism extending \nfrom –1 to +1 along the x- and y-axes. Along the z-axis, the view volume ex-\ntends either from –1 to +1 (OpenGL) or from 0 to 1 (DirectX). We call this coor-\nFigure 10.33.  An orthographic view volume.\nFar \nPlane\nyV\nNear \nPlane\nxV\nzV\n(r, b, n)\n(r, b, f)\n(r, t, f)\n(l, t, f)\n(l, b, n)\n(l, t, n)\n(l, b, f)\n(r, t, n)\nFar \nPlane\nyH\nNear \nPlane\nxH\nzH\n(1, –1, –1)\n(1, –1, 1)\n(1, 1, 1)\n(–1, 1, 1)\n(–1, –1, –1)\n(–1, 1, –1)\nFigure 10.34.  The canonical view volume in homogeneous clip space.\n\n\n437 \ndinate system “clip space” because the view volume planes are axis-aligned, \nmaking it convenient to clip triangles to the view volume in this space (even \nwhen a perspective projection is being used). The canonical clip-space view \nvolume for OpenGL is depicted in Figure 10.34. Notice that the z-axis of clip \nspace goes into the screen, with y up and x to the right. In other words, homo-\ngeneous clip space is usually left -handed.\nPerspective Projection\nAn excellent explanation of perspective projection is given in Section 4.5.1 of \n[28], so we won’t repeat it here. Instead, we’ll simply present the perspective \nprojection matrix \nV\nH\n→\nM\n below. (The subscript V→H indicates that this ma-\ntrix transforms vertices from view space into homogeneous clip space.) If we \ntake view space to be right-handed, then the near plane intersects the z-axis \nat z = –n, and the far plane intersects it at z = –f. The virtual screen’s left , right, \nbott om, and top edges lie at x = l, x = r, y = b, and y = t on the near plane, respec-\ntively. (Typically the virtual screen is centered on the camera-space z-axis, in \nwhich case l = –r and b = –t, but this isn’t always the case.) Using these deﬁ ni-\ntions, the perspective projection matrix for OpenGL is as follows:\n \n \n2\n0\n0\n  0\n2\n0\n0\n  0\n.\n1\n2\n0\n0\n  0\nV\nH\nn\nr\nl\nn\nt\nb\nf\nn\nr\nl\nt\nb\nr\nl\nt\nb\nf\nn\nnf\nf\nn\n→\n⎡\n⎤\n⎛\n⎞\n⎜\n⎟\n⎢\n⎥\n⎝\n⎠\n−\n⎢\n⎥\n⎢\n⎥\n⎛\n⎞\n⎜\n⎟\n⎢\n⎥\n⎝\n⎠\n−\n⎢\n⎥\n=⎢\n⎥\n⎛\n⎞\n⎛\n⎞⎛\n⎞\n+\n+\n+\n⎜\n⎟\n⎢\n⎥\n−\n−\n⎜\n⎟⎜\n⎟\n⎝\n⎠⎝\n⎠\n−\n−\n−\n⎝\n⎠\n⎢\n⎥\n⎢\n⎥\n⎛\n⎞\n⎢\n⎥\n⎜\n⎟\n−\n⎢\n⎥\n−\n⎝\n⎠\n⎣\n⎦\nM\n \nDirectX deﬁ nes the z-axis extents of the clip-space view volume to lie in \nthe range [0, 1] rather thanin the range  [–1, 1] as OpenGL does. We can easily \nadjust the perspective projection matrix to account for DirectX’s conventions \nas follows:\n \n(\n)\n \nDirectX\n2\n0\n0\n  0\n2\n0\n0\n  0\n.\n1\n0\n0\n  0\nV\nH\nn\nr\nl\nn\nt\nb\nf\nr\nl\nt\nb\nr\nl\nt\nb\nf\nn\nnf\nf\nn\n→\n⎡\n⎤\n⎛\n⎞\n⎜\n⎟\n⎢\n⎥\n⎝\n⎠\n−\n⎢\n⎥\n⎢\n⎥\n⎛\n⎞\n⎜\n⎟\n⎢\n⎥\n⎝\n⎠\n−\n⎢\n⎥\n=⎢\n⎥\n⎛\n⎞\n⎛\n⎞⎛\n⎞\n+\n+\n⎜\n⎟\n⎢\n⎥\n−\n−\n⎜\n⎟⎜\n⎟\n⎝\n⎠⎝\n⎠\n−\n−\n−\n⎝\n⎠\n⎢\n⎥\n⎢\n⎥\n⎛\n⎞\n⎢\n⎥\n⎜\n⎟\n−\n⎢\n⎥\n−\n⎝\n⎠\n⎣\n⎦\nM\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n\n\n438 \n10. The Rendering Engine\nDivision by Z\n Perspective projection results in each vertex’s x- and y-coordinates being di-\nvided by its z-coordinate. This is what produces perspective foreshortening . \nTo understand why this happens, consider multiplying a view-space point \nV\np\n expressed in four-element homogeneous coordinates by the OpenGL per-\nspective projection matrix:\n \n \n \n2\n0\n0\n  0\n2\n0\n0\n  0\n[\n1] \n.\n1\n2\n0\n0\n  0\nH\nV\nV\nH\nVx\nVy\nVz\nn\nr\nl\nn\nt\nb\np\np\np\nf\nn\nr\nl\nt\nb\nr\nl\nt\nb\nf\nn\nnf\nf\nn\n→\n=\n⎡\n⎤\n⎛\n⎞\n⎜\n⎟\n⎢\n⎥\n⎝\n⎠\n−\n⎢\n⎥\n⎢\n⎥\n⎛\n⎞\n⎜\n⎟\n⎢\n⎥\n⎝\n⎠\n−\n⎢\n⎥\n=\n⎢\n⎥\n⎛\n⎞\n⎛\n⎞⎛\n⎞\n+\n+\n+\n⎜\n⎟\n⎢\n⎥\n−\n−\n⎜\n⎟⎜\n⎟\n⎝\n⎠⎝\n⎠\n−\n−\n−\n⎝\n⎠\n⎢\n⎥\n⎢\n⎥\n⎛\n⎞\n⎢\n⎥\n⎜\n⎟\n−\n⎢\n⎥\n−\n⎝\n⎠\n⎣\n⎦\np\np M\nThe result of this multiplication takes the form\n \n .\nH\nVz\na\nb\nc\np\n⎡\n⎤\n=\n−\n⎣\n⎦\np\n \n(10.1)\nWhen we convert any homogeneous vector into three dimensional coor-\ndinates, the x-, y-, and z-components are divided by the w-component:\n \n .\ny\nx\nz\nx\ny\nz\nw\nw\nw\nw\n⎡\n⎤\n⎡\n⎤≡\n⎣\n⎦⎢\n⎥\n⎣\n⎦ \nSo, aft er dividing Equation (10.1) by the homogeneous w-component, which is \nreally just the negative view-space z-coordinate \nVz\np\n−\n, we have:\n \n \n[\n].\nH\nVz\nVz\nVz\nHx\nHy\nHz\na\nb\nc\np\np\np\np\np\np\n⎡\n⎤\n=⎢\n⎥\n−\n−\n−\n⎣\n⎦\n=\np\nThus the homogeneous clip space coordinates have been divided by the view-\nspace z-coordinate, which is what causes perspective foreshortening.\nPerspective-Correct Vertex Attribute Interpolation\nIn Section 10.1.2.4, we learned that vertex att ributes are interpolated in order to \ndetermine appropriate values for them within the interior of a triangle. Att ri-\nbute interpolation is performed in screen space. We iterate over each pixel of the \nscreen and att empt to determine the value of each att ribute at the correspond-\ning location on the surface of the triangle. When rendering a scene with a perspec-\n\n\n439 \ntive projection, we must do this very carefully so as to account for perspective \nforeshortening. This is known as perspective-correct att ribute interpolation .\nA derivation of perspective-correct interpolation is beyond our scope, but \nsuﬃ  ce it to say that we must divide our interpolated att ribute values by the \ncorresponding z-coordinates (depths) at each vertex. For any pair of vertex at-\ntributes A1 and A2, we can write the interpolated att ribute at a percentage t of \nthe distance between them as follows:\n \n1\n2\n1\n2\n1\n2\n1\n2\n(1\n)\nLERP\n, \n, \n.\nz\nz\nz\nz\nz\nA\nA\nA\nA\nA\nt\np\np\np\np\np\n⎛\n⎞\n⎜\n⎟\n=\n−\n+\n=\n⎝\n⎠\nt\nt\n \nRefer to [28] for an excellent derivation of the math behind perspective-correct \natt ribute interpolation.\nOrthographic Projection\nAn orthographic projection is performed by the following matrix :\n(\n)\n \northo\n2\n0\n0\n0\n2\n0\n0\n0\n.\n2\n0\n0\n0\n1\nV\nH\nr\nl\nt\nb\nf\nn\nf\nn\nr\nl\nt\nb\nr\nl\nt\nb\nf\nn\n→\n⎡\n⎤\n⎛\n⎞\n⎜\n⎟\n⎢\n⎥\n⎝\n⎠\n−\n⎢\n⎥\n⎢\n⎥\n⎛\n⎞\n⎜\n⎟\n⎢\n⎥\n⎝\n⎠\n−\n⎢\n⎥\n=⎢\n⎥\n⎛\n⎞\n⎜\n⎟\n⎢\n⎥\n−−\n⎝\n⎠\n⎢\n⎥\n⎢\n⎥\n⎛\n⎞\n⎛\n⎞⎛\n⎞\n+\n+\n+\n⎢\n⎥\n⎜\n⎟\n−\n−\n−\n⎜\n⎟⎜\n⎟\n⎢\n⎥\n⎝\n⎠⎝\n⎠\n−\n−\n−\n⎝\n⎠\n⎣\n⎦\nM\n \nThis is just an everyday scale-and-translate matrix. (The upper-left  3 × 3 \ncontains a diagonal nonuniform scaling matrix, and the lower row contains \nthe translation.) Since the view volume is a rectangular prism in both view \nspace and clip space, we need only scale and translate our vertices to convert \nfrom one space to the other.\n10.1.4.5. Screen Space and Aspect Ratios\nScreen space is a two-dimensional coordinate system whose axes are mea-\nsured in terms of screen pixels. The x-axis typically points to the right, with \nthe origin at the top-left  corner of the screen and y pointing down. (The reason \nfor the inverted y-axis is that CRT monitors scan the screen from top to bot-\ntom.) The ratio of screen width to screen height is known as the aspect ratio. \nThe most common aspect ratios are 4:3 (the aspect ratio of a traditional tele-\nvision screen) and 16:9 (the aspect ratio of a movie screen or HDTV). These \naspect ratios are illustrated in Figure 10.35.\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n\n\n440 \n10. The Rendering Engine\nWe can render triangles expressed in homogeneous clip space by simply \ndrawing their (x, y) coordinates and ignoring z. But before we do, we scale \nand shift  the clip-space coordinates so that they lie in screen space rather than \nwithin the normalized unit square. This scale-and-shift  operation is known as \nscreen mapping .\n10.1.4.6. The Frame Buffer\nThe ﬁ nal rendered image is stored in a bitmapped color buﬀ er known as the \nframe buﬀ er . Pixel colors are usually stored in RGBA8888 format, although other \nframe buﬀ er formats are supported by most graphics cards as well. Some com-\nmon formats include RGB565, RGB5551, and one or more palett ed modes.\nThe display hardware (CRT, ﬂ at-screen monitor, HDTV, etc.) reads the \ncontents of the frame buﬀ er at a periodic rate of 60 Hz for NTSC televisions \nused in North America and Japan, or 50 Hz for PAL /SECAM televisions used \nin Europe and many other places in the world. Rendering engines typically \nmaintain at least two frame buﬀ ers. While one is being scanned by the dis-\nplay hardware, the other one can be updated by the rendering engine. This is \nknown as double buﬀ ering . By swapping or “ﬂ ipping” the two buﬀ ers during \nthe vertical blanking interval (the period during which the CRT’s electron gun is \nbeing reset to the top-left  corner of the screen), double buﬀ ering ensures that \nthe display hardware always scans the complete frame buﬀ er. This avoids a \njarring eﬀ ect known as tearing , in which the upper portion of the screen dis-\nplays the newly rendered image while the bott om shows the remnants of the \nprevious frame’s image.\nSome engines make use of three frame buﬀ ers—a technique aptly known \nas triple buﬀ ering . This is done so that the rendering engine can start work on \nthe next frame, even when the previous frame is still being scanned by the \ndisplay hardware. For example, the hardware might still be scanning buﬀ er A \nwhen the engine ﬁ nishes drawing buﬀ er B. With triple buﬀ ering, it can pro-\nxS\n4:3\nyS\nxS\n16:9\nyS\nFigure 10.35.  The two most prevalent screen space aspect ratios are 4:3 and 16:9.\n",
      "page_number": 441,
      "chapter_number": 23,
      "summary": "This chapter covers segment 23 (pages 441-462). Key topics include lighting, textures, and textured. For example, a one-dimensional texture \ncould be used to store sampled values of a complex math function, a color-to-\ncolor mapping table, or any other kind of look-up table (LUT).",
      "keywords": [
        "light",
        "texture",
        "Rendering Engine",
        "space",
        "Texel Density",
        "Depth-Buffered Triangle Rasterization",
        "Space Texel Density",
        "rendering",
        "light source",
        "Phong reﬂ ection",
        "Phong Lighting Model",
        "Triangle Rasterization",
        "View Space",
        "texture space",
        "view"
      ],
      "concepts": [
        "lighting",
        "textures",
        "textured",
        "model",
        "render",
        "rendered",
        "color",
        "screen",
        "vectors",
        "space"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 43,
          "title": "Segment 43 (pages 424-431)",
          "relevance_score": 0.73,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 48,
          "title": "Segment 48 (pages 456-465)",
          "relevance_score": 0.72,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 42,
          "title": "Segment 42 (pages 401-412)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 47,
          "title": "Segment 47 (pages 458-468)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 45,
          "title": "Segment 45 (pages 431-439)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 463-481)",
      "start_page": 463,
      "end_page": 481,
      "detection_method": "synthetic",
      "content": "441 \nceed to render a new frame into buﬀ er C, rather than idling while it waits for \nthe display hardware to ﬁ nish scanning buﬀ er A.\nRender Targets\nAny buﬀ er into which the rendering engine draws graphics is known as a ren-\nder target . As we’ll see later in this chapter, rendering engines make use of all \nsorts of other oﬀ -screen render targets, in addition to the frame buﬀ ers. These \ninclude the depth buﬀ er , the stencil buﬀ er , and various other buﬀ ers used for \nstoring intermediate rendering results.\n10.1.4.7. Triangle Rasterization and Fragments\nTo produce an image of a triangle on-screen, we need to ﬁ ll in the pixels it \noverlaps. This process is known as rasterization . During rasterization, the tri-\nangle’s surface is broken into pieces called fragments , each one representing a \nsmall region of the triangle’s surface that corresponds to a single pixel on the \nscreen. (In the case of multisample antialiasing, a fragment corresponds to a \nportion of a pixel—see below.)\nA fragment is like a pixel in training. Before it is writt en into the frame \nbuﬀ er, it must pass a number of tests (described in more depth below). If it \nfails any of these tests, it will be discarded. Fragments that pass the tests are \nshaded (i.e., their colors are determined), and the fragment color is either writ-\nten into the frame buﬀ er or blended with the pixel color that’s already there. \nFigure 10.36 illustrates how a fragment becomes a pixel.\nFragment\nPixel\nFigure 10.36.  A fragment is a small region of a triangle corresponding to a pixel on the \nscreen. It passes through the rendering pipeline and is either discarded or its color is written \ninto the frame buffer.\n10.1. Foundations of Depth-Buffered Triangle Rasterization\nAntialiasing\n When a triangle is rasterized, its edges can look jagged—the familiar “stair \nstep” eﬀ ect we have all come to know and love (or hate). Technically speak-\n\n\n442 \n10. The Rendering Engine\ning, aliasing arises because we are using a discrete set of pixels to sample an \nimage that is really a smooth, continuous two-dimensional signal. (In the fre-\nquency domain, sampling causes a signal to be shift ed and copied multiple \ntimes along the frequency axis. Aliasing literally means that these copies of \nthe signal overlap and get confused with one another.)\nAntialiasing is a technique that reduces the visual artifacts caused by alias-\ning. In eﬀ ect, antialiasing causes the edges of the triangle to be blended with \nthe surrounding colors in the frame buﬀ er.\nThere are a number of ways to antialias a 3D rendered image. In full-screen \nantialiasing (FSAA), the image is rendered into a frame buﬀ er that is twice \nas wide and twice as tall as the actual screen. The resulting image is down-\nsampled to the desired resolution aft erwards. FSAA can be expensive because \nrendering a double-sized frame means ﬁ lling four times the number of pixels. \nFSAA frame buﬀ ers also consume four times the memory of a regular frame \nbuﬀ er.\nModern graphics hardware can antialias a rendered image without ac-\ntually rendering a double-size image, via a technique called multisample an-\ntialiasing (MSAA). The basic idea is to break a triangle down into more than \none fragment per pixel. These supersampled fragments are combined into a \nsingle pixel at the end of the pipeline. MSAA does not require a double-width \nframe buﬀ er, and it can handle higher levels of supersampling as well. (4× and \n8× supersampling are commonly supported by modern GPUs.)\n10.1.4.8. Occlusion and the Depth Buffer\nWhen rendering two triangles that overlap each other in screen space, we \nneed some way of ensuring that the triangle that is closer to the camera will \nappear on top. We could accomplish this by always rendering our triangles in \nFigure 10.37.  The painter’s algorithm renders triangles in a back-to-front order to produce \nproper triangle occlusion. However, the algorithm breaks down when triangles intersect one \nanother.\n\n\n443 \nback-to-front order (the so-called painter’s algorithm ). However, as shown in \nFigure 10.37, this doesn’t work if the triangles are intersecting one another.\nTo implement triangle occlusion properly, independent of the order in \nwhich the triangles are rendered, rendering engines use a technique known \nas depth buﬀ ering or z-buﬀ ering. The depth buﬀ er is a full-screen buﬀ er that \ntypically contains 16- or 24-bit ﬂ oating-point depth information for each pix-\nel in the frame buﬀ er. Every fragment has a z-coordinate that measures its \ndepth “into” the screen. (The depth of a fragment is found by interpolating \nthe depths of the triangle’s vertices.) When a fragment’s color is writt en into \nthe frame buﬀ er, it depth is stored into the corresponding pixel of the depth \nbuﬀ er. When another fragment (from another triangle) is drawn into the same \npixel, the engine compares the new fragment’s depth to the depth already \npresent in the depth buﬀ er. If the fragment is closer to the camera (i.e., if it \nhas a smaller depth), it overwrites the pixel in the frame buﬀ er. Otherwise the \nfragment is discarded.\nZ-Fighting and the W-Buffer\nWhen rendering parallel surfaces that are very close to one another, it’s im-\nportant that the rendering engine can distinguish between the depths of the \ntwo planes. If our depth buﬀ er had inﬁ nite precision, this would never be \na problem. Unfortunately, a real depth buﬀ er only has limited precision, so \nthe depth values of two planes can collapse into a single discrete value when \nthe planes are close enough together. When this happens, the more-distant \nplane’s pixels start to “poke through” the nearer plane, resulting in a noisy \neﬀ ect known as z-ﬁ ghting .\nTo reduce z-ﬁ ghting to a minimum across the entire scene, we would like \nto have equal precision whether we’re rendering surfaces that are close to the \ncamera or far away. However, with z-buﬀ ering this is not the case. The preci-\nsion of clip-space z-depths (\nHz\np\n) are not evenly distributed across the entire \nrange from the near plane to the far plane, because of the division by the view-\nspace z-coordinate. Because of the shape of the 1/z curve, most of the depth \nbuﬀ er’s precision is concentrated near the camera.\nThe plot of the function \n1/\nHz\nVz\np\np\n=\n shown in Figure 10.38 demonstrates \nthis eﬀ ect. Near the camera, the distance between two planes in view space \nVz\np\nΔ\n gets transformed into a reasonably large delta in clip space, \n.\nHz\np\nΔ\n But \nfar from the camera, this same separation gets transformed into a tiny delta in \nclip space. The result is z ﬁ ghting, and it becomes rapidly more prevalent as \nobjects get farther away from the camera.\nTo circumvent this problem, we would like to store view-space z-coor-\ndinates (\nVz\np\n) in the depth buﬀ er instead of clip-space z-coordinates (\nHz\np\n). \nView-space z-coordinates vary linearly with the distance from the camera, so \n10.1. Foundations of Depth-Buffered Triangle Rasterization\n\n\n444 \n10. The Rendering Engine\nusing them as our depth measure achieves uniform precision across the en-\ntire depth range. This technique is called w-buﬀ ering , because the view-space \nz-coordinate conveniently appears in the w-component of our homogeneous \nclip-space coordinates. (Recall from Equation (10.1) that \nHw\nVz\np\np\n=−\n.)\nThe terminology can be a very confusing here. The z- and w-buﬀ ers store \ncoordinates that are expressed in clip space. But in terms of view-space coordi-\nnates, the z-buﬀ er stores 1/z (i.e., 1/\nVz\np\n) while the w-buﬀ er stores z (i.e., \nVz\np\n)!\nWe should note here that the w-buﬀ ering approach is a bit more expen-\nsive than its z-based counterpart. This is because with w-buﬀ ering, we cannot \nlinearly interpolate depths directly. Depths must be inverted prior to interpo-\nlation and then re-inverted prior to being stored in the w-buﬀ er.\n10.2. The Rendering Pipeline\nNow that we’ve completed our whirlwind tour of the major theoretical and \npractical underpinnings of triangle rasterization, let’s turn our att ention to \nhow it is typically implemented. In real-time game rendering engines, the \nhigh-level rendering steps described in Section 10.1 are implemented using \na soft ware/hardware architecture known as a pipeline . A pipeline is just an or-\ndered chain of computational stages, each with a speciﬁ c purpose, operating \non a stream of input data items and producing a stream of output data.\nEach stage of a pipeline can typically operate independently of the other \nstages. Hence, one of the biggest advantages of a pipelined architecture is that \nit lends itself extremely well to parallelization . While the ﬁ rst stage is chewing \non one data element, the second stage can be processing the results previously \nproduced by the ﬁ rst stage, and so on down the chain.\nParallelization can also be achieved within an individual stage of the \npipeline. For example, if the computing hardware for a particular stage is du-\nFigure 10.38.  A plot of the function 1/pVz, showing how most of the precision lies close to \nthe camera.\nΔpHz\nΔpVz\nΔpVz\nΔpHz\npHz = 1/pVz\npHz = 1/pVz\n\n\n445 \nplicated N times on the die, N data elements can be processed in parallel by \nthat stage. A parallelized pipeline is shown in Figure 10.39. Ideally the stages \noperate in parallel (most of the time), and certain stages are capable of operat-\ning on multiple data items simultaneously as well.\nThe throughput of a pipeline measures how many data items are processed \nper second overall. The pipeline’s latency measures the amount of time it takes \nfor a single data element to make it through the entire pipeline. The latency \nof an individual stage measures how long that stage takes to process a single \nitem. The slowest stage of a pipeline dictates the throughput of the entire pipe-\nline. It also has an impact on the average latency of the pipeline as a whole. \nTherefore, when designing a rendering pipeline, we att empt to minimize and \nbalance latency across the entire pipeline and eliminate bott lenecks. In a well-\ndesigned pipeline, all the stages operate simultaneously, and no stage is ever \nidle for very long waiting for another stage to become free.\n10.2.1. Overview of the Rendering Pipeline\nSome graphics texts divide the rendering pipeline into three coarse-grained \nstages. In this book, we’ll extend this pipeline back even further, to encompass \nthe oﬄ  ine tools used to create the scenes that are ultimately rendered by the \ngame engine. The high level stages in our pipeline are:\nz Tools stage (oﬄ  ine). Geometry and surface properties (materials) are de-\nﬁ ned.\nz Asset conditioning stage (oﬄ  ine). The geometry and material data are pro-\ncessed by the asset conditioning pipeline (ACP) into an engine-ready \nformat.\nStage 3\nStage 1\nStage 2\nTime\nFigure 10.39.  A parallelized pipeline. The stages all operate in parallel and some stages are \ncapable of operating on multiple data items simultaneously as well.\n10.2. The Rendering Pipeline\n\n\n446 \n10. The Rendering Engine\nz Application stage (CPU). Potentially visible mesh instances are identiﬁ ed \nand submitt ed to the graphics hardware along with their materials for \nrendering.\nz Geometry processing stage (GPU). Vertices are transformed and lit and \nprojected into homogeneous clip space. Triangles are processed by the \noptional geometry shader and then clipped to the frustum.\nz Rasterization stage (GPU). Triangles are converted into fragments that are \nshaded, passed through various tests (z test, alpha test, stencil test, etc.) \nand ﬁ nally blended into the frame buﬀ er.\n10.2.1.1. How the Rendering Pipeline Transforms Data\n It’s interesting to note how the format of geometry data changes as it passes \nthrough the rendering pipeline. The tools and asset conditioning stages deal \nwith meshes and materials. The application stage deals in terms of mesh in-\nstances and submeshes, each of which is associated with a single material. \nDuring the geometry stage, each submesh is broken down into individual ver-\ntices, which are processed largely in parallel. At the conclusion of this stage, \nthe triangles are reconstructed from the fully transformed and shaded verti-\nces. In the rasterization stage, each triangle is broken into fragments, and these \nfragments are either discarded, or they are eventually writt en into the frame \nbuﬀ er as colors. This process is illustrated in Figure 10.40.\nTools\nACP\nApplication\nGeometry \nProcessing\nVertice\nVertices\nMesh \nInstance\nSubmeshes\nTextures\nMaterials\nTextures\nMesh\nMaterials\nMaterials\nTextures\nRasterization\nVertice\nFragments\nVertice\nPixels\nVertice\nTriangles\nFigure 10.40.  The format of geometric data changes radically as it passes through the vari-\nous stages of the rendering pipeline.\n\n\n447 \n10.2.1.2. Implementation of the Pipeline\nThe ﬁ rst two stages of the rendering pipeline are implemented oﬄ  ine, usually \nexecuted by a PC or Linux machine. The application stage is run either by the \nmain CPU of the game console or PC, or by parallel processing units like the \nPS3’s SPUs. The geometry and rasterization stages are usually implemented \non the graphics processing unit (GPU). In the following sections, we’ll explore \nsome of the details of how each of these stages is implemented.\n10.2.2. The Tools Stage\nIn the tools stage, meshes are authored by 3D modelers in a digital content \ncreation (DCC) application like Maya , 3ds Max , Lightwave , Soft image/XSI , \nSketchUp , etc. The models may be deﬁ ned using any convenient surface de-\nscription—NURBS, quads, triangles, etc. However, they are invariably tessel-\nlated into triangles prior to rendering by the runtime portion of the pipeline.\nThe vertices of a mesh may also be skinned . This involves associating \neach vertex with one or more joints in an articulated skeletal structure, along \nwith weights describing each joint’s relative inﬂ uence over the vertex. Skin-\nning information and the skeleton are used by the animation system to drive \nthe movements of a model—see Chapter 11 for more details.\nMaterials are also deﬁ ned by the artists during the tools stage. This in-\nvolves selecting a shader for each material, selecting textures as required by \nthe shader, and specifying the conﬁ guration parameters and options of each \nshader. Textures are mapped onto the surfaces, and other vertex att ributes are \nalso deﬁ ned, oft en by “painting” them with some kind of intuitive tool within \nthe DCC application.\nMaterials are usually authored using a commercial or custom in-house \nmaterial editor . The material editor is sometimes integrated directly into the \nDCC application as a plug-in, or it may be a stand-alone program. Some mate-\nrial editors are live-linked to the game, so that material authors can see what \nthe materials will look like in the real game. Other editors provide an oﬄ  ine \n3D visualization view. Some editors even allow shader programs to be writt en \nand debugged by the artist or a shader engineer. NVIDIA’s Fx Composer is an \nexample of such a tool; it is depicted in Figure 10.41.\nBoth FxComposer and Unreal Engine 3 provide powerful graphical shad-\ning languages . Such tools allow rapid prototyping of visual eﬀ ects by con-\nnecting various kinds of nodes together with a mouse. These tools generally \nprovide a WYSIWYG display of the resulting material. The shaders created \nby a graphical language usually need to be hand-optimized by a rendering \nengineer, because a graphical language invariably trades some runtime per-\n10.2. The Rendering Pipeline\n\n\n448 \n10. The Rendering Engine\nFigure 10.42.  The Unreal Engine 3 graphical shader language.\nFigure 10.41.  Nvidia’s Fx Composer allows shader programs to be written, previsualized, and \ndebugged easily.\nformance for its incredible ﬂ exibility, generality, and ease of use. The Unreal \ngraphical shader editor is shown in Figure 10.42.\nMaterials may be stored and managed with the individual meshes. How-\never, this can lead to duplication of data—and eﬀ ort. In many games, a rela-\ntively small number of materials can be used to deﬁ ne a wide range of objects \nin the game. For example, we might deﬁ ne some standard, reusable materials \n\n\n449 \nlike wood, rock, metal, plastic, cloth, skin, and so on. There’s no reason to du-\nplicate these materials inside every mesh. Instead, many game teams build up \na library of materials from which to choose, and the individual meshes refer \nto the materials in a loosely-coupled manner.\n10.2.3. The Asset Conditioning Stage\nThe asset conditioning stage is itself a pipeline, sometimes called the asset \nconditioning pipeline or ACP. As we saw in Section 6.2.1.4, its job is to export, \nprocess, and link together multiple types of assets into a cohesive whole. For \nexample, a 3D model is comprised of geometry (vertex and index buﬀ ers), \nmaterials, textures, and an optional skeleton. The ACP ensures that all of the \nindividual assets referenced by a 3D model are available and ready to be load-\ned by the engine.\nGeometric and material data is extracted from the DCC application and \nis usually stored in a platform-independent intermediate format. The data is \nthen further processed into one or more platform-speciﬁ c formats, depend-\ning on how many target platforms the engine supports. Ideally the platform-\nspeciﬁ c assets produced by this stage are ready to load into memory and use \nwith litt le or no postprocessing at runtime. For example, mesh data targeted \nfor the Xbox 360 might be output as index and vertex buﬀ ers that are ready \nto be uploaded to video RAM; on the PS3, geometry might be produced in \ncompressed data streams that are ready to be DMA’d to the SPUs for decom-\npression. The ACP oft en takes the needs of the material/shader into account \nwhen building assets. For example, a particular shader might require tangent \nand bitangent vectors as well as a vertex normal; the ACP could generate these \nvectors automatically.\nHigh-level scene graph data structures may also be computed during the \nasset conditioning stage. For example, static level geometry may be processed \nin order to build a BSP tree. (As we’ll investigate in Section 10.2.7.4, scene \ngraph data structures help the rendering engine to very quickly determine \nwhich objects should be rendered, given a particular camera position and ori-\nentation.)\nExpensive lighting calculations are oft en done oﬄ  ine as part of the as-\nset conditioning stage. This is called static lighting ; it may include calcula-\ntion of light colors at the vertices of a mesh (this is called “baked” vertex \nlighting), construction of texture maps that encode per-pixel lighting in-\nformation known as light maps , calculation of precomputed radiance transfer \n(PRT) coeﬃ  cients (usually represented by spherical harmonic functions), \nand so on.\n10.2. The Rendering Pipeline\n\n\n450 \n10. The Rendering Engine\n10.2.4. A Brief History of the GPU\nIn the early days of game development, all rendering was done on the CPU. \nGames like Castle Wolfenstein 3D and Doom pushed the limits of what early \nPCs could do, rendering interactive 3D scenes without any help from special-\nized graphics hardware (other than a standard VGA card).\nAs the popularity of these and other PC games took oﬀ , graphics hard-\nware was developed to oﬄ  oad work from the CPU. The earliest graphics ac-\ncelerators, like 3Dfx’s Voodoo line of cards, handled only the most expensive \nstage in the pipeline—the rasterization stage. Subsequent graphics accelera-\ntors provided support for the geometry processing stage as well.\nAt ﬁ rst, graphics hardware provided only a hard-wired but conﬁ gurable \nimplementation known as the ﬁ xed-function pipeline . This technology was \nknown as hardware transformation and lighting , or hardware T&L for short. Later, \ncertain substages of the pipeline were made programmable. Engineers could \nnow write programs called shaders to control exactly how the pipeline pro-\ncessed vertices (vertex shaders) and fragments (fragment shaders, more common-\nly known as pixel shaders). With the introduction of DirectX 10, a third type of \nshader known as a geometry shader was added. It permits rendering engineers \nto modify, cull, or create entire primitives (triangles, lines, and points).\nGraphics hardware has evolved around a specialized type of micropro-\ncessor known as the graphics processing unit or GPU. A GPU is designed to \nmaximize throughput of the pipeline, which it achieves through massive par-\nallelization . For example, a modern GPU like the GeForce 8800 can process \n128 vertices or fragments simultaneously.\nEven in its fully programmable form, a GPU is not a general-purpose \nmicroprocessor—nor should it be. A GPU achieves its high processing speeds \n(on the order of teraﬂ ops on today’s GPUs) by carefully controlling the ﬂ ow of \ndata through the pipeline . Certain pipeline stages are either entirely ﬁ xed in \ntheir function, or they are conﬁ gurable but not programmable. Memory can \nonly be accessed in controlled ways, and specialized data caches are used to \nminimize unnecessary duplication of computations.\nIn the following sections, we’ll brieﬂ y explore the architecture of a mod-\nern GPU and see how the runtime portion of the rendering pipeline is typi-\ncally implemented. We’ll speak primarily about current GPU architectures, \nwhich are used on personal computers with the latest graphics cards and on \nconsole platforms like the Xbox 360 and the PS3. However, not all platforms \nsupport all of the features we’ll be discussing here. For example, the Wii does \nnot support programmable shaders, and most PC games need to support fall-\nback rendering solutions to support older graphics cards with only limited \nprogrammable shader support.\n\n\n451 \n10.2.5. The GPU Pipeline\n Virtually all GPUs break the pipeline into the substages described below and \ndepicted in Figure 10.43. Each stage is shaded to indicate whether its function-\nality is programmable, ﬁ xed but conﬁ gurable, or ﬁ xed and non-conﬁ gurable.\n10.2.5.1. Vertex Shader\n This stage is fully programmable. It is responsible for transformation and \nshading/lighting of individual vertices. The input to this stage is a single ver-\ntex (although in practice many vertices are processed in parallel). Its position \nand normal are typically expressed in model space or world space. The vertex \nshader handles transformation from model space to view space via the model-\nview transform. Perspective projection is also applied, as well as per-vertex \nlighting and texturing calculations, and skinning for animated characters. The \nvertex shader can also perform procedural animation by modifying the posi-\ntion of the vertex. Examples of this include foliage that sways in the breeze or \nan undulating water surface. The output of this stage is a fully transformed \nand lit vertex, whose position and normal are expressed in homogeneous clip \nspace (see Section 10.1.4.4).\nOn modern GPUs, the vertex shader has full access to texture data—a ca-\npability that used to be available only to the pixel shader. This is particularly \nuseful when textures are used as stand-alone data structures like height maps \nor look-up tables .\n10.2.5.2. Geometry Shader\n This optional stage is also fully programmable. The geometry shader oper-\nates on entire primitives (triangles, lines, and points) in homogeneous clip \nspace. It is capable of culling or modifying input primitives, and it can also \ngenerate new primitives. Typical uses include shadow volume extrusion (see \nConfigurable\nFixed-Function\nProgrammable\nPrimitive \nAssembly\nGeometry \nShader\nClipping\nScreen \nMapping\nTriangle \nSetup\nTriangle \nTraversal\nEarly \nZ Test\nPixel \nShader\nMerge \n/ ROP\nStream \nOutput\nVertex \nShader\nFrame \nBuffer\nFigure 10.43.  The geometry processing and rasterization stages of the rendering pipeline, as \nimplemented by a typical GPU. The white stages are programmable, the light grey stages are \nconﬁ gurable, and the dark grey boxes are ﬁ xed-function.\n10.2. The Rendering Pipeline\n\n\n452 \n10. The Rendering Engine\nSection 10.3.3.1), rendering the six faces of a cube map (see Section 10.3.1.4), \nfur ﬁ n extrusion around silhouett e edges of meshes, creation of particle \nquads from point data (see Section 10.4.1), dynamic tessellation, fractal sub-\ndivision of line segments for lightning eﬀ ects, cloth simulations, and the list \ngoes on.\n10.2.5.3. Stream Output\nModern GPUs permit the data that has been processed up to this point in the \npipeline to be writt en back to memory. From there, it can then be looped back \nto the top of the pipeline for further processing. This feature is called stream \noutput .\nStream output permits a number of intriguing visual eﬀ ects to be achieved \nwithout the aid of the CPU. An excellent example is hair rendering. Hair is \noft en represented as a collection of cubic spline curves. It used to be that hair \nphysics simulation would be done on the CPU. The CPU would also tessellate \nthe splines into line segments. Finally the GPU would render the segments.\nWith stream output, the GPU can do the physics simulation on the control \npoints of the hair splines within the vertex shader. The geometry shader tes-\nsellates the splines, and the stream output feature is used to write the tessel-\nlated vertex data to memory. The line segments are then piped back into the \ntop of the pipeline so they can be rendered.\n10.2.5.4. Clipping\nThe clipping stage chops oﬀ  those portions of the triangles that straddle the \nfrustum . Clipping is done by identifying vertices that lie outside the frustum \nand then ﬁ nding the intersection of the triangle’s edges with the planes of \nthe frustum. These intersection points become new vertices that deﬁ ne one or \nmore clipped triangles.\nThis stage is ﬁ xed in function, but it is somewhat conﬁ gurable. For ex-\nample, user-deﬁ ned clipping planes can be added in addition to the frustum \nplanes. This stage can also be conﬁ gured to cull triangles that lie entirely out-\nside the frustum.\n10.2.5.5. Screen Mapping\nScreen mapping simply scales and shift s the vertices from homogeneous clip \nspace into screen space. This stage is entirely ﬁ xed and non-conﬁ gurable.\n10.2.5.6. Triangle Setup\nDuring triangle setup , the rasterization hardware is initialized for eﬃ  cient \nconversion of the triangle into fragments. This stage is not conﬁ gurable.\n\n\n453 \n10.2.5.7. Triangle Traversal\n Each triangle is broken into fragments (i.e., rasterized) by the triangle travers-\nal stage. Usually one fragment is generated for each pixel, although with mul-\ntisample antialiasing (MSAA), multiple fragments are created per pixel (see \nSection 10.1.4.7). The triangle traversal stage also interpolates vertex att ributes \nin order to generate per-fragment att ributes for processing by the pixel shader. \nPerspective-correct interpolation is used where appropriate. This stage’s func-\ntionality is ﬁ xed and not conﬁ gurable.\n10.2.5.8. Early Z Test\n Many graphics cards are capable of checking the depth of the fragment at this \npoint in the pipeline, discarding it if it is being occluded by the pixel already \nin the frame buﬀ er. This allows the (potentially very expensive) pixel shader \nstage to be skipped entirely for occluded fragments.\nSurprisingly, not all graphics hardware supports depth testing at this \nstage of the pipeline. In older GPU designs, the z test was done along with al-\npha testing, aft er the pixel shader had run. For this reason, this stage is called \nthe early z test or early depth test stage.\n10.2.5.9. Pixel Shader\n This stage is fully programmable. Its job is to shade (i.e., light and otherwise \nprocess) each fragment. The pixel shader can also discard fragments, for ex-\nample because they are deemed to be entirely transparent. The pixel shader \ncan address one or more texture maps, run per-pixel lighting calculations, and \ndo whatever else is necessary to determine the fragment’s color.\nThe input to this stage is a collection of per-fragment att ributes (which \nhave been interpolated from the vertex att ributes by the triangle traversal \nstage). The output is a single color vector describing the desired color of the \nfragment.\n10.2.5.10.  Merging / Raster Operations Stage\nThe ﬁ nal stage of the pipeline is known as the merging stage or blending stage, \nalso known as the raster operations stage or ROP in NVIDIA parlance. This \nstage is not programmable, but it is highly conﬁ gurable. It is responsible for \nrunning various fragment tests including the depth test (see Section 10.1.4.8), \nalpha test (in which the values of the fragment’s and pixel’s alpha channels can \nbe used to reject certain fragments), and stencil test (see Section 10.3.3.1).\nIf the fragment passes all of the tests, its color is blended (merged) with \nthe color that is already present in the frame buﬀ er. The way in which blend-\ning occurs is controlled by the alpha blending function —a function whose basic \n10.2. The Rendering Pipeline\n\n\n454 \n10. The Rendering Engine\nstructure is hard-wired, but whose operators and parameters can be conﬁ g-\nured in order to produce a wide variety of blending operations.\nAlpha blending is most commonly used to render semi-transparent ge-\nometry. In this case, the following blending function is used:\n \n(1\n)\n.\nD\nS\nS\nS\nD\nA\nA\n′ =\n+\n−\nC\nC\nC\n \nThe subscripts S and D stand for “source” (the incoming fragment) and “des-\ntination” (the pixel in the frame buﬀ er), respectively. Therefore, the color that \nis writt en into the frame buﬀ er (\nD′\nC ) is a weighted average of the existing frame \nbuﬀ er contents (\nD\nC ) and the color of the fragment being drawn (\nS\nC ). The \nblend weight (\nS\nA ) is just the source alpha of the incoming fragment.\nFor alpha blending to look right, the semi-transparent and translucent \nsurfaces in the scene must be sorted and rendered in back-to-front order, af-\nter the opaque geometry has been rendered to the frame buﬀ er. This is be-\ncause aft er alpha blending has been performed, the depth of the new fragment \noverwrites the depth of the pixel with which it was blended. In other words, \nthe depth buﬀ er ignores transparency (unless depth writes have been turned \noﬀ , of course). If we are rendering a stack of translucent objects on top of an \nopaque backdrop, the resulting pixel color should ideally be a blend between \nthe opaque surface’s color and the colors of all of the translucent surfaces in \nthe stack. If we try to render the stack in any order other than back-to-front, \ndepth test failures will cause some of the translucent fragments to be discard-\ned, resulting in an incomplete blend (and a rather odd-looking image).\nOther alpha blending functions can be deﬁ ned as well, for purposes other \nthan transparency blending. The general blending equation takes the form \n(\n)\n(\n),\nD\nS\nS\nD\nD\n′ =\n⊗\n+\n⊗\nC\nw\nC\nw\nC\n where the weighting factors wS and wD can be \nselected by the programmer from a predeﬁ ned set of values including zero, \none, source or destination color, source or destination alpha, and one minus \nthe source or destination color or alpha. The operator ⊗ is either a regular \nscalar-vector multiplication or a component-wise vector-vector multiplication \n(a Hadamard product —see Section 4.2.4.1) depending on the data types of wS\nand wD.\n10.2.6. Programmable Shaders\n Now that we have an end-to-end picture of the GPU pipeline in mind, let’s \ntake a deeper look at the most interesting part of the pipeline—the program-\nmable shaders. Shader architectures have evolved signiﬁ cantly since their \nintroduction with DirectX 8. Early shader models supported only low-level as-\nsembly language programming, and the instruction set and register set of the \npixel shader diﬀ ered signiﬁ cantly from those of the vertex shader. DirectX \n\n\n455 \n9 brought with it support for high-level C-like shader languages such as Cg \n(C for graphics), HLSL (High-Level Shading Language —Microsoft ’s imple-\nmentation of the Cg language), and GLSL (OpenGL shading language). With \nDirectX 10, the geometry shader was introduced, and with it came a uniﬁ ed \nshader architecture called shader model 4.0 in DirectX parlance. In the uniﬁ ed \nshader model, all three types of shaders support roughly the same instruction \nset and have roughly the same set of capabilities, including the ability to read \ntexture memory.\nA shader takes a single element of input data and transforms it into zero \nor more elements of output data.\nz In the case of the vertex shader, the input is a vertex whose position and \nnormal are expressed in model space or world space. The output of the \nvertex shader is a fully transformed and lit vertex, expressed in homo-\ngeneous clip space.\nz The input to the geometry shader is a single n-vertex primitive—a point \n(n = 1), line segment (n = 2), or triangle (n = 3)—with up to n additional \nvertices that act as control points. The output is zero or more primitives, \npossibly of a diﬀ erent type than the input. For example, the geometry \nshader could convert points into two-triangle quads, or it could trans-\nform triangles into triangles but optionally discard some triangles, and \nso on.\nz The pixel shader’s input is a fragment whose att ributes have been in-\nterpolated from the three vertices of the triangle from which it came. \nThe output of the pixel shader is the color that will be writt en into the \nframe buﬀ er (presuming the fragment passes the depth test and other \noptional tests). The pixel shader is also capable of discarding fragments \nexplicitly, in which case it produces no output.\n10.2.6.1. Accessing Memory\n Because the GPU implements a data processing pipeline, access to RAM is \nvery carefully controlled. A shader program cannot read from or write to \nmemory directly. Instead, its memory accesses are limited to two methods: \nregisters and texture maps.\nShader Registers\n A shader can access RAM indirectly via registers. All GPU registers are in 128-\nbit SIMD format. Each register is capable of holding four 32-bit ﬂ oating-point \nor integer values (represented by the float4 data type in the Cg language). \nSuch a register can contain a four-element vector in homogeneous coordinates \nor a color in RGBA format, with each component in 32-bit ﬂ oating-point for-\n10.2. The Rendering Pipeline\n\n\n456 \n10. The Rendering Engine\nmat. Matrices can be represented by groups of three or four registers (rep-\nresented by built-in matrix types like float4x4 in Cg). A GPU register can \nalso be used to hold a single 32-bit scalar, in which case the value is usually \nreplicated across all four 32-bit ﬁ elds. Some GPUs can operate on 16-bit ﬁ elds, \nknown as halfs. (Cg provides various built-in types like half4 and half4x4\nfor this purpose.)\nRegisters come in four ﬂ avors, as follows:\nz Input registers. These registers are the shader’s primary source of input \ndata. In a vertex shader, the input registers contain att ribute data ob-\ntained directly from the vertices. In a pixel shader, the input registers \ncontain interpolated vertex att ribute data corresponding to a single \nfragment. The values of all input registers are set automatically by the \nGPU prior to invoking the shader.\nz Constant registers. The values of constant registers are set by the applica-\ntion and can change from primitive to primitive. Their values are con-\nstant only from the point of view of the shader program. They provide \na secondary form of input to the shader. Typical contents include the \nmodel-view matrix, the projection matrix, light parameters, and any \nother parameters required by the shader that are not available as vertex \natt ributes.\nz Temporary registers. These registers are for use by the shader program inter-\nnally and are typically used to store intermediate results of calculations.\nz Output registers. The contents of these registers are ﬁ lled in by the shader \nand serve as its only form of output. In a vertex shader, the output regis-\nters contain vertex att ributes such as the transformed position and nor-\nmal vectors in homogeneous clip space, optional vertex colors, texture \ncoordinates, and so on. In a pixel shader, the output register contains \nthe ﬁ nal color of the fragment being shaded.\nThe application provides the values of the constant registers when it sub-\nmits primitives for rendering. The GPU automatically copies vertex or frag-\nment att ribute data from video RAM into the appropriate input registers prior \nto calling the shader program, and it also writes the contents of the output \nregisters back into RAM at the conclusion of the program’s execution so that \nthe data can be passed to the next stage of the pipeline.\nGPUs typically cache output data so that it can be reused without be-\ning recalculated. For example, the post-transform vertex cache stores the most-\nrecently processed vertices emitt ed by the vertex shader. If a triangle is en-\ncountered that refers to a previously-processed vertex, it will be read from the \npost-transform vertex cache if possible—the vertex shader need only be called \n\n\n457 \nagain if the vertex in question has since been ejected from the cache to make \nroom for newly processed vertices.\nTextures\nA shader also has direct read-only access to texture maps. Texture data is ad-\ndressed via texture coordinates, rather than via absolute memory addresses. \nThe GPU’s texture samplers automatically ﬁ lter the texture data, blending val-\nues between adjacent texels or adjacent mipmap levels as appropriate. Texture \nﬁ ltering can be disabled in order to gain direct access to the values of particu-\nlar texels. This can be useful when a texture map is used as a data table, for \nexample.\nShaders can only write to texture maps in an indirect manner—by render-\ning the scene to an oﬀ -screen frame buﬀ er that is interpreted as a texture map \nby subsequent rendering passes. This feature is known as render to texture .\n10.2.6.2. Introduction to High-Level Shader Language Syntax\nHigh-level shader languages like Cg and GLSL are modeled aft er the C pro-\ngramming language. The programmer can declare functions, deﬁ ne a simple \nstruct, and perform arithmetic. However, as we said above, a shader pro-\ngram only has access to registers and textures. As such, the struct and vari-\nable we declare in Cg or GLSL is mapped directly onto registers by the shader \ncompiler. We deﬁ ne these mappings in the following ways:\nz Semantics . Variables and struct members can be suﬃ  xed with a co-\nlon followed by a keyword known as a semantic. The semantic tells the \nshader compiler to bind the variable or data member to a particular \nvertex or fragment att ribute. For example, in a vertex shader we might \ndeclare an input struct whose members map to the position and color \natt ributes of a vertex as follows:\n struct VtxOut\n {\n  float4 pos : POSITION; // map to the position  \n         // \nattribute\n  float4 color : COLOR; \n// map to the color attribute\n };\nz Input versus output. The compiler determines whether a particular vari-\nable or struct should map to input or output registers from the context \nin which it is used. If a variable is passed as an argument to the shader \nprogram’s main function, it is assumed to be an input; if it is the return \nvalue of the main function, it is taken to be an output.\n10.2. The Rendering Pipeline\n\n\n458 \n10. The Rendering Engine\n  VtxOut vshaderMain(VtxIn in) // in maps to input   \n \n           \n  // \nregisters\n  {\n   \nVtxOut out;\n   \n// ...\n   \nreturn out;  \n// out maps to output registers\n  }\nz Uniform declaration . To gain access to the data supplied by the applica-\ntion via the constant registers, we can declare a variable with the key-\nword uniform. For example, the model-view matrix could be passed to \na vertex shader as follows:\n \nVtxOut vshaderMain(VtxIn in,\n \n                   uniform float4x4 modelViewMatrix)\n {\n  VtxOut \nout;\n  // \n...\n  return \nout; \n }\nArithmetic operations can be performed by invoking C-style operators, \nor by calling intrinsic functions as appropriate. For example, to multiply the \ninput vertex position by the model-view matrix, we could write:\n \nVtxOut vshaderMain(VtxIn in,\n \n                   uniform float4x4 modelViewMatrix)\n {\n  VtxOut \nout;\nout.pos = mul(modelViewMatrix, in.pos);\n \n \nout.color = float4(0, 1, 0, 1); // RGBA green\n  return \nout; \n }\nData is obtained from textures by calling special intrinsic functions that \nread the value of the texels at a speciﬁ ed texture coordinate. A number of vari-\nants are available for reading one-, two- and three-dimensional textures in \nvarious formats, with and without ﬁ ltering. Special texture addressing modes \nare also available for accessing cube maps and shadow maps. References to \nthe texture maps themselves are declared using a special data type known as \na texture sampler declaration. For example, the data type sampler2D repre-\nsents a reference to a typical two-dimensional texture. The following simple \nCg pixel shader applies a diﬀ use texture to a triangle:\nstruct FragmentOut\n{\n  float4 color : COLOR;\n};\n\n\n459 \nFragmentOut pshaderMain(float2 uv : TEXCOORD0,\n                        uniform sampler2D texture)\n{\n \nFragmentOut out;\n \nout.color = tex2D(texture, uv); \n// look up texel at  \n  \n          // \n(u,v)\n \nreturn out;\n}\n10.2.6.3. Effect Files\n By itself, a shader program isn’t particularly useful. Additional information is \nrequired by the GPU pipeline in order to call the shader program with mean-\ningful inputs. For example, we need to specify how the application-speciﬁ ed \nparameters, like the model-view matrix, light parameters, and so on, map to \nthe uniform variables declared in the shader program. In addition, some vi-\nsual eﬀ ects require two or more rendering passes, but a shader program only \ndescribes the operations to be applied during a single rendering pass. If we \nare writing a game for the PC platform, we will need to deﬁ ne “fallback” ver-\nsions of some of our more-advanced rendering eﬀ ects, so that they will work \neven on older graphics cards. To tie our shader program(s) together into a \ncomplete visual eﬀ ect, we turn to a ﬁ le format known as an eﬀ ect ﬁ le.\nDiﬀ erent rendering engines implement eﬀ ects in slightly diﬀ erent ways. \nIn Cg, the eﬀ ect ﬁ le format is known as CgFX . Ogre3D uses a ﬁ le format very \nsimilar to CgFX known as a material ﬁ le. GLSL eﬀ ects can be described using \nthe COLLADA format, which is based on XML. Despite the diﬀ erences, eﬀ ects \ngenerally take on the following hierarchical format:\nz At global scope, structs, shader programs (implemented as various \n“main” functions), and global variables (which map to application-\nspeciﬁ ed constant parameters) are deﬁ ned.\nz One or more techniques are deﬁ ned. A technique represents one way to \nrender a particular visual eﬀ ect. An eﬀ ect typically provides a primary \ntechnique for its highest-quality implementation and possibly a number \nof fall back techniques for use on lower-powered graphics hardware.\nz Within each technique, one or more passes are deﬁ ned. A pass describes \nhow a single full-frame image should be rendered. It typically includes \na reference to a vertex, geometry and/or pixel shader program’s “main” \nfunction, various parameter bindings, and optional render state sett ings.\n10.2.6.4. Further Reading\nIn this section, we’ve only had a small taste of what high-level shader pro-\ngramming is like—a complete tutorial is beyond our scope here. For a much \n10.2. The Rendering Pipeline\n",
      "page_number": 463,
      "chapter_number": 24,
      "summary": "As we’ll see later in this chapter, rendering engines make use of all \nsorts of other oﬀ -screen render targets, in addition to the frame buﬀ ers Key topics include stages, render, and data.",
      "keywords": [
        "rendering pipeline",
        "shader",
        "rendering",
        "stage",
        "pipeline",
        "pixel shader",
        "Vertex Shader",
        "rendering engine",
        "frame buﬀ",
        "data",
        "buﬀ",
        "vertex",
        "GPU",
        "geometry shader",
        "Triangle"
      ],
      "concepts": [
        "stages",
        "render",
        "data",
        "vertex",
        "triangle",
        "graphics",
        "graphical",
        "textures",
        "fragments",
        "fragment"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 43,
          "title": "Segment 43 (pages 424-431)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 48,
          "title": "Segment 48 (pages 469-476)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 36,
          "title": "Segment 36 (pages 347-355)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 6,
          "title": "Segment 6 (pages 50-58)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 59,
          "title": "Segment 59 (pages 570-580)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 482-502)",
      "start_page": 482,
      "end_page": 502,
      "detection_method": "synthetic",
      "content": "460 \n10. The Rendering Engine\nmore-detailed introduction to Cg shader programming, refer to the Cg tu-\ntorial available on NVIDIA’s website at htt p://developer.nvidia.com/object/\ncg_tutorial_home.html.\n10.2.7. The Application Stage\nNow that we understand how the GPU works, we can discuss the pipeline \nstage that is responsible for driving it—the application stage . This stage has \nthree roles:\n \n1. Visibility determination. Only objects that are visible (or at least poten-\ntially visible) should be submitt ed to the GPU, lest we waste valuable \nresources processing triangles that will never be seen.\n \n2. Submitt ing geometry to the GPU for rendering. Submesh-material pairs are \nsent to the GPU via a rendering call like DrawIndexedPrimitive()\n(DirectX) or glDrawArrays() (OpenGL), or via direct construction of \nthe GPU command list. The geometry may be sorted for optimal render-\ning performance. Geometry might be submitt ed more than once if the \nscene needs to be rendered in multiple passes.\n \n3. Controlling shader parameters and render state. The uniform parameters \npassed to the shader via constant registers are conﬁ gured by the ap-\nplication stage on a per-primitive basis. In addition, the application \nstage must set all of the conﬁ gurable parameters of the non-program-\nmable pipeline stages to ensure that each primitive is rendered ap-\npropriately.\nIn the following sections, we’ll brieﬂ y explore how the application stage per-\nforms these tasks.\n10.2.7.1. Visibility Determination\nThe cheapest triangles are the ones you never draw. So it’s incredibly impor-\ntant to cull objects from the scene that do not contribute to the ﬁ nal rendered \nimage prior to submitt ing them to the GPU. The process of constructing the \nlist of visible mesh instances is known as visibility determination .\nFrustum Culling\nIn frustum culling , all objects that lie entirely outside the frustum are exclud-\ned from our render list. Given a candidate mesh instance, we can determine \nwhether or not it lies inside the frustum by performing some simple tests be-\ntween the object’s bounding volume and the six frustum planes. The bounding \nvolume is usually a sphere, because spheres are particularly easy to cull. For \n\n\n461 \neach frustum plane, we move the plane inward a distance equal to the radius \nof the sphere, then we determine on which side of each modiﬁ ed plane the \ncenter point of the sphere lies. If the sphere is found to be on the front side of \nall six modiﬁ ed planes, the sphere is inside the frustum.\nA scene graph data structure, described in Section 10.2.7.4, can help opti-\nmize frustum culling by allowing us to ignore objects whose bounding spheres \nare nowhere close to being inside the frustum.\nOcclusion and Potentially Visible Sets\nEven when objects lie entirely within the frustum, they may occlude one an-\nother. Removing objects from the visible list that are entirely occluded by \nother objects is called occlusion culling . In crowded environments viewed from \nground level, there can be a great deal of inter-object occlusion, making oc-\nclusion culling extremely important. In less crowded scenes, or when scenes \nare viewed from above, much less occlusion may be present and the cost of \nocclusion culling may outweigh its beneﬁ ts.\nGross occlusion culling of a large-scale environment can be done by pre-\ncalculating a potentially visible set (PVS). For any given camera vantage point, \na PVS lists those scene objects that might be visible. A PVS errs on the side of \nincluding objects that aren’t actually visible, rather than excluding objects that \nactually would have contributed to the rendered scene.\nOne way to implement a PVS system is to chop the level up into regions \nof some kind. Each region can be provided with a list of the other regions \nthat can be seen when the camera is inside it. These PVSs might be manu-\nally speciﬁ ed by the artists or game designers. More commonly, an automated \noﬄ  ine tool generates the PVS based on user-speciﬁ ed regions. Such a tool \nusually operates by rendering the scene from various randomly distributed \nvantage points within a region. Every region’s geometry is color coded, so the \nlist of visible regions can be found by scanning the resulting frame buﬀ er and \ntabulating the region colors that are found. Because automated PVS tools are \nimperfect, they typically provide the user with a mechanism for tweaking the \nresults, either by manually placing vantage points for testing, or by manually \nspecifying a list of regions that should be explicitly included or excluded from \na particular region’s PVS.\nPortals\nAnother way to determine what portions of a scene are visible is to use portals . \nIn portal rendering, the game world is divided up into semiclosed regions \nthat are connected to one another via holes, such as windows and doorways. \nThese holes are called portals. They are usually represented by polygons that \ndescribe their boundaries.\n10.2. The Rendering Pipeline\n\n\n462 \n10. The Rendering Engine\nTo render a scene with portals, we start by rendering the region that con-\ntains the camera. Then, for each portal in the region, we extend a frustum-like \nvolume consisting of planes extending from the camera’s focal point through \neach edge of the portal’s bounding polygon. The contents of the neighboring \nregion can be culled to this portal volume in exactly the same way geometry is \nculled against the camera frustum. This ensures that only the visible geometry \nin the adjacent regions will be rendered. Figure 10.44 provides an illustration \nof this technique.\nOcclusion Volumes (Antiportals)\nIf we ﬂ ip the portal concept on its head, pyramidal volumes can also be used \nto describe regions of the scene that cannot be seen because they are being \noccluded by an object. These volumes are known as occlusion volumes or anti-\nportals . To construct an occlusion volume, we ﬁ nd the silhouett e edges of each \nFigure 10.44.  Portals are used to deﬁ ne frustum-like volumes which are used to cull the con-\ntents of neighboring regions. In this example, objects A, B, and D will be culled because they \nlie outside one of the portals; the other objects will be visible.\nA\nH\nE\nD\nF\nG\nB\nC\nFigure 10.45.  As a result of the antiportals corresponding to objects A, B, and C, objects D, E, \nF, and G are culled. Therefore only A, B, C, and H are visible.\n\n\n463 \noccluding object and extend planes outward from the camera’s focal point \nthrough each of these edges. We test more-distant objects against these oc-\nclusion volumes and cull them if they lie entirely within the occlusion region. \nThis is illustrated in Figure 10.45.\nPortals are best used when rendering enclosed indoor environments with \na relatively small number of windows and doorways between “rooms.” In \nthis kind of scene, the portals occupy a relatively small percentage of the total \nvolume of the camera frustum, resulting in a large number of objects outside \nthe portals which can be culled. Antiportals are best applied to large outdoor \nenvironments, in which nearby objects oft en occlude large swaths of the cam-\nera frustum. In this case, the antiportals occupy a relatively large percent-\nage of the total camera frustum volume, resulting in large numbers of culled \nobjects.\n10.2.7.2. Primitive Submission\n Once a list of visible geometric primitives has been generated, the individual \nprimitives must be submitt ed to the GPU pipeline for rendering. This can be \naccomplished by making calls to DrawIndexedPrimitive() in DirectX or \nglDrawArrays() in OpenGL.\nRender State\n As we learned in Section 10.2.5, the functionality of many of the GPU pipeline’s \nstages is ﬁ xed but conﬁ gurable. And even programmable stages are driven in \npart by conﬁ gurable parameters. Some examples of these conﬁ gurable param-\neters are listed below (although this is by no means a complete list!)\nz world-view matrix;\nz light direction vectors;\nz texture bindings (i.e., which textures to use for a given material/\nshader);\nz texture addressing and ﬁ ltering modes;\nz time base for scrolling textures and other animated eﬀ ects;\nz z test (enabled or disabled);\nz alpha blending options.\nThe set of all conﬁ gurable parameters within the GPU pipeline is known \nas the hardware state or render state. It is the application stage’s responsibility to \nensure that the hardware state is conﬁ gured properly and completely for each \nsubmitt ed primitive. Ideally these state sett ings are described completely by \nthe material associated with each submesh. So the application stage’s job boils \n10.2. The Rendering Pipeline\n\n\n464 \n10. The Rendering Engine\ndown to iterating through the list of visible mesh instances, iterating over each \nsubmesh-material pair, sett ing the render state based on the material’s speciﬁ -\ncations, and then calling the low level primitive submission functions (Draw-\nIndexedPrimitive(), glDrawArrays() or similar).\nState Leaks\nIf we forget to set some aspect of the render state between submitt ed primi-\ntives, the sett ings used on the previous primitive will “leak” over onto the new \nprimitive. A render state leak might manifest itself as an object with the wrong \ntexture or an incorrect lighting eﬀ ect, for example. Clearly it’s important that \nthe application stage never allow state leaks to occur.\nThe GPU Command List\nThe application stage actually communicates with the GPU via a command \nlist . These commands interleave render state sett ings with references to the \ngeometry that should be drawn. For example, to render objects A and B with \nmaterial 1, followed by objects C, D, and E using material 2, the command list \nmight look like this:\nz Set render state for material 1 (multiple commands, one per render state \nsett ing).\nz Submit primitive A.\nz Submit primitive B.\nz Set render state for material 2 (multiple commands).\nz Submit primitive C.\nz Submit primitive D.\nz Submit primitive E.\nUnder the hood, API functions like DrawIndexedPrimitive() actu-\nally just construct and submit GPU command lists. The cost of these API calls \ncan themselves be too high for some applications. To maximize performance, \nsome game engines build GPU command lists manually or by calling a low-\nlevel rendering API like the PS3’s libgcm library.\n10.2.7.3. Geometry Sorting\nRender state sett ings are global—they apply to the entire GPU as a whole. \nSo in order to change render state sett ings, the entire GPU pipeline must be \nﬂ ushed before the new sett ings can be applied. This can cause massive perfor-\nmance degradation if not managed carefully.\nClearly we’d like to change render sett ings as infrequently as possible. \nThe best way to accomplish this is to sort our geometry by material. That way, \n\n\n465 \nwe can install material A’s sett ings, render all geometry associated with mate-\nrial A, and then move on to material B.\nUnfortunately, sorting geometry by material can have a detrimental eﬀ ect \non rendering performance because it increases overdraw —a situation in which \nthe same pixel is ﬁ lled multiple times by multiple overlapping triangles. Cer-\ntainly some overdraw is necessary and desirable, as it is the only way to prop-\nerly alpha-blend transparent and translucent surfaces into a scene. However, \noverdraw of opaque pixels is always a waste of GPU bandwidth.\nThe early z test is designed to discard occluded fragments before the ex-\npensive pixel shader has a chance to execute. But to take maximum advantage \nof early z, we need to draw the triangles in front-to-back order. That way, the \nclosest triangles will ﬁ ll the z-buﬀ er right oﬀ  the bat, and all of the fragments \ncoming from more-distant triangles behind them can be quickly discarded, \nwith litt le or no overdraw.\nZ Prepass to the Rescue\nHow can we reconcile the need to sort geometry by material with the conﬂ ict-\ning need to render opaque geometry in a front-to-back order? The answer lies \nin a GPU feature known as z prepass .\nThe idea behind z prepass is to render the scene twice: the ﬁ rst time to \ngenerate the contents of the z-buﬀ er as eﬃ  ciently as possible and the second \ntime to populate the frame buﬀ er with full color information (but this time \nwith no overdraw, thanks to the contents of the z-buﬀ er). The GPU provides a \nspecial double-speed rendering mode in which the pixel shaders are disabled, \nand only the z-buﬀ er is updated. Opaque geometry can be rendered in front-\nto-back order during this phase, to minimize the time required to generate \nthe z-buﬀ er contents. Then the geometry can be resorted into material order \nand rendered in full color with minimal stage changes for maximum pipeline \nthroughput.\nOnce the opaque geometry has been rendered, transparent surfaces can \nbe drawn in back-to-front order. Unfortunately, there is no general solution \nto the material sorting problem for transparent geometry. We must render it \nin back-to-front order to achieve the proper alpha-blended result. Therefore \nwe must accept the cost of frequent state changes when drawing transparent \ngeometry (unless our particular game’s usage of transparent geometry is such \nthat a speciﬁ c optimization can be implemented).\n10.2.7.4. Scene Graphs\nModern game worlds can be very large. The majority of the geometry in most \nscenes does not lie within the camera frustum, so frustum culling all of these \n10.2. The Rendering Pipeline\n\n\n466 \n10. The Rendering Engine\nobjects explicitly is usually incredibly wasteful. Instead, we would like to de-\nvise a data structure that manages all of the geometry in the scene and allows \nus to quickly discard large swaths of the world that are nowhere near the cam-\nera frustum prior to performing detailed frustum culling. Ideally, this data \nstructure should also help us to sort the geometry in the scene, either in front-\nto-back order for the z prepass or in material order for full-color rendering.\nSuch a data structure is oft en called a scene graph , in reference to the graph-\nlike data structures oft en used by ﬁ lm rendering engines and DCC tools like \nMaya. However, a game’s scene graph needn’t be a graph, and in fact the data \nstructure of choice is usually some kind of tree. The basic idea behind most \nof these data structures is to partition three-dimensional space in a way that \nmakes it easy to discard regions that do not intersect the frustum, without \nhaving to frustum cull all of the individual objects within them. Examples \ninclude quadtrees and octress, BSP trees, kd-trees, and spatial hashing tech-\nniques.\nQuadtrees and Octrees\nA quadtree divides space into quadrants recursively. Each level of recursion \nis represented by a node in the quadtree with four children, one for each \nquadrant. The quadrants are typically separated by vertically oriented, ax-\nis-aligned planes, so that the quadrants are square or rectangular. However, \nsome quadtrees subdivide space using arbitrarily-shaped regions.\nQuadtrees can be used to store and organize virtually any kind of spa-\ntially-distributed data. In the context of rendering engines, quadtrees are of-\nten used to store renderable primitives such as mesh instances, subregions of \nterrain geometry, or individual triangles of a large static mesh, for the pur-\nposes of eﬃ  cient frustum culling. The renderable primitives are stored at the \nFigure 10.46.  A top-down view of a space divided recursively into quadrants for storage in a \nquadtree, based on the criterion of one point per region.\n\n\n467 \nleaves of the tree, and we usually aim to achieve a roughly uniform number of \nprimitives within each leaf region. This can be achieved by deciding whether \nto continue or terminate the subdivision based on the number of primitives \nwithin a region.\nTo determine which primitives are visible within the camera frustum, \nwe walk the tree from the root to the leaves, checking each region for inter-\nsection with the frustum. If a given quadrant does not intersect the frustum, \nthen we know that none of its child regions will do so either, and we can stop \ntraversing that branch of the tree. This allows us to search for potentially \nvisible primitives much more quickly than would be possible with a linear \nsearch (usually in O(log n) time). An example of a quadtree subdivision of \nspace is shown in Figure 10.46.\nAn octree is the three-dimensional equivalent of a quadtree, dividing space \ninto eight subregions at each level of the recursive subdivision. The regions of \nan octree are oft en cubes or rectangular prisms but can be arbitrarily-shaped \nthree-dimensional regions in general.\nBounding Sphere Trees\nIn the same way that a quadtree or octree subdivides space into (usually) \nrectangular regions, a bounding sphere tree divides space into spherical regions \nhierarchically. The leaves of the tree contain the bounding spheres of the ren-\nderable primitives in the scene. We collect these primitives into small logical \ngroups and calculate the net bounding sphere of each group. The groups are \nthemselves collected into larger groups, and this process continues until we \nhave a single group with a bounding sphere that encompasses the entire vir-\ntual world. To generate a list of potentially visible primitives, we walk the tree \nfrom the root to the leaves, testing each bounding sphere against the frustum, \nand only recursing down branches that intersect it.\nBSP Trees\nA binary space partitioning (BSP) tree divides space in half recursively until \nthe objects within each half-space meet some predeﬁ ned criteria (much as a \nquadtree divides space into quadrants). BSP trees have numerous uses, in-\ncluding collision detection and constructive solid geometry, as well as its most \nwell-known application as a method for increasing the performance of frus-\ntum culling and geometry sorting for 3D graphics. A kd-tree is a generaliza-\ntion of the BSP tree concept to k dimensions.\nIn the context of rendering, a BSP tree divides space with a single plane at \neach level of the recursion. The dividing planes can be axis-aligned, but more \ncommonly each subdivision corresponds to the plane of a single triangle in \nthe scene. All of the other triangles are then categorized as being either on \n10.2. The Rendering Pipeline\n\n\n468 \n10. The Rendering Engine\nthe front side or the back side of the plane. Any triangles that intersect the \ndividing plane are themselves divided into three new triangles, so that every \ntriangle lies either entirely in front of or entirely behind the plane, or is copla-\nnar with it. The result is a binary tree with a dividing plane and one or more \ntriangles at each interior node and triangles at the leaves.\nA BSP tree can be used for frustum culling in much the same way a \nquadtree, octree, or bounding sphere tree can. However, when generated with \nindividual triangles as described above, a BSP tree can also be used to sort tri-\nangles into a strictly back-to-front or front-to-back order. This was particularly \nimportant for early 3D games like Doom, which did not have the beneﬁ t of a \nz-buﬀ er and so were forced to use the painter’s algorithm (i.e., to render the \nscene from back to front) to ensure proper inter-triangle occlusion.\nGiven a camera view point in 3D space, a back-to-front sorting algorithm \nwalks the tree from the root. At each node, we check whether the view point \nis in front of or behind that node’s dividing plane. If the camera is in front of \na node’s plane, we visit the node’s back children ﬁ rst, then draw any triangles \nthat are coplanar with its dividing plane, and ﬁ nally we visit its front chil-\ndren. Likewise, when the camera’s view point is found to be behind a node’s \ndividing plane, we visit the node’s front children ﬁ rst, then draw the triangles \ncoplanar with the node’s plane, and ﬁ nally we visit its back children. This \ntraversal scheme ensures that the triangles farthest from the camera will be \nvisited before those that are closer to it, and hence it yields a back-to-front \nFigure 10.47.  An example of back-to-front traversal of the triangles in a BSP tree. The tri-\nangles are shown edge-on in two dimensions for simplicity, but in a real BSP tree the triangles \nand dividing planes would be arbitrarily oriented in space.\nA\nB\nC\nD2\nD1\nCamera\nVisit A\nCam is in front\n    Visit B\n        Leaf node\nDraw B\nDraw A\n    Visit C\n        Cam is in front\n            V isit D 1\n                Leaf node\nDraw D1\nDraw C\n            V isit D 2\n                Leaf node\nDraw D2\nA\nD2\nD1\nC\nB\n\n\n469 \nordering. Because this algorithm traverses all of the triangles in the scene, the \norder of the traversal is independent of the direction the camera is looking. A \nsecondary frustum culling step would be required in order to traverse only \nvisible triangles. A simple BSP tree is shown in Figure 10.47, along with the \ntree traversal that would be done for the camera position shown.\nFull coverage of BSP tree generation and usage algorithms is beyond our \nscope here. See htt p://www.ccs.neu.edu/home/donghui/teaching/slides/ge-\nometry/BSP2D.ppt and htt p://www.gamedev.net/reference/articles/article657.\nasp for more details on BSP trees.\n10.2.7.5. Choosing a Scene Graph\n Clearly there are many diﬀ erent kinds of scene graphs. Which data structure \nto select for your game will depend upon the nature of the scenes you expect \nto be rendering. To make the choice wisely, you must have a clear understand-\ning of what is required—and more importantly what is not required—when \nrendering scenes for your particular game.\nFor example, if you’re implementing a ﬁ ghting game, in which two char-\nacters batt le it out in a ring surrounded by a mostly static environment, you \nmay not need much of a scene graph at all. If your game takes place primarily \nin enclosed indoor environments, a BSP tree or portal system may serve you \nwell. If the action takes place outdoors on relatively ﬂ at terrain, and the scene \nis viewed primarily from above (as might be the case in a real-time strategy \ngame or god game), a simple quad tree might be all that’s required to achieve \nhigh rendering speeds. On the other hand, if an outdoor scene is viewed pri-\nmarily from the point of view of someone on the ground, we may need addi-\ntional culling mechanisms. Densely populated scenes can beneﬁ t from an oc-\nclusion volume (antiportal) system, because there will be plenty of occluders. \nOn the other hand, if your outdoor scene is very sparse, adding an antiportal \nsystem probably won’t pay dividends (and might even hurt your frame rate).\nUltimately, your choice of scene graph should be based on hard data ob-\ntained by actually measuring the performance of your rendering engine. You \nmay be surprised to learn where all your cycles are actually going! But once \nyou know, you can select scene graph data structures and/or other optimiza-\ntions to target the speciﬁ c problems at hand.\n10.3. Advanced Lighting and Global Illumination\nIn order to render photorealistic scenes, we need physically accurate global \nillumination algorithms. A complete coverage of these techniques is beyond \nour scope. In the following sections, we will brieﬂ y outline the most prevalent \n10.3. Advanced Lighting and Global Illumination\n\n\n470 \n10. The Rendering Engine\ntechniques in use within the game industry today. Our goal here is to provide \nyou with an awareness of these techniques and a jumping oﬀ  point for further \ninvestigation. For an excellent in-depth coverage of this topic, see [8].\n10.3.1. Image-Based Lighting\nA number of advanced lighting and shading techniques make heavy use of \nimage data, usually in the form of two-dimensional texture maps. These are \ncalled image-based lighting algorithms.\n10.3.1.1. Normal Mapping\nA normal map speciﬁ es a surface normal direction vector at each texel. This al-\nlows a 3D modeler to provide the rendering engine with a highly detailed de-\nscription of a surface’s shape, without having to tessellate the model to a high \ndegree (as would be required if this same information were to be provided \nvia vertex normals). Using a normal map, a single ﬂ at triangle can be made to \nlook as though it were constructed from millions of tiny triangles. An example \nof normal mapping is shown in Figure 10.48.\nThe normal vectors are typically encoded in the RGB color channels of the \ntexture, with a suitable bias to overcome the fact that RGB channels are strictly \npositive while normal vector components can be negative. Sometimes only \ntwo coordinates are stored in the texture; the third can be easily calculated at \nruntime, given the assumption that the surface normals are unit vectors.\nFigure 10.48.  An example of a normal-mapped surface.\n10.3.1.2. Height Maps: Parallax and Relief Mapping\nAs its name implies, a height map encodes the height of the ideal surface above \nor below the surface of the triangle. Height maps are typically encoded as \ngrayscale images, since we only need a single height value per texel.\n\n\n471 \nHeight maps are oft en used for parallax mapping and relief mapping —two \ntechniques that can make a planar surface appear to have rather extreme \nheight variation that properly self-occludes and self-shadows. Figure 10.49 \nshows an example of parallax occlusion mapping implemented in DirectX 9.\nA height map can also be used as a cheap way to generate surface normals. \nThis technique was used in the early days of bump mapping . Nowadays, most \ngame engines store surface normal information explicitly in a normal map, \nrather than calculating the normals from a height map.\n10.3.1.3. Specular/Gloss Maps\nWhen light reﬂ ects directly oﬀ  a shiny surface, we call this specular reﬂ ection. \nThe intensity of a specular reﬂ ection depends on the relative angles of the \nviewer, the light source, and the surface normal. As we saw in Section 10.1.3.2, \nthe specular intensity takes the form \n(\n) ,\nSk\nα\n⋅\nR V\n where R is the reﬂ ection of \nthe light’s direction vector about the surface normal, V is the direction to the \nviewer, kS is the overall specular reﬂ ectivity of the surface, and α is called the \nspecular power.\nMany surfaces aren’t uniformly glossy. For example, when a person’s face \nis sweaty and dirty, wet regions appear shiny, while dry or dirty areas appear \ndull. We can encode high-detail specularity information in a special texture \nmap known as a specular map .\nIf we store the value of kS in the texels of a specular map, we can control \nhow much specular reﬂ ection should be applied at each texel. This kind of \nspecular map is sometimes called a gloss map . It is also called a specular mask, \nbecause zero-valued texels can be used to “mask oﬀ ” regions of the surface \nwhere we do not want specular reﬂ ection applied. If we store the value of α \nin our specular map, we can control the amount of “focus” our specular high-\nFigure 10.49.  DirectX 9 parallax occlusion mapping. The surface is actually a ﬂ at disc; a height \nmap texture is used to deﬁ ne the surface details.\n10.3. Advanced Lighting and Global Illumination\n\n\n472 \n10. The Rendering Engine\nlights will have at each texel. This kind of texture is called a specular power map . \nAn example of a gloss map is shown in Figure 10.50.\n10.3.1.4. Environment Mapping\nAn environment map looks like a panoramic photograph of the environment \ntaken from the point of view of an object in the scene, covering a full 360 \ndegrees horizontally and either 180 degrees or 360 degrees vertically. An envi-\nronment map acts like a description of the general lighting environment sur-\nrounding an object. It is generally used to inexpensively render reﬂ ections.\nThe two most common formats are spherical environment maps and cubic \nenvironment maps . A spherical map looks like a photograph taken through a \nﬁ sheye lens, and it is treated as though it were mapped onto the inside of a \nsphere whose radius is inﬁ nite, centered about the object being rendered. The \nproblem with sphere maps is that they are addressed using spherical coordi-\nnates. Around the equator, there is plenty of resolution both horizontally and \nvertically. However, as the vertical (azimuthal) angle approaches vertical, the \nresolution of the texture along the horizontal (zenith) axis decreases to a single \ntexel. Cube maps were devised to avoid this problem.\nA cube map looks like a composite photograph pieced together from pho-\ntos taken in the six primary directions (up, down, left , right, front, and back). \nDuring rendering, a cube map is treated as though it were mapped onto the \nsix inner surfaces of a box at inﬁ nity, centered on the object being rendered.\nTo read the environment map texel corresponding to a point P on the \nsurface of an object, we take the ray from the camera to the point P and reﬂ ect \nFigure 10.50.  This screen shot from EA’s Fight Night Round 3 shows how a gloss map can \nbe used to control the degree of specular reﬂ ection that should be applied to each texel of \na surface.\n\n\n473 \nit about the surface normal at P. The reﬂ ected ray is followed until it intersects \nthe sphere or cube of the environment map. The value of the texel at this inter-\nsection point is used when shading the point P.\n10.3.1.5. Three-Dimensional Textures\nModern graphics harware also includes support for three-dimensional tex-\ntures. A 3D texture can be thought of as a stack of 2D textures. The GPU knows \nhow to address and ﬁ lter a 3D texture, given a three-dimensional texture co-\nordinate (u, v, w).\nThree-dimensional textures can be useful for describing the appearance \nor volumetric properties of an object. For example, we could render a marble \nsphere and allow it to be cut by an arbitrary plane. The texture would look \ncontinuous and correct across the cut no matt er where it was made, because \nthe texture is well-deﬁ ned and continuous throughout the entire volume of \nthe sphere.\n10.3.2. High Dynamic Range Lighting\nA display device like a television set or CRT monitor can only produce a lim-\nited range of intensities. This is why the color channels in the frame buﬀ er are \nlimited to a zero to one range. But in the real world, light intensities can grow \narbitrarily large. High dynamic range (HDR) lighting att empts to capture this \nwide range of light intensities.\nHDR lighting performs lighting calculations without clamping the result-\ning intensities arbitrarily. The resulting image is stored in a format that per-\nmits intensities to grow beyond one. The net eﬀ ect is an image in which ex-\ntreme dark and light regions can be represented without loss of detail within \neither type of region.\nPrior to display on-screen, a process called tone mapping is used to shift  \nand scale the image’s intensity range into the range supported by the display \ndevice. Doing this permits the rendering engine to reproduce many real-world \nvisual eﬀ ects, like the temporary blindness that occurs when you walk from \na dark room into a brightly lit area, or the way light seems to bleed out from \nbehind a brightly back-lit object (an eﬀ ect known as bloom ).\nOne way to represent an HDR image is to store the R, G, and B chan-\nnels using 32-bit ﬂ oating point numbers, instead of 8-bit integers. Another \nalternative is to employ an entirely diﬀ erent color model altogether. The log-\nLUV color model is a popular choice for HDR lighting. In this model, color \nis represented as an intensity channel (L) and two chromaticity channels \n(U and V). Because the human eye is more sensitive to changes in intensity \n10.3. Advanced Lighting and Global Illumination\n\n\n474 \n10. The Rendering Engine\nthan it is to changes in chromaticity, the L channel is stored in 16 bits while \nU and V are given only eight bits each. In addition, L is represented using a \nlogarithmic scale (base two) in order to capture a very wide range of light \nintensities.\n10.3.3. Global Illumination\nAs we noted in Section 10.1.3.1, global illumination refers to a class of light-\ning algorithms that account for light’s interactions with multiple objects in the \nscene, on its way from the light source to the virtual camera . Global illumina-\ntion accounts for eﬀ ects like the shadows that arise when one surface occludes \nanother, reﬂ ections, caustics, and the way the color of one object can “bleed” \nonto the objects around it. In the following sections, we’ll take a brief look \nat some of the most common global illumination techniques. Some of these \nmethods aim to reproduce a single isolated eﬀ ect, like shadows or reﬂ ections. \nOthers like radiosity and ray tracing methods aim to provide a holistic model \nof global light transport.\n10.3.3.1. Shadow Rendering\nShadows are created when a surface blocks light’s path. The shadows caused \nby an ideal point light source would be sharp, but in the real world shadows \nhave blurry edges; this is called the penumbra . A penumbra arises because \nreal-world light sources cover some area and so produce light rays that graze \nthe edges of an object at diﬀ erent angles.\nThe two most prevalent shadow rendering techniques are shadow vol-\numes and shadow maps. We’ll brieﬂ y describe each in the sections below. In \nboth techniques, objects in the scene are generally divided into three catego-\nries: objects that cast shadows, objects that are to receive shadows, and ob-\njects that are entirely excluded from consideration when rendering shadows. \nLikewise, the lights are tagged to indicate whether or not they should gener-\nate shadows. This important optimization limits the number of light-object \ncombinations that need to be processed in order to produce the shadows in \na scene.\nShadow Volumes\n In the shadow volume technique, each shadow caster is viewed from the \nvantage point of a shadow-generating light source, and the shadow caster’s \nsilhouett e edges are identiﬁ ed. These edges are extruded in the direction of \nthe light rays emanating from the light source. The result is a new piece of \ngeometry that describes the volume of space in which the light is occluded by \nthe shadow caster in question. This is shown in Figure 10.51.\n\n\n475 \nA shadow volume is used to generate a shadow by making use of a special \nfull-screen buﬀ er known as the stencil buﬀ er . This buﬀ er stores a single inte-\nger value corresponding to each pixel of the screen. Rendering can be masked \nby the values in the stencil buﬀ er—for example, we could conﬁ gure the GPU \nto only render fragments whose corresponding stencil values are non-zero. In \naddition, the GPU can be conﬁ gured so that rendered geometry updates the \nvalues in the stencil buﬀ er in various useful ways.\nTo render shadows, the scene is ﬁ rst drawn to generate an unshadowed \nimage in the frame buﬀ er, along with an accurate z-buﬀ er . The stencil buﬀ er \nis cleared so that it contains zeros at every pixel. Each shadow volume is then \nrendered from the point of view of the camera in such a way that front-facing \ntriangles increase the values in the stencil buﬀ er by one, while back-facing \ntriangles decrease them by one. In areas of the screen where the shadow vol-\nume does not appear at all, of course the stencil buﬀ er’s pixels will be left  \ncontaining zero. The stencil buﬀ er will also contain zeros where both the front \nand back faces of the shadow volume are visible, because the front face will \nincrease the stencil value but the back face will decrease it again. In areas \nwhere the back face of the shadow volume has been occluded by “real” scene \ngeometry, the stencil value will be one. This tells us which pixels of the screen \nare in shadow. So we can render shadows in a third pass, by simply darkening \nthose regions of the screen that contain a non-zero stencil buﬀ er value.\nShadow Maps\nThe shadow mapping technique is eﬀ ectively a per-fragment depth test per-\nformed from the point of view of the light instead of from the point of view \nof the camera. The scene is rendered in two steps: First, a shadow map texture \nFigure 10.51.  A shadow volume generated by extruding the silhouette edges of a shadow \ncasting object as seen from the point of view of the light source.\n10.3. Advanced Lighting and Global Illumination\n\n\n476 \n10. The Rendering Engine\nis generated by rendering the scene from the point of view of the light source \nand saving oﬀ  the contents of the depth buﬀ er. Second, the scene is rendered \nas usual, and the shadow map is used to determine whether or not each frag-\nment is in shadow. At each fragment in the scene, the shadow map tells us \nwhether or not the light is being occluded by some geometry that is closer to \nthe light source, in just the same way that the z-buﬀ er tells us whether a frag-\nment is being occluded by a triangle that is closer to the camera.\nA shadow map contains only depth information—each texel records how \nfar away it is from the light source. Shadow maps are therefore typically ren-\ndered using the hardware’s double-speed z-only mode (since all we care about \nis the depth information). For a point light source, a perspective projection is \nused when rendering the shadow map; for a directional light source, an ortho-\ngraphic projection is used instead.\nTo render a scene using a shadow map, we draw the scene as usual from \nthe point of view of the camera. For each vertex of every triangle, we calculate \nits position in light space —i.e., in the same “view space” that was used when \ngenerating the shadow map in the ﬁ rst place. These light space coordinates \ncan be interpolated across the triangle, just like any other vertex att ribute. This \ngives us the position of each fragment in light space. To determine whether a \ngiven fragment is in shadow or not, we convert the fragment’s light-space (x, \ny)-coordinates into texture coordinates (u, v) within the shadow map. We then \ncompare the fragment’s light-space z-coordinate with the depth stored at the \ncorresponding texel in the shadow depth map. If the fragment’s light-space z \nis farther away from the light than the texel in the shadow map, then it must be \noccluded by some other piece of geometry that is closer to the light source—\nhence it is in shadow. Likewise, if the fragment’s light-space z is closer to the \nlight source than the texel in the shadow map, then it is not occluded and is \nnot in shadow. Based on this information, the fragment’s color can be adjusted \naccordingly. The shadow mapping process is illustrated in Figure 10.52.\nFigure 10.52.  The far left image is a shadow map—the contents of the z-buffer as rendered \nfrom the point of view of a particular light source. The pixels of the center image are black \nwhere the light-space depth test failed (fragment in shadow) and white where it succeeded \n(fragment not in shadow). The far right image shows the ﬁ nal scene rendered with shadows.\n\n\n477 \n10.3.3.2. Ambient Occlusion\nAmbient occlusion is a technique for modeling contact shadows —the soft  shad-\nows that arise when a scene is illuminated by only ambient light. In eﬀ ect, am-\nbient occlusion describes how “accessible” each point on a surface is to light \nin general. For example, the interior of a section of pipe is less accessible to \nambient light than its exterior. If the pipe were placed outside on an overcast \nday, its interior would generally appear darker than its exterior.\nFigure 10.53 shows the level of ambient occlusion across an object’s sur-\nface. Ambient occlusion is measured at a point on a surface by constructing \na hemisphere with a very large radius centered on that point and determing \nwhat percentage of that hemisphere’s area is visible from the point in ques-\ntion. It can be precomputed oﬄ  ine for static objects, because ambient occlu-\nsion is independent of view direction and the direction of incident light. It is \ntypically stored in a texture map that records the level of ambient occlusion at \neach texel across the surface.\n10.3.3.3. Reﬂ ections\nReﬂ ections occur when light bounces oﬀ  a highly specular (shiny) surface pro-\nducing an image of another portion of the scene in the surface. Reﬂ ections \ncan be implemented in a number of ways. Environment maps are used to \nFigure 10.53.  A dragon \nrendered with ambient \nocclusion.\nFigure 10.54.  Mirror reﬂ ections in Luigi’s Mansion implemented by rendering the scene to a \ntexture that is subsequently applied to the mirror’s surface.\n10.3. Advanced Lighting and Global Illumination\n\n\n478 \n10. The Rendering Engine\nproduce general reﬂ ections of the surrounding environment on the surfaces \nof shiny objects. Direct reﬂ ections in ﬂ at surfaces like mirrors can be produced \nby reﬂ ecting the camera’s position about the plane of the reﬂ ective surface and \nthen rendering the scene from that reﬂ ected point of view into a texture . The \ntexture is then applied to the reﬂ ective surface in a second pass.\n10.3.3.4. Caustics\nCaustics are the bright specular highlights arising from intense reﬂ ections or \nrefractions from very shiny surfaces like water or polished metal. When the \nreﬂ ective surface moves, as is the case for water, the caustic eﬀ ects glimmer \nand “swim” across the surfaces on which they fall. Caustic eﬀ ects can be pro-\nduced by projecting a (possibly animated) texture containing semi-random \nbright highlights onto the aﬀ ected surfaces. An example of this technique is \nshown in Figure 10.55.\nFigure 10.55.  Water caustics produced by projecting an animated texture onto the affected \nsurfaces.\n10.3.3.5. Subsurface Scattering\nWhen light enters a surface at one point, is scatt ered beneath the surface, \nand then reemerges at a diﬀ erent point on the surface, we call this subsurface \nscatt ering . This phenomenon is responsible for the “warm glow” of human \nskin, wax, and marble statues. Subsurface scatt ering is described by a more-\nadvanced variant of the BRDF (see Section 10.1.3.2) known as the BSSRDF \n(bidirectional surface scatt ering reﬂ ectance distribution function).\nSubsurface scatt ering can be simulated in a number of ways. Depth-map–\nbased subsurface scatt ering renders a shadow map (see Section 10.3.3.1), but \ninstead of using it to determine which pixels are in shadow, it is used to mea-\nsure how far a beam of light would have to travel in order to pass all the way \n\n\n479 \nthrough the occluding object. The shadowed side of the object is then given \nan artiﬁ cial diﬀ use lighting term whose intensity is inversely proportional to \nthe distance the light had to travel in order to emerge on the opposite side of \nthe object. This causes objects to appear to be glowing slightly on the side op-\nposite to the light source but only where the object is relatively thin. For more \ninformation on subsurface scatt ering techniques, see htt p://htt p.developer.\nnvidia.com/GPUGems/gpugems_ch16.html.\n10.3.3.6. Precomputed Radiance Transfer (PRT)\nPrecomputed radiance transfer (PRT) is a relatively new technique that att empts \nto simulate the eﬀ ects of radiosity-based rendering methods in real time. It \ndoes so by precomputing and storing a complete description of how an inci-\ndent light ray would interact with a surface (reﬂ ect, refract, scatt er, etc.) when \napproaching from every possible direction. At runtime, the response to a par-\nticular incident light ray can be looked up and quickly converted into very \naccurate lighting results.\nIn general the light’s response at a point on the surface is a complex func-\ntion deﬁ ned on a hemisphere centered about the point. A compact repre-\nsentation of this function is required to make the PRT technique practical. A \ncommon approach is to approximate the function as a linear combination of \nspherical harmonic basis functions. This is essentially the three-dimensional \nequivalent of encoding a simple scalar function f(x) as a linear combination of \nshift ed and scaled sine waves.\nThe details of PRT are far beyond our scope. For more information, see \nhtt p://web4.cs.ucl.ac.uk/staﬀ /j.kautz/publications/prtSIG02.pdf. PRT lighting \nFigure 10.56.  On the left, a dragon rendered without subsurface scattering (i.e., using a BRDF \nlighting model). On the right, the same dragon rendered with subsurface scattering (i.e., using \na BSSRDF model). Images rendered by Rui Wang at the University of Virginia.\n10.3. Advanced Lighting and Global Illumination\n\n\n480 \n10. The Rendering Engine\ntechniques are demonstrated in a DirectX sample program available in the \nDirectX SDK—see htt p://msdn.microsoft .com/en-us/library/bb147287.aspx \nfor more details.\n10.3.4. Deferred Rendering\nIn traditional triangle-rasterization–based rendering, all lighting and shad-\ning calculations are performed on the triangle fragments in view space. The \nproblem with this technique is that it is inherently ineﬃ  cient. For one thing, \nwe potentially do work that we don’t need to do. We shade the vertices of tri-\nangles, only to discover during the rasterization stage that the entire triangle \nis being depth-culled by the z test. Early z tests help eliminate unnecessary \npixel shader evaluations, but even this isn’t perfect. What’s more, in order to \nhandle a complex scene with lots of lights, we end up with a proliferation of \ndiﬀ erent versions of our vertex and pixel shaders—versions that handle dif-\nFigure 10.57.  Screenshots from Killzone 2, showing some of the typical components of the \nG-buffer used in deferred rendering. The upper image shows the ﬁ nal rendered image. Below \nit, clockwise from the upper left, are the albedo (diffuse) color, depth, view-space normal, \nscreen space 2D motion vector (for motion blurring), specular power, and specular intensity.\n",
      "page_number": 482,
      "chapter_number": 25,
      "summary": "This chapter covers segment 25 (pages 482-502). Key topics include light, rendering, and render. The Rendering Engine\nmore-detailed introduction to Cg shader programming, refer to the Cg tu-\ntorial available on NVIDIA’s website at htt p://developer.nvidia.com/object/\ncg_tutorial_home.html.",
      "keywords": [
        "Rendering Engine",
        "Rendering",
        "shadow map",
        "scene",
        "light",
        "Shadow",
        "map",
        "surface",
        "objects",
        "GPU",
        "point",
        "BSP tree",
        "light source",
        "Global Illumination",
        "geometry"
      ],
      "concepts": [
        "light",
        "rendering",
        "render",
        "objects",
        "maps",
        "mapping",
        "map",
        "surfaces",
        "triangles",
        "regions"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 43,
          "title": "Segment 43 (pages 424-431)",
          "relevance_score": 0.67,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 45,
          "title": "Segment 45 (pages 431-439)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 5,
          "title": "Segment 5 (pages 41-49)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 54,
          "title": "Segment 54 (pages 523-530)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 59,
          "title": "Segment 59 (pages 570-580)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 503-525)",
      "start_page": 503,
      "end_page": 525,
      "detection_method": "synthetic",
      "content": "481 \nferent numbers of lights, diﬀ erent types of lights, diﬀ erent numbers of skin-\nning weights, etc.\nDeferred rendering is an alternative way to shade a scene that addresses \nmany of these problems. In deferred rendering, the majority of the lighting \ncalculations are done in screen space, not view space. We eﬃ  ciently render \nthe scene without worrying about lighting. During this phase, we store all \nthe information we’re going to need to light the pixels in a “deep” frame buf-\nfer known as the G-buﬀ er . Once the scene has been fully rendered, we use \nthe information in the G-buﬀ er to perform our lighting and shading calcula-\ntions. This is usually much more eﬃ  cient than view-space lighting, avoids the \nproliferation of shader variants, and permits some very pleasing eﬀ ects to be \nrendered relatively easily.\nThe G-buﬀ er may be physically implemented as a collection of buﬀ ers, \nbut conceptually it is a single frame buﬀ er containing a rich set of informa-\ntion about the lighting and surface properties of the objects in the scene at \nevery pixel on the screen. A typical G-buﬀ er might contain the following per-\npixel att ributes: depth, surface normal in clip space, diﬀ use color, specular \npower, even precomputed radiance transfer (PRT) coeﬃ  cients. The following \nsequence of screen shots from Guerrilla Games’ Killzone 2 shows some of the \ntypical components of the G-buﬀ er.\nAn in-depth discussion of deferred rendering is beyond our scope, but \nthe folks at Guerrilla Games have prepared an excellent presentation on the \ntopic, which is available at htt p://www.guerrilla-games.com/publications/\ndr_kz2_rsx_dev07.pdf.\n10.4. Visual Effects and Overlays\nThe rendering pipeline we’ve discussed to this point is responsible primarily \nfor rendering three-dimensional solid objects. A number of specialized render-\ning systems are typically layered on top of this pipeline, responsible for ren-\ndering visual elements like particle eﬀ ects, decals (small geometry overlays \nthat represent bullet holes, cracks, scratches, and other surface details), hair \nand fur, rain or falling snow, water , and other specialized visual eﬀ ects. Full-\nscreen post eﬀ ects may be applied, including vignett e (slight blur around the \nedges of the screen), motion blur, depth of ﬁ eld blurring, artiﬁ cial/enhanced \ncolorization, and the list goes on. Finally, the game’s menu system and heads-\nup display (HUD) are typically realized by rendering text and other two- or \nthree-dimensional graphics in screen space overlaid on top of the three-\ndimensional scene.\n10.4. Visual Effects and Overlays\n\n\n482 \n10. The Rendering Engine\nAn in-depth coverage of these engine systems is beyond our scope. In the \nfollowing sections, we’ll provide a brief overview of these rendering systems, \nand point you in the direction of additional information.\n10.4.1. Particle Effects\nA particle rendering system is concerned with rendering amorphous objects \nlike clouds of smoke, sparks, ﬂ ame, and so on. These are called particle eﬀ ects. \nThe key features that diﬀ erentiate a particle eﬀ ect from other kinds of render-\nable geometry are as follows:\nz It is composed of a very large number of relatively simple pieces of geom-\netry—most oft en simple cards called quads, composed of two triangles \neach.\nz The geometry is oft en camera-facing (i.e., billboarded ), meaning that the \nengine must take steps to ensure that the face normals of each quad \nalways point directly at the camera’s focal point.\nz Its materials are almost always semi-transparent or translucent. As such, \nparticle eﬀ ects have some stringent rendering order constraints that do \nnot apply to the majority of opaque objects in a scene.\nz Particles animate in a rich variety of ways. Their positions, orientations, \nsizes (scales), texture coordinates, and many of their shader parameters \nvary from frame to frame. These changes are deﬁ ned either by hand-\nauthored animation curves or via procedural methods.\nz Particles are typically spawned and killed continually. A particle emitt er \nis a logical entity in the world that creates particles at some user-speci-\nﬁ ed rate; particles are killed when they hit a predeﬁ ned death plane, or \nFigure 10.58.  Some particle effects.\n\n\n483 \nwhen they have lived for a user-deﬁ ned length of time, or as decided by \nsome other user-speciﬁ ed criteria.\nParticle eﬀ ects could be rendered using regular triangle mesh geometry \nwith appropriate shaders. However, because of the unique characteristics \nlisted above, a specialized particle eﬀ ect animation and rendering system is \nalways used to implement them in a real production game engine. A few ex-\nample particle eﬀ ects are shown in Figure 10.58.\nParticle system design and implementation is a rich topic that could oc-\ncupy many chapters all on its own. For more information on particle systems, \nsee [1] Section 10.7, [14] Section 20.5, [9] Section 13.7 and [10] Section 4.1.2.\n10.4.2. Decals\nA decal is a relatively small piece of geometry that is overlaid on top of the reg-\nular geometry in the scene, allowing the visual appearance of the surface to be \nmodiﬁ ed dynamically. Examples include bullet holes, foot prints, scratches, \ncracks, etc.\nThe approach most oft en used by modern engines is to model a decal as \na rectangular area that is to be projected along a ray into the scene. This gives \nrise to a rectangular prism in 3D space. Whatever surface the prism intersects \nﬁ rst becomes the surface of the decal. The triangles of the intersected geom-\netry are extracted and clipped against the four bounding planes of the decal’s \nprojected prism. The resulting triangles are texture-mapped with a desired \ndecal texture by generating appropriate texture coordinates for each vertex. \nThese texture-mapped triangles are then rendered over the top of the regular \nscene, oft en using parallax mapping to give them the illusion of depth and \nwith a slight z-bias (usually implemented by shift ing the near plane slightly) \nso they don’t experience z-ﬁ ghting with the geometry on which they are over-\nFigure 10.59.  Parallax-mapped decals from Uncharted: Drake’s Fortune.\n10.4. Visual Effects and Overlays\n\n\n484 \n10. The Rendering Engine\nlaid. The result is the appearance of a bullet hole, scratch or other kind of sur-\nface modiﬁ cation. Some bullet-hole decals are depicted in Figure 10.59.\nFor more information on creating and rendering decals, see [7] Section \n4.8, and [28] Section 9.2.\n10.4.3. Environmental Effects\nAny game that takes place in a somewhat natural or realistic environment \nrequires some kind of environmental rendering eﬀ ects. These eﬀ ects are usu-\nally implemented via specialized rendering systems. We’ll take a brief look at \na few of the more common of these systems in the following sections.\n10.4.3.1. Skies\nThe sky in a game world needs to contain vivid detail, yet technically speak-\ning it lies an extremely long distance away from the camera. Therefore we \ncannot model it as it really is and must turn instead to various specialized \nrendering techniques.\nOne simple approach is to ﬁ ll the frame buﬀ er with the sky texture prior \nto rendering any 3D geometry. The sky texture should be rendered at an ap-\nproximate 1:1 texel-to-pixel ratio, so that the texture is roughly or exactly the \nresolution of the screen. The sky texture can be rotated and scrolled to corre-\nspond to the motions of the camera in-game. During rendering of the sky, we \nmake sure to set the depth of all pixels in the frame buﬀ er to the maximum \npossible depth value. This ensures that the 3D scene elements will always sort \non top of the sky. The arcade hit Hydro Thunder rendered its skies in exactly \nthis manner.\nFor games in which the player can look in any direction, we can use a sky \ndome or sky box . The dome or box is rendered with its center always at the cam-\nera’s current location, so that it appears to lie at inﬁ nity, no matt er where the \ncamera moves in the game world. As with the sky texture approach, the sky \nbox or dome is rendered before any other 3D geometry, and all of the pixels \nin the frame buﬀ er are set to the maximum z-value when the sky is rendered. \nThis means that the dome or box can actually be tiny, relative to other objects \nin the scene. Its size is irrelevant, as long as it ﬁ lls the entire frame buﬀ er when \nit is drawn. For more information on sky rendering, see [1] Section 10.3 and \n[38] page 253.\nClouds are oft en implemented with a specialized rendering and anima-\ntion system as well. In early games like Doom and Quake, the clouds were just \nplanes with scrolling semi-transparent cloud textures on them. More-recent \ncloud techniques include camera-facing cards (billboards), particle-eﬀ ect \nbased clouds, and volumetric cloud eﬀ ects.\n\n\n485 \n10.4.3.2. Terrain\nThe goal of a terrain system is to model the surface of the earth and provide \na canvas of sorts upon which other static and dynamic elements can be laid \nout. Terrain is sometimes modeled explicitly in a package like Maya. But if the \nplayer can see far into the distance, we usually want some kind of dynamic \ntessellation or other level of detail (LOD) system. We may also need to limit \nthe amount of data required to represent very large outdoor areas.\nHeight ﬁ eld terrain is one popular choice for modeling large terrain areas. \nThe data size can be kept relatively small because a height ﬁ eld is typically \nstored in a grayscale texture map. In most height-ﬁ eld– based terrain systems, \nthe horizontal (y = 0) plane is tessellated in a regular grid patt ern, and the \nheights of the terrain vertices are determined by sampling the height ﬁ eld \ntexture. The number of triangles per unit area can be varied based on distance \nfrom the camera, thereby allowing large-scale features to be seen in the dis-\ntance, while still permitt ing a good deal of detail to be represented for nearby \nterrain. An example of a terrain deﬁ ned via a height ﬁ eld bitmap is shown in \nFigure 10.60.\nTerrain systems usually provide specialized tools for “painting” the height \nﬁ eld itself, carving out terrain features like roads, rivers, and so on. Texture \nmapping in a terrain system is oft en a blend between four or more textures. \nThis allows artists to “paint” in grass, dirt, gravel, and other terrain features \nby simply exposing one of the texture layers. The layers can be cross-blended \nfrom one to another to provide smooth textural transitions. Some terrain tools \nalso permit sections of the terrain to be cut out to permit buildings, trenches, \nand other specialized terrain features to be inserted in the form of regular \nmesh geometry. Terrain authoring tools are sometimes integrated directly into \nthe game world editor , while in other engines they may be stand-alone tools.\nFigure 10.60.  A grayscale height ﬁ eld bitmap (left) can be used to control the vertical posi-\ntions of the vertices in a terrain grid mesh (right). In this example, a water plane intersects the \nterrain mesh to create islands.\n10.4. Visual Effects and Overlays\n\n\n486 \n10. The Rendering Engine\nOf course, height ﬁ eld terrain is just one of many options for modeling the \nsurface of the Earth in a game. For more information on terrain rendering, see \n[6] Sections 4.16 through 4.19 and [7] Section 4.2.\n10.4.3.3. Water\nWater renderers are commonplace in games nowadays. There are lots of dif-\nferent possible kinds of water, including oceans, pools, rivers, waterfalls, foun-\ntains, jets, puddles, and damp solid surfaces. Each type of water generally \nrequires some specialized rendering technology. Some also require dynamic \nmotion simulations . Large bodies of water may require dynamic tessellation \nor other LOD methodologies similar to those employed in a terrain system.\nWater systems sometimes interact with a game’s rigid body dynamics \nsystem (ﬂ otation, force from water jets, etc.) and with gameplay (slippery sur-\nfaces, swimming mechanics, diving mechanics, riding vertical jets of water, \nand so on). Water eﬀ ects are oft en created by combining disparate render-\ning technologies and subsystems. For example, a waterfall might make use \nof specialized water shaders, scrolling textures, particle eﬀ ects for mist at the \nbase, a decal-like overlay for foam, and the list goes on. Today’s games oﬀ er \nsome prett y amazing water eﬀ ects, and active research into technologies like \nreal-time ﬂ uid dynamics promises to make water simulations even richer and \nmore realistic in the years ahead. For more information on water rendering \nand simulation techniques, see [1] Sections 9.3, 9.5, and 9.6, [13], and [6] Sec-\ntions 2.6 and 5.11.\n10.4.4. Overlays\nMost games have heads-up displays, in-game graphical user interfaces, and \nmenu systems. These overlays are typically comprised of two- and three-di-\nmensional graphics rendered directly in view space or screen space .\nOverlays are generally rendered aft er the primary scene, with z testing \ndisabled to ensure that they appear on top of the three-dimensional scene. \nTwo-dimensional overlays are typically implemented by rendering quads (tri-\nangle pairs) in screen space using an orthographic projection. Three-dimen-\nsional overlays may be rendered using an orthographic projection or via the \nregular perspective projection with the geometry positioned in view space so \nthat it follows the camera around.\n10.4.4.1. Normalized Screen Coordinates\nThe coordinates of two-dimensional overlays can be measured in terms of \nscreen pixels. However, if your game is going to be expected to support mul-\ntiple screen resolutions (which is very common in PC games), it’s a far bett er \n\n\n487 \nidea to use normalized screen coordinates . Normalized coordinates range from \nzero to one along one of the two axes (but not both—see below), and they can \neasily be scaled into pixel-based measurements corresponding to an arbitrary \nscreen resolution. This allows us to lay out our overlay elements without wor-\nrying about screen resolution at all (and only having to worry a litt le bit about \naspect ratio).\nIt’s easiest to deﬁ ne normalized coordinates so that they range from 0.0 \nto 1.0 along the y-axis. At a 4:3 aspect ratio, this means that the x-axis would \nrange from 0.0 to 1.333 (= 4 / 3), while at 16:9 the x-axis’ range would be from \n0.0 to 1.777 (= 16 / 9). It’s important not to deﬁ ne our coordinates so that they \nrange from zero to one along both axes. Doing this would cause square visual \nelements to have unequal x and y dimensions—or put another way, a visual \nelement with seemingly square dimensions would not look like a square on-\nscreen! Moreover, our “square” elements would stretch diﬀ erently at diﬀ erent \naspect ratios—deﬁ nitely not an acceptable state of aﬀ airs.\n10.4.4.2. Relative Screen Coordinates\nTo really make normalized coordinates work well, it should be possible to \nspecify coordinates in absolute or relative terms. For example, positive co-\nordinates might be interpreted as being relative to the top-left  corner of the \nscreen, while negative coordinates are relative to the bott om-right corner. That \nway, if I want a HUD element to be a certain distance from the right or bott om \nedges of the screen, I won’t have to change its normalized coordinates when \nthe aspect ratio changes. We might want to allow an even richer set of possible \nalignment choices, such as aligning to the center of the screen or aligning to \nanother visual element.\nThat said, you’ll probably have some overlay elements that simply cannot \nbe laid out using normalized coordinates in such a way that they look right at \nboth the 4:3 and 16:9 aspect ratios. You may want to consider having two distinct \nlayouts, one for each aspect ratio, so you can ﬁ ne-tune them independently.\n10.4.4.3. Text and Fonts\nA game engine’s text /font system is typically implemented as a special kind of \ntwo-dimensional (or sometimes three-dimensional) overlay. At its core, a text \nrendering system needs to be capable of displaying a sequence of character \nglyphs corresponding to a text string, arranged in various orientations on the \nscreen. A font is oft en implemented via a texture map containing the vari-\nous required glyphs. A font description ﬁ le provides information such as the \nbounding boxes of each glyph within the texture, and font layout information \nsuch as kerning, baseline oﬀ sets, and so on.\n10.4. Visual Effects and Overlays\n\n\n488 \n10. The Rendering Engine\nA good text/font system must account for the diﬀ erences in character sets \nand reading directions inherent in various languages. Some text systems also \nprovide various fun features like the ability to animate characters across the \nscreen in various ways, the ability to animate individual characters, and so \non. Some game engines even go so far as to implement a subset of the Adobe \nFlash standard in order to support a rich set of two-dimensional eﬀ ects in \ntheir overlays and text. However, it’s important to remember when imple-\nmenting a game font system that only those features that are actually required \nby the game should be implemented. There’s no point in furnishing your en-\ngine with an advanced text animation if your game never needs to display \nanimated text!\n10.4.5. Gamma Correction\nCRT monitors tend to have a nonlinear response to luminance values. That is, \nif a linearly-increasing ramp of R, G, or B values were to be sent to a CRT, the \nimage that would result on-screen would be perceptually nonlinear to the hu-\nman eye. Visually, the dark regions of the image would look darker than they \nshould. This is illustrated in Figure 10.61.\nThe gamma response curve of a typical CRT display can be modeled quite \nsimply by the formula\nout\nin,\nV\nV γ\n=\n \nwhere γCRT > 1. To correct for this eﬀ ect, the colors sent to the CRT display \nare usually passed through an inverse transformation (i.e., using a gamma \nvalue γcorr < 1). The value of γCRT for a typical CRT monitor is 2.2, so the correc-\nFigure 10.61.  The effect of a CRT’s gamma response on image quality and how the effect can \nbe corrected for. Image courtesy of www.wikipedia.org.\n\n\n489 \ntion value is usually γcorr = 1/2.2 = 0.455. These gamma encoding and decoding \ncurves are shown in Figure 10.62.\nGamma encoding can be performed by the 3D rendering engine to ensure \nthat the values in the ﬁ nal image are properly gamma-corrected. One problem \nthat is encountered, however, is that the bitmap images used to represent tex-\nture maps are oft en gamma-corrected themselves. A high-quality rendering \nengine takes this fact into account, by gamma-decoding the textures prior to \nrendering and then re-encoding the gamma of the ﬁ nal rendered scene so that \nits colors can be reproduced properly on-screen.\n10.4.6. Full-Screen Post Effects\nFull-screen post eﬀ ects are eﬀ ects applied to a rendered three-dimensional \nscene that provide additional realism or a stylized look. These eﬀ ects are of-\nten implemented by passing the entire contents of the screen through a pixel \nshader that applies the desired eﬀ ect(s). This can be accomplished by render-\ning a full-screen quad that has been mapped with a texture containing the \nunﬁ ltered scene. A few examples of full-screen post eﬀ ects are given below:\nz Motion blur . This is typically implemented by rendering a buﬀ er of \nscreen-space velocity vectors and using this vector ﬁ eld to selectively \nblur the rendered image. Blurring is accomplished by passing a con-\nvolution kernel over the image (see “Image Smoothing and Sharpening \nby Discrete Convolution” by Dale A. Schumacher, published in [4], for \ndetails).\nFigure 10.62.  Gamma encoding and decoding curves. Image courtesy of www.wikipedia.org.\n10.4. Visual Effects and Overlays\n\n\n490 \n10. The Rendering Engine\nz Depth of ﬁ eld blur . This blur eﬀ ect can be produced by using the contents \nof the depth buﬀ er to adjust the degree of blur applied at each pixel.\nz Vignett e . In this ﬁ lmic eﬀ ect, the brightness or saturation of the image \nis reduced at the corners of the screen for dramatic eﬀ ect. It is some-\ntimes implemented by literally rendering a texture overlay on top of the \nscreen. A variation on this eﬀ ect is used to produce the classic circular \neﬀ ect used to indicate that the player is looking through a pair of bin-\noculars or a weapon scope.\nz Colorization . The colors of screen pixels can be altered in arbitrary ways \nas a post-processing eﬀ ect. For example, all colors except red could be \ndesaturated to grey to produce a striking eﬀ ect similar to the famous \nscene of the litt le girl in the red coat from Schindler’s List.\n10.5. Further Reading\nWe’ve covered a lot of material in a very short space in this chapter, but we’ve \nonly just scratched the surface. No doubt you’ll want to explore many of these \ntopics in much greater detail. For an excellent overview of the entire process of \ncreating three-dimensional computer graphics and animation for games and \nﬁ lm, I highly recommend [23]. The technology that underlies modern real-\ntime rendering is covered in excellent depth in [1], while [14] is well known as \nthe deﬁ nitive reference guide to all things related to computer graphics. Other \ngreat books on 3D rendering include [42], [9], and [10]. The mathematics of \n3D rendering is covered very well in [28]. No graphics programmer’s library \nwould be complete without one or more books from the Graphics Gems series \n([18], [4], [24], [19], and [36]) and/or the GPU Gems series ([13], [38], and [35]). \nOf course, this short reference list is only the beginning—you will undoubt-\nedly encounter a great many more excellent books on rendering and shaders \nover the course of your career as a game programmer.\n\n\n491\n11\nAnimation Systems\nT\nhe majority of modern 3D games revolve around characters —oft en human \nor humanoid, sometimes animal or alien. Characters are unique because \nthey need to move in a ﬂ uid, organic way. This poses a host of new technical \nchallenges, over and above what is required to simulate and animate rigid \nobjects like vehicles, projectiles, soccer balls, and Tetris pieces. The task of im-\nbuing characters with natural-looking motion is handled by an engine compo-\nnent known as the character animation system.\nAs we’ll see, an animation system gives game designers a powerful suite \nof tools that can be applied to non-characters as well as characters. Any game \nobject that is not 100% rigid can take advantage of the animation system. So \nwhenever you see a vehicle with moving parts, a piece of articulated machin-\nery, trees waving gently in the breeze, or even an exploding building in a \ngame, chances are good that the object makes at least partial use of the game \nengine’s animation system.\n11.1. Types of Character Animation\nCharacter animation technology has come a long way since Donkey Kong. At \nﬁ rst, games employed very simple techniques to provide the illusion of life-\nlike movement. As game hardware improved, more-advanced techniques be-\n\n\n492 \n11. Animation Systems\ncame feasible in real time. Today, game designers have a host of powerful \nanimation methods at their disposal. In this section, we’ll take a brief look \nat the evolution of character animation and outline the three most-common \ntechniques used in modern game engines.\n11.1.1. \nCel Animation\nThe precursor to all game animation techniques is known as traditional anima-\ntion, or hand-drawn animation . This is the technique used in the earliest animat-\ned cartoons. The illusion of motion is produced by displaying a sequence of \nstill pictures known as frames in rapid succession. Real-time 3D rendering can \nbe thought of as an electronic form of traditional animation, in that a sequence \nof still full-screen images is presented to the viewer over and over to produce \nthe illusion of motion.\nCel animation is a speciﬁ c type of traditional animation. A cel is a transpar-\nent sheet of plastic on which images can be painted or drawn. An animated \nsequence of cels can be placed on top of a ﬁ xed background painting or draw-\ning to produce the illusion of motion without having to redraw the static back-\nground over and over.\nThe electronic equivalent to cel animation is a technology known as sprite \nanimation. A sprite is a small bitmap that can be overlaid on top of a full-screen \nbackground image without disrupting it, oft en drawn with the aid of special-\nized graphics hardware. Hence, a sprite is to 2D game animation what a cel \nwas to traditional animation. This technique was a staple during the 2D game \nera. Figure 11.1 shows the famous sequence of sprite bitmaps that were used \nto produce the illusion of a running humanoid character in almost every Mat-\ntel Intellivision game ever made. The sequence of frames was designed so that \nit animates smoothly even when it is repeated indeﬁ nitely—this is known as \na looping animation . This particular animation would be called a run cycle in \nmodern parlance, because it makes the character appear to be running. Char-\nacters typically have a number of looping animation cycles, including various \nidle cycles, a walk cycle, and a run cycle.\nFigure 11.1.  The sequence of sprite bitmaps used in most Intellivision games.\n11.1.2. Rigid Hierarchical Animation\nWith the advent of 3D graphics, sprite techniques began to lose their appeal. \nDoom made use of a sprite-like animation system: Its monsters were nothing \n\n\n493 \n11.1. Types of Character Animation\nmore than camera-facing quads, each of which displayed a sequence of texture \nbitmaps (known as an animated texture ) to produce the illusion of motion. And \nthis technique is still used today for low-resolution and/or distant objects—for \nexample crowds in a stadium, or hordes of soldiers ﬁ ghting a distant batt le \nin the background. But for high-quality foreground characters, 3D graphics \nbrought with it the need for improved character animation methods.\nThe earliest approach to 3D character animation is a technique known as \nrigid hierarchical animation. In this approach, a character is modeled as a col-\nlection of rigid pieces. A typical break-down for a humanoid character might \nbe pelvis, torso, upper arms, lower arms, upper legs, lower legs, hands, feet, \nand head. The rigid pieces are constrained to one another in a hierarchical \nfashion, analogous to the manner in which a mammal’s bones are connected \nat the joints. This allows the character to move naturally. For example, when \nthe upper arm is moved, the lower arm and hand will automatically follow it. \nA typical hierarchy has the pelvis at the root, with the torso and upper legs as \nits immediate children, and so on as shown below:\nPelvis\n Torso\n  UpperRightArm\n   LowerRightArm\n    RightHand\n  UpperLeftArm\n   UpperLeftArm\n    LeftHand\n  Head\n UpperRightLeg\n  LowerRightLeg\n   RightFoot\n UpperLeftLeg\n  UpperLeftLeg\n   LeftFoot\nThe big problem with the rigid hierarchy technique is that the behavior of \nthe character’s body is oft en not very pleasing due to “cracking” at the joints. \nThis is illustrated in Figure 11.2. Rigid hierarchical animation works well for \nFigure 11.2.  Cracking at the joints is a big problem in rigid hierarchical animation.\n\n\n494 \n11. Animation Systems\nrobots and machinery that really is constructed of rigid parts, but it breaks \ndown under scrutiny when applied to “ﬂ eshy” characters.\n11.1.3. Per-Vertex Animation and Morph Targets\nRigid hierarchical animation tends to look unnatural because it is rigid. What \nwe really want is a way to move individual vertices so that triangles can stretch \nto produce more-natural looking motion.\nOne way to achieve this is to apply a brute-force technique known as \nper-vertex animation. In this approach, the vertices of the mesh are animated \nby an artist, and motion data is exported which tells the game engine how to \nmove each vertex at runtime. This technique can produce any mesh deforma-\ntion imaginable (limited only by the tessellation of the surface). However, it \nis a data-intensive technique, since time-varying motion information must be \nstored for each vertex of the mesh. For this reason, it has litt le application to \nreal-time games.\nA variation on this technique known as morph target animation is used in \nsome real-time games. In this approach, the vertices of a mesh are moved by \nan animator to create a relatively small set of ﬁ xed, extreme poses. Animations \nare produced by blending between two or more of these ﬁ xed poses at runtime. \nThe position of each vertex is calculated using a simple linear interpolation \n(LERP) between the vertex’s positions in each of the extreme poses.\nThe morph target technique is oft en used for facial animation, because \nthe human face is an extremely complex piece of anatomy, driven by roughly \n50 muscles. Morph target animation gives an animator full control over every \nvertex of a facial mesh, allowing him or her to produce both subtle and ex-\ntreme movements that approximate the musculature of the face well. Figure \n11.3 shows a set of facial morph targets.\nFigure 11.3.  A set of facial morph targets for NVIDIA’s Dawn character.\n\n\n495 \n11.1.4. Skinned Animation\nAs the capabilities of game hardware improved further, an animation tech-\nnology known as skinned animation was developed. This technique has many \nof the beneﬁ ts of per-vertex and morph target animation—permitt ing the tri-\nangles of an animated mesh to deform. But it also enjoys the much more-\neﬃ  cient performance and memory usage characteristics of rigid hierarchical \nanimation. It is capable of producing reasonably realistic approximations to \nthe movement of skin and clothing.\nSkinned animation was ﬁ rst used by games like Super Mario 64, and it \nis still the most prevalent technique in use today, both by the game industry \nand the feature ﬁ lm industry. A host of famous modern game and movie char-\nacters, including the dinosaurs from Jurrassic Park, Solid Snake (Metal Gear \nSolid 4), Gollum (Lord of the Rings), Nathan Drake (Uncharted: Drake’s Fortune), \nBuzz Lightyear (Toy Story), and Marcus Fenix (Gears of War) were all animated, \nin whole or in part, using skinned animation techniques. The remainder of \nthis chapter will be devoted primarily to the study of skinned/skeletal anima-\ntion.\nIn skinned animation, a skeleton is constructed from rigid “bones ,” just as \nin rigid hierarchical animation. However, instead of rendering the rigid pieces \non-screen, they remain hidden. A smooth continuous triangle mesh called a \nskin is bound to the joints of the skeleton; its vertices track the movements of \nthe joints. Each vertex of the skin mesh can be weighted to multiple joints, so \nthe skin can stretch in a natural way as the joints move.\nFigure 11.4.  Eric Browning’s Crank the Weasel character, with internal skeletal structure.\n11.1. Types of Character Animation\n\n\n496 \n11. Animation Systems\nIn Figure 11.4, we see Crank the Weasel, a game character designed by \nEric Browning for Midway Home Entertainment in 2001. Crank’s outer skin \nis composed of a mesh of triangles, just like any other 3D model. However, \ninside him we can see the rigid bones and joints that make his skin move.\n11.1.5. Animation Methods as Data Compression Techniques\nThe most ﬂ exible animation system conceivable would give the animator con-\ntrol over literally every inﬁ nitesimal point on an object’s surface. Of course, \nanimating like this would result in an animation that contains a potentially \ninﬁ nite amount of data! Animating the vertices of a triangle mesh is a simpli-\nﬁ cation of this ideal—in eﬀ ect, we are compressing the amount of information \nneeded to describe an animation by restricting ourselves to moving only the \nvertices. (Animating a set of control points is the analog of vertex animation \nfor models constructed out of higher-order patches.) Morph targets can be \nthought of as an additional level of compression, achieved by imposing addi-\ntional constraints on the system—vertices are constrained to move only along \nlinear paths between a ﬁ xed number of predeﬁ ned vertex positions. Skeletal \nanimation is just another way to compress vertex animation data by imposing \nconstraints. In this case, the motions of a relatively large number of vertices \nare constrained to follow the motions of a relatively small number of skeletal \njoints.\nWhen considering the trade-oﬀ s between various animation techniques, \nit can be helpful to think of them as compression methods, analogous in many \nrespects to video compression techniques. We should generally aim to select \nthe animation method that provides the best compression without producing \nunacceptable visual artifacts. Skeletal animation provides the best compres-\nsion when the motion of a single joint is magniﬁ ed into the motions of many \nvertices. A character’s limbs act like rigid bodies for the most part, so they can \nbe moved very eﬃ  ciently with a skeleton. However, the motion of a face tends \nto be much more complex, with the motions of individual vertices being more \nindependent. To convincingly animate a face using the skeletal approach, the \nrequired number of joints approaches the number of vertices in the mesh, thus \ndiminishing its eﬀ ectiveness as a compression technique. This is one reason \nwhy morph target techniques are oft en favored over the skeletal approach for \nfacial animation. (Another common reason is that morph targets tend to be a \nmore natural way for animators to work.)\n11.2. Skeletons\nA skeleton is comprised of a hierarchy of rigid pieces known as joints . In the \ngame industry, we oft en use the terms “joint” and “bone” interchangeably, \n\n\n497 \nbut the term bone is actually a misnomer. Technically speaking, the joints are \nthe objects that are directly manipulated by the animator, while the bones \nare simply the empty spaces between the joints. As an example, consider the \npelvis joint in the Crank the Weasel character model. It is a single joint, but be-\ncause it connects to four other joints (the tail, the spine, and the left  and right \nhip joints), this one joint appears to have four bones sticking out of it. This is \nshown in more detail in Figure 11.5. Game engines don’t care a whip about \nbones—only the joints matt er. So whenever you hear the term “bone” being \nused in the industry, remember that 99% of the time we are actually speaking \nabout joints.\n11.2.1. The Skeleal Hierarchy\nAs we’ve mentioned, the joints in a skeleton form a hierarchy or tree structure. \nOne joint is selected as the root, and all other joints are its children, grandchil-\ndren, and so on. A typical joint hierarchy for skinned animation looks almost \nidentical to a typical rigid hierarchy. For example, a humanoid character’s \njoint hierarchy might look something like this:\nPelvis\n LowerSpine\n  MiddleSpine\n   UpperSpine\n    RightShoulder\n     RightElbow\n      RightHand\n       RightThumb\nFigure 11.5.  The pelvis joint of this character connects to four other joints (tail, spine, and two \nlegs), and so it produces four bones.\n11.2. Skeletons\n\n\n498 \n11. Animation Systems\n       RightIndexFinger\n       RightMiddleFinger\n       RightRingFinger\n       RightPinkyFinger\n    LeftShoulder\n     LeftElbow\n      LeftHand\n       LeftThumb\n       LeftIndexFinger\n       LeftMiddleFinger\n       LeftRingFinger\n       LeftPinkyFinger\n    Neck\n     Head\n      LeftEye\n      RightEye\nvarious face joints\n RightThigh\n  RightKnee\n   RightAnkle\n LeftThigh\n  LeftKnee\n   LeftAnkle\nWe usually assign each joint an index from 0 to N – 1. Because each joint \nhas one and only one parent, the hierarchical structure of a skeleton can be \nfully described by storing the index of its parent with each joint. The root \njoint has no parent, so its parent index usually contains an invalid index such \nas –1.\n11.2.2. Representing a Skeleton in Memory\nA skeleton is usually represented by a small top-level data structure that \ncontains an array of data structures for the individual joints. The joints are \nusually listed in an order that ensures a child joint will always appear aft er \nits parent in the array. This implies that joint zero is always the root of the \nskeleton.\nJoint indices  are usually used to refer to joints within animation data struc-\ntures. For example, a child joint typically refers to its parent joint by specifying \nits index. Likewise, in a skinned triangle mesh, a vertex refers to the joint or \njoints to which it is bound by index. This is much more eﬃ  cient than referring \nto joints by name, both in terms of the amount of storage required (a joint in-\ndex can usually be 8 bits wide) and in terms of the amount of time it takes to \nlook up a referenced joint (we can use the joint index to jump immediately to \na desired joint in the array).\n\n\n499 \nEach joint data structure typically contains the following information:\nThe \n• \nname of the joint, either as a string or a hashed 32-bit string id.\nThe \n• \nindex of the joint’s parent within the skeleton.\nThe \n• \ninverse bind pose transform of the joint. The bind pose of a joint is the \nposition, orientation, and scale of that joint at the time it was bound to \nthe vertices of the skin mesh. We usually store the inverse of this trans-\nformation for reasons we’ll explore in more depth below.\nA typical skeleton data structure might look something like this:\nstruct Joint\n{\n \nMatrix4x3   m_invBindPose; // inverse bind pose   \n \n  // \ntransform\n \nconst char* m_name;        // human-readable joint  \n \n  // \nname\n \nU8          m_iParent;      // parent index or 0xFF   \n  // \nif root\n};\nstruct Skeleton\n{\n U32 \n \n \n  m_jointCount;  // number of joints\nJoint*      m_aJoint;      // array of joints\n};\n11.3. Poses\n No matt er what technique is used to produce an animation, be it cel-based, \nrigid hierarchical, or skinned/skeletal, every animation takes place over time. \nA character is imbued with the illusion of motion by arranging the character’s \nbody into a sequence of discrete, still poses and then displaying those poses \nin rapid succession, usually at a rate of 30 or 60 poses per second. (Actually, as \nwe’ll see in Section 11.4.1.1, we oft en interpolate between adjacent poses rather \nthan displaying a single pose verbatim.) In skeletal animation, the pose of the \nskeleton directly controls the vertices of the mesh, and posing is the anima-\ntor’s primary tool for breathing life into her characters. So clearly, before we \ncan animate a skeleton, we must ﬁ rst understand how to pose it.\nA skeleton is posed by rotating, translating, and possibly scaling its joints \nin arbitrary ways. The pose of a joint is deﬁ ned as the joint’s position, orien-\ntation, and scale, relative to some frame of reference. A joint pose is usually \n11.3. Poses\n\n\n500 \n11. Animation Systems\nrepresented by a 4 × 4 or 4 × 3 matrix, or by an SQT data structure (scale, \nquaternion rotation and vector translation). The pose of a skeleton is just the \nset of all of its joints’ poses and is normally represented as a simple array of \nmatrices or SQTs.\n11.3.1. Bind Pose\nTwo diﬀ erent poses of the same skeleton are shown in Figure 11.6. The pose \non the left  is a special pose known as the bind pose , also sometimes called the \nreference pose or the rest pose. This is the pose of the 3D mesh prior to being \nbound to the skeleton (hence the name). In other words, it is the pose that the \nmesh would assume if it were rendered as a regular, unskinned triangle mesh, \nwithout any skeleton at all. The bind pose is also called the T-pose because the \ncharacter is usually standing with his feet slightly apart and his arms out-\nstretched in the shape of the lett er T. This particular stance is chosen because \nit keeps the limbs away from the body and each other, making the process of \nbinding the vertices to the joints easier.\nFigure 11.6.  Two different poses of the same skeleton. The pose on the left is the special pose \nknown as bind pose.\n11.3.2. Local Poses\nA joint’s pose is most oft en speciﬁ ed relative to its parent joint. A parent-rela-\ntive pose allows a joint to move naturally. For example, if we rotate the shoul-\nder joint, but leave the parent-relative poses of the elbow, wrist and ﬁ ngers \n\n\n501 \nunchanged, the entire arm will rotate about the shoulder in a rigid manner, as \nwe’d expect. We sometimes use the term local pose to describe a parent-relative \npose. Local poses are almost always stored in SQT format, for reasons we’ll \nexplore when we discuss animation blending.\nGraphically, many 3D authoring packages like Maya represent joints as \nsmall spheres. However, a joint has a rotation and a scale, not just a trans-\nlation, so this visualization can be a bit misleading. In fact, a joint actually \ndeﬁ nes a coordinate space, no diﬀ erent in principle from the other spaces \nwe’ve encountered (like model space, world space, or view space). So it is best \nto picture a joint as a set of Cartesian coordinate axes. Maya gives the user \nthe option of displaying a joint’s local coordinate axes —this is shown in Fig-\nure 11.7.\nMathematically, a joint pose is nothing more than an aﬃ  ne transformation. \nThe pose of joint j can be writt en as the 4 × 4 aﬃ  ne transformation matrix Pj , \nwhich is comprised of a translation vector Tj , a 3 × 3 diagonal scale matrix Sj \nand a 3 × 3 rotation matrix Rj. The pose of an entire skeleton Pskel can be writt en \nas the set of all poses Pj , where j ranges from 0 to N – 1 :\n \n{ }\n \n1\nskel\n0\n,\n1\n \n \n.\nj\nj\nj\nj\nN\nj\nj\n−\n=\n⎡\n⎤\n=⎢\n⎥\n⎣\n⎦\n=\nS R\n0\nP\nT\nP\nP\n \nFigure 11.7.  Every joint in a skeletal hierarchy deﬁ nes a set of local coordinate space axes, \nknown as joint space.\n11.3. Poses\n\n\n502 \n11. Animation Systems\n11.3.2.1. Joint Scale\n Some game engines assume that joints will never be scaled, in which case Sj\nis simply omitt ed and assumed to be the identity matrix. Other engines make \nthe assumption that scale will be uniform if present, meaning it is the same in \nall three dimensions. In this case, scale can be represented using a single scalar \nvalue sj. Some engines even permit nonuniform scale, in which case scale can \nbe compactly represented by the three-element vector sj = [ sjx  sjy  sjz ]. The ele-\nments of the vector sj correspond to the three diagonal elements of the 3 × 3 \nscaling matrix Sj , so it is not really a vector per se. Game engines almost never \npermit shear, so Sj is almost never represented by a full 3 × 3 scale/shear ma-\ntrix, although it certainly could be.\nThere are a number of beneﬁ ts to omitt ing or constraining scale in a pose \nor animation. Clearly using a lower-dimensional scale representation can save \nmemory. (Uniform scale requires a single ﬂ oating-point scalar per joint per \nanimation frame, while nonuniform scale requires three ﬂ oats, and a full 3 × 3 \nscale-shear matrix requires nine.) Restricting our engine to uniform scale has \nthe added beneﬁ t of ensuring that the bounding sphere of a joint will never \nbe transformed into an ellipsoid, as it could be when scaled in a nonuniform \nmanner. This greatly simpliﬁ es the mathematics of frustum and collision tests \nin engines that perform such tests on a per-joint basis.\n11.3.2.2. Representing a Joint Pose in Memory\n As we mentioned above, joint poses are usually stored in SQT format. In C++ \nsuch a data structure might look like this, where Q is ﬁ rst to ensure proper \nalignment and optimal structure packing. (Can you see why?)\nstruct JointPose\n{\n Quaternion m_rot; \n // \nQ\n Vector3 \n m_trans; \n// T\n F32 \n \n \n \nm_scale; // S (uniform scale only)\n};\nIf nonuniform scale is permitt ed, we might deﬁ ne a joint pose like this \ninstead:\nstruct JointPose\n{\n Quaternion m_rot; \n // \nQ\n Vector3 \n m_trans; \n// T\n Vector3 \n m_scale; \n// S\n U8 \n   padding[8];\n};\n\n\n503 \nThe local pose of an entire skeleton can be represented as follows, where \nit is understood that the array m_aLocalPose is dynamically allocated to con-\ntain just enough occurrences of JointPose to match the number of joints in \nthe skeleton.\nstruct SkeletonPose\n{\nSkeleton*  \nm_pSkeleton;  // skeleton + num joints\nJointPose* m_aLocalPose; \n // local joint poses\n};\n11.3.2.3. The Joint Pose as a Change of Basis\n It’s important to remember that a local joint pose is speciﬁ ed relative to the \njoint’s immediate parent. Any aﬃ  ne transformation can be thought of as trans-\nforming points and vectors from one coordinate space to another. So when \nthe joint pose transform Pj is applied to a point or vector that is expressed in \nthe coordinate system of the joint j, the result is that same point or vector ex-\npressed in the space of the parent joint.\nAs we’ve done in earlier chapters, we’ll adopt the convention of using \nsubscripts to denote the direction of a transformation. Since a joint pose takes \npoints and vectors from the child joint’s space (C) to that of its parent joint (P), \nwe can write it \nC\nP\n(\n)j\n→\nP\n. Alternatively, we can introduce the function p(j) which \nreturns the parent index of joint j, and write the local pose of joint j as \np(  )\nj\nj\n→\nP\n.\nOn occasion we will need to transform points and vectors in the opposite \ndirection—from parent space into the space of the child joint. This transformation \nis just the inverse of the local joint pose. Mathematically, \n(\n)\n1\np(  )\np(  )\nj\nj\nj\nj\n−\n→\n→\n=\nP\nP\n.\n11.3.3. Global Poses\nSometimes it is convenient to express a joint’s pose in model space or world \nspace. This is called a global pose . Some engines express global poses in matrix \nform, while others use the SQT format.\nMathematically, the model-space pose of a joint (j→M) can be found by \nwalking the skeletal hierarchy from the joint in question all the way to the \nroot, multiplying the local poses (j→p(j)) as we go. Consider the hierarchy \nshown in Figure 11.8. The parent space of the root joint is deﬁ ned to be model \nspace, so p(0)\nM\n≡\n. The model-space pose of joint J2 can therefore be writt en \nas follows:\n \n \n \n2\nM\n2\n1\n1\n0\n0\nM.\n→\n→\n→\n→\n=\nP\nP\nP\nP\n \nLikewise, the model-space pose of joint J5 is just \n \n \n \n5\nM\n5\n4\n4\n3\n3\n0\n0\nM.\n→\n→\n→\n→\n→\n=\nP\nP\nP\nP\nP\n11.3. Poses\n",
      "page_number": 503,
      "chapter_number": 26,
      "summary": "This chapter covers segment 26 (pages 503-525). Key topics include animation, animated, and animal. Visual Effects and Overlays\nThe rendering pipeline we’ve discussed to this point is responsible primarily \nfor rendering three-dimensional solid objects.",
      "keywords": [
        "joint",
        "animation",
        "joint pose",
        "pose",
        "rendering",
        "Animation Systems",
        "character animation",
        "game",
        "Rigid Hierarchical Animation",
        "system",
        "Rendering Engine",
        "character",
        "local joint pose",
        "screen",
        "character animation system"
      ],
      "concepts": [
        "animation",
        "animated",
        "animal",
        "animates",
        "animator",
        "animations",
        "joints",
        "rendering",
        "render",
        "games"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 7",
          "chapter": 46,
          "title": "Segment 46 (pages 443-452)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 48,
          "title": "Segment 48 (pages 469-476)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "makinggames",
          "chapter": 6,
          "title": "Segment 6 (pages 43-50)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 43,
          "title": "Segment 43 (pages 424-431)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 59,
          "title": "Segment 59 (pages 570-580)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 526-548)",
      "start_page": 526,
      "end_page": 548,
      "detection_method": "synthetic",
      "content": "504 \n11. Animation Systems\nIn general, the global pose (joint-to-model transform) of any joint j can be \nwritt en as follows:\n0\nM\np(  ),\nj\ni\ni\ni\n→\n→\n=\n=∏\nP\nP\nj\n \n(11.1) \nwhere it is implied that i becomes p(i) (the parent of joint i) aft er each iteration \nin the product, and p(0)\nM\n≡\n.\n11.3.3.1. Representing a Global Pose in Memory\n We can extend our SkeletonPose data structure to include the global pose \nas follows, where again we dynamically allocate the m_aGlobalPose array \nbased on the number of joints in the skeleton:\nstruct SkeletonPose\n{\nSkeleton*  \nm_pSkeleton;     // skeleton + num joints\nJointPose* m_aLocalPose; \n    // local joint poses\nMatrix44*  \nm_aGlobalPose; // global joint poses\n};\n11.4. Clips\nIn a ﬁ lm , every aspect of each scene is carefully planned out before any anima-\ntions are created. This includes the movements of every character and prop in \nthe scene, and even the movements of the camera. This means that an entire \nscene can be animated as one long, contiguous sequence of frames. And char-\nacters need not be animated at all whenever they are oﬀ -camera.\n0\n1\n2\n3\n4\n5\nxM\nyM\nFigure 11.8.  A global pose can be calculated by walking the hierarchy from the joint in \nquestion towards the root and model space origin, concatenating the child-to-parent (local) \ntransforms of each joint as we go.\n\n\n505 \n11.4. Clips\nGame animation is diﬀ erent. A game is an interactive experience, so one \ncannot predict beforehand how the characters are going to move and behave. \nThe player has full control over his or her character and usually has partial \ncontrol over the camera as well. Even the decisions of the computer-driven \nnon-player characters are strongly inﬂ uenced by the unpredictable actions of \nthe human player. As such, game animations are almost never created as long, \ncontiguous sequences of frames. Instead, a game character’s movement must \nbe broken down into a large number of ﬁ ne-grained motions. We call these \nindividual motions animation clips , or sometimes just animations.\nEach clip causes the character to perform a single well-deﬁ ned action. \nSome clips are designed to be looped —for example, a walk cycle or run cycle . \nOthers are designed to be played once—for example, throwing an object, or \ntripping and falling to the ground. Some clips aﬀ ect the entire body of the \ncharacter—the character jumping into the air for instance. Other clips aﬀ ect \nonly a part of the body—perhaps the character waving his right arm. The \nmovements of any one game character are typically broken down into literally \nthousands of clips.\nThe only exception to this rule is when game characters are involved in \na noninteractive portion of the game, known as an in-game cinematic (IGC), \nnoninteractive sequence (NIS), or full-motion video (FMV). Noninteractive se-\nquences are typically used to communicate story elements that do not lend \nthemselves well to interactive gameplay, and they are created in much the \nsame way computer-generated ﬁ lms are made (although they oft en make use \nof in-game assets like character meshes, skeletons, and textures). The terms \nIGC and NIS typically refer to noninteractive sequences that are rendered in \nreal time by the game engine itself. The term FMV applies to sequences that \nhave been prerendered to an MP4, WMV, or other type of movie ﬁ le and are \nplayed back at runtime by the engine’s full-screen movie player.\nA variation on this style of animation is a semi-interactive sequence \nknown as a quick time event (QTE). In a QTE, the player must hit a butt on at \nthe right moment during an otherwise noninteractive sequence in order to see \nthe success animation and proceed; otherwise a failure animation is played, \nand the player must try again, possibly losing a life or suﬀ ering some other \nconsequence as a result.\n11.4.1. The Local Time Line\nWe can think of every animation clip as having a local time line , usually de-\nnoted by the independent variable t. At the start of a clip t = 0 and at the end \nt = T, where T is the duration of the clip. Each unique value of the variable t is \ncalled a time index . An example of this is shown in Figure 11.9.\n\n\n506 \n11. Animation Systems\n11.4.1.1. \nPose Interpolation and Continuous Time\nIt’s important to realize that the rate at which frames are displayed to the \nviewer is not necessarily the same as the rate at which poses are created by the \nanimator. In both ﬁ lm and game animation, the animator almost never poses \nthe character every 1/30 or 1/60 of a second. Instead, the animator generates \nimportant poses known as key poses or key frames at speciﬁ c times within the \nclip, and the computer calculates the poses in between via linear or curve-\nbased interpolation. This is illustrated in Figure 11.10.\nBecause of the animation engine’s ability to interpolate poses (which we’ll \nexplore in depth later in this chapter), we can actually sample the pose of the \ncharacter at any time during the clip—not just on integer frame indices. In \nother words, an animation clip’s time line is continuous. In computer anima-\ntion, the time variable t is a real (ﬂ oating-point) number, not an integer.\nFilm animation doesn’t take full advantage of the continuous nature of \nthe animation time-line, because its frame rate is locked at exactly 24, 30, or \n60 frames per second. In ﬁ lm, the viewer sees the characters’ poses at frames \nAnimation A: Local Time\nt = 0\nt = (0.4)T\nt = T\nt = (0.8)T\nFigure 11.9.  The local time line of an animation showing poses at selected time indices.\ninterpolated\nposes\nkey pose 2\nkey pose 1\nFigure 11.10.  An animator creates a relatively small number of key poses, and the engine ﬁ lls \nin the rest of the poses via interpolation.\n\n\n507 \n1, 2, 3, and so on—there’s never any need to ﬁ nd a character’s pose on frame \n3.7, for example. So in ﬁ lm animation, the animator doesn’t pay much (if any) \natt ention to how the character looks in between the integral frame indices.\nIn contrast, a real-time game’s frame rate always varies a litt le, depending \non how much load is currently being placed on the CPU and GPU. Also, game \nanimations are sometimes time-scaled in order to make the character appear to \nmove faster or slower than originally animated. So in a real-time game, an ani-\nmation clip is almost never sampled on integer frame numbers. In theory, with \na time scale of 1.0, a clip should be sampled at frames 1, 2, 3, and so on. But \nin practice, the player might actually see frames 1.1, 1.9, 3.2, and so on. And if \nthe time scale is 0.5, then the player might actually see frames 1.1, 1.4, 1.9, 2.6, \n3.2, and so on. A negative time scale can even be used to play an animation in \nreverse. So in game animation, time is both continuous and scalable.\n11.4.1.2. Time Units\n Because an animation’s time line is continuous, time is best measured in units \nof seconds. Time can also be measured in units of frames , presuming we deﬁ ne \nthe duration of a frame beforehand. Typical frame durations are 1/30 or 1/60 \nof a second for game animation. However, it’s important not to make the mis-\ntake of deﬁ ning your time variable t as an integer that counts whole frames. \nNo matt er which time units are selected, t should be a real (ﬂ oating-point) \nquantity, a ﬁ xed-point number, or an integer that measures subframe time \nintervals. The goal is to have suﬃ  cient resolution in your time measurements \nfor doing things like “tweening” between frames or scaling an animation’s \nplay-back speed.\n11.4.1.3. Frame versus Sample\nUnfortunately, the term frame has more than one common meaning in the \ngame industry. This can lead to a great deal of confusion. Sometimes a frame \nis taken to be a period of time that is 1/30 or 1/60 of a second in duration. But in \nother contexts, the term frame is applied to a single point in time (e.g., we might \nspeak of the pose of the character “at frame 42”).\nI personally prefer to use the term sample to refer to a single point in time, \nand I reserve the word frame to describe a time period that is 1/30 or 1/60 of a \nsecond in duration. So for example, a one-second animation created at a rate \nof 30 frames per second would consist of 31 samples and would be 30 frames in \nduration, as shown in Figure 11.11. The term “sample” comes from the ﬁ eld \nof signal processing. A continuous-time signal (i.e., a function f(t)) can be con-\nverted into a set of discrete data points by sampling that signal at uniformly-\nspaced time intervals. See htt p://en.wikipedia.org/wiki/Sampling_%28signal_\nprocessing%29 for more information on sampling.\n11.4. Clips\n\n\n508 \n11. Animation Systems\n11.4.1.4. Frames, Samples and Looping Clips\nWhen a clip is designed to be played over and over repeatedly, we say it is a \nlooped animation . If we imagine two copies of a 1-second (30-frame/31-sample) \nclip laid back-to-front, then sample 31 of the ﬁ rst clip will coincide exactly in \ntime with sample 1 of the second clip, as shown in Figure 11.12. For a clip to \nloop properly, then, we can see that the pose of the character at the end of the \nclip must exactly match the pose at the beginning. This, in turn, implies that \nthe last sample of a looping clip (in our example, sample 31) is redundant. \nMany game engines therefore omit the last sample of a looping clip.\nThis leads us to the following rules governing the number of samples and \nframes in any animation clip:\nIf a clip is \n• \nnon-looping, an N-frame animation will have N + 1 unique \nsamples.\nIf a clip is \n• \nlooping, then the last sample is redundant, so an N-frame ani-\nmation will have N unique samples.\n26\n27\n28\n29\n30\n1\n2\n3\n4\n5\n...\n31\nSamples:\nFrames:\n30\n29\n28\n27\n26\n6\n5\n4\n3\n2\n1\nFigure 11.11.  A one-second animation sampled at 30 frames per second is 30 frames in duration \nand consists of 31 samples.\n30\n29\n28\n27\n26\n6\n5\n4\n3\n2\n30\n29\n28\n27\n26\n5\n4\n3\n2\n1\n31\n1\n...\n...\n...\n...\n...\n...\nFigure 11.12.  The last sample of a looping clip coincides in time with its ﬁ rst sample and is, \ntherefore, redundant.\n11.4.1.5. Normalized Time (Phase)\nIt is sometimes convenient to employ a normalized time unit u, such that u = 0 \nat the start of the animation, and u = 1 at the end, no matt er what its duration \nT may be. We sometimes refer to normalized time as the phase of the animation \nclip, because u acts like the phase of a sine wave when the animation is looped. \nThis is illustrated in Figure 11.13.\n\n\n509 \nNormalized time is useful when synchronizing two or more animation \nclips that are not necessarily of the same absolute duration. For example, we \nmight want to smoothly cross-fade from a 2-second (60-frame) run cycle into \na 3-second (90-frame) walk cycle. To make the cross-fade look good, we want \nto ensure that the two animations remain synchronized at all times, so that the \nfeet line up properly in both clips. We can accomplish this by simply sett ing \nthe normalized start time of the walk clip, uwalk to match the normalized time \nindex of the run clip, urun. We then advance both clips at the same normalized \nrate, so that they remain in sync. This is quite a bit easier and less error-prone \nthan doing the synchronization using the absolute time indices twalk and trun.\n11.4.2. The Global Time Line\nJust as every animation clip has a local time line (whose clock starts at 0 at \nthe beginning of the clip), every character in a game has a global time line \n(whose clock starts when the character is ﬁ rst spawned into the game world, \nor perhaps at the start of the level or the entire game). In this book, we’ll use \nthe time variable τ to measure global time, so as not to confuse it with the local \ntime variable t.\nWe can think of playing an animation as simply mapping that clip’s local \ntime line onto the character’s global time line. For example, Figure 11.14 illus-\ntrates playing animation clip A starting at a global time of τstart = 102 seconds.\nA: Normalized Local Time\nu = 0\nu = 0.4\nu = 1\nu = 0.8\nFigure 11.13.  An animation clip, showing normalized time units.\nClip A\nt = 0 sec\n5 sec\nτstart = 102 sec\nτ = 105 sec\n110 sec\nFigure 11.14.  Playing animation clip A starting at a global time of 102 seconds.\n11.4. Clips\n\n\n510 \n11. Animation Systems\nAs we saw above, playing a looping animation is like laying down an \ninﬁ nite number of back-to-front copies of the clip onto the global time line. \nWe can also imagine looping an animation a ﬁ nite number of times, which \ncorresponds to laying down a ﬁ nite number of copies of the clip. This is il-\nlustrated in Figure 11.15.\nTime-scaling a clip makes it appear to play back more quickly or more \nslowly than originally animated. To accomplish this, we simply scale the im-\nage of the clip when it is laid down onto the global time line. Time-scaling is \nmost naturally expressed as a playback rate , which we’ll denote R. For example, \nif an animation is to play back at twice the speed (R = 2), then we would scale \nthe clip’s local time line to one-half (1/R = 0.5) of its normal length when map-\nping it onto the global time line. This is shown in Figure 11.16.\nPlaying a clip in reverse corresponds to using a time scale of –1, as shown \nin Figure 11.17.\nClip A\n110 sec\nτstart = 102 sec\nClip A\n...\nτ = 105 sec\nFigure 11.15.  Playing a looping animation corresponds to laying down multiple back-to-back \ncopies of the clip.\nClip A\nτstart = 102 sec\nτ = 105 sec\nClip A\nR = 2\n(scale t by 1/R = 0.5)\nt = 0 sec\nt = 5 sec\nt = 0 sec\n5 sec\nFigure 11.16.  Playing an animation at twice the speed corresponds to scaling its local time line \nby a factor of ½.\nt = 5 sec\n0 sec\nτstart = 102 sec\nτ = 105 sec\n110 sec\n A pilC\nClip A\nt = 0 sec\n5 sec\nR = –1\n(ﬂip t)\nFigure 11.17.  Playing a clip in reverse corresponds to a time scale of –1.\n\n\n511 \nIn order to map an animation clip onto a global time line, we need the fol-\nlowing pieces of information about the clip:\nits global start time\n• \n τstart ,\nits playback rate \n• \nR,\nits duration \n• \nT,\nand the number of times it should loop, which we’ll denote \n• \nN.\nGiven this information, we can map from any global time τ to the correspond-\ning local time t, and vice-versa, using the following two relations:\n \nstart\n(\n),\nt\nR\n=\nτ−τ\n \n(11.2)\n \nstart\n1 .t\nR\nτ=τ\n+\n \nIf the animation doesn’t loop (N = 1), then we should clamp t into the valid \nrange [0, T] before using it to sample a pose from the clip:\n \n[\n]\nstart\n0\nclamp\n(\n)  .\nT\nt\nR\n=\nτ−τ\n \nIf the animation loops forever (N = ∞), then we bring t into the valid range \nby taking the remainder of the result aft er dividing by the duration T. This is \naccomplished via the modulo operator (mod, or % in C/C++), as shown below:\n(\n)\nstart\n(\n)   mod .\nt\nR\nT\n=\nτ−τ\n \nIf the clip loops a ﬁ nite number of times (1 < N < ∞), we must ﬁ rst clamp t \ninto the range [0, NT] and then modulo that result by T in order to bring t into \na valid range for sampling the clip:\nMost game engines work directly with local animation time lines and don’t \nuse the global time line directly. However, working directly in terms of global \ntimes can have some incredibly useful beneﬁ ts. For one thing, it makes syn-\nchronizing animations trivial.\n11.4.3. Comparison of Local and Global Clocks\nThe animation system must keep track of the time indices of every animation \nthat is currently playing. To do so, we have two choices:\nLocal clocks\n• \n . In this approach, each clip has its own local clock, usually \nrepresented by a ﬂ oating-point time index stored in units of seconds or \nframes, or in normalized time units (in which case it is oft en called the \nphase of the animation). At the moment the clip begins to play, the local \n11.4. Clips\n[\n]\nstart\n0\n(clamp\n(\n)  \n)  mod .\nNT\nt\nR\nT\n=\nτ−τ\n\n\n512 \n11. Animation Systems\ntime index t is usually taken to be zero. To advance the animations for-\nward in time, we advance the local clocks of each clip individually. If a \nclip has a non-unit playback rate R, the amount by which its local clock \nadvances must be scaled by R.\nGlobal clock\n• \n . In this approach, the character has a global clock, usually mea-\nsured in seconds, and each clip simply records the global time at which it \nstarted playing, τstart. The clips’ local clocks are calculated from this infor-\nmation using Equation (11.2), rather than being stored explicitly.\nThe local clock approach has the beneﬁ t of being simple, and it is the most \nobvious choice when designing an animation system. However, the global \nclock approach has some distinct advantages, especially when it comes to syn-\nchronizing animations, either within the context of a single character or across \nmultiple characters in a scene.\n11.4.3.1. Synchronizing Animations with a Local Clock\nWith a local clock approach, we said that the origin of a clip’s local time line \n(t = 0) is usually deﬁ ned to coincide with the moment at which the clip starts \nplaying. Thus, to synchronize two or more clips, they must be played at ex-\nactly the same moment in game time. This seems simple enough, but it can \nbecome quite tricky when the commands used to play the animations are \ncoming from disparate engine subsystems.\nFor example, let’s say we want to synchronize the player character’s punch \nanimation with a non-player character’s corresponding hit reaction anima-\ntion.  The problem is that the player’s punch is initiated by the player subsys-\ntem in response to detecting that a butt on was hit on the joy pad. Meanwhile, \nthe NPC ’s hit reaction animation is played by the artiﬁ cial intelligence (AI) \nsubsystem. If the AI code runs before the player code in the game loop, there \nwill be a one-frame delay between the start of the player’s punch and the start \nof the NPC’s reaction. And if the player code runs before the AI code, then the \nopposite problem occurs when an NPC tries to punch the player. If a message-\npassing (event) system is used to communicate between the two subsystems, \nadditional delays might be incurred (see Section 14.7 for more details). This \nproblem is illustrated in Figure 11.18.\nvoid GameLoop()\n{\n \nwhile (!quit)\n {\n \n \n// preliminary updates... \n \n \nUpdateAllNpcs(); // react to punch event  \n \n \n \n   // \nfrom last frame\n\n\n513 \n \n \n// more updates...\n \n \nUpdatePlayer(); // punch button hit – start punch  \n       \n    // anim, and send event to NPC to  \n       \n    // \nreact\n \n \n// still more updates...\n }\n}\n11.4.3.2. Synchronizing Animations with a Global Clock\nA global clock approach helps to alleviate many of these synchronization \nproblems, because the origin of the time line (τ = 0) is common across all clips \nby deﬁ nition. If two or more animations’ global start times are numerically \nequal, the clips will start in perfect synchronization. If their play back rates \nare also equal, then they will remain in sync with no drift . It no longer matt ers \nwhen the code that plays each animation executes. Even if the AI code that \nplays the hit reaction ends up running a frame later than the player’s punch \ncode, it is still trivial to keep the two clips in sync by simply noting the global \nstart time of the punch and sett ing the global start time of the reaction anima-\ntion to match it. This is shown in Figure 11.19.\nOf course, we do need to ensure that the two character’s global clocks \nmatch, but this is trivial to do. We can either adjust the global start times to \ntake account of any diﬀ erences in the characters’ clocks, or we can simply have \nall characters in the game share a single master clock.\nFigure 11.18.  The order of execution of disparate gameplay systems can introduce animation synchro-\nnization problems when local clocks are used.\n11.4. Clips\n\n\n514 \n11. Animation Systems\n11.4.4. A Simple Animation Data Format\nTypically, animation data is extracted from a Maya scene ﬁ le by sampling the \npose of the skeleton discretely at a rate of 30 or 60 samples per second. A sam-\nple comprises a full pose for each joint in the skeleton. The poses are usually \nstored in SQT format: For each joint j, the scale component is either a single \nﬂ oating-point scalar Sj , or a three-element vector Sj = [ Sjx  Sjy  Sjz ]. The rotation-\nal component is of course a four-element quaternion Qj = [ Qjx  Qjy  Qjz  Qjw ]. \nAnd the translational component is a three-element vector Tj = [ Tjx  Tjy  Tjz ]. \nWe sometimes say that an animation consists of up to 10 channels per joint, \nin reference to the 10 components of Sj , Qj , and Tj. This is illustrated in Fig-\nure 11.20. \nFigure 11.19.  A global clock approach can alleviate animation synchronization problems.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nSamples\nJoint 0\nT0\nQ0\nS0\nJoint 1\nT1\nQ1\nS1\n...\ny\nx\nz\ny\nx\nz\nw\ny\nx\nz\n...\n...\nFigure 11.20.  An uncompressed animation clip contains 10 channels of ﬂ oating-point data \nper sample, per joint.\n\n\n515 \nIn C++, an animation clip can be represented in many diﬀ erent ways. Here \nis one possibility:\nstruct JointPose { ... }; // SQT, defined as above\nstruct AnimationSample\n{\nJointPose*       m_aJointPose; // array of joint   \n \n           \n// poses\n};\nstruct AnimationClip\n{\n \nSkeleton*        m_pSkeleton;\n \nF32              m_framesPerSecond;\n \nU32              m_frameCount;\nAnimationSample* m_aSamples; // array of samples\n \nbool             m_isLooping;\n};\nAn animation clip is authored for a speciﬁ c skeleton and generally won’t \nwork on any other skeleton. As such, our example AnimationClip data struc-\nture contains a reference to its skeleton, m_pSkeleton. (In a real engine, this \nmight be a unique skeleton id rather than a Skeleton* pointer. In this case, \nthe engine would presumably provide a way to quickly and conveniently look \nup a skeleton by its unique id.)\nThe number of JointPoses in the m_aJointPose array within each sam-\nple is presumed to match the number of joints in the skeleton. The number \nof samples in the m_aSamples array is dictated by the frame count and by \nwhether or not the clip is intended to loop. For a non-looping animation, the \nnumber of samples is (m_frameCount + 1). However, if the animation loops, \nthen the last sample is identical to the ﬁ rst sample and is usually omitt ed. In \nthis case, the sample count is equal to m_frameCount.\nIt’s important to realize that in a real game engine, animation data isn’t \nactually stored in this simplistic format. As we’ll see in Section 11.8, the data \nis usually compressed in various ways to save memory.\n11.4.4.1. Animation Retargeting\n We said above that an animation is typically only compatible with a single \nskeleton. An exception to this rule can be made for skeletons that are closely \nrelated. For example, if a group of skeletons are identical except for a number \nof optional leaf joints that do not aﬀ ect the fundamental hierarchy, then an an-\n11.4. Clips\n\n\n516 \n11. Animation Systems\nimation authored for one of these skeletons should work on any of them. The \nonly requirement is that the engine be capable of ignoring animation channels \nfor joints that cannot be found in the skeleton being animated.\nOther more-advanced techniques exist for retargeting animations au-\nthored for one skeleton so that they work on a diﬀ erent skeleton. This is an \nactive area of research, and a full discussion of the topic is beyond the scope \nof this book. For more information, see for example htt p://portal.acm.org/cita-\ntion.cfm?id=1450621 and htt p://chrishecker.com/Real-time_Motion_Retarget-\ning_to_Highly_Varied_User-Created_Morphologies.\n11.4.5. Continuous Channel Functions\n The samples of an animation clip are really just deﬁ nitions of continuous func-\ntions over time. You can think of these as 10 scalar-valued functions of time \nper joint, or as two vector-valued functions and one quaternion-valued func-\ntion per joint. Theoretically, these channel functions are smooth and continu-\nous across the entire clip’s local time line, as shown in Figure 11.21 (with the \nexception of explicitly authored discontinuities like camera cuts). In practice, \nhowever, many game engines interpolate linearly between the samples, in \nwhich case the functions actually used are piece-wise linear approximations to \nthe underlying continuous functions. This is depicted in Figure 11.22.\nFigure 11.21.  The animation samples in a clip deﬁ ne continuous functions over time.\nt\nQy3\nSamples\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nFigure 11.22.  Many game engines use a piece-wise linear approximation when interpolating \nchannel functions.\n\n\n517 \n11.4.6. Metachannels\n Many games permit additional “metachannels” of data to be deﬁ ned for an \nanimation. These channels can encode game-speciﬁ c information that doesn’t \nhave to do directly with posing the skeleton but which needs to be synchro-\nnized with the animation.\nIt is quite common to deﬁ ne a special channel that contains event triggers \nat various time indices, as shown in Figure 11.23. Whenever the animation’s \nlocal time index passes one of these triggers, an event is sent to the game en-\ngine, which can respond as it sees ﬁ t. (We’ll discuss events in detail in Chap-\nter 14.) One common use of event triggers is to denote at which points during \nthe animation certain sound or particle eﬀ ects should be played. For example, \nwhen the left  or right foot touches the ground, a footstep sound and a “cloud \nof dust” particle eﬀ ect could be initiated.\nAnother common practice is to permit special joints, known in Maya as \nlocators , to be animated along with the joints of the skeleton itself. Because a \njoint or locator is just an aﬃ  ne transform, these special joints can be used to \nencode the position and orientation of virtually any object in the game.\nA typical application of animated locators is to specify how the game’s \ncamera should be positioned and oriented during an animation. In Maya, a \nlocator is constrained to a camera, and the camera is then animated along with \nthe joints of the character(s) in the scene. The camera’s locator is exported and \nused in-game to move the game’s camera around during the animation. The \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nSamples\nJoint 0\nT0\nQ0\nS0\nJoint 1\nT1\nQ1\nS1\nOther\nFootstep\nLeft\nFootstep\nRight\nReload\nWeapon\nEvents\n...\n...\n...\n...\nFigure 11.23.  A special event trigger channel can be added to an animation clip in order to \nsynchronize sound effects, particle effects, and other game events with an animation.\n11.4. Clips\n\n\n518 \n11. Animation Systems\nﬁ eld of view (focal length) of the camera, and possibly other camera att ributes, \ncan also be animated by placing the relevant data into one or more additional \nﬂ oating-point channels .\nOther examples of non-joint animation channels include:\ntexture coordinate scrolling,\n• \ntexture animation\n• \n (a special case of texture coordinate scrolling in which \nframes are arranged linearly within a texture, and the texture is scrolled \nby one complete frame at each iteration),\nanimated material parameters (color, specularity, transparency, etc.),\n• \nanimated lighting parameters (radius, cone angle, intensity, color, etc.),\n• \nany other parameters that need to change over time and are in some \n• \nway synchronized with an animation.\n11.5. Skinning and Matrix Palette Generation\nWe’ve seen how to pose a skeleton by rotating, translating, and possibly scal-\ning its joints. And we know that any skeletal pose can be represented math-\nematically as a set of local (\np( )\nj\nj\n→\nP\n) or global (\nM\nj→\nP\n) joint pose transformations, \none for each joint j. Next, we will explore the process of att aching the vertices \nof a 3D mesh to a posed skeleton. This process is known as skinning .\n11.5.1. Per-Vertex Skinning Information\nA skinned mesh is att ached to a skeleton by means of its vertices. Each vertex \ncan be bound to one or more joints. If bound to a single joint, the vertex tracks \nthat joint’s movement exactly. If bound to two or more joints, the vertex’s posi-\ntion becomes a weighted average of the positions it would have assumed had it \nbeen bound to each joint independently.\nTo skin a mesh to a skeleton, a 3D artist must supply the following ad-\nditional information at each vertex:\nthe \n• \nindex or indices of the joint(s) to which it is bound,\nfor each joint, a \n• \nweighting factor describing how much inﬂ uence that joint \nshould have on the ﬁ nal vertex position.\nThe weighting factors are assumed to add to one, as is customary when calcu-\nlating any weighted average.\nUsually a game engine imposes an upper limit on the number of joints \nto which a single vertex can be bound. A four-joint limit is typical for a num-\nber of reasons. First, four 8-bit joint indices can be packed into a 32-bit word, \n\n\n519 \nwhich is convenient. Also, while it’s prett y easy to see a diﬀ erence in quality \nbetween a two-, three-, and even a four-joint-per-vertex model, most people \ncannot see a quality diﬀ erence as the number of joints per vertex is increased \nbeyond four.\nBecause the joint weights must sum to one, the last weight can be omitt ed \nand oft en is. (It can be calculated at runtime as \n3\n0\n1\n2\n1\n(\n)\nw\nw\nw\nw\n= −\n+\n+\n.) As \nsuch, a typical skinned vertex data structure might look as follows:\nstruct SkinnedVertex\n{\n    float m_position[3];    //   (Px, Py, Pz)\n    float m_normal[3];      //    (Nx, Ny, Nz)\n    float m_u, m_v;         // texture coordinates \n  \n \n       \n  //   (u, \nv)\n    U8    m_jointIndex[4];  // joint indices\n    float m_jointWeight[3]; // joint weights, last one   \n    \n       \n   // \nomitted\n};\n11.5.2. The Mathematics of Skinning\nThe vertices of a skinned mesh track the movements of the joint(s) to which \nthey are bound. To make this happen mathematically, we would like to ﬁ nd a \nmatrix that can transform the vertices of the mesh from their original positions \n(in bind pose) into new positions that correspond to the current pose of the \nskeleton. We shall call such a matrix a skinning matrix.\nLike all mesh vertices, the position of a skinned vertex is speciﬁ ed in mod-\nel space. This is true whether its skeleton is in bind pose, or in any other pose. \nSo the matrix we seek will transform vertices from model space (bind pose) \nto model space (current pose ). Unlike the other transforms we’ve seen thus \nfar, such as the model-to-world transform or the world-to-view transform, \na skinning matrix is not a change of basis transform. It morphs vertices into \nnew positions, but the vertices are in model space both before and aft er the \ntransformation.\n11.5.2.1. Simple Example: One-Jointed Skeleton\nLet us derive the basic equation for a skinning matrix. To keep things simple at \nﬁ rst, we’ll work with a skeleton consisting of a single joint. We therefore have \ntwo coordinate spaces to work with: model space, which we’ll denote with \nthe subscript M, and the joint space of our one and only joint, which will be \nindicated by the subscript J. The joint’s coordinate axes start out in bind pose, \n11.5. Skinning and Matrix Palette Generation\n\n\n520 \n11. Animation Systems\nwhich we’ll denote with the superscript B. At any given moment during an \nanimation, the joint’s axes move to a new position and orientation in model \nspace—we’ll indicate this current pose with the superscript C.\nNow consider a single vertex that is skinned to our joint. In bind pose, \nits model-space position is \nB\nM\nv . The skinning process calculates the vertex’s \nnew model-space position in the current pose, \nC\nM\nv . This is illustrated in Fig-\nure 11.24.\nThe “trick” to ﬁ nding the skinning matrix for a given joint is to realize \nthat the position of a vertex bound to a joint is constant when expressed in that \njoint’s coordinate space. So we take the bind-pose position of the vertex in model \nspace, convert it into joint space, move the joint into its current pose, and ﬁ -\nnally convert the vertex back into model space. The net eﬀ ect of this round trip \nfrom model space to joint space and back again is to “morph” the vertex from \nbind pose into the current pose.\nReferring to the illustration in Figure 11.25, let’s assume that the coordi-\nnates of the vertex \nB\nM\nv\n are (4, 6) in model space (when the skeleton is in bind \npose). We convert this vertex into its equivalent joint space coordinates \nj\nv , \nwhich are roughly (1, 3) as shown in the diagram. Because the vertex is bound \nto the joint, its joint space coordinates will always be (1, 3) no matt er how the \njoint may move. Once we have the joint in the desired current pose, we con-\nvert the vertex’s coordinates back into model space, which we’ll denote with \nthe symbol \nC\nM\nv\n. In our diagram, these coordinates are roughly (18, 2). So the \nskinning transformation has morphed our vertex from (4, 6) to (18, 2) in model \nspace, due entirely to the motion of the joint from its bind pose to the current \npose shown in the diagram.\nLooking at the problem mathematically, we can denote the bind pose of the \njoint j in model space by the matrix \nM\nj→\nB\n. This matrix transforms a point or \nxM\nyM\nxB\nyB\nxC\nyC\nModel Space Axes\nBind pose \nvertex position, \nin model space\nBind Pose \nJoint Space \nAxes\nCurrent \nPose Joint \nSpace Axes\nCurrent pose \nvertex position, \nin model space\nvM\nB\nv M\nC\nFigure 11.24.  Bind pose and current pose of a simple, one-joint skeleton and a single vertex \nbound to that joint.\n\n\n521 \nvector whose coordinates are expressed in joint j’s space into an equivalent set \nof model space coordinates . Now, consider a vertex \nB\nM\nv\n whose coordinates \nare expressed in model space with the skeleton in bind pose. To convert these \nvertex coordinates into the space of joint j, we simply multiply it by the inverse \nbind pose matrix, \n1\nM\nM\n(\n)\nj\nj\n−\n→\n→\n=\nB\nB\n:\n \n \n \nB\nB\n1\nM\nM\nM\nM\n(\n)\n.\nj\nj\nj\n−\n→\n→\n=\n=\nv\nv\nB\nv\nB\n \n(11.3)\nLikewise, we can denote the joint’s current pose (i.e., any pose that is not \nbind pose) by the matrix \nM\nj→\nC\n. To convert \nj\nv  from joint space back into mod-\nel space, we simply multiply it by the current pose matrix as follows:\n \n \nC\nM\nM.\nj\nj→\n=\nv\nv C\nIf we expand \nj\nv  using Equation (11.3), we obtain an equation that takes our \nvertex directly from its position in bind pose to its position in the current \npose:\n \n \n \n \n \nC\nM\nM\nB\n1\nM\nM\nM\nB\nM\n(\n)\n.\nj\nj\nj\nj\nj\n→\n−\n→\n→\n=\n=\n=\nv\nv C\nv\nB\nC\nv\nK\n (11.4) \n \nThe combined matrix \n1\nM\nM\n(\n)\nj\nj\nj\n−\n→\n→\n=\nK\nB\nC\n is known as a skinning matrix .\n11.5.2.2. Extension to Multijointed Skeletons\nIn the example above, we considered only a single joint. However, the math we \nderived above actually applies to any joint in any skeleton imaginable, because \nwe formulated everything in terms of global poses (i.e., joint space to model \nspace transforms). To extend the above formulation to a skeleton containing \nmultiple joints, we therefore need to make only two minor adjustments:\nxM\nyM\nxB\nyB\nxC\nyC\n1. Transform into\n    joint space\nv M\nB\nv M\nC\nv j\nv j\n3. Transform back\n    into model space\n2. Move joint into\n    current pose\nFigure 11.25.  By transforming a vertex’s position into joint space, it can be made to “track” \nthe joint’s movements.\n11.5. Skinning and Matrix Palette Generation\n\n\n522 \n11. Animation Systems\nWe must make sure that our \n1. \nM\nj→\nB\n and \nM\nj→\nC\nmatrices are calculated \nproperly for the joint in question, using Equation (11.1). \nM\nj→\nB\n and \nM\nj→\nC\n are just the bind pose and current pose equivalents, respectively, \nof the matrix \nM\nj→\nP\n given in that equation.\nWe must calculate an array of skinning matrices \n2. \nj\nK , one for each joint j. \nThis array is known as a matrix palett e . The matrix palett e is passed to the \nrendering engine when rendering a skinned mesh. For each vertex, the \nrenderer looks up the appropriate joint’s skinning matrix in the palett e \nand uses it to transform the vertex from bind pose into current pose.\nWe should note here that the current pose matrix \nM\nj→\nC\n changes every \nframe as the character assumes diﬀ erent poses over time. However, the in-\nverse bind-pose matrix is constant throughout the entire game, because the \nbind pose of the skeleton is ﬁ xed when the model is created. Therefore, the \nmatrix \n1\nM\n(\n)\nj\n−\n→\nB\n is generally cached with the skeleton, and needn’t be calcu-\nlated at runtime. Animation engines generally calculate local poses for each \njoint (\np( )\nj\nj\n→\nC\n), then use Equation (11.1) to convert these into global poses \n(\nM\nj→\nC\n), and ﬁ nally multiply each global pose by the corresponding cached \ninverse bind pose matrix (\n1\nM\n(\n)\nj\n−\n→\nB\n) in order to generate a skinning matrix \n(\nj\nK ) for each joint.\n11.5.2.3. Incorporating the Model-to-World Transform\nEvery vertex must eventually be transformed from model space into world \nspace. Some engines therefore premultiply the palett e of skinning matrices by \nthe object’s model-to-world transform. This can be a useful optimization, as \nit saves the rendering engine one matrix multiply per vertex when rendering \nskinned geometry. (With hundreds of thousands of vertices to process, this \nsavings can really add up!)\nTo incorporate the model-to-world transform into our skinning matrices, \nwe simply concatenate it to the regular skinning matrix equation, as follows:\n \n \n1\nW\nM\nM\nM\nW\n(\n)\n(\n)\n.\nj\nj\nj\n−\n→\n→\n→\n=\nK\nB\nC\nM\nSome engines bake the model-to-world transform into the skinning ma-\ntrices like this, while others don’t. The choice is entirely up to the engineer-\ning team and is driven by all sorts of factors. For example, one situation in \nwhich we would deﬁ nitely not want to do this is when a single animation \nis being applied to multiple characters simultaneously—a technique known \nas animation instancing that is commonly used for animating large crowds of \ncharacters. In this case we need to keep the model-to-world transforms sepa-\nrate so that we can share a single matrix palett e across all characters in the \ncrowd.\n\n\n523 \n11.5.2.4. Skinning a Vertex to Multiple Joints\n When a vertex is skinned to more than one joint, we calculate its ﬁ nal position \nby assuming it is skinned to each joint individually, calculating a model space \nposition for each joint and then taking a weighted average of the resulting posi-\ntions. The weights are provided by the character rigging artist, and they must \nalways sum to one. (If they do not sum to one, they should be re-normalized \nby the tools pipeline.)\nThe general formula for a weighted average of N quantities \n0a  through \n1\nN\na\n−, with weights \n0\nw  through \n1\nN\nw\n− and with \n \n \nThis works equally well for vector quantities ai. So, for a vertex skinned to N \njoints with indices \n0j  through \n1\nN\nj\n− and weights \n0\nw  through \n1\nN\nw\n−, we can \nextend Equation (11.4) as follows:\n \n \n1\nC\nB\nM\nM\n0\n,\ni\nN\nĳ\ni\nw\n−\n=\n=∑\nv\nv\nK\nwhere \nij\nK  is the skinning matrix for the joint \nij .\n11.6. Animation Blending\nThe term animation blending refers to any technique that allows more than one ani-\nmation clip to contribute the ﬁ nal pose of the character. To be more precise, blend-\ning combines two or more input poses to produce an output pose for the skeleton.\nBlending usually combines two or more poses at a single point in time, \nand generates an output at that same moment in time. In this context, blend-\ning is used to combine two or more animations into a host of new animations, \nwithout having to create them manually. For example, by blending an injured \nwalk animation with an uninjured walk, we can generate various intermedi-\nate levels of apparent injury for our character while he is walking. As another \nexample, we can blend between an animation in which the character is aim-\ning to the left  and one in which he’s aiming to the right, in order to make the \ncharacter aim along any desired angle between the two extremes. Blending \ncan be used to interpolate between extreme facial expressions, body stances, \nlocomotion modes, and so on.\nBlending can also be used to ﬁ nd an intermediate pose between two \nknown poses at diﬀ erent points in time. This is used when we want to ﬁ nd the \npose of a character at a point in time that does not correspond exactly to one of \n11.6. Animation Blending\n1\n0\n1\nN\ni\ni\nw\n−\n=\n=\n∑\n,  \nis\n  \n1\n0\nˆ\n.\nN\ni i\ni\na\nw a\n−\n=\n=∑\n\n\n524 \n11. Animation Systems\nthe sampled frames available in the animation data. We can also use temporal \nanimation blending to smoothly transition from one animation to another, by \ngradually blending from the source animation to the destination over a short \nperiod of time.\n11.6.1. LERP Blending\nGiven two skeletal poses \n{\n}\n1\n0\n (\n)\n \nN\nskel\nA\nA j\nj\n−\n=\n=\nP\nP\n and \n{\n}\n1\n0\n (\n)\n \nN\nskel\nB\nB j\nj\n−\n=\n=\nP\nP\n, we wish \nto ﬁ nd an intermediate pose \nskel\nLERP\nP\n between these two extremes. This can be \ndone by performing a linear interpolation (LERP) between the local poses of \neach individual joint in each of the two source poses. This can be writt en as \nfollows:\n \nLERP\n(\n)\nLERP (\n) , (\n) , \n(1\n)(\n)\n(\n) .\nj\nA j\nB j\nA j\nB j\n⎡\n⎤\n=\nβ\n⎣\n⎦\n=\n−β\n+ β\nP\nP\nP\nP\nP\n \n(11.5)\nThe interpolated pose of the whole skeleton is simply the set of interpolated \nposes for all of the joints:\n \n{\n}\n1\nskel\nLERP\nLERP\n0\n (\n)\n \n.\nN\nj\nj\n−\n=\n=\nP\nP\n \n(11.6)\nIn these equations, β is called the blend percentage or blend factor. When \nβ = 0, the ﬁ nal pose of the skeleton will exactly match \nskel\nA\nP\n; when β = 1, the \nﬁ nal pose will match \nskel\nB\nP\n. When β is between zero and one, the ﬁ nal pose \nis an intermediate between the two extremes. This eﬀ ect is illustrated in Fig-\nure 11.10.\nWe’ve glossed over one small detail here: We are linearly interpolating \njoint poses , which means interpolating 4×4 transformation matrices. But, as we \nsaw in Chapter 4, interpolating matrices directly is not practical. This is one of \nthe reasons why local poses are usually expressed in SQT format—doing so \nallows us to apply the LERP operation deﬁ ned in Section 4.2.5 to each compo-\nnent of the SQT individually. The linear interpolation of the translation com-\nponent T of an SQT is just a straightforward vector LERP:\n \nLERP\n(\n)\nLERP[(\n) , (\n) , ]\n(1\n)(\n)\n(\n) .\nj\nA j\nB j\nA j\nB j\n=\nβ\n=\n−β\n+ β\nT\nT\nT\nT\nT\n \n(11.7)\nThe linear interpolation of the rotation component is a quaternion LERP or \nSLERP (spherical linear interpolation):\n \nLERP\n(q\n)\nLERP[(q ) , (q ) , ]\n(1\n)(q )\n(q )\nj\nA j\nB j\nA j\nB j\n=\nβ\n=\n−β\n+ β\n \n(11.8a)\nor\n\n\n525 \n \nLERP\n(q\n)\nSLERP[(q ) , (q ) , ]\nsin((1\n) )\nsin(\n)\n(q )\n(q ) .\nsin( )\nsin( )\nj\nA j\nB j\nA j\nB j\n=\nβ\n−β θ\nβθ\n=\n+\nθ\nθ\n \n(11.8b)\nFinally, the linear interpolation of the scale component is either a scalar or vec-\ntor LERP, depending on the type of scale (uniform or nonuniform ) supported \nby the engine:\n \nLERP\n(\n)\nLERP[(\n) , (\n) , ]\n(1\n)(\n)\n(\n)\nj\nA j\nB j\nA j\nB j\n=\nβ\n=\n−β\n+ β\ns\ns\ns\ns\ns\n \n(11.9a)\nor\n \nLERP\n(\n)\nLERP[(\n) , (\n) , ]\n(1\n)(\n)\n(\n) .\nj\nA j\nB j\nA j\nB j\ns\ns\ns\ns\ns\n=\nβ\n=\n−β\n+ β\n \n(11.9b)\nWhen linearly interpolating between two skeletal poses, the most natural-\nlooking intermediate pose is generally one in which each joint pose is inter-\npolated independently of the others, in the space of that joint’s immediate \nparent. In other words, pose blending is generally performed on local poses. If \nwe were to blend global poses directly in model space, the results would tend \nto look biomechanically implausible.\nBecause pose blending is done on local poses, the linear interpolation of \nany one joint’s pose is totally independent of the interpolations of the other \njoints in the skeleton. This means that linear pose interpolation can be per-\nformed entirely in parallel on multiprocessor architectures.\n11.6.2. Applications of LERP Blending\nNow that we understand the basics of LERP blending, let’s have a look at \nsome typical gaming applications.\n11.6.2.1. Temporal Interpolation\n As we mentioned in Section 11.4.1.1, game animations are almost never sam-\npled exactly on integer frame indices. Because of variable frame rate, the play-\ner might actually see frames 0.9, 1.85, and 3.02, rather than frames 1, 2, and \n3 as one might expect. In addition, some animation compression techniques \ninvolve storing only disparate key frames, spaced at uneven intervals across \nthe clip’s local time line. In either case, we need a mechanism for ﬁ nding in-\ntermediate poses between the sampled poses that are actually present in the \nanimation clip.\nLERP blending is used to ﬁ nd these intermediate poses. As an example, \nlet’s imagine that our animation clip contains evenly-spaced pose samples at \n11.6. Animation Blending\n\n\n526 \n11. Animation Systems\ntimes 0, Δt, 2Δt, 3Δt, and so on. To ﬁ nd a pose at time t = (2.18)Δt, we simply \nﬁ nd the linear interpolation between the poses at times 2Δt and 3Δt, using a \nblend percentage of β = 0.18.\nIn general, we can ﬁ nd the pose at time t given pose samples at any two \ntimes t1 and t2 that bracket t, as follows:\n \n1\n2\n1\n2\n( )\nLERP[\n( ), \n( ), ( )]\n(1\n( ))\n( )\n( )\n( ),\nj\nj\nj\nj\nj\nt =\nβ\n=\n−β\n+ β\nP\nP\nP\nP\nP\nt\nt\nt\nt\nt\nt\nt\n \n(11.10)\nwhere the blend factor β(t) is the ratio\n \n1\n2\n1\n( )\n.\nt\nt\nt\n−\nβ\n=\n−\nt\nt\n \n(11.11)\n11.6.2.2. Motion Continuity: Cross-Fading\n Game characters are animated by piecing together a large number of ﬁ ne-\ngrained animation clips. If your animators are any good, the character will ap-\npear to move in a natural and physically plausible way within each individual \nclip. However, it is notoriously diﬃ  cult to achieve the same level of quality \nwhen transitioning from one clip to the next. The vast majority of the “pops” \nwe see in game animations occur when the character transitions from one clip \nto the next.\nIdeally, we would like the movements of each part of a character’s body to \nbe perfectly smooth, even during transitions. In other words, the three-dimen-\nsional paths traced out by each joint in the skeleton as it moves should contain \nno sudden “jumps.” We call this C0 continuity ; it is illustrated in Figure 11.26.\nNot only should the paths themselves be continuous, but their ﬁ rst deriv-\natives (velocity curves) should be continuous as well. This is called C1 continu-\nity (or continuity of velocity and momentum). The perceived quality and real-\nism of an animated character’s movement improves as we move to higher and \nhigher order continuity. For example, we might want to achieve C2 continuity, \nin which the second derivatives of the motion paths (acceleration curves) are \nalso continuous.\nt\nTx7\nt\nTx7\ndiscontinuity\nC0 continuous\nnot C0 continuous\nFigure 11.26.  The channel function on the left has C0 continuity, while the path on the right \ndoes not.\n",
      "page_number": 526,
      "chapter_number": 27,
      "summary": "This chapter covers segment 27 (pages 526-548). Key topics include animation, animated, and animations.",
      "keywords": [
        "Animation",
        "time",
        "animation clip",
        "pose",
        "joint",
        "Local Time Line",
        "clip",
        "Global Time Line",
        "Time Line",
        "Local Time",
        "Global Time",
        "Animation Systems",
        "bind pose",
        "joint space",
        "current pose"
      ],
      "concepts": [
        "animation",
        "animated",
        "animations",
        "animator",
        "pose",
        "posing",
        "posed",
        "time",
        "joint",
        "clips"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 7",
          "chapter": 46,
          "title": "Segment 46 (pages 443-452)",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 50,
          "title": "Segment 50 (pages 487-494)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 11,
          "title": "Segment 11 (pages 100-107)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "makinggames",
          "chapter": 17,
          "title": "Segment 17 (pages 140-147)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 33,
          "title": "Segment 33 (pages 314-325)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 549-568)",
      "start_page": 549,
      "end_page": 568,
      "detection_method": "synthetic",
      "content": "527 \nStrict mathematical continuity up to C1 or higher is oft en infeasible to \nachieve. However, LERP-based animation blending can be applied to achieve \na reasonably pleasing form of C0 motion continuity. It usually also does a \nprett y good job of approximating C1 continuity. When applied to transitions \nbetween clips in this manner, LERP blending is sometimes called cross-fading . \nLERP blending can introduce unwanted artifacts, such as the dreaded “sliding \nfeet” problem, so it must be applied judiciously.\nTo cross-fade between two animations, we overlap the time lines of the \ntwo clips by some reasonable amount, and then blend the two clips together. \nThe blend percentage β starts at zero at time tstart  , meaning that we see only \nclip A when the cross-fade begins. We gradually increase β until it reaches a \nvalue of one at time tend . At this point only clip B will be visible, and we can \nretire clip A altogether. The time interval over which the cross-fade occurs \n(Δtblend = tend – tstart) is sometimes called the blend time .\nTypes of Cross-Fades\nThere are two common ways to perform a cross-blended transition:\nSmooth transition\n• \n . Clips A and B both play simultaneously as β increases \nfrom zero to one. For this to work well, the two clips must be looping \nanimations, and their time lines must be synchronized so that the posi-\ntions of the legs and arms in one clip match up roughly with their posi-\ntions in the other clip. (If this is not done, the cross-fade will oft en look \ntotally unnatural.) This technique is illustrated in Figure 11.27.\nFrozen transition\n• \n . The local clock of clip A is stopped at the moment clip \nB starts playing. Thus the pose of the skeleton from clip A is frozen \nwhile clip B gradually takes over the movement. This kind of transi-\ntional blend works well when the two clips are unrelated and cannot be \nClip A\nt\nClip B\nβ\n1\n0\ntstart\ntend\nFigure 11.27.  A smooth transition, in which the local clocks of both clips keep running during \nthe transition.\n11.6. Animation Blending\n\n\n528 \n11. Animation Systems\ntime-synchronized, as they must be when performing a smooth transi-\ntion. This approach is depicted in Figure 11.28.\nWe can also control how the blend factor β varies during the transition. \nIn Figure 11.27 and Figure 11.28, the blend factor varied linearly with time. \nTo achieve an even smoother transition, we could vary β according to a cubic \nfunction of time, such as a one-dimensional Bézier. When such a curve is ap-\nplied to a currently-running clip that is being blended out, it is known as an \nease-out curve; when it is applied to a new clip that is being blended in, it is \nknown as an ease-in curve. This is shown in Figure 11.29.\nThe equation for a Bézier ease-in/ease-out curve is given below. It returns \nthe value of β at any time t within the blend interval. βstart is the blend factor \nat the start of the blend interval, tstart , and βend is the ﬁ nal blend factor at time \ntend. The parameter u is the normalized time between tstart and tend , and for con-\nvenience we’ll also deﬁ ne v = 1 – u (the inverse normalized time). Note that \nthe Bézier tangents Tstart and Tend are taken to be equal to the corresponding \nClip A\nt\nClip B\nβ\n1\n0\nA’s local timeline \nfreezes here\ntstart\ntend\nFigure 11.28.  A frozen transition, in which clip A’s local clock is stopped during the transi-\ntion.\nClip A\nt\nClip B\nβ\n1\n0\ntstart\ntend\nFigure 11.29.  A smooth transition, with a cubic ease-in/ease-out curve applied to the blend \nfactor.\n\n\n529 \nblend factors βstart and βend , because this yields a well-behaved curve for our \npurposes:\n \nstart\nend\nstart\n3\n2\n2\n3\nstart\nstart\nend\nend\n3\n2\n2\n3\nstart\nend\nlet  \nand  \n1\n:\n( )\n(\n)\n(3\n)\n(3\n)\n(\n)\n(\n3\n)\n(3\n)\n.\nu\nt\nv\nu\nt\nv\nv u T\nvu T\nu\nv\nv u\nvu\nu\n⎛\n⎞\n−\n=⎜\n⎟\n⎝\n⎠\n−\n= −\nβ\n=\nβ\n+\n+\n+\nβ\n=\n+\nβ\n+\n+\nβ\nt\nt\nCore Poses\nThis is an appropriate time to mention that motion continuity can actually \nbe achieved without blending if the animator ensures that the last pose in any \ngiven clip matches the ﬁ rst pose of the clip that follows it. In practice, anima-\ntors oft en decide upon a set of core poses —for example, we might have a core \npose for standing upright, one for crouching, one for lying prone, and so on. \nBy making sure that the character starts in one of these core poses at the begin-\nning of every clip and returns to a core pose at the end, C0 continuity can be \nachieved by simply ensuring that the core poses match when animations are \nspliced together. C1 or higher-order motion continuity can also be achieved \nby ensuring that the character’s movement at the end of one clip smoothly \ntransitions into the motion at the start of the next clip. This is easily achieved \nby authoring a single smooth animation and then breaking it into two or more \nclips.\n11.6.2.3. Directional Locomotion\nLERP-based animation blending is oft en applied to character locomotion. \nWhen a real human being walks or runs, he can change the direction in which \nhe is moving in two basic ways: First, he can turn his entire body to change \ndirection, in which case he always faces in the direction he’s moving. I’ll call \nTargeted\nPivotal\nPath of \nMovement\nFigure 11.30.  In pivotal movement, the character faces the direction she is moving and pivots \nabout her vertical axis to turn. In targeted movement, the movement direction need not \nmatch the facing direction.\n11.6. Animation Blending\n\n\n530 \n11. Animation Systems\nthis pivotal movement, because the person pivots about his vertical axis when \nhe turns. Second, he can keep facing in one direction, while walking forward, \nbackward, or sideways (known as straﬁ ng in the gaming world) in order to \nmove in a direction that is independent of his facing direction. I’ll call this \ntargeted movement, because it is oft en used in order to keep one’s eye—or one’s \nweapon—trained on a target while moving. These two movement styles are \nillustrated in Figure 11.30.\nTargeted Movement\nTo implement targeted movement, the animator authors three separate looping \nanimation clips—one moving forward, one straﬁ ng to the left , and one straf-\ning to the right. I’ll call these directional locomotion clips. The three directional \nclips are arranged around the circumference of a semicircle, with forward at \n0 degrees, left  at 90 degrees and right at –90 degrees. With the character’s fac-\ning direction ﬁ xed at 0 degrees, we ﬁ nd the desired movement direction on \nthe semicircle, select the two adjacent movement animations, and blend them \ntogether via LERP-based blending. The blend percentage β is determined by \nhow close the angle of movement is to the angles of two adjacent clips. This is \nillustrated in Figure 11.31.\nNote that we did not include backward movement in our blend, for a full \ncircular blend. This is because blending between a sideways strafe and a back-\nward run cannot be made to look natural in general. The problem is that when \nstraﬁ ng to the left , the character usually crosses its right foot in front of its left  \nso that the blend into the pure forward run animation looks correct. Likewise, \nthe right strafe is usually authored with the left  foot crossing in front of the \nright. When we try to blend such strafe animations directly into a backward \nrun, one leg will start to pass through the other, which looks extremely awk-\nFigure 11.31.  Targeted movement can be implemented by blending together looping locomo-\ntion clips that move in each of the four principal directions.\n\n\n531 \nward and unnatural. There are a number of ways to solve this problem. One \nfeasible approach is to deﬁ ne two hemispherical blends, one for forward mo-\ntion and one for backward motion, each with strafe animations that have been \ncraft ed to work properly when blended with the corresponding straight run. \nWhen passing from one hemisphere to the other, we can play some kind of \nexplicit transition animation so that the character has a chance to adjust its gait \nand leg crossing appropriately.\nPivotal Movement\nTo implement pivotal movement, we can simply play the forward locomotion \nloop while rotating the entire character about its vertical axis to make it turn. \nPivotal movement looks more natural if the character’s body doesn’t remain \nbolt upright when it is turning—real humans tend to lean into their turns a \nlitt le bit. We could try slightly tilting the vertical axis of the character as a \nwhole, but that would cause problems with the inner foot sinking into the \nground while the outer foot comes oﬀ  the ground. A more natural-looking \nresult can be achieved by animating three variations on the basic forward \nwalk or run—one going perfectly straight, one making an extreme left  turn, \nand one making an extreme right turn. We can then LERP-blend between \nthe straight clip and the extreme left  turn clip to implement any desired lean \nangle.\n11.6.3. Complex LERP Blends\n In a real game engine, characters make use of a wide range of complex blends \nfor various purposes. It can be convenient to “prepackage” certain commonly \nused types of complex blends for ease of use. In the following sections, we’ll \ninvestigate a few popular types of prepackaged complex blends.\n11.6.3.1. Generalized One-Dimensional LERP Blending\nLERP blending can be easily extended to more than two animation clips, us-\ning a technique I call one-dimensional LERP blending. We deﬁ ne a new blend \nparameter b that lies in any linear range desired (e.g., –1 to +1, or from 0 to 1, \nor even from 27 to 136). Any number of clips can be positioned at arbitrary \npoints along this range, as shown in Figure 11.32. For any given value of b, we \nselect the two clips immediately adjacent to it and blend them together using \nEquation (11.5). If the two adjacent clips lie at points b1 and b2 , then the blend \npercentage β can be determined using a technique analogous to that used in \nEquation (11.10), as follows:\n \n \n \n(11.12)\n1\n2\n1\n.\nb\nb\nb\nb\n−\nβ=\n−\n11.6. Animation Blending\n\n\n532 \n11. Animation Systems\nTargeted movement is just a special case of one-dimensional LERP blend-\ning. We simply straighten out the circle on which the directional animation \nclips were placed and use the movement direction angle θ as the param-\neter b (with a range of –90 to 90 degrees). Any number of animation clips \ncan be placed onto this blend range at arbitrary angles. This is shown in Fig-\nure 11.33.\n11.6.3.2. Simple Two-Dimensional LERP Blending\nSometimes we would like to smoothly vary two aspects of a character’s motion \nsimultaneously. For example, we might want the character to be capable of \naiming his weapon vertically and horizontally. Or we might want to allow our \ncharacter to vary her pace length and the separation of her feet as she moves. \nWe can extend one-dimensional LERP blending to two dimensions in order to \nachieve these kinds of eﬀ ects.\nClip A\nb0\nb1\nb2\nb3\nb4\nClip B\nClip C\nClip D Clip E\nb\n1\n2\n1\nb\nb\nb\nb\n−\n−\n=\nβ\nFigure 11.32.  A generalized linear blend between N animation clips.\nFigure 11.33.  The directional clips used in targeted movement can be thought of as a special \ncase of one-dimensional LERP blending.\n\n\n533 \nIf we know that our 2D blend involves only four animation clips, and if \nthose clips are positioned at the four corners of a square region, then we can \nﬁ nd a blended pose by performing two 1D blends. Our generalized blend fac-\ntor b becomes a two-dimensional blend vector b = [ bx  by ]. If b lies within the \nsquare region bounded by our four clips, we can ﬁ nd the resulting pose by \nfollowing these steps:\nUsing the horizontal blend factor \n• \nbx , ﬁ nd two intermediate poses, one \nbetween the top two animation clips and one between the bott om two \nclips. These two poses can be found by performing two simple one-di-\nmensional LERP blends.\nThen, using the vertical blend factor \n• \nby , ﬁ nd the ﬁ nal pose by LERP-\nblending the two intermediate poses together.\nThis technique is illustrated in Figure 11.34.\n11.6.3.3. Triangular Two-Dimensional LERP Blending\nThe simple 2D blending technique we investigated above only works when \nthe animation clips we wish to blend lie at the corners of a square region. How \ncan we blend between an arbitrary number of clips positioned at arbitrary \nlocations in our 2D blend space?\n Let’s imagine that we have three animation clips that we wish to blend to-\ngether. Each clip, designated by the index i, corresponds to a particular blend \ncoordinate bi = [ bxi  byi ] in our two-dimensional blend space, and these three \nblend coordinates form a triangle in our two-dimensional blend space. Each \nclip i deﬁ nes a set of joint poses {\n}\n1\n0\n (\n)\n \nN\nĳ\nj\n−\n=\nP\n, where j is the joint index and N \nis the number of joints in the skeleton. We wish to ﬁ nd the interpolated pose \nClip A\nb x\nby\nClip B\nClip D\nClip C\nBlend \nAB\nBlend \nCD\nFinal \nBlend\nb\nFigure 11.34.  A simple formulation for 2D animation blending between four clips at the \ncorners of a square region.\n11.6. Animation Blending\n\n\n534 \n11. Animation Systems\nof the skeleton corresponding to an arbitrary point b within the triangle, as \nillustrated in Figure 11.35.\nBut how can we calculate a LERP blend between three animation clips? \nThankfully, the answer is simple: the LERP function can actually operate on \nany number of inputs, because it is really just a weighted average . As with any \nweighted average, the weights must add to one. In the case of a two-input \nLERP blend, we used the weights β and (1 – β), which of course add to one. \nFor a three-input LERP, we simply use three weights, α, β, and γ = (1 – α – β). \nThen we calculate the LERP as follows:\nLERP\n0\n1\n2\n(\n)\n(\n)\n(\n)\n(1\n)(\n) .\nj\nj\nj\nj\n= α\n+ β\n+\n−α−β\nP\nP\nP\nP\n \n(11.13)\nGiven the two-dimensional blend vector b, we ﬁ nd the blend weights α, \nβ, and γ by ﬁ nding the barycentric coordinates of the point b relative to the \ntriangle formed by the three clips in two-dimensional blend space (htt p://\nen.wikipedia.org/wiki/Barycentric_coordinates_%28mathematics%29).  In \ngeneral, the barycentric coordinates of a point b within a triangle with vertices \nb1 , b2 , and b3 are three scalar values (α, β, γ) that satisfy the relations\n \n0\n1\n2\n \n \n \n= α\n+ β\n+γ\nb\nb\nb\nb  \n(11.14)\nand\n \n1.\nα+ β+γ=\nThese are exactly the weights we seek for our three-clip weighted average. \nBarycentric coordinates are illustrated in Figure 11.36.\nNote that plugging the barycentric coordinate (1, 0, 0) into Equation \n(11.14) yields b0 , while (0, 1, 0) gives us b1 and (0, 0, 1) produces b2. Likewise, \nplugging these blend weights into Equation (11.13) gives us poses \n0\n(\n)j\nP\n, \n1\n(\n)j\nP\n, \nClip A\nb0\nb y\nClip B\nClip C\nb\nb1\nb2\nbx\nFinal \nBlend\nFigure 11.35.  Two-dimensional animation blending between three animation clips.\n\n\n535 \nand \n2\n(\n)j\nP\n, respectively. Furthermore, the barycentric coordinate (⅓, ⅓, ⅓) lies \nat the centroid of the triangle and gives us an equal blend between the three \nposes. This is exactly what we’d expect.\n11.6.3.4. Generalized Two-Dimensional LERP Blending\nThe barycentric coordinate technique can be extended to an arbitrary number \nof animation clips positioned at arbitrary locations within the two-dimension-\nal blend space. We won’t describe it in its entirety here, but the basic idea is \nto use a technique known as Delaunay triangulation (htt p://en.wikipedia.org/\nwiki/Delaunay_triangulation) to ﬁ nd a set of triangles given the positions of \nthe various animation clips bi . Once the triangles have been determined, we \ncan ﬁ nd the triangle that encloses the desired point b and then perform a \nthree-clip LERP blend as described above. This is shown in Figure 11.37.\nb0\nby\nb\nb1\nb2\nα\nβ\nγ\nbx\nFigure 11.36.  Various barycentric coordinates within a triangle.\nClip A\nb0\nClip B\nb1\nClip C\nClip D\nClip E\nClip F\nClip G\nClip H\nClip I\nClip J\nb2\nb3\nb4\nb5\nb6\nb7\nb8\nb9\nby\nbx\nFigure 11.37.  Delaunay triangulation between an arbitrary number of animation clips \npositioned at arbitrary locations in two-dimensional blend space.\n11.6. Animation Blending\n\n\n536 \n11. Animation Systems\n11.6.4. Partial-Skeleton Blending\nA human being can control diﬀ erent parts of his or her body independently. \nFor example, I can wave my right arm while walking and pointing at some-\nthing with my left  arm. One way to implement this kind of movement in a \ngame is via a technique known as partial-skeleton blending .\nRecall from Equations (11.5) and (11.6) that when doing regular LERP \nblending, the same blend percentage β was used for every joint in the skeleton. \nPartial-skeleton blending extends this idea by permitt ing the blend percent-\nage to vary on a per-joint basis. In other words, for each joint j, we deﬁ ne a \nseparate blend percentage βj. The set of all blend percentages for the entire \nskeleton { }\n1\n0\n \n \nN\nj\nj\n−\n=\nβ\n is sometimes called a blend mask because it can be used to \n“mask out” certain joints by sett ing their blend percentages to zero.\nAs an example, let’s say we want our character to wave at someone using \nhis right arm and hand. Moreover, we want him to be able to wave whether \nhe’s walking, running, or standing still. To implement this using partial blend-\ning, the animator deﬁ nes three full-body animations: Walk, Run, and Stand. \nThe animator also creates a single waving animation, Wave. A blend mask is \ncreated in which the blend percentages are zero everywhere except for the \nright shoulder, elbow, wrist, and ﬁ nger joints, where they are equal to one:\n \n1,\n right arm,\n0,\notherwise.\nj\nj\n⎧\n∈\n⎨\nβ =\n⎩\n \nWhen Walk, Run, or Stand is LERP-blended with Wave using this blend mask, \nthe result is a character who appears to be walking, running, or standing while \nwaving his right arm.\nPartial blending is useful, but it has a tendency to make a character’s \nmovement look unnatural. This occurs for two basic reasons:\nAn abrupt change in the per-joint blend factors can cause the movements \n• \nof one part of the body to appear disconnected from the rest of the body. \nIn our example, the blend factors change abruptly at the right shoulder \njoint. Hence the animation of the upper spine, neck, and head are being \ndriven by one animation, while the right shoulder and arm joints are \nbeing entirely driven by a diﬀ erent animation. This can look odd. The \nproblem can be mitigated somewhat by gradually changing the blend \nfactors rather than doing it abruptly. (In our example, we might select a \nblend percentage of 0.9 at the right shoulder, 0.5 on the upper spine, and \n0.2 on the neck and mid-spine.)\nThe movements of a real human body are never totally independent. \n• \nFor example, one would expect a person’s wave to look more “bouncy” \n\n\n537 \nand out of control when he or she is running than when he or she is \nstanding still. Yet with partial blending, the right arm’s animation will \nbe identical no matt er what the rest of the body is doing. This problem is \ndiﬃ  cult to overcome using partial blending. Instead, many game devel-\nopers have recently turned to a more natural-looking technique known \nas additive blending.\n11.6.5. Additive Blending \nAdditive blending approaches the problem of combining animations in a to-\ntally new way. It introduces a new kind of animation called a diﬀ erence clip, \nwhich, as its name implies, represents the diﬀ erence between two regular ani-\nmation clips. A diﬀ erence clip can be added onto a regular animation clip in \norder to produce interesting variations in the pose and movement of the char-\nacter. In essence, a diﬀ erence clip encodes the changes that need to be made to \none pose in order to transform it into another pose. Diﬀ erence clips are oft en \ncalled additive animation clips in the game industry. We’ll stick with the term \ndiﬀ erence clip in this book because it more accurately describes what is going \non.\nConsider two input clips called the source clip (S) and the reference clip (R). \nConceptually, the diﬀ erence clip is D = S – R. If a diﬀ erence clip D is added to \nits original reference clip, we get back the source clip (S = D + R). We can also \ngenerate animations that are partway between R and S by adding a percent-\nage of D to R, in much the same way that LERP blending ﬁ nds intermediate \nanimations between two extremes. However, the real beauty of the additive \nblending technique is that once a diﬀ erence clip has been created, it can be \nadded to other unrelated clips, not just to the original reference clip. We’ll call \nthese animations target clips and denote them with the symbol T.\nAs an example, if the reference clip has the character running normally \nand the source clip has him running in a tired manner, then the diﬀ erence clip \nwill contain only the changes necessary to make the character look tired while \nrunning. If this diﬀ erence clip is now applied to a clip of the character walk-\ning, the resulting animation can make the character look tired while walking. \nA whole host of interesting and very natural-looking animations can be cre-\nated by adding a single diﬀ erence clip onto various “regular” animation clips, \nor a collection of diﬀ erence clips can be created, each of which produces a \ndiﬀ erent eﬀ ect when added to a single target animation.\n11.6.5.1. Mathematical Formulation\nA diﬀ erence animation D is deﬁ ned as the diﬀ erence between some source \nanimation S and some reference animation R. So conceptually, the diﬀ erence \n11.6. Animation Blending\n\n\n538 \n11. Animation Systems\npose (at a single point in time) is D = S – R. Of course, we’re dealing with joint \nposes, not scalar quantities, so we cannot simply subtract the poses. In gen-\neral, a joint pose is a 4 × 4 aﬃ  ne transformation matrix \nC\nP\n→\nP\n that transforms \npoints and vectors from the child joint’s local space to the space of its parent \njoint. The matrix equivalent of subtraction is multiplication by the inverse ma-\ntrix. So given the source pose Sj and the reference pose Rj for any joint j in the \nskeleton, we can deﬁ ne the diﬀ erence pose Dj at that joint as follows (for this \ndiscussion, we’ll drop the C→P or j→p(j) subscript, as it is understood that we \nare dealing with child-to-parent pose matrices):\n \n1.\nj\nj\nj\n−\n=\nD\nS R\n \n “Adding” a diﬀ erence pose Dj onto a target pose Tj yields a new additive \npose Aj. This is achieved by simply concatenating the diﬀ erence transform \nand the target transform as follows:\n \n1\n(\n)\n.\nj\nj\nj\nj\nj\nj\n−\n=\n=\nA\nD T\nS R\nT  \n(11.15)\nWe can verify that this is correct by looking at what happens when the diﬀ er-\nence pose is “added” back onto the original reference pose:\n \n1\n.\nj\nj\nj\nj\nj\nj\nj\n−\n=\n=\n=\nA\nD R\nS R\nR\nS\nIn other words, adding the diﬀ erence animation D back onto the original ref-\nerence animation R yields the source animation S, as we’d expect.\nTemporal Interpolation of Difference Clips\nAs we learned in Section 11.4.1.1, game animations are almost never sampled \non integer frame indices. To ﬁ nd a pose at an arbitrary time t, we must oft en \ntemporally interpolate between adjacent pose samples at times t1 and t2. Thank-\nfully, diﬀ erence clips can be temporally interpolated just like their non-addi-\ntive counterparts. We can simply apply Equations (11.10) and (11.11) directly \nto our diﬀ erence clips as if they were ordinary animations.\nNote that a diﬀ erence animation can only be found when the input clips \nS and R are of the same duration. Otherwise there would be a period of time \nduring which either S or R is undeﬁ ned, meaning D would be undeﬁ ned as \nwell.\nAdditive Blend Percentage\nIn games, we oft en wish to blend in only a percentage of a diﬀ erence anima-\ntion to achieve varying degrees of the eﬀ ect it produces. For example, if a \ndiﬀ erence clip causes the character to turn his head 80 degrees to the right, \n\n\n539 \nblending in 50% of the diﬀ erence clip should make him turn his head only \n40 degrees to the right.\nTo accomplish this, we turn once again to our old friend LERP. We wish \nto interpolate between the unaltered target animation and the new animation \nthat would result from a full application of the diﬀ erence  animation. To do \nthis, we extend Equation (11.15) as follows: \nLERP(\n,  \n,  )\n(1\n)(\n)\n(\n).\nj\nj\nj\nj\nj\nj\nj\n=\nβ\n=\n−β\n+ β\nA\nT\nD T\nT\nD T\n \n(11.16)\nAs we saw in Chapter 4, we cannot LERP matrices directly. So Equation \n(11.16) must be broken down into three separate interpolations for S, Q, and T, \njust as we did in Equations (11.7), (11.8), and (11.9).\n11.6.5.2. Additive Blending Versus Partial Blending\nAdditive blending is similar in some ways to partial blending. For example, \nwe can take the diﬀ erence between a standing clip and a clip of standing while \nwaving the right arm. The result will be almost the same as using a partial \nblend to make the right arm wave. However, additive blends suﬀ er less from \nthe “disconnected” look of animations combined via partial blending. This \nis because, with an additive blend, we are not replacing the animation for \na subset of joints or interpolating between two potentially unrelated poses. \nRather, we are adding movement to the original animation—possibly across \nthe entire skeleton. In eﬀ ect, a diﬀ erence animation “knows” how to change a \ncharacter’s pose in order to get him to do something speciﬁ c, like being tired, \naiming his head in a certain direction, or waving his arm. These changes can \nbe applied to a wide variety of animations, and the result oft en looks very \nnatural.\n11.6.5.3. Limitations of Additive Blending\n Of course, additive animation is not a silver bullet. Because it adds movement \nto an existing animation, it can have a tendency to over-rotate the joints in the \nskeleton, especially when multiple diﬀ erence clips are applied simultaneous-\nly. As a simple example, imagine a target animation in which the character’s \nleft  arm is bent at a 90 degree angle. If we add a diﬀ erence animation that also \nrotates the elbow by 90 degrees, then the net eﬀ ect would be to rotate the arm \nby 90 + 90 = 180 degrees. This would cause the lower arm to interpenetrate the \nupper arm—not a comfortable position for most individuals!\nClearly we must be careful when selecting the reference clip and also \nwhen choosing the target clips to which to apply it. Here are some simple \nrules of thumb:\n11.6. Animation Blending\n\n\n540 \n11. Animation Systems\nKeep hip rotations to a minimum in the reference clip.\n• \nThe shoulder and elbow joints should usually be in neutral poses in the \n• \nreference clip to minimize over-rotation of the arms when the diﬀ erence \nclip is added to other targets.\nAnimators should create a new diﬀ erence animation for each core pose \n• \n(e.g., standing upright, crouched down, lying prone, etc.). This allows \nthe animator to account for the way in which a real human would move \nwhen in each of these stances.\nThese rules of thumb can be a helpful starting point, but the only way to \nreally learn how to create and apply diﬀ erence clips is by trial and error or by \napprenticing with animators or engineers who have experience creating and \napplying diﬀ erence animations. If your team hasn’t used additive blending in \nthe past, expect to spend a signiﬁ cant amount of time learning the art of ad-\nditive blending.\n11.6.6. Applications of Additive Blending\n11.6.6.1. Stance Variation\nOne particularly striking application of additive blending is stance variation . \nFor each desired stance, the animator creates a one-frame diﬀ erence anima-\ntion. When one of these single-frame clips is additively blended with a base \nanimation, it causes the entire stance of the character to change drastically \nwhile he continues to perform the fundamental action he’s supposed to per-\nform. This idea is illustrated in Figure 11.38.\nTarget +\nDifference A\nTarget +\nDifference B\nTarget Clip\n(and Reference)\nFigure 11.38.  Two single-frame difference animations A and B can cause a target animation \nclip to assume two totally different stances. (Character from Naughty Dog’s Uncharted: \nDrake’s Fortune.)\n\n\n541 \nTarget Clip\n(and Reference)\nTarget +\nDifference A\nTarget +\nDifference B\nTarget +\nDifference C\nFigure 11.39.  Additive blends can be used to add variation to a repetitive idle animation. \nImages courtesy of Naughty Dog Inc.\n11.6.6.2. Locomotion Noise\n Real humans don’t run exactly the same way with every footfall—there is \nvariation in their movement over time. This is especially true if the person \nis distracted (for example, by att acking enemies). Additive blending can be \nused to layer randomness, or reactions to distractions, on top of an otherwise \nentirely repetitive locomotion cycle. This is illustrated in Figure 11.39.\n11.6.6.3. Aim and Look-At\nAnother common use for additive blending is to permit the character to look \naround or to aim his weapon. To accomplish this, the character is ﬁ rst ani-\nmated doing some action, such as running, with his head or weapon facing \nstraight ahead. Then the animator changes the direction of the head or the \naim of the weapon to the extreme right and saves oﬀ  a one-frame or multi-\nframe diﬀ erence animation. This process is repeated for the extreme left , up, \nand down directions. These four diﬀ erence animations can then be additively \nblended onto the original straight ahead animation clip, causing the character \nto aim right, left , up, down, or anywhere in between.\nThe angle of the aim is governed by the additive blend factor of each clip. \nFor example, blending in 100 percent of the right additive causes the character \n11.6. Animation Blending\n\n\n542 \n11. Animation Systems\nto aim as far right as possible. Blending 50 percent of the left  additive causes \nhim to aim at an angle that is one-half of his left most aim. We can also combine \nthis with an up or down additive to aim diagonally. This is demonstrated in \nFigure 11.40.\n11.6.6.4. Overloading the Time Axis\nIt’s interesting to note that the time axis of an animation clip needn’t be used \nto represent time. For example, a three-frame animation clip could be used to \nprovide three aim poses to the engine—a left  aim pose on frame 1, a forward \naim pose on frame 2, and a right aim pose on frame 3. To make the character \naim to the right, we can simply ﬁ x the local clock of the aim animation on \nframe 3. To perform a 50% blend between aiming forward and aiming right, \nwe can dial in frame 2.5. This is a great example of leveraging existing features \nof the engine for new purposes.\n11.7. Post-Processing\nOnce a skeleton as been posed by one or more animation clips and the results \nhave been blended together using linear interpolation or additive blending, it \nis oft en necessary to modify the pose prior to rendering the character. This is \ncalled animation post-processing . In this section, we’ll look at a few of the most \ncommon kinds of animation post-processing.\nTarget +\nDifference Right\nTarget +\nDifference Left\nTarget Clip\n(and Reference)\n0% Right\n0% Left\n100% Right\n100% Left\nFigure 11.40.  Additive blending can be used to aim a weapon. Screenshots courtesy of \nNaughty Dog Inc.\n\n\n543 \n11.7.1. Procedural Animations\nA procedural animation is any animation generated at runtime rather than be-\ning driven by data exported from an animation tool such as Maya. Sometimes, \nhand-animated clips are used to pose the skeleton initially, and then the pose \nis modiﬁ ed in some way via procedural animation as a post-processing step. \nA procedural animation can also be used as an input to the system in place of \na hand-animated clip.\nFor example, imagine that a regular animation clip is used to make a ve-\nhicle appear to be bouncing up and down on the terrain as it moves. The \ndirection in which the vehicle travels is under player control. We would like \nto adjust the rotation of the front wheels and steering wheel so that they move \nconvincingly when the vehicle is turning. This can be done by post-processing \nthe pose generated by the animation. Let’s assume that the original animation \nhas the front tires pointing straight ahead and the steering wheel in a neutral \nposition. We can use the current angle of turn to create a quaternion about the \nvertical axis that will deﬂ ect the front tires by the desired amount. This quater-\nnion can be multiplied with the front tire joints’ Q channel to produce the ﬁ nal \npose of the tires. Likewise, we can generate a quaternion about the axis of the \nsteering column and multiply it in to the steering wheel joint’s Q channel to \ndeﬂ ect it. These adjustments are made to the local pose, prior to global pose \ncalculation and matrix palett e generation.\nAs another example, let’s say that we wish to make the trees and bushes in \nour game world sway naturally in the wind and get brushed aside when char-\nacters move through them. We can do this by modeling the trees and bushes as \nskinned meshes with simple skeletons. Procedural animation can be used, in \nplace of or in addition to hand-animated clips, to cause the joints to move in a \nnatural-looking way. We might apply one or more sinusoids to the rotation of \nvarious joints to make them sway in the breeze, and when a character moves \nthrough a region containing a bush or grass, we can deﬂ ect its root joint quater-\nnion radially outward to make it appear to be pushed over by the character.\n11.7.2. Inverse Kinematics\nLet’s say we have an animation clip in which a character leans over to pick up \nan object from the ground. In Maya, the clip looks great, but in our production \ngame level, the ground is not perfectly ﬂ at, so sometimes the character’s hand \nmisses the object or appears to pass through it. In this case, we would like to \nadjust the ﬁ nal pose of the skeleton so that the hand lines up exactly with the \ntarget object. A technique known as inverse kinematics (IK) can be used to make \nthis happen.\n11.7. Post-Processing\n\n\n544 \n11. Animation Systems\nA regular animation clip is an example of forward kinematics (FK). In for-\nward kinematics, the input is a set of local joint poses, and the output is a \nglobal pose and a skinning matrix for each joint. Inverse kinematics goes in \nthe other direction: The input is the desired global pose of a single joint, which \nis known as the end eﬀ ector . We solve for the local poses of other joints in the \nskeleton that will bring the end eﬀ ector to the desired location.\nMathematically, IK boils down to an error minimization problem. As with \nmost minimization problems, there might be one solution, many, or none at \nall. This makes intuitive sense: If I try to reach a doorknob that is on the other \nside of the room, I won’t be able to reach it without walking over to it. IK \nworks best when the skeleton starts out in a pose that is reasonably close to the \ndesired target. This helps the algorithm to focus in on the “closest” solution \nand to do so in a reasonable amount of processing time. Figure 11.41 shows \nIK in action.\nImagine a two-joint skeleton, each of which can rotate only about a single \naxis. The rotation of these two joints can be described by a two-dimensional \nangle vector θ = [ θ1  θ2 ]. The set of all possible angles for our two joints forms \na two-dimensional space called conﬁ guration space . Obviously, for more-com-\nplex skeletons with more degrees of freedom per joint, conﬁ guration space be-\ncomes multidimensional, but the concepts described here work equally well \nno matt er how many dimensions we have.\nNow imagine plott ing a three-dimensional graph, where for each combi-\nnation of joint rotations (i.e., for each point in our two-dimensional conﬁ gura-\ntion space), we plot the distance from the end eﬀ ector to the desired target. \nAn example of this kind of plot is shown in Figure 11.42. The “valleys” in \nthis three-dimensional surface represent regions in which the end eﬀ ector is \nas close as possible to the target. When the height of the surface is zero, the \nend eﬀ ector has reached its target. Inverse kinematics, then, att empts to ﬁ nd \nminima (low points) on this surface.\nTarget\nPose \nAfter IK\nOriginal \nPose\nEnd \nEffector\nFigure 11.41.  Inverse kinematics attempts to bring an end effector joint into a target global \npose by minimizing the error between them.\n\n\n545 \nWe won’t get into the details of solving the IK minimization problem here. \nYou can read more about IK at htt p://en.wikipedia.org/wiki/Inverse_kinemat-\nics and in Jason Weber’s article, “Constrained Inverse Kinematics,” in [40].\n11.7.3. Rag Dolls\nA character’s body goes limp when he dies or becomes unconscious. In such \nsituations, we want the body to react in a physically realistic way with its \nsurroundings. To do this, we can use a rag doll . A rag doll is a collection of \nphysically simulated rigid bodies , each one representing a semi-rigid part of \nthe character’s body, such as his lower arm or his upper leg. The rigid bodies \nare constrained to one another at the joints of the character in such a way as to \nproduce natural-looking “lifeless” body movement. The positions and orien-\ntations of the rigid bodies are determined by the physics system and are then \nused to drive the positions and orientations of certain key joints in the charac-\nter’s skeleton. The transfer of data from the physics system to the skeleton is \ntypically done as a post-processing step.\nTo really understand rag doll physics, we must ﬁ rst have an understand-\ning of how the collision and physics systems work. Rag dolls are covered in \nmore detail in Sections 12.4.8.7 and 12.5.3.8.\n11.8. Compression Techniques\nAnimation data can take up a lot of memory. A single joint pose might be \ncomposed of ten ﬂ oating-point channels (three for translation, four for rota-\ntion, and up to three more for scale). Assuming each channel contains a four-\nθ 1\nθ 2\ndtarget\nMinimum\nFigure 11.42.  A three-dimensional plot of the distance from the end effector to the target for \neach point in two-dimensional conﬁ guration space. IK ﬁ nds the local minimum.\n11.8. Compression Techniques\n\n\n546 \n11. Animation Systems\nbyte ﬂ oating-point value, a one-second clip sampled at 30 samples per second \nwould occupy 4 bytes × 10 channels × 30 samples/second = 1200 bytes per joint \nper second, or a data rate of about 1.17 kB per joint per second. For a 100-joint \nskeleton (which is small by today’s standards), an uncompressed animation \nclip would occupy 117 kB per joint per second. If our game contained 1000 \nseconds of animation (which is on the low side for a modern game), the entire \ndata set would occupy a whopping 114.4 MB. That’s probably more than most \ngames can spare, considering that a PLAYSTATION 3 has only 256 MB of main \nRAM and 256 MB of video RAM. Therefore, game engineers invest a signiﬁ -\ncant amount of eﬀ ort into compressing animation data in order to permit the \nmaximum richness and variety of movement at the minimum memory cost.\n11.8.1. Channel Omission\nOne simple way to reduce the size of an animation clip is to omit channels \nthat are irrelevant. Many characters do not require nonuniform scaling, so the \nthree scale channels can be reduced to a single uniform scale channel. In some \ngames, the scale channel can actually be omitt ed altogether for all joints (ex-\ncept possibly the joints in the face). The bones of a humanoid character gener-\nally cannot stretch, so translation can oft en be omitt ed for all joints except the \nroot, the facial joints, and sometimes the collar bones. Finally, because quater-\nnions are always normalized, we can store only three components per quat \n(e.g., x, y, and z) and reconstruct the fourth component (e.g., w) at runtime.\nAs a further optimization, channels whose pose does not change over the \ncourse of the entire animation can be stored as a single sample at time t = 0 plus \na single bit indicating that the channel is constant for all other values of t.\nChannel omission can signiﬁ cantly reduce the size of an animation clip. \nA 100-joint character with no scale and no translation requires only 303 chan-\nnels—three channels for the quaternions at each joint, plus three channels for \nthe root joint’s translation. Compare this to the 1,000 channels that would be \nrequired if all ten channels were included for all 100 joints.\n11.8.2. Quantization\nAnother way to reduce the size of an animation is to reduce the size of each \nchannel. A ﬂ oating-point value is normally stored in 32-bit IEEE format. This \nformat provides 23 bits of precision in the mantissa and an 8-bit exponent. \nHowever, it’s oft en not necessary to retain that kind of precision and range in \nan animation clip. When storing a quaternion, the channel values are guaran-\nteed to lie in the range [–1, 1]. At a magnitude of 1, the exponent of a 32-bit \nIEEE ﬂ oat is zero, and 23 bits of precision give us accuracy down to the sev-\nenth decimal place. Experience shows that a quaternion can be encoded well \n",
      "page_number": 549,
      "chapter_number": 28,
      "summary": "This chapter covers segment 28 (pages 549-568). Key topics include animations, animator, and animating.",
      "keywords": [
        "diﬀ erence clip",
        "clip",
        "animation clips",
        "animation",
        "LERP blending",
        "blend",
        "animation blending",
        "diﬀ erence",
        "diﬀ erence animation",
        "erence clip",
        "additive blending",
        "pose",
        "LERP",
        "Animation Systems",
        "Two-Dimensional LERP Blending"
      ],
      "concepts": [
        "animations",
        "animator",
        "animating",
        "blend",
        "clips",
        "pose",
        "posed",
        "joint",
        "targeted",
        "look"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 7",
          "chapter": 46,
          "title": "Segment 46 (pages 443-452)",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 50,
          "title": "Segment 50 (pages 487-494)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "makinggames",
          "chapter": 6,
          "title": "Segment 6 (pages 43-50)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 48,
          "title": "Segment 48 (pages 469-476)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 47,
          "title": "Segment 47 (pages 453-467)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 569-589)",
      "start_page": 569,
      "end_page": 589,
      "detection_method": "synthetic",
      "content": "547 \nwith only 16 bits of precision, so we’re really wasting 16 bits per channel if we \nstore our quats using 32-bit ﬂ oats.\nConverting a 32-bit IEEE ﬂ oat into an n-bit integer representation is called \nquantization . There are actually two components to this operation: Encoding \nis the process of converting the original ﬂ oating-point value to a quantized \ninteger representation. Decoding is the process of recovering an approxima-\ntion to the original ﬂ oating-point value from the quantized integer. (We can \nonly recover an approximation to the original data—quantization is a lossy com-\npression method because it eﬀ ectively reduces the number of bits of precision \nused to represent the value.)\nTo encode a ﬂ oating-point value as an integer, we ﬁ rst divide the valid \nrange of possible input values into N equally sized intervals. We then deter-\nmine within which interval a particular ﬂ oating-point value lies and represent \nthat value by the integer index of its interval. To decode this quantized value, \nwe simply convert the integer index into ﬂ oating-point format and shift  and \nscale it back into the original range. N is usually chosen to correspond to the \nrange of possible integer values that can be represented by an n-bit integer. \nFor example, if we’re encoding a 32-bit ﬂ oating-point value as a 16-bit integer, \nthe number of intervals would be N = 216 = 65,536.\nJonathan Blow wrote an excellent article on the topic of ﬂ oating-point sca-\nlar quantization in the Inner Product column of Game Developer  Magazine, \navailable at htt p://number-none.com/product/Scalar%20Quantization/index.\nhtml. (Jonathan’s source code is also available at htt p://www.gdmag.com/\nsrc/jun02.zip.) The article presents two ways to map a ﬂ oating-point value \nto an interval during the encoding process: We can either truncate the ﬂ oat \nto the next lowest interval boundary (T encoding), or we can round the ﬂ oat \nto the center of the enclosing interval (R encoding). Likewise, it describes two \napproaches to reconstructing the ﬂ oating-point value from its integer repre-\nsentation: We can either return the value of the left hand side of the interval to \nwhich our original value was mapped (L reconstruction), or we can return the \nvalue of the center of the interval (C reconstruction). This gives us four possible \nencode/decode methods: TL, TC, RL, and RC. Of these, TL and RC are to be \navoided because they tend to remove or add energy to the data set, which can \noft en have disastrous eﬀ ects. TC has the beneﬁ t of being the most eﬃ  cient \nmethod in terms of bandwidth, but it suﬀ ers from a severe problem—there \nis no way to represent the value zero exactly. (If you encode 0.0f, it becomes \na small positive value when decoded.) RL is therefore usually the best choice \nand is the method we’ll demonstrate here.\nThe article only talks about quantizing positive ﬂ oating-point values, and \nin the examples, the input range is assumed to be [0, 1] for simplicity. Howev-\n11.8. Compression Techniques\n\n\n548 \n11. Animation Systems\ner, we can always shift  and scale any ﬂ oating-point range into the range [0, 1]. \nFor example, the range of quaternion channels is [–1, 1], but we can convert \nthis to the range [0, 1] by adding one and then dividing by two.\nThe following pair of routines encode and decode an input ﬂ oating-point \nvalue lying in the range [0, 1] into an n-bit integer, according to Jonathan \nBlow’s RL method. The quantized value is always returned as a 32-bit un-\nsigned integer (U32), but only the least-signiﬁ cant n bits are actually used, as \nspeciﬁ ed by the nBits argument. For example, if you pass nBits==16, you \ncan safely cast the result to a U16.\nU32 CompressUnitFloatRL(F32 unitFloat, U32 nBits)\n{\n \n// Determine the number of intervals based on the  \n \n \n// number of output bits we’ve been asked to produce.\n U32 \nnIntervals = 1u << nBits;\n \n// Scale the input value from the range [0, 1] into   \n \n// the range [0, nIntervals – 1]. We subtract one  \n \n \n// interval because we want the largest output value  \n \n// to fit into nBits bits.\n F32 \nscaled = unitFloat * (F32)(nIntervals – 1u);\n \n// Finally, round to the nearest interval center. We   \n \n// do this by adding 0.5f, and then truncating to the  \n \n// next-lowest interval index (by casting to U32).\n U32 \nrounded = (U32)(scaled * 0.5f);\n \n// Guard against invalid input values.\nif (rounded > nIntervals – 1u)\n \n \nrounded = nIntervals – 1u;\n return \nrounded;\n}\nF32 DecompressUnitFloatRL(U32 quantized, U32 nBits)\n{\n \n// Determine the number of intervals based on the  \n \n \n// number of bits we used when we encoded the value.\n U32 \nnIntervals = 1u << nBits;\n \n// Decode by simply converting the U32 to an F32, and\n \n// scaling by the interval size.\n F32 \nintervalSize = 1.0f / (F32)(nIntervals – 1u);\n F32 \napproxUnitFloat = (F32)quantized * intervalSize;\n return \napproxUnitFloat;\n}\n\n\n549 \nTo handle arbitrary input values in the range [min, max], we can use these \nroutines:\nU32 CompressFloatRL(F32 value, F32 min, F32 max,\n      U32 \nnBits)\n{\n \nF32 unitFloat = (value - min) / (max – min);\n \nU32 quantized = CompressUnitFloatRL(unitFloat,  \n \n \n  nBits);\n \nreturn quantized;\n}\nF32 DecompressFloatRL(U32 quantized, F32 min, F32 max,\n                      U32 nBits)\n{\n \nF32 unitFloat = DecompressUnitFloatRL(quantized,   \n \n  nBits);\n \nF32 value = min + (unitFloat * (max – min));\n \nreturn value;\n}\nLet’s return to our original problem of animation channel compression. \nTo compress and decompress a quaternion’s four components into 16 bits per \nchannel, we simply call CompressFloatRL() and DecompressFloatRL()\nwith min = –1, max = 1, and n = 16:\ninline U16 CompressRotationChannel(F32 qx)\n{\n \nreturn (U16)CompressFloatRL(qx, -1.0f, 1.0f, 16u);\n}\ninline F32 DecompressRotationChannel(U16 qx)\n{\n return \nDecompressFloatRL((U32)qx, -1.0f, 1.0f, 16u);\n}\nCompression of translation channels is a bit trickier than rotations, be-\ncause unlike quaternion channels, the range of a translation channel could \ntheoretically be unbounded. Thankfully, the joints of a character don’t move \nvery far in practice, so we can decide upon a reasonable range of motion and \nﬂ ag an error if we ever see an animation that contains translations outside the \nvalid range. In-game cinematics are an exception to this rule—when an IGC \nis animated in world space, the translations of the characters’ root joints can \ngrow very large. To address this, we can select the range of valid translations \non a per-animation or per-joint basis, depending on the maximum transla-\ntions actually achieved within each clip. Because the data range might diﬀ er \n11.8. Compression Techniques\n\n\n550 \n11. Animation Systems\nfrom animation to animation, or from joint to joint, we must store the range \nwith the compressed clip data. This will add data to each animation, so it may \nor may not be worth the trade-oﬀ .\n// We’ll use a 2 meter range -- your mileage may vary.\nF32 MAX_TRANSLATION = 2.0f;\ninline U16 CompressTranslationChannel(F32 vx)\n{\n \n// Clamp to valid range...\n \nif (value < -MAX_TRANSLATION)\n \n  \nvalue = -MAX_TRANSLATION;\n \nif (value > MAX_TRANSLATION)\n \n  \nvalue = MAX_TRANSLATION;\n \nreturn (U16)CompressFloatRL(vx,\n \n  \n-MAX_TRANSLATION, MAX_TRANSLATION, 16);\n}\ninline F32 DecompressTranslationChannel(U16 vx)\n{\n return \nDecompressFloatRL((U32)vx,\n \n  \n-MAX_TRANSLATION, MAX_TRANSLATION, 16);\n}\n11.8.3. Sampling Frequency and Key Omission\nAnimation data tends to be large for three reasons: ﬁ rst, because the pose of \neach joint can contain upwards of ten channels of ﬂ oating-point data; second, \nbecause a skeleton contains a large number of joints (100 or more for a human-\noid character); third, because the pose of the character is typically sampled \nat a high rate (e.g., 30 frames per second). We’ve seen some ways to address \nthe ﬁ rst problem. We can’t really reduce the number of joints for our high-\nresolution characters, so we’re stuck with the second problem. To att ack the \nthird problem, we can do two things:\nReduce the sample rate overall.\n• \n Some animations look ﬁ ne when exported \nat 15 samples per second, and doing so cuts the animation data size in \nhalf.\nOmit some of the samples.\n• \n If a channel’s data varies in an approximately \nlinear fashion during some interval of time within the clip, we can omit \nall of the samples in this interval except the endpoints. Then, at runtime, \nwe can use linear interpolation to recover the dropped samples.\nThe latt er technique is a bit involved, and it requires us to store informa-\ntion about the time of each sample. This additional data can erode the savings \n\n\n551 \nwe achieved by omitt ing samples in the ﬁ rst place. However, some game en-\ngines have used this technique successfully.\n11.8.4. Curve-Based Compression\n One of the most powerful, easiest-to-use, and best thought-out animation \nAPIs I’ve ever worked with is Granny , by Rad Game Tools. Granny stores \nanimations not as a regularly spaced sequence of pose samples but as a collec-\ntion of nth-order  nonuniform, nonrational B-splines, describing the paths of a \njoint’s S, Q, and T channels over time. Using B-splines allows channels with a \nlot of curvature to be encoded using only a few data points.\nGranny exports an animation by sampling the joint poses at regular in-\ntervals, much like traditional animation data. For each channel, Granny then \nﬁ ts a set of B-splines to the sampled data set to within a user-speciﬁ ed toler-\nance. The end result is an animation clip that is usually signiﬁ cantly smaller \nthan its uniformly sampled, linearly interpolated counterpart. This process is \nillustrated in Figure 11.43.\nB-spline \nsegment 1\nt\nQx1\nSamples\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nB-spline \nsegment 2\nFigure 11.43.  One form of animation compression ﬁ ts B-splines to the animation channel \ndata.\n11.8.5. Selective Loading and Streaming \n The cheapest animation clip is the one that isn’t in memory at all. Most games \ndon’t need every animation clip to be in memory simultaneously. Some clips \napply only to certain classes of character, so they needn’t be loaded during lev-\nels in which that class of character is never encountered. Other clips apply to \none-oﬀ  moments in the game. These can be loaded or streamed into memory \njust before being needed and dumped from memory once they have played.\nMost games load a core set of animation clips into memory when the game \nﬁ rst boots and keep them there for the duration of the game. These include \nthe player character’s core move set and animations that apply to objects that \nreappear over and over throughout the game, such as weapons or power-ups. \n11.8. Compression Techniques\n\n\n552 \n11. Animation Systems\nAll other animations are usually loaded on an as-needed basis. Some game \nengines load animation clips individually, but many package them together \ninto logical groups that can be loaded and unloaded as a unit.\n11.9. Animation System Architecture\nNow that we understand the theory that underlies a game’s animation system, \nlet’s turn our att ention to how such a system is structured from a soft ware ar-\nchitecture standpoint. We’ll also investigate what kinds of interfaces exist be-\ntween the animation system and the other systems in a typical game engine.\nMost animation systems are comprised of up to three distinct layers:\nAnimation pipeline.\n• \n For each animating character and object in the game, \nthe animation pipeline takes one or more animation clips and corre-\nsponding blend factors as input, blends them together, and generates a \nsingle local skeletal pose as output. It also calculates a global pose for \nthe skeleton, and a palett e of skinning matrices for use by the rendering \nengine. Post-processing hooks are usually provided, which permit the \nlocal pose to be modiﬁ ed prior to ﬁ nal global pose and matrix palett e \ngeneration. This is where inverse kinematics (IK), rag doll physics, and \nother forms of procedural animation are applied to the skeleton.\nAction state machine (ASM).\n• \n The actions of a game character (standing, \nwalking, running, jumping, etc.) are usually best modeled via a ﬁ nite \nstate machine , commonly known as the action state machine (ASM). The \nASM subsystem sits atop the animation pipeline and provides a state-\ndriven animation interface for use by virtually all higher-level game \ncode. It ensures that characters can transition smoothly from state to \nstate. In addition, most animation engines permit diﬀ erent parts of the \ncharacter’s body to be doing diﬀ erent, independent actions simultane-\nously, such as aiming and ﬁ ring a weapon while running. This can be ac-\ncomplished by allowing multiple independent state machines to control \na single character via state layers.\nAnimation controllers.\n• \n In many game engines, the behaviors of a player \nor non-player character are ultimately controlled by a high-level sys-\ntem of animation controllers. Each controller is custom-tailored to man-\nage the character’s behavior when in a particular mode. There might \nbe one controller handling the character’s actions when he is ﬁ ghting \nand moving around out in the open (“run-and-gun” mode), one for \nwhen he is in cover, one for driving a vehicle, one for climbing a lad-\nder, and so on. These high-level animation controllers allow most if not \n\n\n553 \nall of the animation-related code to be encapsulated, allowing top-level \nplayer control or AI logic to remain unclutt ered by animation micro-\nmanagement.\nSome game engines draw the lines between these layers diﬀ erently than \nwe do here. Other engines meld two or more of the layers into a single system. \nHowever, all animation engines need to perform these tasks in one form or an-\nother. In the following sections, we’ll explore animation architecture in terms \nof these three layers, noting in our examples when a particular game engine \ntakes a more or less uniﬁ ed approach.\n11.10. The Animation Pipeline\n The operations performed by the low-level animation engine form a pipeline \nthat transforms its inputs (animation clips and blend speciﬁ cations) into the \ndesired outputs (local and global poses, plus a matrix palett e for rendering). \nThe stages of this pipeline are:\n \n1. Clip decompression and pose extraction . In this stage, each individual clip’s \ndata is decompressed, and a static pose is extracted for the time index in \nquestion. The output of this phase is a local skeletal pose for each input \nclip. This pose might contain information for every joint in the skeleton \n(a full-body pose), for only a subset of joints (a partial pose), or it might be \na diﬀ erence pose for use in additive blending.\n \n2. Pose blending . In this stage, the input poses are combined via full-body \nLERP blending, partial-skeleton LERP blending, and/or additive blend-\ning. The output of this stage is a single local pose for all joints in the \nskeleton. This stage is of course only executed when blending more than \none animation clip together—otherwise the output pose from stage 1 \ncan be used directly.\n \n3. Global pose generation. In this stage, the skeletal hierarchy is walked, and \nlocal joint poses are concatenated in order to generate a global pose for \nthe skeleton.\n \n4. Post-processing . In this optional stage, the local and/or global poses of \nthe skeleton can be modiﬁ ed prior to ﬁ nalization of the pose. Post-pro-\ncessing is used for inverse kinematics , rag doll physics, and other forms \nof procedural animation adjustment.\n \n5. Recalculation of global poses. Many types of post-processing require glob-\nal pose information as input but generate local poses as output. Aft er \nsuch a post-processing step has run, we must recalculate the global pose \n11.10. The Animation Pipeline\n\n\n554 \n11. Animation Systems\nfrom the modiﬁ ed local pose. Obviously, a post-processing operation \nthat does not require global pose information can be done between stag-\nes 2 and 3, thus avoiding the need for global pose recalculation.\n \n6. Matrix palett e generation. Once the ﬁ nal global pose has been generated, \neach joint’s global pose matrix is multiplied by the corresponding in-\nverse bind pose matrix. The output of this stage is a palett e of skinning \nmatrices suitable for input to the rendering engine.\nA typical animation pipeline is depicted in Figure 11.44.\n11.10.1. Data Structures\nEvery animation pipeline is architected diﬀ erently, but most operate in terms \nof data structures that are similar to the ones described in this section.\n11.10.1.1. Shared Resource Data\nAs with all game engine systems, a strong distinction must be made between \nshared resource data and per-instance state information. Each individual character \nor object in the game has its own per-instance data structures, but characters \nor objects of the same type typically share a single set of resource data. This \nshared data typically includes the following:\nSkeleton\n• \n . The skeleton describes the joint hierarchy and its bind pose.\nSkinned meshes\n• \n . One or more meshes can be skinned to a single skeleton. \nEach vertex within a skinned mesh contains the indices of one or more \nOutputs\nInputs\nDecompression\nand\nPose Extraction\nBlend\nSpecification\nPose \nBlending\nSkinning \nMatrix \nCalc.\nGlobal \nPose Calc.\nLocal \nPose\nRendering \nEngine\nMatrix \nPalette\nPost-\nProcessing\nSkeleton\nClip(s)\nLocal \nClock(s)\nGlobal \nPose\nGame Play \nSystems\nFigure 11.44.  A typical animation pipeline.\n\n\n555 \njoints within the skeleton, plus weights governing how much inﬂ uence \neach joint should have on that vertex’s position.\nAnimation clips\n• \n . Many hundreds or even thousands of animation clips \nare created for a character’s skeleton. These may be full-body clips, par-\ntial-skeleton clips, or diﬀ erence clips for use in additive blending.\nA UML diagram of these data structures is shown in Figure 11.45. Pay \nparticular att ention to the cardinality and direction of the relationships between \nthese classes. The cardinality is shown just beside the tip or tail of the relation-\nship arrow between classes—a one represents a single instance of the class, \nwhile an asterisk indicates many instances. For any one type of character, there \nwill be one skeleton, one or more meshes, and one or more animation clips. \nThe skeleton is the central unifying element—the skins are att ached to the \nskeleton but don’t have any relationship with the animation clips. Likewise, \nthe clips are targeted at a particular skeleton, but they have no “knowledge” \nof the skin meshes. Figure 11.46 illustrates these relationships.\nGame designers oft en try to reduce the number of unique skeletons in \nthe game to one, or just a few, because each new skeleton generally requires \na whole new set of animation clips. To provide the illusion of many diﬀ erent \n1\n*\n1\n*\n1\n*\nSkeleton\n-uniqueId : int\n-jointCount : int\n-joints : SkeletonJoint\nSkeletonJoint\n-name : string\n-parentIndex : int\n-invBindPose : Matrix44\n1\n*\n1\n*\n1\n*\nMesh\n-indices : int\n-vertices : Vertex\n-skeletonId : int\nAnimationClip\n-nameId : int\n-duration : float\n-poseSamples : AnimationPose\nVertex\n-position : Vector3\n-normal : Vector3\n-uv : Vector2\n-jointIndices : int\n-jointWeights : float\nSQT\n-scale : Vector3\n-rotation : Quaternion\n-translation : Vector3\nAnimationPose\n-jointPoses : SQT\nFigure 11.45.  UML diagram of shared animation resources.\n11.10. The Animation Pipeline\n\n\n556 \n11. Animation Systems\ntypes of characters, it is usually bett er to create multiple meshes skinned to the \nsame skeleton when possible, so that all of the characters can share a single \nset of animations.\n11.10.1.2. Per-Instance Data\n In most games, multiple instances of each character type can appear on-screen \nat the same time. Every instance of a particular character type needs its own \nprivate data structures, allowing it to keep track of its currently playing ani-\nmation clip(s), a speciﬁ cation of how the clips are to be blended together (if \nthere’s more than one), and its current skeletal pose.\nThere is no one universally accepted way to represent per-instance ani-\nmation data. However, virtually every animation engine keeps track of the \nfollowing pieces of information.\nClip state\n• \n . For each playing clip, the following information is main-\ntained:\nLocal clock\n \n□\n . A clip’s local clock describes the point along its lo-\ncal time line at which its current pose should be extracted. This \nmay be replaced by a global start time in some engines. (A com-\nparison between local and global clocks was provided in Sec-\ntion 11.4.3.)\nPlayback rate\n \n□\n . A clip can be played at an arbitrary rate, denoted R in \nSection 11.4.2.\nBlend speciﬁ cation\n• \n . The blend speciﬁ cation is a description of which ani-\nmation clips are currently playing and how these clips are to be blended \ntogether. The degree to which each clip contributes to the ﬁ nal pose is \nSkeleton\nClip N\n...\nSkin A\nSkin B\nSkin C\nClip 1\nClip 2\nClip 3\nother skeletons...\n...\n...\nFigure 11.46.  Many animation clips and one or more meshes target a single skeleton.\n\n\n557 \ncontrolled by one or more blend weights. There are two primary meth-\nods of describing the set of clips that should be blended together: a ﬂ at \nweighted average approach and a tree of blend nodes. When the tree ap-\nproach is used, the structure of the blend tree is usually treated as a \nshared resource, while the blend weights are stored as part of the per-\ninstance state information.\nPartial-skeleton joint weights.\n• \n If a partial-skeleton blend is to be per-\nformed, the degrees to which each joint should contribute to the ﬁ nal \npose are speciﬁ ed via a set of joint weights . In some animation engines, \nthe joint weights are binary: either a joint contributes or it does not. In \nother engines, the weights can lie anywhere from zero (no contribution) \nto one (full contribution).\nLocal pose\n• \n . This is typically an array of SQT data structures, one per joint, \nholding the ﬁ nal pose of the skeleton in parent-relative format. This ar-\nray might also be reused to store an intermediate pose that serves both \nas the input to and the output of the post-processing stage of the pipe-\nline.\nGlobal pose\n• \n . This is an array of SQTs, or 4 × 4 or 4 × 3 matrices, one per \njoint, that holds the ﬁ nal pose of the skeleton in model-space or world-\nspace format. The global pose may serve as an input to the post-pro-\ncessing stage.\nMatrix palett e\n• \n . This is an array of 4 × 4 or 4 × 3 matrices, one per joint, \ncontaining skinning matrices for input to the rendering engine.\n11.10.2. The Flat Weighted Average Blend Representation\nAll but the most rudimentary game engines support animation blending in \nsome form. This means that at any given time, multiple animation clips may \nbe contributing to the ﬁ nal pose of a character’s skeleton. One simple way to \ndescribe how the currently active clips should be blended together is via a \nweighted a verage .\nIn this approach, every animation clip is associated with a blend weight \nindicating how much it should contribute to the ﬁ nal pose of the charac-\nter. A ﬂ at list of all active animation clips (i.e., clips whose blend weights \nare non-zero) is maintained. To calculate the ﬁ nal pose of the skeleton, we \nextract a pose at the appropriate time index for each of the N active clips. \nThen, for each joint of the skeleton, we calculate a simple N-point weighted \naverage of the translation vectors, rotation quaternions, and scale factors \nextracted from the N active animations. This yields the ﬁ nal pose of the \nskeleton.\n11.10. The Animation Pipeline\n\n\n558 \n11. Animation Systems\nThe equation for the weighted average of a set of N vectors { vi } is as fol-\nlows:\n \n1\n0\navg\n1\n0\n.\nN\ni\ni\ni\nN\ni\ni\nw\nw\n−\n=\n−\n=\n=\n∑\n∑\nv\nv\n \nIf the weights are normalized, meaning they sum to one, then this equation can \nbe simpliﬁ ed to the following:\n \n1\n1\navg\n0\n0\n  when \n1 .\nN\nN\ni\ni\ni\ni\ni\nw\nw\n−\n−\n=\n=\n⎛\n⎞\n⎜\n⎟\n=\n=\n⎜\n⎟\n⎝\n⎠\n∑\n∑\nv\nv\n \nIn the case of N = 2, if we let w1 = β and w0 = (1 – β), the weighted average \nreduces to the familiar equation for the linear interpolation (LERP) between \ntwo vectors:\n \nLERP\nLERP[\n, \n, ]\n(1\n)\n.\nA\nB\nA\nB\n=\nβ\n=\n−β\n+ β\nv\nv\nv\nv\nv\n \nWe can apply this same weighted average formulation equally well to quater-\nnions by simply treating them as four-element vectors.\n11.10.2.1. Example: Ogre3D\nThe Ogre3D animation system works in exactly this way. An Ogre::Entity\nrepresents an instance of a 3D mesh (e.g., one particular character walk-\ning around in the game world). The Entity aggregates an object called \nan Ogre::AnimationStateSet, which in turn maintains a list of \nOgre::AnimationState objects, one for each active animation. The \nOgre::AnimationState class is shown in the code snippet below. (A few \nirrelevant details have been omitt ed for clarity.)\n/** Represents the state of an animation clip and the  \n \n \n weight of its influence on the overall pose of the   \n \n character.\n*/\nclass AnimationState\n{\nprotected:\n    String             mAnimationName; // reference to   \n            \n   // clip\n    Real               mTimePos;       // local clock\n    Real               mWeight;        // blend weight\n    bool               mEnabled;       // is this anim   \n            \n   // running?\n\n\n559 \n    bool               mLoop;          // should the\n            \n    // \nanim loop?\npublic:\n    /// Gets the name of the animation.\n    const String& getAnimationName() const;\n    /// Gets the time position (local clock) for this  \n \n   \n/// anim.\n    Real getTimePosition(void) const;\n    /// Sets the time position (local clock) for this  \n \n  /// anim.\n    void setTimePosition(Real timePos);\n    /// Gets the weight (influence) of this animation\n    Real getWeight(void) const;\n    /// Sets the weight (influence) of this animation\n    void setWeight(Real weight);\n    /// Modifies the time position, adjusting for  \n \n \n     \n/// animation duration. This method loops if looping\n  /// \nis enabled.\n    void addTime(Real offset);\n    /// Returns true if the animation has reached the  \n \n /// end of local time line, and is not looping.\n    bool hasEnded(void) const;\n    /// Returns true if this animation is currently   \n \n  /// \nenabled.\n    bool getEnabled(void) const;\n    /// Sets whether or not this animation is enabled.\n    void setEnabled(bool enabled);\n    /// Sets whether or not this animation should loop.\n    void setLoop(bool loop) { mLoop = loop; }\n    /// Gets whether or not this animation loops.\n    bool getLoop(void) const { return mLoop; }\n};\nEach AnimationState keeps track of one animation clip’s local clock and \nits blend weight. When calculating the ﬁ nal pose of the skeleton for a particu-\nlar Ogre::Entity, Ogre’s animation system simply loops through each active \n11.10. The Animation Pipeline\n\n\n560 \n11. Animation Systems\nAnimationState in its AnimationStateSet. A skeletal pose is extracted \nfrom the animation clip corresponding to each state at the time index speciﬁ ed \nby that state’s local clock. For each joint in the skeleton, an N-point weighted \naverage is then calculated for the translation vectors, rotation quaternions, \nand scales, yielding the ﬁ nal skeletal pose.\nOgre and the Playback Rate\nIt is interesting to note that Ogre has no concept of a playback rate (R). If \nit did, we would have expected to see a data member like this in the \nOgre::AnimationState class:\nReal    mPlaybackRate;\nOf course, we can still make animations play more slowly or more quickly in \nOgre by simply scaling the amount of time we pass to the addTime() func-\ntion, but unfortunately, Ogre does not support animation time scaling out of \nthe box.\n11.10.2.2. Example: Granny\nThe Granny animation system, by Rad Game Tools (htt p://www.radgame-\ntools.com/granny.html), provides a ﬂ at, weighted average animation blend-\ning system similar to Ogre’s. Granny permits any number of animations to be \nplayed on a single character simultaneously. The state of each active animation \nis maintained in a data structure known as a granny_control. Granny cal-\nculates a weighted average to determine the ﬁ nal pose, automatically normal-\nizing the weights of all active clips. In this sense, its architecture is virtually \nidentical to that of Ogre’s animation system. But where Granny really shines \nis in its handling of time. Granny uses the global clock approach discussed in \nSection 11.4.3. It allows each clip to be looped an arbitrary number of times or \ninﬁ nitely. Clips can also be time-scaled; a negative time scale allows an anima-\ntion to be played in reverse.\n11.10.3. Blend Trees\n For reasons we’ll explore below, some animation engines represent their blend \nspeciﬁ cations not as a ﬂ at weighted average but as a tree of blend operations. \nAn animation blend tree is an example of what is known in compiler theory \nas an expression tree or a syntax tree . The interior nodes of such a tree are opera-\ntors, and the leaf nodes serve as the inputs to those operators. (More correctly, \nthe interior nodes represent the non-terminals of the grammar , while the leaf \nnodes represent the terminals.) In the following sections, we’ll brieﬂ y revisit \nthe various kinds of animation blends we learned about in Sections 11.6.3 and \n11.6.5 and see how each can be represented by an expression tree.\n\n\n561 \nLERP\nClip A\nClip B\nOutput Pose\nβ\nFigure 11.47.  A binary LERP blend, represented by a binary expression tree.\n11.10.3.1. Binary LERP Blend\nAs we saw in Section 11.6.1, a binary linear interpolation (LERP) blend takes \ntwo input poses and blends them together into a single output pose. A blend \nweight β controls the percentage of the second input pose that should appear \nat the output, while (1 – β) speciﬁ es the percentage of the ﬁ rst input pose. This \ncan be represented by the binary expression tree shown in Figure 11.47.\n11.10.3.2. Generalized One-Dimensional LERP Blend\nIn Section 11.6.3.1, we learned that it can be convenient to deﬁ ne a generalized \none-dimensional LERP blend by placing an arbitrary number of clips along a \nlinear scale. A blend factor b speciﬁ es the desired blend along this scale. Such \na blend can be pictured as an n-input operator, as shown in Figure 11.48.\nGiven a speciﬁ c value for b, such a linear blend can always be transformed \ninto a binary LERP blend. We simply use the two clips immediately adjacent \nto b as the inputs to the binary blend and calculate the blend weight β as speci-\nﬁ ed in Equation (11.12). This is illustrated in Figure 11.48.\nFor this specific value of \nb, this tree converts to...\nβ = 0\nβ = 1\nβ\nb\nbA\nbB\nbC\nbD\nLERP\nOutput Pose\nb\nClip A\nClip B\nClip C\nClip D\nLERP\nClip B\nClip C\nOutput Pose\nβ\nFigure 11.48.  A multi-input expression tree can be used to represent a generalized 1D blend. \nSuch a tree can always be transformed into a binary expression tree for any speciﬁ c value of \nthe blend factor b.\n11.10. The Animation Pipeline\n\n\n562 \n11. Animation Systems\nbx\nLERP\nBottom Left\nBottom Right\nLERP\nTop Left\nTop Right\nOutput Pose\nLERP\nby\nFigure 11.49.  A simple 2D LERP blend, implemented as cascaded binary blends.\n11.10.3.3. Simple Two-Dimensional LERP Blend\nIn Section 11.6.3.2, we saw how a two-dimensional LERP blend can be realized \nby simply cascading the results of two binary LERP blends. Given a desired \ntwo-dimensional blend point b = [ bx  by ], Figure 11.49 shows how this kind of \nblend can be represented in tree form.\n11.10.3.4. Triangular LERP Blend\nSection 11.6.3.3 introduced us to triangular LERP blending, using the barycen-\ntric coordinates α, β, and γ = (1 – α – β) as the blend weights. To represent this \nkind of blend in tree form, we need a ternary (three-input) expression tree \nnode, as shown in Figure 11.50.\nTriangular \nLERP\nOutput Pose\nClip A\nClip B\nClip C\n(γ = 1 −α – β)\nα\nβ\nFigure 11.50.  A triangular 2D LERP blend, represented as a ternary expression tree.\n11.10.3.5. Generalized Triangular LERP Blend\nIn Section 11.6.3.4, we saw that a generalized two-dimensional LERP blend \ncan be speciﬁ ed by placing clips at arbitrary locations on a plane. A desired \noutput pose is speciﬁ ed by a point b = [ bx  by ] on the plane. This kind of \nblend can be represented as a tree node with an arbitrary number of inputs, \nas shown in Figure 11.51.\nA generalized triangular LERP blend can always be transformed into a \nternary tree by using  Delaunay triangulation to identify the triangle that sur-\nrounds the point b. The point is then converted into barycentric coordinates \nα, β, and γ = (1 – α – β), and these coordinates are used as the blend weights \n\n\n563 \nof a ternary blend node with the three clips at the vertices of the triangle as its \ninputs. This is demonstrated in Figure 11.51.\n11.10.3.6. Additive Blend\nSection 11.6.5 described additive blending. This is a binary operation, so it can \nbe represented by a binary tree node, as shown in Figure 11.52. A single blend \nweight β controls the amount of the additive animation that should appear \nat the output—when β = 0, the additive clip does not aﬀ ect the output at all, \nwhile when β = 1, the additive clip has its maximum eﬀ ect on the output.\nAdditive blend nodes must be handled carefully, because the inputs are \nnot interchangeable (as they are with most types of blend operators). One of \nthe two inputs is a regular skeletal pose, while the other is a special kind of \npose known as a diﬀ erence pose (also known as an additive pose). A diﬀ erence \npose may only be applied to a regular pose, and the result of an additive blend \nis another regular pose. This implies that the additive input of a blend node \nmust always be a leaf node, while the regular input may be a leaf or an interior \nnode. If we want to apply more than one additive animation to our character, \nFor this specific value of \nb, this tree converts to...\nbE\nTriangular \nLERP\nOutput Pose\nClip C\nClip D\nClip E\n(γ = 1 −α – β)\nα\nβ\nDelaunay\nLERP\nOutput Pose\nb\nClip A\nClip B\nClip C\nClip D\nClip E\nbA\nb B\nbC\nbD\nb\nβ\nb\nα\nγ\nFigure 11.51.  A generalized 2D blend can be represented by a multi-input expression tree node, \nbut it can always be converted into a ternary tree via Delaunay triangulation.\nClip A\nOutput Pose\nβ\nDiff Clip B\n+\nFigure 11.52.  An additive blend represented as a binary tree.\n11.10. The Animation Pipeline\n\n\n564 \n11. Animation Systems\nwe must use a cascaded binary tree with the additive clips always applied to \nthe additive inputs, as shown in Figure 11.53.\n11.10.4. Cross-Fading Architectures\nAs we saw in Section 11.6.2.2, cross-fading between animations is generally \naccomplished by LERP blending from the previous animation to the next one. \nCross-fades can be implemented in one of two ways, depending on whether \nyour animation engine uses the ﬂ at weighted average architecture or the ex-\npression tree architecture. In this section, we’ll take a look at both implemen-\ntations.\n11.10.4.1. Cross-Fades with a Flat Weighted Average\n In an animation engine that employs the ﬂ at weighted average architecture, \ncross-fades are implemented by adjusting the weights of the clips themselves. \nRecall that any clip whose weight wi = 0 will not contribute to the current pose \nof the character, while those whose weights are non-zero are averaged togeth-\ner to generate the ﬁ nal pose. If we wish to transition smoothly from clip A to \nclip B, we simply ramp up clip B’s weight, wB , while simultaneously ramping \ndown clip A’s weight, wA. This is illustrated in Figure 11.54.\nCross-fading in a weighted average architecture becomes a bit trickier \nwhen we wish to transition from one complex blend to another. As an ex-\nample, let’s say we wish to transition the character from walking to jumping. \nClip A\nβ1\nDiff Clip B\n+\nβ2\nDiff Clip C\n+\nOutput Pose\nβ3\nDiff Clip D\n+\nFigure 11.53.  In order to additively blend more than one difference pose onto a regular “base” \npose, a cascaded binary expression tree must be used.\nt\nw\n1\n0\ntstart\ntend\nwA\nwB\nFigure 11.54.  A simple cross-fade from clip A to clip B, as implemented in a weighted average \nanimation architecture.\n\n\n565 \nLet’s assume that the walk movement is produced by a three-way average \nbetween clips A, B, and C, and that the jump movement is produced by a two-\nway average between clips D and E.\nWe want the character to look like he’s smoothly transitioning from walk-\ning to jumping, without aﬀ ecting how the walk or jump animations look indi-\nvidually. So during the transition, we want to ramp down the ABC clips and \nramp up the DE clips while keeping the relative weights of the ABC and DE clip \ngroups constant. If the cross-fade’s blend factor is denoted by λ, we can meet \nthis requirement by simply sett ing the weights of both clip groups to their de-\nsired values and then multiplying the weights of the source group by (1 – λ) \nand the weights of the destination group by λ.\nLet’s look at a concrete example to convince ourselves that this will work \nproperly. Imagine that before the transition from ABC to DE, the non-zero \nweights are as follows: wA = 0.2, wB = 0.3, and wC = 0.5. Aft er the transition, we \nwant the non-zero weights to be wD = 0.33, and wE = 0.66. So, we set the weights \nas follows:\n \n(1\n)(0.2),\n(0.33),\n(1\n)(0.3),\n(0.66).\n(1\n)(0.5),\nA\nD\nB\nE\nC\nw\nw\nw\nw\nw\n=\n−λ\n= λ\n=\n−λ\n= λ\n=\n−λ\n \n(11.17)\nFrom Equations (11.17), you should be able to convince yourself of the fol-\nlowing:\nWhen \n1. \nλ = 0, the output pose is the correct blend of clips A, B, and C, \nwith zero contribution from clips D and E.\nWhen \n2. \nλ = 1, the output pose is the correct blend of clips D and E, with \nno contribution from A, B ,or C.\nWhen 0 < \n3. \nλ < 1, the relative weights of both the ABC group and the DE \ngroup remain correct, although they no longer add to one. (In fact, group \nABC’s weights add to (1 – λ), and group DE’s weights add to λ.)\nFor this approach to work, the implementation must keep track of \nthe logical groupings between clips (even though, at the lowest level, all \nof the clips’ states are maintained in one big, ﬂ at array—for example, the \nOgre::AnimationStateSet in Ogre). In our example above, the system \nmust “know” that A, B, and C form a group, that D and E form another group, \nand that we wish to transition from group ABC to group DE. This requires ad-\nditional meta-data to be maintained, on top of the ﬂ at array of clip states.\n11.10.4.2. Cross-Fades with Expression Trees\nImplementing a cross-fade in an expression-tree -based animation engine is a \nbit more intuitive than it is in a weighted average architecture. Whether we’re \n11.10. The Animation Pipeline\n\n\n566 \n11. Animation Systems\ntransitioning from one clip to another or from one complex blend to another, \nthe approach is always the same: We simply introduce a new, binary LERP \nnode at the root of the blend tree for the duration of the cross-fade.\nWe’ll denote the blend factor of the cross-fade node with the symbol λ as \nbefore. Its top input is the source tree (which can be a single clip or a complex \nblend), and its bott om input is the destination tree (again a clip or a complex \nblend). During the transition, λ is ramped from zero to one. Once λ = 1, the \ntransition is complete, and the cross-fade LERP node and its top input tree can \nbe retired. This leaves its bott om input tree as the root of the overall blend tree, \nthus completing the transition. This process is illustrated in Figure 11.55.\n11.10.5. Animation Pipeline Optimization\nOptimization is a crucial aspect of any animation pipeline. Some pipelines \nexpose all of their nitt y-gritt y optimization details, eﬀ ectively placing the re-\nsponsibility for proper optimization on the calling code. Others att empt to \nencapsulate most of the optimization details behind a convenient API, but \neven in these cases, the API still must be structured in a particular way so as to \npermit the desired optimizations to be implemented behind the scenes.\nAnimation pipeline optimizations are usually highly speciﬁ c to the archi-\ntecture of the hardware on which the game will run. For example, on mod-\nern hardware architectures, memory access patt erns can greatly aﬀ ect the \nperformance of the code. Cache misses and load-hit-store operations must be \navoided to ensure maximum speed. But on other hardware, ﬂ oating-point op-\nerations might be the bott leneck, in which case the code might be structured \nto take maximum advantage of SIMD vector math. Each hardware platform \nTree\nA\nOutput Pose\nLERP\nOutput Pose\nλ\nTree\nA\nTree\nB\nTree\nB\nOutput Pose\nBefore\nCross-Fade\nDuring\nCross-Fade\nAfter\nCross-Fade\nFigure 11.55.  A cross-fade between two arbitrary blend trees A and B.\n\n\n567 \npresents a unique set of optimization challenges to the programmer. As a re-\nsult, some animation pipeline APIs are highly speciﬁ c to a particular platform. \nOther pipelines att empt to present an API that can be optimized in diﬀ erent \nways on diﬀ erent processors. Let’s take a look at a few examples.\n11.10.5.1. Optimization on the PlayStation 2\nThe PlayStation 2 has a region of ultra-fast memory known as the scratch pad. \nIt also has a fast direct memory access (DMA) controller, which is capable of \ncopying data to and from the scratch pad eﬃ  ciently. Some animation pipelines \ntake advantage of this hardware architecture by arranging for all animation \nblending to take place within the scratch pad. When two skeletal poses are to \nbe blended, they are DMA’d from main RAM to the scratch pad. The blend is \nperformed, and the result is writt en into another buﬀ er within the scratch pad . \nFinally, the resulting pose is DMA’d back into main RAM.\nThe PS2’s DMA controller can move memory around in parallel with the \nmain CPU. So, to maximize throughput, PS2 programmers are always look-\ning for ways to keep the CPU and the DMA controller busy simultaneously. \nOft en the best way to accomplish this is to use a batch-style API, where the \ngame queues up requests for animation blends in a big list and then kicks \neverything oﬀ  in one go. This permits the animation pipeline to maximize \nthe utilization of both the DMA controller and the CPU, because it can feed \na large number of pose requests through the pipeline with no “dead space” \nbetween them and even overlap the DMA of one request with the processing \nof an unrelated request.\n11.10.5.2. Optimization on the PLAYSTATION 3\nAs we saw in Section 7.6.1.2, the PLAYSTATION 3 has six specialized proces-\nsors known as synergistic processing units (SPU). The SPUs execute most code \nmuch more quickly than the main CPU (known as the power processing unit or \nPPU). Each SPU also has a 256 kB region of ultra-fast local store memory for its \nexclusive use. Like the PS2, the PS3 has a powerful DMA controller capable \nof moving memory back and forth between main RAM and the SPUs’ memo-\nries in parallel with computing tasks. If one could write an ideal animation \npipeline for the PS3, as much processing as possible would be executed on \nthe SPUs, and neither the PPU nor any SPU would ever be idle waiting for a \nDMA to complete.\nThis architecture leads to animation pipeline APIs that look similar in \nsome respects to their PlayStation 2 counterparts, in the sense that animation \nrequests are again batched so that they can be interleaved eﬃ  ciently. In ad-\ndition, a PLAYSTATION 3 animation API will usually expose the concept of \nanimation jobs, because a job is a fundamental unit of execution on the SPUs.\n11.10. The Animation Pipeline\n",
      "page_number": 569,
      "chapter_number": 29,
      "summary": "This chapter covers segment 29 (pages 569-589). Key topics include animation, animated, and animations. To decode this quantized value, \nwe simply convert the integer index into ﬂ oating-point format and shift  and \nscale it back into the original range.",
      "keywords": [
        "Animation",
        "pose",
        "Animation pipeline",
        "LERP blend",
        "animation clips",
        "clip",
        "blend",
        "output pose",
        "Animation Systems",
        "LERP",
        "global pose",
        "binary LERP blend",
        "tree",
        "Triangular LERP Blend",
        "nal pose"
      ],
      "concepts": [
        "animation",
        "animated",
        "animations",
        "pose",
        "blend",
        "clip",
        "data",
        "game",
        "tree",
        "inputs"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 7",
          "chapter": 46,
          "title": "Segment 46 (pages 443-452)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 38,
          "title": "Segment 38 (pages 377-385)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 4,
          "title": "Segment 4 (pages 60-78)",
          "relevance_score": 0.51,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 17,
          "title": "Segment 17 (pages 166-173)",
          "relevance_score": 0.51,
          "method": "sentence_transformers"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 8,
          "title": "Segment 8 (pages 140-157)",
          "relevance_score": 0.49,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 590-610)",
      "start_page": 590,
      "end_page": 610,
      "detection_method": "synthetic",
      "content": "568 \n11. Animation Systems\n11.10.5.3. Optimization on the Xbox and Xbox 360\nRather than having specialized memory regions and a DMA controller to \nmove data from region to region, the Xbox and the Xbox 360 both employ a \nuniﬁ ed memory architecture . All processors, including the main CPU (or in \nthe case of the 360, the three PowerPC cores), the GPU, and all other hardware \nsystems, tap into a single big block of main RAM.\nIn theory, the Xbox architecture requires a totally diﬀ erent set of optimi-\nzations than would be required on the PlayStation architectures, and so we \nmight expect to see very diﬀ erent animation APIs between these two plat-\nforms. However, the Xbox serves as an example of how optimizations for one \nplatform can sometimes be beneﬁ cial to other platforms as well. As it turns out, \nboth the Xbox and PlayStation platforms incur massive performance degra-\ndation in the presence of cache misses and load-hit-store memory access pat-\nterns. So, it is beneﬁ cial on both systems to keep animation data as localized as \npossible in physical RAM. An animation pipeline that processes animations in \nlarge batches and operates on data within relatively small regions of memory \n(such as the PS2’s scratch pad or PS3’s SPU memories) will also perform well \non a uniﬁ ed memory architecture like that of the Xbox. Achieving this kind of \nsynergy between platforms is not always possible, and every hardware plat-\nform requires its own speciﬁ c optimizations. However, when such an oppor-\ntunity does arise, it is wise to take advantage of it.\nA good rule of thumb is to optimize your engine for the platform with the \nmost stringent performance restrictions. When your optimized code is ported \nto other platforms with fewer restrictions, there’s a good chance that the opti-\nmizations you made will remain beneﬁ cial, or at worst will have few adverse \naﬀ ects on performance. Going in the other direction—porting from the least \nstringent platform to the more stringent ones—almost always results in less-\nthan-optimal performance on the most stringent platform.\n11.11. Action State Machines\nThe low-level pipeline is the equivalent of OpenGL or DirectX for animation—\nit is very powerful but can be rather inconvenient for direct use by game code. \nTherefore, it is usually convenient to introduce a layer between the low-level \npipeline and the game characters and other clients of the animation system. \nThis layer is usually implemented as a state machine, known as the action state \nmachine or the animation state machine ( ASM) .\nThe ASM sits on top of the animation pipeline, permitt ing the actions of \nthe characters in a game to be controlled in a straightforward, state-driven \n\n\n569 \nmanner. The ASM is also responsible for ensuring that transitions from state \nto state are smooth and natural-looking. Some animation engines permit mul-\ntiple independent state machines to control diﬀ erent aspects of a character’s \nmovement, such as full-body locomotion, upper-body gestures, and facial \nanimations. This can be accomplished by introducing the concept of state lay-\nering. In this section, we’ll explore how a typical animation state machine is \narchitected.\n11.11.1. Animation States\nEach state in an ASM corresponds to an arbitrarily complex blend of simul-\ntaneous animation clips. In a blend tree architecture, each state corresponds \nto a particular predeﬁ ned blend tree. In a ﬂ at weighted average architecture, \na state represents a group of clips with a speciﬁ c set of relative weights. It is \nsomewhat more convenient and expressive to think in terms of blend trees, \nso we will do so for the remainder of this discussion. However, everything \nwe describe here can also be implemented using the ﬂ at weighted average \napproach, as long as additive blending or quaternion SLERP operations are \nnot involved.\nThe blend tree corresponding to a particular animation state can be as \nsimple or as complex as required by the game’s design (provided it remains \nwithin the memory and performance limitations of the engine). For example, \nan “idle” state might be comprised of a single full-body animation. A “run-\nning” state might correspond to a semicircular blend, with straﬁ ng left , run-\nning forward, and straﬁ ng right at the –90 degrees, 0 degrees, and +90 degrees \npoints, respectively. The blend tree for a “running while shooting” state might \ninclude a semicircular directional blend, plus additive or partial-skeleton \nblend nodes for aiming the character’s weapon up, down, left , and right, and \nadditional blends to permit the character to look around with its eyes, head, \nand shoulders. More additive animations might be included to control the \ncharacter’s overall stance, gait, and foot spacing while locomoting and to pro-\nvide a degree of “humanness” through random movement variations.\n11.11.1.1. State and Blend Tree Speciﬁ cations\n Animators, game designers, and programmers usually cooperate to create the \nanimation and control systems for the central characters in a game. These de-\nvelopers need a way to specify the states that make up a character’s ASM, to \nlay out the tree structure of each blend tree, and to select the clips that will \nserve as their inputs. Although the states and blend trees could be hard-coded, \nmost modern game engines provide a data-driven means of deﬁ ning animation \nstates. The goal of a data-driven approach is permit a user to create new ani-\n11.11. Action State Machines\n\n\n570 \n11. Animation Systems\nmation states, remove unwanted states, ﬁ ne-tune existing states, and then see \nthe eﬀ ects of his or her changes reasonably quickly. In other words, the central \ngoal of a data-driven animation engine is to enable rapid iteration .\nThe means by which the users enter animation state data varies widely. \nSome game engines employ a simple, bare-bones approach, allowing anima-\ntion states to be speciﬁ ed in a text ﬁ le with a simple syntax. Other engines pro-\nvide a slick, graphical editor that permits animation states to be constructed \nby dragging atomic components such as clips and blend nodes onto a canvas \nand linking them together in arbitrary ways. Such editors usually provide a \nlive preview of the character so that the user can see immediately how the \ncharacter will look in the ﬁ nal game. In my opinion, the speciﬁ c method cho-\nsen has litt le bearing on the quality of the ﬁ nal game—what matt ers most is \nthat the user can make changes and see the results of those changes reason-\nably quickly and easily.\n11.11.1.2. Custom Blend Tree Node Types\nTo build an arbitrarily complex blend tree, we really only require four atomic \ntypes of blend nodes: clips, binary LERP blends, binary additive blends, and \nternary (triangular) LERP blends. Virtually any blend tree imaginable can be \ncreated as compositions of these atomic nodes.\nA blend tree built exclusively from atomic nodes can quickly become large \nand unwieldy. As a result, many game engines permit custom compound \nnode types to be predeﬁ ned for convenience. The N-dimensional linear blend \nnode discussed in Sections 11.6.3.4 and 11.10.3.2 is an example of a compound \nnode. One can imagine myriad complex blend node types, each one address-\ning a particular problem speciﬁ c to the particular game being made. A soccer \ngame might deﬁ ne a node that allows the character to dribble the ball. A war \ngame could deﬁ ne a special node that handles aiming and ﬁ ring a weapon. \nA brawler could deﬁ ne custom nodes for each ﬁ ght move the characters can \nperform. Once we have the ability to deﬁ ne custom node types, the sky’s the \nlimit.\n11.11.1.3. Example: Naughty Dog’s Uncharted Engine\n The animation engine used in Naughty Dog’s Uncharted: Drake’s Fortune \nand Uncharted 2: Among Thieves employs a simple, text-based approach to \nspecifying animation states. For reasons related to Naughty Dog’s rich his-\ntory with the Lisp language , state speciﬁ cations in the Uncharted engine \nare writt en in a customized version of the Scheme programming language \n(which itself is a Lisp variant). Two basic state types can be used: simple and \ncomplex.\n\n\n571 \nSimple States\nA simple state contains a single animation clip. For example:\n(define-state simple\n    :name \"pirate-b-bump-back\"\n    :clip \"pirate-b-bump-back\"\n    :flags (anim-state-flag no-adjust-to-ground)\n)\nDon’t let the Lisp-style syntax throw you. All this block of code does is to de-\nﬁ ne a state named “pirate-b-bump-back” whose animation clip also happens \nto be named “pirate-b-bump-back.” The :f lags parameter allows users to \nspecify various Boolean options on the state.\nComplex States\nA complex state contains an arbitrary tree of LERP or additive blends. For ex-\nample, the following state deﬁ nes a tree that contains a single binary LERP \nblend node, with two clips (“walk-l-to-r” and “run-l-to-r”) as its inputs:\n(define-state complex\n    :name \"move-l-to-r\"\n:tree\n        (anim-node-lerp\n            (anim-node-clip \"walk-l-to-r\")\n            (anim-node-clip \"run-l-to-r\")\n        )\n)\nThe :tree argument allows the user to specify an arbitrary blend tree, com-\nposed of LERP or additive blend nodes and nodes that play individual anima-\ntion clips.\nFrom this, we can see how the (define-state simple ...) example \nshown above might really work under the hood—it probably deﬁ nes a com-\nplex blend tree containing a single “clip” node, like this:\n(define-state complex\n    :name \"pirate-b-unimog-bump-back\"\n:tree (anim-node-clip \"pirate-b-unimog-bump-back”)\n    :flags (anim-state-flag no-adjust-to-ground)\n)\nThe following complex state shows how blend nodes can be cascaded into \narbitrarily deep blend trees:\n(define-state complex\n    :name \"move-b-to-f\"\n11.11. Action State Machines\n\n\n572 \n11. Animation Systems\n    :tree\n        (anim-node-lerp\n            (anim-node-additive\n                (anim-node-additive\n                    (anim-node-clip \"move-f\")\n                    (anim-node-clip \"move-f-look-lr\")\n                )\n                (anim-node-clip \"move-f-look-ud\")\n            )\n            (anim-node-additive\n                (anim-node-additive\n                    (anim-node-clip \"move-b\")\n                    (anim-node-clip \"move-b-look-lr\")\n                )\n                (anim-node-clip \"move-b-look-ud\")\n            )\n        )\n)\nThis corresponds to the tree shown in Figure 11.56.\nCustom Tree Syntax\nThanks to the powerful macro language in Scheme, custom blend trees can \nalso be deﬁ ned by the user in terms of the basic clip, LERP, and additive blend \nnodes. This allows us to deﬁ ne multiple states, each of which has a nearly \nidentical tree structure but with diﬀ erent input clips or any number of other \nvariations. For example, the complex blend tree used in the state “move-b-to-\nf” shown above could be partially deﬁ ned via a macro as follows:\n(define-syntax look-tree\n    (syntax-rules ()\nLERP\nmove-f\nmove-f-look-lr\n+\nmove-f-look-ud\nmove-b\nmove-b-look-lr\n+\nmove-b-look-ud\n+\n+\nFigure 11.56.  Blend tree corresponding to the example state “move-b-to-f.”\n\n\n573 \n        ((look-tree base-clip look-lr-clip look-ud-clip)\n            ;; This means \"whenever the compiler sees  \n \n \n \n       ;; code of the form (look-tree b lr ud),\n    \n    ;; replace it with the following code...\"\n            (anim-node-additive\n                (anim-node-additive\n                    (anim-node-clip base-clip)\n                    (anim-node-clip look-lr-clip)\n                )\n                (anim-node-clip look-ud-clip)\n            )\n        )\n    )\n)\nThe original “move-b-to-f” state could then be redeﬁ ned in terms of this \nmacro as follows:\n(define-state complex\n    :name \"move-b-to-f\"\n    :tree\n        (anim-node-lerp\n            (look-tree \"move-f\"\n                \"move-f-look-lr\"\n                \"move-f-look-ud\")\n            (look-tree \"move-b\"\n                \"move-b-look-lr\"\n                \"move-b-look-ud\")\n        )\n)\nThe (look-tree ...) macro can be used to deﬁ ne any number of states that \nrequire this same basic tree structure but want diﬀ erent animation clips as \ninputs. They can also combine their “look trees” in any number of ways.\nRapid Iteration\nRapid iteration is achieved in Uncharted with the help of two important tools. \nAn in-game animation viewer allows a character to be spawned into the game \nand its animations controlled via an in-game menu. And a simple command-\nline tool allows animation scripts to be recompiled and reloaded into the run-\nning game on the ﬂ y. To tweak a character’s animations, the user can make \nchanges to the text ﬁ le containing the animation state speciﬁ cations, quick-\nly reload the animation states, and immediately see the eﬀ ects of his or her \nchanges on an animating character in the game.\n11.11. Action State Machines\n\n\n574 \n11. Animation Systems\nFigure 11.57.  The Unreal Engine 3 graphical animation editor.\n11.11.1.4. Example: Unreal Engine 3\nUnreal Engine 3 (UE3) provides its users with a graphical interface to the ani-\nmation system. As shown in Figure 11.57, an animation blend tree in Unreal \nis comprised of a special root node called an AnimTree. This node takes three \nkinds of inputs: animations, morphs, and special nodes known as skel controls. \nThe animation input can be connected to the root of an arbitrarily complex \nblend tree (which happens to be drawn with poses ﬂ owing from right to left —\nopposite of the convention we use in this book). The “morph” input allows \nmorph-target-based animations to drive the character; this is most oft en used \nfor facial animation. The “skel control” inputs allow various kinds of proce-\ndural post-processing, such as inverse kinematics (IK), to be performed on the \npose generated by the animation and/or morph trees.\nThe UE3 Animation Tree\nThe Unreal animation tree is essentially a blend tree. Individual animation \nclips (called sequences in Unreal) are represented by nodes of type Anim\nSequence. A sequence node has a single output, which may either be con-\nnected directly to the “animation” input of the AnimTree node or to other \ncomplex node types. Unreal provides a wide selection of blend node types \nout of the box, including binary blends, four-way two-dimensional blends \n\n\n575 \n(known as blend by aim), and so on. It also provides various special nodes that \nare capable of doing things like scaling the playback rate (R) of a clip, mirror-\ning the animation (which turns a right-handed motion into a left -handed one, \nfor example), and more.\nThe UE3 animation tree is also highly customizable. A programmer can \ncreate new types of nodes that perform arbitrarily complex operations. So the \nUnreal developer is not limited to simple binary and ternary LERP blends. At \nthe time this chapter was writt en, Unreal Engine 3 did not support additive \nanimation blending out of the box, although it’s certainly possible for a game \nteam to extend the Unreal engine to support it.\nIt is interesting to note that Unreal’s approach to character animation is \nnot explicitly state-based. Rather than deﬁ ning multiple states, each with its \nown local blend tree, the Unreal developer typically builds a single monolithic \ntree. The character can be put into diﬀ erent “states” by simply turning on or \noﬀ  certain parts of the tree. Some game teams implement a system for replac-\ning portions of the UE3 animation tree dynamically, so that a game’s mono-\nlithic tree can be broken into more manageable subtrees.\nThe UE3 Post-Processing Tree (Skel Controls)\nAs we have seen, animation post-processing involves procedurally modifying \nthe pose of the skeleton that has been generated by the blend tree. In UE3, \nskel control nodes are used for this purpose. To use a skel control, the user \nﬁ rst creates an input on the AnimTree node corresponding to the joint in the \nskeleton that he or she wishes to control procedurally. Then a suitable skel \ncontrol node is created, and its output is hooked up to the new input on the \nAnimTree node.\nUnreal provides a number of skel controls out of the box, to perform \nfoot IK (which ensures that the feet conform to ground contours), procedural \n“look-at” (which allows the character to look at arbitrary points in space), \nother forms of IK, and so on. As with animation nodes, it is quite easy for a \nprogrammer to create custom skel control nodes in order to meet the particu-\nlar needs of the game being developed.\n11.11.2. Transitions\nTo create a high-quality animating character, we must carefully manage the \ntransitions between states in the action state machine to ensure that the splices \nbetween animations do not have a jarring and unpolished appearance. Most \nmodern animation engines provide a data-driven mechanism for specifying \nexactly how transitions should be handled. In this section, we’ll explore how \nthis mechanism works.\n11.11. Action State Machines\n\n\n576 \n11. Animation Systems\n11.11.2.1. Kinds of Transitions\nThere are many diﬀ erent ways to manage the transition between states. If we \nknow that the ﬁ nal pose of the source state exactly matches the ﬁ rst pose of \nthe destination state, we can simply “pop” from one state to another. Other-\nwise, we can cross-fade from one state to the next. Cross-fading is not always \na suitable choice when transitioning from state to state. For example, there \nis no way that a cross-fade can produce a realistic transition from lying on \nthe ground to standing upright. For this kind of state transition, we need one \nor more custom animations. This kind of transition is oft en implemented by \nintroducing special transitional states into the state machine. These states are \nintended for use only when going from one state to another—they are never \nused as a steady-state node. But because they are full-ﬂ edged states, they can \nbe comprised of arbitrarily complex blend trees. This provides maximum ﬂ ex-\nibility when authoring custom-animated transitions.\n11.11.2.2. Transition Parameters\nWhen describing a particular transition between two states, we generally need \nto specify various parameters, controlling exactly how the transition will oc-\ncur. These include but are not limited to the following.\nSource and destination states.\n• \n To which state(s) does this transition ap-\nply?\nTransition type.\n• \n Is the transition immediate, cross-faded, or performed \nvia a transitional state?\nDuration.\n• \n For cross-faded transitions, we need to specify how long the \ncross-fade should take.\nEase-in/ease-out curve\n• \n type. In a cross-faded transition, we may wish to \nspecify the type of ease-in/ease-out curve to use to vary the blend factor \nduring the fade.\nTransition window\n• \n . Certain transitions can only be taken when the source \nanimation is within a speciﬁ ed window of its local time line. For ex-\nample, a transition from a punch animation to an impact reaction might \nonly make sense when the arm is in the second half of its swing. If an \natt empt to perform the transition is made during the ﬁ rst half of the \nswing, the transition would be disallowed (or a diﬀ erent transition \nmight be selected instead).\n11.11.2.3. The Transition Matrix\nSpecifying transitions between states can be challenging, because the number \nof possible transitions is usually very large. In a state machine with n states, \n\n\n577 \nthe worst-case number of possible transitions is n2. We can imagine a two-\ndimensional square matrix with every possible state listed along both the ver-\ntical and horizontal axes. Such a table can be used to specify all of the possible \ntransitions from any state along the vertical axis to any other state along the \nhorizontal axis.\nIn a real game, this transition matrix is usually quite sparse, because not \nall state-to-state transitions are possible. For example, transitions are usually \ndisallowed from a death state to any other state. Likewise, there is probably \nno way to go from a driving state to a swimming state (without going through \nat least one intermediate state that causes the character to jump out of his \nvehicle!). The number of unique transitions in the table may be signiﬁ cantly \nless even than the number of valid transitions between states. This is because \nwe can oft en re-use a single transition speciﬁ cation between many diﬀ erent \npairs of states.\n11.11.2.4. Implementing a Transition Matrix\nThere are all sorts of ways to implement a transition matrix. We could use a \nspreadsheet application to tabulate all the transitions in matrix form, or we \nmight permit transitions to be authored in the same text ﬁ le used to author our \naction states. If a graphical user interface is provided for state editing, transi-\ntions could be added to this GUI as well. In the following sections, we’ll take a \nbrief look at a few transition matrix implementations from real game engines.\nExample: Wild-Carded Transitions in Medal of Honor: Paciﬁ c Assault\nOn Medal of Honor: Paciﬁ c Assault (MOHPA), we used the sparseness of the \ntransition matrix to our advantage by supporting wild-carded transition spec-\niﬁ cations. For each transition speciﬁ cation, the names of both the source and \ndestination states could contain asterisks (*) as a wild-card character. This al-\nlowed us to specify a single default transition from any state to any other \nstate (via the syntax from=”*” to=”*” ) and then reﬁ ne this global default \neasily for entire categories of states. The reﬁ nement could be taken all the way \ndown to custom transitions between speciﬁ c state pairs when necessary. The \nMOHPA transition matrix looked something like this:\n<transitions>\n  // global default\n  <trans from=\"*\" to=\"*\" type=frozen duration=0.2>\n  ...\n  // default for any walk to any run\n  <trans from=\"walk*\" to=\"run*\" type=smooth  \n    \n  duration=0.15>\n11.11. Action State Machines\n\n\n578 \n11. Animation Systems\n  ...\n  // special handling from any prone to any getting-up   \n  // action (only valid from 2 sec to 7.5 sec on the \n  // local timeline)\n  <trans from=\"*prone\" to=\"*get-up\" type=smooth   \n \n \n  duration=0.1\n  window-start=2.0 window-end=7.5>\n  ...\n  // special case between crouched walking and jumping\n  <trans from=\"walk-crouch\" to=\"jump\" type=frozen  \n \n \n  duration=0.3>\n  ...\n</transitions>\nExample: First-Class Transitions in Uncharted\nIn some animation engines, high-level game code requests transitions from \nthe current state to a new state by naming the destination state explicitly. The \nproblem with this approach is that the calling code must have intimate knowl-\nedge of the names of the states and of which transitions are valid when in a \nparticular state.\nIn Naughty Dog’s Uncharted engine, this problem is overcome by turn-\ning state transitions from secondary implementation details into ﬁ rst-class \nentities. Each state provides a list of valid transitions to other states, and each \ntransition is given a unique name. The names of the transitions are standard-\nized in order to make the eﬀ ect of each transition predictable. For example, \nif a transition is called “walk,” then it always goes from the current state to a \nwalking state of some kind, no matt er what the current state is. Whenever the \nhigh-level animation control code wants to transition from state A to state B, \nit asks for a transition by name (rather than requesting the destination state \nexplicitly). If such a transition can be found and is valid, it is taken; otherwise, \nthe request fails.\nThe following example state deﬁ nes four transitions named “reload,” \n“step-left ,” “step-right,” and “ﬁ re.” The (transition-group ...)  line \ninvokes a previously deﬁ ned group of transitions; it is useful when the \nsame set of transitions is to be used in multiple states. The (transition-\nend ...)  command speciﬁ es a transition that is taken upon reaching the \nend of the state’s local time line if no other transition has been taken before \nthen.\n(define-state complex\n    :name \"s_turret-idle\"\n\n\n579 \n    :tree (aim-tree (anim-node-clip \n       \"turret-aim-all--base\")\n                    \"turret-aim-all--left-right\"\n                    \"turret-aim-all--up-down\")\n:transitions (\n        (transition \"reload\" \"s_turret-reload\"\n            (range - -) :fade-time 0.2)\n        (transition \"step-left\" \"s_turret-step-left\"\n            (range - -) :fade-time 0.2)\n        (transition \"step-right\" \"s_turret-step-right\"\n            (range - -) :fade-time 0.2)\n        (transition \"fire\" \"s_turret-fire\"\n            (range - -) :fade-time 0.1)\n        (transition-group \"combat-gunout-idle^move\")\n        (transition-end \"s_turret-idle\")\n    )\n)\nThe beauty of this approach may be diﬃ  cult to see at ﬁ rst. Its primary \npurpose is to allow transitions and states to be modiﬁ ed in a data-driven man-\nner, without requiring changes to the C++ source code in many cases. This \ndegree of ﬂ exibility is accomplished by shielding the animation control code \nfrom knowledge of the structure of the state graph. For example, let’s say that \nwe have ten diﬀ erent walking states (normal, scared, crouched, injured, and \nso on). All of them can transition into a jumping state, but diﬀ erent kinds \nof walks might require diﬀ erent jump animations (e.g., normal jump, scared \njump, jump from crouch, injured jump, etc.). For each of the ten walking states, \nwe deﬁ ne a transition simply called “jump.” At ﬁ rst, we can point all of these \ntransitions to a single generic “jump” state, just to get things up and running. \nLater, we can ﬁ ne-tune some of these transitions so that they point to custom \njump states. We can even introduce transitional states between some of the \n“walk” states and their corresponding “jump” states. All sorts of changes can \nbe made to the structure of the state graph and the parameters of the transi-\ntions without aﬀ ecting the C++ source code—as long as the names of the transi-\ntions don’t change.\n11.11.3. State Layers\nMost living creatures can do more than one thing at once with their bodies. \nFor example, a human can walk around with her lower body while looking at \n11.11. Action State Machines\n\n\n580 \n11. Animation Systems\nsomething with her shoulders, head, and eyes and making a gesture with her \nhands and arms. The movements of diﬀ erent parts of the body aren’t gener-\nally in perfect sync—certain parts of the body tend to “lead” the movements \nof other parts (e.g., the head leads a turn, followed by the shoulders, the hips, \nand ﬁ nally the legs). In traditional animation, this well-known technique is \nknown as anticipation [44].\nThis kind of movement seems to be at odds with a state-machine-based \napproach to animation. Aft er all, we can only be in one state at a time. So how \ncan we get diﬀ erent parts of the body to operate independently? One solution \nto this problem is to introduce the concept of state layers . Each layer can be \nin only one state at a time, but the layers are temporally independent of one \nanother. The ﬁ nal pose of the skeleton is calculated by evaluating the blend \ntrees on each of the n layers, thus generating n skeletal poses, and then blend-\ning these poses together in a predeﬁ ned manner. This is illustrated in Fig-\nure 11.58.\nThe Uncharted engine uses a layered state architecture. The layers form \na stack, with the bott om-most layer (called the base layer) always producing \na full-body skeletal pose and each upper layer blending in a new full-body, \npartial-skeleton, or additive pose on top of the base pose. Two kinds of layers \nare supported: LERP and additive. A LERP layer blends its output pose with \nthe pose generated by the layer(s) below it. An additive layer assumes that its \noutput pose is always a diﬀ erence pose and uses additive blending to combine \nit with the pose generated by the layer(s) below it. In eﬀ ect, a layered state ma-\nBase Layer\nState A\nState B\nState C\nVariation Layer (Additive)\nD\nE\nG\nGesture Layer (Additive)\nH\nI\nGesture Layer (LERP)\nJ\nK\nF\nTime (τ)\nFigure 11.58.  A layered animation state machine, showing how each layer’s state transitions \nare temporally independent.\n\n\n581 \nchine converts multiple, temporally independent blend trees (one per layer) \ninto a single uniﬁ ed blend tree. This is shown in Figure 11.59.\n11.11.4. Control Parameters\nFrom a soft ware engineering perspective, it can be challenging to orchestrate \nall of the blend weights, playback rates, and other control parameters of a \ncomplex animating character. Diﬀ erent blend weights have diﬀ erent eﬀ ects \non the way the character animates. For example, one weight might control \nthe character’s movement direction, while others control its movement speed, \nhorizontal and vertical weapon aim, head/eye look direction, and so on. We \nneed some way of exposing all of these blend weights to the code that is re-\nsponsible for controlling them.\nNet blend tree\nat time τ\nTime\nH\nF\nB\nτ\nK\nLERP\n+\nTree\nB\nTree\nF\nTree\nH\n+\nTree\nK\nFigure 11.59.  A layered state machine converts the blend trees from multiple states into a \nsingle, uniﬁ ed tree.\n11.11. Action State Machines\n\n\n582 \n11. Animation Systems\nIn a ﬂ at weighted average architecture, we have a ﬂ at list of all the ani-\nmation clips that could possibly be played on the character. Each clip state \nhas a blend weight, a playback rate, and possibly other control parameters. \nThe code that controls the character must look up individual clip states by \nname and adjust each one’s blend weight appropriately. This makes for a sim-\nple interface, but it shift s most of the responsibility for controlling the blend \nweights to the character control system. For example, to adjust the direction \nin which a character is running, the character control code must know that the \n“run” action is comprised of a group of animation clips, named something \nlike “StrafeLeft ,” “RunForward,” “StrafeRight,” and “RunBackward.” It must \nlook up these clip states by name and manually control all four blend weights \nin order to achieve a particular angled run animation. Needless to say, control-\nling animation parameters in such a ﬁ ne-grained way can be tedious and can \nlead to diﬃ  cult-to-understand source code.\nIn a blend tree , a diﬀ erent set of problems arise. Thanks to the tree struc-\nture, the clips are grouped naturally into functional units. Custom tree nodes \ncan encapsulate complex character motions. These are both helpful advantag-\nes over the ﬂ at weighted average approach. However, the control parameters \nare buried within the tree. Code that wishes to control the horizontal look-at \ndirection of the head and eyes needs a priori knowledge of the structure of \nthe blend tree so that it can ﬁ nd the appropriate nodes in the tree in order to \ncontrol their parameters.\nDiﬀ erent animation engines solve these problems in diﬀ erent ways. Here \nare some examples:\nNode search.\n• \n Some engines provide a way for higher-level code to ﬁ nd \nblend nodes in the tree. For example, relevant nodes in the tree can be \ngiven special names, such as “HorizAim” for the node that controls hor-\nizontal weapon aiming. The control code can simply search the tree for \na node of a particular name; if one is found, then we know what eﬀ ect \nadjusting its blend weight will have.\nNamed variables.\n• \n Some engines allow names to be assigned to the indi-\nvidual control parameters. The controlling code can look up a control \nparameter by name in order to adjust its value.\nControl structure.\n• \n In other engines, a simple data structure, such as an \narray of ﬂ oating-point values or a C struct, contains all of the control \nparameters for the entire character. The nodes in the blend tree(s) are \nconnected to particular control parameters, either by being hard-coded \nto use certain struct members or by looking up the parameters by \nname or index.\n\n\n583 \nOf course, there are many other alternatives as well. Every animation en-\ngine tackles this problem in a slightly diﬀ erent way, but the net eﬀ ect is always \nroughly the same.\n11.11.5. Constraints\n We’ve seen how action state machines can be used to specify complex blend \ntrees and how a transition matrix can be used to control how transitions be-\ntween states should work. Another important aspect of character animation \ncontrol is to constrain the movement of the characters and/or objects in the \nscene in various ways. For example, we might want to constrain a weapon \nso that it always appears to be in the hand of the character who is carrying it. \nWe might wish to constrain two characters so that they line up properly when \nshaking hands. A character’s feet are oft en constrained so that they line up \nwith the ﬂ oor, and its hands might be constrained to line up with the rungs \non a ladder or the steering wheel of a vehicle. In this section, we’ll take a brief \nlook at how these constraints are handled in a typical animation system.\n11.11.5.1. Attachments\n Virtually all modern game engines permit objects to be att ached to one another. \nAt its simplest, object-to-object att achment involves constraining the position \nand/or orientation of a particular joint JA within the skeleton of object A so that \nit coincides with a joint JB in the skeleton of object B. An att achment is usually \na parent-child relationship. When the parent’s skeleton moves, the child object \nis adjusted to satisfy the constraint. However, when the child moves, the par-\nent’s skeleton is usually not aﬀ ected. This is illustrated in Figure 11.60.\nSometimes it can be convenient to introduce an oﬀ set between the parent \njoint and the child joint. For example, when placing a gun into a character’s \n… child \nskeleton \nfollows\nparent \nskeleton \nmoves…\nchild \nskeleton \nmoves…\n… parent \nskeleton \nunaffected\nFigure 11.60.  An attachment, showing how movement of the parent automatically produces \nmovement of the child but not vice-versa.\n11.11. Action State Machines\n\n\n584 \n11. Animation Systems\nhand, we could constrain the “Grip” joint of the gun so that it coincides with \nthe “RightWrist” joint of the character. However, this might not produce the \ncorrect alignment of the gun with the hand. One solution to this problem is \nto introduce a special joint into one of the two skeletons. For example, we \ncould add a “RightGun” joint to the character’s skeleton, make it a child of the \n“RightWrist” joint, and position it so that when the “Grip” joint of the gun is \nconstrained to it, the gun looks like it is being held naturally by the character. \nThe problem with this approach, however, is that it increases the number of \njoints in the skeleton. Each joint has a processing cost associated with anima-\ntion blending and matrix palett e calculation and a memory cost for storing its \nanimation keys. So adding new joints is oft en not a viable option.\nWe know that an additional joint added for att achment purposes will not \ncontribute to the pose of the character—it merely introduces an additional \ntransform between the parent and child joint in an att achment. What we re-\nally want, then, is a way to mark certain joints so that they can be ignored by \nthe animation blending pipeline but can still be used for att achment purposes. \nSuch special joints are sometimes called att ach points. They are illustrated in \nFigure 11.61.\nAtt ach points might be modeled in Maya just like regular joints or loca-\ntors , although many game engines deﬁ ne att ach points in a more convenient \nmanner. For example, they might be speciﬁ ed as part of the action state ma-\nchine text ﬁ le or via a custom GUI within the animation authoring tool. This \nallows the animators to focus only on the joints that aﬀ ect the look of the \ncharacter, while the power to control att achments is put conveniently into the \nhands of the people who need it—the game designers and the engineers.\n11.11.5.2. Interobject Registration\n The interactions between game characters and their environments is growing \never more complex and nuanced with each new title. Hence, it is important \nAttachment  \nis equivalent \nto a joint\nFigure 11.61.  An attach point acts like an extra joint between the parent and the child.\n\n\n585 \nto have a system that allows characters and objects to be aligned with one an-\nother when animating. Such a system can be used for in-game cinematics and \ninteractive gameplay elements alike.\nImagine that an animator, working in Maya or some other animation tool, \nsets up a scene involving two characters and a door object. The two charac-\nters shake hands, and then one of them opens the door and they both walk \nthrough it. The animator can ensure that all three actors in the scene line up \nperfectly. However, when the animations are exported, they become three \nseparate clips, to be played on three separate objects in the game world. The \ntwo characters might have been under AI or player control prior to the start of \nthis animated sequence. How, then, can we ensure that the three objects line \nup correctly with one another when the three clips are played back in-game?\nReference Locators\nOne good solution is to introduce a common reference point into all three \nanimation clips. In Maya, the animator can drop a locator (which is just a 3D \ntransform, much like a skeletal joint) into the scene, placing it anywhere that \nseems convenient. Its location and orientation are actually irrelevant, as we’ll \nsee. The locator is tagged in some way to tell the animation export tools that it \nis to be treated specially.\nWhen the three animation clips are exported, the tools store the position \nand orientation of the reference locator, expressed in coordinates that are rela-\ntive to the local object space of each actor , into all three clip’s data ﬁ les. Later, \nwhen the three clips are played back in-game, the animation engine can look \nup the relative position and orientation of the reference locator in all three \nclips. It can then transform the origins of the three objects in such a way as \nto make all three reference locators coincide in world space. The reference \nlocator acts much like an att ach point (Section 11.11.5.1) and, in fact, could be \nimplemented as one. The net eﬀ ect—all three actors now line up with one an-\nother, exactly as they had been aligned in the original Maya scene.\nyMaya\nxMaya\nReference \nLocator\nActor A\nActor B\nActor C\nFigure 11.62.  Original Maya scene containing three actors and a reference locator.\n11.11. Action State Machines\n\n\n586 \n11. Animation Systems\nFigure 11.62 illustrates how the door and the two characters from the \nabove example might be set up in a Maya scene. As shown in Figure 11.63, the \nreference locator appears in each exported animation clip (expressed in that \nactor’s local space). In-game, these local-space reference locators are aligned \nto a ﬁ xed world-space locator in order to re-align the actors, as shown in Fig-\nure 11.64.\nFinding the World-Space Reference Location\nWe’ve glossed over one important detail here—who decides what the world-\nspace position and orientation of the reference locator should be? Each anima-\ntion clip provides the reference locator’s transform in the coordinate space of \nits actor. But we need some way to deﬁ ne where that reference locator should \nbe in world space.\nIn our example with the door and the two characters shaking hands, one \nof the actors is ﬁ xed in the world (the door). So one viable solution is to ask the \ndoor for the location of the reference locator and then align the two characters \nto it. The commands to do accomplish this might look similar to the following \npseudocode.\nvoid playShakingHandsDoorSequence(\n \nActor& door,\n \nActor& characterA,\n \nActor& characterB)\n{\nActor B’s \nClip\nActor C’s \nClip\nActor A’s \nClip\nyA\nxA\nyB\nxB\nxC\nyC\nFigure 11.63.  The reference locator is encoded in each actor’s animation ﬁ le.\nyworld\nx world\nFixed reference \nin world space\nFigure 11.64.  At runtime, the local-space reference transforms are aligned to a world-space \nreference locator, causing the actors to line up properly.\n\n\n587 \n \n// Find the world-space transform of the reference  \n \n \n// locator as specified in the door’s animation.\n Transform \nrefLoc = getReferenceLocatorWs(door,\n  \"shake-hands-door\");\n \n// Play the door’s animation in-place. (It’s alread    \n \n// in the correct place in the world.)\nplayAnimation(\"shake-hands-door\", door);\n \n// Play the two characters’ animations relative to  \n \n \n// the world-space reference locator obtained from \n \n// the door.\nplayAnimationRelativeToReference\n(\"shake-hands-character-a\", characterA, refLoc);\nplayAnimationRelativeToReference\n(\"shake-hands-character-b\", characterB, refLoc);\n}\nAnother option is to deﬁ ne the world-space transform of the reference \nlocator independently of the three actors in the scene. We could place the ref-\nerence locator into the world using our world-building tool, for example (see \nSection 13.3). In this case, the pseudocode above should be changed to look \nsomething like this:\nvoid playShakingHandsDoorSequence(\n \nActor& door,\n \nActor& characterA,\n \nActor& characterB,\n \nActor& refLocatorActor)\n{\n \n// Find the world-space transform of the reference  \n \n \n// locator by simply querying the transform of an  \n \n \n// independent actor (presumably placed into the \n \n// world manually).\n Transform \nrefLoc = getActorTransformWs\n(refLocatorActor);\n \n// Play all animations relative to the world-space\n \n// reference locator obtained above.\nplayAnimationRelativeToReference(\"shake-hands-door\",\n  door, \nrefLoc);\nplayAnimationRelativeToReference\n(\"shake-hands-character-a\", characterA, refLoc);\nplayAnimationRelativeToReference\n(\"shake-hands-character-b\", characterB, refLoc);\n}\n11.11. Action State Machines\n\n\n588 \n11. Animation Systems\n11.11.5.3. Grabbing and Hand IK\n Even aft er using an att achment to connect two objects, we sometimes ﬁ nd that \nthe alignment does not look exactly right in-game. For example, a character \nmight be holding a riﬂ e in her right hand, with her left  hand supporting the \nstock. As the character aims the weapon in various directions, we may no-\ntice that the left  hand no longer aligns properly with the stock at certain aim \nangles. This kind of joint misalignment is caused by LERP blending. Even if \nthe joints in question are aligned perfectly in clip A and in clip B, LERP blend-\ning does not guarantee that those joints will be in alignment when A and B are \nblended together.\nOne solution to this problem is to use inverse kinematics (IK) to correct \nthe position of the left  hand. The basic approach is to determine the desired \ntarget position for the joint in question. IK is then applied to a short chain of \njoints (usually two, three, or four joints), starting with the joint in question \nand progressing up the hierarchy to its parent, grandparent, and so on. The \njoint whose position we are trying to correct is known as the end eﬀ ector. The \nIK solver adjusts the orientations of the end eﬀ ector’s parent joint(s) in order \nto get the end eﬀ ector as close as possible to the target.\nThe API for an IK system usually takes the form of a request to enable or \ndisable IK on a particular chain of joints, plus a speciﬁ cation of the desired \ntarget point. The actual IK calculation is usually done internally by the low-\nlevel animation pipeline. This allows it to do the calculation at the proper \ntime—namely, aft er intermediate local and global skeletal poses have been \ncalculated but before the ﬁ nal matrix palett e calculation.\nSome animation engines allow IK chains to be deﬁ ned a priori. For ex-\nample, we might deﬁ ne one IK chain for the left  arm, one for the right arm, \nand two for the two legs. Let’s assume for the purposes of this example that \na particular IK chain is identiﬁ ed by the name of its end-eﬀ ector joint. (Other \nengines might use an index or handle or some other unique identiﬁ er, but the \nconcept remains the same.) The function to enable an IK calculation might \nlook something like this:\nvoid enableIkChain(\n \nActor& actor,\n \nconst char* endEffectorJointName,\n \nconst Vector3& targetLocationWs);\nand the function to disable an IK chain might look like this:\nvoid disableIkChain(\n \nActor& actor,\n \nconst char* endEffectorJointName);\n",
      "page_number": 590,
      "chapter_number": 30,
      "summary": "This chapter covers segment 30 (pages 590-610). Key topics include state, animation, and animations. Optimization on the Xbox and Xbox 360\nRather than having specialized memory regions and a DMA controller to \nmove data from region to region, the Xbox and the Xbox 360 both employ a \nuniﬁ ed memory architecture.",
      "keywords": [
        "State",
        "Action State Machines",
        "blend tree",
        "Animation",
        "animation state machine",
        "State Machines",
        "blend",
        "tree",
        "animation state",
        "Action State",
        "transition",
        "Animation Systems",
        "character",
        "complex blend tree",
        "blend nodes"
      ],
      "concepts": [
        "state",
        "animation",
        "animations",
        "animators",
        "animating",
        "animates",
        "transitions",
        "transition",
        "transitional",
        "tree"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 1",
          "chapter": 25,
          "title": "Segment 25 (pages 227-237)",
          "relevance_score": 0.67,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 26,
          "title": "Segment 26 (pages 243-252)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 53,
          "title": "Segment 53 (pages 511-519)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 8,
          "title": "Segment 8 (pages 57-67)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 611-631)",
      "start_page": 611,
      "end_page": 631,
      "detection_method": "synthetic",
      "content": "589 \nIK is usually enabled and disabled relatively infrequently, but the world-\nspace target location must be kept up-to-date every frame (if the target is \nmoving). Therefore, the low-level animation pipeline always provides some \nmechanism for updating an active IK target point. For example, the pipeline \nmight allow us to call enableIkChain() multiple times. The ﬁ rst time it is \ncalled, the IK chain is enabled, and its target point is set. All subsequent calls \nsimply update the target point.\nIK is well-suited to making minor corrections to joint alignment when \nthe joint is already reasonably close to its target. It does not work nearly as \nwell when the error between a joint’s desired location and its actual location \nis large. Note also that most IK algorithms solve only for the position of a joint. \nYou may need to write additional code to ensure that the orientation of the end \neﬀ ector aligns properly with its target as well. IK is not a cure-all, and it may \nhave signiﬁ cant performance costs. So always use it judiciously.\n11.11.5.4. Motion Extraction and Foot IK\nIn games, we usually want the locomotion animations of our characters to \nlook realistic and “grounded.” One of the biggest factors contributing to the \nrealism of a locomotion animation is whether or not the feet slide around on \nthe ground. Foot sliding can be overcome in a number of ways, the most com-\nmon of which are motion extraction and foot IK.\nMotion Extraction\n Let’s imagine how we’d animate a character walking forward in a straight \nline. In Maya (or his or her animation package of choice), the animator makes \nFigure 11.65.  In the animation authoring package, the character moves forward in space, and \nits feet appear grounded.\n11.11. Action State Machines\n\n\n590 \n11. Animation Systems\nthe character take one complete step forward, ﬁ rst with the left  foot and then \nwith the right foot. The resulting animation clip is known as a locomotion cycle , \nbecause it is intended to be looped indeﬁ nitely, for as long as the character \nis walking forward in-game. The animator takes care to ensure that the feet \nof the character appear grounded and don’t slide as it moves. The character \nmoves from its initial location on frame 0 to a new location at the end of the \ncycle. This is shown in Figure 11.65.\nNotice that the local-space origin of the character remains ﬁ xed during \nthe entire walk cycle. In eﬀ ect, the character is “leaving his origin behind him” \nas he takes his step forward. Now imagine playing this animation as a loop. \nWe would see the character take one complete step forward, and then pop \nback to where he was on the ﬁ rst frame of the animation. Clearly this won’t \nwork in-game.\nTo make this work, we need to remove the forward motion of the charac-\nter, so that his local-space origin remains roughly under the center of mass of \nthe character at all times. We could do this by zeroing out the forward transla-\ntion of the root joint of the character’s skeleton. The resulting animation clip \nwould make the character look like he’s “moonwalking,” as shown in Fig-\nure 11.66.\nIn order to get the feet to appear to “stick” to the ground the way they \ndid in the original Maya scene, we need the character to move forward \nby just the right amount each frame. We could look at the distance the \ncharacter moved, divide by the amount of time it took for him to get there, \nand hence find his average movement speed. But a character’s forward \nFigure 11.66.  Walk cycle after zeroing out the root joint’s forward motion.\n\n\n591 \nspeed is not constant when walking. This is especially evident when a \ncharacter is limping (quick forward motion on the injured leg, followed \nby slower motion on the “good” leg), but it is true for all natural-looking \nwalk cycles.\nTherefore, before we zero out the forward motion of the root joint, we ﬁ rst \nsave the animation data in a special “extracted motion” channel. This data can \nbe used in-game to move the local-space origin of the character forward by the \nexact amount that the root joint had moved in Maya each frame. The net result \nis that the character will walk forward exactly as he was authored, but now \nhis local-space origin comes along for the ride, allowing the animation to loop \nproperly. This is shown in Figure 11.67.\nIf the character moves forward by 4 feet in the animation and the anima-\ntion takes one second to complete, then we know that the character is moving \nat an average speed of 4 feet/second. To make the character walk at a diﬀ erent \nspeed, we can simply scale the playback rate of the walk cycle animation. For \nexample, to make the character walk at 2 feet/second, we can simply play the \nanimation at half speed (R = 0.5).\nFoot IK\n Motion extraction does a good job of making a character’s feet appear ground-\ned when it is moving in a straight line (or, more correctly, when it moves in a \npath that exactly matches the path animated by the animator). However, a real \ngame character must be turned and moved in ways that don’t coincide with \nthe original hand-animated path of motion (e.g., when moving over uneven \nterrain). This results in additional foot sliding.\nFigure 11.67.  Walk cycle in-game, with extracted root motion data applied to the local-space \norigin of the character.\n11.11. Action State Machines\n\n\n592 \n11. Animation Systems\nOne solution to this problem is to use IK to correct for any sliding in the \nfeet. The basic idea is to analyze the animations to determine during which \nperiods of time each foot is fully in contact with the ground. At the moment a \nfoot contacts the ground, we note its world-space location. For all subsequent \nframes while that foot remains on the ground, we use IK to adjust the pose \nof the leg so that the foot remains ﬁ xed to the proper location. This technique \nsounds easy enough, but gett ing it to look and feel right can be very challeng-\ning. It requires a lot of iteration and ﬁ ne-tuning. And some natural human \nmotions—like leading into a turn by increasing your stride—cannot be pro-\nduced by IK alone.\nIn addition, there is a big trade-oﬀ  between the look of the animations \nand the feel of the character, particularly for a human-controlled character. \nIt’s generally more important for the player character control system to feel \nresponsive and fun than it is for the character’s animations to look perfect. The \nupshot is this: Do not take the task of adding foot IK or motion extraction to \nyour game lightly. Budget time for a lot of trail and error, and be prepared to \nmake trade-oﬀ s to ensure that your player character not only looks good but \nfeels good as well.\n11.11.5.5. Other Kinds of Constraints\nThere are plenty of other possible kinds of constraint systems that can be add-\ned to a game animation engine. Some examples include:\nLook-at\n• \n . This is the ability for characters to look at points of interest in \nthe environment. A character might look at a point with only his or her \neyes, with eyes and head, or with eyes, head, and a twist of the entire \nupper body. Look-at constraints are sometimes implemented using IK \nor procedural joint oﬀ sets, although a more natural look can oft en be \nachieved via additive blending.\nCover registration\n• \n . This is the ability for a character to align perfectly with \nan object that is serving as cover. This is oft en implemented via the ref-\nerence locator technique described above.\nCover entry and departure\n• \n . If a character can take cover, animation blend-\ning and custom entry and departure animations must usually be used \nto get the character into and out of cover.\nTraversal aids\n• \n . The ability for a character to navigate over, under, around, \nor through obstacles in the environment can add a lot of life to a game. \nThis is oft en done by providing custom animations and using a refer-\nence locator to ensure proper registration with the obstacle being over-\ncome.\n\n\n593 \n11.12. Animation Controllers\nThe animation pipeline provides high-speed animation posing and blending \nfacilities, but its interface is usually too cumbersome to be used directly by \ngameplay code. The action state machine provides a more convenient inter-\nface by allowing complex blend trees to be described, oft en in a data-driven \nmanner, and then encapsulated within easy-to-understand logical states. Tran-\nsitions between states can also be deﬁ ned, again oft en in a data-driven way, \nso that gameplay code can be writt en in a ﬁ re-and-forget manner, without \nhaving to micromanage every transition. The ASM system may also provide \na layering mechanism, allowing the motion of a character to be described by \nmultiple state machines running in parallel. But even given the relatively con-\nvenient interface provided by the action state machine, some game teams ﬁ nd \nit convenient to introduce a third layer of soft ware, aimed at providing higher-\nlevel control over how characters animate. As such, it is oft en implemented as \na collection of classes known as animation controllers .\nControllers tend to manage behaviors over relatively long periods of \ntime—on the order of a few seconds or more. Each animation controller is typ-\nically responsible for one type of gross character behavior, like how to behave \nwhen in cover, how to behave when locomoting from one place to another in \nthe game world, or how to drive a vehicle. A controller typically orchestrates \nall aspects of the character’s animation-related behavior. It adjusts blend fac-\ntors to control movement directions, aiming, and so on, manages state transi-\ntions, fades in and out layers, and does whatever else is needed to make the \ncharacter behave as desired.\nOne beneﬁ t of a controller-based design is that all of the code relating to a \nparticular behavioral category is localized in one place. This design also per-\nmits higher-level gameplay systems, like player mechanics or AI , to be writt en \nin a much simpler way, because all of the details of micromanaging the anima-\ntions can be extracted and hidden within the controllers.\nThe animation controller layer takes many diﬀ erent forms and is highly \ndependent upon the needs of the game and the soft ware design philosophies \nof the engineering team. Some teams don’t use animation controllers at all. \nOn other teams, the animation controllers may be tightly integrated into the \nAI and/or player mechanics systems. Still other teams implement a suite of \nrelatively general-purpose controllers that can be shared between the player \ncharacter and the NPCs . For bett er or for worse, there is no one standard way \nto implement animation controllers in the game industry (at least not yet).\n11.12. Animation Controllers\n\n\n595\n12\nCollision and Rigid\nBody Dynamics\nI\nn the real world, solid objects are inherently, well… solid. They generally \navoid doing impossible things, like passing through one another, all by \nthemselves. But in a virtual game world, objects don’t do anything unless we \ntell them to, and game programmers must make an explicit eﬀ ort to ensure \nthat objects do not pass through one another. This is the role of one of the cen-\ntral components of any game engine—the collision detection system.\nA game engine’s collision system is oft en closely integrated with a physics \nengine . Of course, the ﬁ eld of physics is vast, and what most of today’s game \nengines call “physics” is more accurately described as a rigid body dynamics \nsimulation. A rigid body is an idealized, inﬁ nitely hard, non-deformable solid \nobject. The term dynamics refers to the process of determining how these rigid \nbodies move and interact over time under the inﬂ uence of forces. A rigid body \ndynamics simulation allows motion to be imparted to objects in the game \nin a highly interactive and naturally chaotic manner—an eﬀ ect that is much \nmore diﬃ  cult to achieve when using canned animation clips to move things \nabout.\nA dynamics simulation makes heavy use of the collision detection system \nin order to properly simulate various physical behaviors of the objects in the \nsimulation, including bouncing oﬀ  one another, sliding under friction, rolling, \nand coming to rest. Of course, a collision detection system can be used stand-\nalone, without a dynamics simulation—many games do not have a “physics” \n\n\n596 \n12. Collision and Rigid Body Dynamics\nsystem at all. But all games that involve objects moving about in two- or three-\ndimensional space have some form of collision detection.\nIn this chapter, we’ll investigate the architecture of both a typical collision \ndetection system and a typical physics (rigid body dynamics) system. As we \ninvestigate the components of these two closely interrelated systems, we’ll \ntake a look at the mathematics and the theory that underlie them.\n12.1. Do You Want Physics in Your Game?\nNowadays, most game engines have some kind of physical simulation capa-\nbilities. Some physical eﬀ ects, like rag doll deaths, are simply expected by \ngamers. Other eﬀ ects, like ropes, cloth, hair, or complex physically driven ma-\nchinery can add that je ne sais quoi that sets a game apart from its competitors. \nIn recent years, some game studios have started experimenting with advanced \nphysical simulations, including approximate real-time ﬂ uid mechanics eﬀ ects \nand simulations of deformable bodies . But adding physics to a game is not \nwithout costs, and before we commit ourselves to implementing an exhaus-\ntive list of physics-driven features in our game, we should (at the very least) \nunderstand the trade-oﬀ s involved.\n12.1.1. Things You Can Do with a Physics System\n Here are just a few of the things you can do or have with a game physics \nsystem.\nz Detect collisions between dynamic objects and static world geometry.\nz Simulate free rigid bodies under the inﬂ uence of gravity and other forces.\nz Spring-mass systems.\nz Destructible buildings and structures.\nz Ray and shape casts (to determine line of sight, bullet impacts, etc.).\nz Trigger volumes (determine when objects enter, leave, or are inside pre-\ndeﬁ ned regions in the game world).\nz Allow characters to pick up rigid objects.\nz Complex machines (cranes, moving platform puzzles, and so on).\nz Traps (such as an avalanche of boulders).\nz Drivable vehicles with realistic suspensions.\nz Rag doll character deaths.\nz Powered rag doll: a realistic blend between traditional animation and \nrag doll physics.\n\n\n597 \n12.1. Do You Want Physics in Your Game?\nz Dangling props (canteens, necklaces, swords), semi-realistic hair, cloth-\ning movements.\nz Cloth simulations.\nz Water surface simulations and buoyancy.\nz Audio propagation.\nAnd the list goes on.\nWe should note here that in addition to running a physics simulation at \nruntime in our game, we can also run a simulation as part of an oﬄ  ine pre-\nprocessing step in order to generate an animation clip. A number of physics \nplug-ins are available for animation tools like Maya. This is also the approach \ntaken by the Endorphin package by NaturalMotion Inc. (htt p://www.natu-\nralmotion.com/ endorphin.htm). In this chapter, we’ll restrict our discussion \nto runtime rigid body dynamics simulations, but oﬀ -line tools are a power-\nful option, of which we should always remain aware as we plan our game \nprojects.\n12.1.2. Is Physics Fun?\n The presence of a rigid body dynamics system in a game does not necessarily \nmake the game fun. More oft en than not, the inherently chaotic behavior of a \nphysics sim can actually detract from the gameplay experience rather than en-\nhancing it. The fun derived from physics depends on many factors, including \nthe quality of the simulation itself, the care with which it has been integrated \nwith other engine systems, the selection of physics-driven gameplay elements \nversus elements that are controlled in a more direct manner, how the physical \nelements interact with the goals of the player and the abilities of the player \ncharacter, and the genre of game being made.\nLet’s take a look at a few broad game genres and how a rigid body dy-\nnamics system might ﬁ t into each one.\n12.1.2.1. Simulations (Sims)\nThe primary goal of a sim is to accurately reproduce a real-life experience. Ex-\namples include the Flight Simulator, Gran Turismo, and NASCAR Racing series \nof games. Clearly, the realism provided by a rigid body dynamics system ﬁ ts \nextremely well into these kinds of games.\n12.1.2.2. Physics Puzzle Games\nThe whole idea of a physics puzzle is to let the user play around with dynami-\ncally simulated toys. So obviously this kind of game relies almost entirely on \nphysics for its core mechanic. Examples of this genre include Bridge Builder, \n\n\n598 \n12. Collision and Rigid Body Dynamics\nThe Incredible Machine, the online game Fantastic Contraption, and Crayon Phys-\nics for the iPhone.\n12.1.2.3. Sandbox Games\nIn a sandbox game , there may be no objectives at all, or there may be a large \nnumber of optional objectives. The player’s primary objective is usually to \n“mess around” and explore what the objects in the game world can be made \nto do. Examples of sandbox games include Grand Theft  Auto, Spore, and Lit-\ntleBigPlanet.\nSandbox games can put a realistic dynamics simulation to good use, es-\npecially if much of the fun is derived from playing with realistic (or semi-\nrealistic) interactions between objects in the game world. So in these contexts, \nphysics can be fun in and of itself. However, many games trade realism for \nan increased fun factor (e.g., larger-than-life explosions, gravity that is stron-\nger or weaker than normal, etc.). So the dynamics simulation may need to be \ntweaked in various ways to achieve the right “feel.”\n12.1.2.4. Goal-Based and Story-Driven Games\nA goal-based game has rules and speciﬁ c objectives that the player must ac-\ncomplish in order to progress; in a story-driven game , telling a story is of par-\namount importance. Integrating a physics system into these kinds of games \ncan be tricky. We generally give away control in exchange for a realistic simula-\ntion, and this loss of control can inhibit the player’s ability to accomplish goals \nor the game’s ability to tell the story.\nFor example, in a character-based platformer game, we want the player \ncharacter to move in ways that are fun and easy to control but not necessar-\nily physically realistic. In a war game, we might want a bridge to explode \nin a realistic way, but we also may want to ensure that the debris doesn’t \nend up blocking the player’s only path forward. In these kinds of games, \nphysics is oft en not necessarily fun, and in fact it can oft en get in the way \nof fun when the player’s goals are at odds with the physically simulated \nbehaviors of the objects in the game world. Therefore, developers must be \ncareful to apply physics judiciously and take steps to control the behavior \nof the simulation in various ways to ensure it doesn’t get in the way of \ngameplay.\n12.1.3. Impact of Physics on a Game\n Adding a physics simulation to a game can have all sorts of impacts on the \nproject and the gameplay. Here are a few examples across various game de-\nvelopment disciplines.\n\n\n599 \n12.1.3.1. Design Impacts\nz Predictability. The inherent chaos and variability that sets a physically \nsimulated behavior apart from an animated one is also a source of un-\npredictability. If something absolutely must happen a certain way every \ntime, it’s usually bett er to animate it than to try to coerce your dynamics \nsimulation into producing the motion reliably.\nz Tuning and control. The laws of physics (when modeled accurately) are \nﬁ xed. In a game, we can tweak the value of gravity or the coeﬃ  cient of \nrestitution of a rigid body, which gives back some degree of control. \nHowever, the results of tweaking physics parameters are oft en indirect \nand diﬃ  cult to visualize. It’s much harder to tweak a force in order to \nget a character to move in the desired direction than it is to tweak an \nanimation of a character walking.\nz Emergent behaviors . Sometimes physics introduces unexpected features \ninto a game—for example, the rocket-launcher jump trick in Team For-\ntress Classic, the high-ﬂ ying exploding Warthog in Halo, and the ﬂ ying \n“surfb oards” in PsyOps.\nIn general, the game design should usually drive the physics require-\nments of a game engine—not the other way around.\n12.1.3.2. Engineering Impacts\nz Tools pipeline. A good collision/physics pipeline takes time to build and \nmaintain.\nz User interface. How does the player control the physics objects in the \nworld? Does he or she shoot them? Walk into them? Pick them up? Us-\ning a virtual arm, as in Trespasser? Using a “gravity gun,” as in Half-\nLife 2? \nz Collision detection. Collision models intended for use within a dynamics \nsimulation may need to be more detailed and more carefully construct-\ned than their non-physics-driven counterparts.\nz AI . Pathing may not be predictable in the presence of physically simu-\nlated objects. The engine may need to handle dynamic cover points that \ncan move or blow up. Can the AI use the physics to its advantage?\nz Animation and character motion.  Animation-driven objects can clip slight-\nly through one another with few or no ill eﬀ ects, but when driven by a \ndynamics simulation, objects may bounce oﬀ  one another in unexpected \nways or jitt er badly. Collision ﬁ ltering may need to be applied to permit \nobjects to interpenetrate slightly. Mechanisms may need to be put in \nplace to ensure that objects sett le and go to sleep properly.\n12.1. Do You Want Physics in Your Game?\n\n\n600 \n12. Collision and Rigid Body Dynamics\nz Rag doll physics. Rag dolls require a lot of ﬁ ne-tuning and oft en suﬀ er \nfrom instability in the simulation. An animation may drive parts of a \ncharacter’s body into penetration with other collision volumes—when \nthe character turns into a rag doll, these interpenetrations can cause \nenormous instability. Steps must be taken to avoid this.\nz Graphics. Physics-driven motion can have an eﬀ ect on renderable ob-\njects’ bounding volumes (where they would otherwise be static or more \npredictable). The presence of destructible buildings and objects can in-\nvalidate some kinds of precomputed lighting and shadow methods.\nz Networking and multiplayer . Physics eﬀ ects that do not aﬀ ect gameplay \nmay be simulated exclusively (and independently) on each client ma-\nchine. However, physics that has an eﬀ ect on gameplay (such as the \ntrajectory that a grenade follows) must be simulated on the server and \naccurately replicated on all clients.\nz Record and playback . The ability to record gameplay and play it back at \na later time is very useful as a debugging/testing aid, and it can also \nserve as a fun game feature. This feature is much more diﬃ  cult to imple-\nment in the presence of simulated dynamics because chaotic behavior \n(in which the simulation takes a very diﬀ erent path as a result of small \nchanges in initial conditions) and diﬀ erences in the timing of the phys-\nics updates can cause playbacks to fail to match the recorded original.\n12.1.3.3. Art Impacts\nz Additional tool and workﬂ ow complexity. The need to rig up objects with \nmass, friction, constraints, and other att ributes for consumption by the \ndynamics simulation makes the art department’s job more diﬃ  cult as \nwell.\nz More-complex content. We may need multiple visually identical versions of \nan object with diﬀ erent collision and dynamics conﬁ gurations for diﬀ er-\nent purposes—for example, a pristine version and a destructible version.\nz Loss of control . The unpredictability of physics-driven objects can make \nit diﬃ  cult to control the artistic composition of a scene.\n12.1.3.4. Other Impacts\nz Interdisciplinary impacts. The introduction of a dynamics simulation into \nyour game requires close cooperation between engineering, art, and de-\nsign.\nz Production impacts. Physics can add to a project’s development costs, \ntechnical and organizational complexity, and risk.\n\n\n601 \n12.2. Collision/Physics Middleware\nHaving explored the impacts, most teams today do choose to integrate \na rigid body dynamics system into their games. With some careful planning \nand wise choices along the way, adding physics to your game can be reward-\ning and fruitful. And as we’ll see below, third-party middleware is making \nphysics more accessible than ever.\n12.2. Collision/Physics Middleware\n Writing a collision system and rigid body dynamics simulation is challeng-\ning and time-consuming work. The collision/physics system of a game engine \ncan account for a signiﬁ cant percentage of the source code in a typical game \nengine. That’s a lot of code to write and maintain!\nThankfully, a number of robust, high-quality collision/physics engines are \nnow available, either as commercial products or in open-source form. Some of \nthese are listed below. For a discussion of the pros and cons of various phys-\nics SDKs, check out the on-line game development forums (e.g., htt p://www.\ngamedev.net/community/forums/topic.asp?topic_id=463024).\n12.2.1. I-Collide, SWIFT, V-Collide, and RAPID\nI-Collide is an open-source collision detection library developed by the Uni-\nversity of North Carolina at Chapel Hill (UNC). It can detect intersections \nbetween convex volumes. I-Collide has been replaced by a faster, more fea-\nture-rich library called SWIFT . UNC has also developed collision detection \nlibraries that can handle complex non-convex shapes, called V-Collide and \nRAPID . None of these libraries can be used right out of the box in a game, but \nthey might provide a good basis upon which to build a fully functional game \ncollision detection engine. You can read more about I-Collide, SWIFT, and \nthe other UNC geometry libraries at htt p://www.cs.unc.edu/~geom/I_COL-\nLIDE/.\n12.2.2. ODE\nODE stands for “Open Dynamics Engine ” (htt p://www.ode.org). As its name \nimplies, ODE is an open-source collision and rigid body dynamics SDK. Its \nfeature set is similar to a commercial product like Havok. Its beneﬁ ts include \nbeing free (a big plus for small game studios and school projects!) and the \navailability of full source code (which makes debugging much easier and \nopens up the possibility of modifying the physics engine to meet the speciﬁ c \nneeds of a particular game).\n\n\n602 \n12. Collision and Rigid Body Dynamics\n12.2.3. Bullet\nBullet is an open-source collision detection and physics library used by both \nthe game and ﬁ lm industries. Its collision engine is integrated with its dy-\nnamics simulation, but hooks are provided so that the collision system can \nbe used standalone or integrated with other physics engines. It supports con-\ntinuous collision detection (CCD)—also known as time of impact (TOI) collision \ndetection—which as we’ll see below can be extremely helpful when a simu-\nlation includes small, fast-moving objects. The Bullet SDK is available for \ndownload at htt p://code.google.com/p/bullet/, and the Bullet wiki is locat-\ned at htt p://www.bulletphysics.com/mediawiki-1.5.8/index.php?title=Main_\nPage.\n12.2.4. \nTrueAxis\nTrueAxis is another collision/physics SDK. It is free for non-commercial use. \nYou can learn more about TrueAxis at htt p://trueaxis.com.\n12.2.5. PhysX\nPhysX started out as a library called Novodex , produced and distributed by \nAgeia as part of their strategy to market their dedicated physics coprocessor. \nIt was bought by NVIDIA and is being retooled so that it can run using NVID-\nIA’s GPUs as a coprocessor. (It can also run entirely on a CPU, without GPU \nsupport.) It is available at htt p://www.nvidia.com/object/nvidia_physx.html. \nPart of Ageia ’s and NVIDIA’s marketing strategy has been to provide the CPU \nversion of the SDK entirely for free, in order to drive the physics coprocessor \nmarket forward. Developers can also pay a fee to obtain full source code and \nthe ability to customize the library as needed. PhysX is available for PC, Xbox \n360, PLAYSTATION 3, and Wii.\n12.2.6. Havok\nHavok is the gold standard in commercial physics SDKs, providing one of \nthe richest feature sets available and boasting excellent performance charac-\nteristics on all supported platforms. (It’s also the most expensive solution.) \nHavok is comprised of a core collision/physics engine, plus a number of \noptional add-on products including a vehicle physics system, a system for \nmodeling destructible environments, and a fully featured animation SDK \nwith direct integration into Havok’s rag doll physics system. It runs on PC, \nXbox 360, PLAYSTATION 3, and Wii and has been speciﬁ cally optimized \nfor each of these platforms. You can learn more about Havok at htt p://www.\nhavok.com.\n\n\n603 \n12.3. The Collision Detection System\n12.2.7. Physics Abstraction Layer (PAL)\nThe Physics Abstraction Layer (PAL) is an open-source library that allows \ndevelopers to work with more than one physics SDK on a single project. It \nprovides hooks for PhysX (Novodex), Newton, ODE, OpenTissue , Tokamak , \nTrueAxis, and a few other SDKs. You can read more about PAL at htt p://www.\nadrianboeing.com/pal/index.html.\n12.2.8. Digital Molecular Matter (DMM)\nPixelux Entertainment S.A., located in Geneva, Switzerland, has produced a \nunique physics engine that uses ﬁ nite element methods to simulate the dy-\nnamics of deformable and breakable objects, called Digital Molecular Mat-\nter (DMM). The engine has both an oﬄ  ine and a runtime component. It was \nreleased in 2008 and can be seen in action in LucasArts’ Star Wars: The Force \nUnleashed . A discussion of deformable body mechanics is beyond our scope \nhere, but you can read more about DMM at htt p://www.pixeluxentertain-\nment.com.\n12.3. The Collision Detection System\nThe primary purpose of a game engine’s collision detection system is to deter-\nmine whether any of the objects in the game world have come into contact . To \nanswer this question, each logical object is represented by one or more geo-\nmetric shapes . These shapes are usually quite simple, such as spheres, boxes, \nand capsules. However, more-complex shapes can also be used. The collision \nsystem determines whether or not any of the shapes are intersecting (i.e., over-\nlapping) at any given moment in time. So a collision detection system is es-\nsentially a gloriﬁ ed geometric intersection tester.\nOf course, the collision system does more than answer yes/no questions \nabout shape intersection. It also provides relevant information about the na-\nture of each contact. Contact information can be used to prevent unrealistic \nvisual anomalies on-screen, such as objects interpenetrating one another. This \nis generally accomplished by moving all interpenetrating objects apart prior \nto rendering the next frame. Collisions can provide support for an object—one \nor more contacts that together allow the object to come to rest, in equilibrium \nwith gravity and/or any other forces acting on it. Collisions can also be used \nfor other purposes, such as to cause a missile to explode when it strikes its \ntarget or to give the player character a health boost when he passes through \na ﬂ oating health pack. A rigid body dynamics simulation is oft en the most \ndemanding client of the collision system, using it to mimic physically realistic \n\n\n604 \n12. Collision and Rigid Body Dynamics\nbehaviors like bouncing, rolling, sliding, and coming to rest. But, of course, \neven games that have no physics system can still make heavy use of a collision \ndetection engine.\nIn this chapter, we’ll go on a brief high-level tour of how collision detec-\ntion engines work. For an in-depth treatment of this topic, a number of excel-\nlent books on real-time collision detection are available, including [12], [41], \nand [9].\n12.3.1. Collidable Entities\nIf we want a particular logical object in our game to be capable of colliding \nwith other objects, we need to provide it with a collision representation , describ-\ning the object’s shape and its position and orientation in the game world. This \nis a distinct data structure, separate from the object’s gameplay representation \n(the code and data that deﬁ ne its role and behavior in the game) and separate \nfrom its visual representation (which might be an instance of a triangle mesh, a \nsubdivision surface, a particle eﬀ ect, or some other visual representation).\nFrom the point of view of detecting intersections, we generally favor \nshapes that are geometrically and mathematically simple. For example, a rock \nmight be modeled as a sphere for collision purposes; the hood of a car might \nbe represented by a rectangular box ; a human body might be approximated \nby a collection of interconnected capsules (pill-shaped volumes). Ideally, we \nshould resort to a more-complex shape only when a simpler representation \nproves inadequate to achieve the desired behavior in the game. Figure 12.1 \nshows a few examples of using simple shapes to approximate object volumes \nfor collision detection purposes.\nHavok uses the term collidable to describe a distinct, rigid object that can \ntake part in collision detection. It represents each collidable with an instance \nof the C++ class hkpCollidable. PhysX calls its rigid objects actors and rep-\nresents them as instances of the class NxActor. In both of these libraries, a \ncollidable entity contains two basic pieces of information—a shape and a trans-\nFigure 12.1 Simple geometric shapes are often used to approximate the collision volumes of \nthe objects in a game.\n\n\n605 \nform. The shape describes the collidable’s geometric form, and the transform \ndescribes the shape’s position and orientation in the game world. Collidables \nneed transforms for three reasons:\nTechnically speaking, a shape only describes the form of an object (i.e., \n1. \nwhether it is a sphere, a box, a capsule, or some other kind of volume). \nIt may also describe the object’s size (e.g., the radius of a sphere or the \ndimensions of a box). But a shape is usually deﬁ ned with its center \nat the origin and in some sort of canonical orientation relative to the \ncoordinate axes. To be useful, a shape must therefore be transformed in \norder to position and orient it appropriately in world space.\nMany of the objects in a game are dynamic. Moving an arbitrarily \n2. \ncomplex shape through space could be expensive if we had to move \nthe features of the shape (vertices, planes, etc.) individually. But with a \ntransform, any shape can be moved in space inexpensively, no matt er \nhow simple or complex the shape’s features may be.\nThe information describing some of the more-complex kinds of shapes \n3. \ncan take up a non-trivial amount of memory. So it can be beneﬁ cial to \npermit more than one collidable to share a single shape description. For \nexample, in a racing game, the shape information for many of the cars \nmight be identical. In that case, all of the car collidables in the game can \nshare a single car shape.\nAny particular object in the game may have no collidable at all (if it doesn’t \nrequire collision detection services), a single collidable (if the object is a simple \nrigid body), or multiple collidables (each representing one rigid component of \nan articulated robot arm, for example).\n12.3.2. The Collision/Physics World\nA collision system typically keeps track of all of its collidable entities via a \nsingleton data structure known as the collision world . The collision world is a \ncomplete representation of the game world designed explicitly for use by the \ncollision detection system. Havok’s collision world is an instance of the class \nhkpWorld. Likewise, the PhysX world is an instance of NxScene. ODE uses \nan instance of class dSpace to represent the collision world; it is actually the \nroot of a hierarchy of geometric volumes representing all the collidable shapes \nin the game.\nMaintaining all collision information in a private data structure has a \nnumber of advantages over att empting to store collision information with the \ngame objects themselves. For one thing, the collision world need only contain \ncollidables for those game objects that can potentially collide with one another. \n12.3. The Collision Detection System\n\n\n606 \n12. Collision and Rigid Body Dynamics\nThis eliminates the need for the collision system to iterate over any irrelevant \ndata structures. This design also permits collision data to be organized in the \nmost eﬃ  cient manner possible. The collision system can take advantage of \ncache coherency to maximize performance, for example. The collision world \nis also an eﬀ ective encapsulation mechanism, which is generally a plus from \nthe perspectives of understandability, maintainability, testability, and the po-\ntential for soft ware reuse.\n12.3.2.1. The Physics World\nIf a game has a rigid body dynamics system, it is usually tightly integrated \nwith the collision system. It typically shares its “world” data structure with \nthe collision system, and each rigid body in the simulation is usually associat-\ned with a single collidable in the collision system. This design is commonplace \namong physics engines because of the frequent and detailed collision queries \nrequired by the physics system. It’s typical for the physics system to actually \ndrive the operation of the collision system, instructing it to run collision tests \nat least once, and sometimes multiple times, per simulation time step. For this \nreason, the collision world is oft en called the collision/physics world or some-\ntimes just the physics world.\nEach dynamic rigid body in the physics simulation is usually associated \nwith a single collidable object in the collision system (although not all collid-\nables need be dynamic rigid bodies). For example, in Havok, a rigid body is \nrepresented by an instance of the class hkpRigidBody, and each rigid body \nhas a pointer to exactly one hkpCollidable. In PhysX, the concepts of collid-\nable and rigid body are comingled—the NxActor class serves both purposes \n(although the physical properties of the rigid body are stored separately, in an \ninstance of NxBodyDesc). In both SDKs, it is possible to tell a rigid body that \nits location and orientation are to be ﬁ xed in space, meaning that it will be \nomitt ed from the dynamics simulation and will serve as a collidable only.\nDespite this tight integration, most physics SDKs do make at least some \natt empt to separate the collision library from the rigid body dynamics simu-\nlation. This permits the collision system to be used as a standalone library \n(which is important for games that don’t need physics but do need to detect \ncollisions). It also means that a game studio could theoretically replace a phys-\nics SDK’s collision system entirely, without having to rewrite the dynamics \nsimulation. (Practically speaking, this may be a bit harder than it sounds!)\n12.3.3. Shape Concepts\nA rich body of mathematical theory underlies the everyday concept of shape \n(see htt p://en.wikipedia.org/wiki/Shape). For our purposes, we can think of \n\n\n607 \n12.3. The Collision Detection System\na shape simply as a region of space described by a boundary, with a deﬁ nite \ninside and outside. In two dimensions, a shape has area, and its boundary is \ndeﬁ ned either by a curved line or by three or more straight edges (in which \ncase it’s a polygon ). In three dimensions, a shape has volume, and its boundary \nis either a curved surface or is composed of polygons (in which case is it called \na polyhedron ).\nIt’s important to note that some kinds of game objects, like terrain, rivers, \nor thin walls, might be best represented by surfaces . In three-space, a surface \nis a two-dimensional geometric entity with a front and a back but no inside \nor outside. Examples include planes, triangles, subdivision surfaces, and sur-\nfaces constructed from a group of connected triangles or other polygons. Most \ncollision SDKs provide support for surface primitives and extend the term \nshape to encompass both closed volumes and open surfaces.\nIt’s commonplace for collision libraries to allow surfaces to be given vol-\nume via an optional extrusion parameter. Such a parameter speciﬁ es how \n“thick” a surface should be. Doing this helps reduce the occurrence of missed \ncollisions between small, fast-moving objects and inﬁ nitesimally thin surfaces \n(the so-called “bullet through paper” problem—see Section 12.3.5.7).\n12.3.3.1. Intersection\nWe all have an intuitive notion of what an intersection is. Technically speak-\ning, the term comes from set theory (htt p://en.wikipedia.org/wiki/Intersec-\ntion_(set_theory)). The intersection of two sets is comprised of the subset of \nmembers that are common to both sets. In geometrical terms, the intersection \nbetween two shapes is just the (inﬁ nitely large!) set of all points that lie inside \nboth shapes.\n12.3.3.2. Contact\nIn games, we’re not usually interested in ﬁ nding the intersection in the strict-\nest sense, as a set of points. Instead, we want to know simply whether or not \ntwo objects are intersecting. In the event of a collision, the collision system will \nusually provide additional information about the nature of the contact . This \ninformation allows us to separate the objects in a physically plausible and ef-\nﬁ cient way, for example.\nCollision systems usually package contact information into a convenient \ndata structure that can be instanced for each contact detected. For example, \nHavok returns contacts as instances of the class hkContactPoint. Contact \ninformation oft en includes a separating vector —a vector along which we can \nslide the objects in order to eﬃ  ciently move them out of collision. It also typi-\ncally contains information about which two collidables were in contact, in-\n\n\n608 \n12. Collision and Rigid Body Dynamics\ncluding which individual shapes were intersecting and possibly even which \nindividual features of those shapes were in contact. The system may also re-\nturn additional information, such as the velocity of the bodies projected onto \nthe separating normal.\n12.3.3.3. Convexity\nOne of the most important concepts in the ﬁ eld of collision detection is the \ndistinction between convex and non-convex (i.e., concave ) shapes. Technically, a \nconvex shape is deﬁ ned as one for which no ray originating inside the shape \nwill pass through its surface more than once. A simple way to determine if a \nshape is convex is to imagine shrink-wrapping it with plastic ﬁ lm—if it’s con-\nvex, no air pockets will be left  under the ﬁ lm. So in two dimensions, circles, \nrectangles and triangles are all convex, but Pac Man is not. The concept ex-\ntends equally well to three dimensions.\nThe property of convexity is important because, as we’ll see, it’s generally \nsimpler and less computationally intensive to detect intersections between \nconvex shapes than concave ones. See htt p://en.wikipedia.org/wiki/Convex \nfor more information about convex shapes.\n12.3.4. Collision Primitives\nCollision detection systems can usually work with a relatively limited set of \nshape types. Some collision systems refer to these shapes as collision primitives \nbecause they are the fundamental building blocks out of which more-complex \nshapes can be constructed. In this section, we’ll take a brief look at some of the \nmost common types of collision primitives.\n12.3.4.1. Spheres\nThe simplest three-dimensional volume is a sphere . And as you might expect, \nspheres are the most eﬃ  cient kind of collision primitive. A sphere is repre-\nsented by a center point and a radius. This information can be conveniently \npacked into a four-element ﬂ oating-point vector—a format that works par-\nticularly well with SIMD math libraries.\n12.3.4.2. Capsules\nA capsule is a pill-shaped volume, composed of a cylinder and two hemispher-\nical end caps. It can be thought of as a swept sphere —the shape that is traced \nout as a sphere moves from point A to point B. (There are, however, some \nimportant diﬀ erences between a static capsule and a sphere that sweeps out a \ncapsule-shaped volume over time, so the two are not identical.) Capsules are \noft en represented by two points and a radius (Figure 12.2). Capsules are more \n\n\n609 \n12.3. The Collision Detection System\neﬃ  cient to intersect than cylinders or boxes, so they are oft en used to model \nobjects that are roughly cylindrical, such as the limbs of a human body.\n12.3.4.3. Axis-Aligned Bounding Boxes\nAn axis-aligned bounding box (AABB) is a rectangular volume (technically \nknown as a cuboid) whose faces are parallel to the axes of the coordinate sys-\ntem. Of course, a box that is axis-aligned in one coordinate system will not \nnecessarily be axis-aligned in another. So we can only speak about an AABB in \nthe context of the particular coordinate frame(s) with which it aligns.\nAn AABB can be conveniently deﬁ ned by two points: one containing the \nminimum coordinates of the box along each of the three principal axes and the \nother containing its maximum coordinates. This is depicted in Figure 12.3.\nThe primary beneﬁ t of axis-aligned boxes is that they can be tested for \ninterpenetration with other axis-aligned boxes in a highly eﬃ  cient manner. \nThe big limitation of using AABBs is that they must remain axis-aligned at \nall times if their computational advantages are to be maintained. This means \nthat if an AABB is used to approximate the shape of an object in the game, \nthe AABB will have to be recalculated whenever that object rotates. Even if \nan object is roughly box-shaped, its AABB may degenerate into a very poor \napproximation to its shape when the object rotates oﬀ -axis. This is shown in \nFigure 12.4.\nr\nr\nP2\nP1\nFigure 12.2.  A capsule can be represented by two points and a radius.\ny\nx\nxmin\nxmax\nymin\nymax\nFigure 12.3.  An axis-aligned box. \n",
      "page_number": 611,
      "chapter_number": 31,
      "summary": "Therefore, the low-level animation pipeline always provides some \nmechanism for updating an active IK target point Key topics include physics, physical, and games.",
      "keywords": [
        "rigid body dynamics",
        "collision detection system",
        "Collision",
        "collision detection",
        "collision system",
        "rigid body",
        "game",
        "Body Dynamics system",
        "physics",
        "Body Dynamics",
        "system",
        "Physics System",
        "character",
        "dynamics simulation",
        "detection system"
      ],
      "concepts": [
        "physics",
        "physical",
        "games",
        "collision",
        "collisions",
        "animation",
        "animations",
        "animate",
        "animator",
        "animated"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 3",
          "chapter": 20,
          "title": "Segment 20 (pages 190-197)",
          "relevance_score": 0.47,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 13,
          "title": "Segment 13 (pages 121-128)",
          "relevance_score": 0.47,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 33,
          "title": "Segment 33 (pages 314-325)",
          "relevance_score": 0.44,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 40,
          "title": "Segment 40 (pages 383-392)",
          "relevance_score": 0.42,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 16,
          "title": "Segment 16 (pages 140-149)",
          "relevance_score": 0.4,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 632-651)",
      "start_page": 632,
      "end_page": 651,
      "detection_method": "synthetic",
      "content": "610 \n12. Collision and Rigid Body Dynamics\n12.3.4.4. Oriented Bounding Boxes\nIf we permit an axis-aligned box to rotate relative to its coordinate system, \nwe have what is known as an oriented bounding box (OBB). It is oft en repre-\nsented by three half-dimensions (half-width, half-depth, and half-height) and \na transformation, which positions the center of the box and deﬁ nes its orien-\ntation relative to the coordinate axes. Oriented boxes are a commonly used \ncollision primitive because they do a bett er job at ﬁ tt ing arbitrarily oriented \nobjects, yet their representation is still quite simple.\n12.3.4.5. Discrete Oriented Polytopes (DOP)\nA discrete oriented polytope (DOP ) is a more-general case of the AABB and \nOBB. It is a convex polytope that approximates the shape of an object. A DOP \ncan be constructed by taking a number of planes at inﬁ nity and sliding them \nalong their normal vectors until they come into contact with the object whose \nshape is to be approximated. An AABB is a 6-DOP in which the plane normals \nare taken parallel to the coordinate axes. An OBB is also a 6-DOP in which \nthe plane normals are parallel to the object’s natural principal axes. A k-DOP \nis constructed from an arbitrary number of planes k. A common method of \nconstructing a DOP is to start with an OBB for the object in question and then \nbevel the edges and/or corners at 45 degrees with additional planes in an at-\ntempt to yield a tighter ﬁ t. An example of a k-DOP is shown in Figure 12.5.\ny\nx\ny\nx\nFigure 12.4.  An AABB is only a good approximation to a box-shaped object when the object’s \nprincipal axes are roughly aligned with the coorindate system’s axes.\nFigure 12.5.  An OBB that has been beveled on all eight corners is known as a 14-DOP.\n\n\n611 \n12.3.4.6. Arbitrary Convex Volumes\nMost collision engines permit arbitrary convex volumes to be constructed by \na 3D artist in a package like Maya. The artist builds the shape out of polygons \n(triangles or quads). An oﬀ -line tool analyzes the triangles to ensure that they \nactually do form a convex polyhedron. If the shape passes the convexity test, \nits triangles are converted into a collection of planes (essentially a k-DOP), rep-\nresented by k plane equations, or k points and k normal vectors. (If it is found \nto be non-convex, it can still be represented by a polygon soup—described in \nthe next section.) This approach is depicted in Figure 12.6.\nConvex volumes are more expensive to intersection-test than the simpler \ngeometric primitives we’ve discussed thus far. However, as we’ll see in Sec-\ntion 12.3.5.5, certain highly eﬃ  cient intersection-ﬁ nding algorithms such as \nGJK are applicable to these shapes because they are convex.\n12.3.4.7. Poly Soup\nSome collision systems also support totally arbitrary, non-convex shapes. \nThese are usually constructed out of triangles or other simple polygons. For \nFigure 12.6.  An arbitrary convex volume can be represented by a collection of intersecting \nplanes.\nFigure 12.7.  A poly soup is often used to model complex static surfaces such as terrain or \nbuildings.\n12.3. The Collision Detection System\n\n\n612 \n12. Collision and Rigid Body Dynamics\nthis reason, this type of shape is oft en called a polygon soup , or poly soup for \nshort. Poly soups are oft en used to model complex static geometry, such as \nterrain and buildings (Figure 12.7).\nAs you might imagine, detecting collisions with a poly soup is the most \nexpensive kind of collision test. In eﬀ ect, the collision engine must test every \nindividual triangle, and it must also properly handle spurious intersections \nwith triangle edges that are shared between adjacent triangles. As a result, \nmost games try to limit the use of poly soup shapes to objects that will not take \npart in the dynamics simulation.\nDoes a Poly Soup Have an Inside?\nUnlike convex and simple shapes, a poly soup does not necessarily represent \na volume—it can represent an open surface as well. Poly soup shapes oft en \ndon’t include enough information to allow the collision system to diﬀ erenti-\nate between a closed volume and an open surface. This can make it diﬃ  cult to \nknow in which direction to push an object that is interpenetrating a poly soup \nin order to bring the two objects out of collision.\nThankfully, this is by no means an intractable problem. Each triangle in a \npoly soup has a front and a back, as deﬁ ned by the winding order of its verti-\nces. Therefore, it is possible to carefully construct a poly soup shape so that all \nof the polygons’ vertex winding orders are consistent (i.e. adjacent triangles \nalways “face” in the same direction). This gives the entire poly soup a notion \nof “front” and “back.” If we also store information about whether a given poly \nsoup shape is open or closed (presuming that this fact can be ascertained by \noﬀ -line tools), then for closed shapes, we can interpret “front” and “back” to \nmean “outside” and “inside” (or vice-versa, depending on the conventions \nused when constructing the poly soup).\nWe can also “fake” an inside and outside for certain kinds of open poly \nsoup shapes (i.e., surfaces). For example, if the terrain in our game is repre-\nsented by an open poly soup, then we can decide arbitrarily that the front \nof the surface always points away from the Earth. This implies that “front” \nshould always correspond to “outside.” Practically speaking, to make this \nwork, we would probably need to customize the collision engine in some way \nin order to make it aware of our particular choice of conventions.\n12.3.4.8. Compound Shapes\n Some objects that cannot be adequately approximated by a single shape can \nbe approximated well by a collection of shapes. For example, a chair might be \nmodeled out of two boxes—one for the back of the chair and one enclosing the \nseat and all four legs. This is shown in Figure 12.8.\n\n\n613 \nA compound shape can oft en be a more-eﬃ  cient alternative to a poly \nsoup for modeling non-convex objects; two or more convex volumes can oft en \nout-perform a single poly soup shape. What’s more, some collision systems \ncan take advantage of the convex bounding volume of the compound shape as \na whole when testing for collisions. In Havok, this is called midphase collision \ndetection. As the example in Figure 12.9 shows, the collision system ﬁ rst tests \nthe convex bounding volumes of the two compound shapes. If they do not \nintersect, the system needn’t test the subshapes for collisions at all.\n12.3.5. Collision Testing and Analytical Geometry\n A collision system makes use of analytical geometry —mathematical descrip-\ntions of three-dimensional volumes and surfaces—in order to detect inter-\nsections between shapes computationally. See htt p://en.wikipedia.org/wiki/\nAnalytic_geometry for more details on this profound and broad area of re-\nsearch. In this section, we’ll brieﬂ y introduce the concepts behind analytical \ngeometry, show a few common examples, and then discuss the generalized \nGJK intersection testing algorithm for arbitrary convex polyhedra.\nFigure 12.8.  A chair can be modeled using a pair of interconnected box shapes.\nB2\nB3\nB1\nB4\nA1\nA2\nSphere A\nSphere B\nA1\nA2\nB1\nB2\nB3\nB4\nBounding Volume \nHierarchies:\nSphere A\nSphere B\nFigure 12.9.  A collision system need only test the subshapes of a pair of compound shapes \nwhen their convex bounding volumes (in this case, Sphere A and Sphere B) are found to be \nintersecting.\n12.3. The Collision Detection System\n\n\n614 \n12. Collision and Rigid Body Dynamics\n12.3.5.1. Point versus Sphere\n We can determine whether a point p lies within a sphere by simply forming \nthe separation vector s between the point and the sphere’s center c and then \nchecking its length. If it is greater than the radius of the sphere r, then the \npoint lies outside the sphere; otherwise, it lies inside:\n;\nif \n, then  is inside.\nr\n= −\n≤\ns\nc\np\ns\np\n \n12.3.5.2. Sphere versus Sphere\n Determining if two spheres intersect is almost as simple as testing a point \nagainst a sphere. Again, we form a vector s connecting the center points of the \ntwo spheres. We take its length, and compare it with the sum of the radii of the \ntwo spheres. If the length of the separating vector is less than or equal to the \nsum of the radii, the spheres intersect; otherwise, they do not:\n1\n2\n1\n2\n;\nif \n(\n), then spheres intersect.\nr\nr\n=\n−\n≤\n+\ns\nc\nc\ns\n \n(12.1)\nTo avoid the square root operation inherent in calculating the length of vec-\ntor s, we can simply square the entire equation. So Equation (12.1) becomes\n \n \n \n1\n2\n2\n2\n2\n1\n2\n;\n;\nif \n(\n) , then spheres intersect.\nr\nr\n=\n−\n= ⋅\n≤\n+\ns\nc\nc\ns\ns s\ns\n12.3.5.3. The Separating Axis Theorem\nMost collision detection systems make heavy use of a theorem known as \nthe separating axis theorem (htt p://en.wikipedia.org/wiki/Separating_axis_\ntheorem). It states that if an axis can be found along which the projection of \ntwo convex shapes do not overlap, then we can be certain that the two shapes \ndo not intersect at all. If such an axis does not exist and the shapes are convex, \nthen we know for certain that they do intersect. (If the shapes are concave, then \nthey may not be interpenetrating despite the lack of a separating axis. This is \none reason why we tend to favor convex shapes in collision detection.)\nThis theorem is easiest to visualize in two dimensions. Intuitively, it says \nthat if a line can be found, such that object A is entirely on one side of the line \nand object B is entirely on the other side, then objects A and B do not overlap. \nSuch a line is called a separating line, and it is always perpendicular to the sepa-\nrating axis. So once we’ve found a separating line, it’s a lot easier to convince \nourselves that the theory is in fact correct by looking at the projections of our \nshapes onto the axis that is perpendicular to the separating line.\n\n\n615 \nThe projection of a two-dimensional convex shape onto an axis acts like \nthe shadow that the object would leave on a thin wire. It is always a line seg-\nment, lying on the axis, that represents the maximum extents of the object in \nthe direction of the axis. We can also think of a projection as a minimum and \nmaximum coordinate along the axis, which we can write as the fully closed \ninterval [ min\nc\n, max\nc\n]. As you can see in Figure 12.10, when a separating line ex-\nists between two shapes, their projections do not overlap along the separating \naxis. However, the projections may overlap along other, non-separating axes.\nIn three dimensions, the separating line becomes a separating plane, but \nthe separating axis is still an axis (i.e., an inﬁ nite line). Again, the projection of \na three-dimensional convex shape onto an axis is a line segment, which we can \nrepresent by the fully-closed interval [ min\nc\n, max\nc\n].\nSome types of shapes have properties that make the potential separating \naxes obvious. To detect intersections between two such shapes A and B, we can \nproject the shapes onto each potential separating axis in turn and then check \nwhether or not the two projection intervals, [ min\nA\nc\n, max\nA\nc\n] and [ min\nB\nc\n, max\nB\nc\n], are dis-\njoint (i.e., do not overlap). In math terms, the intervals are disjoint if max\nA\nc\n < min\nB\nc\nor if max\nB\nc\n < min\nA\nc\n. If the projection intervals along one of the potential separating \naxes are disjoint, then we’ve found a separating axis, and we know the two \nshapes do not intersect.\nOne example of this principle in action is the sphere-versus-sphere test. \nIf two spheres do not intersect, then the axis parallel to the line segment join-\ning the spheres’ center points will always be a valid separating axis (although \nother separating axes may exist, depending on how far apart the two spheres \nare). To visualize this, consider the limit when the two spheres are just about \nto touch but have not yet come into contact. In that case, the only separating \nA\nB\nNon-Separating Axis\nSeparating Axis\nSeparating \nLine/Plane\nProjection of A\nProjection of B\nA\nB\nFigure 12.10.  The projections of two shapes onto a separating axis are always two disjoint \nline segments. The projections of these same shapes onto a non-separating axis are not \nnecessarily disjoint. If no separating axis exists, the shapes intersect.\n12.3. The Collision Detection System\n\n\n616 \n12. Collision and Rigid Body Dynamics\naxis is the one parallel to the center-to-center line segment. As the spheres \nmove apart, we can rotate the separating axis more and more in either direc-\ntion. This is shown in Figure 12.11.\n12.3.5.4. AABB versus AABB\n To determine whether two AABBs are intersecting, we can again apply the \nseparating axis theorem. The fact that the faces of both AABBs are guaranteed \nto lie parallel to a common set of coordinate axes tells us that if a separating \naxis exists, it will be one of these three coordinate axes.\nSo, to test for intersections between two AABBs, which we’ll call A and B, \nwe merely inspect the minimum and maximum coordinates of the two boxes \nalong each axis independently. Along the x-axis, we have the two intervals \n[ min\nA\nx\n, max\nA\nx\n] and [ min\nB\nx\n, max\nB\nx\n], and we have corresponding intervals for the y- and \nz-axes. If the intervals overlap along all three axes, then the two AABBs are in-\ntersecting—in all other cases, they are not. Examples of intersecting and non-\nintersecting AABBs are shown in Figure 12.12 (simpliﬁ ed to two dimensions \nfor the purposes of illustration). For an in-depth discussion of AABB collision, \nsee htt p://www.gamasutra.com/features/20000203/lander_01.htm.\nSeparating\nLine/Plane\nSeparating Axis\nMany\nSeparating Axes\nMany\nSeparating\nLines/Planes\nFigure 12.11.  When two spheres are an inﬁ nitesimal distance apart, the only separating axis \nlies parallel to the line segment formed by the two spheres’ center points.\ny\nx\ny\nx\nFigure 12.12.  A two-dimensional example of intersecting and non-intersecting AABBs. Notice \nthat even though the second pair of AABBs are intersecting along the x-axis, they are not \nintersecting along the y-axis.\n\n\n617 \n12.3.5.5. Detecting Convex Collisions: The GJK Algorithm\nA very eﬃ  cient algorithm exists for detecting intersections between arbitrary \nconvex polytopes (i.e. convex polygons in two dimensions or convex polyhe-\ndra in three dimensions). It is known as the GJK algorithm, named aft er its \ninventors, E. G. Gilbert, D. W. Johnson, and S. S. Keerthi of the University \nof Michigan. Many papers have been writt en on the algorithm and its vari-\nants, including the original paper (htt p://ieeexplore.ieee.org/xpl/freeabs_all.\njsp?&arnumber=2083), an excellent SIGGRAPH PowerPoint presentation by \nChrister Ericson (htt p://realtimecollisiondetection.net/pubs/SIGGRAPH04_\nEricson_the_GJK_algorithm.ppt), and another great PowerPoint presentation \nby Gino van den Bergen (www.laas.fr/~nic/MOVIE/Workshop/Slides/Gino.\nvander.Bergen.ppt). However, the easiest-to-understand (and most entertain-\ning) description of the algorithm is probably Casey Muratori’s instructional \nvideo entitled, “Implementing GJK,” available online at htt p://mollyrocket.\ncom/353. Because these descriptions are so good, I’ll just give you a feel for the \nessence of the algorithm here and then direct you to the Molly Rocket website \nand the other references cited above for additional details.\nThe GJK algorithm relies on a geometric operation known as the Minkows-\nki diﬀ erence . This fancy-sounding operation is really quite simple: We take \nevery point that lies within shape B and subtract it pairwise from every point \ninside shape A. The resulting set of points { (Ai – Bj) } is the Minkowski dif-\nference.\nThe useful thing about the Minkowski diﬀ erence is that, when applied \nto two convex shapes, it will contain the origin if and only if those two shapes \nintersect. Proof of this statement is a bit beyond our scope, but we can intuit \nwhy it is true by remembering that when we say two shapes A and B intersect, \nwe really mean that there are points within A that are also within B. During the \nprocess of subtracting every point in B from every point in A, we would ex-\npect to eventually hit one of those shared points that lies within both shapes. \nA point minus itself is all zeros, so the Minkowski diﬀ erence will contain the \norigin if (and only if) sphere A and sphere B have points in common. This is \nillustrated in Figure 12.13.\nThe Minkowski diﬀ erence of two convex shapes is itself a convex shape. \nAll we care about is the convex hull of the Minkowski diﬀ erence, not all of the \ninterior points. The basic procedure of GJK is to try to ﬁ nd a tetrahedron (i.e., \na four-sided shape made out of triangles) that lies on the convex hull of the \nMinkwoski diﬀ erence and that encloses the origin. If one can be found, then \nthe shapes intersect; if one cannot be found, then they don’t.\nA tetrahedron is just one case of a geometrical object known as a simplex . \nBut don’t let that name scare you—a simplex is just a collection of points. A \n12.3. The Collision Detection System\n\n\n618 \n12. Collision and Rigid Body Dynamics\nsingle-point simplex is a point, a two-point simplex is a line segment, a three-\npoint simplex is a triangle, and a four-point simplex is a tetrahedron (see Fig-\nure 12.14).\nGJK is an iterative algorithm that starts with a one-point simplex lying \nanywhere within the Minkowski diﬀ erence hull. It then att empts to build \nhigher-order simplexes that might potentially contain the origin. During each \niteration of the loop, we take a look at the simplex we currently have and \ndetermine in which direction the origin lies relative to it. We then ﬁ nd a sup-\nporting vertex of the Minkowski diﬀ erence in that direction—i.e., the vertex \nof the convex hull that is closest to the origin in the direction we’re currently \ngoing. We add that new point to the simplex, creating a higher-order simplex \n(i.e., a point becomes a line segment, a line segment becomes a triangle, and \na triangle becomes a tetrahedron). If the addition of this new point causes the \nsimplex to surround the origin, then we’re done—we know the two shapes \nintersect. On the other hand, if we are unable to ﬁ nd a supporting vertex that \nis closer to the origin than the current simplex, then we know that we can \nnever get there, which implies that the two shapes do not intersect. This idea \nis illustrated in Figure 12.15.\nContains the Origin\ny\nx\nA – B\nDoes not Contain \nthe Origin\ny\nA – B\nA\nB\nA\nB\nx\nFigure 12.13.  The Minkowski difference of two intersecting convex shapes contains the origin, \nbut the Minkowski difference of two non-intersecting shapes does not.\nLine Segment\nPoint\nTriangle\nTetrahedron\nFigure 12.14.  Simplexes containing one, two, three, and four points.\n\n\n619 \nTo truly understand the GJK algorithm, you’ll need to check out the pa-\npers and video I refernce above. But hopefully this description will whet your \nappetite for deeper investigation. Or, at the very least, you can impress your \nfriends by dropping the name “GJK” at parties.\n12.3.5.6. Other Shape-Shape Combinations\nWe won’t cover any of the other shape-shape intersection combinations here, \nas they are covered well in other texts such as [12], [41], and [9]. The key point \nto recognize here, however, is that the number of shape-shape combinations is \nvery large. In fact, for N shape types, the number of pairwise tests required \nis O(N2). Much of the complexity of a collision engine arises because of the \nsheer number of intersection cases it must handle. This is one reason why \nthe authors of collision engines usually try to limit the number of primitive \ntypes—doing so drastically reduces the number of cases the collision detector \nmust handle. (This is also why GJK is popular—it handles collision detection \nbetween all convex shape types in one fell swoop. The only thing that diﬀ ers \nfrom shape type to shape type is the support function used in the algorithm.)\nThere’s also the practical matt er of how to implement the code that se-\nlects the appropriate collision-testing function given two arbitrary shapes that \nare to be tested. Many collision engines use a double dispatch method (htt p://\nen.wikipedia.org/wiki/Double_dispatch). In single dispatch (i.e., virtual func-\ntions), the type of a single object is used to determine which concrete imple-\nmentation of a particular abstract function should be called at runtime. Dou-\nble dispatch extends the virtual function concept to two object types. It can \nbe implemented via a two-dimensional function look-up table keyed by the \ntypes of the two objects being tested. It can also be implemented by arrang-\ning for a virtual function based on the type of object A to call a second virtual \nfunction based on the type of object B.\nLet’s take a look at a real-world example. Havok uses objects known as \ncollision agents (classes derived from hkCollisionAgent) to handle specif-\nNew Point\ny\nx\nNew Point\ny\nx\nSearch \nDirection\nSearch \nDirection\nFigure 12.15.  In the GJK algorithm, if adding a point to the current simplex creates a shape that \ncontains the origin, we know the shapes intersect; if there is no supporting vertex that will \nbring the simplex any closer to the origin, then the shapes do not intersect.\n12.3. The Collision Detection System\n\n\n620 \n12. Collision and Rigid Body Dynamics\nic intersection test cases. Concrete agent classes include hkpSphereSphere\nAgent, hkpSphereCapsuleAgent, hkpGskConvexConvexAgent, and so on. \nThe agent types are referenced by what amounts to a two-dimensional dis-\npatch table, managed by the class hkpCollisionDispatcher. As you’d ex-\npect, the dispatcher’s job is to eﬃ  ciently look up the appropriate agent given \na pair of collidables that are to be collision-tested and then call it, passing the \ntwo collidables as arguments.\n12.3.5.7. Detecting Collisions Between Moving Bodies\n Thus far, we’ve considered only static intersection tests between stationary ob-\njects. When objects move, this introduces some additional complexity. Motion \nin games is usually simulated in discrete time steps. So one simple approach is \nto treat the positions and orientations of each rigid body as stationary at each \ntime step and use static intersection tests on each “snapshot” of the collision \nworld. This technique works as long as objects aren’t moving too fast relative \nto their sizes. In fact, it works so well that many collision/physics engines, \nincluding Havok, use this approach by default.\nHowever, this technique breaks down for small, fast-moving objects. \nImagine an object that is moving so fast that it covers a distance larger than \nits own size (measured in the direction of travel) between time steps. If we \nwere to overlay two consecutive snapshots of the collision world, we’d notice \nthat there is now a gap between the fast-moving object’s images in the two \nsnapshots. If another object happens to lie within this gap, we’ll miss the colli-\nsion with it entirely. This problem, illustrated in Figure 12.16, is known as the \n“bullet through paper” problem, also known as “tunneling.” The following \nsections describe a number of common ways to overcome this problem.\nFigure 12.16.  A small, fast-moving object can leave gaps in its motion path between consecutive \nsnapshots of the collision world, meaning that collisions might be missed entirely.\nSwept Shapes\nOne way to avoid tunneling is to make use of swept shapes . A swept shape is \na new shape formed by the motion of a shape from one point to another over \ntime. For example, a swept sphere is a capsule, and a swept triangle is a trian-\ngular prism (see Figure 12.17).\n\n\n621 \nRather than testing static snapshots of the collision world for intersec-\ntions, we can test the swept shapes formed by moving the shapes from their \npositions and orientations in the previous snapshot to their positions and ori-\nentations in the current snapshot. This approach amounts to linearly interpo-\nlating the motion of the collidables between snapshots, because we generally \nsweep the shapes along line segments from snapshot to snapshot.\nOf course, linear interpolation may not be a good approximation of the \nmotion of a fast-moving collidable. If the collidable is following a curved path, \nthen theoretically we should sweep its shape along that curved path. Unfortu-\nnately, a convex shape that has been swept along a curve is not itself convex, \nso this can make our collision tests much more complex and computationally \nintensive.\nIn addition, if the convex shape we are sweeping is rotating, the resulting \nswept shape is not necessarily convex, even when it is swept along a line seg-\nment. As Figure 12.18 shows, we can always form a convex shape by linearly \nextrapolating the extreme features of the shapes from the previous and cur-\nrent snapshots—but the resulting convex shape is not necessarily an accurate \nrepresentation of what the shape really would have done over the time step. \nPut another way, a linear interpolation is not appropriate in general for ro-\ntating shapes. So unless our shapes are not permitt ed to rotate, intersection \nFigure 12.17.  A swept sphere is a capsule; a swept triangle is a triangular prism.\nFigure 12.18.  A rotating object swept along a line segment does not necessarily generate a \nconvex shape (left). A linear interpolation of the motion does form a convex shape (right), but \nit can be a fairly inaccurate approximation of what actually happened during the time step.\n12.3. The Collision Detection System\n\n\n622 \n12. Collision and Rigid Body Dynamics\ntesting of swept shapes becomes much more complex and computationally \nintensive than its static snapshot-based counterpart.\nSwept shapes can be a useful technique for ensuring that collisions are \nnot missed between static snapshots of the collision world state. However, the \nresults are generally inaccurate when linearly interpolating curved paths or \nrotating collidables, so more-detailed techniques may be required depending \non the needs of the game.\nContinuous Collision Detection (CCD)\nAnother way to deal with the tunneling problem is to employ a technique \nknown as continuous collision detection (CCD). The goal of CCD is to ﬁ nd the \nearliest time of impact (TOI) between two moving objects over a given time in-\nterval.\nCCD algorithms are generally iterative in nature. For each collidable, we \nmaintain both its position and orientation at the previous time step and its \nposition and orientation at the current time. This information can be used \nto linearly interpolate the position and rotation independently, yielding an \napproximation of the collidable’s transform at any time between the previ-\nous and current time steps. The algorithm then searches for the earliest TOI \nalong the motion path. A number of search algorithms are commonly used, \nincluding Brian Mirtich’s conservative advancement method, performing a ray \ncast on the Minkowski sum, or considering the minimum TOI of individual \nfeature pairs. Erwin Coumans of Sony Computer Entertainment describes \nsome of these algorithms in htt p://www.continuousphysics.com/BulletCon-\ntinuousCollisionDetection.pdf along with his own novel variation on the con-\nservative advancement approach.\n12.3.6. Performance Optimizations\nCollision detection is a CPU-intensive task for two reasons:\nThe calculations required to determine whether two shapes intersect are \n1. \nthemselves non-trivial.\nMost game worlds contain a large number of objects, and the number of in-\n2. \ntersection tests required grows rapidly as the number of objects increases.\nTo detect intersections between n objects, the brute-force technique would \nbe to test every possible pair of objects, yielding an O(n2) algorithm. However, \nmuch more eﬃ  cient algorithms are used in practice. Collision engines typically \nemploy some form of spatial hashing (htt p://research.microsoft .com/~hoppe/\nperfecthash.pdf), spatial subdivision, or hierarchical bounding volumes in or-\nder to reduce the number of intersection tests that must be performed.\n\n\n623 \n12.3.6.1. Temporal Coherency\nOne common optimization technique is to take advantage of temporal coher-\nency , also known as frame-to-frame coherency. When collidables are moving at \nreasonable speeds, their positions and orientations are usually quite similar \nfrom time step to time step. We can oft en avoid recalculating certain kinds \nof information every frame by caching the results across multiple time steps. \nFor example, in Havok, collision agents (hkpCollisionAgent) are usually \npersistent between frames, allowing them to reuse calculations from previous \ntime steps as long as the motion of the collidables in question hasn’t invali-\ndated those calculations.\n12.3.6.2. Spatial Partitioning\nThe basic idea of spatial partitioning is to greatly reduce the number of collid-\nables that need to be checked for intersection by dividing space into a number \nof smaller regions. If we can determine (in an inexpensive manner) that a pair \nof collidables do not occupy the same region, then we needn’t perform more-\ndetailed intersection tests on them.\nVarious hierarchical partitioning schemes, such as octrees , binary space \npartitioning (BSP) trees , kd-trees , or sphere trees , can be used to subdivide \nspace for the purposes of collision detection optimization. These trees subdi-\nvide space in diﬀ erent ways, but they all do so in a hierarchical fashion, start-\ning with a gross subdivision at the root of the tree and further subdividing \neach region until suﬃ  ciently ﬁ ne-grained regions have been obtained. The \ntree can then be walked in order to ﬁ nd and test groups of potentially collid-\ning objects for actual intersections. Because the tree partitions space, we know \nthat when we traverse down one branch of the tree, the objects in that branch \ncannot be colliding with objects in other sibling branches.\n12.3.6.3. Broad Phase, Midphase, and Narrow Phase\nHavok uses a three-tiered approach to prune the set of collidables that need to \nbe tested for collisions during each time step.\nz First, gross AABB tests are used to determine which collidables are po-\ntentially intersecting. This is known as broad phase collision detection.\nz Second, the coarse bounding volumes of compound shapes are tested. \nThis is known as midphase collision detection. For example, in a com-\npound shape composed of three spheres, the bounding volume might \nbe a fourth, larger sphere that encloses the other spheres. A compound \nshape may contain other compound shapes, so in general a compound \ncollidable has a bounding volume hierarchy. The midphase traverses \nthis hierarchy in search of subshapes that are potentially intersecting.\n12.3. The Collision Detection System\n\n\n624 \n12. Collision and Rigid Body Dynamics\nz Finally, the collidables’ individual primitives are tested for intersection. \nThis is known as narrow phase collision detection.\nThe Sweep and Prune Algorithm\nIn all of the major collision/physics engines (e.g., Havok, ODE, PhysX), broad \nphase collision detection employs an algorithm known as sweep and prune \n(htt p://en.wikipedia.org/wiki/Sweep_and_prune). The basic idea is to sort \nthe minimum and maximum dimensions of the collidables’ AABBs along the \nthree principal axes, and then check for overlapping AABBs by traversing \nthe sorted lists. Sweep and prune algorithms can make use of frame-to-frame \ncoherency (see Section 12.3.6.1) to reduce an O(n log n) sort operation to an \nexpected O(n) running time. Frame coherency can also aid in the updating of \nAABBs when objects rotate.\n12.3.7. Collision Queries\nAnother responsibility of the collision detection system is to answer hypo-\nthetical questions about the collision volumes in the game world. Examples \ninclude the following:\nz If a bullet travels from the player’s weapon in a given direction, what is \nthe ﬁ rst target it will hit, if any?\nz Can a vehicle move from point A to point B without striking anything \nalong the way?\nz Find all enemy objects within a given radius of a character.\nIn general, such operations are known as collision queries .\nThe most common kind of query is a collision cast, sometimes just called a \ncast. (The terms trace and probe are other common synonyms for “cast.”) A cast \ndetermines what, if anything, a hypothetical object would hit if it were to be \nplaced into the collision world and moved along a ray or line segment. Casts \nare diﬀ erent from regular collision detection operations because the entity be-\ning cast is not really in the collision world—it cannot aﬀ ect the other objects \nin the world in any way. This is why we say that a collision cast answers hypo-\nthetical questions about the collidables in the world.\n12.3.7.1. Ray Casting\nThe simplest type of collision cast is a ray cast , although this name is actually a \nbit of a misnomer. What we’re really casting is a directed line segment —in other \nwords, our casts always have a start point (p0) and an end point (p1). (Most \ncollision systems do not support inﬁ nite rays, due to the parametric formula-\ntion used—see below.) The cast line segment is tested against the collidable \n\n\n625 \nobjects in the collision world. If it intersects any of them, the contact point or \npoints are returned.\nRay casting systems typically describe the line segment via its start point \np0 and a delta vector d that, when added to p0 , yields the end point p1. Any \npoint on this line segment can be found via the following parametric equation , \nwhere the parameter t is permitt ed to vary between zero and one:\n \n \n \n0\n( )\n,   \n[0, 1].\nt =\n+\n∈\np\np\nd\nt\nt\n \nClearly, p0 = p(0) and p1 = p(1). In addition, any contact point along the seg-\nment can be uniquely described by specifying the value of the parameter t cor-\nresponding to the contact. Most ray casting APIs return their contact points as \n“t values,” or they permit a contact point to be converted into its correspond-\ning t by making an additional function call.\nMost collision detection systems are capable of returning the earliest con-\ntact —i.e., the contact point that lies closest to p0 and corresponds to the small-\nest value of t. Some systems are also capable of returning a complete list of all \ncollidables that were intersected by the ray or line segment. The information \nreturned for each contact typically includes the t value, some kind of unique \nidentiﬁ er for the collidable entity that was hit, and possibly other information \nsuch as the surface normal at the point of contact or other relevant properties \nof the shape or surface that was struck. One possible contact point data struc-\nture is shown below.\nstruct RayCastContact\n{\n F32 \nm_t;            // the t value for this  \n \n         \n    // \ncontact\n U32 \nm_collidableId; // which collidable did we   \n         \n   // hit?\n \nVector  \nm_normal;       // surface normal at  \n \n \n         \n   // contact pt.\n \n// other information...\n};\nApplications of Ray Casts\nRay casts are used heavily in games. For example, we might want to ask the \ncollision system whether character A has a direct line of sight to character B. \nTo determine this, we simply cast a directed line segment from the eyes of \ncharacter A to the chest of character B. If the ray hits character B, we know that \nA can “see” B. But if the ray strikes some other object before reaching character \nB, we know that the line of sight is being blocked by that object. Ray casts \n12.3. The Collision Detection System\n\n\n626 \n12. Collision and Rigid Body Dynamics\nare used by weapon systems (e.g., to determine bullet hits), player mechanics \n(e.g., to determine whether or not there is solid ground beneath the character’s \nfeet), AI systems (e.g., line of sight checks, targeting , movement queries, etc.), \nvehicle systems (e.g., to locate and snap the vehicle’s tires to the terrain), and \nso on.\n12.3.7.2. Shape Casting\nAnother common query involves asking the collision system how far an imag-\ninary convex shape would be able to travel along a directed line segment be-\nfore it hits something solid. This is known as a sphere cast when the volume \nbeing cast is a sphere, or a shape cast in general. (Havok calls them linear casts.) \nAs with ray casts, a shape cast is usually described by specifying the start \npoint p0 , the distance to travel d, and of course the type, dimensions, and ori-\nentation of the shape we wish to cast.\nThere are two cases to consider when casting a convex shape.\nThe cast shape is already interpenetrating or contacting at least one other \n1. \ncollidable, preventing it from moving away from its starting location.\nThe cast shape is not intersecting with anything else at its starting loca-\n2. \ntion, so it is free to move a non-zero distance along its path.\nIn the ﬁ rst scenario, the collision system typically reports the contact (s) \nbetween the cast shape and all of the collidables with which it is initially in-\nterpenetrating. These contacts might be inside the cast shape or on its surface, \nas shown in Figure 12.19.\nIn the second case, the shape can move a non-zero distance along the line \nsegment before striking something. Presuming that it hits something, it will \nusually be a single collidable. However, it is possible for a cast shape to strike \nmore than one collidable simultaneously if its trajectory is just right. And of \ncourse, if the impacted collidable is a non-convex poly soup, the cast shape \nContacts\nd\nFigure 12.19.  A cast \nsphere that starts in \npenetration will be un-\nable to move, and the \npossibly many contact \npoints will lie inside the \ncast shape in general.\nContact\nContacts\nd\nd\nFigure 12.20.  If the starting location of a cast shape is not interpenetrating anything, then \nthe shape will move a non-zero distance along its line segment, and its contacts (if any) will \nalways be on its surface.\n\n\n627 \nmay end up touching more than one part of the poly soup simultaneously. We \ncan safely say that no matt er what kind of convex shape is cast, it is possible \n(albeit unlikely) for the cast to generate multiple contact points. The contacts \nwill always be on the surface of the cast shape in this case, never inside it (be-\ncause we know that the cast shape was not interpenetrating anything when it \nstarted its journey). This case is illustrated in Figure 12.20.\nAs with ray casts, some shape casting APIs report only the earliest contact (s) \nexperienced by the cast shape, while others allow the shape to continue along \nits hypothetical path, returning all the contacts it experiences on its journey. \nThis is illustrated in Figure 12.21.\nThe contact information returned by a shape cast is necessarily a bit more \ncomplex than it is for a ray cast. We cannot simply return one or more t val-\nues, because a t value only describes the location of the center point of the \nshape along its path. It tells us nothing of where, on the surface or interior of \nthe shape, it came into contact with the impacted collidable. As a result, most \nshape casting APIs return both a t value and the actual contact point, along \nwith other relevant information (such as which collidable was struck, the sur-\nface normal at the contact point, etc.).\nUnlike ray casting APIs, a shape casting system must always be capable of \nreporting multiple contacts . This is because even if we only report the contact \nwith the earliest t value, the shape may have touched multiple distinct collid-\nables in the game world, or it may be touching a single non-convex collidable \nat more than one point. As a result, collision systems usually return an array \nor list of contact point data structures, each of which might look something \nlike this:\nstruct ShapeCastContact\n{\n F32 \n \n \nm_t;            // the t value for this  \n \n         \n    // \ncontact\n U32 \n \n \nm_collidableId; // which collidable did we   \n         \n    // \nhit?\nContact 1\nd\nContact 2\nContact 3\nFigure 12.21.  A shape casting API might return all contacts instead of only the earliest con-\ntact.\n12.3. The Collision Detection System\n\n\n628 \n12. Collision and Rigid Body Dynamics\n \nPoint  \nm_contactPoint; // location of actual   \n \n         \n   // contact\n \nVector  \nm_normal;       // surface normal at  \n \n \n         \n   // contact pt.\n \n// other information...\n};\nGiven a list of contact points, we oft en want to distinguish between the \ngroups of contact points for each distinct t value. For example, the earliest \ncontact is actually described by the group of contact points that all share the \nminimum t in the list. It’s important to realize that collision systems may or \nmay not return their contact points sorted by t. If it does not, it’s almost always \na good idea to sort the results by t manually. This ensures that if one looks at \nthe ﬁ rst contact point in the list, it will be guaranteed to be among the earliest \ncontact points along the shape’s path.\nApplications of Shape Casts\nShape casts are extremely useful in games. Sphere casts can be used to de-\ntermine whether the virtual camera is in collision with objects in the game \nworld. Sphere or capsule casts are also commonly used to implement charac-\nter movement . For example, in order to slide the character forward on uneven \nterrain, we can cast a sphere or capsule that lies between the character’s feet \nin the direction of motion. We can adjust it up or down via a second cast, to \nensure that it remains in contact with the ground. If the sphere hits a very \nshort vertical obstruction, such as a street curb, it can “pop up” over the curb. \nIf the vertical obstruction is too tall, like a wall, the cast sphere can be slid \nhorizontally along the wall. The ﬁ nal resting place of the cast sphere becomes \nthe character’s new location next frame.\n12.3.7.3. Phantoms\nSometimes, games need to determine which collidable objects lie within \nsome speciﬁ c volume in the game world. For example, we might want the \nlist of all enemies that are within a certain radius of the player character. \nHavok supports a special kind of collidable object known as a phantom for \nthis purpose.\nA phantom acts much like a shape cast whose distance vector d is zero. \nAt any moment, we can ask the phantom for a list of its contacts with other \ncollidables in the world. It returns this data in essentially the same format that \nwould be returned by a zero-distance shape cast.\nHowever, unlike a shape cast, a phantom is persistent in the collision \nworld. This means that it can take full advantage of the temporal coherency \noptimizations used by the collision engine when detecting collisions between \n\n\n629 \n“real” collidables. In fact, the only diﬀ erence between a phantom and a regu-\nlar collidable is that it is “invisible” to all other collidables in the collision \nworld (and it does not take part in the dynamics simulation). This allows it to \nanswer hypothetical questions about what objects it would collide with were \nit a “real” collidable, but it is guaranteed not to have any eﬀ ect of the other \ncollidables—including other phantoms—in the collision world.\n12.3.7.4. Other Types of Queries\nSome collision engines support other kinds of queries in addition to casts. For \nexample, Havok supports closest point queries, which are used to ﬁ nd the set \nof points on other collidables that are closest to a given collidable in the colli-\nsion world.\n12.3.8. Collision Filtering\nIt is quite common for game developers to want to enable or disable collisions \nbetween certain kinds of objects. For example, most objects are permitt ed to \npass through the surface of a body of water —we might employ a buoyancy \nsimulation to make them ﬂ oat, or they might just sink to the bott om, but in \neither case we do not want the water’s surface to appear solid. Most collision \nengines allow contacts between collidables to be accepted or rejected based on \ngame-speciﬁ c critiera. This is known as collision ﬁ ltering .\n12.3.8.1. Collision Masking and Layers\nOne common ﬁ ltering approach is to categorize the objects in the world and \nthen use a look-up table to determine whether certain categories are permitt ed \nto collide with one another or not. For example, in Havok, a collidable can be \na member of one (and only one) collision layer. The default collision ﬁ lter in \nHavok, represented by an instance of the class hkpGroupFilter, maintains \na 32-bit mask for each layer, each bit of which tells the system whether or not \nthat particular layer can collide with one of the other layers.\n12.3.8.2. Collision Callbacks\nAnother ﬁ ltering technique is to arrange for the collision library to invoke a \ncallback function whenever a collision is detected. The callback can inspect the \nspeciﬁ cs of the collision and make the decision to either allow or reject the \ncollision based on suitable criteria. Havok also supports this kind of ﬁ ltering. \nWhen contact points are ﬁ rst added to the world, the contactPointAdded()\ncallback is invoked. If the contact point is later determined to be valid (it may \nnot be if an earlier TOI contact was found), the contactPointConﬁ rmed()\ncallback is invoked. The application may reject contact points in these call-\nbacks if desired.\n12.3. The Collision Detection System\n",
      "page_number": 632,
      "chapter_number": 32,
      "summary": "This chapter covers segment 32 (pages 632-651). Key topics include collision, shape, and points. Oriented boxes are a commonly used \ncollision primitive because they do a bett er job at ﬁ tt ing arbitrarily oriented \nobjects, yet their representation is still quite simple.",
      "keywords": [
        "Collision Detection System",
        "Collision",
        "Collision Detection",
        "shape",
        "Rigid Body Dynamics",
        "convex shape",
        "cast shape",
        "Point",
        "collision system",
        "Detection System",
        "collision world",
        "contact",
        "convex",
        "cast",
        "contact point"
      ],
      "concepts": [
        "collision",
        "shape",
        "points",
        "convex",
        "intersection",
        "intersecting",
        "intersections",
        "objects",
        "colliding",
        "collide"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 3",
          "chapter": 33,
          "title": "Segment 33 (pages 314-325)",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 40,
          "title": "Segment 40 (pages 381-388)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 55,
          "title": "Segment 55 (pages 529-536)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 19,
          "title": "Segment 19 (pages 173-181)",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 32,
          "title": "Segment 32 (pages 306-318)",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 652-672)",
      "start_page": 652,
      "end_page": 672,
      "detection_method": "synthetic",
      "content": "630 \n12. Collision and Rigid Body Dynamics\n12.3.8.3. Game-Speciﬁ c Collision Materials\nGame developers oft en need to categorize the collidable objects in the game \nworld, in part to control how they collide (as with collision ﬁ ltering) and in \npart to control other secondary eﬀ ects, such as the sound that is made or the \nparticle eﬀ ect that is generated when one type of object hits another. For ex-\nample, we might want to diﬀ erentiate between wood, stone, metal, mud, wa-\nter, and human ﬂ esh.\nTo accomplish this, many games implement a collision shape categoriza-\ntion mechanism similar in many respects to the material system used in the \nrendering engine. In fact, some game teams use the term collision material to \ndescribe this categorization. The basic idea is to associate with each collid-\nable surface a set of properties that deﬁ nes how that particular surface should \nbehave from a physical and collision standpoint. Collision properties can in-\nclude sound and particle eﬀ ects, physical properties like coeﬃ  cient of restitu-\ntion or friction coeﬃ  cients, collision ﬁ ltering information, and whatever other \ninformation the game might require.\nFor simple convex primitives, the collision properties are usually associ-\nated with the shape as a whole. For poly soup shapes, the properties might be \nspeciﬁ ed on a per-triangle basis. Because of this latt er usage, we usually try \nto keep the binding between the collision primitive and its collision material \nas compact as possible. A typical approach is to bind collision primitives to \ncollision materials via an 8-, 16-, or 32-bit integer. This integer indexes into \na global array of data structures containing the detailed collision properties \nthemselves.\n12.4. Rigid Body Dynamics\nMany game engines include a physics system for the purposes of simulating the \nmotion of the objects in the virtual game world in a somewhat physically real-\nistic way. Technically speaking, game physics engines are typically concerned \nwith a particular ﬁ eld of physics known as mechanics . This is the study of how \nforces aﬀ ect the behavior of objects. In a game engine, we are particularly \nconcerned with the dynamics of objects—how they move over time. Until very \nrecently, game physics systems have been focused almost exclusively on a \nspeciﬁ c subdiscipline of mechanics known as classical rigid body dynamics . This \nname implies that in a game’s physics simulation, two important simplifying \nassumptions are made:\nz Classical (Newtonian) mechanics . The objects in the simulation are as-\nsumed to obey Newton’s laws of motion . The objects are large enough \n\n\n631 \nthat there are no quantum eﬀ ects, and their speeds are low enough that \nthere are no relativistic eﬀ ects.\nz Rigid bodies . All objects in the simulation are perfectly solid and cannot \nbe deformed. In other words, their shape is constant. This idea meshes \nwell with the assumptions made by the collision detection system. Fur-\nthermore, the assumption of rigidity greatly simpliﬁ es the mathematics \nrequired to simulate the dynamics of solid objects.\nGame physics engines are also capable of ensuring that the motions of \nthe rigid bodies in the game world conform to various constraints . The most \ncommon constraint is that of non-penetration—in other words, objects aren’t \nallowed to pass through one another. Hence the physics system att empts to \nprovide realistic collision responses whenever bodies are found to be interpen-\netrating. This is one of the primary reasons for the tight interconnection be-\ntween the physics engine and the collision detection system.\nMost physics systems also allow game developers to set up other kinds of \nconstraints in order to deﬁ ne realistic interactions between physically simulat-\ned rigid bodies. These may include hinges, prismatic joints (sliders), ball joints, \nwheels, “rag dolls” to emulate unconscious or dead characters, and so on.\nThe physics system usually shares the collision world data structure, and \nin fact it usually drives the execution of the collision detection algorithm as \npart of its time step update routine. There is typically a one-to-one mapping \nbetween the rigid bodies in the dynamics simulation and the collidables man-\naged by the collision engine. For example, in Havok, an hkpRigidBody object \nmaintains a reference to one and only one hkpCollidable (although it is \npossible to create a collidable that has no rigid body). In PhysX, the two con-\ncepts are a bit more tightly integrated—an NxActor serves both as a collidable \nobject and as a rigid body for the purposes of the dynamics simulation. These \nrigid bodies and their corresponding collidables are usually maintained in a \nsingleton data structure known as the collision/physics world, or sometimes just \nthe physics world .\nThe rigid bodies in the physics engine are typically distinct from the logi-\ncal objects that make up the virtual world from a gameplay perspective. The \npositions and orientations of game objects can be driven by the physics simu-\nlation. To accomplish this, we query the physics engine every frame for the \ntransform of each rigid body, and apply it in some way to the transform of \nthe corresponding game object. It’s also possible for a game object’s motion, \nas determined by some other engine system (such as the animation system or \nthe character control system) to drive the position and rotation of a rigid body \nin the physics world. As mentioned in Section 12.3.1, a single logical game \nobject may be represented by one rigid body in the physics world, or by many. \n12.4. Rigid Body Dynamics\n\n\n632 \n12. Collision and Rigid Body Dynamics\nA simple object like a rock, weapon or barrel, might correspond to one rigid \nbody. But an articulated character or a complex machine might be composed \nof many interconnected rigid pieces.\nThe remainder of this chapter will be devoted to investigating how game \nphysics engines work. We’ll brieﬂ y introduce the theory that underlies rigid \nbody dynamics simulations. Then we’ll investigate some of the most common \nfeatures of a game physics system and have a look at how a physics engine \nmight be integrated into a game.\n12.4.1. Some Foundations\nA great many excellent books, articles, and slide presentations have been \nwritt en on the topic of classical rigid body dynamics . A solid foundation in \nanalytical mechanics theory can be obtained from [15]. Even more relevant \nto our discussion are texts like [34], [11], and [25], which have been writt en \nspeciﬁ cally about the kind of physics simulations done by games. Other texts, \nlike [1], [9], and [28], include chapters on rigid body dynamics for games. \nChris Hecker wrote a series of helpful articles on the topic of game physics for \nGame Developer Magazine; Chris has posted these and a variety of other useful \nresources at htt p://chrishecker.com/Rigid_Body_Dynamics. An informative \nslide presentation on dynamics simulation for games was produced by Rus-\nsell Smith, the primary author of ODE; it is available at htt p://www.ode.org/\nslides/parc/dynamics.pdf.\nIn this section, I’ll summarize the fundamental theoretical concepts that \nunderlie the majority of game physics engines. This will be a whirlwind tour \nonly, and by necessity I’ll have to omit some details. Once you’ve read this \nchapter, I strongly encourage you to read at least a few of the additional re-\nsources cited above.\n12.4.1.1. Units\nMost rigid body dynamics simulations operate in the MKS system of units . In \nthis system, distance is measured in meters (abbreviated “m”), mass is mea-\nsured in kilograms (abbreviated “kg”), and time is measured in seconds (ab-\nbreviated “s”). Hence the name MKS.\nYou could conﬁ gure your physics system to use other units if you wanted \nto, but if you do this, you need to make sure everything in the simulation \nis consistent. For example, constants like the acceleration due to gravity g, \nwhich is measured in m/s2 in the MKS system, would have to be re-expressed \nin whatever unit system you select. Most game teams just stick with MKS to \nkeep life simple.\n\n\n633 \n12.4.1.2. Separability of Linear and Angular Dynamics\nAn unconstrained rigid body is one that can translate freely along all three \nCartesian axes and that can rotate freely about these three axes as well. We say \nthat such a body has six degrees of freedom (DOF).\nIt is perhaps somewhat surprising that the motion of an unconstrained \nrigid body can be separated into two independent components:\nz Linear dynamics . This is a description of the motion of the body when \nwe ignore all rotational eﬀ ects. (We can use linear dynamics alone to \ndescribe the motion of an idealized point mass—i.e., a mass that is inﬁ ni-\ntesimally small and cannot rotate.)\nz Angular dynamics . This is a description of the rotational motion of the body.\nAs you can well imagine, this ability to separate the linear and angular com-\nponents of a rigid body’s motion is extremely helpful when analyzing or sim-\nulating its behavior. It means that we can calculate a body’s linear motion \nwithout regard to rotation—as if it were an idealized point mass—and then \nlayer its angular motion on top in order to arrive at a complete description of \nthe body’s motion.\n12.4.1.3. Center of Mass\nFor the purposes of linear dynamics, an unconstrained rigid body acts as \nthough all of its mass were concentrated at a single point known as the center \nof mass (abbreviated CM, or sometimes COM). The center of mass is essen-\ntially the balancing point of the body for all possible orientations. In other \nwords, the mass of a rigid body is distributed evenly around its center of mass \nin all directions.\nFor a body with uniform density, the center of mass lies at the centroid of \nthe body. That is, if we were to divide the body up into N very small pieces, \nadd up the positions of all these pieces as a vector sum, and then divide by the \nnumber of pieces, we’d end up with a prett y good approximation to the loca-\ntion of the center of mass. If the body’s density is not uniform, the position of \neach litt le piece would need to be weighted by that piece’s mass, meaning that \nin general the center of mass is really a weighted average of the pieces’ positions. \nSo we have\n \n \n \n \n,\ni\ni\ni\ni\ni\ni\nCM\ni\ni\nm\nm\nm\nm\n∀\n∀\n∀\n=\n=\n∑\n∑\n∑\nr\nr\nr\n \nwhere the symbol r represents a radius vector or position vector —i.e., a vector \nextending from the world space origin to the point in question. (These sums \n12.4. Rigid Body Dynamics\n\n\n634 \n12. Collision and Rigid Body Dynamics\nbecome integrals in the limit as the sizes and masses of the litt le pieces ap-\nproach zero.)\nThe center of mass always lies inside a convex body, although it may actu-\nally lie outside the body if it is concave. (For example, where would the center \nof mass of the lett er “C” lie?)\n12.4.2. Linear Dynamics\nFor the purposes of linear dynamics , the position of a rigid body can be fully \ndescribed by a position vector rCM that extends from the world space origin \nto the center of mass of the body, as shown in Figure 12.22. Since we’re using \nthe MKS system, position is measured in meters (m). For the remainder of \nthis discussion, we’ll drop the CM subscripts, as it is understood that we are \ndescribing the motion of the body’s center of mass.\ny\nx\nr CM\nFigure 12.22.  For the purposes of linear dynamics, the position of a rigid body can be fully \ndescribed by the position of its center of mass.\n12.4.2.1. Linear Velocity and Acceleration\nThe linear velocity of a rigid body deﬁ nes the speed and direction in which the \nbody’s CM is moving. It is a vector quantity, typically measured in meters per \nsecond (m/s). Velocity is the ﬁ rst time derivative of position, so we can write\n \nv\nr\nr\n( )\n( )\n( ),\nt\nd t\ndt\nt\n=\n= \u0002\n \nwhere the dot over the vector r denotes taking the derivative with respect to \ntime. Diﬀ erentiating a vector is the same as diﬀ erentiating each component \nindependently, so\n \nv t\ndr t\ndt\nr t\nx\nx\nx\n( )\n( )\n( ),\n=\n= \u0002\nand so on for the y- and z-components.\nLinear acceleration is the ﬁ rst derivative of linear velocity with respect to \ntime, or the second derivative of the position of a body’s CM versus time. Accel-\neration is a vector quantity, usually denoted by the symbol a. So we can write\n\n\n635 \na\nv\nv\nr\nr\n( )\n( )\n( )\n( )\n( ).\nt\nd\nt\ndt\nt\nd\nt\ndt\nt\n=\n=\n=\n=\n\u0002\n\u0002\u0002\n2\n2\n12.4.2.2. Force and Momentum\nA force is deﬁ ned as anything that causes an object with mass to accelerate or \ndecelerate. A force has both a magnitude and a direction in space, so all forces \nare represented by vectors. A force is oft en denoted by the symbol F. When N \nforces are applied to a rigid body, their net eﬀ ect on the body’s linear motion \nis found by simply adding up the force vectors:\n \nnet\n1\n.\nN\ni\ni=\n=∑\nF\nF  \n Newton’s famous Second Law states that force is proportional to accelera-\ntion and mass:\n \nF\na\nr\n( )\n( )\n( ).\nt\nm\nt\nm\nt\n=\n=\n \n \u0002\u0002\n \n(12.2)\nAs Newton’s law implies, force is measured in units of kilogram-meters per \nsecond squared (kg-m/s2). This unit is also called the Newton.\nWhen we multiply a body’s linear velocity by its mass, the result is a \nquantity known as linear momentum . It is customary to denote linear momen-\ntum with the symbol p:\n \n \n( )\n( ).\nt\nm\nt\n=\np\nv\n \nWhen mass is constant, Equation (12.2) holds true. But if mass is not con-\nstant, as would be the case for a rocket whose fuel is being gradually used up \nand converted into energy, Equation (12.2) is not exactly correct. The proper \nformulation is actually as follows:\n \n \n( )\n(\n( ))\n( )\n,\nd\nt\nd m\nt\nt\ndt\ndt\n=\n=\np\nv\nF\nwhich of course reduces to the more familiar F = ma when the mass is constant \nand can be brought outside the derivative. Linear momentum is not of much \nconcern to us. However, the concept of momentum will become relevant when \nwe discuss angular dynamics.\n12.4.3. Solving the Equations of Motion\n The central problem in rigid body dynamics is to solve for the motion of \nthe body, given a set of known forces acting on it. For linear dynamics, this \n12.4. Rigid Body Dynamics\n\n\n636 \n12. Collision and Rigid Body Dynamics\nmeans ﬁ nding v(t) and r(t) given knowledge of the net force Fnet(t) and pos-\nsibly other information, such as the position and velocity at some previous \ntime. As we’ll see below, this amounts to solving a pair of ordinary diﬀ er-\nential equations—one to ﬁ nd v(t) given a(t) and the other to ﬁ nd r(t) given \nv(t).\n12.4.3.1. Force as a Function\nA force can be constant, or it can be a function of time as shown above. A force \ncan also be a function of the position of the body, its velocity, or any number \nof other quantities. So in general, the expression for force should really be \nwritt en as follows:\nF\nr\nv\na\nt\nt\nm\nt\n, ( ),\n( ),\n( ).\n \n \n ...\n \n(\n)=\nt\n \n(12.3)\nThis can be rewritt en in terms of the position vector and its ﬁ rst and second \nderivatives as follows:\n \nF\nr\n r\nr\nt \nt\nm\nt \n, (  ), ( )  ,\n( ). \n \n \n ... \n  \n\u0002\n\u0002\u0002\n(\n)= \nt \nFor example, the force exerted by a spring is proportional to how far it has \nbeen stretched away from its natural resting position. In one dimension, with \nthe spring’s resting position at x = 0, we can write\n \n(\n)\n, ( )\n ( ),\nF t x t\nk x t\n=−\nwhere k is the spring constant , a measure of the spring’s stiﬀ ness.\nAs another example, the damping force exerted by a mechanical viscous \ndamper (a so-called dashpot) is proportional to the velocity of the damper’s \npiston. So in one dimension, we can write\n(\n)\n, ( )\n ( ),\nF t v t\nb v t\n=−\nwhere b is a viscous damping coeﬃ  cient.\n12.4.3.2. Ordinary Differential Equations\nIn general, an ordinary diﬀ erential equation (ODE) is an equation involving a \nfunction of one independent variable and various derivatives of that function. \nIf our independent variable is time and our function is x(t), then an ODE is a \nrelation of the form\nd x\ndt\nf\nx t\ndx t\ndt\nd x t\ndt\nd\nx t\ndt\nn\nn\nn\nn\n=\n⎛\n⎝\n−\n−\n,\n( ),\n( ) ,\n( ) ,\n,\n( )\n \n \n \n ...  \n2\n2\n1\n1\n⎜\n⎞\n⎠\n⎟.\nt\nPut another way, the nth derivative of x(t) is expressed as a function f whose \narguments can be time (t), position (x(t)), and any number of derivatives of \nx(t) as long as those derivatives are of lower order than n.\n\n\n637 \nAs we saw in Equation (12.3), force is a function of time, position, and velocity \nin general:\n\u0002\u0002\n\u0002\nr\nF\nr\nr\n( )\n, ( ), ( ) .\nt\nm\nt\nt\n=\n(\n)\n1\n t\n \n(12.18)\nThis clearly qualiﬁ es as an ODE. We wish to solve this ODE in order to ﬁ nd \nv(t) and r(t).\n12.4.3.3. Analytical Solutions\nIn some rare situations, the diﬀ erential equations of motion can be solved ana-\nlytically , meaning that a simple, closed-form function can be found that de-\nscribes the body’s position for all possible values of time t. A common example \nis the vertical motion of a projectile under the inﬂ uence of a constant accelera-\ntion due to gravity, a(t) = [ 0, g, 0 ], where g = –9.8 m/s2. In this case, the ODE \nof motion boils down to\n \n\u0002\u0002y t\ng\n( )\n.\n=\nIntegrating once yields\n \n\u0002y t\ngt\nv\n( )\n,\n=\n+\n0\nwhere v0 is the vertical velocity at time t = 0. Integrating a second time yields \nthe familiar solution\n \ny t\ngt\nv t\ny\n( )\n,\n=\n+\n+\n1\n2\n2\n0\n0  \nwhere y0 is the initial vertical position of the object.\nHowever, analytical solutions are almost never possible in game physics. \nThis is due in part to the fact that closed-form solutions to some diﬀ erential \nequations are simply not known. Moreover, a game is an interactive simula-\ntion, so we cannot predict how the forces in a game will behave over time. This \nmakes it impossible to ﬁ nd simple, closed-form expressions for the positions \nand velocities of the objects in the game as functions of time.\n12.4.4. Numerical Integration\nFor the reasons cited above, game physics engines turn to a technique known \nas numerical integration . With this technique, we solve our diﬀ erential equa-\ntions in a time-stepped manner—using the solution from a previous time step \nto arrive at the solution for the next time step. The duration of the time step is \nusually taken to be (roughly) constant and is denoted by the symbol Δt . Given \nthat we know the body’s position and velocity at the current time t1 and that \nthe force is known as a function of time, position, and/or velocity, we wish to \nﬁ nd the position and velocity at the next time step t2 = t1 + Δt. In other words, \ngiven r(t1), v(t1), and F(t, r, v), the problem is to ﬁ nd r(t2) and v(t2).\n12.4. Rigid Body Dynamics\n\n\n638 \n12. Collision and Rigid Body Dynamics\n12.4.4.1. Explicit Euler\nOne of the simplest numerical solutions to an ODE is known as the explicit \nEuler method. This is the intuitive approach oft en taken by new game program-\nmers. Let’s assume for the moment that we already know the current velocity \nand that we wish to solve the following ODE to ﬁ nd the body’s position on \nthe next frame:\n \nv\nr\n( )\n( ).\nt = \u0002 t  \n(12.4)\nUsing the explicit Euler method, we simply convert the velocity from meters \nper second into meters per frame by multiplying by the time delta, and then \nwe add “one frame’s worth” of velocity onto the current position in order to \nﬁ nd the new position on the next frame. This yields the following approxi-\nmate solution to the ODE given by Equation (12.4):\n \nr\nr\nv\n(\n)\n( )\n( )\n.\nt\nt\n2\n1\n1\n=\n+\n Δ\nt\nt  \n(12.5)\nWe can take an analogous approach to ﬁ nd the body’s velocity next frame \ngiven the net force acting this frame. Hence, the approximate explicit Euler \nsolution to the ODE\n \n \nis as follows:\n \n \n(12.6)\nInterpretations of Explicit Euler\nWhat we’re really doing in Equation (12.5) is assuming that the velocity of the \nbody is constant during the time step. Therefore, we can use the current velocity \nto predict the body’s position on the next frame. The change in position Δr be-\ntween times t1 and t2 is hence Δr = v(t1) Δt. Graphically, if we imagine a plot of the \nposition of the body versus time, we are taking the slope of the function at time \nΔr\nΔt\nt\nr(t1)\nrapprox(t2)\nr(t2)\nr(t)\nt1\nt2\nslope\n= v(t1)\nΔr\nΔt\nFigure 12.23.  In the explicit Euler method, the slope of r(t) at time t1 is used to linearly \nextrapolate from r(t1) to an estimate of the true value of r(t2).\na\nF\nv\n( )\n( )\n( )\nt\nt\nm\nt\n=\n=\nnet\n\u0002\nv\nv\nF\n( )\n( )\n( )\n.\nt\nt\nm\nt\n2\n1\n1\n=\n+\nnet\nΔ\nt\n\n\n639 \nt1 (which is just v(t1)) and extrapolating it linearly to the next time step t2. As \nwe can see in Figure 12.23, linear extrapolation does not necessarily provide us \nwith a particularly good estimate of the true position at the next time step r(t2), \nbut it does work reasonably well as long as the velocity is roughly constant.\nFigure 12.23 suggests another way to interpret the explicit Euler method—\nas an approximation of a derivative . By deﬁ nition, any derivative is the quo-\ntient of two inﬁ nitesimally small diﬀ erences (in our case, dr/dt). The explicit \nEuler method approximates this using the quotient of two ﬁ nite diﬀ erences . In \nother words, dr becomes Δr and dt becomes Δt. This yields\nd\ndt\nt\nt\nt\nt\nt\nt\nr\nr\nv\nr\nr\nr\nr\n≈\n=\n−\n−\n=\n−\nΔ\nΔ\nΔ\n;\n( )\n( )\n( )\n( )\n( ).\n1\n2\n1\n2\n1\n2\n1\nt\nt\nt\nwhich again simpliﬁ es to Equation (12.5). This approximation is really only \nvalid when the velocity is constant over the time step. It is also valid in the \nlimit as Δt tends toward zero (at which point it becomes exactly right). Obvi-\nously, this same analysis can be applied to Equation (12.6) as well.\n12.4.4.2. Properties of Numerical Methods\nWe’ve implied that the explicit Euler method is not particularly accurate. Let’s \npin this idea down more concretely. A numerical solution to an ordinary dif-\nferential equation actually has three important and interrelated properties:\nz Convergence . As the time step Δt tends toward zero, does the approxi-\nmate solution get closer and closer to the real solution?\nz Order . Given a particular numerical approximation to the solution of an \nODE, how “bad” is the error? Errors in numerical ODE solutions are \ntypically proportional to some power of the time step duration Δt, so \nthey are oft en writt en using big “O” notation (e.g., O(Δt2)). We say that \na particular numerical method is of “order n” when its error term is \nO(Δt(n + 1)).\nz Stability . Does the numerical solution tend to “sett le down” over time? \nIf a numerical method adds energy into the system, object velocities will \neventually “explode,” and the system will become unstable. On the other \nhand, if a numerical method tends to remove energy from the system, it \nwill have an overall damping eﬀ ect, and the system will be stable.\nThe concept of order warrants a litt le more explanation. We usually mea-\nsure the error of a numerical method by comparing its approximate equa-\ntion with the inﬁ nite Taylor series expansion of the exact solution to the ODE. \nWe then cancel terms by subtracting the two equations. The remaining Taylor \n12.4. Rigid Body Dynamics\n\n\n640 \n12. Collision and Rigid Body Dynamics\nterms represent the error inherent in the method. For example, the explicit \nEuler equation is\nr\nr\nr\n( )\n( )\n( )\n.\nt\nt\n2\n1\n1\n=\n+\u0002\n Δ\nt\nt  \nThe inﬁ nite Taylor series expansion of the exact solution is\n \nr\nr\nr\nr\nr\n( )\n( )\n( )\n( )\n( )\n....\nt\nt\nt\nt\n2\n1\n1\n1\n2\n1\n2\n1\n6\n1\n3\n=\n+\n+\n+\n+\n\u0002\n\u0002\u0002\n\u0002\u0002\u0002\n \n \n \nΔ\nΔ\nΔ\nt\nt\nt\nt\n \nTherefore, the error is represented by all of the terms aft er the v Δt term, which \nis of order O(Δt2) (because this term dwarfs the other higher-order terms):\n \nE\nr\nr\n=\n+\n+\n=\n1\n2\n1\n2\n1\n6\n1\n3\n2\n\u0002\u0002\n\u0002\u0002\u0002\n( )\n( )\n...\n(\n).\nt\nt\nO\nt\n \n \nΔ\nΔ\nΔ\nt\nt\n \nTo make the error of a method explicit, we’ll oft en write its equation with the \nerror term added in big “O” notation at the end. For example, the explicit Eu-\nler method’s equation is most accurately writt en as follows:\nWe say that the explicit Euler method is an “order one” method because it \nis accurate up to and including the Taylor series term involving Δt to the ﬁ rst \npower. In general, if a method’s error term is O(Δt(n + 1)), then it is said to be an \n“order n” method.\n12.4.4.3. Alternatives to Explicit Euler\nThe explicit Euler method sees quite a lot of use for simple integration tasks in \ngames, producing the best results when the velocity is nearly constant. How-\never, it is not used in general-purpose dynamics simulations because of its high \nerror and poor stability. There are all sorts of other numerical methods for solv-\ning ODEs, including backward Euler (another ﬁ rst-order method), midpoint \nEuler (a second-order method), and the family of Runge-Kutt a methods. (The \nfourth-order Runge-Kutt a, oft en abbreviated “RK4,” is particularly popular.) \nWe won’t describe these in any detail here, as you can ﬁ nd voluminous infor-\nmation about them online and in the literature. The Wikipedia page htt p://\nen.wikipedia.org/wiki/Numerical_ordinary_diﬀ erential_equations serves as \nan excellent jumping-oﬀ  point for learning these methods.\n12.4.4.4. Verlet Integration\nThe numerical ODE method most oft en used in interactive games these \ndays is probably the Verlet method, so I’ll take a moment to describe it in \nsome detail. There are actually two variants of this method: regular Verlet \nand the so-called velocity Verlet . I’ll present both methods here, but I’ll leave \nthe theory and deep explanations to the myriad papers and Web pages avail-\nr\nr\nr\n( )\n( )\n( )\n(\n).\nt\nt\nO\nt\n2\n1\n1\n2\n=\n+\n+\n\u0002\n Δ\nΔ\nt\nt\n\n\n641 \nable on the topic. (For a start, check out htt p://en.wikipedia.org/wiki/Verlet_\nintegration.)\nThe regular Verlet method is att ractive because it achieves a high order \n(low error), is relatively simple and inexpensive to evaluate, and produces a \nsolution for position directly in terms of acceleration in one step (as opposed \nto the two steps normally required to go from acceleration to velocity and then \nfrom velocity to position). The formula is derived by adding two Taylor series \nexpansions, one going forward in time and one going backward in time:\n \nr\nr\nr\nr\nr\n(\n)\n( )\n( )\n( )\n( )\n(\nt\nt\nt\nt\nt\nO\nt\n1\n1\n1\n1\n2\n1\n2\n1\n6\n1\n3\n+\n=\n+\n+\n+\n+\nΔ\nΔ\nΔ\nΔ\nΔ\n\u0002\n\u0002\u0002\n\u0002\u0002\u0002\n \n \n \n4);\nt\nt\nt\nt\n \n \nr\nr\nr\nr\nr\n(\n)\n( )\n( )\n( )\n( )\n(\nt\nt\nt\nt\nt\nO\nt\n1\n1\n1\n1\n2\n1\n2\n1\n6\n1\n3\n−\n=\n−\n+\n−\n+\nΔ\nΔ\nΔ\nΔ\nΔ\n\u0002\n\u0002\u0002\n\u0002\u0002\u0002\n \n \n \n4).\nt\nt\nt\nt\n \nAdding these expressions causes the negative terms to cancel with the corre-\nsponding positive ones. The result gives us the position at the next time step \nin terms of the acceleration and the two (known) positions at the current and \nprevious time steps. This is the regular Verlet method:\n \nr\nr\nr\na\n(\n)\n( )\n(\n)\n( )\n(\n).\nt\nt\nt\nt\nO\nt\n1\n1\n1\n1\n2\n4\n2\n+\n=\n−\n−\n+\n+\nΔ\nΔ\nΔ\nΔ\n \nt\nt\nt\n \nIn terms of net force, the Verlet method becomes\n \nr\nr\nr\nF\n(\n)\n( )\n(\n)\n( )\n(\n).\nt\nt\nt\nt\nm\nt\nO\nt\n1\n1\n1\n1\n2\n4\n2\n+\n=\n−\n−\n+\n+\nΔ\nΔ\nΔ\nΔ\nnet\nt\nt\n \nThe velocity is conspicuously absent from this expression. However, it can \nbe found using the following somewhat inaccurate approximation (among \nother alternatives):\n \nv\nr\nr\n(\n)\n(\n)\n( )\n(\n).\nt\nt\nt\nt\nO\nt\n1\n1\n1\n+\n=\n+\n−\n+\nΔ\nΔ\nΔ\nΔ\nt\nt\n \n12.4.4.5. Velocity Verlet\nThe more commonly used velocity Verlet method is a four-step process in which \nthe time step is divided into two parts to facilitate the solution. Given that \na\nF\nr\nv\n( )\n( , ( ),\n( ))\nt\nt\nt\nm\n1\n1\n1\n1\n=\n \n \nt\n is known, we do the following:\n \n1. Calculate r\nr\nv\na\n(\n)\n( )\n( )\n( )\n.\nt\nt\nt\nt\n1\n1\n1\n1\n2\n1\n2\n+\n=\n+\n+\nΔ\nΔ\nΔ\n \n \nt\nt\nt\n \n \n2. Calculate v\nv\na\n(\n)\n( )\n( )\n.\nt\nt\nt\n1\n1\n2\n1\n1\n2\n1\n+\n=\n+\nΔ\nΔ\n \nt\nt\n \n \n3. Determine a\na\nF\nr\nv\n(\n)\n( )\n(\n, ( ),\n( )).\nt\nt\nt\nt\nm\n1\n2\n2\n2\n2\n+\n=\n=\nΔ\n \n \nt\nt\n \n \n4. Calculate \n \n1\n1\n1\n1\n1\n2\n2\n(\n)\n(\n)\n(\n)\n.\nt +Δ\n=\n+ Δ +\n+Δ\nΔ\nv\nv\na\nt\nt\nt\nt\nt\nt  \nNotice in the third step that the force function depends on the position \nand velocity on the next time step, r(t2) and v(t2). We already calculated r(t2) in \nstep 1, so we have all the information we need as long as the force is not ve-\n12.4. Rigid Body Dynamics\n\n\n642 \n12. Collision and Rigid Body Dynamics\nlocity-dependent. If it is velocity-dependent, then we must approximate next \nframe’s velocity, perhaps using the explicit Euler method.\n12.4.5. Angular Dynamics in Two Dimensions\nUp until now, we’ve focused on analyzing the linear motion of a body’s center \nof mass (which acts as if it were a point mass). As I said earlier, an uncon-\nstrained rigid body will rotate about its center of mass. This means that we can \nlayer the angular motion of a body on top of the linear motion of its center of \nmass in order to arrive at a complete description of the body’s overall motion. \nThe study of a body’s rotational motion in response to applied forces is called \nangular dynamics .\nIn two dimensions, angular dynamics works almost identically to linear \ndynamics. For each linear quantity, there’s an angular analog, and the math-\nematics works out quite neatly. So let’s investigate two-dimensional angular \ndynamics ﬁ rst. As we’ll see, when we extend the discussion into three dimen-\nsions, things get a bit messier, but we’ll burn that bridge when we get to it!\n12.4.5.1. Orientation and Angular Speed\nIn two dimensions, every rigid body can be treated as a thin sheet of mate-\nrial. (Some physics texts refer to such a body as a plane lamina .) All linear mo-\ntion occurs in the xy-plane, and all rotations occur about the z-axis. (Visualize \nwooden puzzle pieces sliding about on an air hockey table.)\nThe orientation of a rigid body in 2D is fully described by an angle θ, \nmeasured in radians relative to some agreed-upon zero rotation. For example, \nwe might specify that θ = 0 when a race car is facing directly down the posi-\ntive x-axis in world space. This angle is of course a time-varying function, so \nwe denote it θ(t).\n12.4.5.2. Angular Speed and Acceleration\nAngular velocity measures the rate at which a body’s rotation angle chang-\nes over time. In two dimensions, angular velocity is a scalar, more correctly \ncalled angular speed , since the term “velocity” really only applies to vectors. \nIt is denoted by the scalar function ω(t) and measured in radians per second \n(rad/s). Angular speed is the derivative of the orientation angle θ(t) with re-\nspect to time:\n \nAngular: ω\nθ\nθ\n( )\n( )\n( );\nt\nd\nt\ndt\nt\n=\n= \u0002\n    Linear: v\nr\nr\n( )\n( )\n( ).\nt\nd t\ndt\nt\n=\n= \u0002\n \nAnd as we’d expect, angular acceleration , denoted α(t) and measured in \nradians per second squared (rad/s2), is the rate of change of angular speed:\n\n\n643 \n \nAngular: \nα\nω\nω\nθ\n( )\n( )\n( )\n( );\nt\nd\nt\ndt\nt\n=\n=\n=\n\u0002\n\u0002\u0002\n  \nt\n Linear: \na\nv\nv\nr\n( )\n( )\n( )\n( ).\nt\nd\nt\ndt\nt\n=\n=\n=\n\u0002\n\u0002\u0002 t\n \n12.4.5.3. Moment of Inertia\nThe rotational equivalent of mass is a quantity known as the moment of inertia . \nJust as mass describes how easy or diﬃ  cult it is to change the linear velocity \nof a point mass, the moment of inertia measures how easy or diﬃ  cult it is to \nchange the angular speed of a rigid body about a particular axis. If a body’s \nmass is concentrated near an axis of rotation, it will be relatively easier to ro-\ntate about that axis, and it will hence have a smaller moment of inertia than a \nbody whose mass is spread out away from that axis. \nSince we’re focusing on two-dimensional angular dynamics right now, \nthe axis of rotation is always z, and a body’s moment of inertia is a simple \nscalar value. Moment of inertia is usually denoted by the symbol I. We won’t \nget into the details of how to calculate the moment of inertia here. For a full \nderivation, see [15].\n12.4.5.4. Torque\nUntil now, we’ve assumed that all forces are applied to the center of mass of a \nrigid body. However, in general, forces can be applied at arbitrary points on a \nbody. If the line of action of a force passes through the body’s center of mass, \nthen the force will produce linear motion only, as we’ve already seen. Other-\nwise, the force will introduce a rotational force known as a torque in addition \nto the linear motion it normally causes. This is illustrated in Figure 12.24.\nWe can calculate torque using a cross product . First, we express the loca-\ntion at which the force is applied as a vector r extending from the body’s center \nof mass to the point of application of the force. (In other words, the vector r \nis in body space , where the origin of body space is deﬁ ned to be the center of \nF1\nF2\nFigure 12.24.  On the left, a force applied to a body’s CM produces purely linear motion. On \nthe right, a force applied off-center will give rise to a torque, producing rotational motion as \nwell as linear motion.\n12.4. Rigid Body Dynamics\n\n\n644 \n12. Collision and Rigid Body Dynamics\nmass .) This is illustrated in Figure 12.25. The torque N caused by a force F ap-\nplied at a location r is\n \n \n(12.7)\nEquation (12.7) implies that torque increases as the force is applied farther \nfrom the center of mass. This explains why a lever can help us to move a heavy \nobject. It also explains why a force applied directly through the center of mass \nproduces no torque and no rotation—the magnitude of the vector r is zero in \nthis case.\nWhen two or more forces are applied to a rigid body, the torque vectors \nproduced by each one can be summed, just as we can sum forces. So in general \nwe are interested in the net torque, Nnet.\nIn two dimensions, the vectors r and F must both lie in the xy-plane, so \nN will always be directed along the positive or negative z-axis. As such, we’ll \ndenote a two-dimensional torque via the scalar Nz , which is just the z-compo-\nnent of the vector N.\nTorque is related to angular acceleration and moment of inertia in much \nthe same way that force is related to linear acceleration and mass:\n \nAngular: \nN\nI\nt\nI\nt\nI\nt\nz =\n=\n=\nα\nθ\n( )\n( )\n( );\n\u0002\u0002\n  \nω\u0002\n Linear: \nF\na\nv\nr\n=\n=\n=\nm\nt\nm\nt\nm t\n( )\n( )\n( ).\n\u0002\n\u0002\u0002\n \n(12.8)\n12.4.5.5. Solving the Angular Equations of Motion in Two Dimensions\nFor the two-dimensional case, we can solve the angular equations of motion \nusing exactly the same numerical integration techniques we applied to the lin-\near dynamics problem. The pair of ODEs that we wish to solve is as follows:\n \nAngular: \nN\nt\nI\nt\nt\nnet\n  \n  \n( )\n( );\n( )\n( );\n=\n=\n\u0002\n\u0002\nω\nω\nθ t\n  Linear: \nF\nv\nv\nr\nnet\n \n( )\n( );\n( )\n( ),\nt\nm\nt\nt\n=\n=\n\u0002\n\u0002 t\n \nand their approximate explicit Euler solutions are\nF\nr\nr sin θ\nFigure 12.25.  Torque is calculated by taking the cross product between a force’s point of \napplication in body space (i.e., relative to the center of mass) and the force vector. The \nvectors are shown here in two dimensions for ease of illustration; if it could be drawn, the \ntorque vector would be directed into the page.\n.\n= ×\nN\nr\nF\n\n\n645 \n \nAngular: \nω\nω\nθ\nθ\nω\n( )\n( )\n( )\n;\n( )\n( )\n( )\n;\nt\nN\nt\nI\nt\nt\nt\n2\n1\n1\n2\n1\n1\n=\n+\n=\n+\nnet\n  \n  \n \nΔ\nΔ\nt\nt\nt\n  Linear: \nv\nv\nF\nr\nr\nv\n( )\n( )\n( )\n;\n( )\n( )\n( )\n.\nt\nt\nm\nt\nt\nt\n2\n1\n1\n2\n1\n1\n=\n+\n=\n+\nnet\n \nΔ\nΔ\nt\nt\nt\n \nOf course, we could apply any of the other more-accurate numerical \nmethods as well, such as the velocity Verlet method (I’ve omitt ed the lin-\near case here for compactness, but compare this to the steps given in Section \n12.4.4.5):\n \n1. Calculate θ\nθ\nω\nα\n(\n)\n( )\n( )\n( )\n.\nt\nt\nt\nt\n1\n1\n1\n1\n2\n1\n2\n+\n=\n+\n+\nΔ\nΔ\nΔ\nt\nt\nt\n \n \n2. Calculate ω\nω\nα\n(\n)\n( )\n( )\n.\nt\nt\nt\n1\n1\n2\n1\n1\n2\n1\n+\n=\n+\nΔ\nΔ\nt\nt\n \n \n3. Determine α\nα\nθ\nω\n(\n)\n( )\n(\n,\n( ),\n( )).\nt\nt\nI\nN\nt\nt\n1\n2\n1\n2\n2\n2\n+\n=\n= −\nΔ\nnet\n \n \nt\nt\n \n \n4. Calculate ω\nω\nα\n(\n)\n(\n)\n(\n)\n.\nt\nt\nt\nt\n1\n1\n1\n2\n1\n2\n1\n+\n=\n+\n+\n+\nΔ\nΔ\nΔ Δ\nt\nt\nt\n \n12.4.6. Angular Dynamics in Three Dimensions\n Angular dynamics in three dimensions is a somewhat more complex topic \nthan its two-dimensional counterpart, although the basic concepts are of \ncourse very similar. In the following section, I’ll give a very brief overview of \nhow angular dynamics works in 3D, focusing primarily on the things that are \ntypically confusing to someone who is new to the topic. For further informa-\ntion, check out Glenn Fiedler’s series of articles on the topic, available at htt p://\ngaﬀ erongames.wordpress.com/game-physics. Another helpful resource is the \npaper entitled “An Introduction to Physically Based Modeling” by David Ba-\nraﬀ  of the Robotics Institute at Carnegie Mellon University, available at htt p://\nwww-2.cs.cmu.edu/~baraﬀ /sigcourse/notesd1.pdf.\n12.4.6.1. The Inertia Tensor\nA rigid body may have a very diﬀ erent distribution of mass about the three \ncoordinate axes. As such, we should expect a body to have diﬀ erent moments \nof inertia about diﬀ erent axes. For example, a long thin rod should be relative-\nly easy to make rotate about its long axis because all the mass is concentrated \nvery close to the axis of rotation. Likewise, the rod should be relatively more \ndiﬃ  cult to make rotate about its short axis because its mass is spread out far-\nther from the axis. This is indeed the case, and it is why a ﬁ gure skater spins \nfaster when she tucks her limbs in close to her body.\nIn three dimensions, the rotational mass of a rigid body is represented \nby a 3 × 3 matrix known as its inertia tensor . It is usually represented by the \nsymbol I (as before, we won’t describe how to calculate the inertia tensor here; \nsee [15] for details):\n12.4. Rigid Body Dynamics\n\n\n646 \n12. Collision and Rigid Body Dynamics\n .\nxx\nxy\nxz\nyx\nyy\nyz\nzx\nzy\nzz\nI\nI\nI\nI\nI\nI\nI\nI\nI\n⎡\n⎤\n⎢\n⎥\n=⎢\n⎥\n⎢\n⎥\n⎣\n⎦\nI\nThe elements lying along the diagonal of this matrix are the moments of \ninertia of the body about its three principal axes, Ixx , Iyy , and Izz. The oﬀ -diago-\nnal elements are called products of inertia . They are zero when the body is sym-\nmetrical about all three principal axes (as would be the case for a rectangular \nbox). When they are non-zero, they tend to produce physically realistic yet \nsomewhat unintuitive motions that the average game player would probably \nthink were “wrong” anyway. Therefore, the inertia tensor is oft en simpliﬁ ed \ndown to the three-element vector [ Ixx  Iyy  Izz ] in game physics engines.\n12.4.6.2. Orientation in Three Dimensions\nIn two dimensions, we know that the orientation of a rigid body can be de-\nscribed by a single angle θ, which measures rotation about the z-axis (assum-\ning the motion is taking place in the xy-plane). In three dimensions, a body’s \norientation could be represented using three Euler angles [ θx  θy  θz ], each \nrepresenting the body’s rotation about one of the three Cartesian axes. How-\never, as we saw in Chapter 4, Euler angles suﬀ er from gimbal lock problems \nand can be diﬃ  cult to work with mathematically. Therefore, the orientation of \na body is more oft en represented using either a 3 × 3 matrix R or a unit quater-\nnion q. We’ll use the quaternion form exclusively in this chapter.\nRecall that a quaternion is a four-element vector whose x-, y-, and z-com-\nponents can be interpreted as a unit vector u lying along the axis of rotation, \nscaled by the sine of the half angle and whose w component is the cosine of \nthe half angle:\n \n( )\n( )\n \n \n2\n2\nq\n[\n]\n[\n]\nsin\ncos\n.\nx\ny\nz\nw\nw\nq\nq\nq\nq\nq\nθ\nθ\n=\n=\n⎡\n⎤\n=⎣\n⎦\nq\nu\n \nA body’s orientation is of course a function of time, so we should write it q(t).\nAgain, we need to select an arbitrary direction to be our zero rotation. For \nexample, we might say that by default, the front of every object will lie along \nthe positive z-axis in world space, with y up and x to the left . Any non-identity \nquaternion will serve to rotate the object away from this canonical world space \norientation. The choice of the canonical orientation is arbitrary, but of course \nit’s important to be consistent across all assets in the game.\n12.4.6.3. Angular Velocity and Momentum in Three Dimensions\nIn three dimensions, angular velocity is a vector quantity, denoted by ω(t). \nThe angular velocity vector can be visualized as a unit-length vector u that \n\n\n647 \ndeﬁ nes the axis of rotation, scaled by the two-dimensional angular velocity \nω\nθ\nu\nu\n= \u0002  of the body about the u-axis. Hence,\n \nω\nω\nθ\n( )\n( ) ( )\n( ) ( ).\nt\nt\nt\nu\nu\n=\n=\n \n \nu\nu\n\u0002\nt\nt\n \nIn linear dynamics, we saw that if there are no forces acting on a body, \nthen the linear acceleration is zero, and linear velocity is constant. In two-\ndimensional angular dynamics, this again holds true: If there are no torques \nacting on a body in two dimensions, then the angular acceleration α is zero, \nand the angular speed ω about the z-axis is constant.\nUnfortunately, this is not the case in three dimensions. It turns out that \neven when a rigid body is rotating in the absence of all forces, its angular \nvelocity vector ω(t) may not be constant because the axis of rotation can con-\ntinually change direction. You can see this eﬀ ect in action when you try to \nspin a rectangular object, like a block of wood, in mid-air in front of you. If \nyou throw the block so that it is rotating about its shortest axis, it will spin in a \nstable way. The orientation of the axis stays roughly constant. The same thing \nhappens if you try to spin the block about its longest axis. But if you try to spin \nthe block around its medium-sized axis, the rotation will be utt erly unstable. \nThe axis of rotation itself changes direction wildly as the object spins. This is \nshown in Figure 12.26.\nThe fact that the angular velocity vector can change in the absence of \ntorques is another way of saying that angular velocity is not conserved. How-\never, a related quantity called the angular momentum does remain constant \nin the absence of forces and hence is conserved. Angular momentum is the \nrotational equivalent of linear momentum:\n \nAngular: L\nI\n( )\n( );\nt =  \n  \nt\nω\n  Linear: \n \n( )\n( ).\nt\nm\nt\n=\np\nv\n \nLike the linear case, angular momentum L(t) is a three-element vector. \nHowever, unlike the linear case, rotational mass (the inertia tensor) is not a \nscalar but rather a 3 × 3 matrix. As such, the expression Iω is computed via a \nmatrix multiplication:\nFigure 12.26.  A rectangular object that is spun about its shortest or longest axis has a \nconstant angular velocity vector. However, when spun about its medium-sized axis, the \ndirection of the angular velocity vector changes wildly.\n12.4. Rigid Body Dynamics\n\n\n648 \n12. Collision and Rigid Body Dynamics\n \n \n( )\n( )\n( )\n( ) .\n( )\n( )\nx\nxx\nxy\nxz\nx\ny\nyx\nyy\nyz\ny\nz\nzx\nzy\nzz\nz\nL t\nI\nI\nI\nt\nL t\nI\nI\nI\nt\nL t\nI\nI\nI\nt\n⎡\n⎤⎡\n⎤⎡\n⎤\nω\n⎢\n⎥⎢\n⎥⎢\n⎥\n=\nω\n⎢\n⎥⎢\n⎥⎢\n⎥\n⎢\n⎥⎢\n⎥⎢\n⎥\nω\n⎣\n⎦⎣\n⎦⎣\n⎦\nBecause the angular velocity ω is not conserved, we do not treat it as \na primary quantity in our dynamics simulations the way we do the linear \nvelocity v. Instead, we treat angular momentum L as the primary quantity. \nThe angular velocity is a secondary quantity, determined only aft er we have \ndetermined the value of L at each time step of the simulation.\n12.4.6.4. Torque in Three Dimensions\nIn three dimensions, we still calculate torque as the cross product between \nthe radial position vector of the point of force application and the force vector \nitself (N = r × F). Equation (12.8) still holds, but we always write it in terms of \nthe angular momentum because angular velocity is not a conserved quantity:\n \nN\nI\nI\nI\nL\n=\n=\n=\n(\n)\n=\n ( )\n( )\n( )\n( ).\nt\nd\nt\ndt\nd\ndt\nt\nd\nt\ndt\nω\nα\nω\n12.4.6.5. Solving the Equations of Angular Motion in Three Dimensions\nWhen solving the equations of angular motion in three dimensions, we might \nbe tempted to take exactly the same approach we used for linear motion and \ntwo-dimensional angular motion. We might guess that the diﬀ erential equa-\ntions of motion should be writt en\n \nA3D(?): \nN\nI\nnet\n \n  \n  \n( )\n( );\n( )\n( );\nt\nt\n=\n=\n\u0002\n\u0002\nω t\nω\nt\nθ\n  L: \nF\nv\nv\nr\nnet\n \n( )\n( );\n( )\n( ),\nt\nm\nt\nt\n=\n=\n\u0002\n\u0002 t\n \nand using the explicit Euler method, we might guess that the approximate \nsolutions to these ODEs would look something like this:\n \nA3D(?): \nI\nN\n( )\n( )\n( )\n;\n( )\n( )\n( )\n;\nt\nt\nt\nt\n2\n1\n1\n1\n2\n1\n1\n=\n+\n=\n+\n−\n \n \n \nnet\n  \n  \nΔ\nΔ\nθ\nω\nω\nω\nt\nt\nθ\nt\n  L: \nv\nv\nF\nr\nr\nv\n( )\n( )\n( )\n;\n( )\n( )\n( )\n.\nt\nt\nm\nt\nt\nt\n2\n1\n1\n2\n1\n1\n=\n+\n=\n+\nnet\n \nΔ\nΔ\nt\nt\nt\n \nHowever, this is not actually correct. The diﬀ erential equations of angular mo-\ntion diﬀ er from their linear and two-dimensional angular counterparts in two \nimportant ways:\nInstead of solving for the angular velocity \n1. \nω, we solve for the angular \nmomentum L directly. We then calculate the angular velocity vector as a \nsecondary quantity using I and L. We do this because angular momen-\ntum is conserved , while angular velocity is not.\n\n\n649 \nWhen solving for the orientation given the angular velocity, we have \n2. \na problem: The angular velocity is a three-element vector, while the \norientation is a four-element quaternion. How can we write an ODE \nrelating a quaternion to a vector? The answer is that we cannot, at \nleast not directly. But what we can do is convert the angular velocity \nvector into quaternion form and then apply a slightly odd-looking \nequation that relates the orientation quaternion to the angular veloc-\nity quaternion.\nIt turns out that when we express a rigid body’s orientation as a quater-\nnion, the derivative of this quaternion is related to the body’s angular velocity \nvector in the following way. First, we construct an angular velocity quaternion. \nThis quaternion contains the three components of the angular velocity vector \nin x, y, and z, with its w-component set to zero:\n \n=⎡⎣\n⎤⎦\nω\nω\nω\nx\ny\nz\n0\nω\n \nNow the diﬀ erential equation relating the orientation quaternion to the angu-\nlar velocity quaternion is (for reasons we won’t get into here) as follows:\n \nd\nt\ndt\nt\nt\nq( )\nq( )\n( ) q( ).\n=\n=\n\u0002\n1\n2\n \nω t\n \nIt’s important to remember here that ω(t) is the angular velocity quaternion as \ndescribed above and that the product ω(t)q(t) is a quaternion product (see Sec-\ntion 4.4.2.1 for details).\nSo, we actually need to write the ODEs of motion as follows (note that I’ve \nrecast the linear ODEs in terms of linear momentum as well, to underscore the \nsimilarities between the two cases):\n \nA3D: \nN\nL\nI\nL\nnet\n  \n  \n  \n \n( )\n( );\n( )\n( );\n( )\n[ ( )\n];\n( )q( )\nt\nt\nt\nt\n=\n=\n=\n=\n−\n\u0002\n1\n1\n2\n0\n\u0002q( );\nt   \nt\nt\nt\nt\nω\nω\nω\nω\n  L: \nF\np\nv\np\nv\nr\nnet( )\n( );\n( )\n( ) ;\n( )\n( ).\nt\nt\nt\nm\nt\n=\n=\n=\n\u0002\n\u0002\nt\nt\n \nUsing the explicit Euler method, the ﬁ nal approximate solution to the angular \nODEs in three dimensions is actually as follows:\n \nL\nL\nN\nL\nr\nF\n( )\n( )\n( )\n( )\n( ) ;\nt\nt\nt\nt\ni\n2\n1\n1\n1\n1\n=\n+\n=\n+\n×\n(\n)\n∑\nnet\nΔ\nΔ\nt\nt\nt\ni\n \n(vectors)\n \n \n \n( )\n[\n( )\n];\nt2\n1\n2\n0\n=\n−\nI\nL t\nω\n \n(quaternion)\nq( )\nq( )\n( ) q( )\n.\nt\nt\nt\n2\n1\n1\n2\n1\n1\n=\n+\n \n Δ\nt\nt\nω\n \n(quaternions)\n12.4. Rigid Body Dynamics\n\n\n650 \n12. Collision and Rigid Body Dynamics\nThe orientation quaternion q(t) should be renormalized periodically to reverse \nthe eﬀ ects of the inevitable accumulation of ﬂ oating-point error.\nAs always, the explicit Euler method is being used here just as an ex-\nample. In a real engine, we would employ velocity Verlet, RK4, or some other \nmore-stable and more-accurate numerical method.\n12.4.7. Collision Response\nEverything we’ve discussed so far assumes that our rigid bodies are neither \ncolliding with anything, nor is their motion constrained in any other way. \nWhen bodies collide with one another, the dynamics simulation must take \nsteps to ensure that they respond realistically to the collision and that they \nare never left  in a state of interpenetration aft er the simulation step has been \ncompleted. This is known as collision response .\n12.4.7.1. Energy\nBefore we discuss collision response, we must understand the concept of en-\nergy . When a force moves a body over a distance, we say that the force does \nwork . Work represents a change in energy—that is, a force either adds energy \nto a system of rigid bodies (e.g., an explosion) or it removes energy from the \nsystem (e.g., friction). Energy comes in two forms. The potential energy V of a \nbody is the energy it has simply because of where it is relative to a force ﬁ eld \nsuch as a gravitational or a magnetic ﬁ eld. (For example, the higher up a body \nis above the surface of the Earth, the more gravitational potential energy it \nhas.) The kinetic energy of a body T represents the energy arising from the \nfact that it is moving relative to other bodies in a system. The total energy \nE = V + T of an isolated system of bodies is a conserved quantity, meaning that \nit remains constant unless energy is being drained from the system or added \nfrom outside the system.\nThe kinetic energy arising from linear motion can be writt en\n \nT\nmv\nlinear\n \n= 1\n2\n2,  \nor in terms of the linear momentum and velocity vectors:\n \nTlinear =\n⋅\n1\n2 p v.  \nAnalogously, the kinetic energy arising from a body’s rotational motion is as \nfollows:\n \n \n Energy and its conservation can be extremely useful concepts when solving \nall sorts of physics problems. We’ll see the role that energy plays in the deter-\nmination of collision responses in the following section.\nTangular =\n⋅\n1\n2 L\n.\nω\n",
      "page_number": 652,
      "chapter_number": 33,
      "summary": "This chapter covers segment 33 (pages 652-672). Key topics include angular, game, and linear. This integer indexes into \na global array of data structures containing the detailed collision properties \nthemselves.",
      "keywords": [
        "Rigid Body Dynamics",
        "Rigid Body",
        "Body Dynamics",
        "Body",
        "Angular velocity",
        "angular velocity vector",
        "Angular Dynamics",
        "Dynamics",
        "Angular",
        "Rigid",
        "explicit Euler method",
        "Velocity",
        "Linear dynamics",
        "body dynamics simulations",
        "Body Dynamics mass"
      ],
      "concepts": [
        "angular",
        "game",
        "linear",
        "collision",
        "dynamics",
        "objects",
        "forces",
        "physical",
        "physics",
        "vector"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 1",
          "chapter": 17,
          "title": "Segment 17 (pages 150-158)",
          "relevance_score": 0.69,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 33,
          "title": "Segment 33 (pages 314-325)",
          "relevance_score": 0.67,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 55,
          "title": "Segment 55 (pages 529-536)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 40,
          "title": "Segment 40 (pages 381-388)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 22,
          "title": "Segment 22 (pages 210-218)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 673-695)",
      "start_page": 673,
      "end_page": 695,
      "detection_method": "synthetic",
      "content": "651 \n12.4.7.2. Impulsive Collision Response\n When two bodies collide in the real world, a complex set of events takes place. \nThe bodies compress slightly and then rebound, changing their velocities and \nlosing energy to sound and heat in the process. Most real-time rigid body \ndynamics simulations approximate all of these details with a simple model \nbased on an analysis of the momenta and kinetic energies of the colliding ob-\njects, called Newton’s law of restitution for instantaneous collisions with no friction. \nIt makes the following simplifying assumptions about the collision:\nz The collision force acts over an inﬁ nitesimally short period of time, turn-\ning it into what we call an idealized impulse . This causes the velocities of \nthe bodies to change instantaneously as a result of the collision.\nz There is no friction at the point of contact between the objects’ surfaces. \nThis is another way of saying that the impulse acting to separate the \nbodies during the collision is normal to both surfaces—there is no tan-\ngential component to the collision impulse. (This is just an idealization \nof course; we’ll get to friction in Section 12.4.7.5.)\nz The nature of the complex submolecular interactions between the bodies \nduring the collision can be approximated by a single quantity known \nas the coeﬃ  cient of restitution , customarily denoted by the symbol ε. This \ncoeﬃ  cient describes how much energy is lost during the collision. When \nε = 1, the collision is perfectly elastic , and no energy is lost. (Picture two \nbilliard balls colliding in mid air.) When ε = 0, the collision is perfectly in-\nelastic , also known as perfectly plastic , and the kinetic energy of both bod-\nies is lost. The bodies will stick together aft er the collision, continuing to \nmove in the direction that their mutual center of mass had been moving \nbefore the collision. (Picture pieces of putt y being slammed together.)\nAll collision analysis is based around the idea that linear momentum is \nconserved. So for two bodies 1 and 2, we can write\n \n \n \n \n \n1\n2\n1\n2\n1\n1\n2\n2\n1\n1\n2\n2\n,         or\n,\nm\nm\nm\nm\n′\n′\n+\n=\n+\n′\n′\n+\n=\n+\np\np\np\np\nv\nv\nv\nv\n \nwhere the primed symbols represent the momenta and velocities aft er the col-\nlision. The kinetic energy of the system is conserved as well, but we must ac-\ncount for the energy lost due to heat and sound by introducing an additional \nenergy loss term Tlost :\n \n \n \n \n \n2\n2\n2\n2\n1\n1\n1\n1\n1\n1\n2\n2\n1\n1\n2\n2\nlost\n2\n2\n2\n2\n.\nm v\nm v\nm v\nm v\nT\n′\n′\n+\n=\n+\n+\n \nIf the collision is perfectly elastic, the energy loss Tlost is zero. If it is perfectly \nplastic, the energy loss is equal to the original kinetic energy of the system, the \n12.4. Rigid Body Dynamics\n\n\n652 \n12. Collision and Rigid Body Dynamics\nprimed kinetic energy sum becomes zero, and the bodies stick together aft er \nthe collision.\nTo resolve a collision using Newton’s law of restitution, we apply an ide-\nalized impulse to the two bodies. An impulse is like a force that acts over an in-\nﬁ nitesimally short period of time and thereby causes an instantaneous change \nin the velocity of the body to which it is applied. We could denote an impulse \nwith the symbol ∆p, since it is a change in momentum (∆p = m∆v). However, \nmost physics texts use the symbol ˆp  (pronounced “p-hat”) instead, so we’ll \ndo the same.\nBecause we assume that there is no friction involved in the collision, the \nimpulse vector must be normal to both surfaces at the point of contact. In oth-\ner words, \n \nˆ\nˆp\n=\np\nn , where n is the unit vector normal to both surfaces. This is \nillustrated in Figure 12.27. If we assume that the surface normal points toward \nbody 1, then body 1 experiences an impulse of ˆp, and body 2 experiences an \nequal but opposite impulse. Hence, the momenta of the two bodies aft er the \ncollision can be writt en in terms of their momenta prior to the collision and \nthe impulse ˆp  as follows:\n1\n1\n2\n2\nˆ\nˆ\n;        \n;\n′\n′\n=\n+\n=\n−\np\np\np\np\np\np\n \n1\n1\n1\n1\n2\n2\n2\n2\nˆ\nˆ\n;        \n;\nm\nm\nm\nm\n′\n′\n=\n+\n=\n−\nv\nv\np\nv\nv\np\n \n \n(12.9)\nThe coeﬃ  cient of restitution provides the key relationship between the rela-\ntive velocities of the bodies before and aft er the collision. Given that the cen-\nters of mass of the bodies have velocities \n1\nv  and \n2\nv  before the collision and \n1′\nv  and \n2′\nv  aft erward, the coeﬃ  cient of restitution ε is deﬁ ned as follows:\n \n2\n1\n2\n1\n(\n)\n(\n).\n′\n′\n−\n=ε\n−\nv\nv\nv\nv\n \n(12.10)\nSolving Equations (12.9) and (12.10) under the temporary assumption \nthat the bodies cannot rotate yields\nn\nBody 1\nBody 2\np^\nFigure 12.27.  In a frictionless collision, the impulse acts along a line normal to both surfaces \nat the point of contact. This line is deﬁ ned by the unit normal vector n.\n1\n1\n2\n2\n1\n2\nˆ\nˆ\n;        \n.\np\np\nm\nm\n′\n′\n=\n+\n=\n−\nv\nv\nn\nv\nv\nn\n\n\n653 \n(\n)\n \n2\n1\n1\n2\n(\n1)\nˆ\nˆ\n .\n1\n1\np\nm\nm\nε+\n⋅−\n⋅\n=\n=\n+\nv\nn\nv\nn\np\nn\nn  \nNotice that if the coeﬃ  cient of restitution is one (perfectly elastic collision) and \nif the mass of body 2 is eﬀ ectively inﬁ nite (as it would be for, say, a concrete \ndriveway), then (1/m2) = 0, v2 = 0, and this expression reduces to a reﬂ ection of \nthe other body’s velocity vector about the contact normal, as we’d expect:\n(\n)\n(\n)\n(\n)\n \n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\nˆ\n2\n ;\nˆ\n2\n \n2\n .\nm\nm\nm\nm\nm\n=−\n⋅\n+\n−\n⋅\n′ =\n=\n=\n−\n⋅\np\nv\nn n\np\np\nv\nv\nn n\nv\nv\nv\nn n\nThe solution gets a bit hairier when we take the rotations of the bodies \ninto account. In this case, we need to look at the velocities of the points of \ncontact on the two bodies rather than the velocities of their centers of mass, \nand we need to calculate the impulse in such a way as to impart a realis-\ntic rotational eﬀ ect as a result of the collision. We won’t get into the details \nhere, but Chris Hecker’s article, available at htt p://chrishecker.com/images/e/\ne7/Gdmphys3.pdf, does an excellent job of describing both the linear and the \nrotational aspects of collision response. The theory behind collision response \nis explained more fully in [15].\n12.4.7.3. Penalty Forces\nAnother approach to collision response is to introduce imaginary forces called \npenalty forces into the simulation. A penalty force acts like a stiﬀ  damped spring \natt ached to the contact points between two bodies that have just interpenetrat-\ned. Such a force induces the desired collision response over a short but ﬁ nite \nperiod of time. Using this approach, the spring constant k eﬀ ectively controls \nthe duration of the interpenetration, and the damping coeﬃ  cient b acts a bit \nlike the restitution coeﬃ  cient. When b = 0, there is no damping—no energy is \nlost, and the collision is perfectly elastic. As b increases, the collision becomes \nmore plastic.\nLet’s take a brief look at some of the pros and cons of the penalty force ap-\nproach to resolving collisions. On the positive side, penalty forces are easy to \nimplement and understand. They also work well when three or more bodies \nare interpenetrating each other. This problem is very diﬃ  cult to solve when \nresolving collisions one pair at a time. A good example is the Sony PS3 demo \nin which a huge number of rubber duckies are poured into a bathtub—the \nsimulation was nice and stable despite the very large number of collisions. \nThe penalty force method is a great way to achieve this.\n12.4. Rigid Body Dynamics\n\n\n654 \n12. Collision and Rigid Body Dynamics\nUnfortunately, because penalty forces respond to penetration (i.e., rela-\ntive position) rather than to relative velocity , the forces may not align with \nthe direction we would intuitively expect, especially during a high-speed col-\nlision. A classic example is a car driving head-on into a truck. The car is low \nwhile the truck is tall. Using only the penalty force method, it is easy to arrive \nat a situation in which the penalty force is vertical, rather than horizontal as \nwe would expect given the velocities of the two vehicles. This can cause the \ntruck to pop its nose up into the air while the car drives under it.\nIn general, the penalty force technique works well for low-speed impacts, \nbut it does not work well at all when objects are moving quickly. It is pos-\nsible to combine the penalty force method with other collision resolution ap-\nproaches in order to strike a balance between stability in the presence of large \nnumbers of interpenetrations and responsiveness and more-intuitive behav-\nior at high velocities.\n12.4.7.4. Using Constraints to Resolve Collisions\n As we’ll investigate in Section 12.4.8, most physics systems permit vari-\nous kinds of constraints to be imposed on the motion of the bodies in the \nsimulation. If collisions are treated as constraints that disallow object in-\nterpenetration, then they can be resolved by simply running the simula-\ntion’s general-purpose constraint solver. If the constraint solver is fast and \nproduces high-quality visual results, this can be an eﬀ ective way to resolve \ncollisions.\n12.4.7.5. Friction\nFriction is a force that arises between two bodies that are in continuous con-\ntact, resisting their movement relative to one another. There are a number of \ntypes of friction. Static friction is the resistance one feels when trying to start \na stationary object sliding along a surface. Dynamic friction is a resisting force \nthat arises when objects are actually moving relative to one another. Sliding \nfriction is a type of dynamic friction that resists movement when an object \nslides along a surface. Rolling friction is a type of static or dynamic friction \nthat acts at the point of contact between a wheel or other round object and the \nsurface it is rolling on. When the surface is very rough, the rolling friction is \nexactly strong enough to cause the wheel to roll without sliding, and it acts \nas a form of static friction. If the surface is somewhat smooth, the wheel may \nslip, and a dynamic form of rolling friction comes into play. Collision friction is \nthe friction that acts instantaneously at the point of contact when two bodies \ncollide while moving. (This is the friction force that we ignored when discuss-\ning Newton’s law of restitution in Section 12.4.7.1.) Various kinds of constraints \n\n\n655 \ncan have friction as well. For example, a rusted hinge or axle might resist be-\ning turned by introducing a friction torque.\nLet’s look at an example to understand the essence of how friction works. \nLinear sliding friction is proportional to the component of an object’s weight \nthat is acting normal to the surface on which it is sliding. The weight of an \nobject is just the force due to gravity , G = mg, which is always directed down-\nward. The component of this force normal to an inclined surface that makes an \nangle θ with the horizontal is just GN = mg cos θ. The friction force f is then\n \n cos ,\nf\nmg\n= μ\nθ  \nwhere the constant of proportionality μ is called the coeﬃ  cient of friction . This \nforce acts tangentially to the surface, in a direction opposite to the att empted \nor actual motion of the object. This is illustrated in Figure 12.28.\nFigure 12.28 also shows the component of the gravitational force acting \ntangent to the surface, GT = mg sin θ. This force tends to make the object accel-\nerate down the plane, but in the presence of sliding friction, it is counteracted \nby f. Hence, the net force tangent to the surface is\n \n \nnet\n( in\ncos ).\nT\nF\nG\nf\nmg s\n=\n−\n=\nθ−μ\nθ\nIf the angle of inclination is such that the expression in parentheses is zero, \nthe object will slide at a constant speed (if already moving) or be at rest. If the \nexpression is greater than zero, the object will accelerate down the surface. If it \nis less than zero, the object will decelerate and eventually come to rest.\n12.4.7.6. Welding\nAn additional problem arises when an object is sliding across a polygon soup. \nRecall that a polygon soup is just what its name implies—a soup of essentially \nunrelated polygons (usually triangles). As an object slides from one triangle \nof this soup to the next, the collision detection system will generate additional \nG = mg\n|GN| =\nmg cos θ\n|G T| =\nmg sin θ\n|f| =\n μmg cos θ\nFigure 12.28.  The force of friction f is proportional to the normal component of the object’s \nweight. The proportionality constant μ is called the coefﬁ cient of friction.\n12.4. Rigid Body Dynamics\n\n\n656 \n12. Collision and Rigid Body Dynamics\nspurious contacts because it will think that the object is about to hit the edge \nof the next triangle. This is illustrated in Figure 12.29.\nThere are a number of solutions to this problem. One is to analyze the \nset of contacts and discard ones that appear to be spurious, based on various \nheuristics and possibly some knowledge of the object’s contacts on a previous \nframe (e.g., if we know the object was sliding along a surface and a contact \nnormal arises that is due to the object being near the edge of its current tri-\nangle, then discard that contact normal). Versions of Havok prior to 4.5 em-\nployed this approach.\nStarting with Havok 4.5, a new technique was implemented that essen-\ntially annotates the mesh with triangle adjacency information. The collision \ndetection system therefore “knows” which edges are interior edges and can \ndiscard spurious collisions reliably and quickly. Havok describes this solution \nas welding , because in eﬀ ect the edges of the triangles in the poly soup are \nwelded to one another.\n12.4.7.7. Coming to Rest, Islands, and Sleeping\nWhen energy is removed from a simulated system via friction, damping, or \nother means, moving objects will eventually come to rest. This seems like a \nnatural consequence of the simulation—something that would just “fall out” \nof the diﬀ erential equations of motion. Unfortunately, in a real computerized \nsimulation, coming to rest is never quite that simple. Various factors such as \nﬂ oating-point error, inaccuracies in the calculation of restitution forces, and \nnumerical instability can cause objects to jitt er forever rather than coming to \nrest as they should. For this reason, most physics engines use various heuris-\ntic methods to detect when objects are oscillating instead of coming to rest \nas they should. Additional energy can be removed from the system to en-\nsure that such objects eventually sett le down, or they can simply be stopped \nabruptly once their average velocity drops below a threshold.\nFigure 12.29.  When an object slides between two adjacent triangles, spurious contacts with \nthe new triangle’s edge can be generated.\nSpurious Contacts \nwith Triangle Edge\n\n\n657 \nWhen an object really does stop moving (ﬁ nds itself in a state of equilib-\nrium ), there is no reason to continue integrating its equations of motion every \nframe. To optimize performance, most physics engines allow dynamic objects \nin the simulation to be put to sleep . This excludes them from the simulation \ntemporarily, although sleeping objects are still active from a collision stand-\npoint. If any force or impulse begins acting on a sleeping object, or if the object \nloses one of the contacts that was holding it in equilibrium, it will be awoken \nso that its dynamic simulation can be resumed.\nSleep Criteria\nVarious criteria can be used to determine whether or not a body qualiﬁ es for \nsleep. It’s not always easy to make this determination in a robust manner for \nall situations. For example, a long pendulum might have very low angular \nmomentum and yet still be moving visibly on-screen.\nThe most commonly used criteria for equilibrium detection include:\nz The body is supported . This means it has three or more contact points \n(or one or more planar contacts) that allow it to att ain equilibrium with \ngravity and any other forces that might be aﬀ ecting it.\nz The body’s linear and angular momentum are below a predeﬁ ned thresh-\nold.\nz A running average of the linear and angular momentum are below a pre-\ndeﬁ ned threshold.\nz The total kinetic energy of the body (T =\n⋅+\n⋅\n1\n2\n1\n2\np v\nL ω) is below a pre-\ndeﬁ ned threshold. The kinetic energy is usually mass-normalized so that \na single threshold can be used for all bodies regardless of their masses.\nz The motion of a body that is about to go to sleep might be progressively \ndamped so that it comes to a smooth stop rather than stopping abruptly.\nSimulation Islands\nBoth Havok and PhysX further optimize their performance by automatically \ngrouping objects that either are interacting or have the potential to interact in \nthe near future into sets called simulation islands . Each simulation island can be \nsimulated independently of all the other islands—an approach that is highly \nconducive to cache coherency optimizations and parallel processing.\nHavok and PhysX both put entire islands to sleep rather than individu-\nal rigid bodies. This approach has its pros and cons. The performance boost \nis obviously larger when a whole group of interacting objects can be put to \nsleep. On the other hand, if even one object in an island is awake, the entire \nisland is awake. Overall, it seems that the pros tend to outweigh the cons, so \n12.4. Rigid Body Dynamics\n\n\n658 \n12. Collision and Rigid Body Dynamics\nthe simulation island design is one we’re likely to continue to see in future \nversions of these SDKs.\n12.4.8. Constraints\nAn unconstrained rigid body has six degrees of freedom (DOF): It can trans-\nlate in three dimensions, and it can rotate about the three Cartesian axes. Con-\nstraints restrict an object’s motion, reducing its degrees of freedom either par-\ntially or completely. Constraints can be used to model all sorts of interesting \nbehaviors in a game. Here are a few examples:\nz a swinging chandelier (point-to-point constraint);\nz a door that can be kicked, slammed, blown of its hinges (hinge con-\nstraint);\nz a vehicle’s wheel assembly (axle constraint with damped springs for \nsuspension);\nz a train or a car pulling a trailer (stiﬀ  spring/rod constraint);\nz a rope or chain (chain of stiﬀ  springs or rods);\nz a rag doll (specialized constraints that mimic the behavior of various \njoints in the human skeleton).\nIn the sections that follow, we’ll brieﬂ y investigate these and some of the \nother most common kinds of constraints typically provided by a physics SDK.\n12.4.8.1. Point-to-Point Constraints\nA point-to-point constraint is the simplest type of constraint. It acts like a ball \nand socket joint—bodies can move in any way they like, as long as a speciﬁ ed \npoint on one body lines up with a speciﬁ ed point on the other body. This is \nillustrated in Figure 12.30.\n12.4.8.2. Stiff Springs\nA stiﬀ  spring constraint is a lot like a point-to-point constraint except that it \nkeeps the two points separated by a speciﬁ ed distance. This kind of constraint \nFigure 12.30.  A \npoint-to-point \nconstraint \nre-\nquires \nthat \na \npoint on body A \nalign with a point \non body B.\nFigure 12.31.  A stiff spring constraint requires that a point on body A be separated from a \npoint on body B by a user-speciﬁ ed distance.\n\n\n659 \nacts like an invisible rod between the two constrained points. Figure 12.31 il-\nlustrates this constraint.\n12.4.8.3. Hinge Constraints\nA hinge constraint limits rotational motion to only a single degree of freedom, \nabout the hinge’s axis. An unlimited hinge acts like an axle, allowing the con-\nstrained object to complete an unlimited number of full rotations. It’s com-\nmon to deﬁ ne limited hinges that can only move through a predeﬁ ned range \nof angles about the one allowed axis. For example, a one-way door can only \nmove through a 180 degree arc, because otherwise it would pass through the \nadjacent wall. Likewise, a two-way door is constrained to move through a \n±180 degree arc. Hinge constraints may also be given a degree of friction in \nthe form of a torque that resists rotation about the hinge’s axis. A limited hinge \nconstraint is shown in Figure 12.32.\nFigure 12.32.  A limited hinge constraint mimics the behavior of a door.\nFigure 12.33.  A prismatic constraint acts like a piston.\n12.4.8.4. Prismatic Constraints\nPrismatic constraints act like a piston: A constrained body’s motion is restrict-\ned to a single translational degree of freedom. A prismatic constraint may or \nmay not permit rotation about the translation axis of the piston. Prismatic \nconstraints can of course be limited or unlimited and may or may not include \nfriction. A prismatic constraint is illustrated in Figure 12.33.\n12.4. Rigid Body Dynamics\n\n\n660 \n12. Collision and Rigid Body Dynamics\n12.4.8.5. Other Common Constraint Types\nMany other types of constraints are possible, of course. Here are just a few \nexamples:\nz Planar . Objects are constrained to move in a two-dimensional plane.\nz Wheel . This is typically a hinge constraint with unlimited rotation, \ncoupled with some form of vertical suspension simulated via a spring-\ndamper as sembly.\nz Pulley . In this specialized constraint, an imaginary rope passes through \na pulley and is att ached to two bodies. The bodies move along the line \nof the rope via a leverage ratio.\nConstraints may be breakable , meaning that aft er enough force is ap-\nplied, they automatically come apart. Alternatively, the game can turn the \nconstraint on and oﬀ  at will, using its own criteria for when the constraint \nshould break.\n12.4.8.6. Constraint Chains\nLong chains of linked bodies are sometimes diﬃ  cult to simulate in a stable \nmanner because of the iterative nature of the constraint solver. A constraint \nchain is a specialized group of constraints with information that tells the \nconstraint solver how the objects are connected. This allows the solver to \ndeal with the chain in a more stable manner than would otherwise be pos-\nsible.\n12.4.8.7. Rag Dolls\nA rag doll is a physical simulation of the way a human body might move \nwhen it is dead or unconscious and hence entirely limp. Rag dolls are created \nby linking together a collection of rigid bodies, one for each semi-rigid part \nof the body. For example, we might have capsules for the feet, calves, thighs, \nhands, upper and lower arms, and head and possibly a few for the torso to \nsimulate the ﬂ exibility of the spine.\nThe rigid bodies in a rag doll are connected to one another via constraints. \nRag doll constraints are specialized to mimic the kinds of motions the joints in \na real human body can perform. We usually make use of constraint chains to \nimprove the stability of the simulation.\nA rag doll simulation is always tightly integrated with the animation \nsystem. As the rag doll moves in the physics world, we extract the positions \nand rotations of the rigid bodies, and use this information to drive the posi-\ntions and orientations of certain joints in the animated skeleton. So in eﬀ ect, a \nrag doll is really just a form of procedural animation that happens to be driven \n\n\n661 \nby the physics system. (See Chapter 11 for more details on skeletal anima-\ntion.)\nOf course, implementing a rag doll is not quite as simple as I’ve made it \nsound here. For one thing, there’s usually not a one-to-one mapping between \nthe rigid bodies in the rag doll and the joints in the animated skeleton —the \nskeleton usually has more joints than the rag doll has bodies. Therefore, we \nneed a system that can map rigid bodies to joints (i.e., one that “knows” to \nwhich joint each rigid body in the rag doll corresponds). There may be addi-\ntional joints between those that are being driven by the rag doll bodies, so the \nmapping system must also be capable of determining the correct pose trans-\nforms for these intervening joints. This is not an exact science. We must apply \nartistic judgment and/or some knowledge of human biomechanics in order to \nachieve a natural-looking rag doll.\n12.4.8.8. Powered Constraints\nConstraints can also be “powered,” meaning that an external engine system \nsuch as the animation system can indirectly control the translations and orien-\ntations of the rigid bodies in the rag doll.\nLet’s take an elbow joint as an example. An elbow acts prett y much like \na limited hinge, with a litt le less than 180 degrees of free rotation. (Actual-\nly, an elbow can also rotate axially, but we’ll ignore that for the purposes of \nthis discussion.) To power this constraint, we model the elbow as a rotational \nspring . Such a spring exerts a torque proportional to the spring’s angle of de-\nﬂ ection away from some predeﬁ ned rest angle, N = –k(θ – θ rest). Now imagine \nchanging the rest angle externally, say by ensuring that it always matches the \nangle of the elbow joint in an animated skeleton. As the rest angle changes, the \nspring will ﬁ nd itself out of equilibrium , and it will exert a torque that tends \nBone\nCollision \nCapsule\nCapsule strikes \nan obstacle\nBone continues \nto move\nFigure 12.34.  Left: with a powered rag doll constraint, and in the absence of any additional \nforces or torques, a rigid body representing the lower arm can be made to exactly track the \nmovements of an animated elbow joint. Right: if an obstacle blocks the motion of the body, it \nwill diverge from that of the animated elbow joint in a realistic way.\n12.4. Rigid Body Dynamics\n\n\n662 \n12. Collision and Rigid Body Dynamics\nto rotate the elbow back into alignment with θ rest. In the absence of any other \nforces or torques, the rigid bodies will exactly track the motion of the elbow \njoint in the animated skeleton. But if other forces are introduced (for example, \nthe lower arm comes in contact with an immovable object), then these forces \nwill play into the overall motion of the elbow joint, allowing it to diverge from \nthe animated motion in a realistic manner. As illustrated in Figure 12.34, this \nprovides the illusion of a human who is trying her best to move in a certain \nway (i.e., the “ideal” motion provided by the animation) but who is some-\ntimes unable to do so due to the limitations of the physical world (e.g., her arm \ngets caught on something as she tries to swing it forward).\n12.4.9. Controlling the Motions of Rigid Bodies\nMost game designs call for a degree of control over the way rigid bodies move \nover and above the way they would move naturally under the inﬂ uence of \ngravity and in response to collisions with other objects in the scene. For ex-\nample:\nz An air vent applies an upward force to any object that enters its shaft  of \ninﬂ uence.\nz A car is coupled to a trailer and exerts a pulling force on it as it moves.\nz A tractor beam exerts a force on an unwitt ing space craft .\nz An anti-gravity device causes objects to hover.\nz The ﬂ ow of a river creates a force ﬁ eld that causes objects ﬂ oating in the \nriver to move downstream.\nAnd the list goes on. Most physics engines typically provide their users with \na number of ways to exert control over the bodies in the simulation. We’ll out-\nline the most common of these mechanisms in the following sections.\n12.4.9.1. Gravity\nGravity is ubiquitous in most games that take place on the surface of the Earth \nor some other planet (or on a spacecraft  with simulated gravity!). Gravity is \ntechnically not a force but rather a (roughly) constant acceleration, so it af-\nfects all bodies equally regardless of their mass. Because of its ubiquitous and \nspecial nature, the magnitude and direction of the gravitational acceleration \nis speciﬁ ed via a global sett ing in most SDKs. (If you’re writing a space game, \nyou can always set gravity to zero to eliminate it from the simulation.)\n12.4.9.2. Applying Forces\nAny number of forces can be applied to the bodies in a game physics simula-\ntion. A force always acts over a ﬁ nite time interval. (If it acted instantaneous-\n\n\n663 \nly, it would be called an impulse—more on that below.) The forces in a game \nare oft en dynamic in nature—they oft en change their directions and/or their \nmagnitudes every frame. So the force-application function in most physics \nSDKs is designed to be called once per frame for the duration of the force’s \ninﬂ uence. The signature of such a function usually looks something like this: \napplyForce(const Vector& forceInNewtons), where the duration of the \nforce is assumed to be Δt.\n12.4.9.3. Applying Torques\nWhen a force is applied such that its line of action passes through the center of \nmass of a body, no torque is generated, and only the body’s linear acceleration \nis aﬀ ected. If it is applied oﬀ -center, it will induce both a linear and a rotational \nacceleration. A pure torque can be applied to a body as well by applying two \nequal and opposite forces to points equidistant from the center of mass. The \nlinear motions induced by such a pair of forces will cancel each other out (since \nfor the purposes of linear dynamics, the forces both act at the center of mass). \nThis leaves only their rotational eﬀ ects. A pair of torque-inducing forces like \nthis is known as a couple (htt p://en.wikipedia.org/wiki/Couple_(mechanics)). \nA special function such as applyTorque(const Vector& torque) may be \nprovided for this purpose. However, if your physics SDK provides no apply\nTorque() function, you can always write one and have it generate a suitable \ncouple instead.\n12.4.9.4. Applying Impulses\nAs we saw in Section 12.4.7.2, an impulse is an instantaneous change in veloc-\nity (or actually, a change in momentum). Technically speaking, an impulse \nis a force that acts for an inﬁ nitesimal amount of time. However, the short-\nest possible duration of force application in a time-stepped dynamics simu-\nlation is Δt, which is not short enough to simulate an impulse adequately. \nAs such, most physics SDKs provide a function with a signature such as \napplyImpulse(const Vector& impulse)  for the purposes of applying \nimpulses to bodies. Of course, impulses come in two ﬂ avors—linear and an-\ngular—and a good SDK should provide functions for applying both types.\n12.4.10. The Collision/Physics Step\nNow that we’ve covered the theory and some of the technical details behind \nimplementing a collision and physics system, let’s take a brief look at how \nthese systems actually perform their updates every frame.\nEvery collision/physics engine performs the following basic tasks during \nits update step . Diﬀ erent physics SDKs may perform these phases in diﬀ erent \n12.4. Rigid Body Dynamics\n\n\n664 \n12. Collision and Rigid Body Dynamics\norders. That said, the technique I’ve seen used most oft en goes something like \nthis:\nThe forces and torques acting on the bodies in the physics world are \n1. \nintegrated forward by Δt in order to determine their tentative positions \nand orientations next frame.\nThe collision detection library is called to determine if any new contacts \n2. \nhave been generated between any of the objects as a result of their \ntentative movement. (The bodies normally keep track of their contacts \nin order to take advantage of temporal coherency. Hence at each step of \nthe simulation, the collision engine need only determine whether any \nprevious contacts have been lost and whether any new contacts have \nbeen added.)\nCollisions are resolved, oft en by applying impulses or penalty forces \n3. \nor as part of the constraint solving step below. Depending on the SDK, \nthis phase may or may not include continuous collision detection (CCD, \notherwise known as time of impact detection or TOI).\nConstraints are satisﬁ ed by the constraint solver.\n4. \nAt the conclusion of step 4, some of the bodies may have moved away from \ntheir tentative positions as determined in step 1. This movement may cause \nadditional interpenetrations between objects or cause other previously sat-\nisﬁ ed constraints to be broken. Therefore, steps 1 through 4 (or sometimes \nonly 2 through 4, depending on how collisions and constraints are resolved) \nare repeated until either (a) all collisions have been successfully resolved \nand all constraints are satisﬁ ed, or (b) a predeﬁ ned maximum number of \niterations has been exceeded. In the latt er case, the solver eﬀ ectively “gives \nup,” with the hope that things will resolve themselves naturally during sub-\nsequent frames of the simulation. This helps to avoid performance spikes \nby amortizing the cost of collision and constraint resolution over multiple \nframes. However, it can lead to incorrect-looking behavior if the errors are \ntoo large or if the time step is too long or is inconsistent. Penalty forces can \nbe blended into the simulation in order to gradually resolve these problems \nover time.\n12.4.10.1. The Constraint Solver\nA constraint solver is essentially an iterative algorithm that att empts to satisfy \na large number of constraints simultaneously by minimizing the error between \nthe actual positions and rotations of the bodies in the physics world and their \nideal positions and rotations as deﬁ ned by the constraints. As such, constraint \nsolvers are essentially iterative error minimization algorithms.\n\n\n665 \nLet’s take a look ﬁ rst at how a constraint solver works in the trivial case \nof a single pair of bodies connected by a single constraint. During each step of \nthe physics simulation, the numerical integrator will ﬁ nd new tentative trans-\nforms for the two bodies. The constraint solver then evaluates their relative \npositions and calculates the error between the positions and orientations of \ntheir shared axis of rotation. If any error is detected, the solver moves the \nbodies in such a way as to minimize or eliminate it. Since there are no other \nbodies in the system, the second iteration of the step should discover no new \ncontacts, and the constraint solver will ﬁ nd that the one hinge constraint is \nnow satisﬁ ed. Hence the loop can exit without further iterations.\nWhen more than one constraint must be satisﬁ ed simultaneously, more \niterations may be required. During each iteration, the numerical integrator \nwill sometimes tend to move the bodies out of alignment with their con-\nstraints, while the constraint solver tends to put them back into alignment. \nWith luck, and a carefully designed approach to minimizing error in the con-\nstraint solver, this feedback loop should eventually sett le into a valid solution. \nHowever, the solution may not always be exact. This is why, in games with \nphysics engines, you sometimes witness seemingly impossible behaviors, like \nchains that stretch (opening up litt le gaps between the links), objects that in-\nterpenetrate brieﬂ y, or hinges that momentarily move beyond their allowable \nranges. The goal of the constraint solver is to minimize error—it’s not always \npossible to eliminate it completely.\n12.4.10.2. Variations between Engines\nThe description given above is of course an over-simpliﬁ cation of what re-\nally goes on in a physics/collision engine every frame. The way in which the \nvarious phases of computation are performed, and their order relative to one \nanother, may vary from physics SDK to physics SDK. For example, some \nkinds of constraints are modeled as forces and torques that are taken care \nof by the numerical integration step rather than being resolved by the con-\nstraint solver. Collision may be run before the integration step rather than \naft er. Collisions may be resolved in any number of diﬀ erent ways. Our goal \nhere is merely to give you a taste of how these systems work. For a detailed \nunderstanding of how any one SDK operates, you’ll want to read its docu-\nmentation and probably also inspect its source code (presuming the relevant \nbits are available for you to read!). The curious and industrious reader can \nget a good start by downloading and experimenting with ODE and/or PhysX, \nas these two SDKs are available for free. You can also learn a great deal from \nODE’s wiki, which is available at htt p://opende.sourceforge.net/wiki/index.\nphp/Main_Page.\n12.4. Rigid Body Dynamics\n\n\n666 \n12. Collision and Rigid Body Dynamics\n12.5. Integrating a Physics Engine \n \ninto Your Game\nObviously, a collision/physics engine is of litt le use by itself—it must be integrat-\ned into your game engine. In this section, we’ll discuss the most common inter-\nface points between the collision/physics engine and the rest of the game code.\n12.5.1. The Linkage between Game Objects and Rigid Bodies\nThe rigid bodies and collidables in the collision/physics world are nothing \nmore than abstract mathematical descriptions. In order for them to be useful \nin the context of a game, we need to link them in some way to their visual \nrepresentations on-screen. Usually, we don’t draw the rigid bodies directly \n(except for debugging purposes). Instead, the rigid bodies are used to describe \nthe shape, size, and physical behavior of the logical objects that make up the \nvirtual game world. We’ll discuss game objects in depth in Chapter 14, but for \nthe time being, we’ll rely on our intuitive notion of what a game object is—a \nlogical entity in the game world, such as a character, a vehicle, a weapon, \na ﬂ oating power-up, and so on. So the linkage between a rigid body in the \nphysics world and its visual representation on-screen is usually indirect, with \nthe logical game object serving as the hub that links the two together. This is \nillustrated in Figure 12.35.\nIn general, a game object is represented in the collision/physics world by \nzero or more rigid bodies. The following list describes three possible scenarios:\nz Zero rigid bodies. Game objects without any rigid bodies in the phys-\nics world act as though they are not solid, because they have no colli-\nsion representation at all. Decorative objects with which the player or \nRigid Body / \nCollidable\nGame \nObject\nMesh \nInstance\nRendering \nEngine\nDebug Draw\nDrive\nUpdate\nSubmit\nFigure 12.35.  Rigid bodies are linked to their visual representations by way of game objects. \nAn optional direct rendering path is usually provided so that the locations of the rigid bodies \ncan be visualized for debugging purposes.\n\n\n667 \nnon-player characters cannot interact, such as birds ﬂ ying overhead or \nportions of the game world that can be seen but never reached, might \nhave no collision. This scenario can also apply to objects whose collision \ndetection is handled manually (without the help of the collision/physics \nengine) for some reason.\nz One rigid body. Most simple game objects need only be represented by a \nsingle rigid body. In this case, the shape of the rigid body’s collidable is \nchosen to closely approximate the shape of the game object’s visual rep-\nresentation, and the rigid body’s position and orientation exactly match \nthe position and orientation of the game object itself.\nz Multiple rigid bodies. Some complex game objects are represented by \nmultiple rigid bodies in the collision/physics world. Examples include \ncharacters, machinery, vehicles, or any object that is composed of multi-\nple solid pieces. Such game objects usually make use of a skeleton (i.e., a \nhierarchy of aﬃ  ne transforms) to track the locations of their component \npieces (although other means are certainly possible as well). The rigid \nbodies are usually linked to the joints of the skeleton in such a way that \nthe position and orientation of each rigid body corresponds to the posi-\ntion and orientation of one of the joints. The joints in the skeleton might \nbe driven by an animation, in which case the associated rigid bodies \nsimply come along for the ride. Alternatively, the physics system might \ndrive the locations of rigid bodies and hence indirectly control the loca-\ntions of the joints. The mapping from joints to rigid bodies may or may \nnot be one-to-one—some joints might be controlled entirely by anima-\ntion, while others are linked to rigid bodies.\nThe linkage between game objects and rigid bodies must be managed by \nthe engine, of course. Typically, each game object will manage its own rigid \nbodies, creating and destroying them when necessary, adding and removing \nthem from the physics world as needed, and maintaining the connection be-\ntween each rigid body’s location and the location of the game object and/or \none of its joints. For complex game objects consisting of multiple rigid bodies, \na wrapper class of some kind may be used to manage them. This insulates \nthe game objects from the nitt y-gritt y details of managing a collection of rigid \nbodies and allows diﬀ erent kinds of game objects to manage their rigid bodies \nin a consistent way.\n12.5.1.1. Physics-Driven Bodies\nIf our game has a rigid body dynamics system, then presumably we want \nthe motions of at least some of the objects in the game to be driven entirely \n12.5. Integrating a Physics Engine into Your Game\n\n\n668 \n12. Collision and Rigid Body Dynamics\nby the simulation. Such game objects are called physics-driven objects. Bits of \ndebris, exploding buildings, rocks rolling down a hillside, empty magazines \nand shell casings—these are all examples of physics-driven objects.\nA physics-driven rigid body is linked to its game object by stepping the \nsimulation and then querying the physics system for the body’s position and \norientation. This transform is then applied either to the game object as a whole \nor to a joint or some other data structure within the game object.\nExample: Building a Safe with a Detachable Door\nWhen physics-driven rigid bodies are linked to the joints of a skeleton, the \nbodies are oft en constrained to produce a desired kind of motion. As an ex-\nample, let’s look at how a safe with a detachable door might be modeled.\nVisually, let’s assume that the safe consists of a single triangle mesh with \ntwo submeshes, one for the housing and one for the door. A two-joint skeleton \nis used to control the motions of these two pieces. The root joint is bound to \nthe housing of the safe, while the child joint is bound to the door in such a way \nthat rotating the door joint causes the door submesh to swing open and shut \nin a suitable way.\nThe collision geometry for the safe is broken into two independent pieces \nas well, one for the housing and one for the door. These two pieces are used \nto create two totally separate rigid bodies in the collision/physics world. The \nrigid body for the safe’s housing is att ached to the root joint in the skeleton, \nand the door’s rigid body is linked to the door joint. A hinge constraint is then \nadded to the physics world to ensure that the door body swings properly \nrelative to the housing when the dynamics of the two rigid bodies are simu-\nlated. The motions of the two rigid bodies representing the housing and the \ndoor are used to update the transforms of the two joints in the skeleton. Once \nthe skeleton’s matrix palett e has been generated by the animation system, the \nrendering engine will end up drawing the housing and door submeshes in the \nlocations of the rigid bodies within the physics world.\nIf the door needs to be blown oﬀ  at some point, the constraint can be \nbroken, and impulses can be applied to the rigid bodies to send them ﬂ y-\ning. Visibly, it will appear to the human player that the door and the housing \nhave become separate objects. But in reality, it’s still a single game object and \na single triangle mesh with two joints and two rigid bodies.\n12.5.1.2. Game-Driven Bodies\nIn most games, certain objects in the game world need to be moved about in \na non-physical way. The motions of such objects might be determined by an \nanimation or by following a spline path, or they might be under the control \n\n\n669 \nof the human player. We oft en want these objects to participate in collision \ndetection—to be capable of pushing the physics-driven objects out of their \nway, for example—but we do not want the physics system to interfere with \ntheir motion in any way. To accommodate such objects, most physics SDKs \nprovide a special type of rigid body known as a game-driven body. (Havok calls \nthese “key framed” bodies.)\nGame-driven bodies do not experience the eﬀ ects of gravity. They are also \nconsidered to be inﬁ nitely massive by the physics system (usually denoted by \na mass of zero, since this is an invalid mass for a physics-driven body). The \nassumption of inﬁ nite mass ensures that forces and collision impulses within \nthe simulation can never change the velocity of a game-driven body.\nTo move a game-driven body around in the physics world, we cannot \nsimply set its position and orientation every frame to match the location of \nthe corresponding game object. Doing so would introduce discontinuities that \nwould be very diﬃ  cult for the physical simulation to resolve. (For example, \na physics-driven body might ﬁ nd itself suddenly interpenetrating a game-\ndriven body, but it would have no information about the game-driven body’s \nmomentum with which to resolve the collision.) As such, game-driven bodies \nare usually moved using impulses —instantaneous changes in velocity that, \nwhen integrated forward in time, will position the bodies in the desired places \nat the end of the time step. Most physics SDKs provide a convenience func-\ntion that will calculate the linear and angular impulses required in order to \nachieve a desired position and orientation on the next frame. When moving \na game-driven body, we do have to be careful to zero out its velocity when it \nis supposed to stop. Otherwise, the body will continue forever along its last \nnon-zero trajectory.\nExample: Animated Safe Door\nLet’s continue our example of the safe with a detachable door. Imagine that \nwe want a character to walk up to the safe, dial the combination, open the \ndoor, deposit some money, and close and lock the door again. Later, we want \na diﬀ erent character to get the money in a rather less-civilized manner—by \nblowing the door oﬀ  the safe. To do this, the safe would be modeled with an \nadditional submesh for the dial and an additional joint that allows the dial to \nbe rotated. No rigid body is required for the dial, however, unless of course \nwe want it to ﬂ y oﬀ  when the door explodes.\nDuring the animated sequence of the person opening and closing the safe, \nits rigid bodies can be put into game-driven mode. The animation now drives \nthe joints, which in turn drive the rigid bodies. Later, when the door is to be \nblown oﬀ , we can switch the rigid bodies into physics-driven mode, break the \nhinge constraint, apply the impulse, and watch the door ﬂ y.\n12.5. Integrating a Physics Engine into Your Game\n\n\n670 \n12. Collision and Rigid Body Dynamics\nAs you’ve probably already noticed, the hinge constraint is not actually \nneeded in this particular example. It would only be required if the door is to \nbe left  open at some point and we want to see the door swinging naturally in \nresponse to the safe being moved or the door being bumped.\n12.5.1.3. Fixed Bodies\nMost game worlds are composed of both static geometry and dynamic objects. \nTo model the static components of the game world, most physics SDKs pro-\nvide a special kind of rigid body known as a ﬁ xed body. Fixed bodies act a bit \nlike game-driven bodies, but they do not take part in the dynamics simulation \nat all. They are, in eﬀ ect, collision-only bodies. This optimization can give a \nbig performance boost to most games, especially those whose worlds contain \nonly a small number of dynamic objects moving around within a large static \nworld.\n12.5.1.4. Havok’s Motion Type\nIn Havok, all types of rigid body are represented by instances of the class hkp\nRigidBody. Each instance contains a ﬁ eld that speciﬁ es its motion type . The \nmotion type tells the system whether the body is ﬁ xed, game-driven (what \nHavok calls “key framed”), or physics-driven (what Havok calls “dynamic”). \nIf a rigid body is created with the ﬁ xed motion type, its type can never be \nchanged. Otherwise, the motion type of a body can be changed dynamically at \nruntime. This feature can be incredibly useful. For example, an object that is in \na character’s hand would be game-driven. But as soon as the character drops \nor throws the object, it would be changed to physics-driven so the dynamics \nsimulation can take over its motion. This is easily accomplished in Havok by \nsimply changing the motion type at the moment of release.\nThe motion type also doubles as a way to give Havok some hints about \nthe inertia tensor of a dynamic body. As such, the “dynamic” motion type is \nbroken into subcategories such as “dynamic with sphere inertia,” “dynamic \nwith box inertia,” and so on. Using the body’s motion type, Havok can decide \nto apply various optimizations based on assumptions about the internal struc-\nture of the inertia tensor.\n12.5.2. Updating the Simulation\nThe physics simulation must of course be updated periodically, usually once \nper frame. This does not merely involve stepping the simulation (numerically \nintegrating, resolving collisions, and applying constraints). The linkages be-\ntween the game objects and their rigid bodies must be maintained as well. If \nthe game needs to apply any forces or impulses to any of the rigid bodies, this \n\n\n671 \nmust also be done every frame. The following steps are required to completely \nupdate the physics simulation:\nz Update game-driven rigid bodies . The transforms of all game-driven rigid \nbodies in the physics world are updated so that they match the trans-\nforms of their counterparts (game objects or joints) in the game world.\nz Update phantoms . A phantom shape acts like a game-driven collidable \nwith no corresponding rigid body. It is used to perform certain kinds \nof collision queries. The locations of all phantoms are updated prior to \nthe physics step, so that they will be in the right places when collision \ndetection is run.\nz Update forces, apply impulses, and adjust constraints. Any forces being ap-\nplied by the game are updated. Any impulses caused by game events \nthat occurred this frame are applied. Constraints are adjusted if neces-\nsary. (For example, a breakable hinge might be checked to determine if \nit has been broken; if so, the physics engine is instructed to remove the \nconstraint.)\nz Step the simulation . We saw in Section 12.4.10 that the collision and phys-\nics engines must both be updated periodically. This involves numerically \nintegrating the equations of motion to ﬁ nd the physical state of all bodies \non the next frame, running the collision detection algorithm to add and \nremove contacts from all rigid bodies in the physics world, resolving col-\nlisions, and applying constraints. Depending on the SDK, these update \nphases may be hidden behind a single atomic step() function, or it \nmay be possible to run them individually.\nz Update physics-driven game objects . The transforms of all physics-driven \nobjects are extracted from the physics world, and the transforms of the \ncorresponding game objects or joints are updated to match.\nz Query phantoms. The contacts of each phantom shape are read aft er the \nphysics step and used to make decisions.\nz Perform collision cast queries . Ray casts and shape casts are kicked oﬀ , either \nsynchronously or asynchronously. When the results of these queries become \navailable, they are used by various engine systems to make decisions.\nThese tasks are usually performed in the order shown above, with the \nexception of ray and shape casts, which can theoretically be done at any time \nduring the game loop. Clearly it makes sense to update game-driven bod-\nies and apply forces and impulses prior to the step, so that the eﬀ ects will \nbe “seen” by the simulation. Likewise, physics-driven game objects should \nalways be updated aft er the step, to ensure that we’re using the most up-to-\ndate body transforms. Rendering typically happens aft er everything else in \n12.5. Integrating a Physics Engine into Your Game\n\n\n672 \n12. Collision and Rigid Body Dynamics\nthe game loop. This ensures that we are rendering a consistent view of the \ngame world at a particular instant in time.\n12.5.2.1. Timing Collision Queries\nIn order to query the collision system for up-to-date information, we need to \nrun our collision queries (ray and shape casts) aft er the physics step has run \nduring the frame. However, the physics step is usually run toward the end of \nthe frame, aft er the game logic has made most of its decisions and the new lo-\ncations of any game-driven physics bodies have been determined. When then \nshould collision queries be run ?\nThis question does not have an easy answer. We have a number of op-\ntions, and most games end up using some or all of them:\nz Base decisions on last frame’s state. In many cases, decisions can be made \ncorrectly based on last frame’s collision information. For example, we \nmight want to know whether or not the player was standing on some-\nthing last frame, in order to decide whether or not he should start falling \nthis frame. In this case, we can safely run our collision queries prior to \nthe physics step.\nz Accept a one-frame lag. Even if we really want to know what is happen-\ning this frame, we may be able to tolerate a one-frame lag in our collision \nquery results. This is usually only true if the objects in question aren’t \nmoving too fast. For example, we might move one object forward in \ntime and then want to know whether or not that object is now in the \nplayer’s line of sight. A one-frame-oﬀ  error in this kind of query may \nnot be noticeable to the player. If this is the case, we can run the collision \nquery prior to the physics step (returning collision information from the \nprevious frame) and then use these results as if they were an approxima-\ntion to the collision state at the end of the current frame.\nz Run the query aft er the physics step. Another approach is to run certain \nqueries aft er the physics step. This is feasible when the decisions being \nmade based on the results of the query can be deferred until late in the \nframe. For example, a rendering eﬀ ect that depends on the results of a \ncollision query could be implemented this way.\n12.5.2.2. Single-Threaded Updating\nA very simple single-threaded game loop might look something like this:\nF32 dt = 1.0f/30.0f;\nfor (;;) // main game loop\n{\n g_hidManager->poll();\n\n\n673 \n g_gameObjectManager->\npreAnimationUpdate(dt);\n g_animationEngine->updateAnimations(dt);\n g_gameObjectManager->\npostAnimationUpdate(dt);\ng_physicsWorld->step(dt);\n g_animationEngine->updateRagDolls(dt);\n g_gameObjectManager->\npostPhysicsUpdate(dt);\n g_animationEngine->finalize();\n g_effectManager->update(dt);\n g_audioEngine->udate(dt);\n \n// etc.\n g_renderManager->render();\n \ndt = calcDeltaTime();\n}\nIn this example, our game objects are updated in three phases: once before an-\nimation runs (during which they can queue up new animations, for example), \nonce aft er the animation system has calculated ﬁ nal local poses and a tenta-\ntive global pose (but before the ﬁ nal global pose and matrix palett e has been \ngenerated), and once aft er the physics system has been stepped.\nz The locations of all game-driven rigid bodies are generally updated in \npreAnimationUpdate() or postAnimationUpdate(). Each game-\ndriven body’s transform is set to match the location of either the game \nobject that owns it or a joint in the owner’s skeleton.\nz The location of each physics-driven rigid body is generally read in \npostPhysicsUpdate() and used to update the location of either the \ngame object or one of the joints in its skeleton.\nOne important concern is the frequency with which you are stepping \nthe physics simulation. Most numerical integrators, collision detection algo-\nrithms, and constraint solvers operate best when the time between steps (Δt) \nis constant. It’s usually a good idea to step your physics/collision SDK with an \nideal 1/30 second or 1/60 second time delta and then govern the frame rate of \nyour overall game loop.\n12.5.2.3. Multithreaded Updating\nThings get a bit more complicated when a physics engine is integrated into \na multiprocessor or multithreaded game engine. In Section 7.6, we saw that \nthere are many possible ways to structure the game loop to take advantage of \nmultiprocessor hardware. Let’s take a brief look at some of the physics-speciﬁ c \nissues that arise when applying these techniques.\n12.5. Integrating a Physics Engine into Your Game\n",
      "page_number": 673,
      "chapter_number": 34,
      "summary": "This chapter covers segment 34 (pages 673-695). Key topics include collision, collisions, and bodies. z There is no friction at the point of contact between the objects’ surfaces.",
      "keywords": [
        "rigid body dynamics",
        "rigid body",
        "rigid bodies",
        "body dynamics",
        "body",
        "bodies",
        "rigid",
        "Collision",
        "Game Objects",
        "game",
        "physics",
        "objects",
        "constraint",
        "game-driven rigid bodies",
        "physics engine"
      ],
      "concepts": [
        "collision",
        "collisions",
        "bodies",
        "objects",
        "constraints",
        "game",
        "forces",
        "physics",
        "physical",
        "updates"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 3",
          "chapter": 33,
          "title": "Segment 33 (pages 314-325)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 55,
          "title": "Segment 55 (pages 529-536)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 17,
          "title": "Segment 17 (pages 150-158)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 21,
          "title": "Segment 21 (pages 198-207)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 22,
          "title": "Segment 22 (pages 210-218)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 696-718)",
      "start_page": 696,
      "end_page": 718,
      "detection_method": "synthetic",
      "content": "674 \n12. Collision and Rigid Body Dynamics\nRunning Physics in a Separate Thread\nOne option is to run the physics/collision engine in a dedicated thread . \nAs you might guess, this kind of design can lead to race conditions. If a \ngame object doesn’t update its game-driven rigid bodies in time, the physics \nthread might end up using out-of-date locations in the simulation. Likewise, \nif the simulation isn’t quite done by the time we want to update our physics-\ndriven objects, the game objects might end up using out-of-date locations as \nwell.\nThis problem can be solved by arranging for the physics and main threads \nto wait for one another—a process known as thread synchronization . This is \ndone via mutexes, semaphores, or critical sections. Thread synchronization \nis usually a relatively expensive operation, so we generally aim to reduce the \nnumber of synchronization points between threads. In the case of the physics \nengine, we need two synchronization points at minimum—one that allows \nthe physics simulation to begin each frame (aft er all game-driven rigid bodies \nhave been updated) and one that notiﬁ es the main thread when the simulation \nis complete (thereby allowing physics-driven bodies to be queried).\nAs part of a strategy to reduce synchronization points, communication \nbetween threads is usually done via a command queue. The main thread locks \na critical section, writes some commands into the queue, and then quickly re-\nleases it. The physics thread picks up the next batch of commands whenever \nit gets the chance, again locking the critical section to ensure that the main \nthread isn’t overwriting the queue during the read.\nIn the presence of collision queries , things get even more complicated. \nTo manage access to the collision/physics world by multiple threads, phys-\nics engines like Havok allow the world to be locked and unlocked separately \nfor reading and for writing. This allows collision queries to be performed at \nany time during the game loop (during which the world is locked for read) \nexcept while the physics world is being updated (during which it is locked \nfor write).\nFork and Join\nThe nice thing about a fork and join architecture for physics is that it essen-\ntially eliminates all inter-thread synchronization issues. The main thread runs \nas usual until the physics system needs to be stepped. Then we fork the step \noﬀ  into separate threads (ideally one per processing core or hardware thread) \nin order to execute it as quickly as possible. When all threads have completed \ntheir work, the results can be collated, and the main thread can continue as in \nthe single-threaded case. Of course, for this to work, the physics system must \nbe designed to support fork and join. Most physics SDKs, including Havok \n\n\n675 \nand PhysX, make use of collision islands—groups of rigid bodies that can be \nsimulated independently of one another. This design lends itself well to a fork \nand join architecture, as the islands can be dynamically distributed among the \navailable threads.\nJobs\nA job model can be particularly useful for physics processing if the physics \nSDK allows the individual phases of its update step (integration, collision de-\ntection, constraint solving, CCD, etc.) to be run independently. This allows \nus to kick oﬀ  each phase whenever it is most convenient and perform useful \nunrelated work in the main thread while we wait for the physics engine to do \nits thing.\nJobs are even more useful when doing collision queries (ray and shape \ncasts). This is because while a game engine typically only needs to step the \nphysics simulation once per frame, collision queries may be required at many \ndiﬀ erent points during the game loop. If lightweight jobs are used to run \nqueries, we can simply kick oﬀ  jobs whenever we need them. On the other \nhand, if collision queries can only be run at certain times during the frame \n(because they are being executed by a fork or a dedicated thread), this makes \nthe job of the game programmer more diﬃ  cult. He or she needs to collect all \nthe collision queries in a queue and then execute them as a batch the next time \nqueries are run during the frame. These two approaches are compared in Fig-\nure 12.36.\nPPU\nSPU\nJob\nJob\nJob\nMain \nThread\nCollision \nThread\nProcess Batch\nProcess Batch\nP...\nJob\nJob\nJob\nJ..\nJ\nGame Loop\nGame Loop\nFigure 12.36.  Collision queries are often batched, to be run at a few well-chosen points dur-\ning the game loop. However, with a job model, queries can be kicked off at any time, without \nthe need to batch them.\n12.5.3. Example Uses of Collision and Physics in a Game\nTo make our discussion of collision and physics more concrete, let’s take a \nhigh-level look at a few common examples of how collision and/or physics \nsimulations are commonly used in real games.\n12.5. Integrating a Physics Engine into Your Game\n\n\n676 \n12. Collision and Rigid Body Dynamics\n12.5.3.1. Simple Rigid Body Game Objects\nMany games include simple physically simulated objects like weapons, rocks \nthat can be picked up and thrown, empty magazines, furniture, objects on \nshelves that can be shot, and so on. Such objects might be implemented by \ncreating a custom game object class and giving it a reference to a rigid body \nin the physics world (e.g., hkRigidBody if we’re using Havok). Or we might \ncreate an add-on component class that handles simple rigid body collision \nand physics, allowing this feature to be added to virtually any type of game \nobject in the engine.\nSimple physics objects usually change their motion type at runtime. They \nare game-driven when being held in a character’s hand and physics-driven \nwhen in free fall aft er having been dropped.\nImagine that a simple physics object is to be placed on a table or shelf, to \nbe knocked oﬀ  at some point by being struck by a bullet or other object. What \nmotion type should it be given initially? Should we make it physics-driven \nand let the simulation put it to sleep until it is struck? Or should we keep \nit game-driven when at rest and change it to physics-driven when hit? This \ndepends largely on the game design. If we require tight control over when \nthe object is allowed to be knocked down, then we might go the game-driven \nroute; otherwise, physics-driven may suﬃ  ce.\n12.5.3.2. Bullet Traces\nWhether or not you approve of game violence, the fact remains that bullets \nand projectiles of one form or another are a big part of most games. Let’s look \nat how these are typically implemented.\nSometimes bullets are implemented using ray casts. On the frame that the \nweapon is ﬁ red, we shoot oﬀ  a ray cast, determine what object was hit, and \nimmediately impart the impact to the aﬀ ected object.\nUnfortunately, the ray cast approach does not account for the travel time \nof the projectile. It also does not account for the slight downward trajectory \ncaused by the inﬂ uence of gravity. If these details are important to the game, \nwe can model our projectiles using real rigid bodies that move through the \ncollision/physics world over time. This is especially useful for slower-moving \nprojectiles, like thrown objects or rockets.\nThere are plenty of issues to consider and deal with when implementing \nbullets and projectiles. Here are a few of the most common ones:\nz Does the ray come from the camera focal point or from the tip of the gun \nin the player character’s hands? This is especially problematic in a third-\nperson shooter, where the ray coming out of the player’s gun usually \n\n\n677 \ndoes not align with the ray coming from the camera focal point through \nthe reticle in the center of the screen. This can lead to situations in which \nthe reticle appears to be on top of a target yet the third-person character \nis clearly behind an obstacle and would not be able to shoot that target \nfrom his point of view. Various “tricks” must usually be employed to \nensure that the player feels like he or she is shooting what he or she is \naiming at while maintaining plausible visuals on the screen.\nz Mismatches between collision geometry and visible geometry can lead \nto situations in which the player can see the target through a small crack \nor just over the edge of some other object and yet the collision geometry \nis solid and hence the bullet cannot reach the target. (This is usually \nonly a problem for the player character.) One solution to this problem is \nto use a render query instead of a collision query to determine if the ray \nactually hit the target. For example, during one of the rendering passes, \nwe could generate a texture in which each pixel stores the unique identi-\nﬁ er of the game object to which it corresponds. We can then query this \ntexture to determine whether or not an enemy character or other suit-\nable target is beneath the weapon’s reticle.\nz AI characters may need to “lead” their shots if projectiles take a ﬁ nite \namount of time to reach their targets.\nz When bullets hit their targets, we may want to trigger a sound or a par-\nticle eﬀ ect, lay down a decal, or perform other tasks.\n12.5.3.3. Grenades\nGrenades in games are sometimes implemented as free-moving physics objects. \nHowever, this leads to a signiﬁ cant loss of control. Some control can be regained \nby imposing various artiﬁ cial forces or impulses on the grenade. For example, \nwe could apply an extreme air drag once the grenade bounces for the ﬁ rst time, \nin an att empt to limit the distance it can bounce away from its target.\nSome game teams actually go so far as to manage the grenade’s motion en-\ntirely manually. The arc of a grenade’s trajectory can be calculated beforehand, \nusing a series of ray casts to determine what target it would hit if released. The \ntrajectory can even be shown to the player via some kind of on-screen display. \nWhen the grenade is thrown, the game moves it along its arc and can then \ncarefully control the bounce so that it never goes too far away from its target, \nwhile still looking natural.\n12.5.3.4. Explosions\nIn a game, an explosion typically has a few components: some kind of visual \neﬀ ect like a ﬁ reball and smoke, audio eﬀ ects to mimic the sound of the explo-\n12.5. Integrating a Physics Engine into Your Game\n\n\n678 \n12. Collision and Rigid Body Dynamics\nsion and its impacts with objects in the world, and a growing damage radius \nthat aﬀ ects any objects in its wake.\nWhen an object ﬁ nds itself in the radius of an explosion, its health is typi-\ncally reduced, and we oft en also want to impart some motion to mimic the ef-\nfect of the shock wave. This might be done via an animation. (For example, the \nreaction of character to an explosion might best be done this way.) We might \nalso wish to allow the impact reaction to be driven entirely by the dynamics \nsimulation. We can accomplish this by having the explosion apply impulses \nto any suitable objects within its radius. It’s prett y easy to calculate direction \nof these impulses—they are typically radial, calculated by normalizing the \nvector from the center of the explosion to the center of the impacted object \nand then scaling this vector by the magnitude of the explosion (and perhaps \nfalling oﬀ  as the distance from the epicenter increases).\nExplosions may interact with other engine systems as well. For example, \nwe might want to impart a “force” to the animated foliage system, causing \ngrass, plants and trees to momentarily bend as a result of the explosion’s \nshock wave.\n12.5.3.5. Destructible Objects\nDestructible objects are commonplace in many games. These objects are pecu-\nliar because they start out in an undamaged state in which they must appear \nto be a single cohesive object, and yet they must be capable of breaking into \nmany separate pieces. We may want the pieces to break oﬀ  one by one, al-\nlowing the object to be “whitt led down” gradually, or we may only require a \nsingle catastrophic explosion.\nDeformable body simulations like DMM can handle destruction naturally. \nHowever, we can also implement breakable objects using rigid body dynamics. \nThis is typically done by dividing a model into a number of breakable pieces \nand assigning a separate rigid body to each one. For reasons of performance op-\ntimization and/or visual quality, we might decide to use special “undamaged” \nversions of the visual and collision geometry, each of which is constructed as \na single solid piece. This model can be swapped out for the damaged version \nwhen the object needs to start breaking apart. In other cases, we may want to \nmodel the object as separate pieces at all times. This might be appropriate if the \nobject is a stack of bricks or a pile of pots and pans, for example.\nTo model a multi-piece object, we could simply stack a bunch of rigid \nbodies and let physics simulation take care of it. This can be made to work \nin good-quality physics engines (although it’s not always trivial to get right). \nHowever, we may want some Hollywood-style eﬀ ects that cannot be achieved \nwith a simple stack of rigid bodies.\n\n\n679 \nFor example, we may want to deﬁ ne the structure of the object. Some \npieces might be indestructible, like the base of a wall or the chassis of a car. \nOthers might be non-structural—they just fall oﬀ  when hit by bullets or other \nobjects. Still other pieces might be structural—if they are hit, not only do they \nfall, but they also impart forces to other pieces lying on top of them. Some \npieces could be explosive—when they are hit, they create secondary explosions \nor propagate damage throughout the structure. We may want some pieces to \nact as valid cover points for characters but not others. This implies that our \nbreakable object system may have some connections to the cover system.\nWe might also want our breakable objects to have a notion of health. Dam-\nage might build up until eventually the whole thing collapses, or each piece \nmight have a health, requiring multiples shots or impacts before it is allowed \nto break. Constraints might also be employed to allow broken pieces to hang \noﬀ  the object rather than coming away from it completely.\nWe may also want our structures to take time to collapse completely. For \nexample, if a long bridge is hit by an explosion at one end, the collapse should \nslowly propagate from one end to the other so that the bridge looks massive. \nThis is another example of a feature the physics system won’t give you for \nfree—it would just wake up all rigid bodies in the simulation island simulta-\nneously. These kinds of eﬀ ects can be implemented through judicious use of \nthe game-driven motion type.\n12.5.3.6. Character Mechanics\nFor a game like bowling, pinball, or Marble Madness, the “main character” is \na ball that rolls around in an imaginary game world. For this kind of game, \nwe could very well model the ball as a free-moving rigid body in the physics \nsimulation and control its movements by applying forces and impulses to it \nduring gameplay.\nIn character-based games, however, we usually don’t take this kind of ap-\nproach. The movement of a humanoid or animal character is usually far too \ncomplex to be controlled adequately with forces and impulses. Instead, we \nusually model characters as a set of game-driven capsule-shaped rigid bodies, \neach one linked to a joint in the character’s animated skeleton. These bodies \nare primarily used for bullet hit detection or to generate secondary eﬀ ects \nsuch as when a character’s arm bumps an object oﬀ  a table. Because these \nbodies are game-driven , they won’t avoid interpenetrations with immovable \nobjects in the physics world, so it is up to the animator to ensure that the char-\nacter’s movements appear believable.\nTo move the character around in the game world, most games use sphere \nor capsule casts to probe in the direction of desired motion. Collisions are \nresolved manually. This allows us to do cool stuﬀ  like:\n12.5. Integrating a Physics Engine into Your Game\n\n\n680 \n12. Collision and Rigid Body Dynamics\nz having the character slide along walls when he runs into them at an \noblique angle;\nz allowing the character to “pop up” over low curbs rather than gett ing \nstuck;\nz preventing the character from entering a “falling” state when he walks \noﬀ  a low curb;\nz preventing the character from walking up slopes that are too steep \n(most games have a cut-oﬀ  angle aft er which the character will slide \nback rather than being able to walk up the slope);\nz adjusting animations to accommodate collisions.\nAs an example of this last point, if the character is running directly into a \nwall at a roughly 90 degree angle, we can let the character “moonwalk” into \nthe wall forever, or we can slow down his animation. We can also do some-\nthing even more slick, like playing an animation in which the character sticks \nout his hand and touches the wall and then idles sensibly until the movement \ndirection changes.\nHavok provides a character controller system that handles many of these \nthings. In Havok’s system, illustrated in Figure 12.37, a character is modeled \nas a capsule phantom that is moved each frame to ﬁ nd a potential new loca-\ntion. A collision contact manifold (i.e., a collection of contact planes, cleaned \nup to eliminate noise) is maintained for the character. This manifold can be \nFigure 12.37.  Havok’s character controller models a character as a capsule-shaped phantom. \nThe phantom maintains a noise-reduced collision manifold (a collection of contact planes) \nthat can be used by the game to make movement decisions.\n\n\n681 \nanalyzed each frame in order to determine how best to move the character, \nadjust animations, and so on.\n12.5.3.7. Camera Collision\nIn many games, the camera follows the player’s character or vehicle around \nin the game world, and it can oft en by rotated or controlled in limited ways \nby the person playing the game. It’s important in such games to never permit \nthe camera to interpenetrate geometry in the scene, as this would break the \nillusion of realism. The camera system is therefore another important client of \nthe collision engine in many games.\nThe basic idea behind most camera collision systems is to surround the \nvirtual camera with one or more sphere phantoms or sphere cast queries that \ncan detect when it is gett ing close to colliding with something. The system can \nrespond by adjusting the camera’s position and/or orientation in some way \nto avoid the potential collision before the camera actually passes through the \nobject in question.\nThis sounds simple enough, but it is actually an incredibly tricky problem \nrequiring a great deal of trial and error to get right. To give you a feel for how \nmuch eﬀ ort can be involved, many game teams have a dedicated engineer \nworking on the camera system for the entire duration of the project. We can’t \npossibly cover camera collision detection and resolution in any depth here, \nbut the following list should give you a sense of some of the most pertinent \nissues to be aware of:\nz Zooming the camera in to avoid collisions works well in a wide variety \nof situations. In a third-person game, you can zoom all the way in to a \nﬁ rst-person view without causing too much trouble (other than making \nsure the camera doesn’t interpenetrate the character’s head in the pro-\ncess).\nz It’s usually a bad idea to drastically change the horizontal angle of the \ncamera in response to collisions, as this tends to mess with camera-rel-\native player controls. However, some degree of horizontal adjustment \ncan work well, depending on what the player is expected to be doing \nat the time. If she is aiming at a target, she’ll be angry with you if you \nthrow oﬀ  her aim to bring the camera out of collision. But if she’s just \nlocomoting through the world, the change in camera orientation may \nfeel entirely natural.\nz You can adjust the vertical angle of the camera to some degree, but it’s \nimportant not to do too much of this or the player will lose track of the \nhorizon and end up looking down onto the top of the player character’s \nhead!\n12.5. Integrating a Physics Engine into Your Game\n\n\n682 \n12. Collision and Rigid Body Dynamics\nz Some games allow the camera to move along an arc lying in a vertical \nplane, perhaps described by a spline . This permits a single HID control \nsuch as the vertical deﬂ ection of the left  thumb stick to control both the \nzoom and the vertical angle of the camera in an intuitive way. (The cam-\nera in Uncharted: Drake’s Fortune works this way.) When the camera comes \ninto collision with objects in the world, it can be automatically moved \nalong this same arc to avoid the collision, the arc might be compressed \nhorizontally, or any number of other approaches might be taken.\nz It’s important to consider not only what’s behind and beside the camera \nbut what is in front of it as well. For example, what should happen if \na pillar or another character comes between the camera and the player \ncharacter? In some games, the oﬀ ending object becomes translucent ; in \nothers, the camera zooms in or swings around to avoid the collision. \nThis may or may not feel good to the person playing the game! How \nyou handle these kinds of situations can make or break the perceived \nquality of your game.\nz You may want the camera to react to collisions diﬀ erently in diﬀ erent \nsituations. For example, when the main character is not engaged in a \nbatt le, it might be acceptable to swing the camera horizontally to avoid \ncollisions. But when the player is trying to ﬁ re at targets, both horizontal \nand vertical camera swings will throw oﬀ  his or her aim, so zoom may \nbe the only option.\nEven aft er taking account of these and many other problematic situations, \nyour camera may not look or feel right! Always budget plenty of time for trial \nand error when implementing a camera collision system.\n12.5.3.8. Rag Doll Integration\n In Section 12.4.8.7, we learned how special types of constraints can be used to \nlink a collection of rigid bodies together to mimic the behavior of a limp (dead \nor unconscious) human body. In this section, we’ll investigate a few of the is-\nsues that arise when integrating rag doll physics into your game.\nAs we saw in Section 12.5.3.6, the gross movements of a conscious char-\nacter are usually determined by performing shape casts or moving a phantom \nshape around in the game world. The detailed movements of the character’s \nbody are typically driven by animations. Game-driven rigid bodies are some-\ntimes att ached to the limbs for the purposes of weapons targeting or to allow \nthe character to knock over other objects in the world.\nWhen a character becomes unconscious, the rag doll system kicks in. \nThe character’s limbs are modeled as capsule-shaped rigid bodies connected \n\n\n683 \nvia constraints and linked to joints in the character’s animated skeleton. The \nphysics system simulates the motions of these bodies, and we update the \nskeletal joints to match, thereby allowing physics to move the character’s \nbody .\nThe set of rigid bodies used for rag doll physics might not be the same \nones aﬃ  xed to the character’s limbs when it was alive. This is because the \ntwo collision models have very diﬀ erent requirements. When the character \nis alive, its rigid bodies are game-driven, so we don’t care if they interpen-\netrate. And in fact, we usually want them to overlap, so there aren’t any holes \nthrough which an enemy character might shoot. But when the character turns \ninto a rag doll, it’s important that the rigid bodies do not interpenetrate, as \nthis would cause the collision resolution system to impart large impulses that \nwould tend to make the limbs explode outward! For these reasons, it’s actually \nquite common for characters to have entirely diﬀ erent collision/physics repre-\nsentations depending on whether they’re conscious or unconscious.\nAnother issue is how to transition from the conscious state to the uncon-\nscious state. A simple LERP animation blend between animation-generated \nand physics-generated poses usually doesn’t work very well, because the phys-\nics pose very quickly diverges from the animation pose. (A blend between two \ntotally unrelated poses usually doesn’t look natural.) As such, we may want to \nuse powered constraints during the transition (see Section 12.4.8.8).\nCharacters oft en interpenetrate background geometry when they are con-\nscious (i.e., when their rigid bodies are game-driven). This means that the rigid \nbodies might be inside another solid object when the character transitions to \nrag doll (physics-driven) mode. This can give rise to huge impulses that cause \nrather wild-looking rag doll behavior in-game. To avoid these problems, it \nis best to author death animations carefully, so that the character’s limbs are \nkept out of collision as best as possible. It’s also important to detect collisions \nvia phantoms or collision callbacks during the game-driven mode so that you \ncan drop the character into rag doll mode the moment any part of his body \ntouches something solid.\nEven when these steps are taken, rag dolls have a tendency to get stuck \ninside other objects. Single-sided collision can be an incredibly important fea-\nture when trying to make rag dolls look good. If a limb is partly embedded \nin a wall, it will tend to be pushed out of the wall rather than staying stuck \ninside it. However, even single-sided collision doesn’t solve all problems. For \nexample, when the character is moving quickly or if the transition to rag doll \nisn’t executed properly, one rigid body in the rag doll can end up on the far \nside of a thin wall. This causes the character to hang in mid air rather than fall-\ning properly to the ground.\n12.5. Integrating a Physics Engine into Your Game\n\n\n684 \n12. Collision and Rigid Body Dynamics\nAnother rag doll feature that is in vogue these days is the ability for un-\nconscious characters to regain consciousness and get back up . To implement \nthis, we need a way to search for a suitable “stand up” animation. We want \nto ﬁ nd an animation whose pose on frame zero most closely matches the rag \ndoll’s pose aft er it has come to rest (which is totally unpredictable in general). \nThis can be done by matching the poses of only a few key joints, like the up-\nper thighs and the upper arms. Another approach is to manually guide the \nrag doll into a pose suitable for gett ing up by the time it comes to rest, using \npowered constraints.\nAs a ﬁ nal note, we should mention that sett ing up a rag doll’s constraints \ncan be a tricky business. We generally want the limbs to move freely but with-\nout doing anything biomechanically impossible. This is one reason special-\nized types of constraints are oft en used when constructing rag dolls. None-\ntheless, you shouldn’t assume that your rag dolls will look great without \nsome eﬀ ort. High-quality physics engines like Havok provide a rich set of \ncontent creation tools that allow an artist to set up constraints within a DCC \npackage like Maya and then test them in real time to see how they might look \nin-game.\nAll in all, gett ing rag doll physics to work in your game isn’t particularly \ndiﬃ  cult, but gett ing it to look good can take a lot of work! As with many things \nin game programming, it’s a good idea to budget plenty of time for trial and \nerror, especially when it’s your ﬁ rst time working with rag dolls.\n12.6. A Look Ahead: Advanced Physics Features\nA rigid body dynamics simulation with constraints can cover an amazing \nrange of physics-driven eﬀ ects in a game. However, such a system clearly has \nits limitations. Recent research and development is seeking to expand physics \nengines beyond constrained rigid bodies. Here are just a few examples:\nz Deformable bodies. As hardware capabilities improve and more-eﬃ  cient \nalgorithms are developed, physics engines are beginning to provide \nsupport for deformable bodies . DMM is an excellent example of such an \nengine.\nz Cloth. Cloth can be modeled as a sheet of point masses, connected by \nstiﬀ  springs. However, cloth is notoriously diﬃ  cult to get right, as many \ndiﬃ  culties arise with respect to collision between cloth and other ob-\njects, numerical stability of the simulation, etc.\nz Hair. Hair can be modeled by a large number of small physically simu-\nlated ﬁ lments, or a simpler approach can be used to make a character’s \n\n\n685 \nhair move as if it were a rope or deformable body. This is an active area \nof research, and the quality of hair in games continues to improve.\nz Water surface simulations and buoyancy. Games have been doing water \nsurface simulations and buoyancy for some time now. This can be done \nvia a special-case system (not part of the physics engine per se), or it can \nbe modeled via forces within the physics simulation. Organic move-\nment of the water surface is oft en a rendering eﬀ ect only and does not \naﬀ ect the physics simulation at all. From the point of view of physics, \nthe water surface is oft en modeled as a plane. For large displacements \nin the water surface, the entire plane might be moved. However, some \ngame teams and researchers are pushing the limits of these simulations, \nallowing for dynamic water surfaces, waves that crest, realistic current \nsimulations, and more.\nz General ﬂ uid dynamics simulations. Right now, ﬂ uid dynamics falls into \nthe realm of specialized simulation libraries. However, this is an active \narea of research, and it may well eventually ﬁ nd its way into mainstream \nphysics engines.\n12.6. A Look Ahead: Advanced Physics Features\n\n\nPart IV\nGameplay\n\n\n13\n \nIntroduction to \nGameplay Systems\nU\np until now, everything we’ve talked about in this book has focused on \ntechnology. We’ve learned that a game engine is a complex, layered soft -\nware system built on top of the hardware, drivers, and operating system of the \ntarget machine. We’ve seen how low-level engine systems provide services \nthat are required by the rest of the engine; how human interface devices such \nas joypads, keyboards, mice, and other devices can allow a human player to \nprovide inputs to the engine; how the rendering engine produces 3D images \non-screen; how the collision system detects and resolves interpenetrations be-\ntween shapes; how the physics simulation causes objects to move in physi-\ncally realistic ways; how the animation system allows characters and objects \nto move naturally. But despite the wide range of powerful features provided \nby these components, if we were to put them all together, we still wouldn’t \nhave a game!\nA game is deﬁ ned not by its technology but by its gameplay . Gameplay can \nbe deﬁ ned as the overall experience of playing a game. The term game mechan-\nics pins down this idea a bit more concretely—it is usually deﬁ ned as the set \nof rules that govern the interactions between the various entities in the game. \nIt also deﬁ nes the objectives of the player(s), criteria for success and failure, \nthe player character’s abilities, the number and types of non-player entities that \nexist within the game’s virtual world, and the overall ﬂ ow of the gaming expe-\nrience as a whole. In many games, these elements are intertwined with a com-\n689\n\n\n690 \n13. Introduction to Gameplay Systems\npelling story and a rich cast of characters. However, story and characters are \ndeﬁ nitely not a necessary part of every video game, as evidenced by wildly \nsuccessful puzzle games like Tetris. In their paper, “A Survey of ‘Game’ Por-\ntability” \n(htt p://www.dcs.shef.ac.uk/intranet/research/resmes/CS0705.pdf), \nAhmed BinSubaih, Steve Maddock, and Daniela Romano of the University of \nSheﬃ  eld refer to the collection of soft ware systems used to implement game-\nplay as a game’s G-factor . In the next three chapters, we’ll explore the crucial \ntools and engine systems that deﬁ ne and manage the game mechanics (a.k.a. \ngameplay, a.k.a. G-factor) of a game.\n13.1. Anatomy of a Game World\nGameplay designs vary widely from genre to genre and from game to game. \nThat said, most 3D games, and a good number of 2D games as well, conform \nmore or less to a few basic structural patt erns. We’ll discuss these patt erns \nin the following sections, but please keep in mind that there are bound to be \ngames out there that do not ﬁ t neatly into this mold.\n13.1.1. World Elements\nMost video games take place in a two- or three-dimensional virtual game \nworld . This world is typically comprised of numerous discrete elements . Gen-\nerally, these elements fall into two categories: static elements and dynamic \nelements. Static elements include terrain, buildings, roads, bridges, and prett y \nmuch anything that doesn’t move or interact with gameplay in an active way. \nDynamic elements include characters, vehicles, weaponry, ﬂ oating power-ups \nand health packs, collectible objects, particle emitt ers, dynamic lights, invis-\nible regions used to detect important events in the game, splines that deﬁ ne \nthe paths of objects, and so on. This breakdown of the game world is illus-\ntrated in Figure 13.1.\nGameplay is generally concentrated within the dynamic elements of a \ngame. Clearly, the layout of the static background plays a crucial role in how \nthe game plays out. For example, a cover-based shooter wouldn’t be very \nmuch fun if it were played in a big, empty, rectangular room. However, the \nsoft ware systems that implement gameplay are primarily concerned with up-\ndating the locations, orientations, and internal states of the dynamic elements, \nsince they are the elements that change over time. The term game state refers to \nthe current state of all dynamic game world elements, taken as a whole.\nThe ratio of dynamic to static elements also varies from game to game. \nMost 3D games consist of a relatively small number of dynamic elements mov-\n\n\n691 \ning about within a relatively large static background area. Other games, like \nthe arcade classic Asteroids or the Xbox 360 retro hit Geometry Wars, have no \nstatic elements to speak of (other than a black screen). The dynamic elements \nof a game are usually more expensive than the static elements in terms of CPU \nresources, so most 3D games are constrained to a limited number of dynamic \nelements. However, the higher the ratio of dynamic to static elements, the \nmore “alive” the game world can seem to the player. As gaming hardware \nbecomes more and more powerful, games are achieving higher and higher \ndynamic-to-static ratios.\nIt’s important to note that the distinction between the dynamic and static \nelements in a game world is oft en a bit blurry. For example, in the arcade game \nHydro Thunder, the waterfalls were dynamic, in the sense that their textures \nanimated, they had dynamic mist eﬀ ects at their bases, and they could be \nplaced into the game world and positioned by a game designer independently \nof the terrain and water surface. However, from an engineering standpoint, \nwaterfalls were treated as static elements because they did not interact with \nthe boats in the race in any way (other than to obscure the player’s view of hid-\nFigure 13.1.  A typical game world is comprised of both static and dynamic elements.\n13.1. Anatomy of a Game World\n\n\n692 \n13. Introduction to Gameplay Systems\nden boost power-ups and secret passageways). Diﬀ erent game engines draw \ndiﬀ erent lines between static and dynamic elements, and some don’t draw a \ndistinction at all (i.e., everything is potentially a dynamic element).\nThe distinction between static and dynamic serves primarily as an opti-\nmization tool—we can do less work when we know that the state of an object \nisn’t going to change. For example, the vertices of a static triangle mesh can \nbe speciﬁ ed in world space, thereby saving the per-vertex matrix multiplica-\ntion normally required to transform from model space to world space during \nrendering. Lighting can be precomputed, in the form of static vertex lighting, \nlight maps, shadow maps, static ambient occlusion information, or precom-\nputed radiance transfer (PRT) spherical harmonics coeﬃ  cients. Virtually any \ncomputation that must be done at runtime for a dynamic world element is a \ngood candidate for precomputation or omission when applied to a static ele-\nment.\nGames with destructible environments are an example of how the line \nbetween the static and dynamic elements in a game world can blur. For in-\nstance, we might deﬁ ne three versions of every static element—an undam-\naged version, a damaged version, and a fully destroyed version. These back-\nground elements act like static world elements most of the time, but they can \nbe swapped dynamically during an explosion to produce the illusion of be-\ncoming damaged. In reality, static and dynamic world elements are just two \nextremes along a gamut of possible optimizations. Where we draw the line \nbetween the two categories (if we draw one at all) shift s as our optimization \nmethodologies change and adapt to the needs of the game design.\n13.1.1.1. \nStatic Geometry\n The geometry of a static world element is oft en deﬁ ned in a tool like Maya. It \nmight be one giant triangle mesh, or it might be broken up into discrete pieces. \nThe static portions of the scene are sometimes built out of instanced geometry . \nInstancing is a memory conservation technique in which a relatively small \nnumber of unique triangle meshes are rendered multiple times throughout \nthe game world, at diﬀ erent locations and orientations, in order to provide the \nillusion of variety. For example, a 3D modeler might create ﬁ ve diﬀ erent kinds \nof short wall sections and then piece them together in random combinations \nin order to construct miles of unique-looking walls.\nStatic visual elements and collision data might also be constructed from \nbrush geometry . This kind of geometry originated with the Quake family of \nengines. A brush describes a shape as a collection of convex volumes, each \nbounded by a set of planes. Brush geometry is fast and easy to create and \nintegrates well into a BSP-tree -based rendering engine. Brushes can be real-\n\n\n693 \nly useful for rapidly blocking out the contents of a game world. This allows \ngameplay to be tested early, when it is cheap to do so. If the layout proves its \nworth, the art team can either texture map and ﬁ ne-tune the brush geometry \nor replace it with more-detailed custom mesh assets. On the other hand, if \nthe level requires redesign, the brush geometry can be easily revised without \ncreating a lot of rework for the art team.\n13.1.2. World Chunks\nWhen a game takes place in a very large virtual world, it is typically divided \ninto discrete playable regions, which we’ll call world chunks . Chunks are also \nknown as levels, maps, stages, or areas. The player can usually see only one, or at \nmost a handful, of chunks at any given moment while playing the game, and \nhe or she progresses from chunk to chunk as the game unfolds.\nOriginally, the concept of “levels” was invented as a mechanism to pro-\nvide greater variety of gameplay within the memory limitations of early gam-\nFigure 13.2.  Many game worlds are divided into chunks for various reasons, including memory \nlimitations, the need to control the ﬂ ow of the game through the world, and as a division-of-\nlabor mechanism during development.\nChunk 2\nChunk 1\n13.1. Anatomy of a Game World\n\n\n694 \n13. Introduction to Gameplay Systems\ning hardware. Only one level could exist in memory at a time, but the player \ncould progress from level to level for a much richer overall experience. Since \nthen, game designs have branched out in many directions, and linear level-\nbased games are much less common today. Some games are essentially still \nlinear, but the delineations between world chunks are usually not as obvious \nto the player as they once were. Other games use a star topology, in which the \nplayer starts in a central hub area and can access other areas at random from \nthe hub (perhaps only aft er they have been unlocked). Others use a graph-like \ntopology, where areas are connected to one another in arbitrary ways. Still \nothers provide the illusion of a vast, open world .\nDespite the richness of modern game designs, all but the smallest of game \nworlds are still divided into chunks of some kind. This is done for a number of \nreasons. First of all, memory limitations are still an important constraint (and \nwill be until game machines with inﬁ nite memory hit the market!). World \nchunks are also a convenient mechanism for controlling the overall ﬂ ow of the \ngame. Chunks can serve as a division-of-labor mechanism as well; each chunk \ncan be constructed and managed by a relatively small group of designers and \nartists. World chunks are illustrated in Figure 13.2.\n13.1.3. High-Level Game Flow\nA game’s high-level ﬂ ow deﬁ nes a sequence, tree, or graph of player objectives . \nObjectives are sometimes called tasks , stages, levels (a term that can also apply \nto world chunks), or waves (if the game is primarily about defeating hordes of \natt acking enemies). The high-level ﬂ ow also provides the deﬁ nition of success \nfor each objective (e.g., clear all the enemies and get the key) and the penalty \nfor failure (e.g., go back to the start of the current area, possibly losing a “life” \nin the process). In a story-driven game, this ﬂ ow might also include various \nin-game movies that serve to advance the player’s understanding of the story \nas it unfolds. These sequences are sometimes called cut-scenes, in-game cin-\nematics (IGC), or noninteractive sequences (NIS). When they are rendered oﬀ -\nline and played back as a full-screen movie, such sequences are usually called \nfull-motion videos (FMV).\nEarly games mapped the objectives of the player one-to-one to particular \nworld chunks (hence the dual meaning of the term “level”). For example, in \nDonkey Kong, each new level presents Mario with a new objective (namely to \nreach the top of the structure and progress to the next level). However, this \none-to-one mapping between world chunks and objectives is less popular in \nmodern game design. Each objective is associated with one or more world \nchunks, but the coupling between chunks and objectives remains deliberately \nloose. This kind of design oﬀ ers the ﬂ exibility to alter game objectives and \n\n\n695 \nworld subdivision independently, which is extremely helpful from a logistic \nand practical standpoint when developing a game. Many games group their \nobjectives into coarser sections of gameplay, oft en called chapters or acts. A \ntypical gameplay architecture is shown in Figure 13.3.\n13.2. Implementing Dynamic Elements: \n \nGame Objects\nThe dynamic elements of a game are usually designed in an object-oriented \nfashion. This approach is intuitive and natural and maps well to the game de-\nsigner’s notion of how the world is constructed. He or she can visualize char-\nacters, vehicles, ﬂ oating health packs, exploding barrels, and myriad other \ndynamic objects moving about in the game. So it is only natural to want to \nbe able to create and manipulate these elements in the game world editor . \nLikewise, programmers usually ﬁ nd it natural to implement dynamic ele-\nments as largely autonomous agents at runtime. In this book, we’ll use the \nterm game object (GO) to refer to virtually any dynamic element within a game \nworld. However, this terminology is by no means standard within the indus-\ntry. Game objects are commonly referred to as entities, actors, or agents, and the \nlist of terms goes on.\nChapter 1\nChunk 1\nChunk 2\nChunk 3\nObjective 1 B\nObjective 1 A\nObjective 1 C\nOptional\nObjective 1D\nObjective 1 E\nObjective 1 G\nOptoinal\nObjective 1 F\nChapter 2\nChunk 4\nChunk 5\nChunk 6\nChunk 7\nObjective 2 B\nObjective 2 A\nObjective 2 C\nObjective 2 D\nObjective 2 G\nOptoinal\nObjective 2 H\nOptional\nObjective 2 F\nOptional\nObjective 2E\nObjective 2 I\nFigure 13.3.  Gameplay objectives are typically arranged in a sequence, tree, or graph, and \neach one maps to one or more game world chunks.\n13.2. Implementing Dynamic Elements: Game Objects\n\n\n696 \n13. Introduction to Gameplay Systems\nAs is customary in object-oriented design, a game object is essentially a \ncollection of att ributes (the current state of the object) and behaviors (how the \nstate changes over time and in response to events). Game objects are usually \nclassiﬁ ed by type . Diﬀ erent types of objects have diﬀ erent att ribute schemas \nand diﬀ erent behaviors. All instances of a particular type share the same at-\ntribute schema and the same set of behaviors, but the values of the att ributes \ndiﬀ er from instance to instance. (Note that if a game object’s behavior is data-\ndriven, say through script code or via a set of data-driven rules governing the \nobject’s responses to events, then behavior too can vary on an instance-by-\ninstance basis.)\nThe distinction between a type and an instance of a type is a crucial one. For \nexample, the game of Pac-Man involves four game object types: ghosts, pellets, \npower pills, and Pac-Man. However, at any moment in time, there may be up \nto four instances of the type “ghost,” 50–100 instances of the type “pellet,” \nfour “power pill” instances, and one instance of the “Pac-Man” type.\nMost object-oriented systems provide some mechanism for the inheritance \nof att ributes, behavior, or both. Inheritance encourages code and design reuse. \nThe speciﬁ cs of how inheritance works varies widely from game to game, but \nmost game engines support it in some form.\n13.2.1. Game Object Models\nIn computer science, the term object model has two related but distinct mean-\nings. It can refer to the set of features provided by a particular programming \nlanguage or formal design language. For example, we might speak of the C++ \nobject model or the OMT object model . It can also refer to a speciﬁ c object-ori-\nented programming interface (i.e., a collection of classes, methods, and inter-\nrelationships designed to solve a particular problem). One example of this \nlatt er usage is the Microsoft  Excel object model , which allows external programs \nto control Excel in various ways. (See htt p://en.wikipedia.org/wiki/Object_\nmodel for further discussion of the term object model.)\nIn this book, we will use the term game object model to describe the facili-\nties provided by a game engine in order to permit the dynamic entities in the \nvirtual game world to be modeled and simulated. In this sense, the term game \nobject model has aspects of both of the deﬁ nitions given above:\nz A game’s object model is a speciﬁ c object-oriented programming inter-\nface intended to solve the particular problem of simulating the speciﬁ c \nset of entities that make up a particular game.\nz Additionally, a game’s object model oft en extends the programming \nlanguage in which the engine was writt en. If the game is implemented \n",
      "page_number": 696,
      "chapter_number": 35,
      "summary": "This chapter covers segment 35 (pages 696-718). Key topics include games, gaming, and objects. If a \ngame object doesn’t update its game-driven rigid bodies in time, the physics \nthread might end up using out-of-date locations in the simulation.",
      "keywords": [
        "game",
        "game world",
        "game object",
        "Rigid Body Dynamics",
        "Physics",
        "world",
        "object",
        "character",
        "Collision",
        "game object model",
        "term game object",
        "dynamic elements",
        "Rigid Body",
        "dynamic game world",
        "physics engine"
      ],
      "concepts": [
        "games",
        "gaming",
        "objects",
        "objectives",
        "collision",
        "collisions",
        "physics",
        "character",
        "dynamics",
        "thread"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 3",
          "chapter": 51,
          "title": "Segment 51 (pages 488-496)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 18,
          "title": "Segment 18 (pages 172-180)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 22,
          "title": "Segment 22 (pages 205-213)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 33,
          "title": "Segment 33 (pages 314-325)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 21,
          "title": "Segment 21 (pages 197-204)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 719-740)",
      "start_page": 719,
      "end_page": 740,
      "detection_method": "synthetic",
      "content": "697 \nin a non-object-oriented language like C, object-oriented facilities can \nbe added by the programmers. And even if the game is writt en in an \nobject-oriented language like C++, advanced features like reﬂ ection, per-\nsistence, and network replication are oft en added. A game object model \nsometimes melds the features of multiple languages. For example, a \ngame engine might combine a compiled programming language such as \nC or C++ with a scripting language like Python, Lua, or Pawn and pro-\nvide a uniﬁ ed object model that can be accessed from either language.\n13.2.2. Tool-Side Design versus Runtime Design\nThe object model presented to the designers via the world editor (discussed \nbelow) needn’t be the same object model used to implement the game at run-\ntime.\nz The tool-side game object model might be implemented at runtime us-\ning a language with no native object-oriented features at all, like C.\nz A single GO type on the tool side might be implemented as a collection \nof classes at runtime (rather than as a single class as one might at ﬁ rst \nexpect).\nz Each tool-side GO might be nothing more than a unique id at runtime, \nwith all of its state data stored in tables or collections of loosely coupled \nobjects.\nTherefore, a game really has two distinct but closely interrelated object models:\nz The tool-side object model is deﬁ ned by the set of game object types seen by \nthe designers within the world editor .\nz The runtime object model is deﬁ ned by whatever set of language con-\nstructs and soft ware systems the programmers have used to implement \nthe tool-side object model at runtime. The runtime object model might \nbe identical to the tool-side model or map directly to it, or it might be \nentirely diﬀ erent than the tool-side model under the hood.\nIn some game engines, the line between the tool-side and runtime designs \nis blurred or non-existent. In others, it is very well delineated. In some engines, \nthe implementation is actually shared between the tools and the runtime. In \nothers, the runtime implementation looks almost totally alien relative to the \ntool-side view of things. Some aspects of the implementation almost always \ncreep up into the tool-side design, and game designers must be cognizant of \nthe performance and memory consumption impacts of the game worlds they \nconstruct and the gameplay rules and object behaviors they design. That said, \n13.2. Implementing Dynamic Elements: Game Objects\n\n\n698 \n13. Introduction to Gameplay Systems\nvirtually all game engines have some form of tool-side object model and a cor-\nresponding runtime implementation of that object model.\n13.3. Data-Driven Game Engines\nIn the early days of game development, games were largely hard-coded by \nprogrammers. Tools, if any, were primitive. This worked because the amount \nof content in a typical game was miniscule, and the bar wasn’t particularly \nhigh, thanks in part to the primitive graphics and sound of which early game \nhardware was capable.\nToday, games are orders of magnitude more complex, and the quality bar \nis so high that game content is oft en compared to the computer-generated ef-\nfects in Hollywood blockbusters. Game teams have grown much larger, but \nthe amount of game content is growing faster than team size. In the most \nrecent generation, deﬁ ned by the Wii, the Xbox 360, and the PLAYSTATION 3, \ngame teams routinely speak of the need to produce ten times the content, with \nteams that are at most 25% larger than in the previous generation. This trend \nmeans that a game team must be capable of producing very large amounts of \ncontent in an extremely eﬃ  cient manner.\nEngineering resources are oft en a production bott leneck because high-\nquality engineering talent is limited and expensive and because engineers \ntend to produce content much more slowly than artists and game designers \n(due to the complexities inherent in computer programming). Most teams \nnow believe that it’s a good idea to put at least some of the power to cre-\nate content directly into the hands of the folks responsible for producing that \ncontent—namely the designers and the artists. When the behavior of a game \ncan be controlled, in whole or in part, by data provided by artists and design-\ners rather than exclusively by soft ware produced by programmers, we say the \nengine is data-driven .\nData-driven architectures can improve team eﬃ  ciency by fully leveraging \nall staﬀ  members to their fullest potential and by taking some of the heat oﬀ  \nthe engineering team. It can also lead to improved iteration times. Whether a \ndeveloper wants to make a slight tweak to the game’s content or completely \nrevise an entire level, a data-driven design allows the developer to see the \neﬀ ects of the changes quickly, ideally with litt le or no help from an engineer. \nThis saves valuable time and can permit the team to polish their game to a \nvery high level of quality.\nThat being said, it’s important to realize that data-driven features oft en \ncome at a heavy cost. Tools must be provided to allow game designers and art-\nists to deﬁ ne game content in a data-driven manner. The runtime code must \n\n\n699 \nbe changed to handle the wide range of possible inputs in a robust way. Tools \nmust also be provided in-game to allow artists and designers to preview their \nwork and troubleshoot problems. All of this soft ware requires signiﬁ cant time \nand eﬀ ort to write, test, and maintain.\nSadly, many teams make a mad rush into data-driven architectures with-\nout stopping to study the impacts of their eﬀ orts on their particular game de-\nsign and the speciﬁ c needs of their team members. In their haste, such teams \noft en dramatically overshoot the mark, producing overly complex tools and \nengine systems that are diﬃ  cult to use, bug-ridden, and virtually impossible \nto adapt to the changing requirements of the project. Ironically, in their eﬀ orts \nto realize the beneﬁ ts of a data-driven design, a team can easily end up with \nsigniﬁ cantly lower productivity than the old-fashioned hard-coded methods.\nEvery game engine should have some data-driven components, but a \ngame team must exercise extreme care when selecting which aspects of the \nengine to data-drive. It’s crucial to weigh the costs of creating a data-driven \nor rapid iteration feature against the amount of time the feature is expected to \nsave the team over the course of the project. It’s also incredibly important to \nkeep the KISS mantra (“keep it simple, stupid”) in mind when designing and \nimplementing data-driven tools and engine systems. To paraphrase Albert \nEinstein, everything in a game engine should be made as simple as possible, \nbut no simpler.\n13.4. The Game World Editor\nWe’ve already discussed data-driven asset-creation tools, such as Maya, Pho-\ntoshop, Havok content tools, and so on. These tools generate individual assets \nfor consumption by the rendering engine, animation system, audio system, \nphysics system, and so on. The analog to these tools in the gameplay space \nis the game world editor —a tool (or a suite of tools) that permits game world \nchunks to be deﬁ ned and populated with static and dynamic elements.\nAll commercial game engines have some kind of world editor tool. A well-\nknown tool called Radiant is used to create maps for the Quake and Doom fam-\nily of engines. A screen shot of Radiant is shown in Figure 13.4. Valve’s Source \nengine, the engine that drives Half-Life 2, The Orange Box  and Team Fortress 2, \nprovides an editor called Hammer (previously distributed under the names \nWorldcraft  and The Forge). Figure 13.5 shows a screen shot of Hammer.\nThe game world editor generally permits the initial states of game objects \n(i.e., the values of their att ributes) to be speciﬁ ed. Most game world editors \nalso give their users some sort of ability to control the behaviors of the dynamic \nobjects in the game world. This control might be via data-driven conﬁ guration \n13.4. The Game World Editor\n\n\n700 \n13. Introduction to Gameplay Systems\nFigure 13.5.  Valve’s Hammer editor for the Source engine.\nFigure 13.4.  The Radiant world editor for the Quake and Doom family of engines.\n\n\n701 \nparameters (e.g., object A should start in an invisible state, object B should \nimmediately att ack the player when spawned, object C is ﬂ ammable, etc.), \nor behavioral control might be via a scripting language, thereby shift ing the \ngame designers’ tasks into the realm of programming. Some world editors \neven allow entirely new types of game objects to be deﬁ ned, with litt le or no \nprogrammer intervention.\n13.4.1. Typical Features of a Game World Editor\nThe design and layout of game world editors varies widely, but most editors \nprovide a reasonably standard set of features. These include, but are certainly \nnot limited to, the following.\n13.4.1.1. World Chunk Creation and Management\nThe unit of world creation is usually a chunk (also known as a level or map—\nsee Section 13.1.2). The game world editor typically allows new chunks to \nbe created and existing chunks to be renamed, broken up, combined, or de-\nstroyed. Each chunk can be linked to one or more static meshes and/or other \nstatic data elements such as AI navigation maps, descriptions of ledges that \ncan be grabbed by the player, cover point deﬁ nitions, and so on. In some en-\ngines, a chunk is deﬁ ned by a single background mesh and cannot exist with-\nout one. In other engines, a chunk may have an independent existence, per-\nhaps deﬁ ned by a bounding volume (e.g., AABB, OBB, or arbitrary polygonal \nregion), and can be populated by zero or more meshes and/or brush geometry \n(see Section 1.7.3.1).\nSome world editors provide dedicated tools for authoring terrain, water , \nand other specialized static elements. In other engines, these elements might \nbe authored using standard DCC applications but tagged in some way to indi-\ncate to the asset conditioning pipeline and/or the runtime engine that they are \nspecial. (For example, in Uncharted: Drake’s Fortune, the water was authored \nas a regular triangle mesh, but it was mapped with a special material that in-\ndicated that it was to be treated as water.) Sometimes, special world elements \nare created and edited in a separate, standalone tool. For example, the height \nﬁ eld terrain in Medal of Honor: Paciﬁ c Assault was authored using a customized \nversion of a tool obtained from another team within Electronic Arts because \nthis was more expedient than trying to integrate a terrain editor into Radiant, \nthe world editor being used on the project at the time.\n13.4.1.2. Game World Visualization\nIt’s important for the user of a game world editor to be able to visualize the \ncontents of the game world . As such, virtually all game world editors provide \n13.4. The Game World Editor\n\n\n702 \n13. Introduction to Gameplay Systems\na three-dimensional perspective view of the world and/or a two-dimensional \northographic projection. It’s common to see the view pane divided into four \nsections, three for top, side, and front orthographic elevations and one for the \n3D perspective view.\nSome editors provide these world views via a custom rendering engine \nintegrated directly into the tool. Other editors are themselves integrated into \na 3D geometry editor like Maya or 3ds Max, so they can simply leverage the \ntool’s viewports. Still other editors are designed to communicate with the ac-\ntual game engine and use it to render the 3D perspective view. Some editors \nare even integrated into the engine itself.\n13.4.1.3. Navigation\nClearly, a world editor wouldn’t be of much use if the user weren’t able to \nmove around within the game world. In an orthographic view, it’s important \nto be able to scroll and zoom in and out. In a 3D view, various camera control \nschemes are used. It may be possible to focus on an individual object and \nrotate around it. It may also be possible to switch into a “ﬂ y through” mode \nwhere the camera rotates about its own focal point and can be moved forward, \nbackward, up, and down and panned left  and right.\nSome editors provide a host of convenience features for navigation. These \ninclude the ability to select an object and focus in on it with a single key press, \nthe ability to save various relevant camera locations and then jump between \nthem, various camera movement speed modes for coarse navigation and ﬁ ne \ncamera control, a Web-browser-like navigation history that can be used to \njump around the game world, and so on.\n13.4.1.4. Selection\nA game world editor is primarily designed to allow the user to populate a \ngame world with static and dynamic elements. As such, it’s important for the \nuser to be able to select individual elements for editing. Some editors only \nallow a single object to be selected at a time, while more-advanced editors \npermit multiobject selections. Objects might be selected via a rubber-band box \nin the orthographic view or by ray-cast style picking in the 3D view. Many \neditors also display a list of all world elements in a scrolling list or tree view so \nthat objects can be found and selected by name. Some world editors also allow \nselections to be named and saved for later retrieval.\nGame worlds are oft en quite densely populated. As such, it can some-\ntimes be diﬃ  cult to select a desired object because other objects are in the way. \nThis problem can be overcome in a number of ways. When using a ray cast \nto select objects in 3D, the editor might allow the user to cycle through all of \n\n\n703 \nthe objects that the ray is currently intersecting rather than always selecting \nthe nearest one. Many editors allow the currently selected object(s) to be tem-\nporarily hidden from view. That way, if you don’t get the object you want the \nﬁ rst time, you can always hide it and try again. As we’ll see in the next section, \nlayers can also be an eﬀ ective way to reduce clutt er and improve the user’s \nability to select objects successfully.\n13.4.1.5. Layers\nSome editors also allow objects to be grouped into predeﬁ ned or user-deﬁ ned \nlayers. This can be an incredibly useful feature, allowing the contents of the \ngame world to be organized sensibly. Entire layers can be hidden or shown to \nreduce clutt er on-screen. Layers might be color-coded for easy identiﬁ cation. \nLayers can be an important part of a division-of-labor strategy, as well. For \nexample, when the lighting team is working on a world chunk, they can hide \nall of the elements in the scene that are not relevant to lighting.\nWhat’s more, if the game world editor is capable of loading and saving \nlayers individually, conﬂ icts can be avoided when multiple people are work-\ning on a single world chunk at the same time. For example, all of the lights \nmight be stored in one layer, all of the background geometry in another, and \nall AI characters in a third. Since each layer is totally independent, the light-\ning, background, and NPC teams can all work simultaneously on the same \nworld chunk.\n13.4.1.6. Property Grid\nThe static and dynamic elements that populate a game world chunk typically \nhave various properties (also known as att ributes) that can be edited by the \nuser. Properties might be simple key-value pairs and be limited to simple \natomic data types like Booleans, integers, ﬂ oating-point numbers, and strings. \nIn some editors, more-complex properties are supported, including arrays of \ndata and nested compound data structures.\nMost world editors display the att ributes of the currently selected object(s) \nin a scrollable property grid view. An example of a property grid is shown in \nFigure 13.6. The grid allows the user to see the current values of each att ribute \nand edit the values by typing, using check boxes or drop-down combo boxes, \ndragging spinner controls up and down, and so on.\nEditing Multiobject Selections\nIn editors that support multiobject selection , the property grid may support \nmultiobject editing as well. This advanced feature displays an amalgam of the \natt ributes of all objects in the selection. If a particular att ribute has the same \nvalue across all objects in the selection, the value is shown as-is, and editing \n13.4. The Game World Editor\n\n\n704 \n13. Introduction to Gameplay Systems\nthe value in the grid causes the property value to be updated in all selected \nobjects. If the att ribute’s value diﬀ ers from object to object within the selection, \nthe property grid typically shows no value at all. In this case, if a new value is \ntyped into the ﬁ eld in the grid, it will overwrite the values in all selected ob-\njects, bringing them all into agreement. Another problem arises when the se-\nlection contains a heterogeneous collection of objects (i.e., objects whose types \ndiﬀ er). Each type of object can potentially have a diﬀ erent set of att ributes, so \nthe property grid must only display those att ributes that are common to all \nobject types in the selection. This can still be useful, however, because game \nobject types oft en inherit from a common base type. For example, most objects \nhave a position and orientation. In a heterogeneous selection, the user can still \nedit these shared att ributes even though more-speciﬁ c att ributes are tempo-\nrarily hidden from view.\nFree-Form Properties\nNormally, the set of properties associated with an object, and the data types of \nthose properties, are deﬁ ned on a per-object-type basis. For example, a render-\nFigure 13.6.  A typical property grid.\n\n\n705 \nable object has a position, orientation, scale, and mesh, while a light has posi-\ntion, orientation, color, intensity, and light type. Some editors also allow addi-\ntional “free-form” properties to be deﬁ ned by the user on a per-instance basis. \nThese properties are usually implemented as a ﬂ at list of key-value pairs . The \nuser is free to choose the name (key) of each free-form property, along with \nits data type and its value. This can be incredibly useful for prototyping new \ngameplay features or implementing one-oﬀ  scenarios.\n13.4.1.7. Object Placement and Alignment Aids\n Some object properties are treated in a special way by the world editor. Typi-\ncally the position, orientation, and scale of an object can be controlled via spe-\ncial handles in the orthographic and perspective viewports, just like in Maya \nor Max. In addition, asset linkages oft en need to be handled in a special way. \nFor example, if we change the mesh associated with an object in the world, the \neditor should display this mesh in the orthographic and 3D perspective view-\nports. As such, the game world editor must have special knowledge of these \nproperties—it cannot treat them generically, as it can most object properties.\nMany world editors provide a host of object placement and alignment \naids in addition to the basic translation, rotation, and scale tools. Many of \nthese features borrow heavily from the feature sets of commercial graphics \nand 3D modeling tools like Photoshop, Maya, Visio, and others. Examples \ninclude snap to grid, snap to terrain, align to object, and many more.\n13.4.1.8. Special Object Types\nJust as some object properties must be handled in a special way by the world ed-\nitor, certain types of objects also require special handling. Examples include:\nz Lights. The world editor usually uses special icons to represent lights, \nsince they have no mesh. The editor may att empt to display the light’s \napproximate eﬀ ect on the geometry in the scene as well, so that design-\ners can move lights around in real time and get a reasonably good feel \nfor how the scene will ultimately look.\nz Particle emitt ers. Visualization of particle eﬀ ects can also be problematic \nin editors that are built on a standalone rendering engine. In this case, \nparticle emitt ers might be displayed using icons only, or some att empt \nmight be made to emulate the particle eﬀ ect in the editor. Of course, this \nis not a problem if the editor is in-game or can communicate with the \nrunning game for live tweaking.\nz Regions . A region is a volume of space that is used by the game to de-\ntect relevant events such as objects entering or leaving the volume or to \n13.4. The Game World Editor\n\n\n706 \n13. Introduction to Gameplay Systems\ndemark areas for various purposes. Some game engines restrict regions \nto being modeled as spheres or oriented boxes, while others may per-\nmit arbitrary convex polygonal shapes when viewed from above, with \nstrictly horizontal sides. Still others might allow regions to be construct-\ned out of more-complex geometry, such as k-DOPs (see Section 12.3.4.5). \nIf regions are always spherical then the designers might be able to make \ndo with a “Radius” property in the property grid, but to deﬁ ne or mod-\nify the extents of an arbitrarily shaped region, a special-case editing tool \nis almost certainly required.\nz Splines. A spline is a three-dimensional curve deﬁ ned by a set of control \npoints and possibly tangent vectors at the points, depending on the type \nof mathematical curve used. Catmull-Rom splines are commonly used \nbecause they are fully deﬁ ned by a set of control points (without tan-\ngents) and the curve always passes through all of the control points. But \nno matt er what type of splines are supported, the world editor typically \nneeds to provide the ability to display the splines in its viewports, and \nthe user must be able to select and manipulate individual control points. \nSome world editors actually support two selection modes—a “coarse” \nmode for selecting objects in the scene and a “ﬁ ne” mode for select-\ning the individual components of a selected object, such as the control \npoints of a spline or the vertices of a region.\n13.4.1.9. Saving and Loading World Chunks\nOf course, no world editor would be complete if it were unable to load and \nsave world chunks . The granularity with which world chunks can be load-\ned and saved diﬀ ers widely from engine to engine. Some engines store each \nworld chunk in a single ﬁ le, while others allow individual layers to be loaded \nand saved independently. Data formats also vary across engines. Some use \ncustom binary formats, others text formats like XML. Each design has its pros \nand cons, but every editor provides the ability to load and save world chunks \nin some form—and every game engine is capable of loading world chunks so \nthat they can be played at runtime.\n13.4.1.10. Rapid Iteration\nA good game world editor usually supports some degree of dynamic tweak-\ning for rapid iteration. Some editors run within the game itself, allowing the \nuser to see the eﬀ ects of his or her changes immediately. Others provide a \nlive connection from the editor to the running game. Still other world editors \noperate entirely oﬀ -line, either as a standalone tool or as a plug-in to a DCC \napplication like Lightwave or Maya. These tools sometimes permit modiﬁ ed \n\n\n707 \ndata to be reloaded dynamically into the running game. The speciﬁ c mecha-\nnism isn’t important—all that matt ers is that users have a reasonably short \nround-trip iteration time (i.e., the time between making a change to the game \nworld and seeing the eﬀ ects of that change in-game). It’s important to realize \nthat iterations don’t have to be instantaneous. Iteration times should be com-\nmensurate with the scope and frequency of the changes being made. For ex-\nample, we might expect tweaking a character’s maximum health to be a very \nfast operation, but when making major changes to the lighting environment \nfor an entire world chunk, a much longer iteration time might be acceptable.\n13.4.2. Integrated Asset Management Tools\nIn some engines, the game world editor is integrated with other aspects of \ngame asset database management, such as deﬁ ning mesh and material prop-\nerties, deﬁ ning animations, blend trees, animation state machines, sett ing up \ncollision and physical properties of objects, managing texture resources, and \nso on. (See Section 6.2.1.2 for a discussion of the game asset database.)\nPerhaps the best-known example of this design in action is UnrealEd , the \neditor used to create content for games built on the Unreal Engine. UnrealEd \nis integrated directly into the game engine, so any changes made in the editor \nare made directly to the dynamic elements in the running game. This makes \nrapid iteration very easy to achieve. But UnrealEd is much more than a game \nworld editor—it is actually a complete content-creation package. It manages \nFigure 13.7.  UnrealEd’s Generic Browser provides access to the entire game asset database.\n13.4. The Game World Editor\n\n\n708 \n13. Introduction to Gameplay Systems\nthe entire database of game assets, from animations to audio clips to triangle \nmeshes to textures to materials and shaders and much more. UnrealEd pro-\nvides its user with a uniﬁ ed, real-time, WYSIWYG view into the entire asset \ndatabase, making it a powerful enabler of any rapid, eﬃ  cient game develop-\nment process. A few screen shots from UnrealEd are shown in Figure 13.7 and \nFigure 13.8.\n13.4.2.1. Data Processing Costs\nIn Section 6.2.1, we learned that the asset conditioning pipeline (ACP) con-\nverts game assets from their various source formats into the formats required \nby the game engine. This is typically a two-step process. First, the asset is \nexported from the DCC application to a platform-independent intermediate \nformat that only contains the data that is relevant to the game. Second, the \nasset is processed into a format that is optimized for a speciﬁ c platform. On a \nproject targeting multiple gaming platforms, a single platform-independent \nasset gives rise to multiple platform-speciﬁ c assets during this second phase.\nOne of the key diﬀ erences between tools pipelines is the point at which \nthis second platform-speciﬁ c optimization step is performed. UnrealEd per-\nFigure 13.8.  UnrealEd also provides a world editor.\n\n\n709 \nforms it when assets are ﬁ rst imported into the editor. This approach pays oﬀ  \nin rapid iteration time when iterating on level design. However, it can make \nthe cost of changing source assets like meshes, animations, audio assets, and \nso on more painful. Other engines like the Source engine and the Quake engine \npay the asset optimization cost when baking out the level prior to running the \ngame. Halo gives the user the option to change raw assets at any time; they are \nconverted into optimized form when they are ﬁ rst loaded into the engine, and \nthe results are cached to prevent the optimization step from being performed \nneedlessly every time the game is run.\n13.4. The Game World Editor\n\n\n711\n14\nRuntime Gameplay\nFoundation Systems\n14.1. Components of the Gameplay \n \nFoundation System\nM\nost game engines provide a suite of runtime soft ware components that \ntogether provide a framework upon which a game’s unique rules, objec-\ntives, and dynamic world elements can be constructed. There is no standard \nname for these components within the game industry, but we will refer to them \ncollectively as the engine’s gameplay foundation system . If a line can reasonably \nbe drawn between the game engine and the game itself, then these systems \nlie just beneath this line. In theory, one can construct gameplay foundation \nsystems that are for the most part game-agnostic. However, in practice, these \nsystems almost always contain genre- or game-speciﬁ c details. In fact, the line \nbetween the engine and the game can probably be best visualized as one big \nblur—a gradient that arcs across these components as it links the engine to \nthe game. In some game engines, one might even go so far as to consider the \ngameplay foundation systems as lying entirely above the engine-game line. \nThe diﬀ erences between game engines are most acute when it comes to the \ndesign and implementation of their gameplay components. That said, there \nare a surprising number of common patt erns across engines, and those com-\nmonalities will be the topic of our discussions here.\n\n\n712 \n14. Runtime Gameplay Foundation Systems\nEvery game engine approaches the problem of gameplay soft ware design \na bit diﬀ erently. However, most engines provide the following major subsys-\ntems in some form:\nz Runtime game object model . This is an implementation of the abstract game \nobject model advertised to the game designers via the world editor .\nz Level management and streaming . This system loads and unloads the \ncontents of the virtual worlds in which gameplay takes place. In many \nengines, level data is streamed into memory during gameplay, thus \nproviding the illusion of a large seamless world (when in fact it is broken \ninto discrete chunks).\nz Real-time object model updating . In order to permit the game objects \nin the world to behave autonomously, each object must be updated \nperiodically. This is where all of the disparate systems in a game engine \ntruly come together into a cohesive whole.\nz Messaging and event handling. Most game objects need to communicate \nwith one another. This is usually done via an abstract messaging system. \nInter-object messages oft en signal changes in the state of the game world \ncalled events. So the messaging system is referred to as the event system \nin many studios.\nz Scripting . Programming high-level game logic in a language like C or \nC++ can be cumbersome. To improve productivity, allow rapid iteration, \nand put more power into the hands of the non-programmers on the \nteam, a scripting language is oft en integrated into the game engine. \nThis language might be text-based, like Python or Lua, or it might be a \ngraphical language, like Unreal’s Kismet.\nz Objectives and game ﬂ ow management. This subsystem manages the play-\ner’s objectives and the overall ﬂ ow of the game. This is usually described \nby a sequence, tree, or graph of player objectives. Objectives are oft en \ngrouped into chapters, especially if the game is highly story-driven as \nmany modern games are. The game ﬂ ow management system manages \nthe overall ﬂ ow of the game, tracks the player’s accomplishment of ob-\njectives, and gates the player from one area of the game world to the \nnext as the objectives are accomplished. Some designers refer to this as \nthe “spine” of the game.\nOf these major systems, the runtime object model is probably the most \ncomplex. It typically provides most, if not all, of the following features:\nz Spawning and destroying game objects dynamically. The dynamic elements \nin a game world oft en need to come and go during gameplay. Health \n\n\n713 \n14.1. Components of the Gameplay Foundation System\npacks disappear once they have been picked up, explosions appear \nand then dissipate, and enemy reinforcements mysteriously come from \naround a corner just when you think you’ve cleared the level. Many \ngame engines provide a system for managing the memory and other re-\nsources associated with dynamically spawned game objects. Other en-\ngines simply disallow dynamic creation or destruction of game objects \naltogether.\nz Linkage to low-level engine systems . Every game object has some kind of \nlinkage to one or more underlying engine systems. Most game objects are \nvisually represented by renderable triangle meshes. Some have particle \neﬀ ects. Many generate sounds. Some animate. Many have collision, \nand some are dynamically simulated by the physics engine. One of the \nprimary responsibilities of the gameplay foundation system is to ensure \nthat every game object has access to the services of the engine systems \nupon which it depends.\nz Real-time simulation of object behaviors . At its core, a game engine is a real-\ntime dynamic computer simulation of an agent-based model. This is just \na fancy way of saying that the game engine needs to update the states \nof all the game objects dynamically over time. The objects may need to \nbe updated in a very particular order, dictated in part by dependencies \nbetween the objects, in part by their dependencies on various engine \nsubsystems, and in part because of the interdependencies between those \nengine subsystems themselves.\nz Ability to deﬁ ne new game object types . Every game’s requirements change \nand evolve as the game is developed. It’s important that the game object \nmodel be ﬂ exible enough to permit new object types to be added easily \nand exposed to the world editor. In an ideal world, it should be possible \nto deﬁ ne a new type of object in an entirely data-driven manner. \nHowever, in many engines, the services of a programmer are required \nin order to add new game object types.\nz Unique object ids . Typical game worlds contain hundreds or even \nthousands of individual game objects of various types. At runtime, it’s \nimportant to be able to identify or search for a particular object. This \nmeans each object needs some kind of unique identiﬁ er. A human-\nreadable name is the most convenient kind of id, but we must be wary \nof the performance costs of using strings at runtime. Integer ids are \nthe most eﬃ  cient choice, but they are very diﬃ  cult for human game \ndevelopers to work with. Arguably the best solution is to use hashed \nstring ids (see Section 5.4.3.1) as our object identiﬁ ers, as they are as \n\n\n714 \n14. Runtime Gameplay Foundation Systems\neﬃ  cient as integers but can be converted back into string form for ease \nof reading.\nz Game object queries . The gameplay foundation system must provide some \nmeans of ﬁ nding objects within the game world. We might want to ﬁ nd \na speciﬁ c object by its unique id, or all the objects of a particular type, or \nwe might want to perform advanced queries based on arbitrary criteria \n(e.g., ﬁ nd all enemies within a 20 meter radius of the player character).\nz Game object references . Once we’ve found the objects, we need some \nmechanism for holding references to them, either brieﬂ y within a single \nfunction or for much longer periods of time. An object reference might \nbe as simple as a pointer to a C++ class instance, or it might be something \nmore sophisticated, like a handle or a reference-counted smart pointer .\nz Finite state machine support. Many types of game objects are best modeled \nas ﬁ nite state machines. Some game engines provide the ability for a \ngame object to exist in one of many possible states, each with its own \natt ributes and behavioral characteristics.\nz Network replication . In a networked multiplayer game, multiple game \nmachines are connected together via a LAN or the Internet. The state of \na particular game object is usually owned and managed by one machine. \nHowever, that object’s state must also be replicated (communicated) to \nthe other machines involved in the multiplayer game so that all players \nhave a consistent view of the object.\nz Saving and loading games / object persistence. Many game engines allow \nthe current states of the game objects in the world to be saved to disk \nand later reloaded. This might be done to support a “save anywhere ” \nsave-game system or as a way of implementing network replication, or \nit might simply be the primary means of loading game world chunks \nthat were authored in the world editor tool. Object persistence usually \nrequires certain language features, such as runtime type identiﬁ cation \n(RTTI), reﬂ ection , and abstract construction . RTTI and reﬂ ection provide \nsoft ware with a means of determining an object’s type, and what att ri-\nbutes and methods its class provides, dynamically at runtime. Abstract \nconstruction allows instances of a class to be created without having \nto hard-code the name of the class—a very useful feature when serial-\nizing an object instance into memory from disk. If RTTI, reﬂ ection, and \nabstract construction are not natively supported in your language of \nchoice, these features can be added manually.\nWe’ll spend the remainder of this chapter delving into each of these subsys-\ntems in depth.\n\n\n715 \n14.2. Runtime Object Model Architectures\n14.2. Runtime Object Model Architectures\nIn the world editor , the game designer is presented with an abstract game \nobject model, which deﬁ nes the various types of dynamic elements that can \nexist in the game, how they behave, and what kinds of att ributes they have. \nAt runtime, the gameplay foundation system must provide a concrete imple-\nmentation of this object model. This is by far the largest component of any \ngameplay foundation system.\nThe runtime object model implementation may or may not bear any re-\nsemblance to the abstract tool-side object model. For example, it might not be \nimplemented in an object-oriented programming language at all, or it might \nuse a collection of interconnected class instances to represent a single abstract \ngame object. Whatever its design, the runtime object model must provide a \nfaithful reproduction of the object types, att ributes, and behaviors advertised \nby the world editor.\nThe runtime object model is the in-game manifestation of the abstract \ntool-side object model presented to the designers in the world editor. Designs \nvary widely, but most game engines follow one of two basic architectural \nstyles:\nz Object-centric. In this style, each tool-side game object is represented at \nruntime by a single class instance or a small collection of interconnected \ninstances. Each object has a set of att ributes and behaviors that are encap-\nsulated within the class (or classes) of which the object is an instance. \nThe game world is just a collection of game objects.\nz Property-centric. In this style, each tool-side game object is represented \nonly by a unique id (implemented as an integer, hashed string id, or \nstring). The properties of each game object are distributed across many \ndata tables, one per property type, and keyed by object id (rather than \nbeing centralized within a single class instance or collection of inter-\nconnected instances). The properties themselves are oft en implemented \nas instances of hard-coded classes. The behavior of a game object is im-\nplicitly deﬁ ned by the collection of properties from which it is com-\nposed. For example, if an object has the “Health” property, then it can be \ndamaged, lose health, and eventually die. If an object has the “MeshIn-\nstance” property, then it can be rendered in 3D as an instance of a tri-\nangle mesh.\nThere are distinct advantages and disadvantages to each of these architec-\ntural styles. We’ll investigate each one in some detail and note where one style \nhas signiﬁ cant potential beneﬁ ts over the other as they arise.\n\n\n716 \n14. Runtime Gameplay Foundation Systems\n14.2.1. Object-Centric Architectures\nIn an object-centric game world object architecture, each logical game object \nis implemented as an instance of a class, or possibly a collection of intercon-\nnected class instances. Under this broad umbrella, many diﬀ erent designs are \npossible. We’ll investigate a few of the most common designs in the following \nsections.\n14.2.1.1. A Simple Object-Based Model in C: Hydro Thunder\nGame object models needn’t be implemented in an object-oriented language \nlike C++ at all. For example, the arcade hit Hydro Thunder , by Midway Home \nEntertainment in San Diego, was writt en entirely in C. Hydro employed a very \nsimple game object model consisting of only a few object types:\nz boats (player- and AI -controlled),\nz ﬂ oating blue and red boost icons,\nz ambient animated objects (animals on the side of the track, etc.),\nz the water surface,\nz ramps,\nz waterfalls,\nz particle e ﬀ ects,\nz race track sectors (two-dimensional polygonal regions connected to one \nanother and together deﬁ ning the watery region in which boats could \nrace),\nz static geometry (terrain , foliage, buildings along the sides of the track, \netc.),\nz two-dimensional heads-up display (HUD) elements.\nA few screen shots of Hydro Thunder are shown in Figure 14.1. Notice the hov-\nering boost icons in both screen shots and the shark swimming by in the left  \nimage (an example of an ambient animated object).\nHydro had a C struct named World_t that stored and managed the con-\ntents of a game world (i.e., a single race track). The world contained pointers \nto arrays of various kinds of game objects. The static geometry was a single \nmesh instance. The water surface, waterfalls, and particle eﬀ ects were each \nrepresented by custom data structures. The boats, boost icons, and other dy-\nnamic objects in the game were represented by instances of a general-purpose \nstruct called WorldOb_t (i.e., a world object). This was Hydro’s equivalent of \na game object as we’ve deﬁ ned it in this chapter.\nThe WorldOb_t data structure contained data members encoding the po-\nsition and orientation of the object, the 3D mesh used to render it, a set of colli-\n\n\n717 \nsion spheres, simple animation state information (Hydro only supported rigid \nhierarchical animation), physical properties like velocity, mass, and buoyancy, \nand other data common to all of the dynamic objects in the game. In addi-\ntion, each WorldOb_t contained three pointers: a void* “user data” pointer, \na pointer to a custom “update” function, and a pointer to a custom “draw” \nfunction. So while Hydro Thunder was not object-oriented in the strictest sense, \nthe Hydro engine did extend its non-object-oriented language (C) to support \nrudimentary implementations of two important OOP features: inheritance and \npolymorphism. The user data pointer permitt ed each type of game object to \nmaintain custom state information speciﬁ c to its type while inheriting the fea-\ntures common to all world objects. For example, the Banshee boat had a dif-\nferent booster mechanism than the Rad Hazard, and each booster mechanism \nrequired diﬀ erent state information to manage its deployment and stowing \nanimations. The two function pointers acted like  virtual functions, allowing \nworld objects to have polymorphic behaviors (via their “update” functions) \nand polymorphic visual appearances (via their “draw” functions).\nstruct WorldOb_s\n{\n \nOrient_t m_transform;  \n/* position/rotation */\n \nMesh3d* \nm_pMesh;  \n \n/* 3D mesh */\n \n/* ... */\n \nvoid*  \nm_pUserData;  \n/* custom state */\n \nvoid     (*m_pUpdate)(); /* polymorphic update */\n \nvoid \n   (*m_pDraw)();  \n/* polymorphic draw */\nFigure 14.1.  Screen shots from the arcade smash Hydro Thunder, developed by Midway Home \nEntertainment in San Diego.\n14.2. Runtime Object Model Architectures\n\n\n718 \n14. Runtime Gameplay Foundation Systems\n};\ntypedef struct WorldOb_s WorldOb_t;\n14.2.1.2. Monolithic Class Hierarchies\nIt’s natural to want to classify game object types taxonomically. This tends to \nlead game programmers toward an object-oriented language that supports \ninheritance. A class hierarchy is the most intuitive and straightforward way to \nrepresent a collection of interrelated game object types. So it is not surprising \nthat the majority of commercial game engines employ a class hierarchy based \ntechnique.\nFigure 14.2 shows a simple class hierarchy that could be used to imple-\nment the game Pac-Man. This hierarchy is rooted (as many are) at a common \nclass called GameObject, which might provide some facilities needed by all \nobject types, such as RTTI or serialization. The MovableObject class repre-\nsents any object that has a position and orientation. RenderableObject gives \nthe object an ability to be rendered (in the case of traditional Pac-Man, via a \nsprite, or in the case of a modern 3D Pac-Man game, perhaps via a triangle \nmesh). From RenderableObject are derived classes for the ghosts, Pac-Man, \npellets, and power pills that make up the game. This is just a hypothetical \nexample, but it illustrates the basic ideas that underlie most game object class \nhierarchies—namely that common, generic functionality tends to exist at the \nroot of the hierarchy, while classes toward the leaves of the hierarchy tend to \nadd increasingly speciﬁ c functionality.\nFigure 14.2.  A hypothetical class hierarchy for the game Pac-Man.\nGameObject\nMovableObject\nRenderableObject\nPacMan\nGhost\nPowerPellet\nPellet\n...\n...\n...\n",
      "page_number": 719,
      "chapter_number": 36,
      "summary": "This chapter covers segment 36 (pages 719-740). Key topics include game, gaming, and object. And even if the game is writt en in an \nobject-oriented language like C++, advanced features like reﬂ ection, per-\nsistence, and network replication are oft en added.",
      "keywords": [
        "Game World Editor",
        "Game World",
        "game object",
        "game",
        "world editor",
        "game object model",
        "game object types",
        "object",
        "world",
        "object model",
        "game engine",
        "runtime object model",
        "game world object",
        "tool-side game object",
        "editor"
      ],
      "concepts": [
        "game",
        "gaming",
        "object",
        "objectives",
        "engine",
        "engineering",
        "editors",
        "worlds",
        "tool",
        "allows"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 3",
          "chapter": 11,
          "title": "Segment 11 (pages 111-118)",
          "relevance_score": 0.72,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 47,
          "title": "Segment 47 (pages 448-460)",
          "relevance_score": 0.7,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 51,
          "title": "Segment 51 (pages 488-496)",
          "relevance_score": 0.69,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 26,
          "title": "Segment 26 (pages 243-252)",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 741-763)",
      "start_page": 741,
      "end_page": 763,
      "detection_method": "synthetic",
      "content": "719 \nA game object class hierarchy usually begins small and simple, and in \nthat form, it can be a powerful and intuitive way to describe a collection of \ngame object types. However, as class hierarchies grow, they have a tendency \nto deepen and widen simultaneously, leading to what I call a monolithic class \nhierarchy . This kind of hierarchy arises when virtually all classes in the game \nobject model inherit from a single, common base class. The Unreal Engine’s \ngame object model is a classic example, as Figure 14.3 illustrates.\n14.2.1.3. Problems with Deep, Wide Hierarchies\nMonolithic class hierarchies tend to cause problems for the game develop-\nment team for a wide range of reasons. The deeper and wider a class hierarchy \ngrows, the more extreme these problems can become. In the following sec-\ntions, we’ll explore some of the most common problems caused by wide, deep \nclass hierarchies.\nActor\nBrush\nController\nAIController\nPlayerController\nInfo\nGameInfo\nPawn\nVehicle\nUnrealPawn\nRedeemerWarhead\nScout\nLight\nInventory\nAmmunition\nPowerups\nWeapon\nHUD\nPickup\nAmmo\nArmorPickup\nWeaponPickup\n...\n...\n...\n...\n...\n...\n...\n...\n...\nFigure 14.3.  An excerpt from the game object class hierarchy from Unreal Tournament 2004.\n14.2. Runtime Object Model Architectures\n\n\n720 \n14. Runtime Gameplay Foundation Systems\nUnderstanding, Maintaining, and Modifying Classes\nThe deeper a class lies within a class hierarchy, the harder it is to understand, \nmaintain, and modify. This is because to understand a class, you really need to \nunderstand all of its parent classes as well. For example, modifying the behav-\nior of an innocuous-looking virtual function in a derived class could violate \nthe assumptions made by any one of the many base classes, leading to subtle, \ndiﬃ  cult-to-ﬁ nd bugs.\nInability to Describe Multidimensional Taxonomies\nA hierarchy inherently classiﬁ es objects according to a particular system of \ncriteria known as a taxonomy . For example, biological taxonomy (also known \nas alpha taxonomy) classiﬁ es all living things according to genetic similarities, \nusing a tree with eight levels: domain, kingdom, phylum, class, order, family, \ngenus, and species. At each level of the tree, a diﬀ erent criterion is used to di-\nvide the myriad life forms on our planet into more and more reﬁ ned groups.\nOne of the biggest problems with any hierarchy is that it can only classify \nobjects along a single “axis”—according to one particular set of criteria—at \neach level of the tree. Once the criteria have been chosen for a particular hier-\narchy, it becomes diﬃ  cult or impossible to classify along an entirely diﬀ erent \nset of “axes.” For example, biological taxonomy classiﬁ es objects according to \ngenetic traits, but it says nothing about the colors of the organisms. In order to \nclassify organisms by color, we’d need an entirely diﬀ erent tree structure.\nIn object-oriented programming, this limitation of hierarchical classiﬁ ca-\ntion oft en manifests itself in the form of wide, deep, and confusing class hier-\narchies. When one analyzes a real game’s class hierarchy, one oft en ﬁ nds that \nits structure att empts to meld a number of diﬀ erent classiﬁ cation criteria into \na single class tree. In other cases, concessions are made in the class hierarchy \nto accommodate a new type of object whose characteristics were not antici-\npated when the hierarchy was ﬁ rst designed. For example, imagine the seem-\nVehicle\nMotorcycle\nSpeedBoat\nCar\nTruck\nHovercraft\nYacht\nLandVehicle\nWaterVehicle\nFigure 14.4.  A seemingly logical class hierarchy describing various kinds of vehicles.\n\n\n721 \ningly logical class hierarchy describing diﬀ erent types of vehicles, depicted in \nFigure 14.4.\nWhat happens when the game designers announce to the programmers \nthat they now want the game to include an amphibious vehicle? Such a vehicle \ndoes not ﬁ t into the existing taxonomic system. This may cause the program-\nmers to panic or, more likely, to “hack” their class hierarchy in various ugly \nand error-prone ways.\nMultiple Inheritance: The Deadly Diamond\nOne solution to the amphibious vehicle problem is to utilize C++’s multiple \ninheritance (MI) features, as shown in Figure 14.5. At ﬁ rst glance, this seems \nlike a good solution. However, multiple inheritance in C++ poses a number \nof practical problems. For example, multiple inheritance can lead to an object \nthat contains multiple copies of its base class’s members—a condition known \nas the “deadly diamond ” or “diamond of death.” (See Section 3.1.1.3 for more \ndetails.)\nThe diﬃ  culties in building an MI class hierarchy that works and that is \nunderstandable and maintainable usually outweigh the beneﬁ ts. As a result, \nmost game studios prohibit or severely limit the use of multiple inheritance in \ntheir class hierarchies.\nVehicle\nAmphibiousVehicle\nLandVehicle\nWaterVehicle\nFigure 14.5.  A diamond-shaped class hierarchy for amphibious vehicles.\nMix-In Classes\nSome teams do permit a limited form of MI, in which a class may have any \nnumber of parent classes but only one grandparent. In other words, a class \nmay inherit from one and only one class in the main inheritance hierarchy, \nbut it may also inherit from any number of mix-in classes (stand-alone classes \nwith no base class). This permits common functionality to be factored out into \na mix-in class and then spot-patched into the main hierarchy wherever it is \nneeded. This is shown in Figure 14.6. However, as we’ll see below, it’s usually \nbett er to compose or aggregate such classes than to inherit from them.\n14.2. Runtime Object Model Architectures\n\n\n722 \n14. Runtime Gameplay Foundation Systems\nThe Bubble-Up Effect\nWhen a monolithic class hierarchy is ﬁ rst designed, the root class(es) are usu-\nally very simple, each one exposing only a minimal feature set. However, as \nmore and more functionality is added to the game, the desire to share code \nbetween two or more unrelated classes begins to cause features to “bubble up ” \nthe hierarchy.\nFor example, we might start out with a design in which only wooden \ncrates can ﬂ oat in water. However, once our game designers see those cool \nﬂ oating crates, they begin to ask for other kinds of ﬂ oating objects, like charac-\nters, bits of paper, vehicles, and so on. Because “ﬂ oating versus non-ﬂ oating” \nwas not one of the original classiﬁ cation criteria when the hierarchy was de-\nsigned, the programmers quickly discover the need to add ﬂ otation to classes \nthat are totally unrelated within the class hierarchy. Multiple inheritance is \nfrowned upon, so the programmers decide to move the ﬂ otation code up the \nhierarchy, into a base class that is common to all objects that need to ﬂ oat. The \nfact that some of the classes that derive from this common base class cannot \nﬂ oat is seen as less of a problem than duplicating the ﬂ otation code across mul-\ntiple classes. (A Boolean member variable called something like m_bCanFloat\nmight even be added to make the distinction clear.) The ultimate result is that \nﬂ otation eventually becomes a feature of the root object in the class hierarchy \n(along with prett y much every other feature in the game).\nThe Actor class in Unreal is a classic example of this “bubble-up eﬀ ect.” It \ncontains data members and code for managing rendering, animation, physics, \nworld interaction, audio eﬀ ects, network replication for multiplayer games, \nGameObject\n+GetHealth()\n+ApplyDamage()\n+IsDead()\n+OnDeath()\nMHealth\n+PickUp()\n+Drop()\n+IsBeingCarried()\nMCarryable\nNPC\nPlayer\nTank\nJeep\nPistol\nMG\nCanteen\nAmmo\nCharacter\nVehicle\nWeapon\nItem\nFigure 14.6.  A class hierarchy with mix-in classes. The MHealth mix-in class adds the notion \nof health and the ability to be killed to any class that inherits it. The MCarryable mix-in class \nallows an object that inherits it to be carried by a Character.\n\n\n723 \nobject creation and destruction, actor iteration (i.e., the ability to iterate over \nall actors meeting a certain criteria and perform some operation on them), \nand message broadcasting. Encapsulating the functionality of various engine \nsubsystems is diﬃ  cult when features are permitt ed to “bubble up” to the root-\nmost classes in a monolithic class hierarchy.\n14.2.1.4. Using Composition to Simplify the Hierarchy\nPerhaps the most prevalent cause of monolithic class hierarchies is over-use \nof the “is-a” relationship in object-oriented design. For example, in a game’s \nGUI, a programmer might decide to derive the class Window from a class \ncalled Rectangle, using the logic that GUI windows are always rectangular. \nHowever, a window is not a rectangle—it has a rectangle, which deﬁ nes its \nboundary. So a more workable solution to this particular design problem is to \nembed an instance of the Rectangle class inside the Window class, or to give \nthe Window a pointer or reference to a Rectangle.\nIn object-oriented design, the “has-a” relationship is known as composi-\ntion . In composition, a class A either contains an instance of class B directly, or \ncontains a pointer or reference to an instance of B. Strictly speaking, in order for \nthe term “composition” to be applicable, class A must own class B. This means \nthat when an instance of class A is created, it automatically creates an instance \nof class B as well; when that instance of A is destroyed, its instance of B is de-\nstroyed, too. We can also link classes to one another via a pointer or reference \nwithout having one of the classes manage the other’s lifetime . In that case, the \ntechnique is usually called aggregation .\nConverting Is-A to Has-A\nConverting “is-a” relationships into “has-a” relationships can be a useful tech-\nnique for reducing the width, depth, and complexity of a game’s class hier-\narchy. To illustrate, let’s take a look at the hypothetical monolithic hierarchy \nshown in Figure 14.7. The root GameObject class provides some basic func-\ntionality required by all game objects (e.g., RTTI, reﬂ ection, persistence via \nserialization, network replication, etc.). The MovableObject class represents \nany game object that has a transform (i.e., a position, orientation, and optional \nscale). RenderableObject adds the ability to be rendered on-screen. (Not all \ngame objects need to be rendered—for example, an invisible TriggerRegion\nclass could be derived directly from MovableObject.) The Collidable\nObject class provides collision information to its instances. The Animating\nObject class grants to its instances the ability to be animated via a skeletal \njoint hierarchy. Finally, the PhysicalObject gives its instances the ability to \nbe physically simulated (e.g., a rigid body falling under the inﬂ uence of grav-\nity and bouncing around in the game world).\n14.2. Runtime Object Model Architectures\n\n\n724 \n14. Runtime Gameplay Foundation Systems\nOne big problem with this class hierarchy is that it limits our design \nchoices when creating new types of game objects. If we want to deﬁ ne an \nobject type that is physically simulated, we are forced to derive its class from \nPhysicalObject even though it may not require skeletal animation. If we \nwant a game object class with collision, it must inherit from Collidable even \nthough it may be invisible and hence not require the services of Renderable.\nA second problem with the hierarchy shown in Figure 14.7 is that it is \ndiﬃ  cult to extend the functionality of the existing classes. For example, let’s \nimagine we want to support morph target animation, so we derive two new \nclasses from AnimatingObject called SkeletalObject and MorphTarget\nObject. If we wanted both of these new classes to have the ability to be physi-\ncally simulated, we’d be forced to re-factor PhysicalObject into two nearly-\nidentical classes, one derived from SkeletalObject and one from Morph\nTargetObject, or turn to multiple inheritance.\nOne solution to these problems is to isolate the various features of a \nGameObject into independent classes, each of which provides a single, well-\ndeﬁ ned service. Such classes are sometimes called components or service objects. \nA componentized design allows us to select only those features we need for \neach type of game object we create. In addition, it permits each feature to be \nmaintained, extended, or re-factored without aﬀ ecting the others. The indi-\nvidual components are also easier to understand, and easier to test, because \nthey are decoupled from one another. Some component classes correspond \ndirectly to a single engine subsystem, such as rendering, animation, collision, \nphysics, audio, etc. This allows these subsystems to remain distinct and well-\nGameObject\nMovableObject\nRenderableObject\nCollidableObject\nAnimatingObject\nPhysicalObject\nFigure 14.7.  A hypo-\nthetical game object \nclass hierarchy us-\ning only inheritance \nto \nassociate \nthe \nclasses.\nFigure 14.8.  Our hypothetical game object class hierarchy, re-factored to favor class composi-\ntion over inheritance.\nGameObject\nTransform\nMeshInstance\nAnimationController\nRigidBody\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n725 \nencapsulated when they are integrated together for use by a particular game \nobject.\nFigure 14.8 shows how our class hierarchy might look aft er re-factoring \nit into components. In this revised design, the GameObject class acts like a \nhub, containing pointers to each of the optional components we’ve deﬁ ned. \nThe MeshInstance component is our replacement for the Renderable\nObject class—it represents an instance of a triangle mesh and encapsulates \nthe knowledge of how to render it. Likewise, the AnimationController\ncomponent replaces AnimatingObject, exposing skeletal animation services \nto the GameObject. Class Transform replaces MovableObject by maintain-\ning the position, orientation, and scale of the object. The RigidBody class rep-\nresents the collision geometry of a game object and provides its GameObject\nwith an interface into the low-level collision and physics systems, replacing \nCollidableObject and PhysicsObject.\nComponent Creation and Ownership\n In this kind of design, it is typical for the “hub” class to own its components, \nmeaning that it manages their lifetimes . But how should a GameObject “know” \nwhich components to create? There are numerous ways to solve this problem, \nbut one of the simplest is provide the root GameObject class with pointers \nto all possible components. Each unique type of game object is deﬁ ned as a \nderived class of GameObject. In the GameObject constructor, all of the com-\nponent pointers are initially set to NULL. Each derived class’s constructor is \nthen free to create whatever components it may need. For convenience, the \ndefault GameObject destructor can clean up all of the components automati-\ncally. In this design, the hierarchy of classes derived from GameObject serves \nas the primary taxonomy for the kinds of objects we want in our game, and the \ncomponent classes serve as optional add-on features.\nOne possible implementation of the component creation and destruction \nlogic for this kind of hierarchy is shown below. However, it’s important to \nrealize that this code is just an example—implementation details vary widely, \neven between engines that employ essentially the same kind of class hierarchy \ndesign.\nclass GameObject\n{\nprotected:\n \n// My transform (position, rotation, scale).\n Transform \n   m_transform;\n \n// Standard components:\n MeshInstance* \nm_pMeshInst;\n14.2. Runtime Object Model Architectures\n\n\n726 \n14. Runtime Gameplay Foundation Systems\n AnimationController* \nm_pAnimController;\n RigidBody*\nm_pRigidBody;\npublic:\nGameObject()\n {\n \n \n// Assume no components by default. Derived  \n \n \n \n \n// classes will override.\n \n \nm_pMeshInst = NULL;\n \n \nm_pAnimController = NULL;\n \n \nm_pRigidBody = NULL;\n }\n~GameObject()\n {\n \n \n// Automatically delete any components created by\n \n \n// derived classes.\n  delete \nm_pMeshInst;\n  delete \nm_pAnimController;\n  delete \nm_pRigidBody;\n }\n \n// ...\n};\nclass Vehicle : public GameObject\n{\nprotected:\n \n// Add some more components specific to Vehicles...\n Chassis* \nm_pChassis;\n Engine* \nm_pEngine;\n \n// ...\npublic:\nVehicle()\n {\n \n \n// Construct standard GameObject components.\n  m_pMeshInst \n= new MeshInstance;\n  m_pRigidBody \n= new RigidBody;\n \n \n// NOTE: We’ll assume the animation controller  \n \n \n \n// must be provided with a reference to the mesh   \n \n \n// instance so that it can provide it with a \n \n \n// matrix palette.\n  m_pAnimController\n   = \nnew AnimationController(*m_pMeshInst);\n\n\n727 \n \n \n// Construct vehicle-specific components.\n  m_pChassis \n= new Chassis(*this,\n      \n  *m_pAnimController);\n  m_pEngine \n= new Engine(*this);\n }\n~Vehicle()\n {\n \n \n// Only need to destroy vehicle-specific  \n \n \n \n \n \n// components, as GameObject cleans up the \n \n \n// standard components for us.\n  delete \nm_pChassis;\n  delete \nm_pEngine;\n }\n};\n14.2.1.5. Generic Components\n Another more-ﬂ exible (but also trickier to implement) alternative is to provide \nthe root game object class with a generic linked list of components. The com-\nponents in such a design usually all derive from a common base class—this \nallows us to iterate over the linked list and perform polymorphic operations, \nsuch as asking each component what type it is or passing an event to each \ncomponent in turn for possible handling. This design allows the root game \nobject class to be largely oblivious to the component types that are available \nand thereby permits new types of components to be created without modify-\ning the game object class in many cases. It also allows a particular game object \nto contain an arbitrary number of instances of each type of component. (The \nGameObject\nTransform\nMeshInstance\nAnimationController\nRigidBody\n+GetType()\n+IsType()\n+ReceiveEvent()\n+Update()\nComponent\n1\n*\nAsterisk indicates zero \nor more instances \n(e.g., linked list).\nFigure 14.9.  A linked list of components can provide ﬂ exibility by allowing the hub game ob-\nject to be unaware of the details of any particular component.\n14.2. Runtime Object Model Architectures\n\n\n728 \n14. Runtime Gameplay Foundation Systems\nhard-coded design permitt ed only a ﬁ xed number, determined by how many \npointers to each component existed within the game object class.)\nThis kind of design is illustrated in Figure 14.9. It is trickier to implement \nthan a hard-coded component model because the game object code must be \nwritt en in a totally generic way. The component classes can likewise make no \nassumptions about what other components might or might not exist within \nthe context of a particular game object. The choice between hard-coding the \ncomponent pointers or using a generic linked list of components is not an easy \none to make. Neither design is clearly superior—they each have their pros and \ncons, and diﬀ erent game teams take diﬀ erent approaches.\n14.2.1.6. Pure Component Models\n What would happen if we were to take the componentization concept to \nits extreme? We would move literally all of the functionality out of our root \nGameObject class into various component classes. At this point, the game ob-\nject class would quite literally be a behavior-less container, with a unique id \nand a bunch of pointers to its components, but otherwise containing no logic \nof its own. So why not eliminate the class entirely? One way to do this is to \ngive each component a copy of the game object’s unique id. The components \nare now linked together into a logical grouping by id. Given a way to quickly \nlook up any component by id, we would no longer need the GameObject\n“hub” class at all. I will use the term pure component model to describe this kind \nof architecture. It is illustrated in Figure 14.10.\nA pure component model is not quite as simple as it ﬁ rst sounds, and it \nis not without its share of problems. For one thing, we still need some way \nof deﬁ ning the various concrete types of game objects our game needs and \nthen arranging for the correct component classes to be instantiated whenever \nFigure 14.10.  In a pure component model, a logical game object is comprised of many compo-\nnents, but the components are linked together only indirectly, by sharing a unique id.\n-m_uniqueId : int = 72\nGameObject\n-m_uniqueId : int = 72\nTransform\n-m_uniqueId : int = 72\nMeshInstance\n-m_uniqueId : int = 72\nAnimationController\n-m_uniqueId : int = 72\nRigidBody\n\n\n729 \nan instance of the type is created. Our GameObject hierarchy used to handle \nconstruction of components for us. Instead, we might use a factory patt ern, \nin which we deﬁ ne factory classes, one per game object type, with a virtual \nconstruction function that is overridden to create the proper components for \neach game object type. Or we might turn to a data-driven model, where the \ngame object types are deﬁ ned in a text ﬁ le that can be parsed by the engine \nand consulted whenever a type is instantiated.\nAnother issue with a components-only design is inter-component com-\nmunication. Our central GameObject acted as a “hub,” marshalling commu-\nnications between the various components. In pure component architectures, \nwe need an eﬃ  cient way for the components making up a single game object \nto talk to one another. This could be done by having each component look up \nthe other components using the game object’s unique id. However, we prob-\nably want a much more eﬃ  cient mechanism—for example the components \ncould be prewired into a circular linked list.\nIn the same sense, sending messages from one game object to another \nis diﬃ  cult in a pure componentized model. We can no longer communicate \nwith the GameObject instance, so we either need to know a priori with which \ncomponent we wish to communicate, or we must multicast to all components \nthat make up the game object in question. Neither option is ideal.\nPure component models can and have been made to work on real game \nprojects. These kinds of models have their pros and cons, but again, they are \nnot clearly bett er than any of the alternative designs. Unless you’re part of a \nresearch and development eﬀ ort, you should probably choose the architecture \nwith which you are most comfortable and conﬁ dent, and which best ﬁ ts the \nneeds of the particular game you are building.\n14.2.2. Property-Centric Architectures\nProgrammers who work frequently in an object-oriented programming lan-\nguage tend to think naturally in terms of objects that contain att ributes (data \nmembers) and behaviors (methods, member functions). This is the object-cen-\ntric view:\nz Object1\nPosition = (0, 3, 15)\n \n□\nOrientation = (0, 43, 0)\n \n□\nz Object2\nPosition = (–12, 0, 8)\n \n□\nHealth = 15\n \n□\n14.2. Runtime Object Model Architectures\n\n\n730 \n14. Runtime Gameplay Foundation Systems\nz Object3\nOrientation = (0, –87, 10)\n \n□\nHowever, it is possible to think primarily in terms of the att ributes, rather \nthan the objects. We deﬁ ne the set of all properties that a game object might \nhave. Then for each property, we build a table containing the values of that \nproperty corresponding to each game object that has it. The property values \nare keyed by the objects’ unique ids. This is what we will call the property-\ncentric vi ew:\nz Position\nObject1 = (0, 3, 15)\n \n□\nObject2 = (–12, 0, 8)\n \n□\nz Orientation\nObject1 = (0, 43, 0)\n \n□\nObject3 = (0, –87, 10)\n \n□\nz Health\nObject2 = 15\n \n□\nProperty-centric object models have been used very successfully on many \ncommercial games, including Deus Ex 2 and the Thief series of games. See Sec-\ntion 14.2.2.5 for more details on exactly how these projects designed their ob-\nject systems.\nA property-centric design is more akin to a relational database than an \nobject model. Each att ribute acts like a column in a database table (or a stand-\nalone table), with the game objects’ unique id as the primary key. Of course, in \nobject-oriented design, an object is deﬁ ned not only by its att ributes, but also \nby its behavior. If all we have are tables of properties, then where do we imple-\nment the behavior? The answer to this question varies somewhat from engine \nto engine, but most oft en the behaviors are implemented in one or both of the \nfollowing places:\nz in the properties themselves, and/or\nz via script code.\nLet’s explore each of these ideas further.\n14.2.2.1. Implementing Behavior via Property Classes\nEach type of property can be implemented as a property class. Properties can be \nas simple as a single Boolean or ﬂ oating-point value or as complex as a render-\nable triangle mesh or an AI “brain.” Each property class can provide behav-\niors via its hard-coded methods (member functions). The overall behavior of \n\n\n731 \na particular game object is determined by the aggregation of the behaviors of \nall its properties.\nFor example, if a game object contains an instance of the Health property, \nit can be damaged and eventually destroyed or killed. The Health object can \nrespond to any att acks made on the game object by decrementing the object’s \nhealth level appropriately. A property object can also communicate with other \nproperty objects within the same game object to produce cooperative behav-\niors. For example, when the Health property detects and responds to an at-\ntack, it could possibly send a message to the AnimatedSkeleton property, \nthereby allowing the game object to play a suitable hit reaction animation. \nSimilarly, when the Health property detects that the game object is about to \ndie or be destroyed, it can talk to the RigidBodyDynamics property to acti-\nvate a physics-driven explosion or a “rag doll” dead body simulation.\n14.2.2.2. Implementing Behavior via Script\nAnother option is to store the property values as raw data in one or more data-\nbase-like tables and use script code to implement a game object’s behaviors. Ev-\nery game object could have a special property called something like ScriptId, \nwhich, if present, speciﬁ es the block of script code (script function, or script \nobject if the scripting language is itself object-oriented) that will manage the ob-\nject’s behavior. Script code could also be used to allow a game object to respond \nto events that occur within the game world. See Section 14.7 for more details on \nevent systems and Section 14.8 for a discussion of game scripting languages.\nIn some property-centric engines, a core set of hard-coded property classes \nare provided by the engineers, but a facility is provided allowing game design-\ners and programmers to implement new property types entirely in script. This \napproach was used successfully on the Dungeon Siege project, for example.\n14.2.2.3. Properties versus Components\nIt’s important to note that many of the authors cited in Section 14.2.2.5 use the \nterm “component” to refer to what I call a “property object ” here. In Section \n14.2.1.4, I used the term “component” to refer to a subobject in an object-cen-\ntric design, which isn’t quite the same as a property object.\nHowever, property objects are very closely related to components in many \nways. In both designs, a single logical game object is made up of multiple sub-\nobjects. The main distinction lies in the roles of the subobjects. In a property-\ncentric design, each subobject deﬁ nes a particular att ribute of the game object \nitself (e.g., health, visual representation, inventory, a particular magic power, \netc.), whereas in a component-based (object-centric) design, the subobjects of-\nten represent linkages to particular low-level engine subsystems (renderer, \nanimation, collision and dynamics, etc.) This distinction is so subtle as to be \n14.2. Runtime Object Model Architectures\n\n\n732 \n14. Runtime Gameplay Foundation Systems\nvirtually irrelevant in many cases. You can call your design a pure component \nmodel (Section 14.2.1.6) or a property-centric design as you see ﬁ t, but at the end \nof the day, you’ll have essentially the same result—a logical game object that is \ncomprised of, and derives its behavior from, a collection of subobjects.\n14.2.2.4. Pros and Cons of Property-Centric Designs\n There are a number of potential beneﬁ ts to an att ribute-centric approach. \nIt tends to be more memory eﬃ  cient, because we need only store att ribute \ndata that is actually in use (i.e., there are never game objects with unused \ndata members). It is also easier to construct such a model in a data-driven \nmanner—designers can deﬁ ne new att ributes easily, without recompiling the \ngame, because there are no game object class deﬁ nitions to be changed. Pro-\ngrammers need only get involved when entirely new types of properties need \nto be added (presuming the property cannot be added via script).\nA property-centric design can also be more cache-friendly than an object-\ncentric model, because data of the same type is stored contiguously in memory. \nThis is a commonplace optimization technique on modern gaming hardware, \nwhere the cost of accessing memory is far higher than the cost of executing \ninstructions and performing calculations. (For example, on the PLAYSTA-\nTION 3, the cost of a single cache miss is equivalent to the cost of executing lit-\nerally thousands of CPU instructions.) By storing data contiguously in RAM, \nwe can reduce or eliminate cache misses, because when we access one element \nof a data array, a large number of its neighboring elements are loaded into the \nsame cache line. This approach to data design is sometimes called the struct of \narrays technique, in contrast to the more-traditional array of structs approach. \nThe diﬀ erences between these two memory layouts are illustrated by the code \nsnippet below. (Note that we wouldn’t really implement a game object model \nin exactly this way—this example is meant only to illustrate the way in which \na property-centric design tends to produce many contiguous arrays of like-\ntyped data, rather than a single array of complex objects.)\nstatic const U32 MAX_GAME_OBJECTS = 1024; \n// Traditional array-of-structs approach.\nstruct GameObject\n{\n U32 \n   m_uniqueId;\n Vector \n  m_pos;\n Quaternion m_rot;\n float \n  m_health;\n \n// ...\n};\nGameObject g_aAllGameObjects[MAX_GAME_OBJECTS];\n\n\n733 \n// Cache-friendlier struct-of-arrays approach.\nstruct AllGameObjects\n{\n U32 \n   m_aUniqueId\n[MAX_GAME_OBJECTS];\n Vector \n  m_aPos\n[MAX_GAME_OBJECTS];\n Quaternion m_aRot\n[MAX_GAME_OBJECTS];\n float \n  m_aHealth\n[MAX_GAME_OBJECTS];\n \n// ...\n};\nAllGameObjects g_allGameObjects;\nAtt ribute-centric models have their share of problems as well. For ex-\nample, when a game object is just a grab bag of properties, it becomes much \nmore diﬃ  cult to enforce relationships between those properties. It can be hard \nto implement a desired large-scale behavior merely by cobbling together the \nﬁ ne-grained behaviors of a group of property objects. It’s also much trickier \nto debug such systems, as the programmer cannot slap a game object into \nthe watch window in the debugger in order to inspect all of its properties at \nonce.\n14.2.2.5. Further Reading\nA number of interesting PowerPoint presentations on the topic of property-\ncentric architectures have been given by prominent engineers in the game \nindustry at various game development conferences. You should be able to \naccess them by visiting the following URLs:\nz Rob Fermier, “Creating a Data Driven Engine,” Game Developer’s Con-\nference, 2002. htt p://www.gamasutra.com/features/gdcarchive/2002/\nrob_fermier.ppt.\nz Scott  Bilas, “A Data-Driven Game Object System,” Game Developer’s \nConference, 2002. htt p://www.drizzle.com/~scott b/gdc/game-objects.\nppt.\nz Alex Duran, “Building Object Systems: Features, Tradeoﬀ s, and \nPitfalls,” Game Developer’s Conference, 2003. htt p://www.gamasutra.\ncom/features/gdcarchive/2003/Duran_Alex.ppt.\nz Jeremy Chatelaine, “Enabling Data Driven Tuning via Existing Tools,” \nGame Developer’s Conference, 2003. htt p://www.gamasutra.com/\nfeatures/gdcarchive/2003/Chatelaine_Jeremy.ppt.\nz Doug Church, “Object Systems,” presented at a game development con-\nference in Seoul, Korea, 2003; conference organized by Chris Hecker, \nCasey Muratori, Jon Blow, and Doug Church. htt p://chrishecker.com/\nimages/6/6f/ObjSys.ppt.\n14.2. Runtime Object Model Architectures\n\n\n734 \n14. Runtime Gameplay Foundation Systems\n14.3. World Chunk Data Formats\n As we’ve seen, a world chunk generally contains both static and dynamic \nworld elements . The static geometry might be represented by one big triangle \nmesh, or it might be comprised of many smaller meshes. Each mesh might be \ninstanced multiple times—for example, a single door mesh might be re-used \nfor all of the doorways in the chunk. The static data usually includes colli-\nsion information stored as a triangle soup , a collection of convex shapes, and/\nor other simpler geometric shapes like planes, boxes, capsules, or spheres. \nOther static elements include volumetric regions that can be used to detect \nevents or delineate areas within the game world, an AI navigation mesh, a set \nof line segments delineating edges within the background geometry that can \nbe grabbed by the player character, and so on. We won’t get into the details \nof these data formats here, because we’ve already discussed most of them in \nprevious chapters.\nThe dynamic portion of the world chunk contains some kind of repre-\nsentation of the game objects within that chunk. A game object is deﬁ ned by \nits att ributes and its behaviors, and an object’s behaviors are determined either \ndirectly or indirectly by its type. In an object-centric design, the object’s type \ndirectly determines which class(es) to instantiate in order to represent the ob-\nject at runtime. In a property-centric design, a game object’s behavior is deter-\nmined by the amalgamation of the behaviors of its properties, but the type still \ndetermines which properties the object should have (or one might say that an \nobject’s properties deﬁ ne its type). So, for each game object, a world chunk \ndata ﬁ le generally contains:\nz The initial values of the object’s att ributes. The world chunk deﬁ nes the \nstate of each game object as it should exist when ﬁ rst spawned into the \ngame world. An object’s att ribute data can be stored in a number of dif-\nferent formats. We’ll explore a few popular formats below.\nz Some kind of speciﬁ cation of the object’s type. In an object-centric engine, \nthis might be a string, a hashed string id, or some other unique type id. \nIn a property-centric design, the type might be stored explicitly, or it \nmight be deﬁ ned implicitly by the collection of properties/att ributes of \nwhich the object is comprised.\n14.3.1. Binary Object Images\nOne way to store a collection of game objects into a disk ﬁ le is to write a binary \nimage of each object into the ﬁ le, exactly as it looks in memory at runtime. \nThis makes spawning game objects trivial. Once the game world chunk has \n\n\n735 \nbeen loaded into memory, we have ready-made images of all our objects, so \nwe simply let them ﬂ y.\nWell, not quite. Storing binary images of “live” C++ class instances is \nproblematic for a number of reasons, including the need to handle pointers \nand virtual tables in a special way, and the possibility of having to endian-\nswap the data within each class instance. (These techniques are described in \ndetail in Section 6.2.2.9.) Moreover, binary object images are inﬂ exible and not \nrobust to making changes. Gameplay is one of the most dynamic and unstable \naspects of any game project, so it is wise to select a data format that supports \nrapid development and is robust to frequent changes. As such, the binary ob-\nject image format is not usually a good choice for storing game object data \n(although this format can be suitable for more stable data structures, like mesh \ndata or collision geometry). \n14.3.2. Serialized Game Object Descriptions\nSerialization is another means of storing a representation of a game object’s in-\nternal state to a disk ﬁ le. This approach tends to be more portable and simpler \nto implement than the binary object image technique. To serialize an object out \nto disk, the object is asked to produce a stream of data that contains enough \ndetail to permit the original object to be reconstructed later. When an object is \nserialized back into memory from disk, an instance of the appropriate class is \ncreated, and then the stream of att ribute data is read in order to initialize the \nnew object’s internal state. If the original serialized data stream was complete, \nthe new object should be identical to the original for all intents and purposes.\nSerialization is supported natively by some programming languages. For \nexample, C# and Java both provide standardized mechanisms for serializing \nobject instances to and from an XML text format. The C++ language unfortu-\nnately does not provide a standardized serialization facility. However, many \nC++ serialization systems have been successfully built, both inside and out-\nside the game industry. We won’t get into all the details of how to write a C++ \nobject serialization system here, but we’ll describe the data format and a few \nof the main systems that need to be writt en in order to get serialization to \nwork in C++.\nSerialization data isn’t a binary image of the object. Instead, it is usually \nstored in a more-convenient and more-portable format. XML is a popular for-\nmat for object serialization because it is well-supported and standardized, it is \nsomewhat human-readable, and it has excellent support for hierarchical data \nstructures, which arise frequently when serializing collections of interrelated \ngame objects. Unfortunately, XML is notoriously slow to parse, which can \nincrease world chunk load times. For this reason, some game engines use a \n14.3. World Chunk Data Formats\n\n\n736 \n14. Runtime Gameplay Foundation Systems\nproprietary binary format that is faster to parse and more compact than XML \ntext.\nThe mechanics of serializing an object to and from disk are usually imple-\nmented in one of two basic ways:\nz We can introduce a pair of virtual functions called something like \nSerializeOut() and SerializeIn() in our base class and arrange \nfor each derived class to provide custom implementations of them that \n“know” how to serialize the att ributes of that particular class.\nz We can implement a reﬂ ection system for our C++ classes. We can then \nwrite a generic system that can automatically serialize any C++ object \nfor which reﬂ ection information is available.\nReﬂ ection is a term used by the C# language, among others. In a nutshell, \nreﬂ ection data is a runtime description of the contents of a class. It stores infor-\nmation about the name of the class, what data members it contains, the types \nof each data member, and the oﬀ set of each member within the object’s mem-\nory image, and it also contains information about all of the class’s member \nfunctions. Given reﬂ ection information for an arbitrary C++ class, we could \nquite easily write a general-purpose object serialization system.\nThe tricky part of a C++ reﬂ ection system is generating the reﬂ ection data \nfor all of the relevant classes. This can be done by encapsulating a class’s data \nmembers in #def ine macros that extract relevant reﬂ ection information by \nproviding a virtual function that can be overridden by each derived class in \norder to return appropriate reﬂ ection data for that class, by hand-coding a \nreﬂ ection data structure for each class, or via some other inventive approach.\nIn addition to att ribute information, the serialization data stream invari-\nably includes the name or unique id of each object’s class or type. The class id \nis used to instantiate the appropriate class when the object is serialized into \nmemory from disk. A class id can be stored as a string, a hashed string id, or \nsome other kind of unique id.\nUnfortunately, C++ provides no way to instantiate a class given only its \nname as a string or id. The class name must be known at compile time, and \nso it must be hard-coded by a programmer (e.g., new ConcreteClass ). To \nwork around this limitation of the language, C++ object serialization systems \ninvariably include a class factory of some kind. A factory can be implemented \nin any number of ways, but the simplest approach is to create a data table that \nmaps each class name/id to some kind of function or functor object that has \nbeen hard-coded to instantiate that particular class. Given a class name or id, \nwe simply look up the corresponding function or functor in the table and call \nit to instantiate the class.\n\n\n737 \n14.3.3. Spawners and Type Schemas\nBoth binary object images and serialization formats have an Achilles heel. They \nare both deﬁ ned by the runtime implementation of the game object types they \nstore, and hence they both require the world editor to contain intimate knowl-\nedge of the game engine’s runtime implementation. For example, in order for \nthe world editor to write out a binary image of a heterogeneous collection of \ngame objects, it must either link directly with the runtime game engine code, \nor it must be painstakingly hand-coded to produce blocks of bytes that exactly \nmatch the data layout of the game objects at runtime. Serialization data is less-\ntightly coupled to the game object’s implementation, but again, the world edi-\ntor either needs to link with runtime game object code in order to gain access \nto the classes’ SerializeIn() and SerializeOut() functions, or it needs \naccess to the classes’ reﬂ ection information in some way.\nThe coupling between the game world editor and the runtime engine \ncode can be broken by abstracting the descriptions of our game objects in an \nimplementation-independent way. For each game object in a world chunk \ndata ﬁ le, we store a litt le block of data, oft en called a spawner . A spawner is \na lightweight, data-only representation of a game object that can be used to \ninstantiate and initialize that game object at runtime. It contains the id of \nthe game object’s tool-side type. It also contains a table of simple key-value \npairs that describe the initial att ributes of the game object. These att ributes \noft en include a model-to-world transform, since most game objects have \na distinct position, orientation, and scale in world space. When the game \nobject is spawned, the appropriate class or classes are instantiated, as de-\ntermined by the spawner’s type. These runtime objects can then consult \nthe dictionary of key-value pairs in order to initialize their data members \nappropriately.\nA spawner can be conﬁ gured to spawn its game object immediately upon \nbeing loaded, or it can lie dormant until asked to spawn at some later time \nduring the game. Spawners can be implemented as ﬁ rst-class objects, so they \ncan have a convenient functional interface and can store useful meta-data in \naddition to object att ributes. A spawner can even be used for purposes other \nthan spawning game objects. For example, in Uncharted: Drake’s Fortune, de-\nsigners used spawners to deﬁ ne important points or coordinate axes in the \ngame world. These were called position spawners or locator spawners. Locators \nhave many uses in a game, such as:\nz deﬁ ning points of interest for an AI character,\nz deﬁ ning a set of coordinate axes relative to which a set of animations \ncan be played in perfect synchronization,\n14.3. World Chunk Data Formats\n\n\n738 \n14. Runtime Gameplay Foundation Systems\nz deﬁ ning the location at which a particle eﬀ ect or audio eﬀ ect should \noriginate,\nz deﬁ ning waypoints along a race track,\nz and the list goes on.\n14.3.3.1. Object Type Schemas\nA game object’s att ributes and behaviors are deﬁ ned by its type. In a game \nworld editor that employs a spawner-based design, a game object type can be \nrepresented by a data-driven schema that deﬁ nes the collection of att ributes \nthat should be visible to the user when creating or editing an object of that \ntype. At runtime, the tool-side object type can be mapped in either a hard-\ncoded or data-driven way to a class or collection of classes that must be instan-\ntiated in order to spawn a game object of the given type.\nType schemas can be stored in a simple text ﬁ le for consumption by the \nworld editor and for inspection and editing by its users. For example, a sche-\nma ﬁ le might look something like this:\nenum LightType\n{\n \nAmbient, Directional, Point, Spot\n}\ntype Light\n{\n String \n   UniqueId;\nLightType   Type;\n Vector \n   Pos;\n Quaternion  Rot;\n Float \n   Intensity \n: min(0.0), max(1.0);\n ColorARGB \n  DiffuseColor;\n ColorARGB \n  SpecularColor;\n ...\n}\ntype Vehicle\n{\n String \n   UniqueId;\n Vector \n   Pos;\n Quaternion  Rot;\n MeshReference \n Mesh;\n Int \n    NumWheels \n: min(2), max(4);\n\n\n739 \n Float \n   TurnRadius;\n Float \n   TopSpeed \n: min(0.0);\n ...\n}\n...\nThe above example brings a few important details to light. You’ll notice \nthat the data types of each att ribute are deﬁ ned, in addition to their names. \nThese can be simple types like strings, integers, and ﬂ oating-point values, or \nthey can be specialized types like vectors, quaternions, ARGB colors, or ref-\nerences to special asset types like meshes, collision data, and so on. In this \nexample, we’ve even provided a mechanism for deﬁ ning enumerated types, \nlike LightType. Another subtle point is that the object type schema provides \nadditional information to the world editor, such as what type of GUI element \nto use when editing the att ribute. Sometimes an att ribute’s GUI requirements \nare implied by its data type—strings are generally edited with a text ﬁ eld, \nBooleans via a check box, vectors via three text ﬁ elds for the x-, y-, and z-\ncoordinates or perhaps via a specialized GUI element designed for manipu-\nlating vectors in 3D. The schema can also specify meta-information for use \nby the GUI, such as minimum and maximum allowable values for integer \nand ﬂ oating-point att ributes, lists of available choices for drop-down combo \nboxes, and so on.\nSome game engines permit object type schemas to be inherited , much like \nclasses. For example, every game object needs to know its type and must have \na unique id so that it can be distinguished from all the other game objects at \nruntime. These att ributes could be speciﬁ ed in a top-level schema, from which \nall other schemas are derived.\n14.3.3.2. Default Attribute Values\nAs you can well imagine, the number of att ributes in a typical game object \nschema can grow quite large. This translates into a lot of data that must be \nspeciﬁ ed by the game designer for each instance of each game object type he \nor she places into the game world. It can be extremely helpful to deﬁ ne default \nvalues in the schema for many of the att ributes. This permits game designers \nto place “vanilla” instances of a game object type with litt le eﬀ ort but still \npermits him or her to ﬁ ne-tune the att ribute values on speciﬁ c instances as \nneeded.\nOne inherent problem with default values arises when the default val-\nue of a particular att ribute changes. For example, our game designers might \nhave originally wanted Orcs to have 20 hit points. Aft er many months of pro-\n14.3. World Chunk Data Formats\n\n\n740 \n14. Runtime Gameplay Foundation Systems\nduction, it might be decided that Orcs should have a more powerful 30 hit \npoints by default. Any new Orcs placed into a game world will now have 30 \nhit points unless otherwise speciﬁ ed. But what about all the Orcs that were \nplaced into game world chunks prior to the change? Do we need to ﬁ nd all of \nthese previously-created Orcs and manually change their hit points to 30?\nIdeally, we’d like to design our spawner system so that changes in de-\nfault values automatically propagate to all preexisting instances that have not \nhad their default values overridden explicitly. One easy way to implement \nthis feature is to simply omit key-value pairs for att ributes whose value does \nnot diﬀ er from the default value. Whenever an att ribute is missing from the \nspawner, the appropriate default can be used. (This presumes that the game \nengine has access to the object type schema ﬁ le, so that it can read in the at-\ntributes’ default values.) In our example, most of the preexisting Orc spawners \nwould have had no HitPoints key-value pair at all (unless of course one of the \nspawner’s hit points had been changed from the default value manually). So \nwhen the default value changes from 20 to 30, these Orcs will automatically \nuse the new value.\nSome engines allow default values to be overridden in derived object \ntypes. For example, the schema for a type called Vehicle might deﬁ ne a de-\nfault TopSpeed of 80 miles per hour. A derived Motorcycle type schema could \noverride this TopSpeed to be 100 miles per hour.\n14.3.3.3. Some Beneifts of Spawners and Type Schemas\n The key beneﬁ ts of separating the spawner from the implementation of the \ngame object are simplicity, ﬂ exibility, and robustness. From a data management \npoint of view, it is much simpler to deal with a table of key-value pairs than it \nis to manage a binary object image with pointer ﬁ x-ups or a custom serialized \nobject format. The key-value pairs approach also makes the data format ex-\ntremely ﬂ exible and robust to changes. If a game object encounters key-value \npairs that it is not expecting to see, it can simply ignore them. Likewise, if the \ngame object is unable to ﬁ nd a key-value pair that it needs, it has the option \nof using a default value instead. This makes a key-value pair data format ex-\ntremely robust to changes made by both the designers and the programmers.\nSpawners also simplify the design and implementation of the game world \neditor, because it only needs to know how to manage lists of key-value pairs \nand object type schemas. It doesn’t need to share code with the runtime game \nengine in any way, and it is only very loosely coupled to the engine’s imple-\nmentation details.\nSpawners and archetypes give game designers and programmers a great \ndeal of ﬂ exibility and power. Designers can deﬁ ne new game object type sche-\n\n\n741 \nmas within the world editor with litt le or no programmer intervention. The \nprogrammer can implement the runtime implementation of these new object \ntypes whenever his or her schedule allows it. The programmer does not need \nto immediately provide an implementation of each new object type as it is \nadded in order to avoid breaking the game. New object data can exist safely in \nthe world chunk ﬁ les with or without a runtime implementation, and runtime \nimplementations can exist with or without corresponding data in the world \nchunk ﬁ le.\n14.4. Loading and Streaming Game Worlds\nTo bridge the gap between the oﬀ -line world editor and our runtime game \nobject model, we need a way to load world chunks into memory and un-\nload them when they are no longer needed. The game world loading sys-\ntem has two main responsibilities: to manage the ﬁ le I/O necessary to load \ngame world chunks and other needed assets from disk into memory and to \nmanage the allocation and deallocation of memory for these resources. The \nengine also needs to manage the spawning and destruction of game objects as \nthey come and go in the game, both in terms of allocating and deallocating \nmemory for the objects and ensuring that the proper classes are instantiated \nfor each game object. In the following sections, we’ll investigate how game \nworlds are loaded and also have a look at how object spawning systems typi-\ncally work.\n14.4.1. Simple Level Loading\n The most straightforward game world loading approach, and the one used by \nall of the earliest games, is to allow one and only one game world chunk (a.k.a. \nlevel) to be loaded at a time. When the game is ﬁ rst started, and between pairs \nof levels, the player sees a static or simply animated two-dimensional loading \nscreen while he or she waits for the level to load.\nMemory management in this kind of design is quite straightforward. As \nwe mentioned in Section 6.2.2.7, a stack-based allocator is very well-suited \nto a one-level-at-a-time world loading design. When the game ﬁ rst runs, any \nresource data that is required across all game levels is loaded at the bott om \nof the stack. I will call this load-and-stay-resident (LSR) data. The location of \nthe stack pointer is recorded aft er the LSR data has been fully loaded. Each \ngame world chunk, along with its associated mesh, texture, audio, anima-\ntion, and other resource data, is loaded on top of the LSR data on the stack. \nWhen the level has been completed by the player, all of its memory can be \n14.4. Loading and Streaming Game Worlds\n",
      "page_number": 741,
      "chapter_number": 37,
      "summary": "This chapter covers segment 37 (pages 741-763). Key topics include classes, objects, and games. However, as class hierarchies grow, they have a tendency \nto deepen and widen simultaneously, leading to what I call a monolithic class \nhierarchy.",
      "keywords": [
        "game object",
        "game object class",
        "object",
        "game",
        "game object type",
        "game object model",
        "Object Model Architectures",
        "object class",
        "Runtime Object Model",
        "object type",
        "object class hierarchy",
        "Gameplay Foundation Systems",
        "game world",
        "object model",
        "class hierarchy"
      ],
      "concepts": [
        "classes",
        "objects",
        "games",
        "gaming",
        "components",
        "data",
        "designed",
        "types",
        "typed",
        "vehicle"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 3",
          "chapter": 51,
          "title": "Segment 51 (pages 488-496)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "Data-Oriented Design",
          "chapter": 5,
          "title": "Hierarchical Level of",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "Software Architecture",
          "chapter": 43,
          "title": "Segment 43 (pages 433-447)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        },
        {
          "book": "Software Architecture",
          "chapter": 8,
          "title": "Segment 8 (pages 63-71)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 764-785)",
      "start_page": 764,
      "end_page": 785,
      "detection_method": "synthetic",
      "content": "742 \n14. Runtime Gameplay Foundation Systems\nfreed by simply resett ing the stack pointer to the top of the LSR data block. \nAt this point, a new level can be loaded in its place. This is illustrated in \nFigure 14.11.\nWhile this design is very simple, it has a number of drawbacks. For one \nthing, the player only sees the game world in discrete chunks—there is no \nway to implement a vast, contiguous, seamless world using this technique. \nAnother problem is that during the time the level’s resource data is being \nloaded, there is no game world in memory. So the player is forced to watch a \ntwo-dimensional loading screen of some sort.\n14.4.2. Toward Seamless Loading: Air Locks\nThe best way to avoid boring level-loading screens is to permit the player \nto continue playing the game while the next world chunk and its associated \nFigure 14.11.  A stack-based memory allocator is extremely well-suited to a one-level-at-a-\ntime world loading system.\n\n\n743 \nresource data are being loaded. One simple approach would be to divide the \nmemory that we’ve set aside for game world assets into two equally sized \nblocks. We could load level A into one memory block, allow the player to start \nplaying level A, and then load level B into the other block using a streaming \nﬁ le I/O library (i.e., the loading code would run in a separate thread ). The big \nproblem with this technique is that it cuts the size of each level in half relative \nto what would be possible with a one-level-at-a-time approach.\nWe can achieve a similar eﬀ ect by dividing the game world memory into \ntwo unequally-sized blocks—a large block that can contain a “full” game \nworld chunk and a small block that is only large enough to contain a tiny \nworld chunk. The small chunk is sometimes known as an “air lock .”\nWhen the game starts, a “full” chunk and an “air lock” chunk are loaded. \nThe player progresses through the full chunk and into the air lock, at which \npoint some kind of gate or other impediment ensures that the player can nei-\nther see the previous full world area nor return to it. The full chunk can then \nbe un-loaded, and a new full-sized world chunk can be loaded. During the \nload, the player is kept busy doing some task within the air lock. The task \nmight be as simple as walking from one end of a hallway to the other, or it \ncould be something more engaging, like solving a puzzle or ﬁ ghting some \nenemies.\nAsynchronous ﬁ le I/O is what enables the full world chunk to be loaded \nwhile the player is simultaneously playing in the air lock region. See Section \n6.1.3 for more details. It’s important to note that an air lock system does not free \nus from displaying a loading screen whenever a new game is started, because \nduring the initial load there is no game world in memory in which to play. \nHowever, once the player is in the game world, he or she needn’t see a loading \nscreen ever again, thanks to air locks and asynchronous data loading.\nHalo for the Xbox used a technique similar to this. The large world areas \nwere invariably connected by smaller, more conﬁ ned areas. As you play Halo, \nwatch for conﬁ ned areas that prevent you from back-tracking—you’ll ﬁ nd one \nroughly every 5-10 minutes of gameplay. Jak 2 for the PlayStation 2 used the \nair lock technique as well. The game world was structured as a hub area (the \nmain city) with a number of oﬀ -shoot areas, each of which was connected to \nthe hub via a small, conﬁ ned air lock region.\n14.4.3. Game World Streaming\nMany game designs call for the player to feel like he or she is playing in a \nhuge, contiguous, seamless world. Ideally, the player should not be conﬁ ned \nto small air lock regions periodically—it would be best if the world simply \nunfolded in front of the player as naturally and believably as possible.\n14.4. Loading and Streaming Game Worlds\n\n\n744 \n14. Runtime Gameplay Foundation Systems\nModern game engines support this kind of seamless world by using a \ntechnique known as streaming . World streaming can be accomplished in vari-\nous ways. The main goals are always (a) to load data while the player is en-\ngaged in regular gameplay tasks and (b) to manage the memory in such a way \nas to eliminate fragmentation while permitt ing data to be loaded and unloaded \nas needed as the player progresses through the game world.\nRecent consoles and PCs have a lot more memory than their predecessors, \nso it is now possible to keep multiple world chunks in memory simultane-\nously. We could imagine dividing our memory space into, say, three equally \nsized buﬀ ers. At ﬁ rst, we load world chunks A, B, and C into these three buf-\nfers and allow the player to start playing through chunk A. When he or she \nenters chunk B and is far enough along that chunk A can no longer be seen, \nwe can unload chunk A and start loading a new chunk D into the ﬁ rst buﬀ er. \nWhen B can no longer be seen, it can be dumped and chunk E loaded. This \nrecycling of buﬀ ers can continue until the player has reached the end of the \ncontiguous game world.\nThe problem with a coarse-grained approach to world streaming is that \nit places onerous restrictions on the size of a world chunk. All chunks in the \nentire game must be roughly the same size—large enough to ﬁ ll up the major-\nity of one of our three memory buﬀ ers but never any larger.\nOne way around this problem is to employ a much ﬁ ner-grained subdivi-\nsion of memory. Rather than streaming relatively large chunks of the world, we \ncan divide every game asset, from game world chunks to foreground meshes \nto textures to animation banks, into equally-sized blocks of data. We can then \nuse a chunky, pool-based memory allocation system like the one described in \nSection 6.2.2.7 to load and unload resource data as needed without having to \nworry about memory fragmentation. This is the technique employed by the \nUncharted: Drake’s Fortune engine.\n14.4.3.1. Determining Which Resources to Load\nOne question that arises when using a ﬁ ne-grained chunky memory allocator \nfor world streaming is how the engine will know what resources to load at \nany given moment during gameplay. In Uncharted: Drake’s Fortune (UDF), we \nused a relatively simple system of level load regions to control the loading and \nunloading of assets.\nUDF is set in two geographically distinct, contiguous game worlds—\nthe jungle and the island. Each of these worlds exists in a single, consistent \nworld space, but they are divided up into numerous geographically adjacent \nchunks. A simple convex volume known as a region encompasses each of the \nchunks; the regions overlap each other somewhat. Each region contains a list \n\n\n745 \nof the world chunks that should be in memory when the player is in that \nregion.\nAt any given moment, the player is within one or more of these regions. \nTo determine the set of world chunks that should be in memory, we simply \ntake the union of the chunk lists from each of the regions enclosing the Na-\nthan Drake character. The level loading system periodically checks this master \nchunk list and compares it against the set of world chunks that are currently \nin memory. If a chunk disappears from the master list, it is unloaded, thereby \nfreeing up all of the allocation blocks it occupied. If a new chunk appears in \nthe list, it is loaded into any free allocation blocks that can be found. The level \nload regions and world chunks are designed in such a way as to ensure that \nthe player never sees a chunk disappear when it is unloaded and that there’s \nenough time between the moment at which a chunk starts loading and the \nmoment its contents are ﬁ rst seen by the player to permit the chunk to be fully \nstreamed into memory. This technique is illustrated in Figure 14.12.\n14.4.4. Memory Management for Object Spawning\nOnce a game world has been loaded into memory, we need to manage the pro-\ncess of spawning the dynamic game objects in the world. Most game engines \nhave some kind of game object spawning system that manages the instantia-\ntion of the class or classes that make up each game object and handles destruc-\ntion of game objects when they are no longer needed. One of the central jobs of \nany object spawning system is to manage the dynamic allocation of memory \nfor newly spawned game objects. Dynamic allocation can be slow, so steps \nmust be taken to ensure allocations are as eﬃ  cient as possible. And because \ngame objects come in a wide variety of sizes, dynamically allocating them can \ncause memory to become fragmented , leading to premature out-of-memory \nconditions. There are a number of diﬀ erent approaches to game object memo-\nry management. We’ll explore a few common ones in the following sections.\n14.4. Loading and Streaming Game Worlds\n1\n2\n3\n4\n1\n2\nLevel 1\nLevel 2\n3\nLevel 2\nLevel 3\n4\nLevel 3\nLevel 4\nFigure 14.12.  A game world divided into chunks. Level load regions, each with a requested \nchunk list, are arranged in such a way as to guarantee that the player never sees a chunk pop \nin or out of view.\n\n\n746 \n14. Runtime Gameplay Foundation Systems\n14.4.4.1. Off-Line Memory Allocation for Object Spawning\nSome game engines solve the problems of allocation speed and memory frag-\nmentation in a rather draconian way, by simply disallowing dynamic mem-\nory allocation during gameplay altogether. Such engines permit game world \nchunks to be loaded and unloaded dynamically, but they spawn in all dy-\nnamic game objects immediately upon loading a chunk. Thereaft er, no game \nobjects can be created or destroyed. You can think of this technique as obeying \na “law of conservation of game objects.” No game objects are created or de-\nstroyed once a world chunk has been loaded. \nThis technique avoids memory fragmentation because the memory re-\nquirements of all the game objects in a world chunk are (a) known a priori \nand (b) bounded. This means that the memory for the game objects can be \nallocated oﬀ -line by the world editor and included as part of the world chunk \ndata itself. All game objects are therefore allocated out of the same memory \nused to load the game world and its resources, and they are no more prone \nto fragmentation than any other loaded resource data. This approach also has \nthe beneﬁ t of making the game’s memory usage patt erns highly predictable. \nThere’s no chance that a large group of game objects is going to spawn into the \nworld unexpectedly, and cause the game to run out of memory.\nOn the downside, this approach can be quite limiting for game designers. \nDynamic object spawning can be simulated by allocating a game object in the \nworld editor but instructing it to be invisible and dormant when the world \nis ﬁ rst loaded. Later, the object can “spawn” by simply activating itself and \nmaking itself visible. But the game designers have to predict the total number \nof game objects of each type that they’ll need when the game world is ﬁ rst \ncreated in the world editor. If they want to provide the player with an inﬁ nite \nsupply of health packs, weapons, enemies, or some other kind of game object, \nthey either need to work out a way to recycle their game objects, or they’re \nout of luck.\n14.4.4.2. Dynamic Memory Management for Object Spawning\nGame designers would probably prefer to work with a game engine that sup-\nports true dynamic object spawning. Although this is more diﬃ  cult to imple-\nment than a static game object spawning approach, it can be implemented in \na number of diﬀ erent ways. \nAgain, the primary problem is memory fragmentation. Because diﬀ erent \ntypes of game objects (and sometimes even diﬀ erent instances of the same \ntype of object) occupy diﬀ erent amounts of memory, we cannot use our fa-\nvorite fragmentation-free allocator—the pool allocator. And because game ob-\njects are generally destroyed in a diﬀ erent order than that in which they were \n\n\n747 \nspawned, we cannot use a stack-based allocator either. Our only choice ap-\npears to be a fragmentation-prone heap allocator. Thankfully, there are many \nways to deal with the fragmentation problem. We’ll investigate a few common \nones in the following sections.\nOne Memory Pool per Object Type\nIf the individual instances of each game object type are guaranteed to all occu-\npy the same amount of memory, we could consider using a separate memory \npool for each object type. Actually, we only need one pool per unique game \nobject size, so object types of the same size can share a single pool.\nDoing this allows us to completely avoid memory fragmentation, but \none limitation of this approach is that we need to maintain lots of separate \npools. We also need to make educated guesses about how many of each type \nof object we’ll need. If a pool has too many elements, we end up wasting \nmemory; if it has too few, we won’t be able to satisfy all of the spawn requests \nat runtime, and game objects will fail to spawn. That said, many commer-\ncial game engines do successfully employ this kind of memory management \ntechnique.\nSmall Memory Allocators\nWe can transform the idea of one pool per game object type into something \nmore workable by allowing a game object to be allocated out of a pool whose \nelements are larger than the object itself. This can reduce the number of unique \nmemory pools we need signiﬁ cantly, at the cost of some potentially wasted \nmemory in each pool.\nFor example, we might create a set of pool allocators, each one with ele-\nments that are twice as large as those of its predecessor—perhaps 8, 16, 32, \n64, 128, 256, and 512 bytes. We can also use a sequence of element sizes that \nconforms to some other suitable patt ern or base the list of sizes on allocation \nstatistics collected from the running game.\nWhenever we try to allocate a game object, we search for the smallest pool \nwhose elements are larger than or equal to the size of the object we’re allocat-\ning. We accept that for some objects, we’ll be wasting space. In return, we al-\nleviate all of our memory fragmentation problems—a reasonably fair trade. If \nwe ever encounter a memory allocation request that is larger than our largest \npool, we can always turn it over to the general-purpose heap allocator, know-\ning that fragmentation of large memory blocks is not nearly as problematic as \nfragmentation involving tiny blocks.\nThis type of allocator is sometimes called a small memory allocator . It can \neliminate fragmentation (for allocations that ﬁ t into one of the pools). It can \nalso speed up memory allocations signiﬁ cantly for small chunks of data, be-\n14.4. Loading and Streaming Game Worlds\n\n\n748 \n14. Runtime Gameplay Foundation Systems\ncause a pool allocation involves two pointer manipulations to remove the ele-\nment from the linked list of free elements—a much less-expensive operation \nthan a general-purpose heap allocation.\nMemory Relocation\nAnother way to eliminate fragmentation is to att ack the problem directly. This \napproach is known as memory relocation . It involves shift ing allocated memory \nblocks down into adjacent free holes to remove fragmentation. Moving the \nmemory is easy, but because we are moving “live” allocated objects, we need \nto be very careful about ﬁ xing up any pointers into the memory blocks we \nmove. See Section 5.2.2.2 for more details.\n14.4.5. Saved Games\n Many games allow the player to save his or her progress, quit the game, and \nthen load up the game at a later time in exactly the state he or she left  it. A saved \ngame system is similar to the world chunk loading system in that it is capable \nof loading the state of the game world from a disk ﬁ le or memory card. But the \nrequirements of this system diﬀ er somewhat from those of a world loading \nsystem, so the two are usually distinct (or overlap only partially).\nTo understand the diﬀ erences between the requirements of these two sys-\ntems, let’s brieﬂ y compare world chunks to saved game ﬁ les. World chunks \nspecify the initial conditions of all dynamic objects in the world, but they also \ncontain a full description of all static world elements. Much of the static infor-\nmation, such as background meshes and collision data, tends to take up a lot \nof disk space. As such, world chunks are sometimes comprised of multiple \ndisk ﬁ les, and the total amount of data associated with a world chunk is usu-\nally large.\nA saved game ﬁ le must also store the current state information of the \ngame objects in the world. However, it does not need to store a duplicate copy \nof any information that can be determined by reading the world chunk data. \nFor example, there’s no need to save out the static geometry in a saved game \nﬁ le. A saved game need not store every detail of every object’s state either. \nSome objects that have no impact on gameplay can be omitt ed altogether. For \nthe other game objects, we may only need to store partial state information. \nAs long as the player can’t tell the diﬀ erence between the state of the game \nworld before and aft er it has been saved and reloaded (or if the diﬀ erences \nare irrelevant to the player), then we have a successful saved game system. \nAs such, saved game ﬁ les tend to be much smaller than world chunk ﬁ les and \nmay place more of an emphasis on data compression and omission. Small ﬁ le \nsizes are especially important when numerous saved game ﬁ les must ﬁ t onto \n\n\n749 \nthe tiny memory cards that were used on older consoles. But even today, with \nconsoles that are equipped with large hard drives, it’s still a good idea to keep \nthe size of a saved game ﬁ le as small as possible.\n14.4.5.1. Check Points\nOne approach to save games is to limit saves to speciﬁ c points in the game, \nknown as check points . The beneﬁ t of this approach is that most of the knowl-\nedge about the state of the game is saved in the current world chunk(s) in the \nvicinity of each check point. This data is always exactly the same, no matt er \nwhich player is playing the game, so it needn’t be stored in the saved game. \nAs a result, saved game ﬁ les based on check points can be extremely small. We \nmight need to store only the name of the last check point reached, plus per-\nhaps some information about the current state of the player character, such as \nthe player’s health, number of lives remaining, what items he has in his inven-\ntory, which weapon(s) he has, and how much ammo each one contains. Some \ngames based on check points don’t even store this information—they start the \nplayer oﬀ  in a known state at each check point. Of course, the downside of a \ngame based on check points is the possibility of user frustration, especially if \ncheck points are few and far between.\n14.4.5.2. Save Anywhere\nSome games support a feature known as save anywhere . As the name implies, \nsuch games permit the state of the game to be saved at literally any point dur-\ning play. To implement this feature, the size of the saved game data ﬁ le must \nincrease signiﬁ cantly. The current locations and internal states of every game \nobject whose state is relevant to gameplay must be saved and then restored \nwhen the game is loaded again later.\nIn a save anywhere design, a saved game data ﬁ le contains basically the \nsame information as a world chunk, minus the world’s static components. It \nis possible to utilize the same data format for both systems, although there \nmay be factors that make this infeasible. For example, the world chunk data \nformat might be designed for ﬂ exibility, but the saved game format might be \ncompressed to minimize the size of each saved game.\nAs we’ve mentioned, one way to reduce the amount of data that needs to \nbe stored in a saved game ﬁ le is to omit certain irrelevant game objects and to \nomit some irrelevant details of others. For example, we needn’t remember the \nexact time index within every animation that is currently playing or the exact \nmomentums and velocities of every physically simulated rigid body. We can \nrely on the imperfect memories of human gamers and save only a rough ap-\nproximation to the game’s state.\n14.4. Loading and Streaming Game Worlds\n\n\n750 \n14. Runtime Gameplay Foundation Systems\n14.5. Object References and World Queries\nEvery game object generally requires some kind of unique id so that it can be \ndistinguished from the other objects in the game, found at runtime, serve as a \ntarget of inter-object communication, and so on. Unique object ids are equally \nhelpful on the tool side, as they can be used to identify and ﬁ nd game objects \nwithin the world editor .\nAt runtime, we invariably need various ways to ﬁ nd game objects. We \nmight want to ﬁ nd an object by its unique id, by its type, or by a set of arbi-\ntrary criteria. We oft en need to perform proximity-based queries, for example \nﬁ nding all enemy aliens within a 10 meter radius of the player character.\nOnce a game object has been found via a query , we need some way to re-\nfer to it. In a language like C or C++, object references might be implemented \nas pointers, or we might use something more sophisticated, like handles or \nsmart pointers . The lifetime of an object reference can vary widely, from the \nscope of a single function call to a period of many minutes.\nIn the following sections, we’ll ﬁ rst investigate various ways to implement \nobject references. Then we’ll explore the kinds of queries we oft en require when \nimplementing gameplay and how those queries might be implemented.\n14.5.1. Pointers\nIn C or C++, the most straightforward way to implement an object reference is \nvia a pointer (or a reference in C++). Pointers are powerful and are just about \nas simple and intuitive as you can get. However, pointers suﬀ er from a num-\nber of problems:\nz Orphaned objects. Ideally, every object should have an owner—another \nobject that is responsible for managing its lifetime—creating it and then \ndeleting it when it is no longer needed. But pointers don’t give the pro-\ngrammer any help in enforcing this rule. The result can be an orphaned \nobject—an object that still occupies memory but is no longer needed or \nreferenced by any other object in the system.\nz Stale pointers . If an object is deleted, ideally we should null-out any and \nall pointers to that object. If we forget to do so, however, we end up \nwith a stale pointer—a pointer to a block of memory that used to be \noccupied by a valid object but is now free memory. If anyone tries to \nread or write data through a stale pointer, the result can be a crash or \nincorrect program behavior. Stale pointers can be diﬃ  cult to track down \nbecause they may continue to work for some time aft er the object has \ndeleted. Only much later, when a new object is allocated on top of the \nstale memory block, does the data actually change and cause a crash.\n\n\n751 \nz Invalid pointers . A programmer is free to store any address in a pointer, \nincluding a totally invalid address. A common problem is dereferencing \na null pointer. These problems can be guarded against by using asser-\ntion macros to check that pointers are never null prior to dereferencing \nthem. Even worse, if a piece of data is misinterpreted as a pointer, deref-\nerencing it can cause the program to read or write an essentially random \nmemory address. This usually results in a crash or other major problem \nthat can be very tough to debug.\nMany game engines make heavy use of pointers, because they are by far \nthe fastest, most eﬃ  cient, and easiest-to-work-with way to implement object \nreferences. However, experienced programmers are always wary of pointers, \nand some game teams turn to more sophisticated kinds of object references, \neither out of a desire to use safer programming practices or out of necessity. \nFor example, if a game engine relocates allocated data blocks at runtime to \neliminate memory fragmentation (see Section 5.2.2.2), simple pointers cannot \nbe used. We either need to use a type of object reference that is robust to mem-\nory relocation, or we need to manually ﬁ x up any pointers into every relocated \nmemory block at the time it is moved.\n14.5.2. Smart Pointers\nA smart pointer is a small object that acts like a pointer for most intents and pur-\nposes but avoids most of the problems inherent with native C/C++ pointers. At \nits simplest, a smart pointer contains a native pointer as a data member and \nprovides a set of overloaded operators that make it act like a pointer in most \nways. Pointers can be dereferenced, so the * and -> operators are overloaded \nto return the address as expected. Pointers can undergo arithmetic operations, \nso the +, -, ++, and -- operators are also overloaded appropriately.\nBecause a smart pointer is an object, it can contain additional meta-data \nand/or take additional steps not possible with a regular pointer. For example, a \nsmart pointer might contain information that allows it to recognize when the ob-\nject to which it points has been deleted and start returning a NULL address if so.\nSmart pointers can also help with object lifetime management by cooper-\nating with one another to determine the number of references to a particular \nobject. This is called reference counting. When the number of smart pointers \nthat reference a particular object drops to zero, we know that the object is no \nlonger needed, so it can be automatically deleted. This can free the program-\nmer from having to worry about object ownership and orphaned objects.\nSmart pointers have their share of problems. For one thing, they are rela-\ntively easy to implement, but they are extremely tough to get right. There are \na great many cases to handle, and the std::auto_ptr class provided by the \n14.5. Object References and World Queries\n\n\n752 \n14. Runtime Gameplay Foundation Systems\nstandard C++ library is widely recognized to be inadequate in many situa-\ntions. The Boost C++ template library provides six diﬀ erent varieties of smart \npointers:\nz scoped_ptr. A pointer to a single object with one owner.\nz scoped_array. A pointer to an array of objects with one owner.\nz shared_ptr. A pointer to an object whose lifetime is shared by multiple \nowners.\nz shared_array. A pointer to an array of objects whose lifetimes are \nshared by multiple owners.\nz weak_ptr. A pointer that does not own or automatically destroy the \nobject it references (whose lifetime is assumed to be managed by a \nshared_ptr).\nz intrusive_ptr. A pointer that implements reference counting by as-\nsuming that the pointed-to object will maintain the reference count it-\nself. Intrusive pointers are useful because they are the same size as a na-\ntive C++ pointer (because no reference-counting apparatus is required) \nand because they can be constructed directly from native pointers.\nProperly implementing a smart pointer class can be a daunting task. Have \na glance at the Boost smart pointer documentation (htt p://www.boost.org/\ndoc/libs/1_36_0/libs/smart_ptr/smart_ptr.htm) to see what I mean. All sorts of \nissues come up, including:\nz type safety of smart pointers,\nz the ability for a smart pointer to be used with an incomplete type,\nz correct smart pointer behavior when an exception occurs,\nz runtime costs, which can be high.\nI have worked on a project that att empted to implement its own smart point-\ners, and we were ﬁ xing all sorts of nasty bugs with them up until the very end \nof the project. My personal recommendation is to stay away from smart point-\ners, or if you must use them, use a mature implementation such as Boost’s \nrather than trying to roll your own.\n14.5.3. Handles\nA handle acts like a smart pointer in many ways, but it is simpler to implement \nand tends to be less prone to problems. A handle is basically an integer index \ninto a global handle table. The handle table, in turn, contains pointers to the \nobjects to which the handles refer. To create a handle, we simply search the \nhandle table for the address of the object in question and store its index in the \nhandle. To dereference a handle, the calling code simply indexes the appropri-\n\n\n753 \nate slot in the handle table and dereferences the pointer it ﬁ nds there. This is \nillustrated in Figure 14.13.\nBecause of the simple level of indirection aﬀ orded by the handle table, \nhandles are much safer and more ﬂ exible than raw pointers. If an object is \ndeleted, it can simply null out its entry in the handle table. This causes all \nexisting handles to the object to be immediately and automatically converted \nto null references. Handles also support memory relocation. When an object \nis relocated in memory, its address can be found in the handle table and up-\ndated appropriately. Again, all existing handles to the object are automatically \nupdated as a result.\nA handle can be implemented as a raw integer. However, the handle table \nindex is usually wrapped in a simple class so that a convenient interface for \ncreating and dereferencing the handle can be provided. \nHandles are prone to the possibility of referencing a stale object. For ex-\nample, let’s say we create a handle to object A, which occupies slot 17 in the \nhandle table. Later, object A is deleted, and slot 17 is nulled out. Later still, a \nnew object B is created, and it just happens to occupy slot 17 in the handle \ntable. If there are still any handles to object A lying around when object B is \ncreated, they will suddenly start referring to object B (instead of null). This is \nalmost certainly not desirable behavior.\nOne simple solution to the stale object problem is to include a unique \nobject id in each handle. That way, when a handle to object A is created, it con-\ntains not only slot index 17, but the object id “A.” When object B takes A’s place \nin the handle table, any left -over handles to A will agree on the handle index \nbut disagree on the object id. This allows stale object A handles to continue \n14.5. Object References and World Queries\nFigure 14.13.  A handle table contains raw object pointers. A handle is simply an index into \nthis table.\nNULL\nNULL\nObject1\nObject2\nObject3\nObject4\nObject5\nHandle Table\nm_handleIndex == 6\n0\n1\n2\n3\n4\n5\n6\nHandle to Object 5\n\n\n754 \n14. Runtime Gameplay Foundation Systems\nto return null when dereferenced rather than returning a pointer to object B \nunexpectedly.\nThe following code snippet shows how a simple handle class might be \nimplemented. Notice that we’ve also included the handle index in the Game\nObject class itself—this allows us to create new handles to a GameObject\nvery quickly without having to search the handle table for its address to de-\ntermine its handle table index.\n// Within the GameObject class, we store a unique id, \n// and also the object’s handle index, for efficient \n// creation of new handles.\nclass GameObject\n{\nprivate:\n \n// ...\n GameObjectId \nm_uniqueId;       // object’s unique id\n \nU32  \n \n   m_handleIndex;    // speedier handle  \n \n   // \ncreation\n \nfriend class GameObjectHandle; // access to id and  \n \n           \n// index\n \n// ...\npublic:\nGameObject() // constructor\n {\n \n \n// The unique id might come from the world editor,  \n \n \n// or it might be assigned dynamically at runtime.\n  m_uniqueId \n= AssignUniqueObjectId();\n \n \n// The handle index is assigned by finding the  \n \n \n \n// first free slot in the handle table.\n  m_handleIndex \n= FindFreeSlotInHandleTable();\n  // \n...\n }\n \n// ...\n};\n// This constant defines the size of the handle table, \n// and hence the maximum number of game objects that can \n// exist at any one time.\nstatic const U32 MAX_GAME_OBJECTS = ...;\n// This is the global handle table -- a simple array of\n// pointers to GameObjects.\nstatic GameObject* g_apGameObject[MAX_GAME_OBJECTS];\n\n\n755 \n// This is our simple game object handle class.\nclass GameObjectHandle\n{\nprivate:\n U32 \nm_handleIndex;        // index into the handle  \n \n         \n      // \ntable\n GameObjectId \nm_uniqueId;  // unique id avoids stale   \n         \n      // \nhandles\npublic:\n explicit \nGameObjectHandle(GameObject& object) :\n  m_handleIndex(object.m_handleIndex),\n  m_uniqueId(object.m_uniqueId)\n {\n }\n \n// This function dereferences the handle.\n GameObject* \nToObject() const\n {\n  GameObject* \npObject \n   = \ng_apGameObject[m_handleIndex];\n \n \nif (pObject != NULL\n \n \n&&  pObject->m_uniqueId == m_uniqueId)\n  {\n   return \npObject;\n  }\n  return \nNULL;\n }\n};\nThis example is functional but incomplete. We might want to implement copy \nsemantics, provide additional constructor variants, and so on. The entries in \nthe global handle table might contain additional information, not just a raw \npointer to each game object. And of course, a ﬁ xed-size handle table imple-\nmentation like this one isn’t the only possible design; handle systems vary \nsomewhat from engine to engine.\nWe should note that one fortunate side beneﬁ t of a global handle table is \nthat it gives us a ready-made list of all active game objects in the system. The \nglobal handle table can be used to quickly and eﬃ  ciently iterate over all game \nobjects in the world, for example. It can also make implementing other kinds \nof queries easier in some cases.\n14.5.4. Game Object Queries\nEvery game engine provides at least a few ways to ﬁ nd game objects at run-\ntime. We’ll call these searches game object queries . The simplest type of query is \nto ﬁ nd a particular game object by its unique id. However, a real game engine \n14.5. Object References and World Queries\n\n\n756 \n14. Runtime Gameplay Foundation Systems\nmakes many other types of game object queries. Here are just a few examples \nof the kinds of queries a game developer might want to make:\nz Find all enemy characters with line of sight to the player.\nz Iterate over all game objects of a certain type.\nz Find all destructible game objects with more than 80% health.\nz Transmit damage to all game objects within the blast radius of an \nexplosion.\nz Iterate over all objects in the path of a bullet or other projectile, in near-\nest-to-farthest order.\nThis list could go on for many pages, and of course its contents are highly \ndependent upon the design of the particular game being made.\nFor maximum ﬂ exibility in performing game object queries, we could \nimagine a general-purpose game object database, complete with the ability to \nformulate arbitrary queries using arbitrary search criteria. Ideally, our game \nobject database would perform all of these queries extremely eﬃ  ciently and \nrapidly, making maximum use of whatever hardware and soft ware resources \nare available.\nIn reality, such an ideal combination of ﬂ exibility and blinding speed is \ngenerally not possible. Instead, game teams usually determine which types \nof queries are most likely to be needed during development of the game, and \nspecialized data structures are implemented to accelerate those particular \ntypes of queries. As new queries become necessary, the engineers either lever-\nage preexisting data structures to implement them, or they invent new ones \nif suﬃ  cient speed cannot be obtained. Here are a few examples of specialized \ndata structures that can accelerate speciﬁ c types of game object queries:\nz Finding game objects by unique id . Pointers or handles to the game objects \ncould be stored in a hash table or binary search tree keyed by unique \nid.\nz Iterating over all objects that meet a particular criterion. The game objects \ncould be presorted into linked lists based on various criteria (presuming \nthe criteria are known a priori). For example, we might construct a list of \nall game objects of a particular type, maintain a list of all objects within \na particular radius of the player, etc.\nz Finding all objects in the path of a projectile or with line of sight to some target \npoint. The collision system is usually leveraged to perform these kinds of \ngame object queries. Most collision systems provide ultra-fast ray casts, \nand some also provide the ability to cast other shapes such as spheres or \narbitrary convex volumes into the world to determine what they hit.\n\n\n757 \nz Finding all objects within a given region or radius. We might consider stor-\ning our game objects in some kind of spatial hash data structure. This \ncould be as simple as a horizontal grid placed over the entire game \nworld or something more sophisticated, such as a quadtree, octt ree, kd-\ntree, or other data structure that encodes spatial proximity.\n14.6. Updating Game Objects in Real Time\n Every game engine, from the simplest to the most complex, requires some \nmeans of updating the internal state of every game object over time. The state \nof a game object can be deﬁ ned as the values of all its att ributes (sometimes \ncalled its properties, and called data members in the C++ language). For example, \nthe state of the ball in Pong is described by its (x, y) position on the screen \nand its velocity (speed and direction of travel). Because games are dynamic, \ntime-based simulations, a game object’s state describes its conﬁ guration at one \nspeciﬁ c instant in time. In other words, a game object’s notion of time is discrete \nrather than continuous. (However, as we’ll see, it’s helpful to think of the ob-\njects’ states as changing continuously and then being sampled discretely by \nthe engine, because it helps you to avoid some common pitfalls.)\nIn the following discussions, we’ll use the symbol Si(t) to denote the state \nof object i at an arbitrary time t. The use of vector notation here is not strictly \nmathematically correct, but it reminds us that a game object’s state acts like \na heterogeneous n-dimensional vector, containing all sorts of information of \nvarious data types. We should note that this usage of the term “state” is not \nthe same as the states in a ﬁ nite state machine . A game object may very well \nbe implemented in terms of one—or many—ﬁ nite state machines, but in that \ncase, a speciﬁ cation of the current state of each FSM would merely be a part of \nthe game object’s overall state vector S(t).\nMost low-level engine subsystems (rendering, animation, collision, \nphysics, audio, and so on) require periodic updating, and the game object \nsystem is no exception. As we saw in Chapter 7, updating is usually done via \na single master loop called the game loop (or possibly via multiple game loops , \neach running in a separate thread ). Virtually all game engines update game \nobject states as part of their main game loop—in other words, they treat the \ngame object model as just another engine subsystem that requires periodic \nservicing.\nGame object updating can therefore be thought of as the process of de-\ntermining the state of each object at the current time Si(t) given its state at a \nprevious time Si(t – Δt). Once all object states have been updated, the current \ntime t becomes the new previous time (t – Δt), and this process repeats for \n14.6. Updating Game Objects in Real Time\n\n\n758 \n14. Runtime Gameplay Foundation Systems\nas long as the game is running. Usually, one or more clocks are maintained \nby the engine—one that tracks real time exactly and possibly others that \nmay or may not correspond to real time. These clocks provide the engine \nwith the absolute time t and/or with the change in time Δt from iteration \nto iteration of the game loop. The clock that drives the updating of game \nobject states is usually permitt ed to diverge from real time. This allows the \nbehaviors of the game objects to be paused, slowed down, sped up, or even \nrun in reverse—whatever is required in order to suit the needs of the game \ndesign. These features are also invaluable for debugging and development \nof the game.\nAs we mentioned in Chapter 1, a game object updating system is an ex-\nample of what is known as a dynamic, real-time, agent-based computer simulation \nin computer science. Game object updating systems also exhibit some aspects \nof discrete event simulations (see Section 14.7 for more details on events). These \nare well-researched areas of computer science, and they have many appli-\ncations outside the ﬁ eld of interactive entertainment. Games are one of the \nmore-complex kinds of agent-based simulation—as we’ll see, updating game \nobject states over time in a dynamic, interactive virtual environment can be \nsurprisingly diﬃ  cult to get right. Game programmers can learn a lot about \ngame object updating by studying the wider ﬁ eld of agent-based and discrete \nevent simulations. And researchers in those ﬁ elds can probably learn a thing \nor two from game engine design as well!\nAs with all high-level game engine systems, every engine takes a slightly \n(or sometimes radically) diﬀ erent approach. However, as before, most game \nteams encounter a common set of problems, and certain design patt erns tend \nto crop up again and again in virtually every engine. In this section, we’ll \ninvestigate these common problems and some common solutions to them. \nPlease bear in mind that game engines may exist that employ very diﬀ er-\nent solutions to the ones described here, and some game designs face unique \nproblems that we can’t possibly cover here.\n14.6.1. A Simple Approach (That Doesn’t Work)\nThe simplest way to update the states of a collection of game objects is to \niterate over the collection and call a virtual function, named something like \nUpdate(), on each object in turn. This is typically done once during each \niteration of the main game loop (i.e., once per frame). Game object classes can \nprovide custom implementations of the Update() function in order to per-\nform whatever tasks are required to advance the state of that type of object \nto the next discrete time index. The time delta from the previous frame can \nbe passed to the update function so that objects can take proper account of \n\n\n759 \nthe passage of time. At its simplest, then, our Update() function’s signature \nmight look something like this:\nvirtual void Update(float dt);\nFor the purposes of the following discussions, we’ll assume that our en-\ngine employs a monolithic object hierarchy, in which each game object is rep-\nresented by a single instance of a single class. However, we can easily extend \nthe ideas here to virtually any object-centric design. For example, to update a \ncomponent-based object model, we could call Update() on every component \nthat makes up each game object, or we could call Update() on the “hub” \nobject and let it update its associated components as it sees ﬁ t. We can also ex-\ntend these ideas to property-centric designs, by calling some sort of Update()\nfunction on each property instance every frame.\nThey say that the devil is in the details, so let’s investigate two important \ndetails here. First, how should we maintain the collection of all game objects? \nAnd second, what kinds of things should the Update() function be respon-\nsible for doing?\n14.6.1.1. Maintaining a Collection of Active Game Objects\nThe collection of active game objects is oft en maintained by a singleton \nmanager class, perhaps named something like GameWorld or GameObject\nManager. The collection of game objects generally needs to be dynamic, be-\ncause game objects are spawned and destroyed as the game is played. Hence a \nlinked list of pointers, smart pointers, or handles to game objects is one simple \nand eﬀ ective approach. (Some game engines disallow dynamic spawning and \ndestroying of game objects; such engines can use a statically-sized array of \ngame object pointers, smart pointers, or handles rather than a linked list.) As \nwe’ll see below, most engines use more-complex data structures to keep track \nof their game objects rather than just a simple, ﬂ at linked list. But for the time \nbeing, we can visualize the data structure as a linked list for simplicity.\n14.6.1.2. Responsibilities of the Update() Function\nA game object’s Update() function is primarily responsible for determining \nthe state of that game object at the current discrete time index Si(t) given its \nprevious state Si(t – Δt). Doing this may involve applying a rigid body dynam-\nics simulation to the object, sampling a preauthored animation, reacting to \nevents that have occurred during the current time step, and so on.\nMost game objects interact with one or more engine subsystems. They \nmay need to animate , be rendered, emit particle eﬀ ects, play audio , collide \nwith other objects and static geometry, and so on. Each of these systems has \nan internal state that must also be updated over time, usually once or a few \n14.6. Updating Game Objects in Real Time\n\n\n760 \n14. Runtime Gameplay Foundation Systems\ntimes per frame. It might seem reasonable and intuitive to simply update all of \nthese subsystems directly from within the game object’s Update() function. \nFor example, consider the following hypothetical update function for a Tank\nobject:\nvirtual void Tank::Update(float dt)\n{\n // \nUpdate the state of the tank itself.\n MoveTank(dt);\n DeflectTurret(dt);\n FireIfNecessary();\n \n// Now update low-level engine subsystems on behalf\n \n// of this tank. (NOT a good idea... see below!)\n m_pAnimationComponent->Update(dt);\n m_pCollisionComponent->Update(dt);\n m_pPhysicsComponent->Update(dt);\n m_pAudioComponent->Update(dt);\n m_pRenderingComponent->draw();\n}\nGiven that our Update() functions are structured like this, the game loop \ncould be driven almost entirely by the updating of the game objects, like this:\nwhile (true)\n{\n PollJoypad();\n \nfloat dt = g_gameClock.CalculateDeltaTime();\n \nfor (each gameObject)\n {\n \n \n// This hypothetical Update() function updates\n \n \n// all engine subsystems!\ngameObject.Update(dt);\n }\n g_renderingEngine.SwapBuffers();\n}\nHowever att ractive the simple approach to object updating shown above \nmay seem, it is usually not viable in a commercial-grade game engine. In the \nfollowing sections, we’ll explore some of the problems with this simplistic ap-\nproach and investigate common ways in which each problem can be solved.\n\n\n761 \n14.6.2. Performance Constraints and Batched Updates\nMost low-level engine systems have extremely stringent performance con-\nstraints. They operate on a large quantity of data, and they must do a large \nnumber of calculations every frame as quickly as possible. As a result, most \nengine systems beneﬁ t from batched updating. For example, it is usually far \nmore eﬃ  cient to update a large number of animations in one batch than it is \nto update each object’s animation interleaved with other unrelated operations, \nsuch as collision detection, physical simulation, and rendering.\nIn most commercial game engines, each engine subsystem is updated di-\nrectly or indirectly by the main game loop rather than being updated on a \nper-game object basis from within each object’s Update() function. If a game \nobject requires the services of a particular engine subsystem, it asks that sub-\nsystem to allocate some subsystem-speciﬁ c state information on its behalf. For \nexample, a game object that wishes to be rendered via a triangle mesh might \nrequest the rendering subsystem to allocate a mesh instance for its use. (A mesh \ninstance represents a single instance of a triangle mesh—it keeps track of the \nposition, orientation, and scale of the instance in world space whether or not \nit is visible, per-instance material data, and any other per-instance information \nthat may be relevant.) The rendering engine maintains a collection of mesh in-\nstances internally. It can manage the mesh instances however it sees ﬁ t in order \nto maximize its own runtime performance. The game object controls how it is \nrendered by manipulating the properties of the mesh instance object, but the \ngame object does not control the rendering of the mesh instance directly. In-\nstead, aft er all game objects have had a chance to update themselves, the ren-\ndering engine draws all visible mesh instances in one eﬃ  cient batch update.\nWith batched updating, a particular game object’s Update() function, \nsuch as that of our hypothetical tank object, might look more like this:\nvirtual void Tank::Update(float dt)\n{\n // \nUpdate the state of the tank itself.\n MoveTank(dt);\n DeflectTurret(dt);\n FireIfNecessary();\n \n// Control the properties of my various engine  \n \n \n \n// subsystem components, but do NOT update \n \n// them here...\n \nif (justExploded)\n {\n  m_pAnimationComponent->PlayAnimation(\"explode\");\n }\n14.6. Updating Game Objects in Real Time\n\n\n762 \n14. Runtime Gameplay Foundation Systems\n \nif (isVisible)\n {\n  m_pCollisionComponent->Activate();\n  m_pRenderingComponent->Show();\n }\n else\n {\n  m_pCollisionComponent->Deactivate();\n  m_pRenderingComponent->Hide();\n }\n \n// etc. \n}\nThe game loop then ends up looking more like this:\nwhile (true)\n{\n PollJoypad();\n \nfloat dt = g_gameClock.CalculateDeltaTime();\n \nfor (each gameObject)\n {\ngameObject.Update(dt);\n }\n g_animationEngine.\nUpdate(dt);\n g_physicsEngine.\nSimulate(dt);\n g_collisionEngine.\nDetectAndResolveCollisions(dt);\n g_audioEngine.\nUpdate(dt);\n g_renderingEngine.\nRenderFrameAndSwapBuffers();\n}\nBatched updating provides many performance beneﬁ ts, including but not \nlimited to:\nz Maximal cache coherency . Batched updating allows an engine subsys-\ntem to achieve maximum cache coherency because its per-object data \nis maintained internally and can be arranged in a single, contiguous \nregion of RAM.\nz Minimal duplication of computations. Global calculations can be done once \nand reused for many game objects rather than being redone for each \nobject.\n\n\n763 \nz Reduced reallocation of resources. Engine subsystems oft en need to allocate \nand manage memory and/or other resources during their updates. If \nthe update of a particular subsystem is interleaved with those of other \nengine subsystems, these resources must be freed and reallocated for \neach game object that is processed. But if the updates are batched, the \nresources can be allocated once per frame and reused for all objects in \nthe batch.\nz Eﬃ  cient pipelining . Many engine subsystems perform a virtually identi-\ncal set of calculations on each and every object in the game world. When \nupdates are batched, new optimizations become possible, and special-\nized hardware resources can be leveraged. For example, the PLAY-\nSTATION 3 provides a batt ery of high-speed microprocessors known \nas SPUs, each of which has its own private high-speed memory area. \nWhen processing a batch of animations, the pose of one character can be \ncalculated while we simultaneously DMA the data for the next charac-\nter into SPU memory. This kind of parallelism cannot be achieved when \nprocessing each object in isolation.\nPerformance beneﬁ ts aren’t the only reason to favor a batch updating ap-\nproach. Some engine subsystems simply don’t work at all when updated on \na per-object basis. For example, if we are trying to resolve collisions within \na system of multiple dynamic rigid bodies, a satisfactory solution cannot be \nfound in general by considering each object in isolation. The interpenetrations \nbetween these objects must be resolved as a group, either via an iterative ap-\nproach or by solving a linear system.\n14.6.3. Object and Subsystem Interdependencies\nEven if we didn’t care about performance, a simplistic per-object updating ap-\nproach breaks down when game objects depend on one another. For example, \na human character might be holding a cat in her arms. In order to calculate \nthe world-space pose of the cat’s skeleton, we ﬁ rst need to calculate the world-\nspace pose of the human. This implies that the order in which objects are up-\ndated is important to the proper functioning of the game.\nAnother related problem arises when engine subsystems depend on one \nanother. For example, a rag doll physics simulation must be updated in con-\ncert with the animation engine. Typically, the animation system produces an \nintermediate, local-space skeletal pose. These joint transforms are converted \nto world space and applied to a system of connected rigid bodies that approxi-\nmate the skeleton within the physics system. The rigid bodies are simulated \nforward in time by the physics system, and then the ﬁ nal resting places of the \n14.6. Updating Game Objects in Real Time\n",
      "page_number": 764,
      "chapter_number": 38,
      "summary": "This chapter covers segment 38 (pages 764-785). Key topics include object, games, and memory. Runtime Gameplay Foundation Systems\nfreed by simply resett ing the stack pointer to the top of the LSR data block.",
      "keywords": [
        "game object",
        "game",
        "Object",
        "game world",
        "Game Object Queries",
        "Updating Game Objects",
        "game object states",
        "world",
        "Streaming Game Worlds",
        "Gameplay Foundation Systems",
        "update game object",
        "world chunk",
        "game world chunks",
        "memory",
        "game object spawning"
      ],
      "concepts": [
        "object",
        "games",
        "memory",
        "memories",
        "updated",
        "update",
        "data",
        "worlds",
        "engines",
        "chunks"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 10,
          "title": "Segment 10 (pages 86-98)",
          "relevance_score": 0.67,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 47,
          "title": "Segment 47 (pages 448-460)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 26,
          "title": "Segment 26 (pages 243-252)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 21,
          "title": "Segment 21 (pages 193-203)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 786-808)",
      "start_page": 786,
      "end_page": 808,
      "detection_method": "synthetic",
      "content": "764 \n14. Runtime Gameplay Foundation Systems\njoints are applied back to their corresponding joints in the skeleton. Finally, \nthe animation system calculates the ﬁ nal world-space pose and skinning ma-\ntrix palett e. So once again, the updating of the animation and physics systems \nmust occur in a particular order in order to produce correct results. These \nkinds of inter-subsystem dependencies are commonplace in game engine de-\nsign.\n14.6.3.1. Phased Updates\n To account for inter-subsystem dependencies, we can explicitly code our en-\ngine subsystem updates in the proper order within the main game loop. For \nexample, to handle the interplay between the animation system and rag doll \nphysics, we might write something like this:\nwhile (true) // main game loop\n{\n \n// ...\n g_animationEngine.\nCalculateIntermediatePoses(dt);\n g_ragdollSystem.\nApplySkeletonsToRagDolls();\n g_physicsEngine.\nSimulate(dt); // runs ragdolls too\n g_collisionEngine.\nDetectAndResolveCollisions(dt);\n g_ragdollSystem.\nApplyRagDollsToSkeletons();\n g_animationEngine.\nFinalizePoseAndMatrixPalette();\n \n// ...\n}\nWe must be careful to update the states of our game objects at the right \ntime during the game loop. This is oft en not as simple as calling a single Up-\ndate() function per game object per frame. Game objects may depend upon \nthe intermediate results of calculations performed by various engine subsys-\ntems. For example, a game object might request that animations be played \nprior to the animation system running its update. However, that same object \nmay also want to procedurally adjust the intermediate pose generated by the \nanimation system prior to that pose being used by the rag doll physics system \nand/or the ﬁ nal pose and matrix palett e being generated. This implies that the \nobject must be updated twice, once before the animation calculates its inter-\nmediate poses and once aft erward.\nMany game engines allow game objects to update at multiple points \nduring the frame. For example, an engine might update game objects three \ntimes—once before animation blending, once aft er  animation blending but \n\n\n765 \nprior to ﬁ nal pose generation, and once aft er ﬁ nal pose generation. This can \nbe accomplished by providing each game object class with three virtual func-\ntions that act as “hooks.” In such a system, the game loop ends up looking \nsomething like this:\nwhile (true) // main game loop\n{\n \n// ...\n \nfor (each gameObject)\n {\n  gameObject.\nPreAnimUpdate(dt);\n }\n g_animationEngine.\nCalculateIntermediatePoses(dt);\n \nfor (each gameObject)\n {\n  gameObject.\nPostAnimUpdate(dt);\n }\n g_ragdollSystem.\nApplySkeletonsToRagDolls();\n g_physicsEngine.\nSimulate(dt); // runs ragdolls too\n g_collisionEngine.\nDetectAndResolveCollisions(dt);\n g_ragdollSystem.\nApplyRagDollsToSkeletons();\n g_animationEngine.\nFinalizePoseAndMatrixPalette();\n \nfor (each gameObject)\n {\n  gameObject.\nFinalUpdate(dt);\n }\n \n// ...\n}\nWe can provide our game objects with as many update phases as we see \nﬁ t. But we must be careful, because iterating over all game objects and calling \na virtual function on each one can be expensive. Also, not all game objects \nrequire all update phases—iterating over objects that don’t require a particu-\nlar phase is a pure waste of CPU bandwidth. One way to minimize the cost \nof iteration is to maintain multiple linked lists of game objects—one for each \nupdate phase. If a particular object wants to be included in one of the update \n14.6. Updating Game Objects in Real Time\n\n\n766 \n14. Runtime Gameplay Foundation Systems\nphases, it adds itself to the corresponding linked list. This avoids having to \niterate over objects that are not interested in a particular update phase.\n14.6.3.2. Bucketed Updates\n In the presence of inter-object dependencies, the phased updates technique de-\nscribed above must be adjusted a litt le. This is because inter-object dependen-\ncies can lead to conﬂ icting rules governing the order of updating. For exam-\nple, let’s imagine that object B is being held by object A. Further, let’s assume \nthat we can only update object B aft er A has been fully updated, including the \ncalculation of its ﬁ nal world-space pose and matrix palett e. This conﬂ icts with \nthe need to batch animation updates of all game objects together in order to \nallow the animation system to achieve maximum throughput.\nInter-object dependencies can be visualized as a forest of dependency \ntrees. The game objects with no parents (no dependencies on any other object) \nrepresent the roots of the forest. An object that depends directly on one of \nthese root objects resides in the ﬁ rst tier of children in one of the trees in the \nforest. An object that depends on a ﬁ rst-tier child becomes a second-tier child, \nand so on. This is illustrated in Figure 14.14.\nOne solution to the problem of conﬂ icting update order requirements is \nto collect objects into independent groups, which we’ll call buckets here for \nlack of a bett er name. The ﬁ rst bucket consists of all root objects in the forest. \nThe second bucket is comprised of all ﬁ rst-tier children. The third bucket con-\ntains all second-tier children, and so on. For each bucket, we run a complete \nupdate of the game objects and the engine systems, complete with all update \nFigure 14.14.  Inter-object update order dependencies can be viewed as a forest of depen-\ndency trees.\n\n\n767 \nphases. Then we repeat the entire process for each bucket until there are no \nmore buckets.\nIn theory, the depths of the trees in our dependency forest are unbounded. \nHowever, in practice, they are usually quite shallow. For example, we might \nhave characters holding weapons, and those characters might or might not be \nriding on a moving platform or a vehicle. To implement this, we only need \nthree tiers in our dependency forest, and hence only three buckets: one for \nplatforms/vehicles, one for characters, and one for the weapons in the charac-\nters’ hands. Many game engines explicitly limit the depth of their dependency \nforest so that they can use a ﬁ xed number of buckets (presuming they use a \nbucketed approach at all—there are of course many other ways to architect a \ngame loop).\nHere’s what a bucketed, phased, batched update loop might look like:\nvoid UpdateBucket(Bucket bucket)\n{\n \n// ...\n \nfor (each gameObject in bucket)\n {\n  gameObject.PreAnimUpdate(dt);\n }\n g_animationEngine.CalculateIntermediatePoses\n  (\nbucket, dt);\n \nfor (each gameObject in bucket)\n {\n  gameObject.PostAnimUpdate(dt);\n }\n g_ragdollSystem.ApplySkeletonsToRagDolls(\nbucket);\n g_physicsEngine.Simulate(\nbucket, dt); // runs   \n \n \n             \n   // \nragdolls too\n g_collisionEngine.DetectAndResolveCollisions\n  (\nbucket, dt);\n g_ragdollSystem.ApplyRagDollsToSkeletons(\nbucket);\n g_animationEngine.FinalizePoseAndMatrixPalette\n  (\nbucket);\n \nfor (each gameObject in bucket)\n {\n  gameObject.FinalUpdate(dt);\n }\n14.6. Updating Game Objects in Real Time\n\n\n768 \n14. Runtime Gameplay Foundation Systems\n \n// ...\n}\nvoid RunGameLoop()\n{\n \nwhile (true)\n {\n  // \n...\nUpdateBucket(g_bucketVehiclesAndPlatforms);\nUpdateBucket(g_bucketCharacters);\nUpdateBucket(g_bucketAttachedObjects);\n  // \n...\n  g_renderingEngine.RenderSceneAndSwapBuffers();\n }\n}\nIn practice, things might a bit more complex than this. For example, some \nengine subsystems like the physics engine might not support the concept of \nbuckets, perhaps because they are third-party SDKs or because they cannot \nbe practically updated in a bucketed manner. However, this bucketed update \nis essentially what we used at Naughty Dog to implement Uncharted: Drake’s \nFortune and are using again for our upcoming title, Uncharted 2: Among Thieves. \nSo it’s a method that has proven to be practical and reasonably eﬃ  cient.\n14.6.3.3. Object State Inconsistencies and One-Frame-Off Lag\nLet’s revisit game object updating, but this time thinking in terms of each ob-\nject’s local notion of time. We said in Section 14.6 that the state of game object i \nat time t can be denoted by a state vector Si(t). When we update a game object, \nwe are converting its previous state vector Si(t1) into a new current state vector \nSi(t2) (where t2 = t1 + Δt).\nIn theory, the states of all game objects are updated from time t1 to time \nt2 instantaneously and in parallel, as depicted in Figure 14.15. However, in \npractice, we can only update the objects one by one—we must loop over each \ngame object and call some kind of update function on each one in turn. If \nwe were to stop the program half-way through this update loop, half of our \ngame objects’ states would have been updated to Si(t2), while the remaining \nhalf would still be in their previous states, Si(t1). This implies that if we were \nto ask two of our game objects what the current time is during the update \nloop, they may or may not agree! What’s more, depending on where exactly \nwe interrupt the update loop, the objects may all be in a partially updated \n\n\n769 \nstate. For example, animation pose blending may have been run, but physics \nand collision resolution may not yet have been applied. This leads us to the \nfollowing rule:\nThe states of all game objects are consistent before and aft er the \nupdate loop, but they may be inconsistent during it.\nThis is illustrated in Figure 14.16.\nThe inconsistency of game object states during the update loop is a major \nsource of confusion and bugs, even among professionals within the game in-\ndustry. The problem rears its head most oft en when game objects query one \nt1\nt\nSA\nObjectA\nSA\nObjectB\nSB\nObjectC\nSC\nObjectD\nSD\nt2\nSB\nSC\nSD\nΔt\nFigure 14.15. In theory, the states of all game objects are updated instantaneously and in \nparallel during each iteration of the game loop.\n14.6. Updating Game Objects in Real Time\nFigure 14.16.  In practice, the states of the game objects are updated one by one. This means \nthat at some arbitrary moment during the update loop, some objects will think the current \ntime is t2 while others think it is still t1. Some objects may be only partially updated, so their \nstates will be internally inconsistent. In effect, the state of such an object lies at a point be-\ntween t1 and t2.\nt1\nt\nSA\nObjectA\nObjectB\nSA\nObjectC\nObjectD\nSC\nt2\nSB\nSD\nSB\nSC\n\n\n770 \n14. Runtime Gameplay Foundation Systems\nanother for state information during the update loop (which implies that there \nis a dependency between them). For example, if object B looks at the velocity \nof object A in order to determine its own velocity at time t, then the program-\nmer must be clear about whether he or she wants to read the previous state of \nobject A, SA(t1), or the new state, SA(t2). If the new state is needed but object A \nhas not yet been updated, then we have an update order problem that can lead \nto a class of bugs known as one-frame-oﬀ  lags . In this type of bug, the state of \none object lags one frame behind the states of its peers, which manifests itself \non-screen as a lack of synchronization between game objects.\n14.6.3.4. Object State Caching\n As described above, one solution to this problem is to group the game ob-\njects into buckets (Section 14.6.3.2). One problem with a simple bucketed up-\ndate approach is that it imposes somewhat arbitrary limitations on the way in \nwhich game objects are permitt ed to query one another for state information. \nIf a game object A wants the updated state vector SB(t2) of another object B, then \nobject B must reside in a previously updated bucket. Likewise, if object A wants \nthe previous state vector SB(t1) of object B, then object B must reside in a yet-to-\nbe-updated bucket. Object A should never ask for the state vector of an object \nwithin its own bucket, because as we stated in the rule above, those state vec-\ntors may be only partially updated.\nOne way to improve consistency is to arrange for each game object to \ncache its previous state vector Si(t1) while it is calculating its new state vector \nSi(t2) rather than overwriting it in-place during its update. This has two im-\nmediate beneﬁ ts. First, it allows any object to safely query the previous state \nvector of any other object without regard to update order. Second, it guar-\nantees that a totally consistent state vector (Si(t1)) will always be available, \neven during the update of the new state vector. To my knowledge there is no \nstandard terminology for this technique, so I’ll call it state caching for lack of \na bett er name.\nAnother beneﬁ t of state caching is that we can linearly interpolate be-\ntween the previous and next states in order to approximate the state of an \nobject at any moment between these two points in time. The Havok physics \nengine maintains the previous and current state of every rigid body in the \nsimulation for just this purpose.\nThe downside of state caching is that it consumes twice the memory of the \nupdate-in-place approach. It also only solves half the problem, because while \nthe previous states at time t1 are fully consistent, the new states at time t2 still \nsuﬀ er from potential inconsistency. Nonetheless, the technique can be useful \nwhen applied judiciously.\n\n\n771 \n14.6.3.5. Time-Stamping\n One easy and low-cost way to improve the consistency of game object states \nis to time-stamp them. It is then a trivial matt er to determine whether a game \nobject’s state vector corresponds to its conﬁ guration at a previous time or the \ncurrent time. Any code that queries the state of another game object during \nthe update loop can assert or explicitly check the time stamp to ensure that the \nproper state information is being obtained.\nTime-stamping does not address the inconsistency of states during the \nupdate of a bucket. However, we can set a global or static variable to reﬂ ect \nwhich bucket is currently being updated. Presumably every game object \n“knows” in which bucket it resides. So we can check the bucket of a queried \ngame object against the currently updating bucket and assert that they are not \nequal in order to guard against inconsistent state queries.\n14.6.4. Designing for Parallelism\nIn Section 7.6, we introduced a number of approaches that allow a game en-\ngine to take advantage of the parallel processing resources that have become \nthe norm in recent gaming hardware. How, then, does parallelism aﬀ ect the \nway in which game object states are updated?\n14.6.4.1. Parallelizing the Game Object Model Itself\n Game object models are notoriously diﬃ  cult to parallelize, for a few reasons. \nGame objects tend to be highly interdependent upon one another and upon \nthe data used and/or generated by numerous engine subsystems. Game ob-\njects communicate with one another, sometimes multiple times during the up-\ndate loop, and the patt ern of communication can be unpredictable and highly \nsensitive to the inputs of the player and the events that are occurring in the \ngame world. This makes it diﬃ  cult to process game object updates in mul-\ntiple threads , for example, because the amount of thread synchronization that \nwould be required to support inter-object communication is usually prohibi-\ntive from a performance standpoint. And the practice of peeking directly into \na foreign game object’s state vector makes it impossible to DMA a game object \nto the isolated memory of a coprocessor, such as the PLAYSTATION 3’s SPU, \nfor updating.\nThat said, game object updating can theoretically be done in parallel. \nTo make it practical, we’d need to carefully design the entire object model \nto ensure that game objects never peek directly into the state vectors of oth-\ner game objects. All inter-object communication would have to be done via \nmessage-passing, and we’d need an eﬃ  cient system for passing messages be-\n14.6. Updating Game Objects in Real Time\n\n\n772 \n14. Runtime Gameplay Foundation Systems\ntween game objects even when those objects reside in totally separate memory \nspaces  or are being processed by diﬀ erent physical CPU cores. Some research \nhas been done into using a distributed programming language, such as Er-\nicsson’s Erlang (htt p://www.erlang.org), to code game object models. Such \nlanguages provide built-in support for parallel processing and message pass-\ning and handle context switching between threads much more eﬃ  ciently and \nquickly than in a language like C or C++, and their programming idioms help \nprogrammers to never “break the rules” that allow concurrent, distributed, \nmultiple agent designs to function properly and eﬃ  ciently.\n14.6.4.2. Interfacing with Concurrent Engine Subsystems\n Although sophisticated, concurrent, distributed object models are theoreti-\ncally feasible and are an area of extremely interesting research, at present \nmost game teams do not use them. Instead, most game teams leave the ob-\nject model in a single thread and use an old-fashioned game loop to update \nthem. They focus their att ention instead on parallelizing many of the lower-\nlevel engine systems upon which the game objects depend. This gives teams \nthe biggest “bang for their buck,” because low-level engine subsystems tend \nto be more performance-critical than the game object model. This is because \nlow-level subsystems must process huge volumes of data every frame, while \nthe amount of CPU power used by the game object model is oft en somewhat \nsmaller. This is an example of the 80-20 rule in action.\nOf course, using a single-threaded game object model does not mean that \ngame programmers can be totally oblivious to parallel programming issues. \nThe object model must still interact with engine subsystems that are them-\nselves running concurrently with the object model. This paradigm shift  re-\nquires game programmers to avoid certain programming paradigms that may \nhave served them well in the pre-parallel-processing era and adopt some new \nones in their place.\nProbably the most important shift  a game programmer must make is to \nbegin thinking asynchronously . As described in Section 7.6.5, this means that \nwhen a game object requires a time-consuming operation to be performed, it \nshould avoid calling a blocking function—a function that does its work direct-\nly in the context of the calling thread, thereby blocking that thread until the \nwork has been completed. Instead, whenever possible, large or expensive jobs \nshould be requested by calling a non-blocking function—a function that sends \nthe request to be executed by another thread, core, or processor and then im-\nmediately returns control to the calling function. The main game loop can \nproceed with other unrelated work, including updating other game objects, \nwhile the original object waits for the results of its request. Later in the same \n\n\n773 \nframe, or next frame, that game object can pick up the results of its request and \nmake use of them.\nBatching is another shift  in thinking for game programmers. As we men-\ntioned in Section 14.6.2, it is more eﬃ  cient to collect similar tasks into batches \nand perform them en masse than it is to run each task independently. This \napplies to the process of updating game object states as well. For example, if \na game object needs to cast 100 rays into the collision world for various pur-\nposes, it is best if those ray cast requests can be queued up and executed as \none big batch. If an existing game engine is being retroﬁ tt ed for parallelism, \nthis oft en requires code to be rewritt en so that it batches requests rather than \ndoing them individually.\nOne particularly tricky aspect of converting synchronous, unbatched code \nto use an asynchronous, batched approach is determining when during the \ngame loop (a) to kick oﬀ  the request and (b) to wait for and utilize the results. \nIn doing this, it is oft en helpful to ask ourselves the following questions:\nz How early can we kick oﬀ  this request? The earlier we make the request, the \nmore likely it is to be done when we actually need the results—and this \nmaximizes CPU utilization by helping to ensure that the main thread \nis never idle waiting for an asynchronous request to complete. So for \nany given request, we should determine the earliest point during the \nframe at which we have enough information to kick it oﬀ , and kick it \nthere.\nz How long can we wait before we need the results of this request? Perhaps we \ncan wait until later in the update loop to do the second half of an opera-\ntion. Perhaps we can tolerate a one-frame lag and use last frame’s results \nto update the object’s state this frame. (Some subsystems like AI can \ntolerate even longer lag times because they update only every few sec-\nonds.) In many circumstances, code that uses the results of a request can \nin fact be deferred until later in the frame, given a litt le thought, some \ncode re-factoring, and possibly some additional caching of intermediate \ndata.\n14.7. Events and Message-Passing\nGames are inherently event-driven. An event is anything of interest that hap-\npens during gameplay. An explosion going oﬀ , the player being sighted by an \nenemy, a health pack gett ing picked up—these are all events. Games generally \nneed a way to (a) notify interested game objects when an event occurs and (b) \narrange for those objects to respond to interesting events in various ways—we \n14.6. Updating Game Objects in Real Time\n\n\n774 \n14. Runtime Gameplay Foundation Systems\ncall this handling the event. Diﬀ erent types of game objects will respond in dif-\nferent ways to an event. The way in which a particular type of game object re-\nsponds to an event is a crucial aspect of its behavior, just as important as how \nthe object’s state changes over time in the absence of any external inputs. For \nexample, the behavior of the ball in Pong is governed in part by its velocity, in \npart by how it reacts to the event of striking a wall or paddle and bouncing oﬀ , \nand in part by what happens when the ball is missed by one of the players.\n14.7.1. The Problem with Statically Typed Function Binding\nOne simple way to notify a game object that an event has occurred is to sim-\nply call a method (member function) on the object. For example, when an \nexplosion goes oﬀ , we could query the game world for all objects within the \nexplosion’s damage radius and then call a virtual function named something \nlike OnExplosion() on each one. This is illustrated by the following pseudo-\ncode:\nvoid Explosion::Update()\n{\n \n// ...\n \nif (ExplosionJustWentOff())\n {\n  GameObjectCollection \ndamagedObjects;\n  g_world.\nQueryObjectsInSphere(GetDamageSphere(),\ndamagedObjects);\n  for \n(each object in damagedObjects)\n  {\n   object.\nOnExplosion(*this);\n  }\n }\n \n// ...\n}\nThe call to OnExplosion() is an example of statically typed late function \nbinding . Function binding is the process of determining which function im-\nplementation to invoke at a particular call location—the implementation is, \nin eﬀ ect, bound to the call. Virtual functions, such as our OnExplosion()\nevent-handling function, are said to be late-bound . This means that the com-\npiler doesn’t actually know which of the many possible implementations of \nthe function is going to be invoked at compile time—only at runtime, when \nthe type of the target object is known, will the appropriate implementation \nbe invoked. We say that a virtual function call is statically typed because the \n\n\n775 \ncompiler does know which implementation to invoke given a particular object \ntype. It knows, for example, that Tank::OnExplosion() should be called \nwhen the target object is a Tank and that Crate::OnExplosion() should be \ncalled when the object is a Crate.\nThe problem with statically typed function binding is that it introduces \na degree of inﬂ exibility into our implementation. For one thing, the virtual \nOnExplosion() function requires all game objects to inherit from a common \nbase class. Moreover, it requires that base class to declare the virtual function\nOnExplosion(), even if not all game objects can respond to explosions. In \nfact, using statically typed virtual functions as event handlers would require \nour base GameObject class to declare virtual functions for all possible events \nin the game! This would make adding new events to the system diﬃ  cult. It \nprecludes events from being created in a data-driven manner—for example, \nwithin the world editing tool. It also provides no mechanism for certain types \nof objects, or certain individual object instances, to register interest in certain \nevents but not others. Every object in the game, in eﬀ ect, “knows” about every \npossible event, even if its response to the event is to do nothing (i.e., to imple-\nment an empty, do-nothing event handler function).\nWhat we really need for our event handlers, then, is dynamically typed \nlate function binding. Some programming languages support this feature na-\ntively (e.g., C#’s delegates ). In other languages, the engineers must implement \nit manually. There are many ways to approach this problem, but most boil \ndown to taking a data-driven approach. In other words, we encapsulate the \nnotion of a function call in an object and pass that object around at runtime in \norder to implement a dynamically typed late-bound function call.\n14.7.2. Encapsulating an Event in an Object\nAn event is really comprised of two components: its type (explosion, friend \ninjured, player spott ed, health pack picked up, etc.) and its arguments . The \narguments provide speciﬁ cs about the event (How much damage did the ex-\nplosion do? Which friend was injured? Where was the player spott ed? How \nmuch health was in the health pack?). We can encapsulate these two compo-\nnents in an object, as shown by the following pseudocode:\nstruct Event\n{\n \nconst U32 MAX_ARGS = 8;\n \nEventType m_type;\n U32 \n  m_numArgs;\n \nEventArg m_aArgs[MAX_ARGS];\n};\n14.7. Events and Message-Passing\n\n\n776 \n14. Runtime Gameplay Foundation Systems\nSome game engines call these things messages or commands instead of events. \nThese names emphasize the idea that informing objects about an event is es-\nsentially equivalent to sending a message or command to those objects.\nPractically speaking, event objects are usually not quite this simple. We \nmight implement diﬀ erent types of events by deriving them from a root event \nclass, for example. The arguments might be implemented as a linked list or a \ndynamically allocated array capable of containing arbitrary numbers of argu-\nments, and the arguments might be of various data types.\nEncapsulating an event (or message) in an object has many beneﬁ ts:\nz Single event handler function. Because the event object encodes its type \ninternally, any number of diﬀ erent event types can be represented by an \ninstance of a single class (or the root class of an inheritance hierarchy). \nThis means that we only need one virtual function to handle all types of \nevents (e.g., virtual void OnEvent(Event& event);).\nz Persistence . Unlike a function call, whose arguments go out of scope \naft er the function returns, an event object stores both its type and its \narguments as data. An event object therefore has persistence. It can be \nstored in a queue for handling at a later time, copied and broadcast to \nmultiple receivers, and so on.\nz Blind event forwarding . An object can forward an event that it receives to \nanother object without having to “know” anything about the event. For \nexample, if a vehicle receives a Dismount event, it can forward it to all \nof its passengers, thereby allowing them to dismount the vehicle, even \nthough the vehicle itself knows nothing about dismounting.\nThis idea of encapsulating an event/message/command in an object is com-\nmonplace in many ﬁ elds of computer science. It is found not only in game \nengines but in other systems like graphical user interfaces, distributed com-\nmunication systems, and many others. The well-known “Gang of Four” de-\nsign patt erns book [17] calls this the Command patt ern.\n14.7.3. Event Types\n There are many ways to distinguish between diﬀ erent types of events. One \nsimple approach in C or C++ is to deﬁ ne a global enum that maps each event \ntype to a unique integer.\nenum EventType\n{\n EVENT_TYPE_LEVEL_STARTED,\n EVENT_TYPE_PLAYER_SPAWNED,\n\n\n777 \n EVENT_TYPE_ENEMY_SPOTTED,\n EVENT_TYPE_EXPLOSION,\n EVENT_TYPE_BULLET_HIT,\n \n// ...\n}\nThis approach enjoys the beneﬁ ts of simplicity and eﬃ  ciency (since integers \nare usually extremely fast to read, write, and compare). However, it also suf-\nfers from two problems. First, knowledge of all event types in the entire game \nis centralized, which can be seen as a form of broken encapsulation (for bett er \nor for worse—opinions on this vary). Second, the event types are hard-coded, \nwhich means new event types cannot easily be deﬁ ned in a data-driven man-\nner. Third, enumerators are just indices, so they are order-dependent. If some-\none accidentally adds a new event type in the middle of the list, the indices \nof all subsequent event ids change, which can cause problems if event ids \nare stored in data ﬁ les. As such, an enumeration-based event typing system \nworks well for small demos and prototypes but does not scale very well at all \nto real games.\nAnother way to encode event types is via strings. This approach is totally \nfree-form, and it allows a new event type to be added to the system by merely \nthinking up a name for it. But it suﬀ ers from many problems, including a \nstrong potential for event name conﬂ icts, the possibility of events not work-\ning because of a simple typo, increased memory requirements for the strings \nthemselves, and the relatively high cost of comparing strings next to that of \ncomparing integers. Hashed string ids can be used instead of raw strings to \neliminate the performance problems and increased memory requirements, \nbut they do nothing to address event name conﬂ icts or typos. Nonetheless, \nthe extreme ﬂ exibility and data-driven nature of a string- or string-id-based \nevent system is considered worth the risks by some game teams.\nTools can be implemented to help avoid some of the risks involved in us-\ning strings to identify events. For example, a central database of all event type \nnames could be maintained. A user interface could be provided to permit new \nevent types to be added to the database. Naming conﬂ icts could be automati-\ncally detected when a new event is added, and the user could be disallowed \nfrom adding duplicate event types. When selecting a preexisting event, the \ntool could provide a sorted list in a drop-down combo box rather than requir-\ning the user to remember the name and type it manually. The event database \ncould also store meta-data about each type of event, including documentation \nabout its purpose and proper usage and information about the number and \ntypes of arguments it supports. This approach can work really well, but we \n14.7. Events and Message-Passing\n\n\n778 \n14. Runtime Gameplay Foundation Systems\nshould not forget to account for the costs of sett ing up such a system, as they \nare not insigniﬁ cant.\n14.7.4. Event Arguments\nThe arguments of an event usually act like the argument list of a function, pro-\nviding information about the event that might be useful to the receiver. Event \narguments can be implemented in all sorts of ways.\nWe might derive a new type of Event class for each unique type of event. The \narguments can then be hard-coded as data members of the class. For example:\nclass ExplosionEvent : public Event\n{\n float \n m_damage;\n Point \n m_center;\n float \n m_radius;\n};\nAnother approach is to store the event’s arguments as a collection of vari-\nants . A variant is a data object that is capable of holding more than one type of \ndata. It usually stores information about the data type that is currently being \nstored, as well as the data itself. In an event system, we might want our argu-\nments to be integers, ﬂ oating-point values, Booleans, or hashed string ids. So \nin C or C++, we could deﬁ ne a variant class that looks something like this:\nstruct Variant\n{\n    enum Type\n    {\n        TYPE_INTEGER,\n        TYPE_FLOAT,\n        TYPE_BOOL,\n        TYPE_STRING_ID,\n        TYPE_COUNT // number of unique types\n    };\n    Type        m_type;\n    union\n    {\n        I32     m_asInteger;\n        F32     m_asFloat;\n        bool    m_asBool;\n        U32     m_asStringId;\n };\n};\n\n\n779 \nThe collection of variants might be implemented as an array with a small, \nﬁ xed maximum size (say 4, 8, or 16 elements). This imposes an arbitrary limit \non the number of arguments that can be passed with an event, but it also side-\nsteps the problems of dynamically allocating memory for each event’s argu-\nment payload, which can be a big beneﬁ t, especially in memory-constrained \nconsole games.\nThe collection of variants might be implemented as a dynamically sized \ndata structure, like a dynamically sized array (like std::vector) or a linked \nlist (like std::list). This provides a great deal of additional ﬂ exibility over a \nﬁ xed-size design, but it incurs the cost of dynamic memory allocation. A pool \nallocator could be used to great eﬀ ect here, presuming that each Variant is \nthe same size.\n14.7.4.1. Event Arguments as Key-Value Pairs\nA fundamental problem with an indexed collection of event arguments is order \ndependency. Both the sender and the receiver of an event must “know” that \nthe arguments are listed in a speciﬁ c order. This can lead to confusion and \nbugs. For example, a required argument might be accidentally omitt ed or an \nextra one added.\nThis problem can be avoided by implementing event arguments as key-\nvalue pairs . Each argument is uniquely identiﬁ ed by its key, so the arguments \ncan appear in any order, and optional arguments can be omitt ed altogether. \nThe argument collection might be implemented as a closed or open hash table, \nwith the keys used to hash into the table, or it might be an array, linked list, or \nbinary search tree of key-value pairs. These ideas are illustrated in Table 14.1. \nThe possibilities are numerous, and the speciﬁ c choice of implementation is \nlargely unimportant as long as the game’s particular requirements have been \neﬀ ectively and eﬃ  ciently met.\n14.7. Events and Message-Passing\nfloat\nValue\n10.3\nint\n25\nbool\ntrue\n\"radius\"\n\"event\"\n\"damage\"\n\"grenade\"\nKey\nType\nstringid\n\"explosion\"\nTable 14.1.  The arguments of an event object can be implemented as a collection of key-value \npairs. The keys help to avoid order-dependency problems because each event argument is \nuniquely identiﬁ ed by its key.\n\n\n780 \n14. Runtime Gameplay Foundation Systems\n14.7.5. Event Handlers\nWhen an event, message, or command is received by a game object, it needs to \nrespond to the event in some way. This is known as handling the event, and it \nis usually implemented by a function or a snippet of script code called an event \nhandler. (We’ll have more to say about game scripting later on.)\nOft en an event handler is a single native virtual function or script function \nthat is capable of handling all types of events (e.g., OnEvent(Event& event)). \nIn this case, the function usually contains some kind of switch statement or \ncascaded if/else-if clause to handle the various types of events that might be \nreceived. A typical event handler function might look something like this:\nvirtual void SomeObject::OnEvent(Event& event)\n{\n \nswitch (event.GetType())\n {\n case \nEVENT_ATTACK:\n  RespondToAttack(event.GetAttackInfo());\n  break;\n case \nEVENT_HEALTH_PACK:\n  AddHealth(event.GetHealthPack().GetHealth());\n  break;\n \n// ...\ndefault:\n \n \n// Unrecognized event.\n  break;\n }\n}\nAlternatively, we might implement a suite of handler functions, one for \neach type of event (e.g., OnThis(), OnThat(), …). However, as we discussed \nabove, a proliferation of event handler functions can be problematic.\nA Windows GUI toolkit called Microsoft  Foundation Classes (MFC) was \nwell-known for its message maps —a system that permitt ed any Windows mes-\nsage to be bound at runtime to an arbitrary non-virtual or virtual function. \nThis avoided the need to declare handlers for all possible Windows messages \nin a single root class, while at the same time avoiding the big switch statement \nthat is commonplace in non-MFC Windows message-handling functions. But \nsuch a system is probably not worth the hassle—a switch statement works re-\nally well and is simple and clear.\n14.7.6. Unpacking an Event’s Arguments\nThe example above glosses over one important detail—namely, how to ex-\ntract data from the event’s argument list in a type-safe manner. For example, \n\n\n781 \nevent.GetHealthPack() presumably returns a HealthPack game object, \nwhich in turn we presume provides a member function called GetHealth(). \nThis implies that the root Event class “knows” about health packs (as well as, \nby extension, every other type of event argument in the game!) This is prob-\nably an impractical design. In a real engine, there might be derived Event\nclasses that provide convenient data-access APIs such as GetHealthPack(). \nOr the event handler might have to unpack the data manually and cast them \nto the appropriate types. This latt er approach raises type safety concerns, al-\nthough practically speaking it usually isn’t a huge problem because the type \nof the event is always known when the arguments are unpacked.\n14.7.7. Chains of Responsibility\nGame objects are almost always dependent upon one another in various ways. \nFor example, game objects usually reside in a transformation hierarchy, which \nallows an object to rest on another object or be held in the hand of a charac-\nter. Game objects might also be made up of multiple interacting components, \nleading to a star topology or a loosely connected “cloud” of component ob-\njects. A sports game might maintain a list of all the characters on each team. In \ngeneral, we can envision the interrelationships between game objects as one \nor more relationship graphs (remembering that a list and a tree are just special \ncases of a graph). A few examples of relationship graphs are shown in Fig-\nure 14.17.\n14.7. Events and Message-Passing\nFigure 14.17.  Game objects are interrelated in various ways, and we can draw graphs depicting \nthese relationships. Any such graph might serve as a distribution channel for events.\nAttachment \nGraph\nEvent1\nEvent3\nComponent \nGraph\nEvent2\nTeam \nGraph\nTeam\nCarter\nEvan\nQuinn\nCooper\nObjectA\nComponentA2\nComponentA1\nComponentA3\nClip\nWeapon\nCharacter\nVehicle\n\n\n782 \n14. Runtime Gameplay Foundation Systems\nIt oft en makes sense to be able to pass events from one object to the next \nwithin these relationship graphs. For example, when a vehicle receives an \nevent, it may be convenient to pass the event to all of the passengers riding on \nthe vehicle, and those passengers may wish to forward the event to the objects \nin their inventories. When a multicomponent game object receives an event, it \nmay be necessary to pass the event to all of the components so that they all get \na crack at handling it. Or when an event is received by a character in a sports \ngame, we might want to pass it on to all of his or her teammates as well.\nThe technique of forwarding events within a graph of objects is a com-\nmon design patt ern in object-oriented, event-driven programming, sometimes \nreferred to as a chain of responsibility [17]. Usually, the order in which the event \nis passed around the system is predetermined by the engineers. The event is \npassed to the ﬁ rst object in the chain, and the event handler returns a Boolean \nor an enumerated code indicating whether or not it recognized and handled \nthe event. If the event is consumed by a receiver, the process of event for-\nwarding stops; otherwise, the event is forwarded on to the next receiver in \nthe chain. An event handler that supports chain-of-responsibility style event \nforwarding might look something like this:\nvirtual bool SomeObject::OnEvent(Event& event)\n{\n     // Call the base class’ handler first.\n     if (BaseClass::OnEvent(event))\n     {\n        return true;\n     }\n \n// Now try to handle the event myself.\n \nswitch (event.GetType())\n {\n case \nEVENT_ATTACK:\n  RespondToAttack(event.GetAttackInfo());\n \n \nreturn false; // OK to forward this event to others.\n case \nEVENT_HEALTH_PACK:\n  AddHealth(event.GetHealthPack().GetHealth());\n \n \nreturn true; // I consumed the event; don’t forward.\n \n// ...\ndefault:\n \n \nreturn false; // I didn’t recognize this event.\n }\n}\nWhen a derived class overrides an event handler, it can be appropriate to \ncall the base class’s implementation as well if the class is augmenting but not re-\n\n\n783 \nplacing the base class’s response. In other situations, the derived class might be \nentirely replacing the response of the base class, in which case the base class’s \nhandler should not be called. This is another kind of responsibility chain.\nEvent forwarding has other applications as well. For example, we might \nwant to multicast an event to all objects within a radius of inﬂ uence (for an \nexplosion, for example). To implement this, we can leverage our game world’s \nobject query mechanism to ﬁ nd all objects within the relevant sphere and then \nforward the event to all of the returned objects.\n14.7.8. Registering Interest in Events\nIt’s reasonably safe to say that most objects in a game do not need to respond \nto every possible event. Most types of game objects have a relatively small set \nof events in which they are “interested.” This can lead to ineﬃ  ciencies when \nmulticasting or broadcasting events, because we need to iterate over a group \nof objects and call each one’s event handler, even if the object is not interested \nin that particular kind of event.\nOne way to overcome this ineﬃ  ciency is to permit game objects to regis-\nter interest in particular kinds of events. For example, we could maintain one \nlinked list of interested game objects for each distinct type of event, or each \ngame object could maintain a bit array, in which the sett ing of each bit corre-\nsponds to whether or not the object is interested in a particular type of event. \nBy doing this, we can avoid calling the event handlers of any objects that do \nnot care about the event. Calling virtual functions can incur a non-trivial per-\nformance hit, especially on consoles with relatively primitive RAM caches, so \nﬁ ltering objects by interest in an event can greatly improve the eﬃ  ciency of \nevent multicasting and broadcasting.\nEven bett er, we might be able to restrict our original game object query to \ninclude only those objects that are interested in the event we wish to multicast. \nFor example, when an explosion goes oﬀ , we can ask the collision system for \nall objects that are within the damage radius and that can respond to Explo-\nsion events. This can save time overall, because we avoid iterating over objects \nthat we know aren’t interested in the event we’re multicasting. Whether or not \nsuch an approach will produce a net gain depends on how the query mecha-\nnism is implemented and the relative costs of ﬁ ltering the objects during the \nquery versus ﬁ ltering them during the multicast iteration.\n14.7.9. To Queue or Not to Queue\n Most game engines provide a mechanism for handling events immediately \nwhen they are sent. In addition to this, some engines also permit events to be \nqueued for handling at an arbitrary future time. Event queuing has some at-\n14.7. Events and Message-Passing\n\n\n784 \n14. Runtime Gameplay Foundation Systems\ntractive beneﬁ ts, but it also increases the complexity of the event system and \nposes some unique problems. We’ll investigate the pros and cons of event \nqueuing in the following sections and learn how such systems are implement-\ned in the process.\n14.7.9.1. Some Beneﬁ ts of Event Queuing\nControl Over When Events are Handled\nWe have seen that we must be careful to update engine subsystems and game \nobjects in a speciﬁ c order to ensure correct behavior and maximize runtime \nperformance. In the same sense, certain kinds of events may be highly sen-\nsitive to exactly when within the game loop they are handled. If all events \nare handled immediately upon being sent, the event handler functions end \nup being called in unpredictable and diﬃ  cult-to-control ways throughout the \ncourse of the game loop. By deferring events via an event queue, the engineers \ncan take steps to ensure that events are only handled when it is safe and ap-\npropriate to do so.\nAbility to Post Events into the Future\nWhen an event is sent, the sender can usually specify a delivery time—for \nexample, we might want the event to be handled later in the same frame, next \nframe, or some number of seconds aft er it was sent. This feature amounts to \nan ability to post events into the future, and it has all sorts of interesting uses. \nWe can implement a simple alarm clock by posting an event into the future. A \nperiodic task, such as blinking a light every two seconds, can be executed by \nposting an event whose handler performs the periodic task and then posts a \nnew event of the same type one period into the future .\nTo implement the ability to post events into the future, each event is \nstamped with a desired delivery time prior to being queued. An event is only \nhandled when the current game clock matches or exceeds its delivery time. An \neasy way to make this work is to sort the events in the queue in order of increas-\ning delivery time. Each frame, the ﬁ rst event on the queue can be inspected and \nits delivery time checked. If the delivery time is in the future, we abort imme-\ndiately because we know that all subsequent events are also in the future. But \nif we see an event whose delivery time is now or in the past, we extract it from \nthe queue and handle it. This continues until an event is found whose delivery \ntime is in the future. The following pseudocode illustrates this process:\n// This function is called at least once per frame. Its \n// job is to dispatch all events whose delivery time is \n// now or in the past.\nvoid EventQueue::DispatchEvents(F32 currentTime)\n{\n\n\n785 \n \n// Look at, but don’t remove, the next event on the   \n \n// queue.\n \nEvent* pEvent = PeekNextEvent();\n \nwhile (pEvent && pEvent->GetDeliveryTime() <=   \n \n \n  currentTime\n)\n {\n \n \n// OK, now remove the event from the queue.\nRemoveNextEvent();\n \n \n// Dispatch it to its receiver’s event handler.\n  pEvent->\nDispatch();\n \n \n// Peek at the next event on the queue (again   \n \n \n \n// without removing it).\n  pEvent \n= PeekNextEvent();\n }\n}\nEvent Prioritization\nEven if our events are sorted by delivery time in the event queue, the order \nof delivery is still ambiguous when two or more events have exactly the same \ndelivery time. This can happen more oft en than you might think, because it is \nquite common for events’ delivery times to be quantized to an integral num-\nber of frames. For example, if two senders request that events be dispatched \n“this frame,” “next frame,” or “in seven frames from now,” then those events \nwill have identical delivery times.\nOne way to resolve these ambiguities is to assign priorities to events. \nWhenever two events have the same timestamp, the one with higher pri-\nority should always be serviced ﬁ rst. This is easily accomplished by ﬁ rst \nsorting the event queue by increasing delivery times and then sorting each \ngroup of events with identical delivery times in order of decreasing prior-\nity.\nWe could allow up to four billion unique priority levels by encoding our \npriorities in a raw, unsigned 32-bit integer, or we could limit ourselves to only \ntwo or three unique priority levels (e.g., low, medium, and high). In every \ngame engine, there exists some minimum number of priority levels that will \nresolve all real ambiguities in the system. It’s usually best to aim as close to \nthis minimum as possible. With a very large number of priority levels, it can \nbecome a small nightmare to ﬁ gure out which event will be handled ﬁ rst in \nany given situation. However, the needs of every game’s event system are dif-\nferent, and your mileage may vary.\n14.7. Events and Message-Passing\n\n\n786 \n14. Runtime Gameplay Foundation Systems\n14.7.9.2. Some Problems with Event Queuing\nIncreased Event System Complexity\nIn order to implement a queued event system, we need more code, additional \ndata structures, and more-complex algorithms than would be necessary to \nimplement an immediate event system. Increased complexity usually trans-\nlates into longer development times and a higher cost to maintain and evolve \nthe system during development of the game.\nDeep-Copying Events and Their Arguments\nWith an immediate event handling approach, the data in an event’s arguments \nneed only persist for the duration of the event handling function (and any \nfunctions it may call). This means that the event and its argument data can \nreside literally anywhere in memory, including on the call stack. For example, \nwe could write a function that looks something like this:\nvoid SendExplosionEventToObject(GameObject& receiver)\n{\n \n// Allocate event args on the call stack.\n \nF32   damage = 5.0f;\n \nPoint centerPoint(-2.0f, 31.5f, 10.0f);\n \nF32   radius = 2.0f;\n \n// Allocate the event on the call stack.\nEvent event(\"Explosion\");\n \nevent.SetArgFloat(\"Damage\", damage);\n \nevent.SetArgPoint(\"Center\", &centerPoint);\n \nevent.SetArgFloat(\"Radius\", radius);\n \n// Send the event, which causes the receiver’s event\n \n// handler to be called immediately, as shown below.\n event.\nSend(receiver);\n //{\n \n//    receiver.OnEvent(event);\n //}\n}\nWhen an event is queued, its arguments must persist beyond the scope of \nthe sending function. This implies that we must copy the entire event object \nprior to storing the event in the queue. We must perform a deep-copy , meaning \nthat we copy not only the event object itself but its entire argument payload as \nwell, including any data to which it may be pointing. Deep-copying the event \nensures that there are no dangling references to data that exist only in the \n",
      "page_number": 786,
      "chapter_number": 39,
      "summary": "This chapter covers segment 39 (pages 786-808). Key topics include events, gaming, and games. Phased Updates\n To account for inter-subsystem dependencies, we can explicitly code our en-\ngine subsystem updates in the proper order within the main game loop.",
      "keywords": [
        "game objects",
        "event",
        "game",
        "object",
        "Gameplay Foundation Systems",
        "game object states",
        "event object",
        "Updating Game Objects",
        "event types",
        "Game Object Model",
        "event system",
        "event handler",
        "Runtime Gameplay Foundation",
        "Foundation Systems",
        "type"
      ],
      "concepts": [
        "events",
        "gaming",
        "games",
        "objects",
        "time",
        "type",
        "typed",
        "typing",
        "updating",
        "updates"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 3",
          "chapter": 26,
          "title": "Segment 26 (pages 243-252)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 25,
          "title": "Segment 25 (pages 227-237)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 51,
          "title": "Segment 51 (pages 488-496)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 47,
          "title": "Segment 47 (pages 448-460)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 809-829)",
      "start_page": 809,
      "end_page": 829,
      "detection_method": "synthetic",
      "content": "787 \nsending function’s scope, and it permits the event to be stored indeﬁ nitely. The \nexample event-sending function shown above still looks basically the same \nwhen using a queued event system, but as you can see in the italicized code \nbelow, the implementation of the Event::Queue() function is a bit more \ncomplex than its Send() counterpart:\nvoid SendExplosionEventToObject(GameObject& receiver)\n{\n \n// We can still allocate event args on the call   \n \n \n// stack.\n \nF32   damage = 5.0f;\n \nPoint centerPoint(-2.0f, 31.5f, 10.0f);\n \nF32   radius = 2.0f;\n \n// Still OK to allocate the event on the call stack.\nEvent event(\"Explosion\");\n \nevent.SetArgFloat(\"Damage\", damage);\n \nevent.SetArgPoint(\"Center\", &centerPoint);\n \nevent.SetArgFloat(\"Radius\", radius);\n \n// This stores the event in the receiver’s queue for\n \n// handling at a future time. Note how the event   \n \n \n// must be deep-copied prior to being enqueued, since \n \n// the original event resides on the call stack and   \n \n// will go out of scope when this function returns.\n event.\nQueue(receiver);\n //{\n \n//    Event* pEventCopy = DeepCopy(event);\n \n//    receiver.EnqueueEvent(pEventCopy);\n //}\n}\nDynamic Memory Allocation for Queued Events\n Deep-copying of event objects implies a need for dynamic memory allocation, \nand as we’ve already noted many times, dynamic allocation is undesirable in \na game engine due to its potential cost and its tendency to fragment memory. \nNonetheless, if we want to queue events, we’ll need to dynamically allocate \nmemory for them.\nAs with all dynamic allocation in a game engine, it’s best if we can select \na fast and fragmentation-free allocator. We might be able to use a pool allocator, \nbut this will only work if all of our event objects are the same size and if their \nargument lists are comprised of data elements that are themselves all the same \nsize. This may well be the case—for example, the arguments might each be a \n14.7. Events and Message-Passing\n\n\n788 \n14. Runtime Gameplay Foundation Systems\nVariant, as described above. If our event objects and/or their arguments can \nvary in size, a small memory allocator might be applicable. (Recall that a small \nmemory allocator maintains multiple pools, one for each of a few predeter-\nmined small allocation sizes.) When designing a queued event system, always \nbe careful to take dynamic allocation requirements into account.\nDebugging Difﬁ culties\n With queued events, the event handler is not called directly by the sender of \nthat event. So, unlike in immediate event handling, the call stack does not tell us \nwhere the event came from. We cannot walk up the call stack in the debugger \nto inspect the state of the sender or the circumstances under which the event \nwas sent. This can make debugging deferred events a bit tricky, and things get \neven more diﬃ  cult when events are forwarded from one object to another.\nSome engines store debugging information that forms a paper trail of the \nevent’s travels throughout the system, but no matt er how you slice it, event \ndebugging is usually much easier in the absence of queuing.\nEvent queuing also leads to interesting and hard-to-track-down race con-\ndition bugs. We may need to pepper multiple event dispatches throughout our \ngame loop, to ensure that events are delivered without incurring unwanted \none-frame delays yet still ensuring that game objects are updated in the proper \norder during the frame. For example, during the animation update, we might \ndetect that a particular animation has run to completion. This might cause an \nevent to be sent whose handler wants to play a new animation. Clearly, we \nwant to avoid a one-frame delay between the end of the ﬁ rst animation and \nthe start of the next. To make this work, we need to update animation clocks \nﬁ rst (so that the end of the animation can be detected and the event sent), then \nwe should dispatch events (so that the event handler has a chance to request \na new animation), and ﬁ nally we can start animation blending (so that the \nﬁ rst frame of the new animation can be processed and displayed). This is il-\nlustrated in the code snippet below:\nwhile (true) // main game loop\n{\n \n// ...\n \n// Update animation clocks. This may detect the end   \n \n// of a clip, and cause EndOfAnimation events to \n \n// be sent.\n g_animationEngine.\nUpdateLocalClocks(dt);\n \n// Next, dispatch events. This allows an  \n \n \n \n \n \n// EndOfAnimation event handler to start up a new \n \n// animation this frame if desired.\n g_eventSystem.\nDispatchEvents();\n\n\n789 \n \n// Finally, start blending all currently-playing   \n \n \n// animations (including any new clips started \n \n// earlier this frame).\n g_animationEngine.\nStartAnimationBlending();\n \n// ...\n}\n14.7.10. Some Problems with Immediate Event Sending\nNot queuing events also has its share of issues. For example, immediate event \nhandling can lead to extremely deep call stacks. Object A might send object B \nan event, and in its event handler, B might send another event, which might \nsend another event, and another, and so on. In a game engine that supports \nimmediate event handling, it’s not uncommon to see a call stack that looks \nsomething like this:\n…\nShoulderAngel::OnEvent()\nEvent::Send()\nCharacer::OnEvent()\nEvent::Send()\nCar::OnEvent()\nEvent::Send()\nHandleSoundEffect()\nAnimationEngine::PlayAnimation()\nEvent::Send()\nCharacter::OnEvent()\nEvent::Send()\nCharacter::OnEvent()\nEvent::Send()\nCharacter::OnEvent()\nEvent::Send()\nCar::OnEvent()\nEvent::Send()\nCar::OnEvent()\nEvent::Send()\nCar::Update()\nGameWorld::UpdateObjectsInBucket()\nEngine::GameLoop()\nmain()\nA deep call stack like this can exhaust available stack space in extreme \ncases (especially if we have an inﬁ nite loop of event sending), but the real \ncrux of the problem here is that every event handler function must be writt en \nto be fully re-entrant. This means that the event handler can be called recur-\nsively without any ill side-eﬀ ects. As a contrived example, imagine a function \n14.7. Events and Message-Passing\n\n\n790 \n14. Runtime Gameplay Foundation Systems\nthat increments the value of a global variable. If the global is supposed to be \nincremented only once per frame, then this function is not re-entrant, because \nmultiple recursive calls to the function will increment the variable multiple \ntimes.\n14.7.11. Data-Driven Event/Message-Passing Systems\n Event systems give the game programmer a great deal of ﬂ exibility over and \nabove what can be accomplished with the statically typed function calling \nmechanisms provided by languages like C and C++. However, we can do bet-\nter. In our discussions thus far, the logic for sending and receiving events is \nstill hard-coded and therefore under the exclusive control of the engineers. If \nwe could make our event system data-driven, we could extend its power into \nthe hands of our game designers.\nThere are many ways to make an event system data-driven. Starting with \nthe extreme of an entirely hard-coded (non-data-driven) event system, we \ncould imagine providing some simple data-driven conﬁ gurability. For exam-\nple, designers might be allowed to conﬁ gure how individual objects, or entire \nclasses of object, respond to certain events. In the world editor , we can imagine \nselecting an object and then bringing up a scrolling list of all possible events \nthat it might receive. For each one, the designer could use drop-down combo \nboxes and check boxes to control if, and how, the object responds, by selecting \nfrom a set of hard-coded, predeﬁ ned choices. For example, given the event \n“PlayerSpott ed,” AI characters might be conﬁ gured to do one of the following \nactions: run away, att ack, or ignore the event altogether. The event systems of \nsome real commercial game engines are implemented in essentially this way.\nAt the other end of the gamut, our engine might provide the game design-\ners with a simple scripting language (a topic we’ll explore in detail in Section \n14.8). In this case, the designer can literally write code that deﬁ nes how a partic-\nular kind of game object will respond to a particular kind of event. In a scripted \nmodel, the designers are really just programmers (working with a somewhat \nless powerful but also easier-to-use and hopefully less error-prone language \nthan the engineers), so anything is possible. Designers might deﬁ ne new types \nof events, send events, and receive and handle events in arbitrary ways.\nThe problem with a simple, conﬁ gurable event system is that it can se-\nverely limit what the game designers are capable of doing on their own, with-\nout the help of a programmer. On the other hand, a fully scripted solution \nhas its own share of problems: Many game designers are not professional \nsoft ware engineers by training, so some designers ﬁ nd learning and using a \nscripting language a daunting task. Designers are also probably more prone to \nintroducing bugs into the game than their engineer counterparts, unless they \n\n\n791 \nhave practiced scripting or programming for some time. This can lead to some \nnasty surprises during alpha.\nAs a result, some game engines aim for a middle ground. They employ \nsophisticated graphical user interfaces to provide a great deal of ﬂ exibility \nwithout going so far as to provide users with a full-ﬂ edged, free-form scripting \nlanguage. One approach is to provide a ﬂ ow-chart-style graphical program-\nming language. The idea behind such a system is to provide the user with a \nlimited and controlled set of atomic operations from which to choose but with \nplenty of freedom to wire them up in arbitrary ways. For example, in response \nto an event like “PlayerSpott ed,” the designer could wire up a ﬂ ow chart that \ncauses a character to retreat to the nearest cover point, play an animation, wait \n5 seconds, and then att ack. A GUI can also provide error-checking and valida-\ntion to help ensure that bugs aren’t inadvertently introduced.\n14.7.11.1. Data Pathway Communication Systems\nOne of the problems with converting a function-call-like event system into \na data-driven system is that diﬀ erent types of events tend to be incompat-\nible. For example, let’s imagine a game in which the player has an electro-\nmagnetic pulse gun. This pulse causes lights and electronic devices to turn \noﬀ , scares small animals, and produces a shock wave that causes any nearby \nplants to sway. Each of these game object types may already have an event \nresponse that performs the desired behavior. A small animal might respond \nto the “Scare” event by scurrying away. An electronic device might respond \nto the “TurnOﬀ ” event by turning itself oﬀ . And plants might have an event \nhandler for a “Wind” event that causes them to sway. The problem is that our \nEMP gun is not compatible with any of these objects’ event handlers. As a re-\nsult, we end up having to implement a new event type, perhaps called “EMP,” \nand then write custom event handlers for every type of game object in order \nto respond to it.\n One solution to this problem is to take the event type out of the equation \nand to think solely in terms of sending streams of data from one game object to \nanother. In such a system, every game object has one or more input ports, to \nwhich a data stream can be connected, and one or more output ports, through \nwhich data can be sent to other objects. Provided we have some way of wir-\ning these ports together, such as a graphical user interface in which ports can \nbe connected to each other via rubber-band lines, then we can construct ar-\nbitrarily complex behaviors. Continuing our example, the EMP gun would \nhave an output port, perhaps named “Fire,” that sends a Boolean signal. Most \nof the time, the port produces the value 0 (false), but when the gun is ﬁ red, it \nsends a brief (one-frame) pulse of the value 1 (true). The other game objects in \n14.7. Events and Message-Passing\n\n\n792 \n14. Runtime Gameplay Foundation Systems\nthe world have binary input ports that trigger various responses. The animals \nmight have a “Scare” input, the electronic devices a “TurnOn” input, and the \nfoliage objects a “Sway” input. If we connect the EMP gun’s “Fire” output port \nto the input ports of these game objects, we can cause the gun to trigger the de-\nsired behaviors. (Note that we’d have to pipe the gun’s “Fire” output through \na node that inverts its input, prior to connecting it to the “TurnOn” input of the \nelectronic devices. This is because we want them to turn oﬀ  when the gun is \nﬁ ring.) The wiring diagram for this example is shown in Figure 14.18.\nProgrammers decide what kinds of port(s) each type of game object will \nhave. Designers using the GUI can then wire these ports together in arbitrary \nways in order to construct arbitrary behaviors in the game. The programmers \nalso provide various other kinds of nodes for use within the graph, such as a \nnode that inverts its input, a node that produces a sine wave, or a node that \noutputs the current game time in seconds.\nVarious types of data might be sent along a data pathway. Some ports \nmight produce or expect Boolean data, while others might be coded to produce \nor expect data in the form of a unit ﬂ oat. Still others might operate on 3D vec-\ntors,  colors, integers, and so on. It’s important in such a system to ensure that \nconnections are only made between ports with compatible data types, or we \nmust provide some mechanism for automatically converting data types when \ntwo diﬀ erently typed ports are connected together. For example, connecting a \nunit-ﬂ oat output to a Boolean input might automatically cause any value less \nAnimal\nScare\nFoliage\nSway\nRadio\nTurnOn\nInvert\nIn\nOut\nEMP Gun\nFire\nFigure 14.18.  The EMP gun produces a 1 at its “Fire” output when ﬁ red. This can be connected \nto any input port that expects a Boolean value, in order to trigger the behavior associated \nwith that input.\n\n\n793 \nthan 0.5 to be converted to false, and any value greater than or equal to 0.5 to \nbe converted to true. This is the essence of GUI-based event systems like Un-\nreal Engine 3’s Kismet. A screen shot of Kismet is shown in Figure 14.19.\n14.7.11.2. Some Pros and Cons of GUI-Based Programming\nThe beneﬁ ts of a  graphical user interface over a straightforward, text-ﬁ le-\nbased scripting language are probably prett y obvious: ease of use, a gradual \nlearning curve with the potential for in-tool help and tool tips to guide the \nuser, and plenty of error-checking. The downsides of a ﬂ ow-chart style GUI \ninclude the high cost to develop, debug, and maintain such a system, the addi-\ntional complexity, which can lead to annoying or sometimes schedule-killing \nbugs, and the fact that designers are sometimes limited in what they can do \nwith the tool. A text-ﬁ le based programming language has some distinct ad-\nvantages over a GUI-based programming system, including its relative sim-\nplicity (meaning that it is much less prone to bugs), the ability to easily search \nand replace within the source code, and the freedom of each user to choose the \ntext editor with which they are most comfortable.\n14.7. Events and Message-Passing\nFigure 14.19.  Unreal Engine 3’s Kismet.\n\n\n794 \n14. Runtime Gameplay Foundation Systems\n14.8. Scripting\nA scripting language can be deﬁ ned as a programming language whose pri-\nmary purpose is to permit users to control and customize the behavior of a \nsoft ware application. For example, the Visual Basic language can be used to \ncustomize the behavior of Microsoft  Excel; both MEL and Python can be used \nto customize the behavior of Maya. In the context of game engines, a script-\ning language is a high-level, relatively easy-to-use programming language that \nprovides its users with convenient access to most of the commonly used fea-\ntures of the engine. As such, a scripting language can be used by program-\nmers and non-programmers alike to develop a new game or to customize—or \n“mod”—an existing game.\n14.8.1. Runtime versus Data Deﬁ nition\nWe should be careful to make an important distinction here. Game scripting \nlanguages generally come in two ﬂ avors:\nz Data-deﬁ nition languages. The primary purpose of a data-deﬁ nition lan-\nguage is to permit users to create and populate data structures that are \nlater consumed by the engine. Such languages are oft en declarative (see \nbelow) and are either executed or parsed oﬀ -line or at runtime when the \ndata is loaded into memory.\nz Runtime scripting languages . Runtime scripting languages are intended to \nbe executed within the context of the engine at runtime. These languag-\nes are usually used to extend or customize the hard-coded functionality \nof the engine’s game object model and/or other engine systems.\nIn this section, we’ll focus primarily on using a runtime scripting language for \nthe purpose of implementing gameplay features by extending and custom-\nizing the game’s object model.\n14.8.2. Programming Language Characteristics\nIn our discussion of scripting languages , it will be helpful for us all to be on \nthe same page with regard to programming language terminology. There are \nall sorts of programming languages out there, but they can be classiﬁ ed ap-\nproximately according to a relatively small number of criteria. Let’s take a \nbrief look at these criteria:\nz Interpreted versus compiled languages. The source code of a compiled lan-\nguage is translated by a program called a compiler into machine code, \nwhich can be executed directly by the CPU. In contrast, the source code \n\n\n795 \nof an interpreted language is either parsed directly at runtime or is pre-\ncompiled into platform-independent byte code , which is then executed \nby a virtual machine at runtime. A virtual machine acts like an emulation \nof an imaginary CPU, and byte code acts like a list of machine language \ninstructions that are consumed by this CPU. The beneﬁ t of a virtual \nmachine is that it can be quite easily ported to almost any hardware \nplatform and embedded within a host application like a game engine. \nThe biggest cost we pay for this ﬂ exibility is execution speed—a virtual \nmachine usually executes its byte code instructions much more slowly \nthan the native CPU executes its machine language instructions.\nz Imperative langages. In an imperative language, a program is described \nby a sequence of instructions, each of which performs an operation and/\nor changes the state of data in memory. C and C++ are imperative lan-\nguages.\nz Declarative languages. A declarative language describes what is to be done \nbut does not specify exactly how the result should be obtained. That de-\ncision is left  up to the people implementing the language. Prolog is an \nexample of a declarative language. Mark-up languages like HTML and \nTeX can also be classiﬁ ed as declarative languages.\nz Functional languages. Functional languages, which are technically a sub-\nset of declarative languages, aim to avoid state altogether. In a func-\ntional language, programs are deﬁ ned by a collection of functions. Each \nfunction produces its results with no side-eﬀ ects (i.e., it causes no ob-\nservable changes to the system, other than to produce its output data). \nA program is constructed by passing input data from one function to the \nnext until the ﬁ nal desired result has been generated. These languages \ntend to be well-suited to implementing data-processing pipelines. Oca-\nml, Haskell, and F# are examples of functional languages.\nz Procedural versus object-oriented languages. In a procedural language, the \nprimary atom of program construction is the procedure (or function). \nThese procedures and functions perform operations, calculate results, \nand/or change the state of various data structures in memory. In con-\nstrast, an object-oriented language’s primary unit of program construc-\ntion is the class, a data structure that is tightly coupled with a set of \nprocedures/functions that “know” how to manage and manipulate the \ndata within that data structure.\nz Reﬂ ective languages. In a reﬂ ective language, information about the data \ntypes, data member layouts, functions, and hierarchical class relation-\nships in the system is available for inspection at runtime. In a non-reﬂ ec-\n14.8. Scripting\n\n\n796 \n14. Runtime Gameplay Foundation Systems\ntive language, the majority of this meta-information is known only at \ncompile time; only a very limited amount of it is exposed to the runtime \ncode. C# is an example of a reﬂ ective language, while C and C++ are \nexamples of non-reﬂ ective languages.\n14.8.2.1. Typical Characteristics of Game Scripting Languages\nThe characteristics that set a game scripting language apart from its native pro-\ngramming language brethren include:\nz Interpreted. Most game scripting languages are interpreted by a virtual \nmachine, not compiled. This choice is made in the interest of ﬂ exibility, \nportability, and rapid iteration (see below). When code is represented \nas platform-independent byte code, it can easily be treated like data by \nthe engine. It can be loaded into memory just like any other asset rather \nthan requiring help from the operating system (as is necessary with a \nDLL on a PC platform or an IRX on the PLAYSTATION 3, for example). \nBecause the code is executed by a virtual machine rather than directly \nby the CPU, the game engine is aﬀ orded a great deal of ﬂ exibility re-\ngarding how and when script code will be run.\nz Lightweight. Most game scripting languages have been designed for \nuse in an embedded system. As such, their virtual machines tend to be \nsimple, and their memory footprints tend to be quite small.\nz Support for rapid iteration. Whenever native code is changed, the program \nmust be recompiled and relinked, and the game must be shut down \nand rerun in order to see the eﬀ ects of the changes. On the other hand, \nwhen script code is changed, the eﬀ ects of the changes can usually be \nseen very rapidly. Some game engines permit script code to be reloaded \non the ﬂ y, without shutt ing down the game at all. Others require the \ngame to be shut down and rerun. But either way, the turnaround time \nbetween making a change and seeing its eﬀ ects in-game is usually much \nfaster than when making changes to the native language source code.\nz Convenience and ease of use. Scripting languages are oft en customized to \nsuit the needs of a particular game. Features can be provided that make \ncommon tasks simple, intuitive, and less error-prone. For example, a \ngame scripting language might provide functions for ﬁ nding game ob-\njects by name, sending and handling events, pausing or manipulating \nthe passage of time, waiting for a speciﬁ ed amount of time to pass, im-\nplementing ﬁ nite state machines, exposing tweakable parameters to the \nworld editor for use by the game designers, or even handling network \nreplication for multiplayer games.\n\n\n797 \n14.8.3. Some Common Game Scripting Languages\nWhen implementing a runtime game scripting system, we have one funda-\nmental choice to make: Do we select a third-party commercial or open-source \nlanguage and customize it to suit our needs, or do we design and implement \na custom language from scratch?\nCreating a custom language from scratch is usually not worth the hassle \nand the cost of maintenance throughout the project. It can also be diﬃ  cult or \nimpossible to hire game designers and programmers who are already familiar \nwith a custom, in-house language, so there’s usually a training cost as well. \nHowever, this is clearly the most ﬂ exible and customizable approach, and that \nﬂ exibility can be worth the investment.\nFor many studios, it is more convenient to select a reasonably well-known \nand mature scripting language and extend it with features speciﬁ c to your \ngame engine. There are a great many third-party scripting languages from \nwhich to choose, and many are mature and robust, having been used in a \ngreat many projects both within and outside the game industry.\nIn the following sections, we’ll explore a number of custom game script-\ning languages and a number of game-agnostic languages that are commonly \nadapted for use in game engines.\n14.8.3.1. QuakeC\nId Soft ware’s John Carmack implemented a custom scripting language for \nQuake, known as QuakeC (QC). This language was essentially a simpliﬁ ed \nvariant of the C programming language with direct hooks into the Quake en-\ngine. It had no support for pointers or deﬁ ning arbitrary structs, but it could \nmanipulate entities (Quake’s name for game objects) in a convenient manner, \nand it could be used to send and receive/handle game events. QuakeC is an \ninterpreted, imperative, procedural programming language.\nThe power that QuakeC put into the hands of gamers is one of the fac-\ntors that gave birth to what is now known as the mod community. Scripting \nlanguages and other forms of data-driven customization allow gamers to \nturn many commercial games into all sorts of new gaming experiences—from \nslight modiﬁ cations on the original theme to entirely new games.\n14.8.3.2. UnrealScript\nProbably the best-known example of an entirely custom scripting language \nis Unreal Engine’s UnrealScript . This language is based on a C++-like syntacti-\ncal style, and it supports most of the concepts that C and C++ programmers \nhave become accustomed to, including classes, local variables, looping, arrays \nand structs for data organization, strings, hashed string ids (called FName in \n14.8. Scripting\n\n\n798 \n14. Runtime Gameplay Foundation Systems\nUnreal), and object references (but not free-form pointers). In addition, Un-\nrealScript provides a number of extremely powerful game-speciﬁ c features, \nwhich we’ll explore brieﬂ y below. UnrealScript is an interpreted, imperative, \nobject-oriented language.\nAbility to Extend the Class Hierarchy\nThis is perhaps UnrealScript’s biggest claim to fame. The Unreal object model \nis essentially a monolithic class hierarchy, with add-on components provid-\ning interfaces to various engine systems. The root classes in the hierarchy are \nknown as native classes, because they are implemented in the native C++ lan-\nguage. But UnrealScript’s real power comes from the fact that it can be used to \nderive new classes that are implemented entirely in script.\nThis may not sound like a big deal until you try to imagine how you \nwould implement such a thing! In eﬀ ect, UnrealScript redeﬁ nes and extends \nC++’s native object model, which is really quite astounding. For native Unreal \nclasses, the UnrealScript source ﬁ les (normally named with the extension .uc) \ntake the place of C++’s header ﬁ les (.h ﬁ les) as the primary deﬁ nition of each \nclass—the UnrealScript compiler actually generates the C++ .h ﬁ les from the .uc \nﬁ les, and the programmer implements the classes in regular .cpp source ﬁ les. \nDoing this allows the UnrealScript compiler to introduce additional features \ninto every Unreal class, and these features permit new script-only classes to be \ndeﬁ ned by users that inherit from native classes or other script-only classes.\nLatent Functions\nLatent functions are functions whose execution may span multiple frames of \ngameplay. A latent function can execute some instructions and then “go to \nsleep” waiting for an event or for a speciﬁ ed amount of time to pass. When the \nrelevant event occurs or the time period elapses, the function is “woken up” by \nthe engine, and it continues executing where it left  oﬀ . This feature is highly use-\nful for managing behaviors in the game that depend upon the passage of time.\nConvenient Linkage to UnrealEd\nThe data members of any UnrealScript-based class can be optionally marked with \na simple annotation, indicating that that data member is to be made available for \nviewing and editing in Unreal’s world editor , UnrealEd . No GUI programming \nis required. This makes data-driven game design extremely easy (as long as Un-\nrealEd’s built-in data member editing GUI suits your needs, of course).\nNetwork Replication for Multiplayer Games\nIndividual data elements in UnrealScript can be marked for replication. In \nUnreal networked games, each game object exists in its full form on one \n\n\n799 \nparticular machine; all the other machines have a lightweight version of the \nobject known as a remote proxy . When you mark a data member for replica-\ntion, you are telling the engine that you want that data to be replicated from \nthe master object to all of the remote proxies. This allows a programmer or \ndesigner to easily control which data should be made available across the \nnetwork. This indirectly controls the amount of network bandwidth required \nby the game.\n14.8.3.3. Lua\nLua is a well-known and popular scripting language that is easy to integrate \ninto an application such as a game engine. The Lua website (htt p://www.lua.\norg/about.html) calls the language the “leading scripting language in games.”\nAccording to the Lua website, Lua’s key beneﬁ ts are:\nz Robust and mature. Lua has been used on numerous commercial prod-\nucts, including Adobe’s Photoshop Lightroom, and many games, includ-\ning World of Warcraft .\nz Good documentation. Lua’s reference manual [21] is complete and \nunderstandable and is available in online and book formats. A number \nof books have been writt en about Lua, including [22] and [43].\nz Excellent runtime performance. Lua executes its byte code more quickly \nand eﬃ  ciently than many other scripting languages.\nz Portable. Out of the box, Lua runs on all ﬂ avors of Windows and \nUNIX, mobile devices, and embedded microprocessors. Lua is writt en \nin a portable manner, making it easy to adapt to new hardware plat-\nforms.\nz Designed for embedded systems. Lua’s memory footprint is very small \n(approximately 350 kB for the interpreter and all libraries).\nz Simple, powerful, and extensible. The core Lua language is very small and \nsimple, but it is designed to support meta-mechanisms that extend its \ncore functionality in virtually limitless ways. For example, Lua itself \nis not an object-oriented language, but OOP support can and has been \nadded via a meta-mechanism.\nz Free. Lua is open source and is distributed under the very liberal MIT \nlicense.\nLua is a dynamically typed language, meaning that variables don’t have \ntypes—only values do. (Every value carries its type information along with it.) \nLua’s primary data structure is the table, also known as an associative array. A \ntable is essentially a list of key-value pairs with an optimized ability to index \ninto the array by key.\n14.8. Scripting\n\n\n800 \n14. Runtime Gameplay Foundation Systems\nLua provides a convenient interface to the C language—the Lua virtual \nmachine can call and manipulate functions writt en in C as easily as it can \nthose writt en in Lua itself.\nLua treats blocks of code, called chunks, as ﬁ rst-class objects that can be \nmanipulated by the Lua program itself. Code can be executed in source code \nformat or in precompiled byte code format. This allows the virtual machine \nto execute a string that contains Lua code, just as if the code were compiled \ninto the original program. Lua also supports some powerful advanced pro-\ngramming constructs, including coroutines. This is a simple form of coopera-\ntive multitasking , in which each thread must yield the CPU to other threads \nexplicitly (rather than being time-sliced as in a preemptive multithreading \nsystem).\nLua does have some pitfalls. For example, its ﬂ exible function binding \nmechanism makes it possible (and quite easy) to redeﬁ ne an important global \nfunction like sin() to perform a totally diﬀ erent task (which is usually not \nsomething one intends to do). But all in all, Lua has proven itself to be an ex-\ncellent choice for use as a game scripting language.\n14.8.3.4. Python\nPython is a procedural, object-oriented, dynamically typed scripting lan-\nguage, designed with ease of use, integration with other programming lan-\nguages, and ﬂ exibility in mind. Like Lua, Python is a common choice for use \nas a game scripting language. According to the oﬃ  cial Python website (htt p://\nwww.python.org), some of Python’s best features include:\nz Clear and readable syntax. Python code is easy to read, in part because \nthe syntax enforces a speciﬁ c indentation style. (It actually parses the \nwhitespace used for intentation in order to determine the scope of each \nline of code.)\nz Reﬂ ective language. Python includes powerful runtime introspection \nfacilities. Classes in Python are ﬁ rst-class objects, meaning they can be \nmanipulated and queried at runtime, just like any other object.\nz Object-oriented. One advantage of Python over Lua is that OOP is built \ninto the core language. This makes integrating Python with a game’s \nobject model a litt le easier.\nz Modular. Python supports hierarchical packages, encouraging clean \nsystem design and good encapsulation.\nz Exception-based error handling. Exceptions make error-handling code in \nPython simpler, more elegant, and more localized than similar code in a \nnon-exception based language.\n\n\n801 \nz Extensive standard libraries and third-party modules. Python libraries exist \nfor virtually every task imaginable. (Really!)\nz Embeddable. Python can be easily embedded into an application, such as \na game engine.\nz Extensive documentation. There’s plenty of documentation and tutorials \non Python, both online and in book form. A good place to start is the \nPython website, htt p://www.python.org.\nPython syntax is reminiscent of C in many respects (for example, its use \nof the = operator for assignment and == for equality testing). However, in \nPython, code indentation serves as the only means of deﬁ ning scope (as opposed \nto C’s opening and closing braces). Python’s primary data structures are the \nlist—a linearly indexed sequence of atomic values or other nested lists—and \nthe dictionary—a table of key-value pairs. Each of these two data structures \ncan hold instances of the other, allowing arbitrarily complex data structures to \nbe constructed easily. In addition, classes—uniﬁ ed collections of data elements \nand functions—are built right into the language.\nPython supports duck typing , which is a style of dynamic typing in which \nthe functional interface of an object determines its type (rather than being de-\nﬁ ned by a static inheritance hierarchy). In other words, any class that supports \na particular interface (i.e., a collection of functions with speciﬁ c signatures) \ncan be used interchangeably with any other class that supports that same \ninterface. This is a powerful paradigm: In eﬀ ect, Python supports polymor-\nphism without requiring the use of inheritance. Duck typing is similar in some \nrespects to C++ template meta-programming, although it is arguably more \nﬂ exible because the bindings between caller and callee are formed dynami-\ncally, at runtime. Duck typing gets its name from the well-known phrase (at-\ntributed to James Whitcomb Riley), “If it walks like a duck and quacks like a \nduck, I would call it a duck.” See htt p://en.wikipedia.org/wiki/Duck_typing \nfor more information on duck typing.\nIn summary, Python is easy to use and learn, embeds easily into a game \nengine, integrates well with a game’s object model, and can be an excellent \nand powerful choice as a game scripting language.\n14.8.3.5. Pawn / Small / Small-C\nPawn is a lightweight, dynamically typed, C-like scripting language created \nby Marc Peter. The language was formerly known as Small, which itself was \nan evolution of an earlier subset of the C language called Small-C, writt en by \nRon Cain and James Hendrix. It is an interpreted language—the source code \nis compiled into byte code (also known as P-code), which is interpreted by a \nvirtual machine at runtime.\n14.8. Scripting\n\n\n802 \n14. Runtime Gameplay Foundation Systems\nPawn was designed to have a small memory footprint and to execute its \nbyte code very quickly. Unlike C, Pawn’s variables are dynamically typed. \nPawn also supports ﬁ nite state machines, including state-local variables. This \nunique feature makes it a good ﬁ t for many game applications. Good online \ndocumentation is available for Pawn (htt p://www.compuphase.com/pawn/\npawn.htm). Pawn is open source and can be used free of charge under the \nZlib/libpng license (htt p://www.opensource.org/licenses/zlib-license.php).\nPawn’s C-like syntax makes it easy to learn for any C/C++ programmer \nand easy to integrate with a game engine writt en in C. Its ﬁ nite state machine \nsupport can be very useful for game programming. It has been used success-\nfully on a number of game projects, including Freaky Flyers by Midway. Pawn \nhas shown itself to be a viable game scripting language.\n14.8.4. Architectures for Scripting\nScript code can play all sorts of roles within a game engine. There’s a gamut of \npossible architectures, from tiny snippets of script code that perform simple \nfunctions on behalf of an object or engine system to high-level scripts that \nmanage the operation of the game. Here are just a few of the possible archi-\ntectures:\nz Scripted callbacks . In this approach, the engine’s functionality is largely \nhard-coded in the native programming language, but certain key bits of \nfunctionality are designed to be customizable. This is oft en implement-\ned via a hook function or callback—a user-supplied function that is called \nby the engine for the purpose of allowing customization. Hook func-\ntions can be writt en in the native language, of course, but they can also \nbe writt en in a scripting language. For example, when updating game \nobjects during the game loop, the engine might call an optional callback \nfunction that can be writt en in script. This gives users the opportunity to \ncustomize the way in which the game object updates itself over time.\nz Scripted event handlers . An event handler is really just a special type of \nhook function whose purpose is to allow a game object to respond to \nsome relevant occurrence within the game world (e.g., responding to \nan explosion going oﬀ ) or within the engine itself (e.g., responding to \nan out-of-memory condition). Many game engines allow users to write \nevent handler hooks in script as well as in the native language.\nz Extending game object types, or deﬁ ning new ones, with script . Some script-\ning languages allow game object types that have been implemented in \nthe native language to be extended via script. In fact, callbacks and event \nhandlers are examples of this on a small scale, but the idea can be ex-\n\n\n803 \ntended even to the point of allowing entirely new types of game objects \nto be deﬁ ned in script. This might be done via inheritance (i.e., deriving a \nclass writt en in script from a class writt en in the native language) or via \ncomposition /\n \naggregation (i.e., att aching an instance of a scripted class to a \nnative game object).\nz Scripted components or properties . In a component- or property-based \ngame object model, it only makes sense to permit new components or \nproperty objects to be constructed partially or entirely in script. This ap-\nproach was used by Gas Powered Games for Dungeon Siege (htt p://www.\ndrizzle.com /~scott b /gdc /game-objects.ppt). The game object model was \nproperty-based, and it was possible to implement properties in either \nC++ or Gas Powered Games’ custom scripting language, Skrit (htt p://\nds.heavengames.com/library/dstk/skrit/skrit). By the end of the project, \nthey had approximately 148 scripted property types and 21 native C++ \nproperty types.\nz Script-driven engine systems . Script might be used to drive an entire \nengine system. For example, the game object model could conceiv-\nably be written entirely in script, calling into the native engine code \nonly when it requires the services of lower-level engine compo-\nnents.\nz Script-driven game. Some game engines actually ﬂ ip the relationship \nbetween the native language and the scripting language on its head. \nIn these engines, the script code runs the whole show, and the native \nengine code acts merely as a library that is called to access certain \nhigh-speed features of the engine. The Panda3D engine (htt p://www.\npanda3d.org) is an example of this kind of architecture. Panda3D games \ncan be writt en entirely in the Python language, and the native engine \n(implemented in C++) acts like a library that is called by script code. \n(Panda3D games can also be writt en entirely in C++.)\n14.8.5. Features of a Runtime Game Scripting Language\nThe primary purpose of many game scripting languages is to implement \ngameplay features, and this is oft en accomplished by augmenting and cus-\ntomizing a game’s object model. In this section, we’ll explore some of the most \ncommon requirements and features of such a scripting system.\n14.8.5.1. Interface with the Native Programming Language\nIn order for a scripting language to be useful, it must not operate in a vacuum. \nIt’s imperative for the game engine to be able to execute script code, and it’s \n14.8. Scripting\n\n\n804 \n14. Runtime Gameplay Foundation Systems\nusually equally important for script code to be capable of initiating operations \nwithin the engine as well. \nA runtime scripting language’s virtual machine is generally embedded \nwithin the game engine. The engine initializes the virtual machine, runs script \ncode whenever required, and manages those scripts’ execution. The unit of \nexecution varies depending on the speciﬁ cs of the language and the game’s \nimplementation.\nz In a functional scripting language, the function is oft en the primary unit of \nexecution. In order for the engine to call a script function, it must look up \nthe byte code corresponding to the name of the desired function and spawn \na virtual machine to execute it (or instruct an existing VM to do  so).\nz In an object-oriented scripting language, classes are typically the prima-\nry unit of execution. In such a system, objects can be spawned and de-\nstroyed, and methods (member functions) can be invoked on individual \nclass instances.\nIt’s usually beneﬁ cial to allow two-way communication between script \nand native code. Therefore, most scripting languages allowing native code to \nbe invoked from script as well. The details are language- and implementation-\nspeciﬁ c, but the basic approach is usually to allow certain script functions to \nbe implemented in the native language rather than in the scripting language. \nTo call an engine function, script code simply makes an ordinary function call. \nThe virtual machine detects that the function has a native implementation, \nlooks up the corresponding native function’s address (perhaps by name or via \nsome other kind of unique function identiﬁ er), and calls it. For example, some \nor all of the member functions of a Python class or module can be implement-\ned using C functions. Python maintains a data structure, known as a method \ntable , that maps the name of each Python function (represented as a string) to \nthe address of the C function that implements it.\nCase Study: Naughty Dog’s DC Language\nAs an example, let’s have a brief look at how Naughty Dog’s runtime scripting \nlanguage, a language called DC, was integrated into the engine.\nDC is a variant of the Scheme language (which is itself a variant of Lisp ). \nChunks of executable code in DC are known as script lambdas , which are the \napproximate equivalent of functions or code blocks in the Lisp family of lan-\nguages. A DC programmer writes script lambdas and identiﬁ es them by giving \nthem globally unique names. The DC compiler converts these script lambdas \ninto chunks of byte code, which are loaded into memory when the game runs \nand can be looked up by name using a simple functional interface in C++.\n\n\n805 \nOnce the engine has a pointer to a chunk of script lambda byte code , it can \nexecute the code by calling a function in the engine and passing the pointer to \nthe byte code to it. The function itself is surprisingly simple. It spins in a loop, \nreading byte code instructions one-by-one, and executing each instruction. \nWhen all instructions have been executed, the function returns.\nThe virtual machine contains a bank of registers, which can hold any kind \nof data the script may want to deal with. This is implemented using a variant \ndata type—a union of all the data types (see 14.7.4 for a discussion of vari-\nants). Some instructions cause data to be loaded into a register; others cause \nthe data held in a register to be looked up and used. There are instructions for \nperforming all of the mathematical operations available in the language, as \nwell as instructions for performing conditional checks—implementations of \nDC’s (if …), (when …), and (cond …) instructions and so on.\nThe virtual machine also supports a function call stack. Script lambdas in \nDC can call other script lambdas (i.e., functions) that have been deﬁ ned by \na script programmer via DC’s (defun …) syntax. Just like any procedural \nprogramming language, a stack is needed to keep track of the states of the \nregisters and the return address when one function calls another. In the DC \nvirtual machine, the call stack is literally a stack of register banks—each new \nfunction gets its own private bank of registers. This prevents us from having \nto save oﬀ  the state of the registers, call the function, and then restore the reg-\nisters when the called function returns. When the virtual machine encounters \na byte code instruction that tells it to call another script lambda, the byte code \nfor that script lambda is looked up by name, a new stack frame is pushed, and \nexecution continues at the ﬁ rst instruction of that script lambda. When the vir-\ntual machine encounters a return instruction, the stack frame is popped from \nthe stack, along with the return “address” (which is really just the index of the \nbyte code instruction in the calling script lambda aft er the one that called the \nfunction in the ﬁ rst place).\nThe following pseudocode should give you a feel for what the core in-\nstruction-processing loop of the DC virtual machine looks like:\nvoid DcExecuteScript(DCByteCode* pCode)\n{\n DCStackFrame* \npCurStackFrame =  \n       \nDcPushStackFrame(pCode);\n \n// Keep going until we run out of stack frames (i.e.,  \n \n// the top-level script lambda \"function\" returns).\n  while (pCurStackFrame != NULL)\n  {\n14.8. Scripting\n\n\n806 \n14. Runtime Gameplay Foundation Systems\n \n \n// Get the next instruction. We will never run    \n \n \n// out, because the return instruction is always \n \n \n// last, and it will pop the current stack frame   \n  // \nbelow.\n  DCInstruction& \ninstr\n   = \npCurStackFrame->GetNextInstruction();\n \n \n// Perform the operation of the instruction.\n  switch \n(instr.GetOperation())\n  {\n  case \nDC_LOAD_REGISTER_IMMEDIATE:\n   {\n \n \n \n \n// Grab the immediate value to be loaded  \n \n    // \nfrom the instruction.\n    Variant& \ndata = instr.GetImmediateValue();\n \n \n \n \n// Also determine into which register to  \n \n    // \nput it.\n    U32 \niReg = instr.GetDestRegisterIndex();\n \n \n \n \n// Grab the register from the stack frame.\n    Variant& \nreg\n     \n= pCurStackFrame->GetRegister(iReg);\n \n \n \n \n// Store the immediate data into the  \n \n \n    // \nregister.\nreg = data;\n   }\n   break;\n \n \n// Other load and store register operations...\n  case \nDC_ADD_REGISTERS:\n   {\n \n \n \n \n// Determine the two registers to add. The\n \n \n \n \n// result will be stored in register A.\n    U32 \niRegA = instr.GetDestRegisterIndex();\n    U32 \niRegB = instr.GetSrcRegisterIndex();\n \n \n \n \n// Grab the 2 register variants from the  \n \n    // \nstack.\n    Variant& \ndataA\n     = \npCurStackFrame->GetRegister(iRegA);\n    Variant& \ndataB\n     = \npCurStackFrame->GetRegister(iRegB);\n \n \n \n \n// Add the registers and store in \n    // \nregister A.\ndataA = dataA + dataB;\n   }\n   break;\n\n\n807 \n \n \n// Other math operations...\n  case \nDC_CALL_SCRIPT_LAMBDA:\n   {\n \n \n \n \n// Determine in which register the name of   \n \n \n \n \n// the script lambda to call is stored. \n \n \n \n \n// (Presumably it was loaded by a previous   \n    // \nload instr.)\n    U32 \niReg = instr.GetSrcRegisterIndex();\n \n \n \n \n// Grab the appropriate register, which  \n \n \n \n \n \n// contains the name of the lambda to call.\n    Variant& \nlambda\n     = \npCurStackFrame->GetRegister(iReg);\n \n \n \n \n// Look up the byte code of the lambda by   \n    // \nname.\n    DCByteCode* \npCalledCode\n     = \nDcLookUpByteCode(lambda.AsStringId());\n \n \n \n \n// Now \"call\" the lambda by pushing a new   \n    // \nstack frame.\n    if \n(pCalledCode)\n    {\npCurStackFrame\n      = \nDcPushStackFrame(pCalledCode);\n    }\n   }\n   break;\n  case \nDC_RETURN:\n   {\n \n \n \n \n// Just pop the stack frame. If we’re in  \n \n \n \n \n \n// the top lambda on the stack, this \n \n \n \n \n// function will return NULL, and the loop   \n    // \nwill terminate.\npCurStackFrame = DcPopStackFrame();\n   }\n   break;\n \n \n// Other instructions...\n  // \n...\n \n \n} // end switch\n \n} // end for\n}\nIn the above example, we assume that the global functions DcPushStack\nFrame() and DcPopStackFrame() manage the stack of register banks for us \n14.8. Scripting\n",
      "page_number": 809,
      "chapter_number": 40,
      "summary": "This chapter covers segment 40 (pages 809-829). Key topics include event, languages, and games. Note how the event   \n \n \n// must be deep-copied prior to being enqueued, since \n \n// the original event resides on the call stack and   \n \n// will go out of scope when this function returns.",
      "keywords": [
        "game scripting language",
        "scripting language",
        "game",
        "language",
        "event",
        "Gameplay Foundation Systems",
        "Game scripting",
        "game object",
        "game engine",
        "Runtime Gameplay Foundation",
        "scripting",
        "code",
        "event system",
        "runtime scripting language",
        "engine"
      ],
      "concepts": [
        "event",
        "languages",
        "games",
        "gaming",
        "scripted",
        "function",
        "functionality",
        "functions",
        "data",
        "code"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 12,
          "title": "Segment 12 (pages 101-108)",
          "relevance_score": 0.71,
          "method": "sentence_transformers"
        },
        {
          "book": "makinggames",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 18,
          "title": "Segment 18 (pages 147-158)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 40,
          "title": "Segment 40 (pages 368-375)",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 11,
          "title": "Segment 11 (pages 99-114)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 830-848)",
      "start_page": 830,
      "end_page": 848,
      "detection_method": "synthetic",
      "content": "808 \n14. Runtime Gameplay Foundation Systems\nin some suitable way and that the global function DcLookUpByteCode() is \ncapable of looking up any script lambda by name. We won’t show implemen-\ntations of those functions here, because the purpose of this example is simply \nto show how the inner loop of a script virtual machine might work, not to \nprovide a complete functional implementation.\nDC script lambdas can also call native functions—i.e., global functions \nwritt en in C++ that serve as hooks into the engine itself. When the virtual ma-\nchine comes across an instruction that calls a native function, the address of \nthe C++ function is looked up by name using a global table that has been hard-\ncoded by the engine programmers. If a suitable C++ function is found, the \narguments to the function are taken from registers in the current stack frame, \nand the function is called. This implies that the C++ function’s arguments are \nalways of type Variant. If the C++ function returns a value, it too must be a \nVariant, and its value will be stored into a register in the current stack frame \nfor possible use by subsequent instructions.\nThe global function table might look something like this:\ntypedef Variant DcNativeFunction(U32 argCount, \n \nVariant* aArgs);\nstruct DcNativeFunctionEntry\n{\n StringId \n   m_name;\n \nDcNativeFunction* m_pFunc;\n};\nDcNativeFunctionEntry g_aNativeFunctionLookupTable[] = {\n \n{ SID(\"get-object-pos\"), DcGetObjectPos },\n \n{ SID(\"animate-object\"), DcAnimateObject },\n \n// etc.\n \n// ...\n};\nA native DC function implementation might look something like the fol-\nlowing. Notice how the Variant arguments are passed to the function as an \narray. The function must verify that the number of arguments passed to it \nequals the number of arguments it expects. It must also verify that the types of \nthe argument(s) are as expected and be prepared to handle errors that the DC \nscript programmer may have made when calling the function.\nVariant DcGetObjectPos(U32 argCount, Variant* aArgs)\n{\n \n// Set up a default return value.\n Variant \nresult;\n result.\nSetAsVector(Vector(0.0f, 0.0f, 0.0f));\n\n\n809 \n \nif (argCount != 1)\n {\n  DcErrorMessage(\"get-object-pos: \n   Invalid \narg count.\\n\");\nreturn result;\n }\n \nif (aArgs[0].GetType() != Variant::TYPE_STRING_ID)\n {\n  DcErrorMessage(\"get-object-pos: \nExpected \n   string \nid.\\n\");\nreturn result;\n }\n StringId \nobjectName = aArgs[0].AsStringId();\n GameObject* \npObject = GameObject::LookUpByName\n(objectName);\n \nif (pObject == NULL)\n {\n  DcErrorMessage(\n \n \n \n\"get-object-pos: Object ‘%s’ not found.\\n\",\n   objectName.ToString());\nreturn result;\n }\n result.\nSetAsVector(pObject->GetPosition());\nreturn result;\n}\n14.8.5.2. Game Object Handles\nScript functions oft en need to interact with game objects, which themselves \nmay be implemented partially or entirely in the engine’s native language. The \nnative language’s mechanisms for referencing objects (e.g., pointers or refer-\nences in C++) won’t necessarily be valid in the scripting language. (It may not \nsupport pointers at all, for example.) Therefore, we need to come up with \nsome reliable way for script code to reference game objects.\nThere are a number of ways to accomplish this. One approach is to refer to \nobjects in script via opaque numeric handles . The script code can obtain object \nhandles in various ways. It might be passed a handle by the engine, or it might \nperform some kind of query, such as asking for the handles of all game objects \nwithin a radius of the player or looking up the handle that corresponds to a \nparticular object name. The script can then perform operations on the game \n14.8. Scripting\n\n\n810 \n14. Runtime Gameplay Foundation Systems\nobject by calling native functions and passing the object’s handle as an argu-\nment. On the native language side, the handle is converted back into a pointer \nto the native object, and then the object can be manipulated as appropriate.\nNumeric handles have the beneﬁ t of simplicity and should be easy to sup-\nport in any scripting language that supports integer data. However, they can \nbe unintuitive and diﬃ  cult to work with. Another alternative is to use the \nnames of the objects, represented as strings, as our handles. This has some \ninteresting beneﬁ ts over the numeric handle technique. For one thing, strings \nare human-readable and intuitive to work with. There is a direct correspon-\ndence to the names of the objects in the game’s world editor. In addition, we \ncan choose to reserve certain special object names and give them “magic” \nmeanings. For example, in Naughty Dog’s scripting language, the reserved \nname “self” always refers to the object to which the currently-running script is \natt ached. This allows game designers to write a script, att ach it to an object in \nthe game, and then use the script to play an animation on the object by simply \nwriting (animate \"self\" name-of-animation).\nUsing strings as object handles has its pitfalls, of course. Strings oft en \noccupy more memory than integer ids. And because strings vary in length, \ndynamic memory allocation is required in order to copy them. String com-\nparisons are slow. Script programmers are apt to make mistakes when typing \nthe names of game objects, which can lead to bugs. In addition, script code can \nbe broken if someone changes the name of an object in the game world editor \nbut forgets to update the name of the object in script.\nHashed string ids overcome most of these problems by converting any \nstrings (regardless of length) into an integer. In theory, hashed string ids enjoy \nthe best of both worlds—they can be read by users just like strings, but they \nhave the runtime performance characteristics of an integer. However, for this \nto work, your scripting language needs to support hashed string ids in some \nway. Ideally, we’d like the script compiler to convert our strings into hashed \nids for us. That way, the runtime code doesn’t have to deal with the strings at \nall, only the hashed ids (except possibly for debugging purposes—it’s nice to \nbe able to see the string corresponding to a hashed id in the debugger). How-\never, this isn’t always possible in all scripting languages. Another approach is \nto allow the user to use strings in script and convert them into hashed ids at \nruntime, whenever a native function is called.\n14.8.5.3. Receiving and Handling Events\nEvents are a ubiquitous communication mechanism in most game engines. By \npermitt ing event handler functions to be writt en in script, we open up a pow-\nerful avenue for customizing the hard-coded behavior of our game.\n\n\n811 \nEvents are usually sent to individual objects and handled within the con-\ntext of that object. Hence scripted event handlers need to be associated with \nan object in some way. Some engines use the game object type system for this \npurpose—scripted event handlers can be registered on a per-object-type basis. \nThis allows diﬀ erent types of game objects to respond in diﬀ erent ways to the \nsame event but ensures that all instances of each type respond in a consis-\ntent and uniform way. The event handler functions themselves can be simple \nscript functions, or they can be members of a class if the scripting language is \nobject-oriented. In either case, the event handler is typically passed a handle \nto the particular object to which the event was sent, much as C++ member \nfunctions are passed the this pointer.\nIn other engines, scripted event handlers are associated with individual \nobject instances rather than with object types. In this approach, diﬀ erent in-\nstances of the same type might respond diﬀ erently to the same event.\nThere are all sorts of other possibilities, of course. For example, in Naughty \nDog’s Uncharted engine, scripts are objects in their own right. They can be as-\nsociated with individual game objects, they can be att ached to regions (convex \nvolumes that are used to trigger game events), or they can exist as standalone \nobjects in the game world. Each script can have multiple states (that is, scripts \nare ﬁ nite state machines in the Uncharted engine). In turn, each state can have \none or more event handler code blocks. When a game object receives an event, \nit has the option of handling the event in native C++. It also checks for an at-\ntached script object, and if one is found, the event is sent to that script’s current \nstate. If the state has an event handler for the event, it is called. Otherwise, the \nscript simply ignores the event.\n14.8.5.4. Sending Events\nAllowing scripts to handle game events that are generated by the engine is \ncertainly a powerful feature. Even more powerful is the ability to generate and \nsend events from script code either back to the engine or to other scripts.\nIdeally, we’d like to be able not only to send predeﬁ ned types of events \nfrom script but to deﬁ ne entirely new event types in script. Implementing this \nis trivial if event types are strings. To deﬁ ne a new event type, the script pro-\ngrammer simply comes up with a new event type name and types it into his \nor her script code. This can be a highly ﬂ exible way for scripts to communicate \nwith one another. Script A can deﬁ ne a new event type and send it to Script B. \nIf Script B deﬁ nes an event handler for this type of event, we’ve implemented \na simple way for Script A to “talk” to Script B. In some game engines, event- or \nmessage-passing is the only supported means of inter-object communication \nin script. This can be an elegant yet powerful and ﬂ exible solution.\n14.8. Scripting\n\n\n812 \n14. Runtime Gameplay Foundation Systems\n14.8.5.5. Object-Oriented Scripting Languages\nSome scripting languages are inherently object-oriented. Others do not sup-\nport objects directly but provide mechanisms that can be used to implement \nclasses and objects. In many engines, gameplay is implemented via an object-\noriented game object model of some kind. So it makes sense to permit some \nform of object-oriented programming in script as well.\nDeﬁ ning Classes in Scripts\nA class is really just a bunch of data with some associated functions. So any \nscripting language that permits new data structures to be deﬁ ned, and pro-\nvides some way to store and manipulate functions, can be used to implement \nclasses. For example, in Lua, a class can be built out of a table that stores data \nmembers and member functions.\nInheritance in Script\nObject-oriented languages do not necessarily support inheritance . However, \nif this feature is available, it can be extremely useful, just as it is in native pro-\ngramming languages like C++.\nIn the context of game scripting languages, there are two kinds of in-\nheritance: deriving scripted classes from other scripted classes and deriving \nscripted classes from native classes. If your scripting language is object-orient-\ned, chances are the former is supported out of the box. However, the latt er is \ntough to implement even if the scripting language supports inheritance. The \nproblem is bridging the gap between two languages and two low-level object \nmodels. We won’t get into the details of how this might be implemented here, \nas the implementation is bound to be speciﬁ c to the pair of languages being \nintegrated. UnrealScript is the only scripting language I’ve seen that allows \nscripted classes to derive from native classes in a seamless way.\nComposition/Aggregation in Script\nWe don’t need to rely on inheritance to extend a hierarchy of classes—we can \nalso use composition or aggregation to similar eﬀ ect. In script, then, all we \nreally need is a way to deﬁ ne classes and associate instances of those classes \nwith objects that have been deﬁ ned in the native programming language. For \nexample, a game object could have a pointer to an optional component writ-\nten entirely in script. We can delegate certain key functionality to the script \ncomponent, if it exists. The script component might have an Update() function \nthat is called whenever the game object is updated, and the scripted compo-\nnent might also be permitt ed to register some of its member functions/meth-\nods as event handlers. When an event is sent to the game object, it calls the \n\n\n813 \nappropriate event handler on the scripted component, thus giving the script \nprogrammer an opportunity to modify or extend the behavior of the natively \nimplemented game object.\n14.8.5.6. Scripted Finite State Machines\nMany problems in game programming can be solved naturally using ﬁ nite \nstate machines (FSM). For this reason, some engines build the concept of ﬁ nite \nstate machines right into the core game object model. In such engines, every \ngame object can have one or more states, and it is the states—not the game \nobject itself—that contain the update function, event handler functions, and \nso on. Simple game objects can be created by deﬁ ning a single state, but more-\ncomplex game objects have the freedom to deﬁ ne multiple states, each with a \ndiﬀ erent update and event-handling behavior.\nIf your engine supports a state-driven game object model, it makes a lot of \nsense to provide ﬁ nite state machine support in the scripting language as well. \nAnd of course, even if the core game object model doesn’t support ﬁ nite state \nmachines natively, one can still provide state-driven behavior by using a state \nmachine on the script side. An FSM can be implemented in any programming \nlanguage by using class instances to represent states, but some scripting lan-\nguages provide tools especially for this purpose. An object-oriented scripting \nlanguage might provide custom syntax that allows a class to contains multiple \nstates, or it might provide tools that help the script programmer easily aggre-\ngate state objects together within a central hub object and then delegate the \nupdate and event-handling functions to it in a straightforward way. But even \nif your scripting language provides no such features, you can always adopt a \nmethodology for implementing FSMs and follow those conventions in every \nscript you write.\n14.8.5.7. Multithreaded Scripts\nIt’s  oft en useful to be able to execute multiple scripts in parallel. This is espe-\ncially true on today’s highly parallelized hardware architectures. If multiple \nscripts can run at the same time, we are in eﬀ ect providing parallel threads of \nexecution in script code, much like the threads provided by most multitasking \noperating systems. Of course, the scripts may not actually run in parallel—if \nthey are all running on a single CPU, the CPU must take turns executing each \none. However, from the point of view of the script programmer, the paradigm \nis one of parallel multithreading.\nMost scripting systems that provide parallelism do so via cooperative \nmultitasking . This means that a script will execute until it explicitly yields to \nanother script. This is in contrast with a preemptive multitasking approach, in \n14.8. Scripting\n\n\n814 \n14. Runtime Gameplay Foundation Systems\nwhich the execution of any script could be interrupted at any time to permit \nanother script to execute.\nOne simple approach to cooperative multitasking in script is to permit \nscripts to explicitly go to sleep, waiting for something relevant to happen. A \nscript might wait for a speciﬁ ed number of seconds to elapse, or it might wait \nuntil a particular event is received. It might wait until another thread of execu-\ntion has reached a predeﬁ ned synchronization point. Whatever the reason, \nwhenever a script goes to sleep, it puts itself on a list of sleeping script threads \nand tells the virtual machine that it can start executing another eligible script. \nThe system keeps track of the conditions that will wake up each sleeping \nscript—when one of these conditions becomes true, the script(s) waiting on \nthe condition are woken up and allowed to continue executing.\nTo see how this works in practice, let’s look at an example of a multi-\nthreaded script. This script manages the animations of two characters and \na door. The two characters are instructed to walk up to the door—each one \nmight take a diﬀ erent, and unpredictable, amount of time to reach it. We’ll \nput the script’s threads to sleep while they wait for the characters to reach the \ndoor. Once they both arrive at the door, one of the two characters opens the \ndoor, which it does by playing an “open door” animation. Note that we don’t \nwant to hard-code the duration of the animation into the script itself. That \nway, if the animators change the animation, we won’t have to go back and \nmodify our script. So we’ll put the threads to sleep again while the wait for the \nanimation to complete. A script that accomplishes this is shown below, using \na simple C-like pseudocode syntax.\nfunction DoorCinematic\n{\n \nthread Guy1\n {\n \n \n// Ask guy1 to walk to the door.\n  CharacterWalkToPoint(guy1, \ndoorPosition);\n \n \nWaitUntil(ARRIVAL); // go to sleep until he gets   \n        // \nthere\n \n \n// OK, we’re there. Tell the other threads via a   \n  // \nsignal.\n  RaiseSignal(\"Guy1Arrived\");\n \n \n// Wait for the other guy to arrive as well.\n  WaitUntil(SIGNAL, \n\"Guy2Arrived\");\n \n \n// Now tell guy1 to play the \"open door\"  \n \n \n \n  // \nanimation.\n  CharacterAnimate(guy1, \n\"OpenDoor\");\n  WaitUntil(ANIMATION_DONE);\n\n\n815 \n \n \n// OK, the door is open. Tell the other threads.\n  RaiseSignal(\"DoorOpen\");\n \n \n// Now walk thru the door.\n  CharacterWalkToPoint(guy1, \nbeyondDoorPosition);\n }\n \nthread Guy2\n {\n \n \n// Ask guy2 to walk to the door.\n  CharacterWalkToPoint(guy2, \ndoorPosition);\n \n \nWaitUntil(ARRIVAL); // go to sleep until he gets   \n        // \nthere\n \n \n// OK, we’re there. Tell the other threads via a   \n  // \nsignal.\n  RaiseSignal(\"Guy2Arrived\");\n \n \n// Wait for the other guy to arrive as well.\n  WaitUntil(SIGNAL, \n\"Guy1Arrived\");\n \n \n// Now wait until guy1 opens the door for me.\n  WaitUntil(SIGNAL, \n\"DoorOpen\");\n \n \n// OK, the door is open. Now walk thru the door.\n  CharacterWalkToPoint(guy2, \nbeyondDoorPosition);\n }\n}\nIn the above, we assume that our hypothetical scripting language pro-\nvides a simple syntax for deﬁ ning threads of execution within a single func-\ntion. We deﬁ ne two threads, one for Guy1 and one for Guy2.\nThe thread for Guy1 tells the character to walk to the door and then goes \nto sleep waiting for his arrival. We’re hand-waving a bit here, but let’s imagine \nthat the scripting language magically allows a thread to go to sleep, wait-\ning until a character in the game arrives at a target point to which he was \nrequested to walk. In reality, this might be implemented by arranging for the \ncharacter to send an event back to the script and then waking the thread up \nwhen the event arrives.\nOnce Guy1 arrives at the door, his thread does two things that warrant \nfurther explanation. First, it raises a signal called “Guy1Arrived.” Second, it \ngoes to sleep waiting for another signal called “Guy2Arrived.” If we look at \nthe thread for Guy2, we see a similar patt ern, only reversed. The purpose of \nthis patt ern of raising a signal and then waiting for another signal is used to \nsynchronize the two threads.\nIn our hypothetical scripting language, a signal is just a Boolean ﬂ ag with \na name. The ﬂ ag starts out false, but when a thread calls RaiseSignal(name), \n14.8. Scripting\n\n\n816 \n14. Runtime Gameplay Foundation Systems\nthe named ﬂ ag’s value changes to true. Other threads can go to sleep, wait-\ning for a particular named signal to become true. When it does, the sleeping \nthread(s) wake up and continue executing. In this example, the two threads \nare using the “Guy1Arrived” and “Guy2Arrived” signals to synchronize with \none another. Each thread raises its signal and then waits for the other thread’s \nsignal. It does not matt er which signal is raised ﬁ rst—only when both signals \nhave been raised will the two threads wake up. And when they do, they will \nbe in perfect synchronization. Two possible scenarios are illustrated in Fig-\nure 14.20, one in which Guy1 arrives ﬁ rst, the other in which Guy2 arrives \nﬁ rst. As you can see, the order in which the signals are raised is irrelevant, and \nthe threads always end up in sync aft er both signals have been raised.\n14.9. High-Level Game Flow\n A game object model provides the foundations upon which a rich and en-\ntertaining collection of game object types can be implemented with which to \npopulate our game worlds. However, by itself, a game object model only per-\nmits us to deﬁ ne the kinds of objects that exist in our game world and how \nthey behave individually. It says nothing of the player’s objectives, what hap-\npens if he or she completes them, and what fate should befall the player if he \nor she fails.\nFor this, we need some kind of system to control high-level game ﬂ ow. \nThis is oft en implemented as a ﬁ nite state machine . Each state usually repre-\nWalk\nSignal\nWait\nWalk\nSignal\n(No Wait)\nGuy1\nGuy2\nSync\nWalk\nSignal\nWait\nWalk\nSignal\n(No Wait)\nGuy1\nGuy2\nSync\nFigure 14.20.  Two examples showing how a simple pattern of raising one signal and then \nwaiting on another can be used to synchronize a pair of script threads.\n\n\n817 \nsents a single player objective or encounter and is associated with a particular \nlocale within the virtual game world. As the player completes each task, the \nstate machine advances to the next state, and the player is presented with a \nnew set of goals. The state machine also deﬁ nes what should happen in the \nevent of the player’s failure to accomplish the necessary tasks or objectives . \nOft en, failure sends the player back to the beginning of the current state, so he \nor she can try again. Sometimes aft er enough failures, the player has run out \nof “lives” and will be sent back to the main menu, where he or she can choose \nto play a new game. The ﬂ ow of the entire game, from the menus to the ﬁ rst \n“level” to the last, can be controlled through this high-level state machine.\nThe task system used in Naughty Dog’s Jak and Daxter and Uncharted fran-\nchises is an example of such a state-machine-based system. It allows for linear \nsequences of states (called tasks at Naughty Dog). It also permits parallel tasks, \nwhere one task branches out into two or more parallel tasks, which eventu-\nally merge back into the main task sequence. This parallel task feature sets \nthe Naughty Dog task graph apart from a regular state machine, since state \nmachines typically can only be in one state at a time.\n14.9. High-Level Game Flow\n\n\nPart V\nConclusion\n\n\n15\nYou Mean There’s More?\nC\nongratulations! You’ve reached the end of your journey through the \nlandscape of game engine architecture in one piece (and hopefully none \nthe worse for wear). With any luck, you’ve learned a great deal about the \nmajor components that comprise a typical game engine. But of course, every \njourney’s end is another’s beginning. There’s a great deal more to be learned \nwithin each and every topic area covered within these pages. As technology \nand computing hardware continue to improve, more things will become pos-\nsible in games—and more engine systems will be invented to support them. \nWhat’s more, this book’s focus was on the game engine itself. We haven’t even \nbegun to discuss the rich world of gameplay programming, a topic that could \nﬁ ll many more volumes.\nIn the following brief sections, I’ll identify a few of the engine and game-\nplay systems we didn’t have room to cover in any depth in this book, and I’ll \nsuggest some resources for those who wish to learn more about them.\n15.1. Some Engine Systems We Didn’t Cover\n15.1.1. Audio\nI mentioned in Section 1.6.13 that audio oft en takes a back seat to other aspects \nof game development, much to the chagrin of the audio engineers, sound de-\n821\n\n\n822 \n15. You Mean There’s More?\nsigners, voice actors, and composers who work so hard to add that all-too-\ncritical fourth dimension to the virtual game world. And yet, sadly, the same \nthing has happened in this book—I am out of room and out of time, so a full \ntreatment of audio will have to wait until the second edition. (In keeping with \na long and painfully unfortunate tradition in game development, once again \naudio gets the shaft !)\nThankfully, a number of other books and online resources do provide a \nwealth of information on audio development. First oﬀ , I recommend reading \nthe documentation for Microsoft ’s XACT sound authoring tool and runtime \nAPI, located on the MSDN website under XACT: Tutorials and Samples (htt p://\nmsdn.microsoft .com/en-us/library/bb172329(VS.85).aspx). XACT supports \nvirtually every audio feature the average game programmer would want, and \nits documentation is quite easy to digest. The Game Programming Gems book \nseries also includes a plethora of articles on audio—see [7] Section 6 and [40] \nSection 6.\n15.1.2. Movie Player\nMost games include a movie player for displaying prerendered movies, also \nknown as full-motion video (FMV). The basic components of the movie player \nare an interface to the streaming ﬁ le I/O system (see Section 6.1.3), a codec to \ndecode the compressed video stream, and some form of synchronization with \nthe audio playback system for the sound track.\nA number of diﬀ erent video encoding standards and corresponding \ncodecs are available, each one suited to a particular type of application. For \nexample, video CDs (VCD) and DVDs use  MPEG-1 and MPEG-2 (H.262) \ncodecs, respectively. The H.261 and H.263 standards are designed primar-\nily for online video conferencing applications. Games oft en use standards \nlike MPEG-4 part 2 (e.g., DivX ), MPEG-4 Part 10 / H.264, Windows Media \nVideo (WMV ), or Bink Video (a standard designed speciﬁ cally for games by \nRad Game Tools, Inc.). See htt p://en.wikipedia.org/wiki/Video_codec and \nhtt p://www.radgametools.com/bnkmain.htm for more information on video \ncodecs.\n15.1.3. Multiplayer Networking\nAlthough we have touched on a number of aspects of multiplayer game ar-\nchitecture and networking (e.g., Sections 1.6.14, 7.7, and 14.8.3.2), this book’s \ncoverage of the topic is far from complete. For an in-depth treatment of multi-\nplayer networking, see [3].\n\n\n823 \n15.2. Gameplay Systems\n15.2. Gameplay Systems\nA game is of course much more than just its engine. On top of the gameplay \nfoundation layer (discussed in Chapter 14), you’ll ﬁ nd a rich assortment of \ngenre- and game-speciﬁ c gameplay systems. These systems tie the myriad \ngame engine technologies described in this book together into a cohesive \nwhole, breathing life into the game.\n15.2.1. Player Mechanics\nPlayer mechanics are of course the most important gameplay system. Each \ngenre is deﬁ ned by a general style of player mechanics and gameplay, and \nof course every game within a genre has its own speciﬁ c designs. As such, \nplayer mechanics is a huge topic. It involves the integration of human inter-\nface device systems, motion simulation, collision detection, animation, and \naudio, not to mention integration with other gameplay systems like the game \ncamera, weapons, cover, specialized traversal mechanics (ladders, swinging \nvines, etc.), vehicle systems, puzzle mechanics, and so on.\nClearly player mechanics are as varied as the games themselves, so there’s \nno one place you can go to learn all about them. It’s best to tackle this topic \nby studying a single genre at a time. Play games and try to reverse-engineer \ntheir player mechanics. Then try to implement them yourself! And as a very \nmodest start on reading, you can check out [7] Section 4.11 for a discussion of \nMario-style platformer player mechanics.\n15.2.2. Cameras\nA game’s camera system is almost as important as the player mechanics. In \nfact, the camera can make or break the gameplay experience. Each genre tends \nto have its own camera control style, although of course every game within a \nparticular genre does it a litt le bit diﬀ erently (and some very diﬀ erently!). See \n[6] Section 4.3 for some basic game camera control techniques. In the follow-\ning paragraphs, I’ll brieﬂ y outline some of the most prevalent kinds of cam-\neras in 3D games, but please note that this is far from a complete list.\nLook-at cameras.\n• \n This type of camera rotates about a target point and can \nbe moved in and out relative to this point.\nFollow cameras.\n• \n This type of camera is prevalent in platformer, third-\nperson shooter, and vehicle-based games. It acts much like a look-at \ncamera focused on the player character/avatar/vehicle, but its motion \ntypically lags the player. A follow camera alos includes advanced \ncollision detection and avoidance logic and provides the human player \n\n\n824 \n15. You Mean There’s More?\nwith some degree of control over the camera’s orientation relative to the \nplayer avatar.\nFirst-person cameras.\n• \n As the player character moves about in the game \nworld, a ﬁ rst-person camera remains aﬃ  xed to the character’s virtual \neyes. The player typically has full control over the direction in which \nthe camera should be pointed, either via mouse or joypad control. \nThe look direction of the camera also translates directly into the aim \ndirection of the player’s weapon, which is typically indicated by a set of \ndisembodied arms and a gun att ached to the bott om of the screen, and a \nreticle at the center of the screen.\nRTS cameras.\n• \n Real-time strategy and god games tend to employ a camera \nthat ﬂ oats above the terrain, looking down at an angle. The camera can \nbe panned about over the terrain, but the pitch and yaw of the camera \nare usually not under direct player control.\nCinematic cameras.\n• \n Most three-dimensional games have at least some \ncinematic moments in which the camera ﬂ ies about within the scene \nin a more ﬁ lmic manner rather than being tethered to an object in the \ngame.\n15.2.3. Artiﬁ cial Intelligence\nAnother major component of most character-based games is artiﬁ cial intelli-\ngence (AI ). At its lowest level, an AI system is usually founded in technologies \nlike basic path ﬁ nding (which commonly makes use of the well-known A* \nalgorithm ), perception systems (line of sight, vision cones, knowledge of the \nenvironment, etc.) and some form of memory.\nOn top of these foundations, character control logic is implemented. \nA character control system determines how to make the character perform \nspeciﬁ c actions like locomoting, navigating unusual terrain features, using \nweapons, driving vehicles, taking cover, and so on. It typically involves com-\nplex interfaces to the collision, physics, and animation systems within the \nengine. Character control is discussed in detail in Sections 11.11 and 11.12.\nAbove the character control layer, an AI system typically has goal sett ing \nand decision making logic, emotional state, group behaviors (coordination, \nﬂ anking, crowd and ﬂ ocking behaviors, etc.), and possibly some advanced \nfeatures like an ability to learn from past mistakes or adapt to a changing \nenvironment.\nOf course, the term “artiﬁ cial intelligence” is a bit of a misnomer when \napplied to games. Game AI is usually more smoke and mirrors than it is an \natt empt at true artiﬁ cial intelligence. It’s important to realize that, in a game, \n\n\n825 \nall that really matt ers is the player’s perception of what is going on. A classic \nexample comes from the game Halo . When Bungie ﬁ rst implemented their AI \nsystem, they included a simple rule that stated that the small “grunt” aliens \nwould all run away when their leader had died. In play test aft er play test, \nno one realized that this was why the litt le guys were running away. Even af-\nter the Bungie team had made various adjustments to the animations and AI \nbehaviors in the game, still no one got the connection. Finally, the developers \nresorted to having one of the grunts say, “Leader dead! Run away!” This just \ngoes to show that all the AI logic in the world doesn’t amount to anything if \nthe player doesn’t perceive the meaning behind it.\nAI programming is a rich topic, and we certainly have not done it justice \nin this book. For more information, see [16], [6] Section 3, [7] Section 3, and \n[40] Section 3.\n15.2.4. Other Gameplay Systems\nClearly there’s a lot more to a game than just player mechanics, cameras, and \nAI. Some games have drivable vehicles, implement specialized types of weap-\nonry, allow the player to destroy the environment with the help of a dynam-\nic physics simulation, let the player create his or her own characters, build \ncustom levels, require the player to solve puzzles, or… Of course, the list of \ngenre- and game-speciﬁ c features, and all of the specialized soft ware systems \nthat implement them, could go on forever. Gameplay systems are as rich and \nvaried as games are. Perhaps this is where your next journey as a game pro-\ngrammer will begin!\n15.2. Gameplay Systems\n",
      "page_number": 830,
      "chapter_number": 41,
      "summary": "This chapter covers segment 41 (pages 830-848). Key topics include game, script, and object. Runtime Gameplay Foundation Systems\nin some suitable way and that the global function DcLookUpByteCode() is \ncapable of looking up any script lambda by name.",
      "keywords": [
        "Game Object",
        "Game",
        "script",
        "Object",
        "game object model",
        "event",
        "Gameplay Foundation Systems",
        "scripting language",
        "player",
        "Systems",
        "Gameplay Systems",
        "state",
        "event handler functions",
        "game object type",
        "function"
      ],
      "concepts": [
        "game",
        "script",
        "object",
        "objectives",
        "function",
        "functions",
        "functional",
        "systems",
        "video",
        "player"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents In Action",
          "chapter": 18,
          "title": "Segment 18 (pages 147-158)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 26,
          "title": "Segment 26 (pages 243-252)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 7,
          "title": "Segment 7 (pages 56-64)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 12,
          "title": "Segment 12 (pages 101-108)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        },
        {
          "book": "makinggames",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 849-853)",
      "start_page": 849,
      "end_page": 853,
      "detection_method": "synthetic",
      "content": "827\n \nReferences\n[1] \nTomas Akenine-Moller, Eric Haines, and Naty Hoﬀ man. Real-Time Rendering \n(3rd Edition). Wellesley, MA: A K Peters, 2008.\n[2] \nAndrei Alexandrescu. Modern C++ Design: Generic Programming and Design \nPatt erns Applied. Resding, MA: Addison-Wesley, 2001.\n[3] \nGrenville Armitage, Mark Claypool and Philip Branch. Networking and Online \nGames: Understanding and Engineering Multiplayer Internet Games. New York, \nNY: John Wiley and Sons, 2006.\n[4] \nJames Arvo (editor). Graphics Gems II. San Diego, CA: Academic Press, \n1991.\n[5] \nGrady Booch, Robert A. Maksimchuk, Michael W. Engel, Bobbi J. Young, \nJim Conallen, and Kelli A. Houston. Object-Oriented Analysis and Design with \nApplications, third edition. Reading, MA: Addison-Wesley, 2007.\n[6] \nMark DeLoura (editor). Game Programming Gems. Hingham, MA: Charles \nRiver Media, 2000.\n[7] \nMark DeLoura (editor). Game Programming Gems 2. Hingham, MA: Charles \nRiver Media, 2001.\n[8] \nPhilip Dutré, Kavita Bala and Philippe Bekaert. Advanced Global Illumination \n(Second Edition). Wellesley, MA: A K Peters, 2006.\n\n\n828 \nReferences\n[9] \nDavid H. Eberly. 3D Game Engine Design: A Practical Approach to Real-Time \nComputer Graphics. San Francisco, CA: Morgan Kaufmann, 2001.\n[10] David H. Eberly. 3D Game Engine Architecture: Engineering Real-Time \nApplications with Wild Magic. San Francisco, CA: Morgan Kaufmann, 2005.\n[11] David H. Eberly. Game Physics. San Francisco, CA: Morgan Kaufmann, \n2003.\n[12] Christer Ericson. Real-Time Collision Detection. San Francisco, CA: Morgan \nKaufmann, 2005.\n[13] Randima Fernando (editor). GPU Gems: Programming Techniques, Tips and \nTricks for Real-Time Graphics. Reading, MA: Addison-Wesley, 2004.\n[14] James D. Foley, Andries van Dam, Steven K. Feiner, and John F. Hughes. \nComputer Graphics: Principles and Practice in C, second edition. Reading, MA: \nAddison-Wesley, 1995.\n[15] Grant R. Fowles and George L. Cassiday. Analytical Mechanics (7th Edition). \nPaciﬁ c Grove, CA: Brooks Cole, 2005.\n[16] John David Funge. AI for Games and Animation: A Cognitive Modeling Approach. \nWellesley, MA: A K Peters, 1999.\n[17] Erich Gamma, Richard Helm, Ralph Johnson, and John M. Vlissiddes. \nDesign Patt erns: Elements of Reusable Object-Oriented Soft ware. Reading, MA: \nAddison-Wesley, 1994.\n[18] Andrew S. Glassner (editor). Graphics Gems I. San Francisco, CA: Morgan \nKaufmann, 1990.\n[19] Paul S. Heckbert (editor). Graphics Gems IV. San Diego, CA: Academic Press, \n1994.\n[20] Maurice Herlihy, Nir Shavit. The Art of Multiprocessor Programming. San \nFrancisco, CA: Morgan Kaufmann, 2008.\n[21] Roberto Ierusalimschy, Luiz Henrique de Figueiredo and Waldemar Celes. \nLua 5.1 Reference Manual. Lua.org, 2006.\n[22] Roberto Ierusalimschy. Programming in Lua, 2nd Edition. Lua.org, 2006.\n[23] Isaac Victor Kerlow. The Art of 3-D Computer Animation and Imaging (Second \nEdition). New York, NY: John Wiley and Sons, 2000.\n[24] David Kirk (editor). Graphics Gems III. San Francisco, CA: Morgan Kaufmann, \n1994.\n[25] Danny Kodicek. Mathematics and Physics for Game Programmers. Hingham, \nMA: Charles River Media, 2005.\n\n\n829 \nReferences\n[26] Raph Koster. A Theory of Fun for Game Design. Phoenix, AZ: Paraglyph, \n2004.\n[27] John Lakos. Large-Scale C++ Soft ware Design. Reading, MA: Addison-Wesley, \n1995.\n[28] Eric Lengyel. Mathematics for 3D Game Programming and Computer Graphics, \n2nd Edition. Hingham, MA: Charles River Media, 2003.\n[29] Tuoc V. Luong, James S. H. Lok, David J. Taylor and Kevin Driscoll. \nInternationalization: Developing Soft ware for Global Markets. New York, NY: \nJohn Wiley & Sons, 1995.\n[30] Steve Maguire. Writing Solid Code: Microsoft ’s Techniques for Developing Bug-\nFree C Programs. Bellevue, WA: Microsoft  Press, 1993.\n[31] Scott  Meyers. Eﬀ ective C++: 55 Speciﬁ c Ways to Improve Your Programs and \nDesigns (3rd Edition). Reading, MA: Addison-Wesley, 2005.\n[32] Scott  Meyers. More Eﬀ ective C++: 35 New Ways to Improve Your Programs and \nDesigns. Reading, MA: Addison-Wesley, 1996.\n[33] Scott  Meyers. Eﬀ ective STL: 50 Speciﬁ c Ways to Improve Your Use of the Standard \nTemplate Library. Reading, MA: Addison-Wesley, 2001.\n[34] Ian Millington. Game Physics Engine Development. San Francisco, CA: Morgan \nKaufmann, 2007.\n[35] Hubert Nguyen (editor). GPU Gems 3. Reading, MA: Addison-Wesley, 2007.\n[36] Alan W. Paeth (editor). Graphics Gems V. San Francisco, CA: Morgan \nKaufmann, 1995.\n[37] C. Michael Pilato, Ben Collins-Sussman, and Brian W. Fitzpatrick. Version \nControl with Subversion, second edition. Sebastopol , CA: O’Reilly Media, \n2008. (Commonly known as “The Subversion Book.” Available online at \nhtt p://svnbook.red-bean.com.)\n[38] Matt  Pharr (editor). GPU Gems 2: Programming Techniques for High-Performance \nGraphics and General-Purpose Computation. Reading, MA: Addison-Wesley, \n2005.\n[39] Bjarne Stroustrup. The C++ Programming Language, special edition (3rd \nedition). Reading, MA: Addison-Wesley, 2000.\n[40] Dante Treglia (editor). Game Programming Gems 3. Hingham, MA: Charles \nRiver Media, 2002.\n[41] Gino van den Bergen. Collision Detection in Interactive 3D Environments. San \nFrancisco, CA: Morgan Kaufmann, 2003.\n\n\n830 \nReferences\n[42] Alan Watt . 3D Computer Graphics (3rd Edition). Reading, MA: Addison \nWesley, 1999.\n[43] James Whitehead II, Bryan McLemore and Matt hew Orlando. World of \nWarcraft  Programming: A Guide and Reference for Creating WoW Addons. New \nYork, NY: John Wiley & Sons, 2008.\n[44] Richard Williams. The Animator’s Survival Kit. London, England: Faber & \nFaber, 2002.\n\n\nGame Engine Architecture\n\nJason Gregory\n\nForeword by Jeff Lander and Matt Whiting\n\nThis book covers both the theory and practice of game engine software development, bringing\ntogether complete coverage of a wide range of topics. The concepts and techniques described\nare the actual ones used by real game studios like Electronic Arts and Naughty Dog. The\nexamples are often grounded in specific technologies, but the discussion extends way beyond\nany particular engine or API. The references and citations make it a great jumping off point for\nthose who wish to dig deeper into any particular aspect of the game development process.\nIntended as the text for a college level series in game programming, this book can also be used by\namateur software engineers, hobbyists, self-taught game programmers, and existing members\nof the game industry. Junior game engineers can use it to solidify their understanding of game\ntechnology and engine architecture. Even senior engineers who specialize in one particular field\nof game development can benefit from the bigger picture presented in these pages.\nTopics include:\n\ne large-scale C++ software architecture in a games context\n\ne mathematics for game programming\n\ne game development tools for debugging, source control, and profiling\n\ne engine subsystems including engine foundation systems, rendering, collision,\nphysics, character animation, and game world object models\n\ne multiplatform game engines\n© game programming in multiprocessor environments\n\ne tool pipelines and the game asset database\n\nJason Gregory has worked as a software engineer in the games industry since March 1999 and\nas a professional software engineer since 1994. He got his start in game programming at Midway\nHome Entertainment in San Diego. He also wrote the Playstation 2/Xbox animation system for\nFreaky Flyers and Crank the Weasel. In 2003, Jason moved to Electronic Arts Los Angeles, where he\nworked on engine and game play technology for Medal of Honor: Pacific Assault and served as a\nlead engineer on the Medal of Honor: Airborne project. Jason is currently a Generalist Programmer\nat Naughty Dog Inc., where he developed engine and gameplay software for Uncharted: Drake's\nFortune and is now working on Uncharted 2: Among Thieves. Jason also teaches courses in game\ntechnology at the University of Southern California.\n\nISBN 978-1-56881-413-1\n\n9 \\| |\n\n131",
      "page_number": 849,
      "chapter_number": 42,
      "summary": "This chapter covers segment 42 (pages 849-853). Key topics include games, engineering, and engine. Modern C++ Design: Generic Programming and Design \nPatt erns Applied.",
      "keywords": [
        "Naty Hoﬀ man",
        "Morgan Kaufmann",
        "Naty Hoﬀ",
        "San Francisco",
        "Charles River Media",
        "Game Programming Gems",
        "Game",
        "Game Programming",
        "Graphics Gems",
        "Programming",
        "Edition",
        "Francisco",
        "Morgan",
        "Kaufmann",
        "Tomas Akenine-Moller"
      ],
      "concepts": [
        "games",
        "engineering",
        "engine",
        "editor",
        "wesley",
        "programming",
        "programs",
        "edition",
        "developing",
        "design"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 7",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 10,
          "title": "Segment 10 (pages 91-99)",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "A Philosophy of Software Design",
          "chapter": 2,
          "title": "Segment 2 (pages 10-17)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 4,
          "title": "Segment 4 (pages 26-33)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        }
      ]
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "chapter": null,
      "content": "Game Engine Architecture\n\nJason Gregory\n\nForeword by Jeff Lander and Matt Whiting",
      "content_length": 81,
      "extraction_method": "OCR"
    },
    {
      "page_number": 2,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 3,
      "chapter": null,
      "content": "Game Engine Architecture\n",
      "content_length": 25,
      "extraction_method": "Direct"
    },
    {
      "page_number": 4,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 5,
      "chapter": null,
      "content": "Game Engine Architecture\nJason Gregory\nA K Peters, Ltd.\nWellesley, Massachusetts\n",
      "content_length": 81,
      "extraction_method": "Direct"
    },
    {
      "page_number": 6,
      "chapter": null,
      "content": "A K Peters/CRC Press\nTaylor & Francis Group\n6000 Broken Sound Parkway NW, Suite 300\nBoca Raton, FL 33487-2742\n© 2009 by Taylor and Francis Group, LLC\nA K Peters/CRC Press is an imprint of Taylor & Francis Group, an Informa business\nNo claim to original U.S. Government works\nPrinted in the United States of America on acid-free paper\n10 9 8 7 6 5 4 3 2 1\nInternational Standard Book Number-13: 978-1-4398-6526-2 (Ebook-PDF)\nThis book contains information obtained from authentic and highly regarded sources. Reasonable efforts have been made to publish reliable data and information, but the author \nand publisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders \nof all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been \nacknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or utilized in any form by any electronic, mechanical, or other means, \nnow known or hereafter invented, including photocopying, microfilming, and recording, or in any information storage or retrieval system, without written permission from the \npublishers.\nFor permission to photocopy or use material electronically from this work, please access www.copyright.com (http://www.copyright.com/) or contact the Copyright Clearance \nCenter, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 978-750-8400. CCC is a not-for-profit organization that provides licenses and registration for a variety of users. For \norganizations that have been granted a photocopy license by the CCC, a separate system of payment has been arranged.\nTrademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used only for identification and explanation without intent to infringe.\nVisit the Taylor & Francis Web site at\nhttp://www.taylorandfrancis.com\nand the A K Peters Web site at\nhttp://www.akpeters.com \n",
      "content_length": 2183,
      "extraction_method": "Direct"
    },
    {
      "page_number": 7,
      "chapter": null,
      "content": "Dedicated to\nTrina, Evan and Quinn Gregory,\nin memory of our heros,\nJoyce Osterhus and Kenneth Gregory.\n",
      "content_length": 104,
      "extraction_method": "Direct"
    },
    {
      "page_number": 8,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 9,
      "chapter": null,
      "content": "vii\nContents\n \nForeword \nxiii\n \nPreface \nxvii\nI \nFoundations \n1\n1 \nIntroduction \n3\n1.1  \nStructure of a Typical Game Team \n5\n1.2 \nWhat Is a Game? \n8\n1.3 \nWhat Is a Game Engine? \n11\n1.4 \nEngine Differences Across Genres \n13\n1.5 \nGame Engine Survey \n25\n1.6 \nRuntime Engine Architecture \n28\n1.7 \n Tools and the Asset Pipeline \n49\n2 \nTools of the Trade \n57\n2.1 \n Version Control \n57\n2.2 \nMicrosoft Visual Studio \n66\n2.3 \nProﬁ ling Tools \n85\n",
      "content_length": 437,
      "extraction_method": "Direct"
    },
    {
      "page_number": 10,
      "chapter": null,
      "content": "viii \nContents\n2.4 \nMemory Leak and Corruption Detection \n87\n2.5 \nOther Tools \n88\n3 \nFundamentals of Software \n \nEngineering for Games \n91\n3.1 \nC++ Review and Best Practices \n91\n3.2 \nData, Code, and Memory in C/C++ \n98\n3.3 \nCatching and Handling Errors \n128\n4 \n3D Math for Games \n137\n4.1 \nSolving 3D Problems in 2D \n137\n4.2 \nPoints and Vectors \n138\n4.3 \nMatrices \n151\n4.4 \nQuaternions \n169\n4.5 \nComparison of Rotational Representations \n177\n4.6 \nOther Useful Mathematical Objects \n181\n4.7 \nHardware-Accelerated SIMD Math \n185\n4.8 \nRandom Number Generation \n192\nII \nLow-Level Engine Systems \n195\n5 \nEngine Support Systems \n197\n5.1 \nSubsystem Start-Up and Shut-Down \n197\n5.2 \nMemory Management \n205\n5.3 \nContainers \n223\n5.4 \nStrings \n242\n5.5 \nEngine Conﬁ guration \n252\n6 \nResources and the File System \n261\n6.1 \nFile System \n262\n6.2 \nThe Resource Manager \n272\n7 \nThe Game Loop and Real-Time Simulation \n303\n7.1 \nThe Rendering Loop \n303\n7.2 \nThe Game Loop \n304\n",
      "content_length": 958,
      "extraction_method": "Direct"
    },
    {
      "page_number": 11,
      "chapter": null,
      "content": "ix \nContents\n7.3 \nGame Loop Architectural Styles \n307\n7.4 \nAbstract Timelines \n310\n7.5 \nMeasuring and Dealing with Time \n312\n7.6 \nMultiprocessor Game Loops \n324\n7.7 \nNetworked Multiplayer Game Loops \n333\n8 \nHuman Interface Devices (HID) \n339\n8.1 \nTypes of Human Interface Devices \n339\n8.2 \nInterfacing with a HID \n341\n8.3 \nTypes of Inputs \n343\n8.4 \nTypes of Outputs \n348\n8.5 \nGame Engine HID Systems \n349\n8.6 \nHuman Interface Devices in Practice \n366\n9 \nTools for Debugging and Development \n367\n9.1 \nLogging and Tracing \n367\n9.2 \nDebug Drawing Facilities \n372\n9.3 \nIn-Game Menus \n379\n9.4 \nIn-Game Console \n382\n9.5 \nDebug Cameras and Pausing the Game \n383\n9.6 \nCheats \n384\n9.7 \nScreen Shots and Movie Capture \n384\n9.8 \nIn-Game Proﬁ ling \n385\nIII Graphics and Motion \n397\n10 The Rendering Engine \n399\n10.1 \nFoundations of Depth-Buffered \n \nTriangle Rasterization \n400\n10.2 \nThe Rendering Pipeline \n444\n10.3 \nAdvanced Lighting and Global Illumination \n469\n10.4 \nVisual Effects and Overlays \n481\n11 \nAnimation Systems \n491\n11.1 \nTypes of Character Animation \n491\n11.2 \nSkeletons \n496\n",
      "content_length": 1080,
      "extraction_method": "Direct"
    },
    {
      "page_number": 12,
      "chapter": null,
      "content": "x \nContents\n11.3 \nPoses \n499\n11.4 \nClips \n504\n11.5 \nSkinning and Matrix Palette Generation \n518\n11.6 \nAnimation Blending \n523\n11.7 \nPost-Processing \n542\n11.8 \nCompression Techniques \n545\n11.9 \nAnimation System Architecture \n552\n11.10 \nThe Animation Pipeline \n553\n11.11 \nAction State Machines \n568\n11.12 \nAnimation Controllers \n593\n12 \nCollision and Rigid Body Dynamics \n595\n12.1 \nDo You Want Physics in Your Game? \n596\n12.2 \nCollision/Physics Middleware \n601\n12.3 \nThe Collision Detection System \n603\n12.4 \nRigid Body Dynamics \n630\n12.5 \nIntegrating a Physics Engine into Your Game \n666\n12.6 \nA Look Ahead: Advanced Physics Features \n684\nIV Gameplay \n687\n13 \nIntroduction to Gameplay Systems \n689\n13.1 \nAnatomy of a Game World \n690\n13.2 \nImplementing Dynamic Elements: Game Objects \n695\n13.3 \nData-Driven Game Engines \n698\n13.4 \nThe Game World Editor \n699\n14 \nRuntime Gameplay Foundation Systems \n711\n14.1 \nComponents of the Gameplay \n \nFoundation System \n711\n14.2 \nRuntime Object Model Architectures \n715\n14.3 \nWorld Chunk Data Formats \n734\n14.4 \nLoading and Streaming Game Worlds \n741\n14.5 \nObject References and World Queries \n750\n14.6 \nUpdating Game Objects in Real Time \n757\n",
      "content_length": 1180,
      "extraction_method": "Direct"
    },
    {
      "page_number": 13,
      "chapter": null,
      "content": "xi \nContents\n14.7 \nEvents and Message-Passing \n773\n14.8 \nScripting \n794\n14.9 \nHigh-Level Game Flow \n817\nV Conclusion \n819\n15 \nYou Mean There’s More? \n821\n15.1 \nSome Engine Systems We Didn’t Cover \n821\n15.2 \nGameplay Systems \n823\n \nReferences \n827\n \nIndex \n831\n",
      "content_length": 260,
      "extraction_method": "Direct"
    },
    {
      "page_number": 14,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 15,
      "chapter": null,
      "content": "xiii\nForeword\nT\nhe very ﬁ rst video game was built entirely out of hardware, but rapid ad-\nvancements in microprocessors have changed all that. These days, video \ngames are played on versatile PCs and specialized video game consoles that \nuse soft ware to make it possible to oﬀ er a tremendous variety of gaming ex-\nperiences. It’s been 50 years since those ﬁ rst primitive games, but the industry \nis still considered by many to be immature. It may be young, but when you \ntake a closer look, you will ﬁ nd that things have been developing rapidly. \nVideo games are now a multibillion-dollar industry covering a wide range of \ndemographics.\nVideo games come in all shapes and sizes, falling into categories or \n“genres” covering everything from solitaire to massively multiplayer online \nrole-playing games, and these games are played on virtually anything with a \nmicrochip in it. These days, you can get games for your PC, your cell phone, \nas well as a number of diﬀ erent specialized gaming consoles—both handheld \nand those that connect to your home TV. These specialized home consoles \ntend to represent the cutt ing edge of gaming technology, and the patt ern of \nthese platforms being released in cycles has come to be called console “gen-\nerations.” The powerhouses of this latest generation are Microsoft ’s Xbox 360 \nand Sony’s PLAYSTATION 3, but the ever-present PC should never be over-\nlooked, and the extremely popular Nintendo Wii represents something new \nthis time around. \n",
      "content_length": 1494,
      "extraction_method": "Direct"
    },
    {
      "page_number": 16,
      "chapter": null,
      "content": "xiv \nForeword\nThe recent explosion of downloadable and casual games has added even \nmore complexity to the diverse world of commercial video games. Even so, \nbig games are still big business. The incredible computing power available \non today’s complicated platforms has made room for increased complexity in \nthe soft ware. Naturally, all this advanced soft ware has to be created by some-\none, and that has driven up the size of development teams—not to mention \ndevelopment costs. As the industry matures, we’re always looking for bett er, \nmore eﬃ  cient ways to build our products, and development teams have be-\ngun compensating for the increased complexity by taking advantage of things \nlike reusable soft ware and middleware.\nWith so many diﬀ erent styles of game on such a wide array of platforms, \nthere cannot be any single ideal soft ware solution. However, certain patt erns \nhave developed, and there is a vast menu of potential solutions out there. The \nproblem today is choosing the right solution to ﬁ t the needs of the particular \nproject. Going deeper, a development team must consider all the diﬀ erent as-\npects of a project and how they ﬁ t together. It is rare to ﬁ nd any one soft ware \npackage that perfectly suits every aspect of a new game design.\nThose of us who are now veterans of the industry found ourselves pio-\nneering unknown territory. Few programmers of our generation have Com-\nputer Science degrees (Matt ’s is in Aeronautical Engineering, and Jason’s is \nin Systems Design Engineering), but these days many colleges are starting to \nprograms and degrees in video games. The students and developers of today \nneed a good place to turn to for solid game-development information. For \npure high-end graphics, there are a lot of sources of very good information \nfrom research to practical jewels of knowledge. However, these sources are \noft en not directly applicable to production game environments or suﬀ er from \nnot having actual production-quality implementations. For the rest of game \ndevelopment, there are so-called beginner books that so gloss over the details \nand act as if they invented everything without giving references that they are \njust not useful or oft en even accurate. Then there are high-end specialty books \nfor various niches like physics, collision, AI, etc. But these can be needlessly \nobtuse or too high level to be understood by all, or the piecemeal approach just \ndoesn’t all ﬁ t together. Many are even so directly tied to a particular piece of \ntechnology as to become rapidly dated as the hardware and soft ware change.\nThen there is the Internet, which is an excellent supplementary tool for \nknowledge gathering. However, broken links, widely inaccurate data, and \nvariable-to-poor quality oft en make it not useful at all unless you know ex-\nactly what you are aft er.\nEnter Jason Gregory, himself an industry veteran with experience at \nNaughty Dog—one of the most highly regarded video game studios in the \n",
      "content_length": 2989,
      "extraction_method": "Direct"
    },
    {
      "page_number": 17,
      "chapter": null,
      "content": "xv \nForeword\nworld. While teaching a course in game programming at USC, Jason found \nhimself facing a shortage of textbooks covering the fundamentals of video-\ngame architecture. Luckily for the rest of us, he has taken it upon himself to \nﬁ ll that gap.\nWhat Jason has done is pull together production-quality knowledge actu-\nally used in shipped game projects and bring together the entire game-devel-\nopment picture. His experience has allowed him to bring together not only \nthe ideas and techniques but also actual code samples and implementation \nexamples to show you how the pieces come together to actually make a game. \nThe references and citations make it a great jumping-oﬀ  point to dig deeper \ninto any particular aspect of the process. The concepts and techniques are the \nactual ones we use to create games, and while the examples are oft en ground-\ned in a technology, they extend way beyond any particular engine or API.\nThis is the kind of book we wanted when we were gett ing started, and we \nthink it will prove very instructive to people just starting out as well as those \nwith experience who would like some exposure to the larger context.\nJeﬀ  Lander\nMatt hew Whiting\n",
      "content_length": 1192,
      "extraction_method": "Direct"
    },
    {
      "page_number": 18,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 19,
      "chapter": null,
      "content": "xvii\nPreface\nW\nelcome to Game Engine Architecture. This book aims to present a com-\nplete discussion of the major components that make up a typical com-\nmercial game engine. Game programming is an immense topic, so we have a \nlot of ground to cover. Nevertheless, I trust you’ll ﬁ nd that the depth of our \ndiscussions is suﬃ  cient to give you a solid understanding of both the theory \nand the common practices employed within each of the engineering disci-\nplines we’ll cover. That said, this book is really just the beginning of a fasci-\nnating and potentially life-long journey. A wealth of information is available \non all aspects of game technology, and this text serves both as a foundation-\nlaying device and as a jumping-oﬀ  point for further learning.\nOur focus in this book will be on game engine technologies and architec-\nture. This means we’ll cover both the theory underlying the various subsys-\ntems that comprise a commercial game engine and also the data structures, \nalgorithms, and soft ware interfaces that are typically used to implement them. \nThe line between the game engine and the game is rather blurry. We’ll fo-\ncus primarily on the engine itself, including a host of low-level foundation \nsystems, the rendering engine, the collision system, the physics simulation, \ncharacter animation, and an in-depth discussion of what I call the gameplay \nfoundation layer. This layer includes the game’s object model, world editor, \nevent system, and scripting system. We’ll also touch on some aspects of game-\n",
      "content_length": 1530,
      "extraction_method": "Direct"
    },
    {
      "page_number": 20,
      "chapter": null,
      "content": "xviii \nPreface\nplay programming, including player mechanics, cameras, and AI. However, \nby necessity, the scope of these discussions will be limited mainly to the ways \nin which gameplay systems interface with the engine.\nThis book is intended to be used as a course text for a two- or three-course \ncollege-level series in intermediate game programming. Of course, it can also \nbe used by amateur soft ware engineers, hobbyists, self-taught game program-\nmers, and existing members of the game industry alike. Junior engineers can \nuse this text to solidify their understanding of game mathematics, engine ar-\nchitecture, and game technology. And some senior engineers who have de-\nvoted their careers to one particular specialty may beneﬁ t from the bigger \npicture presented in these pages, as well.\nTo get the most out of this book, you should have a working knowledge \nof basic object-oriented programming concepts and at least some experience \nprogramming in C++. Although a host of new and exciting languages are be-\nginning to take hold within the game industry, industrial-strength 3D game \nengines are still writt en primarily in C or C++, and any serious game pro-\ngrammer needs to know C++. We’ll review the basic tenets of object-oriented \nprogramming in Chapter 3, and you will no doubt pick up a few new C++ \ntricks as you read this book, but a solid foundation in the C++ language is best \nobtained from [39], [31], and [32]. If your C++ is a bit rusty, I recommend you \nrefer to these or similar books to refresh your knowledge as you read this text. \nIf you have no prior C++ experience, you may want to consider reading at least \nthe ﬁ rst few chapters of [39], or working through a few C++ tutorials online, \nbefore diving into this book.\nThe best way to learn computer programming of any kind is to actually \nwrite some code. As you read through this book, I strongly encourage you to \nselect a few topic areas that are of particular interest to you and come up with \nsome projects for yourself in those areas. For example, if you ﬁ nd character \nanimation interesting, you could start by installing Ogre3D and exploring its \nskinned animation demo. Then you could try to implement some of the anima-\ntion blending techniques described in this book, using Ogre. Next you might \ndecide to implement a simple joypad-controlled animated character that can \nrun around on a ﬂ at plane. Once you have something relatively simple work-\ning, expand upon it! Then move on to another area of game technology. Rinse \nand repeat. It doesn’t particularly matt er what the projects are, as long as \nyou’re practicing the art of game programming, not just reading about it.\nGame technology is a living, breathing thing that can never be entirely \ncaptured within the pages of a book. As such, additional resources, errata, \nupdates, sample code, and project ideas will be posted from time to time on \nthis book’s website at htt p://gameenginebook.com.\n",
      "content_length": 2959,
      "extraction_method": "Direct"
    },
    {
      "page_number": 21,
      "chapter": null,
      "content": "xix \nPreface\nAcknowledgments\nNo book is created in a vacuum, and this one is certainly no exception. This \nbook would not have been possible without the help of my family, friends, \nand colleagues in the game industry, and I’d like to extend warm thanks to \neveryone who helped me to bring this project to fruition.\nOf course, the ones most impacted by a project like this one are invariably \nthe author’s family. So I’d like to start by oﬀ ering a special thank-you to my \nwife Trina, who has been a pillar of strength during this diﬃ  cult time, tak-\ning care of our two boys Evan (age 5) and Quinn (age 3) day aft er day (and \nnight aft er night!) while I holed myself up to get yet another chapter under \nmy belt, forgoing her own plans to accommodate my schedule, doing my \nchores as well as her own (more oft en than I’d like to admit), and always giv-\ning me kind words of encouragement when I needed them the most. I’d also \nlike to thank my eldest son Evan for being patient as he endured the absence \nof his favorite video game playing partner, and his younger brother Quinn \nfor always welcoming me home aft er a long day’s work with huge hugs and \nendless smiles.\nI would also like to extend special thanks to my editors, Matt  Whiting and \nJeﬀ  Lander. Their insightful, targeted, and timely feedback was always right \non the money, and their vast experience in the game industry has helped to \ngive me conﬁ dence that the information presented in these pages is as accu-\nrate and up-to-date as humanly possible. Matt  and Jeﬀ  were both a pleasure \nto work with, and I am honored to have had the opportunity to collaborate \nwith such consummate professionals on this project. I’d like to thank Jeﬀ  in \nparticular for putt ing me in touch with Alice Peters and helping me to get this \nproject oﬀ  the ground in the ﬁ rst place.\nA number of my colleagues at Naughty Dog also contributed to this \nbook, either by providing feedback or by helping me with the structure \nand topic content of one of the chapters. I’d like to thank Marshall Robin \nand Carlos Gonzalez-Ochoa for their guidance and tutelage as I wrote the \nrendering chapter, and Pål-Kristian Engstad for his excellent and insightful \nfeedback on the text and content of that chapter. I’d also like to thank Chris-\ntian Gyrling for his feedback on various sections of the book, including the \nchapter on animation (which is one of his many specialties). My thanks also \ngo to the entire Naughty Dog engineering team for creating all of the in-\ncredible game engine systems that I highlight in this book. Special thanks \ngo to Keith Schaeﬀ er of Electronic Arts for providing me with much of the \nraw content regarding the impact of physics on a game, found in Section \n12.1. I’d also like to thank Paul Keet of Electronic Arts and Steve Ranck, the \n",
      "content_length": 2823,
      "extraction_method": "Direct"
    },
    {
      "page_number": 22,
      "chapter": null,
      "content": "xx \nPreface\nlead engineer on the Hydro Thunder project at Midway San Diego, for their \nmentorship and guidance over the years. While they did not contribute to \nthe book directly, their inﬂ uences are echoed on virtually every page in one \nway or another.\nThis book arose out of the notes I developed for a course called ITP-485: \nProgramming Game Engines, which I have been teaching under the auspices \nof the Information Technology Program at the University of Southern Cali-\nfornia for approximately three years now. I would like to thank Dr. Anthony \nBorquez, the director of the ITP department at the time, for hiring me to de-\nvelop the ITP-485 course curriculum in the ﬁ rst place. I’d also like to extend \nwarm thanks to Ashish Soni, the current ITP director, for his continued sup-\nport and encouragement as ITP-485 continues to evolve.\nMy extended family and friends also deserve thanks, in part for their un-\nwavering encouragement, and in part for entertaining my wife and our two \nboys on so many occasions while I was working. I’d like to thank my sister- and \nbrother-in-law, Tracy Lee and Doug Provins, my cousin-in-law Matt  Glenn, \nand all of our incredible friends, including: Kim and Drew Clark, Sherilyn \nand Jim Kritzer, Anne and Michael Scherer, and Kim and Mike Warner. My \nfather Kenneth Gregory wrote a book on investing in the stock market when \nI was a teenager, and in doing so he inspired me to write a book. For this and \nso much more, I am eternally grateful to him.  I’d also like to thank my mother \nErica Gregory, in part for her insistence that I embark on this project, and in \npart for spending countless hours with me when I was a child, beating the art \nof writing into my cranium—I owe my writing skills (not to mention my work \nethic… and my rather twisted sense of humor…) entirely to her!\nLast but certainly not least, I’d like to thank Alice Peters and Kevin Jack-\nson-Mead, as well as the entire A K Peters staﬀ , for their Herculean eﬀ orts \nin publishing this book. Alice and Kevin have both been a pleasure to work \nwith, and I truly appreciate both their willingness to bend over backwards to \nget this book out the door under very tight time constraints, and their inﬁ nite \npatience with me as a new author.\nJason Gregory\nApril 2009\n",
      "content_length": 2285,
      "extraction_method": "Direct"
    },
    {
      "page_number": 23,
      "chapter": null,
      "content": "I\nFoundations\n",
      "content_length": 14,
      "extraction_method": "Direct"
    },
    {
      "page_number": 24,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 25,
      "chapter": null,
      "content": "1\nIntroduction\nW\nhen I got my ﬁ rst game console in 1979—a way-cool Intellivision sys-\ntem by Matt el—the term “game engine” did not exist. Back then, video \nand arcade games were considered by most adults to be nothing more than \ntoys, and the soft ware that made them tick was highly specialized to both \nthe game in question and the hardware on which it ran. Today, games are \na multi-billion-dollar mainstream industry rivaling Hollywood in size and \npopularity. And the soft ware that drives these now-ubiquitous three-dimen-\nsional  worlds—game engines like id Soft ware’s Quake and Doom engines, Epic \nGames’ Unreal Engine 3 and Valve’s Source engine—have become fully fea-\ntured reusable soft ware development kit s that can be licensed and used to \nbuild almost any game imaginable.\nWhile game engines vary widely in the details of their architecture and \nimplementation, recognizable coarse-grained patt erns are emerging across \nboth publicly licensed game engines and their proprietary in-house counter-\nparts. Virtually all game engines contain a familiar set of core components, in-\ncluding the rendering engine, the collision and physics engine, the animation \nsystem, the audio system, the game world object model, the artiﬁ cial intelli-\ngence system, and so on. Within each of these components, a relatively small \nnumber of semi-standard design alternatives are also beginning to emerge.\nThere are a great many books that cover individual game engine subsys-\ntems, such as three-dimensional graphics, in exhaustive detail. Other books \n3\n",
      "content_length": 1557,
      "extraction_method": "Direct"
    },
    {
      "page_number": 26,
      "chapter": null,
      "content": "4 \n1. Introduction\ncobble together valuable tips and tricks across a wide variety of game technol-\nogy areas. However, I have been unable to ﬁ nd a book that provides its reader \nwith a reasonably complete picture of the entire gamut of components that \nmake up a modern game engine. The goal of this book, then, is to take the \nreader on a guided hands-on tour of the vast and complex landscape of game \nengine architecture.\nIn this book you will learn\nz how real industrial-strength production game engines are architected;\nz how game development teams are organized and work in the real \nworld;\nz which major subsystems and design patt erns appear again and again in \nvirtually every game engine;\nz the typical requirements for each major subsystem;\nz which subsystems are genre- or game-agnostic, and which ones are typ-\nically designed explicitly for a speciﬁ c genre or game;\nz where the engine normally ends and the game begins.\nWe’ll also get a first-hand glimpse into the inner workings of some popu-\nlar game engines, such as Quake and Unreal , and some well-known mid-\ndleware packages, such as the Havok Physics library, the OGRE rendering \nengine, and Rad Game Tools’ Granny 3D animation and geometry man-\nagement toolkit.\nBefore we get started, we’ll review some techniques and tools for large-\nscale soft ware engineering in a game engine context, including\nz the diﬀ erence between logical and physical soft ware architecture;\nz conﬁ guration management, revision control, and build systems;\nz some tips and tricks for dealing with one of the common development \nenvironments for C and C++, Microsoft  Visual Studio.\nIn this book I assume that you have a solid understanding of C++ (the \nlanguage of choice among most modern game developers) and that you un-\nderstand basic soft ware engineering principles. I also assume you have some \nexposure to linear algebra, three-dimensional vector and matrix math, and \ntrigonometry (although we’ll review the core concepts in Chapter 4). Ideally \nyou should have some prior exposure to the basic concepts of real-time and \nevent-driven programming. But never fear—I will review these topics brieﬂ y, \nand I’ll also point you in the right direction if you feel you need to hone your \nskills further before we embark.\n",
      "content_length": 2275,
      "extraction_method": "Direct"
    },
    {
      "page_number": 27,
      "chapter": null,
      "content": "5 \n1.1. Structure of a Typical Game Team\n1.1. \nStructure of a Typical Game Team\nBefore we delve into the structure of a typical game engine, let’s ﬁ rst take a \nbrief look at the structure of a typical game development team. Game  stu-\ndios are usually composed of ﬁ ve basic disciplines: engineers, artists, game \ndesigners, producers, and other management and support staﬀ  (marketing, \nlegal, information technology/technical support, administrative, etc.). Each \ndiscipline can be divided into various subdisciplines. We’ll take a brief look \nat each below.\n1.1.1. \nEngineers\nThe  engineers design and implement the soft ware that makes the game, and \nthe tools, work. Engineers are oft en categorized into two basic groups: runtime \nprogrammers (who work on the engine and the game itself) and tools program-\nmers (who work on the oﬀ -line tools that allow the rest of the development \nteam to work eﬀ ectively). On both sides of the runtime/tools line, engineers \nhave various specialties. Some engineers focus their careers on a single engine \nsystem, such as rendering, artiﬁ cial intelligence, audio, or collision and phys-\nics. Some focus on gameplay programming and scripting, while others prefer \nto work at the systems level and not get too involved in how the game actu-\nally plays. Some engineers are generalists—jacks of all trades who can jump \naround and tackle whatever problems might arise during development.\nSenior engineers are sometimes asked to take on a technical leadership \nrole. Lead engineers usually still design and write code, but they also help to \nmanage the team’s schedule, make decisions regarding the overall technical \ndirection of the project, and sometimes also directly manage people from a \nhuman resources perspective.\nSome companies also have one or more technical directors (TD), whose \njob it is to oversee one or more projects from a high level, ensuring that the \nteams are aware of potential technical challenges, upcoming industry devel-\nopments, new technologies, and so on. The highest engineering-related posi-\ntion at a game studio is the chief technical oﬃ  cer (CTO), if the studio has one. \nThe CTO’s job is to serve as a sort of technical director for the entire studio, as \nwell as serving a key executive role in the company.\n1.1.2. \nArtists\nAs we say in the game industry, “content is king.” The  artists produce all of \nthe visual and audio content in the game, and the quality of their work can \nliterally make or break a game. Artists come in all sorts of ﬂ avors:\n",
      "content_length": 2531,
      "extraction_method": "Direct"
    },
    {
      "page_number": 28,
      "chapter": null,
      "content": "6 \n1. Introduction\nz Concept artists produce sketches and paintings that provide the team \nwith a vision of what the ﬁ nal game will look like. They start their work \nearly in the concept phase of development, but usually continue to pro-\nvide visual direction throughout a project’s life cycle. It is common for \nscreen shots taken from a shipping game to bear an uncanny resem-\nblance to the concept art.\nz 3D modelers produce the three-dimensional geometry for everything \nin the virtual game world. This discipline is typically divided into \ntwo subdisciplines: foreground modelers and background model-\ners. The former create objects, characters, vehicles, weapons, and the \nother objects that populate the game world, while the latt er build \nthe world’s static background geometry (terrain, buildings, bridges, \netc.).\nz Texture artists create the two-dimensional images known as textures, \nwhich are applied to the surfaces of 3D models in order to provide de-\ntail and realism.\nz Lighting artists lay out all of the light sources in the game world, both \nstatic and dynamic, and work with color, intensity, and light direction to \nmaximize the artfulness and emotional impact of each scene.\nz Animators imbue the characters and objects in the game with motion. \nThe animators serve quite literally as actors in a game production, \njust as they do in a CG ﬁ lm production. However, a game animator \nmust have a unique set of skills in order to produce animations that \nmesh seamlessly with the technological underpinnings of the game \nengine.\nz  Motion capture actors are oft en used to provide a rough set of motion \ndata, which are then cleaned up and tweaked by the animators before \nbeing integrated into the game.\nz  Sound designers work closely with the engineers in order to produce and \nmix the sound eﬀ ects and music in the game.\nz  Voice actors provide the voices of the characters in many games.\nz Many games have one or more  composers, who compose an original \nscore for the game.\nAs with engineers, senior artists are oft en called upon to be team lead-\ners. Some game teams have one or more art directors—very senior artists who \nmanage the look of the entire game and ensure consistency across the work of \nall team members.\n",
      "content_length": 2250,
      "extraction_method": "Direct"
    },
    {
      "page_number": 29,
      "chapter": null,
      "content": "7 \n1.1.3. \nGame Designers\nThe  game designers’ job is to design the interactive portion of the player’s \nexperience, typically known as gameplay. Diﬀ erent kinds of designers work \nat diﬀ erent levels of detail. Some (usually senior) game designers work at the \nmacro level, determining the story arc, the overall sequence of chapters or lev-\nels, and the high-level goals and objectives of the player. Other designers work \non individual levels or geographical areas within the virtual game world, lay-\ning out the static background geometry, determining where and when en-\nemies will emerge, placing supplies like weapons and health packs, designing \npuzzle elements, and so on. Still other designers operate at a highly technical \nlevel, working closely with gameplay engineers and/or writing code (oft en in \na high-level scripting language). Some game designers are ex-engineers, who \ndecided they wanted to play a more active role in determining how the game \nwill play.\nSome game teams employ one or more  writers. A game writer’s job can \nrange from collaborating with the senior game designers to construct the story \narc of the entire game, to writing individual lines of dialogue.\nAs with other disciplines, some senior designers play management roles. \nMany game teams have a game director, whose job it is to oversee all aspects \nof a game’s design, help manage schedules, and ensure that the work of indi-\nvidual designers is consistent across the entire product. Senior designers also \nsometimes evolve into producers.\n1.1.4. \nProducers\nThe role of  producer is deﬁ ned diﬀ erently by diﬀ erent studios. In some game \ncompanies, the producer’s job is to manage the schedule and serve as a hu-\nman resources manager. In other companies, producers serve in a senior game \ndesign capacity. Still other studios ask their producers to serve as liaisons be-\ntween the development team and the business unit of the company (ﬁ nance, \nlegal, marketing, etc.). Some smaller studios don’t have producers at all. For \nexample, at Naughty Dog, literally everyone in the company, including the \ntwo co-presidents, play a direct role in constructing the game; team man-\nagement and business duties are shared between the senior members of the \nstudio.\n1.1.5. \nOther Staff\nThe team of people who directly construct the game is typically supported by \na crucial team of support staﬀ . This includes the studio’s executive manage-\n1.1. Structure of a Typical Game Team\n",
      "content_length": 2467,
      "extraction_method": "Direct"
    },
    {
      "page_number": 30,
      "chapter": null,
      "content": "8 \n1. Introduction\nment team, the marketing department (or a team that liaises with an external \nmarketing group), administrative staﬀ , and the IT department, whose job is \nto purchase, install, and conﬁ gure hardware and soft ware for the team and to \nprovide technical support.\n1.1.6. \nPublishers and Studios\nThe marketing, manufacture, and distribution of a game title are usually \nhandled by a publisher, not by the game studio itself. A  publisher is typically \na large corporation, like Electronic Arts, THQ, Vivendi, Sony, Nintendo, etc. \nMany game studios are not aﬃ  liated with a particular publisher. They sell \neach game that they produce to whichever publisher strikes the best deal with \nthem. Other studios work exclusively with a single publisher, either via a long-\nterm publishing contract, or as a fully owned subsidiary of the publishing \ncompany. For example, THQ’s game studios are independently managed, but \nthey are owned and ultimately controlled by THQ. Electronic Arts takes this \nrelationship one step further, by directly managing its studios.  First-party de-\nvelopers are game studios owned directly by the console manufacturers (Sony, \nNintendo, and Microsoft ). For example, Naughty Dog is a ﬁ rst-party Sony \ndeveloper. These studios produce games exclusively for the gaming hardware \nmanufactured by their parent company.\n1.2. \nWhat Is a Game?\nWe probably all have a prett y good intuitive notion of what a  game is. The \ngeneral term “game” encompasses board games like chess and Monopoly, card \ngames like poker and blackjack, casino games like roulett e and slot machines, \nmilitary war games, computer games, various kinds of play among children, \nand the list goes on. In academia we sometimes speak of “game theory,” in \nwhich multiple agents select strategies and tactics in order to maximize their \ngains within the framework of a well-deﬁ ned set of game rules. When used \nin the context of  console or computer-based entertainment, the word “game” \nusually conjures images of a three-dimensional virtual world featuring a hu-\nmanoid, animal, or vehicle as the main character under player control. (Or for \nthe old geezers among us, perhaps it brings to mind images of two-dimen-\nsional classics like Pong, Pac-Man, or Donkey Kong.)  In his excellent book, A \nTheory of Fun for Game Design, Raph Koster deﬁ nes a “game” to be an inter-\nactive experience that provides the player with an increasingly challenging \nsequence of patt erns which he or she learns and eventually masters [26]. Ko-\nster’s assertion is that the activities of learning and mastering are at the heart \n",
      "content_length": 2621,
      "extraction_method": "Direct"
    },
    {
      "page_number": 31,
      "chapter": null,
      "content": "9 \n1.2. What Is a Game?\nof what we call “fun,” just as a joke becomes funny at the moment we “get it” \nby recognizing the patt ern.\nFor the purposes of this book, we’ll focus on the subset of games that \ncomprise two- and three-dimensional virtual worlds with a small number of \nplayers (between one and 16 or thereabouts). Much of what we’ll learn can \nalso be applied to Flash games on the Internet, pure puzzle games like Tetris, \nor massively multiplayer online games (MMOG). But our primary focus will \nbe on game engines capable of producing ﬁ rst-person shooters, third-person \naction/platform games, racing games, ﬁ ghting games, and the like.\n1.2.1. \nVideo Games as Soft Real-Time Simulations\nMost two- and three-dimensional video games are examples of what comput-\ner scientists would call soft  real-time interactive agent-based computer  simulations. \nLet’s break this phrase down in order to bett er understand what it means.\nIn most video games, some subset of the real world—or an imaginary \nworld—is  modeled mathematically so that it can be manipulated by a com-\nputer. The model is an approximation to and a simpliﬁ cation of reality (even \nif it’s an imaginary reality), because it is clearly impractical to include every \ndetail down to the level of atoms or quarks. Hence, the mathematical model \nis a simulation of the real or imagined  game world. Approximation and sim-\npliﬁ cation are two of the game developer’s most powerful tools. When used \nskillfully, even a greatly simpliﬁ ed model can sometimes be almost indistin-\nguishable from reality—and a lot more fun.\nAn agent-based simulation is one in which a number of distinct entities \nknown as “agents” interact. This ﬁ ts the description of most three-dimen-\ntsional computer games very well, where the agents are vehicles, characters, \nﬁ reballs, power dots, and so on. Given the agent-based nature of most games, \nit should come as no surprise that most games nowadays are implemented in \nan object-oriented, or at least loosely object-based, programming language.\nAll interactive video games are temporal  simulations, meaning that the vir-\ntual game world model is dynamic—the state of the game world changes over \ntime as the game’s events and story unfold. A video game must also respond \nto unpredictable inputs from its human player(s)—thus interactive temporal \nsimulations. Finally, most video games present their stories and respond to \nplayer input in real-time , making them interactive real-time  simulations. One \nnotable exception is in the category of turn-based games like computerized \nchess or non-real-time strategy games. But even these types of games usually \nprovide the user with some form of real-time graphical user interface . So for \nthe purposes of this book, we’ll assume that all video games have at least some \nreal-time constraints.\n",
      "content_length": 2847,
      "extraction_method": "Direct"
    },
    {
      "page_number": 32,
      "chapter": null,
      "content": "10 \n1. Introduction\nAt the core of every real-time system is the concept of a  deadline. An obvi-\nous example in video games is the requirement that the screen be updated \nat least 24 times per second in order to provide the illusion of motion. (Most \ngames render the screen at 30 or 60  frames per second because these are mul-\ntiples of an NTSC monitor’s refresh rate.)  Of course, there are many other \nkinds of deadlines in video games as well. A physics simulation may need \nto be updated 120 times per second in order to remain stable. A character’s \nartiﬁ cial intelligence system may need to “think” at least once every second to \nprevent the appearance of stupidity. The audio library may need to be called \nat least once every 1/60 second in order to keep the audio buﬀ ers ﬁ lled and \nprevent audible glitches.\nA “soft ” real-time system is one in which missed deadlines are not cata-\nstrophic. Hence all video games are  soft  real-time systems—if the frame rate \ndies, the human player generally doesn’t!  Contrast this with a  hard real-time \nsystem, in which a missed deadline could mean severe injury to or even the \ndeath of a human operator. The avionics system in a helicopter or the control-\nrod system in a nuclear power plant are examples of hard real-time systems.\nMathematical models can be  analytic or numerical. For example, the ana-\nlytic (closed-form) mathematical model of a rigid body falling under the inﬂ u-\nence of constant acceleration due to gravity is typically writt en as follows:\n \ny(t) = ½ g t2 + v0 t + y0 . \n(1.1)\nAn analytic model can be evaluated for any value of its independent variables, \nsuch as the time t in the above equation, given only the initial conditions v0\nand y0 and the constant g. Such models are very convenient when they can be \nfound. However many problems in mathematics have no closed-form solu-\ntion. And in video games, where the user’s input is unpredictable, we cannot \nhope to model the entire game analytically.\nA  numerical model of the same rigid body under gravity might be\n \ny(t + Δt) = F(y(t), y˙ (t), ÿ(t), …) . \n(1.2)\nThat is, the height of the rigid body at some future time (t + Δt) can be found as \na function of the height and its ﬁ rst and second time derivatives at the current \ntime t. Numerical simulations are typically implemented by running calcula-\ntions repeatedly, in order to determine the state of the system at each discrete \ntime step. Games work in the same way. A main “game loop” runs repeatedly, \nand during each iteration of the loop, various game systems such as artiﬁ cial \nintelligence, game logic, physics simulations, and so on are given a chance to \ncalculate or update their state for the next discrete time step. The results are \nthen “rendered” by displaying graphics, emitt ing sound, and possibly pro-\nducing other outputs such as force feedback on the joypad.\n",
      "content_length": 2876,
      "extraction_method": "Direct"
    },
    {
      "page_number": 33,
      "chapter": null,
      "content": "11 \n1.3. \nWhat Is a Game Engine?\nThe term “ game engine” arose in the mid-1990s in reference to ﬁ rst-person \nshooter (FPS) games like the insanely popular Doom by id Soft ware.  Doom was \narchitected with a reasonably well-deﬁ ned separation between its core soft -\nware components (such as the three-dimensional graphics rendering system, \nthe collision detection system, or the audio system) and the art assets, game \nworlds, and rules of play that comprised the player’s gaming experience. The \nvalue of this separation became evident as developers began licensing games \nand re-tooling them into new products by creating new art, world layouts, \nweapons, characters, vehicles, and game rules with only minimal changes to \nthe “engine” soft ware. This marked the birth of the “mod community ”—a \ngroup of individual gamers and small independent studios that built new \ngames by modifying existing games, using free toolkits provided by the origi-\nnal developers. Towards the end of the 1990s, some games like  Quake III Arena \nand  Unreal were designed with reuse and “ modding” in mind. Engines were \nmade highly customizable via scripting languages like id’s  Quake C, and en-\ngine licensing began to be a viable secondary revenue stream for the develop-\ners who created them. Today, game developers can license a game engine and \nreuse signiﬁ cant portions of its key soft ware components in order to build \ngames. While this practice still involves considerable investment in custom \nsoft ware engineering, it can be much more economical than developing all of \nthe core engine components in-house.\nThe  line between a game and its engine is oft en blurry. Some engines \nmake a reasonably clear distinction, while others make almost no att empt \nto separate the two. In one game, the rendering code might “know” speciﬁ -\ncally how to draw an orc. In another game, the rendering engine might pro-\nvide general-purpose material and shading facilities, and “orc-ness” might \nbe deﬁ ned entirely in data. No studio makes a perfectly clear separation \nbetween the game and the engine, which is understandable considering that \nthe deﬁ nitions of these two components oft en shift  as the game’s design so-\nlidiﬁ es.\nArguably a  data-driven architecture is what differentiates a game en-\ngine from a piece of software that is a game but not an engine. When a \ngame contains hard-coded logic or game rules, or employs special-case \ncode to render specific types of game objects, it becomes difficult or im-\npossible to reuse that software to make a different game. We should prob-\nably reserve the term “game engine” for software that is extensible and \ncan be used as the foundation for many different games without major \nmodification.\n1.3. What Is a Game Engine?\n",
      "content_length": 2767,
      "extraction_method": "Direct"
    },
    {
      "page_number": 34,
      "chapter": null,
      "content": "12 \n1. Introduction\nClearly this is not a black-and-white distinction. We can think of a gamut \nof reusability onto which every engine falls. Figure 1.1 takes a stab at the loca-\ntions of some well-known games/engines along this  gamut.\nOne would think that a game engine could be something akin to Apple \nQuickTime or Microsoft  Windows Media Player—a general-purpose piece of \nsoft ware capable of playing virtually any game content imaginable. However \nthis ideal has not yet been achieved (and may never be). Most game engines \nare carefully craft ed and ﬁ ne-tuned to run a particular game on a particular \nhardware platform. And even the most general-purpose multiplatform en-\ngines are really only suitable for building games in one particular genre, such \nas ﬁ rst-person shooters or racing games. It’s safe to say that the more general-\npurpose a game engine or middleware component is, the less optimal it is for \nrunning a particular game on a particular platform.\nThis phenomenon occurs because designing any eﬃ  cient piece of soft -\nware invariably entails making trade-oﬀ s, and those trade-oﬀ s are based on \nassumptions about how the soft ware will be used and/or about the target \nhardware on which it will run. For example, a rendering engine that was de-\nsigned to handle intimate indoor environments probably won’t be very good \nat rendering vast outdoor environments. The indoor engine might use a BSP \ntree or portal system to ensure that no geometry is drawn that is being oc-\ncluded by walls or objects that are closer to the camera. The outdoor engine, \non the other hand, might use a less-exact occlusion mechanism, or none at all, \nbut it probably makes aggressive use of level-of-detail (LOD) techniques to \nensure that distant objects are rendered with a minimum number of triangles, \nwhile using high resolution triangle meshes for geometry that is close to the \ncamera.\nThe advent of ever-faster computer hardware and specialized graphics \ncards, along with ever-more-eﬃ  cient rendering algorithms and data struc-\ntures, is beginning to soft en the diﬀ erences between the graphics engines of \ndiﬀ erent genres. It is now possible to use a ﬁ rst-person shooter engine to build \na real-time strategy game, for example. However, the trade-oﬀ  between gener-\nCan be “modded” to \nbuild any game in a \nspecific genre\nCan be used to build any \ngame imaginable\nCannot be used to build \nmore than one game\nCan be customized to \nmake very similar games\nQuake III\nEngine\nUnreal\nEngine \n3\nHydro Thunder \nEngine\nProbably \nimpossible\nPacMan\nFigure 1.1.  Game engine reusability gamut.\n",
      "content_length": 2605,
      "extraction_method": "Direct"
    },
    {
      "page_number": 35,
      "chapter": null,
      "content": "13 \n1.4. Engine Differnces Across Genres\nality and optimality still exists. A game can always be made more impressive \nby ﬁ ne-tuning the engine to the speciﬁ c requirements and constraints of a \nparticular game and/or hardware platform.\n1.4. \nEngine Differences Across Genres\nGame engines are typically somewhat  genre speciﬁ c. An engine designed \nfor a two-person ﬁ ghting game in a boxing ring will be very diﬀ erent from a \nmassively multiplayer online game (MMOG) engine or a ﬁ rst-person shooter \n(FPS) engine or a real-time strategy (RTS) engine. However, there is also a \ngreat deal of overlap—all 3D games, regardless of genre, require some form \nof low-level user input from the joypad, keyboard, and/or mouse, some form \nof 3D mesh rendering, some form of heads-up display (HUD) including text \nrendering in a variety of fonts, a powerful audio system, and the list goes on. \nSo while the Unreal Engine, for example, was designed for ﬁ rst-person shoot-\ner games, it has been used successfully to construct games in a number of \nother genres as well, including the wildly popular third-person shooter Gears \nof War by Epic Games; the character-based action-adventure game Grimm, by \nAmerican McGee’s Shanghai-based development studio, Spicy Horse; and \nSpeed Star, a futuristic racing game by South Korea-based Acro Games.\nLet’s take a look at some of the most common game genres and explore \nsome examples of the technology requirements particular to each.\n1.4.1. \nFirst-Person Shooters (FPS)\nThe  ﬁ rst-person shooter (FPS) genre is typiﬁ ed by games like Quake , Unreal \nTournament, Half-Life, Counter-Strike, and Call of Duty (see Figure 1.2). These \ngames have historically involved relatively slow on-foot roaming of a poten-\ntially large but primarily corridor-based world. However, modern ﬁ rst-person \nshooters can take place in a wide variety of virtual environments including \nvast open outdoor areas and conﬁ ned indoor areas. Modern FPS traversal me-\nchanics can include on-foot locomotion, rail-conﬁ ned or free-roaming ground \nvehicles, hovercraft , boats, and aircraft . For an overview of this genre, see \nhtt p://en.wikipedia.org/wiki/First-person_shooter.\nFirst-person games are typically some of the most technologically chal-\nlenging to build, probably rivaled in complexity only by third-person shooter/\naction/platformer games and massively multiplayer games. This is because \nﬁ rst-person shooters aim to provide their players with the illusion of being \nimmersed in a detailed, hyperrealistic world. It is not surprising that many of \nthe game industry’s big technological innovations arose out of the games in \nthis genre.\n",
      "content_length": 2661,
      "extraction_method": "Direct"
    },
    {
      "page_number": 36,
      "chapter": null,
      "content": "14 \n1. Introduction\nFirst-person shooters typically focus on technologies, such as\nz eﬃ  cient rendering of large 3D virtual worlds;\nz a responsive camera control/aiming mechanic;\nz high-ﬁ delity animations of the player’s virtual arms and weapons;\nz a wide range of powerful hand-held weaponry;\nz a forgiving player character motion and collision model, which oft en \ngives these games a “ﬂ oaty” feel;\nz high-ﬁ delity animations and artiﬁ cial intelligence for the non-player \ncharacters (the player’s enemies and allies);\nz small-scale online multiplayer capabilities (typically supporting up to \n64 simultaneous players), and the ubiquitous “death match” gameplay \nmode.\nThe rendering technology employed by ﬁ rst-person shooters is almost \nalways highly optimized and carefully tuned to the particular type of envi-\nFigure 1.2.  Call of Duty 2 (Xbox 360/PLAYSTATION 3).\n",
      "content_length": 875,
      "extraction_method": "Direct"
    },
    {
      "page_number": 37,
      "chapter": null,
      "content": "15 \nronment being rendered. For example, indoor “dungeon crawl” games oft en \nemploy binary space partitioning (BSP) trees or portal -based rendering sys-\ntems. Outdoor FPS games use other kinds of rendering optimizations such as \nocclusion culling , or an oﬄ  ine sectorization of the game world with manual \nor automated speciﬁ cation of which target sectors are visible from each source \nsector.\nOf course, immersing a player in a hyperrealistic game world requires \nmuch more than just optimized high-quality graphics technology. The charac-\nter animations, audio and music, rigid-body physics, in-game cinematics, and \nmyriad other technologies must all be cutt ing-edge in a ﬁ rst-person shooter. \nSo this genre has some of the most stringent and broad technology require-\nments in the industry.\n1.4.2. Platformers and Other Third-Person Games\n“ Platformer” is the term applied to third-person character-based action games \nwhere jumping from platform to platform is the primary gameplay mechanic. \nTypical games from the 2D era include Space Panic, Donkey Kong, Pitfall!, and \n1.4. Engine Differnces Across Genres\nFigure 1.3.  Jak & Daxter: The Precursor Legacy.\n",
      "content_length": 1170,
      "extraction_method": "Direct"
    },
    {
      "page_number": 38,
      "chapter": null,
      "content": "16 \n1. Introduction\nSuper Mario Brothers. The 3D era includes platformers like Super Mario 64, Crash \nBandicoot, Rayman 2, Sonic the Hedgehog, the Jak and Daxter series (Figure 1.3), \nthe Ratchet & Clank series, and more recently Super Mario Galaxy. See htt p://\nen.wikipedia.org/wiki/Platformer for an in-depth discussion of this genre.\nIn terms of their technological requirements, platformers can usually be \nlumped together with third-person shooters and third-person action/adven-\nture games, like Ghost Recon, Gears of War (Figure 1.4), and Uncharted: Drake’s \nFortune.\nThird-person character-based games have a lot in common with ﬁ rst-per-\nson shooters, but a great deal more emphasis is placed on the main character’s \nabilities and locomotion modes. In addition, high-ﬁ delity full-body character \nanimations are required for the player’s avatar, as opposed to the somewhat \nless-taxing animation requirements of the “ﬂ oating arms” in a typical FPS \ngame. It’s important to note here that almost all ﬁ rst-person shooters have an \nonline multiplayer component, so a full-body player avatar must be rendered \nin addition to the ﬁ rst-person arms. However the ﬁ delity of these FPS player \navatars is usually not comparable to the ﬁ delity of the non-player characters \nFigure 1.4.  Gears of War.\n",
      "content_length": 1306,
      "extraction_method": "Direct"
    },
    {
      "page_number": 39,
      "chapter": null,
      "content": "17 \nin these same games; nor can it be compared to the ﬁ delity of the player avatar \nin a third-person game.\nIn a platformer, the main character is oft en cartoon-like and not particu-\nlarly realistic or high-resolution. However, third-person shooters oft en feature \na highly realistic humanoid player character. In both cases, the player charac-\nter typically has a very rich set of actions and animations.\nSome of the technologies speciﬁ cally focused on by games in this genre \ninclude\nz moving platforms, ladders, ropes, trellises, and other interesting loco-\nmotion modes;\nz puzzle-like environmental elements;\nz a third-person “follow camera ” which stays focused on the player char-\nacter and whose rotation is typically controlled by the human player via \nthe right joypad stick (on a console) or the mouse (on a PC—note that \nwhile there are a number of popular third-person shooters on PC, the \nplatformer genre exists almost exclusively on consoles);\nz a complex camera collision system for ensuring that the view point \nnever “clips” through background geometry or dynamic foreground \nobjects.\n1.4.3. \nFighting Games\n Fighting games are typically two-player games involving humanoid char-\nacters pummeling each other in a ring of some sort. The genre is typiﬁ ed \nby games like Soul Calibur and Tekken (see Figure 1.5). The Wikipedia page \nhtt p://en.wikipedia.org/wiki/Fighting_game provides an overview of this \ngenre.\nTraditionally games in the ﬁ ghting genre have focused their technology \neﬀ orts on\nz a rich set of ﬁ ghting animations;\nz accurate hit detection;\nz a user input system capable of detecting complex butt on and joystick \ncombinations;\nz crowds, but otherwise relatively static backgrounds.\nSince the 3D world in these games is small and the camera is centered \non the action at all times, historically these games have had litt le or no need \nfor world subdivision or occlusion culling . They would likewise not be ex-\npected to employ advanced three-dimensional audio propagation models, for \nexample.\n1.4. Engine Differnces Across Genres\n",
      "content_length": 2074,
      "extraction_method": "Direct"
    },
    {
      "page_number": 40,
      "chapter": null,
      "content": "18 \n1. Introduction\nState-of-the-art ﬁ ghting games like EA’s Fight Night Round 3 (Figure 1.6) \nhave upped the technological ante with features like\nz high-deﬁ nition character graphics, including realistic skin shaders with \nsubsurface scatt ering and sweat eﬀ ects;\nz high-ﬁ delity character animations; \nz physics-based cloth and hair simulations for the characters.\nIt’s important to note that some ﬁ ghting games like Heavenly Sword take \nplace in a large-scale virtual world, not a conﬁ ned arena. In fact, many people \nconsider this to be a separate genre, sometimes called a  brawler. This kind of \nﬁ ghting game can have technical requirements more akin to those of a ﬁ rst-\nperson shooter or real-time strategy game.\nFigure 1.5.  Tekken 3 (PlayStation).\n",
      "content_length": 764,
      "extraction_method": "Direct"
    },
    {
      "page_number": 41,
      "chapter": null,
      "content": "19 \n1.4.4. Racing Games\nThe racing  genre encompasses all games whose primary task is driving a car \nor other vehicle on some kind of track. The genre has many subcategories. \nSimulation-focused racing games (“sims”) aim to provide a driving experi-\nence that is as realistic as possible (e.g., Gran Turismo). Arcade racers favor \nover-the-top fun over realism (e.g., San Francisco Rush, Cruisin’ USA, Hydro \nThunder). A relatively new subgenre explores the subculture of street racing \nwith tricked out consumer vehicles (e.g., Need for Speed, Juiced). Kart racing is \na subcategory in which popular characters from platformer games or cartoon \ncharacters from TV are re-cast as the drivers of whacky vehicles (e.g., Mario \nKart, Jak X, Freaky Flyers). “Racing” games need not always involve time-based \ncompetition. Some kart racing games, for example, oﬀ er modes in which play-\ners shoot at one another, collect loot, or engage in a variety of other timed \nand untimed tasks. For a discussion of this genre, see htt p://en.wikipedia.org/\nwiki/Racing_game.\n1.4. Engine Differnces Across Genres\nFigure 1.6.   Fight Night Round 3 (PLAYSTATION 3).\n",
      "content_length": 1148,
      "extraction_method": "Direct"
    },
    {
      "page_number": 42,
      "chapter": null,
      "content": "20 \n1. Introduction\nA racing game is oft en very linear, much like older FPS games. However, \ntravel speed is generally much faster than in a FPS. Therefore more focus is \nplaced on very long corridor-based tracks, or looped tracks, sometimes with \nvarious alternate routes and secret short-cuts. Racing games usually focus all \ntheir graphic detail on the vehicles, track, and immediate surroundings. How-\never, kart racers also devote signiﬁ cant rendering and animation bandwidth \nto the characters driving the vehicles. Figure 1.7 shows a screen shot from the \nlatest installment in the well-known Gran Turismo racing game series, Gran \nTurismo 5.\nSome of the technological properties of a typical racing game include the \nfollowing techniques. \nz Various “tricks” are used when rendering distant background elements, \nsuch as employing two-dimensional cards for trees, hills, and mountains.\nz The track is oft en broken down into relatively simple two-dimension-\nal regions called “sectors.” These data structures are used to optimize \nrendering and visibility determination, to aid in artiﬁ cial intelligence \nand path ﬁ nding for non-human-controlled vehicles, and to solve many \nother technical problems.\nFigure 1.7.  Gran Turismo 5 (PLAYSTATION 3).\n",
      "content_length": 1258,
      "extraction_method": "Direct"
    },
    {
      "page_number": 43,
      "chapter": null,
      "content": "21 \nz The camera typically follows behind the vehicle for a third-person per-\nspective, or is sometimes situated inside the cockpit ﬁ rst-person style.\nz When the track involves tunnels and other “tight” spaces, a good deal \nof eﬀ ort is oft en put into ensuring that the camera does not collide with \nbackground geometry.\n1.4.5. Real-Time Strategy (RTS)\nThe modern  real-time strategy (RTS) genre was arguably deﬁ ned by Dune II: \nThe Building of a Dynasty (1992). Other games in this genre include Warcraft , \nCommand & Conquer, Age of Empires, and Starcraft . In this genre, the player \ndeploys the batt le units in his or her arsenal strategically across a large play-\ning ﬁ eld in an att empt to overwhelm his or her opponent. The game world is \ntypically displayed at an oblique top-down viewing angle. For a discussion of \nthis genre, see htt p://en.wikipedia.org/wiki/Real-time_strategy.\nThe RTS player is usually prevented from signiﬁ cantly changing the \nviewing angle in order to see across large distances. This restriction permits \nFigure 1.8.  Age of Empires.\n1.4. Engine Differnces Across Genres\n",
      "content_length": 1111,
      "extraction_method": "Direct"
    },
    {
      "page_number": 44,
      "chapter": null,
      "content": "22 \n1. Introduction\ndevelopers to employ various optimizations in the rendering engine of an RTS \ngame.\nOlder games in the genre employed a grid-based (cell-based) world con-\nstruction, and an orthographic projection was used to greatly simplify the ren-\nderer. For example, Figure 1.8 shows a screen shot from the classic RTS Age \nof Empires.\nModern RTS games sometimes use perspective projection and a true 3D \nworld, but they may still employ a grid layout system to ensure that units and \nbackground elements, such as buildings, align with one another properly. A \npopular example, Command & Conquer 3, is shown in Figure 1.9.\nSome other common practices in RTS games include the following tech-\nniques.\nFigure 1.9.  Command & Conquer 3.\n",
      "content_length": 742,
      "extraction_method": "Direct"
    },
    {
      "page_number": 45,
      "chapter": null,
      "content": "23 \nz Each unit is relatively low-res, so that the game can support large num-\nbers of them on-screen at once.\nz Height-ﬁ eld terrain is usually the canvas upon which the game is de-\nsigned and played.\nz The player is oft en allowed to build new structures on the terrain in ad-\ndition to deploying his or her forces.\nz User interaction is typically via single-click and area-based selection of \nunits, plus menus or toolbars containing commands, equipment, unit \ntypes, building types, etc.\n1.4.6. Massively Multiplayer Online Games (MMOG)\nThe  massively multiplayer online game (MMOG) genre is typiﬁ ed by games \nlike Neverwinter Nights, EverQuest, World of Warcraft , and Star Wars Galaxies, to \nname a few. An MMOG is deﬁ ned as any game that supports huge numbers of \nsimultaneous players (from thousands to hundreds of thousands), usually all \n1.4. Engine Differnces Across Genres\nFigure 1.10.  World of Warcraft.\n",
      "content_length": 920,
      "extraction_method": "Direct"
    },
    {
      "page_number": 46,
      "chapter": null,
      "content": "24 \n1. Introduction\nplaying in one very large, persistent virtual world (i.e., a world whose internal \nstate persists for very long periods of time, far beyond that of any one player’s \ngameplay session). Otherwise, the gameplay experience of an MMOG is oft en \nsimilar to that of their small-scale multiplayer counterparts. Subcategories of \nthis genre include MMO role-playing games (MMORPG), MMO real-time \nstrategy games (MMORTS), and MMO ﬁ rst-person shooters (MMOFPS). For a \ndiscussion of this genre, see htt p://en.wikipedia.org/wiki/MMOG. Figure 1.10 \nshows a screen shot from the hugely popular MMORPG World of Warcraft .\nAt the heart of all MMOGs is a very powerful batt ery of servers. These \nservers maintain the authoritative state of the game world, manage users sign-\ning in and out of the game, provide inter-user chat or voice-over-IP (VoIP) \nservices, etc. Almost all MMOGs require users to pay some kind of regular \nsubscription fee in order to play, and they may oﬀ er micro-transactions within \nthe game world or out-of-game as well. Hence, perhaps the most important \nrole of the central server is to handle the billing and micro-transactions which \nserve as the game developer’s primary source of revenue.\nGraphics ﬁ delity in an MMOG is almost always lower than its non-mas-\nsively multiplayer counterparts, as a result of the huge world sizes and ex-\ntremely large numbers of users supported by these kinds of games.\n1.4.7. \nOther Genres\nThere are of course many  other game genres which we won’t cover in depth \nhere. Some examples include\nz sports, with subgenres for each major sport (football, baseball, soccer, \ngolf, etc.);\nz role-playing games (RPG);\nz God games, like Populus and Black & White;\nz environmental/social simulation games, like SimCity or The Sims;\nz puzzle games like Tetris;\nz conversions of non-electronic games, like chess, card games, go, etc.;\nz web-based games, such as those oﬀ ered at Electronic Arts’ Pogo site;\nz and the list goes on.\nWe have seen that each game genre has its own particular technologi-\ncal requirements. This explains why game engines have traditionally diﬀ ered \nquite a bit from genre to genre. However, there is also a great deal of tech-\nnological overlap between genres, especially within the context of a single \nhardware platform. With the advent of more and more powerful hardware, \n",
      "content_length": 2367,
      "extraction_method": "Direct"
    },
    {
      "page_number": 47,
      "chapter": null,
      "content": "25 \ndiﬀ erences between genres that arose because of optimization concerns are \nbeginning to evaporate. So it is becoming increasingly possible to reuse the \nsame engine technology across disparate genres, and even across disparate \nhardware platforms.\n1.5. \nGame Engine Survey\n1.5.1. \nThe Quake Family of Engines\nThe ﬁ rst 3D ﬁ rst-person shooter (FPS) game is generally accepted to be  Castle \nWolfenstein 3D (1992). Writt en by id Soft ware of Texas for the PC platform, this \ngame led the game industry in a new and exciting direction. Id Soft ware went \non to create Doom,  Quake , Quake II, and Quake III. All of these engines are very \nsimilar in architecture, and I will refer to them as the Quake family of engines. \nQuake technology has been used to create many other games and even other \nengines. For example, the lineage of Medal of Honor for the PC platform goes \nsomething like this:\nz Quake I II (Id);\nz Sin (Ritual);\nz F.A.K.K. 2 (Ritual);\nz Medal of Honor: Allied Assault (2015 & Dreamworks Interactive);\nz Medal of Honor: Paciﬁ c Assault (Electronic Arts, Los Angeles).\nMany other games based on Quake technology follow equally circuitous paths \nthrough many diﬀ erent games and studios. In fact, Valve’s Source engine (used \nto create the Half-Life games) also has distant roots in Quake technology.\nThe Quake and Quake II source code is freely available, and the original \nQuake engines are reasonably well architected and “clean” (although they \nare of course a bit outdated and writt en entirely in C). These code bases serve \nas great examples of how industrial-strength game engines are built. The full \nsource code to Quake and Quake II is available on id’s website at htt p://www.\nidsoft ware.com/business/techdownloads.\nIf you own the Quake and/or Quake II games, you can actually build the \ncode using Microsoft  Visual Studio and run the game under the debugger \nusing the real game assets from the disk. This can be incredibly instructive. \nYou can set break points, run the game, and then analyze how the engine \nactually works by stepping through the code. I highly recommend down-\nloading one or both of these engines and analyzing the source code in this \nmanner. \n1.5. Game Engine Survey\n",
      "content_length": 2224,
      "extraction_method": "Direct"
    },
    {
      "page_number": 48,
      "chapter": null,
      "content": "26 \n1. Introduction\n1.5.2. \nThe Unreal Family of Engines\nEpic Games Inc. burst onto the FPS scene in 1998 with its legendary game  Un-\nreal . Since then, the Unreal Engine has become a major competitor to Quake \ntechnology in the FPS space. Unreal Engine 2 (UE2) is the basis for Unreal \nTournament 2004 (UT2004) and has been used for countless “mods,” university \nprojects, and commercial games. Unreal Engine 3 (UE3) is the next evolution-\nary step, boasting some of the best tools and richest engine feature sets in \nthe industry, including a convenient and powerful graphical user interface for \ncreating shaders and a graphical user interface for game logic programming \ncalled Kismet. Many games are being developed with UE3 lately, including of \ncourse Epic’s popular Gears of War.\nThe Unreal Engine has become known for its extensive feature set and \ncohesive, easy-to-use tools. The Unreal Engine is not perfect, and most devel-\nopers modify it in various ways to run their game optimally on a particular \nhardware platform. However, Unreal is an incredibly powerful prototyping \ntool and commercial game development platform, and it can be used to build \nvirtually any 3D ﬁ rst-person or third-person game (not to mention games in \nother genres as well).\nThe  Unreal Developer Network (UDN) provides a rich set of documenta-\ntion and other information about the various versions of the Unreal Engine \n(see htt p://udn.epicgames.com). Some of the documentation on Unreal Engine \n2 is freely available, and “mods” can be constructed by anyone who owns a \ncopy of UT2004. However, access to the balance of the UE2 docs and all of the \nUE3 docs are restricted to licensees of the engine. Unfortunately, licenses are \nextremely expensive, and hence out of reach for all independent game devel-\nopers and most small studios as well. But there are plenty of other useful web-\nsites and wikis on Unreal. One popular one is htt p://www.beyondunreal.com.\n1.5.3. \nThe Half Life Source Engine\n Source is the game engine that drives the smash hit  Half-Life 2 and its sequels \nHL2: Episode One, HL2: Episode Two, Team Fortress 2, and Portal (shipped to-\ngether under the title The Orange Box). Source is a high-quality engine, rivaling \nUnreal Engine 3 in terms of graphics capabilities and tool set.\n1.5.4. Microsoft’s XNA Game Studio\nMicrosoft ’s  XNA Game Studio is an easy-to-use and highly accessible game \ndevelopment platform aimed at encouraging players to create their own \ngames and share them with the online gaming community, much as YouTube \nencourages the creation and sharing of home-made videos.\n",
      "content_length": 2609,
      "extraction_method": "Direct"
    },
    {
      "page_number": 49,
      "chapter": null,
      "content": "27 \nXNA is based on Microsoft ’s C# language and the Common Language \nRuntime (CLR). The primary development environment is Visual Studio or \nits free counterpart, Visual Studio Express. Everything from source code \nto game art assets are managed within Visual Studio. With XNA, develop-\ners can create games for the PC platform and Microsoft ’s Xbox 360 console. \nAft er paying a modest fee, XNA games can be uploaded to the  Xbox Live \nnetwork and shared with friends. By providing excellent tools at essentially \nzero cost, Microsoft  has brilliantly opened the ﬂ oodgates for the average \nperson to create new games. XNA clearly has a bright and fascinating future \nahead of it.\n1.5.5. \nOther Commercial Engines\nThere are lots of other commercial game engines out there. Although indie \ndevelopers may not have the budget to purchase an engine, many of these \nproducts have great online documentation and/or wikis that can serve as a \ngreat source of information about game engines and game programming in \ngeneral. For example, check out the  C4 Engine by Terathon Soft ware (htt p://\nwww.terathon.com), a company founded by Eric Lengyel in 2001. Docu-\nmentation for the C4 Engine can be found on Terathon’s website, with ad-\nditional details on the C4 Engine wiki (htt p://www.terathon.com/wiki/index.\nphp?title=Main_Page).\n1.5.6. Proprietary in-House Engines\nMany companies build and maintain proprietary in-house game engines. \nElectronic Arts built many of its RTS games on a proprietary engine called \nSAGE, developed at Westwood Studios. Naughty Dog’s Crash Bandicoot, Jak \nand Daxter series, and most recently Uncharted: Drake’s Fortune franchises were \neach built on in-house engines custom-tailored to the PlayStation, PlayStation \n2, and PLAYSTATION 3 platforms, respectively. And of course, most commer-\ncially licensed game engines like Quake , Source, or the Unreal Engine started \nout as proprietary in-house engines.\n1.5.7. \nOpen Source Engines\nOpen source 3D game engines are engines built by amateur and professional \ngame developers and provided online for free. The term “open source” typi-\ncally implies that source code is freely available and that a somewhat open de-\nvelopment model is employed, meaning almost anyone can contribute code. Li-\ncensing, if it exists at all, is oft en provided under the Gnu Public License (GPL) \nor Lesser Gnu Public License (LGPL). The former permits code to be freely used \n1.5. Game Engine Survey\n",
      "content_length": 2460,
      "extraction_method": "Direct"
    },
    {
      "page_number": 50,
      "chapter": null,
      "content": "28 \n1. Introduction\nby anyone, as long as their code is also freely available; the latt er allows the \ncode to be used even in proprietary for-proﬁ t applications. Lots of other free \nand semi-free licensing schemes are also available for open source projects.\nThere are a staggering number of open source engines available on the \nweb. Some are quite good, some are mediocre, and some are just plain aw-\nful!  The list of game engines provided online at htt p://cg.cs.tu-berlin.de/~ki/\nengines.html will give you a feel for the sheer number of engines that are out \nthere.\nOGRE 3D is a well-architected, easy-to-learn, and easy-to-use 3D render-\ning engine. It boasts a fully featured 3D renderer including advanced lighting \nand shadows , a good skeletal character animation system, a two-dimensional \noverlay system for heads-up display s and graphical user interface s, and a \npost-processing system for full-screen eﬀ ects like bloom . OGRE is, by its au-\nthors’ own admission, not a full game engine, but it does provide many of the \nfoundational components required by prett y much any game engine. \nSome other well-known open source engines are listed here.\nz  Panda3D is a script-based engine. The engine’s primary interface is the \nPython custom scripting language. It is designed to make prototyping \n3D games and virtual worlds convenient and fast.\nz  Yake is a relatively new fully featured game engine built on top of \nOGRE .\nz  Crystal Space is a game engine with an extensible modular architecture.\nz  Torque and  Irrlicht are also well-known and widely used engines.\n1.6. \nRuntime Engine Architecture\nA game engine generally consists of a tool suite and a  runtime component. \nWe’ll explore the architecture of the runtime piece ﬁ rst and then get into tools \narchitecture in the following section.\nFigure 1.11 shows all of the major runtime components that make up a \ntypical 3D game engine. Yeah, it’s big!  And this diagram doesn’t even account \nfor all the tools. Game engines are deﬁ nitely large soft ware systems.\nLike all soft ware systems, game engines are built in layers. Normally up-\nper layers depend on lower layers, but not vice versa. When a lower layer \ndepends upon a higher layer, we call this a circular dependency.  Dependency \ncycles are to be avoided in any soft ware system, because they lead to un-\ndesirable  coupling between systems, make the soft ware untestable, and in-\nhibit  code reuse. This is especially true for a large-scale system like a game \nengine.\n",
      "content_length": 2506,
      "extraction_method": "Direct"
    },
    {
      "page_number": 51,
      "chapter": null,
      "content": "29 \n1.6. Runtime Engine Architecture\nGameplay Foundations\nEvent/Messaging \nSystem\nDynamic Game \nObject Model\nScripting System\nWorld Loading / \nStreaming\nStatic World \nElements\nReal-Time Agent-\nBased Simulation\nHigh-Level Game Flow System/FSM\nSkeletal Animation\nAnimation \nDecompression\nInverse \nKinematics (IK)\nGame-Specific \nPost-Processing\nSub-skeletal \nAnimation\nLERP and \nAdditive Blending\nAnimation \nPlayback\nAnimation State \nTree & Layers\nProfiling & Debugging\nMemory & \nPerformance Stats\nIn-Game Menus \nor Console\nRecording & \nPlayback\nHierarchical \nObject Attachment\n3rd Party SDKs\nHavok, PhysX, \nODE etc.\nDirectX, OpenGL, \nlibgcm, Edge, etc.\nBoost++\nSTL / STLPort\netc.\nKynapse\nEuphoria\nGranny, Havok \nAnimation, etc.\nOS\nDrivers\nHardware (PC, XBOX360, PS3, etc.)\nPlatform Independence Layer\nAtomic Data \nTypes\nPlatform Detection\nCollections and \nIterators\nThreading Library\nHi-Res Timer\nFile System\nNetwork Transport \nLayer (UDP/TCP)\nGraphics \nWrappers\nPhysics/Coll. \nWrapper\nCore Systems\nModule Start-Up \nand Shut-Down\nParsers (CSV, \nXML, etc.)\nAssertions\nUnit Testing\nMath Library\nStrings and \nHashed String Ids\nDebug Printing \nand Logging\nMemory Allocation\nEngine Config\n(INI files etc.)\nProfiling / Stats \nGathering\nObject Handles / \nUnique Ids\nRTTI / Reflection \n& Serialization\nCurves & \nSurfaces Library\nRandom Number \nGenerator\nLocalization \nServices\nAsynchronous\nFile I/O\nMovie Player\nMemory Card I/O \n(Older Consoles)\nResources (Game Assets)\nResource Manager\nTexture \nResource\nMaterial \nResource\n3D Model \nResource\nFont \nResource\nCollision \nResource\nPhysics \nParameters\nGame \nWorld/Map\netc.\nSkeleton \nResource\nHuman Interface \nDevices (HID)\nPhysical Device\nI/O\nGame-Specific \nInterface\nAudio\nAudio Playback / \nManagement\nDSP/Effects\n3D Audio Model\nOnline Multiplayer\nMatch-Making & \nGame Mgmt.\nGame State \nReplication\nObject Authority \nPolicy\nScene Graph / Culling Optimizations\nLOD System\nOcclusion & PVS\nSpatial Subdivision \n(BSP Tree, kd-Tree, …)\nVisual Effects\nParticle & Decal \nSystems\nPost Effects\nHDR Lighting\nPRT Lighting, \nSubsurf. Scatter\nEnvironment \nMapping\nLight Mapping & \nDynamic Shadows\nFront End\nHeads-Up Display \n(HUD)\nFull-Motion Video \n(FMV)\nIn-Game Menus\nIn-Game GUI\nWrappers / Attract \nMode\nIn-Game Cinematics \n(IGC)\nCollision & Physics\nShapes/\nCollidables\nRigid Bodies\nPhantoms\nRay/Shape \nCasting (Queries)\nForces & \nConstraints\nPhysics/Collision \nWorld\nRagdoll \nPhysics\nGAME-SPECIFIC SUBSYSTEMS\nGame-Specific Rendering\nTerrain Rendering\nWater Simulation \n& Rendering\netc.\nPlayer Mechanics\nCollision Manifold\nMovement\nState Machine & \nAnimation\nGame Cameras\nPlayer-Follow \nCamera\nDebug Fly-\nThrough Cam\nFixed Cameras\nScripted/Animated \nCameras\nAI\nSight Traces & \nPerception\nPath Finding (A*)\nGoals & Decision-\nMaking\nActions\n(Engine Interface)\nCamera-Relative \nControls (HID)\nWeapons\nPower-Ups\netc.\nVehicles\nPuzzles\nLow-Level Renderer\nPrimitive \nSubmission\nViewports & \nVirtual Screens\nMaterials & \nShaders\nTexture and \nSurface Mgmt.\nGraphics Device Interface\nStatic & Dynamic \nLighting\nCameras\nText & Fonts\nDebug Drawing\n(Lines etc.)\nSkeletal Mesh \nRendering\nFigure 1.11.  Runtime game engine architecture.\n",
      "content_length": 3149,
      "extraction_method": "Direct"
    },
    {
      "page_number": 52,
      "chapter": null,
      "content": "30 \n1. Introduction\nWhat follows is a brief overview of the components shown in the diagram \nin Figure 1.11. The rest of this book will be spent investigating each of these \ncomponents in a great deal more depth and learning how these components \nare usually integrated into a functional whole.\n1.6.1. \nTarget Hardware\nThe  target hardware layer, shown in isolation in Figure 1.12, represents the \ncomputer system or console on which the game will run. Typical platforms \ninclude Microsoft  Windows- and Linux-based PCs, the Apple iPhone and \nMacintosh, Microsoft ’s Xbox and Xbox 360, Sony’s PlayStation, PlayStation 2, \nPlayStation Portable (PSP), and PLAYSTATION 3, and Nintendo’s DS, Game-\nCube, and Wii. Most of the topics in this book are platform-agnostic, but we’ll \nalso touch on some of the design considerations peculiar to PC or console \ndevelopment, where the distinctions are relevant.\nHardware (PC, XBOX360, PS3, etc.)\nFigure 1.12.  Hardware layer.\nDrivers\nFigure 1.13.  Device driver layer.\n1.6.2. Device Drivers\nAs depicted in Figure 1.13, device drivers are low-level soft ware components \nprovided by the operating system or hardware vendor. Drivers manage hard-\nware resources and shield the operating system and upper engine layers from \nthe details of communicating with the myriad variants of hardware devices \navailable.\n1.6.3. Operating System\nOn a PC, the  operating system (OS) is running all the time. It orchestrates the \nexecution of multiple programs on a single computer, one of which is your \ngame. The OS layer is shown in Figure 1.14. Operating systems like Microsoft  \nWindows employ a time-sliced approach to sharing the hardware with mul-\ntiple running programs, known as pre-emptive multitasking . This means that \na PC game can never assume it has full control of the hardware—it must “play \nnice” with other programs in the system.\n",
      "content_length": 1873,
      "extraction_method": "Direct"
    },
    {
      "page_number": 53,
      "chapter": null,
      "content": "31 \n1.6. Runtime Engine Architecture\nOS\nFigure 1.14.  Operating system layer.\n3rd Party SDKs\nHavok, PhysX, \nODE etc.\nDirectX, OpenGL, \nlibgcm, Edge, etc.\nBoost++\nSTL / STLPort\netc.\nKynapse\nEuphoria\nGranny, Havok \nAnimation, etc.\nFigure 1.15.  Third-party SDK layer.\nOn a console, the operating system is oft en just a thin library layer that is \ncompiled directly into your game executable. On a console, the game typically \n“owns” the entire machine. However, with the introduction of the Xbox 360 \nand PLAYSTATION 3, this is no longer strictly the case. The operating sys-\ntem on these consoles can interrupt the execution of your game, or take over \ncertain system resources, in order to display online messages, or to allow the \nplayer to pause the game and bring up the PS3’s Xross Media Bar or the Xbox \n360’s dashboard, for example. So the gap between console and PC develop-\nment is gradually closing (for bett er or for worse).\n1.6.4. Third-Party SDKs and Middleware\nMost game engines leverage a number of third-party  soft ware development \nkit s (SDKs) and middleware, as shown in Figure 1.15. The functional or class-\nbased interface provided by an SDK is oft en called an  application program-\nming interface (API). We will look at a few examples.\n1.6.4.1. \nData Structures and Algorithms\nLike any soft ware system, games depend heavily on collection data structures \nand algorithms to manipulate them. Here are a few examples of third-party \nlibraries which provide these kinds of services.\nz STL. The C++  standard template library provides a wealth of code and \nalgorithms for managing data structures, strings, and stream-based \nI/O.\nz  STLport . This is a portable, optimized implementation of STL.\nz  Boost . Boost is a powerful data structures and algorithms library, \ndesigned in the style of STL. (The online documentation for Boost is \nalso a great place to learn a great deal about computer science!)\nz  Loki . Loki is a powerful generic programming template library which is \nexceedingly good at making your brain hurt!\n",
      "content_length": 2045,
      "extraction_method": "Direct"
    },
    {
      "page_number": 54,
      "chapter": null,
      "content": "32 \n1. Introduction\nGame developers are divided on the question of whether to use template \nlibraries like STL in their game engines. Some believe that the memory alloca-\ntion patt erns of STL, which are not conducive to high-performance program-\nming and tend to lead to memory fragmentation (see Section 5.2.1.4), make \nSTL unusable in a game. Others feel that the power and convenience of STL \noutweigh its problems, and that most of the problems can in fact be worked \naround anyway. My personal belief is that STL is all right for use on a PC, be-\ncause its advanced virtual memory system renders the need for careful mem-\nory allocation a bit less crucial (although one must still be very careful). On a \nconsole, with limited or no virtual memory facilities and exorbitant cache miss \ncosts, you’re probably bett er oﬀ  writing custom data structures that have pre-\ndictable and/or limited memory allocation patt erns. (And you certainly won’t \ngo far wrong doing the same on a PC game project either.)\n1.6.4.2. Graphics\nMost game rendering engines are built on top of a hardware interface library, \nsuch as the following:\nz  Glide is the 3D graphics SDK for the old Voodoo graphics cards. This \nSDK was popular prior to the era of hardware transform and lighting \n(hardware T&L) which began with DirectX 8.\nz  OpenGL is a widely used portable 3D graphics SDK.\nz  DirectX is Microsoft ’s 3D graphics SDK and primary rival to OpenGL .\nz  libgcm is a low-level direct interface to the PLAYSTATION 3’s RSX graph-\nics hardware, which was provided by Sony as a more eﬃ  cient alterna-\ntive to OpenGL.\nz  Edge is a powerful and highly-eﬃ  cient rendering and animation engine \nproduced by Naughty Dog and Sony for the  PLAYSTATION 3 and used \nby a number of ﬁ rst- and third-party game studios.\n1.6.4.3. Collision and Physics\nCollision detection and  rigid body dynamics (known simply as “physics” \nin the game development community) are provided by the following well-\nknown SDKs.\nz  Havok is a popular industrial-strength physics and collision engine.\nz  PhysX is another popular industrial-strength physics and collision en-\ngine, available for free download from NVIDIA.\nz  Open Dynamics Engine (ODE) is a well-known open source physics/col-\nlision p ackage.\n",
      "content_length": 2264,
      "extraction_method": "Direct"
    },
    {
      "page_number": 55,
      "chapter": null,
      "content": "33 \n1.6.4.4. Character Animation\nA number of commercial animation packages exist, including but certainly \nnot limited to the following.\nz  Granny . Rad Game Tools’ popular Granny toolkit includes robust 3D \nmodel and animation exporters for all the major 3D modeling and ani-\nmation packages like Maya, 3D Studio MAX, etc., a runtime library for \nreading and manipulating the exported model and animation data, and \na powerful runtime animation system. In my opinion, the Granny SDK \nhas the best-designed and most logical animation API of any I’ve seen, \ncommercial or proprietary, especially its excellent handling of time.\nz  Havok Animation . The line between physics and animation is becoming \nincreasingly blurred as characters become more and more realistic. The \ncompany that makes the popular Havok physics SDK decided to create \na complimentary animation SDK, which makes bridging the physics-\nanimation gap much easier than it ever has been.\nz  Edge. The Edge  library produced for the PS3 by the ICE team at Naughty \nDog, the Tools and Technology group of Sony Computer Entertainment \nAmerica, and Sony’s Advanced Technology Group in Europe includes \na powerful and eﬃ  cient animation engine and an eﬃ  cient geometry-\nprocessing engine for rendering.\n1.6.4.5. Artiﬁ cial Intelligence\nz  Kynapse . Until recently, artiﬁ cial intelligence (AI) was handled in a cus-\ntom manner for each game. However, a company called Kynogon has \nproduced a middleware SDK called Kynapse. This SDK provides low-\nlevel AI building blocks such as path ﬁ nding, static and dynamic object \navoidance, identiﬁ cation of vulnerabilities within a space (e.g., an open \nwindow from which an ambush could come), and a reasonably good \ninterface between AI and animation.\n1.6.4.6. Biomechanical Character Models\nz  Endorphin and  Euphoria .  These are animation packages that produce \ncharacter motion using advanced  biomechanical models of realistic hu-\nman movement.\nAs we mentioned above, the line between character animation and phys-\nics is beginning to blur. Packages like Havok Animation try to marry physics \nand animation in a traditional manner, with a human animator providing the \nmajority of the motion through a tool like Maya and with physics augmenting \nthat motion at runtime. But recently a ﬁ rm called Natural Motion Ltd. has pro-\n1.6. Runtime Engine Architecture\n",
      "content_length": 2371,
      "extraction_method": "Direct"
    },
    {
      "page_number": 56,
      "chapter": null,
      "content": "34 \n1. Introduction\nduced a product that att empts to redeﬁ ne how character motion is handled in \ngames and other forms of digital media.\nIts ﬁ rst product, Endorphin , is a Maya plug-in that permits animators \nto run full biomechanical simulations on characters and export the resulting \nanimations as if they had been hand-animated. The biomechanical model ac-\ncounts for center of gravity, the character’s weight distribution, and detailed \nknowledge of how a real human balances and moves under the inﬂ uence of \ngravity and other forces.\nIts second product, Euphoria , is a real-time version of Endorphin intend-\ned to produce physically and biomechanically accurate character motion at \nruntime under the inﬂ uence of unpredictable forces.\n1.6.5. Platform Independence Layer\nMost game engines are required to be capable of running on more than one \nhardware platform. Companies like Electronic Arts and Activision/Blizzard, \nfor example, always target their games at a wide variety of platforms, because \nit exposes their games to the largest possible market. Typically, the only game \nstudios that do not target at least two diﬀ erent platforms per game are ﬁ rst-\nparty studios, like Sony’s Naughty Dog and Insomniac studios. Therefore, \nmost game engines are architected with a  platform independence layer, like \nthe one shown in Figure 1.16. This layer sits atop the hardware, drivers, oper-\nating system, and other third-party soft ware and shields the rest of the engine \nfrom the majority of knowledge of the underlying platform.\nBy wrapping or replacing the most commonly used standard C library \nfunctions, operating system calls, and other foundational application pro-\ngramming interfaces (APIs), the platform independence layer ensures consis-\ntent behavior across all hardware platforms. This is necessary because there is \na good deal of variation across platforms, even among “standardized” librar-\nies like the standard C library.\nPlatform Independence Layer\nAtomic Data \nTypes\nPlatform Detection\nCollections and \nIterators\nThreading Library\nHi-Res Timer\nFile System\nNetwork Transport \nLayer (UDP/TCP)\nGraphics \nWrappers\nPhysics/Coll. \nWrapper\nFigure 1.16.  Platform independence layer.\n1.6.6. Core Systems\nEvery game engine, and really every large, complex C++ soft ware application, \nrequires a grab bag of useful soft ware utilities. We’ll categorize these under \nthe label “core systems.” A typical core systems layer is shown in Figure 1.17. \nHere are a few examples of the facilities the core layer usually provides.\n",
      "content_length": 2547,
      "extraction_method": "Direct"
    },
    {
      "page_number": 57,
      "chapter": null,
      "content": "35 \nz  Assertions are lines of error-checking code that are inserted to catch logi-\ncal mistakes and violations of the programmer’s original assumptions. \nAssertion checks are usually stripped out of the ﬁ nal production build \nof the game.\nz  Memory management. Virtually every game engine implements its own \ncustom memory allocation system(s) to ensure high-speed allocations \nand deallocations and to limit the negative eﬀ ects of memory fragmen-\ntation (see Section 5.2.1.4).\nz  Math library. Games are by their nature highly mathematics-intensive. As \nsuch, every game engine has at least one, if not many, math libraries. These \nlibraries provide facilities for vector and matrix math, quaternion rota-\ntions, trigonometry, geometric operations with lines, rays, spheres, frusta, \netc., spline manipulation, numerical integration, solving systems of equa-\ntions, and whatever other facilities the game programmers require.\nz Custom data structures and algorithms. Unless an engine’s designers de-\ncided to rely entirely on a third-party package such as STL, a suite of \ntools for managing fundamental data structures (linked lists, dynamic \narrays, binary trees, hash maps, etc.) and algorithms (search, sort, etc.) \nis usually required. These are oft en hand-coded to minimize or elimi-\nnate dynamic memory allocation and to ensure optimal runtime perfor-\nmance on the target platform(s).\nA detailed discussion of the most common core engine systems can be \nfound in Part II.\n1.6.7. \nResource Manager\nPresent in every game engine in some form, the  resource manager provides \na uniﬁ ed interface (or suite of interfaces) for accessing any and all types of \ngame assets and other engine input data. Some engines do this in a highly \ncentralized and consistent manner (e.g., Unreal ’s packages, OGRE 3D ’s Re-\nsourceManager class). Other engines take an ad hoc approach, oft en leaving \nit up to the game programmer to directly access raw ﬁ les on disk or within \ncompressed archives such as Quake ’s PAK ﬁ les. A typical resource manager \nlayer is depicted in Figure 1.18.\n1.6. Runtime Engine Architecture\nCore Systems\nModule Start-Up \nand Shut-Down\nParsers (CSV, \nXML, etc.)\nAssertions\nUnit Testing\nMath Library\nStrings and \nHashed String Ids\nDebug Printing \nand Logging\nMemory Allocation\nEngine Config\n(INI files etc.)\nProfiling / Stats \nGathering\nObject Handles / \nUnique Ids\nRTTI / Reflection \n& Serialization\nCurves & \nSurfaces Library\nRandom Number \nGenerator\nLocalization \nServices\nAsynchronous\nFile I/O\nMovie Player\nMemory Card I/O \n(Older Consoles)\nFigure 1.17.  Core engine systems.\n",
      "content_length": 2600,
      "extraction_method": "Direct"
    },
    {
      "page_number": 58,
      "chapter": null,
      "content": "36 \n1. Introduction\nLow-Level Renderer\nPrimitive \nSubmission\nViewports & \nVirtual Screens\nMaterials & \nShaders\nTexture and \nSurface Mgmt.\nGraphics Device Interface\nStatic & Dynamic \nLighting\nCameras\nText & Fonts\nDebug Drawing\n(Lines etc.)\nSkeletal Mesh \nRendering\nFigure 1.19.  Low-level rendering engine.\nResources (Game Assets)\nResource Manager\nTexture \nResource\nMaterial \nResource\n3D Model \nResource\nFont \nResource\nCollision \nResource\nPhysics \nParameters\nGame \nWorld/Map\netc.\nSkeleton \nResource\nFigure 1.18.  Resource manager.\n1.6.8. Rendering Engine\nThe  rendering engine is one of the largest and most complex components of \nany game engine. Renderers can be architected in many diﬀ erent ways. There \nis no one accepted way to do it, although as we’ll see, most modern rendering \nengines share some fundamental design philosophies, driven in large part by \nthe design of the 3D graphics hardware upon which they depend.\nOne common and eﬀ ective approach to rendering engine design is to em-\nploy a layered architecture as follows.\n1.6.8.1. \nLow-Level Renderer\nThe  low-level renderer , shown in Figure 1.19, encompasses all of the raw ren-\ndering facilities of the engine. At this level, the design is focused on rendering \na collection of geometric  primitives as quickly and richly as possible, without \nmuch regard for which portions of a scene may be visible. This component is \nbroken into various subcomponents, which are discussed below.\nGraphics Device Interface\nGraphics SDKs, such as DirectX and OpenGL, require a reasonable amount of \ncode to be writt en just to enumerate the available graphics devices, initialize \nthem, set up render surfaces (back-buﬀ er, stencil buﬀ er etc.), and so on. This \n",
      "content_length": 1716,
      "extraction_method": "Direct"
    },
    {
      "page_number": 59,
      "chapter": null,
      "content": "37 \nis typically handled by a component that I’ll call the  graphics device interface \n(although every engine uses its own terminology).\nFor a PC game engine, you also need code to integrate your renderer with \nthe Windows message loop. You typically write a “ message pump ” that ser-\nvices Windows messages when they are pending and otherwise runs your \nrender loop over and over as fast as it can. This ties the game’s keyboard poll-\ning loop to the renderer’s screen update loop. This coupling is undesirable, \nbut with some eﬀ ort it is possible to minimize the dependencies. We’ll explore \nthis topic in more depth later.\nOther Renderer Components\nThe other components in the low-level renderer cooperate in order to collect \nsubmissions of geometric primitives (sometimes called  render packet s), such as \nmeshes, line lists, point lists, particles , terrain patches, text strings, and what-\never else you want to draw, and render them as quickly as possible.\nThe low-level renderer usually provides a viewport abstraction with an \nassociated camera -to-world matrix and 3D projection parameters, such as ﬁ eld \nof view and the location of the near and far clip plane s. The low-level renderer \nalso manages the state of the graphics hardware and the game’s shaders via \nits material system and its dynamic lighting system. Each submitt ed primitive \nis associated with a material and is aﬀ ected by n dynamic lights. The mate-\nrial describes the texture (s) used by the primitive, what device state sett ings \nneed to be in force, and which vertex and pixel shader to use when rendering \nthe primitive. The lights determine how dynamic lighting calculations will \nbe applied to the primitive. Lighting and shading is a complex topic, which \nis covered in depth in many excellent books on computer graphics, including \n[14], [42], and [1].\n1.6.8.2. Scene Graph/Culling Optimizations\nThe low-level renderer draws all of the geometry submitt ed to it, without \nmuch regard for whether or not that geometry is actually visible (other than \nback-face culling and clipping triangles to the camera frustum). A higher-level \ncomponent is usually needed in order to limit the number of primitives sub-\nmitt ed for rendering, based on some form of visibility determination. This \nlayer is shown in Figure 1.20.\nFor very small game worlds, a simple frustum  cull (i.e., removing objects \nthat the camera cannot “see”) is probably all that is required. For larger game \nworlds, a more advanced spatial subdivision data structure might be used to \nimprove rendering eﬃ  ciency, by allowing the  potentially visible set (PVS) \nof objects to be determined very quickly. Spatial subdivisions can take many \n1.6. Runtime Engine Architecture\n",
      "content_length": 2733,
      "extraction_method": "Direct"
    },
    {
      "page_number": 60,
      "chapter": null,
      "content": "38 \n1. Introduction\nforms, including a  binary space partitioning (BSP) tree, a  quadtree , an  octree , \na  kd-tree , or a  sphere hierarchy . A spatial subdivision is sometimes called a \n scene graph, although technically the latt er is a particular kind of data struc-\nture and does not subsume the former.  Portals or  occlusion culling methods \nmight also be applied in this layer of the rendering engine.\nIdeally, the low-level renderer should be completely agnostic to the type \nof spatial subdivision or scene graph being used. This permits diﬀ erent game \nteams to reuse the primitive submission code, but craft  a PVS determination \nsystem that is speciﬁ c to the needs of each team’s game. The design of the \nOGRE 3D open source rendering engine (htt p://www.ogre3d.org) is a great \nexample of this principle in action. OGRE provides a plug-and-play scene \ngraph architecture. Game developers can either select from a number of pre-\nimplemented scene graph designs, or they can provide a custom scene graph \nimplementation.\n1.6.8.3. Visual Effects\nModern game engines support a wide range of  visual eﬀ ects , as shown in \nFigure 1.21, including\nz  particle system s (for smoke, ﬁ re, water splashes, etc.);\nz  decal systems (for bullet holes, foot prints, etc.);\nz light mapping and environment mapping; \nz dynamic shadows;\nz full-screen  post eﬀ ects , applied aft er the 3D scene has been rendered to \nan oﬀ screen buﬀ er.\nScene Graph / Culling Optimizations\nLOD System\nOcclusion & PVS\nSpatial Subdivision \n(BSP Tree, kd-Tree, …)\nFigure 1.20.  A typical scene graph/spatial subdivision layer, for culling optimization.\nVisual Effects\nParticle & Decal \nSystems\nPost Effects\nHDR Lighting\nPRT Lighting, \nSubsurf. Scatter\nEnvironment \nMapping\nLight Mapping & \nDynamic Shadows\nFigure 1.21.  Visual effects.\n",
      "content_length": 1816,
      "extraction_method": "Direct"
    },
    {
      "page_number": 61,
      "chapter": null,
      "content": "39 \nSome examples of full-screen post eﬀ ects include\nz high dynamic range (HDR) lighting and bloom ;\nz full-screen anti-aliasing (FSAA);\nz color correction and color-shift  eﬀ ects, including bleach bypass , satura-\ntion and de-saturation eﬀ ects, etc.\nIt is common for a game engine to have an eﬀ ects system component that \nmanages the specialized rendering needs of particles, decals, and other vi-\nsual eﬀ ects . The particle and decal systems are usually distinct components \nof the rendering engine and act as inputs to the low-level renderer . On the \nother hand, light mapping , environment mapping, and shadows are usually \nhandled internally within the rendering engine proper. Full-screen post ef-\nfects are either implemented as an integral part of the renderer or as a separate \ncomponent that operates on the renderer’s output buﬀ ers.\n1.6.8.4. Front End\nMost games employ some kind of 2D graphics  overlaid on the 3D scene for \nvarious purposes. These include\nz the game’s heads-up display (HUD);\nz in-game menus, a console, and/or other development tools, which may or \nmay not be shipped with the ﬁ nal product;\nz possibly an in-game  graphical user interface (GUI), allowing the player to \nmanipulate his or her character’s inventory, conﬁ gure units for batt le, or \nperform other complex in-game tasks.\nThis layer is shown in Figure 1.22. Two-dimensional graphics like these are \nusually implemented by drawing textured quads (pairs of triangles) with an \northographic projection . Or they may be rendered in full 3D, with the quads \nbill-boarded so they always face the camera .\nWe’ve also included the  full-motion video (FMV) system in this layer. This \nsystem is responsible for playing full-screen movies that have been recorded \n1.6. Runtime Engine Architecture\nFront End\nHeads-Up Display \n(HUD)\nFull-Motion Video \n(FMV)\nIn-Game Menus\nIn-Game GUI\nWrappers / Attract \nMode\nIn-Game Cinematics \n(IGC)\nFigure 1.22.  Front end graphics.\n",
      "content_length": 1959,
      "extraction_method": "Direct"
    },
    {
      "page_number": 62,
      "chapter": null,
      "content": "40 \n1. Introduction\nearlier (either rendered with the game’s rendering engine or using another \nrendering package).\nA related system is the  in-game cinematics (IGC) system. This component \ntypically allows cinematic sequences to be choreographed within the game it-\nself, in full 3D. For example, as the player walks through a city, a conversation \nbetween two key characters might be implemented as an in-game cinematic. \nIGCs may or may not include the player character(s). They may be done as a \ndeliberate cut-away during which the player has no control, or they may be \nsubtly integrated into the game without the human player even realizing that \nan IGC is taking place.\n1.6.9. Proﬁ ling and Debugging Tools\nGames are real-time systems and, as such, game engineers oft en need to proﬁ le \nthe performance of their games in order to optimize performance. In addition, \nmemory resources are usually scarce, so developers make heavy use of mem-\nory analysis tools as well. The  proﬁ ling and  debugging layer, shown in Figure \n1.23, encompasses these tools and also includes in-game debugging facilities, \nsuch as  debug drawing, an  in-game menu system or console, and the ability to \n record and play back gameplay for testing and debugging purposes.\nThere are plenty of good general-purpose soft ware proﬁ ling tools avail-\nable, including\nz Intel’s VTune,\nz IBM’s Quantify and Purify (part of the PurifyPlus tool suite),\nz Compuware’s Bounds Checker.\nHowever, most game engines also incorporate a suite of custom proﬁ ling \nand debugging tools. For example, they might include one or more of the fol-\nlowing:\nz a mechanism for manually instrumenting the code, so that speciﬁ c sec-\ntions of code can be timed;\nz a facility for displaying the proﬁ ling statistics on-screen while the game \nis running;\nz a facility for dumping performance stats to a text ﬁ le or to an Excel \nspreadsheet;\nz a facility for determining how much memory is being used by the en-\ngine, and by each subsystem, including various on-screen displays;\nz the ability to dump memory usage, high-water mark, and leakage stats \nwhen the game terminates and/or during gameplay;\nProfiling & Debugging\nMemory & \nPerformance Stats\nIn-Game Menus \nor Console\nRecording & \nPlayback\nFigure 1.23.  Proﬁ l-\ning and debugging \ntools.\n",
      "content_length": 2300,
      "extraction_method": "Direct"
    },
    {
      "page_number": 63,
      "chapter": null,
      "content": "41 \n1.6. Runtime Engine Architecture\nz tools that allow debug print statements to be peppered throughout the \ncode, along with an ability to turn on or oﬀ  diﬀ erent categories of debug \noutput and control the level of verbosity of the output;\nz the ability to record game events and then play them back. This is tough \nto get right, but when done properly it can be a very valuable tool for \ntracking down bugs.\n1.6.10. Collision and Physics\n Collision detection is important for every game. Without it, objects would in-\nterpenetrate, and it would be impossible to interact with the virtual world \nin any reasonable way. Some games also include a realistic or semi-realistic \ndynamics simulation . We call this the “physics system” in the game industry, \nalthough the term  rigid body dynamics is really more appropriate, because we \nare usually only concerned with the motion (kinematics) of rigid bodies and \nthe forces and torques (dynamics) that cause this motion to occur. This layer \nis depicted in Figure 1.24.\nCollision and physics are usually quite tightly coupled. This is because \nwhen collisions are detected, they are almost always resolved as part of the \nphysics integration and constraint satisfaction logic. Nowadays, very few \ngame companies write their own collision /physics engine. Instead, a third-\nparty SDK is typically integrated into the engine.\nz  Havok is the gold standard in the industry today. It is feature-rich and \nperforms well across the boards.\nz  PhysX by NVIDIA is another excellent collision and dynamics engine. \nIt was integrated into Unreal Engine 3 and is also available for free as \na standalone product for PC game development. PhysX was originally \ndesigned as the interface to Ageia’s new physics accelerator chip. The \nCollision & Physics\nShapes/\nCollidables\nRigid Bodies\nPhantoms\nRay/Shape \nCasting (Queries)\nForces & \nConstraints\nPhysics/Collision \nWorld\nRagdoll \nPhysics\nFigure 1.24.  Collision and physics subsystem.\n",
      "content_length": 1972,
      "extraction_method": "Direct"
    },
    {
      "page_number": 64,
      "chapter": null,
      "content": "42 \n1. Introduction\nSDK is now owned and distributed by NVIDIA, and the company is \nadapting PhysX to run on its latest GPUs.\nOpen source physics and collision engines are also available. Perhaps the \nbest-known of these is the  Open Dynamics Engine (ODE). For more informa-\ntion, see htt p://www.ode.org. I-Collide, V-Collide, and RAPID are other popu-\nlar non-commercial collision detection engines. All three were developed at the \nUniversity of North Carolina (UNC). For more information, see htt p://www.\ncs.unc.edu/~geom/I_COLLIDE/index.html, htt p://www.cs.unc.edu/~geom/V_\nCOLLIDE/index.html, and htt p://www.cs.unc.edu/~geom/OBB/OBBT.html.\n1.6.11. Animation\nAny game that has organic or semi-organic characters (humans, animals, car-\ntoon characters, or even robots) needs an  animation system. There are ﬁ ve \nbasic types of animation used in games:\nz sprite/texture an imation,\nz rigid body hierarchy animation,\nz skeletal animation,\nz vertex animation, and\nz morph targets.\nSkeletal animation permits a detailed 3D character mesh to be posed by \nan animator using a relatively simple system of bones. As the bones move, the \nvertices of the 3D mesh move with them. Although morph targets and vertex \nanimation are used in some engines, skeletal animation is the most prevalent \nanimation method in games today; as such, it will be our primary focus in this \nbook. A typical skeletal animation system is shown in Figure 1.25.\nSkeletal Animation\nAnimation \nDecompression\nInverse \nKinematics (IK)\nGame-Specific \nPost-Processing\nSub-skeletal \nAnimation\nLERP and \nAdditive Blending\nAnimation \nPlayback\nAnimation State \nTree & Layers\nFigure 1.25.  Skeletal animation subsystem.\n",
      "content_length": 1684,
      "extraction_method": "Direct"
    },
    {
      "page_number": 65,
      "chapter": null,
      "content": "43 \nYou’ll notice in Figure 1.11 that Skeletal Mesh Rendering is a component \nthat bridges the gap between the renderer and the animation system. There \nis a tight cooperation happening here, but the interface is very well deﬁ ned. \nThe animation system produces a pose for every bone in the skeleton, and \nthen these poses are passed to the rendering engine as a palett e of matrices. \nThe renderer transforms each vertex by the matrix or matrices in the palett e, \nin order to generate a ﬁ nal blended vertex position. This process is known as \nskinning.\nThere is also a tight coupling between the animation and physics systems, \nwhen rag dolls are employed. A rag doll is a limp (oft en dead) animated char-\nacter, whose bodily motion is simulated by the physics system. The physics \nsystem determines the positions and orientations of the various parts of the \nbody by treating them as a constrained system of rigid bodies. The animation \nsystem calculates the palett e of matrices required by the rendering engine in \norder to draw the character on-screen.\n1.6.12. Human Interface Devices (HID)\nEvery game needs to process input from the player, obtained from various \nhuman interface device s (HIDs) including\nz the keyboard and mouse,\nz a joypad, or\nz other specialized game controllers, like steering wheels, ﬁ shing rods, \ndance pads, the WiiMote, etc.\nWe sometimes call this component the player I/O component, because we \nmay also provide output to the player through the HID , such as force feed-\nback /rumble on a joypad or the audio produced by the WiiMote. A typical \nHID layer is shown in Figure 1.26.\nThe HID engine component is sometimes architected to divorce the \nlow-level details of the game controller(s) on a particular hardware platform \nfrom the high-level game controls. It massages the raw data coming from the \nhardware, introducing a dead zone around the center point of each joypad \nstick, de-bouncing butt on-press inputs, detecting butt on-down and butt on-\nup events, interpreting and smoothing accelerometer inputs (e.g., from the \nPLAYSTATION 3 Sixaxis controller), and more. It oft en provides a mecha-\nnism allowing the player to customize the mapping between physical controls \nand logical game functions. It sometimes also includes a system for detecting \nchords (multiple butt ons pressed together), sequences (butt ons pressed in se-\nquence within a certain time limit), and gestures (sequences of inputs from the \nbutt ons, sticks, accelerometers, etc.).\n1.6. Runtime Engine Architecture\nHuman Interface \nDevices (HID)\nPhysical Device\nI/O\nGame-Specific \nInterface\nFigure 1.26.  The \nplayer \ninput/out-\nput system, also \nknown as the hu-\nman interface de-\nvice (HID) layer.\n",
      "content_length": 2716,
      "extraction_method": "Direct"
    },
    {
      "page_number": 66,
      "chapter": null,
      "content": "44 \n1. Introduction\n1.6.13. Audio\nAudio is just as important as graphics in any game engine. Unfortunately, \n audio oft en gets less att ention than rendering, physics, animation, AI, and \ngameplay. Case in point: Programmers oft en develop their code with their \nspeakers turned oﬀ ! (In fact, I’ve known quite a few game programmers \nwho didn’t even have speakers or headphones.) Nonetheless, no great game \nis complete without a stunning audio engine. The audio layer is depicted in \nFigure 1.27.\nAudio engines vary greatly in sophistication. Quake ’s and Unreal ’s au-\ndio engines are prett y basic, and game teams usually augment them with \ncustom functionality or replace them with an in-house solution. For DirectX \nplatforms (PC and Xbox 360), Microsoft  provides an excellent audio tool suite \ncalled  XACT . Electronic Arts has developed an advanced, high-powered au-\ndio engine internally called SoundR!OT. In conjunction with ﬁ rst-party stu-\ndios like Naughty Dog, Sony Computer Entertainment America (SCEA) pro-\nvides a powerful 3D audio engine called  Scream, which has been used on \na number of PS3 titles including Naughty Dog’s Uncharted: Drake’s Fortune. \nHowever, even if a game team uses a pre-existing audio engine, every game \nrequires a great deal of custom soft ware development, integration work, ﬁ ne-\ntuning, and att ention to detail in order to produce high-quality audio in the \nﬁ nal product.\n1.6.14. Online Multiplayer/Networking\nMany games permit multiple human players to play within a single virtual \nworld.  Multiplayer games come in at least four basic ﬂ avors.\nz Single-screen multiplayer. Two or more human interface devices (joypads, \nkeyboards, mice, etc.) are connected to a single arcade machine, PC, or \nconsole. Multiple player characters inhabit a single virtual world, and a \nsingle camera keeps all player characters in frame simultaneously. Ex-\namples of this style of multiplayer gaming include Smash Brothers, Lego \nStar Wars, and Gauntlet.\nz  Split-screen multiplayer. Multiple player characters inhabit a single vir-\ntual world, with multiple HIDs att ached to a single game machine, but \neach with its own camera, and the screen is divided into sections so that \neach player can view his or her character.\nz  Networked multiplayer. Multiple computers or consoles are networked \ntogether, with each machine hosting one of the players.\nz Massively multiplayer online games (MMOG). Literally hundreds of \nthousands of users can be playing simultaneously within a giant, per-\nAudio\nAudio Playback / \nManagement\nDSP/Effects\n3D Audio Model\nFigure 1.27.  Audio \nsubsystem.\n",
      "content_length": 2620,
      "extraction_method": "Direct"
    },
    {
      "page_number": 67,
      "chapter": null,
      "content": "45 \nsistent, online virtual world hosted by a powerful batt ery of central \nservers.\nThe multiplayer networking layer is shown in Figure 1.28.\nMultiplayer games are quite similar in many ways to their single-player \ncounterparts. However, support for multiple players can have a profound \nimpact on the design of certain game engine components. The game world \nobject model, renderer, human input device system, player control system, \nand animation systems are all aﬀ ected. Retroﬁ tt ing multiplayer features into \na pre-existing single-player engine is certainly not impossible, although it can \nbe a daunting task. Still, many game teams have done it successfully. That \nsaid, it is usually bett er to design multiplayer features from day one, if you \nhave that luxury.\nIt is interesting to note that going the other way—converting a multi-\nplayer game into a single-player game—is typically trivial. In fact, many game \nengines treat single-player mode as a special case of a multiplayer game, in \nwhich there happens to be only one player. The Quake engine is well known \nfor its client-on-top-of-server mode, in which a single executable, running on a \nsingle PC, acts both as the client and the server in single-player campaigns.\n1.6.15. Gameplay Foundation Systems\nThe term  gameplay refers to the action that takes place in the game, the rules \nthat govern the virtual world in which the game takes place, the abilities of \nthe player character(s) (known as player mechanics) and of the other characters \nand objects in the world, and the goals and objectives of the player(s). Game-\nplay is typically implemented either in the native language in which the rest \nof the engine is writt en, or in a high-level scripting language—or sometimes \nboth. To bridge the gap between the gameplay code and the low-level engine \nsystems that we’ve discussed thus far, most game engines introduce a layer \n1.6. Runtime Engine Architecture\nGameplay Foundations\nEvent/Messaging \nSystem\nDynamic Game \nObject Model\nScripting System\nWorld Loading / \nStreaming\nStatic World \nElements\nReal-Time Agent-\nBased Simulation\nHigh-Level Game Flow System/FSM\nHierarchical \nObject Attachment\nFigure 1.29.  Gameplay foundation systems.\nOnline Multiplayer\nMatch-Making & \nGame Mgmt.\nGame State \nReplication\nObject Authority \nPolicy\nFigure 1.28.  On-\nline \nmultiplayer \nsubsystem.\n",
      "content_length": 2360,
      "extraction_method": "Direct"
    },
    {
      "page_number": 68,
      "chapter": null,
      "content": "46 \n1. Introduction\nthat I’ll call the  gameplay foundations layer (for lack of a standardized name). \nShown in Figure 1.29, this layer provides a suite of core facilities, upon which \ngame-speciﬁ c logic can be implemented conveniently.\n1.6.15.1. Game Worlds and Object Models\nThe gameplay foundations layer introduces the notion of a  game world, con-\ntaining both static and dynamic elements. The contents of the world are usu-\nally modeled in an object-oriented manner (oft en, but not always, using an \nobject-oriented programming language). In this book, the collection of object \ntypes that make up a game is called the  game object model. The game object \nmodel provides a real-time simulation of a heterogeneous collection of objects \nin the virtual game world.\nTypical types of game objects include\nz static background geometry, like buildings, roads, terrain (oft en a spe-\ncial case), etc.;\nz dynamic rigid bodies, such as rocks, soda cans, chairs, etc.;\nz player characters (PC);\nz non-player characters (NPC);\nz weapons;\nz projectiles;\nz vehicles;\nz lights (which may be present in the dynamic scene at run time, or only \nused for static lighting oﬄ  ine);\nz cameras;\nz and the list goes on.\nThe game world model is intimately tied to a  soft ware object model, and \nthis model can end up pervading the entire engine. The term soft ware object \nmodel refers to the set of language features, policies, and conventions used to \nimplement a piece of object-oriented soft ware. In the context of game engines, \nthe soft ware object model answers questions, such as:\nz Is your game engine designed in an object-oriented manner?\nz What language will you use? C? C++? Java? OCaml?\nz How will the static class hierarchy be organized? One giant monolithic \nhierarchy? Lots of loosely coupled components?\nz Will you use templates and policy-based design, or traditional polymor-\nphism?\nz How are objects referenced? Straight old pointers? Smart pointers? \nHandles?\n",
      "content_length": 1969,
      "extraction_method": "Direct"
    },
    {
      "page_number": 69,
      "chapter": null,
      "content": "47 \nz How will objects be uniquely identiﬁ ed? By address in memory only? \nBy name? By a global unique identiﬁ er (GUID)?\nz How are the lifetimes of game objects managed?\nz How are the states of the game objects simulated over time?\nWe’ll explore soft ware object models and game object models in consider-\nable depth in Section 14.2.\n1.6.15.2.  Event System\nGame objects invariably need to communicate with one another. This can be \naccomplished in all sorts of ways. For example, the object sending the message \nmight simply call a member function of the receiver object. An  event-driven \narchitecture, much like what one would ﬁ nd in a typical graphical user inter-\nface, is also a common approach to inter-object communication. In an event-\ndriven system, the sender creates a litt le data structure called an event or mes-\nsage, containing the message’s type and any argument data that are to be sent. \nThe event is passed to the receiver object by calling its  event handler function. \nEvents can also be stored in a queue for handling at some future time.\n1.6.15.3. Scripting System\nMany game engines employ a  scripting language in order to make develop-\nment of game-speciﬁ c gameplay rules and content easier and more rapid. \nWithout a scripting language, you must recompile and relink your game ex-\necutable every time a change is made to the logic or data structures used in the \nengine. But when a scripting language is integrated into your engine, changes \nto game logic and data can be made by modifying and reloading the script \ncode. Some engines allow script to be reloaded while the game continues to \nrun. Other engines require the game to be shut down prior to script recompi-\nlation. But either way, the turn-around time is still much faster than it would \nbe if you had to recompile and relink the game’s executable.\n1.6.15.4. Artiﬁ cial Intelligence Foundations\nTraditionally,  artiﬁ cial intelligence (AI) has fallen squarely into the realm of \ngame-speciﬁ c soft ware—it was usually not considered part of the game en-\ngine per se. More recently, however, game companies have recognized pat-\nterns that arise in almost every AI system, and these foundations are slowly \nstarting to fall under the purview of the engine proper.\nA company called Kynogon has developed a commercial AI engine called \n Kynapse , which acts as an “AI foundation layer” upon which game-speciﬁ c \nAI logic can be quite easily developed. Kynapse provides a powerful suite of \nfeatures, including\n1.6. Runtime Engine Architecture\n",
      "content_length": 2532,
      "extraction_method": "Direct"
    },
    {
      "page_number": 70,
      "chapter": null,
      "content": "48 \n1. Introduction\nz a network of path nodes or roaming volumes, that deﬁ nes areas or paths \nwhere AI characters are free to move without fear of colliding with static \nworld geometry;\nz simpliﬁ ed collision information around the edges of each free-roaming \narea;\nz knowledge of the entrances and exits from a region, and from where in \neach region an enemy might be able to see and/or ambush you;\nz a path-ﬁ nding engine based on the well-known A* algorithm;\nz hooks into the collision system and world model, for line-of-sight (LOS) \ntraces and other perceptions;\nz a custom world model which tells the AI system where all the entities of \ninterest (friends, enemies, obstacles) are, permits dynamic avoidance of \nmoving objects, and so on.\nKynapse also provides an architecture for the AI decision layer, including \nthe concept of brains (one per character), agents (each of which is responsible \nfor executing a speciﬁ c task, such as moving from point to point, ﬁ ring on an \nenemy, searching for enemies, etc.), and actions (responsible for allowing the \ncharacter to perform a fundamental movement, which oft en results in playing \nanimations on the character’s skeleton).\n1.6.16. Game-Speciﬁ c Subsystems\nOn top of the gameplay foundation layer and the other low-level engine com-\nponents,  gameplay programmers and designers cooperate to implement the \nfeatures of the game itself. Gameplay systems are usually numerous, highly \nvaried, and speciﬁ c to the game being developed. As shown in Figure 1.30, \nthese systems include, but are certainly not limited to the mechanics of the \nplayer character, various in-game camera systems, artiﬁ cial intelligence for \nthe control of non-player characters (NPCs), weapon systems, vehicles, and \nGAME-SPECIFIC SUBSYSTEMS\nGame-Specific Rendering\nTerrain Rendering\nWater Simulation \n& Rendering\netc.\nPlayer Mechanics\nCollision Manifold\nMovement\nState Machine & \nAnimation\nGame Cameras\nPlayer-Follow \nCamera\nDebug Fly-\nThrough Cam\nFixed Cameras\nScripted/Animated \nCameras\nAI\nSight Traces & \nPerception\nPath Finding (A*)\nGoals & Decision-\nMaking\nActions\n(Engine Interface)\nCamera-Relative \nControls (HID)\nWeapons\nPower-Ups\netc.\nVehicles\nPuzzles\nFigure 1.30.  Game-speciﬁ c subsystems.\n",
      "content_length": 2235,
      "extraction_method": "Direct"
    },
    {
      "page_number": 71,
      "chapter": null,
      "content": "49 \n1.7. Tools and the Asset Pipeline\nthe list goes on. If a clear line could be drawn between the engine and the \ngame, it would lie between the game-speciﬁ c subsystems and the gameplay \nfoundations layer. Practically speaking, this line is never perfectly distinct. \nAt least some game-speciﬁ c knowledge invariably seeps down through the \ngameplay foundations layer and sometimes even extends into the core of the \nengine itself.\n1.7. \n Tools and the Asset Pipeline\nAny game engine must be fed a great deal of data, in the form of  game assets, \nconﬁ guration ﬁ les, scripts, and so on. Figure 1.31 depicts some of the types of \ngame assets typically found in modern game engines. The thicker dark-grey \narrows show how data ﬂ ows from the tools used to create the original  source \nassets all the way through to the game engine itself. The thinner light-grey ar-\nrows show how the various types of assets refer to or use other assets.\n1.7.1. \nDigital Content Creation Tools\nGames are multimedia applications by nature. A game engine’s input data \ncomes in a wide variety of forms, from 3D mesh data to texture bitmaps to \nanimation data to audio ﬁ les. All of this source data must be created and ma-\nnipulated by artists. The tools that the artists use are called digital content cre-\nation (DCC) applications.\nA DCC application is usually targeted at the creation of one particular \ntype of data—although some tools can produce multiple data types. For ex-\nample, Autodesk’s Maya and 3ds Max are prevalent in the creation of both \n3D meshes and animation data. Adobe’s Photoshop and its ilk are aimed at \ncreating and editing bitmaps (textures). SoundForge is a popular tool for cre-\nating audio clips. Some types of game data cannot be created using an oﬀ -\nthe-shelf DCC app. For example, most game engines provide a custom editor \nfor laying out game worlds. Still, some engines do make use of pre-existing \ntools for  game world layout. I’ve seen game teams use 3ds Max or Maya as a \nworld layout tool, with or without custom plug-ins to aid the user. Ask most \ngame developers, and they’ll tell you they can remember a time when they \nlaid out terrain height ﬁ elds using a simple bitmap editor, or typed world \nlayouts directly into a text ﬁ le by hand. Tools don’t have to be prett y—game \nteams will use whatever tools are available and get the job done. That said, \ntools must be relatively easy to use, and they absolutely must be reliable, if a \ngame team is going to be able to develop a highly polished product in a timely \nmanner.\n",
      "content_length": 2552,
      "extraction_method": "Direct"
    },
    {
      "page_number": 72,
      "chapter": null,
      "content": "50 \n1. Introduction\nDigital Content Creation (DCC) Tools\nGame World\nGame \nObject\nMesh\nSkeletal Hierarchy \nExporter\nSkel. \nHierarchy\nAnimation \nExporter\nAnimation \nCurves\nTGA\nTexture\nDXT Compression\nDXT \nTexture\nWorld Editor\nGame Object \nDefinition Tool\nMaterial\nGame Obj. \nTemplate\nAnimation \nSet\nAnimation Tree \nEditor\nAnimation \nTree\nGame \nObject\nGame \nObject\nAsset \nConditioning \nPipeline\nGAME\nWAV\nsound\nAudio Manager \nTool\nSound \nBank\nMesh Exporter\nPhotoshop\nPhotoshop\nSound Forge or Audio Tool\nSound Forge or Audio Tool\nGame \nObject\nMaya, 3DSMAX, etc.\nMaya, 3DSMAX, etc.\nCustom Material \nPlug-In\nHoudini/Other Particle Tool\nHoudini/Other Particle Tool\nParticle \nSystem\nParticle Exporter\nFigure 1.31.  Tools and the asset pipeline.\n1.7.2. \n Asset Conditioning Pipeline\nThe data formats used by digital content creation (DCC) applications are rare-\nly suitable for direct use in-game. There are two primary reasons for this.\n1. The DCC app’s in-memory model of the data is usually much more \ncomplex than what the game engine requires. For example, Maya stores \na directed acyclic graph (DAG) of scene nodes, with a complex web \nof interconnections. It stores a history of all the edits that have been \nperformed on the ﬁ le. It represents the position, orientation, and scale \nof every object in the scene as a full hierarchy of 3D transformations, \ndecomposed into translation, rotation, scale, and shear components. A \n",
      "content_length": 1425,
      "extraction_method": "Direct"
    },
    {
      "page_number": 73,
      "chapter": null,
      "content": "51 \ngame engine typically only needs a tiny fraction of this information in \norder to render the model in-game.\n2. The DCC application’s ﬁ le format is oft en too slow to read at run time, \nand in some cases it is a closed proprietary format.\nTherefore, the data produced by a DCC app is usually exported to a more ac-\ncessible standardized format, or a custom ﬁ le format, for use in-game.\nOnce data has been exported from the DCC app, it oft en must be fur-\nther processed before being sent to the game engine. And if a game studio \nis shipping its game on more than one platform, the intermediate ﬁ les might \nbe processed diﬀ erently for each target platform. For example, 3D mesh data \nmight be exported to an intermediate format, such as XML or a simple binary \nformat. Then it might be processed to combine meshes that use the same ma-\nterial, or split up meshes that are too large for the engine to digest. The mesh \ndata might then be organized and packed into a memory image suitable for \nloading on a speciﬁ c hardware platform.\nThe pipeline from DCC app to game engine is sometimes called the asset \nconditioning pipeline . Every game engine has this in some form.\n1.7.3. \n3D Model/Mesh Data\nThe visible geometry you see in a game is typically made up of two kinds of \ndata.\n1.7.3.1. \nBrush Geometry\n Brush geometry is deﬁ ned as a collection of convex hulls, each of which is de-\nﬁ ned by multiple planes. Brushes are typically created and edited directly in \nthe game world editor. This is what some would call an “old school” approach \nto creating renderable geometry, but it is still used.\nPros:\nz fast and easy to create;\nz accessible to game designers—oft en used to “block out” a game level for \nprototyping purposes;\nz can serve both as collision volumes and as renderable geometry.\nCons:\nz low-resolution – diﬃ  cult to create complex shapes;\nz cannot support articulated objects or animated characters.\n1.7.3.2. \n3D Models (Meshes)\nFor detailed scene elements,  3D models (also referred to as  meshes) are superior \nto brush geometry. A mesh is a complex shape composed of triangles and ver-\n1.7. Tools and the Asset Pipeline\n",
      "content_length": 2148,
      "extraction_method": "Direct"
    },
    {
      "page_number": 74,
      "chapter": null,
      "content": "52 \n1. Introduction\ntices. (A mesh might also be constructed from quads or higher-order subdivi-\nsion surfaces. But on today’s graphics hardware, which is almost exclusively \ngeared toward rendering rasterized triangles, all shapes must eventually be \ntranslated into triangles prior to rendering.)  A mesh typically has one or more \nmaterials applied to it, in order to deﬁ ne visual surface properties (color, re-\nﬂ ectivity, bumpiness, diﬀ use texture , etc.). In this book, I will use the term \n“mesh ” to refer to a single renderable shape, and “model” to refer to a com-\nposite object that may contain multiple meshes, plus animation data and other \nmetadata for use by the game.\nMeshes are typically created in a 3D modeling package such as  3ds Max, \n Maya, or  Soft Image. A relatively new tool called  ZBrush allows ultra high-\nresolution meshes to be built in a very intuitive way and then down-converted \ninto a lower-resolution model with normal maps to approximate the high-\nfrequency detail.\nExporters must be writt en to extract the data from the digital content \ncreation (DCC) tool (Maya, Max, etc.) and store it on disk in a form that \nis digestible by the engine. The DCC apps provide a host of standard or \nsemi-standard export formats, although none are perfectly suited for game \ndevelopment (with the possible exception of COLLADA). Therefore, game \nteams oft en create custom ﬁ le formats and custom exporters to go with \nthem.\n1.7.4. \nSkeletal Animation Data\nA  skeletal mesh is a special kind of mesh that is bound to a skeletal hierarchy \nfor the purposes of articulated animation. Such a mesh is sometimes called a \nskin, because it forms the skin that surrounds the invisible underlying skel-\neton. Each vertex of a skeletal mesh contains a list of indices indicating to \nwhich joint(s) in the skeleton it is bound. A vertex usually also includes a \nset of joint weights, specifying the amount of inﬂ uence each joint has on the \nvertex.\nIn order to render a skeletal mesh , the game engine requires three distinct \nkinds of data.\n1. the mesh itself,\n2. the skeletal hierarchy (joint names, parent-child relationships and the \nbase pose the skeleton was in when it was originally bound to the mesh ), \nand\n3. one or more animation clips, which specify how the joints should move \nover time.\n",
      "content_length": 2321,
      "extraction_method": "Direct"
    },
    {
      "page_number": 75,
      "chapter": null,
      "content": "53 \nThe mesh and skeleton are oft en exported from the DCC application as a sin-\ngle data ﬁ le. However, if multiple meshes are bound to a single skeleton, then \nit is bett er to export the skeleton as a distinct ﬁ le. The animations are usually \nexported individually, allowing only those animations which are in use to be \nloaded into memory at any given time. However, some game engines allow \na bank of animations to be exported as a single ﬁ le, and some even lump the \nmesh, skeleton, and animations into one monolithic ﬁ le.\nAn unoptimized skeletal animation is deﬁ ned by a stream of 4 × 3 matrix \nsamples, taken at a frequency of at least 30 frames per second, for each of the \njoints in a skeleton (of which there are oft en 100 or more). Thus animation data \nis inherently memory-intensive. For this reason, animation data is almost al-\nways stored in a highly compressed format. Compression schemes vary from \nengine to engine, and some are proprietary. There is no one standardized for-\nmat for game-ready animation data.\n1.7.5. \nAudio Data\nAudio clips are usually exported from  Sound Forge or some other audio pro-\nduction tool in a variety of formats and at a number of diﬀ erent data sam-\npling rates. Audio ﬁ les may be in mono, stereo, 5.1, 7.1, or other multichannel \nconﬁ gurations. Wave ﬁ les (.wav) are common, but other ﬁ le formats such as \nPlayStation ADPCM ﬁ les (.vag and .xvag) are also commonplace. Audio clips \nare oft en organized into banks for the purposes of organization, easy loading \ninto the engine, and streaming.\n1.7.6. \nParticle Systems Data\nModern games make use of complex particle eﬀ ects. These are authored by \nartists who specialize in the creation of visual eﬀ ects . Third-party tools, such \nas  Houdini, permit ﬁ lm-quality eﬀ ects to be authored; however, most game \nengines are not capable of rendering the full gamut of eﬀ ects that can be cre-\nated with Houdini. For this reason, many game companies create a custom \nparticle eﬀ ect editing tool, which exposes only the eﬀ ects that the engine actu-\nally supports. A custom tool might also let the artist see the eﬀ ect exactly as it \nwill appear in-game.\n1.7.7. \nGame World Data and the World Editor\nThe  game world is where everything in a game engine comes together. To my \nknowledge, there are no commercially available  game world editors (i.e., the \ngame world equivalent of Maya or Max). However, a number of commercially \navailable game engines provide good world editors.\n1.7. Tools and the Asset Pipeline\n",
      "content_length": 2520,
      "extraction_method": "Direct"
    },
    {
      "page_number": 76,
      "chapter": null,
      "content": "54 \n1. Introduction\nz Some variant of the  Radiant game editor is used by most game engines \nbased on Quake technology;\nz The Half-Life 2 Source engine provides a world editor called  Hammer;\nz  UnrealEd is the Unreal Engine’s world editor. This powerful tool also \nserves as the asset manager for all data types that the engine can con-\nsume.\nWriting a good world editor is diﬃ  cult, but it is an extremely important \npart of any good game engine.\n1.7.8. Some Approaches to Tool Architecture\nA game engine’s tool suite may be architected in any number of ways. Some \ntools might be standalone pieces of soft ware, as shown in Figure 1.32. Some \ntools may be built on top of some of the lower layers used by the runtime en-\ngine, as Figure 1.33 illustrates. Some tools might be built into the game itself. \nFor example, Quake - and Unreal -based games both boast an in-game console \nthat permits developers and “modders” to type debugging and conﬁ guration \ncommands while running the game.\nAs an interesting and unique example, Unreal ’s world editor and asset \nmanager, UnrealEd , is built right into the runtime game engine. To run the \neditor, you run your game with a command-line argument of “editor.” This \nunique architectural style is depicted in Figure 1.34. It permits the tools to \nhave total access to the full range of data structures used by the engine and \nOS\nDrivers\nHardware (PC, XBOX360, PS3, etc.)\n3rd Party SDKs\nPlatform Independence Layer\nCore Systems\nRun-Time Engine\nTools and World Builder\nFigure 1.32.  Standalone tools architecture.\n",
      "content_length": 1560,
      "extraction_method": "Direct"
    },
    {
      "page_number": 77,
      "chapter": null,
      "content": "55 \navoids a common problem of having to have two representations of every \ndata structure – one for the runtime engine and one for the tools. It also means \nthat running the game from within the editor is very fast (because the game \nis actually already running). Live in-game editing, a feature that is normally \nvery tricky to implement, can be developed relatively easily when the editor is \na part of the game. However, an in-engine editor design like this does have its \nshare of problems. For example, when the engine is crashing, the tools become \nunusable as well. Hence a tight coupling between engine and asset creation \ntools can tend to slow down production.\nOS\nDrivers\nHardware (PC, XBOX360, PS3, etc.)\n3rd Party SDKs\nPlatform Independence Layer\nCore Systems\nRun-Time Engine\nTools and World Builder\nFigure 1.33.  Tools built on a framework shared with the game.\nOS\nDrivers\nHardware (PC, XBOX360, PS3, etc.)\n3\nrd Party SDKs\nPlatform Independence Layer\nCore Systems\nRun-Time Engine\nOther Tools\nWorld Builder\nFigure 1.34.  UnrealEngine’s tool architecture.\n1.7. Tools and the Asset Pipeline\n",
      "content_length": 1102,
      "extraction_method": "Direct"
    },
    {
      "page_number": 78,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 79,
      "chapter": null,
      "content": "57\n2\n \nTools of the Trade\nB\nefore we embark on our journey across the fascinating landscape of game \nengine architecture, it is important that we equip ourselves with some ba-\nsic tools and provisions. In the next two chapters, we will review the soft ware \nengineering concepts and practices that we will need during our voyage.  In \nChapter 2, we’ll explore the tools used by the majority of professional game \nengineers.  Then in Chapter 3, we’ll round out our preparations by reviewing \nsome key topics in the realms of object-oriented programming, design pat-\nterns, and large-scale C++ programming.\nGame development is one of the most demanding and broad areas of soft -\nware engineering, so believe me, we’ll want to be well equipped if we are to \nsafely navigate the sometimes-treacherous terrain we’ll be covering. For some \nreaders, the contents of this chapter and the next will be very familiar. How-\never, I encourage you not to skip these chapters entirely. I hope that they will \nserve as a pleasant refresher; and who knows—you might even pick up a new \ntrick or two.\n2.1. \n Version Control\nA version control system is a tool that permits multiple users to work on a \ngroup of ﬁ les collectively. It maintains a history of each ﬁ le, so that changes \n",
      "content_length": 1267,
      "extraction_method": "Direct"
    },
    {
      "page_number": 80,
      "chapter": null,
      "content": "58 \n2. Tools of the Trade\ncan be tracked and reverted if necessary. It permits multiple users to modify \nﬁ les—even the same ﬁ le—simultaneously, without everyone stomping on \neach other’s work. Version control gets its name from its ability to track the \nversion history of ﬁ les. It is sometimes called source control, because it is pri-\nmarily used by computer programmers to manage their source code. Howev-\ner, version control can be used for other kinds of ﬁ les as well. Version control \nsystems are usually best at managing text ﬁ les, for reasons we will discover \nbelow. However, many game studios use a single version control system to \nmanage both source code ﬁ les (which are text) and game assets like textures, \n3D meshes, animations, and audio ﬁ les (which are usually binary).\n2.1.1. \nWhy Use Version Control?\nVersion control is crucial whenever soft ware is developed by a team of mul-\ntiple engineers. Version control\nz provides a central repository from which engineers can share source \ncode;\nz keeps a history of the changes made to each source ﬁ le;\nz provides mechanisms allowing speciﬁ c versions of the code base to be \ntagged and later retrieved;\nz permits versions of the code to be branched oﬀ  from the main develop-\nment line, a feature oft en used to produce demos or make patches to \nolder versions of the soft ware.\nA source control system can be useful even on a single-engineer project. Al-\nthough its multiuser capabilities won’t be relevant, its other abilities, such \nas maintaining a history of changes, tagging versions, creating branches for \ndemos and patches, tracking bugs, etc., are still invaluable.\n2.1.2. \nCommon Version Control Systems\nHere are the most common source control systems you’ll probably encounter \nduring your career as a game engineer.\nz  SCCS and  RCS. The Source Code Control System (SCCS) and the Revi-\nsion Control System (RCS) are two of the oldest version control systems. \nBoth employ a command-line interface. They are prevalent primarily on \nUNIX platforms.\nz  CVS. The Concurrent Version System (CVS) is a heavy-duty profession-\nal-grade command-line-based source control system, originally built on \ntop of RCS (but now implemented as a standalone tool). CVS is preva-\n",
      "content_length": 2244,
      "extraction_method": "Direct"
    },
    {
      "page_number": 81,
      "chapter": null,
      "content": "59 \n2.1. Version Control\nlent on UNIX systems but is also available on other development plat-\nforms such as Microsoft  Windows. It is open source and licensed under \nthe Gnu General Public License (GPL). CVSNT (also known as WinCVS) \nis a native Windows implementation that is based on, and compatible \nwith, CVS.\nz  Subversion. Subversion is an open source version control system aimed \nat replacing and improving upon CVS. Because it is open source and \nhence free, it is a great choice for individual projects, student projects, \nand small studios.\nz  Git. This is an open source revision control system that has been \nused for many venerable projects, including the Linux kernel. In the \ngit development model, the programmer makes changes to ﬁ les and \ncommits the changes to a branch. The programmer can then merge \nhis changes into any other code branch quickly and easily, because git \n“knows” how to rewind a sequence of diﬀ s and reapply them onto \na new base revision—a process git calls rebasing. The net result is a \nrevision control system that is highly eﬃ  cient and fast when dealing \nwith multiple code branches. More information on git can be found at \nhtt p://git-scm.com/.\nz  Perforce. Perforce is a professional-grade source control system, with \nboth text-based and GUI interfaces. One of Perforce’s claims to fame is \nits concept of change lists. A change list is a collection of source ﬁ les that \nhave been modiﬁ ed as a logical unit. Change lists are checked into the \nrepository atomically – either the entire change list is submitt ed, or none \nof it is. Perforce is used by many game companies, including Naughty \nDog and Electronic Arts.\nz NxN Alienbrain.  Alienbrain is a powerful and feature-rich source control \nsystem designed explicitly for the game industry. Its biggest claim to \nfame is its support for very large databases containing both text source \ncode ﬁ les and binary game art assets, with a customizable user interface \nthat can be targeted at speciﬁ c disciplines such as artists, producers, or \nprogrammers.\nz ClearCase.  ClearCase is professional-grade source control system aimed \nat very large-scale soft ware projects. It is powerful and employs a \nunique user interface that extends the functionality of Windows Explor-\ner. I haven’t seen ClearCase used much in the game industry, perhaps \nbecause it is one of the more expensive version control systems.\nz Microsoft  Visual SourceSafe.  SourceSafe is a light-weight source control \npackage that has been used successfully on some game projects.\n",
      "content_length": 2551,
      "extraction_method": "Direct"
    },
    {
      "page_number": 82,
      "chapter": null,
      "content": "60 \n2. Tools of the Trade\n2.1.3. \nOverview of Subversion and TortoiseSVN\nI have chosen to highlight Subversion in this book for a few reasons. First oﬀ , \nit’s free, which is always nice. It works well and is reliable, in my experience. \nA Subversion central  repository is quite easy to set up; and as we’ll see, there \nare already a number of free repository servers out there, if you don’t want to \ngo to the trouble of sett ing one up yourself. There are also a number of good \nWindows and Mac Subversion clients, such as the freely available Tortois-\neSVN for Windows. So while Subversion may not be the best choice for a large \ncommercial project (I personally prefer Perforce for that purpose), I ﬁ nd it \nperfectly suited to small personal and educational projects. Let’s take a look at \nhow to set up and use Subversion on a Microsoft  Windows PC development \nplatform. As we do so, we’ll review core concepts that apply to virtually any \nversion control system.\nSubversion, like most other version control systems, employs a client-\nserver architecture. The server manages a central repository, in which a ver-\nsion-controlled directory hierarchy is stored. Clients connect to the server and \nrequest operations, such as checking out the latest version of the directory \ntree, committ ing new changes to one or more ﬁ les, tagging revisions, branch-\ning the repository, and so on. We won’t discuss sett ing up a server here; we’ll \nassume you have a server, and instead we will focus on sett ing up and using \nthe client. You can learn how to set up a Subversion server by reading Chap-\nter 6 of [37]. However you probably will never need to do so, because you \ncan always ﬁ nd free Subversion servers. For example, Google provides free \nSubversion code hosting at htt p://code.google.com/.\n2.1.4. Setting up a Code Repository on Google\nThe easiest way to get started with Subversion is to visit htt p://code.google.\ncom/ and set up a free Subversion repository. Create a Google user name and \npassword if you don’t already have one, then navigate to Project Hosting un-\nder Developer Resources (see Figure 2.1). Click “Create a new project,” then \nenter a suitable unique project name, like “mygoogleusername-code.” You can \nenter a summary and/or description if you like, and even provide tags so that \nother users all over the world can search for and ﬁ nd your repository. Click \nthe “Create Project” butt on and you’re oﬀ  to the races.\nOnce you’ve created your repository, you can administer it on the  Google \nCode website. You can add and remove users, control options, and perform a \nwealth of advanced tasks. But all you really need to do next is set up a Subver-\nsion client and start using your repository.\n",
      "content_length": 2729,
      "extraction_method": "Direct"
    },
    {
      "page_number": 83,
      "chapter": null,
      "content": "61 \n2.1.5. \nInstalling  TortoiseSVN\nTortoiseSVN is a popular front-end for Subversion. It extends the functionality \nof the Microsoft  Windows Explorer via a convenient right-click menu and over-\nlay icons to show you the status of your version-controlled ﬁ les and folders.\nTo get TortoiseSVN, visit htt p://tortoisesvn.tigris.org/. Download the lat-\nest version from the download page. Install it by double-clicking the .msi ﬁ le \nthat you’ve downloaded and following the installation wizard’s instructions.\nOnce TortoiseSVN is installed, you can go to any folder in Windows Ex-\nplorer and right-click—TortoiseSVN’s menu extensions should now be vis-\nible. To connect to an existing code repository (such as one you created on \nGoogle Code), create a folder on your local hard disk and then right-click \nand select “SVN Checkout….”  The dialog shown in Figure 2.2 will appear. \nIn the “URL of repository” ﬁ eld, enter your repository’s URL. If you are using \nGoogle Code, it should be htt ps://myprojectname.googlecode.com/svn/trunk, \nwhere myprojectname is whatever you named your project when you ﬁ rst cre-\nated it (e.g., “mygoogleusername-code”).\nIf you forget the URL of your repository, just log in to htt p://code.google.\ncom/, go to “Project Hosting” as before, sign in by clicking the “Sign in” link \nin the upper right-hand corner of the screen, and then click the Sett ings link, \nalso found in the upper right-hand corner of the screen. Click the “My Proﬁ le” \ntab, and you should see your project listed there. Your project’s URL is htt ps://\nmyprojectname.googlecode.com/svn/trunk, where myprojectname is whatever \nname you see listed on the “My Proﬁ le” tab.\nYou should now see the dialog shown in Figure 2.3. The user name \nshould be your Google login name. The password is not your Google login \n2.1. Version Control\nFigure 2.1.  Google Code home page, Project Hosting link.\n",
      "content_length": 1894,
      "extraction_method": "Direct"
    },
    {
      "page_number": 84,
      "chapter": null,
      "content": "62 \n2. Tools of the Trade\npassword—it is an automatically generated password that can be obtained by \nsigning in to your account on Goggle’s “Project Hosting” page and clicking \non the “Sett ings” link. (See above for details.)  Checking the “Save authenti-\ncation” option on this dialog allows you to use your repository without ever \nhaving to log in again.  Only select this option if you are working on your own \npersonal machine—never on a machine that is shared by many users.\n Once you’ve authenticated your user name, TortoiseSVN will download \n(“check out”) the entire contents of your repository to your local disk. If you \njust set up your repository, this will be … nothing! The folder you created \nwill still be empty.  But now it is connected to your Subversion repository on \nGoogle (or wherever your server is located). If you refresh your Windows \nExplorer window (hit F5), you should now see a litt le green and white check-\nmark on your folder. This icon indicates that the folder is connected to a Sub-\nversion repository via TortoiseSVN and that the local copy of the repository \nis up-to-date.\n2.1.6. File Versions, Updating, and Committing\nAs we’ve seen, one of the key purposes of any source control system like Sub-\nversion is to allow multiple programmers to work on a single soft ware code \nbase by maintaining a central repository or “master” version of all the source \ncode on a server. The server maintains a version history for each ﬁ le, as shown \nin Figure 2.4. This feature is crucial to large-scale multiprogrammer soft ware \ndevelopment. For example, if someone makes a mistake and checks in code \nthat “breaks the build,” you can easily go back in time to undo those changes \n(and check the log to see who the culprit was!).  You can also grab a snapshot \nof the code as it existed at any point in time, allowing you to work with, dem-\nonstrate, or patch  previous versions of the soft ware.\nFigure 2.2.  TortoiseSVN initial check-out dialog.\nFigure 2.3.  TortoiseSVN user authentication dialog.\n",
      "content_length": 2033,
      "extraction_method": "Direct"
    },
    {
      "page_number": 85,
      "chapter": null,
      "content": "63 \n Each programmer gets a local copy of the code on his or her machine. In \nthe case of TortoiseSVN, you obtain your initial working copy by “ checking \nout” the repository, as described above. Periodically you should update your \nlocal copy to reﬂ ect any changes that may have been made by other program-\nmers.  You do this by right-clicking on a folder and selecting “SVN Update” \nfrom the pop-up menu.\nYou can work on your local copy of the code base without aﬀ ecting the \nother programmers on the team (Figure 2.5). When you are ready to share \nyour changes with everyone else, you commit your changes to the repository \n(also known as  submitt ing or  checking in). You do this by right-clicking on the \nfolder you want to commit and selecting “ SVN Commit…” from the pop-up \n2.1. Version Control\nFigure 2.6.  TortoiseSVN Commit dialog.\nFoo.cpp (version 1)\nFoo.cpp (version 2)\nFoo.cpp (version 3)\nFoo.cpp (version 4)\nBar.cpp (version 1)\nBar.cpp (version 2)\nBar.cpp (version 3)\nFigure 2.4.  File version histories.\nFoo.cpp (version 4)\nFoo.cpp (local edits)\nFigure 2.5.  Editing the local copy of a ver-\nsion-controlled ﬁ le.\n",
      "content_length": 1133,
      "extraction_method": "Direct"
    },
    {
      "page_number": 86,
      "chapter": null,
      "content": "64 \n2. Tools of the Trade\nmenu. You will get a dialog like the one shown in Figure 2.6, asking you to \nconﬁ rm the changes.\nDuring a commit operation, Subversion generates a  diﬀ  between your lo-\ncal version of each ﬁ le and the latest version of that same ﬁ le in the repository. \nThe term “diﬀ ” means diﬀ erence, and it is typically produced by performing \na line-by-line comparison of the two versions of the ﬁ le. You can double-click \non any ﬁ le in the TortoiseSVN Commit dialog (Figure 2.6) to see the diﬀ s be-\ntween your version and the latest version on the server (i.e., the changes you \nmade). Files that have changed (i.e., any ﬁ les that “have diﬀ s”) are committ ed. \nThis replaces the latest version in the repository with your local version, add-\ning a new entry to the ﬁ le’s version history. Any ﬁ les that have not changed \n(i.e., your local copy is identical to the latest version in the repository) are \nignored by default during a commit. An example commit operation is shown \nin Figure 2.7.\nIf you created any new ﬁ les prior to the commit, they will be listed as \n“non-versioned” in the Commit dialog. You can check the litt le check boxes \nbeside them in order to add them to the repository. Any ﬁ les that you deleted \nlocally will likewise show up as “missing”—if you check their check boxes, \nthey will be deleted from the repository. You can also type a comment in the \nCommit dialog. This comment is added to the repository’s history log, so that \nyou and others on your team will know why these ﬁ les were checked in.\n2.1.7. \nMultiple Check-Out, Branching, and Merging\nSome version control systems require  exclusive check-out. This means that you \nmust ﬁ rst indicate your intentions to modify a ﬁ le by checking it out and lock-\ning it.  The ﬁ le(s) that are checked out to you are writable on your local disk \nand cannot be checked out by anyone else. All other ﬁ les in the repository are \nread-only on your local disk. Once you’re done editing the ﬁ le, you can check \nit in, which releases the lock and commits the changes to the repository for ev-\neryone else to see. The process of exclusively locking ﬁ les for editing ensures \nthat no two people can edit the same ﬁ le simultaneously.\nSubversion, CVS, Perforce, and many other high-quality version control \nsystems also permit  multiple check-out.; i.e., you can be editing a ﬁ le while \nsomeone else is editing that same ﬁ le. Whichever user’s changes are commit-\nted ﬁ rst become the latest version of the ﬁ le in the repository. Any subsequent \ncommits by other users require that programmer to merge his or her changes \nwith the changes made by the programmer(s) who committ ed previously.\nBecause more than one set of changes (diﬀ s) have been made to the same \nﬁ le, the version control system must  merge the changes in order to produce a \nﬁ nal version of the ﬁ le. This is oft en not a big deal, and in fact many conﬂ icts \nFoo.cpp (version 5)\nFoo.cpp (version 4)\nFigure 2.7.  Com-\nmitting local edits \nto the repository.\n",
      "content_length": 3025,
      "extraction_method": "Direct"
    },
    {
      "page_number": 87,
      "chapter": null,
      "content": "65 \ncan be resolved automatically by the version control system. For example, if \nyou changed function f() and another programmer changed function g(), \nthen your edits would have been to a diﬀ erent range of lines in the ﬁ le than \nthose of the other programmer.  In this case, the merge between your changes \nand his or her changes will usually resolve automatically without any con-\nﬂ icts. However, if you were both making changes to the same function f(), \nthen the second programmer to commit his or her changes will need to do a \n three-way merge (see Figure 2.8).\nFor three-way merges to work, the version control server has to be smart \nenough to keep track of which version of each ﬁ le you currently have on your \nlocal disk. That way, when you merge the ﬁ les, the system will know which ver-\nsion is the base version (the common ancestor, such as version 4 in Figure 2.8).\nSubversion permits multiple check-out, and in fact it doesn’t require you \nto check out ﬁ les explicitly at all. You simply start editing the ﬁ les locally—all \nﬁ les are writable on your local disk at all times. (By the way, this is one reason \nthat Subversion doesn’t scale well to large projects, in my opinion. To deter-\nmine which ﬁ les you have changed, Subversion must search the entire tree of \nsource ﬁ les, which can be slow. Version control systems like Perforce, which \nexplicitly keep track of which ﬁ les you have modiﬁ ed, are usually easier to \nwork with when dealing with large amounts of code. But for small projects, \nSubversion’s approach works just ﬁ ne.)\n2.1. Version Control\nFoo.cpp (joe_b)\nFoo.cpp (suzie_q)\njoe_b and suzie_q both \nstart editing Foo.cpp at \nthe same time\nFoo.cpp (version 4)\nFoo.cpp (joe_b)\nFoo.cpp (version 5)\nsuzie_q commits her \nchanges first\njoe_b must now do a 3-way \nmerge, which involves 2 sets\nof diffs:\nFoo.cpp (version 6)\nFoo.cpp (joe_b)\nFoo.cpp (version 5)\nFoo.cpp (version 4)\nFoo.cpp (version 4)\nversion 4 to his local version\nversion 4 to version 5\nFigure 2.8.  Three-way merge due to local edits by two different users.\n",
      "content_length": 2061,
      "extraction_method": "Direct"
    },
    {
      "page_number": 88,
      "chapter": null,
      "content": "66 \n2. Tools of the Trade\nWhen you perform a commit operation by right-clicking on any folder \nand selecting “SVN Commit…” from the pop-up menu, you may be prompt-\ned to merge your changes with changes made by someone else. But if no one \nhas changed the ﬁ le since you last updated your local copy, then your changes \nwill be committ ed without any further action on your part. This is a very con-\nvenient feature, but it can also be dangerous. It’s a good idea to always check \nyour commits carefully to be sure you aren’t committ ing any ﬁ les that you \ndidn’t intend to modify. When TortoiseSVN displays its Commit Files dialog, \nyou can double-click on an individual ﬁ le in order to see the diﬀ s you made \nprior to hitt ing the “OK” butt on.\n2.1.8.  Deleting Files\nWhen a ﬁ le is deleted from the repository, it’s not really gone. The ﬁ le still ex-\nists in the repository, but its latest version is simply marked “deleted” so that \nusers will no longer see the ﬁ le in their local directory trees. You can still see \nand access previous versions of a deleted ﬁ le by right-clicking on the folder in \nwhich the ﬁ le was contained and selecting “Show log” from the TortoiseSVN \nmenu.\nYou can undelete a deleted ﬁ le by updating your local directory to the \nversion immediately before the version in which the ﬁ le was marked deleted. \nThen simply commit the ﬁ le again. This replaces the latest deleted version of \nthe ﬁ le with the version just prior to the deletion, eﬀ ectively undeleting the \nﬁ le.\n2.2. Microsoft Visual Studio\nCompiled languages, such as C++, require a  compiler and  linker in order to \ntransform source code into an executable program. There are many com-\npilers/linkers available for C++, but for the Microsoft  Windows platform \nthe most commonly used package is probably  Microsoft  Visual Studio. The \nfully featured Professional Edition of the product can be purchased at any \nstore that sells Windows soft ware. And Visual Studio Express, its lighter-\nweight cousin, is available for free download at htt p://www.microsoft .com/\nexpress/download/. Documentation on Visual Studio is available online at the \nMicrosoft  Developer’s Network (MSDN) site (htt p://msdn.microsoft .com/en-\nus/library/52f3sw5c.aspx). \nVisual Studio is more than just a compiler and linker. It is an  integrated \ndevelopment environment (IDE), including a slick and fully featured text editor \nfor source code and a powerful source-level and machine-level debugger. In \n",
      "content_length": 2481,
      "extraction_method": "Direct"
    },
    {
      "page_number": 89,
      "chapter": null,
      "content": "67 \nthis book, our primary focus is the Windows platform, so we’ll investigate \nVisual Studio in some depth. Much of what you learn below will be applicable \nto other compilers, linkers, and debuggers, so even if you’re not planning on \never using Visual Studio, I suggest you skim this section for useful tips on us-\ning compilers, linkers, and debuggers in general.\n2.2.1. \nSource Files, Headers, and Translation Units\nA program writt en in C++ is comprised of  source ﬁ les. These typically have a .c, \n.cc, .cxx, or .cpp extension, and they contain the bulk of your program’s source \ncode.  Source ﬁ les are technically known as  translation units, because the com-\npiler translates one source ﬁ le at a time from C++ into machine code.\nA special kind of source ﬁ le, known as a  header ﬁ le, is oft en used in order to \nshare information, such as type declarations and function prototypes, between \ntranslation units. Header ﬁ les are not seen by the compiler. Instead, the C++ \n preprocessor replaces each  #include statement with the contents of the corre-\nsponding header ﬁ le prior to sending the translation unit to the compiler. This \nis a subtle but very important distinction to make. Header ﬁ les exist as distinct \nﬁ les from the point of view of the programmer—but thanks to the preproces-\nsor’s header ﬁ le expansion, all the compiler ever sees are translation units.\n2.2.2. Libraries, Executables, and Dynamic Link Libraries\nWhen a translation unit is compiled, the resulting machine code is placed in \nan  object ﬁ le (ﬁ les with a .obj extension under Windows, or .o under UNIX-\nbased operating systems). The machine code in an object ﬁ le is\nz relocatable, meaning that the memory addresses at which the code re-\nsides have not yet been determined, and\nz unlinked, meaning that any external references to functions and global \ndata that are deﬁ ned outside the translation unit have not yet been re-\nsolved.\nObject ﬁ les can be collected into groups called libraries. A  library is simply \nan archive, much like a Zip or tar ﬁ le, containing zero or more object ﬁ les. Li-\nbraries exist merely as a convenience, permitt ing a large number of object ﬁ les \nto be collected into a single easy-to-use ﬁ le.\nObject ﬁ les and libraries are linked into an  executable by the linker. The \nexecutable ﬁ le contains fully resolved machine code that can be loaded and \nrun by the operating system . The linker’s jobs are\nz to calculate the ﬁ nal relative addresses of all the machine code, as it will \nappear in memory when the program is run, and\n2.2. Microsoft Visual Studio\n",
      "content_length": 2588,
      "extraction_method": "Direct"
    },
    {
      "page_number": 90,
      "chapter": null,
      "content": "68 \n2. Tools of the Trade\nz to ensure that all external references to functions and global data made \nby each translation unit (object ﬁ le) are properly resolved.\nIt’s important to remember that the machine code in an executable ﬁ le is still \nrelocatable, meaning that the addresses of all instructions and data in the ﬁ le \nare still relative to an arbitrary base address, not absolute. The ﬁ nal absolute \nbase address of the program is not known until the program is actually loaded \ninto memory, just prior to running it.\nA  dynamic link library (DLL) is a special kind of library that acts like a \nhybrid between a regular static library and an executable. The DLL acts like \na library, because it contains functions that can be called by any number of \ndiﬀ erent executables. However, a DLL also acts like an executable, because it \ncan be loaded by the operating system independently, and it contains some \nstart-up and shut-down code that runs much the way the main() function in \na C++ executable does.\nThe executables that use a DLL contain partially linked machine code. Most \nof the function and data references are fully resolved within the ﬁ nal execut-\nable, but any references to external functions or data that exist in a DLL re-\nmain unlinked. When the executable is run, the operating system resolves the \naddresses of all unlinked functions by locating the appropriate DLLs, load-\ning them into memory if they are not already loaded, and patching in the \nnecessary memory addresses. Dynamically linked libraries are a very useful \noperating system feature, because individual DLLs can be updated without \nchanging the executable(s) that use them.\n2.2.3. Projects and Solutions\nNow that we understand the diﬀ erence between libraries, executables, and \ndynamic link libraries (DLLs), let’s see how to create them. In Visual Studio, \na project is a collection of source ﬁ les which, when compiled, produce a library, \nan executable, or a DLL. Projects are stored in project ﬁ les with a .vcproj ex-\ntension. In Visual Studio .NET 2003 (version 7), Visual Studio 2005 (version 8), \nand Visual Studio 2008 (version 9), .vcproj ﬁ les are in XML format, so they are \nreasonably easy for a human to read and even edit by hand if necessary.\nAll versions of Visual Studio since version 7 (Visual Studio 2003) employ \n solution ﬁ les (ﬁ les with a .sln extension) as a means of containing and manag-\ning collections of projects. A solution is a collection of dependent and/or in-\ndependent projects intended to build one or more libraries, executables and/\nor DLLs. In the Visual Studio graphical user interface , the Solution Explorer is \nusually displayed along the right or left  side of the main window, as shown \nin Figure 2.9.\n",
      "content_length": 2745,
      "extraction_method": "Direct"
    },
    {
      "page_number": 91,
      "chapter": null,
      "content": "69 \nThe Solution Explorer is a tree view. The solution itself is at the root, with \nthe projects as its immediate children. Source ﬁ les and headers are shown as \nchildren of each project. A project can contain any number of user-deﬁ ned \nfolders, nested to any depth. Folders are for organizational purposes only and \nhave nothing to do with the folder structure in which the ﬁ les may reside \non-disk. However it is common practice to mimic the on-disk folder structure \nwhen sett ing up a project’s folders.\n2.2.4. Build Conﬁ gurations\nThe C/C++ preprocessor, compiler, and linker oﬀ er a wide variety of options \nto control how your code will be built. These options are normally speciﬁ ed \non the command line when the compiler is run.  For example, a typical com-\nmand to build a single translation unit with the Microsoft  compiler might look \nlike this:\nC:\\> cl /c foo.cpp /Fo foo.obj /Wall /Od /Zi\nThis tells the compiler/linker to compile but not link (/c) the translation unit \nnamed foo.cpp, output the result to an object ﬁ le named foo.obj (/Fo foo.obj), \nturn on all  warnings (/Wall), turn oﬀ  all  optimizations (/Od), and generate \ndebugging information (/Zi).\nModern compilers provide so many options that it would be impracti-\ncal and error prone to specify all of them every time you build your code. \nThat’s where  build conﬁ gurations come in. A build conﬁ guration is really just \na collection of preprocessor, compiler, and linker options associated with a \nparticular project in your solution. You can deﬁ ne any number of build con-\n2.2. Microsoft Visual Studio\nFigure 2.9.  The VisualStudio Solution Explorer window.\n",
      "content_length": 1645,
      "extraction_method": "Direct"
    },
    {
      "page_number": 92,
      "chapter": null,
      "content": "70 \n2. Tools of the Trade\nﬁ gurations, name them whatever you want, and conﬁ gure the preprocessor, \ncompiler, and linker options diﬀ erently in each conﬁ guration. By default, the \nsame options are applied to every translation unit in the project, although you \ncan override the global project sett ings on an individual translation unit basis. \n(I recommend avoiding this if at all possible, because it becomes diﬃ  cult to \ntell which .cpp ﬁ les have custom sett ings and which do not.)\nMost projects have at least two build conﬁ gurations, typically called \n“Debug” and “Release.” The release build is for the ﬁ nal shipping soft ware, \nwhile the debug build is for development purposes. A debug build runs more \nslowly than a release build, but it provides the programmer with invaluable \ninformation for developing and debugging the program.\n2.2.4.1. Common Build Options\nThis section lists some of the most common options you’ll want to control via \nbuild conﬁ gurations for a game engine project.\nPreprocessor Settings\nThe C++ preprocessor handles the expansion of #included ﬁ les and the deﬁ -\nnition and substitution of #defined macros. One extremely powerful feature \nof all modern C++ preprocessors is the ability to deﬁ ne preprocessor macros \nvia command-line options (and hence via build conﬁ gurations). Macros de-\nﬁ ned in this way act as though they had been writt en into your source code \nwith a #define statement. For most compilers, the command line option for \nthis is –D or /D, and any number of these directives can be used.\nThis feature allows you to communicate various build options to your \ncode, without having to modify the source code itself.  As a ubiquitous exam-\nple, the symbol _DEBUG is always deﬁ ned for a debug build, while in release \nbuilds the symbol  NDEBUG is deﬁ ned instead. The source code can check these \nﬂ ags and in eﬀ ect “know” whether it is being built in debug or release mode. \nThis is known as conditional compilation. For example:\nvoid f()\n{\n#ifdef _DEBUG\n \nprintf(“Calling function f()\\n”);\n#endif\n \n// ...\n}\nThe compiler is also free to introduce “magic” preprocessor macros into \nyour code, based on its knowledge of the compilation environment and target \nplatform. For example, the macro  __cplusplus is deﬁ ned by most C/C++ \n",
      "content_length": 2291,
      "extraction_method": "Direct"
    },
    {
      "page_number": 93,
      "chapter": null,
      "content": "71 \ncompilers when compiling a C++ ﬁ le. This allows code to be writt en that auto-\nmatically adapts to being compiled for C or C++.\nAs another example, every compiler identiﬁ es itself to the source code \nvia a “magic” preprocessor macro. When compiling code under the Microsoft  \ncompiler, the macro  _MSC_VER is deﬁ ned; when compiling under the  GNU \ncompiler (gcc), the macro  _GNUC_ is deﬁ ned instead, and so on for the oth-\ner compilers. The target platform on which the code will be run is likewise \nidentiﬁ ed via macros. For example, when building for a 32-bit Windows \nmachine, the symbol  _WIN32 is always deﬁ ned. These key features permit \ncross-platform code to be writt en, because they allow your code to “know” \nwhat compiler is compiling it and on which target platform it is destined to \nbe run.\nCompiler Settings\nOne of the most common compiler options controls whether or not the com-\npiler should include  debugging information with the object ﬁ les it produces. \nThis information is used by debuggers to step through your code, display the \nvalues of variables, and so on. Debugging information makes your executa-\nbles larger on disk and also opens the door for hackers to reverse-engineer \nyour code. So, it is always stripped from the ﬁ nal shipping version of your \nexecutable. However, during development, debugging information is invalu-\nable and should always be included in your builds.\nThe compiler can also be told whether or not to expand inline functions. \nWhen  inline function expansion is turned oﬀ , every inline function appears \nonly once in memory, at a distinct address. This makes the task of tracing \nthrough the code in the debugger much simpler, but obviously comes at the \nexpense of the execution speed improvements normally achieved by inlin-\ning.\nInline function expansion is but one example of generalized code trans-\nformations known as  optimizations. The aggressiveness with which the com-\npiler att empts to optimize your code, and the kinds of optimizations its uses, \ncan be controlled via compiler options. Optimizations have a tendency to re-\norder the statements in your code, and they also cause variables to be stripped \nout of the code altogether, or moved around, and can cause CPU registers to \nbe reused for new purposes later in the same function. Optimized code usu-\nally confuses most debuggers, causing them to “lie” to you in various ways, \nand making it diﬃ  cult or impossible to see what’s really going on. As a result, \nall optimizations are usually turned oﬀ  in a debug build. This permits every \nvariable and every line of code to be scrutinized as it was originally coded. \nBut, of course, such code will run much more slowly than its fully optimized \ncounterpart.\n2.2. Microsoft Visual Studio\n",
      "content_length": 2776,
      "extraction_method": "Direct"
    },
    {
      "page_number": 94,
      "chapter": null,
      "content": "72 \n2. Tools of the Trade\nLinker Settings\nThe linker also exposes a number of options. You can control what type of \noutput ﬁ le to produce—an executable or a DLL. You can also specify which \nexternal libraries should be linked into your executable, and which directory \npaths to search in order to ﬁ nd them. A common practice is to link with de-\nbug libraries when building a debug executable and with optimized libraries \nwhen building in release mode.\nLinker options also control things like stack size, the preferred base ad-\ndress of your program in memory, what type of machine the code will run on \n(for machine-speciﬁ c optimizations), and a host of other minutia with which \nwe will not concern ourselves here.\n2.2.4.2. Typical Build Conﬁ gurations\nGame projects oft en have more than just two build conﬁ gurations. Here are a \nfew of the common conﬁ gurations I’ve seen used in game development.\nz  Debug. A debug build is a very slow version of your program, with all \noptimizations turned oﬀ , all function inlining disabled, and full debug-\nging information included. This build is used when testing brand new \ncode and also to debug all but the most trivial problems that arise dur-\ning development.\nz Release. A  release build is a faster version of your program, but with \ndebugging information and assertions still turned on. (See Section \n3.3.3.3 for a discussion of assertions.) This allows you to see your game \nrunning at a speed representative of the ﬁ nal product, but still gives you \nsome opportunity to debug problems.\nz Production. A production conﬁ guration is intended for building the ﬁ nal \ngame that you will ship to your customers. It is sometimes called a “Fi-\nnal” build or “Disk” build. Unlike a release build, all debugging informa-\ntion is stripped out of a  production build, all assertions are usually turned \noﬀ , and optimizations are cranked all the way up. A production build is \nvery tricky to debug, but it is the fastest and leanest of all build types.\nz Tools. Some game studios utilize code libraries that are shared between \noﬄ  ine tools and the game itself. In this scenario, it oft en makes sense \nto deﬁ ne a  “Tools” build, which can be used to conditionally compile \nshared code for use by the tools.  The tools build usually deﬁ nes a pre-\nprocessor macro (e.g., TOOLS_BUILD) that informs the code that it is be-\ning built for use in a tool. For example, one of your tools might require \ncertain C++ classes to expose editing functions that are not needed by \nthe game. These functions could be wrapped in an #ifdef TOOLS_\n",
      "content_length": 2581,
      "extraction_method": "Direct"
    },
    {
      "page_number": 95,
      "chapter": null,
      "content": "73 \nBUILD directive. Since you usually want both debug and release ver-\nsions of your tools, you will probably ﬁ nd yourself creating two tools \nbuilds, named something like “ToolsDebug” and “ToolsRelease.”\nHybrid Builds\nA  hybrid build is a build conﬁ guration in which the majority of the translation \nunits are built in release mode, but a small subset of them is built in debug \nmode. This permits the segment of code that is currently under scrutiny to be \neasily debugged, while the rest of the code continues to run at full speed.\nWith a text-based build system like  make, it is quite easy to set up a hybrid \nbuild which permits users to specify the use of debug mode on a per-transla-\ntion-unit basis. In a nutshell, we deﬁ ne a make variable called something like \n$HYBRID_SOURCES, which lists the names of all translation units (.cpp ﬁ les) \nthat should be compiled in debug mode for our hybrid build. We set up build \nrules for compiling both debug and release versions of every translation unit, \nand arrange for the resulting object ﬁ les (.obj/.o) to be placed into two diﬀ er-\nent folders, one for debug and one for release. The ﬁ nal link rule is set up to \nlink with the debug versions of the object ﬁ les listed in $HYBRID_SOURCES\nand with the release versions of all other object ﬁ les. If we’ve set it up properly, \nmake’s dependency rules will take care of the rest.\nUnfortunately, this is not so easy to do in Visual Studio, because its build \nconﬁ gurations are designed to be applied on a per-project basis, not per trans-\nlation unit. The crux of the problem is that we cannot easily deﬁ ne a list of \nthe translation units that we want to build in debug mode. However, if your \nsource code is already organized into libraries, you can set up a “Hybrid” \nbuild conﬁ guration at the solution level, which picks and chooses between \ndebug and release builds on a per-project (and hence per-library) basis. This \nisn’t as ﬂ exible as having control on a per-translation-unit basis, but it does \nwork reasonably well if your libraries are suﬃ  ciently granular.\nBuild Conﬁ gurations and Testability\nThe more build conﬁ gurations your project supports, the more diﬃ  cult test-\ning becomes. Although the diﬀ erences between the various conﬁ gurations \nmay be slight, there’s a ﬁ nite probability that a critical bug may exist in one \nof them but not in the others. Therefore, each build conﬁ guration must be \ntested equally thoroughly. Most game studios do not formally test their debug \nbuilds, because the debug conﬁ guration is primarily intended for internal use \nduring initial development of a feature and for the debugging of problems \nfound in one of the other conﬁ gurations. However, if your testers spend most \nof their time testing your release conﬁ guration, then you cannot simply make \na production build of your game the night before Gold Master and expect it \n2.2. Microsoft Visual Studio\n",
      "content_length": 2929,
      "extraction_method": "Direct"
    },
    {
      "page_number": 96,
      "chapter": null,
      "content": "74 \n2. Tools of the Trade\nto have an identical bug proﬁ le to that of the release build. Practically speak-\ning, the test team must test both your release and production builds equally \nthroughout alpha and beta, to ensure that there aren’t any nasty surprises \nlurking in your production build. In terms of  testability, there is a clear advan-\ntage to keeping your build conﬁ gurations to a minimum, and in fact some stu-\ndios have no production build for this reason—they simply ship their release \nbuild once it has been thoroughly tested.\n2.2.4.3.  Project Conﬁ guration Tutorial\nRight-clicking on any project in the Solution Explorer and selecting “Proper-\nties…” from the menu brings up the project’s “Property Pages” dialog. The \ntree view on the left  shows various categories of sett ings. Of these, the three \nwe will use most are\nz Conﬁ guration Properties/General,\nz Conﬁ guration Properties/Debugging,\nz Conﬁ guration Properties/C++,\nz Conﬁ guration Properties/Linker.\nConﬁ gurations Drop-Down Combo Box\nNotice the drop-down combo box labeled “Conﬁ guration:” at the top-left  cor-\nner of the window. All of the properties displayed on these property pages ap-\nply separately to each build conﬁ guration. If you set a property for the debug \nconﬁ guration, this does not necessarily mean that the same sett ing exists for \nthe release conﬁ guration.\nIf you click on the combo box to drop down the list, you’ll ﬁ nd that you \ncan select a single conﬁ guration or multiple conﬁ gurations, including “All \nconﬁ gurations.” As a rule of thumb, try to do most of your build conﬁ guration \nediting with “All conﬁ gurations” selected. That way, you won’t have to make \nthe same edits multiple times, once for each conﬁ guration—and you don’t risk \nsett ing things up incorrectly in one of the conﬁ gurations by accident. How-\never, be aware that some sett ings need to be diﬀ erent between the debug and \nrelease conﬁ gurations. For example, function inlining and code optimization \nsett ings should of course be diﬀ erent between debug and release builds.\nGeneral Tab\nOn the General tab, shown in Figure 2.10, the most useful ﬁ elds are the fol-\nlowing.\nz Output directory. This deﬁ nes where the ﬁ nal product(s) of the build will \ngo—namely the executable, library, or DLL that the compiler/linker ul-\ntimately outputs.\n",
      "content_length": 2330,
      "extraction_method": "Direct"
    },
    {
      "page_number": 97,
      "chapter": null,
      "content": "75 \nz Intermediate directory. This deﬁ nes where intermediate ﬁ les, primarily \nobject ﬁ les (.obj extension), are placed during a build. Intermediate ﬁ les \nare never shipped with your ﬁ nal program—they are only required \nduring the process of building your executable, library, or DLL. Hence, \nit is a good idea to place intermediate ﬁ les in a diﬀ erent directory than \nthe ﬁ nal products (.exe, .lib or .dll ﬁ les).\nNote that VisualStudio provides a macro facility which may be used \nwhen specifying directories and other sett ings in the “Project Property Pages” \ndialog. A macro is essentially a named variable that contains a global value and \nthat can be referred to in your project conﬁ guration sett ings.\nMacros are invoked by writing the name of the macro enclosed in paren-\ntheses and preﬁ xed with a dollar sign (e.g., $(ConfigurationName)). Some \ncommonly used macros are listed below.\nz $(TargetFileName). The name of the ﬁ nal executable, library, or DLL \nﬁ le being built by this project.\nz $(TargetPath). The full path of the folder containing the ﬁ nal execut-\nable, library, or DLL.\nz $(ConfigurationName). The name of the build conﬁ g, typically “De-\nbug” or “Release.”\n2.2. Microsoft Visual Studio\nFigure 2.10.  Visual Studio project property pages—General page.\n",
      "content_length": 1287,
      "extraction_method": "Direct"
    },
    {
      "page_number": 98,
      "chapter": null,
      "content": "76 \n2. Tools of the Trade\nz $(OutDir). The value of the “Output Directory” ﬁ eld speciﬁ ed in this \ndialog.\nz $(IntDir). The value of the “Intermediate Directory” ﬁ eld in this \ndialog.\nz $(VCInstallDir). The directory in which Visual Studio’s standard C \nlibrary is currently installed.\nThe beneﬁ t of using macros instead of hard-wiring your conﬁ guration \nsett ings is that a simple change of the global macro’s value will automatically \naﬀ ect all conﬁ guration sett ings in which the macro is used. Also, some macros \nlike $(ConfigurationName) automatically change their values depending \non the build conﬁ guration, so using them can permit you to use identical set-\ntings across all your conﬁ gurations.\nTo see a complete list of all available macros, click in either the “Output \nDirectory” ﬁ eld or the “Intermediate Directory” ﬁ eld on the “General” tab, \nclick the litt le arrow to the right of the text ﬁ eld, select “Edit…” and then click \nthe “Macros” butt on in the dialog that comes up.\nDebugging Tab\nThe “Debugging” tab is where the name and location of the executable to \ndebug is speciﬁ ed. On this page, you can also specify the command-line \nargument(s) that should be passed to the program when it runs.  We’ll discuss \ndebugging your program in more depth below.\nC/C++ Tab\nThe C/C++ tab controls compile-time language sett ings—things that aﬀ ect \nhow your source ﬁ les will be compiled into object ﬁ les (.obj extension). The \nsett ings on this page do not aﬀ ect how your object ﬁ les are linked into a ﬁ nal \nexecutable or DLL.\nYou are encouraged to explore the various subpages of the C/C++ tab to \nsee what kinds of sett ings are available. Some of the most commonly used set-\ntings include the following.\nz General Tab/Include Directories. This ﬁ eld lists the on-disk directories that \nwill be searched when looking for #included header ﬁ les.\nImportant: It is always best to specify these directories using relative \npaths and/or with Visual Studio macros like $(OutDir) or $(IntDir). \nThat way, if you move your build tree to a diﬀ erent location on disk or to \nanother computer with a diﬀ erent root folder, everything will continue \nto work properly.\nz General Tab/Debug Information. This ﬁ eld controls whether or not debug \ninformation is generated. Typically both debug and release conﬁ gura-\n",
      "content_length": 2330,
      "extraction_method": "Direct"
    },
    {
      "page_number": 99,
      "chapter": null,
      "content": "77 \ntions include debugging information so that you can track down prob-\nlems during development of your game. The ﬁ nal production build will \nhave all the debug info stripped out to prevent hacking.\nz Preprocessor Tab/Preprocessor Deﬁ nitions. This very handy ﬁ eld lists any \nnumber of C/C++ preprocessor symbols that should be deﬁ ned in the \ncode when it is compiled. See Preprocessor Sett ings in Section 2.2.4.1 for a \ndiscussion of preprocessor-deﬁ ned symbols.\nLinker Tab\nThe “Linker” tab lists properties that aﬀ ect how your object code ﬁ les will be \nlinked into an executable or DLL. Again, you are encouraged to explore the \nvarious subpages. Some commonly used sett ings follow.\nz General Tab/Output File. This sett ing lists the name and location of the \nﬁ nal product of the build, usually an executable or DLL.\nz General Tab/Additional Library Directories. Much like the C/C++ Include \nDirectories ﬁ eld, this ﬁ eld lists zero or more directories that will be \nsearched when looking for libraries and object ﬁ les to link into the ﬁ nal \nexecutable.\nz Input Tab/Additional Dependencies. This ﬁ eld lists external libraries that you \nwant linked into your executable or DLL. For example, the Ogre libraries \nwould be listed here if you are building an Ogre-enabled application.\nNote that Visual Studio employs various “magic spells” to specify librar-\nies that should be linked into an executable. For example, a special #pragma\ninstruction in your source code can be used to instruct the linker to automati-\ncally link with a particular library.  For this reason, you may not see all of the \nlibraries you’re actually linking to in the “Additional Dependencies” ﬁ eld. (In \nfact, that’s why they are called additional dependencies.) You may have noticed, \nfor example, that Direct X applications do not list all of the DirectX libraries \nmanually in their “Additional Dependencies” ﬁ eld. Now you know why.\n2.2.4.4. Creating New .vcproj Files\nWith so many preprocessor, compiler, and linker options, all of which must \nbe set properly, creating a new project may seem like an impossibly daunting \ntask. I usually take one of the following two approaches when creating a new \nVisual Studio project.\nUse a Wizard\nVisual Studio provides a wide variety of wizards to create new projects of \nvarious kinds. If you can ﬁ nd a wizard that does what you want, this is the \neasiest way to create a new project.\n2.2. Microsoft Visual Studio\n",
      "content_length": 2449,
      "extraction_method": "Direct"
    },
    {
      "page_number": 100,
      "chapter": null,
      "content": "78 \n2. Tools of the Trade\nCopy an Existing Project\nIf I am creating a project that is similar to an existing project that I know al-\nready works, I’ll oft en just copy that .vcproj ﬁ le and then modify it as neces-\nsary. In Visual Studio 2005, this is very easy. You simply copy the .vcproj ﬁ le \non disk, then add the newly copied project to your solution by right-clicking \nthe solution in the Solution Explorer and selecting “Add…” and “Existing \nproject…” from the pop-up menus.\nOne caveat when copying project ﬁ les is that the name of the project is \nstored inside the .vcproj ﬁ le itself. So when you load up the new project for the \nﬁ rst time in Visual Studio 2005, it will still have the original name. To rectify \nthis, you can simply select the project in the Solution Explorer window, and \nhit F2 to rename it appropriately.\nAnother problem arises when the name of the executable, library, or DLL \nthat the project creates is speciﬁ ed explicitly in the .vcproj ﬁ le. For example, \nthe executable might be speciﬁ ed as “C:\\MyGame\\bin\\MyGame.exe” or \n“$(OutDir)\\MyGame.exe.” In this case, you’ll need to open the .vcproj ﬁ le \nand do a global search-and-replace of the executable, library, or DLL name \nand/or its directory path. This is not too diﬃ  cult. Project ﬁ les are XML, so you \ncan rename your copied .vcproj ﬁ le to have an “.xml” extension and then open \nit in Visual Studio (or any other XML or raw text editor). One elegant solution \nto this problem is to use Visual Studio’s macro system when specifying all out-\nput ﬁ les in your project. For example, if you specify the output executable as \n“$(OutDir)\\$(ProjectName).exe”, then the project’s name will automati-\ncally be reﬂ ected in the name of the output executable ﬁ le.\nI should mention that using a text editor to manipulate .vcproj ﬁ les is not \nalways to be avoided.  In fact, the practice is quite common, at least in my ex-\nperience. For example, let’s say you decided to move the folder containing all \nof your graphics header ﬁ les to a new path on disk. Rather than manually open \neach project in turn, open the Project Property Pages window, navigate to the \nC/C++ tab, and ﬁ nally update the include path manually, it’s much easier and \nless error-prone to edit the ﬁ les as XML text and do a search-and-replace. You \ncan even do a “Replace in ﬁ les” operation in Visual Studio for mass edits.\n2.2.5. Debugging Your Code\nOne of the most important skills any programmer can learn is how to eﬀ ec-\ntively debug code. This section provides some useful debugging tips and \ntricks. Some are applicable to any debugger and some are speciﬁ c to Microsoft  \nVisual Studio. However, you can usually ﬁ nd an equivalent to Visual Studio’s \ndebugging features in other debuggers, so this section should prove useful \neven if you don’t use Visual Studio to debug your code.\n",
      "content_length": 2855,
      "extraction_method": "Direct"
    },
    {
      "page_number": 101,
      "chapter": null,
      "content": "79 \n2.2.5.1. The Start-Up Project\nA Visual Studio solution can contain more than one project. Some of these \nprojects build executables, while others build libraries or DLLs. It’s possible \nto have more than one project that builds an executable in a single solution. \nHowever, you cannot debug more than one program at a time. For this reason, \nVisual Studio provides a sett ing known as the “Start-Up Project.”  This is the \nproject that is considered “current” for the purposes of the debugger.\nThe start-up project is highlighted in bold in the Solution Explorer. \nHitt ing F5 to run your program in the debugger will run the .exe built by the \nstart-up project (if the start-up project builds an executable).\n2.2.5.2. Break Points\nBreak points are the bread and butt er of code debugging. A break point in-\nstructs the program to stop at a particular line in your source code so that you \ncan inspect what’s going on.\nIn Visual Studio, select a line and hit F9 to toggle a break point. When you \nrun your program and the line of code containing the break point is about to \nbe executed, the debugger will stop the program. We say that the break point \nhas been “hit.” A litt le arrow will show you which line of code the CPU’s pro-\ngram counter is currently on. This is shown in Figure 2.11.\n2.2. Microsoft Visual Studio\nFigure 2.11.  Setting a break point in Visual Studio.\n2.2.5.3. Stepping through Your Code\nOnce a break point has been hit, you can  single-step your code by hitt ing the \nF10 key. The yellow program-counter arrow moves to show you the lines as \nthey execute. Hitt ing F11 steps into a function call (i.e., the next line of code \nyou’ll see is the ﬁ rst line of the called function), while F10 steps over that func-\n",
      "content_length": 1741,
      "extraction_method": "Direct"
    },
    {
      "page_number": 102,
      "chapter": null,
      "content": "80 \n2. Tools of the Trade\ntion call (i.e., the debugger calls the function at full speed and then breaks \nagain on the line right aft er the call).\n2.2.5.4. The  Call Stack\nThe call stack window, shown in Figure 2.12, shows you the stack of functions \nthat have been called at any given moment during the execution of your code. \nTo display the call stack (if it is not already visible), go to the “Debug” menu \non the main menu bar, select “Windows,” and then “Call Stack.”\nOnce a break point has been hit (or the program is manually paused), you \ncan move up and down the call stack by double-clicking on entries in the “Call \nStack” window. This is very useful for inspecting the chain of function calls \nthat were made between main() and the current line of code. For example, \nyou might trace back to the root cause of a bug in a parent function which has \nmanifested itself in a deeply nested child function.\nFigure 2.12.  The call stack window.\n2.2.5.5. The  Watch Window\nAs you step through your code and move up and down the call stack, you will \nwant to be able to inspect the values of the variables in your program. This \nis what watch windows are for. To open a watch window, go to the “Debug” \nmenu, select “Windows…,” then select “Watch…,” and ﬁ nally select one of \n“Watch 1” through “Watch 4.” (Visual Studio allows you to open up to four \nwatch windows simultaneously.) Once a watch window is open, you can type \nthe names of variables into the window or drag expressions in directly from \nyour source code.\nAs you can see in Figure 2.13, variables with simple data types are shown \nwith their values listed immediately to the right of their names. Complex \ndata types are shown as litt le tree views that can be easily expanded to “drill \n",
      "content_length": 1758,
      "extraction_method": "Direct"
    },
    {
      "page_number": 103,
      "chapter": null,
      "content": "81 \ndown” into virtually any nested structure. The base class of a class is always \nshown as the ﬁ rst child of an instance of a derived class. This allows you to \ninspect not only the class’ data members, but also the data members of its base \nclass(es).\nYou can type virtually any valid C/C++ expression into the watch window, \nand Visual Studio will evaluate that expression and att empt to display the \nresulting value. For example, you could type “5+3” and Visual Studio will \ndisplay “8.” You can cast variables from one type to another by using C or C++ \ncasting syntax. For example, typing “(float)myIntegerVariable * 0.5f” \nin the watch window will display the value of myIntegerVariable divided \nby two, as a ﬂ oating-point value.\nYou can even call functions in your program from within the watch window. \nVisual Studio re-evaluates the expressions typed into the watch window(s) \nautomatically, so if you invoke a function in the watch window, it will be \ncalled every time you hit a break point or single-step your code. This allows \nyou to leverage the functionality of your program in order to save yourself \nwork when trying to interpret the data that you’re inspecting in the debug-\nger. For example, let’s say that your game engine provides a function called \nquatToAngleDeg() which converts a quaternion to an angle of rotation in \ndegrees. You can call this function in the watch window in order to easily in-\nspect the rotation angle of any quaternion from within the debugger.\nYou can also use various suﬃ  xes on the expressions in the watch window \nin order to change the way Visual Studio displays the data, as shown in Fig-\nure 2.14.\nz The “,d” suﬃ  x forces values to be displayed in decimal notation.\nz The “,x” suﬃ  x forces values to be displayed in hexadecimal notation.\n2.2. Microsoft Visual Studio\nFigure 2.13.  Visual Studio’s watch window.\n",
      "content_length": 1874,
      "extraction_method": "Direct"
    },
    {
      "page_number": 104,
      "chapter": null,
      "content": "82 \n2. Tools of the Trade\nz The “,n” suﬃ  x (where n is any positive integer) forces Visual Studio to \ntreat the value as an array with n elements. This allows you to expand \narray data that is referenced through a pointer.\nBe careful when expanding very large data structures in the watch window, be-\ncause it can sometimes slow the debugger down to the point of being unusable.\n2.2.5.6.  Data Break Points\nRegular break points trip when the CPU’s program counter hits a particular \nmachine instruction or line of code. However, another incredibly useful fea-\nture of modern debuggers is the ability to set a break point that trips when-\never a speciﬁ c memory address is writt en to (i.e., changed). These are called \ndata break points, because they are triggered by changes to data, or sometimes \nhardware break points, because they are implemented via a special feature of the \nCPU’s hardware—namely the ability to raise an interrupt when a predeﬁ ned \nmemory address is writt en to.\nHere’s how data break points are typically used. Let’s say you are tracking \ndown a bug that manifests itself as a zero (0.0f) value mysteriously appear-\ning inside a member variable of a particular object called m_angle that should \nalways contain a nonzero angle. You have no idea which function might be \nwriting that zero into your variable. However, you do know the address of the \nvariable. (You can just type “&object.m_angle” into the watch window to \nﬁ nd its address.) To track down the culprit, you can set a data break point on \nthe address of object.m_angle, and then simply let the program run. When \nthe value changes, the debugger will stop automatically. You can then inspect \nthe call stack to catch the oﬀ ending function red-handed.\nTo set a data break point in Visual Studio, take the following steps.\nz Bring up the “Breakpoints” window found on the “Debug” menu under \n“Windows” and then “Breakpoints” (Figure 2.15).\nz Select the “New” drop-down butt on in the upper-left  corner of the win-\ndow.\nFigure 2.14.  Comma sufﬁ xes in the Visual Studio watch window.\n",
      "content_length": 2072,
      "extraction_method": "Direct"
    },
    {
      "page_number": 105,
      "chapter": null,
      "content": "83 \nz Select “New Data Breakpoint.”\nz Type in the raw address or an address-valued expression, such as \n“&myVariable” (Figure 2.16).\nThe “Byte count” ﬁ eld should almost always contain the value 4. This is \nbecause 32-bit Pentium CPUs can really only inspect 4-byte (32-bit) values na-\ntively. Specifying any other data size requires the debugger to do some trickery \nwhich tends to slow your program’s execution to a crawl (if it works at all).\n2.2.5.7. Conditional Break Points\nYou’ll also notice in the “Break Points” window that you can set conditions \nand hit counts on any type break point—data break points or regular line-of-\ncode break points.\nA  conditional break point causes the debugger to evaluate the C/C++ expres-\nsion you provide every time the break point is hit. If the expression is true, the \ndebugger stops your program and gives you a chance to see what’s going on. \nIf the expression is false, the break point is ignored and the program contin-\nues. This is very useful for sett ing break points that only trip when a function \nis called on a particular instance of a class. For example, let’s say you have \na game level with 20 tanks on-screen, and you want to stop your program \nFigure 2.16.  Deﬁ ning a data break point.\nFigure 2.15.  The Visual Studio break points window.\n2.2. Microsoft Visual Studio\n",
      "content_length": 1330,
      "extraction_method": "Direct"
    },
    {
      "page_number": 106,
      "chapter": null,
      "content": "84 \n2. Tools of the Trade\nwhen the third tank, whose memory address you know to be 0x12345678, \nis running. By sett ing the break point’s condition express to something like \n“(unsigned)this == 0x12345678”, you can restrict the break point only to \nthe class instance whose memory address (this pointer) is 0x12345678.\nSpecifying a  hit count for a break point causes the debugger to decrement \na counter every time the break point is hit, and only actually stop the program \nwhen that counter reaches zero. This is really useful for situations where your \nbreak point is inside a loop, and you need to inspect what’s happening during \nthe 376th iteration of the loop (e.g., the 376th element in an array). You can’t \nvery well sit there and hit the F5 key 375 times! But you can let the hit count \nfeature of Visual Studio do it for you.\nOne note of caution: Conditional break points cause the debugger to eval-\nuate the conditional expression every time the break point is hit, so they can \nbog down the performance of the debugger and your game.\n2.2.5.8.  Debugging Optimized Builds\nI mentioned above that it can be very tricky to debug problems using a release \nbuild, due primarily to the way the compiler optimizes the code. Ideally, every \nprogrammer would prefer to do all of his or her debugging in a debug build. \nHowever, this is oft en not possible. Sometimes a bug occurs so rarely that \nyou’ll jump at any chance to debug the problem, even if it occurs in a release \nbuild on someone else’s machine. Other bugs only occur in your release build, \nbut magically disappear whenever you run the debug build. These dreaded \nrelease-only bugs are sometimes caused by uninitialized variables, because vari-\nables and dynamically allocated memory blocks are oft en set to zero in debug \nmode, but are left  containing garbage in a release build. Other common causes \nof release-only bugs include code that has been accidentally omitt ed from the \nrelease build (e.g., when important code is erroneously placed inside an asser-\ntion statement), data structures whose size or data member packing changes \nbetween debug and release builds, bugs that are only triggered by inlining or \ncompiler-introduced optimizations, and (in rare cases) bugs in the compiler’s \noptimizer itself, causing it to emit incorrect code in a fully optimized build.\nClearly, it behooves every programmer to be capable of debugging prob-\nlems in a release build, unpleasant as it may seem. The best ways to reduce the \npain of debugging optimized code is to practice doing it and to expand your \nskill set in this area whenever you have the opportunity. Here are a few tips.\nz Learn to read and step through  disassembly in the debugger. In a release build, \nthe debugger oft en has trouble keeping track of which line of source \ncode is currently being executed. Thanks to instruction reordering, \nyou’ll oft en see the program counter jump around erratically within the \n",
      "content_length": 2953,
      "extraction_method": "Direct"
    },
    {
      "page_number": 107,
      "chapter": null,
      "content": "85 \nfunction when viewed in source code mode. However, things become \nsane again when you work with the code in disassembly mode (i.e., step \nthrough the assembly language instructions individually). Every C/C++ \nprogrammer should be at least a litt le bit familiar with the architecture \nand assembly language of their target CPU(s). That way, even if the de-\nbugger is confused, you won’t be.\nz Use registers to deduce variables’ values or addresses. The debugger will \nsometimes be unable to display the value of a variable or the contents of \nan object in a release build. However, if the program counter is not too \nfar away from the initial use of the variable, there’s a good chance its ad-\ndress or value is still stored in one of the CPU’s registers. If you can trace \nback through the disassembly to where the variable is ﬁ rst loaded into \na register, you can oft en discover its value or its address by inspecting \nthat register. Use the register window, or type the name of the register \ninto a watch window, to see its contents.\nz Inspect variables and object contents by address. Given the address of a vari-\nable or data structure, you can usually see its contents by casting the \naddress to the appropriate type in a watch window. For example, if we \nknow that an instance of the Foo class resides at address 0x1378A0C0, we \ncan type “(Foo*)0x1378A0C0” in a watch window, and the debugger \nwill interpret that memory address as if it were a pointer to a Foo object.\nz Leverage static and global variables. Even in an optimized build, the de-\nbugger can usually inspect global and static variables. If you cannot de-\nduce the address of a variable or object, keep your eye open for a static \nor global that might contain its address, either directly or indirectly. For \nexample, if we want to ﬁ nd the address of an internal object within the \nphysics system, we might discover that it is in fact stored in a member \nvariable of the global PhysicsWorld object.\nz Modify the code. If you can reproduce a release-only bug relatively eas-\nily, consider modifying the source code to help you debug the problem. \nAdd print statements so you can see what’s going on. Introduce a global \nvariable to make it easier to inspect a problematic variable or object in \nthe debugger. Add code to detect a problem condition or to isolate a \nparticular instance of a class.\n2.3. Proﬁ ling Tools\nGames are typically high-performance real-time programs. As such, game en-\ngine programmers are always looking for ways to speed up their code. There \n2.3. Proﬁ ling Tools\n",
      "content_length": 2567,
      "extraction_method": "Direct"
    },
    {
      "page_number": 108,
      "chapter": null,
      "content": "86 \n2. Tools of the Trade\nis a well-known, albeit rather unscientiﬁ c, rule of thumb known as the  Pareto \nprinciple (see htt p://en.wikipedia.org/wiki/Pareto_principle). It is also known \nas the  80-20 rule, because it states that in many situations, 80% of the eﬀ ects \nof some event come from only 20% of the possible causes. In computer sci-\nence, we oft en use a variant of this principle known as the 90-10 rule, which \nstates that 90% of the wall clock time spent running any piece of soft ware is \naccounted for by only 10% of the code. In other words, if you optimize 10% of \nyour code, you can potentially realize 90% of all the gains in execution speed \nyou’ll ever realize.\nSo, how do you know which 10% of your code to optimize? For that, you \nneed a  proﬁ ler.  A proﬁ ler is a tool that measures the execution time of your \ncode. It can tell you how much time is spent in each function. You can then di-\nrect your optimizations toward only those functions that account for the lion’s \nshare of the execution time.\nSome proﬁ lers also tell you how many times each function is called. This \nis an important dimension to understand. A function can eat up time for two \nreasons: (a) it takes a long time to execute on its own, or (b) it is called fre-\nquently. For example, a function that runs an A* algorithm to compute the \noptimal paths through the game world might only be called a few times each \nframe, but the function itself may take a signiﬁ cant amount of time to run.  On \nthe other hand, a function that computes the dot product may only take a few \ncycles to execute, but if you call it hundreds of thousands of times per frame, \nit might drag down your game’s frame rate.\nEven more information can be obtained, if you use the right proﬁ ler. Some \nproﬁ lers report the call graph, meaning that for any given function, you can \nsee which functions called it (these are known as parent functions) and which \nfunctions it called (these are known as child functions or descendants). You can \neven see what percentage of the function’s time was spent calling each of its \ndescendants and the percentage of the overall running time accounted for by \neach individual function.\nProﬁ lers fall into two broad categories.\n1.  Statistical proﬁ lers. This kind of proﬁ ler is designed to be unobtrusive, \nmeaning that the target code runs at almost the same speed, wheth-\ner or not proﬁ ling is enabled. These proﬁ lers work by sampling the \nCPU’s program counter register periodically and noting which func-\ntion is currently running. The number of samples taken within each \nfunction yields an approximate percentage of the total running time \nthat is eaten up by that function. Intel’s VTune is the gold standard in \nstatistical proﬁ lers for Windows machines employing Intel Pentium \nprocessors, and it is now also available for Linux.  See htt p://www.\n",
      "content_length": 2872,
      "extraction_method": "Direct"
    },
    {
      "page_number": 109,
      "chapter": null,
      "content": "87 \nintel.com/cd/soft ware/products/ asmo-na /eng /vtune /239144.htm \nfor \ndetails.\n2.  Instrumenting proﬁ lers. This kind of proﬁ ler is aimed at providing the \nmost accurate and comprehensive timing data possible, but at the ex-\npense of real-time execution of the target program—when proﬁ ling is \nturned on, the target program usually slows to a crawl. These proﬁ lers \nwork by preprocessing your executable and inserting special prologue \nand epilogue code into every function. The prologue and epilogue code \ncalls into a proﬁ ling library, which in turn inspects the program’s call \nstack and records all sorts of details, including which parent function \ncalled the function in question and how many times that parent has \ncalled the child. This kind of proﬁ ler can even be set up to monitor every \nline of code in your source program, allowing it to report how long each \nline is taking to execute. The results are stunningly accurate and com-\nprehensive, but turning on proﬁ ling can make a game virtually unplay-\nable. IBM’s Rational Quantify, available as part of the Rational Purify \nPlus tool suite, is an excellent instrumenting proﬁ ler.  See htt p://www.\nibm.com/developerworks/rational/library/957.html for an introduction \nto proﬁ ling with Quantify.\nMicrosoft  has also published a proﬁ ler that is a hybrid between the two \napproaches.  It is called LOP, which stands for low-overhead proﬁ ler. It uses \na statistical approach, sampling the state of the processor periodically, which \nmeans it has a low impact on the speed of the program’s execution. However, \nwith each sample it analyzes the call stack, thereby determining the chain of \nparent functions that resulted in each sample. This allows LOP to provide \ninformation normally not available with a statistical proﬁ ler, such as the dis-\ntribution of calls across parent functions.\n2.3.1. \nList of Proﬁ lers\nThere are a great many proﬁ ling tools available. See htt p://en.wikipedia.org/\nwiki/List_of_performance_analysis_tool for a reasonably comprehensive list.\n2.4. Memory Leak and Corruption Detection\nTwo other problems that plague C and C++ programmers are  memory leaks \nand  memory corruption. A memory leak occurs when memory is allocated \nbut never freed. This wastes memory and eventually leads to a potentially \nfatal out-of-memory condition. Memory corruption occurs when the program \ninadvertently writes data to the wrong memory location, overwriting the im-\n2.4. Memory Leak and Corruption Detection\n",
      "content_length": 2497,
      "extraction_method": "Direct"
    },
    {
      "page_number": 110,
      "chapter": null,
      "content": "88 \n2. Tools of the Trade\nportant data that was there—while simultaneously failing to update the mem-\nory location where that data should have been writt en. Blame for both of these \nproblems falls squarely on the language feature known as the pointer.\nA pointer is a powerful tool. It can be an agent of good when used prop-\nerly—but it can also be all-too-easily transformed into an agent of evil. If a \npointer points to memory that has been freed, or if it is accidentally assigned \na nonzero integer or ﬂ oating-point value, it becomes a dangerous tool for cor-\nrupting memory, because data writt en through it can quite literally end up \nanywhere. Likewise, when pointers are used to keep track of allocated mem-\nory, it is all too easy to forget to free the memory when it is no longer needed. \nThis leads to memory leaks.\nClearly good coding practices are one approach to avoiding pointer-re-\nlated memory problems. And it is certainly possible to write solid code that \nessentially never corrupts or leaks memory. Nonetheless, having a tool to help \nyou detect potential memory corruption and leak problems certainly can’t \nhurt. Thankfully, many such tools exist.\nMy personal favorite is IBM’s  Rational Purify, which comes as part of the \nPurify Plus tool kit. Purify instruments your code prior to running it, in order \nto hook into all pointer dereferences and all memory allocations and dealloca-\ntions made by your code. When you run your code under Purify, you get a \nlive report of the problems—real and potential—encountered by your code. \nAnd when the program exits, you get a detailed memory leak report. Each \nproblem is linked directly to the source code that caused the problem, making \ntracking down and ﬁ xing these kinds of problems relatively easy. You can ﬁ nd \nmore information on Purify at htt p://www-306.ibm.com/soft ware/awdtools\n/purify.\nAnother popular tool is Bounds Checker by CompuWare. It is similar \nto Purify in purpose and functionality. You can ﬁ nd more information on \nBounds Checker at htt p://www.compuware.com/products/devpartner/visualc\n.htm.\n2.5. Other Tools\nThere are a number of other commonly used tools in a game programmer’s \ntoolkit. We won’t cover them in any depth here, but the following list will \nmake you aware of their existence and point you in the right direction if you \nwant to learn more.\nz Diﬀ erence tools. A diﬀ erence tool, or diﬀ  tool, is a program that com-\npares two versions of a text ﬁ le and determines what has changed be-\n",
      "content_length": 2503,
      "extraction_method": "Direct"
    },
    {
      "page_number": 111,
      "chapter": null,
      "content": "89 \ntween them. (See htt p://en.wikipedia.org/wiki/Diﬀ  for a discussion of \ndiﬀ  tools.) Diﬀ s are usually calculated on a line-by-line basis, although \nmodern diﬀ  tools can also show you a range of characters on a changed \nline that have been modiﬁ ed. Most version control systems come with \na diﬀ  tool. Some programmers like a particular diﬀ  tool and conﬁ gure \ntheir version control soft ware to use the tool of their choice. Popular \ntools include ExamDiﬀ  (htt p://www.prestosoft .com/edp_examdiﬀ .asp), \nAraxisMerge (htt p://www.araxis.com), WinDiﬀ  (available in the Op-\ntions Packs for most Windows versions and available from many inde-\npendent websites as well), and the GNU diﬀ  tools package (htt p://www.\ngnu.org/soft ware/diﬀ utils/diﬀ utils.html).\nz  Three-way merge tools. When two people edit the same ﬁ le, two inde-\npendent sets of diﬀ s are generated. A tool that can merge two sets of \ndiﬀ s into a ﬁ nal version of the ﬁ le that contains both person’s changes \nis called a three-way merge tool. The name “three-way” refers to the \nfact that three versions of the ﬁ le are involved: the original, user A’s \nversion, and user B’s version. (See htt p://en.wikipedia.org/wiki/3-way_\nmerge#Three-way_merge for a discussion of two-way and three-way \nmerge technologies.) Many merge tools come with an associated diﬀ  \ntool. Some popular merge tools include AraxisMerge (htt p://www.arax-\nis.com) and WinMerge (htt p://winmerge.org). Perforce also comes with \nan excellent three-way merge tool (htt p://www.perforce.com/perforce/\nproducts/merge.html).\nz Hex editors. A  hex editor is a program used for inspecting and modify-\ning the contents of binary ﬁ les. The data are usually displayed as in-\ntegers in hexadecimal format, hence the name. Most good hex editors \ncan display data as integers from one byte to 16 bytes each, in 32- and \n64-bit ﬂ oating point format and as ASCII text. Hex editors are particu-\nlarly useful when tracking down problems with binary ﬁ le formats or \nwhen reverse-engineering an unknown binary format—both of which \nare relatively common endeavors in game engine development circles. \nThere are quite literally a million diﬀ erent hex editors out there; I’ve \nhad good luck with HexEdit by Expert Commercial Soft ware (htt p://\nwww.expertcomsoft .com/index.html), but your mileage may vary.\nAs a game engine programmer you will undoubtedly come across other \ntools that make your life easier, but I hope this chapter has covered the main \ntools you’ll use on a day-to-day basis.\n2.5. Other Tools\n",
      "content_length": 2548,
      "extraction_method": "Direct"
    },
    {
      "page_number": 112,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 113,
      "chapter": null,
      "content": "91\n3\nFundamentals of Software \nEngineering for Games\nI\nn this chapter, we’ll brieﬂ y review the basic concepts of object-oriented pro-\ngramming and then delve into some advanced topics which should prove \ninvaluable in any soft ware engineering endeavor (and especially when creat-\ning games). As with Chapter 2, I hope you will not to skip this chapter en-\ntirely; it’s important that we all embark on our journey with the same set of \ntools and supplies.\n3.1. \nC++ Review and Best Practices\n3.1.1. \nBrief Review of Object-Oriented Programming\nMuch of what we’ll discuss in this book assumes you have a solid understand-\ning of the principles of object-oriented design. If you’re a bit rusty, the follow-\ning section should serve as a pleasant and quick review. If you have no idea \nwhat I’m talking about in this section, I recommend you pick up a book or two \non  object-oriented programming (e.g., [5]) and C++ in particular (e.g., [39] and \n[31]) before continuing.\n3.1.1.1. \nClasses and Objects\nA  class is a collection of att ributes (data) and behaviors (code) which together \nform a useful, meaningful whole. A class is a speciﬁ cation describing how in-\n",
      "content_length": 1164,
      "extraction_method": "Direct"
    },
    {
      "page_number": 114,
      "chapter": null,
      "content": "92 \n3. Fundamentals of Software Engineering for Games\ndividual instances of the class, known as objects, should be constructed. For \nexample, your pet Rover is an instance of the class “dog.” Thus there is a one-\nto-many relationship between a class  and its instances.\n3.1.1.2. \n Encapsulation\nEncapsulation means that an object presents only a limited interface to the out-\nside world; the object’s internal state and implementation details are kept hid-\nden. Encapsulation simpliﬁ es life for the user of the class, because he or she \nneed only understand the class’ limited interface, not the potentially intricate \ndetails of its implementation. It also allows the programmer who wrote the \nclass to ensure that its instances are always in a logically consistent state.\n3.1.1.3. \n Inheritance\nInheritance allows new classes to be deﬁ ned as extensions to pre-existing class-\nes. The new class modiﬁ es or extends the data, interface, and/or behavior of \nthe existing class. If class Child extends class Parent, we say that Child in-\nherits from or is derived from Parent. In this relationship, the class Parent is \nknown as the base class or superclass, and the class Child is the derived class \nor subclass. Clearly, inheritance leads to hierarchical (tree-structured) relation-\nships between classes.\nInheritance creates an “is-a” relationship between classes. For example, \na circle is a type of shape. So if we were writing a 2D drawing application, \nit would probably make sense to derive our Circle class from a base class \ncalled Shape.\nWe can draw diagrams of class hierarchies using the conventions deﬁ ned \nby the  Uniﬁ ed Modeling Language (UML). In this notation, a rectangle repre-\nsents a class, and an arrow with a hollow triangular head represents inheritance. \nThe inheritance arrow points from child class to parent. See Figure 3.1 for an ex-\nample of a simple class hierarchy represented as a UML  static class diagram.\nShape\nCircle\nRectangle\nTriangle\nFigure 3.1.  UML static class diagram depicting a simple class hierarchy.\n Multiple In heritance\nSome languages support multiple inheritance (MI), meaning that a class can \nhave more than one parent class. In theory MI can be quite elegant, but in \n",
      "content_length": 2225,
      "extraction_method": "Direct"
    },
    {
      "page_number": 115,
      "chapter": null,
      "content": "93 \n3.1. C++ Review and Best Practices\npractice this kind of design usually gives rise to a lot of confusion and techni-\ncal diﬃ  culties (see htt p://en.wikipedia.org/wiki/Multiple_inheritance). This is \nbecause multiple inheritance transforms a simple tree of classes into a poten-\ntially complex graph. A class graph can have all sorts of problems that never \nplague a simple tree—for example, the  deadly diamond (htt p://en.wikipedia.\norg/wiki/Diamond_problem), in which a derived class ends up containing two \ncopies of a grandparent base class (see Figure 3.2). In C++,  virtual inheritance al-\nlows one to avoid this doubling of the grandparent’s data.\nMost C++ soft ware developers avoid multiple inheritance completely or \nonly permit it in a limited form. A common rule of thumb is to allow only \nsimple, parentless classes to be multiply inherited into an otherwise strictly \nsingle-inheritance hierarchy. Such classes are sometimes called mix-in classes \nClassA\nClassB\nClassC\nClassD\nClassA\nClassA\nClassB\nClassB’s \nmemory layout:\nClassA’s \nmemory layout:\nClassA\nClassC\nClassC’s \nmemory layout:\nClassA\nClassB\nClassD’s \nmemory layout:\nClassA\nClassC\nClassD\nFigure 3.2.  “Deadly diamond” in a multiple inheritance hierarchy.\n+Draw()\nShape\n+Draw()\nCircle\n+Draw()\nRectangle\n+Draw()\nTriangle\n+Animate()\nAnimator\nAnimator is a hypothetical mix-in\nclass that adds animation\nfunctionality to whatever class it\nis inherited by.\nFigure 3.3.  Example of a mix-in class.\n",
      "content_length": 1469,
      "extraction_method": "Direct"
    },
    {
      "page_number": 116,
      "chapter": null,
      "content": "94 \n3. Fundamentals of Software Engineering for Games\nbecause they can be used to introduce new functionality at arbitrary points in a \nclass tree. See Figure 3.3 for a somewhat contrived example of a  mix-in class.\n3.1.1.4. \n Polymorphism\nPolymorphism is a language feature that allows a collection of objects of diﬀ er-\nent types to be manipulated through a single common interface. The common \ninterface makes a heterogeneous collection of objects appear to be homoge-\nneous, from the point of view of the code using the interface.\nFor example, a 2D painting program might be given a list of various \nshapes to draw on-screen. One way to draw this heterogeneous collection of \nshapes is to use a switch statement to perform diﬀ erent drawing commands \nfor each distinct type of shape.\nvoid drawShapes(std::list<Shape*> shapes)\n{\n \nstd::list<Shape*>::iterator pShape = shapes.begin();\n \nstd::list<Shape*>::iterator pEnd = shapes.end();\n \nfor ( ; pShape != pEnd; ++pShape)\n {\nswitch (pShape->mType)\n  {\n  case \nCIRCLE:\n \n \n \n// draw shape as a circle\n   break;\n  case \nRECTANGLE:\n \n \n \n// draw shape as a rectangle\n   break;\n  case \nTRIANGLE:\n \n \n \n// draw shape as a triangle\n   break;\n  //...\n  }\n }\n}\nThe problem with this approach is that the drawShapes() function needs \nto “know” about all of the kinds of shapes that can be drawn. This is ﬁ ne in a \nsimple example, but as our code grows in size and complexity, it can become \ndiﬃ  cult to add new types of shapes to the system. Whenever a new shape \ntype is added, one must ﬁ nd every place in the code base where knowledge \nof the set of shape types is embedded—like this switch statement—and add a \ncase to handle the new type.\nThe solution is to insulate the majority of our code from any knowledge of \nthe types of objects with which it might be dealing. To accomplish this, we can \n",
      "content_length": 1846,
      "extraction_method": "Direct"
    },
    {
      "page_number": 117,
      "chapter": null,
      "content": "95 \ndeﬁ ne classes for each of the types of shapes we wish to support. All of these \nclasses would inherit from the common base class Shape. A virtual function—\nthe C++ language’s primary polymorphism mechanism—would be deﬁ ned \ncalled Draw(), and each distinct shape class would implement this function \nin a diﬀ erent way. Without “knowing” what speciﬁ c types of shapes it has \nbeen given, the drawing function can now simply call each shape’s Draw()\nfunction in turn.\nstruct Shape\n{\nvirtual void Draw() = 0; \n// pure virtual function\n};\nstruct Circle : public Shape\n{\nvirtual void Draw()\n {\n \n \n// draw shape as a circle\n }\n};\nstruct Rectangle : public Shape\n{\nvirtual void Draw()\n {\n \n \n// draw shape as a rectangle\n }\n};\nstruct Triangle : public Shape\n{\nvoid Draw()\n {\n \n \n// draw shape as a triangle\n }\n};\nvoid drawShapes(std::list<Shape*> shapes)\n{\n \nstd::list<Shape*>::iterator pShape = shapes.begin();\n \nstd::list<Shape*>::iterator pEnd = shapes.end();\n \nfor ( ; pShape != pEnd; ++pShape)\n {\n  pShape->Draw();\n }\n}\n3.1. C++ Review and Best Practices\n",
      "content_length": 1060,
      "extraction_method": "Direct"
    },
    {
      "page_number": 118,
      "chapter": null,
      "content": "96 \n3. Fundamentals of Software Engineering for Games\n3.1.1.5. \n Composition and  Aggregation\nComposition is the practice of using a group of interacting objects to accomplish \na high-level task. Composition creates a “has-a” or “uses-a” relationship be-\ntween classes. (Technically speaking, the “has-a” relationship is called com-\nposition, while the “uses-a” relationship is called aggregation.) For example, a \nspace ship has an engine, which in turn has a fuel tank. Composition/aggrega-\ntion usually results in the individual classes being simpler and more focused. \nInexperienced object-oriented programmers oft en rely too heavily on inheri-\ntance and tend to underutilize aggregation and composition.\nAs an example, imagine that we are designing a graphical user interface \nfor our game’s front end. We have a class Window that represents any rectan-\ngular GUI element. We also have a class called Rectangle that encapsulates \nthe mathematical concept of a rectangle. A naïve programmer might derive \nthe Window class from the Rectangle class (using an “is-a” relationship). But \nin a more ﬂ exible and well-encapsulated design, the Window class would refer \nto or contain a Rectangle (employing a “has-a” or “uses-a” relationship). This \nmakes both classes simpler and more focused and allows the classes to be \nmore easily tested, debugged, and reused.\n3.1.1.6. \nDesign Patterns\nWhen the same type of problem arises over and over, and many diﬀ erent pro-\ngrammers employ a very similar solution to that problem, we say that a  design \npatt ern has arisen. In object-oriented programming, a number of common de-\nsign patt erns have been identiﬁ ed and described by various authors. The most \nwell-known book on this topic is probably the “Gang of Four” book [17].\nHere are a few examples of common general-purpose design patt erns.\nz Singleton. This patt ern ensures that a particular class has only one in-\nstance (the singleton instance) and provides a global point of access to it.\nz Iterator. An iterator provides an eﬃ  cient means of accessing the indi-\nvidual elements of a collection, without exposing the collection’s under-\nlying implementation. The iterator “knows” the implementation details \nof the collection, so that its users don’t have to.\nz Abstract factory. An abstract factory provides an interface for creating \nfamilies of related or dependent classes without specifying their con-\ncrete classes.\nThe game industry has its own set of design patt erns, for addressing \nproblems in every realm from rendering to collision to animation to audio. \nIn a sense, this book is all about the high-level design patt erns prevalent in \nmodern 3D game engine design.\n",
      "content_length": 2687,
      "extraction_method": "Direct"
    },
    {
      "page_number": 119,
      "chapter": null,
      "content": "97 \n3.1.2. \n Coding Standards: Why and How Much?\nDiscussions of coding conventions among engineers can oft en lead to heated \n“religious” debates. I do not wish to spark any such debate here, but I will go \nso far as to suggest that following at least some minimal coding standards is a \ngood idea. Coding standards exist for two primary reasons.\n1. Some standards make the code more readable, understandable, and \nmaintainable.\n2. Other conventions help to prevent programmers from shooting them-\nselves in the foot. For example, a coding standard might encourage the \nprogrammer to use only a smaller, more testable, and less error-prone \nsubset of the whole language. The C++ language is rife with possibili-\nties for abuse, so this kind of coding standard is particularly important \nwhen using C++.\nIn my opinion, the most important things to achieve in your coding con-\nventions are the following.\nz Interfaces are king. Keep your  interfaces (.h ﬁ les) clean, simple, minimal, \neasy to understand, and well-commented.\nz Good names encourage understanding and avoid confusion. Stick to intuitive \nnames that map directly to the purpose of the class, function, or vari-\nable in question. Spend time up-front identifying a good name. Avoid \na naming scheme that requires programmers to use a look-up table in \norder to decipher the meaning of your code. Remember that high-level \nprogramming languages like C++ are intended for humans to read. (If \nyou disagree, just ask yourself why you don’t write all your soft ware \ndirectly in machine language.)\nz Don’t clutt er the  global namespace. Use C++ namespaces or a common \nnaming preﬁ x to ensure that your symbols don’t collide with symbols \nin other libraries. (But be careful not to overuse namespaces, or nest \nthem too deeply.) Name #defined symbols with extra care; remember \nthat C++ preprocessor macros are really just text substitutions, so they \ncut across all C/C++ scope and namespace boundaries.\nz Follow C++ best practices. Books like the Eﬀ ective C++ series by Scott  Mey-\ners [31, 32], Meyers’ Eﬀ ective STL [33], and Large-Scale C++ Soft ware De-\nsign by John Lakos [27] provide excellent guidelines that will help keep \nyou out of trouble.\nz Be consistent. The rule I try to use is as follows: If you’re writing a body \nof code from scratch, feel free to invent any convention you like—then \nstick to it. When editing pre-existing code, try to follow whatever con-\nventions have already been established.\n3.1. C++ Review and Best Practices\n",
      "content_length": 2513,
      "extraction_method": "Direct"
    },
    {
      "page_number": 120,
      "chapter": null,
      "content": "98 \n3. Fundamentals of Software Engineering for Games\nz Make errors stick out. Joel Spolsky wrote an excellent article on coding \nconventions, which can be found at htt p://www.joelonsoft ware.com /\narticles /Wrong.html. Joel suggests that the “cleanest” code is not neces-\nsarily code that looks neat and tidy on a superﬁ cial level, but rather the \ncode that is writt en in a way that makes common programming errors \neasier to see. Joel’s articles are always fun and educational, and I highly \nrecommend this one.\n3.2. Data, Code, and Memory in C/C++\n3.2.1. \nNumeric Representations\nNumbers are at the heart of everything that we do in game engine development \n(and soft ware development in general). Every soft ware engineer should under-\nstand how numbers are represented and stored by a computer. This section will \nprovide you with the basics you’ll need throughout the rest of the book.\n3.2.1.1. \nNumeric Bases\nPeople think most naturally in base ten, also known as  decimal notation. In this \nnotation, ten distinct digits are used (0 through 9), and each digit from right \nto left  represents the next highest power of 10. For example, the number 7803 \n= (7×103) + (8×102) + (0×101) + (3×100) = 7000 + 800 + 0 + 3.\nIn computer science, mathematical quantities such as integers and real-\nvalued numbers need to be stored in the computer’s memory. And as we know, \ncomputers store numbers in  binary format, meaning that only the two digits 0 \nand 1 are available. We call this a base-two representation, because each digit \nfrom right to left  represents the next highest power of 2. Computer scientists \nsometimes use a preﬁ x of “0b” to represent binary numbers. For example, the \nbinary number 0b1101 is equivalent to decimal 13, because 0b1101 = (1×23) + \n(1×22) + (0×21) + (1×20) = 8 + 4 + 0 + 1 = 13.\nAnother common notation popular in computing circles is  hexadecimal, or \nbase 16. In this notation, the 10 digits 0 through 9 and the six lett ers A through \nF are used; the lett ers A through F replace the decimal values 10 through 15, \nrespectively. A preﬁ x of “0x” is used to denote hex numbers in the C and C++ \nprogramming languages. This notation is popular because computers gener-\nally store data in groups of 8 bits known as bytes, and since a single hexadeci-\nmal digit represents 4 bits exactly, a pair of hex digits represents a byte. For \nexample, the value 0xFF = 0b11111111 = 255 is the largest number that can be \nstored in 8 bits (1 byte). Each digit in a hexadecimal number, from right to left , \nrepresents the next power of 16. So, for example, 0xB052 = (11×163) + (0×162) + \n(5×161) + (2×160) = (11×4096) + (0×256) + (5×16) + (2×1) = 45,138.\n",
      "content_length": 2681,
      "extraction_method": "Direct"
    },
    {
      "page_number": 121,
      "chapter": null,
      "content": "99 \n3.2. Data, Code, and Memory in C/C++\n3.2.1.2. Signed and Unsigned Integers\nIn computer science, we use both  signed and  unsigned integers. Of course, \nthe term “unsigned integer” is actually a bit of a misnomer—in mathematics, \nthe whole numbers or natural numbers range from 0 (or 1) up to positive inﬁ nity, \nwhile the integers range from negative inﬁ nity to positive inﬁ nity. Neverthe-\nless, we’ll use computer science lingo in this book and stick with the terms \n“signed integer” and “unsigned integer.”\nMost modern personal computers and game consoles work most easily \nwith integers that are 32 bits or 64 bits wide (although 8- and 16-bit integers \nare also used a great deal in game programming as well). To represent a 32-\nbit unsigned integer, we simply encode the value using binary notation (see \nabove). The range of possible values for a 32-bit unsigned integer is 0x00000000 \n(0) to 0xFFFFFFFF (4,294,967,295).\nTo represent a signed integer in 32 bits, we need a way to diﬀ erentiate be-\ntween positive and negative vales. One simple approach would be to reserve \nthe most signiﬁ cant bit as a sign bit—when this bit is zero the value is positive, \nand when it is one the value is negative. This gives us 31 bits to represent the \nmagnitude of the value, eﬀ ectively cutt ing the range of possible magnitudes \nin half (but allowing both positive and negative forms of each distinct magni-\ntude, including zero).\nMost microprocessors use a slightly more eﬃ  cient technique for encod-\ning negative integers, called  two’s complement notation. This notation has only \none representation for the value zero, as opposed to the two representations \npossible with simple sign bit (positive zero and negative zero). In 32-bit two’s \ncomplement notation, the value 0xFFFFFFFF is interpreted to mean –1, and \nnegative values count down from there. Any value with the most signiﬁ cant \nbit set is considered negative. So values from 0x00000000 (0) to 0x7FFFFFFF \n(2,147,483,647) represent positive integers, and 0x80000000 (–2,147,483,648) to \n0xFFFFFFFF (–1) represent negative integers.\n3.2.1.3.  Fixed-Point Notation\nIntegers are great for representing whole numbers, but to represent fractions \nand irrational numbers we need a diﬀ erent format that expresses the concept \nof a decimal point.\nOne early approach taken by computer scientists was to use ﬁ xed-point \nnotation. In this notation, one arbitrarily chooses how many bits will be used \nto represent the whole part of the number, and the rest of the bits are used \nto represent the fractional part. As we move from left  to right (i.e., from the \nmost signiﬁ cant bit to the least signiﬁ cant bit), the magnitude bits represent \ndecreasing powers of two (…, 16, 8, 4, 2, 1), while the fractional bits represent \n",
      "content_length": 2786,
      "extraction_method": "Direct"
    },
    {
      "page_number": 122,
      "chapter": null,
      "content": "100 \n3. Fundamentals of Software Engineering for Games\ndecreasing inverse powers of two (1/2 , 1/4 , 1/8 , 1/16 , …). For example, to store the \nnumber –173.25 in 32-bit ﬁ xed-point notation, with one sign bit, 16 bits for the \nmagnitude and 15 bits for the fraction, we ﬁ rst convert the sign, the whole part \nand fractional part into their binary equivalents individually (negative = 0b1, \n173 = 0b0000000010101101, and 0.25 = 1/4 = 0b010000000000000). Then we pack \nthose values together into a 32-bit integer. The ﬁ nal result is 0x8056A000. This \nis illustrated in Figure 3.4.\nThe problem with ﬁ xed-point notation is that it constrains both the range \nof magnitudes that can be represented and the amount of precision we can \nachieve in the fractional part. Consider a 32-bit ﬁ xed-point value with 16 bits \nfor the magnitude, 15 bits for the fraction, and a sign bit. This format can only \nrepresent magnitudes up to ±65,535, which isn’t particularly large. To over-\ncome this problem, we employ a ﬂ oating-point representation.\n3.2.1.4. Floating-Point Notation\nIn  ﬂ oating-point notation, the position of the decimal place is arbitrary and is \nspeciﬁ ed with the help of an exponent. A ﬂ oating-point number is broken into \nthree parts: the mantissa, which contains the relevant digits of the number on \nboth sides of the decimal point, the exponent, which indicates where in that \nstring of digits the decimal point lies, and a sign bit, which of course indicates \nwhether the value is positive or negative. There are all sorts of diﬀ erent ways \nto lay out these three components in memory, but the most common standard \nis IEEE-754. It states that a 32-bit ﬂ oating-point number will be represented \nwith the sign in the most signiﬁ cant bit, followed by 8 bits of exponent, and \nﬁ nally 23 bits of mantissa.\nThe value v represented by a sign bit s, an exponent e and a mantissa m is \nv = s × 2(e – 127) × (1 + m).\nThe sign bit s has the value +1 or –1. The exponent e is biased by 127 so \nthat negative exponents can be easily represented. The mantissa begins with \nan implicit 1 that is not actually stored in memory, and the rest of the bits are \ninterpreted as inverse powers of two. Hence the value represented is really 1 \n+ m, where m is the fractional value stored in the mantissa.\n31\n15\n0\nmagnitude (16 bits)\nfraction (15 bits)\n1\n= –173.25\nsign\n0x80\n0x56\n0xA0\n0x00\n1 0\n0 0 0\n0 0\n0 0 1\n0 1\n0 1 1\n0 1\n0 1 0\n0 0\n0 0 0\n0 0\n0 0 0\n0 0\nFigure 3.4.  Fixed-point notation with 16-bit magnitude and 16-bit fraction.\n",
      "content_length": 2527,
      "extraction_method": "Direct"
    },
    {
      "page_number": 123,
      "chapter": null,
      "content": "101 \nFor example, the bit patt ern shown in Figure 3.5 represents the value \n0.15625, because s = 0 (indicating a positive number), e = 0b01111100 = 124, \nand m = 0b0100… = 0×2–1 + 1×2–2 = ¼. Therefore,\n \nv = s × 2(e – 127) × (1 + m)\n= (+1) × 2(124 – 127) × (1 + 1/4)\n    \n= 2–3 × 5/4 \n \n \n \n             (3.1)\n    \n= 1/8 × 5/4\n    \n= 0.125 × 1.25 = 0.15625.\nThe Trade-Off between Magnitude and Precision\nThe  precision of a ﬂ oating-point number increases as the magnitude decreases, \nand vice versa. This is because there are a ﬁ xed number of bits in the mantissa, \nand these bits must be shared between the whole part and the fractional part \nof the number. If a large percentage of the bits are spent representing a large \nmagnitude, then a small percentage of bits are available to provide fractional \nprecision. In physics the term  signiﬁ cant digits is typically used to describe this \nconcept (htt p://en.wikipedia.org/wiki/Signiﬁ cant_digits).\nTo understand the trade-oﬀ  between magnitude and precision, let’s look \nat the largest possible ﬂ oating-point value, FLT_MAX ≈ 3.403×1038, whose rep-\nresentation in 32-bit IEEE ﬂ oating-point format is 0x7F7FFFFF. Let’s break this \ndown:\nz The largest absolute value that we can represent with a 23-bit mantissa \nis 0x00FFFFFF in hexadecimal, or 24 consecutive binary ones—that’s 23 \nones in the mantissa, plus the implicit leading one.\nz An exponent of 255 has a special meaning in the IEEE-754 format—it is \nused for values like not-a-number (NaN) and inﬁ nity—so it cannot be \nused for regular numbers. Hence the maximum 8-bit exponent is actu-\nally 254, which translates into 127 aft er subtracting the implicit bias of \n127.\nSo  FLT_MAX is 0x00FFFFFF×2127 = 0xFFFFFF00000000000000000000000000. In \nother words, our 24 binary ones were shift ed up by 127 bit positions, leav-\ning 127 – 23 = 104 binary zeros (or 104/4 = 26 hexadecimal zeros) aft er the \n3.2. Data, Code, and Memory in C/C++\n0\n31\n23\n0\nexponent (8 bits)\n0\n1 1 1\n1 1\n0 0 0\n1 0\n0 0 0\n0 0\n0 0 0\n0 0\n0 0 0\n0 0\n0 0 0\n0 0\nmantissa (23 bits)\nsign\n= 0.15625\nFigure 3.5.  IEEE-754 32-bit ﬂ oating-point format.\n",
      "content_length": 2128,
      "extraction_method": "Direct"
    },
    {
      "page_number": 124,
      "chapter": null,
      "content": "102 \n3. Fundamentals of Software Engineering for Games\nleast signiﬁ cant digit of the mantissa. Those trailing zeros don’t correspond \nto any actual bits in our 32-bit ﬂ oating-point value—they just appear out of \nthin air because of the exponent. If we were to subtract a small number (where \n“small” means any number composed of fewer than 26 hexadecimal digits) \nfrom FLT_MAX, the result would still be FLT_MAX, because those 26 least sig-\nniﬁ cant hexadecimal digits don’t really exist!\nThe opposite eﬀ ect occurs for ﬂ oating-point values whose magnitudes \nare much less than one. In this case, the exponent is large but negative, and \nthe signiﬁ cant digits are shift ed in the opposite direction. We trade the ability \nto represent large magnitudes for high precision. In summary, we always have \nthe same number of signiﬁ cant digits (or really signiﬁ cant bits) in our ﬂ oating-\npoint numbers, and the exponent can be used to shift  those signiﬁ cant bits into \nhigher or lower ranges of magnitude.\nAnother subtlety to notice is that there is a ﬁ nite gap between zero and the \nsmallest nonzero value we can represent with any ﬂ oating-point notation. The \nsmallest nonzero magnitude we can represent is FLT_MIN = 2–126 ≈ 1.175×10–38, \nwhich has a binary representation of 0x00800000 (i.e., the exponent is 0x01, \nor –126 aft er subtracting the bias, and the mantissa is all zeros, except for the \nimplicit leading one). There is no way to represent a nonzero magnitude that \nis smaller than 1.175×10–38, because the next smallest valid value is zero. Put \nanother way, the real number line is quantized when using a ﬂ oating-point \nrepresentation.\nFor a particular ﬂ oating-point representation, the machine epsilon is de-\nﬁ ned to be the smallest ﬂ oating-point value ε  that satisﬁ es the equation, 1 + \nε  ≠ 1. For an IEEE-754 ﬂ oating-point number, with its 23 bits of precision, the \nvalue of ε is 2–23, which is approximately 1.192×10–7. The most signiﬁ cant digit \nof ε falls just inside the range of signiﬁ cant digits in the value 1.0, so adding \nany value smaller than ε to 1.0 has no eﬀ ect. In other words, any new bits con-\ntributed adding a value smaller than ε will get “chopped oﬀ ” when we try to \nﬁ t the sum into a mantissa with only 23 bits.\nThe concepts of limited precision and the machine epsilon have real im-\npacts on game soft ware. For example, let’s say we use a ﬂ oating-point vari-\nable to track absolute game time in seconds. How long can we run our game \nbefore the magnitude of our clock variable gets so large that adding 1/30th of \na second to it no longer changes its value? The answer is roughly 12.9 days. \nThat’s longer than most games will be left  running, so we can probably get \naway with using a 32-bit ﬂ oating-point clock measured in seconds in a game. \nBut clearly it’s important to understand the limitations of the ﬂ oating-point \nformat, so that we can predict potential problems and take steps to avoid them \nwhen necessary.\n",
      "content_length": 2986,
      "extraction_method": "Direct"
    },
    {
      "page_number": 125,
      "chapter": null,
      "content": "103 \nIEEE Floating-Point Bit Tricks\nSee [7], Section 2.1, for a few really useful IEEE ﬂ oating-point “bit tricks” that \ncan make ﬂ oating-point calculations lightning-fast.\n3.2.1.5. Atomic Data Types\nAs you know, C and C++ provide a number of  atomic data types. The C and \nC++ standards provide guidelines on the relative sizes and signedness of these \ndata types, but each compiler is free to deﬁ ne the types slightly diﬀ erently in \norder to provide maximum performance on the target hardware.\nz  char. A char is usually 8 bits and is generally large enough to hold an \nASCII or UTF-8 character (see Section 5.4.4.1). Some compilers deﬁ ne \nchar to be signed, while others use unsigned chars by default.\nz int, short, long. An int is supposed to hold a signed integer value \nthat is the most eﬃ  cient size for the target platform; it is generally de-\nﬁ ned to be 32 bits wide on Pentium class PCs. A short is intended to \nbe smaller than an int and is 16 bits on many machines. A long is as \nlarge as or larger than an int and may be 32 or 64 bits, depending on \nthe hardware.\nz float. On most modern compilers, a float is a 32-bit IEEE-754 ﬂ oat-\ning-point value.\nz double. A double is a double-precision (i.e., 64-bit) IEEE-754 ﬂ oating-\npoint value.\nz bool. A bool is a true/false value. The size of a bool varies widely across \ndiﬀ erent compilers and hardware architectures. It is never implemented \nas a single bit, but some compilers deﬁ ne it to be 8 bits while others use \na full 32 bits.\nCompiler-Speciﬁ c Sized Types\nThe standard C/C++ atomic data types were designed to be portable and \ntherefore nonspeciﬁ c. However, in many soft ware engineering endeavors, in-\ncluding game engine programming, it is oft en important to know exactly how \nwide a particular variable is. The Visual Studio C/C++ compiler deﬁ nes the fol-\nlowing extended keywords for declaring variables that are an explicit number \nof bits wide: __int8,  __int16, __int32, and __int64.\nSIMD Types\nThe CPUs on many modern computers and game consoles have a special-\nized type of  arithmetic logic unit (ALU) referred to as a vector processor or \nvector unit. A vector processor supports a form of parallel processing known \nas single instruction, multiple data (SIMD), in which a mathematical operation \n3.2. Data, Code, and Memory in C/C++\n",
      "content_length": 2326,
      "extraction_method": "Direct"
    },
    {
      "page_number": 126,
      "chapter": null,
      "content": "104 \n3. Fundamentals of Software Engineering for Games\nis performed on multiple quantities in parallel, using a single machine in-\nstruction. In order to be processed by the vector unit, two or more quanti-\nties are packed into a 64- or 128-bit CPU register. In game programming, \nthe most commonly used SIMD register format packs four 32-bit IEEE-754 \nﬂ oating-point quantities into a 128-bit SIMD register. This format allows us to \nperform calculations such as vector dot products and matrix multiplications \nmuch more eﬃ  ciently than would be possible with a SISD (single instruction, \nsingle data) ALU.\nEach microprocessor has a diﬀ erent name for its SIMD instruction set, \nand the compilers that target those microprocessors use a custom syntax to \ndeclare SIMD variables. For example, on a Pentium class CPU, the SIMD in-\nstruction set is known as SSE (streaming  SIMD extensions), and the Microsoft  \nVisual Studio compiler provides the built-in data type  __m128 to represent a \nfour-ﬂ oat SIMD quantity.  The PowerPC class of CPUs used on the PLAYSTA-\nTION 3 and Xbox 360 calls its SIMD instruction set  Altivec, and the Gnu C++ \ncompiler uses the syntax vector float to declare a packed four-ﬂ oat SIMD \nvariable. We’ll discuss how SIMD programming works in more detail in Sec-\ntion 4.7.\nPortable Sized Types\nMost other compilers have their own “sized” data types, with similar seman-\ntics but slightly diﬀ erent syntax. Because of these diﬀ erences between compil-\ners, most game engines achieve source code portability by deﬁ ning their own \ncustom atomic data types. For example, at Naughty Dog we use the following \natomic types:\nz  F32 is a 32-bit IEEE-754 ﬂ oating-point value.\nz U8, I8, U16, I16,  U32,  I32, U64, and I64 are unsigned and signed 8-, \n16-, 32-, and 64-bit integers, respectively.\nz  U32F and  I32F are “fast” unsigned and signed 32-bit values, respec-\ntively. Each of these data types acts as though it contains a 32-bit value, \nbut it actually occupies 64 bits in memory. This permits the PS3’s cen-\ntral PowerPC-based processor (called the PPU) to read and write these \nvariables directly into its 64-bit registers, providing a signiﬁ cant speed \nboost over reading and writing 32-bit variables.\nz  VF32 represents a packed four-ﬂ oat SIMD value.\nOGRE ’s Atomic Data Types\nOGRE deﬁ nes a number of atomic types of its own. Ogre::uint8, Ogre::\nuint16 and Ogre::uint32 are the basic unsigned sized integral types.\n",
      "content_length": 2450,
      "extraction_method": "Direct"
    },
    {
      "page_number": 127,
      "chapter": null,
      "content": "105 \nOgre ::Real deﬁ nes a real ﬂ oating-point value. It is usually deﬁ ned to \nbe 32 bits wide (equivalent to a float), but it can be redeﬁ ned globally to be \n64 bits wide (like a double) by deﬁ ning the preprocessor macro OGRE_DOU-\nBLE_PRECISION to 1. This ability to change the meaning of Ogre::Real is \ngenerally only used if one’s game has a particular requirement for double-\nprecision math, which is rare. Graphics chips (GPUs) always perform their \nmath with 32-bit or 16-bit ﬂ oats, the CPU/FPU is also usually faster when \nworking in single-precision, and SIMD vector instructions operate on 128-bit \nregisters that contain four 32-bit ﬂ oats each. Hence most games tend to stick \nto single-precision ﬂ oating-point math.\nThe data types Ogre ::uchar, Ogre::ushort, Ogre::uint and \nOgre::ulong are just shorthand notations for C/C++’s unsigned char, un-\nsigned short, and unsigned long, respectively. As such, they are no more \nor less useful than their native C/C++ counterparts.\nThe types Ogre ::Radian and Ogre::Degree are particularly interest-\ning. These classes are wrappers around a simple Ogre::Real value. The pri-\nmary role of these types is to permit the angular units of hard-coded literal \nconstants to be documented and to provide automatic conversion between \nthe two unit systems. In addition, the type Ogre::Angle represents an angle \nin the current “default” angle unit. The programmer can deﬁ ne whether the \ndefault will be radians or degrees when the OGRE application ﬁ rst starts \nup.\nPerhaps surprisingly, OGRE does not provide a number of sized atomic \ndata types that are commonplace in other game engines. For example, it de-\nﬁ nes no signed 8-, 16-, or 64-bit integral types. If you are writing a game en-\ngine on top of OGRE, you will probably ﬁ nd yourself deﬁ ning these types \nmanually at some point.\n3.2.1.6. Multi-Byte Values and Endianness\nValues that are larger than eight bits (one byte) wide are called  multi-byte quan-\ntities. They’re commonplace on any soft ware project that makes use of integers \nand ﬂ oating-point values that are 16 bits or wider. For example, the integer \nvalue 4660 = 0x1234 is represented by the two bytes 0x12 and 0x34. We call \n0x12 the most signiﬁ cant byte (MSB) and 0x34 the least signiﬁ cant byte (LSB). \nIn a 32-bit value, such as 0xABCD1234, the MSB is 0xAB and the LSB is 0x34. \nThe same concepts apply to 64-bit integers and to 32- and 64-bit ﬂ oating-point \nvalues as well.\nMulti-byte integers can be stored into memory in one of two ways, and \ndiﬀ erent microprocessors may diﬀ er in their choice of storage method (see \nFigure 3.6).\n3.2. Data, Code, and Memory in C/C++\n",
      "content_length": 2657,
      "extraction_method": "Direct"
    },
    {
      "page_number": 128,
      "chapter": null,
      "content": "106 \n3. Fundamentals of Software Engineering for Games\nz Litt le-endian. If a microprocessor stores the least signiﬁ cant byte (LSB) of \na multi-byte value at a lower memory address than the most signiﬁ cant \nbyte (MSB), we say that the processor is  litt le-endian. On a litt le-endian \nmachine, the number 0xABCD1234 would be stored in memory using \nthe consecutive bytes 0x34, 0x12, 0xCD, 0xAB.\nz Big-endian. If a microprocessor stores the most signiﬁ cant byte (MSB) of \na multi-byte value at a lower memory address than the least signiﬁ cant \nbyte (LSB), we say that the processor is  big-endian. On a big-endian ma-\nchine, the number 0xABCD1234 would be stored in memory using the \nbytes 0xAB, 0xCD, 0x12, 0x34.\nMost programmers don’t need to think much about  endianness. How-\never, when you’re a game programmer, endianness can become a bit of a thorn \nin your side. This is because games are usually developed on a PC or Linux ma-\nchine running an Intel Pentium processor (which is litt le-endian), but run on \na console such as the Wii, Xbox 360, or PLAYSTATION 3—all three of which \nutilize a variant of the PowerPC processor (which can be conﬁ gured to use \neither endianness, but is big-endian by default). Now imagine what happens \nwhen you generate a data ﬁ le for consumption by your game engine on an \nIntel processor and then try to load that data ﬁ le into your engine running on \na PowerPC processor. Any multi-byte value that you wrote out into that data \nﬁ le will be stored in litt le-endian format. But when the game engine reads the \nﬁ le, it expects all of its data to be in big-endian format. The result? You’ll write \n0xABCD1234, but you’ll read 0x3412CDAB, and that’s clearly not what you \nintended!\nThere are at least two solutions to this problem.\n1. You could write all your data ﬁ les as text and store all multi-byte num-\nbers as sequences of decimal digits, one character (one byte) per digit. \nThis would be an ineﬃ  cient use of disk space, but it would work.\nU32 value = 0xABCD1234;\nU8* pBytes = (U8*)&value;\nBig-endian\n0xAB\n0xCD\npBytes + 0x0\n0x12\n0x34\npBytes + 0x1\npBytes + 0x2\npBytes + 0x3\nLittle-endian\n0x34\n0x12\n0xCD\n0xAB\npBytes + 0x0\npBytes + 0x1\npBytes + 0x2\npBytes + 0x3\nFigure 3.6.  Big- and little-endian representations of the value 0xABCD1234.\n",
      "content_length": 2293,
      "extraction_method": "Direct"
    },
    {
      "page_number": 129,
      "chapter": null,
      "content": "107 \n2. You can have your tools endian-swap the data prior to writing it into a \nbinary data ﬁ le. In eﬀ ect, you make sure that the data ﬁ le uses the endi-\nanness of the target microprocessor (the game console), even if the tools \nare running on a machine that uses the opposite endianness.\nInteger Endian-Swapping\n Endian-swapping an integer is not conceptually diﬃ  cult. You simply start at \nthe most signiﬁ cant byte of the value and swap it with the least signiﬁ cant \nbyte; you continue this process until you reach the half-way point in the value. \nFor example, 0xA7891023 would become 0x231089A7.\nThe only tricky part is knowing which bytes to swap. Let’s say you’re writ-\ning the contents of a C struct or C++ class from memory out to a ﬁ le. To \nproperly endian-swap this data, you need to keep track of the locations and \nsizes of each data member in the struct and swap each one appropriately \nbased on its size. For example, the structure\nstruct Example\n{\n    U32   m_a;\n    U16   m_b;\n    U32   m_c;\n};\nmight be writt en out to a data ﬁ le as follows:\nvoid writeExampleStruct(Example& ex, Stream& stream)\n{\n    stream.writeU32(swapU32(ex.m_a));\n    stream.writeU16(swapU16(ex.m_b));\n    stream.writeU32(swapU32(ex.m_c));\n}\nand the swap functions might be deﬁ ned like this:\ninline U16 swapU16(U16 value)\n{\n    return ((value & 0x00FF) << 8)\n         | ((value & 0xFF00) >> 8);\n}\ninline U32 swapU32(U32 value)\n{\n return \n   ((value & 0x000000FF) << 24)\n         | ((value & 0x0000FF00) << 8)\n         | ((value & 0x00FF0000) >> 8)\n         | ((value & 0xFF000000) >> 24);\n}\n3.2. Data, Code, and Memory in C/C++\n",
      "content_length": 1626,
      "extraction_method": "Direct"
    },
    {
      "page_number": 130,
      "chapter": null,
      "content": "108 \n3. Fundamentals of Software Engineering for Games\nYou cannot simply cast the Example object into an array of bytes and \nblindly swap the bytes using a single general-purpose function. We need to \nknow both which data members to swap and how wide each member is; and each \ndata member must be swapped individually.\nFloating-Point Endian-Swapping\nLet’s take a brief look at how ﬂ oating-point endian-swapping diﬀ ers from in-\nteger endian-swapping. As we’ve seen, an IEEE-754 ﬂ oating-point value has \na detailed internal structure involving some bits for the mantissa, some bits \nfor the exponent, and a sign bit. However, you can endian-swap it just as if it \nwere an integer, because bytes are bytes. You can reinterpret ﬂ oats as integers \nby using C++’s reinterpret_cast operator on a pointer to the ﬂ oat; this is \nknown as type punning. But punning can lead to optimization bugs when  strict \naliasing is enabled. (See htt p://cocoawithlove.com/2008/04/using-pointers-to-\nrecast-in-c-is-bad.html for an excellent description of this problem.) One con-\nvenient approach is to use a union, as follows:\nunion U32F32\n{\n    U32    m_asU32;\n    F32    m_asF32;\n};\ninline F32 swapF32(F32 value)\n{\n    U32F32 u;\n    u.m_asF32 = value;\n       // endian-swap as integer\n  u.m_asU32 \n= swapU32(u.m_asU32);\n  return \nu.m_asF32;\n}\n3.2.2. Declarations, Deﬁ nitions, and Linkage\n3.2.2.1. Translation Units Revisited\nAs we saw in Chapter 2, a C or C++ program is comprised of translation units. \nThe compiler translates one .cpp ﬁ le at a time, and for each one it generates \nan output ﬁ le called an object ﬁ le (.o or .obj). A .cpp ﬁ le is the smallest unit of \ntranslation operated on by the compiler; hence, the name “ translation unit.” \nAn object ﬁ le contains not only the compiled machine code for all of the func-\ntions deﬁ ned in the .cpp ﬁ le, but also all of its global and static variables. In ad-\ndition, an object ﬁ le may contain  unresolved references to functions and global \nvariables deﬁ ned in other .cpp ﬁ les.\n",
      "content_length": 2027,
      "extraction_method": "Direct"
    },
    {
      "page_number": 131,
      "chapter": null,
      "content": "109 \n3.2. Data, Code, and Memory in C/C++\n???\nUnresolved Reference\n???\nMultiply-Defined Symbol\n???\nfoo.cpp\nU32 gGlobalA ;\nU32 gGlobalB ;\nvoid f ()\n{\n    // ...\n    gGlobalC = 5.3f;\n    gGlobalD = -2;\n    // ...\n}\nextern U 32 gGlobalC ;\nbar.cpp\nF32 gGlobalC ;\nvoid g ()\n{\n    // ...\n    U32 a = gGlobalA ;\n    // ...\n    f();\n    // ...\n    gGlobalB = 0;\n}\nextern U 32 gGlobalA ;\nextern U 32 gGlobalB ;\nextern void f ();\nspam.cpp\nU32 gGlobalA ;\nvoid h()\n{\n    // ...\n}\nFigure 3.9.  The two most common linker errors.\nfoo.cpp\nU32 gGlobalA ;\nU32 gGlobalB ;\nvoid f()\n{\n    // ...\n    gGlobalC = 5.3f;\n    // ...\n}\nextern U 32 gGlobalC ;\nbar.cpp\nF32 gGlobalC ;\nvoid g ()\n{\n    // ...\n    U32 a = gGlobalA ;\n    // ...\n    f();\n    // ...\n    gGlobalB = 0;\n}\nextern U 32 gGlobalA ;\nextern U 32 gGlobalB ;\nextern void f ();\nFigure 3.7.  Unresolved external references in two translation units.\nfoo.cpp\nU32 gGlobalA ;\nU32 gGlobalB ;\nvoid f ()\n{\n    // ...\n    gGlobalC = 5.3f;\n    // ...\n}\nextern U 32 gGlobalC ;\nbar.cpp\nF32 gGlobalC ;\nvoid g()\n{\n    // ...\n    U32 a = gGlobalA ;\n    // ...\n    f();\n    // ...\n    gGlobalB = 0;\n}\nextern U 32 gGlobalA ;\nextern U 32 gGlobalB ;\nextern void f ();\nFigure 3.8.  Fully resolved external references after successful linking.\n",
      "content_length": 1262,
      "extraction_method": "Direct"
    },
    {
      "page_number": 132,
      "chapter": null,
      "content": "110 \n3. Fundamentals of Software Engineering for Games\nThe compiler only operates on one translation unit at a time, so whenever \nit encounters a reference to an external global variable or function, it must \n“go on faith” and assume that the entity in question really exists, as shown \nin Figure 3.7. It is the linker’s job to combine all of the object ﬁ les into a ﬁ nal \nexecutable image. In doing so, the linker reads all of the object ﬁ les and at-\ntempts to resolve all of the unresolved cross-references between them. If it is \nsuccessful, an executable image is generated containing all of the functions, \nglobal variables, and static variables, with all cross-translation-unit references \nproperly resolved. This is depicted in Figure 3.8.\nThe linker’s primary job is to resolve external references, and in this ca-\npacity it can generate only two kinds of errors:\n1. The target of an extern reference might not be found, in which case the \nlinker generates an  “unresolved symbol” error.\n2. The linker might ﬁ nd more than one variable or function with the same \nname, in which case it generates a  “multiply deﬁ ned symbol” error.\nThese two situations are shown in Figure 3.9.\n3.2.2.2. Declaration versus Deﬁ nition\nIn the C and C++ languages, variables and functions must be declared and de-\nﬁ ned before they can be used. It is important to understand the diﬀ erence be-\ntween a declaration and a deﬁ nition in C and C++.\nz A  declaration is a description of a data object or function. It provides the \ncompiler with the name of the entity and its  data type or function signature \n(i.e., return type and argument type(s)).\nz A  deﬁ nition, on the other hand, describes a unique region of memory in \nthe program. This memory might contain a variable, an instance of a \nstruct or class, or the machine code of a function.\nIn other words, a declaration is a reference to an entity, while a deﬁ nition is the \nentity itself. A deﬁ nition is always a declaration, but the reverse is not always \nthe case—it is possible to write a pure declaration in C and C++ that is not a \ndeﬁ nition.\nFunctions are deﬁ ned by writing the body of the function immediately af-\nter the signature, enclosed in curly braces:\nfoo.cpp\n// definition of the max() function\nint max(int a, int b) \n{\n",
      "content_length": 2284,
      "extraction_method": "Direct"
    },
    {
      "page_number": 133,
      "chapter": null,
      "content": "111 \n \nreturn (a > b) ? a : b;\n}\n// definition of the min() function\nint min(int a, int b) \n{\n \nreturn (a <= b) ? a : b;\n}\nA pure declaration can be provided for a function so that it can be used in \nother translation units (or later in the same translation unit). This is done by \nwriting a function signature followed by a semicolon, with an optional preﬁ x \nof extern:\nfoo.h\nextern int max(int a, int b); // a function declaration\nint min(int a, int b);  \n     // also a declaration (the\n         \n    // ‘extern’ is optional/ \n \n         // \nassumed)\nVariables and instances of classes and structs are deﬁ ned by writing the \ndata type followed by the name of the variable or instance, and an optional \narray speciﬁ er in square brackets:\nfoo.cpp\n// All of these are variable definitions:\nU32 gGlobalInteger = 5;\nF32 gGlobalFloatArray[16];\nMyClass gGlobalInstance;\nA global variable deﬁ ned in one translation unit can optionally be declared for \nuse in other translation units by using the extern keyword:\nfoo.h\n// These are all pure declarations:\nextern U32 gGlobalInteger; \nextern F32 gGlobalFloatArray[16];\nextern MyClass gGlobalInstance;\nMultiplicity of Declarations and Deﬁ nitions\nNot surprisingly, any particular data object or function in a C/C++ program \ncan have multiple identical declarations, but each can have only one deﬁ ni-\ntion. If two or more identical deﬁ nitions exist in a single translation unit, \nthe compiler will notice that multiple entities have the same name and \nﬂ ag an error. If two or more identical deﬁ nitions exist in diﬀ erent transla-\n3.2. Data, Code, and Memory in C/C++\n",
      "content_length": 1615,
      "extraction_method": "Direct"
    },
    {
      "page_number": 134,
      "chapter": null,
      "content": "112 \n3. Fundamentals of Software Engineering for Games\ntion units, the compiler will not be able to identify the problem, because \nit operates on one translation unit at a time. But in this case, the linker \nwill give us a “multiply deﬁ ned symbol” error when it tries to resolve the \ncross-references.\nDeﬁ nitions in Header Files and Inlining\nIt is usually dangerous to place deﬁ nitions in header ﬁ les. The reason for this \nshould be prett y obvious: If a header ﬁ le containing a deﬁ nition is  #included \ninto more than one .cpp ﬁ le, it’s a sure-ﬁ re way of generating a “multiply de-\nﬁ ned symbol” linker error.\n Inline function deﬁ nitions are an exception to this rule, because each in-\nvocation of an inline function gives rise to a brand new copy of that function’s \nmachine code, embedded directly into the calling function. In fact, inline func-\ntion deﬁ nitions must be placed in header ﬁ les if they are to be used in more \nthan one translation unit. Note that it is not suﬃ  cient to tag a function declara-\ntion with the inline keyword in a .h ﬁ le and then place the body of that func-\ntion in a .cpp ﬁ le. The compiler must be able to “see” the body of the function \nin order to inline it. For example:\nfoo.h\n// This function definition will be inlined properly.\ninline int max(int a, int b)\n{\n \nreturn (a > b) ? a : b;\n}\n// This declaration cannot be inlined because the \n// compiler cannot “see” the body of the function.\ninline int min(int a, int b);\nfoo.cpp\n// The body of min() is effectively “hidden” from the\n// compiler, and so it can ONLY be inlined within \n// foo.cpp.\nint min(int a, int b)\n{\n \nreturn (a <= b) ? a : b;\n}\nThe inline keyword is really just a hint to the compiler. It does a cost/\nbeneﬁ t analysis of each inline function, weighing the size of the function’s \ncode versus the potential performance beneﬁ ts of inling it, and the compiler \ngets the ﬁ nal say as to whether the function will really be inlined or not. Some \ncompilers provide syntax like __forceinline, allowing the programmer \n",
      "content_length": 2036,
      "extraction_method": "Direct"
    },
    {
      "page_number": 135,
      "chapter": null,
      "content": "113 \nto bypass the compiler’s cost/beneﬁ t analysis and control function inlining \ndirectly.\n3.2.2.3.  Linkage\nEvery deﬁ nition in C and C++ has a property known as linkage. A deﬁ nition \nwith  external linkage is visible to and can be referenced by translation units \nother than the one in which it appears. A deﬁ nition with  internal linkage can \nonly be “seen” inside the translation unit in which it appears and thus cannot \nbe referenced by other translation units. We call this property linkage because \nit dictates whether or not the linker is permitt ed to cross-reference the entity \nin question. So, in a sense, linkage is the translation unit’s equivalent of the \n public: and  private: keywords in C++ class deﬁ nitions.\nBy default, deﬁ nitions have external linkage. The  static keyword is \nused to change a deﬁ nition’s linkage to internal. Note that two or more identi-\ncal static deﬁ nitions in two or more diﬀ erent .cpp ﬁ les are considered to be \ndistinct entities by the linker (just as if they had been given diﬀ erent names), \nso they will not generate a “multiply deﬁ ned symbol” error. Here are some \nexamples:\nfoo.cpp\n// This variable can be used by other .cpp files \n//  (external linkage).\nU32 gExternalVariable; \n// This variable is only usable within foo.cpp (internal\n// linkage).\nstatic U32 gInternalVariable;\n// This function can be called from other .cpp files \n//  (external linkage).\nvoid externalFunction()\n{\n \n// ...\n}\n// This function can only be called from within foo.cpp\n// (internal linkage).\nstatic void internalFunction()\n{\n \n// ...\n}\nbar.cpp\n// This declaration grants access to foo.cpp’s variable.\nextern U32 gExternalVariable;\n3.2. Data, Code, and Memory in C/C++\n",
      "content_length": 1712,
      "extraction_method": "Direct"
    },
    {
      "page_number": 136,
      "chapter": null,
      "content": "114 \n3. Fundamentals of Software Engineering for Games\n// This ‘gInternalVariable’ is distinct from the one \n// defined in foo.cpp – no error. We could just as \n// well have named it gInternalVariableForBarCpp – the \n// net effect is the same.\nstatic U32 gInternalVariable;\n// This function is distinct from foo.cpp’s \n// version – no error. It acts as if we had named it \n// internalFunctionForBarCpp().\nstatic void internalFunction()\n{\n \n// ...\n}\n// ERROR – multiply defined symbol!\nvoid externalFunction()\n{\n \n// ...\n}\nTechnically speaking, declarations don’t have a linkage property at all, be-\ncause they do not allocate any storage in the executable image; therefore, there \nis no question as to whether or not the linker should be permitt ed to cross-\nreference that storage. A declaration is merely a reference to an entity deﬁ ned \nelsewhere. However, it is sometimes convenient to speak about declarations \nas having internal linkage, because a declaration only applies to the transla-\ntion unit in which it appears. If we allow ourselves to loosen our terminology \nin this manner, then declarations always have internal linkage—there is no \nway to cross-reference a single declaration in multiple .cpp ﬁ les. (If we put a \ndeclaration in a header ﬁ le, then multiple .cpp ﬁ les can “see” that declaration, \nbut they are in eﬀ ect each gett ing a distinct copy of the declaration, and each \ncopy has internal linkage within that translation unit.)\nThis leads us to the real reason why inline function deﬁ nitions are permit-\nted in header ﬁ les: It is because inline functions have internal linkage by de-\nfault, just as if they had been declared static. If multiple .cpp ﬁ les #include\na header containing an inline function deﬁ nition, each translation unit gets a \nprivate copy of that function’s body, and no “multiply deﬁ ned symbol” errors \nare generated. The linker sees each copy as a distinct entity.\n3.2.3. C/C++ Memory Layout\nA program writt en in C or C++ stores its data in a number of diﬀ erent places in \nmemory. In order to understand how storage is allocated and how the various \n",
      "content_length": 2107,
      "extraction_method": "Direct"
    },
    {
      "page_number": 137,
      "chapter": null,
      "content": "115 \ntypes of C/C++ variables work, we need to understand the memory layout of \na C/C++ program.\n3.2.3.1. Executable Image\nWhen a C/C++ program is built, the linker creates an executable ﬁ le. Most UN-\nIX-like operating system s, including many game consoles, employ a popular \nexecutable ﬁ le format called the  executable and linking format (ELF). Executable \nﬁ les on those systems therefore have an .elf extension. The Windows execut-\nable format is similar to the ELF format; executables under Windows have \nan .exe extension. Whatever its format, the executable ﬁ le always contains a \npartial image of the program as it will exist in memory when it runs. I say a \n“partial” image because the program generally allocates memory at runtime \nin addition to the memory laid out in its executable image.\nThe  executable image is divided into contiguous blocks called  segments \nor sections. Every operating system lays things out a litt le diﬀ erently, and the \nlayout may also diﬀ er slightly from executable to executable on the same op-\nerating system. But the image is usually comprised of at least the following \nfour segments:\n1. Text segment. Sometimes called the code segment, this block contains  ex-\necutable machine code for all functions deﬁ ned by the program.\n2. Data segment. This segment contains all initialized global and static vari-\nables. The memory needed for each global variable is laid out exactly \nas it will appear when the program is run, and the proper initial values \nare all ﬁ lled in. So when the executable ﬁ le is loaded into memory, the \ninitialized  global and  static variables are ready to go.\n3. BSS  segment. “BSS” is an outdated name which stands for “block started \nby symbol.” This segment contains all of the uninitialized global and stat-\nic variables deﬁ ned by the program. The C and C++ languages explicitly \ndeﬁ ne the initial value of any uninitialized global or static variable to be \nzero. But rather than storing a potentially very large block of zeros in \nthe BSS section, the linker simply stores a count of how many zero bytes \nare required to account for all of the uninitialized globals and statics in \nthe segment. When the executable is loaded into memory, the operating \nsystem reserves the requested number of bytes for the BSS section and \nﬁ lls it with zeros prior to calling the program’s entry point (e.g. main()\nor WinMain()).\n4. Read-only data segment. Sometimes called the rodata segment, this seg-\nment contains any read-only (constant) global data deﬁ ned by the pro-\ngram. For example, all ﬂ oating-point constants (e.g., const float kPi \n3.2. Data, Code, and Memory in C/C++\n",
      "content_length": 2650,
      "extraction_method": "Direct"
    },
    {
      "page_number": 138,
      "chapter": null,
      "content": "116 \n3. Fundamentals of Software Engineering for Games\n= 3.141592f;) and all global object instances that have been declared \nwith the const keyword (e.g., const Foo gReadOnlyFoo; ) reside in \nthis segment. Note that integer constants (e.g., const int kMaxMon-\nsters = 255; ) are oft en used as manifest constants by the compiler, \nmeaning that they are inserted directly into the machine code wherever \nthey are used. Such constants occupy storage in the text segment, but \nthey are not present in the read-only data segment.\nGlobal variables, i.e., variables deﬁ ned at ﬁ le  scope outside any function or \nclass declaration, are stored in either the data or BSS segments, depending on \nwhether or not they have been initialized. The following global will be stored \nin the data segment, because it has been initialized:\nfoo.cpp\nF32 gInitializedGlobal = -2.0f;\nand the following global will be allocated and initialized to zero by the operat-\ning system , based on the speciﬁ cations given in the BSS segment, because it \nhas not been initialized by the programmer:\nfoo.cpp\nF32 gUninitializedGlobal;\nWe’ve seen that the static keyword can be used to give a global vari-\nable or function deﬁ nition internal linkage, meaning that it will be “hidden” \nfrom other translation units. The static keyword can also be used to declare \na global variable within a function. A function-static variable is lexically scoped \nto the function in which it is declared (i.e., the variable’s name can only be \n“seen” inside the function). It is initialized the ﬁ rst time the function is called \n(rather than before main() is called as with ﬁ le-scope statics). But in terms of \nmemory layout in the executable image, a function-static variable acts identi-\ncally to a ﬁ le-static global variable—it is stored in either the data or BSS seg-\nment based on whether or not it has been initialized.\nvoid readHitchhikersGuide(U32 book)\n{\n \nstatic U32 sBooksInTheTrilogy = 5; \n// data segment\n static \nU32 sBooksRead;     // \nBSS segment\n \n// ...\n}\n3.2.3.2. Program Stack\nWhen an executable program is loaded into memory and run, the operating \nsystem reserves an area of memory for the program  stack. Whenever a function \nis called, a contiguous area of stack memory is pushed onto the stack—we call \nthis block of memory a  stack frame. If function a() calls another function b(), \n",
      "content_length": 2364,
      "extraction_method": "Direct"
    },
    {
      "page_number": 139,
      "chapter": null,
      "content": "117 \na new stack frame for b() is pushed on top of a()’s frame. When b() returns, \nits stack frame is popped, and execution continues wherever a() left  oﬀ .\nA stack frame stores three kinds of data:\n1. It stores the  return address of the calling function, so that execution may \ncontinue in the calling function when the called function returns.\n2. The contents of all relevant CPU  registers are saved in the stack frame. \nThis allows the new function to use the registers in any way it sees ﬁ t, \nwithout fear of overwriting data needed by the calling function. Upon \nreturn to the calling function, the state of the registers is restored so that \nexecution of the calling function may resume. The return value of the \ncalled function, if any, is usually left  in a speciﬁ c register so that the call-\ning function can retrieve it, but the other registers are restored to their \noriginal values.\n3. The stack frame also contains all  local variables declared by the func-\ntion; these are also known as  automatic variables. This allows each dis-\ntinct function invocation to maintain its own private copy of every local \nvariable, even when a function calls itself recursively. (In practice, some \nlocal variables are actually allocated to CPU registers rather than being \nstored in the stack frame but, for the most part, such variables operate as \nif they were allocated within the function’s stack frame.) For example:\n3.2. Data, Code, and Memory in C/C++\na()’s\nstack\nframe\nsaved CPU registers\nreturn address\naLocalsA1[5]\nlocalA2\na()’s\nstack\nframe\nsaved CPU registers\nreturn address\naLocalsA1[5]\nlocalA2\na()’s\nstack\nframe\nsaved CPU registers\nreturn address\naLocalsA1[5]\nlocalA2\nb()’s\nstack\nframe\nsaved CPU registers\nreturn address\nlocalB1\nlocalB2\nb()’s\nstack\nframe\nsaved CPU registers\nreturn address\nlocalB1\nlocalB2\nsaved CPU registers\nreturn address\nlocalC1\nc()’s\nstack\nframe\nfunction a() is called\nfunction b() is called\nfunction c() is called\nFigure 3.10.  Stack frames.\n",
      "content_length": 1981,
      "extraction_method": "Direct"
    },
    {
      "page_number": 140,
      "chapter": null,
      "content": "118 \n3. Fundamentals of Software Engineering for Games\n \nvoid someFunction()\n {\n  U32 \nanInteger;\n  // \n...\n }\nPushing and popping stack frames is usually implemented by adjusting \nthe value of a single register in the CPU, known as the stack pointer. Figure \n3.10 illustrates what happens when the functions shown below are executed.\nvoid c()\n{\n \nU32 localC1;\n \n// ...\n}\nF32 b()\n{\n \nF32 localB1;\n \nI32 localB2;\n \n// ... \n \nc(); \n// call function c()\n \n// ...\n \nreturn localB1;\n}\nvoid a()\n{\n \nU32 aLocalsA1[5];\n \n// ...\n \nF32 localA2 = b();  \n// call function b()\n \n// ...\n}\nWhen a function containing automatic variables returns, its stack frame \nis abandoned and all automatic variables in the function should be treated as \nif they no longer exist. Technically, the memory occupied by those variables \nis still there in the abandoned stack frame—but that memory will very likely \nbe overwritt en as soon as another function is called. A common error involves \nreturning the address of a local variable, like this:\n",
      "content_length": 1017,
      "extraction_method": "Direct"
    },
    {
      "page_number": 141,
      "chapter": null,
      "content": "119 \nU32* getMeaningOfLife()\n{\n \nU32 anInteger = 42;\n \nreturn &anInteger;\n}\nYou might get away with this if you use the returned pointer immediately and \ndon’t call any other functions in the interim. But more oft en than not, this kind \nof code will crash—in ways that can be diﬃ  cult to debug.\n3.2.3.3. Dynamic Allocation Heap\nThus far, we’ve seen that a program’s data can be stored as global or static \nvariables or as local variables. The globals and statics are allocated within the \nexecutable image, as deﬁ ned by the data and BSS segments of the executable \nﬁ le. The locals are allocated on the program stack. Both of these kinds of stor-\nage are statically deﬁ ned, meaning that the size and layout of the memory \nis known when the program is compiled and linked. However, a program’s \nmemory requirements are oft en not fully known at compile time. A program \nusually needs to allocate additional memory dynamically.\nTo allow for  dynamic allocation, the operating system maintains a block \nof memory that can be allocated by a running program by calling malloc()\nand later returned to the pool for use by other programs by calling free(). \nThis memory block is known as  heap memory, or the free store. When we al-\nlocate memory dynamically, we sometimes say that this memory resides on \nthe heap.\nIn C++, the global  new and  delete operators are used to allocate and free \nmemory to and from the heap. Be wary, however—individual classes may \noverload these operators to allocate memory in custom ways, and even the \nglobal new and delete operators can be overloaded, so you cannot simply as-\nsume that new is always allocating from the heap.\nWe will discuss dynamic memory allocation in more depth in Chap-\nter 6. For additional information, see htt p://en.wikipedia.org/wiki/Dynamic_\nmemory_allocation.\n3.2.4. Member Variables\nC structs and C++  classes allow  variables to be grouped into logical units. \nIt’s important to remember that a class or struct declaration allocates no \nmemory. It is merely a description of the layout of the data—a cookie cutt er \nwhich can be used to stamp out  instances of that struct or class later on. \nFor example:\n3.2. Data, Code, and Memory in C/C++\n",
      "content_length": 2206,
      "extraction_method": "Direct"
    },
    {
      "page_number": 142,
      "chapter": null,
      "content": "120 \n3. Fundamentals of Software Engineering for Games\nstruct Foo \n \n// struct declaration\n{\n U32 \n mUnsignedValue;\n F32 \n mFloatValue;\n \nbool mBooleanValue;\n};\nOnce a struct or class has been declared, it can be allocated (deﬁ ned) in \nany of the ways that an atomic data type can be allocated, for example,\nz as an automatic variable, on the program stack;\n \nvoid someFunction()\n {\n  Foo \nlocalFoo;\n  // \n...\n }\nz as a global, ﬁ le-static or function-static;\n \nFoo gFoo;\n \nstatic Foo sFoo;\n \nvoid someFunction()\n {\n \n \nstatic Foo sLocalFoo;\n  // \n...\n }\nz dynamically allocated from the heap. In this case, the pointer or refer-\nence variable used to hold the address of the data can itself be allocated \nas an automatic, global, static, or even dynamically.\n \nFoo* gpFoo = NULL; // global pointer to a Foo\n \nvoid someFunction()\n {\n \n \n// allocate a Foo instance from the heap\n \n \ngpFoo = new Foo;\n  // \n...\n \n \n// allocate another Foo, assign to local  \n \n \n \n  // \npointer\n \n \nFoo* pAnotherFoo = new Foo;\n  // \n...\n \n \n// allocate a POINTER to a Foo from the heap\n \n \nFoo** ppFoo = new Foo*;\n \n \n(*ppFoo) = pAnotherFoo;\n }\n",
      "content_length": 1127,
      "extraction_method": "Direct"
    },
    {
      "page_number": 143,
      "chapter": null,
      "content": "121 \n3.2.4.1. Class-Static Members\nAs we’ve seen, the  static keyword has many diﬀ erent meanings depending \non context:\nz When used at ﬁ le scope, static means “restrict the visibility of this \nvariable or function so it can only be seen inside this .cpp ﬁ le.”\nz When used at function scope, static means “this variable is a global, \nnot an automatic, but it can only be seen inside this function.”\nz When used inside a struct or  class declaration, static means “this \nvariable is not a regular member variable, but instead acts just like a \nglobal.”\nNotice that when static is used inside a class declaration, it does not \ncontrol the visibility of the variable (as it does when used at ﬁ le scope)—\nrather, it diﬀ erentiates between regular per-instance member variables \nand per-class variables that act like globals. The visibility of a class-static \nvariable is determined by the use of public:, protected: or private:\nkeywords in the class declaration. Class-static variables are automatically \nincluded within the namespace of the class or struct in which they are \ndeclared. So the name of the class or struct must be used to disambigu-\nate the variable whenever it is used outside that class or struct (e.g., \nFoo::sVarName).\nLike an extern declaration for a regular global variable, the declaration \nof a class-static variable within a class allocates no memory. The memory for \nthe class-static variable must be deﬁ ned in a .cpp ﬁ le. For example:\nfoo.h\nclass Foo\n{\npublic:\n \nstatic F32 sClassStatic; \n \n// allocates no   \n \n \n          // \nmemory!\n};\nfoo.cpp\nF32 Foo::sClassStatic = -1.0f; \n// define memory and  \n \n          // \ninit\n3.2.5. Object Layout in Memory\nIt’s useful to be able to visualize the memory layout of your classes and \nstructs. This is usually prett y straightforward—we can simply draw a box \nfor the struct or class, with horizontal lines separating data members. An \n3.2. Data, Code, and Memory in C/C++\n",
      "content_length": 1945,
      "extraction_method": "Direct"
    },
    {
      "page_number": 144,
      "chapter": null,
      "content": "122 \n3. Fundamentals of Software Engineering for Games\nexample of such a diagram for the struct Foo listed below is shown in Fig-\nure 3.11.\nstruct Foo\n{\n U32 \n  mUnsignedValue;\n F32 \n  mFloatValue;\n I32 \n  mSignedValue;\n};\nThe sizes of the data members are important and should be represented \nin your diagrams. This is easily done by using the width of each data member \nto indicate its size in bits—i.e., a 32-bit integer should be roughly four times \nthe width of an 8-bit integer (see Figure 3.12).\nstruct Bar\n{\n U32 \n  mUnsignedValue;\n F32 \n  mFloatValue;\n \nbool   mBooleanValue; // diagram assumes this is 8 bits\n};\n3.2.5.1. Alignment and Packing\nAs we start to think more carefully about the layout of our structs and classes \nin memory, we may start to wonder what happens when small data members \nare interspersed with larger members. For example:\nstruct InefficientPacking\n{\n U32 \n \nmU1; \n// 32 bits\n F32 \n \nmF2; \n// 32 bits\n \nU8  \nmB3; \n// 8 bits\n I32 \n \nmI4; \n// 32 bits\n \nbool \nmB5; \n// 8 bits\n \nchar* mP6; \n// 32 bits\n};\nYou might imagine that the compiler simply  packs the data members into \nmemory as tightly as it can. However, this is not usually the case. Instead, \nthe compiler will typically leave “holes” in the layout, as depicted in Fig-\nure 3.13. (Some compilers can be requested not to leave these holes by us-\ning a preprocessor directive like #pragma pack , or via command-line op-\ntions; but the default behavior is to space out the members as shown in Fig-\nure 3.13.)\nmU1\nmF2\nmB3\nmI4\nmB5\nmP6\n+0x0\n+0x4\n+0x8\n+0xC\n+0x10\n+0x14\nFigure 3.13.  Inefﬁ cient \nstruct packing due to \nmixed data member \nsizes.\nmUnsignedValue\nmFloatValue\nmSignedValue\n+0x0\n+0x4\n+0x8\nFigure 3.11.  Memory \nlayout of a simple \nstruct.\nmUnsignedValue\nmFloatValue\nmBooleanValue\n+0x0\n+0x4\n+0x8\nFigure 3.12.  A memory \nlayout using width to \nindicate member sizes.\n",
      "content_length": 1862,
      "extraction_method": "Direct"
    },
    {
      "page_number": 145,
      "chapter": null,
      "content": "123 \nWhy does the compiler leave these “holes?” The reason lies in the fact that \nevery data type has a natural  alignment which must be respected in order to \npermit the CPU to read and write memory eﬀ ectively. The alignment of a data \nobject refers to whether its address in memory is a multiple of its size (which is \ngenerally a power of two):\nz An object with one-byte alignment resides at any memory address.\nz An object with two-byte alignment resides only at even addresses (i.e., \naddresses whose least signiﬁ cant nibble is 0x0, 0x2, 0x4, 0x8, 0xA, 0xC, \nor 0xE).\nz An object with four-byte alignment resides only at addresses that are a \nmultiple of four (i.e., addresses whose least signiﬁ cant nibble is 0x0, 0x4, \n0x8, or 0xC).\nz A 16-byte aligned object resides only at addresses that are a multiple of \n16 (i.e., addresses whose least signiﬁ cant nibble is 0x0).\nAlignment is important because many modern processors can actually \nonly read and write properly aligned blocks of data. For example, if a program \nrequests that a 32-bit (four-byte) integer be read from address 0x6A341174, the \nmemory controller will load the data happily because the address is four-byte \naligned (in this case, its least signiﬁ cant nibble is 0x4). However, if a request is \nmade to load a 32-bit integer from address 0x6A341173, the memory control-\nler now has to read two four-byte blocks: the one at 0x6A341170 and the one \nat 0x6A341174. It must then mask and shift  the two parts of the 32-bit integer \nand logically OR them together into the destination register on the CPU. This \nis shown in Figure 3.14.\nSome microprocessors don’t even go this far. If you request a read or write \nof unaligned data, you might just get garbage. Or your program might just \ncrash altogether! (The PlayStation 2 is a notable example of this kind of intol-\nerance for unaligned data.)\nDiﬀ erent data types have diﬀ erent alignment requirements. A good rule \nof thumb is that a data type should be aligned to a boundary equal to the \nwidth of the data type in bytes. For example, 32-bit values generally have a \nfour-byte alignment requirement, 16-bit values should be two-byte aligned, \nand 8-bit values can be stored at any address (one-byte aligned). On CPUs that \nsupport SIMD vector math, the SIMD registers each contain four 32-bit ﬂ oats, \nfor a total of 128 bits or 16 bytes. And as you would guess, a four-ﬂ oat SIMD \nvector typically has a 16-byte alignment requirement.\nThis brings us back to those “holes” in the layout of struct Ineffi-\ncientPacking shown in Figure 3.13. When smaller data types like 8-bit bools \nare interspersed with larger types like 32-bit integers or floats in a structure \n3.2. Data, Code, and Memory in C/C++\n",
      "content_length": 2733,
      "extraction_method": "Direct"
    },
    {
      "page_number": 146,
      "chapter": null,
      "content": "124 \n3. Fundamentals of Software Engineering for Games\nor class, the compiler introduces padding (holes) in order to ensure that every-\nthing is properly aligned. It’s a good idea to think about alignment and pack-\ning when declaring your data structures. By simply rearranging the members \nof struct InefficientPacking from the example above, we can eliminate \nsome of the wasted padding space, as shown below and in Figure 3.15:\n \nstruct MoreEfficientPacking\n {\n  U32 \n \nmU1; \n// 32 bits (4-byte aligned)\n  F32 \n \nmF2; \n// 32 bits (4-byte aligned)\n  I32 \n \nmI4; \n// 32 bits (4-byte aligned)\n \n \nchar* mP6; \n// 32 bits (4-byte aligned)\n \n \nU8  \nmB3; \n// 8 bits (1-byte aligned)\n \n \nbool \nmB5; \n// 8 bits (1-byte aligned)\n };\nYou’ll notice in Figure 3.15 that the size of the structure as a whole is \nnow 20 bytes, not 18 bytes as we might expect, because it has been padded \nby two bytes at the end. This padding is added by the compiler to ensure \nproper alignment of the structure in an array context. That is, if an array of \nthese structs is deﬁ ned and the ﬁ rst element of the array is aligned, then the \npadding at the end guarantees that all subsequent elements will also be aligned \nproperly.\nThe alignment of a structure as a whole is equal to the largest alignment \nrequirement among its members. In the example above, the largest mem-\nber alignment is four-byte, so the structure as a whole should be four-byte \nCPU\nalignedValue\n0x6A341170\n0x6A341174\n0x6A341178\nregister\n-alignedValue\n0x6A341170\n0x6A341174\n0x6A341178\nun-\n-alignedValue\nun-\nshift\nshift\n-alignedValue\nun-\nAligned read from\n0x6A341174\nUnaligned read from\n0x6A341173\nCPU\nregister\nFigure 3.14.  Aligned and unaligned reads of a 32-bit integer.\n(pad)\nmU1\nmF2\nmB3\nmI4\nmB5\nmP6\n+0x0\n+0x4\n+0x8\n+0xC\n+0x10\nFigure 3.15.  More ef-\nﬁ cient \npacking \nby \ngrouping small mem-\nbers together.\n",
      "content_length": 1855,
      "extraction_method": "Direct"
    },
    {
      "page_number": 147,
      "chapter": null,
      "content": "125 \naligned. I usually like to add explicit padding to the end of my structs, to make \nthe wasted space visible and explicit, like this:\n \nstruct BestPacking\n {\n \n U32  \nmU1; \n \n// 32 bits (4-byte aligned)\n \n F32  \nmF2; \n \n// 32 bits (4-byte aligned)\n \n I32  \nmI4; \n \n// 32 bits (4-byte aligned)\n \n char* \nmP6; \n \n// 32 bits (4-byte aligned)\n \n U8 \n \nmB3; \n \n// 8 bits (1-byte aligned)\n \n bool     mB5; \n \n// 8 bits (1-byte aligned)\nU8\n_pad[2]; // explicit padding\n };\n3.2.5.2. Memory Layout of C++ Classes\nTwo things make C++ classes a litt le diﬀ erent from C structures in terms of \nmemory layout:  inheritance and virtual functions. \nWhen class B inherits from class A, B’s data members simply appear im-\nmediately aft er A’s in memory, as shown in Figure 3.16. Each new derived \nclass simply tacks its data members on at the end, although alignment re-\nquirements may introduce padding between the classes. (Multiple inheritance \ndoes some whacky things, like including multiple copies of a single base class \nin the memory layout of a derived class. We won’t cover the details here, be-\ncause game programmers usually prefer to avoid  multiple inheritance alto-\ngether anyway.)\nIf a class contains or inherits one or more virtual functions, then four ad-\nditional bytes (or however many bytes a pointer occupies on the target hard-\nware) are added to the class layout, typically at the very beginning of the \nclass’ layout. These four bytes are collectively called the virtual table pointer \nor vpointer, because they contain a pointer to a data structure known as the \nvirtual function table or vtable. The vtable for a particular class contains pointers \nto all the virtual functions that it declares or inherits. Each concrete class has \nits own virtual table, and every instance of that class has a pointer to it, stored \nin its vpointer.\nThe virtual function table is at the heart of  polymorphism, because it al-\nlows code to be writt en that is ignorant of the speciﬁ c concrete classes it is deal-\ning with. Returning to the ubiquitous example of a Shape base class with de-\nrived classes for Circle, Rectangle, and Triangle, let’s imagine that Shape\ndeﬁ nes a virtual function called Draw(). The derived classes all override \nthis function, providing distinct implementations named Circle::Draw(), \nRectangle::Draw(), and Triangle::Draw(). The virtual table for any \nclass derived from Shape will contain an entry for the Draw() function, but \nthat entry will point to diﬀ erent function implementations, depending on the \n3.2. Data, Code, and Memory in C/C++\nA\nB\n+0x0\n+sizeof(A)\nFigure 3.16.  Effect of \ninheritance on class \nlayout.\n",
      "content_length": 2651,
      "extraction_method": "Direct"
    },
    {
      "page_number": 148,
      "chapter": null,
      "content": "126 \n3. Fundamentals of Software Engineering for Games\nconcrete class. Circle’s vtable will contain a pointer to Circle::Draw(), \nwhile Rectangle’s virtual table will point to Rectangle::Draw(), and Tri-\nangle’s vtable will point to Triangle::Draw(). Given an arbitrary point-\ner to a Shape (Shape* pShape), the code can simply dereference the vtable \npointer, look up the Draw() function’s entry in the vtable, and call it. The \nresult will be to call Circle::Draw() when pShape points to an instance \nof Circle, Rectangle::Draw() when pShape points to a Rectangle, and \nTriangle::Draw() when pShape points to a Triangle.\nThese ideas are illustrated by the following code excerpt. Notice that the \nbase class Shape deﬁ nes two virtual functions, SetId() and Draw(), the lat-\nter of which is declared to be pure virtual. (This means that Shape provides \nno default implementation of the Draw() function, and derived classes must \noverride it if they want to be instantiable.) Class Circle derives from Shape, \nadds some data members and functions to manage its center and radius, and \noverrides the Draw()function; this is depicted in Figure 3.17. Class Triangle\nalso derives from Shape. It adds an array of Vector3 objects to store its three \nvertices and adds some functions to get and set the individual vertices. Class \nTriangle overrides Draw() as we’d expect, and for illustrative purposes it \nalso overrides SetId(). The memory image generated by the Triangle class \nis shown in Figure 3.18.\nclass Shape\n{\npublic:\nvirtual void  SetId(int id) { m_id = id; }\n \nint  \n \n \nGetId() const { return m_id; }\nvirtual void  Draw() = 0; // pure virtual – no impl.\nprivate:\n int \n \n  m_id;\n};\nShape::m_id\nCircle::m_center\nCircle::m_radius\nvtable pointer\npointer to SetId()\npointer to Draw ()\n+0x00\n+0x04\n+0x08\n+0x14\npShape1\nInstance of Circle\nCircle’s Virtual Table\nCircle::Draw()\n{\n    // code to draw a Circle\n}\nShape::SetId(int id )\n{\n    m_id = id;\n}\nFigure 3.17.  pShape1 points to an instance of class Circle.\n",
      "content_length": 2011,
      "extraction_method": "Direct"
    },
    {
      "page_number": 149,
      "chapter": null,
      "content": "127 \nclass Circle : public Shape\n{\npublic:\n    void   \nSetCenter(const Vector3& c) { m_center=c; }\n \nVector3  GetCenter() const { return m_center; }\n \nvoid   \nSetRadius(float r) { m_radius = r; }\n \nfloat   \nGetRadius() const { return m_radius; }\nvirtual void Draw()\n {\n \n \n// code to draw a circle\n }\nprivate:\n Vector3 \n  m_center;\n float \n  m_radius;\n};\nclass Triangle : public Shape\n{\npublic:\n \nvoid \n \nSetVertex(int i, const Vector3& v);\n \nVector3 \nGetVertex(int i) const { return m_vtx[i]; }\nvirtual void Draw()\n {\n \n \n// code to draw a triangle\n }\nvirtual void SetId(int id)\n {\n  Shape::SetId(id);\nFigure 3.18.  pShape2 points to an instance of class Triangle.\nShape::m_id\nTriangle::m_vtx[0]\nTriangle::m_vtx[1]\nvtable pointer\npointer to SetId()\npointer to Draw ()\n+0x00\n+0x04\n+0x08\n+0x14\npShape2\nInstance of Triangle\nTriangle’s Virtual Table\nTriangle ::Draw()\n{\n    // code to draw a Triangle\n}\nTriangle ::SetId(int id)\n{\n    Shape ::SetId(id);\n    // do additional work\n    // specific to Triangles\n}\nTriangle::m_vtx[2]\n+0x20\n3.2. Data, Code, and Memory in C/C++\n",
      "content_length": 1069,
      "extraction_method": "Direct"
    },
    {
      "page_number": 150,
      "chapter": null,
      "content": "128 \n3. Fundamentals of Software Engineering for Games\n \n \n// do additional work specific to Triangles...\n }\nprivate:\n Vector3 \n  m_vtx[3];\n};\n// -----------------------------\nvoid main(int, char**)\n{\nShape* pShape1 = new Circle;\n \nShape* pShape2 = new Triangle;\n \n// ...\n pShape1->Draw();\n pShape2->Draw();\n \n// ...\n}\n3.3. Catching and Handling Errors\nThere are a number of ways to catch and handle error conditions in a game \nengine. As a game programmer, it’s important to understand these diﬀ erent \nmechanisms, their pros and cons, and when to use each one.\n3.3.1. \nTypes of Errors\nIn any soft ware project there are two basic kinds of  error conditions: user er-\nrors and programmer errors. A user error occurs when the user of the program \ndoes something incorrect, such as typing an invalid input, att empting to open \na ﬁ le that does not exist, etc. A programmer error is the result of a bug in the \ncode itself. Although it may be triggered by something the user has done, the \nessence of a programmer error is that the problem could have been avoided if \nthe programmer had not made a mistake, and the user has a reasonable expec-\ntation that the program should have handled the situation gracefully.\nOf course, the deﬁ nition of “user” changes depending on context. In the \ncontext of a game project, user errors can be roughly divided into two catego-\nries: errors caused by the person playing the game and errors caused by the \npeople who are making the game during development. It is important to keep \ntrack of which type of user is aﬀ ected by a particular error and handle the er-\nror appropriately.\n",
      "content_length": 1619,
      "extraction_method": "Direct"
    },
    {
      "page_number": 151,
      "chapter": null,
      "content": "129 \nThere’s actually a third kind of user—the other programmers on your \nteam. (And if you are writing a piece of game middleware soft ware, like \nHavok or OpenGL, this third category extends to other programmers all over \nthe world who are using your library.) This is where the line between user er-\nrors and programmer errors gets blurry. Let’s imagine that programmer A writes \na function f(), and programmer B tries to call it. If B calls f() with invalid \narguments (e.g., a NULL pointer, or an out-of-range array index), then this \ncould be seen as a user error by programmer A, but it would be a program-\nmer error from B’s point of view. (Of course, one can also argue that program-\nmer A should have anticipated the passing of invalid arguments and should \nhave handled them gracefully, so the problem really is a programmer error, \non A’s part.) The key thing to remember here is that the line between user and \nprogrammer can shift  depending on context—it is rarely a black-and-white \ndistinction.\n3.3.2. Handling Errors\nWhen handling errors, the requirements diﬀ er signiﬁ cantly between the two \ntypes. It is best to handle user errors as gracefully as possible, displaying some \nhelpful information to the user and then allowing him or her to continue \nworking—or in the case of a game, to continue playing. Programmer errors, \non the other hand, should not be handled with a graceful “inform and contin-\nue” policy. Instead, it is usually best to halt the program and provide detailed \nlow-level debugging information, so that a programmer can quickly identify \nand ﬁ x the problem. In an ideal world, all programmer errors would be caught \nand ﬁ xed before the soft ware ships to the public.\n3.3.2.1. Handling Player Errors\nWhen the “user” is the person playing your game, errors should obviously be \nhandled within the context of gameplay. For example, if the player att empts to \nreload a weapon when no ammo is available, an audio cue and/or an anima-\ntion can indicate this problem to the player without taking him or her “out of \nthe game.” \n3.3.2.2. Handling Developer Errors\nWhen the “user” is someone who is making the game, such as an artist, ani-\nmator or game designer, errors may be caused by an invalid asset of some sort. \nFor example, an animation might be associated with the wrong skeleton, or a \ntexture might be the wrong size, or an audio ﬁ le might have been sampled at \nan unsupported sample rate. For these kinds of developer errors, there are two \ncompeting camps of thought.\n3.3. Catching and Handling Errors\n",
      "content_length": 2553,
      "extraction_method": "Direct"
    },
    {
      "page_number": 152,
      "chapter": null,
      "content": "130 \n3. Fundamentals of Software Engineering for Games\nOn the one hand, it seems important to prevent bad game assets from \npersisting for too long. A game typically contains literally thousands of assets, \nand a problem asset might get “lost,” in which case one risks the possibility of \nthe bad asset surviving all the way into the ﬁ nal shipping game. If one takes \nthis point of view to an extreme, then the best way to handle bad game assets \nis to prevent the entire game from running whenever even a single problem-\natic asset is encountered. This is certainly a strong incentive for the developer \nwho created the invalid asset to remove or ﬁ x it immediately.\nOn the other hand, game development is a messy and iterative process, \nand generating “perfect” assets the ﬁ rst time around is rare indeed. By this \nline of thought, a game engine should be robust to almost any kind of problem \nimaginable, so that work can continue even in the face of totally invalid game \nasset data. But this too is not ideal, because the game engine would become \nbloated with error-catching and error-handling code that won’t be needed \nonce the development pace sett les down and the game ships. And the prob-\nability of shipping the product with “bad” assets becomes too high.\nIn my experience, the best approach is to ﬁ nd a middle ground between \nthese two extremes. When a developer error occurs, I like to make the error \nobvious and then allow the team to continue to work in the presence of the \nproblem. It is extremely costly to prevent all the other developers on the team \nfrom working, just because one developer tried to add an invalid asset to the \ngame. A game studio pays its employees well, and when multiple team mem-\nbers experience downtime, the costs are multiplied by the number of people \nwho are prevented from working. Of course, we should only handle errors in \nthis way when it is practical to do so, without spending inordinate amounts of \nengineering time, or bloating the code.\nAs an example, let’s suppose that a particular mesh cannot be loaded. In \nmy view, it’s best to draw a big red box in the game world at the places that \nmesh would have been located, perhaps with a text string hovering over each \none that reads, “Mesh blah-dee-blah failed to load.” This is superior to printing \nan easy-to-miss message to an error log. And it’s far bett er than just crashing \nthe game, because then no one will be able to work until that one mesh refer-\nence has been repaired. Of course, for particularly egregious problems it’s ﬁ ne \nto just spew an error message and crash. There’s no silver bullet for all kinds \nof problems, and your judgment about what type of error handling approach \nto apply to a given situation will improve with experience.\n3.3.2.3. Handling Programmer Errors\nThe best way to detect and handle programmer errors (a.k.a. bugs) is oft en \nto embed error-checking code into your source code and arrange for failed \n",
      "content_length": 2960,
      "extraction_method": "Direct"
    },
    {
      "page_number": 153,
      "chapter": null,
      "content": "131 \nerror checks to halt the program. Such a mechanism is known as an assertion \nsystem; we’ll investigate assertions in detail in Section 3.3.3.3. Of course, as we \nsaid above, one programmer’s user error is another programmer’s bug; hence, \nassertions are not always the right way to handle every programmer error. \nMaking a judicious choice between an assertion and a more graceful error \nhandling technique is a skill that one develops over time.\n3.3.3. Implementation of Error Detection and Handling\nWe’ve looked at some philosophical approaches to handling errors. Now let’s \nturn our att ention to the choices we have as programmers when it comes to \nimplementing error detection and handling code.\n3.3.3.1. \nError Return Codes\nA common approach to handling errors is to return some kind of failure code \nfrom the function in which the problem is ﬁ rst detected. This could be a Bool-\nean value indicating success or failure or it could be an “impossible” value, \none that is outside the range of normally returned results. For example, a \nfunction that returns a positive integer or ﬂ oating-point value could return a \nnegative value to indicate that an error occurred. Even bett er than a Boolean or \nan “impossible” return value, the function could be designed to return an enu-\nmerated value to indicate success or failure. This clearly separates the error \ncode from the output(s) of the function, and the exact nature of the problem \ncan be indicated on failure (e.g., enum Error { kSuccess, kAssetNot-\nFound, kInvalidRange, ... };).\nThe calling function should intercept error return codes and act appro-\npriately. It might handle the error immediately. Or it might work around the \nproblem, complete its own execution, and then pass the error code on to what-\never function called it.\n3.3.3.2. Exceptions\nError return codes are a simple and reliable way to communicate and respond \nto error conditions. However, error return codes have their drawbacks. Per-\nhaps the biggest problem with error return codes is that the function that \ndetects an error may be totally unrelated to the function that is capable of \nhandling the problem. In the worst-case scenario, a function that is 40 calls \ndeep in the call stack might detect a problem that can only be handled by \nthe top-level game loop, or by main(). In this scenario, every one of the 40 \nfunctions on the call stack would need to be writt en so that it can pass an \nappropriate error code all the way back up to the top-level error-handling \nfunction.\n3.3. Catching and Handling Errors\n",
      "content_length": 2559,
      "extraction_method": "Direct"
    },
    {
      "page_number": 154,
      "chapter": null,
      "content": "132 \n3. Fundamentals of Software Engineering for Games\nOne way to solve this problem is to throw an exception.  Structured excep-\ntion handling (SEH) is a very powerful feature of C++. It allows the function \nthat detects a problem to communicate the error to the rest of the code with-\nout knowing anything about which function might handle the error. When an \nexception is thrown, relevant information about the error is placed into a data \nobject of the programmer’s choice known as an exception object. The call stack \nis then automatically unwound, in search of a calling function that wrapped \nits call in a try-catch block. If a try-catch block is found, the exception object \nis matched against all possible catch blocks and if a match is found, the cor-\nresponding catch block’s code is executed. The destructors of any automatic \nvariables are called as needed during the stack unwinding.\nThe ability to separate error detection from error handling in such a clean \nway is certainly att ractive, and exception handling is an excellent choice for \nsome soft ware projects. However, SEH adds a lot of overhead to the program. \nEvery stack frame must be augmented to contain additional information re-\nquired by the stack unwinding process. Also, the stack unwind is usually very \nslow—on the order of two to three times more expensive than simply return-\ning from the function. Also, if even one function in your program (or a library \nthat your program links with) uses SEH, your entire program must use SEH. \nThe compiler can’t know which functions might be above you on the call stack \nwhen you throw an exception.\nTherefore, there’s a prett y strong argument for turning oﬀ  structured ex-\nception handling in your game engine altogether. This is the approach em-\nployed at Naughty Dog and also on most of the projects I’ve worked on at \nElectronic Arts and Midway. Console game engines should probably never \nuse SEH, because of a console’s limited memory and processing bandwidth. \nHowever, a game engine that is intended to be run on a personal computer \nmight be able to use SEH without any problems.\nThere are many interesting articles on this topic on the web. Here are links \nto a few of them:\nz htt p://www.joelonsoft ware.com/items/2003/10/13.html\nz htt p://www.nedbatchelder.com/text/exceptions-vs-status.html\nz htt p://www.joelonsoft ware.com/items/2003/10/15.html\n3.3.3.3. Assertions\nAn assertion is a line of code that checks an expression. If the expression evalu-\nates to true, nothing happens. But if the expression evaluates to false, the pro-\ngram is stopped, a message is printed, and the debugger is invoked if possible. \nSteve Maguire provides an in-depth discussion of assertions in his must-read \nbook, Writing Solid Code [30].\n",
      "content_length": 2763,
      "extraction_method": "Direct"
    },
    {
      "page_number": 155,
      "chapter": null,
      "content": "133 \nAssertions check a programmer’s assumptions. They act like land mines \nfor bugs. They check the code when it is ﬁ rst writt en to ensure that it is func-\ntioning properly. They also ensure that the original assumptions continue to \nhold for long periods of time, even when the code around them is constantly \nchanging and evolving. For example, if a programmer changes code that \nused to work, but accidentally violates its original assumptions, they’ll hit \nthe land mine. This immediately informs the programmer of the problem \nand permits him or her to rectify the situation with minimum fuss. Without \nassertions , bugs have a tendency to “hide out” and manifest themselves later \nin ways that are diﬃ  cult and time-consuming to track down. But with as-\nsertions embedded in the code, bugs announce themselves the moment they \nare introduced—which is usually the best moment to ﬁ x the problem, while \nthe code changes that caused the problem are fresh in the programmer’s \nmind.\nAssertions are implemented as a #define macro, which means that the \nassertion checks can be stripped out of the code if desired, by simply changing \nthe #define. The cost of the assertion checks can usually be tolerated during \ndevelopment, but stripping out the assertions prior to shipping the game can \nbuy back that litt le bit of crucial performance if necessary.\nAssertion Implementation\nAssertions are usually implemented via a combination of a #defined macro \nthat evaluates to an if/else clause, a function that is called when the asser-\ntion fails (the expression evaluates to false), and a bit of assembly code that \nhalts the program and breaks into the debugger when one is att ached. Here’s \na typical implementation:\n#if ASSERTIONS_ENABLED\n \n// define some inline assembly that causes a break     \n \n// into the debugger – this will be different on each  \n \n// target CPU\n \n#define debugBreak() asm { int 3 }\n \n// check the expression and fail if it is false\n \n#define ASSERT(expr) \\\n \n \nif (expr) { } \\\n  else \n\\\n  { \n\\\n   reportAssertionFailure(#expr, \n\\\n          \n__FILE__, \\\n          \n__LINE__); \\\n   debugBreak(); \n\\\n  }\n3.3. Catching and Handling Errors\n",
      "content_length": 2168,
      "extraction_method": "Direct"
    },
    {
      "page_number": 156,
      "chapter": null,
      "content": "134 \n3. Fundamentals of Software Engineering for Games\n#else\n \n#define ASSERT(expr) \n \n// evaluates to nothing\n#endif\nLet’s break down this deﬁ nition so we can see how it works:\nz The outer #if/#else/#endif is used to strip assertions from the \ncode base. When ASSERTIONS_ENABLED is nonzero, the ASSERT()\nmacro is deﬁ ned in its fully glory, and all assertion checks in the code \nwill be included in the program. But when assertions are turned oﬀ , \nASSERT(expr) evaluates to nothing, and all instances of it in the code \nare eﬀ ectively removed.\nz The debugBreak() macro evaluates to whatever assembly-language \ninstructions are required in order to cause the program to halt and the \ndebugger to take charge (if one is connected). This diﬀ ers from CPU to \nCPU, but it is usually a single assembly instruction.\nz The ASSERT() macro itself is deﬁ ned using a full if/else statement (as \nopposed to a lone if). This is done so that the macro can be used in any \ncontext, even within other unbracketed if/else statements.\nHere’s an example of what would happen if ASSERT() were deﬁ ned \nusing a solitary if:\n \n#define ASSERT(expr)  if (!(expr)) debugBreak()\n \nvoid f()\n {\n \n \nif (a < 5)\n   ASSERT(a \n>= 0);\n  else\n   doSomething(a);\n }\nThis expands to the following incorrect code:\n \nvoid f()\n {\n \n \nif (a < 5)\n \n \n \nif (!(a >= 0))\n    debugBreak();\n \n \n \nelse \n// Oops! Bound to the wrong if()!\n    doSomething(a);\n }\nz The else clause of an ASSERT() macro does two things. It displays \nsome kind of message to the programmer indicating what went wrong, \n",
      "content_length": 1556,
      "extraction_method": "Direct"
    },
    {
      "page_number": 157,
      "chapter": null,
      "content": "135 \nand then it breaks into the debugger. Notice the use of #expr as the ﬁ rst \nargument to the message display function. The pound (#) preprocessor \noperator causes the expression expr to be turned into a string, thereby \nallowing it to be printed out as part of the assertion failure message.\nz Notice also the use of __FILE__ and __LINE__. These compiler-deﬁ ned \nmacros magically contain the .cpp ﬁ le name and line number of the line \nof code on which they appear. By passing them into our message dis-\nplay function, we can print the exact location of the problem.\nI highly recommend the use of assertions in your code. However, it’s im-\nportant to be aware of their performance cost. You may want to consider de-\nﬁ ning two kinds of assertion macros. The regular ASSERT() macro can be left  \nactive in all builds, so that errors are easily caught even when not running \nin debug mode. A second assertion macro, perhaps called SLOW_ASSERT(), \ncould be activated only in debug builds. This macro could then be used in \nplaces where the cost of assertion checking is too high to permit inclusion \nin release builds. Obviously SLOW_ASSERT() is of lower utility, because it is \nstripped out of the version of the game that your testers play every day. But at \nleast these assertions become active when programmers are debugging their \ncode.\nIt’s also extremely important to use assertions properly. They should be \nused to catch bugs in the program itself—never to catch user errors. Also, as-\nsertions should always cause the entire game to halt when they fail. It’s usu-\nally a bad idea to allow assertions to be skipped by testers, artists, designers, \nand other non-engineers. (This is a bit like the boy who cried wolf: if assertions \ncan be skipped, then they cease to have any signiﬁ cance, rendering them inef-\nfective.) In other words, assertions should only be used to catch fatal errors. If \nit’s OK to continue past an assertion, then it’s probably bett er to notify the user \nof the error in some other way, such as with an on-screen message, or some \nugly bright-orange 3D graphics. For a great discussion on the proper usage of \nassertions, see htt p://www.wholesalealgorithms.com/blog9.\n3.3. Catching and Handling Errors\n",
      "content_length": 2240,
      "extraction_method": "Direct"
    },
    {
      "page_number": 158,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 159,
      "chapter": null,
      "content": "137\n4\n3D Math for Games\nA game is a mathematical model of a virtual world simulated in real-time on \na computer of some kind. Therefore, mathematics pervades everything we do \nin the game industry. Game programmers make use of virtually all branches \nof mathematics, from trigonometry to algebra to statistics to calculus. How-\never, by far the most prevalent kind of mathematics you’ll be doing as a game \nprogrammer is 3D vector and matrix math (i.e., 3D linear algebra ).\nEven this one branch of mathematics is very broad and very deep, so we \ncannot hope to cover it in any great depth in a single chapter. Instead, I will \natt empt to provide an overview of the mathematical tools needed by a typical \ngame programmer. Along the way, I’ll oﬀ er some tips and tricks which should \nhelp you keep all of the rather confusing concepts and rules straight in your \nhead. For an excellent in-depth coverage of 3D math for games, I highly rec-\nommend Eric Lengyel’s book on the topic [28].\n4.1. \nSolving 3D Problems in 2D\nMany of the mathematical operations we’re going to learn about in the follow-\ning chapter work equally well in 2D and 3D. This is very good news, because \nit means you can sometimes solve a 3D vector problem by thinking and draw-\ning pictures in 2D (which is considerably easier to do!) Sadly, this equivalence \n",
      "content_length": 1331,
      "extraction_method": "Direct"
    },
    {
      "page_number": 160,
      "chapter": null,
      "content": "138 \n4. 3D Math for Games\nFigure 4.1. A point rep-\nresented in Cartesian \ncoordinates.\nPy\nx\nz\ny\nPz\nPx\nP\nPh\nP\nh\nr\nPr\nθ\nPθ\nFigure \n4.2. \nA \npoint represent-\ned in cylindrical \ncoordinates.\nbetween 2D and 3D does not hold all the time. Some operations, like the cross \nproduct, are only deﬁ ned in 3D, and some problems only make sense when all \nthree dimensions are considered. Nonetheless, it almost never hurts to start by \nthinking about a simpliﬁ ed two-dimensional version of the problem at hand. \nOnce you understand the solution in 2D, you can think about how the prob-\nlem extends into three dimensions. In some cases, you’ll happily discover that \nyour 2D result works in 3D as well. In others, you’ll be able to ﬁ nd a coor-\ndinate system in which the problem really is two-dimensional. In this book, \nwe’ll employ two-dimensional diagrams wherever the distinction between 2D \nand 3D is not relevant.\n4.2. Points and Vectors\nThe majority of modern 3D games are made up of three-dimensional objects \nin a virtual world. A game engine needs to keep track of the positions, orien-\ntations, and scales of all these objects, animate them in the game world, and \ntransform them into screen space so they can be rendered on screen. In games, \n3D objects are almost always made up of triangles, the vertices of which are \nrepresented by points. So before we learn how to represent whole objects in \na game engine, let’s ﬁ rst take a look the point and its closely related cousin, \nthe vector.\n4.2.1. Points and Cartesian Coordinates\nTechnically speaking, a point is a location in n-dimensional space. (In games, \nn is usually equal to 2 or 3.) The Cartesian coordinate system is by far the \nmost common coordinate system employed by game programmers. It uses \ntwo or three mutually perpendicular axes to specify a position in 2D or 3D \nspace. So a point P is represented by a pair or triple of real numbers, (Px , Py) \nor (Px , Py , Pz).\nOf course, the Cartesian coordinate system is not our only choice. Some \nother common systems include:\nz Cylindrical coordinates . This system employs a vertical “height” axis h, a \nradial axis r emanating out from the vertical, and a yaw angle theta (θ). \nIn cylindrical coordinates, a point P is represented by the triple of num-\nbers (Ph , Pr , Pθ). This is illustrated in Figure 4.2.\nz Spherical coordinates . This system employs a pitch angle phi (φ), a yaw \nangle theta (θ), and a radial measurement r. Points are therefore rep-\nresented by the triple of numbers (Pr , Pφ , Pθ). This is illustrated in Fig-\nure 4.3.\n",
      "content_length": 2560,
      "extraction_method": "Direct"
    },
    {
      "page_number": 161,
      "chapter": null,
      "content": "139 \n4.2. Points and Vectors\nFigure \n4.3. A point \nrepresented in spherical \ncoordinates.\nr\nθ\nφ\nPr\nP\nPθ\nPφ\nRight-Handed\nx\nz\ny\nLeft-Handed\nx\ny\nz\nFigure 4.4. Left- and right-handed Cartesian coordinate systems.\nCartesian coordinates are by far the most widely used coordinate system \nin game programming. However, always remember to select the coordinate \nsystem that best maps to the problem at hand. For example, in the game Crank \nthe Weasel by Midway Home Entertainment, the main character Crank runs \naround an art-deco city picking up loot. I wanted to make the items of loot \nswirl around Crank’s body in a spiral, gett ing closer and closer to him until \nthey disappeared. I represented the position of the loot in cylindrical coor-\ndinates relative to the Crank character’s current position. To implement the \nspiral animation, I simply gave the loot a constant angular speed in θ, a small \nconstant linear speed inward along its radial axis r, and a very slight constant \nlinear speed upward along the h-axis so the loot would gradually rise up to \nthe level of Crank’s pants pockets. This extremely simple animation looked \ngreat, and it was much easier to model using cylindrical coordinates than it \nwould have been using a Cartesian system.\n4.2.2. Left-Handed vs. Right-Handed Coordinate Systems\nIn three-dimensional Cartesian coordinates, we have two choices when ar-\nranging our three mutually perpendicular axes: right-handed (RH) and left -\nhanded (LH). In a right-handed coordinate system, when you curl the ﬁ ngers \nof your right hand around the z-axis with the thumb pointing toward positive \nz coordinates, your ﬁ ngers point from the x-axis toward the y-axis. In a left -\nhanded coordinate system the same thing is true using your left  hand.\nThe only diﬀ erence between a left -handed coordinate system and a right-\nhanded coordinate system is the direction in which one of the three axes is \npointing. For example, if the y-axis points upward and x points to the right, \nthen z comes toward us (out of the page) in a right-handed system, and away \nfrom us (into the page) in a left -handed system. Left - and right-handed Carte-\nsian coordinate systems are depicted in Figure 4.4.\n",
      "content_length": 2204,
      "extraction_method": "Direct"
    },
    {
      "page_number": 162,
      "chapter": null,
      "content": "140 \n4. 3D Math for Games\nIt is easy to convert from LH to RH coordinates and vice-versa. We sim-\nply ﬂ ip the direction of any one axis, leaving the other two axes alone. It’s \nimportant to remember that the rules of mathematics do not change between \nLH and RH coordinate systems. Only our interpretation of the numbers—our \nmental image of how the numbers map into 3D space—changes. Left -handed \nand right-handed conventions apply to visualization only, not to the underly-\ning mathematics. (Actually, handedness does matt er when dealing with cross \nproducts in physical simulations, but we can safely ignore these subtleties \nfor the majority of our game programming tasks. For more information, see \nhtt p://en.wikipedia.org/wiki/Pseudovector .)\nThe mapping between the numerical representation and the visual repre-\nsentation is entirely up to us as mathematicians and programmers. We could \nchoose to have the y-axis pointing up, with z forward and x to the left  (RH) \nor right (LH). Or we could choose to have the z-axis point up. Or the x-axis \ncould point up instead—or down. All that matt ers is that we decide upon a \nmapping, and then stick with it consistently.\nThat being said, some conventions do tend to work bett er than others for \ncertain applications. For example, 3D graphics programmers typically work \nwith a left -handed coordinate system, with the y-axis pointing up, x to the \nright and positive z pointing away from the viewer (i.e., in the direction the \nvirtual camera is pointing). When 3D graphics are rendered onto a 2D screen \nusing this particular coordinate system, increasing z-coordinates correspond \nto increasing depth into the scene (i.e., increasing distance away from the vir-\ntual camera). As we will see in subsequent chapters, this is exactly what is \nrequired when using a z-buﬀ ering scheme for depth occlusion.\n4.2.3. Vectors\nA vector is a quantity that has both a magnitude and a direction in n-dimensional \nspace. A vector can be visualized as a directed line segment extending from a \npoint called the tail to a point called the head. Contrast this to a scalar (i.e., an \nordinary real-valued number), which represents a magnitude but has no di-\nrection. Usually scalars are writt en in italics (e.g., v) while vectors are writt en \nin boldface (e.g., v).\nA 3D vector can be represented by a triple of scalars (x, y, z), just as a point \ncan be. The distinction between points and vectors is actually quite subtle. \nTechnically, a vector is just an oﬀ set relative to some known point. A vector \ncan be moved anywhere in 3D space—as long as its magnitude and direction \ndon’t change, it is the same vector.\nA vector can be used to represent a point, provided that we ﬁ x the tail of \nthe vector to the origin of our coordinate system. Such a vector is sometimes \n",
      "content_length": 2818,
      "extraction_method": "Direct"
    },
    {
      "page_number": 163,
      "chapter": null,
      "content": "141 \n4.2. Points and Vectors\ncalled a position vector or radius vector. For our purposes, we can interpret any \ntriple of scalars as either a point or a vector, provided that we remember that \na position vector is constrained such that its tail remains at the origin of the \nchosen coordinate system. This implies that points and vectors are treated in \nsubtly diﬀ erent ways mathematically. One might say that points are absolute, \nwhile vectors are relative.\nThe vast majority of game programmers use the term “vector” to refer \nboth to points (position vectors) and to vectors in the strict linear algebra sense \n(purely directional vectors). Most 3D math libraries also use the term “vector” \nin this way. In this book, we’ll use the term “direction vector ” or just “direc-\ntion” when the distinction is important. Be careful to always keep the diﬀ er-\nence between points and directions clear in your mind (even if your math \nlibrary doesn’t). As we’ll see in Section 4.3.6.1, directions need to be treated \ndiﬀ erently from  points when converting them into homogeneous coordinates \nfor manipulation with 4 × 4 matrices, so gett ing the two types of vector mixed \nup can and will lead to bugs in your code.\n4.2.3.1. Cartesian Basis Vectors\nIt is oft en useful to deﬁ ne three orthogonal unit vectors (i.e., vectors that are mu-\ntually perpendicular and each with a length equal to one), corresponding to \nthe three principal Cartesian axes. The unit vector along the x-axis is typically \ncalled i, the y-axis unit vector is called j, and the z-axis unit vector is called k. \nThe vectors i, j, and k are sometimes called Cartesian basis ve ctors .\nAny point or vector can be expressed as a sum of scalars (real numbers) \nmultiplied by these unit basis vectors. For example,\n \n(5, 3, –2) = 5i + 3j – 2k. \n4.2.4. Vector Operations\nMost of the mathematical operations that you can perform on scalars can be \napplied to vectors as well. There are also some new operations that apply only \nto vectors.\n4.2.4.1. Multiplication by a Scalar\nMultiplication of a vector a by a scalar s is accomplished by multiplying the \nindividual components of a by s:\n \nsa = ( sax , say , saz ). \nMultiplication by a scalar has the eﬀ ect of scaling the magnitude of the \nvector, while leaving its direction unchanged, as shown in Figure 4.5. Multi-\nplication by –1 ﬂ ips the direction of the vector (the head becomes the tail and \nvice-versa).\n",
      "content_length": 2428,
      "extraction_method": "Direct"
    },
    {
      "page_number": 164,
      "chapter": null,
      "content": "142 \n4. 3D Math for Games\na + b\n–b\nb\na\na – b\nFigure 4.6. Vector addition and subtraction.\nThe scale factor can be diﬀ erent along each axis. We call this nonuniform \nscale , and it can be represented as the component-wise product of a scaling vector \ns and the vector in question, which we’ll denote with the ⊗ operator. Techni-\ncally speaking, this special kind of product between two vectors is known as \nthe Hadamard product . It is rarely used in the game industry—in fact, nonuni-\nform scaling is one of its only commonplace uses in games:\n \n \n(4.1)\nAs we’ll see in Section 4.3.7.3, a scaling vector s is really just a compact way to \nrepresent a 3 × 3 diagonal scaling matrix S. So another way to write Equation \n(4.1) is as follows:\n \n \n4.2.4.2. Addition and Subtraction\nThe addition of two vectors a and b is deﬁ ned as the vector whose components \nare the sums of the components of a and b. This can be visualized by placing the \nhead of vector a onto the tail of vector b—the sum is then the vector from the \ntail of a to the head of b:\n \na + b = [ (ax + bx), (ay + by), (az + bz) ]. \nVector subtraction a – b is nothing more than addition of a and –b (i.e., the \nresult of scaling b by –1, which ﬂ ips it around). This corresponds to the vector \nv\n2v\nv\nFigure 4.5. Multiplication of a vector by the scalar 2.\n \n(\n,\n,\n).\nx x\ny\ny\nz z\ns a\ns a\ns a\n⊗=\ns\na\n \n0\n0\n[\n] 0\n0\n[\n].\n0\n0\nx\nx\ny\nz\ny\nx x\ny y\nz z\nz\ns\na\na\na\ns\ns a\ns a\ns a\ns\n⎡\n⎤\n⎢\n⎥\n=\n=\n⎢\n⎥\n⎢\n⎥\n⎣\n⎦\naS\n",
      "content_length": 1460,
      "extraction_method": "Direct"
    },
    {
      "page_number": 165,
      "chapter": null,
      "content": "143 \n4.2. Points and Vectors\na\nax\nay\n|a|\nFigure 4.7. Magnitude of a vector (shown in 2D for ease of illustration).\nwhose components are the diﬀ erence between the components of a and the \ncomponents of b:\n \na – b = [ (ax – bx), (ay – by), (az – bz) ]. \nVector addition and subtraction are depicted in Figure 4.6.\nAdding and Subtracting Points and Directions\n You can add and subtract direction vectors freely. However, technically speak-\ning, points cannot be added to one another—you can only add a direction \nvector to a point, the result of which is another point. Likewise, you can take \nthe diﬀ erence between two points, resulting in a direction vector. These opera-\ntions are summarized below:\nz direction + direction = direction\nz direction – direction = direction\nz point + direction = point\nz point – point = direction\nz point + point = nonsense (don’t do it!)\n4.2.4.3. Magnitude\nThe magnitude of a vector is a scalar representing the length of the vector as \nit would be measured in 2D or 3D space. It is denoted by placing vertical bars \naround the vector’s boldface symbol. We can use the Pythagorean theorem to \ncalculate a vector’s magnitude, as shown in Figure 4.7:\n2\n2\n2 .\nx\ny\nz\na\na\na\n=\n+\n+\na\n4.2.4.4. Vector Operations in Action\nBelieve it or not, we can already solve all sorts of real-world game problems \ngiven just the vector operations we’ve learned thus far. When trying to solve a \nproblem, we can use operations like addition, subtraction, scaling, and mag-\nnitude to generate new data out of the things we already know. For example, \n",
      "content_length": 1561,
      "extraction_method": "Direct"
    },
    {
      "page_number": 166,
      "chapter": null,
      "content": "144 \n4. 3D Math for Games\nif we have the current position vector of an A.I. character P1, and a vector v \nrepresenting her current velocity , we can ﬁ nd her position on the next frame \nP2 by scaling the velocity vector by the frame time interval Δt, and then adding \nit to the current position. As shown in Figure 4.8, the resulting vector equation \nis P2 = P1 + (Δt)v. (This is known as explicit Euler integration —it’s actually only \nvalid when the velocity is constant, but you get the idea.)\nAs another example, let’s say we have two spheres, and we want to know \nwhether they intersect. Given that we know the center points of the two \nspheres, C1 and C2, we can ﬁ nd a direction vector between them by simply \nsubtracting the points, d = C2 – C1. The magnitude of this vector d = |d| de-\ntermines how far apart the spheres’ centers are. If this distance is less than the \nsum of the spheres’ radii, they are intersecting; otherwise they’re not. This is \nshown in Figure 4.9.\nSquare roots are expensive to calculate on most computers, so game \nprogrammers should always use the squared magnitude whenever it is valid to do \nso:\n \n \nUsing the squared magnitude is valid when comparing the relative lengths of \ntwo vectors (“is vector a longer than vector b?”), or when comparing a vector’s \nmagnitude to some other (squared) scalar quantity. So in our sphere-sphere \nintersection test, we should calculate d2 =       and compare this to the squared \nsum of the radii, (r1 + r2)2 for maximum speed. When writing high-perfor-\nmance soft ware, never take a square root when you don’t have to!\nvΔt\n1\nP\n2P\ny\nx\nFigure 4.8. Simple vec-\ntor addition can be used \nto ﬁ nd a character’s po-\nsition in the next frame, \ngiven her position and \nvelocity ln the current \nframe.\n1\nC\n2\nC\ny\nx\n1r\n2r\nd\n2\nC  –\n1\nC\nFigure 4.9.  A sphere-sphere intersection test involves only vector subtraction, vector mag-\nnitude, and ﬂ oating-point comparison operations.\n(\n) \n2\n2\n2\n2 .\nx\ny\nz\na\na\na\n=\n+\n+\na\n4.2.4.5. Normalization and Unit Vectors\nA unit vector is a vector with a magnitude (length) of one. Unit vectors are very \nuseful in 3D mathematics and game programming, for reasons we’ll see below.\n2\nd\n",
      "content_length": 2181,
      "extraction_method": "Direct"
    },
    {
      "page_number": 167,
      "chapter": null,
      "content": "145 \n4.2. Points and Vectors\nGiven an arbitrary vector v of length v =     , we can convert it to a unit \nvector u that points in the same direction as v, but has unit length. To do this, \nwe simply multiply v by the reciprocal of its magnitude. We call this normal-\nization :\n \n \n4.2.4.6. Normal Vectors\nA vector is said to be normal to a surface if it is perpendicular to that surface. \nNormal vectors are highly useful in games and computer graphics. For ex-\nample, a plane can be deﬁ ned by a point and a normal vector. And in 3D \ngraphics, lighting calculations make heavy use of normal vectors to deﬁ ne \nthe direction of surfaces relative to the direction of the light rays impinging \nupon them.\nNormal vectors are usually of unit length, but they do not need to be. Be \ncareful not to confuse the term “normalization” with the term “normal vec-\ntor.” A normalized vector is any vector of unit length. A normal vector is any \nvector that is perpendicular to a surface, whether or not it is of unit length.\n4.2.4.7. Dot Product and Projection\nVectors can be multiplied, but unlike scalars there are a number of diﬀ erent \nkinds of vector multiplication. In game programming, we most oft en work \nwith the following two kinds of multiplication:\nz the dot product (a.k.a. scalar product or inner product), and\nz the cross product (a.k.a. vector product or outer product).\nThe dot product of two vectors yields a scalar; it is deﬁ ned by adding the \nproducts of the individual components of the two vectors:\n \n (a scalar). \nThe dot product can also be writt en as the product of the magnitudes of the \ntwo vectors and the cosine of the angle between them:\n \n \nThe dot product is commutative (i.e., the order of the two vectors can be \nreversed) and distributive over addition:\n \n \n \n \n1 .\nv\n=\n=\nv\nu\nv\nv\nx x\ny y\nz z\na b\na b\na b\nd\n⋅=\n+\n+\n=\na b\nv\n \n \n \ncos( ).\n⋅=\nθ\na b\na b\n ;\n⋅= ⋅\na b\nb a\n \n(\n)\n.\n⋅\n+\n= ⋅+ ⋅\na\nb\nc\na b\na c\n",
      "content_length": 1923,
      "extraction_method": "Direct"
    },
    {
      "page_number": 168,
      "chapter": null,
      "content": "146 \n4. 3D Math for Games\nAnd the dot product combines with scalar multiplication as follows:\n \n \nVector Projection\nIf u is a unit vector (     = 1), then the dot product (a ⋅ u) represents the length \nof the projection of vector a onto the inﬁ nite line deﬁ ned by the direction of \nu, as shown in Figure 4.10. This projection concept works equally well in 2D \nor 3D and is highly useful for solving a wide variety of three-dimensional \nproblems.\nFigure 4.10. Vector projection using the dot product.\n \n(\n).\ns\ns\ns\n⋅= ⋅\n=\n⋅\na b\na\nb\na b\nu\nMagnitude as a Dot Product\nThe squared magnitude of a vector can be found by taking the dot product of \nthat vector with itself. Its magnitude is then easily found by taking the square \nroot:\n \n \nThis works because the cosine of zero degrees is 1, so all that is left  is\n \nDot Product Tests\nDot products are great for testing if two vectors are collinear or perpendicular, \nor whether they point in roughly the same or roughly opposite directions. For \nany two arbitrary vectors a and b, game programmers oft en use the following \ntests, as shown in Figure 4.11:\nz Collinear. (a ⋅ b) =         = ab (i.e., the angle between them is exactly 0 \ndegrees—this dot product equals +1 when a and b are unit vectors).\nz Collinear but opposite.  (a ⋅ b) = –ab (i.e., the angle between them is 180 \ndegrees—this dot product equals –1 when a and b are unit vectors).\n2\n;\n.\n= ⋅\n=\n⋅\na\na a\na\na a\n \n2 .\n=\na a\na\n \na b\n",
      "content_length": 1441,
      "extraction_method": "Direct"
    },
    {
      "page_number": 169,
      "chapter": null,
      "content": "147 \n4.2. Points and Vectors\nz Perpendicular. (a ⋅ b) = 0 (i.e., the angle between them is 90 degrees).\nz Same direction.  (a ⋅ b) > 0 (i.e., the angle between them is less than 90 \ndegrees).\nz Opposite directions.  (a ⋅ b) < 0 (i.e., the angle between them is greater than \n90 degrees).\nSome Other Applications of the Dot Product\nDot products can be used for all sorts of things in game programming. For ex-\nample, let’s say we want to ﬁ nd out whether an enemy is in front of the player \ncharacter or behind him. We can ﬁ nd a vector from the player’s position P to \nthe enemy’s position E by simple vector subtraction (v = E – P). Let’s assume \nwe have a vector f pointing in the direction that the player is facing . (As we’ll \nsee in Section 4.3.10.3, the vector f can be extracted directly from the player’s \nmodel-to-world matrix .) The dot product d = v ⋅ f can be used to test whether \nthe enemy is in front of or behind the player—it will be positive when the \nenemy is in front and negative when the enemy is behind.\n(a · b) = ab\n(a · b) = –ab\n(a · b) = 0\n(a · b) > 0\n(a · b) < 0\na\nb\na\nb\na\nb\na\nb\nb\na\nFigure 4.11. Some common dot product tests.\nFigure 4.12. The dot product can be used to ﬁ nd the height of a point above or below a \nplane.\n",
      "content_length": 1251,
      "extraction_method": "Direct"
    },
    {
      "page_number": 170,
      "chapter": null,
      "content": "148 \n4. 3D Math for Games\nThe dot product can also be used to ﬁ nd the height of a point above or \nbelow a plane (which might be useful when writing a moon-landing game for \nexample). We can deﬁ ne a plane with two vector quantities: a point Q lying \nanywhere on the plane, and a unit vector n that is perpendicular (i.e., normal) \nto the plane. To ﬁ nd the height h of a point P above the plane, we ﬁ rst calculate \na vector from any point on the plane (Q will do nicely) to the point in ques-\ntion P. So we have v = P – Q. The dot product of vector v with the unit-length \nnormal vector n is just the projection of v onto the line deﬁ ned by n. But that \nis exactly the height we’re looking for. Therefore, h = v ⋅ n = (P – Q) ⋅ n. This \nis illustrated in Figure 4.12.\n4.2.4.8. Cross Product\nThe cross product (also known as the outer product or vector product) of two vec-\ntors yields another vector that is perpendicular to the two vectors being multi-\nplied, as shown in Figure 4.13. The cross product operation is only deﬁ ned in \nthree dimensions:\n \n \nMagnitude of the Cross Product\nThe magnitude of the cross product vector is the product of the magnitudes of \nthe two vectors and the sine of the angle between them. (This is similar to the \ndeﬁ nition of the dot product, but it replaces the cosine with the sine.)\n \n \nThe magnitude of the cross product                is equal to the area of the par-\nallelogram whose sides are a and b, as shown in Figure 4.14. Since a triangle \nis one-half of a parallelogram, the area of a triangle whose vertices are speci-\nﬁ ed by the position vectors V1 , V2 , and V3 can be calculated as one-half of the \nmagnitude of the cross product of any two of its sides:\n \n \na × b\na\nb\nFigure 4.13. The cross \nproduct of vectors a \nand b (right-handed).\nV2\nV1\nV3\na = (V2 – V1)\nb = (V3 – V1)\n|a × b|\nFigure 4.14. Area of a parallelogram expressed as the magnitude of a cross product.\n[(\n),\n(\n),\n(\n)]\n(\n)\n(\n)\n(\n) .\ny z\nz y\nz x\nx z\nx y\ny x\ny z\nz y\nz x\nx z\nx y\ny x\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\n=\n−\n=\n−\na\nb\ni\n−\n−\nj\nk\n+\n−\n+\n−\n×\n1\ntriangle\n2\n1\n3\n1\n2 (\n)\n(\n) .\nA\n=\n−\n×\n−\nV\nV\nV\nV\n \n \n \nsin( ).\n×\n=\nθ\na\nb\na b\n×\na\nb\n",
      "content_length": 2163,
      "extraction_method": "Direct"
    },
    {
      "page_number": 171,
      "chapter": null,
      "content": "149 \nDirection of the Cross Product\nWhen using a right-handed coordinate system, you can use the right-hand rule \nto determine the direction of the cross product. Simply cup your ﬁ ngers such \nthat they point in the direction you’d rotate vector a to move it on top of vector \nb, and the cross product (a × b) will be in the direction of your thumb.\nNote that the cross product is deﬁ ned by the left -hand rule when using \na left -handed coordinate system. This means that the direction of the cross \nproduct changes depending on the choice of coordinate system. This might \nseem odd at ﬁ rst, but remember that the handedness of a coordinate system \ndoes not aﬀ ect the mathematical calculations we carry out—it only changes \nour visualization of what the numbers look like in 3D space. When converting \nfrom a RH system to a LH system or vice-versa, the numerical representations \nof all the points and vectors stay the same, but one axis ﬂ ips. Our visualization \nof everything is therefore mirrored along that ﬂ ipped axis. So if a cross prod-\nuct just happens to align with the axis we’re ﬂ ipping (e.g., the z-axis), it needs \nto ﬂ ip when the axis ﬂ ips. If it didn’t, the mathematical deﬁ nition of the cross \nproduct itself would have to be changed so that the z-coordinate of the cross \nproduct comes out negative in the new coordinate system. I wouldn’t lose too \nmuch sleep over all of this. Just remember: when visualizing a cross product, \nuse the right-hand rule in a right-handed coordinate system and the left -hand \nrule in a left -handed coordinate system.\nProperties of the Cross Product\nThe cross product is not commutative (i.e., order matt ers):\n \na × b ≠ b × a. \nHowever, it is anti-commutative :\n \na × b = – b × a. \nThe cross product is distributive over addition:\n \na × (b + c) = (a × b) + (a × c). \nAnd it combines with scalar multiplication as follows:\n \n(sa) × b = a × (sb) = s(a × b). \nThe Cartesian basis vectors are related by cross products as follows:\n \n \n \n \n \n \n \n(\n)\n(\n)\n,\n(\n)\n(\n)\n,\n(\n)\n(\n)\n.\n×\n=−×\n=\n×\n=−\n× =\n× =−\n×\n=\nj\nk\nk\nj\ni\nk\ni\ni\nk\nj\ni\nj\nj\ni\nk\n4.2. Points and Vectors\n",
      "content_length": 2111,
      "extraction_method": "Direct"
    },
    {
      "page_number": 172,
      "chapter": null,
      "content": "150 \n4. 3D Math for Games\nThese three cross products deﬁ ne the direction of positive rotations about the \nCartesian axes. The positive rotations go from x to y (about z), from y to z \n(about x) and from z to x (about y). Notice how the rotation about the y-axis \n“reversed” alphabetically, in that it goes from z to x (not from x to z). As we’ll \nsee below, this gives us a hint as to why the matrix for rotation about the y-axis \nlooks inverted when compared to the matrices for rotation about the x- and \nz-axes.\nThe Cross Product in Action\nThe cross product has a number of applications in games. One of its most \ncommon uses is for ﬁ nding a vector that is perpendicular to two other vectors. \nAs we’ll see in Section 4.3.10.2, if we know an object’s local unit basis vectors, \n(ilocal , jlocal , and klocal), we can easily ﬁ nd a matrix representing the object’s \norientation. Let’s assume that all we know is the object’s klocal vector—i.e., the \ndirection in which the object is facing. If we assume that the object has no roll \nabout klocal , then we can ﬁ nd ilocal by taking the cross product between klocal\n(which we already know) and the world-space up vector jworld (which equals \n[0  1  0]). We do so as follows: ilocal = normalize(jworld × klocal). We can then ﬁ nd \njlocal by simply crossing ilocal and klocal as follows: jlocal = klocal × ilocal.\nA very similar technique can be used to ﬁ nd a unit vector normal to the \nsurface of a triangle or some other plane. Given three points on the plane P1 ,\nP2 , and P3 , the normal vector is just n = normalize[(P2 – P1) × (P3 – P1)].\nCross products are also used in physics simulations. When a force is ap-\nplied to an object, it will give rise to rotational motion if and only if it is ap-\nplied oﬀ -center. This rotational force is known as a torque , and it is calculated \nas follows. Given a force F, and a vector r from the center of mass to the point \nat which the force is applied, the torque N = r × F.\n4.2.5. Linear Interpolation of Points and Vectors\nIn games, we oft en need to ﬁ nd a vector that is midway between two known \nvectors. For example, if we want to smoothly animate an object from point A \nto point B over the course of two seconds at 30 frames per second, we would \nneed to ﬁ nd 60 intermediate positions between A and B.\nA linear interpolation is a simple mathematical operation that ﬁ nds an in-\ntermediate point between two known points. The name of this operation is \noft en shortened to LERP. The operation is deﬁ ned as follows, where β ranges \nfrom 0 to 1 inclusive:\n \n \n \n(\n, , )\n(1\n)\n[(1\n)\n,\n(1\n)\n,\n(1\n)\n].\nx\nx\ny\ny\nz\nz\nA\nB\nA\nB\nA\nB\n=\nβ =\n−β\n+ β\n=\n−β\n+ β\n−β\n+ β\n−β\n+ β\nL\nLERP A B\nA\nB\n",
      "content_length": 2679,
      "extraction_method": "Direct"
    },
    {
      "page_number": 173,
      "chapter": null,
      "content": "151 \n4.3. Matrices\nGeometrically, L = LERP(A, B, β) is the position vector of a point that lies \nβ percent of the way along the line segment from point A to point B, as shown \nin Figure 4.15. Mathematically, the LERP function is just a weighted average of \nthe two input vectors, with weights (1 – β) and β, respectively. Notice that the \nweights always add to 1, which is a general requirement for any weighted \naverage.\n4.3. Matrices\nA matrix is a rectangular array of  m × n scalars. Matrices are a convenient way \nof representing linear transformations such as translation, rotation, and scale. \nA matrix M is usually writt en as a grid of scalars Mrc enclosed in square \nbrackets, where the subscripts r and c represent the row and column indices \nof the entry, respectively. For example, if M is a 3 × 3 matrix, it could be writ-\nten as follows:\n \n \nWe can think of the rows and/or columns of a 3 × 3 matrix as 3D vectors. \nWhen all of the row and column vectors of a 3 × 3 matrix are of unit magni-\ntude, we call it a special orthogonal matrix. This is also known as an isotropic \nmatrix, or an orthonormal matrix. Such matrices represent pure rotations.\nUnder certain constraints, a 4 × 4 matrix can represent arbitrary 3D trans-\nformations , including translations , rotations , and changes in scale . These are \ncalled transformation matrices , and they are the kinds of matrices that will be \nmost useful to us as game engineers. The transformations represented by a \nmatrix are applied to a point or vector via matrix multiplication. We’ll inves-\ntigate how this works below.\nAn aﬃ  ne matrix is a 4 × 4 transformation matrix that preserves parallelism \nof lines and relative distance ratios, but not necessarily absolute lengths and \nangles. An aﬃ  ne matrix is any combination of the following operations: rota-\ntion, translation, scale and/or shear.\n4.3.1. \nMatrix Multiplication\n The product P of two matrices A and B is writt en P = AB. If A and B are \ntransformation matrices, then the product P is another transformation matrix \nthat performs both of the original transformations. For example, if A is a scale \nmatrix and B is a rotation, the matrix P would both scale and rotate the points \nA\nL = LERP(A, B, 0.4)\nB\nβ = 0\nβ = 1\nβ = 0.4\nFigure 4.15. Linear in-\nterpolation (LERP) be-\ntween points A and B, \nwith β = 0.4.\n \n11\n12\n13\n21\n22\n23\n31\n32\n33\n.\nM\nM\nM\nM\nM\nM\nM\nM\nM\n⎡\n⎤\n⎢\n⎥\n=⎢\n⎥\n⎢\n⎥\n⎣\n⎦\nM\n",
      "content_length": 2411,
      "extraction_method": "Direct"
    },
    {
      "page_number": 174,
      "chapter": null,
      "content": "152 \n4. 3D Math for Games\nor vectors to which it is applied. This is particularly useful in game program-\nming, because we can precalculate a single matrix that performs a whole se-\nquence of transformations and then apply all of those transformations to a \nlarge number of vectors eﬃ  ciently.\nTo calculate a matrix product, we simply take dot products between the \nrows of the nA × mA matrix A and the columns of the nB × mB matrix B. Each dot \nproduct becomes one component of the resulting matrix P. The two matrices \ncan be multiplied as long as the inner dimensions are equal (i.e., mA = nB). For \nexample, if A and B are 3 × 3 matrices, then\n \nP = AB, \n \n         \nMatrix multiplication is not commutative. In other words, the order in \nwhich matrix multiplication is done matt ers:\n \n \nAB ≠ BA . \nWe’ll see exactly why this matt ers in Section 4.3.2.\nMatrix multiplication is oft en called concatenation, because the product \nof n transformation matrices is a matrix that concatenates, or chains together, \nthe original sequence of transformations in the order the matrices were mul-\ntiplied.\n4.3.2. Representing Points and Vectors as Matrices\nPoints and vectors can be represented as row matrices (1 × n) or column matrices \n(n × 1), where n is the dimension of the space we’re working with (usually 2 or \n3). For example, the vector v = (3, 4, –1) can be writt en either as\n \n \nor as\n \n \nThe choice between column and row vectors is a completely arbitrary \none, but it does aﬀ ect the order in which matrix multiplications are writt en. \nThis happens because when multiplying matrices, the inner dimensions of the \ntwo matrices must be equal, so:\n11\nrow1\ncol1\n21\nrow2\ncol1\n31\nrow3\ncol1\n;\n;\n;\nP\nP\nP\n=\n⋅\n=\n⋅\n=\n⋅\nA\nB\nA\nB\nA\nB\n12\nrow1\ncol2\n22\nrow2\ncol2\n32\nrow3\ncol2\n;\n;\n;\nP\nP\nP\n=\n⋅\n=\n⋅\n=\n⋅\nA\nB\nA\nB\nA\nB\n13\nrow1\ncol3\n23\nrow2\ncol3\n33\nrow3\ncol3\n;\n;\n.\nP\nP\nP\n=\n⋅\n=\n⋅\n=\n⋅\nA\nB\nA\nB\nA\nB\n \n1\n[3\n4\n1],\n=\n−\nv\nT\n2\n1\n3\n4\n.\n1\n⎡\n⎤\n⎢\n⎥\n=\n=\n⎢\n⎥\n⎢\n⎥\n−\n⎣\n⎦\nv\nv\n",
      "content_length": 1945,
      "extraction_method": "Direct"
    },
    {
      "page_number": 175,
      "chapter": null,
      "content": "153 \n4.3. Matrices\nz to multiply a 1 × n row vector by an n × n matrix, the vector must appear \nto the left  of the matrix (                             ), whereas\nz to multiply an n × n matrix by an n × 1 column vector, the vector must \nappear to the right of the matrix (                             ).\nIf multiple transformation matrices A, B, and C are applied in order to a \nvector v, the transformations “read” from left  to right when using row vectors, \nbut from right to left  when using column vectors. The easiest way to remember \nthis is to realize that the matrix closest to the vector is applied ﬁ rst. This is il-\nlustrated by the parentheses below:\n \nv’ = ( ( ( vA ) B ) C )    Row vectors: read left -to-right; \n \nv’ = ( C ( B ( Av ) ) )    Column vectors: read right-to-left . \nIn this book we’ll adopt the row vector convention, because the left -to-right \norder of transformations is most intuitive to read for English-speaking people. \nThat said, be very careful to check which convention is used by your game \nengine, and by other books, papers, or web pages you may read. You can \nusually tell by seeing whether vector-matrix multiplications are writt en with \nthe vector on the left  (for row vectors) or the right (for column vectors) of the \nmatrix. When using column vectors, you’ll need to transpose all the matrices \nshown in this book.\n4.3.3. The Identity Matrix\nThe identity matrix is a matrix that, when multiplied by any other matrix, \nyields the very same matrix. It is usually represented by the symbol I. The \nidentity matrix is always a square matrix with 1’s along the diagonal and 0’s \neverywhere else:\n \n \n \nAI = IA ≡ A . \n4.3.4. Matrix Inversion\nThe inverse of a matrix A is another matrix (denoted A–1) that undoes the eﬀ ects \nof matrix A. So, for example, if A rotates objects by 37 degrees about the z-axis, \nthen A–1 will rotate by –37 degrees about the z-axis. Likewise, if A scales objects \nto be twice their original size, then A–1 scales objects to be half-sized. When a ma-\ntrix is multiplied by its own inverse, the result is always the identity matrix, so\n \n1\n1\nn\nn\nn n\n×\n×\n×\n′\n=\nv\nv\nM\n \n1\n1\nn\nn n\nn\n×\n×\n×\n′\n=\nv\nM\nv\n \n3 3\n1\n0\n0\n0\n1\n0 ;\n0\n0\n1\n×\n⎡\n⎤\n⎢\n⎥\n=⎢\n⎥\n⎢\n⎥\n⎣\n⎦\nI\n",
      "content_length": 2220,
      "extraction_method": "Direct"
    },
    {
      "page_number": 176,
      "chapter": null,
      "content": "154 \n4. 3D Math for Games\n  \n \n         Not all matrices have inverses. However, all aﬃ  ne matri-\nces (combinations of pure rotations, translations, scales, and shears) do have \ninverses. Gaussian elimination or LU decomposition can be used to ﬁ nd the \ninverse, if one exists.\nSince we’ll be dealing with matrix multiplication a lot, it’s important to \nnote here that the inverse of a sequence of concatenated matrices can be writt en \nas the reverse concatenation of the individual matrices’ inverses. For example,\n \n(ABC)–1 = C–1 B–1 A–1. \n4.3.5. Transposition\nThe transpose of a matrix M is denoted MT. It is obtained by reﬂ ecting the en-\ntries of the original matrix across its diagonal. In other words, the rows of the \noriginal matrix become the columns of the transposed matrix, and vice-versa:\n \n \nThe transpose is useful for a number of reasons. For one thing, the inverse \nof an orthonormal (pure rotation) matrix is exactly equal to its transpose—\nwhich is good news, because it’s much cheaper to transpose a matrix than it is \nto ﬁ nd its inverse in general. Transposition can also be important when mov-\ning data from one math library to another, because some libraries use column \nvectors while others expect row vectors. The matrices used by a row-vector-\nbased library will be transposed relative to those used by a library that employs \nthe column vector convention.\nAs with the inverse, the transpose of a sequence of concatenated matrices \ncan be rewritt en as the reverse concatenation of the individual matrices’ trans-\nposes. For example,\n \n(ABC)T = CT   BT  AT. \nThis will prove useful when we consider how to apply transformation matri-\nces to points and vectors.\n4.3.6. Homogeneous Coordinates\n You may recall from high-school algebra that a 2 × 2 matrix can represent a \nrotation in two dimensions. To rotate a vector r through an angle of φ degrees \n(where positive rotations are counter-clockwise), we can write\n \n \n1\n1\n(\n)\n(\n)\n.\n−\n−\n≡\n≡\nA A\nA\nA\nI\n \n \ncos\nsin\n[\n]\n[\n]\n.\nsin\ncos\nx\ny\nx\ny\nr\nr\nr\nr\nφ\nφ\n⎡\n⎤\n′\n′ =\n⎢\n⎥\n−\nφ\nφ\n⎣\n⎦\n \nT\n.\na\nb\nc\na\nd\ng\nd\ne\nf\ng\nh\ni\nc\nf     i\n⎡\n⎤\n⎡\n⎤\n⎢\n⎥\n⎢\n⎥\n=\n⎢\n⎥\n⎢\n⎥\n⎢\n⎥\n⎢\n⎥\n⎣\n⎦\n⎣\n⎦\ne\nh\nb\n",
      "content_length": 2141,
      "extraction_method": "Direct"
    },
    {
      "page_number": 177,
      "chapter": null,
      "content": "155 \n4.3. Matrices\nIt’s probably no surprise that rotations in three dimensions can be represented \nby a 3 × 3 matrix. The two-dimensional example above is really just a three-\ndimensional rotation about the z-axis, so we can write\n \n \nThe question naturally arises: Can a 3 × 3 matrix be used to represent \ntranslations? Sadly, the answer is no. The result of translating a point r by a \ntranslation t requires adding the components of t to the components of r in-\ndividually:\n \n \nMatrix multiplication involves multiplication and addition of matrix ele-\nments, so the idea of using multiplication for translation seems promising. \nBut, unfortunately, there is no way to arrange the components of t within a 3 × \n3 matrix such that the result of multiplying it with the column vector r yields \nsums like (rx + tx).\nThe good news is that we can obtain sums like this if we use a 4 × 4 matrix. \nWhat would such a matrix look like? Well, we know that we don’t want any \nrotational eﬀ ects, so the upper 3 × 3 should contain an identity matrix. If we \narrange the components of t across the bott om-most row of the matrix and \nset the fourth element of the r vector (usually called w) equal to 1, then taking \nthe dot product of the vector r with column 1 of the matrix will yield (1 × rx) + \n(0 × ry) + (0 × rz) + (tx × 1) = (rx + tx), which is exactly what we want. If the bott om \nright-hand corner of the matrix contains a 1 and the rest of the fourth column \ncontains zeros, then the resulting vector will also have a 1 in its w component. \nHere’s what the ﬁ nal 4 × 4 translation matrix looks like:\n \n \nWhen a point or vector is extended from three dimensions to four in this \nmanner, we say that it has been writt en in homogeneous coordinates. A point in \nhomogeneous coordinates always has w = 1. Most of the 3D matrix math done \nby game engines is performed using 4 × 4 matrices with four-element points \nand vectors writt en in homogeneous coordinates.\n \n  cos\nsin\n0\n[\n]\n[\n] \nsin\ncos\n0 .\n0\n0\n1\nx\ny\nz\nx\ny\nz\nr\nr\nr\nr\nr\nr\n⎡\n⎤\nφ\nφ\n⎢\n⎥\n′\n′\n′ =\n−\nφ\nφ\n⎢\n⎥\n⎢\n⎥\n⎣\n⎦\n \n[(\n)\n(\n)\n(\n)].\nx\nx\ny\ny\nz\nz\nr\nt\nr\nt\nr\nt\n+ =\n+\n+\n+\nr\nt\n \n1\n0\n0\n0\n0\n1\n0\n0\n[\n1] 0\n0\n1\n0\n1\n       \n[(\n)\n(\n)\n(\n)\n1].\nx\ny\nz\nx\ny\nz\nx\nx\ny\ny\nz\nz\nr\nr\nr\nt\nr\nt\nr\nt\nr\nt\n⎡\n⎤\n⎢\n⎥\n⎢\n⎥\n+ =\n⎢\n⎥\n⎢\n⎥\n⎣\n⎦\n=\n+\n+\n+\nr\nt\nt\nt\n",
      "content_length": 2265,
      "extraction_method": "Direct"
    },
    {
      "page_number": 178,
      "chapter": null,
      "content": "156 \n4. 3D Math for Games\n4.3.6.1. Transforming Direction Vectors\nMathematically, points (position vectors) and direction vectors are treated in \nsubtly diﬀ erent ways. When transforming a point by a matrix, the translation, \nrotation, and scale of the matrix are all applied to the point. But when trans-\nforming a direction by a matrix, the translational eﬀ ects of the matrix are ig-\nnored. This is because direction vectors have no translation per se—applying \na translation to a direction would alter its magnitude, which is usually not \nwhat we want.\nIn homogeneous coordinates, we achieve this by deﬁ ning points to have \ntheir w components equal to one, while direction vectors have their w com-\nponents equal to zero. In the example below, notice how the w = 0 component \nof the vector v multiplies with the t vector in the matrix, thereby eliminating \ntranslation in the ﬁ nal result:\n \n \nTechnically, a point in homogeneous (four-dimensional) coordinates can \nbe converted into non-homogeneous (three-dimensional) coordinates by di-\nviding the x, y, and z components by the w component:\n \n \nThis sheds some light on why we set a point’s w component to one and a vec-\ntor’s w component to zero. Dividing by w = 1 has no eﬀ ect on the coordinates \nof a point, but dividing a pure direction vector’s components by w = 0 would \nyield inﬁ nity. A point at inﬁ nity in 4D can be rotated but not translated, be-\ncause no matt er what translation we try to apply, the point will remain at in-\nﬁ nity. So in eﬀ ect, a pure direction vector in three-dimensional space acts like \na point at inﬁ nity in four-dimensional homogeneous space.\n4.3.7. Atomic Transformation Matrices\nAny aﬃ  ne transformation matrix can be created by simply concatenating a \nsequence of 4 × 4 matrices representing pure translations, pure rotations, pure \nscale operations, and/or pure shears. These atomic transformation building \nblocks are presented below. (We’ll omit shear from these discussions, as it \ntends to be used only rarely in games.)\nNotice that all aﬃ  ne 4 × 4 transformation matrices can be partitioned into \nfour components:\n \n \n \n[\n0] \n[(\n0 )\n0]\n[\n0].\n1\n⎡\n⎤=\n+\n=\n⎢\n⎥\n⎣\n⎦\nU\n0\nv\nvU\nt\nvU\nt\n \n[\n]\n.\ny\nx\nz\nx\ny\nz\nw\nw\nw\nw\n⎡\n⎤\n≡⎢\n⎥\n⎣\n⎦\n \n3 3\n3 1\n1 3\n.\n1\n×\n×\n×\n⎡\n⎤\n⎢\n⎥\n⎣\n⎦\nU\n0\nt\n",
      "content_length": 2267,
      "extraction_method": "Direct"
    },
    {
      "page_number": 179,
      "chapter": null,
      "content": "157 \n4.3. Matrices\nz the upper 3 × 3 matrix U, which represents the rotation and/or scale,\nz a 1 × 3 translation vector t,\nz a 3 × 1 vector of zeros 0 = [ 0 0 0 ]T, and\nz a scalar 1 in the bott om-right corner of the matrix.\nWhen a point is multiplied by a matrix that has been partitioned like this, the \nresult is as follows:\n \n \n4.3.7.1. \nTranslation\n The following matrix translates a point by the vector t:\n \n \nor in partitioned shorthand:\n \n \nTo invert a pure translation matrix, simply negate the vector t (i.e., negate tx ,\nty , and tz).\n4.3.7.2. Rotation\n All 4  × 4 pure rotation matrices have the form:\n \n \nThe t vector is zero and the upper 3 × 3 matrix R contains cosines and sines of \nthe rotation angle, measured in radians.\nThe following matrix represents rotation about the x-axis by an angle φ:\n  \n \n \n3 3\n3 1\n 1 3\n 1 3\n1 3\n[\n1]\n[\n1] \n[(\n)\n1].\n1\n×\n×\n×\n×\n×\n⎡\n⎤\n′\n=\n=\n+\n⎢\n⎥\n⎣\n⎦\nU\n0\nr\nr\nrU\nt\nt\n \n1\n0\n0\n0\n0\n1\n0\n0\n[\n1] 0\n0\n1\n0\n1\n       \n[(\n)\n(\n)\n(\n)\n1],\nx\ny\nz\nx\ny\nz\nx\nx\ny\ny\nz\nz\nr\nr\nr\nt\nr\nt\nr\nt\nr\nt\n⎡\n⎤\n⎢\n⎥\n⎢\n⎥\n+ =\n⎢\n⎥\n⎢\n⎥\n⎣\n⎦\n=\n+\n+\n+\nr\nt\nt\nt\n \n[\n1] \n[(\n)\n1].\n1\n⎡\n⎤=\n+\n⎢\n⎥\n⎣\n⎦\nI\n0\nr\nr\nt\nt\n \n[\n1] \n[\n1].\n1\n⎡\n⎤=\n⎢\n⎥\n⎣\n⎦\nR\n0\nr\nrR\n0\n \n1\n0\n0\n0\n0\n  cos\nsin\n0\nrotate ( , )\n[\n1] \n.\n0\nsin\ncos\n0\n0\n0\n0\n1\nx\nx\ny\nz\nr\nr\nr\n⎡\n⎤\n⎢\n⎥\nφ\nφ\n⎢\n⎥\nφ =\n⎢\n⎥\n−\nφ\nφ\n⎢\n⎥\n⎣\n⎦\nr\n",
      "content_length": 1260,
      "extraction_method": "Direct"
    },
    {
      "page_number": 180,
      "chapter": null,
      "content": "158 \n4. 3D Math for Games\nThe matrix below represents rotation about the y-axis by an angle θ. Notice \nthat this one is transposed relative to the other two—the positive and negative \nsine terms have been reﬂ ected across the diagonal:\n \n \nThis matrix represents rotation about the z-axis by an angle γ:\n \n \nHere are a few observations about these matrices:\nz The 1 within the upper 3 × 3 always appears on the axis we’re rotating \nabout, while the sine and cosine terms are oﬀ -axis.\nz Positive rotations go from x to y (about z), from y to z (about x), and from \nz to x (about y). The z to x rotation “wraps around,” which is why the \nrotation matrix about the y-axis is transposed relative to the other two. \n(Use the right-hand or left -hand rule to remember this.)\nz The inverse of a pure rotation is just its transpose. This works because \ninverting a rotation is equivalent to rotating by the negative angle. You \nmay recall that cos(–θ) = cos(θ) while sin(–θ) = –sin(θ), so negating the \nangle causes the two sine terms to eﬀ ectively switch places, while the \ncosine terms stay put.\n4.3.7.3. Scale\n The following matrix scales the point r by a factor of sx along the x-axis, sy\nalong the y-axis, and sz along the z-axis:\n  \n \n \ncos\n0\nsin\n0\n0\n1\n0\n0\nrotate ( , )\n[\n1] \n.\nsin\n0\n  cos\n0\n0\n0\n0\n1\ny\nx\ny\nz\nr\nr\nr\nθ\n−\nθ\n⎡\n⎤\n⎢\n⎥\n⎢\n⎥\nθ =\n⎢\n⎥\nθ\nθ\n⎢\n⎥\n⎣\n⎦\nr\n \n  cos\nsin\n0\n0\nsin\ncos\n0\n0\nrotate ( , )\n[\n1] \n.\n0\n0\n1\n0\n0\n0\n0\n1\nz\nx\ny\nz\nr\nr\nr\nγ\nγ\n⎡\n⎤\n⎢\n⎥\n−\nγ\nγ\n⎢\n⎥\nγ =\n⎢\n⎥\n⎢\n⎥\n⎣\n⎦\nr\n \n \n \n0\n0\n0\n0\n0\n0\n[\n1] 0\n0\n0\n0\n0\n0\n1\n    \n[\n1].\nx\ny\nx\ny\nz\nz\nx x\ny y\nz z\ns\ns\nr\nr\nr\ns\ns r\ns r\ns r\n⎡\n⎤\n⎢\n⎥\n⎢\n⎥\n=\n⎢\n⎥\n⎢\n⎥\n⎣\n⎦\n=\nrS\n",
      "content_length": 1600,
      "extraction_method": "Direct"
    },
    {
      "page_number": 181,
      "chapter": null,
      "content": "159 \n4.3. Matrices\nor in partitioned shorthand:\n \n \nHere are some observations about this kind of matrix:\nz To invert a scaling matrix, simply substitute sx , sy , and sz with their re-\nciprocals (i.e., 1/sx , 1/sy , and 1/sz).\nz When the scale factor along all three axes is the same (sx = sy = sz), we call \nthis uniform scale. Spheres remain spheres under uniform scale, whereas \nunder nonuniform scale they become ellipsoids. To keep the mathemat-\nics of bounding sphere checks simple and fast, many game engines im-\npose the restriction that only uniform scale may be applied to render-\nable geometry or collision primitives.\nz When a uniform scale matrix Su and a rotation matrix R are concat-\nenated, the order of multiplication is unimportant (i.e., SuR = RSu). This \nonly works for uniform scale!\n4.3.8. 4 × 3 Matrices\n The rightmost column of an aﬃ  ne 4 × 4 matrix always contains the vector \n[ 0  0  0  1 ]T. As such, game programmers oft en omit the fourth column to \nsave memory. You’ll encounter 4 × 3 aﬃ  ne matrices frequently in game math \nlibraries.\n4.3.9. Coordinate Spaces\nWe’ve seen how to apply transformations to points and direction vectors us-\ning 4 × 4 matrices. We can extend this idea to rigid objects by realizing that \nsuch an object can be thought of as an inﬁ nite collection of points. Applying \na transformation to a rigid object is like applying that same transformation to \nevery point within the object. For example, in computer graphics an object is \nusually represented by a mesh of triangles, each of which has three vertices \nrepresented by points. In this case, the object can be transformed by applying \na transformation matrix to all of its vertices in turn.\nWe said above that a point is a vector whose tail is ﬁ xed to the origin of \nsome coordinate system. This is another way of saying that a point (position \nvector) is always expressed relative to a set of coordinate axes. The triplet of \nnumbers representing a point changes numerically whenever we select a new \nset of coordinate axes. In Figure 4.16, we see a point P represented by two \ndiﬀ erent position vectors—the vector PA gives the position of P relative to the \n \n \n3 3\n3 3\n[\n1] \n[\n1].\n1\n×\n×\n⎡\n⎤=\n⎢\n⎥\n⎣\n⎦\nS\n0\nr\nrS\n0\n",
      "content_length": 2229,
      "extraction_method": "Direct"
    },
    {
      "page_number": 182,
      "chapter": null,
      "content": "160 \n4. 3D Math for Games\n“A” axes, while the vector PB gives the position of that same point relative to a \ndiﬀ erent set of axes “B.”\nIn physics, a set of coordinate axes represents a frame of reference, so \nwe sometimes refer to a set of axes as a coordinate frame (or just a frame). \nPeople in the game industry also use the term coordinate space (or simply \nspace) to refer to a set of coordinate axes. In the following sections, we’ll look \nat a few of the most common coordinate spaces used in games and computer \ngraphics.\n4.3.9.1. Model Space\nWhen a triangle mesh is created in a tool such as Maya or 3DStudioMAX, the \npositions of the triangles’ vertices are speciﬁ ed relative to a Cartesian coordi-\nnate system which we call model space (also known as object space or local space). \nThe model space origin is usually placed at a central location within the object, \nsuch as at its center of mass, or on the ground between the feet of a humanoid \nor animal character.\nMost game objects have an inherent directionality. For example, an air-\nplane has a nose, a tail ﬁ n, and wings that correspond to the front, up, and \nleft /right directions. The model space axes are usually aligned to these natural \ndirections on the model, and they’re given intuitive names to indicate their \ndirectionality as illustrated in Figure 4.17. \nz Front. This name is given to the axis that points in the direction that the \nobject naturally travels or faces. In this book, we’ll use the symbol F to \nrefer to a unit basis vector along the front axis.\nz Up. This name is given to the axis that points towards the top of the \nobject. The unit basis vector along this axis will be denoted U.\nz Left  or right. The name “left ” or “right” is given to the axis that points \ntoward the left  or right side of the object. Which name is chosen de-\npends on whether your game engine uses left -handed or right-handed \nxA\nyA\nxB\nyB\nPA = (2, 3)\nPB = (1, 5)\nFigure 4.16. Position vectors for the point P relative to different coordinate axes.\n",
      "content_length": 2023,
      "extraction_method": "Direct"
    },
    {
      "page_number": 183,
      "chapter": null,
      "content": "161 \n4.3. Matrices\ncoordinates. The unit basis vector along this axis will be denoted L or R, \nas appropriate.\nThe mapping between the (front, up, left ) labels and the (x, y, z) axes is com-\npletely arbitrary. A common choice when working with right-handed axes is \nto assign the label front to the positive z-axis, the label left  to the positive x-axis, \nand the label up to the positive y-axis (or in terms of unit basis vectors, F = k, \nL = i, and U = j). However, it’s equally common for +x to be front and +z to be \nright (F = i, R = k, U = j). I’ve also worked with engines in which the z-axis is \noriented vertically. The only real requirement is that you stick to one conven-\ntion consistently throughout your engine.\nAs an example of how intuitive axis names can reduce confusion, consid-\ner Euler angles (pitch, yaw, roll), which are oft en used to describe an aircraft ’s \norientation. It’s not possible to deﬁ ne pitch, yaw, and roll angles in terms of \nthe (i, j, k) basis vectors because their orientation is arbitrary. However, we can \ndeﬁ ne pitch, yaw, and roll in terms of the (L, U, F) basis vectors, because their \norientations are clearly deﬁ ned. Speciﬁ cally,\nz pitch is rotation about L or R,\nz yaw is rotation about U, and\nz roll is rotation about F.\n4.3.9.2. World Space\nWorld space is a ﬁ xed coordinate space, in which the positions, orientations, \nand scales of all objects in the game world are expressed. This coordinate \nspace ties all the individual objects together into a cohesive virtual world.\nThe location of the world-space origin is arbitrary, but it is oft en placed \nnear the center of the playable game space to minimize the reduction in ﬂ oat-\ning-point precision that can occur when (x, y, z) coordinates grow very large. \nLikewise, the orientation of the x-, y-, and z-axes is arbitrary, although most \nle\u0002\nfront\nup\nFigure 4.17. One possible choice of the model-space front, left and up axis basis vectors for \nan airplane.\n",
      "content_length": 1972,
      "extraction_method": "Direct"
    },
    {
      "page_number": 184,
      "chapter": null,
      "content": "162 \n4. 3D Math for Games\nof the engines I’ve encountered use either a y-up or a z-up convention. The \ny-up convention was probably an extension of the two-dimensional conven-\ntion found in most mathematics textbooks, where the y-axis is shown going \nup and the x-axis going to the right. The z-up convention is also common, be-\ncause it allows a top-down orthographic view of the game world to look like \na traditional two-dimensional xy-plot.\nAs an example, let’s say that our aircraft ’s left  wingtip is at (5, 0, 0) in mod-\nel space. (In our game, front vectors correspond to the positive z-axis in model \nspace with y up, as shown in Figure 4.17.) Now imagine that the jet is facing \ndown the positive x-axis in world space, with its model-space origin at some \narbitrary location, such as (–25, 50, 8). Because the F vector of the airplane, \nwhich corresponds to +z in model space, is facing down the +x-axis in world \nspace, we know that the jet has been rotated by 90 degrees about the world \ny-axis. So if the aircraft  were sitt ing at the world space origin, its left  wingtip \nwould be at (0, 0, –5) in world space. But because the aircraft ’s origin has been \ntranslated to (–25, 50, 8), the ﬁ nal position of the jet’s left  wingtip in model \nspace is (–25, 50, [8 – 5]) = (–25, 50, 3). This is illustrated in Figure 4.18.\nWe could of course populate our friendly skies with more than one Lear \njet. In that case, all of their left  wingtips would have coordinates of (5, 0, 0) \nin model space. But in world space, the left  wingtips would have all sorts of \ninteresting coordinates, depending on the orientation and translation of each \naircraft .\n4.3.9.3. View Space\nView space (also known as camera space) is a coordinate frame ﬁ xed to the cam-\nera. The view space origin is placed at the focal point of the camera. Again, \nany axis orientation scheme is possible. However, a y-up convention with z \nAirport\nz W\nxW\nxM\nz M\n(5,0,0)M\n(–25,50,3)W\n(–25,50,8)W\nAircraft:\nLeft \nWingtip:\nFigure 4.18.  A lear jet whose left wingtip is at (5, 0, 0) in model space. If the jet is rotated by 90 \ndegrees about the world-space y-axis, and its model-space origin translated to (–25, 50, 8) in \nworld space, then its left wingtip would end up at (–25, 50, 3) when expressed in world space \ncoordinates.\n",
      "content_length": 2308,
      "extraction_method": "Direct"
    },
    {
      "page_number": 185,
      "chapter": null,
      "content": "163 \n4.3. Matrices\nincreasing in the direction the camera is facing (left -handed) is typical because \nit allows z coordinates to represent depths into the screen . Other engines and \nAPIs, such as OpenGL , deﬁ ne view space to be right-handed, in which case the \ncamera faces towards negative z, and z coordinates represent negative depths.\n4.3.10. Change of Basis\nIn games and computer graphics, it is oft en quite useful to convert an object’s \nposition, orientation, and scale from one coordinate system into another. We \ncall this operation a change of basis .\n4.3.10.1. Coordinate Space Hierarchies\nCoordinate frames are relative. That is, if you want to quantify the position, \norientation, and scale of a set of axes in three-dimensional space, you must \nspecify these quantities relative to some other set of axes (otherwise the num-\nbers would have no meaning). This implies that coordinate spaces form a hi-\nerarchy—every coordinate space is a child of some other coordinate space, and \nthe other space acts as its parent. World space has no parent; it is at the root \nof the coordinate-space tree, and all other coordinate systems are ultimately \nspeciﬁ ed relative to it, either as direct children or more-distant relatives.\n4.3.10.2. Building a Change of Basis Matrix\nThe matrix that transforms points and directions from any child coordinate \nsystem C to its parent coordinate system P can be writt en \nC\nP\n→\nM\n (pronounced \n“C to P”). The subscript indicates that this matrix transforms points and direc-\ntions from child space to parent space. Any child-space position vector PC can \nbe transformed into a parent-space position vector PP as follows:\nLeft-Handed\nx\nz\ny\nRight-Handed\nz\nx\ny\nVirtual \nScreen\nVirtual \nScreen\nFigure 4.19. Left- and right-handed examples of view space, also known as camera space.\n",
      "content_length": 1824,
      "extraction_method": "Direct"
    },
    {
      "page_number": 186,
      "chapter": null,
      "content": "164 \n4. 3D Math for Games\n \n \n \nIn this equation,\nz iC is the unit basis vector along the child space x-axis, expressed in par-\nent space coordinates;\nz jC is the unit basis vector along the child space y-axis, in parent space;\nz kC is the unit basis vector along the child space z-axis, in parent space;\nz tC is the translation of the child coordinate system relative to parent \nspace.\nThis result should not be too surprising. The tC vector is just the transla-\ntion of the child space axes relative to parent space, so if the rest of the ma-\ntrix were identity, the point (0, 0, 0) in child space would become tC in parent \nspace, just as we’d expect. The iC , jC , and kC unit vectors form the upper 3 × 3 \nof the matrix, which is a pure rotation matrix because these vectors are of unit \nlength. We can see this more clearly by considering a simple example, such as \na situation in which child space is rotated by an angle γ about the z-axis, with \nno translation. The matrix for such a rotation is given by \n \n \n(4.2)\nBut in Figure 4.20, we can see that the coordinates of the iC and jC vectors, \nexpressed in parent space, are iC = [ cos γ  sin γ  0 ] and jC = [ –sin γ  cos γ  0 ]. \nWhen we plug these vectors into our formula for \nC\nP\n→\nM\n, with kC = [ 0  0  1 ], it \nexactly matches the matrix rotatez(r, γ) from Equation (4.2).\nScaling the Child Axes\nScaling of the child coordinate system is accomplished by simply scaling the \nunit basis vectors appropriately. For example, if child space is scaled up by a \n \n  cos\nsin\n0\n0\nsin\ncos\n0\n0\nrotate ( , )\n[\n1] \n.\n0\n0\n1\n0\n0\n0\n0\n1\nz\nx\ny\nz\nr\nr\nr\nγ\nγ\n⎡\n⎤\n⎢\n⎥\n−\nγ\nγ\n⎢\n⎥\nγ =\n⎢\n⎥\n⎢\n⎥\n⎣\n⎦\nr\n \n \nC\nP\n;\n0\n0\n0\n0\n0\n0\n.\n0\n1\nP\nC\nC\nP\nC\nC\nC\nC\nCx\nCy\nCz\nCx\nCy\nCz\nCx\nCy\nCz\nCx\nCy\nCz\ni\ni\ni\nj\nj\nj\nk\nk\nk\nt\n→\n→\n=\n⎡\n⎤\n⎢\n⎥\n⎢\n⎥\n=⎢\n⎥\n⎢\n⎥\n⎣\n⎦\n⎡\n⎤\n⎢\n⎥\n⎢\n⎥\n=⎢\n⎥\n⎢\n⎥\n⎣\n⎦\nP\nP M\ni\nj\nM\nk\nt\nt\nt\n",
      "content_length": 1816,
      "extraction_method": "Direct"
    },
    {
      "page_number": 187,
      "chapter": null,
      "content": "165 \n4.3. Matrices\nfactor of two, then the basis vectors iC , jC , and kC will be of length 2 instead \nof unit length.\n4.3.10.3. Extracting Unit Basis Vectors from a Matrix\n The fact that we can build a change of basis matrix out of a translation and \nthree Cartesian basis vectors gives us another powerful tool: Given any aﬃ  ne \n4 × 4 transformation matrix, we can go in the other direction and extract the \nchild-space basis vectors iC , jC , and kC from it by simply isolating the appropri-\nate rows of the matrix (or columns if your math library uses column vectors).\nThis can be incredibly useful. Let’s say we are given a vehicle’s model-\nto-world transform as an aﬃ  ne 4 × 4 matrix (a very common representation). \nThis is really just a change of basis matrix, transforming points in model space \ninto their equivalents in world space. Let’s further assume that in our game, \nthe positive z-axis always points in the direction that an object is facing. So, to \nﬁ nd a unit vector representing the vehicle’s facing direction, we can simply ex-\ntract kC directly from the model-to-world matrix (by grabbing its third row). \nThis vector will already be normalized and ready to go.\n4.3.10.4. Transforming Coordinate Systems versus Vectors\nWe’ve said that the matrix \nC\nP\n→\nM\n transforms points and directions from child \nspace into parent space. Recall that the fourth row of \nC\nP\n→\nM\n contains tC , the \ntranslation of the child coordinate axes relative to the world space axes. There-\nfore, another way to visualize the matrix \nC\nP\n→\nM\n is to imagine it taking the \nparent coordinate axes and transforming them into the child axes. This is the \nreverse of what happens to points and direction vectors. In other words, if a \nmatrix transforms vectors from child space to parent space, then it also trans-\nforms coordinate axes from parent space to child space. This makes sense when \nyou think about it—moving a point 20 units to the right with the coordinate \naxes ﬁ xed is the same as moving the coordinate axes 20 units to the left  with \nthe point ﬁ xed. This concept is illustrated in Figure 4.21.\nx\ny\ncos(γ)\nsin(γ)\n–sin(γ)\ncos(γ)\nγ\nγ\niC\njC\nFigure 4.20. Change of basis when child axes are rotated by an angle γ relative to parent.\n",
      "content_length": 2244,
      "extraction_method": "Direct"
    },
    {
      "page_number": 188,
      "chapter": null,
      "content": "166 \n4. 3D Math for Games\nOf course, this is just another point of potential confusion. If you’re think-\ning in terms of coordinate axes, then transformations go in one direction, but \nif you’re thinking in terms of points and vectors, they go in the other direction! \nAs with many confusing things in life, your best bet is probably to choose \na single “canonical” way of thinking about things and stick with it. For ex-\nample, in this book we’ve chosen the following conventions:\nz Transformations apply to vectors (not coordinate axes).\nz Vectors are writt en as rows (not columns).\nTaken together, these two conventions allow us to read sequences of ma-\ntrix multiplications from left  to right and have them make sense (e.g., \n \n \n \nD\nA\nA\nB\nB\nC\nC\nD\n→\n→\n→\n=\nP\nP M\nM\nM\n). Obviously if you start thinking about the coordi-\nnate axes moving around rather than the points and vectors, you either have \nto read the transforms from right to left , or ﬂ ip one of these two conventions \naround. It doesn’t really matt er what conventions you choose as long as you \nﬁ nd them easy to remember and work with.\nThat said, it’s important to note that certain problems are easier to think \nabout in terms of vectors being transformed, while others are easier to work \nwith when you imagine the coordinate axes moving around. Once you get good \nat thinking about 3D vector and matrix math, you’ll ﬁ nd it prett y easy to ﬂ ip \nback and forth between conventions as needed to suit the problem at hand.\n4.3.11. Transforming Normal Vectors\n A normal vector is a special kind of vector, because in addition to (usually!) be-\ning of unit length, it carries with it the additional requirement that it should \nalways remain perpendicular to whatever surface or plane it is associated with. \nSpecial care must be taken when transforming a normal vector, to ensure that \nboth its length and perpendicularity properties are maintained.\nx\ny\nx'\ny'\ny\nx\nP'\nP\nP\nFigure 4.21. Two ways to interpret a transformation matrix. On the left, the point moves \nagainst a ﬁ xed set of axes. On the right, the axes move in the opposite direction while the \npoint remains ﬁ xed.\n",
      "content_length": 2142,
      "extraction_method": "Direct"
    },
    {
      "page_number": 189,
      "chapter": null,
      "content": "167 \nIn general, if a point or (non-normal) vector can be rotated from space A to \nspace B via the 3  ×  3 marix \nA\nB\n→\nM\n, then a normal vector n will be transformed \nfrom space A to space B via the inverse transpose of that matrix, \n1\nT\nA\nB\n(\n)\n−\n→\nM\n. We \nwill not prove or derive this result here (see [28], Section 3.5 for an excellent \nderivation). However, we will observe that if the matrix \nA\nB\n→\nM\n contains only \nuniform scale and no shear, then the angles between all surfaces and vectors in \nspace B will be the same as they were in space A. In this case, the matrix \nA\nB\n→\nM\nwill actually work just ﬁ ne for any vector, normal or non-normal. However, \nif \nA\nB\n→\nM\n contains nonuniform scale or shear (i.e., is non-orthogonal), then the \nangles between surfaces and vectors are not preserved when moving from \nspace A to space B. A vector that was normal to a surface in space A will not \nnecessarily be perpendicular to that surface in space B. The inverse transpose \noperation accounts for this distortion, bringing normal vectors back into per-\npendicularity with their surfaces even when the transformation involves non-\nuniform scale or shear.\n4.3.12. Storing Matrices in Memory\n In the C and C++ languages, a two-dimensional array is oft en used to store a \nmatrix. Recall that in C/C++ two-dimensional array syntax, the ﬁ rst subscript \nis the row and the second is the column, and that the column index varies fast-\nest as you move through memory sequentially.\nfloat m[4][4]; // [row][col], col varies fastest\n// \"flatten\" the array to demonstrate ordering\nfloat* pm = &m[0][0]; \nASSERT( &pm[0] == &m[0][0] );\nASSERT( &pm[1] == &m[0][1] );\nASSERT( &pm[2] == &m[0][2] );\n// etc.\nWe have two choices when storing a matrix in a two-dimensional C/C++ \narray. We can either\nstore the vectors (\n1. \niC , jC , kC , tC) contiguously in memory (i.e., each row \ncontains a single vector), or\nstore the vectors \n2. \nstrided in memory (i.e., each column contains one vector).\nThe beneﬁ t of approach (1) is that we can address any one of the four vec-\ntors by simply indexing into the matrix and interpreting the four contiguous \nvalues we ﬁ nd there as a 4-element vector. This layout also has the beneﬁ t of \nmatching up exactly with row vector matrix equations (which is another reason \nwhy I’ve selected row vector notation for this book). Approach (2) is some-\ntimes necessary when doing fast matrix-vector multiplies using a vector-en-\n4.3. Matrices\n",
      "content_length": 2465,
      "extraction_method": "Direct"
    },
    {
      "page_number": 190,
      "chapter": null,
      "content": "168 \n4. 3D Math for Games\nabled (SIMD) microprocessor, as we’ll see later in this chapter. In most game \nengines I’ve personally encountered, matrices are stored using approach (1), \nwith the vectors in the rows of the two-dimensional C/C++ array. This is shown \nbelow:\nfloat M[4][4];\nM[0][0]=ix;  M[0][1]=iy;  M[0][2]=iz;  M[0][3]=0.0f;\nM[1][0]=jx;  M[1][1]=jy;  M[1][2]=jz;  M[1][3]=0.0f;\nM[2][0]=kx;  M[2][1]=ky;  M[2][2]=kz;  M[2][3]=0.0f;\nM[3][0]=tx;  M[3][1]=ty;  M[3][2]=tz;  M[3][3]=1.0f;\nThe matrix M looks like this when viewed in a debugger:\nM[][]\n [0]\n  [0] \nix\n  [1] \niy\n  [2] \niz\n  [3] \n0.0000\n [1]\n  [0] \njx\n  [1] \njy\n  [2] \njz\n  [3] \n0.0000\n [2]\n  [0] \nkx\n  [1] \nky\n  [2] \nkz\n  [3] \n0.0000\n [3]\n  [0] \ntx\n  [1] \nty\n  [2] \ntz\n  [3] \n1.0000\nOne easy way to determine which layout your engine uses is to ﬁ nd a \nfunction that builds a 4 × 4 translation matrix. (Every good 3D math library \nprovides such a function.) You can then inspect the source code to see where \nthe elements of the t vector are being stored. If you don’t have access to the \nsource code of your math library (which is prett y rare in the game industry), \nyou can always call the function with an easy-to-recognize translation like \n(4, 3, 2), and then inspect the resulting matrix. If row 3 contains the values 4.0, \n3.0, 2.0, 1.0, then the vectors are in the rows, otherwise the vectors are in \nthe columns.\n",
      "content_length": 1395,
      "extraction_method": "Direct"
    },
    {
      "page_number": 191,
      "chapter": null,
      "content": "169 \n4.4. Quaternions\n4.4. Quaternions\n We’ve seen that a 3  ×  3 matrix can be used to represent an arbitrary rotation in \nthree dimensions. However, a matrix is not always an ideal representation of \na rotation, for a number of reasons:\nWe need nine ﬂ oating-point values to represent a rotation, which seems \n1. \nexcessive considering that we only have three degrees of freedom—\npitch, yaw, and roll.\nRotating a vector requires a vector-matrix multiplication, which involves \n2. \nthree dot products, or a total of nine multiplications and six additions. \nWe would like to ﬁ nd a rotational representation that is less expensive \nto calculate, if possible.\nIn games and computer graphics, it’s oft en important to be able to ﬁ nd \n3. \nrotations that are some percentage of the way between two known rota-\ntions. For example, if we are to smoothly animate a camera from some \nstarting orientation A to some ﬁ nal orientation B over the course of a \nfew seconds, we need to be able to ﬁ nd lots of intermediate rotations be-\ntween A and B over the course of the animation. It turns out to be diﬃ  -\ncult to do this when the A and B orientations are expressed as matrices.\nThankfully, there is a rotational representation that overcomes these three \nproblems. It is a mathematical object known as a quaternion. A quaternion \nlooks a lot like a four-dimensional vector, but it behaves quite diﬀ erently. \nWe usually write quaternions using non-italic, non-boldface type, like this: \nq = [ qx  qy  qz  qw ].\nQuaternions were developed by Sir William Rowan Hamilton in 1843 as \nan extension to the complex numbers. They were ﬁ rst used to solve prob-\nlems in the area of mechanics. Technically speaking, a quaternion obeys a \nset of rules known as a four-dimensional normed division algebra over the real \nnumbers. Thankfully, we won’t need to understand the details of these rather \nesoteric algebraic rules. For our purposes, it will suﬃ  ce to know that the unit-\nlength quaternions (i.e., all quaternions obeying the constraint qx2 + qy2 + qz2 + \nqw2 = 1) represent three-dimensional rotations.\nThere are a lot of great papers, web pages, and presentations on quater-\nnions available on the web, for further reading. Here’s one of my favorites: \nhtt p://graphics.ucsd.edu/courses/cse169_w05/CSE169_04.ppt.\n4.4.1. Unit Quaternions as 3D Rotations\nA unit quaternion can be visualized as a three-dimensional vector plus a \nfourth scalar coordinate. The vector part qV is the unit axis of rotation, scaled \n",
      "content_length": 2503,
      "extraction_method": "Direct"
    },
    {
      "page_number": 192,
      "chapter": null,
      "content": "170 \n4. 3D Math for Games\nby the sine of the half-angle of the rotation. The scalar part qS is the cosine of \nthe half-angle. So the unit quaternion q can be writt en as follows:\n \n \nwhere a is a unit vector along the axis of rotation, and θ is the angle of rota-\ntion. The direction of the rotation follows the right-hand rule , so if your thumb \npoints in the direction of a, positive rotations will be in the direction of your \ncurved ﬁ ngers.\nOf course, we can also write q as a simple four-element vector:\n \n \nA unit quaternion is very much like an axis+angle representation of a ro-\ntation (i.e., a four-element vector of the form [ a  θ ]). However, quaternions \nare more convenient mathematically than their axis+angle counterparts, as we \nshall see below.\n4.4.2. Quaternion Operations\nQuaternions support some of the familiar operations from vector algebra, \nsuch as magnitude and vector addition. However, we must remember that the \nsum of two unit quaternions does not represent a 3D rotation, because such a \nquaternion would not be of unit length. As a result, you won’t see any quater-\nnion sums in a game engine, unless they are scaled in some way to preserve \nthe unit length requirement.\n4.4.2.1. Quaternion Multiplication\n One of the most important operations we will perform on quaternions is that \nof multiplication. Given two quaternions p and q representing two rotations P \nand Q, respectively, the product pq represents the composite rotation (i.e., ro-\ntation Q followed by rotation P). There are actually quite a few diﬀ erent kinds \nof quaternion multiplication, but we’ll restrict this discussion to the variety \nused in conjunction with 3D rotations, namely the Grassman product. Using \nthis deﬁ nition, the product pq is deﬁ ned as follows:\n \npq\n(\n)\n(\n) .\nS\nV\nS\nV\nV\nV\nS S\nV\nV\np\nq\np q\n⎡\n⎤\n=\n+\n+\n×\n−\n⋅\n⎣\n⎦\nq\np\np\nq\np\nq\n \n2\n2\nq\n[\n]\n[ sin\ncos ],\nV\nS\nq\nθ\nθ\n=\n=\nq\na\n \n \n \n2\n2\n2\n2\n \nq\n[\n], where\nsin ,\nsin ,\nsin ,\n \ncos .\n \nx\ny\nz\nw\nx\nVx\nx\ny\nVy\ny\nz\nVz\nz\nw\nS\nq\nq\nq\nq\nq\nq\na\nq\nq\na\nq\nq\na\nq\nq\nθ\nθ\nθ\nθ\n=\n=\n=\n=\n=\n=\n=\n=\n=\n",
      "content_length": 2035,
      "extraction_method": "Direct"
    },
    {
      "page_number": 193,
      "chapter": null,
      "content": "171 \n4.4. Quaternions\nNotice how the Grassman product is deﬁ ned in terms of a vector part, which \nends up in the x, y, and z components of the resultant quaternion, and a scalar \npart, which ends up in the w component.\n4.4.2.2. Conjugate and Inverse\n The inverse of a quaternion q is denoted q–1 and is deﬁ ned as a quaternion \nwhich, when multiplied by the original, yields the scalar 1 (i.e., qq–1 = 0i + 0j \n+ 0k + 1). The quaternion [ 0  0  0  1 ] represents a zero rotation (which makes \nsense since sin(0) = 0 for the ﬁ rst three components, and cos(0) = 1 for the last \ncomponent).\nIn order to calculate the inverse of a quaternion, we must ﬁ rst deﬁ ne a \nquantity known as the conjugate. This is usually denoted q* and it is deﬁ ned \nas follows:\n \n \nIn other words, we negate the vector part but leave the scalar part unchaged.\nGiven this deﬁ nition of the quaternion conjugate, the inverse quaternion \nq–1 is deﬁ ned as follows:\n \n \nOur quaternions are always of unit length (i.e., |q| = 1), because they represent \n3D rotations. So, for our purposes, the inverse and the conjugate are identical:\n \n \nThis fact is incredibly useful, because it means we can always avoid doing \nthe (relatively expensive) division by the squared magnitude when inverting \na quaternion, as long as we know a priori that the quaternion is normalized. \nThis also means that inverting a quaternion is generally much faster than in-\nverting a 3  ×  3 matrix—a fact that you may be able to leverage in some situa-\ntions when optimizing your engine.\nConjugate and Inverse of a Product\n The conjugate of a quaternion product (pq) is equal to the reverse product of \nthe conjugates of the individual quaternions:\n \n \nLikewise the inverse of a quaternion product is equal to the reverse product of \nthe inverses of the individual quaternions:\n \n \n(4.3)\n \nq*\n[\n].\nV\nS\nq\n= −q\n \n \n1\n2\nq*\nq\n.\nq\n−=\n \n \n1\nq\nq*\n[\n]     when    q\n1.\nV\nS\nq\n−=\n= −\n=\nq\n(pq)*\nq* p* .\n=\n1\n1\n1\n(pq)\nq\np\n.\n−\n−\n−\n=\n",
      "content_length": 1967,
      "extraction_method": "Direct"
    },
    {
      "page_number": 194,
      "chapter": null,
      "content": "172 \n4. 3D Math for Games\nThis is analogous to the reversal that occurs when transposing or inverting \nmatrix products.\n4.4.3. Rotating Vectors with Quaternions\nHow can we apply a quaternion rotation to a vector ? The ﬁ rst step is to rewrite \nthe vector in quaternion form . A vector is a sum involving the unit basis vectors \ni, j, and k. A quaternion is a sum involving i, j, and k, but with a fourth scalar \nterm as well. So it makes sense that a vector can be writt en as a quaternion \nwith its scalar term qS equal to zero. Given the vector v, we can write a cor-\nresponding quaternion v = [ v  0 ] = [ vx  vy  vz  0 ].\nIn order to rotate a vector v by a quaternion q, we pre-multiply the vec-\ntor (writt en in its quaternion form v) by q and then post-multiply it by the \ninverse quaternion, q–1. Therefore, the rotated vector v’ can be found as fol-\nlows:\n \n \nThis is equivalent to using the quaternion conjugate, because our quaternions \nare always unit length:\n \n \n(4.4)\nThe rotated vector v’ is obtained by simply extracting it from its quaternion \nform v’.\nQuaternion multiplication can be useful in all sorts of situations in real \ngames. For example, let’s say that we want to ﬁ nd a unit vector describing the \ndirection in which an aircraft  is ﬂ ying. We’ll further assume that in our game, \nthe positive z-axis always points toward the front of an object by convention. \nSo the forward unit vector of any object in model space is always FM  ≡ [ 0  0  1 ] \nby deﬁ nition. To transform this vector into world space, we can simply take \nour aircraft ’s orientation quaternion q and use it with Equation (4.4) to rotate \nour model-space vector FM into its world space equivalent FW (aft er converting \nthese vectors into quaternion form, of course):\n \n \n4.4.3.1. Quaternion Concatenation\n Rotations can be concatenated in exactly the same way that matrix-based trans-\nformations can, by multiplying the quaternions together. For example, consid-\ner three distinct rotations, represented by the quaternions q1 , q2 , and q3 , with \nmatrix equivalents R1 , R2 , and R3. We want to apply rotation 1 ﬁ rst, followed \nby rotation 2 and ﬁ nally rotation 3. The composite rotation matrix Rnet can be \nfound and applied to a vector v as follows:\nv\nrotate(q, )\nqvq* .\n′=\n=\nv\n \n1\n1\nF\nqF q\nq [0\n0\n1\n0] q\n.\nW\nM\n−\n−\n=\n=\n1\nv\nrotate(q, )\nqvq\n.\n−\n′=\n=\nv\n",
      "content_length": 2352,
      "extraction_method": "Direct"
    },
    {
      "page_number": 195,
      "chapter": null,
      "content": "173 \n4.4. Quaternions\n \n \nLikewise, the composite rotation quaternion qnet can be found and applied to \nvector v (in its quaternion form, v) as follows:\n \n \nNotice how the quaternion product must be performed in an order opposite \nto that in which the rotations are applied (q3q2q1). This is because quaternion \nrotations always multiply on both sides of the vector, with the uninverted \nquaternions on the left  and the inverted quaternions on the right. As we saw \nin Equation (4.3), the inverse of a quaternion product is the reverse product of \nthe individual inverses, so the uninverted quaternions read right-to-left  while \nthe inverted quaternions read left -to-right.\n4.4.4. Quaternion-Matrix Equivalence\n We can convert any 3D rotation freely between a 3  ×  3 matrix representation \nR and a quaternion representation q. If we let q = [ qV  qS ] = [ qVx  qVy  qVz  qS ] = \n[ x  y  z  w ], then we can ﬁ nd R as follows:\n \n \n Likewise, given R we can ﬁ nd q as follows (where q[0] = qVx , q[1] = qVy , \nq[2] = qVz , and q[3] = qS). This code assumes that we are using row vectors \nin C/C++ (i.e., that the rows of matrix R[row][col] correspond to the rows \nof the matrix R shown above). The code was adapted from a Gamasutra article \nby Nick Bobic, published on July 5, 1998, which is available here: htt p://www.\ngamasutra.com/view/feature/3278/rotating_objects_using_quaternions.php. \nFor a discussion of some even faster methods for converting a matrix to a \nquaternion, leveraging various assumptions about the nature of the matrix, \nsee htt p://www.euclideanspace.com/maths/geometry/rotations/conversions/\nmatrixToQuaternion/index.htm.\nvoid matrixToQuaternion(\n \nconst float R[3][3], \n \nfloat   \n  q[/*4*/])\n{\n \nfloat trace = R[0][0] + R[1][1] + R[2][2];\nnet\n1\n2\n3\n1\n2\n3\nnet\n;\n.\n=\n′=\n=\nR\nR R R\nv\nvR R R\nvR\nnet\n3\n2\n1\n1\n1\n1\n1\n3\n2\n1\n1\n2\n3\nnet\nnet\nq\nq q q ;\nv\nq q q vq\nq\nq\nq\nvq\n.\n−\n−\n−\n−\n=\n′=\n=\n \n2\n2\n2\n2\n2\n2\n1\n2\n2\n2\n2\n2\n2\n2\n2\n1\n2\n2\n2\n2\n.\n2\n2\n2\n2\n1\n2\n2\ny\nz\nxy\nzw\nxz\nyw\nxy\nzw\nx\nz\nyz\nxw\nxz\nyw\nyz\nxw\nx\ny\n⎡\n⎤\n−\n−\n+\n−\n⎢\n⎥\n=\n−\n−\n−\n+\n⎢\n⎥\n⎢\n⎥\n+\n−\n−\n−\n⎣\n⎦\nR\n",
      "content_length": 2059,
      "extraction_method": "Direct"
    },
    {
      "page_number": 196,
      "chapter": null,
      "content": "174 \n4. 3D Math for Games\n \n// check the diagonal\n \nif (trace > 0.0f)\n {\n \n  \nfloat s = sqrt(trace + 1.0f);\n \n  \nq[3] = s * 0.5f;\n \n  \nfloat t = 0.5f / s;\n \n  \nq[0] = (R[2][1] - R[1][2]) * t;\n \n  \nq[1] = (R[0][2] - R[2][0]) * t;\n \n  \nq[2] = (R[1][0] - R[0][1]) * t;\n }\n else\n {\n \n  \n// diagonal is negative\n     \nint i = 0;\n \n  \nif (R[1][1] > R[0][0]) i = 1;\n \n  \nif (R[2][2] > R[i][i]) i = 2;\n \n  \nstatic const int NEXT[3] = {1, 2, 0};\n \n  \nj = NEXT[i];\n \n  \nk = NEXT[j];\n \n  \nfloat s = sqrt ((R[i][i] \n  \n \n    \n     \n      -  (R[j][j] \n+ R[k][k])) \n  \n \n    \n        + \n1.0f);\n \n  \nq[i] = s * 0.5f;\n \n  float t;\n \n  \nif (s != 0.0)  \nt = 0.5f / s;\n  \n \nelse     t \n= s;\n \n  \nq[3] = (R[k][j] - R[j][k]) * t;\n \n  \nq[j] = (R[j][i] + R[i][j]) * t;\n \n  \nq[k] = (R[k][i] + R[i][k]) * t;\n }\n}\n4.4.5. Rotational Linear Interpolation\n Rotational interpolation has many applications in the animation, dynamics \nand camera systems of a game engine. With the help of quaternions, rotations \ncan be easily interpolated just as vectors and points can.\nThe easiest and least computationally intensive approach is to perform \na four-dimensional vector LERP on the quaternions you wish to interpolate. \nGiven two quaternions qA and qB representing rotations A and B, we can \nﬁ nd an intermediate rotation qLERP that is β percent of the way from A to B as \nfollows:\n",
      "content_length": 1350,
      "extraction_method": "Direct"
    },
    {
      "page_number": 197,
      "chapter": null,
      "content": "175 \nNotice that the resultant interpolated quaternion had to be renormalized. This \nis necessary because the LERP operation does not preserve a vector’s length \nin general.\nGeometrically, qLERP = LERP(qA , qB , β) is the quaternion whose orientation \nlies  β percent of the way from orientation A to orientation B, as shown (in two \ndimensions for clarity) in Figure 4.22. Mathematically, the LERP operation re-\nsults in a weighed average of the two quaternions, with weights (1 – β) and β \n(notice that (1 – β) + β = 1).\n4.4.5.1. Spherical Linear Interpolation\n The problem with the LERP operation is that it does not take account of the \nfact that quaternions are really points on a four-dimensional hypersphere. A \nLERP eﬀ ectively interpolates along a chord of the hypersphere, rather than \nalong the surface of the hypersphere itself. This leads to rotation animations \nthat do not have a constant angular speed when the parameter β is changing \nat a constant rate. The rotation will appear slower at the end points and faster \nin the middle of the animation.\n \nLERP\n(1\n)q\nq\nq\nLERP(q ,q , )\n(1\n)q\nq\n(1\n)\n(1\n)\nnormalize\n.\n(1\n)\n(1\n)\nA\nB\nA\nB\nA\nB\nT\nAx\nBx\nAy\nBy\nAz\nBz\nAw\nBw\nq\nq\nq\nq\nq\nq\nq\nq\n−β\n+ β\n=\nβ =\n−β\n+ β\n⎛\n⎞\n−β\n+ β\n⎡\n⎤\n⎜\n⎟\n⎢\n⎥\n⎜\n⎟\n⎢\n⎥\n−β\n+ β\n⎜\n⎟\n⎢\n⎥\n=\n⎜\n⎟\n⎢\n⎥\n−β\n+ β\n⎜\n⎟\n⎢\n⎥\n⎜\n⎟\n⎢\n⎥\n⎜\n⎟\n−β\n+ β\n⎣\n⎦\n⎝\n⎠\nqA (β = 0)\nqLERP = LERP(qA, qB, 0.4)\nqB (β = 1)\nFigure 4.22. Linear interpolation (LERP) between quaternions qA and qB.\n4.4. Quaternions\n",
      "content_length": 1446,
      "extraction_method": "Direct"
    },
    {
      "page_number": 198,
      "chapter": null,
      "content": "176 \n4. 3D Math for Games\nTo solve this problem, we can use a variant of the LERP operation known \nas spherical linear interpolation, or SLERP for short. The SLERP operation uses \nsines and cosines to interpolate along a great circle of the 4D hypersphere, \nrather than along a chord, as shown in Figure 4.23. This results in a constant \nangular speed when β varies at a constant rate.\nThe formula for SLERP is similar to the LERP formula, but the weights \n(1 – β) and β are replaced with weights wp and wq involving sines of the angle \nbetween the two quaternions.\nwhere\nThe cosine of the angle between any two unit-length quaternions can \nbe found by taking their four-dimensional dot product. Once we know \ncos(θ), we can calculate the angle θ and the various sines we need quite \neasily:\n \n \nqA (β = 0)\nqLERP = LERP(qA, qB, 0.4)\nqB (β = 1)\nqSLERP = SLERP(qA, qB, 0.4)\n0.4 along chord\n0.4 along arc\nFigure 4.23. Spherical linear interpolation along a great circle arc of a 4D hypersphere.\nSLERP(p,q, )\np\nq,\np\nq\nw\nw\nβ =\n+\nsin((1\n) ) ,\nsin( )\nsin(\n).\nsin( )\np\nq\nw\nw\n−β θ\n=\nθ\nβθ\n=\nθ\n1\ncos( )\np q\n;\ncos\n(p q).\nx x\ny\ny\nz\nz\nw w\np q\np q\np q\np q\n−\nθ =    =\n+\n+\n+\nθ=\n⋅\n⋅\n",
      "content_length": 1165,
      "extraction_method": "Direct"
    },
    {
      "page_number": 199,
      "chapter": null,
      "content": "177 \n4.4.5.2. To SLERP or Not to SLERP (That’s Still the Question)\nThe jury is still out on whether or not to use SLERP in a game engine. Jonathan \nBlow wrote a great article positing that SLERP is too expensive, and LERP’s \nquality is not really that bad—therefore, he suggests, we should understand \nSLERP but avoid it in our game engines (see htt p://number-none.com/prod-\nuct/Understanding%20Slerp,%20Then%20Not%20Using%20It/index.html). \nOn the other hand, some of my colleagues at Naughty Dog have found that \na good SLERP implementation performs nearly as well as LERP. (For exam-\nple, on the PS3’s SPUs, Naughty Dog’s Ice team’s implementation of SLERP \ntakes 20 cycles per joint, while its LERP implementation takes 16.25 cycles per \njoint.) Therefore, I’d personally recommend that you proﬁ le your SLERP and \nLERP implementations before making any decisions. If the performance hit \nfor SLERP isn’t unacceptable, I say go for it, because it may result in slightly \nbett er-looking animations. But if your SLERP is slow (and you cannot speed \nit up, or you just don’t have the time to do so), then LERP is usually good \nenough for most purposes.\n4.5. Comparison of Rotational Representations\nWe’ve seen that rotations can be represented in quite a few diﬀ erent ways. \nThis section summarizes the most common rotational representations and \noutlines their pros and cons. No one representation is ideal in all situations. \nUsing the information in this section, you should be able to select the best \nrepresentation for a particular application.\n4.5.1. Euler Angles\n We brieﬂ y explored Euler angles in Section 4.3.9.1. A rotation represented via \nEuler angles consists of three scalar values: yaw, pitch, and roll. These quanti-\nties are sometimes represented by a 3D vector [ θY  θP  θR ].\nThe beneﬁ ts of this representation are its simplicity, its small size (three \nﬂ oating-point numbers), and its intuitive nature—yaw, pitch, and roll are easy \nto visualize. You can also easily interpolate simple rotations about a single axis. \nFor example, it’s trivial to ﬁ nd intermediate rotations between two distinct yaw \nangles by linearly interpolating the scalar θY. However, Euler angles cannot be \ninterpolated easily when the rotation is about an arbitrarily-oriented axis.\nIn addition, Euler angles are prone to a condition known as gimbal lock . \nThis occurs when a 90-degree rotation causes one of the three principal axes \nto “collapse” onto another principal axis. For example, if you rotate by 90 \ndegrees about the x-axis, the y-axis collapses onto the z-axis. This prevents \n4.5. Comparison of Rotational Representations\n",
      "content_length": 2642,
      "extraction_method": "Direct"
    },
    {
      "page_number": 200,
      "chapter": null,
      "content": "178 \n4. 3D Math for Games\nany further rotations about the original y-axis, because rotations about y and z \nhave eﬀ ectively become equivalent.\nAnother problem with Euler angles is that the order in which the rotations \nare performed around each axis matt ers. The order could be PYR, YPR, RYP, \nand so on, and each ordering may produce a diﬀ erent composite rotation. \nNo one standard rotation order exists for Euler angles across all disciplines \n(although certain disciplines do follow speciﬁ c conventions). So the rotation \nangles [ θY  θP  θR ] do not uniquely deﬁ ne a particular rotation—you need to \nknow the rotation order to interpret these numbers properly.\nA ﬁ nal problem with Euler angles is that they depend upon the mapping \nfrom the x-, y -, and z-axes onto the natural front, left /right, and up directions for \nthe object being rotated. For example, yaw is always deﬁ ned as rotation about \nthe up axis, but without additional information we cannot tell whether this \ncorresponds to a rotation about x, y, or z.\n4.5.2. 3 × 3 M atrices\n A 3 × 3 matrix is a convenient and eﬀ ective rotational representation for a \nnumber of reasons. It does not suﬀ er from gimbal lock , and it can represent \narbitrary rotations uniquely. Rotations can be applied to points and vectors in \na straightforward manner via matrix multiplication (i.e., a series of dot prod-\nucts). Most CPUs and all GPUs now have built-in support for hardware-accel-\nerated dot products and matrix multiplication. Rotations can also be reversed \nby ﬁ nding an inverse matrix, which for a pure rotation matrix is the same \nthing as ﬁ nding the transpose—a trivial operation. And 4 × 4 matrices oﬀ er a \nway to represent arbitrary aﬃ  ne transformations—rotations, translations, and \nscaling—in a totally consistent way.\nHowever, rotation matrices are not particularly intuitive. Looking at a big \ntable of numbers doesn’t help one picture the corresponding transformation \nin three-dimensional space. Also, rotation matrices are not easily interpolated. \nFinally, a rotation matrix takes up a lot of storage (nine ﬂ oating-point num-\nbers) relative to Euler angles.\n4.5.3. Axis + Angle\n We can represent rotations as a unit vector deﬁ ning the axis of rotation plus a \nscalar for the angle of rotation. This is known as an axis+angle representation, \nand it is sometimes denoted by the four-dimensional vector [ a  θ ] , where a \nis the axis of rotation and θ the angle in radians. In a right-handed coordinate \nsystem, the direction of a positive rotation is deﬁ ned by the right-hand rule, \nwhile in a left -handed system we use the left -hand rule instead.\n",
      "content_length": 2642,
      "extraction_method": "Direct"
    },
    {
      "page_number": 201,
      "chapter": null,
      "content": "179 \nThe beneﬁ ts of the axis+angle representation are that it is reasonably intu-\nitive and also compact (only requires four ﬂ oating-point numbers, as opposed \nto the nine required for a 3 × 3 matrix).\nOne important limitation of the axis+angle representation is that rota-\ntions cannot be easily interpolated. Also, rotations in this format cannot be \napplied to points and vectors in a straightforward way—one needs to convert \nthe axis+angle representation into a matrix or quaternion ﬁ rst.\n4.5.4. Quaternions\n As we’ve seen, a unit-length quaternion can represent 3D rotations in a man-\nner analogous to the axis+angle representation. The primary diﬀ erence be-\ntween the two representations is that a quaternion’s axis of rotation is scaled \nby the sine of the half angle of rotation, and instead of storing the angle in the \nfourth component of the vector, we store the cosine of the half angle.\nThe quaternion formulation provides two immense beneﬁ ts over the \naxis+angle representation. First, it permits rotations to be concatenated and \napplied directly to points and vectors via quaternion multiplication. Second, \nit permits rotations to be easily interpolated via simple LERP or SLERP op-\nerations. Its small size (four ﬂ oating-point numbers) is also a beneﬁ t over the \nmatrix formulation.\n4.5.5. SQT Transformations\n By itself, a quaternion can only represent a rotation, whereas a 4  ×  4 matrix \ncan represent an arbitrary aﬃ  ne transformation (rotation, translation, and \nscale). When a quaternion is combined with a translation vector and a scale \nfactor (either a scalar for uniform scaling or a vector for nonuniform scaling), \nthen we have a viable alternative to the 4  ×  4 matrix representation of aﬃ  ne \ntransformations. We sometimes call this an SQT transform, because it contains \na scale factor, a quaternion for rotation, and a translation vector.\nor \nSQT transforms are widely used in computer animation because of their \nsmaller size (eight ﬂ oats for uniform scale, or ten ﬂ oats for nonuniform scale, \nas opposed to the 12 ﬂ oating-point numbers needed for a 4  ×  3 matrix) and \ntheir ability to be easily interpolated. The translation vector and scale factor \nare interpolated via LERP, and the quaternion can be interpolated with either \nLERP or SLERP.\nSQT\n[\nq\n]   (uniform scale ),\ns\ns\n=\nt\nSQT\n[\nq\n]   (non-uniform scale vector ).\n= s\nt\ns\n4.5. Comparison of Rotational Representations\n",
      "content_length": 2430,
      "extraction_method": "Direct"
    },
    {
      "page_number": 202,
      "chapter": null,
      "content": "180 \n4. 3D Math for Games\n4.5.6. Dual Quaternions\n Complete transformations involving rotation, translation, and scale can be \nrepresented using a mathematical object known as a dual quaternion. A dual \nquaternion is like an ordinary quaternion, except that its four components are \ndual numbers instead of regular real-valued numbers. A dual number can be \nwritt en as the sum of a non-dual part and a dual part as follows: \n0\nˆ\n.\na\na\naε\n=\n+ε\nHere ε is a magical number called the dual unit, deﬁ ned as \n2\n0.\nε =\n (This is \nanalogous to the imaginary number \n1\ni= − used when writing a complex \nnumber as the sum of a real and an imaginary part: \n.\nc\na\nib\n= +\n)\nBecause each dual number can be represented by two real numbers \n(the non-dual and dual parts), a dual quaternion can be represented by an \neight-element vector. It can also be represented as the sum of two ordinary \nquaternions, where the second one is multiplied by the dual unit, as follows: \n \n \nA full discussion of dual numbers and dual quaternions is beyond our \nscope here. However, a number of excellent articles on them exist online and \nin the literature. I recommend starting with htt ps://www.cs.tcd.ie/publica-\ntions/tech-reports/reports.06/TCD-CS-2006-46.pdf.\n4.5.7. Rotations and Degrees of Freedom\nThe term “degrees of freedom ” (or DOF for short) refers to the number of mu-\ntually-independent ways in which an object’s physical state (position and ori-\nentation) can change. You may have encountered the phrase “six degrees of \nfreedom” in ﬁ elds such as mechanics, robotics, and aeronautics. This refers \nto the fact that a three-dimensional object (whose motion is not artiﬁ cially \nconstrained) has three degrees of freedom in its translation (along the x-, y-, \nand z-axes) and three degrees of freedom in its rotation (about the x-, y-, and \nz-axes), for a total of six degrees of freedom.\nThe DOF concept will help us to understand how diﬀ erent rotational rep-\nresentations can employ diﬀ erent numbers of ﬂ oating-point parameters, yet \nall specify rotations with only three degrees of freedom. For example, Euler \nangles require three ﬂ oats, but axis+angle and quaternion representations use \nfour ﬂ oats, and a 3  ×  3 matrix takes up nine ﬂ oats. How can these representa-\ntions all describe 3-DOF rotations?\nThe answer lies in constraints . All 3D rotational representations employ \nthree or more ﬂ oating-point parameters, but some representations also have \none or more constraints on those parameters. The constraints indicate that the \nparameters are not independent—a change to one parameter induces changes \nto the other parameters in order to maintain the validity of the constraint(s). \n0\nˆq\nq\nq .\nε\n=\n+ε\n",
      "content_length": 2708,
      "extraction_method": "Direct"
    },
    {
      "page_number": 203,
      "chapter": null,
      "content": "181 \nIf we subtract the number of constraints from the number of ﬂ oating-point \nparameters, we arrive at the number of degrees of freedom—and this number \nshould always be three for a 3D rotation:\n \nNDOF = Nparameters – Nconstraints. \n(4.5)\nThe following list shows Equation (4.5) in action for each of the rotational \nrepresentations we’ve encountered in this book.\nz Euler Angles. 3 parameters – 0 constraints = 3 DOF.\nz Axis+Angle. 4 parameters – 1 constraint = 3 DOF.\n \nConstraint: Axis is constrained to be unit length.\nz Quaternion. 4 parameters – 1 constraint = 3 DOF.\n \nConstraint: Quaternion is constrained to be unit length.\nz 3  ×  3 Matrix. 9 parameters – 6 constraints = 3 DOF.\n \nConstraints: All three rows and all three columns must be of unit length \n(when treated as three-element vectors).\n4.6. Other Useful Mathematical Objects\nAs game engineers, we will encounter a host of other mathematical objects, \nin addition to points, vectors, matrices and quaternions. This section brieﬂ y \noutlines the most common of these.\n4.6.1. \nLines, Rays, and Line Segments\nAn inﬁ nite line can be represented by a point P0 plus a unit vector u in the \ndirection of the line. A parametric equation of a line traces out every possible \npoint P along the line by starting at the initial point P0 and moving an arbi-\ntrary distance t along the direction of the unit vector u. The inﬁ nitely large set \nof points P becomes a vector function of the scalar parameter t:\n \nP(t) = P0 + t u,  where   –∞ < t < +∞. \n(4.73)\nThis is depicted in Figure 4.24.\nt = 0\nt = 1\nt = 2\nt = 3\nt = –1\nu\nP0\nFigure 4.24. Parametric equation of a line.\n4.6. Other Useful Mathematical Objects\n",
      "content_length": 1669,
      "extraction_method": "Direct"
    },
    {
      "page_number": 204,
      "chapter": null,
      "content": "182 \n4. 3D Math for Games\nx\nz\ny\nC\nr\nFigure 4.27. Point-radius representation of a sphere.\nA ray is a line that extends to inﬁ nity in only one direction. This is easily \nexpressed as P(t) with the constraint t ≥ 0, as shown in Figure 4.25.\nA line segment is bounded at both ends by P0 and P1. It too can be repre-\nsented by P(t), in either one of the following two ways (where L = P1 – P0 and \nL = |L| is the length of the line segment):\n \n1. P(t) = P0 + tu,  where 0 ≤ t ≤ L, or\n \n2. P(t) = P0 + tL, where 0 ≤ t ≤ 1.\nThe latt er format, depicted in Figure 4.26, is particularly convenient because \nthe parameter t is normalized; in other words, t always goes from zero to one, \nno matt er which particular line segment we are dealing with. This means we \ndo not have to store the constraint L in a separate ﬂ oating-point parameter; it \nis already encoded in the vector L = Lu (which we have to store anyway).\nt = 0\nt = 1\nt = 2\nt = 3\nu\nP0\nFigure 4.25. Parametric equation of a ray.\nt = 0\nt = 1\nL = P1 – P0\nP0\nP1\nt = 0.5\nFigure 4.26. Parametric equation of a line segment, with normalized parameter t.\n4.6.2. Spheres\nSpheres are ubiquitous in game engine programming. A sphere is typically \ndeﬁ ned as a center point C plus a radius r, as shown in Figure 4.27. This packs \n",
      "content_length": 1273,
      "extraction_method": "Direct"
    },
    {
      "page_number": 205,
      "chapter": null,
      "content": "183 \nnicely into a four-element vector, [ Cx Cy Cz r ]. As we’ll see below when we dis-\ncuss SIMD vector processing, there are distinct beneﬁ ts to being able to pack \ndata into a vector containing four 32-bit ﬂ oats (i.e., a 128-bit package).\n4.6.3. Planes\n A plane is a 2D surface in 3D space. As you may recall from high school alge-\nbra, the equation of a plane is oft en writt en as follows:\n \nAx + By + Cz + D = 0. \nThis equation is satisﬁ ed only for the locus of points P = [ x  y  z ] that lie on \nthe plane.\nPlanes can be represented by a point P0 and a unit vector n that is normal \nto the plane. This is sometimes called point-normal form , as depicted in Fig-\nure 4.28.\nIt’s interesting to note that when the parameters A, B, and C from the tra-\nditional plane equation are interpreted as a 3D vector, that vector lies in the di-\nrection of the plane normal. If the vector [ A  B  C ] is normalized to unit length, \nthen the normalized sub-vector [ a  b  c ] = n, and the normalized parameter \n2\n2\n2\nd\nD\nA\nB\nC\n=\n+\n+\n is just the distance from the plane to the origin . The sign \nof d is positive if the plane’s normal vector (n) is pointing toward the origin \n(i.e., the origin is on the “front” side of the plane) and negative if the normal \nis pointing away from the origin (i.e., the origin is “behind” the plane). In \nfact, the normalized equation ax + by + cz + d = 0 is just another way of writing \n(n x P) = –  d, which means that when any point P on the plane is projected onto \nthe plane normal n, the length of that projection will be –  d.\nA plane can actually be packed into a four-element vector, much like a \nsphere can. To do so, we observe that to describe a plane uniquely, we need \nonly the normal vector n = [ a  b  c ] and the distance from the origin d. The \nfour-element vector L = [ n  d ] = [ a  b  c  d ] is a compact and convenient way \nto represent and store a plane in memory. Note that when P is writt en in ho-\nmogeneous coordinates with w = 1, the equation (L x P) = 0 is yet another way \nof writing (n x P) = –  d. (These equations are satisﬁ ed for all points P that lie \non the plane L.)\nPlanes deﬁ ned in four-element vector form can be easily transformed \nfrom one coordinate space to another. Given a matrix \nA\nB\n→\nM\n that transforms \npoints and (non-normal) vectors from space A to space B, we already know \nthat to transform a normal vector such as the plane’s n vector, we need to use \nthe inverse transpose of that matrix, \n1\nT\nA\nB\n(\n)\n−\n→\nM\n. So it shouldn’t be a big surprise \nto learn that applying the inverse transpose of a matrix to a four-element plane \nvector L will, in fact, correctly transform that plane from space A to space B. \nP0\nn\nFigure 4.28. A plane \nin point-normal form.\n4.6. Other Useful Mathematical Objects\n",
      "content_length": 2784,
      "extraction_method": "Direct"
    },
    {
      "page_number": 206,
      "chapter": null,
      "content": "184 \n4. 3D Math for Games\nWe won’t derive or prove this result any further here, but a thorough explana-\ntion of why this litt le “trick” works is provided in Section 4.2.3 of [28].\n4.6.4. Axis-Aligned Bounding Boxes (AABB)\nAn axis-aligned bounding box (AABB) is a 3D cuboid whose six rectangular \nfaces are aligned with a particular coordinate frame’s mutually orthogonal \naxes. As such, an AABB can be represented by a six-element vector containing \nthe minimum and maximum coordinates along each of the 3 principal axes, \n[ xmin , xmax , ymin , ymax , zmin , zmax ], or two points Pmin and Pmax.\nThis simple representation allows for a particularly convenient and in-\nexpensive method of testing whether a point P is inside or outside any given \nAABB. We simply test if all of the following conditions are true:\n        Px ≥ xmin  and  Px ≤ xmax  and\n \n        Py ≥ ymin  and  Py ≤ ymax  and \nPz ≥ zmin   and  Pz ≤ zmax.\nBecause intersection tests are so speedy, AABBs are oft en used as an “early \nout” collision check; if the AABBs of two objects do not intersect, then there is \nno need to do a more detailed (and more expensive) collision test.\n4.6.5. Oriented Bounding Boxes (OBB)\nAn oriented bounding box (OBB) is a cuboid that has been oriented so \nas to align in some logical way with the object it bounds. Usually an OBB \naligns with the local-space axes of the object. Hence it acts like an AABB \nin local space, although it may not necessarily align with the world space \naxes.\nVarious techniques exist for testing whether or not a point lies within \nan OBB, but one common approach is to transform the point into the OBB’s \n“aligned” coordinate system and then use an AABB intersection test as pre-\nsented above.\n4.6.6. Frusta\nAs shown in Figure 4.29, a frustum is a group of six planes that deﬁ ne a trun-\ncated pyramid shape. Frusta are commonplace in 3D rendering because they \nconveniently deﬁ ne the viewable region of the 3D world when rendered via a \nperspective projection from the point of view of a virtual camera. Four of the \nplanes bound the edges of the screen space, while the other two planes repre-\nsent the the near and far clipping planes (i.e., they deﬁ ne the minimum and \nmaximum z coordinates possible for any visible point).\nNear\nFar\nLeft\nRight\nTop\nBottom\nFigure 4.29. A frustum.\n",
      "content_length": 2319,
      "extraction_method": "Direct"
    },
    {
      "page_number": 207,
      "chapter": null,
      "content": "185 \n4.7. Hardware-Accelerated SIMD Math\nOne convenient representation of a frustum is as an array of six planes, \neach of which is represented in point-normal form (i.e., one point and one \nnormal vector per plane).\nTesting whether a point lies inside a frustum is a bit involved, but the basic \nidea is to use dot products to determine whether the point lies on the front or \nback side of each plane. If it lies inside all six planes, it is inside the frustum.\nA helpful trick is to transform the world-space point being tested, by \napplying the camera’s perspective projection to it. This takes the point from \nworld space into a space known as homogeneous clip space. In this space, the \nfrustum is just an axis-aligned cuboid (AABB). This permits much simpler in/\nout tests to be performed.\n4.6.7. Convex Polyhedral Regions\nA convex polyhedral region is deﬁ ned by an arbitrary set of planes, all with nor-\nmals pointing inward (or outward). The test for whether a point lies inside \nor outside the volume deﬁ ned by the planes is relatively straightforward; it \nis similar to a frustum test, but with possibly more planes. Convex regions \nare very useful for implementing arbitrarily-shaped trigger regions in games. \nMany engines employ this technique; for example, the Quake engine’s ubiqui-\ntous brushes are just volumes bounded by planes in exactly this way.\n4.7. Hardware-Accelerated SIMD Math\nSIMD stands for “single instruction multiple data .” This refers to the ability of \nmost modern microprocessors to perform a single mathematical operation on \nmultiple data items in parallel, using a single machine instruction. For exam-\nple, the CPU might multiply four pairs of ﬂ oating-point numbers in parallel \nwith a single instruction. SIMD is widely used in game engine math libraries, \nbecause it permits common vector operations such as dot products and matrix \nmultiplication to be performed extremely rapidly.\nIntel ﬁ rst introduced MMX instructions with their Pentium line of CPUs \nin 1994. These instructions permitt ed SIMD calculations to be performed on \n8-, 16-, and 32-bit integers packed into special 64-bit MMX registers. Intel fol-\nlowed this up with various revisions of an extended instruction set called \nStreaming SIMD Extensions, or SSE, the ﬁ rst version of which appeared in the \nPentium III processor. The SSE instruction set utilizes 128-bit registers that can \ncontain integer or IEEE ﬂ oating-point data.\nThe SSE mode most commonly used by game engines is called packed 32-\nbit ﬂ oating-point mode. In this mode, four 32-bit float values are packed into \n",
      "content_length": 2593,
      "extraction_method": "Direct"
    },
    {
      "page_number": 208,
      "chapter": null,
      "content": "186 \n4. 3D Math for Games\na single 128-bit register; four operations such as additions or multiplications \nare performed in parallel on four pairs of ﬂ oats using a single instruction. This \nis just what the doctor ordered when multiplying a four-element vector by a \n4  ×  4 matrix!\n4.7.1.1. \nSSE Registers\nIn packed 32-bit ﬂ oating-point mode, each 128-bit SSE register contains four \n32-bit ﬂ oats. The individual ﬂ oats within an SSE register are conveniently re-\nferred to as [ x  y  z  w ], just as they would be when doing vector/matrix math \nin homogeneous coordinates on paper (see Figure 4.30). To see how the SSE \nregisters work, here’s an example of a SIMD instruction:\n \naddps xmm0, xmm1\nThe addps instruction adds the four ﬂ oats in the 128-bit XMM0 register with \nthe four ﬂ oats in the XMM1 register, and stores the four results back into \nXMM0. Put another way:\n \nxmm0.x = xmm0.x + xmm1.x;\n \nxmm0.y = xmm0.y + xmm1.y; \n \nxmm0.z = xmm0.z + xmm1.z;\n \n  xmm0.w = xmm0.w + xmm1.w.\nThe four ﬂ oating-point values stored in an SSE register can be extracted \nto or loaded from memory or registers individually, but such operations tend \nto be comparatively slow. Moving data between the x87 FPU registers and the \nSSE registers is particularly bad, because the CPU has to wait for either the x87 \nor the SSE unit to spit out its pending calculations. This stalls out the CPU’s \nentire instruction execution pipeline and results in a lot of wasted cycles. In a \nnutshell, code that mixes regular float mathematics with SSE mathematics \nshould be avoided like the plague.\nTo minimize the costs of going back and forth between memory, x87 FPU \nregisters, and SSE registers, most SIMD math libraries do their best to leave \ndata in the SSE registers for as long as possible. This means that even scalar \nvalues are left  in SSE registers, rather than transferring them out to float\nvariables. For example, a dot product between two vectors produces a scalar \nresult, but if we leave that result in an SSE register it can be used later in other \nx\ny\nz\nw\n32 bits\n32 bits\n32 bits\n32 bits\nFigure 4.30. The four components of an SSE register in 32-bit ﬂ oating-point mode.\n",
      "content_length": 2174,
      "extraction_method": "Direct"
    },
    {
      "page_number": 209,
      "chapter": null,
      "content": "187 \n4.7. Hardware-Accelerated SIMD Math\nvector calculations without incurring a transfer cost. Scalars are represented \nby duplicating the single ﬂ oating-point value across all four “slots” in an SSE \nregister. So to store the scalar s in an SSE register, we’d set x = y = z = w = s.\n4.7.1.2. The __m128 Data Type\nUsing one of these magic SSE 128-bit values in C or C++ is quite easy. The \nMicrosoft  Visual Studio compiler provides a predeﬁ ned data type called \n__m128. This data type can be used to declare global variables, automatic vari-\nables, and even class and structure members. In many cases, variables of this \ntype will be stored in RAM. But when used in calculations, __m128 values are \nmanipulated directly in the CPU’s SSE registers. In fact, declaring automatic \nvariables and function arguments to be of type __m128 oft en results in the \ncompiler storing those values directly in SSE registers, rather than keeping \nthem in RAM on the program stack.\nAlignment of __m128 Variables\n When an __m128 variable is stored in RAM, it is the programmer’s responsi-\nbility to ensure that the variable is aligned to a 16-byte address boundary. This \nmeans that the hexadecimal address of an __m128 variable must always end \nin the nibble 0x0. The compiler will automatically pad structures and classes \nso that if the entire struct or class is aligned to a 16-byte boundary, all of the \n__m128 data members within it will be properly aligned as well. If you de-\nclare an automatic or global struct/class containing one or more __m128s, the \ncompiler will align the object for you. However, it is still your responsibility \nto align dynamically allocated data structures (i.e., data allocated with new or \nmalloc()); the compiler can’t help you there.\n4.7.1.3. \nCoding with SSE Intrinsics\nSSE mathematics can be done in raw assembly language, or via inline assem-\nbly in C or C++. However, writing code like this is not only non-portable, it’s \nalso a big pain in the butt . To make life easier, modern compilers provide \nintrinsics —special commands that look and behave like regular C functions, \nbut are really boiled down to inline assembly code by the compiler. Many in-\ntrinsics translate into a single assembly language instruction, although some \nare macros that translate into a sequence of instructions.\nIn order to use the __m128 data type and SSE intrinsics, your .cpp ﬁ le \nmust #include <xmmintrin.h>.\nAs an example, let’s take another look at the addps assembly language \ninstruction. This instruction can be invoked in C/C++ using the intrinsic _mm\n_add_ps(). Here’s a side-by-side comparison of what the code would look \nlike with and without the use of the intrinsic.\n",
      "content_length": 2695,
      "extraction_method": "Direct"
    },
    {
      "page_number": 210,
      "chapter": null,
      "content": "188 \n4. 3D Math for Games\n__m128 addWithAssembly(\n   __m128 a,\n   __m128 b)\n{\n   __m128 r;\n   __asm\n   {\n      movaps xmm0, \n \n xmmword ptr [a]\n      movaps xmm1, \n \n xmmword ptr [b]\n      addps  xmm0, xmm1\n      movaps xmmword ptr [r],\n \n xmm0\n   }\n   return r;\n}\n__m128 addWithIntrinsics(\n   __m128 a,\n   __m128 b)\n{\n   __m128 r = \n_mm_add_ps(a, b);\n   return r;\n}\nIn the assembly language version, we have to use the __asm keyword to \ninvoke inline assembly instructions, and we must create the linkage between \nthe input parameters a and b and the SSE registers xmm0 and xmm1 manually, \nvia movaps instructions. On the other hand, the version using intrinsics is \nmuch more intuitive and clear, and the code is smaller. There’s no inline as-\nsembly, and the SSE instruction looks just like a regular function call.\nIf you’d like to experiment with these example functions, they can be in-\nvoked via the following test bed main() function. Notice the use of another \nintrinsic, _mm_load_ps(), which loads values from an in-memory array of \nfloats into an __m128 variable (i.e., into an SSE register). Also notice that \nwe are forcing our four global float arrays to be 16-byte aligned via the \n__declspec(align(16)) directive—if we omit these directives, the pro-\ngram will crash.\n#include <xmmintrin.h>\n// ... function definitions from above ...\n__declspec(align(16)) float A[]={2.0f,-1.0f,3.0f,4.0f};\n__declspec(align(16)) float B[]={-1.0f,3.0f,4.0f,2.0f};\n__declspec(align(16)) float C[]={0.0f,0.0f,0.0f,0.0f};\n__declspec(align(16)) float D[]={0.0f,0.0f,0.0f,0.0f};\nint main(int argc, char* argv[])\n{\n \n// load a and b from floating-point data arrays above\n \n__m128 a = _mm_load_ps(&A[0]);\n \n__m128 b = _mm_load_ps(&B[0]);\n",
      "content_length": 1729,
      "extraction_method": "Direct"
    },
    {
      "page_number": 211,
      "chapter": null,
      "content": "189 \n \n// test the two functions\n \n__m128 c = addWithAssembly(a, b);\n \n__m128 d = addWithIntrinsics(a, b);\n \n// store the original values back to check that they\n \n// weren’t overwritten\n_mm_store_ps(&A[0], a);\n_mm_store_ps(&B[0], b);\n \n// store results into float arrays so we can print  \n \n \n// them\n_mm_store_ps(&C[0], c);\n_mm_store_ps(&D[0], d);\n \n// inspect the results\n \nprintf(“%g %g %g %g\\n”, A[0], A[1], A[2], A[3]);\n \nprintf(“%g %g %g %g\\n”, B[0], B[1], B[2], B[3]);\n \nprintf(“%g %g %g %g\\n”, C[0], C[1], C[2], C[3]);\n \nprintf(“%g %g %g %g\\n”, D[0], D[1], D[2], D[3]);\n \nreturn 0;\n}\n4.7.1.4. Vector-Matrix Multiplication with SSE\n Let’s take a look at how vector-matrix multiplication might be implemented \nusing SSE instructions. We want to multiply the 1 × 4 vector v with the 4 × 4 \nmatrix M to generate a result vector r.\n \n \nThe multiplication involves taking the dot product of the row vector v \nwith the columns of matrix M. So to do this calculation using SSE instructions, \nwe might ﬁ rst try storing v in an SSE register (__m128), and storing each of \nthe columns of M in SSE registers as well. Then we could calculate all of the \nproducts vkMĳ  in parallel using only four mulps instructions, like this:\n11\n12\n13\n14\n21\n22\n23\n24\n31\n32\n33\n34\n41\n42\n43\n44\n11\n12\n13\n14\n21\n22\n23\n24\n31\n32\n33\n34\n41\n42\n43\n44\n;\n[\n]\n[\n] \n(\n(\n(\n(\n)\n)\n)\n)\nx\ny\nz\nw\nx\ny\nz\nw\nx\nx\nx\nx\ny\ny\ny\ny\nz\nz\nz\nz\nw\nw\nw\nw\nM\nM\nM\nM\nM\nM\nM\nM\nr\nr\nr\nr\nv\nv\nv\nv\nM\nM\nM\nM\nM\nM\nM\nM\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\n=\n⎡\n⎤\n⎢\n⎥\n⎢\n⎥\n=\n⎢\n⎥\n⎢\n⎥\n⎣\n⎦\n⎡\n⎤\n⎢\n⎥\n+\n+\n+\n+\n⎢\n⎥\n=⎢\n⎥\n+\n+\n+\n+\n⎢\n⎥\n+\n+\n+\n+\n⎣\n⎦\nr\nvM\n .\n4.7. Hardware-Accelerated SIMD Math\n",
      "content_length": 1629,
      "extraction_method": "Direct"
    },
    {
      "page_number": 212,
      "chapter": null,
      "content": "190 \n4. 3D Math for Games\n__m128 mulVectorMatrixAttempt1(__m128 v,\n   __m128 Mcol1, __m128 Mcol2, \n   __m128 Mcol3, __m128 Mcol4)\n{\n \n__m128 vMcol1 = _mm_mul_ps(v, Mcol1);\n \n__m128 vMcol2 = _mm_mul_ps(v, Mcol2);\n \n__m128 vMcol3 = _mm_mul_ps(v, Mcol3);\n \n__m128 vMcol4 = _mm_mul_ps(v, Mcol4);\n \n// ... then what?\n}\nThe above code would yield the following intermediate results:\n vMcol1\n = [ vxM11  vyM21  vzM31  vwM41 ];\n \nvMcol2 = [ vxM12  vyM22  vzM32  vwM42 ]; \n \nvMcol3 = [ vxM13  vyM23  vzM33  vwM43 ];\n \nvMcol4 = [ vxM14  vyM24  vzM34  vwM44 ].\nBut the problem with doing it this way is that we now have to add “across \nthe registers” in order to generate the results we need. For example, rx = \n(vxM11 + vyM21 + vzM31 + vwM41), so we’d need to add the four components of \nvMcol1 together. Adding across a register like this is diﬃ  cult and ineﬃ  cient, \nand moreover it leaves the four components of the result in four separate SSE \nregisters, which would need to be combined into the single result vector r. We \ncan do bett er.\nThe “trick” here is to multiply with the rows of M, not its columns. \nThat way, we’ll have results that we can add in parallel, and the ﬁ nal sums \nwill end up in the four components of a single SSE register representing \nthe output vector r. However, we don’t want to multiply v as-is with the \nrows of M—we want to multiply vx with all of row 1, vy with all of row 2, \nvz with all of row 3, and vw with all of row 4. To do this, we need to replicate \na single component of v, such as vx, across a register to yield a vector like \n[ vx  vx  vx  vx ]. Then we can multiply the replicated component vectors by the \nappropriate rows of M.\nThankfully there’s a powerful SSE instruction which can replicate values \nlike this. It is called shufps, and it’s wrapped by the intrinsic _mm_shuffle_\nps(). This beast is a bit complicated to understand, because it’s a general-\npurpose instruction that can shuﬄ  e the components of an SSE register around \nin arbitrary ways. However, for our purposes we need only know that the \nfollowing macros replicate the x, y, z or w components of a vector across an \nentire register:\n#define SHUFFLE_PARAM(x, y, z, w) \\\n   ((x) | ((y) << 2) | ((z) << 4) | ((w) << 6))\n",
      "content_length": 2234,
      "extraction_method": "Direct"
    },
    {
      "page_number": 213,
      "chapter": null,
      "content": "191 \n#define\n_mm_replicate_x_ps(v) \\\n   _mm_shuffle_ps((v), (v), SHUFFLE_PARAM(0, 0, 0, 0))\n#define\n_mm_replicate_y_ps(v) \\\n   _mm_shuffle_ps((v), (v), SHUFFLE_PARAM(1, 1, 1, 1))\n#define\n_mm_replicate_z_ps(v) \\\n   _mm_shuffle_ps((v), (v), SHUFFLE_PARAM(2, 2, 2, 2))\n#define\n_mm_replicate_w_ps(v) \\\n   _mm_shuffle_ps((v), (v), SHUFFLE_PARAM(3, 3, 3, 3))\nGiven these convenient macros, we can write our vector-matrix multipli-\ncation function as follows:\n__m128 mulVectorMatrixAttempt2(__m128 v,\n   __m128 Mrow1, __m128 Mrow2, \n   __m128 Mrow3, __m128 Mrow4)\n{\n \n__m128 xMrow1 = _mm_mul_ps(_mm_replicate_x_ps(v),  \n \n   Mrow1);\n \n__m128 yMrow2 = _mm_mul_ps(_mm_replicate_y_ps(v),  \n \nMrow2);\n \n__m128 zMrow3 = _mm_mul_ps(_mm_replicate_z_ps(v),  \n \nMrow3);\n \n__m128 wMrow4 = _mm_mul_ps(_mm_replicate_w_ps(v),  \n \n Mrow4);\n \n__m128 result = _mm_add_ps(xMrow1, yMrow2);\n \nresult        = _mm_add_ps(result, zMrow3);\n \nresult        = _mm_add_ps(result, wMrow4);\n \nreturn result;\n}\nThis code produces the following intermediate vectors:\n xMrow1\n = [ vxM11  vxM12  vxM13  vxM14 ];\n \nyMrow2 = [ vyM21  vyM22  vyM23  vyM24 ]; \n                                   zMrow3 = [ vzM31  vzM32  vzM33  vzM34 ];\n \n    wMrow4 = [ vwM41  vwM42  vwM43  vwM44 ].\nAdding these four vectors in parallel produces our result r:\n \n \n4.7. Hardware-Accelerated SIMD Math\n \n11\n12\n13\n14\n21\n22\n23\n24\n31\n32\n33\n34\n41\n42\n43\n44\n(\n(\n(\n(\n.\n)\n)\n)\n)\nx\nx\nx\nx\ny\ny\ny\ny\nz\nz\nz\nz\nw\nw\nw\nw\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\nv M\n⎡\n⎤\n⎢\n⎥\n+\n+\n+\n+\n⎢\n⎥\n=⎢\n⎥\n+\n+\n+\n+\n⎢\n⎥\n+\n+\n+\n+\n⎣\n⎦\nr\n",
      "content_length": 1557,
      "extraction_method": "Direct"
    },
    {
      "page_number": 214,
      "chapter": null,
      "content": "192 \n4. 3D Math for Games\nOn some CPUs, the code shown above can be optimized even further by \nusing a rather handy multiply-and-add instruction, usually denoted madd. This \ninstruction multiplies its ﬁ rst two arguments and then adds the result to its \nthird argument. Unfortunately SSE doesn’t support a madd instruction, but we \ncan fake it reasonably well with a macro like this:\n#define _mm_madd_ps(a, b, c) \\\n \n_mm_add_ps(_mm_mul_ps((a), (b)), (c))\n__m128 mulVectorMatrixFinal(__m128 v,\n   __m128 Mrow1, __m128 Mrow2, \n   __m128 Mrow3, __m128 Mrow4)\n{\n \n__m128 result;\n \nresult = _mm_mul_ps (_mm_replicate_x_ps(v), Mrow1);\n \nresult = _mm_madd_ps(_mm_replicate_y_ps(v), Mrow2,  \n \n result);\n \nresult = _mm_madd_ps(_mm_replicate_z_ps(v), Mrow3,  \n \n result);\n \nresult = _mm_madd_ps(_mm_replicate_w_ps(v), Mrow4,  \n \n result);\n \nreturn result;\n}\nWe can of course perform matrix-matrix multiplication using a similar \napproach. Check out htt p://msdn.microsoft .com for a full listing of the SSE \nintrinsics for the Microsoft  Visual Studio compiler.\n4.8. Random Number Generation\nRandom numbers are ubiquitous in game engines, so it behooves us to have \na brief look at the two most common random number generators, the linear \ncongruential generator and the Mersenne Twister. It’s important to realize that \nrandom number generators are just very complicated but totally deterministic \npre-deﬁ ned sequences of numbers. For this reason, we call the sequences they \nproduce pseudo-random. What diﬀ erentiates a good generator from a bad one \nis how long the sequence of numbers is before it repeats (its period), and how \nwell the sequences hold up under various well-known randomness tests.\n4.8.1. Linear Congruential Generators\nLinear congruential generators are a very fast and simple way to generate a \nsequence of pseudo-random numbers. Depending on the platform, this algo-\nrithm is sometimes used in the standard C library’s rand() function. How-\n",
      "content_length": 1957,
      "extraction_method": "Direct"
    },
    {
      "page_number": 215,
      "chapter": null,
      "content": "193 \never, your mileage may vary, so don’t count on rand() being based on any \nparticular algorithm. If you want to be sure, you’ll be bett er oﬀ  implementing \nyour own random number generator.\nThe linear congruential algorithm is explained in detail in the book Nu-\nmerical Recipes in C, so I won’t go into the details of it here.\nWhat I will say is that this random number generator does not produce \nparticularly high-quality pseudo-random sequences. Given the same initial \nseed value, the sequence is always exactly the same. The numbers produced \ndo not meet many of the criteria widely accepted as desirable, such as a long \nperiod, low- and high-order bits that have similarly-long periods, and absence \nof sequential or spatial correlation between the generated values.\n4.8.2. Mersenne Twister\nThe Mersenne Twister pseudo-random number generator algorithm was de-\nsigned speciﬁ cally to improve upon the various problems of the linear con-\ngruential algorithm. Wikipedia provides the following description of the ben-\neﬁ ts of the algorithm:\nIt was designed to have a colossal period of 2\n1. \n19937 − 1 (the creators of the \nalgorithm proved this property). In practice, there is litt le reason to use \nlarger ones, as most applications do not require 219937 unique combina-\ntions (219937 ≈ 4.3 × 106001).\nIt has a very high order of dimensional equidistribution (see linear \n2. \ncongruential generator). Note that this means, by default, that there is \nnegligible serial correlation between successive values in the output se-\nquence.\nIt passes numerous tests for statistical randomness, including the strin-\n3. \ngent Diehard tests.\nIt is fast.\n4. \nVarious implementations of the Twister are available on the web, includ-\ning a particularly cool one that uses SIMD vector instructions for an extra \nspeed boost, called SFMT (SIMD-oriented fast Mersenne Twister). SFMT can \nbe downloaded from htt p://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/\nSFMT/index.html.\n4.8.3. Mother-of-All and Xorshift\nIn 1994, George Marsaglia, a computer scientist and mathematician best known \nfor developing the Diehard batt ery of tests of randomness (htt p://www.stat.\nfsu.edu/pub/diehard), published a pseudo-random number generation algo-\n4.8. Random Number Generation\n",
      "content_length": 2262,
      "extraction_method": "Direct"
    },
    {
      "page_number": 216,
      "chapter": null,
      "content": "194 \n4. 3D Math for Games\nrithm that is much simpler to implement and runs faster than the Mersenne \nTwister algorithm. He claimed that it could produce a sequence of 32-bit pseu-\ndo-random numbers with a period of non-repetition of 2250. It passed all of the \nDiehard tests and still stands today as one of the best pseudo-random number \ngenerators for high-speed applications. He called his algorithm the Mother of \nAll Pseudo-Random Number Generators , because it seemed to him to be the only \nrandom number generator one would ever need.\nLater, Marsaglia published another generator called Xorshift  , which is be-\ntween Mersenee and Mother-of-All in terms of randomness, but runs slightly \nfaster than Mother.\nYou can read about George Marsaglia at htt p://en.wikipedia.org/wiki/\nGeorge_Marsaglia, and about the Mother-of-All generator at ft p://ft p.forth.\norg/pub/C/mother.c and at htt p://www.agner.org/random. You can down-\nload a PDF of George’s paper on Xorshift  at htt p://www.jstatsoft .org/v08/i14/\npaper.\n",
      "content_length": 1021,
      "extraction_method": "Direct"
    },
    {
      "page_number": 217,
      "chapter": null,
      "content": "Part II\nLow-Level \nEngine Systems\n",
      "content_length": 34,
      "extraction_method": "Direct"
    },
    {
      "page_number": 218,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 219,
      "chapter": null,
      "content": "5\nEngine Support Systems\nE\nvery game engine requires some low-level support systems that manage \nmundane but crucial tasks, such as starting up and shutt ing down the en-\ngine, conﬁ guring engine and game features, managing the engine’s memory \nusage, handling access to ﬁ le system(s), providing access to the wide range of \nheterogeneous asset types used by the game (meshes, textures, animations, \naudio, etc.), and providing debugging tools for use by the game development \nteam. This chapter will focus on the lowest-level support systems found in \nmost game engines. In the chapters that follow, we will explore some of the \nlarger core systems, including resource management, human interface devic-\nes, and in-game debugging tools.\n5.1. \nSubsystem Start-Up and Shut-Down\n A game engine is a complex piece of soft ware consisting of many interacting \nsubsystems. When the engine ﬁ rst starts up, each subsystem must be conﬁ g-\nured and initialized in a speciﬁ c order. Interdependencies between subsys-\ntems implicitly deﬁ ne the order in which they must be started—i.e., if sub-\nsystem B depends on subsystem A, then A will need to be started up before B \ncan be initialized. Shut-down typically occurs in the reverse order, so B would \nshut down ﬁ rst, followed by A.\n197\n",
      "content_length": 1280,
      "extraction_method": "Direct"
    },
    {
      "page_number": 220,
      "chapter": null,
      "content": "198 \n5. Engine Support Systems\n5.1.1. \nC++ Static Initialization Order (or Lack Thereof)\n Since the programming language used in most modern game engines is C++, \nwe should brieﬂ y consider whether C++’s native start-up and shut-down se-\nmantics can be leveraged in order to start up and shut down our engine’s sub-\nsystems. In C++, global and static objects are constructed before the program’s \nentry point (main(), or WinMain() under Windows) is called. However, \nthese constructors are called in a totally unpredictable order. The destructors \nof global and static class instances are called aft er main() (or WinMain()) \nreturns, and once again they are called in an unpredictable order. Clearly this \nbehavior is not desirable for initializing and shutt ing down the subsystems \nof a game engine, or indeed any soft ware system that has interdependencies \nbetween its global objects.\nThis is somewhat unfortunate, because a common design patt ern for im-\nplementing major subsystems such as the ones that make up a game engine \nis to deﬁ ne a singleton class (oft en called a manager ) for each subsystem. If C++ \ngave us more control over the order in which global and static class instances \nwere constructed and destroyed, we could deﬁ ne our singleton instances as \nglobals, without the need for dynamic memory allocation. For example, we \ncould write: \nclass RenderManager\n{\npublic:\n RenderManager()\n {\n \n \n// start up the manager...\n }\n ~RenderManager()\n {\n \n \n// shut down the manager...\n }\n \n// ...\n};\n// singleton instance\nstatic RenderManager gRenderManager;\nAlas, with no way to directly control construction and destruction order, this \napproach won’t work.\n5.1.1.1. \nConstruct On Demand\n There is one C++ “trick” we can leverage here. A static variable that is declared \nwithin a function will not be constructed before main() is called, but rather \n",
      "content_length": 1869,
      "extraction_method": "Direct"
    },
    {
      "page_number": 221,
      "chapter": null,
      "content": "199 \n5.1. Subsystem Start-Up and Shut-Down\non the ﬁ rst invocation of that function. So if our global singleton is function-\nstatic, we can control the order of construction for our global singletons.\nclass RenderManager\n{\npublic:\n \n// Get the one and only instance.\nstatic RenderManager& get()\n {\n \n \n// This function-static will be constructed on the\n \n \n// first call to this function.\nstatic RenderManager sSingleton;\n  return \nsSingleton;\n }\n RenderManager()\n {\n \n \n// Start up other managers we depend on, by  \n \n \n \n \n// calling their get() functions first...\nVideoManager::get();\nTextureManager::get();\n \n \n// Now start up the render manager.\n  // \n...\n }\n ~RenderManager()\n {\n \n \n// Shut down the manager.\n  // \n...\n }\n};\nYou’ll ﬁ nd that many soft ware engineering textbooks suggest this de-\nsign, or a variant that involves dynamic allocation of the singleton as shown \nbelow.\nstatic RenderManager& get()\n {\n  static \nRenderManager* gpSingleton = NULL;\n \n \nif (gpSingleton == NULL)\n  {\n   gpSingleton \n= new RenderManager;\n  }\n  ASSERT(gpSingleton);\n  return \n*gpSingleton;\n }\n",
      "content_length": 1088,
      "extraction_method": "Direct"
    },
    {
      "page_number": 222,
      "chapter": null,
      "content": "200 \n5. Engine Support Systems\nUnfortunately, this still gives us no way to control destruction order. It \nis possible that C++ will destroy one of the managers upon which the \nRenderManager depends for its shut-down procedure, prior to the \nRenderManager’s destructor being called. In addition, it’s diﬃ  cult to predict \nexactly when the RenderManager singleton will be constructed, because the \nconstruction will happen on the ﬁ rst call to RenderManager::get()—and \nwho knows when that might be? Moreover, the programmers using the class \nmay not be expecting an innocuous-looking get() function to do something \nexpensive, like allocating and initializing a heavy-weight singleton. This is an \nunpredictable and dangerous design. Therefore we are prompted to resort to \na more direct approach that gives us greater control.\n5.1.2. \nA Simple Approach That Works\n Let’s presume that we want to stick with the idea of singleton managers for \nour subsystems. In this case, the simplest “brute-force” approach is to deﬁ ne \nexplicit start-up and shut-down functions for each singleton manager class. \nThese functions take the place of the constructor and destructor, and in fact \nwe should arrange for the constructor and destructor to do absolutely nothing. \nThat way, the start-up and shut-down functions can be explicitly called in the \nrequired order from within main() (or from some over-arching singleton object \nthat manages the engine as a whole). For example:\nclass RenderManager\n{\npublic:\n RenderManager()\n {\n// do nothing\n }\n ~RenderManager()\n {\n// do nothing\n }\n void \nstartUp()\n {\n \n \n// start up the manager...\n }\n void \nshutDown()\n {\n \n \n// shut down the manager...\n }\n",
      "content_length": 1684,
      "extraction_method": "Direct"
    },
    {
      "page_number": 223,
      "chapter": null,
      "content": "201 \n \n// ...\n};\nclass PhysicsManager    { /* similar... */ };\nclass AnimationManager  { /* similar... */ };\nclass MemoryManager     { /* similar... */ };\nclass FileSystemManager { /* similar... */ };\n// ...\nRenderManager           gRenderManager;\nPhysicsManager          gPhysicsManager;\nAnimationManager        gAnimationManager;\nTextureManager          gTextureManager;\nVideoManager            gVideoManager;\nMemoryManager           gMemoryManager;\nFileSystemManager       gFileSystemManager;\n// ...\nint main(int argc, const char* argv)\n{\n \n// Start up engine systems in the correct order.\n gMemoryManager.\nstartUp();\n gFileSystemManager.\nstartUp();\n gVideoManager.\nstartUp();\n gTextureManager.\nstartUp();\n gRenderManager.\nstartUp();\n gAnimationManager.\nstartUp();\n gPhysicsManager.\nstartUp();\n \n// ...\n \n// Run the game.\n gSimulationManager.\nrun();\n \n// Shut everything down, in reverse order.\n \n// ...\n gPhysicsManager.\nshutDown();\n gAnimationManager.\nshutDown();\n gRenderManager.\nshutDown();\n gFileSystemManager.\nshutDown();\n gMemoryManager.\nshutDown();\n \nreturn 0;\n}\n5.1. Subsystem Start-Up and Shut-Down\n",
      "content_length": 1112,
      "extraction_method": "Direct"
    },
    {
      "page_number": 224,
      "chapter": null,
      "content": "202 \n5. Engine Support Systems\nThere are “more elegant” ways to accomplish this. For example, you could \nhave each manager register itself into a global priority queue and then walk \nthis queue to start up all the managers in the proper order. You could deﬁ ne \nthe manger-to-manager dependency graph by having each manager explicitly \nlist the other managers upon which it depends and then write some code to \ncalculate the optimal start-up order given their interdependencies. You could \nuse the construct-on-demand approach outlined above. In my experience, the \nbrute-force approach always wins out, because:\nIt’s simple and easy to implement.\n• \nIt’s explicit. You can see and understand the start-up order immediately \n• \nby just looking at the code.\nIt’s easy to debug and maintain. If something isn’t starting early enough, \n• \nor is starting too early, you can just move one line of code.\nOne minor disadvantage to the brute-force manual start-up and shut-\ndown method is that you might accidentally shut things down in an order \nthat isn’t strictly the reverse of the start-up order. But I wouldn’t lose any sleep \nover it. As long as you can start up and shut down your engine’s subsystems \nsuccessfully, you’re golden.\n5.1.3. \nSome Examples from Real Engines\nLet’s take a brief look at some examples of engine start-up and shut-down \ntaken from real game engines.\n5.1.3.1. \nOgre3D\nOgre3D is by its authors’ admission a rendering engine, not a game engine \nper se. But by necessity it provides many of the low-level features found in \nfull-ﬂ edged game engines, including a simple and elegant start-up and shut-\ndown mechanism. Everything in Ogre is controlled by the singleton object \nOgre::Root. It contains pointers to every other subsystem in Ogre and man-\nages their creation and destruction. This makes it very easy for a programmer \nto start up Ogre—just new an instance of Ogre::Root and you’re done.\nHere are a few excerpts from the Ogre source code so we can see what \nit’s doing:\nOgreRoot.h\nclass _OgreExport Root : public Singleton<Root>\n{\n // \n<some code omitted...>\n \n// Singletons\n \nLogManager* mLogManager;\n",
      "content_length": 2134,
      "extraction_method": "Direct"
    },
    {
      "page_number": 225,
      "chapter": null,
      "content": "203 \n \nControllerManager* mControllerManager;\n \nSceneManagerEnumerator* mSceneManagerEnum;\n \nSceneManager* mCurrentSceneManager;\n \nDynLibManager* mDynLibManager;\n \nArchiveManager* mArchiveManager;\n \nMaterialManager* mMaterialManager;\n \nMeshManager* mMeshManager;\n \nParticleSystemManager* mParticleManager;\n \nSkeletonManager* mSkeletonManager;\n \nOverlayElementFactory* mPanelFactory;\n \nOverlayElementFactory* mBorderPanelFactory;\n \nOverlayElementFactory* mTextAreaFactory;\n \nOverlayManager* mOverlayManager;\n \nFontManager* mFontManager;\n \nArchiveFactory *mZipArchiveFactory;\n \nArchiveFactory *mFileSystemArchiveFactory;\n \nResourceGroupManager* mResourceGroupManager;\n \nResourceBackgroundQueue* mResourceBackgroundQueue;\n \nShadowTextureManager* mShadowTextureManager;\n // \netc.\n};\nOgreRoot.cpp\nRoot::Root(const String& pluginFileName,\n \n \n \n const String& configFileName, \n \n \n \n const String& logFileName): \n \n \n  mLogManager(0), \n \n \n  mCurrentFrame(0),\n \n \n  mFrameSmoothingTime(0.0f),\n \n \n  mNextMovableObjectTypeFlag(1),\n \n \n  mIsInitialised(false)\n{\n \n// superclass will do singleton checking\n \nString msg;\n \n// Init\n \nmActiveRenderer = 0;\n mVersion\n  = \nStringConverter::toString(OGRE_VERSION_MAJOR)\n  + \n\".\"\n  + \nStringConverter::toString(OGRE_VERSION_MINOR) \n  + \n\".\"\n  + \nStringConverter::toString(OGRE_VERSION_PATCH)\n \n \n+ OGRE_VERSION_SUFFIX + \" \"\n \n \n+ \"(\" + OGRE_VERSION_NAME + \")\";\n \nmConfigFileName = configFileName;\n \n// Create log manager and default log file if there   \n \n// is no log manager yet\n5.1. Subsystem Start-Up and Shut-Down\n",
      "content_length": 1553,
      "extraction_method": "Direct"
    },
    {
      "page_number": 226,
      "chapter": null,
      "content": "204 \n5. Engine Support Systems\n \nif(LogManager::getSingletonPtr() == 0)\n {\n \n \nmLogManager = new LogManager();\n \n \nmLogManager->createLog(logFileName, true, true);\n }\n \n// Dynamic library manager\n \nmDynLibManager = new DynLibManager();\n \nmArchiveManager = new ArchiveManager();\n \n// ResourceGroupManager\n \nmResourceGroupManager = new ResourceGroupManager();\n \n// ResourceBackgroundQueue\n mResourceBackgroundQueue \n \n \n= new ResourceBackgroundQueue();\n // \nand so on...\nOgre provides a templated Ogre::Singleton base class from which all of its \nsingleton (manager) classes derive. If you look at its implementation, you’ll \nsee that Ogre::Singleton does not use deferred construction, but instead \nrelies on Ogre::Root to explicitly new each singleton. As we discussed above, \nthis is done to ensure that the singletons are created and destroyed in a well-\ndeﬁ ned order.\n5.1.3.2. Naughty Dog’s Uncharted: Drake’s Fortune\nThe Uncharted: Drake’s Fortune engine created by Naughty Dog Inc. uses a \nsimilar explicit technique for starting up its subsystems. You’ll notice by look-\ning at the following code that engine start-up is not always a simple sequence \nof allocating singleton instances. A wide range of operating system services, \nthird party libraries, and so on must all be started up during engine initial-\nization. Also, dynamic memory allocation is avoided wherever possible, so \nmany of the singletons are statically-allocated objects (e.g., g_fileSystem, \ng_languageMgr, etc.) It’s not always prett y, but it gets the job done.\nErr BigInit()\n{\n init_exception_handler();\n \nU8* pPhysicsHeap = new(kAllocGlobal, kAlign16)  \n \n \n  U8[ALLOCATION_GLOBAL_PHYS_HEAP]; \n PhysicsAllocatorInit(pPhysicsHeap, \n  ALLOCATION_GLOBAL_PHYS_HEAP);\n g_textDb.Init();\n g_textSubDb.Init();\n",
      "content_length": 1783,
      "extraction_method": "Direct"
    },
    {
      "page_number": 227,
      "chapter": null,
      "content": "205 \n5.2. Memory Management\n g_spuMgr.Init();\n g_drawScript.InitPlatform();\n PlatformUpdate();\n \nthread_t init_thr;\n \nthread_create(&init_thr, threadInit, 0, 30,  \n \n \n \n \n \n64*1024, 0, \"Init\");\n \nchar masterConfigFileName[256];\n snprintf(masterConfigFileName, \n \n       \n  sizeof(masterConfigFileName), \n  MASTER_CFG_PATH);\n {\n \n \nErr err = ReadConfigFromFile(\n   masterConfigFileName);\n  if \n(err.Failed())\n  {\n \n \n \nMsgErr(\"Config file not found (%s).\\n\",  \n \n \n    masterConfigFileName);\n  }\n }\n \nmemset(&g_discInfo, 0, sizeof(BootDiscInfo));\n \nint err1 = GetBootDiscInfo(&g_discInfo);\n \nMsg(\"GetBootDiscInfo() : 0x%x\\n\", err1);\n \nif(err1 == BOOTDISCINFO_RET_OK)\n {\n \n \nprintf(\"titleId       : [%s]\\n\", \n   g_discInfo.titleId);\n \n \nprintf(\"parentalLevel : [%d]\\n\", \n   g_discInfo.parentalLevel);\n }\n g_fileSystem.Init(g_gameInfo.m_onDisc);\n g_languageMgr.Init();\n \nif (g_shouldQuit) return Err::kOK;\n // \nand so on...\n5.2. Memory Management\n As game developers, we are always trying to make our code run more quickly. \nThe performance of any piece of soft ware is dictated not only by the algo-\nrithms it employs, or the eﬃ  ciency with which those algorithms are coded, \n",
      "content_length": 1176,
      "extraction_method": "Direct"
    },
    {
      "page_number": 228,
      "chapter": null,
      "content": "206 \n5. Engine Support Systems\nbut also by how the program utilizes memory (RAM). Memory aﬀ ects perfor-\nmance in two ways:\n \n1. Dynamic memory allocation via malloc() or C++’s global operator new\nis a very slow operation. We can improve the performance of our code \nby either avoiding dynamic allocation altogether or by making use of \ncustom memory allocators that greatly reduce allocation costs.\nOn modern CPUs, the performance of a piece of soft ware is oft en \n2. \ndominated by its memory access patt erns . As we’ll see, data that is located \nin small, contiguous blocks of memory can be operated on much more \neﬃ  ciently by the CPU than if that same data were to be spread out across \na wide range of memory addresses. Even the most eﬃ  cient algorithm, \ncoded with the utmost care, can be brought to its knees if the data upon \nwhich it operates is not laid out eﬃ  ciently in memory.\nIn this section, we’ll learn how to optimize our code’s memory utilization \nalong these two axes.\n5.2.1. \nOptimizing Dynamic Memory Allocation\n Dynamic memory allocation via malloc() and free() or C++’s global new\nand delete operators—also known as heap allocation—is typically very slow. \nThe high cost can be att ributed to two main factors. First, a heap allocator is \na general-purpose facility, so it must be writt en to handle any allocation size, \nfrom one byte to one gigabyte. This requires a lot of management overhead, \nmaking the malloc() and free() functions inherently slow. Second, on most \noperating systems a call to malloc() or free() must ﬁ rst context-switch from \nuser mode into kernel mode, process the request, and then context-switch \nback to the program. These context switches can be extraordinarily expensive. \nOne rule of thumb oft en followed in game development is:\nKeep heap allocations to a minimum, and never allocate from the \nheap within a tight loop.\nOf course, no game engine can entirely avoid dynamic memory alloca-\ntion, so most game engines implement one or more custom allocators. A \ncustom allocator can have bett er performance characteristics than the oper-\nating system’s heap allocator for two reasons. First, a custom allocator can \nsatisfy requests from a preallocated memory block (itself allocated using \nmalloc() or new, or declared as a global variable). This allows it to run in \nuser mode and entirely avoid the cost of context-switching into the operat-\n",
      "content_length": 2405,
      "extraction_method": "Direct"
    },
    {
      "page_number": 229,
      "chapter": null,
      "content": "207 \ning system. Second, by making various assumptions about its usage pat-\nterns, a custom allocator can be much more eﬃ  cient than a general-purpose \nheap allocator.\nIn the following sections, we’ll take a look at some common kinds of cus-\ntom allocators. For additional information on this topic, see Christian Gyr-\nling’s excellent blog post, htt p://www.swedishcoding.com/2008/08/31/are-we-\nout-of-memory.\n5.2.1.1. \nStack-Based Allocators\n Many games allocate memory in a stack-like fashion. Whenever a new game \nlevel is loaded, memory is allocated for it. Once the level has been loaded, \nlitt le or no dynamic memory allocation takes place. At the conclusion of \nthe level, its data is unloaded and all of its memory can be freed. It makes \na lot of sense to use a stack-like data structure for these kinds of memory \nallocations.\nA stack allocator is very easy to implement. We simply allocate a large con-\ntiguous block of memory using malloc() or global new, or by declaring a \nglobal array of bytes (in which case the memory is eﬀ ectively allocated out of \nthe executable’s BSS segment). A pointer to the top of the stack is maintained. \nAll memory addresses below this pointer are considered to be in use, and all \naddresses above it are considered to be free. The top pointer is initialized to \nthe lowest memory address in the stack. Each allocation request simply moves \nthe pointer up by the requested number of bytes. The most-recently allocated \nblock can be freed by simply moving the top pointer back down by the size \nof the block.\nIt is important to realize that with a stack allocator, memory cannot be \nfreed in an arbitrary order. All frees must be performed in an order oppo-\nsite to that in which they were allocated. One simple way to enforce these \nrestrictions is to disallow individual blocks from being freed at all. Instead, \nwe can provide a function that rolls the stack top back to a previously-marked \nlocation, thereby freeing all blocks between the current top and the roll-back \npoint.\nIt’s important to always roll the top pointer back to a point that lies \nat the boundary between two allocated blocks, because otherwise new al-\nlocations would overwrite the tail end of the top-most block. To ensure \nthat this is done properly, a stack allocator oft en provides a function that \nreturns a marker representing the current top of the stack. The roll-back \nfunction then takes one of these markers as its argument. This is depicted \nin Figure 5.1. The interface of a stack allocator oft en looks something like \nthis.\n5.2. Memory Management\n",
      "content_length": 2585,
      "extraction_method": "Direct"
    },
    {
      "page_number": 230,
      "chapter": null,
      "content": "208 \n5. Engine Support Systems\nclass StackAllocator\n{\npublic:\n \n// Stack marker: Represents the current top of the  \n \n \n// stack. You can only roll back to a marker, not to  \n \n// arbitrary locations within the stack.\n \ntypedef U32 Marker;\n \n// Constructs a stack allocator with the given total   \n \n// size.\n explicit \nStackAllocator(U32 stackSize_bytes);\n \n// Allocates a new block of the given size from stack  \n \n// top.\n void* \nalloc(U32 size_bytes);\n \n// Returns a marker to the current stack top.\n Marker \ngetMarker();\n \n// Rolls the stack back to a previous marker.\n void \nfreeToMarker(Marker marker);\n \n// Clears the entire stack (rolls the stack back to   \n \n// zero).\n void \nclear();\nObtain marker after allocating blocks A and B.\nA\nB\nAllocate additional blocks C , D and E.\nA\nB\nC\nD\nE\nFree back to marker.\nA\nB\nFigure 5.1. Stack allocation, and freeing back to a marker.\n",
      "content_length": 882,
      "extraction_method": "Direct"
    },
    {
      "page_number": 231,
      "chapter": null,
      "content": "209 \nprivate:\n \n// ...\n};\nDouble-Ended Stack Allocators\nA single memory block can actually contain two stack allocators—one which \nallocates up from the bott om of the block and one which allocates down from \nthe top of the block. A double-ended stack allocator is useful because it uses \nmemory more eﬃ  ciently by allowing a trade-oﬀ  to occur between the memory \nusage of the bott om stack and the memory usage of the top stack. In some situ-\nations, both stacks may use roughly the same amount of memory and meet in \nthe middle of the block. In other situations, one of the two stacks may eat up \na lot more memory than the other stack, but all allocation requests can still be \nsatisﬁ ed as long as the total amount of memory requested is not larger than \nthe block shared by the two stacks. This is depicted in Figure 5.2.\nIn Midway’s Hydro Thunder arcade game, all memory allocations are \nmade from a single large block of memory managed by a double-ended stack \nallocator. The bott om stack is used for loading and unloading levels (race \ntracks), while the top stack is used for temporary memory blocks that are al-\nlocated and freed every frame. This allocation scheme worked extremely well \nand ensured that Hydro Thunder never suﬀ ered from memory fragmentation \nproblems (see Section 5.2.1.4). Steve Ranck, Hydro Thunder’s lead engineer, de-\nscribes this allocation technique in depth in [6], Section 1.9.\nLower\nUpper\nFigure 5.2. A double-ended stack allocator.\n5.2. Memory Management\n5.2.1.2. Pool Allocators\n It’s quite common in game engine programming (and soft ware engineering in \ngeneral) to allocate lots of small blocks of memory, each of which are the same \nsize. For example, we might want to allocate and free matrices, or iterators, or \nlinks in a linked list, or renderable mesh instances. For this type of memory \nallocation patt ern, a pool allocator is oft en the perfect choice.\nA pool allocator works by preallocating a large block of memory whose \nsize is an exact multiple of the size of the elements that will be allocated. For \nexample, a pool of 4  ×  4 matrices would be an exact multiple of 64 bytes (16 el-\nements per matrix times four bytes per element). Each element within the pool \nis added to a linked list of free elements; when the pool is ﬁ rst initialized, the \nfree list contains all of the elements. Whenever an allocation request is made, \n",
      "content_length": 2392,
      "extraction_method": "Direct"
    },
    {
      "page_number": 232,
      "chapter": null,
      "content": "210 \n5. Engine Support Systems\nwe simply grab the next free element oﬀ  the free list and return it. When an \nelement is freed, we simply tack it back onto the free list. Both allocations and \nfrees are O(1) operations, since each involves only a couple of pointer ma-\nnipulations, no matt er how many elements are currently free. (The notation \nO(1) is an example of big “O” notation. In this case it means that the execution \ntime of both allocations and frees are roughly constant and do not depend on \nthings like the number of elements currently in the pool. See Section 5.3.3 for \nan explanation of big “O” notation.)\nThe linked list of free elements can be a singly-linked list, meaning that \nwe need a single pointer (four bytes on most machines) for each free ele-\nment. Where should we obtain the memory for these pointers? Certainly \nthey could be stored in a separate preallocated memory block, occupying \n(sizeof(void*) * numElementsInPool) bytes. However, this is unduly \nwasteful. We need only realize that the blocks on the free list are, by deﬁ nition, \nfree memory blocks. So why not use the free blocks themselves to store the \nfree list’s “next” pointers? This litt le “trick” works as long as elementSize >=\nsizeof(void*).\nIf each element is smaller than a pointer, then we can use pool element in-\ndices instead of pointers to implement our linked list. For example, if our pool \ncontains 16-bit integers, then we can use 16-bit indices as the “next pointers” \nin our linked list. This works as long as the pool doesn’t contain more than 216\n= 65,536 elements.\n5.2.1.3. Aligned Allocations\nAs we saw in Section 3.2.5.1, every variable and data object has an alignment \nrequirement. An 8-bit integer variable can be aligned to any address, but a \n32-bit integer or ﬂ oating-point variable must be 4-byte aligned, meaning its \naddress can only end in the nibbles 0x0, 0x4, 0x8 or 0xC. A 128-bit SIMD vector \nvalue generally has a 16-byte alignment requirement, meaning that its mem-\nory address can end only in the nibble 0x0. On the PS3, memory blocks that \nare to be transferred to an SPU via the direct memory access (DMA) controller \nshould be 128-bit aligned for maximum DMA throughput, meaning they can \nonly end in the bytes 0x00 or 0x80.\nAll memory allocators must be capable of returning aligned memory \nblocks. This is relatively straightforward to implement. We simply allocate \na litt le bit more memory than was actually requested, adjust the address of \nthe memory block upward slightly so that it is aligned properly, and then re-\nturn the adjusted address. Because we allocated a bit more memory than was \nrequested, the returned block will still be large enough, even with the slight \nupward adjustment.\n",
      "content_length": 2741,
      "extraction_method": "Direct"
    },
    {
      "page_number": 233,
      "chapter": null,
      "content": "211 \nIn most implementations, the number of additional bytes allocated is \nequal to the alignment. For example, if the request is for a 16-byte aligned \nmemory block, we would allocate 16 additional bytes. This allows for the \nworst-case address adjustment of 15 bytes, plus one extra byte so that we can \nuse the same calculations even if the original block is already aligned. This \nsimpliﬁ es and speeds up the code at the expense of one wasted byte per al-\nlocation. It’s also important because, as we’ll see below, we’ll need those extra \nbytes to store some additional information that will be used when the block \nis freed.\nWe determine the amount by which the block’s address must be adjusted \nby masking oﬀ  the least-signiﬁ cant bits of the original block’s memory ad-\ndress, subtracting this from the desired alignment, and using the result as \nthe adjustment oﬀ set. The alignment should always be a power of two (four-\nbyte and 16-byte alignments are typical), so to generate the mask we simply \nsubtract one from the alignment. For example, if the request is for a 16-byte \naligned block, then the mask would be (16 – 1) = 15 = 0x0000000F. Taking \nthe bitwise AND of this mask and any misaligned address will yield the \namount by which the address is misaligned. For example, if the originally-\nallocated block’s address is 0x50341233, ANDing this address with the mask \n0x0000000F yields 0x00000003, so the address is misaligned by three bytes. \nTo align the address, we add (alignment – misalignment) = (16 – 3) = 13 = \n0xD bytes to it. The ﬁ nal aligned address is therefore 0x50341233 + 0xD = \n0x50341240.\nHere’s one possible implementation of an aligned memory allocator:\n// Aligned allocation function. IMPORTANT: 'alignment'   \n// must be a power of 2 (typically 4 or 16).\nvoid* allocateAligned(U32 size_bytes, U32 alignment)\n{\n \n// Determine total amount of memory to allocate.\n \nU32 expandedSize_bytes = size_bytes + alignment;\n \n// Allocate an unaligned block & convert address to a  \n \n// U32.\n U32 \nrawAddress\n  = \n(U32)allocateUnaligned(expandedSize_bytes);\n \n// Calculate the adjustment by masking off the lower   \n \n// bits of the address, to determine how \"misaligned\"  \n \n// it is.\n \nU32 mask = (alignment – 1);\n \nU32 misalignment = (rawAddress & mask);\n U32 \nadjustment = alignment – misalignment;\n5.2. Memory Management\n",
      "content_length": 2353,
      "extraction_method": "Direct"
    },
    {
      "page_number": 234,
      "chapter": null,
      "content": "212 \n5. Engine Support Systems\n \n// Calculate the adjusted address, and return as a  \n \n \n// pointer.\n \nU32 alignedAddress = rawAddress + adjustment;\n \nreturn (void*)alignedAddress;\n}\nWhen this block is later freed, the code will pass us the adjusted address, \nnot the original address we allocated. How, then, do we actually free the mem-\nory? We need some way to convert an adjusted address back into the original, \npossibly misaligned address.\nTo accomplish this, we simply store some meta-information in those \nextra bytes we allocated in order to align the data in the ﬁ rst place. The \nsmallest adjustment we might make is one byte. That’s enough room to \nstore the number of bytes by which the address was adjusted (since it will \nnever be more than 256). We always store this information in the byte im-\nmediately preceding the adjusted address (no matt er how many bytes of \nadjustment we actually added), so that it is trivial to ﬁ nd it again, given the \nadjusted address. Here’s how the modiﬁ ed allocateAligned() function \nwould look.\n// Aligned allocation function. IMPORTANT: ‘alignment’   \n// must be a power of 2 (typically 4 or 16).\nvoid* allocateAligned(U32 size_bytes, U32 alignment)\n{\n \n// Clients must call allocateUnaligned() and\n \n// freeUnaligned() if alignment == 1.\n \nASSERT(alignment > 1);\n \n// Determine total amount of memory to allocate.\n \nU32 expandedSize_bytes = size_bytes + alignment;\n \n// Allocate an unaligned block & convert address to a  \n \n// U32.\n \nU32 rawAddress\n  = \n(U32)allocateUnaligned(expandedSize_bytes);\n \n// Calculate the adjustment by masking off the lower   \n \n// bits of the address, to determine how “misaligned”  \n \n// it is.\n \nU32 mask = (alignment – 1);\n \nU32 misalignment = (rawAddress & mask);\n \nU32 adjustment = alignment – misalignment;\n \n// Calculate the adjusted address, and return as a  \n \n \n// pointer.\n \nU32 alignedAddress = rawAddress + adjustment;\n",
      "content_length": 1918,
      "extraction_method": "Direct"
    },
    {
      "page_number": 235,
      "chapter": null,
      "content": "213 \n \n// Store the adjustment in the four bytes immediately\n \n// preceding the adjusted address that we’re   \n \n \n \n// returning.\n U32* \npAdjustment = (U32*)(alignedAddress – 4);\n*pAdjustment = adjustment;\n \nreturn (void*)alignedAddress;\n}\nAnd here’s how the corresponding freeAligned() function would be imple-\nmented.\nvoid freeAligned(void* p)\n{\n \nU32 alignedAddress = (U32)p;\n \nU8* pAdjustment = (U8*)(alignedAddress – 4);\n U32 \nadjustment = (U32)*pAdjustment;\n \nU32 rawAddress = alignedAddress – adjustment;\nfreeUnaligned((void*)rawAddress);\n}\n5.2.1.4. Single-Frame and Double-Buffered Memory Allocators\nVirtually all game engines allocate at least some temporary data during the \ngame loop. This data is either discarded at the end of each iteration of the loop \nor used on the next frame and then discarded. This allocation patt ern is so \ncommon that many engines support single- and double-buﬀ ered allocators.\nSingle-Frame Allocators\nA single-frame allocator is implemented by reserving a block of memory and \nmanaging it with a simple stack allocator as described above. At the beginning \nof each frame, the stack’s “top” pointer is cleared to the bott om of the memory \nblock. Allocations made during the frame grow toward the top of the block. \nRinse and repeat.\nStackAllocator g_singleFrameAllocator;\n// Main Game Loop\nwhile (true) \n{\n \n// Clear the single-frame allocator’s buffer every  \n \n \n// frame.\n g_singleFrameAllocator.\nclear();\n5.2. Memory Management\n",
      "content_length": 1475,
      "extraction_method": "Direct"
    },
    {
      "page_number": 236,
      "chapter": null,
      "content": "214 \n5. Engine Support Systems\n \n// ...\n \n// Allocate from the single-frame buffer. We never \n \n// need to free this data! Just be sure to use it  \n \n \n// only this frame.\n \nvoid* p = g_singleFrameAllocator.alloc(nBytes);\n \n// ...\n}\nOne of the primary beneﬁ ts of a single-frame allocator is that allocated \nmemory needn’t ever be freed—we can rely on the fact that the allocator will \nbe cleared at the start of every frame. Single-frame allocators are also blind-\ningly fast. The one big negative is that using a single-frame allocator requires \na reasonable level of discipline on the part of the programmer. You need to \nrealize that a memory block allocated out of the single-frame buﬀ er will only \nbe valid during the current frame. Programmers must never cache a pointer to \na single-frame memory block across the frame boundary!\nDouble-Buffered Allocators\nA double-buﬀ ered allocator allows a block of memory allocated on frame i to \nbe used on frame (i + 1). To accomplish this, we create two single-frame stack \nallocators of equal size and then ping-pong between them every frame.\nclass DoubleBufferedAllocator\n{\n U32 \nm_curStack;\n StackAllocator \nm_stack[2];\npublic:\n void \nswapBuffers()\n {\n \n \nm_curStack = (U32)!m_curStack;\n }\n void \nclearCurrentBuffer()\n {\n  m_stack[m_curStack].\nclear();\n }\n void* \nalloc(U32 nBytes)\n {\n  return \nm_stack[m_curStack].alloc(nBytes);\n }\n \n// ...\n};\n",
      "content_length": 1397,
      "extraction_method": "Direct"
    },
    {
      "page_number": 237,
      "chapter": null,
      "content": "215 \n// ...\nDoubleBufferedAllocator g_doubleBufAllocator;\n// Main Game Loop\nwhile (true) \n{\n //\nClear the single-frame allocator every frame as   \n// before.\ng_singleFrameAllocator.clear();\n \n// Swap the active and inactive buffers of the double\n \n// buffered allocator.\n g_doubleBufAllocator.\nswapBuffers();\n \n// Now clear the newly active buffer, leaving last  \n \n \n// frame’s buffer intact.\n g_doubleBufAllocator.\nclearCurrentBuffer();\n \n// ...\n \n// Allocate out of the current buffer, without  \n \n \n \n// disturbing last frame’s data. Only use this data   \n // \nthis frame or next frame. Again, this memory never\n \n// needs to be freed.\n \nvoid* p = g_doubleBufAllocator.alloc(nBytes);\n \n// ...\n}\nThis kind of allocator is extremely useful for caching the results of asyn-\nchronous processing on a multicore game console like the Xbox 360 or the \nPLAYSTATION 3. On frame i, we can kick oﬀ  an asynchronous job on one of \nthe PS3’s SPUs, handing it the address of a destination buﬀ er that has been \nallocated from our double-buﬀ ered allocator. The job runs and produces its \nresults some time before the end of frame i, storing them into the buﬀ er we \nprovided. On frame (i + 1), the buﬀ ers are swapped. The results of the job \nare now in the inactive buﬀ er, so they will not be overwritt en by any double-\nbuﬀ ered allocations that might be made during this frame. As long as we use \nthe results of the job before frame (i + 2), our data won’t be overwritt en.\n5.2.2. Memory Fragmentation\n Another problem with dynamic heap allocations is that memory can become \nfragmented over time. When a program ﬁ rst runs, its heap memory is entirely \nfree. When a block is allocated, a contiguous region of heap memory of the \n5.2. Memory Management\n",
      "content_length": 1747,
      "extraction_method": "Direct"
    },
    {
      "page_number": 238,
      "chapter": null,
      "content": "216 \n5. Engine Support Systems\nappropriate size is marked as “in use,” and the remainder of the heap remains \nfree. When a block is freed, it is marked as such, and adjacent free blocks are \nmerged into a single, larger free block. Over time, as allocations and dealloca-\ntions of various sizes occur in random order, the heap memory begins to look \nlike a patchwork of free and used blocks. We can think of the free regions as \n“holes” in the fabric of used memory. When the number of holes becomes \nlarge, and/or the holes are all relatively small, we say the memory has become \nfragmented. This is illustrated in Figure 5.3.\nThe problem with memory fragmentation is that allocations may fail \neven when there are enough free bytes to satisfy the request. The crux of the \nproblem is that allocated memory blocks must always be contiguous. For ex-\nample, in order to satisfy a request of 128 kB, there must exist a free “hole” \nthat is 128 kB or larger. If there are 2 holes, each of which is 64 kB in size, then \nenough bytes are available but the allocation fails because they are not contigu-\nous bytes.\nfree\nAfter one allocation...\nAfter eight allocations...\nAfter eight allocations and three frees...\nAfter n allocations and m frees...\nfree\nused\nFigure 5.3.  Memory fragmentation.\n",
      "content_length": 1288,
      "extraction_method": "Direct"
    },
    {
      "page_number": 239,
      "chapter": null,
      "content": "217 \nMemory fragmentation is not as much of a problem on operating sys-\ntems that support virtual memory . A virtual memory system maps discontigu-\nous blocks of physical memory known as pages into a virtual address space, in \nwhich the pages appear to the application to be contiguous. Stale pages can \nbe swapped to the hard disk when physical memory is in short supply and \nreloaded from disk when they are needed. For a detailed discussion of how \nvirtual memory works, see htt p://lyle.smu.edu/~kocan/7343/fall05/slides/\nchapter08.ppt. Most embedded systems cannot aﬀ ord to implement a virtual \nmemory system. While some modern consoles do technically support it, most \nconsole game engines still do not make use of virtual memory due to the in-\nherent performance overhead.\n5.2.2.1. Avoiding Fragmentation with Stack and Pool Allocators\n The detrimental eﬀ ects of memory fragmentation can be avoided by using \nstack and/or pool allocators.\nA stack allocator is impervious to fragmentation because allocations are \n• \nalways contiguous, and blocks must be freed in an order opposite to \nthat in which they were allocated. This is illustrated in Figure 5.4.\nA pool allocator is also free from fragmentation problems. Pools \n• \ndo be-\ncome fragmented, but the fragmentation never causes premature out-\nof-memory conditions as it does in a general-purpose heap. Pool alloca-\ntion requests can never fail due to a lack of a large enough contiguous \nfree block, because all of the blocks are exactly the same size. This is \nshown in Figure 5.5.\n5.2. Memory Management\nFigure 5.4.  A stack allocator is free from fragmentation problems.\nSingle free block, always contiguous\nAllocated blocks, always contiguous\ndeallocation\nallocation\nAllocated and free blocks all the same size\nFigure 5.5.  A pool allocator is not degraded by fragmentation.\n",
      "content_length": 1843,
      "extraction_method": "Direct"
    },
    {
      "page_number": 240,
      "chapter": null,
      "content": "218 \n5. Engine Support Systems\n5.2.2.2. Defragmentation and Relocation\n When diﬀ erently-sized objects are being allocated and freed in a random or-\nder, neither a stack-based allocator nor a pool-based allocator can be used. In \nsuch cases, fragmentation can be avoided by periodically defragmenting the \nheap. Defragmentation involves coalescing all of the free “holes” in the heap \nby shift ing allocated blocks from higher memory addresses down to lower \naddresses (thereby shift ing the holes up to higher addresses). One simple al-\ngorithm is to search for the ﬁ rst “hole” and then take the allocated block im-\nmediately above the hole and shift  it down to the start of the hole. This has the \neﬀ ect of “bubbling up” the hole to a higher memory address. If this process is \nrepeated, eventually all the allocated blocks will occupy a contiguous region \nof memory at the low end of the heap’s address space, and all the holes will \nhave bubbled up into one big hole at the high end of the heap. This is illus-\ntrated in Figure 5.6.\nThe shift ing of memory blocks described above is not particularly tricky \nto implement. What is tricky is accounting for the fact that we’re moving al-\nlocated blocks of memory around. If anyone has a pointer into one of these al-\nlocated blocks, then moving the block will invalidate the pointer.\nThe solution to this problem is to patch any and all pointers into a shift ed \nmemory block so that they point to the correct new address aft er the shift . \nThis procedure is known as pointer relocation . Unfortunately, there is no gen-\neral-purpose way to ﬁ nd all the pointers that point into a particular region \nA\nB\nC\nD\nE\nA\nB\nC\nD\nE\nA\nB\nC\nD\nE\nA\nB\nC\nD\nE\nA\nB\nC\nD\nE\nFigure 5.6.  Defragmentation by shifting allocated blocks to lower addresses.\n",
      "content_length": 1784,
      "extraction_method": "Direct"
    },
    {
      "page_number": 241,
      "chapter": null,
      "content": "219 \nof memory. So if we are going to support memory defragmentation in our \ngame engine, programmers must either carefully keep track of all the pointers \nmanually so they can be relocated, or pointers must be abandoned in favor of \nsomething inherently more amenable to relocation, such as smart pointers or \nhandles.\nA smart pointer is a small class that contains a pointer and acts like a \npointer for most intents and purposes. But because a smart pointer is a class, \nit can be coded to handle memory relocation properly. One approach is to \narrange for all smart pointers to add themselves to a global linked list. When-\never a block of memory is shift ed within the heap, the linked list of all smart \npointers can be scanned, and each pointer that points into the shift ed block of \nmemory can be adjusted appropriately.\nA handle is usually implemented as an index into a non-relocatable ta-\nble which itself contains the pointers. When an allocated block is shift ed in \nmemory, the handle table can be scanned and all relevant pointers found and \nupdated automatically. Because the handles are just indices into the pointer \ntable, their values never change no matt er how the memory blocks are shift ed, \nso the objects that use the handles are never aﬀ ected by memory relocation.\nAnother problem with relocation arises when certain memory blocks can-\nnot be relocated. For example, if you are using a third-party library that does \nnot use smart pointers or handles, it’s possible that any pointers into its data \nstructures will not be relocatable. The best way around this problem is usu-\nally to arrange for the library in question to allocate its memory from a special \nbuﬀ er outside of the relocatable memory area. The other option is to simply \naccept that some blocks will not be relocatable. If the number and size of the \nnon-relocatable blocks are both small, a relocation system will still perform \nquite well.\nIt is interesting to note that all of Naughty Dog’s engines have supported \ndefragmentation. Handles are used wherever possible to avoid the need to re-\nlocate pointers. However, in some cases raw pointers cannot be avoided. These \npointers are carefully tracked and relocated manually whenever a memory \nblock is shift ed due to defragmentation. A few of Naughty Dog’s game object \nclasses are not relocatable for various reasons. However, as mentioned above, \nthis doesn’t pose any practical problems, because the number of such objects \nis always very small, and their sizes are tiny when compared to the overall \nsize of the relocatable memory area.\nAmortizing Defragmentation Costs\nDefragmentation can be a slow operation because it involves copying memory \nblocks. However, we needn’t fully defragment the heap all at once. Instead, \nthe cost can be amortized over many frames. We can allow up to N allocated \n5.2. Memory Management\n",
      "content_length": 2876,
      "extraction_method": "Direct"
    },
    {
      "page_number": 242,
      "chapter": null,
      "content": "220 \n5. Engine Support Systems\nblocks to be shift ed each frame, for some small value of N like 8 or 16. If our \ngame is running at 30 frames per second, then each frame lasts 1/30 of a sec-\nond (33 ms). So the heap can usually be completely defragmented in less than \none second without having any noticeable eﬀ ect on the game’s frame rate. As \nlong as allocations and deallocations aren’t happening at a faster rate than \nthe defragmentation shift s, the heap will remain mostly defragmented at all \ntimes.\nThis approach is only valid when the size of each block is relatively small, \nso that the time required to move a single block does not exceed the time al-\nlott ed to relocation each frame. If very large blocks need to be relocated, we \ncan oft en break them up into two or more subblocks, each of which can be \nrelocated independently. This hasn’t proved to be a problem in Naughty Dog’s \nengine, because relocation is only used for dynamic game objects, and they \nare never larger than a few kilobytes—and usually much smaller.\n5.2.3. Cache Coherency\n To understand why memory access patt erns aﬀ ect performance, we need \nﬁ rst to understand how modern processors read and write memory. Access-\ning main system RAM is always a slow operation, oft en taking thousands of \nprocessor cycles to complete. Contrast this with a register access on the CPU \nitself, which takes on the order of tens of cycles or sometimes even a single \ncycle. To reduce the average cost of reading and writing to main RAM, mod-\nern processors utilize a high-speed memory cache.\nA cache is a special type of memory that can be read from and writt en to \nby the CPU much more quickly than main RAM. The basic idea of memory \ncaching is to load a small chunk of memory into the high-speed cache when-\never a given region of main RAM is ﬁ rst read. Such a memory chunk is called \na cache line and is usually between 8 and 512 bytes, depending on the micro-\nprocessor architecture. On subsequent read operations, if the requested data \nalready exists in the cache, it is loaded from the cache directly into the CPU’s \nregisters—a much faster operation than reading from main RAM. Only if the \nrequired data is not already in the cache does main RAM have to be accessed. \nThis is called a cache miss . Whenever a cache miss occurs, the program is forced \nto wait for the cache line to be refreshed from main RAM.\nSimilar rules may apply when writing data to RAM. The simplest kind \nof cache is called a write-through cache ; in such a cache design, all writes to \nthe cache are simply mirrored to main RAM immediately. However, in a \nwrite-back (or copy-back ) cache design, data is ﬁ rst writt en into the cache and \nthe cache line is only ﬂ ushed out to main RAM under certain circumstances, \nsuch as when a dirty cache line needs to be evicted in order to read in a new \n",
      "content_length": 2857,
      "extraction_method": "Direct"
    },
    {
      "page_number": 243,
      "chapter": null,
      "content": "221 \ncache line from main RAM, or when the program explicitly requests a ﬂ ush to \noccur.\nObviously cache misses cannot be totally avoided, since data has to move \nto and from main RAM eventually. However, the trick to high-performance \ncomputing is to arrange your data in RAM and code your algorithms in such \na way that the minimum number of cache misses occur. We’ll see exactly how \nto accomplish this below.\n5.2.3.1. Level 1 and Level 2 Caches\nWhen caching techniques were ﬁ rst developed, the cache memory was locat-\ned on the motherboard, constructed from a faster and more expensive type \nof memory module than main RAM in order to give it the required boost in \nspeed. However, cache memory was expensive, so the cache size was usually \nquite small—on the order of 16 kB. As caching techniques evolved, an even \nfaster type of cache memory was developed that was located on the CPU die \nitself. This gave rise to two distinct types of cache memory: an on-die level 1 \n(L1) cache and an on-motherboard level 2 (L2) cache. More recently, the L2 \ncache has also migrated onto the CPU die (see Figure 5.7).\nThe rules for moving data back and forth between main RAM are of course \ncomplicated by the presence of a level 2 cache. Now, instead of data hopping \nfrom RAM to cache to CPU and back again, it must make two hops—ﬁ rst from \nmain RAM to the L2 cache, and then from L2 cache to L1 cache. We won’t go \ninto the speciﬁ cs of these rules here. (They diﬀ er slightly from CPU to CPU \nanyway.) But suﬃ  ce it to say that RAM is slower than L2 cache memory, and \nL2 cache is slower than L1 cache. Hence L2 cache misses are usually more \nexpensive than L1 cache misses, all other things being equal.\n5.2. Memory Management\nCPU Die\nCPU\nL1\nCache\nL2\nCache\nMain RAM\nslower\nslowest\nfast\nFigure 5.7.  Level 1 and level 2 caches.\n",
      "content_length": 1829,
      "extraction_method": "Direct"
    },
    {
      "page_number": 244,
      "chapter": null,
      "content": "222 \n5. Engine Support Systems\nA load-hit-store is a particularly bad kind of cache miss, prevalent on the \nPowerPC architectures found in the Xbox 360 and PLAYSTATION 3, in which \nthe CPU writes data to a memory address and then reads the data back before \nit has had a chance to make its way through the CPU’s instruction pipeline and \nout into the L1 cache. See htt p://assemblyrequired.crashworks.org/2008/07/08/\nload-hit-stores-and-the-_ _restrict-keyword for more details.\n5.2.3.2. Instruction Cache and Data Cache\nWhen writing high-performance code for a game engine or for any other per-\nformance-critical system, it is important to realize that both data and code are \ncached. The instruction cache (I-cache) is used to preload executable machine \ncode before it runs, while the data cache (D-cache) is used to speed up reading \nand writing of data to main RAM. Most processors separate the two caches \nphysically. Hence it is possible for a program to slow down because of an I-\ncache miss or because of a D-cache miss.\n5.2.3.3. Avoiding Cache Misses\nThe best way to avoid D-cache misses is to organize your data in contiguous \nblocks that are as small as possible and then access them sequentially. This \nyields the minimum number of cache misses. When the data is contiguous \n(i.e., you don’t “jump around” in memory a lot), a single cache miss will load \nthe maximum amount of relevant data in one go. When the data is small, it \nis more likely to ﬁ t into a single cache line (or at least a minimum number \nof cache lines). And when you access your data sequentially (i.e., you don’t \n“jump around” within the contiguous memory block), you achieve the mini-\nmum number of cache misses, since the CPU never has to reload a cache line \nfrom the same region of RAM.\nAvoiding I-cache misses follows the same basic principle as avoiding D-\ncache misses. However, the implementation requires a diﬀ erent approach. \nThe compiler and linker dictate how your code is laid out in memory, so you \nmight think you have litt le control over I-cache misses. However, most C/C++ \nlinkers follow some simple rules that you can leverage, once you know what \nthey are:\nThe machine code for a single function is almost always contiguous in \n• \nmemory. That is, the linker almost never splits a function up in order \nto intersperse another function in the middle. (Inline functions are the \nexception to this rule—more on this topic below.)\nFunctions are laid out in memory in the order they appear in the \n• \ntranslation unit’s source code (.cpp ﬁ le).\n",
      "content_length": 2548,
      "extraction_method": "Direct"
    },
    {
      "page_number": 245,
      "chapter": null,
      "content": "223 \nTherefore, functions in a single translation unit are always contiguous \n• \nin memory. That is, the linker never splits up a complied translation unit \n(.obj ﬁ le) in order to intersperse code from some other translation unit.\nSo, following the same principles that apply to avoiding D-cache misses, \nwe should follow the rules of thumb listed below.\nKeep high-performance code \n• \nas small as possible, in terms of number of \nmachine language instructions. (The compiler and linker take care of \nkeeping our functions contiguous in memory.)\nAvoid calling functions\n• \n from within a performance-critical section of \ncode.\nIf you do have to call a function, place it as \n• \nclose as possible to the calling \nfunction—preferably immediately before or aft er the calling function \nand never in a diﬀ erent translation unit (because then you completely \nlose control over its proximity to the calling function).\nUse inline functions judiciously. Inlining a small function can be a big \n• \nperformance boost. However, too much inlining bloats the size of the \ncode, which can cause a performance-critical section of code to no \nlonger ﬁ t within the cache. Let’s say we write a tight loop that processes \na large amount of data—if the entire body of that loop doesn’t ﬁ t into \nthe cache, then we are signing up for two I-cache misses during every \niteration of the loop. In such a situation, it is probably best to rethink the \nalgorithm and/or implementation so that less code is required within \ncritical loops.\n5.3. Containers\nGame programmers employ a wide variety of collection-oriented data struc-\ntures, also known as containers or collections. The job of a container is always \nthe same—to house and manage zero or more data elements; however, the \ndetails of how they do this varies greatly, and each type of container has its \npros and cons. Common container data types include, but are certainly not \nlimited to, the following.\nArray\n• \n . An ordered, contiguous collection of elements accessed by index. \nThe length of the array is usually statically deﬁ ned at compile time. It \nmay be multidimensional. C and C++ support these natively (e.g., int\na[5]).\nDynamic array\n• \n . An array whose length can change dynamically at \nruntime (e.g., STL’s std::vector)\n5.3. Containers\n",
      "content_length": 2289,
      "extraction_method": "Direct"
    },
    {
      "page_number": 246,
      "chapter": null,
      "content": "224 \n5. Engine Support Systems\nLinked list\n• \n . An ordered collection of elements not stored contiguously \nin memory but rather linked to one another via pointers (e.g., STL’s \nstd::list).\nStack\n• \n . A container that supports the last-in-ﬁ rst-out (LIFO) model \nfor adding and removing elements, also known as push/pop (e.g., \nstd::stack).\nQueue\n• \n . A container that supports the ﬁ rst-in-ﬁ rst-out (FIFO) model for \nadding and removing elements (e.g., std::queue).\nDeque\n• \n . A double-ended queue—supports eﬃ  cient insertion and removal \nat both ends of the array (e.g., std::deque).\nPriority queue\n• \n . A container that permits elements to be added in any or-\nder and then removed in an order deﬁ ned by some property of the ele-\nments themselves (i.e., their priority). It can be thought of as a list that \nstays sorted at all times. A priority queue is typically implemented as a \nbinary search tree (e.g., std::priority_queue).\nTree\n• \n . A container in which elements are grouped hierarchically. Each ele-\nment (node) has zero or one parent and zero or more children. A tree is \na special case of a DAG (see below).\nBinary search tree (BST)\n• \n  . A tree in which each node has at most two chil-\ndren, with an order property to keep the nodes sorted by some well-de-\nﬁ ned criteria. There are various kinds of binary search trees, including \nred-black trees, splay trees, SVL trees, etc.\nBinary heap\n• \n . A binary tree that maintains itself in sorted order, much like \na binary search tree, via two rules: the shape property, which speciﬁ es that \nthe tree must be fully ﬁ lled and that the last row of the tree is ﬁ lled from \nleft  to right; and the heap property, which states that every node is, by some \nuser-deﬁ ned criterion, “greater than” or “equal to” all of its children.\nDictionary\n• \n . A table of key-value pairs. A value can be “looked up” ef-\nﬁ ciently given the corresponding key. A dictionary is also known as a \nmap or hash table, although technically a hash table is just one possible \nimplementation of a dictionary (e.g., std::map, std::hash_map).\nSet\n• \n . A container that guarantees that all elements are unique according to \nsome criteria. A set acts like a dictionary with only keys, but no values.\nGraph\n• \n . A collection of nodes connected to one another by unidirectional \nor bidirectional pathways in an arbitrary patt ern.\nDirected acyclic graph (DAG)\n• \n . A collection of nodes with unidirectional \n(i.e., directed) interconnections, with no cycles (i.e., there is no non-empty \npath that starts and ends on the same node).\n",
      "content_length": 2573,
      "extraction_method": "Direct"
    },
    {
      "page_number": 247,
      "chapter": null,
      "content": "225 \n5.3.1. \nContainer Operations\nGame engines that make use of container classes inevitably make use of vari-\nous commonplace algorithms as well. Some examples include:\nInsert.\n• \n Add a new element to the container. The new element might be \nplaced at the beginning of the list, or the end, or in some other location; \nor the container might not have a notion of ordering at all.\nRemove.\n• \n Remove an element from the container; may require a ﬁ nd op-\neration (see below). However if an iterator is available that refers to the \ndesired element, it may be more eﬃ  cient to remove the element using \nthe iterator.\nSequential access (iteration).\n• \n Accessing each element of the container in \nsome “natural” predeﬁ ned order.\nRandom access.\n• \n Accessing elements in the container in an arbitrary or-\nder.\nFind.\n• \n Search a container for an element that meets a given criterion. \nThere are all sorts of variants on the ﬁ nd operation, including ﬁ nding \nin reverse, ﬁ nding multiple elements, etc. In addition, diﬀ erent types of \ndata structures and diﬀ erent situations call for diﬀ erent algorithms (see \nhtt p://en.wikipedia.org/wiki/Search_algorithm).\nSort\n• \n . Sort the contents of a container according to some given criteria. \nThere are many diﬀ erent sorting algorithms, including bubble sort, se-\nlection sort, insertion sort, quicksort, and so on. (See htt p://en.wikipedia.\norg/wiki/Sorting_algorithm for details.)\n5.3.2. Iterators\nAn iterator is a litt le class that “knows” how to eﬃ  ciently visit the elements \nin a particular kind of container. It acts like an array index or pointer—it \nrefers to one element in the container at a time, it can be advanced to the \nnext element, and it provides some sort of mechanism for testing whether \nor not all elements in the container have been visited. As an example, the \nﬁ rst of the following two code snippets iterates over a C-style array using a \npointer, while the second iterates over an STL linked list using almost identi-\ncal syntax.\nvoid processArray(int container[], int numElements)\n{\n \nint* pBegin = &container[0];\n \nint* pEnd = &container[numElements];\n5.3. Containers\n",
      "content_length": 2149,
      "extraction_method": "Direct"
    },
    {
      "page_number": 248,
      "chapter": null,
      "content": "225 \n5.3.1. \nContainer Operations\nGame engines that make use of container classes inevitably make use of vari-\nous commonplace algorithms as well. Some examples include:\nInsert.\n• \n Add a new element to the container. The new element might be \nplaced at the beginning of the list, or the end, or in some other location; \nor the container might not have a notion of ordering at all.\nRemove.\n• \n Remove an element from the container; may require a ﬁ nd op-\neration (see below). However if an iterator is available that refers to the \ndesired element, it may be more eﬃ  cient to remove the element using \nthe iterator.\nSequential access (iteration).\n• \n Accessing each element of the container in \nsome “natural” predeﬁ ned order.\nRandom access.\n• \n Accessing elements in the container in an arbitrary or-\nder.\nFind.\n• \n Search a container for an element that meets a given criterion. \nThere are all sorts of variants on the ﬁ nd operation, including ﬁ nding \nin reverse, ﬁ nding multiple elements, etc. In addition, diﬀ erent types of \ndata structures and diﬀ erent situations call for diﬀ erent algorithms (see \nhtt p://en.wikipedia.org/wiki/Search_algorithm).\nSort\n• \n . Sort the contents of a container according to some given criteria. \nThere are many diﬀ erent sorting algorithms, including bubble sort, se-\nlection sort, insertion sort, quicksort, and so on. (See htt p://en.wikipedia.\norg/wiki/Sorting_algorithm for details.)\n5.3.2. Iterators\nAn iterator is a litt le class that “knows” how to eﬃ  ciently visit the elements \nin a particular kind of container. It acts like an array index or pointer—it \nrefers to one element in the container at a time, it can be advanced to the \nnext element, and it provides some sort of mechanism for testing whether \nor not all elements in the container have been visited. As an example, the \nﬁ rst of the following two code snippets iterates over a C-style array using a \npointer, while the second iterates over an STL linked list using almost identi-\ncal syntax.\nvoid processArray(int container[], int numElements)\n{\n \nint* pBegin = &container[0];\n \nint* pEnd = &container[numElements];\n5.3. Containers\n",
      "content_length": 2149,
      "extraction_method": "Direct"
    },
    {
      "page_number": 249,
      "chapter": null,
      "content": "226 \n5. Engine Support Systems\n \nfor (int* p = pBegin; p != pEnd; ++p)\n {\n \n \nint element = *p;\n \n \n// process element...\n }\n}\nvoid processList(std::list<int>& container)\n{\n std::list<int>::\niterator pBegin = container.begin();\n std::list<int>::\niterator pEnd = container.end();\n std::list<inf>::\niterator p;\n \nfor (p = pBegin; p != pEnd; ++p)\n {\n \n \nint element = *p;\n \n \n// process element...\n }\n}\nThe key beneﬁ ts to using an iterator over att empting to access the con-\ntainer’s elements directly are:\nDirect access would break the container class’ encapsulation. An iterator, \n• \non the other hand, is typically a friend of the container class, and as such \nit can iterate eﬃ  ciently without exposing any implementation details \nto the outside world. (In fact, most good container classes hide their \ninternal details and cannot be iterated over without an iterator.)\nAn iterator can simplify the process of iterating. Most iterators act like \n• \narray indices or pointers, so a simple loop can be writt en in which the \niterator is incremented and compared against a terminating condition—\neven when the underlying data structure is arbitrarily complex. For \nexample, an iterator can make an in-order depth-ﬁ rst tree traversal look \nno more complex than a simple array iteration.\n5.3.2.1. Preincrement versus Postincrement\nNotice in the above example that we are using C++’s preincrement operator , \n++p, rather than the postincrement operator , p++. This is a subtle but some-\ntimes important optimization. The preincrement operator returns the value of \nthe operand aft er the increment has been performed, whereas postincrement \nreturns the previous, unincremented value. Hence preincrement can simply \nincrement the pointer or iterator in place and return a reference to it. Postin-\ncrement must cache the old value, then increment the pointer or iterator, and \nﬁ nally return the cached value. This isn’t a big deal for pointers or integer \n",
      "content_length": 1954,
      "extraction_method": "Direct"
    },
    {
      "page_number": 250,
      "chapter": null,
      "content": "228 \n5. Engine Support Systems\ntion. If an algorithm executes a subalgorithm n times, and the subalgorithm is \nO(log n), then the resulting algorithm would be O(n log n).\nTo select an appropriate container class, we should look at the opera-\ntions that we expect to be most common, then select the container whose per-\nformance characteristics for those operations are most favorable. The most \ncommon orders you’ll encounter are listed here from fastest to slowest: O(1), \nO(log n), O(n), O(n log n), O(n2), O(nk) for k > 2.\nWe should also take the memory layout and usage characteristics \nof our containers into account. For example, an array (e.g., int a[5]  or\nstd::vector) stores its elements contiguously in memory and requires no \noverhead storage for anything other than the elements themselves. (Note that \na dynamic array does require a small ﬁ xed overhead.) On the other hand, a \nlinked list (e.g., std::list) wraps each element in a “link” data structure \nthat contains a pointer to the next element and possibly also a pointer to the \nprevious element, for a total of up to eight bytes of overhead per element. Also, \nthe elements in a linked list need not be contiguous in memory and oft en \naren’t. A contiguous block of memory is usually much more cache-friendly \nthan a set of disparate memory blocks. Hence, for high-speed algorithms, ar-\nrays are usually bett er than linked lists in terms of cache performance (unless \nthe nodes of the linked list are themselves allocated from a small, contiguous \nmemory block of memory, which is rare but not entirely unheard of). But a \nlinked list is bett er for situations in which speed of inserting and removing \nelements is of prime importance.\n5.3.4. Building Custom Container Classes\n Many game engines provide their own custom implementations of the com-\nmon container data structures. This practice is especially prevalent in console \ngame engines and games targeted at mobile phone and PDA platforms. The \nreasons for building these classes yourself include:\nTotal control.\n• \n You control the data structure’s memory requirements, the \nalgorithms used, when and how memory is allocated, etc.\nOpportunities for optimization.\n• \n You can optimize your data structures \nand algorithms to take advantage of hardware features speciﬁ c to the \nconsole(s) you are targeting; or ﬁ ne-tune them for a particular applica-\ntion within your engine.\nCustomizability.\n• \n You can provide custom algorithms not prevalent in \nthird-party libraries like STL (for example, searching for the n most-\nrelevant elements in a container, instead of just the single most-rele-\nvant).\n",
      "content_length": 2629,
      "extraction_method": "Direct"
    },
    {
      "page_number": 251,
      "chapter": null,
      "content": "229 \nElimination of external dependencies.\n• \n Since you built the soft ware your-\nself, you are not beholden to any other company or team to maintain it. \nIf problems arise, they can be debugged and ﬁ xed immediately, rather \nthan waiting until the next release of the library (which might not be \nuntil aft er you have shipped your game!)\nWe cannot cover all possible data structures here, but let’s look at a few \ncommon ways in which game engine programmers tend to tackle contain-\ners.\n5.3.4.1. To Build or Not to Build\nWe will not discuss the details of how to implement all of these data types \nand algorithms here—a plethora of books and online resources are available \nfor that purpose. However, we will concern ourselves with the question of \nwhere to obtain implementations of the types and algorithms that you need. \nAs game engine designers, we have a number of choices:\nBuild the needed data structures manually.\n1. \nRely on third-party implementations. Some common choices include\n2. \nthe C++ standard template library (STL),\na. \na variant of STL, such as STLport,\nb. \nthe powerful and robust Boost libraries (htt p://www.boost.org).\nc. \nBoth STL and Boost are att ractive, because they provide a rich and power-\nful set of container classes covering prett y much every type of data structure \nimaginable. In addition, both of these packages provide a powerful suite of \ntemplate-based generic algorithms—implementations of common algorithms, \nsuch as ﬁ nding an element in a container, which can be applied to virtually \nany type of data object. However, third-party packages like these may not be \nappropriate for some kinds of game engines. And even if we decide to use a \nthird-party package, we must select between Boost and the various ﬂ avors of \nSTL, or another third-party library. So let’s take a moment to investigate some \nof the pros and cons of each approach.\nSTL\nThe beneﬁ ts of the standard template library include:\nSTL oﬀ ers a rich set of features.\n• \nReasonably robust implementations are available on a wide variety of \n• \nplatforms.\nSTL comes “standard” with virtually all C++ compilers.\n• \n5.3. Containers\n",
      "content_length": 2144,
      "extraction_method": "Direct"
    },
    {
      "page_number": 252,
      "chapter": null,
      "content": "230 \n5. Engine Support Systems\nHowever, the STL also has numerous drawbacks, including:\nSTL has a steep learning curve. The documentation is now quite good, \n• \nbut the header ﬁ les are cryptic and diﬃ  cult to understand on most plat-\nforms.\nSTL is oft en slower than a data structure that has been craft ed speciﬁ -\n• \ncally for a particular problem.\nSTL also almost always eats up more memory than a custom-designed \n• \ndata structure.\nSTL does a lot of dynamic memory allocation, and it’s sometimes chal-\n• \nlenging to control its appetite for memory in a way that is suitable for \nhigh-performance, memory-limited console games.\nSTL’s implementation and behavior varies slightly from compiler to \n• \ncompiler, making its use in multiplatform engines more diﬃ  cult.\nAs long as the programmer is aware of the pitfalls of STL and uses it ju-\ndiciously, it can have a place in game engine programming. It is best suited \nto a game engine that will run on a personal computer platform, because the \nadvanced virtual memory systems on modern PCs make memory allocation \ncheaper, and the probability of running out of physical RAM is oft en negli-\ngible. On the other hand, STL is not generally well-suited for use on memory-\nlimited consoles that lack advanced CPUs and virtual memory. And code that \nuses STL may not port easily to other platforms. Here are some rules of thumb \nthat I use:\nFirst and foremost, be aware of the performance and memory character-\n• \nistics of the particular STL class you are using.\nTry to avoid heavier-weight STL classes in code that you believe will be \n• \na performance bott leneck.\nPrefer STL in situations where memory is not at a premium. For ex-\n• \nample, embedding a std::list inside a game object is OK, but em-\nbedding a std::list inside every vertex of a 3D mesh is probably not \na good idea. Adding every vertex of your 3D mesh to a std::list is \nprobably also not OK—the std::list class dynamically allocates a \nsmall “link” object for every element inserted into it, and that can result \nin a lot of tiny, fragmented memory allocations.\nIf your engine is to be multiplatform, I highly recommend \n• \nSTLport \n(htt p://www.stlport.org), an implementation of STL that was speciﬁ cally \ndesigned to be portable across a wide range of compilers and target \nplatforms, more eﬃ  cient, and more feature-rich than the original STL \nimplementations.\n",
      "content_length": 2388,
      "extraction_method": "Direct"
    },
    {
      "page_number": 253,
      "chapter": null,
      "content": "231 \nThe Medal of Honor: Paciﬁ c Assault engine for the PC made heavy use of \nSTL, and while MOHPA did have its share of frame rate problems, the team \nwas able to work around the performance problems caused by STL (primarily \nby carefully limiting and controlling its use). Ogre3D, the popular object-ori-\nented rendering library that we use for some of the examples in this book, also \nmakes heavy use of STL. Your mileage may vary. Using STL on a game engine \nproject is certainly feasible, but it must be used with utmost care.\nBoost\nThe Boost project was started by members of the C++ Standards Committ ee \nLibrary Working Group, but it is now an open-source project with many con-\ntributors from across the globe. The aim of the project is to produce libraries \nthat extend and work together with STL, for both commercial and non-com-\nmercial use. Many of the Boost libraries have already been included in the \nC++ Standards Committ ee’s Library Technical Report (TR1), which is a step \ntoward becoming part of a future C++ standard. Here is a brief summary of \nwhat Boost brings to the table:\nBoost provides a lot of useful facilities not available in STL.\n• \nIn some cases, Boost provides alternatives to work around certain prob-\n• \nlems with STL’s design or implementation.\nBoost does a great job of handling some very complex problems, like \n• \nsmart pointers. (Bear in mind that smart pointers are complex beasts, \nand they can be performance hogs. Handles are usually preferable; see \nSection 14.5 for details.)\nThe Boost libraries’ documentation is usually excellent. Not only does \n• \nthe documentation explain what each library does and how to use it, but \nin most cases it also provides an excellent in-depth discussion of the de-\nsign decisions, constraints, and requirements that went into construct-\ning the library. As such, reading the Boost documentation is a great way \nto learn about the principles of soft ware design.\nIf you are already using STL, then Boost can serve as an excellent exten-\nsion and/or alterative to many of STL’s features. However, be aware of the \nfollowing caveats:\nMost of the core Boost classes are templates, so all that one needs in \n• \norder to use them is the appropriate set of header ﬁ les. However, some \nof the Boost libraries build into rather large .lib ﬁ les and may not be \nfeasible for use in very small-scale game projects.\nWhile the world-wide Boost community is an excellent support net-\n• \nwork, the Boost libraries come with no guarantees. If you encounter a \n5.3. Containers\n",
      "content_length": 2544,
      "extraction_method": "Direct"
    },
    {
      "page_number": 254,
      "chapter": null,
      "content": "232 \n5. Engine Support Systems\nbug, it will ultimately be your team’s responsibility to work around it \nor ﬁ x it.\nBackward compatibility may not be supported.\n• \nThe Boost libraries are distributed under the Boost Soft ware License. \n• \nRead the license information (htt p://www.boost.org/more/license_info.\nhtml) carefully to be sure it is right for your engine.\nLoki\n There is a rather esoteric branch of C++ programming known as template meta-\nprogramming. The core idea is to use the compiler to do a lot of the work that \nwould otherwise have to be done at runtime by exploiting the template fea-\nture of C++ and in eﬀ ect “tricking” the compiler into doing things it wasn’t \noriginally designed to do. This can lead to some startlingly powerful and use-\nful programming tools.\nBy far the most well-known and probably most powerful template meta-\nprogramming library for C++ is Loki, a library designed and writt en by Andrei \nAlexandrescu (whose home page is at htt p://www.erdani.org). The library can \nbe obtained from SourceForge at htt p://loki-lib.sourceforge.net.\nLoki is extremely powerful; it is a fascinating body of code to study and \nlearn from. However, its two big weaknesses are of a practical nature: (a) its \ncode can be daunting to read and use, much less truly understand, and (b) \nsome of its components are dependent upon exploiting “side-eﬀ ect” behav-\niors of the compiler that require careful customization in order to be made \nto work on new compilers. So Loki can be somewhat tough to use, and it \nis not as portable as some of its “less-extreme” counterparts. Loki is not for \nthe faint of heart. That said, some of Loki’s concepts such as policy-based pro-\ngramming can be applied to any C++ project, even if you don’t use the Loki \nlibrary per se. I highly recommend that all soft ware engineers read Andrei’s \nground-breaking book, Modern C++ Design [2], from which the Loki library \nwas born.\n5.3.4.2. Dynamic Arrays and Chunky Allocation\n Fixed-size C-style arrays are used quite a lot in game programming, because \nthey require no memory allocation, are contiguous and hence cache-friendly, \nand support many common operations such as appending data and searching \nvery eﬃ  ciently.\nWhen the size of an array cannot be determined a priori, programmers \ntend to turn either to linked lists or dynamic arrays. If we wish to maintain the \nperformance and memory characteristics of ﬁ xed-length arrays, then the dy-\nnamic array is oft en the data structure of choice.\n",
      "content_length": 2503,
      "extraction_method": "Direct"
    },
    {
      "page_number": 255,
      "chapter": null,
      "content": "233 \nThe easiest way to implement a dynamic array is to allocate an n-element \nbuﬀ er initially and then grow the list only if an att empt is made to add more \nthan n elements to it. This gives us the favorable characteristics of a ﬁ xed-\nsize array but with no upper bound. Growing is implemented by allocating \na new larger buﬀ er, copying the data from the original buﬀ er into the new \nbuﬀ er, and then freeing the original buﬀ er. The size of the buﬀ er is increased \nin some orderly manner, such as adding n to it on each grow, or doubling it \non each grow. Most of the implementations I’ve encountered never shrink the \narray, only grow it (with the notable exception of clearing the array to zero \nsize, which might or might not free the buﬀ er). Hence the size of the array be-\ncomes a sort of “high water mark .” The STL std::vector class works in this \nmanner.\nOf course, if you can establish a high water mark for your data, then you’re \nprobably bett er oﬀ  just allocating a single buﬀ er of that size when the engine \nstarts up. Growing a dynamic array can be incredibly costly due to realloca-\ntion and data copying costs. The impact of these things depends on the sizes \nof the buﬀ ers involved. Growing can also lead to fragmentation when dis-\ncarded buﬀ ers are freed. So, as with all data structures that allocate memory, \ncaution must be exercised when working with dynamic arrays. Dynamic ar-\nrays are probably best used during development, when you are as yet unsure \nof the buﬀ er sizes you’ll require. They can always be converted into ﬁ xed size \narrays once suitable memory budgets have been established.)\n5.3.4.3. Linked Lists\n If contiguous memory is not a primary concern, but the ability to insert and \nremove elements at random is paramount, then a linked list is usually the data \nstructure of choice. Linked lists are quite easy to implement, but they’re also \nquite easy to get wrong if you’re not careful. This section provides a few tips \nand tricks for creating robust linked lists.\nThe Basics of Linked Lists\nA linked list is a very simple data structure. Each element in the list has a \npointer to the next element, and, in a doubly-linked list , it also has a pointer to \nthe previous element. These two pointers are referred to as links. The list as a \nwhole is tracked using a special pair of pointers called the head and tail point-\ners. The head pointer points to the ﬁ rst element, while the tail pointer points \nto the last element.\nInserting a new element into a doubly-linked list involves adjusting the \nnext pointer of the previous element and the previous pointer of the next ele-\nment to both point at the new element and then sett ing the new element’s next \n5.3. Containers\n",
      "content_length": 2729,
      "extraction_method": "Direct"
    },
    {
      "page_number": 256,
      "chapter": null,
      "content": "234 \n5. Engine Support Systems\nand previous pointers appropriately as well. There are four cases to handle \nwhen adding a node to a linked list:\nAdding the ﬁ rst element to a previously-empty list;\n• \nPrepending an element before the current head element;\n• \nAppending an element aft er the current tail element;\n• \nInserting an interior element.\n• \nThese cases are illustrated in Figure 5.8.\nRemoving an element involves the same kinds of operations in and \naround the node being removed. Again there are four cases: removing the \nhead element, removing the tail element, removing an interior element, and \nremoving the last element (emptying the list).\nThe Link Data Structure\nLinked list code isn’t particularly tough to write, but it can be error-prone. \nAs such, it’s usually a good idea to write a general-purpose linked list facility \nthat can be used to manage lists of any element type. To do this, we need to \nseparate the data structure that contains the links (i.e., the next and previ-\nous pointers) from the element data structure. The link data structure is typi-\ncally a simple struct or class, oft en called something like Link, Node, or \nLinkNode, and templated on the type of element to which it refers. It will usu-\nally look something like this.\nHead\nTail\nHead\nTail\nHead\nTail\nHead\nTail\nHead\nTail\nHead\nTail\nHead\nTail\nHead\nTail\nAdd First\nPrepend\n(Push Front)\nInsert\nAppend\n(Push Back)\nFigure 5.8. The four cases that must be handled when adding an element to a linked list: add \nﬁ rst, prepend, append, and insert.\n",
      "content_length": 1534,
      "extraction_method": "Direct"
    },
    {
      "page_number": 257,
      "chapter": null,
      "content": "235 \ntemplate< typename ELEMENT >\nstruct Link\n{\n \nLink<ELEMENT>* m_pPrev;\n \nLink<ELEMENT>* m_pNext;\nELEMENT*   m_pElem;\n};\nExtrusive Lists\nAn extrusive list is a linked list in which the Link data structures are entirely \nseparate from the element data structures. Each Link contains a pointer to the \nelement, as shown in the example. Whenever an element is to be inserted into \na linked list, a link is allocated for it, and the pointers to the element and the \nnext and previous links are set up appropriately. When an element is removed \nfrom a linked list, its link can be freed.\nThe beneﬁ t of the extrusive design is that an element can reside in mul-\ntiple linked lists simultaneously—all we need is one link per list. The down \nside is that the Link objects must be dynamically allocated. Oft en a pool al-\nlocator (see Section 5.2.1.2) is used to allocate links, because they are always \nexactly the same size (viz., 12 bytes on a machine with 32-bit pointers). A pool \nallocator is an excellent choice due to its speed and its freedom from fragmen-\ntation problems.\nIntrusive Lists\nAn intrusive list is a linked list in which the Link data structure is embedded \nin the target element itself. The big beneﬁ t of this approach is that we no lon-\nger need to dynamically allocate the links—we get a link “for free” whenever \nwe allocate an element. For example, we might have:\nclass SomeElement\n{\nLink<SomeElement>\nm_link;\n \n// other members...\n};\nWe can also derive our element class from class Link. Using inheri-\ntance like this is virtually identical to embedding a Link as the ﬁ rst member \nof the class, but it has the additional beneﬁ t of allowing a pointer to a link \n(Link<SomeElement>*) to be down-cast into a pointer to the element itself \n(SomeElement*). This means we can eliminate the back-pointer to the ele-\nment that would otherwise have to be embedded within the Link. Here’s how \nsuch a design might be implemented in C++.\n5.3. Containers\n",
      "content_length": 1968,
      "extraction_method": "Direct"
    },
    {
      "page_number": 258,
      "chapter": null,
      "content": "236 \n5. Engine Support Systems\ntemplate< typename ELEMENT >\nstruct Link\n{\n \nLink<ELEMENT>* m_pPrev;\n \nLink<ELEMENT>*  m_pNext;\n// No ELEMENT* pointer required, thanks to\n \n// inheritance.\n};\nclass SomeElement : public Link<SomeElement>\n{\n \n// other members...\n};\nThe big pitfall of the intrusive linked list design is that it prevents an ele-\nment from residing in more than one linked list at a time (because each ele-\nment has one and only one link). We can allow an element to be a member of \nN concurrent lists by providing it with N embedded link instances (in which \ncase we cannot use the inheritance method). However, the number N must \nbe ﬁ xed a priori, so this approach is still not quite as ﬂ exible as the extrusive \ndesign.\nThe choice between intrusive and extrusive linked lists depends on the \napplication and the constraints under which you are operating. If dynamic \nmemory allocation must be avoided at all costs, then an intrusive list is prob-\nably best. If you can aﬀ ord the overhead of pool allocation, then an extrusive \ndesign may be preferable. Sometimes only one of the two approaches will \nbe feasible. For example, if we wish to store instances of a class deﬁ ned by a \nthird-party library in a linked list and are unable or unwilling to modify that \nlibrary’s source code, then an extrusive list is the only option.\nHead and Tail Pointers: Circular Lists\n To fully implement a linked list, we need to provide a head and a tail pointer. \nThe simplest approach is to embed these pointers in their own data structure, \nperhaps called LinkedList, as follows.\ntemplate< typename ELEMENT >\nclass LinkedList\n{\n Link<ELEMENT>* \nm_pTail;\n Link<ELEMENT>* \nm_pHead;\n \n// member functions for manipulating the list...\n};\nYou may have noticed that there isn’t much diﬀ erence between a \nLinkedList and a Link—they both contain a pair of pointers to Link. As it \n",
      "content_length": 1880,
      "extraction_method": "Direct"
    },
    {
      "page_number": 259,
      "chapter": null,
      "content": "237 \nturns out, there are some distinct beneﬁ ts to using an instance of class Link to \nmanage the head and tail of the list, like this:\ntemplate< typename ELEMENT >\nclass LinkedList\n{\n Link<ELEMENT> \nm_root; \n \n// contains head and tail\n \n// member functions for manipulating the list...\n};\nThe embedded m_root member is a Link, no diﬀ erent from any other Link in \nthe list (except that its m_pElement member will always be NULL). This allows \nus to make the linked list circular as shown in Figure 5.9. In other words, the \nm_pNext pointer of the last “real” node in the list points to m_root, as does \nthe m_pPrev pointer of the ﬁ rst “real” node in the list.\nThis design is preferable to the one involving two “loose” pointers for the \nhead and tail, because it simpliﬁ es the logic for inserting and removing ele-\nments. To see why this is the case, consider the code that would be required \nto remove an element from a linked list when “loose” head and tail pointers \nare being used.\nvoid LinkedList::remove(Link<ELEMENT>& link)\n{\n \nif (link.m_pNext)\n \n \nlink.m_pNext->m_pPrev = link.m_pPrev;\n else\n \n \n// Removing last element in the list.\nm_pTail = link.m_pPrev;\n \nif (link.m_pPrev)\n \n \nlink.m_pPrev->m_pNext = link.m_pNext;\n else\n \n \n// Removing first element in the list.\nm_pHead = link.m_pNext;\n5.3. Containers\nHead\nTail\nm_root\nFigure 5.9. When the head and tail pointers are stored in a link, the linked list can be made \ncircular, which simpliﬁ es the implementation and has some additional beneﬁ ts.\n",
      "content_length": 1515,
      "extraction_method": "Direct"
    },
    {
      "page_number": 260,
      "chapter": null,
      "content": "238 \n5. Engine Support Systems\n \nlink.m_pPrev = link.m_pNext = NULL;\n}\nThe code is a bit simpler when we use the m_root design:\nvoid LinkedList::remove(Link<ELEMENT>& link)\n{\n \n// The link must currently be a member of the list.\n \nASSERT(link.m_pNext != NULL);\n \nASSERT(link.m_pPrev != NULL);\n \nlink.m_pNext->m_pPrev = link.m_pPrev;\n \nlink.m_pPrev->m_pNext = link.m_pNext;\n \n// Do this to indicate the link is no longer in any   \n \n// list.\n \nlink.m_pPrev = link.m_pNext = NULL;\n}\nThe example code shown above highlights an additional beneﬁ t of the \ncircularly linked list approach: A link’s m_pPrev and m_pNext pointers are \nnever null, unless the link is not a member of any list (i.e., the link is unused/\ninactive). This gives us a simple test for list membership.\nContrast this with the “loose” head/tail pointer design. In that case, the \nm_pPrev pointer of the ﬁ rst element in the list is always null, as is the m_pN-\next pointer of the last element. And if there is only one element in the list, that \nlink’s next and previous pointers will both be null. This makes it impossible to \nknow whether or not a given Link is a member of a list or not.\nSingly-Linked Lists\nA singly-linked list is one in which the elements have a next pointer, but no pre-\nvious pointer. (The list as a whole might have both a head and a tail pointer, or \nit might have only a head pointer.) Such a design is obviously a memory saver, \nbut the cost of this approach becomes evident when inserting or removing an \nelement from the list. We have no m_pPrev pointer, so we need to traverse the \nlist from the head in order to ﬁ nd the previous element, so that its m_pNext\npointer can be updated appropriately. Therefore, removal is an O(1) operation \nfor a doubly-linked list, but it’s an O(n) operation for a singly-linked list.\nThis inherent insertion and removal cost is oft en prohibitive, so most \nlinked lists are doubly linked. However, if you know for certain that you will \nonly ever add and remove elements from the head of the list (as when imple-\nmenting a stack), or if you always add to the head and remove from the tail (as \nwith a queue—and your list has both a head and a tail pointer), then you can \nget away with a singly-linked list and save yourself some memory.\n",
      "content_length": 2269,
      "extraction_method": "Direct"
    },
    {
      "page_number": 261,
      "chapter": null,
      "content": "239 \n5.3.4.4. Dictionaries and Hash Tables\nA dictionary is a table of key-value pairs . A value in the dictionary can be \nlooked up quickly, given its key. The keys and values can be of any data type. \nThis kind of data structure is usually implemented either as a binary search \ntree or as a hash table.\nIn a binary tree implementation, the key-value pairs are stored in the \nnodes of the binary tree, and the tree is maintained in key-sorted order. Look-\ning up a value by key involves performing an O(log n) binary search.\nIn a hash table implementation, the values are stored in a ﬁ xed-size table, \nwhere each slot in the table represents one or more keys. To insert a key-value \npair into a hash table, the key is ﬁ rst converted into integer form via a pro-\ncess known as hashing (if it is not already an integer). Then an index into the \nhash table is calculated by taking the hashed key modulo the size of the table. \nFinally, the key-value pair is stored in the slot corresponding to that index. \nRecall that the modulo operator (% in C/C++) ﬁ nds the remainder of dividing \nthe integer key by the table size. So if the hash table has ﬁ ve slots, then a key of \n3 would be stored at index 3 (3 % 5 == 3), while a key of 6 would be stored \nat index 1 (6 % 5 == 1). Finding a key-value pair is an O(1) operation in the \nabsence of collisions.\nCollisions: Open and Closed Hash Tables\n Sometimes two or more keys end up occupying the same slot in the hash table. \nThis is known as a collision. There are two basic ways to resolve a collision, giv-\ning rise to two diﬀ erent kinds of hash tables:\nOpen\n• \n . In an open hash table (see Figure 5.10), collisions are resolved \nby simply storing more than one key-value pair at each index, usually \nin the form of a linked list. This approach is easy to implement and \nimposes no upper bound on the number of key-value pairs that can be \nstored. However, it does require memory to be allocated dynamically \nwhenever a new key-value pair is added to the table.\nClosed\n• \n . In a closed hash table (see Figure 5.11), collisions are resolved via \na process of probing until a vacant slot is found. (“Probing” means apply-\ning a well-deﬁ ned algorithm to search for a free slot.) This approach is \na bit more diﬃ  cult to implement, and it imposes an upper limit on the \nnumber of key-value pairs that can reside in the table (because each slot \ncan hold only one key-value pair). But the main beneﬁ t of this kind of \nhash table is that it uses up a ﬁ xed amount of memory and requires no dy-\nnamic memory allocation. Therefore it is oft en a good choice in a console \nengine.\n5.3. Containers\n",
      "content_length": 2641,
      "extraction_method": "Direct"
    },
    {
      "page_number": 262,
      "chapter": null,
      "content": "240 \n5. Engine Support Systems\nHashing\nHashing is the process of turning a key of some arbitrary data type into an \ninteger, which can be used modulo the table size as an index into the table. \nMathematically, given a key k, we want to generate an integer hash value h us-\ning the hash function H, and then ﬁ nd the index i into the table as follows:\n \nh = H(k), \n \ni = h mod N,  \nwhere N is the number of slots in the table, and the symbol mod represents the \nmodulo operation, i.e., ﬁ nding the remainder of the quotient h/N.\nIf the keys are unique integers, the hash function can be the identity func-\ntion, H(k) = k. If the keys are unique 32-bit ﬂ oating-point numbers, a hash func-\ntion might simply re-interpret the bit patt ern of the 32-bit ﬂ oat as if it were a \n32-bit integer.\nU32 hashFloat(float f)\n{\n union\n {\n  float \nasFloat;\nSlot 0\nSlot 1\nSlot 2\nSlot 3\nSlot 4\n(55, apple)\n(0, orange)\n(26, grape)\n(33, plum)\nFigure 5.10. An open hash table.\n(55, apple)\n(0, orange)\ncollision!\n(33, plum)\n(55, apple)\n(26, grape)\n(33, plum)\n(0, orange)\n(26, grape)\nprobe to\nfind new \nslot\n0\n1\n2\n3\n4\n0\n1\n2\n3\n4\nFigure 5.11. A closed hash table.\n",
      "content_length": 1140,
      "extraction_method": "Direct"
    },
    {
      "page_number": 263,
      "chapter": null,
      "content": "241 \n \n \nU32   asU32;\n \n} u;\n \nu.asFloat = f;\n \nreturn u.asU32;\n}\nIf the key is a string, we can employ a string hashing function, which combines \nthe ASCII or UTF codes of all the characters in the string into a single 32-bit \ninteger value.\nThe quality of the hashing function H(k) is crucial to the eﬃ  ciency of the \nhash table. A “good” hashing function is one that distributes the set of all valid \nkeys evenly across the table, thereby minimizing the likelihood of collisions. \nA hash function must also be reasonably quick to calculate and deterministic \nin the sense that it must produce the exact same output every time it is called \nwith an indentical input.\nStrings are probably the most prevalent type of key you’ll encounter, so \nit’s particularly helpful to know a “good” string hashing function. Here are a \nfew reasonably good ones:\nLOOKUP3 by Bob Jenkins (htt p://burtleburtle.net/bob/c/lookup3.c).\n• \nCyclic redundancy check functions, such as CRC-32 (htt p://en.wikipedia.\n• \norg/wiki/Cyclic_redundancy_check).\nMessage-digest algorithm 5 (MD5), a cryptographic hash which yields \n• \nexcellent results but is quite expensive to calculate (htt p://en.wikipedia.\norg/wiki/MD5).\nA number of other excellent alternatives can be found in an article by \n• \nPaul Hsieh available at htt p://www.azillionmonkeys.com/qed/hash.\nhtml.\nImplementing a Closed Hash Table\nIn a closed hash table, the key-value pairs are stored directly in the table, rath-\ner than in a linked list at each table entry. This approach allows the program-\nmer to deﬁ ne a priori the exact amount of memory that will be used by the \nhash table. A problem arises when we encounter a collision —two keys that end \nup wanting to be stored in the same slot in the table. To address this, we use a \nprocess known as probing.\nThe simplest approach is linear probing . Imagining that our hashing func-\ntion has yielded a table index of i, but that slot is already occupied, we simply \ntry slots (i + 1), (i + 2), and so on until an empty slot is found (wrapping around \nto the start of the table when i = N). Another variation on linear probing is to \nalternate searching forwards and backwards, (i + 1), (i – 1), (i + 2), (i – 2), and \n5.3. Containers\n",
      "content_length": 2228,
      "extraction_method": "Direct"
    },
    {
      "page_number": 264,
      "chapter": null,
      "content": "242 \n5. Engine Support Systems\nso on, making sure to modulo the resulting indices into the valid range of the \ntable.\nLinear probing tends to cause key-value pairs to “clump up.” To avoid \nthese clusters, we can use an algorithm known as quadratic probing . We start at \nthe occupied table index i and use the sequence of probes ij = (i  ± j 2) for j = 1, 2, \n3, …. In other words, we try (i + 12), (i – 12), (i + 22), (i – 22), and so on, remem-\nbering to always modulo the resulting index into the valid range of the table.\nWhen using closed hashing, it is a good idea to make your table size a \nprime number. Using a prime table size in conjunction with quadratic probing \ntends to yield the best coverage of the available table slots with minimal clus-\ntering. See htt p://www.cs.utk.edu/~eĳ khout/594-LaTeX/handouts/hashing-\nslides.pdf for a good discussion of why prime hash table sizes are preferable.\n5.4. Strings\nStrings are ubiquitous in almost every soft ware project, and game engines are \nno exception. On the surface, the string may seem like a simple, fundamental \ndata type. But when you start using strings in your projects, you will quickly \ndiscover a wide range of design issues and constraints, all of which must be \ncarefully accounted for.\n5.4.1. The Problem with Strings\nThe most fundamental question is how strings should be stored and managed \nin your program. In C and C++, strings aren’t even an atomic type—they are \nimplemented as arrays of characters. The variable length of strings means we \neither have to hard-code limitations on the sizes of our strings, or we need to \ndynamically allocate our string buﬀ ers. C++ programmers oft en prefer to use \na string class, rather than deal directly with character arrays. But then, which \nstring class should we use? STL provides a reasonably good string class, but if \nyou’ve decided not to use STL you might be stuck writing your own.\nAnother big string-related problem is that of localization —the process of \nadapting your soft ware for release in other languages. This is also known as \ninternationalization, or I18N for short. Any string that you display to the user \nin English must be translated into whatever languages you plan to support. \n(Strings that are used internally to the program but are never displayed to the \nuser are exempt from localization, of course.) This not only involves making \nsure that you can represent all the character glyphs of all the languages you \nplan to support (via an appropriate set of fonts), but it also means ensuring \nthat your game can handle diﬀ erent text orientations. For example, Chinese \n",
      "content_length": 2621,
      "extraction_method": "Direct"
    },
    {
      "page_number": 265,
      "chapter": null,
      "content": "243 \ntext is oriented vertically instead of horizontally, and some languages like He-\nbrew read right-to-left . Your game also needs to gracefully deal with the pos-\nsibility that a translated string will be either much longer, or much shorter, \nthan its English counterpart.\nFinally, it’s important to realize that strings are used internally within a \ngame engine for things like resource ﬁ le names and object ids. For example, \nwhen a game designer lays out a level, it’s highly convenient to permit him or \nher to identify the objects in the level using meaningful names, like “Player-\nCamera,” “enemy-tank-01,” or “explosionTrigger.”\nHow our engine deals with these internal strings oft en has pervasive ram-\niﬁ cations on the performance of the game. This is because strings are inherent-\nly expensive to work with at runtime. Comparing or copying ints or floats \ncan be accomplished via simple machine language instructions. On the other \nhand, comparing strings requires an O(n) scan of the character arrays using a \nfunction like strcmp() (where n is the length of the string). Copying a string \nrequires an O(n) memory copy, not to mention the possibility of having to \ndynamically allocate the memory for the copy. During one project I worked \non, we proﬁ led our game’s performance only to discover that strcmp() and \nstrcpy() were the top two most expensive functions! By eliminating unnec-\nessary string operations and using some of the techniques outlined in this \nsection, we were able to all but eliminate these functions from our proﬁ le, and \nincrease the game’s frame rate signiﬁ cantly. (I’ve heard similar stories from \ndevelopers at a number of diﬀ erent studios.)\n5.4.2. String Classes\nString classes can make working with strings much more convenient for the \nprogrammer. However, a string class can have hidden costs that are diﬃ  cult \nto see until the game is proﬁ led. For example, passing a string to a function \nusing a C-style character array is fast because the address of the ﬁ rst character \nis typically passed in a hardware register. On the other hand, passing a string \nobject might incur the overhead of one or more copy constructors, if the func-\ntion is not declared or used properly. Copying strings might involve dynamic \nmemory allocation, causing what looks like an innocuous function call to end \nup costing literally thousands of machine cycles.\nFor this reason, in game programming I generally like to avoid string \nclasses. However, if you feel a strong urge to use a string class, make sure you \npick or implement one that has acceptable runtime performance character-\nistics—and be sure all programmers that use it are aware of its costs. Know \nyour string class: Does it treat all string buﬀ ers as read-only? Does it utilize \nthe copy on write optimization? (See htt p://en.wikipedia.org/wiki/Copy-on-\n5.4. Strings\n",
      "content_length": 2869,
      "extraction_method": "Direct"
    },
    {
      "page_number": 266,
      "chapter": null,
      "content": "244 \n5. Engine Support Systems\nwrite.) As a rule of thumb, always pass string objects by reference, never by \nvalue (as the latt er oft en incurs string-copying costs). Proﬁ le your code early \nand oft en to ensure that your string class isn’t becoming a major source of lost \nframe rate!\nOne situation in which a specialized string class does seem justiﬁ able \nto me is when storing and managing ﬁ le system paths . Here, a hypothetical \nPath class could add signiﬁ cant value over a raw C-style character array. For \nexample, it might provide functions for extracting the ﬁ lename, ﬁ le exten-\nsion or directory from the path. It might hide operating system diﬀ erences by \nautomatically converting Windows-style backslashes to UNIX-style forward \nslashes or some other operating system’s path separator. Writing a Path class \nthat provides this kind of functionality in a cross-platform way could be high-\nly valuable within a game engine context. (See Section 6.1.1.4 for more details \non this topic.)\n5.4.3. Unique Identiﬁ ers\n The objects in any virtual game world need to be uniquely identiﬁ ed in some \nway. For example, in Pac Man we might encounter game objects named “pac_\nman,” “blinky,” “pinky,” “inky,” and “clyde.” Unique object identiﬁ ers allow \ngame designers to keep track of the myriad objects that make up their game \nworlds and also permit those objects to be found and operated on at runtime \nby the engine. In addition, the assets from which our game objects are con-\nstructed—meshes, materials, textures, audio clips, animations, and so on—all \nneed unique identiﬁ ers as well.\nStrings seem like a natural choice for such identiﬁ ers. Assets are oft en \nstored in individual ﬁ les on disk, so they can usually be identiﬁ ed uniquely by \ntheir ﬁ le paths, which of course are strings. And game objects are created by \ngame designers, so it is natural for them to assign their objects understandable \nstring names, rather than have to remember integer object indices, or 64- or \n128-bit globally unique identiﬁ ers (GUIDs). However, the speed with which \ncomparisons between unique identiﬁ ers can be made is of paramount impor-\ntance in a game, so strcmp() simply doesn’t cut it. We need a way to have \nour cake and eat it too—a way to get all the descriptiveness and ﬂ exibility of a \nstring, but with the speed of an integer.\n5.4.3.1. Hashed String Ids\n One good solution is to hash our strings. As we’ve seen, a hash function maps \na string onto a semi-unique integer. String hash codes can be compared just \nlike any other integers, so comparisons are fast. If we store the actual strings \nin a hash table, then the original string can always be recovered from the hash \n",
      "content_length": 2699,
      "extraction_method": "Direct"
    },
    {
      "page_number": 267,
      "chapter": null,
      "content": "245 \ncode. This is useful for debugging purposes and to permit hashed strings to \nbe displayed on-screen or in log ﬁ les. Game programmers sometimes use the \nterm string id to refer to such a hashed string. The Unreal engine uses the term \nname instead (implemented by class FName).\nAs with any hashing system, collisions are a possibility (i.e., two diﬀ erent \nstrings might end up with the same hash code). However, with a suitable hash \nfunction, we can all but guarantee that collisions will not occur for all rea-\nsonable input strings we might use in our game. Aft er all, a 32-bit hash code \nrepresents more than four billion possible values. So if our hash function does \na good job of distributing strings evenly throughout this very large range, we \nare unlikely to collide. At Naughty Dog, we used a variant of the CRC-32 al-\ngorithm to hash our strings, and we didn’t encounter a single collision in over \ntwo years of development on Uncharted: Drake’s Fortune.\n5.4.3.2. Some Implementation Ideas\nConceptually, it’s easy enough to run a hash function on your strings in order \nto generate string ids. Practically speaking, however, it’s important to con-\nsider when the hash will be calculated. Most game engines that use string \nids do the hashing at runtime. At Naughty Dog, we permit runtime hash-\ning of strings, but we also preprocess our source code using a simple utility \nthat searches for macros of the form SID(any-string) and translates each one \ndirectly into the appropriate hashed integer value. This permits string ids to \nbe used anywhere that an integer manifest constant can be used, including \nthe constant case labels of a switch statement. (The result of a function call \nthat generates a string id at runtime is not a constant, so it cannot be used as \na case label.)\nThe process of generating a string id from a string is sometimes called \ninterning the string, because in addition to hashing it, the string is typi-\ncally also added to a global string table. This allows the original string to \nbe recovered from the hash code later. You may also want your tools to be \ncapable of hashing strings into string ids. That way, when the tool generates \ndata for consumption by your engine, the strings will already have been \nhashed.\nThe main problem with interning a string is that it is a slow operation. \nThe hashing function must be run on the string, which can be an expensive \nproposition, especially when a large number of strings are being interned. \nIn addition, memory must be allocated for the string, and it must be copied \ninto the lookup table. As a result (if you are not generating string ids at \ncompile-time), it is usually best to intern each string only once and save oﬀ  \nthe result for later use. For example, it would be preferable to write code like \n5.4. Strings\n",
      "content_length": 2820,
      "extraction_method": "Direct"
    },
    {
      "page_number": 268,
      "chapter": null,
      "content": "246 \n5. Engine Support Systems\nthis because the latt er implementation causes the strings to be unnecessarily \nre-interned every time the function f() is called.\nstatic StringId\nsid_foo = internString(“foo”);\nstatic StringId\nsid_bar = internString(“bar”);\n// ...\nvoid f(StringId id)\n{\n \nif (id == sid_foo)\n {\n \n \n// handle case of id == “foo”\n }\n \nelse if (id == sid_bar)\n {\n \n \n// handle case of id == “bar”\n }\n}\nThis approach is much less eﬃ  cient.\nvoid f(StringId id)\n{\n \nif (id == internString(“foo”))\n {\n \n \n// handle case of id == “foo”\n }\n \nelse if (id == internString(“bar”))\n {\n \n \n// handle case of id == “bar”\n }\n}\nHere’s one possible implementation of internString().\nstringid.h\ntypedef U32 StringId;\nextern StringId internString(const char* str);\nstringid.cpp\nstatic HashTable<StringId, const char*> gStringIdTable;\n",
      "content_length": 830,
      "extraction_method": "Direct"
    },
    {
      "page_number": 269,
      "chapter": null,
      "content": "247 \nStringId internString(const char* str)\n{\n \nStringId sid = hashCrc32(str);\n \nHashTable<StringId, const char*>::iterator it\n  = \ngStringIdTable.find(sid);\n \nif (it == gStringTable.end())\n {\n \n \n// This string has not yet been added to the   \n \n \n \n// table. Add it, being sure to copy it in case   \n \n   // the original was dynamically allocated and \n \n \n// might later be freed.\n \n \ngStringTable[sid] = strdup(str);\n }\n \nreturn sid;\n}\nAnother idea employed by the Unreal Engine is to wrap the string id and \na pointer to the corresponding C-style character array in a tiny class. In the \nUnreal Engine, this class is called FName.\nUsing Debug Memory for Strings\nWhen using string ids, the strings themselves are only kept around for human \nconsumption. When you ship your game, you almost certainly won’t need \nthe strings—the game itself should only ever use the ids. As such, it’s a good \nidea to store your string table in a region of memory that won’t exist in the \nretail game. For example, a PS3 development kit has 256 MB of retail memory, \nplus an additional 256 MB of “debug” memory that is not available on a retail \nunit. If we store our strings in debug memory, we needn’t worry about their \nimpact on the memory footprint of the ﬁ nal shipping game. (We just need to \nbe careful never to write production code that depends on the strings being \navailable!)\n5.4.4. Localization\n Localization of a game (or any soft ware project) is a big undertaking. It is a \ntask which is best handled by planning for it from day one and accounting for \nit at every step of development. However, this is not done as oft en as we all \nwould like. Here are some tips that should help you plan your game engine \nproject for localization. For an in-depth treatment of soft ware localization, \nsee [29].\n5.4. Strings\n",
      "content_length": 1813,
      "extraction_method": "Direct"
    },
    {
      "page_number": 270,
      "chapter": null,
      "content": "248 \n5. Engine Support Systems\n5.4.4.1. Unicode\n The problem for most English-speaking soft ware developers is that they are \ntrained from birth (or thereabouts!) to think of strings as arrays of 8-bit ASCII \ncharacter codes (i.e., characters following the ANSI standard). ANSI strings \nwork great for a language with a simple alphabet, like English. But they just \ndon’t cut it for languages with complex alphabets containing a great many \nmore characters, sometimes totally diﬀ erent glyphs than English’s 26 lett ers. \nTo address the limitations of the ANSI standard, the Unicode character set \nsystem was devised.\nPlease set down this book right now and read the article entitled, “The \nAbsolute Minimum Every Soft ware Developer Absolutely, Positively Must \nKnow About Unicode and Character Sets (No Excuses!)” by Joel Spolsky. You \ncan ﬁ nd it here: htt p://www.joelonsoft ware.com/articles/Unicode.html. (Once \nyou’ve done that, please pick up the book again!)\nAs Joel describes in his article, Unicode is not a single standard but actu-\nally a family of related standards. You will need to select the speciﬁ c standard \nthat best suits your needs. The two most common choices I’ve seen used in \ngame engines are UTF-8 and UTF-16.\nUTF-8\n In UTF-8, the character codes are 8 bits each, but certain characters occupy \nmore than one byte. Hence the number of bytes occupied by a UTF-8 character \nstring is not necessarily the length of the string in characters. This is known as \na multibyte character set (MBCS), because each character may take one or more \nbytes of storage.\nOne of the big beneﬁ ts of the UTF-8 encoding is that it is backwards-com-\npatible with the ANSI encoding. This works because the ﬁ rst character of a \nmultibyte character sequence always has its most signiﬁ cant bit set (i.e., lies \nbetween 128 and 255, inclusive). Since the standard ANSI character codes are \nall less than 128, a plain old ANSI string is a valid and unambiguous UTF-8 \nstring as well.\nUTF-16\n The UTF-16 standard employs a simpler, albeit more expensive, approach. \nEach character takes up exactly 16 bits (whether it needs all of those bits or \nnot). As a result, dividing the number of bytes occupied by the string by two \nyields the number of characters. This is known as a wide character set (WCS), \nbecause each character is 16 bits wide instead of the 8 bits used by “regular” \nANSI chars.\n",
      "content_length": 2397,
      "extraction_method": "Direct"
    },
    {
      "page_number": 271,
      "chapter": null,
      "content": "249 \nUnicode under Windows\nUnder Microsoft  Windows, the data type wchar_t is used to represent a single \n“wide ” UTF-16 character (WCS), while the char type is used both for ANSI \nstrings and for multibyte UTF-16 strings (MBCS). What’s more, Windows per-\nmits you to write code that is character set independent. To accomplish this, a \ndata type known as TCHAR is provided. The data type TCHAR is a typedef\nto char when building your application in ANSI mode and is a typedef to \nwchar_t when building your application in UTF-16 (WCS) mode. (For consis-\ntency, the type WCHAR is also provided as a synonym for wchar_t.)\nThroughout the Windows API, a preﬁ x or suﬃ  x of “w,” “wcs,” or “W” \nindicates wide (UTF-16) characters; a preﬁ x or suﬃ  x of “t,” “tcs,” or “T” \nindicates the current character type (which might be ANSI or might be UTF-\n16, depending on how your application was built); and no preﬁ x or suf-\nﬁ x indicates plain old ANSI. STL uses a similar convention—for example, \nstd::string is STL’s ANSI string class, while std::wstring is its wide \ncharacter equivalent.\nPrett y much every standard C library function that deals with strings has \nequivalent WCS and MBCS versions under Windows. Unfortunately, the API \ncalls don’t use the terms UTF-8 and UTF-16, and the names of the functions \naren’t always 100% consistent. This all leads to some confusion among pro-\ngrammers who aren’t in the know. (But you aren’t one of those programmers!) \nTable 5.1 lists some examples.\nWindows also provides functions for translating between ANSI character \nstrings, multibyte UTF-8 strings, and wide UTF-16 strings. For example, wcs-\ntombs() converts a wide UTF-16 string into a multibyte UTF-8 string.\nComplete documentation for these functions can be found on Microsoft ’s \nMSDN web site. Here’s a link to the documentation for strcmp() and its ilk, \nfrom which you can quite easily navigate to the other related string-manip-\nulation functions using the tree view on the left -hand side of the page, or via \nthe search bar: htt p://msdn2.microsoft .com/en-us/library/kk6xf663(VS.80).\naspx.\nANSI\nWCS\nMBCS\nstrcmp()\nwcscmp()\n_mbscmp()\nstrcpy()\nwcscpy()\n_mbscpy()\nstrlen()\nwcslen()\n_mbstrlen()\nTable 5.1. Variants of some common standard C library string functions for use with ANSI, \nwide and multibyte character sets.\n5.4. Strings\n",
      "content_length": 2338,
      "extraction_method": "Direct"
    },
    {
      "page_number": 272,
      "chapter": null,
      "content": "250 \n5. Engine Support Systems\nUnicode on Consoles\nThe Xbox 360 soft ware development kit (XDK) uses WCS strings prett y much \nexclusively, for all strings—even for internal strings like ﬁ le paths. This is cer-\ntainly one valid approach to the localization problem, and it makes for very \nconsistent string handling throughout the XDK. However, the UTF-16 encod-\ning is a bit wasteful on memory, so diﬀ erent game engines may employ diﬀ er-\nent conventions. At Naughty Dog, we use 8-bit char strings throughout our \nengine, and we handle foreign languages via a UTF-8 encoding. The choice of \nencoding is not important, as long as you select one as early in the project as \npossible and stick with it consistently.\n5.4.4.2. Other Localization Concerns\nEven once you have adapted your soft ware to use Unicode characters, there \nare still a host of other localization problems to contend with. For one thing, \nstrings aren’t the only place where localization issues arise. Audio clips in-\ncluding recorded voices must be translated. Textures may have English words \npainted into them that require translation. Many symbols have diﬀ erent mean-\nings in diﬀ erent cultures. Even something as innocuous as a no-smoking sign \nmight be misinterpreted in another culture. In addition, some markets draw the \nboundaries between the various game-rating levels diﬀ erently. For example, in \nJapan a Teen-rated game is not permitt ed to show blood of any kind, whereas \nin North America small red blood spatt ers are considered acceptable.\nFor strings, there are other details to worry about as well. You will need \nto manage a database of all human-readable strings in your game, so that \nthey can all be reliably translated. The soft ware must display the proper lan-\nguage given the user’s installation sett ings. The formatt ing of the strings may \nbe totally diﬀ erent in diﬀ erent languages—for example, Chinese is writt en \nvertically, and Hebrew reads right-to-left . The lengths of the strings will vary \ngreatly from language to language. You’ll also need to decide whether to ship \na single DVD or Blu-ray disc that contains all languages or ship diﬀ erent discs \nfor particular territories.\nThe most crucial components in your localization system will be the cen-\ntral database of human-readable strings and an in-game system for looking \nup those strings by id. For example, let’s say you want a heads-up display \nthat lists the score of each player with “Player 1 Score:” and “Player 2 Score:” \nlabels and that also displays the text “Player 1 Wins” or “Player 2 Wins” at \nthe end of a round. These four strings would be stored in the localization \ndatabase under unique ids that are understandable to you, the developer of \nthe game. So our database might use the ids “p1score,” “p2score,” “p1wins,” \nand “p2wins,” respectively. Once our game’s strings have been translated into \n",
      "content_length": 2885,
      "extraction_method": "Direct"
    },
    {
      "page_number": 273,
      "chapter": null,
      "content": "251 \nFrench, our database would look something like the simple example shown in \nTable 5.2. Additional columns can be added for each new language your game \nsupports.\nThe exact format of this database is up to you. It might be as simple as \na Microsoft  Excel worksheet that can be saved as a comma-separated values \n(CSV ) ﬁ le and parsed by the game engine or as complex as a full-ﬂ edged Or-\nacle database. The speciﬁ cs of the string database are largely unimportant to \nthe game engine, as long as it can read in the string ids and the correspond-\ning Unicode strings for whatever language(s) your game supports. (However, \nthe speciﬁ cs of the database may be very important from a practical point of \nview, depending upon the organizational structure of your game studio. A \nsmall studio with in-house translators can probably get away with an Excel \nspreadsheet located on a network drive. But a large studio with branch oﬃ  ces \nin Britain, Europe, South America, and Japan would probably ﬁ nd some kind \nof distributed database a great deal more amenable.)\nAt runtime, you’ll need to provide a simple function that returns the Uni-\ncode string in the “current” language, given the unique id of that string. The \nfunction might be declared like this:\nwchar_t getLocalizedString(const char* id);\nand it might be used like this:\nvoid drawScoreHud(const Vector3& score1Pos,\n \n \n \n \n         const \nVector3& score2Pos)\n{\n renderer.displayTextOrtho(\ngetLocalizedString(\"p1score\"),\n  score1Pos);\n renderer.displayTextOrtho(\ngetLocalizedString(\"p2score\"),\n  score2Pos);\n \n// ...\n }\nId\nEnglish\nFrench\np1score\n“Player 1 Score”\n“Grade Joueur 1”\np2score\n“Player 2 Score”\n“Grade Joueur 2”\np1wins\n“Player 1 wins!”\n“Joueur un gagne!”\np2wins\n“Player 2 wins!”\n“Joueur deux gagne!”\nTable 5.2. Example of a string database used for localization.\n5.4. Strings\n",
      "content_length": 1850,
      "extraction_method": "Direct"
    },
    {
      "page_number": 274,
      "chapter": null,
      "content": "252 \n5. Engine Support Systems\nOf course, you’ll need some way to set the “current” language globally. This \nmight be done via a conﬁ guration sett ing which is ﬁ xed during the installa-\ntion of the game. Or you might allow users to change the current language on \nthe ﬂ y via an in-game menu. Either way, the sett ing is not diﬃ  cult to imple-\nment; it can be as simple as a global integer variable specifying the index of \nthe column in the string table from which to read (e.g., column one might be \nEnglish, column two French, column three Spanish, and so on).\nOnce you have this infrastructure in place, your programmers must re-\nmember to never display a raw string to the user. They must always use the id of \na string in the database and call the look-up function in order to retrieve the \nstring in question.\n5.5. Engine Conﬁ guration\n Game engines are complex beasts, and they invariably end up having a large \nnumber of conﬁ gurable options. Some of these options are exposed to the \nplayer via one or more options menus in-game. For example, a game might \nexpose options related to graphics quality, the volume of music and sound ef-\nfects, or controller conﬁ guration. Other options are created for the beneﬁ t of \nthe game development team only and are either hidden or stripped out of the \ngame completely before it ships. For example, the player character’s maximum \nwalk speed might be exposed as an option so that it can be ﬁ ne-tuned during \ndevelopment, but it might be changed to a hard-coded value prior to ship.\n5.5.1. \nLoading and Saving Options\n A conﬁ gurable option can be implemented trivially as a global variable or a \nmember variable of a singleton class. However, conﬁ gurable options are not \nparticularly useful unless their values can be conﬁ gured, stored on a hard \ndisk, memory card, or other storage medium and later retrieved by the game. \nThere are a number of simple ways to load and save conﬁ guration options:\nText conﬁ guration ﬁ les.\n• \n By far the most common method of saving and \nloading conﬁ guration options is by placing them into one or more text \nﬁ les. The format of these ﬁ les varies widely from engine to engine, but it \nis usually very simple. For example, Windows INI ﬁ les (which are used \nby the Ogre3D renderer) consist of ﬂ at lists of key-value pairs grouped \ninto logical sections.\n[SomeSection]\n Key1=Value1\n Key2=Value2\n",
      "content_length": 2390,
      "extraction_method": "Direct"
    },
    {
      "page_number": 275,
      "chapter": null,
      "content": "253 \n[AnotherSection]\n Key3=Value3\n Key4=Value4\n Key5=Value5\nThe XML format is another common choice for conﬁ gurable game op-\ntions ﬁ les.\nCompressed binary ﬁ les.\n• \n Most modern consoles have hard disk drives in \nthem, but older consoles could not aﬀ ord this luxury. As a result, all \ngame consoles since the Super Nintendo Entertainment System (SNES) \nhave come equipped with proprietary removable memory cards that \npermit both reading and writing of data. Game options are sometimes \nstored on these cards, along with saved games. Compressed binary ﬁ les \nare the format of choice on a memory card, because the storage space \navailable on these cards is oft en very limited.\nThe Windows registry\n• \n . The Microsoft  Windows operating system pro-\nvides a global options database known as the registry. It is stored as a \ntree, where the interior nodes (known as registry keys) act like ﬁ le fold-\ners, and the leaf nodes store the individual options as key-value pairs. \nAny application, game or otherwise, can reserve an entire subtree (i.e., a \nregistry key) for its exclusive use, and then store any set of options with-\nin it. The Windows registry acts like a carefully-organized collection of \nINI ﬁ les, and in fact it was introduced into Windows as a replacement \nfor the ever-growing network of INI ﬁ les used by both the operating \nsystem and Windows applications.\nCommand line options\n• \n . The command line can be scanned for option set-\ntings. The engine might provide a mechanism for controlling any option \nin the game via the command line, or it might expose only a small sub-\nset of the game’s options here.\nEnvironment variables\n• \n . On personal computers running Windows, Linux, \nor MacOS, environment variables are sometimes used to store conﬁ gu-\nration options as well.\nOnline user proﬁ les.\n• \n With the advent of online gaming communities like \nXbox Live , each user can create a proﬁ le and use it to save achievements, \npurchased and unlockable game features, game options, and other in-\nformation. The data is stored on a central server and can be accessed by \nthe player wherever an Internet connection is available.\n5.5.2. Per-User Options\nMost game engines diﬀ erentiate between global options and per-user options . \nThis is necessary because most games allow each player to conﬁ gure the game \n5.5. Engine Conﬁ guration\n",
      "content_length": 2359,
      "extraction_method": "Direct"
    },
    {
      "page_number": 276,
      "chapter": null,
      "content": "254 \n5. Engine Support Systems\nto his or her liking. It is also a useful concept during development of the game, \nbecause it allows each programmer, artist, and designer to customize his or \nher work environment without aﬀ ecting other team members.\nObviously care must be taken to store per-user options in such a way that \neach player “sees” only his or her options and not the options of other play-\ners on the same computer or console. In a console game, the user is typically \nallowed to save his or her progress, along with per-user options such as con-\ntroller preferences, in “slots” on a memory card or hard disk. These slots are \nusually implemented as ﬁ les on the media in question.\nOn a Windows machine, each user has a folder under C:\\Documents and \nSett ings containing information such as the user’s desktop, his or her My Doc-\numents folder, his or her Internet browsing history and temporary ﬁ les, and \nso on. A hidden subfolder named Application Data is used to store per-user \ninformation on a per-application basis; each application creates a folder un-\nder Application Data and can use it to store whatever per-user information it \nrequires.\nWindows games sometimes store per-user conﬁ guration data in the reg-\nistry. The registry is arranged as a tree, and one of the top-level children of \nthe root node, called HKEY_CURRENT_USER, stores sett ings for whichever user \nhappens to be logged on. Every user has his or her own subtree in the registry \n(stored under the top-level subtree HKEY_USERS), and HKEY_CURRENT_USER\nis really just an alias to the current user’s subtree. So games and other applica-\ntions can manage per-user conﬁ guration options by simply reading and writ-\ning them to keys under the HKEY_CURRENT_USER subtree.\n5.5.3. Conﬁ guration Management in Some Real Engines\nIn this section, we’ll take a brief look at how some real game engines manage \ntheir conﬁ guration options.\n5.5.3.1. Example: Quake’s CVARs\n The Quake family of engines uses a conﬁ guration management system known \nas console variables, or CVARs for short. A CVAR is just a ﬂ oating-point or \nstring global variable whose value can be inspected and modiﬁ ed from within \nQuake’s in-game console. The values of some CVARs can be saved to disk and \nlater reloaded by the engine.\nAt runtime, CVARs are stored in a global linked list. Each CVAR is a dy-\nnamically-allocated instance of struct cvar_t, which contains the variable’s \nname, its value as a string or ﬂ oat, a set of ﬂ ag bits, and a pointer to the next \nCVAR in the linked list of all CVARs. CVARs are accessed by calling Cvar_\nGet(), which creates the variable if it doesn’t already exist and modiﬁ ed by \n",
      "content_length": 2677,
      "extraction_method": "Direct"
    },
    {
      "page_number": 277,
      "chapter": null,
      "content": "255 \ncalling Cvar_Set(). One of the bit ﬂ ags, CVAR_ARCHIVE, controls whether or \nnot the CVAR will be saved into a conﬁ guration ﬁ le called conﬁ g.cfg. If this ﬂ ag \nis set, the value of the CVAR will persist across multiple runs of the game.\n5.5.3.2. Example: Ogre3D\n The Ogre3D rendering engine uses a collection of text ﬁ les in Windows INI \nformat for its conﬁ guration options. By default, the options are stored in three \nﬁ les, each of which is located in the same folder as the executable program:\nplugins.cfg\n• \n contains options specifying which optional engine plug-ins \nare enabled and where to ﬁ nd them on disk.\nresources.cfg\n• \n contains a search path specifying where game assets (a.k.a. \nmedia, a.k.a. resources) can be found.\nogre.cfg\n• \n contains a rich set of options specifying which renderer (DirectX \nor OpenGL) to use and the preferred video mode, screen size, etc.\nOut of the box, Ogre provides no mechanism for storing per-user conﬁ gu-\nration options. However, the Ogre source code is freely available, so it would \nbe quite easy to change it to search for its conﬁ guration ﬁ les in the user’s C:\\\nDocuments and Sett ings folder instead of in the folder containing the execut-\nable. The Ogre::ConfigFile class makes it easy to write code that reads and \nwrites brand new conﬁ guration ﬁ les, as well.\n5.5.3.3. Example: Uncharted: Drake’s Fortune\n Naughty Dog’s Uncharted engine makes use of a number of conﬁ guration \nmechanisms.\nIn-Game Menu Settings\nThe Uncharted engine supports a powerful in-game menu system, allowing \ndevelopers to control global conﬁ guration options and invoke commands. \nThe data types of the conﬁ gurable options must be relatively simple (primar-\nily Boolean, integer, and ﬂ oating-point variables), but this limitation did not \nprevent the developers of Uncharted from creating literally hundreds of useful \nmenu-driven options.\nEach conﬁ guration option is implemented as a global variable. When the \nmenu option that controls an option is created, the address of the global vari-\nable is provided, and the menu item directly controls its value. As an exam-\nple, the following function creates a submenu item containing some options \nfor Uncharted’s rail vehicles (the vehicles used in the “Out of the Frying Pan” \njeep chase level). It deﬁ nes menu items controlling three global variables: two \nBooleans and one ﬂ oating-point value. The items are collected onto a menu, \n5.5. Engine Conﬁ guration\n",
      "content_length": 2460,
      "extraction_method": "Direct"
    },
    {
      "page_number": 278,
      "chapter": null,
      "content": "256 \n5. Engine Support Systems\nand a special item is returned that will bring up the menu when selected. \nPresumably the code calling this function would add this item to the parent \nmenu that it is building.\nDMENU::ItemSubmenu * CreateRailVehicleMenu()\n{\n \nextern bool g_railVehicleDebugDraw2D;\n \nextern bool g_railVehicleDebugDrawCameraGoals;\n \nextern float g_railVehicleFlameProbability;\n \nDMENU::Menu * pMenu = new DMENU::Menu(\n  \"RailVehicle\");\n    pMenu->PushBackItem(\n  new \nDMENU::ItemBool(\"Draw 2D Spring Graphs\",\n   \nDMENU::ToggleBool,\n  &\ng_railVehicleDebugDraw2D));\n    pMenu->PushBackItem(\n  new \nDMENU::ItemBool(\"Draw Goals (Untracked)\",\n  DMENU::ToggleBool,\n &g_railVehicleDebugDrawCameraGoals));\n \nDMENU::ItemFloat * pItemFloat;\n \npItemFloat = new DMENU::ItemFloat(\n  \"FlameProbability\", \n  DMENU::\nEditFloat, 5, \"%5.2f\",\n  &\ng_railVehicleFlameProbability);\n \npItemFloat->SetRangeAndStep(0.0f, 1.0f, 0.1f, 0.01f);\n pMenu->PushBackItem(pItemFloat);\n \nDMENU::ItemSubmenu * pSubmenuItem;\n \npSubmenuItem = new DMENU::ItemSubmenu(\n  \"RailVehicle...\", \npMenu);\n \nreturn pSubmenuItem;\n}\nThe value of any option can be saved by simply marking it with the circle \nbutt on on the PS3 joypad when the corresponding menu item is selected. The \nmenu sett ings are saved in an INI-style text ﬁ le, allowing the saved global vari-\nables to retain the values across multiple runs of the game. The ability to con-\ntrol which options are saved on a per-menu-item basis is highly useful, because \nany option which is not saved will take on its programmer-speciﬁ ed default \nvalue. If a programmer changes a default, all users will “see” the new value, \nunless of course a user has saved a custom value for that particular option.\n",
      "content_length": 1727,
      "extraction_method": "Direct"
    },
    {
      "page_number": 279,
      "chapter": null,
      "content": "257 \nCommand Line Arguments\nThe Uncharted engine scans the command line for a predeﬁ ned set of special \noptions. The name of the level to load can be speciﬁ ed, along with a number \nof other commonly-used arguments.\nScheme Data Deﬁ nitions\n The vast majority of engine and game conﬁ guration information in Uncharted \nis speciﬁ ed using a Lisp -like language called Scheme . Using a proprietary data \ncompiler, data structures deﬁ ned in the Scheme language are transformed \ninto binary ﬁ les that can be loaded by the engine. The data compiler also spits \nout header ﬁ les containing C struct declarations for every data type deﬁ ned \nin Scheme. These header ﬁ les allow the engine to properly interpret the data \ncontained in the loaded binary ﬁ les. The binary ﬁ les can even be recompiled \nand reloaded on the ﬂ y, allowing developers to alter the data in Scheme and \nsee the eﬀ ects of their changes immediately (as long as data members are not \nadded or removed, as that would require a recompile of the engine).\nThe following example illustrates the creation of a data structure specify-\ning the properties of an animation. It then exports three unique animations to \nthe game. You may have never read Scheme code before, but for this relatively \nsimple example it should be prett y self-explanatory. One oddity you’ll notice \nis that hyphens are permitt ed within Scheme symbols, so simple-animation\nis a single symbol (unlike in C/C++ where simple-animation would be the \nsubtraction of two variables, simple and animation).\nsimple-animation.scm\n;; Define a new data type called simple-animation.\n(deftype simple-animation ()\n (\n \n \n(name             string)\n \n \n(speed            float   :default 1.0)\n \n \n(fade-in-seconds  float   :default 0.25)\n \n \n(fade-out-seconds float   :default 0.25)\n )\n)\n;; Now define three instances of this data structure...\n(define-export anim-walk\n \n(new simple-animation\n  :name \n“walk”\n  :speed \n1.0\n )\n)\n5.5. Engine Conﬁ guration\n",
      "content_length": 1974,
      "extraction_method": "Direct"
    },
    {
      "page_number": 280,
      "chapter": null,
      "content": "258 \n5. Engine Support Systems\n(define-export anim-walk-fast\n \n(new simple-animation\n  :name \n\"walk\"\n  :speed \n2.0\n )\n)\n(define-export anim-jump\n \n(new simple-animation\n  :name \n\"jump\"\n  :fade-in-seconds \n0.1\n  :fade-out-seconds \n0.1\n )\n)\nThis Scheme code would generate the following C/C++ header ﬁ le:\nsimple-animation.h\n// WARNING: This file was automatically generated from \n// Scheme. Do not hand-edit.\nstruct SimpleAnimation\n{\n \nconst char* \nm_name;\n float \n  m_speed;\n float \n  m_fadeInSeconds;\n float \n  m_fadeOutSeconds;\n};\nIn-game, the data can be read by calling the LookupSymbol() function, which \nis templated on the data type returned, as follows:\n#include \"simple-animation.h\"\nvoid someFunction()\n{\nSimpleAnimation* pWalkAnim\n  = \nLookupSymbol<SimpleAnimation*>(\"anim-walk\");\nSimpleAnimation* pFastWalkAnim\n  = \nLookupSymbol<SimpleAnimation*>(\n   \"\nanim-walk-fast\");\nSimpleAnimation* pJumpAnim\n  = \nLookupSymbol<SimpleAnimation*>(\"anim-jump\");\n \n// use the data here...\n}\n",
      "content_length": 987,
      "extraction_method": "Direct"
    },
    {
      "page_number": 281,
      "chapter": null,
      "content": "259 \nThis system gives the programmers a great deal of ﬂ exibility in deﬁ n-\ning all sorts of conﬁ guration data—from simple Boolean, ﬂ oating-point, and \nstring options all the way to complex, nested, interconnected data structures. \nIt is used to specify detailed animation trees, physics parameters, player me-\nchanics, and so on.\n5.5. Engine Conﬁ guration\n",
      "content_length": 360,
      "extraction_method": "Direct"
    },
    {
      "page_number": 282,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 283,
      "chapter": null,
      "content": "261\n6\nResources and \nthe File System\nG\names are by nature multimedia experiences. A game engine therefore \nneeds to be capable of loading and managing a wide variety of diﬀ erent \nkinds of media—texture bitmaps, 3D mesh data, animations, audio clips, col-\nlision and physics data, game world layouts, and the list goes on. Moreover, \nbecause memory is usually scarce, a game engine needs to ensure that only \none copy of each media ﬁ le is loaded into memory at any given time. For ex-\nample, if ﬁ ve meshes share the same texture, then we would like to have only \none copy of that texture in memory, not ﬁ ve. Most game engines employ some \nkind of resource manager (a.k.a. asset manager, a.k.a. media manager) to load and \nmanage the myriad resources that make up a modern 3D game.\nEvery resource manager makes heavy use of the ﬁ le system. On a per-\nsonal computer, the ﬁ le system is exposed to the programmer via a library \nof operating system calls. However, game engines oft en “wrap” the native \nﬁ le system API in an engine-speciﬁ c API, for two primary reasons. First, the \nengine might be cross-platform, in which case the game engine’s ﬁ le system \nAPI can shield the rest of the soft ware from diﬀ erences between diﬀ erent \ntarget hardware platforms. Second, the operating system’s ﬁ le system API \nmight not provide all the tools needed by a game engine. For example, many \nengines support ﬁ le streaming (i.e., the ability to load data “on the ﬂ y” while \nthe game is running), yet most operating systems don’t provide a streaming \nﬁ le system API out of the box. Console game engines also need to provide ac-\n",
      "content_length": 1626,
      "extraction_method": "Direct"
    },
    {
      "page_number": 284,
      "chapter": null,
      "content": "262 \n6. Resources and the File System\ncess to a variety of removable and non-removable media, from memory sticks \nto optional hard drives to a DVD-ROM or Blu-ray ﬁ xed disk to network ﬁ le \nsystems (e.g., Xbox Live or the PlayStation Network , PSN). The diﬀ erences \nbetween various kinds of media can likewise be “hidden” behind a game \nengine’s ﬁ le system API.\nIn this chapter, we’ll ﬁ rst explore the kinds of ﬁ le system APIs found in \nmodern 3D game engines. Then we’ll see how a typical resource manager \nworks.\n6.1. \nFile System\nA game engine’s ﬁ le system API typically addresses the following areas of \nfunctionality:\nz manipulating ﬁ le names and paths,\nz opening, closing, reading and writing individual ﬁ les,\nz scanning the contents of a directory,\nz handling asynchronous ﬁ le I/O requests (for streaming).\nWe’ll take a brief look at each of these in the following sections.\n6.1.1. \nFile Names and Paths\nA path is a string describing the location of a ﬁ le or directory within a ﬁ le sys-\ntem hierarchy. Each operating system uses a slightly diﬀ erent path format, but \npaths have essentially the same structure on every operating system. A path \ngenerally takes the following form:\nvolume/directory1/ directory2/…/directoryN/ﬁ le-name\n \nor\nvolume/directory1/directory2/…/directory(N – 1)/directoryN\nIn other words, a path generally consists of an optional volume speciﬁ er fol-\nlowed by a sequence of path components separated by a reserved path separa-\ntor character such as the forward or backward slash (/ or \\). Each component \nnames a directory along the route from the root directory to the ﬁ le or direc-\ntory in question. If the path speciﬁ es the location of a ﬁ le, the last compo-\nnent in the path is the ﬁ le name; otherwise it names the target directory. The \nroot directory is usually indicated by a path consisting of the optional volume \nspeciﬁ er followed by a single path separator character (e.g., / on UNIX, or C:\\\non Windows).\n",
      "content_length": 1964,
      "extraction_method": "Direct"
    },
    {
      "page_number": 285,
      "chapter": null,
      "content": "263 \n6.1. File System\n6.1.1.1. \nDifferences Across Operating Systems\nEach operating system introduces slight variations on this general path struc-\nture. Here are some of the key diﬀ erences between Microsoft  DOS , Microsoft  \nWindows , the UNIX family of operating systems, and Apple Macintosh OS:\nz UNIX uses a forward slash (/) as its path component separator, while \nDOS and older versions of Windows used a backslash (\\) as the path \nseparator. Recent versions of Windows allow either forward or back-\nward slashes to be used to separate path components, although some \napplications still fail to accept forward slashes.\nz Mac OS 8 and 9 use the colon (:) as the path separator character. Mac \nOS X is based on UNIX, so it supports UNIX’s forward slash notation.\nz UNIX and its variants don’t support volumes as separate directory hi-\nerarchies. The entire ﬁ le system is contained within a single monolithic \nhierarchy, and local disk drives, network drives, and other resources are \nmounted so that they appear to be subtrees within the main hierarchy. As \na result, UNIX paths never have a volume speciﬁ er.\nz On Microsoft  Windows, volumes can be speciﬁ ed in two ways. A local \ndisk drive is speciﬁ ed using a single lett er followed by a colon (e.g., the \nubiquitous C:). A remote network share can either be mounted so that \nit looks like a local disk, or it can be referenced via a volume speciﬁ er \nconsisting of two backslashes followed by the remote computer name \nand the name of a shared directory or resource on that machine (e.g., \n\\\\some-computer\\some-share). This double backslash notation is an \nexample of the Universal Naming Convention (UNC).\nz Under DOS and early versions of Windows, a ﬁ le name could be up to \neight characters in length, with a three-character extension which was \nseparated from the main ﬁ le name by a dot. The extension described \nthe ﬁ le’s type, for example .txt for a text ﬁ le or .exe for an executable \nﬁ le. In recent Windows implementations, ﬁ le names can contain any \nnumber of dots (as they can under UNIX), but the characters aft er the \nﬁ nal dot are still interpreted as the ﬁ le’s extension by many applications \nincluding the Windows Explorer.\nz Each operating system disallows certain characters in the names of ﬁ les \nand directories. For example, a colon cannot appear anywhere in a Win-\ndows or DOS path except as part of a drive lett er volume speciﬁ er. Some \noperating systems permit a subset of these reserved characters to ap-\npear in a path as long as the path is quoted in its entirety or the oﬀ end-\ning character is escaped by preceding it with a backslash or some other \n",
      "content_length": 2651,
      "extraction_method": "Direct"
    },
    {
      "page_number": 286,
      "chapter": null,
      "content": "264 \n6. Resources and the File System\nreserved escape character. For example, ﬁ le and directory names may \ncontain spaces under Windows, but such a path must be surrounded by \ndouble quotes in certain contexts.\nz Both UNIX and Windows have the concept of a current working directory \nor CWD (also known as the present working directory or PWD). The CWD \ncan be set from a command shell via the cd (change directory) command \non both operating systems, and it can be queried by typing cd with \nno arguments under Windows or by executing the pwd command on \nUNIX. Under UNIX there is only one CWD. Under Windows, each vol-\nume has its own private CWD.\nz Operating systems that support multiple volumes, like Windows, also \nhave the concept of a current working volume. From a Windows com-\nmand shell, the current volume can be set by entering its drive lett er and \na colon followed by the Enter key (e.g., C:<Enter>).\nz Consoles oft en also employ a set of predeﬁ ned path preﬁ xes to repre-\nsent multiple volumes. For example, PLAYSTATION 3 uses the preﬁ x \n/dev_bdvd/ to refer to the Bluray disk drive, while /dev_hddx/ refers \nto one or more hard disks (where x is the index of the device). On a PS3 \ndevelopment kit, /app_home/ maps to a user-deﬁ ned path on whatever \nhost machine is being used for development. During development, the \ngame usually reads its assets from /app_home/ rather than from the \nBluray or the hard disk.\n6.1.1.2. \nAbsolute and Relative Paths\nAll paths are speciﬁ ed relative to some location within the ﬁ le system. When a \npath is speciﬁ ed relative to the root directory, we call it an absolute path . When \nit is relative to some other directory in the ﬁ le system hierarchy, we call it a \nrelative pa th .\nUnder both UNIX and Windows, absolute paths start with a path sepa-\nrator character (/ or \\), while relative paths have no leading path separator. \nOn Windows, both absolute and relative paths may have an optional volume \nspeciﬁ er—if the volume is omitt ed, then the path is assumed to refer to the \ncurrent working volume.\nThe following paths are all absolute:\nWindows\nz C:\\Windows\\System32\nz D:\\ (root directory on the D: volume)\nz \\ (root directory on the current working volume)\n",
      "content_length": 2225,
      "extraction_method": "Direct"
    },
    {
      "page_number": 287,
      "chapter": null,
      "content": "265 \nz \\game\\assets\\animation\\walk.anim (current working volume)\nz \\\\joe-dell\\Shared_Files\\Images\\foo.jpg (network path)\nUNIX\nz /usr/local/bin/grep\nz /game/src/audio/effects.cpp\nz / (root directory)\nThe following paths are all relative:\nWindows\nz System32 (relative to CWD \\Windows on the current volume)\nz X:animation\\walk.anim (relative to CWD \\game\\assets on the X:\nvolume)\nUNIX\nz bin/grep (relative to CWD /usr/local)\nz src/audio/effects.cpp (relative to CWD /game)\n6.1.1.3. \nSearch Paths\nThe term path should not be confused with the term search path. A path is a \nstring representing the location of a single ﬁ le or directory within the ﬁ le \nsystem hierarchy. A search path is a string containing a list of paths, each sepa-\nrated by a special character such as a colon or semicolon, which is searched \nwhen looking for a ﬁ le. For example, when you run any program from a com-\nmand prompt, the operating system ﬁ nds the executable ﬁ le by searching \neach directory on the search path contained in the shell’s PATH environment \nvariable.\nSome game engines also use search paths to locate resource ﬁ les. For ex-\nample, the Ogre3D rendering engine uses a resource search path contained in \na text ﬁ le named resources.cfg. The ﬁ le provides a simple list of directories \nand Zip archives that should be searched in order when trying to ﬁ nd an as-\nset. That said, searching for assets at runtime is a time-consuming proposition. \nUsually there’s no reason our assets’ paths cannot be known a priori. Presum-\ning this is the case, we can avoid having to search for assets at all—which is \nclearly a superior approach.\n6.1.1.4. \nPath APIs\nClearly paths are much more complex than simple strings. There are many \nthings a programmer may need to do when dealing with paths, such as isolat-\ning the directory, ﬁ lename and extension, canonicalizing a path, converting \n6.1. File System\n",
      "content_length": 1889,
      "extraction_method": "Direct"
    },
    {
      "page_number": 288,
      "chapter": null,
      "content": "266 \n6. Resources and the File System\nback and forth between absolute and relative paths, and so on. It can be ex-\ntremely helpful to have a feature-rich API to help with these tasks.\nMicrosoft  Windows provides an API for this purpose. It is implement-\ned by the dynamic link library shlwapi.dll, and exposed via the header \nﬁ le shlwapi.h. Complete documentation for this API is provided on the \nMicrosoft  Developer’s Network (MSDN) at the following URL: htt p://msdn2.\nmicrosoft .com/en-us/library/bb773559(VS.85).aspx.\nOf course, the shlwapi API is only available on Win32 platforms. Sony \nprovides a similar API for use on the PLAYSTATION 3. But when writing a \ncross-platform game engine, we cannot use platform-speciﬁ c APIs directly. A \ngame engine may not need all of the functions provided by an API like sh-\nlwapi anyway. For these reasons, game engines oft en implement a stripped-\ndown path-handling API that meets the engine’s particular needs and works \non every operating system targeted by the engine. Such an API can be imple-\nmented as a thin wrapper around the native API on each platform or it can be \nwritt en from scratch.\n6.1.2. Basic File I/O\nThe standard C library provides two APIs for opening, reading, and writing \nthe contents of ﬁ les—one buﬀ ered and the other unbuﬀ ered. Every ﬁ le I/O \nAPI requires data blocks known as buﬀ ers to serve as the source or destination \nof the bytes passing between the program and the ﬁ le on disk. We say a ﬁ le \nI/O API is buﬀ ered when the API manages the necessary input and output data \nbuﬀ ers for you. With an unbuﬀ ered API, it is the responsibility of the pro-\ngrammer using the API to allocate and manage the data buﬀ ers. The standard \nC library’s buﬀ ered ﬁ le I/O routines are sometimes referred to as the stream \nI/O API, because they provide an abstraction which makes disk ﬁ les look like \nstreams of bytes.\nThe standard C library functions for buﬀ ered and un-buﬀ ered ﬁ le I/O are \nlisted in Table 6.1.\nThe standard C library I/O functions are well-documented, so we will not \nrepeat detailed documentation for them here. For more information, please \nrefer to htt p://msdn2.microsoft .com/en-us/library/c565h7xx(VS.71).aspx for \nMicrosoft ’s implementation of the buﬀ ered (stream I/O) API, and to htt p://\nmsdn2.microsoft .com/en-us/library/40bbyw78(VS.71).aspx \nfor \nMicrosoft ’s \nimplementation of the unbuﬀ ered (low-level I/O) API.\nOn UNIX and its variants, the standard C library’s unbuﬀ ered I/O routes \nare native operating system calls. However, on Microsoft  Windows these rou-\ntines are merely wrappers around an even lower-level API. The Win32 func-\ntion CreateFile() creates or opens a ﬁ le for writing or reading, ReadFile()\n",
      "content_length": 2725,
      "extraction_method": "Direct"
    },
    {
      "page_number": 289,
      "chapter": null,
      "content": "267 \nand WriteFile() read and write data, respectively, and CloseFile() closes \nan open ﬁ le handle. The advantage to using low-level system calls as opposed \nto standard C library functions is that they expose all of the details of the na-\ntive ﬁ le system. For example, you can query and control the security att ributes \nof ﬁ les when using the Windows native API—something you cannot do with \nthe standard C library.\nSome game teams ﬁ nd it useful to manage their own buﬀ ers. For example, \nthe Red Alert 3 team at Electronic Arts observed that writing data into log ﬁ les \nwas causing signiﬁ cant performance degradation. They changed the logging \nsystem so that it accumulated its output into a memory buﬀ er, writing the \nbuﬀ er out to disk only when it was ﬁ lled. Then they moved the buﬀ er dump \nroutine out into a separate thread to avoid stalling the main game loop.\n6.1.2.1. \nTo Wrap or Not To Wrap\n A game engine can be writt en to use the standard C library’s ﬁ le I/O functions or \nthe operating system’s native API. However, many game engines wrap the ﬁ le \nI/O API in a library of custom I/O functions. There are at least three advantages \nto wrapping the operating system’s I/O API. First, the engine programmers \ncan guarantee identical behavior across all target platforms, even when native \nlibraries are inconsistent or buggy on a particular platform. Second, the API \ncan be simpliﬁ ed down to only those functions actually required by the engine, \nwhich keeps maintenance eﬀ orts to a minimum. Third, extended functionality \ncan be provided. For example, the engine’s custom wrapper API might be ca-\npable of dealing ﬁ les on a hard disk, a DVD-ROM or Blu-ray disk on a console, \n6.1. File System\nOperation\nBuﬀ ered API\nUnbuﬀ ered API\nOpen a ﬁ le\nfopen()\nopen()\nClose a ﬁ le\nfclose()\nclose()\nRead from a ﬁ le\nfread()\nread()\nWrite to a ﬁ le\nfwrite()\nwrite()\nSeek to an oﬀ set\nfseek()\nseek()\nReturn current oﬀ set\nftell()\ntell()\nRead a single line\nfgets()\nn/a\nWrite a single line\nfputs()\nn/a\nRead formatt ed string\nfscanf()\nn/a\nWrite formatt ed string\nfprintf()\nn/a\nQuery ﬁ le status\nfstat()\nstat()\nTable 6.1.  Buffered and unbuffered ﬁ le operations in the standard C library.\n",
      "content_length": 2201,
      "extraction_method": "Direct"
    },
    {
      "page_number": 290,
      "chapter": null,
      "content": "268 \n6. Resources and the File System\nﬁ les on a network (e.g., remote ﬁ les managed by Xbox Live or PSN ), and also \nwith ﬁ les on memory sticks or other kinds of removable media.\n6.1.2.2. Synchronous File I/O\n Both of the standard C library’s ﬁ le I/O libraries are synchronous, meaning that \nthe program making the I/O request must wait until the data has been com-\npletely transferred to or from the media device before continuing. The fol-\nlowing code snippet demonstrates how the entire contents of a ﬁ le might be \nread into an in-memory buﬀ er using the synchronous I/O function fread(). \nNotice how the function syncReadFile() does not return until all the data \nhas been read into the buﬀ er provided.\nbool syncReadFile(const char* filePath,\n \nU8* buffer, size_t bufferSize, size_t& rBytesRead)\n{\n \nFILE* handle = fopen(filePath, \"rb\");\n \nif (handle)\n {\n// BLOCK here until all data has been read.\n \n \nsize_t bytesRead = fread(buffer, 1, bufferSize,   \n   handle);\n \n \nint err = ferror(handle); // get error if any\n  fclose(handle);\n \n \nif (0 == err)\n  {\n   rBytesRead \n= bytesRead;\n   return \ntrue;\n  }\n }\n \nreturn false;\n}\nvoid main(int argc, const char* argv[])\n{\n \nU8 testBuffer[512];\n \nsize_t bytesRead = 0;\n \nif (syncReadFile(\"C:\\\\testfile.bin\",\n \n \ntestBuffer, sizeof(testBuffer), bytesRead))\n {\n \n \nprintf(\"success: read %u bytes\\n\", bytesRead);\n// Contents of buffer can be used here...\n }\n}\n",
      "content_length": 1411,
      "extraction_method": "Direct"
    },
    {
      "page_number": 291,
      "chapter": null,
      "content": "269 \n6.1.3. Asynchronous File I/O\n Streaming refers to the act of loading data in the background while the main \nprogram continues to run. Many games provide the player with a seamless, \nload-screen-free playing experience by streaming data for upcoming levels \nfrom the DVD-ROM, Blu-ray disk, or hard drive while the game is being \nplayed. Audio and texture data are probably the most commonly streamed \ntypes of data, but any type of data can be streamed, including geometry, level \nlayouts, and animation clips.\nIn order to support streaming, we must utilize an asynchronous ﬁ le I/O \nlibrary, i.e., one which permits the program to continue to run while its I/O re-\nquests are being satisﬁ ed. Some operating systems provide an asynchronous \nﬁ le I/O library out of the box. For example, the Windows Common Language \nRuntime (CLR, the virtual machine upon which languages like Visual BASIC, \nC#, managed C++ and J# are implemented) provides functions like System.\nIO.BeginRead() and System.IO.BeginWrite(). An asynchronous API \nknown as fios is available for the PLAYSTATION 3. If an asynchronous ﬁ le \nI/O library is not available for your target platform, it is possible to write one \nyourself. And even if you don’t have to write it from scratch, it’s probably a \ngood idea to wrap the system API for portability.\nThe following code snippet demonstrates how the entire contents of a ﬁ le \nmight be read into an in-memory buﬀ er using an asynchronous read opera-\ntion. Notice that the asyncReadFile() function returns immediately—the \ndata is not present in the buﬀ er until our callback function asyncReadCom-\nplete() has been called by the I/O library.\nAsyncRequestHandle g_hRequest; // handle to async I/O  \n \n          \n// request\nU8 g_asyncBuffer[512];         // input buffer\nstatic void asyncReadComplete(AsyncRequestHandle   \n \n \n hRequest);\nvoid main(int argc, const char* argv[])\n{\n \n// NOTE: This call to asyncOpen() might itself be an\n \n// asynchronous call, but we’ll ignore that detail  \n \n \n// here and just assume it’s a blocking function.\n \nAsyncFileHandle hFile = asyncOpen(\n  \"C:\\\\testfile.bin\");\n \nif (hFile)\n {\n6.1. File System\n",
      "content_length": 2155,
      "extraction_method": "Direct"
    },
    {
      "page_number": 292,
      "chapter": null,
      "content": "270 \n6. Resources and the File System\n \n \n// This function requests an I/O read, then  \n \n \n \n \n// returns immediately (non-blocking).\n  g_hRequest \n= asyncReadFile(\n \n \n \nhFile,                  // file handle\n \n \n \ng_asyncBuffer,          // input buffer\n \n \n \nsizeof(g_asyncBuffer),  // size of buffer\nasyncReadComplete);     // callback function\n }\n \n// Now go on our merry way...\n \n// (This loop simulates doing real work while we wait  \n \n// for the I/O read to complete.)\n \nfor (;;)\n {\n  OutputDebugString(\"zzz...\\n\");\n  Sleep(50);\n }\n}\n// This function will be called when the data has been read.\nstatic void asyncReadComplete(\n \nAsyncRequestHandle hRequest)\n{\n \nif (hRequest == g_hRequest \n  && \nasyncWasSuccessful(hRequest))\n {\n \n \n// The data is now present in g_asyncBuffer[] and  \n \n \n// can be used. Query for the number of bytes   \n \n \n \n// actually read:\n \n \nsize_t bytes = asyncGetBytesReadOrWritten(\n   hRequest);\n  char \nmsg[256];\n \n \nsprintf(msg, \"async success, read %u bytes\\n\",  \n \n   bytes); \n  OutputDebugString(msg);\n }\n}\nMost asynchronous I/O libraries permit the main program to wait for an \nI/O operation to complete some time aft er the request was made. This can be \nuseful in situations where only a limited amount of work can be done before \nthe results of a pending I/O request are needed. This is illustrated in the fol-\nlowing code snippet.\nU8 g_asyncBuffer[512];         // input buffer\nvoid main(int argc, const char* argv[])\n{\n",
      "content_length": 1466,
      "extraction_method": "Direct"
    },
    {
      "page_number": 293,
      "chapter": null,
      "content": "271 \n \nAsyncRequestHandle hRequest = ASYNC_INVALID_HANDLE;\n \nAsyncFileHandle hFile = asyncOpen(\n  \"C:\\\\testfile.bin\");\n \nif (hFile)\n {\n \n \n// This function requests an I/O read, then  \n \n \n \n \n// returns immediately (non-blocking).\n  hRequest \n= asyncReadFile(\n \n \n \nhFile,                  // file handle\n \n \n \ng_asyncBuffer,          // input buffer\n \n \n \nsizeof(g_asyncBuffer),  // size of buffer\nNULL);                  // no callback\n }\n \n// Now do some limited amount of work...\n \nfor (int i = 0; i < 10; ++i)\n {\n  OutputDebugString(\"zzz...\\n\");\n  Sleep(50);\n }\n \n// We can’t do anything further until we have that  \n \n \n// data, so wait for it here.\nasyncWait(hRequest);\n \nif (asyncWasSuccessful(hRequest))\n {\n \n \n// The data is now present in g_asyncBuffer[] and  \n \n \n// can be used. Query for the number of bytes   \n \n \n \n// actually read:\n \n \nsize_t bytes = asyncGetBytesReadOrWritten(\n   hRequest);\n  char \nmsg[256];\n \n \nsprintf(msg, \"async success, read %u bytes\\n\",  \n \n   bytes);\n  OutputDebugString(msg);\n }\n}\nSome asynchronous I/O libraries allow the programmer to ask for an esti-\nmate of how long a particular asynchronous operation will take to complete. \nSome APIs also allow you to set deadlines on a request (eﬀ ectively prioritizes \nthe request relative to other pending requests), and to specify what happens \nwhen a request misses its deadline (e.g., cancel the request, notify the pro-\ngram and keep trying, etc.)\n6.1. File System\n",
      "content_length": 1458,
      "extraction_method": "Direct"
    },
    {
      "page_number": 294,
      "chapter": null,
      "content": "272 \n6. Resources and the File System\n6.1.3.1. \nPriorities\nIt’s important to remember that ﬁ le I/O is a real-time system, subject to dead-\nlines just like the rest of the game. Therefore, asynchronous I/O operations \noft en have varying priorities. For example, if we are streaming audio from \nthe hard disk or Bluray and playing it on the ﬂ y, loading the next buﬀ er-full \nof audio data is clearly higher priority than, say, loading a texture or a chunk \nof a game level. Asynchronous I/O systems must be capable of suspending \nlower-priority requests, so that higher-priority I/O requests have a chance to \ncomplete within their deadlines.\n6.1.3.2. How Asynchronous File I/O Works\nAsynchronous ﬁ le I/O works by handling I/O requests in a separate thread . \nThe main thread calls functions that simply place requests on a queue and \nthen return immediately. Meanwhile, the I/O thread picks up requests from \nthe queue and handles them sequentially using blocking I/O routines like \nread() or fread(). When a request is completed, a callback provided by \nthe main thread is called, thereby notifying it that the operation is done. If the \nmain thread chooses to wait for an I/O request to complete, this is handled via \na semaphore. (Each request has an associated semaphore, and the main thread \ncan put itself to sleep waiting for that semaphore to be signaled by the I/O \nthread upon completion of the request.)\nVirtually any synchronous operation you can imagine can be transformed \ninto an asynchronous operation by moving the code into a separate thread—\nor by running it on a physically separate processor, such as on one of the six \nsynergistic processing units (SPUs) on the PLAYSTATION 3. See Section 7.6 \nfor more details.\n6.2. The Resource Manager\n Every game is constructed from a wide variety of resources (sometimes called \nassets or media). Examples include meshes, materials, textures, shader pro-\ngrams, animations, audio clips, level layouts, collision primitives, physics pa-\nrameters, and the list goes on. A game’s resources must be managed, both in \nterms of the oﬄ  ine tools used to create them, and in terms of loading, unload-\ning, and manipulating them at runtime. Therefore every game engine has a \nresource manager of some kind.\nEvery resource manager is comprised of two distinct but integrated com-\nponents. One component manages the chain of oﬀ -line tools used to create the \nassets and transform them into their engine-ready form. The other component \n",
      "content_length": 2490,
      "extraction_method": "Direct"
    },
    {
      "page_number": 295,
      "chapter": null,
      "content": "273 \nmanages the resources at runtime, ensuring that they are loaded into memory \nin advance of being needed by the game and making sure they are unloaded \nfrom memory when no longer needed.\nIn some engines, the resource manager is a cleanly-designed, uniﬁ ed, \ncentralized subsystem that manages all types of resources used by the game. \nIn other engines, the resource manager doesn’t exist as a single subsystem \nper se, but is rather spread across a disparate collection of subsystems, per-\nhaps writt en by diﬀ erent individuals at various times over the engine’s long \nand sometimes colorful history. But no matt er how it is implemented, a re-\nsource manager invariably takes on certain responsibilities and solves a well-\nunderstood set of problems. In this section, we’ll explore the functionality \nand some of the implementation details of a typical game engine resource \nmanager.\n6.2.1. Off-Line Resource Management and the Tool Chain\n6.2.1.1. \nRevision Control for Assets\n On a small game project, the game’s assets can be managed by keeping loose \nﬁ les sitt ing around on a shared network drive with an ad hoc directory struc-\nture. This approach is not feasible for a modern commercial 3D game, com-\nprised of a massive number and variety of assets. For such a project, the team \nrequires a more formalized way to track and manage its assets.\nSome game teams use a source code revision control system to manage \ntheir resources. Art source ﬁ les (Maya scenes, Photoshop .PSD ﬁ les, Illustrator \nﬁ les, etc.) are checked in to Perforce or a similar package by the artists. This \napproach works reasonably well, although some game teams build custom \nasset management tools to help ﬂ att en the learning curve for their artists. Such \ntools may be simple wrappers around a commercial revision control system, \nor they might be entirely custom.\nDealing with Data Size\nOne of the biggest problems in the revision control of art assets is the sheer \namount of data. Whereas C++ and script source code ﬁ les are small, relative \nto their impact on the project, art ﬁ les tend to be much, much larger. Because \nmany source control systems work by copying ﬁ les from the central reposito-\nry down to the user’s local machine, the sheer size of the asset ﬁ les can render \nthese packages almost entirely useless.\nI’ve seen a number of diﬀ erent solutions to this problem employed at \nvarious studios. Some studios turn to commercial revision control systems \nlike Alienbrain that have been speciﬁ cally designed to handle very large data \n6.2. The Resource Manager\n",
      "content_length": 2570,
      "extraction_method": "Direct"
    },
    {
      "page_number": 296,
      "chapter": null,
      "content": "274 \n6. Resources and the File System\nsizes. Some teams simply “take their lumps” and allow their revision control \ntool to copy assets locally. This can work, as long as your disks are big enough \nand your network bandwidth suﬃ  cient, but it can also be ineﬃ  cient and slow \nthe team down. Some teams build elaborate systems on top of their revision \ncontrol tool to ensure that a particular end-user only gets local copies of the \nﬁ les he or she actually needs. In this model, the user either has no access to \nthe rest of the repository or can access it on a shared network drive when \nneeded.\nAt Naughty Dog we use a proprietary tool that makes use of UNIX symbol-\nic links to virtually eliminate data copying, while permitt ing each user to have \na complete local view of the asset repository. As long as a ﬁ le is not checked \nout for editing, it is a symlink to a master ﬁ le on a shared network drive. Sym-\nbolic links occupy very litt le space on the local disk, because it is nothing more \nthan a directory entry. When the user checks out a ﬁ le for editing, the symlink \nis removed, and a local copy of the ﬁ le replaces it. When the user is done edit-\ning and checks the ﬁ le in, the local copy becomes the new master copy, its revi-\nsion history is updated in a master database, and the local ﬁ le turns back into \na symlink. This systems works very well, but it requires the team to build their \nown revision control system from scratch; I am unaware of any commercial \ntool that works like this. Also, symbolic links are a UNIX feature—such a tool \ncould probably be built with Windows junctions (the Windows equivalent of \na symbolic link), but I haven’t seen anyone try it as yet.\n6.2.1.2. The Resource Database\n As we’ll explore in depth in the next section, most assets are not used in their \noriginal format by the game engine. They need to pass through some kind of \nasset conditioning pipeline, whose job it is to convert the assets into the binary \nformat needed by the engine. For every resource that passes through the asset \nconditioning pipeline, there is some amount of metadata that describes how \nthat resource should be processed. When compressing a texture bitmap, we \nneed to know what type of compression best suits that particular image. When \nexporting an animation, we need to know what range of frames in Maya \nshould be exported. When exporting character meshes out of a Maya scene \ncontaining multiple characters, we need to know which mesh corresponds to \nwhich character in the game.\nTo manage all of this metadata, we need some kind of database. If we are \nmaking a very small game, this database might be housed in the brains of the \ndevelopers themselves. I can hear them now: “Remember: the player’s anima-\ntions need to have the ‘ﬂ ip X’ ﬂ ag set, but the other characters must not have it \nset… or… rats… is it the other way around?”\n",
      "content_length": 2885,
      "extraction_method": "Direct"
    },
    {
      "page_number": 297,
      "chapter": null,
      "content": "275 \nClearly for any game of respectable size, we simply cannot rely on the \nmemories of our developers in this manner. For one thing, the sheer volume of \nassets becomes overwhelming quite quickly. Processing individual resource \nﬁ les by hand is also far too time-consuming to be practical on a full-ﬂ edged \ncommercial game production. Therefore, every professional game team has \nsome kind of semi-automated resource pipeline, and the data that drives the \npipeline is stored in some kind of resource database.\nThe resource database takes on vastly diﬀ erent forms in diﬀ erent game \nengines. In one engine, the metadata describing how a resource should be \nbuilt might be embedded into the source assets themselves (e.g., it might be \nstored as so-called blind data within a Maya ﬁ le). In another engine, each \nsource resource ﬁ le might be accompanied by a small text ﬁ le that describes \nhow it should be processed. Still other engines encode their resource build-\ning metadata in a set of XML ﬁ les, perhaps wrapped in some kind of custom \ngraphical user interface. Some engines employ a true relational database, such \nas Microsoft  Access, MySQL, or conceivably even a heavy-weight database \nlike Oracle.\nWhatever its form, a resource database must provide the following basic \nfunctionality:\nz The ability to deal with multiple types of resources, ideally (but certainly \nnot necessarily) in a somewhat consistent manner.\nz The ability to create new resources.\nz The ability to delete resources.\nz The ability to inspect and modify existing resources.\nz The ability to move a resource’s source ﬁ le(s) from one location to an-\nother on-disk. (This is very helpful because artists and game designers \noft en need to rearrange assets to reﬂ ect changing project goals, re-think-\ning of game designs, feature additions and cuts, etc.)\nz The ability of a resource to cross-reference other resources (e.g., the ma-\nterial used by a mesh, or the collection of animations needed by level \n17). These cross-references typically drive both the resource building \nprocess and the loading process at runtime.\nz The ability to maintain referential integrity of all cross-references within \nthe database and to do so in the face of all common operations such as \ndeleting or moving resources around.\nz The ability to maintain a revision history, complete with a log of who \nmade each change and why.\nz It is also very helpful if the resource database supports searching or \nquerying in various ways. For example, a developer might want to \n6.2. The Resource Manager\n",
      "content_length": 2565,
      "extraction_method": "Direct"
    },
    {
      "page_number": 298,
      "chapter": null,
      "content": "276 \n6. Resources and the File System\nknow in which levels a particular animation is used or which textures \nare referenced by a set of materials. Or they might simply be trying to \nﬁ nd a resource whose name momentarily escapes them.\nIt should be prett y obvious from looking at the above list that creating a \nreliable and robust resource database is no small task. When designed well \nand implemented properly, the resource database can quite literally make \nthe diﬀ erence between a team that ships a hit game and a team that spins its \nwheels for 18 months before being forced by management to abandon the \nproject (or worse). I know this to be true, because I’ve personally experienced \nboth.\n6.2.1.3. Some Successful Resource Database Designs\nEvery game team will have diﬀ erent requirements and make diﬀ erent deci-\nsions when designing their resource database. However, for what it’s worth, \nhere are some designs that have worked well in my own experience:\nUnreal Engine 3\n Unreal’s resource database is managed by their über-tool, UnrealEd . UnrealEd \nis responsible for literally everything, from resource metadata management to \nasset creation to level layout and more. UnrealEd has its drawbacks, but its \nsingle biggest beneﬁ t is that UnrealEd is a part of the game engine itself. This \npermits assets to be created and then immediately viewed in their full glory, \nexactly as they will appear in-game. The game can even be run from within \nUnrealEd, in order to visualize the assets in their natural surroundings and \nsee if and how they work in-game.\nAnother big beneﬁ t of UnrealEd is what I would call one-stop shopping. \nUnrealEd’s Generic Browser (depicted in Figure 6.1) allows a developer to \naccess literally every resource that is consumed by the engine. Having a sin-\ngle, uniﬁ ed, and reasonably-consistent interface for creating and managing \nall types of resources is a big win. This is especially true considering that the \nresource data in most other game engines is fragmented across countless in-\nconsistent and oft en cryptic tools. Just being able to ﬁ nd any resource easily in \nUnrealEd is a big plus.\nUnreal can be less error-prone than many other engines, because assets \nmust be explicitly imported into Unreal’s resource database. This allows re-\nsources to be checked for validity very early in the production process. In \nmost game engines, any old data can be thrown into the resource database, \nand you only know whether or not that data is valid when it is eventually \nbuilt—or sometimes not until it is actually loaded into the game at runtime. \nBut with Unreal, assets can be validated as soon as they are imported into \n",
      "content_length": 2670,
      "extraction_method": "Direct"
    },
    {
      "page_number": 299,
      "chapter": null,
      "content": "277 \nUnrealEd. This means that the person who created the asset gets immediate \nfeedback as to whether his or her asset is conﬁ gured properly.\nOf course, Unreal’s approach has some serious drawbacks. For one thing, \nall resource data is stored in a small number of large package ﬁ les . These ﬁ les \nare binary, so they are not easily merged by a revision control package like \nCVS, Subversion, or Perforce. This presents some major problems when more \nthan one user wants to modify resources that reside in a single package. Even \nif the users are trying to modify diﬀ erent resources, only one user can lock the \npackage at a time, so the other has to wait. The severity of this problem can be \nreduced by dividing resources into relatively small, granular packages, but it \ncannot practically be eliminated.\nReferential integrity is quite good in UnrealEd, but there are still some \nproblems. When a resource is renamed or moved around, all references to it \nare maintained automatically using a dummy object that remaps the old re-\n6.2. The Resource Manager\nFigure 6.1.  UnrealEd’s Generic Browser.\n",
      "content_length": 1104,
      "extraction_method": "Direct"
    },
    {
      "page_number": 300,
      "chapter": null,
      "content": "278 \n6. Resources and the File System\nsource to its new name/location. The problem with these dummy remapping \nobjects is that they hang around and accumulate and sometimes cause prob-\nlems, especially if a resource is deleted. Overall, Unreal’s referential integrity \nis quite good, but it is not perfect.\nDespite its problems, UnrealEd is by far the most user-friendly, well-in-\ntegrated, and streamlined asset creation toolkit, resource database, and asset- \nconditioning pipeline that I have ever worked with.\nNaughty Dog’s Uncharted: Drake’s Fortune Engine\n For Uncharted: Drake’s Fortune (UDF), Naughty Dog stored its resource \nmetadata in a MySQL database. A custom graphical user interface was writt en \nto manage the contents of the database. This tool allowed artists, game design-\ners, and programmers alike to create new resources, delete existing resources, \nand inspect and modify resources as well. This GUI was a crucial component \nof the system, because it allowed users to avoid having to learn the intricacies \nof interacting with a relational database via SQL.\nThe original MySQL database used on UDF did not provide a useful his-\ntory of the changes made to the database, nor did it provide a good way to roll \nback “bad” changes. It also did not support multiple users editing the same \nresource, and it was diﬃ  cult to administer. Naughty Dog has since moved \naway from MySQL in favor of an XML ﬁ le-based asset database, managed \nunder Perforce.\nBuilder, Naughty Dog’s resource database GUI, is depicted in Figure 6.2. \nThe window is broken into two main sections: a tree view showing all resourc-\nes in the game on the left  and a properties window on the right, allowing the \nresource(s) that are selected in the tree view to be viewed and edited. The re-\nsource tree contains folders for organizational purposes, so that the artists and \ngame designers can organize their resources in any way they see ﬁ t. Various \ntypes of resources can be created and managed within any folder, including \nactors and levels, and the various subresources that comprise them (primar-\nily meshes, skeletons, and animations). Animations can also be grouped into \npseudo-folders known as bundles. This allows large groups of animations to \nbe created and then managed as a unit, and prevents a lot of wasted time drag-\nging individual animations around in the tree view.\nThe asset conditioning pipeline on UDF consists of a set of resource ex-\nporters, compilers, and linkers that are run from the command line. The engine \nis capable of dealing with a wide variety of diﬀ erent kinds of data objects, but \nthese are packaged into one of two types of resource ﬁ les: actors and levels. An \nactor can contain skeletons, meshes, materials, textures, and/or animations. \nA level contains static background meshes, materials and textures, and also \nlevel-layout information. To build an actor, one simply types ba name-of-actor \n",
      "content_length": 2934,
      "extraction_method": "Direct"
    },
    {
      "page_number": 301,
      "chapter": null,
      "content": "279 \non the command line; to build a level, one types bl name-of-level. These com-\nmand-line tools query the database to determine exactly how to build the actor \nor level in question. This includes information on how to export the assets \nfrom DCC tools like Maya, Photoshop etc., how to process the data, and how \nto package it into binary .pak ﬁ les that can be loaded by the game engine. This \nis much simpler than in many engines, where resources have to be exported \nmanually by the artists—a time-consuming, tedious, and error-prone task.\nThe beneﬁ ts of the resource pipeline design used by Naughty Dog in-\nclude:\nz Granular resources. Resources can be manipulated in terms of logical en-\ntities in the game—meshes, materials, skeletons, and animations. These \n6.2. The Resource Manager\nFigure 6.2. The front-end GUI for Naughty Dog’s off-line resource database, Builder.\n",
      "content_length": 880,
      "extraction_method": "Direct"
    },
    {
      "page_number": 302,
      "chapter": null,
      "content": "280 \n6. Resources and the File System\nresource types are granular enough that the team almost never has \nconﬂ icts in which two users want to edit the same resource simultane-\nously.\nz The necessary features (and no more). The Builder tool provides a powerful \nset of features that meet the needs of the team, but Naughty Dog didn’t \nwaste any resources creating features they didn’t need.\nz Obvious mapping to source ﬁ les. A user can very quickly determine which \nsource assets (native DCC ﬁ les, like Maya .ma ﬁ les or photoshop .psd \nﬁ les) make up a particular resource.\nz Easy to change how DCC data is exported and processed. Just click on the \nresource in question and twiddle its processing properties within the \nresource database GUI.\nz Easy to build assets. Just type ba or bl followed by the resource name on \nthe command line. The dependency system takes care of the rest.\nOf course, the UDF tool chain has some drawbacks as well, including:\nz Lack of visualization tools. The only way to preview an asset is to load \nit into the game or the model/animation viewer (which is really just a \nspecial mode of the game itself).\nz The tools aren’t fully integrated. Naughty Dog uses one tool to lay out \nlevels, another to manage the majority of resources in the resource data-\nbase, and a third to set up materials and shaders (this is not part of the \nresource database front-end). Building the assets is done on the com-\nmand line. It might be a bit more convenient if all of these functions \nwere to be integrated into a single tool. However, Naughty Dog has no \nplans to do this, because the beneﬁ t would probably not outweigh the \ncosts involved.\nOgre’s Resource Manager System\n Ogre3D is a rendering engine, not a full-ﬂ edged game engine. That said, Ogre \ndoes boast a reasonably complete and very well-designed runtime resource \nmanager. A simple, consistent interface is used to load virtually any kind of \nresource. And the system has been designed with extensibility in mind. Any \nprogrammer can quite easily implement a resource manager for a brand new \nkind of asset and integrate it easily into Ogre’s resource framework.\nOne of the drawbacks of Ogre’s resource manager is that it is a runtime- \nonly solution. Ogre lacks any kind of oﬀ -line resource database. Ogre does \nprovide some exporters which are capable of converting a Maya ﬁ le into a \nmesh that can be used by Ogre (complete with materials, shaders, a skeleton \nand optional animations). However, the exporter must be run manually from \n",
      "content_length": 2525,
      "extraction_method": "Direct"
    },
    {
      "page_number": 303,
      "chapter": null,
      "content": "281 \nwithin Maya itself. Worse, all of the metadata describing how a particular \nMaya ﬁ le should be exported and processed must be entered by the user do-\ning the export.\nIn summary, Ogre’s runtime resource manager is powerful and well-de-\nsigned. But Ogre would beneﬁ t a great deal from an equally powerful and \nmodern resource database and asset conditioning pipeline on the tools side.\nMicrosoft’s XNA\n XNA is a game development toolkit by Microsoft , targeted at the PC and Xbox \n360 platforms. XNA’s resource management system is unique, in that it lever-\nages the project management and build systems of the Visual Studio IDE to \nmanage and build the assets in the game as well. XNA’s game development \ntool, Game Studio Express, is just a plug-in to Visual Studio Express. You can \nread more about Game Studio Express at htt p://msdn.microsoft .com/en-us/\nlibrary/bb203894.aspx.\n6.2.1.4. The Asset Conditioning Pipeline\nIn Section 1.7, we learned that resource data is typically created using ad-\nvanced digital content creation (DCC) tools like Maya, Z-Brush, Photoshop, or \nHoudini. However, the data formats used by these tools are usually not suit-\nable for direct consumption by a game engine. So the majority of resource data \nis passed through an asset conditioning pipeline (ACP) on its way to the game \nengine. The ACP is sometimes referred to as the resource conditioning pipeline \n(RCP), or simply the tool chain.\nEvery resource pipeline starts with a collection of source assets in native \nDCC formats (e.g., Maya .ma or .mb ﬁ les, Photoshop .psd ﬁ les, etc.)  These \nassets are typically passed through three processing stages on their way to the \ngame engine:\n \n1. Exporters . We need some way of gett ing the data out of the DCC’s na-\ntive format and into a format that we can manipulate. This is usually \naccomplished by writing a custom plug-in for the DCC in question. It \nis the plug-in’s job to export the data into some kind of intermediate ﬁ le \nformat that can be passed to later stages in the pipeline. Most DCC ap-\nplications provide a reasonably convenient mechanism for doing this. \nMaya actually provides three: a C++ SDK, a scripting language called \nMEL , and most recently a Python interface as well.\n \nIn cases where a DCC application provides no customization hooks, we \ncan always save the data in one of the DCC tool’s native formats. With \nany luck, one of these will be an open format, a reasonably-intuitive text \nformat, or some other format that we can reverse engineer. Presuming \n6.2. The Resource Manager\n",
      "content_length": 2557,
      "extraction_method": "Direct"
    },
    {
      "page_number": 304,
      "chapter": null,
      "content": "282 \n6. Resources and the File System\nthis is the case, we can pass the ﬁ le directly to the next stage of the pipe-\nline.\n \n2. Resource compilers . We oft en have to “massage” the raw data exported \nfrom a DCC application in various ways in order to make it game-ready. \nFor example, we might need to rearrange a mesh’s triangles into strips, or \ncompress a texture bitmap, or calculate the arc lengths of the segments of \na Catmull-Rom spline. Not all types of resources need to be compiled—\nsome might be game-ready immediately upon being exported.\n \n3. Resource linkers . Multiple resource ﬁ les sometimes need to be combined \ninto a single useful package prior to being loaded by the game engine. \nThis mimics the process of linking together the object ﬁ les of a compiled \nC++ program into an executable ﬁ le, and so this process is sometimes \ncalled resource linking. For example, when building a complex compos-\nite resource like a 3D model, we might need to combine the data from \nmultiple exported mesh ﬁ les, multiple material ﬁ les, a skeleton ﬁ le, and \nmultiple animation ﬁ les into a single resource. Not all types of resources \nneed to be linked—some assets are game-ready aft er the export or com-\npile steps.\nResource Dependencies and Build Rules\n Much like compiling the source ﬁ les in a C or C++ project and then linking \nthem into an executable, the asset conditioning pipeline processes source as-\nsets (in the form of Maya geometry and animation ﬁ les, Photoshop PSD ﬁ les, \nraw audio clips, text ﬁ les, etc.), converts them into game-ready form, and then \nlinks them together into a cohesive whole for use by the engine. And just like \nthe source ﬁ les in a computer program, game assets oft en have interdepen-\ndencies. (For example, a mesh refers to one or more materials, which in turn \nrefer to various textures.) These interdependencies typically have an impact \non the order in which assets must be processed by the pipeline. (For example, \nwe might need to build a character’s skeleton before we can process any of \nthat character’s animations.) In addition, the dependencies between assets tell \nus which assets need to be rebuilt when a particular source asset changes.\nBuild dependencies revolve not only around changes to the assets them-\nselves, but also around changes to data formats. If the format of the ﬁ les used \nto store triangle meshes changes, for instance, all meshes in the entire game \nmay need to be reexported and/or rebuilt. Some game engines employ data \nformats that are robust to version changes. For example, an asset may contain \na version number, and the game engine may include code that “knows” how \nto load and make use of legacy assets. The downside of such a policy is that \nasset ﬁ les and engine code tend to become bulky. When data format changes \n",
      "content_length": 2815,
      "extraction_method": "Direct"
    },
    {
      "page_number": 305,
      "chapter": null,
      "content": "283 \nare relatively rare, it may be bett er to just bite the bullet and reprocess all the \nﬁ les when format changes do occur.\nEvery asset conditioning pipeline requires a set of rules that describe the \ninterdependencies between the assets, and some kind of build tool that can \nuse this information to ensure that the proper assets are built, in the proper \norder, when a source asset is modiﬁ ed. Some game teams roll their own build \nsystem. Others use an established tool, such as make. Whatever solution is \nselected, teams should treat their build dependency system with utmost care. \nIf you don’t, changes to sources assets may not trigger the proper assets to \nbe rebuilt. The result can be inconsistent game assets, which may lead to vi-\nsual anomalies or even engine crashes. In my personal experience, I’ve wit-\nnessed countness hours wasted in tracking down problems that could have \nbeen avoided had the asset interdependencies been properly speciﬁ ed and the \nbuild system implemented to use them reliably.\n6.2.2. Runtime Resource Management\n Let us turn our att ention now to how the assets in our resource database are \nloaded, managed, and unloaded within the engine at runtime.\n6.2.2.1. Responsibilities of the Runtime Resource Manager\nA game engine’s runtime resource manager takes on a wide range of responsi-\nbilities, all related to its primary mandate of loading resources into memory:\nz Ensures that only one copy of each unique resource exists in memory at \nany given time.\nz Manages the lifetime of each resource loads needed resources and un-\nloads resources that are no longer needed.\nz Handles loading of composite resources. A composite resource is a resource \ncomprised of other resources. For example, a 3D model is a composite \nresource that consists of a mesh, one or more materials, one or more \ntextures, and optionally a skeleton and multiple skeletal animations.\nz Maintains referential integrity . This includes internal referential integrity \n(cross-references within a single resource) and external referential integ-\nrity (cross-references between resources). For example, a model refers to \nits mesh and skeleton; a mesh refers to its materials, which in turn refer \nto texture resources; animations refer to a skeleton, which ultimately \nties them to one or more models. When loading a composite resource, \nthe resource manager must ensure that all necessary subresources are \nloaded, and it must patch in all of the cross-references properly.\nz Manages the memory usage of loaded resources and ensures that re-\nsources are stored in the appropriate place(s) in memory.\n6.2. The Resource Manager\n",
      "content_length": 2641,
      "extraction_method": "Direct"
    },
    {
      "page_number": 306,
      "chapter": null,
      "content": "284 \n6. Resources and the File System\nz Permits custom processing to be performed on a resource aft er it has been \nloaded, on a per-resource-type basis. This process is sometimes known \nas logging in or load-initializing the resource.\nz Usually (but not always) provides a single uniﬁ ed interface through \nwhich a wide variety of resource types can be managed. Ideally a re-\nsource manager is also easily extensible, so that it can handle new types \nof resources as they are needed by the game development team.\nz Handles streaming (i.e., asynchronous resource loading), if the engine \nsupports this feature.\n6.2.2.2. Resource File and Directory Organization\n In some game engines (typically PC engines), each individual resource is \nmanaged in a separate “loose” ﬁ le on-disk. These ﬁ les are typically con-\ntained within a tree of directories whose internal organization is designed \nprimarily for the convenience of the people creating the assets; the engine \ntypically doesn’t care where resource ﬁ les are located within the resource \ntree. Here’s a typical resource directory tree for a hypothetical game called \nSpace Evaders:\nSpaceEvaders\nRoot directory for entire game.\n  Resources\nRoot of all resources.\n    Characters\nNon-player character models and animations.\n      Pirate\nModels and animations for pirates.\n      Marine\nModels and animations for marines.\n      ...\n    Player\nPlayer character models and animations.\n    Weapons\nModels and animations for weapons.\n      Pistol\nModels and animations for the pistol.\n      Rifle\nModels and animations for the riﬂ e.\n      BFG\nModels and animations for the big... uh… gun.\n      ...\n    Levels\nBackground geometry and level layouts.\n      Level1\nFirst level’s resources.\n      Level2\nSecond level’s resources.\n      ...\n    Objects\nMiscellaneous 3D objects.\n      Crate\nThe ubiquitous breakable crate.\n      Barrel\nThe ubiquitous exploding barrel.\nOther engines package multiple resources together in a single ﬁ le, such as \na ZIP archive, or some other composite ﬁ le (perhaps of a proprietary format). \n",
      "content_length": 2067,
      "extraction_method": "Direct"
    },
    {
      "page_number": 307,
      "chapter": null,
      "content": "285 \nThe primary beneﬁ t of this approach is improved load times. When loading \ndata from ﬁ les, the three biggest costs are seek times (i.e., moving the read head \nto the correct place on the physical media), the time required to open each \nindividual ﬁ le, and the time to read the data from the ﬁ le into memory. Of \nthese, the seek times and ﬁ le-open times can be non-trivial on many operating \nsystems. When a single large ﬁ le is used, all of these costs are minimized. A \nsingle ﬁ le can be organized sequentially on the disk, reducing seek times to \na minimum. And with only one ﬁ le to open, the cost of opening individual \nresource ﬁ les is eliminated.\nThe Ogre3D rendering engine’s resource manager permits resources to \nexist as loose ﬁ les on disk, or as virtual ﬁ les within a large ZIP archive. The \nprimary beneﬁ ts of the ZIP format are the following:\n \n1. It is an open format. The zlib and zziplib libraries used to read and \nwrite ZIP archives are freely available. The zlib SDK is totally free (see \nhtt p://www.zlib.net), while the zziplib SDK falls under the Lesser Gnu \nPublic License (LGPL) (see htt p://zziplib.sourceforge.net).\n \n2. The virtual ﬁ les within a ZIP archive “remember” their relative paths. This \nmeans that a ZIP archive “looks like” a raw ﬁ le system for most in-\ntents and purposes. The Ogre resource manager identiﬁ es all resources \nuniquely via strings that appear to be ﬁ le system paths. However, these \npaths sometimes identify virtual ﬁ les within a ZIP archive instead of \nloose ﬁ les on disk, and a game programmer needn’t be aware of the dif-\nference in most situations.\n \n3. ZIP archives may be compressed. This reduces the amount of disk space \noccupied by resources. But, more importantly, it again speeds up load \ntimes, as less data need be loaded into memory from the ﬁ xed disk. This \nis especially helpful when reading data from a DVD-ROM or Blu-ray \ndisk, as the data transfer rates of these devices are much slower than a \nhard disk drive. Hence the cost of decompressing the data aft er it has \nbeen loaded into memory is oft en more than oﬀ set by the time saved in \nloading less data from the device.\n \n4. ZIP archives are modular. Resources can be grouped together into a ZIP \nﬁ le and managed as a unit. One particularly elegant application of this \nidea is in product localization. All of the assets that need to be local-\nized (such as audio clips containing dialogue and textures that contain \nwords or region-speciﬁ c symbols) can be placed in a single ZIP ﬁ le, and \nthen diﬀ erent versions of this ZIP ﬁ le can be generated, one for each \nlanguage or region. To run the game for a particular region, the engine \nsimply loads the corresponding version of the ZIP archive.\n6.2. The Resource Manager\n",
      "content_length": 2773,
      "extraction_method": "Direct"
    },
    {
      "page_number": 308,
      "chapter": null,
      "content": "286 \n6. Resources and the File System\nUnreal Engine 3 takes a similar approach, with a few important diﬀ er-\nences. In Unreal, all resources must be contained within large composite \nﬁ les known as packages (a.k.a. “pak ﬁ les”) . No loose disk ﬁ les are permitt ed. \nThe format of a package ﬁ le is proprietary. The Unreal Engine’s game editor, \nUnrealEd, allows developers to create and manage packages and the resourc-\nes they contain.\n6.2.2.3. Resource File Formats\n Each type of resource ﬁ le potentially has a diﬀ erent format. For example, a \nmesh ﬁ le is always stored in a diﬀ erent format than that of a texture bitmap. \nSome kinds of assets are stored in standardized, open formats. For example, \ntextures are typically stored as Targa ﬁ les (TGA), Portable Network Graph-\nics ﬁ les (PNG), Tagged Image File Format ﬁ les (TIFF), Joint Photographic Ex-\nperts Group ﬁ les (JPEG), or Windows Bitmap ﬁ les (BMP)—or in a standard-\nized compressed format such as DirectX’s S3 Texture Compression family of \nformats (S3TC, also known as DXTn or DXTC). Likewise, 3D mesh data is \noft en exported out of a modeling tool like Maya or Lightwave into a stan-\ndardized format such as OBJ or COLLADA for consumption by the game \nengine.\nSometimes a single ﬁ le format can be used to house many diﬀ erent types \nof assets. For example, the Granny SDK by Rad Game Tools (htt p://www.rad-\ngametools.com) implements a ﬂ exible open ﬁ le format that can be used to \nstore 3D mesh data, skeletal hierarchies, and skeletal animation data. (In fact \nthe Granny ﬁ le format can be easily repurposed to store virtually any kind of \ndata imaginable.)\nMany game engine programmers roll their own ﬁ le formats for various \nreasons. This might be necessary if no standardized format provides all of \nthe information needed by the engine. Also, many game engines endeavor to \ndo as much oﬀ -line processing as possible in order to minimize the amount \nof time needed to load and process resource data at runtime. If the data \nneeds to conform to a particular layout in memory, for example, a raw binary \nformat might be chosen so that the data can be laid out by an oﬀ -line tool \n(rather than att empting to format it at runtime aft er the resource has been \nloaded).\n6.2.2.4. Resource GUIDs\n Every resource in a game must have some kind of globally unique identiﬁ er \n(GUID). The most common choice of GUID is the resource’s ﬁ le system path \n(stored either as a string or a 32-bit hash). This kind of GUID is intuitive, be-\ncause it clearly maps each resource to a physical ﬁ le on-disk. And it’s guar-\n",
      "content_length": 2587,
      "extraction_method": "Direct"
    },
    {
      "page_number": 309,
      "chapter": null,
      "content": "287 \nanteed to be unique across the entire game, because the operating system al-\nready guarantees that no two ﬁ les will have the same path.\nHowever, a ﬁ le system path is by no means the only choice for a resource \nGUID. Some engines use a less-intuitive type of GUID, such as a 128-bit hash \ncode, perhaps assigned by a tool that guarantees uniqueness. In other engines, \nusing a ﬁ le system path as a resource identiﬁ er is infeasible. For example, \nUnreal Engine 3 stores many resources in a single large ﬁ le known as a pack-\nage, so the path to the package ﬁ le does not uniquely identify any one re-\nsource. To overcome this problem, an Unreal package ﬁ le is organized into \na folder hierarchy containing individual resources. Unreal gives each indi-\nvidual resource within a package a unique name which looks much like a ﬁ le \nsystem path. So in Unreal, a resource GUID is formed by concatenating the \n(unique) name of the package ﬁ le with the in-package path of the resource \nin question. For example, the Gears of War resource GUID Locust_Boomer.\nPhysicalMaterials.  LocustBoomerLeather identiﬁ es a material called \nLocustBoomerLeather within the PhysicalMaterials folder of the \nLocust_Boomer package ﬁ le.\n6.2.2.5. The Resource Registry\nIn order to ensure that only one copy of each unique resource is loaded into \nmemory at any given time, most resource managers maintain some kind of \nregistry of loaded resources. The simplest implementation is a dictionary—i.e., \na collection of key-value pairs . The keys contain the unique ids of the resources, \nwhile the values are typically pointers to the resources in memory.\nWhenever a resource is loaded into memory, an entry for it is added to the \nresource registry dictionary, using its GUID as the key. Whenever a resource is \nunloaded, its registry entry is removed. When a resource is requested by the \ngame, the resource manager looks up the resource by its GUID within the re-\nsource registry. If the resource can be found, a pointer to it is simply returned. \nIf the resource cannot be found, it can either be loaded automatically or a \nfailure code can be returned.\nAt ﬁ rst blush, it might seem most intuitive to automatically load a re-\nquested resource if it cannot be found in the resource registry. And in fact, \nsome game engines do this. However, there are some serious problems with \nthis approach. Loading a resource is a slow operation, because it involves lo-\ncating and opening a ﬁ le on disk, reading a potentially large amount of data \ninto memory (from a potentially slow device like a DVD-ROM drive), and \nalso possibly performing post-load initialization of the resource data once it \nhas been loaded. If the request comes during active gameplay, the time it takes \nto load the resource might cause a very noticeable hitch in the game’s frame \n6.2. The Resource Manager\n",
      "content_length": 2860,
      "extraction_method": "Direct"
    },
    {
      "page_number": 310,
      "chapter": null,
      "content": "288 \n6. Resources and the File System\nrate, or even a multi-second freeze. For this reason, engines tend to take one of \ntwo alternative approaches:\nResource loading might be disallowed completely during active game-\n1. \nplay. In this model, all of the resources for a game level are loaded en \nmasse just prior to gameplay, usually while the player watches a loading \nscreen or progress bar of some kind.\nResource loading might be done \n2. \nasynchronously (i.e., the data might be \nstreamed). In this model, while the player is engaged in level A, the re-\nsources for level B are being loaded in the background. This approach \nis preferable because it provides the player with a load-screen-free play \nexperience. However, it is considerably more diﬃ  cult to implement.\n6.2.2.6. Resource Lifetime\nThe lifetime of a resource is deﬁ ned as the time period between when it is ﬁ rst \nloaded into memory and when its memory is reclaimed for other purposes. \nOne of the resource manager’s jobs is to manage resource lifetimes—either \nautomatically, or by providing the necessary API functions to the game, so it \ncan manage resource lifetimes manually.\nEach resource has its own lifetime requirements:\nz Some resources must be loaded when the game ﬁ rst starts up and must \nstay resident in memory for the entire duration of the game. That is, \ntheir lifetimes are eﬀ ectively inﬁ nite. These are sometimes called load-\nand-stay-resident (LSR) resources. Typical examples include the player \ncharacter’s mesh, materials, textures and core animations, textures and \nfonts used on the heads-up display (HUD), and the resources for all of \nthe standard-issue weapons used throughout the game. Any resource \nthat is visible or audible to the player throughout the entire game (and \ncannot be loaded on the ﬂ y when needed) should be treated as an LSR \nresource.\nz Other resources have a lifetime that matches that of a particular game \nlevel. These resources must be in memory by the time the level is ﬁ rst \nseen by the player and can be dumped once the player has permanently \nleft  the level.\nz Some resources might have a lifetime that is shorter than the duration of \nthe level in which they are found. For example, the animations and au-\ndio clips that make up an in-game cut-scene (a mini-movie that advances \nthe story or provides the player with important information) might be \nloaded in advance of the player seeing the cut-scene and then dumped \nonce the cut-scene has played.\n",
      "content_length": 2481,
      "extraction_method": "Direct"
    },
    {
      "page_number": 311,
      "chapter": null,
      "content": "289 \nz Some resources like background music, ambient sound eﬀ ects, or full-\nscreen movies are streamed “live” as they play. The lifetime of this kind \nof resource is diﬃ  cult to deﬁ ne, because each byte only persists in mem-\nory for a tiny fraction of a second, but the entire piece of music sounds \nlike it lasts for a long period of time. Such assets are typically loaded in \nchunks of a size that matches the underlying hardware’s requirements. \nFor example, a music track might be read in 4 kB chunks, because that \nmight be the buﬀ er size used by the low-level sound system. Only two \nchunks are ever present in memory at any given moment—the chunk \nthat is currently playing and the chunk immediately following it that is \nbeing loaded into memory.\nThe question of when to load a resource is usually answered quite easily, \nbased on knowledge of when the asset is ﬁ rst seen by the player. However, the \nquestion of when to unload a resource and reclaim its memory is not so eas-\nily answered. The problem is that many resources are shared across multiple \nlevels. We don’t want to unload a resource when level A is done, only to im-\nmediately reload it because level B needs the same resource.\nOne solution to this problem is to reference-count the resources. When-\never a new game level needs to be loaded, the list of all resources used by that \nlevel is traversed, and the reference count for each resource is incremented \nby one (but they are not loaded yet). Next, we traverse the resources of any \nunneeded levels and decrement their reference counts by one; any resource \nwhose reference count drops to zero is unloaded. Finally, we run through the \nlist of all resources whose reference count just went from zero to one and load \nthose assets into memory.\nFor example, imagine that level 1 uses resources A, B, and C, and that \nlevel 2 uses resources B, C, D, and E. (B and C are shared between both levels.) \nTable 6.2 shows the reference counts of these ﬁ ve resources as the player plays \nthrough levels 1 and 2. In this table, reference counts are shown in boldface \ntype to indicate that the corresponding resource actually exists in memory, \nwhile a grey background indicates that the resource is not in memory. A refer-\nence count in parentheses indicates that the corresponding resource data is \nbeing loaded or unloaded.\n6.2.2.7. Memory Management for Resources\nResource management is closely related to memory management , because we \nmust inevitably decide where the resources should end up in memory once \nthey have been loaded. The destination of every resource is not always the \nsame. For one thing, certain types of resources must reside in video RAM. \nTypical examples include textures, vertex buﬀ ers, index buﬀ ers, and shader \n6.2. The Resource Manager\n",
      "content_length": 2792,
      "extraction_method": "Direct"
    },
    {
      "page_number": 312,
      "chapter": null,
      "content": "290 \n6. Resources and the File System\ncode. Most other resources can reside in main RAM, but diﬀ erent kinds of \nresources might need to reside within diﬀ erent address ranges. For example, a \nresource that is loaded and stays resident for the entire game (LSR resources) \nmight be loaded into one region of memory, while resources that are loaded \nand unloaded frequently might go somewhere else.\nThe design of a game engine’s memory allocation subsystem is usually \nclosely tied to that of its resource manager. Sometimes we will design the re-\nsource manager to take best advantage of the types of memory allocators we \nhave available; or vice-versa, we may design our memory allocators to suit the \nneeds of the resource manager.\nAs we saw in Section 5.2.1.4, one of the primary problems facing any re-\nsource management system is the need to avoid fragmenting memory as re-\nsources are loaded and unloaded. We’ll discuss a few of the more-common \nsolutions to this problem below.\nHeap-Based Resource Allocation\n One approach is to simply ignore memory fragmentation issues and use a \ngeneral-purpose heap allocator to allocate your resources (like the one imple-\nmented by malloc() in C, or the global new operator in C++). This works best \nif your game is only intended to run on personal computers, on operating \nsystems that support advanced virtual memory allocation. On such a system, \nphysical memory will become fragmented, but the operating system’s abil-\nity to map non-contiguous pages of physical RAM into a contiguous virtual \nmemory space helps to mitigate some of the eﬀ ects of fragmentation.\nIf your game is running on a console with limited physical RAM and only \na rudimentary virtual memory manager (or none whatsoever), then fragmen-\ntation will become a problem. In this case, one alternative is to defragment \nyour memory periodically. We saw how to do this in Section 5.2.2.2.\nEvent\nA\nB\nC\nD\nE\nInitial state\n0\n0\n0\n0\n0\nLevel 1 counts incremented\n1\n1\n1\n0\n0\nLevel 1 loads\n(1)\n(1)\n(1)\n0\n0\nLevel 1 plays\n1\n1\n1\n0\n0\nLevel 2 counts incremented\n1\n2\n2\n1\n1\nLevel 1 counts decremented\n0\n1\n1\n1\n1\nLevel 1 unloads, level 2 loads\n(0)\n1\n1\n(1)\n(1)\nLevel 2 plays\n0\n1\n1\n1\n1\nTable 6.2.  Resource usage as two levels load and unload.\n",
      "content_length": 2239,
      "extraction_method": "Direct"
    },
    {
      "page_number": 313,
      "chapter": null,
      "content": "291 \nStack-Based Resource Allocation\n A stack allocator does not suﬀ er from fragmentation problems, because mem-\nory is allocated contiguously and freed in an order opposite to that in which it \nwas allocated. A stack allocator can be used to load resources if the following \ntwo conditions are met:\nz The game is linear and level-centric (i.e., the player watches a loading \nscreen, then plays a level, then watches another loading screen, then \nplays another level).\nz Each level ﬁ ts into memory in its entirety.\nPresuming that these requirements are satisﬁ ed, we can use a stack alloca-\ntor to load resources as follows: When the game ﬁ rst starts up, the load-and-\nstay-resident (LSR) resources are allocated ﬁ rst. The top of the stack is then \nmarked, so that we can free back to this position later. To load a level, we sim-\nply allocate its resources on the top of the stack. When the level is complete, \nwe can simply set the stack top back to the marker we took earlier, thereby \nfreeing all of the level’s resources in one fell swoop without disturbing the LSR \nresources. This process can be repeated for any number of levels, without ever \nfragmenting memory. Figure 6.3 illustrates how this is accomplished.\n6.2. The Resource Manager\nFigure 6.3.  Loading resources using a stack allocator.\n",
      "content_length": 1307,
      "extraction_method": "Direct"
    },
    {
      "page_number": 314,
      "chapter": null,
      "content": "292 \n6. Resources and the File System\nA double-ended stack allocator can be used to augment this approach. \nTwo stacks are deﬁ ned within a single large memory block. One grows up \nfrom the bott om of the memory area, while the other grows down from the \ntop. As long as the two stacks never overlap, the stacks can trade memory re-\nsources back and forth naturally—something that wouldn’t be possible if each \nstack resided in its own ﬁ xed-size block.\nOn Hydro Thunder, Midway used a double-ended stack allocator. The low-\ner stack was used for persistent data loads, while the upper was used for tem-\nporary allocations that were freed every frame. Another way a double-ended \nstack allocator can be used is to ping-pong level loads. Such an approach was \nused at Bionic Games Inc. for one of their projects. The basic idea is to load a \ncompressed version of level B into the upper stack, while the currently-active \nlevel A resides (in uncompressed form) in the lower stack. To switch from \nlevel A to level B, we simply free level A’s resources (by clearing the lower \nstack) and then decompress level B from the upper stack into the lower stack. \nDecompression is generally much faster than loading data from disk, so this \napproach eﬀ ectively eliminates the load time that would otherwise be experi-\nenced by the player beween levels.\nPool-Based Resource Allocation\n Another resource allocation technique that is common in game engines that \nsupport streaming is to load resource data in equally-sized chunks. Because \nthe chunks are all the same size, they can be allocated using a pool allocator (see \nSection 5.2.1.2). When resources are later unloaded, the chunks can be freed \nwithout causing fragmentation.\nOf course, a chunk-based allocation approach requires that all resource \ndata be laid out in a manner that permits division into equally-sized chunks. \nWe cannot simply load an arbitrary resource ﬁ le in chunks, because the ﬁ le \nmight contain a contiguous data structure like an array or a very large struct\nthat is larger than a single chunk. For example, if the chunks that contain an \narray are not arranged sequentially in RAM, the continuity of the array will \nbe lost, and array indexing will cease to function properly. This means that all \nresource data must be designed with “chunkiness” in mind. Large contigu-\nous data structures must be avoided in favor of data structures that are either \nsmall enough to ﬁ t within a single chunk or do not require contiguous RAM \nto function properly (e.g., linked lists).\nEach chunk in the pool is typically associated with a particular game lev-\nel. (One simple way to do this is to give each level a linked list of its chunks.) \nThis allows the engine to manage the lifetimes of each chunk appropriately, \neven when multiple levels with diﬀ erent life spans are in memory concur-\n",
      "content_length": 2854,
      "extraction_method": "Direct"
    },
    {
      "page_number": 315,
      "chapter": null,
      "content": "293 \nrently. For example, when level A is loaded, it might allocate and make use of \nN chunks. Later, level B might allocate an additional M chunks. When level \nA is eventually unloaded, its N chunks are returned to the free pool. If level \nB is still active, its M chunks need to remain in memory. By associating each \nchunk with a speciﬁ c level, the lifetimes of the chunks can be managed easily \nand eﬃ  ciently. This is illustrated in Figure 6.4.\nOne big trade-oﬀ  inherent in a “chunky”  resource allocation scheme is \nwasted space. Unless a resource ﬁ le’s size is an exact multiple of the chunk \nsize, the last chunk in a ﬁ le will not be fully utilized (see Figure 6.5). Choos-\ning a smaller chunk size can help to mitigate this problem, but the smaller the \nchunks, the more onerous the restrictions on the layout of the resource data. \n(As an extreme example, if a chunk size of one byte were selected, then no \ndata structure could be larger than a single byte—clearly an untenable situ-\nation.) A typical chunk size is on the order of a few kilobytes. For example \nat Naughty Dog, we use a chunky resource allocator as part of our resource \nstreaming system, and our chunks are 512 kB in size. You may also want to \nconsider selecting a chunk size that is a multiple of the operating system’s I/O \nbuﬀ er size to maximize eﬃ  ciency when loading individual chunks.\n6.2. The Resource Manager\nFile A\nChunk 1\nFile A\nChunk 2\nFile A\nChunk 3\nFile B\nChunk 1\nFile B\nChunk 2\nFile C\nChunk 1\nFile C\nChunk 2\nFile C\nChunk 3\nFile C\nChunk 4\nFile D\nChunk 1\nFile D\nChunk 2\nFile D\nChunk 3\nFile E\nChunk 1\nFile E\nChunk 2\nFile E\nChunk 3\nFile E\nChunk 4\nFile E\nChunk 5\nFile E\nChunk 6\nLevel X\n(files A, D)\nLevel Y\n(files B, C, E)\nFigure 6.4.  Chunky allocation of resources for levels A and B.\nFile size:\n1638 kB\nUnused:\n410 kB\nChunk 4\nChunk 1\nChunk 2\nChunk 3\nChunk size:\n512 kB each\nFigure 6.5.  The last chunk of a resource ﬁ le is often not fully utilized.\n",
      "content_length": 1949,
      "extraction_method": "Direct"
    },
    {
      "page_number": 316,
      "chapter": null,
      "content": "294 \n6. Resources and the File System\nResource Chunk Allocators\n One way to limit the eﬀ ects of wasted chunk memory is to set up a special \nmemory allocator that can utilize the unused portions of chunks. As far as I’m \naware, there is no standardized name for this kind of allocator, but we will call \nit a resource chunk allocator for lack of a bett er name.\nA resource chunk allocator is not particularly diﬃ  cult to implement. We \nneed only maintain a linked list of all chunks that contain unused memory, \nalong with the locations and sizes of each free block. We can then allocate \nfrom these free blocks in any way we see ﬁ t. For example, we might manage \nthe linked list of free blocks using a general-purpose heap allocator. Or we \nmight map a small stack allocator onto each free block; whenever a request for \nmemory comes in, we could then scan the free blocks for one whose stack has \nenough free RAM, and then use that stack to satisfy the request.\nUnfortunately, there’s a rather grotesque-looking ﬂ y in our ointment here. \nIf we allocate memory in the unused regions of our resource chunks, what hap-\npens when those chunks are freed? We cannot free part of a chunk—it’s an all \nor nothing proposition. So any memory we allocate within an unused portion \nof a resource chunk will magically disappear when that resource is unloaded.\nA simple solution to this problem is to only use our free-chunk alloca-\ntor for memory requests whose lifetimes match the lifetime of the level with \nwhich a particular chunk is associated. In other words, we should only al-\nlocate memory out of level A’s chunks for data that is associated exclusively \nwith level A and only allocate from B’s chunks memory that is used exclu-\nsively by level B. This requires our resource chunk allocator to manage each \nlevel’s chunks separately. And it requires the users of the chunk allocator to \nspecify which level they are allocating for, so that the correct linked list of free \nblocks can be used to satisfy the request.\nThankfully, most game engines need to allocate memory dynamically \nwhen loading resources, over and above the memory required for the resource \nﬁ les themselves. So a resource chunk allocator can be a fruitful way to reclaim \nchunk memory that would otherwise have been wasted.\nSectioned Resource Files\n Another useful idea that is related to “chunky” resource ﬁ les is the concept \nof ﬁ le sections. A typical resource ﬁ le might contain between one and four sec-\ntions, each of which is divided into one or more chunks for the purposes of \npool allocation as described above. One section might contain data that is des-\ntined for main RAM, while another section might contain video RAM data. \nAnother section could contain temporary data that is needed during the load-\ning process but is discarded once the resource has been completely loaded. Yet \n",
      "content_length": 2869,
      "extraction_method": "Direct"
    },
    {
      "page_number": 317,
      "chapter": null,
      "content": "295 \nanother section might contain debugging information. This debug data could \nbe loaded when running the game in debug mode, but not loaded at all in \nthe ﬁ nal production build of the game. The Granny SDK’s ﬁ le system (htt p://\nwww.radgametools.com) is an excellent example of how to implement ﬁ le \nsectioning in a simple and ﬂ exible manner.\n6.2.2.8. Composite Resources and Referential Integrity\n Usually a game’s resource database consists of multiple resource ﬁ les, each ﬁ le \ncontaining one or more data objects. These data objects can refer to and depend \nupon one another in arbitrary ways. For example, a mesh data structure might \ncontain a reference to its material, which in turn contains a list of references to \ntextures. Usually cross-references imply dependency (i.e., if resource A refers \nto resource B, then both A and B must be in memory in order for the resources \nto be functional in the game.) In general, a game’s resource database can be \nrepresented by a directed graph of interdependent data objects.\nCross-references between data objects can be internal (a reference between \ntwo objects within a single ﬁ le) or external (a reference to an object in a dif-\nferent ﬁ le). This distinction is important because internal and external cross-\nreferences are oft en implemented diﬀ erently. When visualizing a game’s re-\nsource database, we can draw dott ed lines surrounding individual resource \nﬁ les to make the internal/external distinction clear—any edge of the graph \nthat crosses a dott ed line ﬁ le boundary is an external reference, while edges \nthat do not cross ﬁ le boundaries are internal. This is illustrated in Fiure 6.6.\n6.2. The Resource Manager\nFigure 6.6.  Example of a resource database dependency graph.\n",
      "content_length": 1754,
      "extraction_method": "Direct"
    },
    {
      "page_number": 318,
      "chapter": null,
      "content": "296 \n6. Resources and the File System\nWe sometimes use the term composite resource to describe a self-suﬃ  cient \ncluster of interdependent resources. For example, a model is a composite re-\nsource consisting of one or more triangle meshes, an optional skeleton, and an \noptional collection of animations. Each mesh is mapped with a material, and \neach material refers to one or more textures. To fully load a composite resource \nlike a 3D model into memory, all of its dependent resources must be loaded \nas well.\n6.2.2.9. Handling Cross-References between Resources\nOne of the more-challenging aspects of implementing a resource manager is \nmanaging the cross-references between resource objects and guaranteeing \nthat referential integrity is maintained. To understand how a resource man-\nager accomplishes this, let’s look at how cross-references are represented in \nmemory, and how they are represented on-disk.\nIn C++, a cross-reference between two data objects is usually implemented \nvia a pointer or a reference. For example, a mesh might contain the data mem-\nber Material* m_pMaterial (a pointer) or Material& m_material (a ref-\nerence) in order to refer to its material. However, pointers are just memory \naddresses—they lose their meaning when taken out of the context of the run-\nning application. In fact, memory addresses can and do change even between \nsubsequent runs of the same application. Clearly when storing data to a disk \nﬁ le, we cannot use pointers to describe inter-object dependencies.\nGUIDs As Cross-References\nOne good approach is to store each cross-reference as a string or hash code \ncontaining the unique id of the referenced object. This implies that every re-\nsource object that might be cross-referenced must have a globally unique identi-\nﬁ er or GUID.\nTo make this kind of cross-reference work, the runtime resource manager \nmaintains a global resource look-up table. Whenever a resource object is load-\ned into memory, a pointer to that object is stored in the table with its GUID as \nthe look-up key. Aft er all resource objects have been loaded into memory and \ntheir entries added to the table, we can make a pass over all of the objects and \nconvert all of their cross-references into pointers, by looking up the address \nof each cross-referenced object in the global resource look-up table via that \nobject’s GUID.\nPointer Fix-Up Tables\nAnother approach that is oft en used when storing data objects into a binary \nﬁ le is to convert the pointers into ﬁ le oﬀ sets. Consider a group of C structs or \nC++ objects that cross-reference each other via pointers. To store this group \n",
      "content_length": 2626,
      "extraction_method": "Direct"
    },
    {
      "page_number": 319,
      "chapter": null,
      "content": "297 \nof objects into a binary ﬁ le, we need to visit each object once (and only once) \nin an arbitrary order and write each object’s memory image into the ﬁ le se-\nquentially. This has the eﬀ ect of serializing the objects into a contiguous image \nwithin the ﬁ le, even when their memory images are not contiguous in RAM. \nThis is shown in Figure 6.7.\nBecause the objects’ memory images are now contiguous within the ﬁ le, \nwe can determine the oﬀ set of each object’s image relative to the beginning of \nthe ﬁ le. During the process of writing the binary ﬁ le image, we locate every \npointer within every data object, convert each pointer into an oﬀ set, and store \nthose oﬀ sets into the ﬁ le in place of the pointers. We can simply overwrite the \npointers with their oﬀ sets, because the oﬀ sets never require more bits to store \nthan the original pointers. In eﬀ ect, an oﬀ set is the binary ﬁ le equivalent of a \npointer in memory. (Do be aware of the diﬀ erences between your development \nplatform and your target platform. If you write out a memory image on a 64-\nbit Windows machine, its pointers will all be 64 bits wide and the resulting ﬁ le \nwon’t be compatible with a 32-bit console.)\nOf course, we’ll need to convert the oﬀ sets back into pointers when the \nﬁ le is loaded into memory some time later. Such conversions are known as \npointer ﬁ x-ups . When the ﬁ le’s binary image is loaded, the objects contained \nin the image retain their contiguous layout. So it is trivial to convert an oﬀ set \ninto a pointer. We merely add the oﬀ set to the address of the ﬁ le image as a \nwhole. This is demonstrated by the code snippet below, and illustrated in \nFigure 6.8.\n6.2. The Resource Manager\nAddresses:\nOffsets:\nRAM\nBinary File\nObject 1\nObject 2\nObject 3\nObject 4\n0x0\n0x240\n0x4A0\n0x7F0\nObject 1\nObject 4\nObject 2\nObject 3\n0x2A080\n0x2D750\n0x2F110\n0x32EE0\nFigure 6.7. In-memory object images become contiguous when saved into a binary ﬁ le.\n",
      "content_length": 1952,
      "extraction_method": "Direct"
    },
    {
      "page_number": 320,
      "chapter": null,
      "content": "298 \n6. Resources and the File System\nU8* ConvertOffsetToPointer(U32 objectOffset,\n                           U8* pAddressOfFileImage)\n{\n \nU8* pObject = pAddressOfFileImage + objectOffset;\n \nreturn pObject;\n}\nThe problem we encounter when trying to convert pointers into oﬀ sets, \nand vice-versa, is how to ﬁ nd all of the pointers that require conversion. This \nproblem is usually solved at the time the binary ﬁ le is writt en. The code that \nwrites out the images of the data objects has knowledge of the data types and \nclasses being writt en, so it has knowledge of the locations of all the pointers \nAddresses:\nOffsets:\nRAM\nBinary File\nObject 1\nObject 2\nObject 3\nObject 4\n0x0\n0x240\n0x4A0\n0x7F0\nObject 1\nObject 4\nObject 2\nObject 3\n0x2A080\n0x2D750\n0x2F110\n0x32EE0\n0x32EE0\n0x2F110\n0x2A080\n0x4A0\n0x240\n0x0\nPointers converted \nto offsets; locations \nof pointers stored in \nfix-up table.\nFix-Up Table\n0x200\n0x340\n0x810\nPointers to various \nobjects are present.\n3 pointers\nFigure 6.9.  A pointer ﬁ x-up table.\nAddresses:\nOffsets:\nRAM\nBinary File\nObject 1\nObject 2\nObject 3\nObject 4\n0x0\n0x240\n0x4A0\n0x7F0\nObject 1\nObject 4\nObject 2\nObject 3\n0x30100\n0x30340\n0x305A0\n0x308 F0\nFigure 6.8.  Contiguous resource ﬁ le image, after it has been loaded into RAM.\n",
      "content_length": 1252,
      "extraction_method": "Direct"
    },
    {
      "page_number": 321,
      "chapter": null,
      "content": "299 \nwithin each object. The locations of the pointers are stored into a simple table \nknown as a pointer ﬁ x-up table. This table is writt en into the binary ﬁ le along \nwith the binary images of all the objects. Later, when the ﬁ le is loaded into \nRAM again, the table can be consulted in order to ﬁ nd and ﬁ x up every point-\ner. The table itself is just a list of oﬀ sets within the ﬁ le—each oﬀ set represents \na single pointer that requires ﬁ xing up. This is illustrated in Figure 6.9.\nStoring C++ Objects as Binary Images: Constructors\n One important step that is easy to overlook when loading C++ objects from a \nbinary ﬁ le is to ensure that the objects’ constructors are called. For example, \nif we load a binary image containing three objects—an instance of class A, an \ninstance of class B, and an instance of class C—then we must make sure that \nthe correct constructor is called on each of these three objects.\nThere are two common solutions to this problem. First, you can simply \ndecide not to support C++ objects in your binary ﬁ les at all. In other words, \nrestrict yourself to plain old data structures (PODS)—i.e., C structs and C++ \nstructs and classes that contain no virtual functions and trivial do-nothing con-\nstructors (See htt p://en.wikipedia.org/wiki/Plain_Old_Data_Structures for a \nmore complete discussion of PODS.)\nSecond, you can save oﬀ  a table containing the oﬀ sets of all non-PODS \nobjects in your binary image along with some indication of which class each \nobject is an instance of. Then, once the binary image has been loaded, you can \niterate through this table, visit each object, and call the appropriate construc-\ntor using placement new syntax (i.e., calling the constructor on a preallocated \nblock of memory). For example, given the oﬀ set to an object within the binary \nimage, we might write:\nvoid* pObject = ConvertOffsetToPointer(objectOffset);\n::new(pObject) ClassName; // placement-new syntax\nwhere ClassName is the class of which the object is an instance.\nHandling External References\nThe two approaches described above work very well when applied to resourc-\nes in which all of the cross-references are internal—i.e., they only reference \nobjects within a single resource ﬁ le. In this simple case, you can load the bi-\nnary image into memory and then apply the pointer ﬁ x-ups to resolve all the \ncross-references. But when cross-references reach out into other resource ﬁ les, \na slightly augmented approach is required.\nTo successfully represent an external cross-reference, we must specify not \nonly the oﬀ set or GUID of the data object in question, but also the path to the \nresource ﬁ le in which the referenced object resides.\n6.2. The Resource Manager\n",
      "content_length": 2723,
      "extraction_method": "Direct"
    },
    {
      "page_number": 322,
      "chapter": null,
      "content": "300 \n6. Resources and the File System\nThe key to loading a multi-ﬁ le composite resource is to load all of the \ninterdependent ﬁ les ﬁ rst. This can be done by loading one resource ﬁ le and \nthen scanning through its table of cross-references and loading any externally-\nreferenced ﬁ les that have not already been loaded. As we load each data object \ninto RAM, we can add the object’s address to the master look-up table . Once \nall of the interdependent ﬁ les have been loaded and all of the objects are pres-\nent in RAM, we can make a ﬁ nal pass to ﬁ x up all of the pointers using the \nmaster look-up table to convert GUIDs or ﬁ le oﬀ sets into real addresses.\n6.2.2.10. Post-Load Initialization\nIdeally, each and every resource would be completely prepared by our oﬀ -line \ntools, so that it is ready for use the moment it has been loaded into memory. \nPractically speaking, this is not always possible. Many types of resources re-\nquire at least some “massaging” aft er having been loaded, in order to prepare \nthem for use by the engine. In this book, I will use the term post-load initializa-\ntion to refer to any processing of resource data aft er it has been loaded. Other \nengines may use diﬀ erent terminology. (For example, at Naughty Dog we call \nthis logging in a resource.) Most resource managers also support some kind of \ntear-down step prior to a resource’s memory being freed. (At Naughty Dog, \nwe call this logging out a resource.)\nPost-load initialization generally comes in one of two varieties:\nz In some cases, post-load initialization is an unavoidable step. For ex-\nample, the vertices and indices that describe a 3D mesh are loaded into \nmain RAM, but they almost always need to be transferred into video \nRAM. This can only be accomplished at runtime, by creating a Direct X \nvertex buﬀ er or index buﬀ er, locking it, copying or reading the data into \nthe buﬀ er, and then unlocking it.\nz In other cases, the processing done during post-load initialization is \navoidable (i.e., could be moved into the tools), but is done for conve-\nnience or expedience. For example, a programmer might want to add the \ncalculation of accurate arc lengths to our engine’s spline library. Rather \nthan spend the time to modify the tools to generate the arc length data, \nthe programmer might simply calculate it at runtime during post-load \ninitialization. Later, when the calculations are perfected, this code can \nbe moved into the tools, thereby avoiding the cost of doing the calcula-\ntions at runtime.\nClearly, each type of resource has its own unique requirements for post-\nload initialization and tear-down. So resource managers typically permit these \ntwo steps to be conﬁ gurable on a per-resource-type basis. In a non-object-ori-\n",
      "content_length": 2753,
      "extraction_method": "Direct"
    },
    {
      "page_number": 323,
      "chapter": null,
      "content": "301 \nented language like C, we can envision a look-up table that maps each type of \nresource to a pair of function pointers, one for post-load initialization and one \nfor tear-down. In an object-oriented language like C++, life is even easier—we \ncan make use of polymorphism to permit each class to handle post-load ini-\ntialization and tear-down in a unique way.\nIn C++, post-load initialization could be implemented as a special con-\nstructor, and tear-down could be done in the class’ destructor. However, there \nare some problems with using constructors and destructors for this purpose. \n(For example, constructors cannot be virtual in C++, so it would be diﬃ  cult \nfor a derived class to modify or augment the post-load initialization of its base \nclass.) Many developers prefer to defer post-load initialization and tear-down \nto plain old virtual functions. For example, we might choose to use a pair of \nvirtual functions named something sensible like Init() and Destroy().\nPost-load initialization is closely related to a resource’s memory allocation \nstrategy, because new data is oft en generated by the initialization routine. In \nsome cases, the data generated by the post-load initialization step augments the \ndata loaded from the ﬁ le. (For example, if we are calculating the arc lengths \nof the segments of a Catmull-Rom spline curve aft er it has been loaded, we \nwould probably want to allocate some additional memory in which to store \nthe results.) In other cases, the data generated during post-load initialization \nreplaces the loaded data. (For example, we might allow mesh data in an older \nout-of-date format to be loaded and then automatically converted into the lat-\nest format for backwards compatibility reasons.) In this case, the loaded data \nmay need to be discarded, either partially or in its entirety, aft er the post-load \nstep has generated the new data.\nThe Hydro Thunder engine had a simple but powerful way of handling \nthis. It would permit resources to be loaded in one of two ways: (a) directly \ninto its ﬁ nal resting place in memory, or (b) into a temporary area of memory. \nIn the latt er case, the post-load initialization routine was responsible for copy-\ning the ﬁ nalized data into its ultimate destination; the temporary copy of the \nresource would be discarded aft er post-load initialization was complete. This \nwas very useful for loading resource ﬁ les that contained both relevant and \nirrelevant data. The relevant data would be copied into its ﬁ nal destination \nin memory, while the irrelevant data would be discarded. For example, mesh \ndata in an out-of-date format could be loaded into temporary memory and \nthen converted into the latest format by the post-load initialization routine, \nwithout having to waste any memory keeping the old-format data kicking \naround.\n6.2. The Resource Manager\n",
      "content_length": 2860,
      "extraction_method": "Direct"
    },
    {
      "page_number": 324,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 325,
      "chapter": null,
      "content": "303\n7\nThe Game Loop and \nReal-Time Simulation\nG\names are real-time, dynamic, interactive computer simulations . As such, \ntime plays an incredibly important role in any electronic game. There are \nmany diﬀ erent kinds of time to deal with in a game engine—real time , game \ntime , the local timeline of an animation, the actual CPU cycles spent within \na particular function, and the list goes on. Every engine system might deﬁ ne \nand manipulate time diﬀ erently. We must have a solid understanding of all \nthe ways time can be used in a game. In this chapter, we’ll take a look at how \nreal-time, dynamic simulation soft ware works and explore the common ways \nin which time plays a role in such a simulation.\n7.1. \nThe Rendering Loop\nIn a graphical user interface (GUI), of the sort found on a Windows PC or a \nMacintosh, the majority of the screen’s contents are static. Only a small part \nof any one window is actively changing appearance at any given moment. \nBecause of this, graphical user interfaces have traditionally been drawn on-\nscreen via a technique known as rectangle invalidation , in which only the small \nportions of the screen whose contents have actually changed are re-drawn. \nOlder 2D video games used similar techniques to minimize the number of \npixels that needed to be drawn.\n",
      "content_length": 1304,
      "extraction_method": "Direct"
    },
    {
      "page_number": 326,
      "chapter": null,
      "content": "304 \n7. The Game Loop and Real-Time Simulation\nReal-time 3D computer graphics are implemented in an entirely diﬀ erent \nway. As the camera moves about in a 3D scene, the entire contents of the screen \nor window change continually, so the concept of invalid rectangles no longer \napplies. Instead, an illusion of motion and interactivity is produced in much \nthe same way that a movie produces it—by presenting the viewer with a se-\nries of still images in rapid succession.\nObviously, producing a rapid succession of still images on-screen requires \na loop. In a real-time rendering application, this is sometimes known as the \nrender loop . At its simplest, a rendering loop is structured as follows:\nwhile (!quit)\n{\n \n// Update the camera transform based on interactive   \n \n// inputs or by following a predefined path.\nupdateCamera();\n \n// Update positions, orientations and any other   \n \n \n// relevant visual state of any dynamic elements \n \n// in the scene.\nupdateSceneElements();\n \n// Render a still frame into an off-screen frame   \n \n \n// buffer known as the \"back buffer\".\nrenderScene();\n \n// Swap the back buffer with the front buffer, making  \n \n// the most-recently-rendered image visible \n \n// on-screen. (Or, in windowed mode, copy (blit) the   \n \n// back buffer’s contents to the front buffer.\nswapBuffers();\n}\n7.2. The Game Loop\n A game is composed of many interacting subsystems, including device I/O, \nrendering, animation, collision detection and resolution, optional rigid body \ndynamics simulation, multiplayer networking, audio, and the list goes on. \nMost game engine subsystems require periodic servicing while the game is \nrunning. However, the rate at which these subsystems need to be serviced var-\nies from subsystem to subsystem. Animation typically needs to be updated \nat a rate of 30 or 60 Hz, in synchronization with the rendering subsystem. \nHowever, a dynamics simulation may actually require more frequent updates \n",
      "content_length": 1952,
      "extraction_method": "Direct"
    },
    {
      "page_number": 327,
      "chapter": null,
      "content": "305 \n7.2. The Game Loop\n(e.g., 120 Hz). Higher-level systems, like AI , might only need to be serviced \nonce or twice per second, and they needn’t necessarily be synchronized with \nthe rendering loop at all.\nThere are a number of ways to implement the periodic updating of our \ngame engine subsystems. We’ll explore some of the possible architectures in a \nmoment. But for the time being, let’s stick with the simplest way to update our \nengine’s subsystems—using a single loop to update everything. Such a loop \nis oft en called the game loop, because it is the master loop that services every \nsubsystem in the engine.\n7.2.1. \nA Simple Example: Pong\nPong is a well-known genre of table tennis video games that got its start in \n1958, in the form of an analog computer game called Tennis for Two, created \nby William A. Higinbotham at the Brookhaven National Laboratory and dis-\nplayed on an oscilloscope. The genre is best known by its later incarnations on \ndigital computers—the Magnavox Oddysey game Table Tennis and the Atari \narcade game Pong.\nIn pong, a ball bounces back and forth between two movable vertical pad-\ndles and two ﬁ xed horizontal walls. The human players control the positions \nof the paddles via control wheels. (Modern re-implementations allow control \nvia a joystick, the keyboard, or some other human interface device.) If the ball \npasses by a paddle without striking it, the other team wins the point and the \nball is reset for a new round of play.\nThe following pseudocode demonstrates what the game loop of a pong \ngame might look like at its core :\nvoid main() // Pong\n{\ninitGame();\nwhile (true) // game loop\n {\nreadHumanInterfaceDevices();\n  if \n(quitButtonPressed())\n  {\n \n \n \nbreak; // exit the game loop\n  }\nmovePaddles();\nmoveBall();\n",
      "content_length": 1772,
      "extraction_method": "Direct"
    },
    {
      "page_number": 328,
      "chapter": null,
      "content": "306 \n7. The Game Loop and Real-Time Simulation\ncollideAndBounceBall();\n  if \n(ballImpactedSide(LEFT_PLAYER))\n  {\nincremenentScore(RIGHT_PLAYER);\nresetBall();\n  }\n \n \nelse if (ballImpactedSide(RIGHT_PLAYER))\n  {\nincrementScore(LEFT_PLAYER);\nresetBall();\n  }\nrenderPlayfield();\n }\n}\nClearly this example is somewhat contrived. The original pong games were \ncertainly not implemented by redrawing the entire screen at a rate of 30 \nframes per second. Back then, CPUs were so slow that they could barely mus-\nter the power to draw two lines for the paddles and a box for the ball in real \ntime. Specialized 2D sprite hardware was oft en used to draw moving objects \non-screen. However, we’re only interested in the concepts here, not the imple-\nmentation details of the original Pong.\nAs you can see, when the game ﬁ rst runs, it calls initGame() to do \nwhatever set-up might be required by the graphics system, human I/O de-\nvices, audio system, etc. Then the main game loop is entered. The statement \nwhile (true) tells us that the loop will continue forever, unless interrupted \ninternally. The ﬁ rst thing we do inside the loop is to read the human interface \ndevice(s). We check to see whether either human player pressed the “quit” \nbutt on—if so, we exit the game via a break statement. Next, the positions of \nthe paddles are adjusted slightly upward or downward in movePaddles(), \nbased on the current deﬂ ection of the control wheels, joysticks, or other I/O \ndevices. The function moveBall() adds the ball’s current velocity vector to its \nposition in order to ﬁ nd its new position next frame. In collideAndBounce-\nBall(), this position is then checked for collisions against both the ﬁ xed hori-\nzontal walls and the paddles. If collisions are detected, the ball’s position is re-\ncalculated to account for any bounce. We also note whether the ball impacted \neither the left  or right edge of the screen. This means that it missed one of the \npaddles, in which case we increment the other player’s score and reset the ball \nfor the next round. Finally, renderPlayﬁeld() draws the entire contents of \nthe screen.\n",
      "content_length": 2121,
      "extraction_method": "Direct"
    },
    {
      "page_number": 329,
      "chapter": null,
      "content": "307 \n7.3. Game Loop Architectural Styles\n7.3. Game Loop Architectural Styles\nGame loops can be implemented in a number of diﬀ erent ways—but at their \ncore, they usually boil down to one or more simple loops, with various embel-\nlishments. We’ll explore a few of the more common architectures below.\n7.3.1. \nWindows Message Pumps\nOn a Windows platform, games need to service messages from the Windows \noperating system in addition to servicing the various subsystems in the game \nengine itself. Windows games therefore contain a chunk of code known as a \nmessage pump . The basic idea is to service Windows messages whenever they \narrive and to service the game engine only when no Windows messages are \npending. A message pump typically looks something like this:\nwhile (true)\n{\n \n// Service any and all pending Windows messages.\n \nMSG msg;\nwhile (PeekMessage(&msg, NULL, 0, 0) > 0)\n {\nTranslateMessage(&msg);\nDispatchMessage(&msg);\n }\n \n// No more Windows messages to process – run one   \n \n \n// iteration of our \"real\" game loop.\nRunOneIterationOfGameLoop();\n}\nOne of the side-eﬀ ects of implementing the game loop like this is that Win-\ndows messages take precedence over rendering and simulating the game. As \na result, the game will temporarily freeze whenever you resize or drag the \ngame’s window around on the desktop.\n7.3.2. Callback-Driven Frameworks\nMost game engine subsystems and third-party game middleware packages \nare structured as libraries . A library is a suite of functions and/or classes that \n",
      "content_length": 1517,
      "extraction_method": "Direct"
    },
    {
      "page_number": 330,
      "chapter": null,
      "content": "308 \n7. The Game Loop and Real-Time Simulation\ncan be called in any way the application programmer sees ﬁ t. Libraries pro-\nvide maximum ﬂ exibility to the programmer. But libraries are sometimes dif-\nﬁ cult to use, because the programmer must understand how to properly use \nthe functions and classes they provide.\nIn contrast, some game engines and game middleware packages are \nstructured as frameworks . A framework is a partially-constructed applica-\ntion—the programmer completes the application by providing custom im-\nplementations of missing functionality within the framework (or overriding \nits default behavior). But he or she has litt le or no control over the overall \nﬂ ow of control within the application, because it is controlled by the frame-\nwork.\nIn a framework-based rendering engine or game engine, the main game \nloop has been writt en for us, but it is largely empty. The game programmer can \nwrite callback functions in order to “ﬁ ll in” the missing details. The Ogre3D \nrendering engine is an example of a library that has been wrapped in a frame-\nwork. At the lowest level, Ogre provides functions that can be called directly \nby a game engine programmer. However, Ogre also provides a framework that \nencapsulates knowledge of how to use the low-level Ogre library eﬀ ectively. If \nthe programmer chooses to use the Ogre framework, he or she derives a class \nfrom Ogre::FrameListener and overrides two virtual functions: frame-\nStarted() and frameEnded(). As you might guess, these functions are called \nbefore and aft er the main 3D scene has been rendered by Ogre, respectively. \nThe Ogre framework’s implementation of its internal game loop looks some-\nthing like the following pseudocode. (See Ogre::Root::renderOneFrame()\nin OgreRoot.cpp for the actual source code.)\nwhile (true)\n{\n \nfor (each frameListener)\n {\n  frameListener.\nframeStarted();\n }\nrenderCurrentScene();\n \nfor (each frameListener)\n {\n  frameListener.\nframeEnded();\n }\nfinalizeSceneAndSwapBuffers();\n}\n",
      "content_length": 2002,
      "extraction_method": "Direct"
    },
    {
      "page_number": 331,
      "chapter": null,
      "content": "309 \nA particular game’s frame listener implementation might look something like \nthis.\nclass GameFrameListener : public Ogre::FrameListener\n{\npublic:\n \nvirtual void frameStarted(const FrameEvent& event)\n {\n \n  \n// Do things that must happen before the 3D scene  \n \n  \n// is rendered (i.e., service all game engine   \n \n \n  // subsystems).\n \n  pollJoypad(event);\n \n  updatePlayerControls(event);\n \n  updateDynamicsSimulation(event);\n \n  resolveCollisions(event);\n \n  updateCamera(event);\n \n  // etc.\n }\n \nvirtual void frameEnded(const FrameEvent& event)\n {\n \n  \n// Do things that must happen after the 3D scene \n \n  \n// has been rendered.\n \n  drawHud(event);\n \n  // etc.\n }\n};\n7.3.3. Event-Based Updating\nIn games, an event is any interesting change in the state of the game or its \nenvironment. Some examples include: the human player pressing a butt on \non the joypad, an explosion going oﬀ , an enemy character spott ing the player, \nand the list goes on. Most game engines have an event system, which permits \nvarious engine subsystems to register interest in particular kinds of events \nand to respond to those events when they occur (see Section 14.7 for details). \nA game’s event system is usually very similar to the event/messaging system \nunderlying virtually all graphical user interfaces (for example, Microsoft  Win-\ndows’ window messages, the event handling system in Java’s AWT, or the \nservices provided by C#’s delegate and event keywords).\nSome game engines leverage their event system in order to implement \nthe periodic servicing of some or all of their subsystems. For this to work, the \nevent system must permit events to be posted into the future—that is, to be \nqueued for later delivery. A game engine can then implement periodic updat-\n7.3. Game Loop Architectural Styles\n",
      "content_length": 1798,
      "extraction_method": "Direct"
    },
    {
      "page_number": 332,
      "chapter": null,
      "content": "310 \n7. The Game Loop and Real-Time Simulation\ning by simply posting an event. In the event handler, the code can perform \nwhatever periodic servicing is required. It can then post a new event 1/30 or \n1/60 of a second into the future, thus continuing the periodic servicing for as \nlong as it is required.\n7.4. Abstract Timelines\nIn game programming, it can be extremely useful to think in terms of abstract \ntimelines . A timeline is a continuous, one-dimensional axis whose origin (t = 0) \ncan lie at any arbitrary location relative to other timelines in the system. A \ntimeline can be implemented via a simple clock variable that stores absolute \ntime values in either integer or ﬂ oating-point format.\n7.4.1. \nReal Time\n We can think of times measured directly via the CPU’s high-resolution timer \nregister (see Section 7.5.3) as lying on what we’ll call the real timeline. The ori-\ngin of this timeline is deﬁ ned to coincide with the moment the CPU was last \npowered on or reset. It measures times in units of CPU cycles (or some mul-\ntiple thereof), although these time values can be easily converted into units of \nseconds by multiplying them by the frequency of the high-resolution timer on \nthe current CPU.\n7.4.2. Game Time\n We needn’t limit ourselves to working with the real timeline exclusively. We \ncan deﬁ ne as many other timeline(s) as we need, in order to solve the prob-\nlems at hand. For example, we can deﬁ ne a game timeline that is technically \nindependent of real time. Under normal circumstances, game time coincides \nwith real time. If we wish to pause the game, we can simply stop updating \nthe game timeline temporarily. If we want our game to go into slow-motion, \nwe can update the game clock more slowly than the real-time clock. All sorts \nof eﬀ ects can be achieved by scaling and warping one timeline relative to an-\nother.\nPausing or slowing down the game clock is also a highly useful debug-\nging tool. To track down a visual anomaly, a developer can pause game time \nin order to freeze the action. Meanwhile, the rendering engine and debug ﬂ y-\nthrough camera can continue to run, as long as they are governed by a dif-\nferent clock (either the real-time clock, or a separate camera clock). This allows \nthe developer to ﬂ y the camera around the game world to inspect it from \nany angle desired. We can even support single-stepping the game clock, by \n",
      "content_length": 2392,
      "extraction_method": "Direct"
    },
    {
      "page_number": 333,
      "chapter": null,
      "content": "311 \n7.4. Abstract Timelines\nadvancing the game clock by one target frame interval (e.g., 1/30 of a second) \neach time a “single-step” butt on is pressed on the joypad or keyboard while \nthe game is in a paused state.\nWhen using the approach described above, it’s important to realize that \nthe game loop is still running when the game is paused—only the game clock \nhas stopped. Single-stepping the game by adding 1/30 of a second to a paused \ngame clock is not the same thing as sett ing a break point in your main loop, \nand then hitt ing the F5 key repeatedly to run one iteration of the loop at a \ntime. Both kinds of single-stepping can be useful for tracking down diﬀ erent \nkinds of problems. We just need to keep the diﬀ erences between these ap-\nproaches in mind.\n7.4.3. Local and Global Timelines\nWe can envision all sorts of other timelines. For example, an animation clip or \naudio clip might have a local timeline, with its origin (t = 0) deﬁ ned to coincide \nwith the start of the clip. The local timeline measures how time progressed \nwhen the clip was originally authored or recorded. When the clip is played \nback in-game, we needn’t play it at the original rate. We might want to speed \nup an animation, or slow down an audio sample. We can even play an anima-\ntion backwards by running its local clock in reverse.\nAny one of these eﬀ ects can be visualized as a mapping between the lo-\ncal timeline and a global timeline, such as real time or game time. To play an \nanimation clip back at its originally-authored speed, we simply map the start \nof the animation’s local timeline (t = 0) onto the desired start time \nstart\n(\n)\nτ =τ\nalong the global timeline. This is shown in Figure 7.1.\nTo play an animation clip back at half speed, we can imagine scaling the \nlocal timeline to twice its original size prior to mapping it onto the global \ntimeline. To accomplish this, we simply keep track of a time scale factor or \nplayback rate R, in addition to the clip’s global start time \nstart.\nτ\n  This is illus-\ntrated in Figure 7.2. A clip can even be played in reverse, by using a negative \ntime scale (R < 0) as shown in Figure 7.3.\nClip A\nt = 0 sec\n5 sec\nτstart = 102 sec\nτ = 105 sec\n110 sec\nFigure 7.1. Playing an animation clip can be envisioned as mapping its local timeline onto the \nglobal game timeline.\n",
      "content_length": 2328,
      "extraction_method": "Direct"
    },
    {
      "page_number": 334,
      "chapter": null,
      "content": "312 \n7. The Game Loop and Real-Time Simulation\n7.5. Measuring and Dealing with Time\nIn this section, we’ll investigate some of the subtle and not-so-subtle distinc-\ntions between diﬀ erent kinds of timelines and clocks and see how they are \nimplemented in real game engines.\n7.5.1. \nFrame Rate and Time Deltas\nThe frame rate of a real-time game describes how rapidly the sequence of still \n3D frames is presented to the viewer. The unit of Hertz (Hz), deﬁ ned as the \nnumber of cycles per second, can be used to describe the rate of any periodic \nprocess. In games and ﬁ lm, frame rate is typically measured in frames per sec-\nond (FPS), which is the same thing as Hertz for all intents and purposes. Films \ntraditionally run at 24 FPS. Games in North America and Japan are typically \nrendered at 30 or 60 FPS, because this is the natural refresh rate of the NTSC \ncolor television standard used in these regions. In Europe and most of the rest \nClip A\nτstart = 102 sec\nτ = 105 sec\nClip A\nR = 2\n(scale t by 1/R = 0.5)\nt = 0 sec\nt = 5 sec\nt = 0 sec\n5 sec\nFigure 7.2. Animation play-back speed can be controlled by simply scaling the local time line \nprior to mapping it onto the global time line.\nt = 5 sec\n0 sec\nτstart = 102 sec\nτ = 105 sec\n110 sec\n A pilC\nClip A\nt = 0 sec\n5 sec\nR = –1\n(ﬂip t)\nFigure 7.3.  Playing an animation in reverse is like mapping the clip to the global time line with \na time scale of R = –1.\n",
      "content_length": 1419,
      "extraction_method": "Direct"
    },
    {
      "page_number": 335,
      "chapter": null,
      "content": "313 \nof the world, games update at 50 FPS, because this is the natural refresh rate \nof a PAL or SECAM color television signal.\nThe amount of time that elapses between frames is known as the frame \ntime, time delta , or delta time. This last term is commonplace because the dura-\ntion between frames is oft en represented mathematically by the symbol Δt. \n(Technically speaking, Δt should really be called the frame period, since it is \nthe inverse of the frame frequency: T = 1/f. But game programmers hardly ever \nuse the term “period” in this context.) If a game is being rendered at exactly \n30 FPS, then its delta time is 1/30 of a second, or 33.3 ms (milliseconds). At \n60 FPS, the delta time is half as big, 1/60 of a second or 16.6 ms. To really know \nhow much time has elapsed during one iteration of the game loop, we need to \nmeasure it. We’ll see how this is done below.\nWe should note here that milliseconds are a common unit of time mea-\nsurement in games. For example, we might say that animation is taking 4 ms, \nwhich implies that it occupies about 12% of the entire frame (4 / 33.3 ≈ 0.12). \nOther common units include seconds and machine cycles. We’ll discuss time \nunits and clock variables in more depth below.\n7.5.2. From Frame Rate to Speed\nLet’s imagine that we want to make a spaceship ﬂ y through our game world at \na constant speed of 40 meters per second (or in a 2D game, we might specify \nthis as 40 pixels per second!) One simple way to accomplish this is to multiply \nthe ship’s speed v (measured in meters per second) by the duration of one \nframe Δt (measured in seconds), yielding a change in position Δx = v Δt (which \nis measured in meters per frame). This position delta can then be added to the \nship’s current position x1 , in order to ﬁ nd its position next frame: x2 = x1 + Δx \n= x1 + v Δt.\nThis is actually a simple form of numerical integration known as the explicit \nEuler method (see Section 12.4.4). It works well as long as the speeds of our \nobjects are roughly constant. To handle variable speeds, we need to resort to \nsomewhat more-complex integration methods. But all numerical integration \ntechniques make use of the elapsed frame time Δt in one way or another. So \nit is safe to say that the perceived speeds of the objects in a game are dependent \nupon the frame duration, Δt. Hence a central problem in game programming \nis to determine a suitable value for Δt. In the sections that follow, we’ll discuss \nvarious ways of doing this.\n7.5.2.1. Old-School CPU-Dependent Games\nIn many early video games, no att empt was made to measure how much real \ntime had elapsed during the game loop. The programmers would essentially \n7.5. Measuring and Dealing with Time\n",
      "content_length": 2716,
      "extraction_method": "Direct"
    },
    {
      "page_number": 336,
      "chapter": null,
      "content": "314 \n7. The Game Loop and Real-Time Simulation\nignore Δt altogether and instead specify the speeds of objects directly in terms \nof meters (or pixels, or some other distance unit) per frame. In other words, \nthey were, perhaps unwitt ingly, specifying object speeds in terms of Δx = v Δt, \ninstead of in terms of v.\nThe net eﬀ ect of this simplistic approach was that the perceived speeds of \nthe objects in these games were entirely dependent upon the frame rate that \nthe game was actually achieving on a particular piece of hardware. If this kind \nof game were to be run on a computer with a faster CPU than the machine for \nwhich it was originally writt en, the game would appear to be running in fast \nforward. For this reason, I’ll call these games CPU-dependent games .\nSome older PCs provided a “Turbo” butt on to support these kinds of \ngames. When the Turbo butt on was pressed, the PC would run at its fastest \nspeed, but CPU-dependent games would run in fast forward. When the Turbo \nbutt on was not pressed, the PC would mimic the processor speed of an older \ngeneration of PCs, allowing CPU-dependent games writt en for those PCs to \nrun properly.\n7.5.2.2. Updating Based on Elapsed Time\n To make our games CPU-independent, we must measure Δt in some way, rath-\ner than simply ignoring it. Doing this is quite straightforward. We simply read \nthe value of the CPU’s high resolution timer twice—once at the beginning of \nthe frame and once at the end. Then we subtract, producing an accurate mea-\nsure of Δt for the frame that has just passed. This delta is then made available \nto all engine subsystems that need it, either by passing it to every function that \nwe call from within the game loop or by storing it in a global variable or en-\ncapsulating it within a singleton class of some kind. (We’ll describe the CPU’s \nhigh resolution timer in more detail Section 7.5.3.)\nThe approach outlined above is used by many game engines. In fact, I am \ntempted to go out on a limb and say that most game engines use it. However, \nthere is one big problem with this technique: We are using the measured value \nof Δt taken during frame k as an estimate of the duration of the upcoming frame \n(k + 1). This isn’t necessarily very accurate. (As they say in investing, “past per-\nformance is not a guarantee of future results.”) Something might happen next \nframe that causes it to take much more time (or much less) than the current \nframe. We call such an event a frame-rate spike.\nUsing last frame’s delta as an estimate of the upcoming frame can have \nsome very real detrimental eﬀ ects. For example, if we’re not careful it can put \nthe game into a “viscious cycle” of poor frame times. Let’s assume that our \nphysics simulation is most stable when updated once every 33.3 ms (i.e., at \n30 Hz). If we get one bad frame, taking say 57 ms, then we might make the \n",
      "content_length": 2871,
      "extraction_method": "Direct"
    },
    {
      "page_number": 337,
      "chapter": null,
      "content": "315 \nmistake of stepping the physics system twice on the next frame, presumably to \n“cover” the 57 ms that has passed. Those two steps take roughly twice as long \nto complete as a regular step, causing the next frame to be at least as bad as \nthis one was, and possibly worse. This only serves to exacerbate and prolong \nthe problem.\n7.5.2.3. Using a Running Average\nIt is true that game loops tend to have at least some frame-to-frame coher-\nency . If the camera is pointed down a hallway containing lots of expensive-to-\ndraw objects on one frame, there’s a good chance it will still be pointed down \nthat hallway on the next. Therefore, one reasonable approach is to average the \nframe-time measurements over a small number of frames and use that as the \nnext frame’s estimate of Δt . This allows the game to adapt to varying frame \nrate, while soft ening the eﬀ ects of momentary performance spikes. The longer \nthe averaging interval, the less responsive the game will be to varying frame \nrate, but spikes will have less of an impact as well.\n7.5.2.4. Governing the Frame Rate\nWe can avoid the inaccuracy of using last frame’s Δt as an estimate of this \nframe’s duration altogether, by ﬂ ipping the problem on its head. Rather than \ntrying to guess at what next frame’s duration will be, we can instead att empt \nto guarantee that every frame’s duration will be exactly 33.3 ms (or 16.6 ms if \nwe’re running at 60 FPS). To do this, we measure the duration of the current \nframe as before. If the measured duration is less than the ideal frame time, we \nsimply put the main thread to sleep until the target frame time has elapsed. \nIf the measured duration is more than the ideal frame time, we must “take \nour lumps” and wait for one more whole frame time to elapse. This is called \nframe-rate governing .\nClearly this approach only works when your game’s frame rate is reason-\nably close to your target frame rate on average. If your game is ping-ponging \nbetween 30 FPS and 15 FPS due to frequent “slow” frames, then the game’s \nquality can degrade signiﬁ cantly. As such, it’s still a good idea to design all \nengine systems so that they are capable of dealing with arbitrary frame dura-\ntions. During development, you can leave the engine in “variable frame rate” \nmode, and everything will work as expected. Later on, when the game is get-\nting closer to achieving its target frame rate consistently, we can switch on \nframe-rate governing and start to reap its beneﬁ ts.\nKeeping the frame rate consistent can be important for a number of rea-\nsons. Some engine systems, such as the numerical integrators used in a phys-\nics simulation, operate best when updated at a constant rate. A consistent \n7.5. Measuring and Dealing with Time\n",
      "content_length": 2745,
      "extraction_method": "Direct"
    },
    {
      "page_number": 338,
      "chapter": null,
      "content": "316 \n7. The Game Loop and Real-Time Simulation\nframe rate also looks bett er, and as we’ll see in the next section, it can be used \nto avoid the tearing that can occur when the video buﬀ er is updated at a rate \nthat doesn’t match the refresh rate of the monitor.\nIn addition, when elapsed frame times are consistent, features like record \nand play back become a lot more reliable. As its name implies, the record and \nplay back feature allows a player’s gameplay experience to be recorded and \nlater played back in exactly the same way. This can be a fun game feature, and \nit’s also a valuable testing and debugging tool. For example, diﬃ  cult-to-ﬁ nd \nbugs can be reproduced by simply playing back a recorded game that dem-\nonstrates the bug.\nTo implement record and play back, we make note of every relevant event \nthat occurs during gameplay, saving each one in a list along with an accurate \ntime stamp. The list of events can then be replayed with exactly the same tim-\ning, using the same initial conditions, and an identical initial random seed. \nIn theory, doing this should produce a gameplay experience that is indis-\ntinguishable from the original playthrough. However, if the frame rate isn’t \nconsistent, things may not happen in exactly the same order. This can cause \n“drift ,” and prett y soon your AI characters are ﬂ anking when they should \nhave fallen back.\n7.5.2.5. The Vertical Blanking Interval\nA visual anomaly known as tearing occurs when the back buﬀ er is swapped \nwith the front buﬀ er while the electron gun in the CRT monitor is only part \nway through its scan. When tearing occurs, the top portion of the screen shows \nthe old image, while the bott om portion shows the new one. To avoid tearing, \nmany rendering engines wait for the vertical blanking interval of the monitor \n(the time during which the electron gun is being reset to the top-left  corner of \nthe screen) before swapping buﬀ ers.\nWaiting for the v-blank interval is another form of frame-rate governing . It \neﬀ ectively clamps the frame rate of the main game loop to a multiple of the \nscreen’s refresh rate. For example, on an NTSC monitor that refreshes at a rate \nof 60 Hz, the game’s real update rate is eﬀ ectively quantized to a multiple \nof 1/60 of a second. If more than 1/60 of a second elapses between frames, \nwe must wait until the next v-blank interval, which means waiting 2/60 of a \nsecond (30 FPS). If we miss two v-blanks, then we must wait a total of 3/60 of \na second (20 FPS), and so on. Also, be careful not to make assumptions about \nthe frame rate of your game, even when it is synchronized to the v-blank in-\nterval; remember that the PAL and SECAM standards are based around an \nupdate rate of 50 Hz, not 60 Hz.\n",
      "content_length": 2739,
      "extraction_method": "Direct"
    },
    {
      "page_number": 339,
      "chapter": null,
      "content": "317 \n7.5.3. Measuring Real Time with a High-Resolution Timer\n We’ve talked a lot about measuring the amount of real “wall clock” time that \nelapses during each frame. In this section, we’ll investigate how such timing \nmeasurements are made in detail.\nMost operating systems provide a function for querying the system time, \nsuch as the standard C library function time(). However, such functions are \nnot suitable for measuring elapsed times in a real-time game, because they \ndo not provide suﬃ  cient resolution. For example, time() returns an integer \nrepresenting the number of seconds since midnight, January 1, 1970, so its reso-\nlution is one second—far too coarse, considering that a frame takes only tens \nof milliseconds to execute.\nAll modern CPUs contain a high-resolution timer , which is usually imple-\nmented as a hardware register that counts the number of CPU cycles (or some \nmultiple thereof) that have elapsed since the last time the processor was pow-\nered on or reset. This is the timer that we should use when measuring elapsed \ntime in a game, because its resolution is usually on the order of the duration \nof a few CPU cycles. For example, on a 3 GHz Pentium processor, the high-\nresolution timer increments once per CPU cycle, or 3 billion times per second. \nHence the resolution of the high-res timer is 1 / 3 billion = 3.33 × 10–10 seconds = \n0.333 ns (one-third of a nanosecond). This is more than enough resolution for \nall of our time-measurement needs in a game.\nDiﬀ erent microprocessors and diﬀ erent operating systems provide dif-\nferent ways to query the high-resolution timer. On a Pentium, a special instruc-\ntion called rdtsc (read time-stamp counter) can be used, although the Win32 \nAPI wraps this facility in a pair of functions: QueryPerformanceCounter()\nreads the 64-bit counter register and QueryPerformanceFrequency()\nreturns the number of counter increments per second for the current CPU. \nOn a PowerPC architecture, such as the chips found in the Xbox 360 and \nPLAYSTATION 3, the instruction mftb (move from time base register) can \nbe used to read the two 32-bit time base registers, while on other PowerPC \narchitectures, the instruction mfspr (move from special-purpose register) is \nused instead.\nA CPU’s high-resolution timer register is 64 bits wide on most processors, \nto ensure that it won’t wrap too oft en. The largest possible value of a 64-bit un-\nsigned integer is 0xFFFFFFFFFFFFFFFF ≈ 1.8 × 1019 clock ticks. So, on a 3 GHz \nPentium processor that updates its high-res timer once per CPU cycle, the \nregister’s value will wrap back to zero once every 195 years or so—deﬁ nitely \nnot a situation we need to lose too much sleep over. In contrast, a 32-bit integer \nclock will wrap aft er only about 1.4 seconds at 3 GHz.\n7.5. Measuring and Dealing with Time\n",
      "content_length": 2822,
      "extraction_method": "Direct"
    },
    {
      "page_number": 340,
      "chapter": null,
      "content": "318 \n7. The Game Loop and Real-Time Simulation\n7.5.3.1. \nHigh-Resolution Clock Drift\n Be aware that even timing measurements taken via a high-resolution timer can \nbe inaccurate in certain circumstances. For example, on some multicore pro-\ncessors , the high-resolution timers are independent on each core, and they can \n(and do) drift  apart. If you try to compare absolute timer readings taken on dif-\nferent cores to one another, you might end up with some strange results—even \nnegative time deltas. Be sure to keep an eye out for these kinds of problems.\n7.5.4. Time Units and Clock Variables\nWhenever we measure or specify time durations in a game, we have two \nchoices to make:\nWhat \n1. \ntime units should be used? Do we want to store our times in \nseconds, or milliseconds, or machine cycles… or in some other unit?\nWhat \n2. \ndata type should be used to store time measurements? Should we \nemploy a 64-bit integer, or a 32-bit integer, or a 32-bit ﬂ oating point \nvariable ?\nThe answers to these questions depend on the intended purpose of a given \nmeasurement. This gives rise to two more questions: How much precision \ndo we need? And what range of magnitudes do we expect to be able to rep-\nresent?\n7.5.4.1. 64-Bit Integer Clocks\nWe’ve already seen that a 64-bit unsigned integer clock, measured in machine \ncycles, supports both an extremely high precision (a single cycle is 0.333 ns in \nduration on a 3 GHz CPU) and a broad range of magnitudes (a 64-bit clock \nwraps once roughly every 195 years at 3 GHz). So this is the most ﬂ exible time \nrepresentation, presuming you can aﬀ ord 64 bits worth of storage.\n7.5.4.2. 32-Bit Integer Clocks\nWhen measuring relatively short durations with high precision, we can turn \nto a 32-bit integer clock, measured in machine cycles. For eample, to proﬁ le \nthe performance of a block of code, we might do something like this:\n// Grab a time snapshot.\nU64 tBegin = readHiResTimer();\n// This is the block of code whose performance we wish \n// to measure.\ndoSomething();\ndoSomethingElse();\nnowReallyDoSomething();\n",
      "content_length": 2063,
      "extraction_method": "Direct"
    },
    {
      "page_number": 341,
      "chapter": null,
      "content": "319 \n// Measure the duration.\nU64 tEnd = readHiResTimer();\nU32 dtCycles = static_cast<U32>(tEnd – tBegin);\n// Now use or cache the value of dtCycles...\nNotice that we still store the raw time measurements in 64-bit integer \nvariables. Only the time delta dt is stored in a 32-bit variable. This circum-\nvents potential problems with wrapping at the 32-bit boundary. For example, \nif tBegin == 0x12345678FFFFFFB7 and tEnd == 0x1234567900000039, \nthen we would measure a negative time delta if we were to truncate the indi-\nvidual time measurements to 32 bits each prior to subtracting them.\n7.5.4.3. 32-Bit Floating-Point Clocks\n Another common approach is to store relatively small time deltas in ﬂ oating- \npoint format, measured in units of seconds. To do this, we simply multiply a \nduration measured in CPU cycles by the CPU’s clock frequency, which is in \ncycles per second. For example:\n// Start off assuming an ideal frame time (30 FPS).\nF32 dtSeconds = 1.0f / 30.0f;\n// Prime the pump by reading the current time.\nU64 tBegin = readHiResTimer();\nwhile (true) // main game loop\n{\nrunOneIterationOfGameLoop(dtSeconds);\n \n// Read the current time again, and calculate the  \n \n \n// delta.\n U64 \ntEnd = readHiResTimer();\ndtSeconds = (F32)(tEnd – tBegin)\n \n          * (F32)getHiResTimerFrequency();\n \n// Use tEnd as the new tBegin for next frame.\ntBegin = tEnd;\n}\nNotice once again that we must be careful to subtract the two 64-bit time \nmeasurements before converting them into ﬂ oating point format. This ensures \nthat we don’t store too large a magnitude into a 32-bit ﬂ oating point variable.\n7.5.4.4. Limitations of Floating Point Clocks\nRecall that in a 32-bit IEEE ﬂ oat, the 23 bits of the mantissa are dynamically \ndistributed between the whole and fractional parts of the value, by way \n7.5. Measuring and Dealing with Time\n",
      "content_length": 1837,
      "extraction_method": "Direct"
    },
    {
      "page_number": 342,
      "chapter": null,
      "content": "320 \n7. The Game Loop and Real-Time Simulation\nof the exponent (see Section 3.2.1.4). Small magnitudes require only a few \nbits, leaving plenty of bits of precision for the fraction. But once the magni-\ntude of our clock grows too large, its whole part eats up more bits, leaving \nfewer bits for the fraction. Eventually, even the least-signiﬁ cant bits of the \nwhole part become implicit zeros. This means that we must be cautious \nwhen storing long durations in a ﬂ oating-point clock variable. If we keep \ntrack of the amount of time that has elapsed since the game was started, a \nﬂ oating-point clock will eventually become inaccurate to the point of being \nunusable.\nFloating-point clocks are usually only used to store relatively short time \ndeltas, measuring at most a few minutes, and more oft en just a single frame \nor less. If an absolute-valued ﬂ oating-point clock is used in a game, you will \nneed to reset the clock to zero periodically, to avoid accumulation of large \nmagnitudes.\n7.5.4.5. Other Time Units\nSome game engines allow timing values to be speciﬁ ed in a game-deﬁ ned \nunit that is ﬁ ne-grained enough to permit a 32-bit integer format to be used, \nprecise enough to be useful for a wide range of applications within the en-\ngine, and yet large enough that the 32-bit clock won’t wrap too oft en. One \ncommon choice is a 1/300 second time unit. This works well because (a) it is \nﬁ ne-grained enough for many purposes, (b) it only wraps once every 165.7 \ndays, and (c) it is an even multiple of both NTSC and PAL refresh rates. A \n60 FPS frame would be 5 such units in duration, while a 50 FPS frame would \nbe 6 units in duration.\nObviously a 1/300 second time unit is not precise enough to handle subtle \neﬀ ects, like time-scaling an animation. (If we tried to slow a 30 FPS anima-\ntion down to less than 1/10 of its regular speed, we’d be out of precision!) So \nfor many purposes, it’s still best to use ﬂ oating-point time units, or machine \ncycles. But a 1/300 second time unit can be used eﬀ ectively for things like \nspecifying how much time should elapse between the shots of an automatic \nweapon, or how long an AI -controlled character should wait before starting \nhis patrol, or the amount of time the player can survive when standing in a \npool of acid.\n7.5.5. Dealing with Break Points\n When your game hits a break point, its loop stops running and the debug-\nger takes over. However, the CPU continues to run, and the real-time clock \ncontinues to accrue cycles. A large amount of wall clock time can pass while \nyou are inspecting your code at a break point. When you allow the program \n",
      "content_length": 2630,
      "extraction_method": "Direct"
    },
    {
      "page_number": 343,
      "chapter": null,
      "content": "321 \nto continue, this can lead to a measured frame time many seconds, or even \nminutes or hours in duration!\nClearly if we allow such a huge delta-time to be passed to the subsystems \nin our engine, bad things will happen. If we are lucky, the game might con-\ntinue to function properly aft er lurching forward many seconds in a single \nframe. Worse, the game might just crash.\nA simple approach can be used to get around this problem. In the main \ngame loop, if we ever measure a frame time in excess of some predeﬁ ned up-\nper limit (e.g., 1/10 of a second), we can assume that we have just resumed ex-\necution aft er a break point, and we set the delta time artiﬁ cially to 1/30 or 1/60 \nof a second (or whatever the target frame rate is). In eﬀ ect, the game becomes \nframe-locked for one frame, in order to avoid a massive spike in the measured \nframe duration.\n// Start off assuming the ideal dt (30 FPS).\nF32 dt = 1.0f / 30.0f;\n// Prime the pump by reading the current time.\nU64 tBegin = readHiResTimer();\nwhile (true) // main game loop\n{\n updateSubsystemA(dt);\n updateSubsystemB(dt);\n \n// ...\n renderScene();\n swapBuffers();\n \n// Read the current time again, and calculate an   \n \n \n// estimate of next frame’s delta time.\n \nU64 tEnd = readHiResTimer();\n \ndt = (F32)(tEnd – tBegin) / (F32) \n   getHiResTimerFrequency();\n \n// If dt is too large, we must have resumed from a  \n \n \n// break point -- frame-lock to the target rate this   \n \n// frame.\n \nif (dt > 1.0f/10.0f)\n {\n \n \ndt = 1.0f/30.0f;\n }\n \n// Use tEnd as the new tBegin for next frame.\n \ntBegin = tEnd;\n}\n7.5. Measuring and Dealing with Time\n",
      "content_length": 1610,
      "extraction_method": "Direct"
    },
    {
      "page_number": 344,
      "chapter": null,
      "content": "322 \n7. The Game Loop and Real-Time Simulation\n7.5.6. A Simple Clock Class\nSome game engines encapsulate their clock variables in a class. An engine \nmight have a few instances of this class—one to represent real “wall clock” \ntime, another to represent “game time” (which can be paused, slowed down \nor sped up relative to real time), another to track time for full-motion videos, \nand so on. A clock class is reasonably straightforward to implement. I’ll pres-\nent a simple implementation below, making note of a few common tips, tricks, \nand pitfalls in the process.\nA clock class typically contains a variable that tracks the absolute time \nthat has elapsed since the clock was created. As described above, it’s im-\nportant to select a suitable data type and time unit for this variable. In the \nfollowing example, we’ll store absolute times in the same way the CPU \ndoes—with a 64-bit unsigned integer, measured in machine cycles. There \nare other possible implementations, of course, but this is probably the sim-\nplest.\nA clock class can support some nift y features, like time-scaling. This can \nbe implemented by simply multiplying the measured time delta by an arbi-\ntrary scale factor prior to adding it to the clock’s running total. We can also \npause time by simply skipping its update while the clock is paused. Single-\nstepping a clock can be implemented by adding a ﬁ xed time interval to a \npaused clock in response to a butt on press on the joypad or keyboard. All of \nthis is demonstrated by the example Clock class shown below.\nclass Clock\n{\n U64 \nm_timeCycles;\n F32 \nm_timeScale;\n \nbool  \nm_isPaused;\nstatic F32 \ns_cyclesPerSecond;\n \nstatic inline U64 secondsToCycles(F32 timeSeconds) \n {\n \n \nreturn (U64)(timeSeconds * s_cyclesPerSecond);\n }\n \n// WARNING: Dangerous -- only use to convert small\n \n// durations into seconds.\n \nstatic inline F32 cyclesToSeconds(U64 timeCycles)\n {\n \n \nreturn (F32)timeCycles / s_cyclesPerSecond;\n }\n",
      "content_length": 1952,
      "extraction_method": "Direct"
    },
    {
      "page_number": 345,
      "chapter": null,
      "content": "323 \npublic:\n \n// Call this when the game first starts up.\n \nstatic void init()\n {\n  s_cyclesPerSecond \n   = \n(F32)readHiResTimerFrequency();\n }\n \n// Construct a clock.\n explicit \nClock(F32 startTimeSeconds = 0.0f) :\n  m_timeCycles(\nsecondsToCycles(startTimeSeconds)),\n  m_timeScale(\n1.0f), // default to unscaled\n  m_isPaused(\nfalse)  // default to running\n {\n }\n \n// Return the current time in cycles. NOTE that we do  \n \n// not return absolute time measurements in floating   \n \n// point seconds, because a 32-bit float doesn’t have  \n \n// enough precision. See calcDeltaSeconds().\n U64 \ngetTimeCycles() const\n {\n  return \nm_timeCycles;\n }\n \n// Determine the difference between this clock’s  \n \n// absolute time and that of another clock, in \n \n// seconds. We only return time deltas as floating  \n \n \n// point seconds, due to the precision limitations of  \n \n// a 32-bit float.\n F32 \ncalcDeltaSeconds(const Clock& other)\n {\n \n \nU64 dt = m_timeCycles – other.m_timeCycles;\n  return \ncyclesToSeconds(dt);\n }\n \n// This function should be called once per frame,  \n \n   // with the real measured frame time delta in seconds.\n void \nupdate(F32 dtRealSeconds)\n {\n  if \n(!m_isPaused)\n  { \n  \n   U64 \ndtScaledCycles\n    \n= secondsToCycles(\n    dtRealSeconds \n* m_timeScale);\n   m_timeCycles \n+= dtScaledCycles;\n  }\n }\n7.5. Measuring and Dealing with Time\n",
      "content_length": 1350,
      "extraction_method": "Direct"
    },
    {
      "page_number": 346,
      "chapter": null,
      "content": "324 \n7. The Game Loop and Real-Time Simulation\n void \nsetPaused(bool isPaused)\n {\n \n \nm_isPaused = isPaused;\n }\n bool \nisPaused() const\n {\n  return \nm_isPaused;\n }\n void \nsetTimeScale(F32 scale)\n {\n \n \nm_timeScale = scale;\n }\n F32 \ngetTimeScale() const\n {\n  return \nm_timeScale;\n }\n void \nsingleStep()\n {\n  if \n(m_isPaused)\n  {\n \n \n \n// Add one ideal frame interval; don’t forget   \n \n \n \n// to scale it by our current time scale!\n   U64 \ndtScaledCycles = secondsToCycles(\n    (\n1.0f/30.0f) * m_timeScale);\n   m_timeCycles \n+= dtScaledCycles;\n  }\n }\n};\n7.6. Multiprocessor Game Loops\nNow that we’ve investigated basic single-threaded game loops and learned \nsome of the ways in which time is commonly measured and manipulated \nin a game engine, let’s turn our att ention to some more complex kinds of \ngame loops. In this section, we’ll explore how game loops have evolved to \ntake advantage of modern multiprocessor hardware. In the following sec-\ntion, we’ll see how networked multiplayer games typically structure their \ngame loops.\nIn 2004, microprocessor manufacturers industry-wide encountered a \nproblem with heat dissipation that prevented them from producing faster \n",
      "content_length": 1176,
      "extraction_method": "Direct"
    },
    {
      "page_number": 347,
      "chapter": null,
      "content": "325 \nCPUs. Moore’s Law , which predicts an approximate doubling in transistor \ncounts every 18 to 24 months, still holds true. But in 2004, its assumed cor-\nrelation with doubling processor speeds was shown to be no longer val-\nid. As a result, microprocessor manufacturers shift ed their focus toward \nmulticore CPUs. (For more information on this trend, see Microsoft ’s “The \nManycore Shift  Whitepaper,” available at htt p://www.microsoft post.com/\nmicrosoft -download/the-manycore-shift -white-paper, and “Multicore Erod-\ning Moore’s Law” by Dean Dauger, available at htt p://www.macresearch.\norg/multicore_eroding_moores_law.) The net eﬀ ect on the soft ware industry \nwas a major shift  toward parallel processing techniques. As a result, mod-\nern game engines running on multicore systems like the Xbox 360 and the \nPLAYSTATION 3 can no longer rely on a single main game loop to service \ntheir subsystems.\nThe shift  from single core to multicore has been painful. Multithreaded \nprogram design is a lot harder than single-threaded programming. Most \ngame companies took on the transformation gradually, by selecting a hand-\nful of engine subsystems for parallelization, and leaving the rest under the \ncontrol of the old, single-threaded main loop. By 2008, most game studios had \ncompleted the transformation for the most part and have embraced parallel-\nism to varying degrees within their engines.\nWe don’t have room here for a full treatise on parallel programming \narchitectures and techniques. (Refer to [20] for an in-depth discussion of \nthis topic.) However, we will take a brief look at some of the most common \nways in which game engines leverage multicore hardware. There are many \ndiﬀ erent soft ware architectures possible—but the goal of all of these archi-\ntectures is to maximize hardware utilization (i.e., to att empt to minimize \nthe amount of time during which any particular hardware thread, core or \nCPU is idle).\n7.6.1. \nMultiprocessor Game Console Architectures\nThe Xbox 360 and the PLAYSTATION 3 are both multiprocessor consoles. In \norder to have a meaningful discussion of parallel soft ware architectures, let’s \ntake a brief look at how these two consoles are structured internally.\n7.6.1.1. \nXbox 360\nThe Xbox 360 consists of three identical PowerPC processor cores. Each core \nhas a dedicated L1 instruction cache and L1 data cache, and the three cores \nshare a single L2 cache. The three cores and the GPU share a uniﬁ ed 512 MB \npool of RAM, which can be used for executable code, application data, tex-\ntures, video RAM—you name it. The Xbox 360 architecture is described in \n7.6. Multiprocessor Game Loops\n",
      "content_length": 2651,
      "extraction_method": "Direct"
    },
    {
      "page_number": 348,
      "chapter": null,
      "content": "326 \n7. The Game Loop and Real-Time Simulation\nMain RAM\n(512 MB)\nPowerPC\nCore 0\nPowerPC\nCore 1\nPowerPC\nCore 2\nL1\nData\nL1\nInstr\nL1\nData\nL1\nInstr\nL1\nData\nL1\nInstr\nShared L2 Cache\nGPU\nFigure 7.4. A simpliﬁ ed view of the Xbox 360 hardware architecture.\na great deal more depth in the PowerPoint presentation entited “Xbox 360 \nSystem Architecture,” by Jeﬀ  Andrews and Nick Baker of the Xbox Semicon-\nductor Technology Group, available at htt p://www.hotchips.org/archives/\nhc17/3_Tue/HC17.S8/HC17.S8T4.pdf. However, the preceding extremely brief \noverview should suﬃ  ce for our purposes. Figure 7.4 shows the Xbox 360’s \narchitecture in highly simpliﬁ ed form.\n7.6.1.2. PLAYSTATION 3\nThe PLAYSTATION 3 hardware makes use of the Cell Broadband Engine \n(CBE) architecture (see Figure 7.5), developed jointly by Sony, Toshiba, and \nIBM. The PS3 takes a radically diﬀ erent approach to the one employed by the \nXbox 360. Instead of three identical processors, it contains a number of diﬀ er-\nent types of processors, each designed for speciﬁ c tasks. And instead of a uni-\nﬁ ed memory architecture, the PS3 divides its RAM into a number of blocks, \neach of which is designed for eﬃ  cient use by certain processing units in the \nsystem. The architecture is described in detail at htt p://www.blachford.info/\ncomputer/Cell/Cell1_v2.html, but the following overview and the diagram \nshown in Figure 7.5 should suﬃ  ce for our purposes.\nThe PS3’s main CPU is called the Power Processing Unit (PPU). It is a \nPowerPC processor, much like the ones found in the Xbox 360. In addition \nto this central processor, the PS3 has six coprocessors known as Synergistic \nProcessing Units (SPUs). These coprocessors are based around the PowerPC \ninstruction set, but they have been streamlined for maximum performance.\nThe GPU on the PS3 has a dedicated 256 MB of video RAM. The PPU has \naccess to 256 MB of system RAM. In addition, each SPU has a dedicated high-\nspeed 256 kB RAM area called its local store (LS). Local store memory performs \nabout as eﬃ  ciently as an L1 cache, making the SPUs blindingly fast.\n",
      "content_length": 2094,
      "extraction_method": "Direct"
    },
    {
      "page_number": 349,
      "chapter": null,
      "content": "327 \nThe SPUs never read directly from main RAM. Instead, a direct memory \naccess (DMA) controller allows blocks of data to be copied back and forth \nbetween system RAM and the SPUs’ local stores. These data transfers happen \nin parallel, so both the PPU and SPUs can be doing useful calculations while \nthey wait for data to arrive.\n7.6.2. SIMD\nAs we saw in Section 4.7, most modern CPUs (including the Xbox 360’s three \nPowerPC processors, and the PS3’s PPU and SPUs) provide a class of instruc-\ntions known as single instruction, multiple data (SIMD). Such instructions can \nperform a particular operation on more than one piece of data simultaneously, \nand as such they represent a ﬁ ne-grained form of hardware parallelism. CPUs \nprovide a number of diﬀ erent SIMD instruction variants, but by far the most \ncommonly-used in games are instructions that operate on four 32-bit ﬂ oating-\npoint values in parallel, because they allow 3D vector and matrix math to be \nperformed four times more quickly than with their single instruction, single \ndata (SISD) counterparts.\nRetroﬁ tt ing existing 3D math code to leverage SIMD instructions can be \ntricky, although the task is much easier if a well-encapsulated 3D math li-\nbrary was used in the original code. For example, if a dot product is calcu-\nlated in long hand everywhere (e.g., ﬂoat d =  a.x * b.x + a.y * b.y \n+ a.z * b.z;), then a very large amount of code will need to be re-writt en. \nHowever, if dot products are calculated by calling a function (e.g., ﬂoat d = \nDot(a, b);), and if vectors are treated largely as black boxes throughout the \ncode base, then retroﬁ tt ing for SIMD can be accomplished by modifying the \n7.6. Multiprocessor Game Loops\nVideo RAM\n(256 MB)\nGPU\nSystem RAM\n(256 MB)\n...\nPPU\nL1\nData\nL1\nInstr\nL2 Cache\nSPU0\nLocal\nStore\n(256 kB)\nSPU1\nLocal\nStore\n(256 kB)\nSPU6\nLocal\nStore\n(256 kB)\nDMA \nController\nDMA Bus\nFigure 7.5. Simpliﬁ ed view of the PS3’s cell broadband architecture.\n",
      "content_length": 1963,
      "extraction_method": "Direct"
    },
    {
      "page_number": 350,
      "chapter": null,
      "content": "328 \n7. The Game Loop and Real-Time Simulation\n3D math library, without having to modify much if any of the calling code \n(except perhaps to ensure alignment of vector data to 16-byte boundaries).\n7.6.3. Fork and Join\nAnother way to utilize multicore or multiprocessor hardware is to adapt di-\nvide-and-conquer algorithms for parallelism. This is oft en called the fork/join \napproach. The basic idea is to divide a unit of work into smaller subunits, dis-\ntribute these workloads onto multiple processing cores or hardware threads \n(fork), and then merge the results once all workloads have been completed \n(join). When applied to the game loop, the fork/join architecture results in a \nmain loop that looks very similar to its single-threaded counterpart, but with \nsome of the major phases of the update loop being parallelized. This architec-\nture is illustrated in Figure 7.6.\nLet’s take a look at a concrete example. Blending animations using linear \ninterpolation (LERP) is an operation that can be done on each joint indepen-\ndently of all other joints within a skeleton (see Section 11.5.2.2). We’ll assume \nthat we want to blend pairs of skeletal poses for ﬁ ve characters, each of which \nhas 100 joints, meaning that we need to process 500 pairs of joint poses.\nTo parallelize this task, we can divide the work into N batches, each con-\ntaining roughly 500/N joint-pose pairs, where N is selected based on the avail-\nMain \nThread\nHID\nUpdate Game \nObjects\nRagdoll Physics\nPost Animation \nGame Object Update\nFork\nJoin\nFork\nJoin\netc.\nPose \nBlending\nPose \nBlending\nPose \nBlending\nSimulate/\nIntegrate\nSimulate/\nIntegrate\nSimulate/\nIntegrate\nFigure 7.6. Fork and join used to parallelize selected CPU-intensive phases of the game loop.\n",
      "content_length": 1741,
      "extraction_method": "Direct"
    },
    {
      "page_number": 351,
      "chapter": null,
      "content": "329 \nable processing resources. (On the Xbox 360, N should probably be 3 or 6, \nbecause the console has three cores with two hardware threads each. On a \nPS3, N might range anywhere from 1 to 6, depending on how many SPUs are \navailable.) We then “fork” (i.e., create) N threads, requesting each one to work \non a diﬀ erent group of pose pairs. The main thread can either continue doing \nsome useful but work that is independent of the animation blending task, or it \ncan go to sleep , waiting on a semaphore that will tell it when all of the worker \nthreads have completed their tasks. Finally, we “join” the individual resultant \njoint poses into a cohesive whole—in this case, by calculating the ﬁ nal global \npose of each of our ﬁ ve skeletons. (The global pose calculation needs access \nto the local poses of all the joints in each skeleton, so it doesn’t parallelize \nwell within a single skeleton. However, we could imagine forking again to \ncalculate the global pose, this time with each thread working on one or more \nwhole skeletons.)\nYou can ﬁ nd sample code illustrating how to fork and join worker \nthreads using Win32 system calls at htt p://msdn.microsoft .com/en-us/library/\nms682516(VS.85).aspx.\n7.6.4. One Thread per Subsystem\nYet another approach to multitasking is to assign particular engine subsys-\ntems to run in separate threads . A master thread controls and synchronizes \nthe operations of these secondary subsystem threads and also continues to \nhandle the lion’s share of the game’s high-level logic (the main game loop ). \nOn a hardware platform with multiple physical CPUs or hardware threads, \nthis design allows these threaded engine subsystems to execute in parallel. \nThis design is well suited to any engine subsystem that performs a relative-\nly isolated function repeatedly, such as a rendering engine, physics simula-\ntion, animation pipeline, or audio engine. The architecture is depicted in \nFigure 7.7.\nThreaded architectures are usually supported by some kind of thread \nlibrary on the target hardware system. On a personal computer running \nWindows, the Win32 thread API is usually used. On a UNIX-based system, \na library like pthreads might be the best choice. On the PLAYSTATION 3, a \nlibrary known as SPURS permits workloads to be run on the six synergistic \nprocessing units (SPUs). SPURS provides two primary ways to run code on \nthe SPUs—the task model and the job model . The task model can be used to \nsegregate engine subsystems into coarse-grained independent units of execu-\ntion that act very much like threads. We’ll discuss the SPURS job model in the \nnext section.\n7.6. Multiprocessor Game Loops\n",
      "content_length": 2655,
      "extraction_method": "Direct"
    },
    {
      "page_number": 352,
      "chapter": null,
      "content": "330 \n7. The Game Loop and Real-Time Simulation\n7.6.5. Jobs\nOne problem with the multithreaded approach is that each thread represents \na relatively coarse-grained chunk of work (e.g., all animation tasks are in one \nthread, all collision and physics tasks in another). This can place restrictions \non how the various processors in the system can be utilized . If one of the \nsubsystem threads has not completed its work, the progress of other threads, \nincluding that of the main game loop, may be blocked.\nAnother way to take advantage of parallel hardware architecture is to \ndivide up the work that is done by the game engine into multiple small, rela-\ntively independent jobs . A job is best thought of as a pairing between a chunk \nof data and a bit of code that operates on that data. When a job is ready to be \nrun, it is placed on a queue, to be picked up and worked on by the next avail-\nable processing unit. This approach is supported on the PLAYSTATION 3 via \nthe SPURS job model. The main game loop runs on the PPU, and the six SPUs \nare used as job processors. Each job’s code and data are sent to an SPU’s local \nstore via a DMA transfer. The SPU processes the job, and then it DMAs its \nresults back to main RAM.\nFigure 7.7.  One thread per major engine subsystem.\n",
      "content_length": 1281,
      "extraction_method": "Direct"
    },
    {
      "page_number": 353,
      "chapter": null,
      "content": "331 \nAs shown in Figure 7.8, the fact that jobs are relatively ﬁ ne-grained and \nindependent of one another helps to maximize processor utilization. It can \nalso reduce or eliminate some of the restrictions placed on the main thread \nin the one-thread-per-subsystem design. This architecture also scales up or \ndown naturally to hardware with any number of processing units (something \nthe one-thread-per-subsystem architecture does not do particularly well).\n7.6.6. Asynchronous Program Design\n When writing or retroﬁ tt ing a game engine to take advantage of multitasking \nhardware, programmers must be careful to design their code in an asynchro-\nnous manner. This means that the results of an operation will usually not be \navailable immediately aft er requesting it, as they would be in a synchronous \ndesign. For example, a game might request that a ray be cast into the world, in \norder to determine whether the player has line-of-sight to an enemy character. \nIn a synchronous design, the ray cast would be done immediately in response \nto the request, and when the ray casting function returned, the results would \nbe available, as shown below.\nFigure 7.8.  In a job architecture, work is broken down into ﬁ ne-grained chunks that can \nbe picked up by any available processor. This can help maximize processor utilization, while \nproviding the main game loop with improved ﬂ exibility.\n7.6. Multiprocessor Game Loops\n",
      "content_length": 1426,
      "extraction_method": "Direct"
    },
    {
      "page_number": 354,
      "chapter": null,
      "content": "332 \n7. The Game Loop and Real-Time Simulation\nwhile (true) // main game loop\n{\n \n// ...\n \n// Cast a ray to see if the player has line of sight   \n \n// to the enemy.\n RayCastResult \nr = castRay(playerPos, enemyPos);\n \n// Now process the results...\n \nif (r.hitSomething() && isEnemy(r.getHitObject()))\n {\n \n \n// Player can see the enemy.\n  // \n...\n }\n \n// ...\n}\nIn an asynchronous design, a ray cast request would be made by calling \na function that simply sets up and enqueues a ray cast job, and then returns \nimmediately. The main thread can continue doing other unrelated work while \nthe job is being processed by another CPU or core. Later, once the job has been \ncompleted, the main thread can pick up the results of the ray cast query and \nprocess them:\nwhile (true) // main game loop\n{\n \n// ...\n \n// Cast a ray to see if the player has line of sight   \n \n// to the enemy.\n RayCastResult \nr;\nrequestRayCast(playerPos, enemyPos, &r);\n \n// Do other unrelated work while we wait for the   \n \n \n// other CPU to perform the ray cast for us.\n// ...\n \n// OK, we can’t do any more useful work. Wait for the\n \n// results of our ray cast job. If the job is   \n \n \n \n// complete, this function will return immediately.\n \n// Otherwise, the main thread will idle until they  \n \n \n// are ready...\nwaitForRayCastResults(&r);\n \n// Process results...\n \nif (r.hitSomething() && isEnemy(r.getHitObject()))\n {\n \n \n// Player can see the enemy.\n  // \n...\n }\n",
      "content_length": 1442,
      "extraction_method": "Direct"
    },
    {
      "page_number": 355,
      "chapter": null,
      "content": "333 \n \n// ...\n}\nIn many instances, asynchronous code can kick oﬀ  a request on one frame, \nand pick up the results on the next. In this case, you may see code that looks \nlike this:\nRayCastResult r;\nbool rayJobPending = false;\nwhile (true) // main game loop\n{\n \n// ...\n \n// Wait for the results of last frame’s ray cast job.\n \nif (rayJobPending)\n {\nwaitForRayCastResults(&r);\n \n \n// Process results...\n  if \n(r.hitSomething() && isEnemy(r.getHitObject()))\n  {\n \n \n \n// Player can see the enemy.\n   // \n...\n  }\n }\n \n// Cast a new ray for next frame.\nrayJobPending = true;\nrequestRayCast(playerPos, enemyPos, &r);\n \n// Do other work...\n// ...\n}\n7.7. Networked Multiplayer Game Loops\nThe game loop of a networked multiplayer game is particularly interesting, \nso we’ll have a brief look at how such loops are structured. We don’t have \nroom here to go into the all of the details of how multiplayer games work. \n(Refer to [3] for an excellent in-depth discussion of the topic.) However, we’ll \nprovide a brief overview of the two most-common multiplayer architectures \nhere, and then look at how these architectures aﬀ ect the structure of the game \nloop.\n7.7. Networked Multiplayer Game Loops\n",
      "content_length": 1191,
      "extraction_method": "Direct"
    },
    {
      "page_number": 356,
      "chapter": null,
      "content": "334 \n7. The Game Loop and Real-Time Simulation\n7.7.1. \nClient-Server\nIn the client-server model, the vast majority of the game’s logic runs on a single \nserver machine. Hence the server’s code closely resembles that of a non-net-\nworked single-player game. Multiple client machines can connect to the server \nin order to take part in the online game. The client is basically a “dumb” ren-\ndering engine that also reads human interface devices and controls the local \nplayer character, but otherwise simply renders whatever the server tells it to \nrender. Great pains are taken in the client code to ensure that the inputs of the \nlocal human player are immediately translated into the actions of the player’s \ncharacter on-screen. This avoids what would otherwise be an extremely an-\nnoying sense of delayed reaction on the part of the player character. But other \nthan this so-called player prediction code, the client is usually not much more \nthan a rendering and audio engine, combined with some networking code.\nThe server may be running on a dedicated machine, in which case we say \nit is running in dedicated server mode. However, the client and server needn’t \nbe on separate machines, and in fact it is quite typical for one of the client ma-\nchines to also be running the server. In fact, in many client-server multiplayer \ngames, the single-player game mode is really just a degenerate multiplayer \ngame, in which there is only one client, and both the client and server are run-\nning on the same machine. This is known as client-on-top-of-server mode.\nThe game loop of a client-server multiplayer game can be implemented \nin a number of diﬀ erent ways. Since the client and server are conceptually \nseparate entities, they could be implemented as entirely separate processes \n(i.e., separate applications). They could also be implemented as two separate \nthreads of execution, within a single process. However, both of these ap-\nproaches require quite a lot of overhead to permit the client and server to \ncommunicate locally, when being run in client-on-top-of-server mode. As a \nresult, a lot of multiplayer games run both client and server in a single thread, \nserviced by a single game loop.\nIt’s important to realize that the client and server code can be updated \nat diﬀ erent rates. For example, in Quake, the server runs at 20 FPS (50 ms per \nframe), while the client typically runs at 60 FPS (16.6 ms per frame). This is \nimplemented by running the main game loop at the faster of the two rates \n(60 FPS) and then servicing the server code once roughly every three frames. \nIn reality, the amount of time that has elapsed since the last server update is \ntracked, and when it reaches or exceeds 50 ms, a server frame is run and the \ntimer is reset. Such a game loop might look something like this:\nF32 dtReal = 1.0f/30.0f; // the real frame delta time\nF32 dtServer = 0.0f;     // the server’s delta time\n",
      "content_length": 2925,
      "extraction_method": "Direct"
    },
    {
      "page_number": 357,
      "chapter": null,
      "content": "335 \nU64 tBegin = readHiResTimer();\nwhile (true) // main game loop\n{\n \n// Run the server at 50 ms intervals.\ndtServer += dtReal;\n \nif (dtServer >= 0.05f)  // 50 ms\n {\n \n  \nrunServerFrame(0.05f);\n \n  \ndtServer -= 0.05f;  // reset for next update\n }\n \n// Run the client at maximum frame rate.\nrunClientFrame(dtReal);\n \n// Read the current time, and calculate an estimate\n \n// of next frame’s real delta time.\n \nU64 tEnd = readHiResTimer();\ndtReal   = (F32)(tEnd – tBegin)\n \n  \n \n     / \n(F32)getHiResTimerFrequency();\n \n// Use tEnd as the new tBegin for next frame.\n \ntBegin = tEnd;\n}\n7.7.2. Peer-to-Peer\nIn the peer-to-peer multiplayer architecture, every machine in the online game \nacts somewhat like a server, and somewhat like a client. One and only one \nmachine has authority over each dynamic object in the game. So each machine \nacts like a server for those objects over which it has authority. For all other ob-\njects in the game world, the machine acts like a client, rendering the objects in \nwhatever state is provided to it by that object’s remote authority.\nThe structure of a peer-to-peer multiplayer game loop is much simpler \nthan a client-server game loop, in that at the top-most level, it looks very much \nlike a single-player game loop. However, the internal details of the code can be \na bit more confusing. In a client-server model, it is usually quite clear which \ncode is running on the server and which code is client-side. But in a peer-to-\npeer architecture, much of the code needs to be set up to handle two possible \ncases: one in which the local machine has authority over the state of an object \nin the game, and one in which the object is just a dumb proxy for a remote \nauthoritative representation. These two modes of operation are oft en imple-\nmented by having two kinds of game objects—a full-ﬂ edged “real” game ob-\n7.7. Networked Multiplayer Game Loops\n",
      "content_length": 1891,
      "extraction_method": "Direct"
    },
    {
      "page_number": 358,
      "chapter": null,
      "content": "336 \n7. The Game Loop and Real-Time Simulation\nject, over which the local machine has authority and a “proxy ” version that \ncontains a minimal subset of the state of the remote object.\nPeer-to-peer architectures are made even more complex because author-\nity over an object sometimes needs to migrate from machine to machine. For \nexample, if one computer drops out of the game, all of the objects over which \nit had authority must be picked up by the other machines in the game. Like-\nwise, when a new machine joins the game, it should ideally take over author-\nity of some game objects from other machines, in order to balance the load. \nThe details are beyond the scope of this book. The key point here is that multi-\nplayer architectures can have profound eﬀ ects on the structure of a game’s \nmain loop.\n7.7.3. Case Study: Quake II\nThe following is an excerpt from the Quake II game loop . The source code for \nQuake, Quake II, and Quake 3 Arena is available on Id Soft ware’s website, htt p://\nwww.idsoft ware.com. As you can see, all of the elements we’ve discussed are \npresent, including the Windows message pump (in the Win32 version of the \ngame), calculation of the real frame delta time , ﬁ xed-time and time-scaled \nmodes of operation, and servicing of both server-side and client-side engine \nsystems.\nint WINAPI WinMain (HINSTANCE hInstance,\n      HINSTANCE \nhPrevInstance,\n      LPSTR \nlpCmdLine, int nCmdShow)\n{\n MSG \n   msg;\n int \n   time, \noldtime, newtime;\n char \n  *cddir;\n \nParseCommandLine (lpCmdLine);\n \nQcommon_Init (argc, argv);\n \noldtime = Sys_Milliseconds ();\n/* main window message loop */\n \nwhile (1)\n {\n \n \n// Windows message pump.\n \n \nwhile (PeekMessage (&msg, NULL, 0, 0, \n   PM_NOREMOVE))\n  {\n \n \n \nif (!GetMessage (&msg, NULL, 0, 0))\n    Com_Quit \n();\n   sys_msg_time \n= msg.time;\n",
      "content_length": 1818,
      "extraction_method": "Direct"
    },
    {
      "page_number": 359,
      "chapter": null,
      "content": "337 \n   TranslateMessage \n(&msg);\n  DispatchMessage \n(&msg);\n  }\n \n \n// Measure real delta time in milliseconds.\n  do\n  {\n \n \n \nnewtime = Sys_Milliseconds ();\n \n \n \ntime = newtime - oldtime;\n \n \n} while (time < 1);\n \n \n// Run a frame of the game.\nQcommon_Frame (time);\n \n \noldtime = newtime;\n }\n \n// never gets here\n \nreturn TRUE;\n}\nvoid Qcommon_Frame (int msec)\n{\n \nchar *s;\n int \n \ntime_before, time_between, time_after;\n \n// [some details omitted...]\n \n// Handle fixed-time mode and time scaling.\n \nif (fixedtime->value)\n \n \nmsec = fixedtime->value;\n \nelse if (timescale->value)\n {\n \n \nmsec *= timescale->value;\n \n \nif (msec < 1)\n   msec \n= 1;\n }\n \n// Service the in-game console.\n do\n {\n \n \ns = Sys_ConsoleInput ();\n  if \n(s)\n   Cbuf_AddText \n(va(\"%s\\n\",s));\n \n} while (s);\n \nCbuf_Execute ();\n7.7. Networked Multiplayer Game Loops\n",
      "content_length": 835,
      "extraction_method": "Direct"
    },
    {
      "page_number": 360,
      "chapter": null,
      "content": "338 \n7. The Game Loop and Real-Time Simulation\n \n// Run a server frame.\nSV_Frame (msec);\n \n// Run a client frame.\nCL_Frame (msec);\n \n// [some details omitted...]\n}\n",
      "content_length": 164,
      "extraction_method": "Direct"
    },
    {
      "page_number": 361,
      "chapter": null,
      "content": "339\n8\nHuman Interface \nDevices (HID)\nG\names are interactive computer simulations, so the human player(s) need \nsome way of providing inputs to the game. All sorts of human interface \ndevices (HID) exist for gaming, including joysticks, joypads, keyboards and \nmice, track balls, the  Wii remote, and specialized input devices like steering \nwheels, ﬁ shing rods, dance pads, and even electric guitars. In this chapter, \nwe’ll investigate how game engines typically read, process, and utilize the \ninputs from human interface devices. We’ll also have a look at how outputs \nfrom these devices provide feedback to the human player.\n8.1. \nTypes of Human Interface Devices\nA wide range of human interface devices are available for gaming purposes. \nConsoles like the Xbox 360 and PS3 come equipped with joypad controllers, as \nshown in Figure 8.1. Nintendo’s Wii console is well known for its unique and \ninnovative WiiMote controller, shown in Figure 8.2. PC games are generally \neither controlled via a keyboard and the mouse, or via a joypad. (Microsoft  \ndesigned the Xbox 360 joypad so that it can be used both on the Xbox 360 and \non Windows/DirectX PC platforms.) As shown in Figure 8.3, arcade machines \nhave one or more built-in controllers, such as a joystick and various butt ons, or \na track ball, a steering wheel, etc. An arcade machine’s input device is usually \n",
      "content_length": 1374,
      "extraction_method": "Direct"
    },
    {
      "page_number": 362,
      "chapter": null,
      "content": "340 \n8. Human Interface Devices (HID)\nFigure 8.1. Standard joypads for the Xbox 360 and PLAYSTATION 3 consoles.\nFigure 8.2. The innovative WiiMote for the Nintendo Wii.\nFigure 8.3. Various custom input devices for the arcade game Mortal Kombat II by Midway.\nFigure 8.4.  Many specialized input devices are available for use with consoles.\n",
      "content_length": 339,
      "extraction_method": "Direct"
    },
    {
      "page_number": 363,
      "chapter": null,
      "content": "341 \n8.2. Interfacing with a HID\nsomewhat customized to the game in question, although input hardware is \noft en re-used among arcade machines produced by the same manufacturer.\nOn console platforms, specialized input devices and adapters are usually \navailable, in addition to the “standard” input device such as the joypad. For \nexample, guitar and drum devices are available for the Guitar Hero series of \ngames, steering wheels can be purchased for driving games, and games like \nDance Dance Revolution use a special dance pad device. Some of these devices \nare shown in Figure 8.4.\nThe Nintendo WiiMote is one of the most ﬂ exible input devices on the \nmarket today. As such, it is oft en adapted to new purposes, rather than re-\nplaced with an entirely new device. For example, Mario Kart Wii comes with \na pastic steering wheel adapter into which the WiiMote can be inserted (see \nFigure 8.5).\n8.2. Interfacing with a HID\nAll human interface devices provide input to the game soft ware, and some \nalso allow the soft ware to provide feedback to the human player via various \nkinds of outputs as well. Game soft ware reads and writes HID inputs and \noutputs in various ways, depending on the speciﬁ c design of the device in \nquestion.\n8.2.1. Polling\nSome simple devices, like game pads and old-school joysticks, are read by \npolling the hardware periodically (usually once per iteration of the main game \nloop). This means explicitly querying the state of the device, either by read-\ning hardware registers directly, reading a memory-mapped I/O port, or via a \nhigher-level soft ware interface (which, in turn, reads the appropriate registers \nor memory-mapped I/O ports). Likewise, outputs might be sent to the HID by \nFigure 8.5. Steering wheel adapter for the Nintendo Wii.\n",
      "content_length": 1784,
      "extraction_method": "Direct"
    },
    {
      "page_number": 364,
      "chapter": null,
      "content": "342 \n8. Human Interface Devices (HID)\nwriting to special registers or memory-mapped I/O addresses, or via a higher-\nlevel API that does our dirty work for us.\nMicrosoft ’s XInput API, for use with Xbox 360 game pads on both the \nXbox 360 and Windows PC platforms, is a good example of a simple polling \nmechanism. Every frame, the game calls the function XInputGetState(). \nThis function communicates with the hardware and/or drivers, reads the data \nin the appropriate way, and packages it all up for convenient use by the soft -\nware. It returns a pointer to an XINPUT_STATE struct, which in turn contains \nan embedded instance of a struct called XINPUT_GAMEPAD. This struct con-\ntains the current states of all of the controls (butt ons, thumb sticks, and trig-\ngers) on the device.\n8.2.2. Interrupts\nSome HIDs only send data to the game engine when the state of the controller \nchanges in some way. For example, a mouse spends a lot of its time just sitt ing \nstill on the mouse pad. There’s no reason to send a continuous stream of data \nbetween the mouse and the computer when the mouse isn’t moving—we need \nonly transmit information when it moves, or a butt on is pressed or released.\nThis kind of device usually communicates with the host computer via \nhardware interrupts . An interrupt is an electronic signal generated by the hard-\nware, which causes the CPU to temporarily suspend execution of the main \nprogram and run a small chunk of code called an interrupt service routine (ISR). \nInterrupts are used for all sorts of things, but in the case of a HID, the ISR code \nwill probably read the state of the device, store it oﬀ  for later processing, and \nthen relinquish the CPU back to the main program. The game engine can pick \nup the data the next time it is convenient to do so.\n8.2.3. Wireless Devices\nThe inputs and outputs of a Bluetooth device, like the WiiMote, the \nDualShock 3 and the Xbox 360 wireless controller, cannot be read and writ-\nten by simply accessing registers or memory-mapped I/O ports. Instead, the \nsoft ware must “talk” to the device via the Bluetooth protocol. The soft ware \ncan request the HID to send input data (such as the states of its butt ons) back \nto the host, or it can send output data (such as rumble sett ings or a stream of \naudio data) to the device. This communication is oft en handled by a thread \nseparate from the game engine’s main loop, or at least encapsulated behind a \nrelatively simple interface that can be called from the main loop. So from the \npoint of view of the game programmer, the state of a Bluetooth device can be \nmade to look prett y much indistinguishable from a traditional polled device.\n",
      "content_length": 2675,
      "extraction_method": "Direct"
    },
    {
      "page_number": 365,
      "chapter": null,
      "content": "343 \n8.3. Types of Inputs\n8.3. Types of Inputs\nAlthough human interface devices for games vary widely in terms of form \nfactor and layout, most of the inputs they provide fall into one of a small num-\nber of categories. We’ll investigate each category in depth below.\n8.3.1. Digital Buttons\nAlmost every HID has at least a few digital butt ons . These are butt ons that can \nonly be in one of two states: pressed and not pressed. Game programmers oft en \nrefer to a pressed butt on as being down and a non-pressed butt on as being up.\nElectrical engineers speak of a circuit containing a switch as being closed \n(meaning electricity is ﬂ owing through the circuit) or open (no electricity is \nﬂ owing—the circuit has inﬁ nite resistance). Whether closed corresponds to \npressed or not pressed depends on the hardware. If the switch is normally open, \nthen when it is not pressed (up), the circuit is open, and when it is pressed \n(down), the circuit is closed. If the switch is normally closed, the reverse is true—\nthe act of pressing the butt on opens the circuit.\nIn soft ware, the state of a digital butt on (pressed or not pressed) is usually \nrepresented by a single bit. It’s common for 0 to represent not pressed (up) \nand 1 to represent pressed (down). But again, depending on the nature of the \ncircuitry, and the decisions made by the programmers who wrote the device \ndriver, the sense of these values might be reversed.\nIt is quite common for the states of all of the butt ons on a device to be \npacked into a single unsigned integer value. For example, in Microsoft ’s \nXInput API, the state of the Xbox 360 joypad is returned in a struct called \nXINPUT_GAMEPAD, shown below.\ntypedef struct _XINPUT_GAMEPAD {\n    WORD\nwButtons;\n    BYTE  bLeftTrigger;\n    BYTE  bRightTrigger;\n    SHORT sThumbLX;\n    SHORT sThumbLY;\n    SHORT sThumbRX;\n    SHORT sThumbRY;\n} XINPUT_GAMEPAD;\nThis struct contains a 16-bit unsigned integer (WORD) variable named \nwButtons that holds the state of all butt ons. The following masks deﬁ ne \n",
      "content_length": 2034,
      "extraction_method": "Direct"
    },
    {
      "page_number": 366,
      "chapter": null,
      "content": "344 \n8. Human Interface Devices (HID)\nwhich physical butt on corresponds to each bit in the word. (Note that bits 10 \nand 11 are unused.)\n#define XINPUT_GAMEPAD_DPAD_UP          0x0001 // bit 0\n#define XINPUT_GAMEPAD_DPAD_DOWN        0x0002 // bit 1\n#define XINPUT_GAMEPAD_DPAD_LEFT        0x0004 // bit 2\n#define XINPUT_GAMEPAD_DPAD_RIGHT       0x0008 // bit 3\n#define XINPUT_GAMEPAD_START            0x0010 // bit 4\n#define XINPUT_GAMEPAD_BACK             0x0020 // bit 5\n#define XINPUT_GAMEPAD_LEFT_THUMB       0x0040 // bit 6\n#define XINPUT_GAMEPAD_RIGHT_THUMB      0x0080 // bit 7\n#define XINPUT_GAMEPAD_LEFT_SHOULDER    0x0100 // bit 8\n#define XINPUT_GAMEPAD_RIGHT_SHOULDER   0x0200 // bit 9\n#define XINPUT_GAMEPAD_A                0x1000 // bit 12\n#define XINPUT_GAMEPAD_B                0x2000 // bit 13\n#define XINPUT_GAMEPAD_X                0x4000 // bit 14\n#define XINPUT_GAMEPAD_Y                0x8000 // bit 15\nAn individual butt on’s state can be read by masking the wButtons word \nwith the appropriate bit mask via C/C++’s bitwise AND operator (&) and then \nchecking if the result is non-zero. For example, to determine if the A butt on is \npressed (down), we would write:\nbool IsButtonADown(const XINPUT_GAMEPAD& pad)\n{\n \n// Mask off all bits but bit 12 (the A button).\n \nreturn ((pad.wButtons & XINPUT_GAMEPAD_A) != 0);\n}\n8.3.2. Analog Axes and Buttons\nAn analog input is one that can take on a range of values (rather than just 0 \nor 1). These kinds of inputs are oft en used to represent the degree to which \na trigger is pressed, or the two-dimensional position of a joystick (which is \nrepresented using two analog inputs, one for the x-axis and one for the y-axis, \n",
      "content_length": 1690,
      "extraction_method": "Direct"
    },
    {
      "page_number": 367,
      "chapter": null,
      "content": "345 \nas shown in Figure 8.6). Because of this common usage, analog inputs are \nsometimes called analog axes , or just axes.\nOn some devices, certain butt ons are analog as well, meaning that the \ngame can actually detect how hard the player is pressing on them. However, \nthe signals produced by analog butt ons are usually too noisy to be particu-\nlarly usable. I have yet to see a game that uses analog butt on inputs eﬀ ectively \n(although some may very well exist!)\nStrictly speaking, analog inputs are not really analog by the time they \nmake it to the game engine. An analog input signal is usually digitized, mean-\ning it is quantized and represented using an integer in soft ware. For example, \nan analog input might range from –32,768 to 32,767 if represented by a 16-bit \nsigned integer. Sometimes analog inputs are converted to ﬂ oating-point—\nthe values might range from –1 to 1, for instance. But as we know from Sec-\ntion 3.2.1.3, ﬂ oating-point numbers are really just quantized digital values as \nwell.\nReviewing the deﬁ nition of XINPUT_GAMEPAD (repeated below), we can \nsee that Microsoft  chose to represent the deﬂ ections of the left  and right thumb \nsticks on the Xbox 360 gamepad using 16-bit signed integers (sThumbLX\nand sThumbLY for the left  stick and sThumbRX and sThumbRY for the right). \nHence, these values range from –32,768 (left  or down) to 32,767 (right or up). \nHowever, to represent the positions of the left  and right shoulder triggers, \nMicrosoft  chose to use 8-bit unsigned integers (bLeftTrigger and bRight-\nTrigger respectively). These input values range from 0 (not pressed) to 255 \n(fully pressed). Diﬀ erent game machines use diﬀ erent digital representions \nfor their analog axes.\ntypedef struct _XINPUT_GAMEPAD {\n    WORD  wButtons;\nx\ny\n(1, 1)\n(–1, –1)\n(0.1, 0.3)\nFigure 8.6.  Two analog inputs can be used to represent the x and y deﬂ ection of a joystick.\n8.3. Types of Inputs\n",
      "content_length": 1930,
      "extraction_method": "Direct"
    },
    {
      "page_number": 368,
      "chapter": null,
      "content": "346 \n8. Human Interface Devices (HID)\n// 8-bit unsigned\n    BYTE\nbLeftTrigger;\n    BYTE\nbRightTrigger;\n// 16-bit signed\n    SHORT sThumbLX;\n    SHORT sThumbLY;\n    SHORT sThumbRX;\n    SHORT sThumbRY;\n} XINPUT_GAMEPAD;\n8.3.3. Relative Axes\nThe position of an analog butt on, trigger, joystick, or thumb stick is absolute, \nmeaning that there is a clear understanding of where zero lies. However, the \ninputs of some devices are relative . For these devices, there is no clear location \nat which the input value should be zero. Instead, a zero input indicates that \nthe position of the device has not changed, while non-zero values represent \na delta from the last time the input value was read. Examples include mice, \nmouse wheels, and track balls.\n8.3.4. Accelerometers\nThe PLAYSTATION 3’s Sixaxis and DualShock 3 joypads, and the Nintendo \nWiiMote , all contain acceleration sensors (accelerometers ). These devices can \ndetect acceleration along the three principle axes (x, y, and z), as shown in Fig-\nure 8.7. These are relative analog inputs, much like a mouse’s two-dimensional \naxes. When the controller is not accelerating these inputs are zero, but when \nthe controller is accelerating, they measure the acceleration up to ±3 g along \neach axis, quantized into three signed 8-bit integers, one for each of x, y, and z.\nx\ny\nz\nFigure 8.7. Accelerometer axes for the WiiMote.\n8.3.5. 3D Orientation with the WiiMote or Sixaxis\nSome Wii and PS3 games make use of the three accelerometers in the WiiMote \nor Sixaxis joypad to estimate the orientation of the controller in the player’s \n",
      "content_length": 1590,
      "extraction_method": "Direct"
    },
    {
      "page_number": 369,
      "chapter": null,
      "content": "347 \nhand. For example, in Super Mario Galaxy, Mario hops onto a large ball and \nrolls it around with his feet. To control Mario in this mode, the WiiMote is held \nwith the IR sensor facing the ceiling. Tilting the WiiMote left , right, forward, \nor back causes the ball to accelerate in the corresponding direction.\nA trio of accelerometers can be used to detect the orientation of the \nWiiMote or Sixaxis joypad, because of the fact that we are playing these games \non the surface of the Earth where there is a constant downward acceleration \ndue to gravity of 1g (≈ 9.8 m/s2). If the controller is held perfectly level, with \nthe IR sensor pointing toward your TV set, the vertical (z) acceleration should \nbe approximately –1 g.\nIf the controller is held upright, with the IR sensor pointing toward the \nceiling, we would expect to see a 0 g acceleration on the z sensor, and +1 g \non the y sensor (because it is now experiencing the full gravitational eﬀ ect). \nHolding the WiiMote at a 45-degree angle should produce roughly sin(45°) = \ncos(45°) = 0.707 g on both the y and z inputs. Once we’ve calibrated the accel-\nerometer inputs to ﬁ nd the zero points along each axis, we can calculate pitch, \nyaw, and roll easily, using inverse sine and cosine operations.\nTwo caveats here: First, if the person holding the WiiMote is not hold-\ning it still, the accelerometer inputs will include this acceleration in their val-\nues, invalidating our math. Second, the z-axis of the accelerometer has been \ncalibrated to account for gravity, but the other two axes have not. This means \nthat the z-axis has less precision available for detecting orientation. Many Wii \ngames request that the user hold the WiiMote in a non-standard orientation, \nsuch as with the butt ons facing the player’s chest, or with the IR sensor point-\ning toward the ceiling. This maximizes the precision of the orientation read-\ning, by placing the x- or y-accelerometer axis in line with gravity, instead of the \ngravity-calibrated z- axis. For more information on this topic, see htt p://druid.\ncaughq.org/presentations/turbo/Wiimote-Hacking.pdf and htt p://www.wiili.\norg/index.php/Motion_analysis.\n8.3.6. Cameras\nThe WiiMote has a unique feature not found on any other standard console \nHID—an infrared (IR) sensor. This sensor is essentially a low-resolution cam-\nera that records a two-dimension infrared image of whatever the WiiMote is \npointed at. The Wii comes with a “sensor bar” that sits on top of your televi-\nsion set and contains two infrared light emitt ing diodes (LEDs). In the image \nrecorded by the IR camera, these LEDs appear as two bright dots on an oth-\nerwise dark background. Image processing soft ware in the WiiMote analyzes \nthe image and isolates the location and size of the two dots. (Actually, it can \ndetect and transmit the locations and sizes of up to four dots.) This position \n8.3. Types of Inputs\n",
      "content_length": 2910,
      "extraction_method": "Direct"
    },
    {
      "page_number": 370,
      "chapter": null,
      "content": "348 \n8. Human Interface Devices (HID)\nand size information can be read by the console via a Bluetooth wireless con-\nnection.\nThe position and orientation of the line segment formed by the two dots \ncan be used to determine the pitch, yaw, and roll of the WiiMote (as long as it \nis being pointed toward the sensor bar). By looking at the separation between \nthe dots, soft ware can also determine how close or far away the WiiMote is \nfrom the TV. Some soft ware also makes use of the sizes of the dots. This is il-\nlustrated in Figure 8.8.\nAnother popular camera device is Sony’s EyeToy for the PlayStation line \nof consoles, shown in Figure 8.9. This device is basically a high quality color \ncamera, which can be used for a wide range of applications. It can be used \nfor simple video conferencing, like any web cam. It could also conceivably be \nused much like the WiiMote’s IR camera, for position, orientation, and depth \nsensing. The gamut of possibilities for these kinds of advanced input devices \nhas only begun to be tapped by the gaming community.\n8.4. Types of Outputs\nHuman interface devices are primarily used to transmit inputs from the play-\ner to the game soft ware. However, some HIDs can also provide feedback to \nthe human player via various kinds of outputs.\n8.4.1. Rumble\nGame pads like the PlayStation’s DualShock line of controllers and the Xbox \nand Xbox 360 controllers have a rumble feature. This allows the controller to \nvibrate in the player’s hands, simulating the turbulence or impacts that the \nFigure 8.8.  The Wii sensor bar houses two infrared LEDs which produce two bright spots on \nthe image recorded by the WiiMote’s IR camera.\nFigure 8.9. Sony’s Eye-\nToy for the PlaySta-\ntion3.\n",
      "content_length": 1720,
      "extraction_method": "Direct"
    },
    {
      "page_number": 371,
      "chapter": null,
      "content": "349 \n8.4. Types of Outputs\ncharacter in the game world might be experiencing. Vibrations are usually \nproduced by one or more motors, each of which rotates a slightly unbalanced \nweight at various speeds. The game can turn these motors on and oﬀ , and con-\ntrol their speeds to produce diﬀ erent tactile eﬀ ects in the player’s hands.\n8.4.2. Force-Feedback\nForce feedback is a technique in which an actuator on the HID is driven by \na motor in order to slightly resist the motion the human operator is trying to \nimpart to it. It is common in arcade driving games, where the steering wheel \nresists the player’s att empt to turn it, simulating diﬃ  cult driving conditions or \ntight turns. As with rumble, the game soft ware can typically turn the motor(s) \non and oﬀ , and can also control the strength and direction of the forces ap-\nplied to the actuator.\n8.4.3. Audio\nAudio is usually a stand-alone engine system. However, some HIDs provide \noutputs that can be utilized by the audio system. For example, the WiiMote \ncontains a small, low-quality speaker. The Xbox 360 controller has a headset \njack and can be used just like any USB audio device for both output (speak-\ners) and input (microphone). One common use of USB headsets is for multi-\nplayer games, in which human players can communicate with one another via \na voice over IP (VOIP) connection.\n8.4.4. Other Inputs and Outputs\nHuman interface devices may of course support many other kinds of inputs \nand outputs. On some older consoles like the Sega Dreamcast, the memory card \nslots were located on the game pad. The Xbox 360 game pad, the Sixaxis and \nDualShock 3, and the WiiMote all have four LEDs which can be illuminated by \ngame soft ware if desired. And of course specialized devices like musical instru-\nments, dance pads, etc. have their own particular kinds of inputs and outputs.\nInnovation is actively taking place in the ﬁ eld of human interfaces. Some \nof the most interesting areas today are gestural interfaces and thought-con-\ntrolled devices. We can certainly expect more innovation from console and \nHID manufacturers in years to come.\n8.5. Game Engine HID Systems\nMost game engines don’t use “raw” HID inputs directly. The data is usually \nmassaged in various ways to ensure that the inputs coming from the HID \n",
      "content_length": 2299,
      "extraction_method": "Direct"
    },
    {
      "page_number": 372,
      "chapter": null,
      "content": "350 \n8. Human Interface Devices (HID)\ntranslate into smooth, pleasing, intuitive behaviors in-game. In addition, most \nengines introduce at least one additional level of indirection between the HID \nand the game in order to abstract HID inputs in various ways. For example, a \nbutt on-mapping table might be used to translate raw butt on inputs into logi-\ncal game actions, so that human players can re-assign the butt ons’ functions \nas they see ﬁ t. In this section, we’ll outline the typical requirements of a game \nengine HID system and then explore each one in some depth.\n8.5.1. Typical Requirements\n A game engine’s HID system usually provides some or all of the following \nfeatures:\nz dead zones,\nz analog signal ﬁ ltering,\nz event detection (e.g., butt on up, butt on down),\nz detection of butt on sequences and multibutt on combinations (known as \nchords),\nz gesture detection,\nz management of multiple HIDs for multiple players,\nz multiplatform HID support,\nz controller input re-mapping,\nz context-sensitive inputs,\nz the ability to temporarily disable certain inputs.\n8.5.2. Dead Zone\n A joystick, thumb stick, shoulder trigger, or any other analog axis produces \ninput values that range between a predeﬁ ned minimum and maximum value, \nwhich we’ll call Imin and Imax. When the control is not being touched, we would \nexpect it to produce a steady and clear “undisturbed” value, which we’ll call \nI0. The undisturbed value is usually numerically equal to zero, and it either \nlies half-way between Imin and Imax for a centered, two-way control like a joy-\nstick axis, or it coincides with Imin for a one-way control like a trigger.\nUnfortunately, because HIDs are analog devices by nature, the voltage pro-\nduced by the device is noisy, and the actual inputs we observe may ﬂ uctuate \nslightly around I0. The most common solution to this problem is to introduce a \nsmall dead zone around I0. The dead zone might be deﬁ ned as [I0 – δ , I0 + δ] for \na joy stick, or [I0  , I0 + δ] for a trigger. Any input values that are within the dead \nzone are simply clamped to I0. The dead zone must be wide enough to account \n",
      "content_length": 2128,
      "extraction_method": "Direct"
    },
    {
      "page_number": 373,
      "chapter": null,
      "content": "351 \nfor the noisiest inputs generated by an undisturbed control, but small enough \nnot to interfere with the player’s sense of the HID’s responsiveness.\n8.5.3. Analog Signal Filtering\n Signal noise is a problem even when the controls are not within their dead \nzones. This noise can sometimes cause the in-game behaviors controlled by \nthe HID to appear jerky or unnatural. For this reason, many games ﬁ lter the \nraw inputs coming from the HID. A noise signal is usually of a high-frequency, \nrelative to the signal produced by the human player. Therefore, one solution \nis to pass the raw input data through a simple low-pass ﬁ lter , prior to it being \nused by the game.\nA discrete ﬁ rst-order low-pass ﬁ lter can be implemented by combining \nthe current unﬁ ltered input value with last frame’s ﬁ ltered input. If we denote \nthe sequence of unﬁ ltered inputs by the time-varying function u(t) and the \nﬁ ltered inputs by f(t), where t denotes time, then we can write\n \n \n(8.1)\nwhere the parameter a is determined by the frame duration Δt and a ﬁ ltering \nconstant RC (which is just the product of the resistance and the capacitance in \na traditional analog RC low-pass ﬁ lter circuit):\n \n \n(8.2)\nThis can be implemented trivially in C or C++ as follows, where it is assumed \nthe calling code will keep track of last frame’s ﬁ ltered input for use on the \nsubsequent frame. For more information, see htt p://en.wikipedia.org/wiki/\nLow-pass_ﬁ lter.\nF32 lowPassFilter(F32 unfilteredInput,\n                  F32 lastFramesFilteredInput,\n                  F32 rc, F32 dt)\n{\n \nF32 a = dt / (rc + dt);\n \nreturn (1 – a) * lastFramesFilteredInput\n         + a * unfilteredInput;\n}\nAnother way to ﬁ lter HID input data is to calculate a simple moving av-\nerage . For example, if we wish to average the input data over a 3/30 second \n(3 frame) interval, we simply store the raw input values in a 3-element circular \n8.5. Game Engine HID Systems\n( )\n(1\n) (\n)\n( ),\nf\n=\n−\n−Δ +\nt\nf t\nt\nt\na\nau\n.\nt\na\nRC\nt\nΔ\n=\n+Δ\n",
      "content_length": 2001,
      "extraction_method": "Direct"
    },
    {
      "page_number": 374,
      "chapter": null,
      "content": "352 \n8. Human Interface Devices (HID)\nbuﬀ er. The ﬁ ltered input value is then the sum of the values in this array at \nany moment, divided by 3. There are a few minor details to account for when \nimplementing such a ﬁ lter. For example, we need to properly handle the ﬁ rst \ntwo frames of input, during which the 3-element array has not yet been ﬁ lled \nwith valid data. However, the implementation is not particularly complicated. \nThe code below shows one way to properly implement an N-element moving \naverage.\ntemplate< typename TYPE, int SIZE >\nclass MovingAverage\n{\n TYPE \n m_samples[SIZE];\n TYPE \n m_sum;\n U32 \n  m_curSample;\n U32 \n  m_sampleCount;\npublic:\n \nMovingAverage() :\n \n  m_sum(static_cast<TYPE>(0)),\n \n  m_curSample(0),\n \n  m_sampleCount(0)\n {\n }\n void \naddSample(TYPE data)\n {\n \n  \nif (m_sampleCount == SIZE)\n \n  {\n \n  \n \nm_sum -= m_samples[m_curSample];\n \n  }\n \n  else\n \n  {\n  \n \n ++m_sampleCount;\n \n  }\n \n  \nm_samples[m_curSample] = data;\n \n  \nm_sum += data;\n \n  ++m_curSample;\n \n  \nif (m_curSample >= SIZE)\n \n  {\n \n  \n \nm_curSample = 0;\n \n  }\n \n }\n F32 \ngetCurrentAverage() const\n {\n",
      "content_length": 1104,
      "extraction_method": "Direct"
    },
    {
      "page_number": 375,
      "chapter": null,
      "content": "353 \n \n  \nif (m_sampleCount != 0)\n \n  {\n  \n \n return \nstatic_cast<F32>(m_sum)\n \n  \n \n  / static_cast<F32>(m_sampleCount);\n \n  }\n \n  return 0.0f;\n }\n};\n8.5.4. Detecting Input Events\n The low-level HID interface typically provides the game with the current \nstates of the device’s various inputs. However, games are oft en interested \nin detecting events, such as changes in state, rather than just inspecting the \ncurrent state each frame. The most common HID events are probably butt on \ndown (pressed) and butt on up (released), but of course we can detect other \nkinds of events as well.\n8.5.4.1. Button Up and Button Down\nLet’s assume for the moment that our butt ons’ input bits are 0 when not pressed \nand 1 when pressed. The easiest way to detect a change in butt on state is to \nkeep track of the butt ons’ state bits as observed last frame and compare them \nto the state bits observed this frame. If they diﬀ er, we know an event occurred. \nThe current state of each butt on tells us whether the event is a butt on-up or a \nbutt on-down.\nWe can use simple bit-wise operators to detect butt on-down and but-\nton-up events. Given a 32-bit word buttonStates, containing the current \nstate bits of up to 32 butt ons, we want to generate two new 32-bit words: \none for butt on-down events which we’ll call buttonDowns and one for \nbutt on-up events which we’ll call buttonUps. In both cases, the bit corre-\nsponding to each butt on will be 0 if the event has not occurred this frame \nand 1 if it has. To implement this, we also need last frame’s butt on states, \nprevButtonStates.\nThe exclusive OR (XOR) operator produces a 0 if its two inputs are iden-\ntical and a 1 if they diﬀ er. So if we apply the XOR operator to the previous \nand current butt on state words, we’ll get 1’s only for butt ons whose states \nhave changed between last frame and this frame. To determine whether the \nevent is a butt on-up or a butt on-down, we need to look at the current state \nof each butt on. Any butt on whose state has changed that is currently down \ngenerates a butt on-down event, and vice-versa for butt on-up events. The fol-\nlowing code applies these ideas in order to generate our two butt on event \nwords:\n8.5. Game Engine HID Systems\n",
      "content_length": 2236,
      "extraction_method": "Direct"
    },
    {
      "page_number": 376,
      "chapter": null,
      "content": "354 \n8. Human Interface Devices (HID)\nclass ButtonState\n{\n \nU32 m_buttonStates;  \n// current frame’s button   \n \n  \n       // \nstates\n \nU32 m_prevButtonStates; // previous frame’s states\n U32 \nm_buttonDowns;  \n// 1 = button pressed this  \n \n  \n       // \nframe\n U32 \nm_buttonUps;  \n \n// 1 = button released this   \n  \n       // \nframe\n void \nDetectButtonUpDownEvents()\n {\n \n  // Assuming that m_buttonStates and \n \n  // m_prevButtonStates are valid, generate \n \n  // m_buttonDowns and m_buttonUps.\n \n  // First determine which bits have changed via  \n \n \n  // XOR.\n \n  U32 buttonChanges = m_buttonStates \n  \n       \n   \n^ m_prevButtonStates;\n \n  // Now use AND to mask off only the bits that are  \n \n  // DOWN. \n \n  \nm_buttonDowns = buttonChanges & m_buttonStates;\n \n  // Use AND-NOT to mask off only the bits that are   \n \n  // UP.\n \n  \nm_buttonUps = buttonChanges & (~m_buttonStates);\n }\n \n// ...\n};\n8.5.4.2. Chords\nA chord is a group of butt ons that, when pressed at the same time, produce a \nunique behavior in the game. Here are a few examples:\nz Super Mario Galaxy’s start-up screen requires you to press the A and B \nbutt ons on the WiiMote together in order to start a new game.\nz Pressing the 1 and 2 butt ons on the WiiMote at the same time put it into \nBluetooth discovery mode (no matt er what game you’re playing).\nz The “grapple” move in many ﬁ ghting games is triggered by a two-but-\nton combination.\n",
      "content_length": 1417,
      "extraction_method": "Direct"
    },
    {
      "page_number": 377,
      "chapter": null,
      "content": "355 \nz For development purposes, holding down both the left  and right trig-\ngers on the DualShock 3 in Uncharted: Drake’s Fortune allows the player \ncharacter to ﬂ y anywhere in the game world, with collisions turned oﬀ . \n(Sorry, this doesn’t work in the shipping game!) Many games have a \ncheat like this to make development easier. (It may or may not be trig-\ngered by a chord, of course.) It is called no-clip mode in the Quake engine, \nbecause the character’s collision volume is not clipped to the valid play-\nable area of the world. Other engines use diﬀ erent terminology.\nDetecting chords is quite simple in principle: We merely watch the states \nof two or more butt ons and only perform the requested operation when all of \nthem are down.\nThere are some subtleties to account for, however. For one thing, if the \nchord includes a butt on or butt ons that have other purposes in the game, we \nmust take care not to perform both the actions of the individual butt ons and \nthe action of chord when it is pressed. This is usually done by including a \ncheck that the other butt ons in the chord are not down when detecting the \nindividual butt on-presses.\nAnother ﬂ y in the ointment is that humans aren’t perfect, and they oft en \npress one or more of the butt ons in the chord slightly earlier than the rest. So our \nchord-detection code must be robust to the possibility that we’ll observe one or \nmore individual butt ons on frame i and the rest of the chord on frame i + 1 (or \neven multiple frames later). There are a number of ways to handle this:\nz You can design your butt on inputs such that a chord always does \nthe actions of the individual butt ons plus some additional action. For \nexample, if pressing L1 ﬁ res the primary weapon and L2 lobs a grenade, \nperhaps the L1  +  L2 chord could ﬁ re the primary weapon, lob a grenade, \nand send out an energy wave that doubles the damage done by these \nweapons. That way, whether or not the individual butt ons are detected \nbefore the chord or not, the behavior will be identical from the point of \nview of the player.\nz You can introduce a delay between when an individual butt on-down \nevent is seen and when it “counts” as a valid game event. During the \ndelay period (say 2 or 3 frames), if a chord is detected, then it takes \nprecedence over the individual butt on-down events. This gives the \nhuman player some leeway in performing the chord.\nz You can detect the chord when the butt ons are pressed, but wait to \ntrigger the eﬀ ect until the butt ons are released again.\nz You can begin the single-butt on move immediately and allow it to be \npreempted by the chord move.\n8.5. Game Engine HID Systems\n",
      "content_length": 2674,
      "extraction_method": "Direct"
    },
    {
      "page_number": 378,
      "chapter": null,
      "content": "356 \n8. Human Interface Devices (HID)\n8.5.4.3. Sequences and Gesture Detection\n The idea of introducing a delay between when a butt on actually goes down \nand when it really “counts” as down is a special case of gesture detection. A \ngesture is a sequence of actions performed via a HID by the human player \nover a period of time. For example, in a ﬁ ghting game or brawler, we might \nwant to detect a sequence of butt on presses, such as A-B-A. We can extend this \nidea to non-butt on inputs as well. For example, A-B-A-Left -Right-Left , where \nthe latt er three actions are side-to-side motions of one of the thumb sticks on \nthe game pad. Usually a sequence or gesture is only considered to be valid if \nit is performed within some maximum time-frame. So a rapid A-B-A within a \nquarter of a second might “count,” but a slow A-B-A performed over a second \nor two might not.\nGesture detection is generally implemented by keeping a brief history of \nthe HID actions performed by the player. When the ﬁ rst component of the \ngesture is detected, it is stored in the history buﬀ er, along with a time stamp \nindicating when it occurred. As each subsequent component is detected, the \ntime between it and the previous component is checked. If it is within the \nallowable time window, it too is added to the history buﬀ er. If the entire se-\nquence is completed within the allott ed time (i.e., the history buﬀ er is ﬁ lled), \nan event is generated telling the rest of the game engine that the gesture \nhas occurred. However, if any non-valid intervening inputs are detected, or \nif any component of the gesture occurs outside of its valid time window, \nthe entire history buﬀ er is reset, and the player must start the gesture over \nagain.\nLet’s look at three concrete examples, so we can really understand how \nthis works.\nRapid Button Tapping\nMany games require the user to tap a butt on rapidly in order to perform an ac-\ntion. The frequency of the butt on presses may or may not translate into some \nquantity in the game, such as the speed with which the player character runs \nor performs some other action. The frequency is usually also used to deﬁ ne \nthe validity of the gesture—if the frequency drops below some minimum val-\nue, the gesture is no longer considered valid.\nWe can detect the frequency of a butt on press by simply keeping track of \nthe last time we saw a butt on-down event for the butt on in question. We’ll call \nthis Tlast  . The frequency f is then just the inverse of the time interval between \npresses (ΔT = Tcur – Tlast  and f = 1/ΔT). Every time we detect a new butt on-down \nevent, we calculate a new frequency f. To implement a minimum valid fre-\nquency, we simply check f against the minimum frequency fmin (or we can just \n",
      "content_length": 2758,
      "extraction_method": "Direct"
    },
    {
      "page_number": 379,
      "chapter": null,
      "content": "357 \ncheck ΔT against the maximum period ΔTmax = 1/fmin directly). If this threshold \nis satisiﬁ ed, we update the value of Tlast   , and the gesture is considered to be \non-going. If the threshold is not satisﬁ ed, we simply don’t update Tlast  . The \ngesture will be considered invalid until a new pair of rapid-enough butt on-\ndown events occurs. This is illustrated by the following pseudocode:\nclass ButtonTapDetector\n{\n U32 \n \nm_buttonMask; // which button to observe (bit   \n        \n// mask)\n F32 \n \nm_dtMax;      // max allowed time between   \n \n        \n// presses\n F32 \n \nm_tLast;      // last button-down event, in  \n \n        \n// seconds\npublic:\n \n// Construct an object that detects rapid tapping of   \n \n// the given button (identified by an index).\nButtonTapDetector(U32 buttonId, F32 dtMax) :\n \n \nm_buttonMask(1U << buttonId),\n  m_dtMax(dtMax),\n \n \nm_tLast(CurrentTime() – dtMax) // start out  \n \n \n            \n// invalid\n {\n }\n \n// Call this at any time to query whether or not the\n \n// gesture is currently being performed.\n void \nIsGestureValid() const\n {\n \n \nF32 t = CurrentTime();\n \n \nF32 dt = t – m_tLast;\n \n \nreturn (dt < m_dtMax);\n }\n \n// Call this once per frame.\n void \nUpdate()\n {\n  if \n(ButtonsJustWentDown(m_buttonMask))\n  {\n   m_tLast \n= CurrentTime();\n  }\n }\n};\nIn the above code excerpt, we assume that each butt on is identiﬁ ed by a \nunique id. The id is really just an index, ranging from 0 to N – 1 (where N is \nthe number of butt ons on the HID in question). We convert the butt on id to a \n8.5. Game Engine HID Systems\n",
      "content_length": 1559,
      "extraction_method": "Direct"
    },
    {
      "page_number": 380,
      "chapter": null,
      "content": "358 \n8. Human Interface Devices (HID)\nbit mask by shift ing an unsigned 1 bit to the left  by an amount equaling the \nbutt on’s index (1U << buttonId ). The function ButtonsJustWentDown()\nreturns a non-zero value if any one of the butt ons speciﬁ ed by the given bit \nmask just went down this frame. Here, we’re only checking for a single but-\nton-down event, but we can and will use this same function later to check for \nmultiple simultaneous butt on-down events.\nMultibutton Sequence\nLet’s say we want to detect the sequence A-B-A, performed within at most one \nsecond. We can detect this butt on sequence as follows: We maintain a variable \nthat tracks which butt on in the sequence we’re currently looking for. If we de-\nﬁ ne the sequence with an array of butt on ids (e.g., aButtons[3] = {A, B, \nA}), then our variable is just an index i into this array. It starts out initialized \nto the ﬁ rst butt on in the sequence, i = 0. We also maintain a start time for the \nentire sequence, Tstart  , much as we did in the rapid butt on-pressing example.\nThe logic goes like this: Whenever we see a butt on-down event that match-\nes the butt on we’re currently looking for, we check its time stamp against the \nstart time of the entire sequence, Tstart  . If it occurred within the valid time \nwindow, we advance the current butt on to the next butt on in the sequence; \nfor the ﬁ rst butt on in the sequence only (i = 0), we also update Tstart  . If we see \na butt on-down event that doesn’t match the next butt on in the sequence, or \nif the time delta has grown too large, we reset the butt on index i back to the \nbeginning of the sequence and set Tstart to some invalid value (such as 0). This \nis illustrated by the code below.\nclass ButtonSequenceDetector\n{\n \nU32* \nm_aButtonIds;  // sequence of buttons to watch for \n U32 \n m_buttonCount; \n// number of buttons in sequence\n F32 \n \nm_dtMax;       // max time for entire sequence\n U32 \n \nm_iButton; \n  // next button to watch for in seq.\n F32 \n \nm_tStart;      // start time of sequence, in   \n        \n    // \nseconds\npublic:\n \n// Construct an object that detects the given button\n \n// sequence. When the sequence is successfully   \n   \n \n// detected, the given event is broadcast, so the  \n \n// rest of the game can respond in an appropriate way.\nButtonSequenceDetector(U32* aButtonIds, \n        \n  U32 buttonCount,\n \n                       F32 dtMax, \n        \n  EventId eventIdToSend) :\n  m_aButtonIds(aButtonIds),\n  m_buttonCount(buttonCount),\n",
      "content_length": 2503,
      "extraction_method": "Direct"
    },
    {
      "page_number": 381,
      "chapter": null,
      "content": "359 \n  m_dtMax(dtMax),\n \n \nm_eventId(eventIdToSend), // event to send when   \n          \n    // complete\n \n \nm_iButton(0),             // start of sequence\n \n \nm_tStart(0)               // initial value  \n          \n    // irrelevant\n {\n }\n \n// Call this once per frame.\n void \nUpdate()\n {\n \n \nASSERT(m_iButton < m_buttonCount);\n \n \n// Determine which button we’re expecting next, as\n \n \n// a bit mask (shift a 1 up to the correct bit  \n \n  // \nindex).\n  U32 \nbuttonMask = (1U << m_aButtonId[m_iButton]);\n \n \n// If any button OTHER than the expected button   \n \n \n// just went down, invalidate the sequence. (Use   \n \n \n// the bitwise NOT operator to check for all other  \n  // \nbuttons.)\n  if \n(ButtonsJustWentDown(~buttonMask))\n  {\n \n \n \nm_iButton = 0; // reset\n  }\n \n \n// Otherwise, if the expected button just went  \n \n \n// down, check dt and update our state appropriately.\n \n \nelse if (ButtonsJustWentDown(buttonMask))\n  {\n \n \n \nif (m_iButton == 0)\n   {\n \n \n \n \n// This is the first button in the   \n \n \n    // \nsequence.\n    m_tStart \n= CurrentTime();\n \n \n \n \n++m_iButton; // advance to next button\n   }\n   else\n   {\n \n \n \n \nF32 dt = CurrentTime() – m_tStart;\n    if \n(dt < m_dtMax)\n    {\n     // \nSequence is still valid.\n8.5. Game Engine HID Systems\n",
      "content_length": 1259,
      "extraction_method": "Direct"
    },
    {
      "page_number": 382,
      "chapter": null,
      "content": "360 \n8. Human Interface Devices (HID)\n \n \n \n \n \n++m_iButton; // advance to next button\n     // \nIs the sequence complete?\n     if \n(m_iButton == m_buttonCount)\n     {\nBroadcastEvent(m_eventId);\n      m_iButton \n= 0; // reset\n     }\n    }\n    else\n    {\n     // \nSorry, not fast enough.\n     m_iButton \n= 0; // reset\n    }\n   }\n  }\n }\n};\nThumb Stick Rotation\nAs an example of a more-complex gesture, let’s see how we might detect when \nthe player is rotating the left  thumb stick in a clockwise circle. We can detect \nthis quite easily by dividing the two-dimensional range of possible stick po-\nsitions into quadrants, as shown in Figure 8.10. In a clockwise rotation, the \nstick passes through the upper-left  quadrant, then the upper-right, then the \nlower-right, and ﬁ nally the lower-left . We can treat each of these cases like a \nbutt on press and detect a full rotation with a slightly modiﬁ ed version of the \nsequence detection code shown above. We’ll leave this one as an exercise for \nthe reader. Try it!\nx\ny\nUL\nUR\nLL\nLR\nFigure 8.10.  Detecting circular rotations of the stick by dividing the 2D range of stick inputs \ninto quadrants.\n",
      "content_length": 1147,
      "extraction_method": "Direct"
    },
    {
      "page_number": 383,
      "chapter": null,
      "content": "361 \n8.5.5. Managing Multiple HIDs for Multiple Players\n Most game machines allow two or more HIDs to be att ached for multiplayer \ngames. The engine must keep track of which devices are currently att ached \nand route each one’s inputs to the appropriate player in the game. This implies \nthat we need some way of mapping controllers to players. This might be as \nsimple as a one-to-one mapping between controller index and player index, \nor it might be something more sophisticated, such as assigning controllers to \nplayers at the time the user hits the Start butt on.\nEven in a single-player game with only one HID, the engine needs to be \nrobust to various exceptional conditions, such as the controller being acciden-\ntally unplugged or running out of batt eries. When a controller’s connection \nis lost, most games pause gameplay, display a message, and wait for the con-\ntroller to be reconnected. Some multiplayer games suspend or temporarily \nremove the avatar corresponding to a removed controller, but allow the other \nplayers to continue playing the game; the removed/suspended avatar might \nreactivate when the controller is reconnected.\nOn systems with batt ery-operated HIDs, the game or the operating sys-\ntem is responsible for detecting low-batt ery conditions. In response, the play-\ner is usually warned in some way, for example via an unobtrusive on-screen \nmessage and/or a sound eﬀ ect.\n8.5.6. Cross-Platform HID Systems\n Many game engines are cross-platform. One way to handle HID inputs and \noutputs in such an engine would be to sprinkle conditional compilation di-\nrectives all over the code, wherever interactions with the HID take place, as \nshown below. This is clearly not an ideal solution, but it does work.\n#if TARGET_XBOX360\n \nif (ButtonsJustWentDown(XB360_BUTTONMASK_A))\n#elif TARGET_PS3\n \nif (ButtonsJustWentDown(PS3_BUTTONMASK_TRIANGLE))\n#elif TARGET_WII\n \nif (ButtonsJustWentDown(WII_BUTTONMASK_A))\n#endif\n{\n \n// do something...\n}\nA bett er solution is to provide some kind of hardware abstraction layer, there-\nby insulating the game code from hardware-speciﬁ c details.\nIf we’re lucky, we can abstract most of the diﬀ erences beween the HIDs \non the diﬀ erent platforms by a judicious choice of abstract butt on and axis \n8.5. Game Engine HID Systems\n",
      "content_length": 2292,
      "extraction_method": "Direct"
    },
    {
      "page_number": 384,
      "chapter": null,
      "content": "362 \n8. Human Interface Devices (HID)\nids. For example, if our game is to ship on Xbox 360 and PS3, the layout \nof the controls (butt ons, axes and triggers) on these two joypads are almost \nidentical. The controls have diﬀ erent ids on each platform, but we can come \nup with generic control ids that cover both types of joypad quite easily. For \nexample:\nenum AbstractControlIndex\n{\n \n// Start and back buttons\n AINDEX_START, \n \n \n   // Xbox 360 Start, PS3 Start\n \nAINDEX_BACK_PAUSE,    // Xbox 360 Back, PS3 Pause\n \n// Left D-pad\n AINDEX_LPAD_DOWN,\n AINDEX_LPAD_UP,\n AINDEX_LPAD_LEFT,\n AINDEX_LPAD_RIGHT,\n \n// Right \"pad\" of four buttons\n \nAINDEX_RPAD_DOWN, \n   // Xbox 360 A, PS3 X\n \nAINDEX_RPAD_UP,  \n   // Xbox 360 Y, PS3 Triangle\n \nAINDEX_RPAD_LEFT, \n   // Xbox 360 X, PS3 Square\n \nAINDEX_RPAD_RIGHT,    // Xbox 360 B, PS3 Circle\n \n// Left and right thumb stick buttons\n \nAINDEX_LSTICK_BUTTON,  // Xbox 360 LThumb, PS3 L3,\n        \n  // Xbox white\n \nAINDEX_RSTICK_BUTTON,  // Xbox 360 RThumb, PS3 R3,\n        \n  // Xbox black\n \n// Left and right shoulder buttons\n \nAINDEX_LSHOULDER, \n \n// Xbox 360 L shoulder, PS3 L1\n \nAINDEX_RSHOULDER, \n \n// Xbox 360 R shoulder, PS3 R1\n \n// Left thumb stick axes\n AINDEX_LSTICK_X,\n AINDEX_LSTICK_Y,\n \n// Right thumb stick axes\n AINDEX_RSTICK_X,\n AINDEX_RSTICK_Y,\n \n// Left and right trigger axes\n \nAINDEX_LTRIGGER,  \n   // Xbox 360 –Z, PS3 L2\n \nAINDEX_RTRIGGER,  \n   // Xbox 360 +Z, PS3 R2\n};\n",
      "content_length": 1435,
      "extraction_method": "Direct"
    },
    {
      "page_number": 385,
      "chapter": null,
      "content": "363 \nOur abstraction layer can translate between the raw control ids on the cur-\nrent target hardware into our abstract control indices . For example, whenever \nwe read the state of the butt ons into a 32-bit word, we can perform a bit-swiz-\nzling operation that rearranges the bits into the proper order to correspond to \nour abstract indices. Analog inputs can likewise be shuﬄ  ed around into the \nproper order.\nIn performing the mapping between physical and abstract controls, we’ll \nsometimes need to get a bit clever. For example, on the Xbox, the left  and right \ntriggers act as a single axis, producing negative values when the left  trigger is \npressed, zero when neither is trigger is pressed, and positive values when the \nright trigger is pressed. To match the behavior of the PlayStation’s DualShock \ncontroller, we might want to separate this axis into two distinct axes on the \nXbox, scaling the values appropriately so the range of valid values is the same \non all platforms.\nThis is certainly not the only way to handle HID I/O in a multiplatform \nengine. We might want to take a more functional approach, for example, by \nnaming our abstract controls according to their function in the game, rather \nthan their physical locations on the joypad. We might introduce higher-level \nfunctions that detect abstract gestures, with custom detection code on each \nplatform, or we might just bite the bullet and write platform-speciﬁ c versions \nof all of the game code that requires HID I/O. The possibilities are numerous, \nbut virtually all cross-platform game engines insulate the game from hard-\nware details in some manner.\n8.5.7. Input Re-Mapping\n Many games allow the player some degree of choice with regard to the func-\ntionality of the various controls on the physical HID. A common option is \nthe sense of the vertical axis of the right thumb stick for camera control in a \nconsole game. Some folks like to push forward on the stick to angle the camera \nup, while others like an inverted control scheme, where pulling back on the \nstick angles the camera up (much like an airplane control stick). Other games \nallow the player to select between two or more predeﬁ ned butt on mappings. \nSome PC games allow the user full control over the functions of individual \nkeys on the keyboard, the mouse butt ons, and the mouse wheel, plus a choice \nbetween various control schemes for the two mouse axes.\nTo implement this, we turn to a favorite saying of an old professor of \nmine, Professor Jay Black of the University of Waterloo, “Every problem in \ncomputer science can be solved with a level of indirection.” We assign each \nfunction in the game a unique id and then provide a simple table which maps \neach physical or abstract control index to a logical function in the game. When-\n8.5. Game Engine HID Systems\n",
      "content_length": 2830,
      "extraction_method": "Direct"
    },
    {
      "page_number": 386,
      "chapter": null,
      "content": "364 \n8. Human Interface Devices (HID)\never the game wishes to determine whether a particular logical game function \nshould be activated, it looks up the corresponding abstract or physical control \nid in the table and then reads the state of that control. To change the mapping, \nwe can either swap out the entire table wholesale, or we can allow the user to \nedit individual entries in the table.\nWe’re glossing over a few details here. For one thing, diﬀ erent controls \nproduce diﬀ erent kinds of inputs. Analog axes may produce values ranging \nfrom –32,768 to 32,767, or from 0 to 255, or some other range. The states of \nall the digital butt ons on a HID are usually packed into a single machine \nword. Therefore, we must be careful to only permit control mappings that \nmake sense. We cannot use a butt on as the control for a logical game func-\ntion that requires an axis, for example. One way around this problem is to \nnormalize all of the inputs. For example, we could re-scale the inputs from \nall analog axes and butt ons into the range [0, 1]. This isn’t quite as helpful as \nyou might at ﬁ rst think, because some axes are inherently bidirectional (like \na joy stick) while others are unidirectional (like a trigger). But if we group \nour controls into a few classes, we can normalize the inputs within those \nclasses, and permit remapping only within compatible classes. A reason-\nable set of classes for a standard console joypad and their normalized input \nvalues might be:\nz Digital butt ons. States are packed into a 32-bit word, one bit per butt on.\nz Unidirectional absolute axes (e.g., triggers, analog butt ons). Produce ﬂ oat-\ning-point input values in the range [0, 1].\nz Bidirectional absolute axes (e.g., joy sticks). Produce ﬂ oating-point input \nvalues in the range [–1, 1].\nz Relative axes (e.g., mouse axes, wheels, track balls). Produce ﬂ oating-point \ninput values in the range [–1, 1] , where ±1 represents the maximum \nrelative oﬀ set possible within a single game frame (i.e., during a period \nof 1/30 or 1/60 of a second).\n8.5.8. Context-Sensitive Controls\n In many games, a single physical control can have diﬀ erent functions, depend-\ning on context. A simple example is the ubiquitous “use” butt on. If pressed \nwhile standing in front of a door, the “use” butt on might cause the character to \nopen the door. If it is pressed while standing near an object, it might cause the \nplayer character to pick up the object, and so on. Another common example is \na modal control scheme. When the player is walking around, the controls are \nused to navigate and control the camera. When the player is riding a vehicle, \nthe controls are used to steer the vehicle, and the camera controls might be \ndiﬀ erent as well.\n",
      "content_length": 2748,
      "extraction_method": "Direct"
    },
    {
      "page_number": 387,
      "chapter": null,
      "content": "365 \nContext-sensitive controls are reasonably straightforward to imple-\nment via a state machine. Depending on what state we’re in, a particu-\nlar HID control may have a diﬀ erent purpose. The tricky part is deciding \nwhat state to be in. For example, when the context-sensitive “use” butt on \nis pressed, the player might be standing at a point equidistant between a \nweapon and a health pack, facing the center point between them. Which \nobject do we use in this case? Some games implement a priority system to \nbreak ties like this. Perhaps the weapon has a higher weight than the health \npack, so it would “win” in this example. Implementing context-sensitive \ncontrols isn’t rocket science, but it invariably requires lots of trial-and-error \nto get it feeling and behaving just right. Plan on lots of iteration and focus \ntesting!\nAnother related concept is that of control ownership. Certain controls on \nthe HID might be “owned” by diﬀ erent parts of the game. For example, some \ninputs are for player control, some for camera control, and still others are for \nuse by the game’s wrapper and menu system (pausing the game, etc.) Some \ngame engines introduce the concept of a logical device, which is composed of \nonly a subset of the inputs on the physical device. One logical device might \nbe used for player control, while another is used by the camera system, and \nanother by the menu system.\n8.5.9. Disabling Inputs\n In most games, it is sometimes necessary to disallow the player from control-\nling his or her character. For example, when the player character is involved in \nan in-game cinematic, we might want to disable all player controls temporar-\nily; or when the player is walking through a narrow doorway, we might want \nto temporarily disable free camera rotation.\nOne rather heavy-handed approach is to use a bit mask to disable indi-\nvidual controls on the input device itself. Whenever the control is read, the \ndisable mask is checked, and if the corresponding bit is set, a neutral or zero \nvalue is returned instead of the actual value read from the device. We must be \nparticularly cautious when disabling controls, however. If we forget to reset \nthe disable mask, the game can get itself into a state where the player looses \nall control forever, and must restart the game. It’s important to check our logic \ncarefully, and it’s also a good idea to put in some fail-safe mechanisms to en-\nsure that the disable mask is cleared at certain key times, such as whenever the \nplayer dies and re-spawns.\nDisabling a HID input masks it for all possible clients, which can \nbe overly limiting. A better approach is probably to put the logic for \ndisabling specific player actions or camera behaviors directly into the \n8.5. Game Engine HID Systems\n",
      "content_length": 2772,
      "extraction_method": "Direct"
    },
    {
      "page_number": 388,
      "chapter": null,
      "content": "366 \n8. Human Interface Devices (HID)\nplayer or camera code itself. That way, if the camera decides to ignore \nthe deflection of the right thumb stick, for example, other game engine \nsystems still have the freedom to read the state of that stick for other \npurposes.\n8.6. Human Interface Devices in Practice\nCorrect and smooth handling of human interface devices is an important part \nof any good game. Conceptually speaking, HIDs may seem quite straightfor-\nward. However, there can be quite a few “gotchas” to deal with, including \nvariations between diﬀ erent physical input devices, proper implementation \nof low-pass ﬁ ltering, bug-free handling of control scheme mappings, achiev-\ning just the right “feel” in your joypad rumble, limitations imposed by console \nmanufacturers via their technical requirements checklists (TRCs), and the list \ngoes on. A game team should expect to devote a non-trivial amount of time \nand engineering bandwidth to a careful and complete implementation of the \nhuman interface device system. This is extremely important because the HID \nsystem forms the underpinnings of your game’s most precious resource—its \nplayer mechanics.\n",
      "content_length": 1167,
      "extraction_method": "Direct"
    },
    {
      "page_number": 389,
      "chapter": null,
      "content": "367\n9\n \nTools for Debugging \nand Development\nD\neveloping game soft ware is a complex, intricate, math-intensive, and er-\nror-prone business. So it should be no surprise that virtually every pro-\nfessional game team builds a suite of tools for themselves, in order to make \nthe game development process easier and less error-prone. In this chapter, \nwe’ll take a look at the development and debugging tools most oft en found in \nprofessional-grade game engines.\n9.1. \nLogging and Tracing\n Remember when you wrote your ﬁ rst program in BASIC or Pascal? (OK, may-\nbe you don’t. If you’re signiﬁ cantly younger than me—and there’s a prett y \ngood chance of that—you probably wrote your ﬁ rst program in Java, or maybe \nPython or Lua.) In any case, you probably remember how you debugged your \nprograms back then. You know, back when you thought a debugger was one of \nthose glowing blue insect zapper things? You probably used print statements \nto dump out the internal state of your program. C/C++ programmers call this \nprintf debugging (aft er the standard C library function, printf()).\nIt turns out that printf debugging is still a perfectly valid thing to do—even \nif you know that a debugger isn’t a device for frying hapless insects at night. \nEspecially in real-time programming, it can be diﬃ  cult to trace certain kinds \n",
      "content_length": 1329,
      "extraction_method": "Direct"
    },
    {
      "page_number": 390,
      "chapter": null,
      "content": "368 \n9. Tools for Debugging and Development\nof bugs using breakpoints and watch windows. Some bugs are timing-depen-\ndent; they only happen when the program is running at full speed. Other bugs \nare caused by a complex sequence of events too long and intricate to trace \nmanually one-by-one. In these situations, the most powerful debugging tool \nis oft en a sequence of print statements.\nEvery game platform has some kind of console or teletype (TTY) output \ndevice. Here are some examples:\nz In a console application writt en in C/C++, running under Linux or \nWin32, you can produce output in the console by printing to stdout or \nstderr via printf(), fprintf(), or STL’s iostream interface.\nz Unfortunately, printf() and iostream don’t work if your game is built \nas a windowed application under Win32, because there’s no console \nin which to display the output. However, if you’re running under the \nVisual Studio debugger, it provides a debug console to which you can \nprint via the Win32 function OutputDebugString().\nz On the PLAYSTATION 3, an application known as the Target Manager \nruns on your PC and allows you to launch programs on the console. \nThe Target Manager includes a set of TTY output windows to which \nmessages can be printed by the game engine.\nSo printing out information for debugging purposes is almost always as easy \nas adding calls to printf() throughout your code. However, most game en-\ngines go a bit farther than this. In the following sections, we’ll investigate the \nkinds of printing facilities most game engines provide.\n9.1.1. \nFormatted Output with OutputDebugString()\nThe Win32 function OutputDebugString() is great for printing debug-\nging information to Visual Studio’s Debug Output window. However, unlike \nprintf(), OutputDebugString() does not support formatt ed output—it can \nonly print raw strings in the form of char arrays. For this reason, most Windows \ngame engines wrap OutputDebugString() in a custom function, like this:\n#include <stdio.h>     // for va_list et al\n#ifndef WIN32_LEAN_AND_MEAN\n#define WIN32_LEAN_AND_MEAN 1\n#endif\n#include <windows.h>   // for OutputDebugString()\nint VDebugPrintF(const char* format, va_list argList)\n{\n \nconst U32 MAX_CHARS = 1023;\n \nstatic char s_buffer[MAX_CHARS + 1];\n",
      "content_length": 2261,
      "extraction_method": "Direct"
    },
    {
      "page_number": 391,
      "chapter": null,
      "content": "369 \n9.1. Logging and Tracing\n \nint charsWritten\n \n \n= vsnprintf(s_buffer, MAX_CHARS, format, argList);\n \ns_buffer[MAX_CHARS] = ‘\\0’; // be sure to \n          \n  // NIL-terminate\n \n// Now that we have a formatted string, call the  \n \n// Win32 API.\nOutputDebugString(s_buffer);\n \nreturn charsWritten;\n}\nint DebugPrintF(const char* format, ...)\n{\n \nva_list argList;\nva_start(argList, format);\n \nint charsWritten = VDebugPrintF(format, argList);\nva_end(argList);\n \nreturn charsWritten;\n}\nNotice that two functions are implemented: DebugPrintF()takes a \nvariable-length argument list (speciﬁ ed via the ellipsis, …), while VDebug\nPrintF()takes a va_list argument. This is done so that programmers can \nbuild additional printing functions in terms of VDebugPrintF(). (It’s impos-\nsible to pass ellipses from one function to another, but it is possible to pass \nva_lists around.)\n9.1.2. Verbosity\n Once you’ve gone to the trouble of adding a bunch of print statements to your \ncode in strategically chosen locations, it’s nice to be able to leave them there, \nin case they’re needed again later. To permit this, most engines provide some \nkind of mechanism for controlling the level of verbosity via the command-line, \nor dynamically at runtime. When the verbosity level is at its minimum value \n(usually zero), only critical error messages are printed. When the verbosity is \nhigher, more of the print statements embedded in the code start to contribute \nto the output.\nThe simplest way to implement this is to store the current verbosity level \nin a global integer variable, perhaps called g_verbosity. We then provide a \nVerboseDebugPrintF() function whose ﬁ rst argument is the verbosity level \nat or above which the message will be printed. This function could be imple-\nmented as follows:\n",
      "content_length": 1789,
      "extraction_method": "Direct"
    },
    {
      "page_number": 392,
      "chapter": null,
      "content": "370 \n9. Tools for Debugging and Development\nint g_verbosity = 0;\nvoid VerboseDebugPrintF(int verbosity,\n \nconst char* format, ...)\n{\n \n// Only print when the global verbosity level is\n \n// high enough.\n \nif (g_verbosity >= verbosity)\n {\n \n  va_list argList;\n \n  va_start(argList, format);\n \n  \nVDebugPrintF(format, argList);\n \n  va_end(argList);\n }\n}\n9.1.3. Channels\n It’s also extremely useful to be able to categorize your debug output into chan-\nnels. One channel might contain messages from the animation system, while \nanother might be used to print messages from the physics system, for exam-\nple.\nOn some platforms, like the PLAYSTATION 3, debug output can be di-\nrected to one of 14 distinct TTY windows. In addition, messages are mirrored \nto a special TTY window that contains the output from all of the other 14 \nwindows. This makes it very easy for a developer to focus in on only the mes-\nsages he or she wants to see. When working on an animation problem, one \ncan simply ﬂ ip to the animation TTY and ignore all the other output. When \nworking on a general problem of unknown origin, the “all” TTY can be con-\nsulted for clues.\nOther platforms like Windows provide only a single debug output con-\nsole. However, even on these systems it can be helpful to divide your output \ninto channels. The output from each channel might be assigned a diﬀ erent \ncolor. You might also implement ﬁ lters, which can be turned on and oﬀ  at \nruntime, and restrict output to only a speciﬁ ed channel or set of channels. \nIn this model, if a developer is debugging an animation-related problem, for \nexample, he or she can simply ﬁ lter out all of the channels except the anima-\ntion channel.\nA channel-based debug output system can be implemented quite easily \nby adding an additional channel argument to our debug printing function. \nChannels might be numbered, or bett er, assigned symbolic values via a C/C++ \nenum declaration. Or channels might be named using a string or hashed string \n",
      "content_length": 1989,
      "extraction_method": "Direct"
    },
    {
      "page_number": 393,
      "chapter": null,
      "content": "371 \nid. The printing function can simply consult the list of active channels and \nonly print the message if the speciﬁ ed channel is among them.\nIf you don’t have more than 32 or 64 channels, it can be helpful to identify \nthe channels via a 32- or 64-bit mask. This makes implementing a channel \nﬁ lter as easy as specifying a single integer. When a bit in the mask is 1, the cor-\nresponding channel is active; when the bit is 0, the channel is muted.\n9.1.4. Mirroring Output to a File\n It’s a good idea to mirror all debug output to one or more log ﬁ les (e.g., one \nﬁ le per channel). This permits problems to be diagnosed aft er the fact. Ideally \nthe log ﬁ le(s) should contain all of the debug output, independent of the cur-\nrent verbosity level and active channels mask. This allows unexpected prob-\nlems to be caught and tracked down by simply inspecting the most-recent \nlog ﬁ les.\nYou may want to consider ﬂ ushing your log ﬁ le(s) aft er every call to your \ndebug output function to ensure that if the game crashes the log ﬁ le(s) won’t \nbe missing the last buﬀ er-full of output. The last data printed is usually the \nmost crucial to determine the cause of a crash, so we want to be sure that the \nlog ﬁ le always contains the most up-to-date output. Of course, ﬂ ushing the \noutput buﬀ er can be expensive. So you should only ﬂ ush buﬀ ers aft er every \ndebug output call if either (a) you are not doing a lot of logging, or (b) you \ndiscover that it is truly necessary on your particular platform. If ﬂ ushing is \ndeemed to be necessary, you can always provide an engine conﬁ guration op-\ntion to turn it on and oﬀ .\n9.1.5. Crash Reports\n Some game engines produce special text output and/or log ﬁ les when the \ngame crashes. In most operating systems, a top-level exception handler can \nbe installed that will catch most crashes. In this function, you could print out \nall sorts of useful information. You could even consider emailing the crash \nreport to the entire programming team. This can be incredibly enlightening \nfor the programmers: When they see just how oft en the art and design teams \nare crashing, they may discover a renewed sense of urgency in their debug-\nging tasks!\nHere are just a few examples of the kinds of information you can include \nin a crash report:\nz Current level(s) being played at the time of the crash.\nz World-space location of the player character when the crash occurred.\nz Animation/action state of the player when the game crashed.\n9.1. Logging and Tracing\n",
      "content_length": 2513,
      "extraction_method": "Direct"
    },
    {
      "page_number": 394,
      "chapter": null,
      "content": "372 \n9. Tools for Debugging and Development\nz Gameplay script(s) that were running at the time of the crash. (This can \nbe especially helpful if the script is the cause of the crash!)\nz Stack trace. Most operating systems provide a mechanism for walking \nthe call stack (although they are nonstandard and highly platform \nspeciﬁ c). With such a facility, you can print out the symbolic names of \nall non-inline functions on the stack at the time the crash occurred.\nz State of all memory allocators in the engine (amount of memory free, \ndegree of fragmentation, etc.). This kind of data can be helpful when \nbugs are caused by low-memory conditions, for example.\nz Any other information you think might be relevant when tracking down \nthe cause of a crash.\n9.2. Debug Drawing Facilities\nModern interactive games are driven almost entirely by math. We use math \nto position and orient objects in the game world, move them around, test for \ncollisions, cast rays to determine lines of sight, and of course use matrix mul-\ntiplication to transform objects from object space to world space and even-\ntually into screen space for rendering. Almost all modern games are three-\ndimensional, but even in a two-dimensional game it can be very diﬃ  cult to \nmentally visualize the results of all these mathematical calculations. For this \nreason, most good game engines provide an API for drawing colored lines, \nsimple shapes, and 3D text. We call this a debug drawing facility, because the \nlines, shapes, and text that are drawn with it are intended for visualization \nduring development and debugging and are removed prior to shipping the \ngame.\nA debug drawing API can save you huge amounts of time. For example, \nif you are trying to ﬁ gure out why your projectiles are not hitt ing the enemy \ncharacters, which is easier? Deciphering a bunch of numbers in the debugger? \nOr drawing a line showing the trajectory of the projectile in three dimensions \nwithin your game? With a debug drawing API, logical and mathematical er-\nrors become immediately obvious. One might say that a picture is worth 1,000 \nminutes of debugging.\nHere are some examples of debug drawing in action within Naughty \nDog’s Uncharted: Drake’s Fortune engine. The following screen shots were all \ntaken within our play-test level, one of many special levels we use for testing \nout new features and debugging problems in the game.\nz Figure 9.1 shows how a single line can help developers understand \nwhether a target is within the line of sight of an enemy character. You’ll \n",
      "content_length": 2545,
      "extraction_method": "Direct"
    },
    {
      "page_number": 395,
      "chapter": null,
      "content": "373 \nFigure 9.1.  Visualizing the line of sight from an NPC to the player.\n9.2. Debug Drawing Facilities\nalso notice some debug text rendered just above the head of the enemy, \nin this case showing weapon ranges, a damage multiplier, the distance \nto the target, and the character’s percentage chance of striking the tar-\nget. Being able to print out arbitrary information in three-dimensional \nspace is an incredibly useful feature.\nz Figure 9.2 shows how a wireframe sphere can be used to visualize the \ndynamically expanding blast radius of an explosion.\nz Figure 9.3 shows how spheres can be used to visualize the radii used \nby Drake when searching for ledges to hang from in the game. A red \nline shows the ledge he is currently hanging from. Notice that in this \ndiagram, white text is displayed in the upper left -hand corner of the \nscreen. In the Uncharted: Drake’s Fortune engine, we have the ability to \ndisplay text in two-dimensional screen space, as well as in full 3D. This \ncan be useful when you want the text to be displayed independently of \nthe current camera angle.\nz Figure 9.4 shows an AI character that has been placed in a special de-\nbugging mode. In this mode, the character’s brain is eﬀ ectively turned \n",
      "content_length": 1234,
      "extraction_method": "Direct"
    },
    {
      "page_number": 396,
      "chapter": null,
      "content": "374 \n9. Tools for Debugging and Development\nFigure 9.2.  Visualizing the expanding blast sphere of an explosion.\nFigure 9.3.  Spheres and vectors used in Drake’s ledge hang and shimmy system.\n",
      "content_length": 192,
      "extraction_method": "Direct"
    },
    {
      "page_number": 397,
      "chapter": null,
      "content": "375 \noﬀ , and the developer is given full control over the character’s move-\nments and actions via a simple heads-up menu. The developer can paint \ntarget points in the game world by simply aiming the camera and can \nthen instruct the character to walk, run, or sprint to the speciﬁ ed points. \nThe user can also tell the character to enter or leave nearby cover, ﬁ re its \nweapon, and so on.\n9.2.1. Debug Drawing API\nA debug drawing API generally needs to satisfy the following requirements:\nz The API should be simple and easy to use.\nz It should support a useful set of primitives , including (but not limited \nto):\nlines,\n \n□\nspheres,\n \n□\npoints (usually represented as small crosses or spheres, because a \n \n□\nsingle pixel is very diﬃ  cult to see),\nFigure 9.4.  Manually controlling an NPC’s actions for debugging purposes.\n9.2. Debug Drawing Facilities\n",
      "content_length": 860,
      "extraction_method": "Direct"
    },
    {
      "page_number": 398,
      "chapter": null,
      "content": "376 \n9. Tools for Debugging and Development\ncoordinate axes (typically the \n \n□\nx-axis is drawn in red, y in green and \nz in blue),\nbounding boxes, and\n \n□\nformatt ed text.\n \n□\nz It should provide a good deal of ﬂ exibility in controlling how primitives \nare drawn, including:\ncolor,\n \n□\nline width,\n \n□\nsphere radii,\n \n□\nthe size of points, lengths of coordinate axes, and dimensions of oth-\n \n□\ner “canned” primitives.\nz It should be possible to draw primitives in world space (full 3D, using \nthe game camera’s perspective projection matrix) or in screen space (ei-\nther using an orthographic projection, or possibly a perspective projec-\ntion). World-space primitives are useful for annotating objects in the \n3D scene. Screen-space primitives are helpful for displaying debugging \ninformation in the form of a heads-up display that is independent of \ncamera position or orientation.\nz It should be possible to draw primitives with or without depth testing \nenabled.\nWhen depth testing is enabled, the primitives will be occluded by \n \n□\nreal objects in your scene. This makes their depth easy to visualize, \nbut it also means that the primitives may sometimes be diﬃ  cult to \nsee or totally hidden by the geometry of your scene.\nWith depth testing disabled, the primitives will “hover” over the real \n \n□\nobjects in the scene. This makes it harder to gauge their real depth, \nbut it also ensures that no primitive is ever hidden from view.\nz It should be possible to make calls to the drawing API from anywhere \nin your code. Most rendering engines require that geometry be submit-\nted for rendering during a speciﬁ c phase of the game loop, usually at \nthe end of each frame. So this requirement implies that the system must \nqueue up all incoming debug drawing requests, so that they may be \nsubmitt ed at the proper time later on.\nz Ideally, every debug primitive should have a lifetime associated with it. \nThe lifetime controls how long the primitive will remain on-screen aft er \nhaving been requested. If the code that is drawing the primitive is called \nevery frame, the lifetime can be one frame—the primitive will remain \non-screen because it will be refreshed every frame. However, if the code \n",
      "content_length": 2212,
      "extraction_method": "Direct"
    },
    {
      "page_number": 399,
      "chapter": null,
      "content": "377 \nthat draws the primitive is called rarely or intermitt ently (e.g., a func-\ntion that calculates the initial velocity of a projectile), then you do not \nwant the primitive to ﬂ icker on-screen for just one frame and then dis-\nappear. In such situations the programmer should be able to give his or \nher debug primitives a longer lifetime, on the order of a few seconds.\nz It’s also important that the debug drawing system be capable of han-\ndling a large number of debug primitives eﬃ  ciently. When you’re draw-\ning debug information for 1,000 game objects, the number of primitives \ncan really add up, and you don’t want your game to be unusable when \ndebug drawing is turned on.\nThe debug drawing API in Naughty Dog’s Uncharted: Drake’s Fortune en-\ngine looks something like this:\nclass DebugDrawManager\n{\npublic:\n \n// Adds a line segment to the debug drawing queue.\n void \nAddLine(  \nconst Point& fromPosition,\n      const \nPoint& toPosition,\n      Color \ncolor,\n      float \nlineWidth = 1.0f,\n      float \nduration = 0.0f,\n      bool \ndepthEnabled = true);\n \n// Adds an axis-aligned cross (3 lines converging at   \n \n// a point) to the debug drawing queue.\n void \nAddCross( \nconst Point& position,\n      Color \ncolor,\n      float \nsize,\n      float \nduration = 0.0f,\n      bool \ndepthEnabled = true);\n \n// Adds a wireframe sphere to the debug drawing queue.\n void \nAddSphere( const Point& centerPosition,\n      float \nradius,\n      Color \ncolor,\n      float \nduration = 0.0f,\n      bool \ndepthEnabled = true);\n \n// Adds a circle to the debug drawing queue.\n void \nAddCircle( const Point& centerPosition,\n      const \nVector& planeNormal,\n      float \nradius,\n      Color \ncolor,\n      float \nduration = 0.0f,\n      bool \ndepthEnabled = true);\n9.2. Debug Drawing Facilities\n",
      "content_length": 1783,
      "extraction_method": "Direct"
    },
    {
      "page_number": 400,
      "chapter": null,
      "content": "378 \n9. Tools for Debugging and Development\n \n// Adds a set of coordinate axes depicting the  \n \n \n \n// position and orientation of the given \n \n// transformation to the debug drawing queue.\n void \nAddAxes(  \nconst Transform& xfm,\n      Color \ncolor,\n      float \nsize,\n      float \nduration = 0.0f,\n      bool \ndepthEnabled = true);\n \n// Adds a wireframe triangle to the debug drawing  \n \n \n// queue.\n void \nAddTriangle( \nconst Point& vertex0,\n      const \nPoint& vertex1,\n      const \nPoint& vertex2,\n      Color \ncolor,\n      float \nlineWidth = 1.0f,\n      float \nduration = 0.0f,\n      bool \ndepthEnabled = true);\n \n// Adds an axis-aligned bounding box to the debug  \n \n \n// queue.\n void \nAddAABB( const Point& minCoords,\n     const \nPoint& maxCoords,\n     Color \ncolor,\n     float \nlineWidth = 1.0f,\n     float \nduration = 0.0f,\n     bool \ndepthEnabled = true);\n \n// Adds an oriented bounding box to the debug queue.\n void \nAddOBB( const Mat44& centerTransform,\n     const \nVector& scaleXYZ,\n     Color \ncolor,\n     float \nlineWidth = 1.0f,\n     float \nduration = 0.0f,\n     bool \ndepthEnabled = true);\n \n// Adds a text string to the debug drawing queue.\n void \nAddString( const Point& pos,\n      const \nchar* text,\n      Color \ncolor,\n      float \nduration = 0.0f,\n      bool \ndepthEnabled = true);\n};\n// This global debug drawing manager is configured for \n// drawing in full 3D with a perspective projection.\nextern DebugDrawManager g_debugDrawMgr;\n",
      "content_length": 1457,
      "extraction_method": "Direct"
    },
    {
      "page_number": 401,
      "chapter": null,
      "content": "379 \n9.3. In-Game Menus\n// This global debug drawing manager draws its \n// primitives in 2D screen space. The (x,y) coordinates  \n// of a point specify a 2D location on-screen, and the \n// z coordinate contains a special code that indicates \n// whether the (x,y) coordidates are measured in absolute \n// pixels or in normalized coordinates that range from \n// 0.0 to 1.0. (The latter mode allows drawing to be \n// independent of the actual resolution of  the screen.)\nextern DebugDrawManager g_debugDrawMgr2D;\nHere’s an example of this API being used within game code:\nvoid Vehicle::Update()\n{\n \n// Do some calculations...\n \n// Debug-draw my velocity vector.\n \nPoint start = GetWorldSpacePosition();\n \nPoint end = start + GetVelocity();\ng_debugDrawMgr.AddLine(start, end, kColorRed);\n \n// Do some other calculations...\n \n// Debug-draw my name and number of passengers.\n {\n  char b\nuffer[128];\n \n \nsprintf(buffer, \"Vehicle %s: %d passengers\",\n   GetName(), GetN\numPassengers());\ng_debugDrawMgr.AddString(GetWorldSpacePosition(),\n \n \n \nbuffer, kColorWhite, 0.0f, false);\n }\n}\nYou’ll notice that the names of the drawing functions use the verb “add” \nrather than “draw.” This is because the debug primitives are typically not \ndrawn immediately when the drawing function is called. Instead, they are \nadded to a list of visual elements that will be drawn at a later time. Most high-\nspeed 3D rendering engines require that all visual elements be maintained in \na scene data structure so that they can be drawn eﬃ  ciently, usually at the end \nof the game loop. We’ll learn a lot more about how rendering engines work \nin Chapter 10.\n9.3. In-Game Menus\n Every game engine has a large number of conﬁ guration options and features. \nIn fact, each major subsystem, including rendering, animation, collision, \n",
      "content_length": 1802,
      "extraction_method": "Direct"
    },
    {
      "page_number": 402,
      "chapter": null,
      "content": "380 \n9. Tools for Debugging and Development\nphysics, audio, networking, player mechanics, AI, and so on, exposes its own \nspecialized conﬁ guration options. It is highly useful to programmers, artists, \nand game designers alike to be able to conﬁ gure these options while the game \nis running, without having to change the source code, recompile and relink \nthe game executable, and then rerun the game. This can greatly reduce the \namount of time the game development team spends on debugging problems \nand sett ing up new levels or game mechanics.\nFigure 9.5.  Main development menu in Uncharted.\nFigure 9.6.  Rendering submenu.\n",
      "content_length": 631,
      "extraction_method": "Direct"
    },
    {
      "page_number": 403,
      "chapter": null,
      "content": "381 \n9.4. In-Game Console\nOne simple and convenient way to permit this kind of thing is to provide \na system of in-game menus. Items on an in-game menu can do any number of \nthings, including (but certainly not limited to):\nz toggling global Boolean sett ings,\nz adjusting global integer and ﬂ oating-point values,\nz calling arbitrary functions, which can perform literally any task within \nthe engine,\nFigure 9.7.  Mesh options subsubmenu.\nFigure 9.8.  Background meshes turned off.\n",
      "content_length": 484,
      "extraction_method": "Direct"
    },
    {
      "page_number": 404,
      "chapter": null,
      "content": "382 \n9. Tools for Debugging and Development\nz bringing up submenus, allowing the menu system to be organized hier-\narchically for easy navigation.\nAn in-game menu should be easy and convenient to bring up, perhaps via \na simple butt on-press on the joypad. (Of course, you’ll want to choose a but-\nton combination that doesn’t occur during normal gameplay.) Bringing up the \nmenus usually pauses the game. This allows the developer to play the game \nuntil the moment just before a problem occurs, then pause the game by bring-\ning up the menus, adjust engine sett ings in order to visualize the problem \nmore clearly, and then un-pause the game to inspect the problem in depth.\nLet’s take a brief look at how the menu system works in the Uncharted: \nDrake’s Fortune engine, by Naughty Dog. Figure 9.5 shows the top-level menu. \nIt contains submenus for each major subsystem in the engine. In Figure 9.6, \nwe’ve drilled down one level into the Rendering… submenu. Since the render-\ning engine is a highly complex system, its menu contains many submenus con-\ntrolling various aspects of rendering. To control the way in which 3D meshes \nare rendered, we drill down further into the Mesh Options… submenu, shown \nin Figure 9.7. On this menu, we can turn oﬀ  rendering of all static background \nmeshes, leaving only the dynamic foreground meshes visible. This is shown \nin Figure 9.8.\n9.4. In-Game Console\n Some engines provide an in-game console, either in lieu of or in addition to an \nin-game menu system. An in-game console provides a command-line inter-\nface to the game engine’s features, much as a DOS command prompt provides \nusers with access to various features of the Windows operating system, or a \ncsh, tcsh, ksh or bash shell prompt provides users with access to the features \nof UNIX-like operating systems. Much like a menu system, the game engine \nconsole can provide commands allowing a developer to view and manipulate \nglobal engine sett ings, as well as running arbitrary commands.\nA console is somewhat less convenient than a menu system, especially for \nthose who aren’t very fast typists. However, a console can be much more pow-\nerful than a menu. Some in-game consoles provide only a rudimentary set \nof hard-coded commands, making them about as ﬂ exible as a menu system. \nBut others provide a rich interface to virtually every feature of the engine. A \nscreen shot of the in-game console in Quake 4 is shown in Figure 9.9.\nSome game engines provide a powerful scripting language that can be \nused by programmers and game designers to extend the functionality of the \nengine, or even build entirely new games. If the in-game console “speaks” \n",
      "content_length": 2666,
      "extraction_method": "Direct"
    },
    {
      "page_number": 405,
      "chapter": null,
      "content": "383 \nthis same scripting language, then anything you can do in script can also be \ndone interactively via the console. We’ll explore scripting languages in depth \nin Section 14.8.\n9.5. Debug Cameras and Pausing the Game\nAn in-game menu or console system is best accompanied by two other crucial \nfeatures: (a) the ability to detach the camera from the player character and ﬂ y \nit around the game world in order to scrutinize any aspect of the scene, and \n(b) the ability to pause , un-pause and single-step the game (see Section 7.5.6). \nWhen the game is paused, it is important to still be able to control the camera. \nTo support this, we can simply keep the rendering engine and camera controls \nrunning, even when the game’s logical clock is paused.\nSlow motion mode is another incredibly useful feature for scrutinizing \nanimations, particle eﬀ ects, physics and collision behaviors, AI behaviors, \nand the list goes on. This feature is easy to implement. Presuming we’ve tak-\nen care to update all gameplay elements using a clock that is distinct from \nthe real-time clock, we can put the game into slo-mo by simply updating the \ngameplay clock at a rate that is slower than usual. This approach can also \nbe used to implement a fast-motion mode, which can be useful for moving \n9.6. Cheats\nFigure 9.9.  The in-game console in Quake 4, overlaid on top of the main game menu.\n",
      "content_length": 1381,
      "extraction_method": "Direct"
    },
    {
      "page_number": 406,
      "chapter": null,
      "content": "384 \n9. Tools for Debugging and Development\nrapidly through time-consuming portions of gameplay in order to get to an \narea of interest.\n9.6. Cheats\nWhen developing or debugging a game, it’s important to allow the user to \nbreak the rules of the game in the name of expediency. Such features are aptly \nnamed cheats . For example, many engines allow you to “pick up” the player \ncharacter and ﬂ y him or her around in the game world, with collisions dis-\nabled so he or she can pass through all obstacles. This can be incredibly help-\nful for testing out gameplay. Rather than taking the time to actually play the \ngame in an att empt to get the player character into some desirable location, \nyou can simply pick him up, ﬂ y him over to where you want him to be, and \nthen drop him back into his regular gameplay mode.\nOther useful cheats include, but are certainly not limited to:\nz Invincible player. As a developer, you oft en don’t want to be bothered \nhaving to defend yourself from enemy characters, or worrying about \nfalling from too high a height, as you test out a feature or track down a \nbug.\nz Give player weapon. It’s oft en useful to be able to give the player any \nweapon in the game for testing purposes.\nz Inﬁ nite ammo. When you’re trying to kill bad guys to test out the weap-\non system or AI hit reactions, you don’t want to be scrounging for \nclips!\nz Select player mesh. If the player character has more than one “costume,” \nit can be useful to be able to select any of them for testing purposes.\nObviously this list could go on for pages. The sky’s the limit—you can add \nwhatever cheats you need in order to develop or debug the game. You might \neven want to expose some of your favorite cheats to the players of the ﬁ nal \nshipping game. Players can usually activate cheats by entering unpublished \ncheat codes on the joypad or keyboard, and/or by accomplishing certain objec-\ntives in the game.\n9.7. Screen Shots and Movie Capture\nAnother extremely useful facility is the ability to capture screen shots and \nwrite them to disk in a suitable image format such as Windows Bitmap ﬁ les \n",
      "content_length": 2113,
      "extraction_method": "Direct"
    },
    {
      "page_number": 407,
      "chapter": null,
      "content": "385 \n(.bmp) or Targa (.tga). The details of how to capture a screen shot vary from \nplatform to platform, but they typically involve making a call to the graphics \nAPI that allows the contents of the frame buﬀ er to be transferred from video \nRAM to main RAM, where it can be scanned and converted into the image \nﬁ le format of your choice. The image ﬁ les are typically writt en to a predeﬁ ned \nfolder on disk and named using a date and time stamp to guarantee unique \nﬁ le names.\nYou may want to provide your users with various options controlling \nhow screen shots are to be captured. Some common examples include:\nz Whether or not to include debug lines and text in the screen shot.\nz Whether or not to include heads-up display (HUD) elements in the \nscreen shot.\nz The resolution at which to capture. Some engines allow high resolution \nscreen shots to be captured, perhaps by modifying the projection matrix \nso that separate screen shots can be taken of the four quadrants of the \nscreen at normal resolution and then combined into the ﬁ nal high-res \nimage.\nz Simple camera animations. For example, you could allow the user to \nmark the starting and ending positions and orientations of the camera. \nA sequence of screen shots could then be taken while gradually interpo-\nlating the camera from the start location to the ending location.\nSome engines also provide a full-ﬂ edged movie capture mode. Such a sys-\ntem captures a sequence of screen shots at the target frame rate of the game, \nwhich are typically processed oﬄ  ine to generate a movie ﬁ le in a suitable \nformat such as AVI or MP4.\nCapturing a screen shot is usually a relatively slow operation, due in \npart to the time required to transfer the frame buﬀ er data from video RAM \nto main RAM (an operation for which the graphics hardware is usually not \noptimized), and in larger part to the time required to write image ﬁ les to disk. \nIf you want to capture movies in real time (or at least close to real time), you’ll \nalmost certainly need to store the captured images to a buﬀ er in main RAM, \nonly writing them out to disk when the buﬀ er has been ﬁ lled (during which \nthe game will typically be frozen).\n9.8. In-Game Proﬁ ling\n Games are real-time systems, so achieving and maintaining a high frame rate \n(usually 30 FPS or 60 FPS) is important. Therefore, part of any game program-\n9.8. In-Game Proﬁ ling \n",
      "content_length": 2388,
      "extraction_method": "Direct"
    },
    {
      "page_number": 408,
      "chapter": null,
      "content": "386 \n9. Tools for Debugging and Development\nmer’s job is ensuring that his or her code runs eﬃ  ciently and within budget. \nAs we saw when we discussed the 80-20 and 90-10 rules in Chapter 2, a large \npercentage of your code probably doesn’t need to be optimized. The only way \nto know which bits require optimization is to measure your game’s performance. \nWe discussed various third-party proﬁ ling tools in Chapter 2. However, these \ntools have various limitations and may not be available at all on a console. For \nFigure 9.11.  The Uncharted 2 engine also provides a proﬁ le hierarchy display that allows the \nuser to drill down into particular function calls in inspect their costs.\nFigure 9.10. The proﬁ le category display in the Uncharted 2: Among Theives engine shows \ncoarse timing ﬁ gures for various top-level engine systems.\n",
      "content_length": 839,
      "extraction_method": "Direct"
    },
    {
      "page_number": 409,
      "chapter": null,
      "content": "387 \nthis reason, and/or for convenience, many game engines provide an in-game \nproﬁ ling tool of some sort.\nTypically an in-game proﬁ ler permits the programmer to annotate blocks \nof code which should be timed and give them human-readable names. The \nproﬁ ler measures the execution time of each annotated block via the CPU’s \nhi-res timer, and stores the results in memory. A heads-up display is provided \nwhich shows up-to-date execution times for each code block (examples are \nshown in Figure 9.10, Figure 9.11, and Figure 9.12). The display oft en provides \nthe data in various forms, including raw numbers of cycles, execution times \nin micro-seconds, and percentages relative to the execution time of the entire \nframe.\n9.8.1. Hierarchical Proﬁ ling\n Computer programs writt en in an imperative language are inherently hierar-\nchical—a function calls other functions, which in turn call still more functions. \nFor example, let’s imagine that function a() calls functions b() and c(), and \nfunction b() in turn calls functions d(), e() and f(). The pseudocode for this \nis shown below.\nvoid a()\n{\n b();\n c();\n}\n9.8. In-Game Proﬁ ling \nFigure 9.12. The timeline mode in Uncharted 2 shows exactly when various operations are \nperformed across a single frame on the PS3’s SPUs, GPU and PPU.\n",
      "content_length": 1296,
      "extraction_method": "Direct"
    },
    {
      "page_number": 410,
      "chapter": null,
      "content": "388 \n9. Tools for Debugging and Development\nvoid b()\n{\n d();\n e();\n f();\n}\nvoid c() { ... }\nvoid d() { ... }\nvoid e() { ... }\nvoid f() { ... }\nAssuming function a() is called directly from main(), this function call hier-\narchy is shown in Figure 9.13.\nWhen debugging a program, the call stack shows only a snapshot of this \ntree. Speciﬁ cally, it shows us the path from whichever function in the hierarchy \nis currently executing all the way to the root function in the tree. In C/C++, the \nroot function is usually main() or WinMain(), although technically this func-\ntion is called by a start-up function that is part of the standard C runtime library \n(CRT), so that function is the true root of the hierarchy. If we set a breakpoint in \nfunction e(), for example, the call stack would look something like this:\ne()\nÅ The currently-executing function.\nb()\na()\nmain()\n_crt_startup()\nÅ Root of the call hierarchy.\nThis call stack is depicted in Figure 9.14 as a pathway from function e() to the \nroot of the function call tree.\nf()\na()\nb()\nc()\nd()\ne()\nFigure 9.13.  A hy-\npothetical \nfunc-\ntion call hierar-\nchy.\nf()\na()\nb()\nc()\nd()\ne()\nmain()\n_crt_startup()\nFigure 9.14.  Call stack resulting from setting a break point in function e().\n",
      "content_length": 1240,
      "extraction_method": "Direct"
    },
    {
      "page_number": 411,
      "chapter": null,
      "content": "389 \n9.8.1.1. \nMeasuring Execution Times Hierarchically\nIf we measure the execution time of a single function, the time we measure \nincludes the execution time of any the child functions called and all of their \ngrandchildren, great grandchildren, and so on as well. To properly interpret \nany proﬁ ling data we might collect, we must be sure to take the function call \nhierarchy into account.\nMany commercial proﬁ lers can automatically instrument every single \nfunction in your program. This permits them to measure both the inclusive \nand exclusive execution times of every function that is called during a proﬁ l-\ning session. As the name implies, inclusive times measure the execution time \nof the function including all of its children, while exclusive times measure \nonly the time spent in the function itself. (The exclusive time of a function \ncan be calculated by subtracting the inclusive times of all its immediate chil-\ndren from the inclusive time of the function in question.) In addition, some \nproﬁ lers record how many times each function is called. This is an impor-\ntant piece of information to have when optimizing a program, because it al-\nlows you to diﬀ erentiate between functions that eat up a lot of time internally \nand functions that eat up time because they are called a very large number of \ntimes.\nIn contrast, in-game proﬁ ling tools are not so sophisticated and usually \nrely on manual instrumentation of the code. If our game engine’s main loop \nis structured simply enough, we may be able to obtain valid data at a coarse \nlevel without thinking much about the function call hierarchy. For example, a \ntypical game loop might look roughly like this:\nwhile (!quitGame)\n{\n PollJoypad();\n UpdateGameObjects();\n UpdateAllAnimations();\n PostProcessJoints();\n DetectCollisions();\n RunPhysics();\n GenerateFinalAnimationPoses();\n UpdateCameras();\n RenderScene();\n UpdateAudio();\n}\nWe could proﬁ le this game at a very coarse level by measuring the execution \ntimes of each major phase of the game loop:\nwhile (!quitGame)\n{\n9.8. In-Game Proﬁ ling \n",
      "content_length": 2075,
      "extraction_method": "Direct"
    },
    {
      "page_number": 412,
      "chapter": null,
      "content": "390 \n9. Tools for Debugging and Development\n {\n  PROFILE(\"Poll \nJoypad\");\n  PollJoypad();\n }\n {\n \n \nPROFILE(\"Game Object Update\");\n  UpdateGameObjects();\n }\n {\n  PROFILE(\"Animation\");\n  UpdateAllAnimations();\n }\n {\n  PROFILE(\"Joint \nPost-Processing\");\n  PostProcessJoints();\n }\n {\n  PROFILE(\"Collision\");\n  DetectCollisions();\n }\n {\n  PROFILE(\"Physics\");\n  RunPhysics();\n }\n {\n  PROFILE(\"Animation \nFinaling\");\n  GenerateFinalAnimationPoses();\n }\n {\n  PROFILE(\"Cameras\");\n  UpdateCameras();\n }\n {\n  PROFILE(\"Rendering\");\n  RenderScene();\n }\n {\n  PROFILE(\"Audio\");\n  UpdateAudio();\n }\n}\nThe PROFILE() macro shown above would probably be implemented as a \nclass whose constructor starts the timer and whose destructor stops the timer \nand records the execution time under the given name. Thus it only times the \ncode within its containing block, by nature of the way C++ automatically con-\nstructs and destroys objects as they go in and out of scope.\n",
      "content_length": 949,
      "extraction_method": "Direct"
    },
    {
      "page_number": 413,
      "chapter": null,
      "content": "391 \nstruct AutoProfile\n{\n \nAutoProfile(const char* name)\n {\n \n \nm_name = name;\n \n \nm_startTime = QueryPerformanceCounter();\n }\n ~AutoProfile()\n {\n \n \n__int64 endTime = QueryPerformanceCounter();\n \n \n__int64 elapsedTime = endTime – m_startTime;\n  g_profileManager.storeSample(m_name, \nelapsedTime);\n }\n \nconst char* m_name;\n __int64 \n  m_startTime;\n};\n#define PROFILE(name) AutoProfile p(name)\nThe problem with this simplistic approach is that it breaks down when \nused within deeper levels of function call nesting. For example, if we embed \nadditional PROFILE() annotations within the RenderScene() function, we \nneed to understand the function call hierarchy in order to properly interpret \nthose measurements.\nOne solution to this problem is to allow the programmer who is an-\nnotating the code to indicate the hierarchical interrelationships between \nproﬁ ling samples. For example, any PROFILE(...) samples taken with-\nin the RenderScene() function could be declared to be children of the \nPROFILE(\"Rendering\") sample. These relationships are usually set up sepa-\nrately from the annotations themselves, by predeclaring all of the sample bins. \nFor example, we might set up the in-game proﬁ ler during engine initialization \nas follows:\n// This code declares various profile sample \"bins\", \n// listing the name of the bin and the name of its \n// parent bin, if any.\nProfilerDeclareSampleBin(\"Rendering\", NULL);\n \nProfilerDeclareSampleBin(\"Visibility\", \"Rendering\");\n \nProfilerDeclareSampleBin(\"ShaderSetUp\", \"Rendering\");\n  ProfilerDeclareSampleBin(\"Materials\", \n\"Shaders\");\n \nProfilerDeclareSampleBin(\"SubmitGeo\", \"Rendering\");\nProfilerDeclareSampleBin(\"Audio\", NULL);\n ...\n9.8. In-Game Proﬁ ling \n",
      "content_length": 1705,
      "extraction_method": "Direct"
    },
    {
      "page_number": 414,
      "chapter": null,
      "content": "392 \n9. Tools for Debugging and Development\nThis approach still has its problems. Speciﬁ cally, it works well when every \nfunction in the call hierarchy has only one parent, but it breaks down when \nwe try to proﬁ le a function that is called by more than one parent function. \nThe reason for this should be prett y obvious. We’re statically declaring our \nsample bins as if every function can only appear once in the function call hi-\nerarchy, but actually the same function can reappear many times in the tree, \neach time with a diﬀ erent parent. The result can be misleading data, because a \nfunction’s time will be included in one of the parent bins, but really should be \ndistributed across all of its parents’ bins. Most game engines don’t make an at-\ntempt to remedy this problem, since they are primarily interested in proﬁ ling \ncoarse-grained functions that are only called from one speciﬁ c location in the \nfunction call hierarchy. But this limitation is something to be aware of when \nproﬁ ling your code with a simple in-engine proﬁ le of the sort found in most \ngame engines.\nWe would also like to account for how many times a given function is \ncalled. In the example above, we know that each of the functions we proﬁ led \nare called exactly once per frame. But other functions, deeper in the func-\ntion call hierarchy, may be called more than once per frame. If we measure \nfunction x() to take 2 ms to execute, it’s important to know whether it takes \n2 ms to execute on its own, or whether it executes in 2 μs but was called 1000 \ntimes during the frame. Keeping track of the number of times a function is \ncalled per frame is quite simple—the proﬁ ling system can simply increment \na counter each time a sample is received and reset the counters at the start of \neach frame.\n9.8.2. Exporting to Excel\nSome game engines permit the data captured by the in-game proﬁ ler to be \ndumped to a text ﬁ le for subsequent analysis. I ﬁ nd that a comma-separat-\ned values (CSV ) format is best, because such ﬁ les can be loaded easily into \na Microsoft  Excel spreadsheet, where the data can be manipulated and ana-\nlyzed in myriad ways. I wrote such an exporter for the Medal of Honor: Paciﬁ c \nAssault engine. The columns corresponded to the various annotated blocks, \nand each row represented the proﬁ ling sample taken during one frame of \nthe game’s execution. The ﬁ rst column contained frame numbers and the sec-\nond actual game time measured in seconds. This allowed the team to graph \nhow the performance statistics varied over time and to determine how long \neach frame actually took to execute. By adding some simple formulae to the \nexported spreadsheet, we could calculate frame rates, execution time percent-\nages, and so on.\n",
      "content_length": 2749,
      "extraction_method": "Direct"
    },
    {
      "page_number": 415,
      "chapter": null,
      "content": "393 \n9.9. In-Game Memory Stats and Leak Detection\nIn addition to runtime performance (i.e., frame rate), most game engines are \nalso constrained by the amount of memory available on the target hardware. \nPC games are least aﬀ ected by such constraints, because modern PCs have \nsophisticated virtual memory managers. But even PC games are constrained \nby the memory limitations of their so-called “min spec” machine—the least-\npowerful machine on which the game is guaranteed to run, as promised by \nthe publisher and stated on the game’s packaging.\n For this reason, most game engines implement custom memory-tracking \ntools. These tools allow the developers to see how much memory is being \nused by each engine subsystem and whether or not any memory is leaking \n(i.e., memory is allocated but never freed). It’s important to have this informa-\ntion, so that you can make informed decisions when trying to cut back the \nmemory usage of your game so that it will ﬁ t onto the console or type of PC \nyou are targeting.\nKeeping track of how much memory a game actually uses can be a sur-\nprisingly tricky job. You’d think you could simply wrap malloc()/free() or \nnew/delete in a pair of functions or macros that keep track of the amount of \nmemory that is allocated and freed. However, it’s never that simple for a few \nreasons:\n \n1. You oft en can’t control the allocation behavior of other people’s code. Unless \nyou write the operating system, drivers, and the game engine entire-\nly from scratch, there’s a good chance you’re going to end up linking \nyour game with at least some third-party libraries. Most good libraries \nprovide memory allocation hooks, so that you can replace their allocators \nwith your own. But some do not. It’s oft en diﬃ  cult to keep track of the \nmemory allocated by each and every third-party library you use in your \ngame engine—but it usually can be done if you’re thorough and selec-\ntive in your choice of third-party libraries.\n \n2. Memory comes in diﬀ erent ﬂ avors. For example, a PC has two kinds of \nRAM: main RAM and video RAM (the memory residing on your graph-\nics card, which is used primarily for geometry and texture data). Even \nif you manage to track all of the memory allocations and deallocations \noccurring within main RAM, it can be well neigh impossible to track \nvideo RAM usage. This is because graphics APIs like DirectX actually \nhide the details of how video RAM is being allocated and used from the \ndeveloper. On a console, life is a bit easier, only because you oft en end \nup having to write a video RAM manager yourself. This is more diﬃ  cult \n9.9. In-Game Memory Stats and Leak Detection \n",
      "content_length": 2656,
      "extraction_method": "Direct"
    },
    {
      "page_number": 416,
      "chapter": null,
      "content": "394 \n9. Tools for Debugging and Development\nthan using DirectX, but at least you have complete knowledge of what’s \ngoing on.\n \n3. Allocators come in diﬀ erent ﬂ avors. Many games make use of specialized \nallocators for various purposes. For example, the Uncharted: Drake’s \nFortune engine has a global heap for general-purpose allocations, a spe-\ncial heap for managing the memory created by game objects as they \nspawn into the game world and are destroyed, a level-loading heap for \ndata that is streamed into memory during gameplay, a stack allocator \nfor single-frame allocations (the stack is cleared automatically every \nframe), an allocator for video RAM, and a debug memory heap used only \nfor allocations that will not be needed in the ﬁ nal shipping game. Each \nof these allocators grabs a large hunk of memory when the game starts \nup and then manages that memory block itself. If we were to track all \nthe calls to new and delete, we’d see one new for each of these six al-\nlocators and that’s all. To get any useful information, we really need \nto track all of the allocations within each of these allocators’ memory \nblocks.\nMost professional game teams expend a signiﬁ cant amount of eﬀ ort on \ncreating in-engine memory-tracking tools that provide accurate and detailed \ninformation. The resulting tools usually provide their output in a variety of \nforms. For example, the engine might produce a detailed dump of all memory \nallocations made by the game during a speciﬁ c period of time. The data might \ninclude high water marks for each memory allocator or each game system, \nindicating the maximum amount of physical RAM required by each. Some \nengines also provide heads-up displays of memory usage while the game is \nFigure 9.15.  Tabular memory statistics from the Uncharted 2: Among Thieves engine.\n",
      "content_length": 1823,
      "extraction_method": "Direct"
    },
    {
      "page_number": 417,
      "chapter": null,
      "content": "395 \nrunning. This data might be tabular, as shown in Figure 9.15, or graphical as \nshown in Figure 9.16.\nIn addition, when low-memory or out-of-memory conditions arise, a good \nengine will provide this information in as helpful a way as possible. When PC \ngames are developed, the game team usually works on high-powered PCs \nwith more RAM than the min-spec machine being targeted. Likewise, console \ngames are developed on special development kits which have more memory \nthan a retail console. So in both cases, the game can continue to run even when \nit technically has run out of memory (i.e., would no longer ﬁ t on a retail con-\nsole or min-spec PC). When this kind of out-of-memory condition arises, the \ngame engine can display a message saying something like, “Out of memory—\nthis level will not run on a retail system.”\nThere are lots of other ways in which a game engine’s memory tracking \nsystem can aid developers in pinpointing problems as early and as conve-\nniently as possible. Here are just a few examples:\nz If a model fails to load, a bright red text string could be displayed in 3D \nhovering in the game world where that object would have been.\nz If a texture fails to load, the object could be drawn with an ugly pink \ntexture that is very obviously not part of the ﬁ nal game.\nz If an animation fails to load, the character could assume a special (pos-\nsibly humorous) pose that indicates a missing animation, and the name \nof the missing asset could hover over the character’s head.\nThe key to providing good memory analysis tools is (a) to provide accurate \ninformation, (b) to present the data in a way that is convenient and that makes \nproblems obvious, and (c) to provide contextual information to aid the team \nin tracking down the root cause of problems when they occur. \nFigure 9.16.  A graphical memory usage display, also from Uncharted 2.\n9.9. In-Game Memory Stats and Leak Detection \n",
      "content_length": 1921,
      "extraction_method": "Direct"
    },
    {
      "page_number": 418,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 419,
      "chapter": null,
      "content": "Part III\nGraphics and Motion\n",
      "content_length": 29,
      "extraction_method": "Direct"
    },
    {
      "page_number": 420,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 421,
      "chapter": null,
      "content": "399\n10\nThe Rendering Engine\nW\nhen most people think about computer and video games, the ﬁ rst thing \nthat comes to mind is the stunning three-dimensional graphics. Real-\ntime 3D rendering is an exceptionally broad and profound topic, so there’s \nsimply no way to cover all of the details in a single chapter. Thankfully there \nare a great many excellent books and other resources available on this topic. \nIn fact, real-time 3D graphics is perhaps one of the best covered of all the tech-\nnologies that make up a game engine. The goal of this chapter, then, is to pro-\nvide you with a broad understanding of real-time rendering technology and \nto serve as a jumping-oﬀ  point for further learning. Aft er you’ve read through \nthese pages, you should ﬁ nd that reading other books on 3D graphics seems \nlike a journey through familiar territory. You might even be able to impress \nyour friends at parties (… or alienate them…)\nWe’ll begin by laying a solid foundation in the concepts, theory, and math-\nematics that underlie any real-time 3D rendering engine. Next, we’ll have \na look at the soft ware and hardware pipelines used to turn this theoretical \nframework into reality. We’ll discuss some of the most common optimization \ntechniques and see how they drive the structure of the tools pipeline and the \nruntime rendering API in most engines. We’ll end with a survey of some of the \nadvanced rendering techniques and lighting models in use by game engines \ntoday. Throughout this chapter, I’ll point you to some of my favorite books \n",
      "content_length": 1540,
      "extraction_method": "Direct"
    },
    {
      "page_number": 422,
      "chapter": null,
      "content": "400 \n10. The Rendering Engine\nand other resources that should help you to gain an even deeper understand-\ning of the topics we’ll cover here.\n10.1. Foundations of Depth-Buffered \nTriangle Rasterization\nWhen you boil it down to its essence, rendering a three-dimensional scene \ninvolves the following basic steps:\nz A virtual scene is described, usually in terms of 3D surfaces represented \nin some mathematical form.\nz A virtual camera is positioned and oriented to produce the desired view \nof the scene. Typically the camera is modeled as an idealized focal point, \nwith an imaging surface hovering some small distance in front of it, \ncomposed of virtual light sensors corresponding to the picture elements \n(pixels ) of the target display device .\nz Various light sources are deﬁ ned. These sources provide all the light rays \nthat will interact with and reﬂ ect oﬀ  the objects in the environment and \neventually ﬁ nd their way onto the image-sensing surface of the virtual \ncamera.\nz The visual properties of the surfaces in the scene are described. This de-\nﬁ nes how light should interact with each surface.\nz For each pixel within the imaging rectangle, the rendering engine calcu-\nlates the color and intensity of the light ray(s) converging on the virtual \ncamera’s focal point through that pixel. This is known as solving the ren-\ndering equation (also called the shading equation).\nThis high-level rendering process is depicted in Figure 10.1.\nMany diﬀ erent technologies can be used to perform the basic render-\ning steps described above. The primary goal is usually photorealism , although \nsome games aim for a more stylized look (e.g., cartoon, charcoal sketch, wa-\ntercolor, and so on). As such, rendering engineers and artists usually att empt \nto describe the properties of their scenes as realistically as possible and to \nuse light transport models that match physical reality as closely as possible. \nWithin this context, the gamut of rendering technologies ranges from tech-\nniques designed for real-time performance at the expense of visual ﬁ delity, \nto those designed for photorealism but which are not intended to operate in \nreal time.\nReal-time rendering engines perform the steps listed above repeatedly, \ndisplaying rendered images at a rate of 30, 50, or 60 frames per second to \n",
      "content_length": 2313,
      "extraction_method": "Direct"
    },
    {
      "page_number": 423,
      "chapter": null,
      "content": "401 \n10.1. Foundations of Depth-Buffered Triangle Rasterization\nprovide the illusion of motion. This means a real-time rendering engine has at \nmost 33.3 ms to generate each image (to achieve a frame rate of 30 FPS). Usu-\nally much less time is available, because bandwidth is also consumed by other \nengine systems like animation, AI, collision detection, physics simulation, au-\ndio, player mechanics, and other gameplay logic. Considering that ﬁ lm ren-\ndering engines oft en take anywhere from many minutes to many hours to \nrender a single frame, the quality of real-time computer graphics these days \nis truly astounding.\n10.1.1. Describing a Scene\nA real-world scene is composed of objects. Some objects are solid, like a brick, \nand some are amorphous, like a cloud of smoke, but every object occupies a \nvolume of 3D space. An object might be opaque (in which case light cannot \npass through its volume), transparent (in which case light passes through it \nwithout being scatt ered, so that we can see a reasonably clear image of what-\never is behind the object), or translucent (meaning that light can pass through \nthe object but is scatt ered in all directions in the process, yielding only a blur \nof colors that hint at the objects behind it).\nOpaque objects can be rendered by considering only their surfaces . We \ndon’t need to know what’s inside an opaque object in order to render it, be-\ncause light cannot penetrate its surface. When rendering a transparent or \ntranslucent object, we really should model how light is reﬂ ected, refracted, \nscatt ered, and absorbed as it passes through the object’s volume. This requires \nknowledge of the interior structure and properties of the object. However, \nmost game engines don’t go to all that trouble. They just render the surfaces \nVirtual \nScreen\n(Near Plane)\nxC\nzC\nyC\nRendered \nImage\nCamera \nFrustum\nCamera\nFigure 10.1. The high-level rendering approach used by virtually all 3D computer graphics \ntechnologies.\n",
      "content_length": 1980,
      "extraction_method": "Direct"
    },
    {
      "page_number": 424,
      "chapter": null,
      "content": "402 \n10. The Rendering Engine\nof transparent and translucent objects in almost the same way opaque objects \nare rendered. A simple numeric opacity measure known as alpha is used to \ndescribe how opaque or transparent a surface is. This approach can lead to \nvarious visual anomalies (for example, surface features on the far side of the \nobject may be rendered incorrectly), but the approximation can be made to \nlook reasonably realistic in many cases. Even amorphous objects like clouds \nof smoke are oft en represented using particle eﬀ ects, which are typically com-\nposed of large numbers of semi-transparent rectangular cards. Therefore, it’s \nsafe to say that most game rendering engines are primarily concerned with \nrendering surfaces.\n10.1.1.1. Representations Used by High-End Rendering Packages\nTheoretically, a surface is a two-dimensional sheet comprised of an inﬁ nite \nnumber of points in three-dimensional space. However, such a description is \nclearly not practical. In order for a computer to process and render arbitrary \nsurfaces, we need a compact way to represent them numerically.\nSome surfaces can be described exactly in analytical form, using a para-\nmetric surface equation . For example, a sphere centered at the origin can be rep-\nresented by the equation x2 + y2 + z2 = r2. However, parametric equations aren’t \nparticularly useful for modeling arbitrary shapes.\nIn the ﬁ lm industry, surfaces are oft en represented by a collection of rect-\nangular patches each formed from a two-dimensional spline deﬁ ned by a small \nnumber of control points. Various kinds of splines are used, including Bézi-\ner surfaces (e.g., bicubic patches , which are third-order Béziers —see htt p://\nen.wikipedia.org/wiki/Bezier_surface for more information), nonuniform \nrational B-splines (NURBS—see htt p://en.wikipedia.org/wiki/Nurbs), Bézi-\ner triangles, and N-patches (also known as normal patches—see htt p://www.\ngamasutra.com/features/20020715/mollerhaines_01.htm for more details). \nModeling with patches is a bit like covering a statue with litt le rectangles of \ncloth or paper maché.\nHigh-end ﬁ lm rendering engines like Pixar’s RenderMan use subdivision \nsurfaces to deﬁ ne geometric shapes. Each surface is represented by a mesh of \ncontrol polygons (much like a spline), but the polygons can be subdivided \ninto smaller and smaller polygons using the Catmull-Clark algorithm. This \nsubdivision typically proceeds until the individual polygons are smaller than \na single pixel in size. The biggest beneﬁ t of this approach is that no matt er \nhow close the camera gets to the surface, it can always be subdivided further \nso that its silhouett e edges won’t look faceted. To learn more about subdivi-\nsion surfaces, check out the following great article on Gamasutra: htt p://www.\ngamasutra.com/features/20000411/sharp_pfv.htm.\n",
      "content_length": 2852,
      "extraction_method": "Direct"
    },
    {
      "page_number": 425,
      "chapter": null,
      "content": "403 \n10.1.1.2. Triangle Meshes\n Game developers have traditionally modeled their surfaces using triangle \nmeshes. Triangles serve as a piece-wise linear approximation to a surface, \nmuch as a chain of connected line segments acts as a piece-wise approxima-\ntion to a function or curve (see Figure 10.2).\nTriangles are the polygon of choice for real-time rendering because they \nhave the following desirable properties:\nz The triangle is the simplest type of polygon. Any fewer than three vertices, \nand we wouldn’t have a surface at all.\nz A triangle is always planar. Any polygon with four or more vertices need \nnot have this property, because while the ﬁ rst three vertices deﬁ ne a \nplane, the fourth vertex might lie above or below that plane.\nz Triangles remain triangles under most kinds of transformations, including \naﬃ  ne transforms and perspective projections. At worst, a triangle viewed \nedge-on will degenerate into a line segment. At every other orientation, \nit remains triangular.\nz Virtually all commercial graphics-acceleration hardware is designed around \ntriangle rasterization. Starting with the earliest 3D graphics accelerators \nfor the PC, rendering hardware has been designed almost exclusively \naround triangle rasterization. This decision can be traced all the way \nback to the ﬁ rst soft ware rasterizers used in the earliest 3D games like \nCastle Wolfenstein 3D and Doom . Like it or not, triangle-based technolo-\ngies are entrenched in our industry and probably will be for years to \ncome.\nTessellation\nThe term tessellation describes a process of dividing a surface up into a collec-\ntion of discrete polygons (which are usually either quadrilaterals, also known \nas quads, or triangles). Triangulation is tessellation of a surface into triangles.\nOne problem with the kind of triangle mesh used in games is that its level \nof tessellation is ﬁ xed by the artist when he or she creates it. Fixed tessellation \nFigure 10.2. A mesh of \ntriangles is a linear ap-\nproximation to a sur-\nface, just as a series of \nconnected \nline \nseg-\nments can serve as a \nlinear approximation to \na function or curve.\nx\nf(x)\nFigure 10.3. Fixed tessellation can cause an object’s silhouette edges to look blocky, especially \nwhen the object is close to the camera.\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n",
      "content_length": 2337,
      "extraction_method": "Direct"
    },
    {
      "page_number": 426,
      "chapter": null,
      "content": "404 \n10. The Rendering Engine\ncan cause an object’s silhouett e edges to look blocky, as shown in Figure 10.3; \nthis is especially noticeable when the object is close to the camera.\nIdeally, we’d like a solution that can arbitrarily increase tessellation as an \nobject gets closer to the virtual camera. In other words, we’d like to have a uni-\nform triangle-to-pixel density, no matt er how close or far away the object is. \nSubdivision surfaces can achieve this ideal—surfaces can be tessellated based \non distance from the camera, so that every triangle is less than one pixel in \nsize.\nGame developers oft en att empt to approximate this ideal of uniform tri-\nangle-to-pixel density by creating a chain of alternate versions of each triangle \nmesh, each known as a level of detail (LOD). The ﬁ rst LOD, oft en called LOD 0, \nrepresents the highest level of tessellation; it is used when the object is very \nclose to the camera. Subsequent LODs are tessellated at lower and lower reso-\nlutions (see Figure 10.4). As the object moves farther away from the camera, \nthe engine switches from LOD 0 to LOD 1 to LOD 2, and so on. This allows the \nrendering engine to spend the majority of its time transforming and lighting \nthe vertices of the objects that are closest to the camera (and therefore occupy \nthe largest number of pixels on-screen).\nSome game engines apply dynamic tessellation techniques to expansive \nmeshes like water or terrain. In this technique, the mesh is usually represented \nby a height ﬁ eld deﬁ ned on some kind of regular grid patt ern. The region of \nthe mesh that is closest to the camera is tessellated to the full resolution of \nthe grid. Regions that are farther away from the camera are tessellated using \nfewer and fewer grid points.\nProgressive meshes are another technique for dynamic tessellation and \nLODing. With this technique, a single high-resolution mesh is created for dis-\nplay when the object is very close to the camera. (This is essentially the LOD 0 \nFigure 10.4. A chain of LOD meshes, each with a ﬁ xed level of tessellation, can be used to \napproximate uniform triangle-to-pixel density. The leftmost torus is constructed from 5000 \ntriangles, the center torus from 450 triangles, and the rightmost torus from 200 triangles.\n",
      "content_length": 2276,
      "extraction_method": "Direct"
    },
    {
      "page_number": 427,
      "chapter": null,
      "content": "405 \nmesh.) This mesh is automatically detessellated as the object gets farther away \nby collapsing certain edges. In eﬀ ect, this process automatically generates a \nsemi-continuous chain of LODs. See htt p://research.microsoft .com/en-us/um/\npeople/hoppe/pm.pdf for a detailed discussion of progressive mesh technol-\nogy.\n10.1.1.3. Constructing a Triangle Mesh\n Now that we understand what triangle meshes are and why they’re used, let’s \ntake a brief look at how they’re constructed.\nWinding Order\nA triangle is deﬁ ned by the position vectors of its three vertices, which we can \ndenote p1 , p2 , and p3. The edges of a triangle can be found by simply subtract-\ning the position vectors of adjacent vertices. For example,\n \ne12 = p2 – p1 ,\ne13 = p3 – p1 ,\n \ne23 = p3 – p2.\nThe normalized cross product of any two edges deﬁ nes a unit face normal N:\n \n12\n13\n12\n13\n.\n×\n=\n×\ne\ne\nN\ne\ne\n \nThese derivations are illustrated in Figure 10.5. To know the direction of the \nface normal (i.e., the sense of the edge cross product), we need to deﬁ ne which \nside of the triangle should be considered the front (i.e., the outside surface of \nan object) and which should be the back (i.e., its inside surface). This can be \ndeﬁ ned easily by specifying a winding order —clockwise (CW) or counterclock-\nwise (CCW).\nMost low-level graphics APIs give us a way to cull back-facing triangles \nbased on winding order. For example, if we set the cull mode parameter in Di-\nFigure 10.5. Deriving the edges and plane of a triangle from its vertices.\np1\np2\np3\ne12\nN\ne13\ne23\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n",
      "content_length": 1611,
      "extraction_method": "Direct"
    },
    {
      "page_number": 428,
      "chapter": null,
      "content": "406 \n10. The Rendering Engine\nrect3D (D3DRS_CULL) to D3DCULLMODE_CW, then any triangle whose vertices \nwind in a clockwise fashion in screen space will be treated as a back-facing \ntriangle and will not be drawn.\nBack face culling is important because we generally don’t want to waste \ntime drawing triangles that aren’t going to be visible anyway. Also, rendering \nthe back faces of transparent objects can actually cause visual anomalies. The \nchoice of winding order is an arbitrary one, but of course it must be consistent \nacross all assets in the entire game. Inconsistent winding order is a common \nerror among junior 3D modelers.\nTriangle Lists\nThe easiest way to deﬁ ne a mesh is simply to list the vertices in groups of \nthree, each triple corresponding to a single triangle. This data structure is \nknown as a triangle list ; it is illustrated in Figure 10.6.\nFigure 10.6.  A triangle list.\nV0\nV1\nV2\nV3\nV4\nV5\nV6\nV7\n...\nV5\nV7\nV6\nV0\nV5\nV1\nV1\nV2\nV3\nV0\nV1\nV3\nIndexed Triangle Lists\nYou probably noticed that many of the vertices in the triangle list shown in \nFigure 10.6 were duplicated, oft en multiple times. As we’ll see in Section \n10.1.2.1, we oft en store quite a lot of metadata with each vertex, so repeating \nthis data in a triangle list wastes memory. It also wastes GPU bandwidth, be-\ncause a duplicated vertex will be transformed and lit multiple times.\nFor these reasons, most rendering engines make use of a more eﬃ  cient \ndata structure known as an indexed triangle list . The basic idea is to list the \nvertices once with no duplication and then to use light-weight vertex indi-\nces (usually occupying only 16 bits each) to deﬁ ne the triples of vertices that \nconstitute the triangles. The vertices are stored in an array known as a vertex \n",
      "content_length": 1767,
      "extraction_method": "Direct"
    },
    {
      "page_number": 429,
      "chapter": null,
      "content": "407 \nbuﬀ er (DirectX) or vertex array (OpenGL). The indices are stored in a separate \nbuﬀ er known as an index buﬀ er or index array. This technique is shown in Fig-\nure 10.7.\nStrips and Fans\nSpecialized mesh data structures known as triangle strips and triangle fans are \nsometimes used for game rendering. Both of these data structures eliminate \nthe need for an index buﬀ er, while still reducing vertex duplication to some \ndegree. They accomplish this by predeﬁ ning the order in which vertices must \nappear and how they are combined to form triangles.\nV0\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nIndices\n0\n1\n3\n1\n2\n3\n0\n5\n1\n...\n5\n7\n6\nVertices\nV0\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nFigure 10.7.  An indexed triangle list.\nInterpreted \nas triangles:\n0\n1\n2\n1\n3\n2\n2\n3\n4\n3\n5\n4\nV0\nV1\nV2\nV3\nV4\nV5\nVertices\nV0\nV1\nV2\nV3\nV4\nV5\nFigure 10.8.  A triangle strip.\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n",
      "content_length": 878,
      "extraction_method": "Direct"
    },
    {
      "page_number": 430,
      "chapter": null,
      "content": "408 \n10. The Rendering Engine\nIn a strip, the ﬁ rst three vertices deﬁ ne the ﬁ rst triangle. Each subsequent \nvertex forms an entirely new triangle, along with its previous two neigh-\nbors. To keep the winding order of a triangle strip consistent, the previous \ntwo neighbor vertices swap places aft er each new triangle. A triangle strip is \nshown in Figure 10.8.\nIn a fan, the ﬁ rst three vertices deﬁ ne the ﬁ rst triangle and each subse-\nquent vertex deﬁ nes a new triangle with the previous vertex and the ﬁ rst ver-\ntex in the fan. This is illustrated in Figure 10.9.\nVertex Cache Optimization\n When a GPU processes an indexed triangle list, each triangle can refer to any \nvertex within the vertex buﬀ er. The vertices must be processed in the order \nthey appear within the triangles, because the integrity of each triangle must be \nmaintained for the rasterization stage. As vertices are processed by the vertex \nshader , they are cached for reuse. If a subsequent primitive refers to a vertex \nthat already resides in the cache, its processed att ributes are used instead of \nreprocessing the vertex.\nStrips and fans are used in part because they can potentially save memory \n(no index buﬀ er required) and in part because they tend to improve the cache \ncoherency of the memory accesses made by the GPU to video RAM. Even \nbett er, we can use an indexed strip or indexed fan to virtually eliminate vertex \nduplication (which can oft en save more memory than eliminating the index \nbuﬀ er), while still reaping the cache coherency beneﬁ ts of the strip or fan ver-\ntex ordering.\nIndexed triangle lists can also be cache-optimized without restricting \nourselves to strip or fan vertex ordering. A vertex cache optimizer is an oﬄ  ine \ngeometry processing tool that att empts to list the triangles in an order that \nFigure 10.9.  A triangle fan.\nInterpreted \nas triangles:\n0\n1\n2\n0\n2\n3\n0\n3\n4\nV0\nV1\nV2\nV3\nV4\nVertices\nV0\nV4\nV3\nV2\nV1\n",
      "content_length": 1938,
      "extraction_method": "Direct"
    },
    {
      "page_number": 431,
      "chapter": null,
      "content": "409 \noptimizes vertex reuse within the cache. It generally takes into account factors \nsuch as the size of the vertex cache(s) present on a particular type of GPU and \nthe algorithms used by the GPU to decide when to cache vertices and when \nto discard them. For example, the vertex cache optimizer included in Sony’s \nEdge geometry processing library can achieve rendering throughput that is up \nto 4% bett er than what is possible with triangle stripping.\n10.1.1.4. Model Space\nThe position vectors of a triangle mesh’s vertices are usually speciﬁ ed relative \nto a convenient local coordinate system called model space , local space,  or object \nspace. The origin of model space is usually either in the center of the object or \nat some other convenient location, like on the ﬂ oor between the feet of a char-\nacter or on the ground at the horizontal centroid of the wheels of a vehicle.\nAs we learned in Section 4.3.9.1, the sense of the model space axes is ar-\nbitrary, but the axes typically align with the natural “front,” “left ” or “right,” \nand “up” directions on the model. For a litt le mathematical rigor, we can de-\nﬁ ne three unit vectors F, L (or R), and U and map them as desired onto the \nunit basis vectors i, j, and k (and hence to the x-, y-, and z-axes, respectively) \nin model space. For example, a common mapping is L = i, U = j, and F = k. \nThe mapping is completely arbitrary, but it’s important to be consistent for all \nmodels across the entire engine. Figure 10.10 shows one possible mapping of \nthe model space axes for an aircraft  model.\nL = i\nF = k\nU = j\nFigure 10.10.  One possible mapping of the model space axes.\n10.1.1.5. World Space and Mesh Instancing\nMany individual meshes are composed into a complete scene by position-\ning and orienting them within a common coordinate system known as world \nspace . Any one mesh might appear many times in a scene—examples include \na street lined with identical lamp posts, a faceless mob of soldiers, or a swarm \nof spiders att acking the player. We call each such object a mesh instance .\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n",
      "content_length": 2127,
      "extraction_method": "Direct"
    },
    {
      "page_number": 432,
      "chapter": null,
      "content": "410 \n10. The Rendering Engine\nA mesh instance contains a reference to its shared mesh data and also \nincludes a transformation matrix that converts the mesh’s vertices from model \nspace to world space, within the context of that particular instance. This ma-\ntrix is called the model-to-world matrix, or sometimes just the world matrix. Us-\ning the notation from Section 4.3.10.2, this matrix can be writt en as follows:\n \n \n(\n)\n,\n1\nM\nW\nM\nW\nM\n→\n→\n⎡\n⎤\n=⎢\n⎥\n⎣\n⎦\nRS\n0\nM\nt\nwhere the upper 3 × 3 matrix (\n)M\nW\n→\nRS\n rotates and scales model-space ver-\ntices into world space, and \nM\nt\n is the translation of the model space axes ex-\npressed in world space. If we have the unit model space basis vectors \n,\nM\ni\n \n,\nM\nj\nand \n,\nM\nk\n expressed in world space coordinates, this matrix can also be writt en \nas follows:\n \n \n0\n0\n.\n0\n1\nM\nM\nM\nW\nM\nM\n→\n⎡\n⎤\n⎢\n⎥\n⎢\n⎥\n=⎢\n⎥\n⎢\n⎥\n⎢\n⎥\n⎣\n⎦\ni\nj\nM\nk\nt\n \nGiven a vertex expressed in model-space coordinates, the rendering en-\ngine calculates its world-space equivalent as follows:\n \n.\nW\nM\nM\nW\n→\n=\nv\nv M\nWe can think of the matrix MM→W as a description of the position and orienta-\ntion of the model space axes themselves, expressed in world space coordi-\nnates. Or we can think of it as a matrix that transforms vertices from model \nspace to world space.\nWhen rendering a mesh, the model-to-world matrix is also applied to the \nsurface normals of the mesh (see Section 10.1.2.1). Recall from Section 4.3.11, \nthat in order to transform normal vectors properly, we must multiply them \nby the inverse transpose of the model-to-world matrix. If our matrix does not \ncontain any scale or shear, we can transform our normal vectors correctly by \nsimply sett ing their w components to zero prior to multiplication by the mod-\nel-to-world matrix, as described in Section 4.3.6.1.\nSome meshes like buildings, terrain, and other background elements are \nentirely static and unique. The vertices of these meshes are oft en expressed in \nworld space, so their model-to-world matrices are identity and can be ignored.\n10.1.2. Describing the Visual Properties of a Surface\n In order to properly render and light a surface, we need a description of its \nvisual properties. Surface properties include geometric information, such as the \n",
      "content_length": 2245,
      "extraction_method": "Direct"
    },
    {
      "page_number": 433,
      "chapter": null,
      "content": "411 \ndirection of the surface normal at various points on the surface. They also \nencompass a description of how light should interact with the surface. This \nincludes diﬀ use color, shininess/reﬂ ectivity, roughness or texture, degree of \nopacity or transparency, index of refraction, and other optical properties. Sur-\nface properties might also include a speciﬁ cation of how the surface should \nchange over time (e.g., how an animated character’s skin should track the \njoints of its skeleton or how the surface of a body of water should move).\nThe key to rendering photorealistic images is properly accounting for \nlight’s behavior as it interacts with the objects in the scene. Hence rendering \nengineers need to have a good understanding of how light works, how it is \ntransported through an environment, and how the virtual camera “senses” it \nand translates it into the colors stored in the pixels on-screen.\n10.1.2.1. Introduction to Light and Color\nLight is electromagnetic radiation; it acts like both a wave and a particle in \ndiﬀ erent situations. The color of light is determined by its intensity I and its \nwavelength λ  (or its frequency f, where f = 1/λ). The visible gamut ranges from \na wavelength of 740 nm (or a frequency of 430 THz) to a wavelength of 380 nm \n(750 THz). A beam of light may contain a single pure wavelength (i.e., the \ncolors of the rainbow, also known as the spectral colors ), or it may contain a \nmixture of various wavelengths. We can draw a graph showing how much of \neach frequency a given beam of light contains, called a spectral plot . White light \ncontains a litt le bit of all wavelengths, so its spectral plot would look roughly \nlike a box extending across the entire visible band. Pure green light contains \nonly one wavelength, so its spectral plot would look like a single inﬁ nitesi-\nmally narrow spike at about 570 THz.\nLight-Object Interactions\nLight can have many complex interactions with matt er . Its behavior is gov-\nerned in part by the medium through which it is traveling and in part by the \nshape and properties of the interfaces between diﬀ erent types of media (air-\nsolid, air-water, water-glass, etc.). Technically speaking, a surface is really just \nan interface between two diﬀ erent types of media.\nDespite all of its complexity, light can really only do four things:\nz It can be absorbed ;\nz It can be reﬂ ected ;\nz It can be transmitt ed through an object, usually being refracted (bent) in \nthe process;\nz It can be diﬀ racted when passing through very narrow openings.\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n",
      "content_length": 2607,
      "extraction_method": "Direct"
    },
    {
      "page_number": 434,
      "chapter": null,
      "content": "412 \n10. The Rendering Engine\nMost photorealistic rendering engines account for the ﬁ rst three of these be-\nhaviors; diﬀ raction is not usually taken into account because its eﬀ ects are \nrarely noticeable in most scenes.\nOnly certain wavelengths may be absorbed by a surface, while others are \nreﬂ ected. This is what gives rise to our perception of the color of an object. \nFor example, when white light falls on a red object, all wavelengths except \nred are absorbed, hence the object appears red. The same perceptual eﬀ ect is \nachieved when red light is cast onto a white object—our eyes don’t know the \ndiﬀ erence.\nReﬂ ections can be diﬀ use , meaning that an incoming ray is scatt ered equal-\nly in all directions. Reﬂ ections can also be specular , meaning that an incident \nlight ray will reﬂ ect directly or be spread only into a narrow cone. Reﬂ ections \ncan also be anisotropic , meaning that the way in which light reﬂ ects from a sur-\nface changes depending on the angle at which the surface is viewed.\nWhen light is transmitt ed through a volume, it can be scatt ered (as is the \ncase for translucent objects), partially absorbed (as with colored glass), or re-\nfracted (as happens when light travels through a prism). The refraction an-\ngles can be diﬀ erent for diﬀ erent wavelengths, leading to spectral spreading. \nThis is why we see rainbows when light passes through raindrops and glass \nprisms. Light can also enter a semi-solid surface, bounce around, and then exit \nthe surface at a diﬀ erent point from the one at which it entered the surface. We \ncall this subsurface scatt ering , and it is one of the eﬀ ects that gives skin, wax, \nand marble their characteristic warm appearance.\nColor Spaces and Color Models\nA color model is a three-dimensional coordinate system that measures colors. \nA color space is a speciﬁ c standard for how numerical colors in a particular \ncolor model should be mapped onto the colors perceived by human beings in \nthe real world. Color models are typically three-dimensional because of the \nthree types of color sensors (cones) in our eyes, which are sensitive to diﬀ erent \nwavelengths of light.\nThe most commonly used color model in computer graphics is the RGB \nmodel. In this model, color space is represented by a unit cube, with the rela-\ntive intensities of red, green, and blue light measured along its axes. The red, \ngreen, and blue components are called color channels . In the canonical RGB \ncolor model, each channel ranges from zero to one. So the color (0, 0, 0) repre-\nsents black, while (1, 1, 1) represents white.\nWhen colors are stored in a bitmapped image , various color formats can \nbe employed. A color format is deﬁ ned in part by the number of bits per pixel \nit occupies and, more speciﬁ cally, the number of bits used to represent each \ncolor channel. The RGB888 format uses eight bits per channel, for a total of \n",
      "content_length": 2900,
      "extraction_method": "Direct"
    },
    {
      "page_number": 435,
      "chapter": null,
      "content": "413 \n24 bits per pixel. In this format, each channel ranges from 0 to 255 rather than \nfrom zero to one. RGB565 uses ﬁ ve bits for red and blue and six for green, for \na total of 16 bits per pixel. A palett ed format might use eight bits per pixel to \nstore indices into a 256-element color palett e, each entry of which might be \nstored in RGB888 or some other suitable format. \nA number of other color models are also used in 3D rendering. We’ll see \nhow the log-LUV color model is used for high dynamic range (HDR) lighting \nin Section 10.3.1.5.\nOpacity and the Alpha Channel\nA fourth channel called alpha  is oft en tacked on to RGB color vectors. As men-\ntioned in Section 10.1.1, alpha measures the opacity of an object. When stored \nin an image pixel, alpha represents the opacity of the pixel.\nRGB color formats can be extended to include an alpha channel, in which \ncase they are referred to as RGBA or ARGB color formats. For example, \nRGBA8888 is a 32 bit-per-pixel format with eight bits each for red, green, blue, \nand alpha. RGBA5551 is a 16 bit-per-pixel format with one-bit alpha; in this \nformat, colors can either be fully opaque or fully transparent.\n10.1.2.2. Vertex Attributes\nThe simplest way to describe the visual properties of a surface is to specify \nthem at discrete points on the surface. The vertices of a mesh are a conve-\nnient place to store surface properties, in which case they are called vertex \natt ributes .\nA typical triangle mesh includes some or all of the following att ributes at \neach vertex. As rendering engineers, we are of course free to deﬁ ne any ad-\nditional att ributes that may be required in order to achieve a desired visual \neﬀ ect on-screen.\nz Position vector (pi = [ pix  piy  piz ]). This is the 3D position of the ith vertex in \nthe mesh. It is usually speciﬁ ed in a coordinate space local to the object, \nknown as model space.\nz Vertex normal (ni = [ nix  niy  niz ]). This vector deﬁ nes the unit surface nor-\nmal at the position of vertex i. It is used in per-vertex dynamic lighting \ncalculations.\nz Vertex tangent (ti = [ tix  tiy  tiz ]) and bitangent (bi = [ bix  biy  biz ]). These two \nunit vectors lie perpendicular to one another and to the vertex normal \nni. Together, the three vectors ni , ti , and bi deﬁ ne a set of coordinate axes \nknown as tangent space . This space is used for various per-pixel lighting \ncalculations, such as normal mapping and environment mapping. (The \n10.1. Foundations of Depth-Buffered Triangle Rasterization\n",
      "content_length": 2513,
      "extraction_method": "Direct"
    },
    {
      "page_number": 436,
      "chapter": null,
      "content": "414 \n10. The Rendering Engine\nbitangent bi is sometimes confusingly called the binormal , even though \nit is not normal to the surface.)\nz Diﬀ use color (di = [ dRi  dGi  dBi  dAi ]). This four-element vector describes \nthe diﬀ use color of the surface, expressed in the RGB color space. It \ntypically also includes a speciﬁ cation of the opacity or alpha (A) of the \nsurface at the position of the vertex. This color may be calculated oﬀ -line \n(static lighting) or at runtime (dynamic lighting).\nz Specular color (si = [ sRi  sGi  sBi  sAi ]). This quantity describes the color of the \nspecular highlight that should appear when light reﬂ ects directly from a \nshiny surface onto the virtual camera’s imaging plane.\nz Texture coordinates (uĳ  = [ uĳ   vĳ  ]). Texture coordinates allow a two- (or \nsometimes three-) dimensional bitmap to be “shrink wrapped” onto the \nsurface of a mesh—a process known as texture mapping. A texture co-\nordinate (u, v) describes the location of a particular vertex within the \ntwo-dimensional normalized coordinate space of the texture. A triangle \ncan be mapped with more than one texture; hence it can have more than \none set of texture coordinates. We’ve denoted the distinct sets of texture \ncoordinates via the subscript j above.\nz Skinning weights (kĳ  , wĳ ). In skeletal animation, the vertices of a mesh are \natt ached to individual joints in an articulated skeleton. In this case, each \nvertex must specify to which joint it is att ached via an index, k. A vertex \ncan be inﬂ uenced by multiple joints, in which case the ﬁ nal vertex posi-\ntion becomes a weighted average of these inﬂ uences. Thus, the weight of \neach joint’s inﬂ uence is denoted by a weighting factor w. In general, a \nvertex i can have multiple joint inﬂ uences j, each denoted by the pair of \nnumbers [ kĳ   wĳ  ].\n10.1.2.3. Vertex Formats\n Vertex att ributes are typically stored within a data structure such as a C \nstruct or a C++ class. The layout of such a data structure is known as a ver-\ntex format. Diﬀ erent meshes require diﬀ erent combinations of att ributes and \nhence need diﬀ erent vertex formats. The following are some examples of com-\nmon vertex formats:\n// Simplest possible vertex – position only (useful for\n// shadow volume extrusion, silhouette edge detection\n// for cartoon rendering, z prepass, etc.)\nstruct Vertex1P\n{\n Vector3 \n m_p; \n // \nposition\n};\n",
      "content_length": 2394,
      "extraction_method": "Direct"
    },
    {
      "page_number": 437,
      "chapter": null,
      "content": "415 \n// A typical vertex format with position, vertex normal \n// and one set of texture coordinates.\nstruct Vertex1P1N1UV\n{\n Vector3 \nm_p; \n // \nposition\n \nVector3 \nm_n; \n \n// vertex normal\n F32 \n \n \nm_uv[2]; // (u, v) texture coordinate\n};\n// A skinned vertex with position, diffuse and specular \n// colors and four weighted joint influences.\nstruct Vertex1P1D1S2UV4J\n{\n Vector3 \nm_p; \n // \nposition\n \nColor4  \nm_d; \n \n// diffuse color and translucency\n Color4 \n m_S; \n // \nspecular color\n F32 \n \n \nm_uv0[2]; // first set of tex coords\n F32 \n \n \nm_uv1[2]; // second set of tex coords\n \nU8  \n \nm_k[4]; \n// four joint indices, and...\n F32 \n \n \nm_w[3]; \n// three joint weights, for   \n \n       // \nskinning\n       // \n(fourth calc’d from other    \n       // \nthree)\n};\nClearly the number of possible permutations of vertex att ributes—and \nhence the number of distinct vertex formats—can grow to be extremely large. \n(In fact the number of formats is theoretically unbounded, if one were to per-\nmit any number of texture coordinates and/or joint weights.) Management \nof all these vertex formats is a common source of headaches for any graphics \nprogrammer.\nSome steps can be taken to reduce the number of vertex formats that an \nengine has to support. In practical graphics applications, many of the theo-\nretically possible vertex formats are simply not useful, or they cannot be \nhandled by the graphics hardware or the game’s shaders. Some game teams \nalso limit themselves to a subset of the useful/feasible vertex formats in or-\nder to keep things more manageable. For example, they might only allow \nzero, two, or four joint weights per vertex, or they might decide to support \nno more than two sets of texture coordinates per vertex. Modern GPUs are \ncapable of extracting a subset of att ributes from a vertex data structure, so \ngame teams can also choose to use a single “überformat” for all meshes and \nlet the hardware select the relevant att ributes based on the requirements of the \nshader.\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n",
      "content_length": 2064,
      "extraction_method": "Direct"
    },
    {
      "page_number": 438,
      "chapter": null,
      "content": "416 \n10. The Rendering Engine\n10.1.2.4. Attribute Interpolation\n The att ributes at a triangle’s vertices are just a coarse, discretized approxima-\ntion to the visual properties of the surface as a whole. When rendering a tri-\nangle, what really matt ers are the visual properties at the interior points of the \ntriangle as “seen” through each pixel on-screen. In other words, we need to \nknow the values of the att ributes on a per-pixel basis, not a per-vertex basis.\nOne simple way to determine the per-pixel values of a mesh’s surface at-\ntributes is to linearly interpolate the per-vertex att ribute data. When applied to \nvertex colors, att ribute interpolation is known as Gouraud shading . An example \nof Gouraud shading applied to a triangle is shown in Figure 10.11, and its ef-\nfects on a simple triangle mesh are illustrated in Figure 10.12. Interpolation is \nroutinely applied to other kinds of vertex att ribute information as well, such \nas vertex normals, texture coordinates, and depth.\nFigure 10.11.  A Gouraud-shaded triangle with different shades of gray at the vertices.\nFigure 10.12.  Gouraud shading can make faceted objects appear to be smooth.\nVertex Normals and Smoothing\nAs we’ll see in Section 10.1.3, lighting is the process of calculating the color of \nan object at various points on its surface, based on the visual properties of the \nsurface and the properties of the light impinging upon it. The simplest way to \nlight a mesh is to calculate the color of the surface on a per-vertex basis. In other \nwords, we use the properties of the surface and the incoming light to calculate \nthe diﬀ use color of each vertex (di). These vertex colors are then interpolated \nacross the triangles of the mesh via Gouraud shading.\n",
      "content_length": 1750,
      "extraction_method": "Direct"
    },
    {
      "page_number": 439,
      "chapter": null,
      "content": "417 \nIn order to determine how a ray of light will reﬂ ect from a point on a sur-\nface, most lighting models make use of a vector that is normal to the surface at \nthe point of the light ray’s impact. Since we’re performing lighting calculations \non a per-vertex basis, we can use the vertex normal ni for this purpose. There-\nfore, the directions of a mesh’s vertex normals can have a signiﬁ cant impact on \nthe ﬁ nal appearance of a mesh.\nAs an example, consider a tall, thin, four-sided box. If we want the box \nto appear to be sharp-edged, we can specify the vertex normals to be perpen-\ndicular to the faces of the box. As we light each triangle, we will encounter the \nsame normal vector at all three vertices, so the resulting lighting will appear \nﬂ at, and it will abruptly change at the corners of the box just as the vertex \nnormals do.\nWe can also make the same box mesh look a bit like a smooth cylinder by \nspecifying vertex normals that point radially outward from the box’s center \nline. In this case, the vertices of each triangle will have diﬀ erent vertex nor-\nmals, causing us to calculate diﬀ erent colors at each vertex. Gouraud shading \nwill smoothly interpolate these vertex colors, resulting in lighting that appears \nto vary smoothly across the surface. This eﬀ ect is illustrated in Figure 10.13.\nFigure 10.13. The directions of a mesh’s vertex normals can have a profound effect on the \ncolors calculated during per-vertex lighting calculations.\n10.1.2.5. Textures\n When triangles are relatively large, specifying surface properties on a per-ver-\ntex basis can be too coarse-grained. Linear att ribute interpolation isn’t always \nwhat we want, and it can lead to undesirable visual anomalies.\nAs an example, consider the problem of rendering the bright specular \nhighlight that can occur when light shines on a glossy object. If the mesh is \n10.1. Foundations of Depth-Buffered Triangle Rasterization\n",
      "content_length": 1929,
      "extraction_method": "Direct"
    },
    {
      "page_number": 440,
      "chapter": null,
      "content": "418 \n10. The Rendering Engine\nhighly tessellated, per-vertex lighting combined with Gouraud shading can \nyield reasonably good results. However, when the triangles are too large, the \nerrors that arise from linearly interpolating the specular highlight can become \njarringly obvious, as shown in Figure 10.14.\nTo overcome the limitations of per-vertex surface att ributes, rendering en-\ngineers use bitmapped images known as texture maps. A texture oft en contains \ncolor information and is usually projected onto the triangles of a mesh. In this \ncase, it acts a bit like those silly fake tatt oos we used to apply to our arms when \nwe were kids. But a texture can contain other kinds of visual surface proper-\nties as well as colors. And a texture needn’t be projected onto a mesh—for \nexample, a texture might be used as a stand-alone data table. The individual \npicture elements of a texture are called texels to diﬀ erentiate them from the \npixels on the screen.\nThe dimensions of a texture bitmap are constrained to be powers of two \non some graphics hardware. Typical texture dimensions include 256 × 256, \n512 × 512, 1024 × 1024, and 2048 × 2048, although textures can be any size on \nmost hardware, provided the texture ﬁ ts into video memory. Some graph-\nics hardware imposes additional restrictions, such as requiring textures to be \nsquare, or lift s some restrictions, such as not constraining texture dimensions \nto be powers of two.\nTypes of Textures\nThe most common type of texture is known as a diﬀ use map , or albedo map . It \ndescribes the diﬀ use surface color at each texel on a surface and acts like a de-\ncal or paint job on the surface.\nOther types of textures are used in computer graphics as well, including \nnormal maps (which store unit normal vectors at each texel, encoded as RGB \nvalues), gloss maps (which encode how shiny a surface should be at each texel), \nFigure 10.14.  Linear interpolation of vertex attributes does not always yield an adequate \ndescription of the visual properties of a surface, especially when tessellation is low.\n",
      "content_length": 2073,
      "extraction_method": "Direct"
    },
    {
      "page_number": 441,
      "chapter": null,
      "content": "419 \nenvironment maps (which contain a picture of the surrounding environment \nfor rendering reﬂ ections), and many others. See Section 10.3.1 for a discussion \nof how various types of textures can be used for image-based lighting and \nother eﬀ ects.\nWe can actually use texture maps to store any information that we happen \nto need in our lighting calculations. For example, a one-dimensional texture \ncould be used to store sampled values of a complex math function, a color-to-\ncolor mapping table, or any other kind of look-up table (LUT) .\nTexture Coordinates\n Let’s consider how to project a two-dimensional texture onto a mesh. To do \nthis, we deﬁ ne a two-dimensional coordinate system known as texture space . \nA texture coordinate is usually represented by a normalized pair of numbers \ndenoted (u, v). These coordinates always range from (0, 0) at the bott om left  \ncorner of the texture to (1, 1) at the top right. Using normalized coordinates \nlike this allows the same coordinate system to be used regardless of the di-\nmensions of the texture.\nTo map a triangle onto a 2D texture, we simply specify a pair of texture \ncoordinates (ui, vi) at each vertex i. This eﬀ ectively maps the triangle onto the \nimage plane in texture space. An example of texture mapping is depicted in \nFigure 10.15.\nFigure 10.15.  An example of texture mapping. The triangles are shown both in three-dimen-\nsional space and in texture space.\nTexture Addressing Modes\n Texture coordinates are permitt ed to extend beyond the [0, 1] range. The \ngraphics hardware can handle out-of-range texture coordinates in any one of \n10.1. Foundations of Depth-Buffered Triangle Rasterization\n",
      "content_length": 1671,
      "extraction_method": "Direct"
    },
    {
      "page_number": 442,
      "chapter": null,
      "content": "420 \n10. The Rendering Engine\nthe following ways. These are known as texture addressing modes; which mode \nis used is under the control of the user.\nz Wrap. In this mode, the texture is repeated over and over in every direc-\ntion. All texture coordinates of the form (ju, kv) are equivalent to the \ncoordinate (u, v), where j and k are arbitrary integers.\nz Mirror. This mode acts like wrap mode, except that the texture is mir-\nrored about the v-axis for odd integer multiples of u, and about the u- \naxis for odd integer multiples of v.\nz Clamp. In this mode, the colors of the texels around the outer edge of the \ntexture are simply extended when texture coordinates fall outside the \nnormal range.\nz Border color. In this mode, an arbitrary user-speciﬁ ed color is used for the \nregion outside the [0, 1] texture coordinate range.\nThese texture addressing modes are depicted in Figure 10.16.\nFigure 10.16.  Texture addressing modes.\nTexture Formats\n Texture bitmaps can be stored on disk in virtually any image format provided \nyour game engine includes the code necessary to read it into memory. Com-\nmon formats include Targa (.tga), Portable Network Graphics (.png), Win-\ndows Bitmap (.bmp), and Tagged Image File Format (.tif). In memory, textures \nare usually represented as two-dimensional (strided) arrays of pixels using \n",
      "content_length": 1334,
      "extraction_method": "Direct"
    },
    {
      "page_number": 443,
      "chapter": null,
      "content": "421 \nvarious color formats, including RGB888, RGBA8888, RGB565, RGBA5551, \nand so on.\nMost modern graphics cards and graphics APIs support compressed tex-\ntures . DirectX supports a family of compressed formats known as DXT or S3 \nTexture Compression (S3TC). We won’t cover the details here, but the basic \nidea is to break the texture into 2 × 2 blocks of pixels and use a small color pal-\nett e to store the colors for each block. You can read more about S3 compressed \ntexture formats at htt p://en.wikipedia.org/wiki/S3_Texture_Compression.\nCompressed textures have the obvious beneﬁ t of using less memory than \ntheir uncompressed counterparts. An additional unexpected plus is that they \nare faster to render with as well. S3 compressed textures achieve this speed-up \nbecause of more cache-friendly memory access patt erns—4 × 4 blocks of ad-\njacent pixels are stored in a single 64- or 128-bit machine word—and because \nmore of the texture can ﬁ t into the cache at once. Compressed textures do \nsuﬀ er from compression artifacts. While the anomalies are usually not notice-\nable, there are situations in which uncompressed textures must be used.\nTexel Density and Mipmapping\nImagine rendering a full-screen quad (a rectangle composed of two triangles) \nthat has been mapped with a texture whose resolution exactly matches that of \nthe screen. In this case, each texel maps exactly to a single pixel on-screen, and \nwe say that the texel density (ratio of texels to pixels) is one. When this same \nquad is viewed at a distance, its on-screen area becomes smaller. The resolu-\ntion of the texture hasn’t changed, so the quad’s texel density is now greater \nthan one (meaning that more than one texel is contributing to each pixel).\nClearly texel density is not a ﬁ xed quantity—it changes as a texture-\nmapped object moves relative to the camera. Texel density aﬀ ects the memory \nconsumption and the visual quality of a three-dimensional scene. When the \ntexel density is much less than one, the texels become signiﬁ cantly larger than \na pixel on-screen, and you can start to see the edges of the texels. This destroys \nthe illusion. When texel density is much greater than one, many texels contrib-\nute to a single pixel on-screen. This can cause a moiré banding patt ern , as shown \nin Figure 10.17. Worse, a pixel’s color can appear to swim and ﬂ icker as diﬀ er-\nent texels within the boundaries of the pixel dominate its color depending on \nsubtle changes in camera angle or position. Rendering a distant object with a \nvery high texel density can also be a waste of memory if the player can never \nget close to it. Aft er all, why keep such a high-res texture in memory if no one \nwill ever see all that detail?\nIdeally we’d like to maintain a texel density that is close to one at all times, \nfor both nearby and distant objects. This is impossible to achieve exactly, but \nit can be approximated via a technique called mipmapping . For each texture, \n10.1. Foundations of Depth-Buffered Triangle Rasterization\n",
      "content_length": 3027,
      "extraction_method": "Direct"
    },
    {
      "page_number": 444,
      "chapter": null,
      "content": "422 \n10. The Rendering Engine\nwe create a sequence of lower-resolution bitmaps, each of which is one-half \nthe width and one-half the height of its predecessor. We call each of these \nimages a mipmap, or mip level. For example, a 64 × 64 texture would have the \nfollowing mip levels: 64 × 64, 32 × 32, 16 × 16, 8 × 8, 4 × 4, 2 × 2, and 1 × 1, as \nshown in Figure 10.18. Once we have mipmapped our textures, the graphics \nhardware selects the appropriate mip level based on a triangle’s distance away \nfrom the camera, in an att empt to maintain a texel density that is close to one. \nFor example, if a texture takes up an area of 40 × 40 on-screen, the 64 × 64 mip \nlevel might be selected; if that same texture takes up only a 10 × 10 area, the \n16 × 16 mip level might be used. As we’ll see below, trilinear ﬁ ltering allows \nthe hardware to sample two adjacent mip levels and blend the results. In this \ncase, a 10 × 10 area might be mapped by blending the 16 × 16 and 8 × 8 mip \nlevels together.\nFigure 10.17.  A texel density greater than one can lead to a moiré pattern.\nFigure 10.18.  Mip levels for a 64×64 texture.\nWorld Space Texel Density\nThe term “texel density ” can also be used to describe the ratio of texels to world \nspace area on a textured surface. For example, a two meter cube mapped with \na 256 × 256 texture would have a texel density of 2562/22 = 16,384. I will call this \nworld space texel density to diﬀ erentiate it from the screen space texel density \nwe’ve been discussing thus far.\n",
      "content_length": 1513,
      "extraction_method": "Direct"
    },
    {
      "page_number": 445,
      "chapter": null,
      "content": "423 \nWorld-space texel density need not be close to one, and in fact the speciﬁ c \nvalue will usually be much greater than one and depends entirely upon your \nchoice of world units. Nonetheless, it is important for objects to be texture \nmapped with a reasonably consistent world space texel density. For example, \nwe would expect all six sides of a cube to occupy the same texture area. If this \nwere not the case, the texture on one side of the cube would have a lower-res-\nolution appearance than another side, which can be noticeable to the player. \nMany game studios provide their art teams with guidelines and in-engine \ntexel density visualization tools in an eﬀ ort to ensure that all objects in the \ngame have a reasonably consistent world space texel density.\nTexture Filtering\n When rendering a pixel of a textured triangle, the graphics hardware samples \nthe texture map by considering where the pixel center falls in texture space. \nThere is usually not a clean one-to-one mapping between texels and pixels, \nand pixel centers can fall at any place in texture space, including directly on \nthe boundary between two or more texels. Therefore, the graphics hardware \nusually has to sample more than one texel and blend the resulting colors to \narrive at the actual sampled texel color. We call this texture ﬁ ltering.\nMost graphics cards support the following kinds of texture ﬁ ltering:\nz Nearest neighbor . In this crude approach, the texel whose center is closest to \nthe pixel center is selected. When mipmapping is enabled, the mip level is \nselected whose resolution is nearest to but greater than the ideal theoreti-\ncal resolution needed to achieve a screen-space texel density of one.\nz Bilinear . In this approach, the four texels surrounding the pixel center \nare sampled, and the resulting color is a weighted average of their col-\nors (where the weights are based on the distances of the texel centers \nfrom the pixel center). When mipmapping is enabled, the nearest mip \nlevel is selected.\nz Trilinear . In this approach, bilinear ﬁ ltering is used on each of the two \nnearest mip levels (one higher-res than the ideal and the other lower-\nres), and these results are then linearly interpolated. This eliminates \nabrupt visual boundaries between mip levels on-screen.\nz Anisotropic . Both bilinear and trilinear ﬁ ltering sample 2 × 2 square \nblocks of texels. This is the right thing to do when the textured sur-\nface is being viewed head-on, but it’s incorrect when the surface is at an \noblique angle relative to the virtual screen plane. Anisotropic ﬁ ltering \nsamples texels within a trapezoidal region corresponding to the view \nangle, thereby increasing the quality of textured surfaces when viewed \nat an angle.\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n",
      "content_length": 2804,
      "extraction_method": "Direct"
    },
    {
      "page_number": 446,
      "chapter": null,
      "content": "424 \n10. The Rendering Engine\n10.1.2.6. Materials\nA material is a complete description of the visual properties of a mesh. This \nincludes a speciﬁ cation of the textures that are mapped to its surface and also \nvarious higher-level properties, such as which shader programs to use when \nrendering the mesh, the input parameters to those shaders, and other parame-\nters that control the functionality of the graphics acceleration hardware itself.\nWhile technically part of the surface properties description, vertex att ri-\nbutes are not considered to be part of the material. However, they come along \nfor the ride with the mesh, so a mesh-material pair contains all the informa-\ntion we need to render the object. Mesh-material pairs are sometimes called \nrender packets, and the term “geometric primitive” is sometimes extended to \nencompass mesh-material pairs as well.\nA 3D model typically uses more than one material. For example, a mod-\nel of a human would have separate materials for the hair, skin, eyes, teeth, \nand various kinds of clothing. For this reason, a mesh is usually divided into \nsubmeshes , each mapped to a single material. The Ogre3D rendering engine \nimplements this design via its Ogre::SubMesh class.\n10.1.3. Lighting Basics\nLighting is at the heart of all CG rendering. Without good lighting, an other-\nwise beautifully modeled scene will look ﬂ at and artiﬁ cial. Likewise, even the \nFigure 10.19.  A variation on the classic “Cornell box” scene illustrating how realistic lighting \ncan make even the simplest scene appear photorealistic.\n",
      "content_length": 1568,
      "extraction_method": "Direct"
    },
    {
      "page_number": 447,
      "chapter": null,
      "content": "425 \nsimplest of scenes can be made to look extremely realistic when it is lit accu-\nrately. The classic “Cornell box” scene, shown in Figure 10.19, is an excellent \nexample of this.\nThe following sequence of screen shots from Naughty Dog’s Uncharted: \nDrake’s Fortune is another good illustration of the importance of lighting. In \nFigure 10.20, the scene is rendered without textures. Figure 10.21 shows the \nsame scene with diﬀ use textures applied. The fully lit scene is shown in Fig-\nure 10.22. Notice the marked jump in realism when lighting is applied to the \nscene.\nFigure 10.20.  A scene from Uncharted: Drake’s Fortune rendered without textures.\nFigure 10.21.  The same UDF scene with only diffuse textures applied.\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n",
      "content_length": 786,
      "extraction_method": "Direct"
    },
    {
      "page_number": 448,
      "chapter": null,
      "content": "426 \n10. The Rendering Engine\nThe term shading is oft en used as a loose generalization of lighting plus \nother visual eﬀ ects. As such, “shading” encompasses procedural deformation \nof vertices to simulate the motion of a water surface, generation of hair curves \nor fur shells, tessellation of high-order surfaces, and prett y much any other \ncalculation that’s required to render a scene.\nIn the following sections, we’ll lay the foundations of lighting that we’ll \nneed in order to understand graphics hardware and the rendering pipeline. \nWe’ll return to the topic of lighting in Section 10.3, where we’ll survey some \nadvanced lighting and shading techniques.\n10.1.3.1. Local and Global Illumination Models\nRendering engines use various mathematical models of light-surface and light-\nvolume interactions called light transport models . The simplest models only ac-\ncount for direct lighting in which light is emitt ed, bounces oﬀ  a single object in \nthe scene, and then proceeds directly to the imaging plane of the virtual cam-\nera. Such simple models are called local illumination models , because only the \nlocal eﬀ ects of light on a single object are considered; objects do not aﬀ ect one \nanother’s appearance in a local lighting model. Not surprisingly, local models \nwere the ﬁ rst to be used in games, and they are still in use today—local light-\ning can produce surprisingly realistic results in some circumstances.\nTrue photorealism can only be achieved by accounting for indirect light-\ning , where light bounces multiple times oﬀ  many surfaces before reaching the \nvirtual camera. Lighting models that account for indirect lighting are called \nglobal illumination models . Some global illumination models are targeted at \nsimulating one speciﬁ c visual phenomenon, such as producing realistic shad-\nFigure 10.22.  The UDF scene with full lighting.\n",
      "content_length": 1870,
      "extraction_method": "Direct"
    },
    {
      "page_number": 449,
      "chapter": null,
      "content": "427 \nows, modeling reﬂ ective surfaces, accounting for interreﬂ ection between ob-\njects (where the color of one object aﬀ ects the colors of surrounding objects), \nand modeling caustic eﬀ ects (the intense reﬂ ections from water or a shiny \nmetal surface). Other global illumination models att empt to provide a holis-\ntic account of a wide range of optical phenomena. Ray tracing and radiosity \nmethods are examples of such technologies.\nGlobal illumination is described completely by a mathematical formula-\ntion known as the rendering equation or shading equation. It was introduced in \n1986 by J. T. Kajiya as part of a seminal SIGGRAPH paper. In a sense, every \nrendering technique can be thought of as a full or partial solution to the ren-\ndering equation, although they diﬀ er in their fundamental approach to solv-\ning it and in the assumptions, simpliﬁ cations, and approximations they make. \nSee htt p://en.wikipedia.org/wiki/Rendering_equation, [8], [1], and virtually \nany other text on advanced rendering and lighting for more details on the \nrendering equation.\n10.1.3.2. The Phong Lighting Model\nThe most common local lighting model employed by game rendering engines \nis the Phong reﬂ ection model . It models the light reﬂ ected from a surface as a \nsum of three distinct terms:\nz The ambient term models the overall lighting level of the scene. It is a \ngross approximation of the amount of indirect bounced light present \nin the scene. Indirect bounces are what cause regions in shadow not to \nappear totally black.\nz The diﬀ use term accounts for light that is reﬂ ected uniformly in all direc-\ntions from each direct light source. This is a good approximation to the \nway in which real light bounces oﬀ  a matt e surface, such as a block of \nwood or a piece of cloth.\nz The specular term models the bright highlights we sometimes see when \nviewing a glossy surface. Specular highlights occur when the view-\ning angle is closely aligned with a path of direct reﬂ ection from a light \nsource.\nFigure 10.23 shows how the ambient, diﬀ use, and specular terms add together \nto produce the ﬁ nal intensity and color of a surface.\nTo calculate Phong reﬂ ection at a speciﬁ c point on a surface, we require \na number of input parameters. The Phong model is normally applied to all \nthree color channels (R, G and B) independently, so all of the color parameters \nin the following discussion are three-element vectors. The inputs to the Phong \nmodel are:\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n",
      "content_length": 2528,
      "extraction_method": "Direct"
    },
    {
      "page_number": 450,
      "chapter": null,
      "content": "428 \n10. The Rendering Engine\nz the viewing direction vector V = [ Vx  Vy  Vz ], which extends from the \nreﬂ ection point to the virtual camera’s focal point (i.e., the negation of \nthe camera’s world-space “front” vector);\nz the \nambient \nlight \nintensity \nfor \nthe \nthree \ncolor \nchannels, \nA = [ AR  AG  AB ];\nz the surface normal N = [ Nx  Ny  Nz ] at the point the light ray impinges \non the surface;\nz the surface reﬂ ectance properties, which are\nthe ambient reﬂ ectivity\n \n□\n kA,\nthe diﬀ use reﬂ ectivity \n \n□\nkD,\nthe specular reﬂ ectivity \n \n□\nkS,\nand a specular “glossiness” exponent \n \n□\nα;\nz and, for each light source i, \nthe light’s color and intensity \n \n□\nCi = [ CRi  CGi  CBi ],\nthe direction vector \n \n□\nLi from the reﬂ ection point to the light source.\nIn the Phong model, the intensity I of light reﬂ ected from a point can be ex-\npressed with the following vector equation:\n \n(\n)\n(\n)\n,\nA\nD\ni\nS\ni\ni\ni\nk\nk\nk\nα\n⎡\n⎤\n=\n+\n⋅\n+\n⋅\n⎣\n⎦\n∑\nI\nA\nN L\nR\nV\nC\n \nwhere the sum is taken over all lights i aﬀ ecting the point in question. This can \nbe broken into three scalar equations, one for each color channel:\n \n(\n)\n(\n)\n,\n(\n)\n(\n)\n,\n(\n)\n(\n)\n.\nR\nA\nR\nD\ni\nS\ni\nRi\ni\nG\nA\nG\nD\ni\nS\ni\nGi\nB\nA\nB\nD\ni\nS\ni\nBi\nI\nk A\nk\nk\nC\nI\nk A\nk\nk\nC\nI\nk A\nk\nk\nC\nα\nα\nα\n⎡\n⎤\n=\n+\n⋅\n+\n⋅\n⎣\n⎦\n⎡\n⎤\n=\n+\n⋅\n+\n⋅\n⎣\n⎦\n⎡\n⎤\n=\n+\n⋅\n+\n⋅\n⎣\n⎦\n∑\n∑\n∑\nN L\nR\nV\nN L\nR\nV\nN L\nR\nV\ni\ni\n \nFigure 10.23. Ambient, diffuse and specular terms are summed to calculate Phong \nreﬂ ection.\n",
      "content_length": 1426,
      "extraction_method": "Direct"
    },
    {
      "page_number": 451,
      "chapter": null,
      "content": "429 \nIn these equations, the vector Ri = [ Rxi  Ryi  Rzi ] is the reﬂ ection of the light ray’s \ndirection vector Li about the surface normal N.\nThe vector Ri can be easily calculated via a bit of vector math. Any vec-\ntor can be expressed as a sum of its tangential and normal components. For \nexample, we can break up the light direction vector L as follows:\n \n.\nT\nN\n=\n+\nL\nL\nL\n \nWe know that the dot product (N · L) represents the projection of L normal \nto the surface (a scalar quantity). So the normal component LN is just the unit \nnormal vector N scaled by this dot product:\n \n(\n) .\nN =\n⋅\nL\nN L N\nThe reﬂ ected vector R has the same normal component as L but the opposite \ntangential component (–LT). So we can ﬁ nd R as follows:\n \n \nThis equation can be used to ﬁ nd all of the Ri values corresponding to the light \ndirections Li.\nBlinn-Phong\nThe Blinn-Phong lighting model is a variation on Phong shading that calcu-\nlates specular reﬂ ection in a slightly diﬀ erent way. We deﬁ ne the vector H to \nbe the vector that lies halfway between the view vector V and the light direc-\ntion vector L. The Blinn-Phong specular component is then (N · H)a, as op-\nposed to Phong’s (R · V)α. The exponent a is slightly diﬀ erent than the Phong \nexponent α, but its value is chosen in order to closely match the equivalent \nPhong specular term.\nThe Blinn-Phong model oﬀ ers increased runtime eﬃ  ciency at the cost of \nsome accuracy, although it actually matches empirical results more closely \nthan Phong for some kinds of surfaces. The Blinn-Phong model was used \nalmost exclusively in early computer games and was hard-wired into the \nﬁ xed-function pipelines of early GPUs. See htt p://en.wikipedia.org/wiki/\nBlinn%E2%80%93Phong_shading_model for more details.\nBRDF Plots\nThe three terms in the Phong lighting model are special cases of a general local \nreﬂ ection model known as a bidirectional reﬂ ection distribution function (BRDF ). \n10.1. Foundations of Depth-Buffered Triangle Rasterization\n(\n)\n2\n;\n2(\n)\n.\nN\nT\nN\nN\nN\n=\n−\n=\n−\n−\n=\n−\n=\n⋅\n−\nR\nL\nL\nL\nL\nL\nL\nL\nR\nN L N\nL\n",
      "content_length": 2069,
      "extraction_method": "Direct"
    },
    {
      "page_number": 452,
      "chapter": null,
      "content": "430 \n10. The Rendering Engine\nA BRDF calculates the ratio of the outgoing (reﬂ ected) radiance along a given \nviewing direction V to the incoming irradiance along the incident ray L.\nA BRDF can be visualized as a hemispherical plot, where the radial dis-\ntance from the origin represents the intensity of the light that would be seen if \nthe reﬂ ection point were viewed from that direction. The diﬀ use Phong reﬂ ec-\ntion term is kD(N · L). This term only accounts for the incoming illumination \nray L, not the viewing angle V. Hence the value of this term is the same for all \nviewing angles. If we were to plot this term as a function of the viewing angle \nin three dimensions, it would look like a hemisphere centered on the point at \nwhich we are calculating the Phong reﬂ ection. This is shown in two dimen-\nsions in Figure 10.24.\nThe specular term of the Phong model is kS(R · V)α. This term is dependent \non both the illumination direction L and the viewing direction V. It produces \na specular “hot spot” when the viewing angle aligns closely with the reﬂ ection \nR of the illumination direction L about the surface normal. However, its con-\ntribution falls oﬀ  very quickly as the viewing angle diverges from the reﬂ ected \nillumination direction. This is shown in two dimensions in Figure 10.25.\n10.1.3.3. Modeling Light Sources\n In addition to modeling the light’s interactions with surfaces, we need to de-\nscribe the sources of light in the scene. As with all things in real-time rendering, \nwe approximate real-world light sources using various simpliﬁ ed models.\nFigure 10.24.  The diffuse term of the Phong reﬂ ection model is dependent upon N • L, but is \nindependent of the viewing angle V.\nFigure 10.25.  The specular term of the Phong reﬂ ection model is at its maximum when the \nviewing angle V coincides with the reﬂ ected light direction R and drops off quickly as V di-\nverges from R.\n",
      "content_length": 1910,
      "extraction_method": "Direct"
    },
    {
      "page_number": 453,
      "chapter": null,
      "content": "431 \nStatic Lighting\n The fastest lighting calculation is the one you don’t do at all. Lighting is there-\nfore performed oﬀ -line whenever possible. We can precalculate Phong reﬂ ec-\ntion at the vertices of a mesh and store the results as diﬀ use vertex color at-\ntributes. We can also precalculate lighting on a per pixel basis and store the \nresults in a kind of texture map known as a light map . At runtime, the light \nmap texture is projected onto the objects in the scene in order to determine the \nlight’s eﬀ ects on them.\nYou might wonder why we don’t just bake lighting information directly \ninto the diﬀ use textures in the scene. There are a few reasons for this. For one \nthing, diﬀ use texture maps are oft en tiled and/or repeated throughout a scene, \nso baking lighting into them wouldn’t be practical. Instead, a single light map \nis usually generated per light source and applied to any objects that fall within \nthat light’s area of inﬂ uence. This approach permits dynamic objects to move \npast a light source and be properly illuminated by it. It also means that our \nlight maps can be of a diﬀ erent (oft en lower) resolution than our diﬀ use tex-\nture maps. Finally, a “pure” light map usually compresses bett er than one that \nincludes diﬀ use color information.\nAmbient Lights\nAn ambient light corresponds to the ambient term in the Phong lighting model. \nThis term is independent of the viewing angle and has no speciﬁ c direction. \nAn ambient light is therefore represented by a single color, corresponding to \nthe A color term in the Phong equation (which is scaled by the surface’s ambi-\nent reﬂ ectivity kA at runtime). The intensity and color of ambient light may \nvary from region to region within the game world.\nDirectional Lights\nA directional light models a light source that is eﬀ ectively an inﬁ nite distance \naway from the surface being illuminated—like the sun. The rays emanating \nfrom a directional light are parallel, and the light itself does not have any \nparticular location in the game world. A directional light is therefore modeled \nas a light color C and a direction vector L. A directional light is depicted in \nFigure 10.26.\nPoint (Omni-Directional) Lights\nA point light (omni-directional light) has a distinct position in the game world \nand radiates uniformly in all directions. The intensity of the light is usually \nconsidered to fall oﬀ  with the square of the distance from the light source, \nand beyond a predeﬁ ned maximum radius its eﬀ ects are simply clamped to \nzero. A point light is modeled as a light position P, a source color/intensity C, \nFigure 10.26.  Model \nof a directional light \nsource.\nFigure 10.27.  Mod-\nel of a point light \nsource.\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n",
      "content_length": 2770,
      "extraction_method": "Direct"
    },
    {
      "page_number": 454,
      "chapter": null,
      "content": "432 \n10. The Rendering Engine\nand a maximum radius rmax. The rendering engine only applies the eﬀ ects of a \npoint light to those surfaces that fall within is sphere of inﬂ uence (a signiﬁ cant \noptimization). Figure 10.27 illustrates a point light.\nSpot Lights\nA spot light acts like a point light whose rays are restricted to a cone-shaped \nregion, like a ﬂ ashlight. Usually two cones are speciﬁ ed with an inner and an \nouter angle. Within the inner cone, the light is considered to be at full inten-\nsity. The light intensity falls oﬀ  as the angle increases from the inner to the \nouter angle, and beyond the outer cone it is considered to be zero. Within \nboth cones, the light intensity also falls oﬀ  with radial distance. A spot light is \nmodeled as a position P, a source color C, a central direction vector L, a maxi-\nmum radius rmax , and inner and outer cone angles θmin and θmax. Figure 10.28 \nillustrates a spot light source.\nArea Lights\n All of the light sources we’ve discussed thus far radiate from an idealized \npoint, either at inﬁ nity or locally. A real light source almost always has a non-\nzero area—this is what gives rise to the umbra and penumbra in the shadows \nit casts.\nRather than trying to model area lights explicitly, CG engineers oft en use \nvarious “tricks” to account for their behavior. For example to simulate a pen-\numbra, we might cast multiple shadows and blend the results, or we might \nblur the edges of a sharp shadow in some manner.\nEmissive Objects\n Some surfaces in a scene are themselves light sources. Examples include ﬂ ash-\nlights, glowing crystal balls, ﬂ ames from a rocket engine, and so on. Glowing \nsurfaces can be modeled using an emissive texture map —a texture whose colors \nare always at full intensity, independent of the surrounding lighting environ-\nment. Such a texture could be used to deﬁ ne a neon sign, a car’s headlights, \nand so on.\nSome kinds of emissive objects are rendered by combining multiple tech-\nniques. For example, a ﬂ ashlight might be rendered using an emissive texture \nfor when you’re looking head-on into the beam, a colocated spot light that \ncasts light into the scene, a yellow translucent mesh to simulate the light cone, \nsome camera-facing transparent cards to simulate lens ﬂ are (or a bloom eﬀ ect \nif high dynamic range lighting is supported by the engine), and a projected \ntexture to produce the caustic eﬀ ect that a ﬂ ashlight has on the surfaces it il-\nluminates. The ﬂ ashlight in Luigi’s Mansion is a great example of this kind of \neﬀ ect combination, as shown in Figure 10.29.\nFigure 10.28.  Model \nof a spot light source.\n",
      "content_length": 2628,
      "extraction_method": "Direct"
    },
    {
      "page_number": 455,
      "chapter": null,
      "content": "433 \n10.1.4. The Virtual Camera\nIn computer graphics, the virtual camera is much simpler than a real camera \nor the human eye. We treat the camera as an ideal focal point with a rectangu-\nlar virtual sensing surface called the imaging rectangle ﬂ oating some small dis-\ntance in front of it. The imaging rectangle consists of a grid of square or rect-\nangular virtual light sensors, each corresponding to a single pixel on-screen. \nRendering can be thought of as the process of determining what color and \nintensity of light would be recorded by each of these virtual sensors.\n10.1.4.1. View Space\nThe focal point of the virtual camera is the origin of a 3D coordinate system \nknown as view space or camera space. The camera usually “looks” down the \npositive or negative z-axis in view space, with y up and x to the left  or right. \nTypical left - and right-handed view space axes are illustrated in Figure 10.30.\nFigure 10.29.  The ﬂ ashlight in Luigi’s Mansion is composed of numerous visual effects, in-\ncluding a cone of translucent geometry for the beam, a dynamic spot light to cast light into \nthe scene, an emissive texture on the lens, and camera-facing cards for the lens ﬂ are.\nLeft-Handed\nRight-Handed\nVirtual \nScreen\nVirtual \nScreen\nFrustum\nFrustum\nxC\nzC\nyC\nxC\nzC\nyC\nFigure 10.30.  Left- and right-handed camera space axes.\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n",
      "content_length": 1397,
      "extraction_method": "Direct"
    },
    {
      "page_number": 456,
      "chapter": null,
      "content": "434 \n10. The Rendering Engine\nThe camera’s position and orientation can be speciﬁ ed using a view-to-\nworld matrix , just as a mesh instance is located in the scene with its model-to-\nworld matrix. If we know the position vector and three unit basis vectors of \ncamera space, expressed in world-space coordinates, the view-to-world ma-\ntrix can be writt en as follows, in a manner analogous to that used to construct \na model-to-view matrix:\n \n \n0\n0\n.\n0\n1\nV\nV\nV\nW\nV\nV\n→\n⎡\n⎤\n⎢\n⎥\n⎢\n⎥\n=⎢\n⎥\n⎢\n⎥\n⎢\n⎥\n⎣\n⎦\ni\nj\nM\nk\nt\n \nWhen rendering a triangle mesh, its vertices are transformed ﬁ rst from \nmodel space to world space, and then from world space to view space. To \nperform this latt er transformation, we need the world-to-view matrix , which \nis the inverse of the view-to-world matrix. This matrix is sometimes called the \nview matrix:\n \n1\nview\n(\n)\n.\nW\nV\nV\nW\n−\n→\n→\n=\n=\nM\nM\nM\nBe careful here. The fact that the camera’s matrix is inverted relative to the \nmatrices of the objects in the scene is a common point of confusion and bugs \namong new game developers.\nThe world-to-view matrix is oft en concatenated to the model-to-world \nmatrix prior to rendering a particular mesh instance. This combined matrix is \ncalled the model-view matrix in OpenGL. We precalculate this matrix so that the \nrendering engine only needs to do a single matrix multiply when transform-\ning vertices from model space into view space:\n \n \nmodel view.\nM\nV\nM\nW\nW\nV\n→\n→\n→\n-\n=\n=\nM\nM\nM\nM\n \n10.1.4.2. Projections\nIn order to render a 3D scene onto a 2D image plane, we use a special kind \nof transformation known as a projection . The perspective projection is the most \ncommon projection in computer graphics, because it mimics the kinds of im-\nages produced by a typical camera. With this projection, objects appear small-\ner the farther away they are from the camera—an eﬀ ect known as perspective \nforeshortening .\nThe length-preserving orthographic projection is also used by some games, \nprimarily for rendering plan views (e.g., front, side, and top) of 3D models or \ngame levels for editing purposes, and for overlaying 2D graphics onto the \nscreen for heads-up displays (HUDs) and the like. Figure 10.31 illustrates how \na cube would look when rendered with these two types of projections.\n",
      "content_length": 2265,
      "extraction_method": "Direct"
    },
    {
      "page_number": 457,
      "chapter": null,
      "content": "435 \n10.1.4.3. The View Volume and the Frustum\nThe region of space that the camera can “see” is known as the view volume . A \nview volume is deﬁ ned by six planes. The near plane corresponds to the virtual \nimage-sensing surface. The four side planes correspond to the edges of the \nvirtual screen. The far plane is used as a rendering optimization to ensure that \nextremely distant objects are not drawn. It also provides an upper limit for the \ndepths that will be stored in the depth buﬀ er (see Section 10.1.4.8).\nWhen rendering the scene with a perspective projection, the shape of the \nview volume is a truncated pyramid known as a frustum . When using an or-\nthographic projection, the view volume is a rectangular prism. Perspective \nand orthographic view volumes are illustrated in Figure 10.32 and Figure \n10.33, respectively.\nThe six planes of the view volume can be represented compactly using six \nfour-element vectors (nxi , nyi , nzi , di), where n = (nx , ny , nz) is the plane normal \nand d is its perpendicular distance from the origin. If we prefer the point-\nnormal plane representation, we can also describe the planes with six pairs of \nvectors (Qi, ni), where Q is the arbitrary point on the plane and n is the plane \nnormal. (In both cases, i is the index of the plane.)\nFigure 10.31.  A cube rendered using a perspective projection (on the left) and an ortho-\ngraphic projection (on the right).\nFar \nPlane\nyV\nNear \nPlane\nxV\nzV\n(r, b, n)\n(r, b, f)\n(r, t, f)\n(l, t, f)\n(l, b, n)\n(l, t, n)\n(l, b, f)\n(r, t, n)\nFigure 10.32.  A perspective view volume (frustum).\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n",
      "content_length": 1643,
      "extraction_method": "Direct"
    },
    {
      "page_number": 458,
      "chapter": null,
      "content": "436 \n10. The Rendering Engine\n10.1.4.4. Projection and Homogeneous Clip Space\nBoth perspective and orthographic projections transform points in view space \ninto a coordinate space called homogeneous clip space . This three-dimensional \nspace is really just a warped version of view space. The purpose of clip space \nis to convert the camera-space view volume into a canonical view volume that \nis independent both of the kind of projection used to convert the 3D scene into \n2D screen space, and of the resolution and aspect ratio of the screen onto which \nthe scene is going to be rendered.\nIn clip space, the canonical view volume is a rectangular prism extending \nfrom –1 to +1 along the x- and y-axes. Along the z-axis, the view volume ex-\ntends either from –1 to +1 (OpenGL) or from 0 to 1 (DirectX). We call this coor-\nFigure 10.33.  An orthographic view volume.\nFar \nPlane\nyV\nNear \nPlane\nxV\nzV\n(r, b, n)\n(r, b, f)\n(r, t, f)\n(l, t, f)\n(l, b, n)\n(l, t, n)\n(l, b, f)\n(r, t, n)\nFar \nPlane\nyH\nNear \nPlane\nxH\nzH\n(1, –1, –1)\n(1, –1, 1)\n(1, 1, 1)\n(–1, 1, 1)\n(–1, –1, –1)\n(–1, 1, –1)\nFigure 10.34.  The canonical view volume in homogeneous clip space.\n",
      "content_length": 1150,
      "extraction_method": "Direct"
    },
    {
      "page_number": 459,
      "chapter": null,
      "content": "437 \ndinate system “clip space” because the view volume planes are axis-aligned, \nmaking it convenient to clip triangles to the view volume in this space (even \nwhen a perspective projection is being used). The canonical clip-space view \nvolume for OpenGL is depicted in Figure 10.34. Notice that the z-axis of clip \nspace goes into the screen, with y up and x to the right. In other words, homo-\ngeneous clip space is usually left -handed.\nPerspective Projection\nAn excellent explanation of perspective projection is given in Section 4.5.1 of \n[28], so we won’t repeat it here. Instead, we’ll simply present the perspective \nprojection matrix \nV\nH\n→\nM\n below. (The subscript V→H indicates that this ma-\ntrix transforms vertices from view space into homogeneous clip space.) If we \ntake view space to be right-handed, then the near plane intersects the z-axis \nat z = –n, and the far plane intersects it at z = –f. The virtual screen’s left , right, \nbott om, and top edges lie at x = l, x = r, y = b, and y = t on the near plane, respec-\ntively. (Typically the virtual screen is centered on the camera-space z-axis, in \nwhich case l = –r and b = –t, but this isn’t always the case.) Using these deﬁ ni-\ntions, the perspective projection matrix for OpenGL is as follows:\n \n \n2\n0\n0\n  0\n2\n0\n0\n  0\n.\n1\n2\n0\n0\n  0\nV\nH\nn\nr\nl\nn\nt\nb\nf\nn\nr\nl\nt\nb\nr\nl\nt\nb\nf\nn\nnf\nf\nn\n→\n⎡\n⎤\n⎛\n⎞\n⎜\n⎟\n⎢\n⎥\n⎝\n⎠\n−\n⎢\n⎥\n⎢\n⎥\n⎛\n⎞\n⎜\n⎟\n⎢\n⎥\n⎝\n⎠\n−\n⎢\n⎥\n=⎢\n⎥\n⎛\n⎞\n⎛\n⎞⎛\n⎞\n+\n+\n+\n⎜\n⎟\n⎢\n⎥\n−\n−\n⎜\n⎟⎜\n⎟\n⎝\n⎠⎝\n⎠\n−\n−\n−\n⎝\n⎠\n⎢\n⎥\n⎢\n⎥\n⎛\n⎞\n⎢\n⎥\n⎜\n⎟\n−\n⎢\n⎥\n−\n⎝\n⎠\n⎣\n⎦\nM\n \nDirectX deﬁ nes the z-axis extents of the clip-space view volume to lie in \nthe range [0, 1] rather thanin the range  [–1, 1] as OpenGL does. We can easily \nadjust the perspective projection matrix to account for DirectX’s conventions \nas follows:\n \n(\n)\n \nDirectX\n2\n0\n0\n  0\n2\n0\n0\n  0\n.\n1\n0\n0\n  0\nV\nH\nn\nr\nl\nn\nt\nb\nf\nr\nl\nt\nb\nr\nl\nt\nb\nf\nn\nnf\nf\nn\n→\n⎡\n⎤\n⎛\n⎞\n⎜\n⎟\n⎢\n⎥\n⎝\n⎠\n−\n⎢\n⎥\n⎢\n⎥\n⎛\n⎞\n⎜\n⎟\n⎢\n⎥\n⎝\n⎠\n−\n⎢\n⎥\n=⎢\n⎥\n⎛\n⎞\n⎛\n⎞⎛\n⎞\n+\n+\n⎜\n⎟\n⎢\n⎥\n−\n−\n⎜\n⎟⎜\n⎟\n⎝\n⎠⎝\n⎠\n−\n−\n−\n⎝\n⎠\n⎢\n⎥\n⎢\n⎥\n⎛\n⎞\n⎢\n⎥\n⎜\n⎟\n−\n⎢\n⎥\n−\n⎝\n⎠\n⎣\n⎦\nM\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n",
      "content_length": 2056,
      "extraction_method": "Direct"
    },
    {
      "page_number": 460,
      "chapter": null,
      "content": "438 \n10. The Rendering Engine\nDivision by Z\n Perspective projection results in each vertex’s x- and y-coordinates being di-\nvided by its z-coordinate. This is what produces perspective foreshortening . \nTo understand why this happens, consider multiplying a view-space point \nV\np\n expressed in four-element homogeneous coordinates by the OpenGL per-\nspective projection matrix:\n \n \n \n2\n0\n0\n  0\n2\n0\n0\n  0\n[\n1] \n.\n1\n2\n0\n0\n  0\nH\nV\nV\nH\nVx\nVy\nVz\nn\nr\nl\nn\nt\nb\np\np\np\nf\nn\nr\nl\nt\nb\nr\nl\nt\nb\nf\nn\nnf\nf\nn\n→\n=\n⎡\n⎤\n⎛\n⎞\n⎜\n⎟\n⎢\n⎥\n⎝\n⎠\n−\n⎢\n⎥\n⎢\n⎥\n⎛\n⎞\n⎜\n⎟\n⎢\n⎥\n⎝\n⎠\n−\n⎢\n⎥\n=\n⎢\n⎥\n⎛\n⎞\n⎛\n⎞⎛\n⎞\n+\n+\n+\n⎜\n⎟\n⎢\n⎥\n−\n−\n⎜\n⎟⎜\n⎟\n⎝\n⎠⎝\n⎠\n−\n−\n−\n⎝\n⎠\n⎢\n⎥\n⎢\n⎥\n⎛\n⎞\n⎢\n⎥\n⎜\n⎟\n−\n⎢\n⎥\n−\n⎝\n⎠\n⎣\n⎦\np\np M\nThe result of this multiplication takes the form\n \n .\nH\nVz\na\nb\nc\np\n⎡\n⎤\n=\n−\n⎣\n⎦\np\n \n(10.1)\nWhen we convert any homogeneous vector into three dimensional coor-\ndinates, the x-, y-, and z-components are divided by the w-component:\n \n .\ny\nx\nz\nx\ny\nz\nw\nw\nw\nw\n⎡\n⎤\n⎡\n⎤≡\n⎣\n⎦⎢\n⎥\n⎣\n⎦ \nSo, aft er dividing Equation (10.1) by the homogeneous w-component, which is \nreally just the negative view-space z-coordinate \nVz\np\n−\n, we have:\n \n \n[\n].\nH\nVz\nVz\nVz\nHx\nHy\nHz\na\nb\nc\np\np\np\np\np\np\n⎡\n⎤\n=⎢\n⎥\n−\n−\n−\n⎣\n⎦\n=\np\nThus the homogeneous clip space coordinates have been divided by the view-\nspace z-coordinate, which is what causes perspective foreshortening.\nPerspective-Correct Vertex Attribute Interpolation\nIn Section 10.1.2.4, we learned that vertex att ributes are interpolated in order to \ndetermine appropriate values for them within the interior of a triangle. Att ri-\nbute interpolation is performed in screen space. We iterate over each pixel of the \nscreen and att empt to determine the value of each att ribute at the correspond-\ning location on the surface of the triangle. When rendering a scene with a perspec-\n",
      "content_length": 1748,
      "extraction_method": "Direct"
    },
    {
      "page_number": 461,
      "chapter": null,
      "content": "439 \ntive projection, we must do this very carefully so as to account for perspective \nforeshortening. This is known as perspective-correct att ribute interpolation .\nA derivation of perspective-correct interpolation is beyond our scope, but \nsuﬃ  ce it to say that we must divide our interpolated att ribute values by the \ncorresponding z-coordinates (depths) at each vertex. For any pair of vertex at-\ntributes A1 and A2, we can write the interpolated att ribute at a percentage t of \nthe distance between them as follows:\n \n1\n2\n1\n2\n1\n2\n1\n2\n(1\n)\nLERP\n, \n, \n.\nz\nz\nz\nz\nz\nA\nA\nA\nA\nA\nt\np\np\np\np\np\n⎛\n⎞\n⎜\n⎟\n=\n−\n+\n=\n⎝\n⎠\nt\nt\n \nRefer to [28] for an excellent derivation of the math behind perspective-correct \natt ribute interpolation.\nOrthographic Projection\nAn orthographic projection is performed by the following matrix :\n(\n)\n \northo\n2\n0\n0\n0\n2\n0\n0\n0\n.\n2\n0\n0\n0\n1\nV\nH\nr\nl\nt\nb\nf\nn\nf\nn\nr\nl\nt\nb\nr\nl\nt\nb\nf\nn\n→\n⎡\n⎤\n⎛\n⎞\n⎜\n⎟\n⎢\n⎥\n⎝\n⎠\n−\n⎢\n⎥\n⎢\n⎥\n⎛\n⎞\n⎜\n⎟\n⎢\n⎥\n⎝\n⎠\n−\n⎢\n⎥\n=⎢\n⎥\n⎛\n⎞\n⎜\n⎟\n⎢\n⎥\n−−\n⎝\n⎠\n⎢\n⎥\n⎢\n⎥\n⎛\n⎞\n⎛\n⎞⎛\n⎞\n+\n+\n+\n⎢\n⎥\n⎜\n⎟\n−\n−\n−\n⎜\n⎟⎜\n⎟\n⎢\n⎥\n⎝\n⎠⎝\n⎠\n−\n−\n−\n⎝\n⎠\n⎣\n⎦\nM\n \nThis is just an everyday scale-and-translate matrix. (The upper-left  3 × 3 \ncontains a diagonal nonuniform scaling matrix, and the lower row contains \nthe translation.) Since the view volume is a rectangular prism in both view \nspace and clip space, we need only scale and translate our vertices to convert \nfrom one space to the other.\n10.1.4.5. Screen Space and Aspect Ratios\nScreen space is a two-dimensional coordinate system whose axes are mea-\nsured in terms of screen pixels. The x-axis typically points to the right, with \nthe origin at the top-left  corner of the screen and y pointing down. (The reason \nfor the inverted y-axis is that CRT monitors scan the screen from top to bot-\ntom.) The ratio of screen width to screen height is known as the aspect ratio. \nThe most common aspect ratios are 4:3 (the aspect ratio of a traditional tele-\nvision screen) and 16:9 (the aspect ratio of a movie screen or HDTV). These \naspect ratios are illustrated in Figure 10.35.\n10.1. Foundations of Depth-Buffered Triangle Rasterization\n",
      "content_length": 2085,
      "extraction_method": "Direct"
    },
    {
      "page_number": 462,
      "chapter": null,
      "content": "440 \n10. The Rendering Engine\nWe can render triangles expressed in homogeneous clip space by simply \ndrawing their (x, y) coordinates and ignoring z. But before we do, we scale \nand shift  the clip-space coordinates so that they lie in screen space rather than \nwithin the normalized unit square. This scale-and-shift  operation is known as \nscreen mapping .\n10.1.4.6. The Frame Buffer\nThe ﬁ nal rendered image is stored in a bitmapped color buﬀ er known as the \nframe buﬀ er . Pixel colors are usually stored in RGBA8888 format, although other \nframe buﬀ er formats are supported by most graphics cards as well. Some com-\nmon formats include RGB565, RGB5551, and one or more palett ed modes.\nThe display hardware (CRT, ﬂ at-screen monitor, HDTV, etc.) reads the \ncontents of the frame buﬀ er at a periodic rate of 60 Hz for NTSC televisions \nused in North America and Japan, or 50 Hz for PAL /SECAM televisions used \nin Europe and many other places in the world. Rendering engines typically \nmaintain at least two frame buﬀ ers. While one is being scanned by the dis-\nplay hardware, the other one can be updated by the rendering engine. This is \nknown as double buﬀ ering . By swapping or “ﬂ ipping” the two buﬀ ers during \nthe vertical blanking interval (the period during which the CRT’s electron gun is \nbeing reset to the top-left  corner of the screen), double buﬀ ering ensures that \nthe display hardware always scans the complete frame buﬀ er. This avoids a \njarring eﬀ ect known as tearing , in which the upper portion of the screen dis-\nplays the newly rendered image while the bott om shows the remnants of the \nprevious frame’s image.\nSome engines make use of three frame buﬀ ers—a technique aptly known \nas triple buﬀ ering . This is done so that the rendering engine can start work on \nthe next frame, even when the previous frame is still being scanned by the \ndisplay hardware. For example, the hardware might still be scanning buﬀ er A \nwhen the engine ﬁ nishes drawing buﬀ er B. With triple buﬀ ering, it can pro-\nxS\n4:3\nyS\nxS\n16:9\nyS\nFigure 10.35.  The two most prevalent screen space aspect ratios are 4:3 and 16:9.\n",
      "content_length": 2136,
      "extraction_method": "Direct"
    },
    {
      "page_number": 463,
      "chapter": null,
      "content": "441 \nceed to render a new frame into buﬀ er C, rather than idling while it waits for \nthe display hardware to ﬁ nish scanning buﬀ er A.\nRender Targets\nAny buﬀ er into which the rendering engine draws graphics is known as a ren-\nder target . As we’ll see later in this chapter, rendering engines make use of all \nsorts of other oﬀ -screen render targets, in addition to the frame buﬀ ers. These \ninclude the depth buﬀ er , the stencil buﬀ er , and various other buﬀ ers used for \nstoring intermediate rendering results.\n10.1.4.7. Triangle Rasterization and Fragments\nTo produce an image of a triangle on-screen, we need to ﬁ ll in the pixels it \noverlaps. This process is known as rasterization . During rasterization, the tri-\nangle’s surface is broken into pieces called fragments , each one representing a \nsmall region of the triangle’s surface that corresponds to a single pixel on the \nscreen. (In the case of multisample antialiasing, a fragment corresponds to a \nportion of a pixel—see below.)\nA fragment is like a pixel in training. Before it is writt en into the frame \nbuﬀ er, it must pass a number of tests (described in more depth below). If it \nfails any of these tests, it will be discarded. Fragments that pass the tests are \nshaded (i.e., their colors are determined), and the fragment color is either writ-\nten into the frame buﬀ er or blended with the pixel color that’s already there. \nFigure 10.36 illustrates how a fragment becomes a pixel.\nFragment\nPixel\nFigure 10.36.  A fragment is a small region of a triangle corresponding to a pixel on the \nscreen. It passes through the rendering pipeline and is either discarded or its color is written \ninto the frame buffer.\n10.1. Foundations of Depth-Buffered Triangle Rasterization\nAntialiasing\n When a triangle is rasterized, its edges can look jagged—the familiar “stair \nstep” eﬀ ect we have all come to know and love (or hate). Technically speak-\n",
      "content_length": 1917,
      "extraction_method": "Direct"
    },
    {
      "page_number": 464,
      "chapter": null,
      "content": "442 \n10. The Rendering Engine\ning, aliasing arises because we are using a discrete set of pixels to sample an \nimage that is really a smooth, continuous two-dimensional signal. (In the fre-\nquency domain, sampling causes a signal to be shift ed and copied multiple \ntimes along the frequency axis. Aliasing literally means that these copies of \nthe signal overlap and get confused with one another.)\nAntialiasing is a technique that reduces the visual artifacts caused by alias-\ning. In eﬀ ect, antialiasing causes the edges of the triangle to be blended with \nthe surrounding colors in the frame buﬀ er.\nThere are a number of ways to antialias a 3D rendered image. In full-screen \nantialiasing (FSAA), the image is rendered into a frame buﬀ er that is twice \nas wide and twice as tall as the actual screen. The resulting image is down-\nsampled to the desired resolution aft erwards. FSAA can be expensive because \nrendering a double-sized frame means ﬁ lling four times the number of pixels. \nFSAA frame buﬀ ers also consume four times the memory of a regular frame \nbuﬀ er.\nModern graphics hardware can antialias a rendered image without ac-\ntually rendering a double-size image, via a technique called multisample an-\ntialiasing (MSAA). The basic idea is to break a triangle down into more than \none fragment per pixel. These supersampled fragments are combined into a \nsingle pixel at the end of the pipeline. MSAA does not require a double-width \nframe buﬀ er, and it can handle higher levels of supersampling as well. (4× and \n8× supersampling are commonly supported by modern GPUs.)\n10.1.4.8. Occlusion and the Depth Buffer\nWhen rendering two triangles that overlap each other in screen space, we \nneed some way of ensuring that the triangle that is closer to the camera will \nappear on top. We could accomplish this by always rendering our triangles in \nFigure 10.37.  The painter’s algorithm renders triangles in a back-to-front order to produce \nproper triangle occlusion. However, the algorithm breaks down when triangles intersect one \nanother.\n",
      "content_length": 2057,
      "extraction_method": "Direct"
    },
    {
      "page_number": 465,
      "chapter": null,
      "content": "443 \nback-to-front order (the so-called painter’s algorithm ). However, as shown in \nFigure 10.37, this doesn’t work if the triangles are intersecting one another.\nTo implement triangle occlusion properly, independent of the order in \nwhich the triangles are rendered, rendering engines use a technique known \nas depth buﬀ ering or z-buﬀ ering. The depth buﬀ er is a full-screen buﬀ er that \ntypically contains 16- or 24-bit ﬂ oating-point depth information for each pix-\nel in the frame buﬀ er. Every fragment has a z-coordinate that measures its \ndepth “into” the screen. (The depth of a fragment is found by interpolating \nthe depths of the triangle’s vertices.) When a fragment’s color is writt en into \nthe frame buﬀ er, it depth is stored into the corresponding pixel of the depth \nbuﬀ er. When another fragment (from another triangle) is drawn into the same \npixel, the engine compares the new fragment’s depth to the depth already \npresent in the depth buﬀ er. If the fragment is closer to the camera (i.e., if it \nhas a smaller depth), it overwrites the pixel in the frame buﬀ er. Otherwise the \nfragment is discarded.\nZ-Fighting and the W-Buffer\nWhen rendering parallel surfaces that are very close to one another, it’s im-\nportant that the rendering engine can distinguish between the depths of the \ntwo planes. If our depth buﬀ er had inﬁ nite precision, this would never be \na problem. Unfortunately, a real depth buﬀ er only has limited precision, so \nthe depth values of two planes can collapse into a single discrete value when \nthe planes are close enough together. When this happens, the more-distant \nplane’s pixels start to “poke through” the nearer plane, resulting in a noisy \neﬀ ect known as z-ﬁ ghting .\nTo reduce z-ﬁ ghting to a minimum across the entire scene, we would like \nto have equal precision whether we’re rendering surfaces that are close to the \ncamera or far away. However, with z-buﬀ ering this is not the case. The preci-\nsion of clip-space z-depths (\nHz\np\n) are not evenly distributed across the entire \nrange from the near plane to the far plane, because of the division by the view-\nspace z-coordinate. Because of the shape of the 1/z curve, most of the depth \nbuﬀ er’s precision is concentrated near the camera.\nThe plot of the function \n1/\nHz\nVz\np\np\n=\n shown in Figure 10.38 demonstrates \nthis eﬀ ect. Near the camera, the distance between two planes in view space \nVz\np\nΔ\n gets transformed into a reasonably large delta in clip space, \n.\nHz\np\nΔ\n But \nfar from the camera, this same separation gets transformed into a tiny delta in \nclip space. The result is z ﬁ ghting, and it becomes rapidly more prevalent as \nobjects get farther away from the camera.\nTo circumvent this problem, we would like to store view-space z-coor-\ndinates (\nVz\np\n) in the depth buﬀ er instead of clip-space z-coordinates (\nHz\np\n). \nView-space z-coordinates vary linearly with the distance from the camera, so \n10.1. Foundations of Depth-Buffered Triangle Rasterization\n",
      "content_length": 2990,
      "extraction_method": "Direct"
    },
    {
      "page_number": 466,
      "chapter": null,
      "content": "444 \n10. The Rendering Engine\nusing them as our depth measure achieves uniform precision across the en-\ntire depth range. This technique is called w-buﬀ ering , because the view-space \nz-coordinate conveniently appears in the w-component of our homogeneous \nclip-space coordinates. (Recall from Equation (10.1) that \nHw\nVz\np\np\n=−\n.)\nThe terminology can be a very confusing here. The z- and w-buﬀ ers store \ncoordinates that are expressed in clip space. But in terms of view-space coordi-\nnates, the z-buﬀ er stores 1/z (i.e., 1/\nVz\np\n) while the w-buﬀ er stores z (i.e., \nVz\np\n)!\nWe should note here that the w-buﬀ ering approach is a bit more expen-\nsive than its z-based counterpart. This is because with w-buﬀ ering, we cannot \nlinearly interpolate depths directly. Depths must be inverted prior to interpo-\nlation and then re-inverted prior to being stored in the w-buﬀ er.\n10.2. The Rendering Pipeline\nNow that we’ve completed our whirlwind tour of the major theoretical and \npractical underpinnings of triangle rasterization, let’s turn our att ention to \nhow it is typically implemented. In real-time game rendering engines, the \nhigh-level rendering steps described in Section 10.1 are implemented using \na soft ware/hardware architecture known as a pipeline . A pipeline is just an or-\ndered chain of computational stages, each with a speciﬁ c purpose, operating \non a stream of input data items and producing a stream of output data.\nEach stage of a pipeline can typically operate independently of the other \nstages. Hence, one of the biggest advantages of a pipelined architecture is that \nit lends itself extremely well to parallelization . While the ﬁ rst stage is chewing \non one data element, the second stage can be processing the results previously \nproduced by the ﬁ rst stage, and so on down the chain.\nParallelization can also be achieved within an individual stage of the \npipeline. For example, if the computing hardware for a particular stage is du-\nFigure 10.38.  A plot of the function 1/pVz, showing how most of the precision lies close to \nthe camera.\nΔpHz\nΔpVz\nΔpVz\nΔpHz\npHz = 1/pVz\npHz = 1/pVz\n",
      "content_length": 2123,
      "extraction_method": "Direct"
    },
    {
      "page_number": 467,
      "chapter": null,
      "content": "445 \nplicated N times on the die, N data elements can be processed in parallel by \nthat stage. A parallelized pipeline is shown in Figure 10.39. Ideally the stages \noperate in parallel (most of the time), and certain stages are capable of operat-\ning on multiple data items simultaneously as well.\nThe throughput of a pipeline measures how many data items are processed \nper second overall. The pipeline’s latency measures the amount of time it takes \nfor a single data element to make it through the entire pipeline. The latency \nof an individual stage measures how long that stage takes to process a single \nitem. The slowest stage of a pipeline dictates the throughput of the entire pipe-\nline. It also has an impact on the average latency of the pipeline as a whole. \nTherefore, when designing a rendering pipeline, we att empt to minimize and \nbalance latency across the entire pipeline and eliminate bott lenecks. In a well-\ndesigned pipeline, all the stages operate simultaneously, and no stage is ever \nidle for very long waiting for another stage to become free.\n10.2.1. Overview of the Rendering Pipeline\nSome graphics texts divide the rendering pipeline into three coarse-grained \nstages. In this book, we’ll extend this pipeline back even further, to encompass \nthe oﬄ  ine tools used to create the scenes that are ultimately rendered by the \ngame engine. The high level stages in our pipeline are:\nz Tools stage (oﬄ  ine). Geometry and surface properties (materials) are de-\nﬁ ned.\nz Asset conditioning stage (oﬄ  ine). The geometry and material data are pro-\ncessed by the asset conditioning pipeline (ACP) into an engine-ready \nformat.\nStage 3\nStage 1\nStage 2\nTime\nFigure 10.39.  A parallelized pipeline. The stages all operate in parallel and some stages are \ncapable of operating on multiple data items simultaneously as well.\n10.2. The Rendering Pipeline\n",
      "content_length": 1873,
      "extraction_method": "Direct"
    },
    {
      "page_number": 468,
      "chapter": null,
      "content": "446 \n10. The Rendering Engine\nz Application stage (CPU). Potentially visible mesh instances are identiﬁ ed \nand submitt ed to the graphics hardware along with their materials for \nrendering.\nz Geometry processing stage (GPU). Vertices are transformed and lit and \nprojected into homogeneous clip space. Triangles are processed by the \noptional geometry shader and then clipped to the frustum.\nz Rasterization stage (GPU). Triangles are converted into fragments that are \nshaded, passed through various tests (z test, alpha test, stencil test, etc.) \nand ﬁ nally blended into the frame buﬀ er.\n10.2.1.1. How the Rendering Pipeline Transforms Data\n It’s interesting to note how the format of geometry data changes as it passes \nthrough the rendering pipeline. The tools and asset conditioning stages deal \nwith meshes and materials. The application stage deals in terms of mesh in-\nstances and submeshes, each of which is associated with a single material. \nDuring the geometry stage, each submesh is broken down into individual ver-\ntices, which are processed largely in parallel. At the conclusion of this stage, \nthe triangles are reconstructed from the fully transformed and shaded verti-\nces. In the rasterization stage, each triangle is broken into fragments, and these \nfragments are either discarded, or they are eventually writt en into the frame \nbuﬀ er as colors. This process is illustrated in Figure 10.40.\nTools\nACP\nApplication\nGeometry \nProcessing\nVertice\nVertices\nMesh \nInstance\nSubmeshes\nTextures\nMaterials\nTextures\nMesh\nMaterials\nMaterials\nTextures\nRasterization\nVertice\nFragments\nVertice\nPixels\nVertice\nTriangles\nFigure 10.40.  The format of geometric data changes radically as it passes through the vari-\nous stages of the rendering pipeline.\n",
      "content_length": 1761,
      "extraction_method": "Direct"
    },
    {
      "page_number": 469,
      "chapter": null,
      "content": "447 \n10.2.1.2. Implementation of the Pipeline\nThe ﬁ rst two stages of the rendering pipeline are implemented oﬄ  ine, usually \nexecuted by a PC or Linux machine. The application stage is run either by the \nmain CPU of the game console or PC, or by parallel processing units like the \nPS3’s SPUs. The geometry and rasterization stages are usually implemented \non the graphics processing unit (GPU). In the following sections, we’ll explore \nsome of the details of how each of these stages is implemented.\n10.2.2. The Tools Stage\nIn the tools stage, meshes are authored by 3D modelers in a digital content \ncreation (DCC) application like Maya , 3ds Max , Lightwave , Soft image/XSI , \nSketchUp , etc. The models may be deﬁ ned using any convenient surface de-\nscription—NURBS, quads, triangles, etc. However, they are invariably tessel-\nlated into triangles prior to rendering by the runtime portion of the pipeline.\nThe vertices of a mesh may also be skinned . This involves associating \neach vertex with one or more joints in an articulated skeletal structure, along \nwith weights describing each joint’s relative inﬂ uence over the vertex. Skin-\nning information and the skeleton are used by the animation system to drive \nthe movements of a model—see Chapter 11 for more details.\nMaterials are also deﬁ ned by the artists during the tools stage. This in-\nvolves selecting a shader for each material, selecting textures as required by \nthe shader, and specifying the conﬁ guration parameters and options of each \nshader. Textures are mapped onto the surfaces, and other vertex att ributes are \nalso deﬁ ned, oft en by “painting” them with some kind of intuitive tool within \nthe DCC application.\nMaterials are usually authored using a commercial or custom in-house \nmaterial editor . The material editor is sometimes integrated directly into the \nDCC application as a plug-in, or it may be a stand-alone program. Some mate-\nrial editors are live-linked to the game, so that material authors can see what \nthe materials will look like in the real game. Other editors provide an oﬄ  ine \n3D visualization view. Some editors even allow shader programs to be writt en \nand debugged by the artist or a shader engineer. NVIDIA’s Fx Composer is an \nexample of such a tool; it is depicted in Figure 10.41.\nBoth FxComposer and Unreal Engine 3 provide powerful graphical shad-\ning languages . Such tools allow rapid prototyping of visual eﬀ ects by con-\nnecting various kinds of nodes together with a mouse. These tools generally \nprovide a WYSIWYG display of the resulting material. The shaders created \nby a graphical language usually need to be hand-optimized by a rendering \nengineer, because a graphical language invariably trades some runtime per-\n10.2. The Rendering Pipeline\n",
      "content_length": 2775,
      "extraction_method": "Direct"
    },
    {
      "page_number": 470,
      "chapter": null,
      "content": "448 \n10. The Rendering Engine\nFigure 10.42.  The Unreal Engine 3 graphical shader language.\nFigure 10.41.  Nvidia’s Fx Composer allows shader programs to be written, previsualized, and \ndebugged easily.\nformance for its incredible ﬂ exibility, generality, and ease of use. The Unreal \ngraphical shader editor is shown in Figure 10.42.\nMaterials may be stored and managed with the individual meshes. How-\never, this can lead to duplication of data—and eﬀ ort. In many games, a rela-\ntively small number of materials can be used to deﬁ ne a wide range of objects \nin the game. For example, we might deﬁ ne some standard, reusable materials \n",
      "content_length": 639,
      "extraction_method": "Direct"
    },
    {
      "page_number": 471,
      "chapter": null,
      "content": "449 \nlike wood, rock, metal, plastic, cloth, skin, and so on. There’s no reason to du-\nplicate these materials inside every mesh. Instead, many game teams build up \na library of materials from which to choose, and the individual meshes refer \nto the materials in a loosely-coupled manner.\n10.2.3. The Asset Conditioning Stage\nThe asset conditioning stage is itself a pipeline, sometimes called the asset \nconditioning pipeline or ACP. As we saw in Section 6.2.1.4, its job is to export, \nprocess, and link together multiple types of assets into a cohesive whole. For \nexample, a 3D model is comprised of geometry (vertex and index buﬀ ers), \nmaterials, textures, and an optional skeleton. The ACP ensures that all of the \nindividual assets referenced by a 3D model are available and ready to be load-\ned by the engine.\nGeometric and material data is extracted from the DCC application and \nis usually stored in a platform-independent intermediate format. The data is \nthen further processed into one or more platform-speciﬁ c formats, depend-\ning on how many target platforms the engine supports. Ideally the platform-\nspeciﬁ c assets produced by this stage are ready to load into memory and use \nwith litt le or no postprocessing at runtime. For example, mesh data targeted \nfor the Xbox 360 might be output as index and vertex buﬀ ers that are ready \nto be uploaded to video RAM; on the PS3, geometry might be produced in \ncompressed data streams that are ready to be DMA’d to the SPUs for decom-\npression. The ACP oft en takes the needs of the material/shader into account \nwhen building assets. For example, a particular shader might require tangent \nand bitangent vectors as well as a vertex normal; the ACP could generate these \nvectors automatically.\nHigh-level scene graph data structures may also be computed during the \nasset conditioning stage. For example, static level geometry may be processed \nin order to build a BSP tree. (As we’ll investigate in Section 10.2.7.4, scene \ngraph data structures help the rendering engine to very quickly determine \nwhich objects should be rendered, given a particular camera position and ori-\nentation.)\nExpensive lighting calculations are oft en done oﬄ  ine as part of the as-\nset conditioning stage. This is called static lighting ; it may include calcula-\ntion of light colors at the vertices of a mesh (this is called “baked” vertex \nlighting), construction of texture maps that encode per-pixel lighting in-\nformation known as light maps , calculation of precomputed radiance transfer \n(PRT) coeﬃ  cients (usually represented by spherical harmonic functions), \nand so on.\n10.2. The Rendering Pipeline\n",
      "content_length": 2656,
      "extraction_method": "Direct"
    },
    {
      "page_number": 472,
      "chapter": null,
      "content": "450 \n10. The Rendering Engine\n10.2.4. A Brief History of the GPU\nIn the early days of game development, all rendering was done on the CPU. \nGames like Castle Wolfenstein 3D and Doom pushed the limits of what early \nPCs could do, rendering interactive 3D scenes without any help from special-\nized graphics hardware (other than a standard VGA card).\nAs the popularity of these and other PC games took oﬀ , graphics hard-\nware was developed to oﬄ  oad work from the CPU. The earliest graphics ac-\ncelerators, like 3Dfx’s Voodoo line of cards, handled only the most expensive \nstage in the pipeline—the rasterization stage. Subsequent graphics accelera-\ntors provided support for the geometry processing stage as well.\nAt ﬁ rst, graphics hardware provided only a hard-wired but conﬁ gurable \nimplementation known as the ﬁ xed-function pipeline . This technology was \nknown as hardware transformation and lighting , or hardware T&L for short. Later, \ncertain substages of the pipeline were made programmable. Engineers could \nnow write programs called shaders to control exactly how the pipeline pro-\ncessed vertices (vertex shaders) and fragments (fragment shaders, more common-\nly known as pixel shaders). With the introduction of DirectX 10, a third type of \nshader known as a geometry shader was added. It permits rendering engineers \nto modify, cull, or create entire primitives (triangles, lines, and points).\nGraphics hardware has evolved around a specialized type of micropro-\ncessor known as the graphics processing unit or GPU. A GPU is designed to \nmaximize throughput of the pipeline, which it achieves through massive par-\nallelization . For example, a modern GPU like the GeForce 8800 can process \n128 vertices or fragments simultaneously.\nEven in its fully programmable form, a GPU is not a general-purpose \nmicroprocessor—nor should it be. A GPU achieves its high processing speeds \n(on the order of teraﬂ ops on today’s GPUs) by carefully controlling the ﬂ ow of \ndata through the pipeline . Certain pipeline stages are either entirely ﬁ xed in \ntheir function, or they are conﬁ gurable but not programmable. Memory can \nonly be accessed in controlled ways, and specialized data caches are used to \nminimize unnecessary duplication of computations.\nIn the following sections, we’ll brieﬂ y explore the architecture of a mod-\nern GPU and see how the runtime portion of the rendering pipeline is typi-\ncally implemented. We’ll speak primarily about current GPU architectures, \nwhich are used on personal computers with the latest graphics cards and on \nconsole platforms like the Xbox 360 and the PS3. However, not all platforms \nsupport all of the features we’ll be discussing here. For example, the Wii does \nnot support programmable shaders, and most PC games need to support fall-\nback rendering solutions to support older graphics cards with only limited \nprogrammable shader support.\n",
      "content_length": 2901,
      "extraction_method": "Direct"
    },
    {
      "page_number": 473,
      "chapter": null,
      "content": "451 \n10.2.5. The GPU Pipeline\n Virtually all GPUs break the pipeline into the substages described below and \ndepicted in Figure 10.43. Each stage is shaded to indicate whether its function-\nality is programmable, ﬁ xed but conﬁ gurable, or ﬁ xed and non-conﬁ gurable.\n10.2.5.1. Vertex Shader\n This stage is fully programmable. It is responsible for transformation and \nshading/lighting of individual vertices. The input to this stage is a single ver-\ntex (although in practice many vertices are processed in parallel). Its position \nand normal are typically expressed in model space or world space. The vertex \nshader handles transformation from model space to view space via the model-\nview transform. Perspective projection is also applied, as well as per-vertex \nlighting and texturing calculations, and skinning for animated characters. The \nvertex shader can also perform procedural animation by modifying the posi-\ntion of the vertex. Examples of this include foliage that sways in the breeze or \nan undulating water surface. The output of this stage is a fully transformed \nand lit vertex, whose position and normal are expressed in homogeneous clip \nspace (see Section 10.1.4.4).\nOn modern GPUs, the vertex shader has full access to texture data—a ca-\npability that used to be available only to the pixel shader. This is particularly \nuseful when textures are used as stand-alone data structures like height maps \nor look-up tables .\n10.2.5.2. Geometry Shader\n This optional stage is also fully programmable. The geometry shader oper-\nates on entire primitives (triangles, lines, and points) in homogeneous clip \nspace. It is capable of culling or modifying input primitives, and it can also \ngenerate new primitives. Typical uses include shadow volume extrusion (see \nConfigurable\nFixed-Function\nProgrammable\nPrimitive \nAssembly\nGeometry \nShader\nClipping\nScreen \nMapping\nTriangle \nSetup\nTriangle \nTraversal\nEarly \nZ Test\nPixel \nShader\nMerge \n/ ROP\nStream \nOutput\nVertex \nShader\nFrame \nBuffer\nFigure 10.43.  The geometry processing and rasterization stages of the rendering pipeline, as \nimplemented by a typical GPU. The white stages are programmable, the light grey stages are \nconﬁ gurable, and the dark grey boxes are ﬁ xed-function.\n10.2. The Rendering Pipeline\n",
      "content_length": 2275,
      "extraction_method": "Direct"
    },
    {
      "page_number": 474,
      "chapter": null,
      "content": "452 \n10. The Rendering Engine\nSection 10.3.3.1), rendering the six faces of a cube map (see Section 10.3.1.4), \nfur ﬁ n extrusion around silhouett e edges of meshes, creation of particle \nquads from point data (see Section 10.4.1), dynamic tessellation, fractal sub-\ndivision of line segments for lightning eﬀ ects, cloth simulations, and the list \ngoes on.\n10.2.5.3. Stream Output\nModern GPUs permit the data that has been processed up to this point in the \npipeline to be writt en back to memory. From there, it can then be looped back \nto the top of the pipeline for further processing. This feature is called stream \noutput .\nStream output permits a number of intriguing visual eﬀ ects to be achieved \nwithout the aid of the CPU. An excellent example is hair rendering. Hair is \noft en represented as a collection of cubic spline curves. It used to be that hair \nphysics simulation would be done on the CPU. The CPU would also tessellate \nthe splines into line segments. Finally the GPU would render the segments.\nWith stream output, the GPU can do the physics simulation on the control \npoints of the hair splines within the vertex shader. The geometry shader tes-\nsellates the splines, and the stream output feature is used to write the tessel-\nlated vertex data to memory. The line segments are then piped back into the \ntop of the pipeline so they can be rendered.\n10.2.5.4. Clipping\nThe clipping stage chops oﬀ  those portions of the triangles that straddle the \nfrustum . Clipping is done by identifying vertices that lie outside the frustum \nand then ﬁ nding the intersection of the triangle’s edges with the planes of \nthe frustum. These intersection points become new vertices that deﬁ ne one or \nmore clipped triangles.\nThis stage is ﬁ xed in function, but it is somewhat conﬁ gurable. For ex-\nample, user-deﬁ ned clipping planes can be added in addition to the frustum \nplanes. This stage can also be conﬁ gured to cull triangles that lie entirely out-\nside the frustum.\n10.2.5.5. Screen Mapping\nScreen mapping simply scales and shift s the vertices from homogeneous clip \nspace into screen space. This stage is entirely ﬁ xed and non-conﬁ gurable.\n10.2.5.6. Triangle Setup\nDuring triangle setup , the rasterization hardware is initialized for eﬃ  cient \nconversion of the triangle into fragments. This stage is not conﬁ gurable.\n",
      "content_length": 2345,
      "extraction_method": "Direct"
    },
    {
      "page_number": 475,
      "chapter": null,
      "content": "453 \n10.2.5.7. Triangle Traversal\n Each triangle is broken into fragments (i.e., rasterized) by the triangle travers-\nal stage. Usually one fragment is generated for each pixel, although with mul-\ntisample antialiasing (MSAA), multiple fragments are created per pixel (see \nSection 10.1.4.7). The triangle traversal stage also interpolates vertex att ributes \nin order to generate per-fragment att ributes for processing by the pixel shader. \nPerspective-correct interpolation is used where appropriate. This stage’s func-\ntionality is ﬁ xed and not conﬁ gurable.\n10.2.5.8. Early Z Test\n Many graphics cards are capable of checking the depth of the fragment at this \npoint in the pipeline, discarding it if it is being occluded by the pixel already \nin the frame buﬀ er. This allows the (potentially very expensive) pixel shader \nstage to be skipped entirely for occluded fragments.\nSurprisingly, not all graphics hardware supports depth testing at this \nstage of the pipeline. In older GPU designs, the z test was done along with al-\npha testing, aft er the pixel shader had run. For this reason, this stage is called \nthe early z test or early depth test stage.\n10.2.5.9. Pixel Shader\n This stage is fully programmable. Its job is to shade (i.e., light and otherwise \nprocess) each fragment. The pixel shader can also discard fragments, for ex-\nample because they are deemed to be entirely transparent. The pixel shader \ncan address one or more texture maps, run per-pixel lighting calculations, and \ndo whatever else is necessary to determine the fragment’s color.\nThe input to this stage is a collection of per-fragment att ributes (which \nhave been interpolated from the vertex att ributes by the triangle traversal \nstage). The output is a single color vector describing the desired color of the \nfragment.\n10.2.5.10.  Merging / Raster Operations Stage\nThe ﬁ nal stage of the pipeline is known as the merging stage or blending stage, \nalso known as the raster operations stage or ROP in NVIDIA parlance. This \nstage is not programmable, but it is highly conﬁ gurable. It is responsible for \nrunning various fragment tests including the depth test (see Section 10.1.4.8), \nalpha test (in which the values of the fragment’s and pixel’s alpha channels can \nbe used to reject certain fragments), and stencil test (see Section 10.3.3.1).\nIf the fragment passes all of the tests, its color is blended (merged) with \nthe color that is already present in the frame buﬀ er. The way in which blend-\ning occurs is controlled by the alpha blending function —a function whose basic \n10.2. The Rendering Pipeline\n",
      "content_length": 2605,
      "extraction_method": "Direct"
    },
    {
      "page_number": 476,
      "chapter": null,
      "content": "454 \n10. The Rendering Engine\nstructure is hard-wired, but whose operators and parameters can be conﬁ g-\nured in order to produce a wide variety of blending operations.\nAlpha blending is most commonly used to render semi-transparent ge-\nometry. In this case, the following blending function is used:\n \n(1\n)\n.\nD\nS\nS\nS\nD\nA\nA\n′ =\n+\n−\nC\nC\nC\n \nThe subscripts S and D stand for “source” (the incoming fragment) and “des-\ntination” (the pixel in the frame buﬀ er), respectively. Therefore, the color that \nis writt en into the frame buﬀ er (\nD′\nC ) is a weighted average of the existing frame \nbuﬀ er contents (\nD\nC ) and the color of the fragment being drawn (\nS\nC ). The \nblend weight (\nS\nA ) is just the source alpha of the incoming fragment.\nFor alpha blending to look right, the semi-transparent and translucent \nsurfaces in the scene must be sorted and rendered in back-to-front order, af-\nter the opaque geometry has been rendered to the frame buﬀ er. This is be-\ncause aft er alpha blending has been performed, the depth of the new fragment \noverwrites the depth of the pixel with which it was blended. In other words, \nthe depth buﬀ er ignores transparency (unless depth writes have been turned \noﬀ , of course). If we are rendering a stack of translucent objects on top of an \nopaque backdrop, the resulting pixel color should ideally be a blend between \nthe opaque surface’s color and the colors of all of the translucent surfaces in \nthe stack. If we try to render the stack in any order other than back-to-front, \ndepth test failures will cause some of the translucent fragments to be discard-\ned, resulting in an incomplete blend (and a rather odd-looking image).\nOther alpha blending functions can be deﬁ ned as well, for purposes other \nthan transparency blending. The general blending equation takes the form \n(\n)\n(\n),\nD\nS\nS\nD\nD\n′ =\n⊗\n+\n⊗\nC\nw\nC\nw\nC\n where the weighting factors wS and wD can be \nselected by the programmer from a predeﬁ ned set of values including zero, \none, source or destination color, source or destination alpha, and one minus \nthe source or destination color or alpha. The operator ⊗ is either a regular \nscalar-vector multiplication or a component-wise vector-vector multiplication \n(a Hadamard product —see Section 4.2.4.1) depending on the data types of wS\nand wD.\n10.2.6. Programmable Shaders\n Now that we have an end-to-end picture of the GPU pipeline in mind, let’s \ntake a deeper look at the most interesting part of the pipeline—the program-\nmable shaders. Shader architectures have evolved signiﬁ cantly since their \nintroduction with DirectX 8. Early shader models supported only low-level as-\nsembly language programming, and the instruction set and register set of the \npixel shader diﬀ ered signiﬁ cantly from those of the vertex shader. DirectX \n",
      "content_length": 2793,
      "extraction_method": "Direct"
    },
    {
      "page_number": 477,
      "chapter": null,
      "content": "455 \n9 brought with it support for high-level C-like shader languages such as Cg \n(C for graphics), HLSL (High-Level Shading Language —Microsoft ’s imple-\nmentation of the Cg language), and GLSL (OpenGL shading language). With \nDirectX 10, the geometry shader was introduced, and with it came a uniﬁ ed \nshader architecture called shader model 4.0 in DirectX parlance. In the uniﬁ ed \nshader model, all three types of shaders support roughly the same instruction \nset and have roughly the same set of capabilities, including the ability to read \ntexture memory.\nA shader takes a single element of input data and transforms it into zero \nor more elements of output data.\nz In the case of the vertex shader, the input is a vertex whose position and \nnormal are expressed in model space or world space. The output of the \nvertex shader is a fully transformed and lit vertex, expressed in homo-\ngeneous clip space.\nz The input to the geometry shader is a single n-vertex primitive—a point \n(n = 1), line segment (n = 2), or triangle (n = 3)—with up to n additional \nvertices that act as control points. The output is zero or more primitives, \npossibly of a diﬀ erent type than the input. For example, the geometry \nshader could convert points into two-triangle quads, or it could trans-\nform triangles into triangles but optionally discard some triangles, and \nso on.\nz The pixel shader’s input is a fragment whose att ributes have been in-\nterpolated from the three vertices of the triangle from which it came. \nThe output of the pixel shader is the color that will be writt en into the \nframe buﬀ er (presuming the fragment passes the depth test and other \noptional tests). The pixel shader is also capable of discarding fragments \nexplicitly, in which case it produces no output.\n10.2.6.1. Accessing Memory\n Because the GPU implements a data processing pipeline, access to RAM is \nvery carefully controlled. A shader program cannot read from or write to \nmemory directly. Instead, its memory accesses are limited to two methods: \nregisters and texture maps.\nShader Registers\n A shader can access RAM indirectly via registers. All GPU registers are in 128-\nbit SIMD format. Each register is capable of holding four 32-bit ﬂ oating-point \nor integer values (represented by the float4 data type in the Cg language). \nSuch a register can contain a four-element vector in homogeneous coordinates \nor a color in RGBA format, with each component in 32-bit ﬂ oating-point for-\n10.2. The Rendering Pipeline\n",
      "content_length": 2497,
      "extraction_method": "Direct"
    },
    {
      "page_number": 478,
      "chapter": null,
      "content": "456 \n10. The Rendering Engine\nmat. Matrices can be represented by groups of three or four registers (rep-\nresented by built-in matrix types like float4x4 in Cg). A GPU register can \nalso be used to hold a single 32-bit scalar, in which case the value is usually \nreplicated across all four 32-bit ﬁ elds. Some GPUs can operate on 16-bit ﬁ elds, \nknown as halfs. (Cg provides various built-in types like half4 and half4x4\nfor this purpose.)\nRegisters come in four ﬂ avors, as follows:\nz Input registers. These registers are the shader’s primary source of input \ndata. In a vertex shader, the input registers contain att ribute data ob-\ntained directly from the vertices. In a pixel shader, the input registers \ncontain interpolated vertex att ribute data corresponding to a single \nfragment. The values of all input registers are set automatically by the \nGPU prior to invoking the shader.\nz Constant registers. The values of constant registers are set by the applica-\ntion and can change from primitive to primitive. Their values are con-\nstant only from the point of view of the shader program. They provide \na secondary form of input to the shader. Typical contents include the \nmodel-view matrix, the projection matrix, light parameters, and any \nother parameters required by the shader that are not available as vertex \natt ributes.\nz Temporary registers. These registers are for use by the shader program inter-\nnally and are typically used to store intermediate results of calculations.\nz Output registers. The contents of these registers are ﬁ lled in by the shader \nand serve as its only form of output. In a vertex shader, the output regis-\nters contain vertex att ributes such as the transformed position and nor-\nmal vectors in homogeneous clip space, optional vertex colors, texture \ncoordinates, and so on. In a pixel shader, the output register contains \nthe ﬁ nal color of the fragment being shaded.\nThe application provides the values of the constant registers when it sub-\nmits primitives for rendering. The GPU automatically copies vertex or frag-\nment att ribute data from video RAM into the appropriate input registers prior \nto calling the shader program, and it also writes the contents of the output \nregisters back into RAM at the conclusion of the program’s execution so that \nthe data can be passed to the next stage of the pipeline.\nGPUs typically cache output data so that it can be reused without be-\ning recalculated. For example, the post-transform vertex cache stores the most-\nrecently processed vertices emitt ed by the vertex shader. If a triangle is en-\ncountered that refers to a previously-processed vertex, it will be read from the \npost-transform vertex cache if possible—the vertex shader need only be called \n",
      "content_length": 2751,
      "extraction_method": "Direct"
    },
    {
      "page_number": 479,
      "chapter": null,
      "content": "457 \nagain if the vertex in question has since been ejected from the cache to make \nroom for newly processed vertices.\nTextures\nA shader also has direct read-only access to texture maps. Texture data is ad-\ndressed via texture coordinates, rather than via absolute memory addresses. \nThe GPU’s texture samplers automatically ﬁ lter the texture data, blending val-\nues between adjacent texels or adjacent mipmap levels as appropriate. Texture \nﬁ ltering can be disabled in order to gain direct access to the values of particu-\nlar texels. This can be useful when a texture map is used as a data table, for \nexample.\nShaders can only write to texture maps in an indirect manner—by render-\ning the scene to an oﬀ -screen frame buﬀ er that is interpreted as a texture map \nby subsequent rendering passes. This feature is known as render to texture .\n10.2.6.2. Introduction to High-Level Shader Language Syntax\nHigh-level shader languages like Cg and GLSL are modeled aft er the C pro-\ngramming language. The programmer can declare functions, deﬁ ne a simple \nstruct, and perform arithmetic. However, as we said above, a shader pro-\ngram only has access to registers and textures. As such, the struct and vari-\nable we declare in Cg or GLSL is mapped directly onto registers by the shader \ncompiler. We deﬁ ne these mappings in the following ways:\nz Semantics . Variables and struct members can be suﬃ  xed with a co-\nlon followed by a keyword known as a semantic. The semantic tells the \nshader compiler to bind the variable or data member to a particular \nvertex or fragment att ribute. For example, in a vertex shader we might \ndeclare an input struct whose members map to the position and color \natt ributes of a vertex as follows:\n struct VtxOut\n {\n  float4 pos : POSITION; // map to the position  \n         // \nattribute\n  float4 color : COLOR; \n// map to the color attribute\n };\nz Input versus output. The compiler determines whether a particular vari-\nable or struct should map to input or output registers from the context \nin which it is used. If a variable is passed as an argument to the shader \nprogram’s main function, it is assumed to be an input; if it is the return \nvalue of the main function, it is taken to be an output.\n10.2. The Rendering Pipeline\n",
      "content_length": 2265,
      "extraction_method": "Direct"
    },
    {
      "page_number": 480,
      "chapter": null,
      "content": "458 \n10. The Rendering Engine\n  VtxOut vshaderMain(VtxIn in) // in maps to input   \n \n           \n  // \nregisters\n  {\n   \nVtxOut out;\n   \n// ...\n   \nreturn out;  \n// out maps to output registers\n  }\nz Uniform declaration . To gain access to the data supplied by the applica-\ntion via the constant registers, we can declare a variable with the key-\nword uniform. For example, the model-view matrix could be passed to \na vertex shader as follows:\n \nVtxOut vshaderMain(VtxIn in,\n \n                   uniform float4x4 modelViewMatrix)\n {\n  VtxOut \nout;\n  // \n...\n  return \nout; \n }\nArithmetic operations can be performed by invoking C-style operators, \nor by calling intrinsic functions as appropriate. For example, to multiply the \ninput vertex position by the model-view matrix, we could write:\n \nVtxOut vshaderMain(VtxIn in,\n \n                   uniform float4x4 modelViewMatrix)\n {\n  VtxOut \nout;\nout.pos = mul(modelViewMatrix, in.pos);\n \n \nout.color = float4(0, 1, 0, 1); // RGBA green\n  return \nout; \n }\nData is obtained from textures by calling special intrinsic functions that \nread the value of the texels at a speciﬁ ed texture coordinate. A number of vari-\nants are available for reading one-, two- and three-dimensional textures in \nvarious formats, with and without ﬁ ltering. Special texture addressing modes \nare also available for accessing cube maps and shadow maps. References to \nthe texture maps themselves are declared using a special data type known as \na texture sampler declaration. For example, the data type sampler2D repre-\nsents a reference to a typical two-dimensional texture. The following simple \nCg pixel shader applies a diﬀ use texture to a triangle:\nstruct FragmentOut\n{\n  float4 color : COLOR;\n};\n",
      "content_length": 1730,
      "extraction_method": "Direct"
    },
    {
      "page_number": 481,
      "chapter": null,
      "content": "459 \nFragmentOut pshaderMain(float2 uv : TEXCOORD0,\n                        uniform sampler2D texture)\n{\n \nFragmentOut out;\n \nout.color = tex2D(texture, uv); \n// look up texel at  \n  \n          // \n(u,v)\n \nreturn out;\n}\n10.2.6.3. Effect Files\n By itself, a shader program isn’t particularly useful. Additional information is \nrequired by the GPU pipeline in order to call the shader program with mean-\ningful inputs. For example, we need to specify how the application-speciﬁ ed \nparameters, like the model-view matrix, light parameters, and so on, map to \nthe uniform variables declared in the shader program. In addition, some vi-\nsual eﬀ ects require two or more rendering passes, but a shader program only \ndescribes the operations to be applied during a single rendering pass. If we \nare writing a game for the PC platform, we will need to deﬁ ne “fallback” ver-\nsions of some of our more-advanced rendering eﬀ ects, so that they will work \neven on older graphics cards. To tie our shader program(s) together into a \ncomplete visual eﬀ ect, we turn to a ﬁ le format known as an eﬀ ect ﬁ le.\nDiﬀ erent rendering engines implement eﬀ ects in slightly diﬀ erent ways. \nIn Cg, the eﬀ ect ﬁ le format is known as CgFX . Ogre3D uses a ﬁ le format very \nsimilar to CgFX known as a material ﬁ le. GLSL eﬀ ects can be described using \nthe COLLADA format, which is based on XML. Despite the diﬀ erences, eﬀ ects \ngenerally take on the following hierarchical format:\nz At global scope, structs, shader programs (implemented as various \n“main” functions), and global variables (which map to application-\nspeciﬁ ed constant parameters) are deﬁ ned.\nz One or more techniques are deﬁ ned. A technique represents one way to \nrender a particular visual eﬀ ect. An eﬀ ect typically provides a primary \ntechnique for its highest-quality implementation and possibly a number \nof fall back techniques for use on lower-powered graphics hardware.\nz Within each technique, one or more passes are deﬁ ned. A pass describes \nhow a single full-frame image should be rendered. It typically includes \na reference to a vertex, geometry and/or pixel shader program’s “main” \nfunction, various parameter bindings, and optional render state sett ings.\n10.2.6.4. Further Reading\nIn this section, we’ve only had a small taste of what high-level shader pro-\ngramming is like—a complete tutorial is beyond our scope here. For a much \n10.2. The Rendering Pipeline\n",
      "content_length": 2431,
      "extraction_method": "Direct"
    },
    {
      "page_number": 482,
      "chapter": null,
      "content": "460 \n10. The Rendering Engine\nmore-detailed introduction to Cg shader programming, refer to the Cg tu-\ntorial available on NVIDIA’s website at htt p://developer.nvidia.com/object/\ncg_tutorial_home.html.\n10.2.7. The Application Stage\nNow that we understand how the GPU works, we can discuss the pipeline \nstage that is responsible for driving it—the application stage . This stage has \nthree roles:\n \n1. Visibility determination. Only objects that are visible (or at least poten-\ntially visible) should be submitt ed to the GPU, lest we waste valuable \nresources processing triangles that will never be seen.\n \n2. Submitt ing geometry to the GPU for rendering. Submesh-material pairs are \nsent to the GPU via a rendering call like DrawIndexedPrimitive()\n(DirectX) or glDrawArrays() (OpenGL), or via direct construction of \nthe GPU command list. The geometry may be sorted for optimal render-\ning performance. Geometry might be submitt ed more than once if the \nscene needs to be rendered in multiple passes.\n \n3. Controlling shader parameters and render state. The uniform parameters \npassed to the shader via constant registers are conﬁ gured by the ap-\nplication stage on a per-primitive basis. In addition, the application \nstage must set all of the conﬁ gurable parameters of the non-program-\nmable pipeline stages to ensure that each primitive is rendered ap-\npropriately.\nIn the following sections, we’ll brieﬂ y explore how the application stage per-\nforms these tasks.\n10.2.7.1. Visibility Determination\nThe cheapest triangles are the ones you never draw. So it’s incredibly impor-\ntant to cull objects from the scene that do not contribute to the ﬁ nal rendered \nimage prior to submitt ing them to the GPU. The process of constructing the \nlist of visible mesh instances is known as visibility determination .\nFrustum Culling\nIn frustum culling , all objects that lie entirely outside the frustum are exclud-\ned from our render list. Given a candidate mesh instance, we can determine \nwhether or not it lies inside the frustum by performing some simple tests be-\ntween the object’s bounding volume and the six frustum planes. The bounding \nvolume is usually a sphere, because spheres are particularly easy to cull. For \n",
      "content_length": 2228,
      "extraction_method": "Direct"
    },
    {
      "page_number": 483,
      "chapter": null,
      "content": "461 \neach frustum plane, we move the plane inward a distance equal to the radius \nof the sphere, then we determine on which side of each modiﬁ ed plane the \ncenter point of the sphere lies. If the sphere is found to be on the front side of \nall six modiﬁ ed planes, the sphere is inside the frustum.\nA scene graph data structure, described in Section 10.2.7.4, can help opti-\nmize frustum culling by allowing us to ignore objects whose bounding spheres \nare nowhere close to being inside the frustum.\nOcclusion and Potentially Visible Sets\nEven when objects lie entirely within the frustum, they may occlude one an-\nother. Removing objects from the visible list that are entirely occluded by \nother objects is called occlusion culling . In crowded environments viewed from \nground level, there can be a great deal of inter-object occlusion, making oc-\nclusion culling extremely important. In less crowded scenes, or when scenes \nare viewed from above, much less occlusion may be present and the cost of \nocclusion culling may outweigh its beneﬁ ts.\nGross occlusion culling of a large-scale environment can be done by pre-\ncalculating a potentially visible set (PVS). For any given camera vantage point, \na PVS lists those scene objects that might be visible. A PVS errs on the side of \nincluding objects that aren’t actually visible, rather than excluding objects that \nactually would have contributed to the rendered scene.\nOne way to implement a PVS system is to chop the level up into regions \nof some kind. Each region can be provided with a list of the other regions \nthat can be seen when the camera is inside it. These PVSs might be manu-\nally speciﬁ ed by the artists or game designers. More commonly, an automated \noﬄ  ine tool generates the PVS based on user-speciﬁ ed regions. Such a tool \nusually operates by rendering the scene from various randomly distributed \nvantage points within a region. Every region’s geometry is color coded, so the \nlist of visible regions can be found by scanning the resulting frame buﬀ er and \ntabulating the region colors that are found. Because automated PVS tools are \nimperfect, they typically provide the user with a mechanism for tweaking the \nresults, either by manually placing vantage points for testing, or by manually \nspecifying a list of regions that should be explicitly included or excluded from \na particular region’s PVS.\nPortals\nAnother way to determine what portions of a scene are visible is to use portals . \nIn portal rendering, the game world is divided up into semiclosed regions \nthat are connected to one another via holes, such as windows and doorways. \nThese holes are called portals. They are usually represented by polygons that \ndescribe their boundaries.\n10.2. The Rendering Pipeline\n",
      "content_length": 2759,
      "extraction_method": "Direct"
    },
    {
      "page_number": 484,
      "chapter": null,
      "content": "462 \n10. The Rendering Engine\nTo render a scene with portals, we start by rendering the region that con-\ntains the camera. Then, for each portal in the region, we extend a frustum-like \nvolume consisting of planes extending from the camera’s focal point through \neach edge of the portal’s bounding polygon. The contents of the neighboring \nregion can be culled to this portal volume in exactly the same way geometry is \nculled against the camera frustum. This ensures that only the visible geometry \nin the adjacent regions will be rendered. Figure 10.44 provides an illustration \nof this technique.\nOcclusion Volumes (Antiportals)\nIf we ﬂ ip the portal concept on its head, pyramidal volumes can also be used \nto describe regions of the scene that cannot be seen because they are being \noccluded by an object. These volumes are known as occlusion volumes or anti-\nportals . To construct an occlusion volume, we ﬁ nd the silhouett e edges of each \nFigure 10.44.  Portals are used to deﬁ ne frustum-like volumes which are used to cull the con-\ntents of neighboring regions. In this example, objects A, B, and D will be culled because they \nlie outside one of the portals; the other objects will be visible.\nA\nH\nE\nD\nF\nG\nB\nC\nFigure 10.45.  As a result of the antiportals corresponding to objects A, B, and C, objects D, E, \nF, and G are culled. Therefore only A, B, C, and H are visible.\n",
      "content_length": 1385,
      "extraction_method": "Direct"
    },
    {
      "page_number": 485,
      "chapter": null,
      "content": "463 \noccluding object and extend planes outward from the camera’s focal point \nthrough each of these edges. We test more-distant objects against these oc-\nclusion volumes and cull them if they lie entirely within the occlusion region. \nThis is illustrated in Figure 10.45.\nPortals are best used when rendering enclosed indoor environments with \na relatively small number of windows and doorways between “rooms.” In \nthis kind of scene, the portals occupy a relatively small percentage of the total \nvolume of the camera frustum, resulting in a large number of objects outside \nthe portals which can be culled. Antiportals are best applied to large outdoor \nenvironments, in which nearby objects oft en occlude large swaths of the cam-\nera frustum. In this case, the antiportals occupy a relatively large percent-\nage of the total camera frustum volume, resulting in large numbers of culled \nobjects.\n10.2.7.2. Primitive Submission\n Once a list of visible geometric primitives has been generated, the individual \nprimitives must be submitt ed to the GPU pipeline for rendering. This can be \naccomplished by making calls to DrawIndexedPrimitive() in DirectX or \nglDrawArrays() in OpenGL.\nRender State\n As we learned in Section 10.2.5, the functionality of many of the GPU pipeline’s \nstages is ﬁ xed but conﬁ gurable. And even programmable stages are driven in \npart by conﬁ gurable parameters. Some examples of these conﬁ gurable param-\neters are listed below (although this is by no means a complete list!)\nz world-view matrix;\nz light direction vectors;\nz texture bindings (i.e., which textures to use for a given material/\nshader);\nz texture addressing and ﬁ ltering modes;\nz time base for scrolling textures and other animated eﬀ ects;\nz z test (enabled or disabled);\nz alpha blending options.\nThe set of all conﬁ gurable parameters within the GPU pipeline is known \nas the hardware state or render state. It is the application stage’s responsibility to \nensure that the hardware state is conﬁ gured properly and completely for each \nsubmitt ed primitive. Ideally these state sett ings are described completely by \nthe material associated with each submesh. So the application stage’s job boils \n10.2. The Rendering Pipeline\n",
      "content_length": 2228,
      "extraction_method": "Direct"
    },
    {
      "page_number": 486,
      "chapter": null,
      "content": "464 \n10. The Rendering Engine\ndown to iterating through the list of visible mesh instances, iterating over each \nsubmesh-material pair, sett ing the render state based on the material’s speciﬁ -\ncations, and then calling the low level primitive submission functions (Draw-\nIndexedPrimitive(), glDrawArrays() or similar).\nState Leaks\nIf we forget to set some aspect of the render state between submitt ed primi-\ntives, the sett ings used on the previous primitive will “leak” over onto the new \nprimitive. A render state leak might manifest itself as an object with the wrong \ntexture or an incorrect lighting eﬀ ect, for example. Clearly it’s important that \nthe application stage never allow state leaks to occur.\nThe GPU Command List\nThe application stage actually communicates with the GPU via a command \nlist . These commands interleave render state sett ings with references to the \ngeometry that should be drawn. For example, to render objects A and B with \nmaterial 1, followed by objects C, D, and E using material 2, the command list \nmight look like this:\nz Set render state for material 1 (multiple commands, one per render state \nsett ing).\nz Submit primitive A.\nz Submit primitive B.\nz Set render state for material 2 (multiple commands).\nz Submit primitive C.\nz Submit primitive D.\nz Submit primitive E.\nUnder the hood, API functions like DrawIndexedPrimitive() actu-\nally just construct and submit GPU command lists. The cost of these API calls \ncan themselves be too high for some applications. To maximize performance, \nsome game engines build GPU command lists manually or by calling a low-\nlevel rendering API like the PS3’s libgcm library.\n10.2.7.3. Geometry Sorting\nRender state sett ings are global—they apply to the entire GPU as a whole. \nSo in order to change render state sett ings, the entire GPU pipeline must be \nﬂ ushed before the new sett ings can be applied. This can cause massive perfor-\nmance degradation if not managed carefully.\nClearly we’d like to change render sett ings as infrequently as possible. \nThe best way to accomplish this is to sort our geometry by material. That way, \n",
      "content_length": 2121,
      "extraction_method": "Direct"
    },
    {
      "page_number": 487,
      "chapter": null,
      "content": "465 \nwe can install material A’s sett ings, render all geometry associated with mate-\nrial A, and then move on to material B.\nUnfortunately, sorting geometry by material can have a detrimental eﬀ ect \non rendering performance because it increases overdraw —a situation in which \nthe same pixel is ﬁ lled multiple times by multiple overlapping triangles. Cer-\ntainly some overdraw is necessary and desirable, as it is the only way to prop-\nerly alpha-blend transparent and translucent surfaces into a scene. However, \noverdraw of opaque pixels is always a waste of GPU bandwidth.\nThe early z test is designed to discard occluded fragments before the ex-\npensive pixel shader has a chance to execute. But to take maximum advantage \nof early z, we need to draw the triangles in front-to-back order. That way, the \nclosest triangles will ﬁ ll the z-buﬀ er right oﬀ  the bat, and all of the fragments \ncoming from more-distant triangles behind them can be quickly discarded, \nwith litt le or no overdraw.\nZ Prepass to the Rescue\nHow can we reconcile the need to sort geometry by material with the conﬂ ict-\ning need to render opaque geometry in a front-to-back order? The answer lies \nin a GPU feature known as z prepass .\nThe idea behind z prepass is to render the scene twice: the ﬁ rst time to \ngenerate the contents of the z-buﬀ er as eﬃ  ciently as possible and the second \ntime to populate the frame buﬀ er with full color information (but this time \nwith no overdraw, thanks to the contents of the z-buﬀ er). The GPU provides a \nspecial double-speed rendering mode in which the pixel shaders are disabled, \nand only the z-buﬀ er is updated. Opaque geometry can be rendered in front-\nto-back order during this phase, to minimize the time required to generate \nthe z-buﬀ er contents. Then the geometry can be resorted into material order \nand rendered in full color with minimal stage changes for maximum pipeline \nthroughput.\nOnce the opaque geometry has been rendered, transparent surfaces can \nbe drawn in back-to-front order. Unfortunately, there is no general solution \nto the material sorting problem for transparent geometry. We must render it \nin back-to-front order to achieve the proper alpha-blended result. Therefore \nwe must accept the cost of frequent state changes when drawing transparent \ngeometry (unless our particular game’s usage of transparent geometry is such \nthat a speciﬁ c optimization can be implemented).\n10.2.7.4. Scene Graphs\nModern game worlds can be very large. The majority of the geometry in most \nscenes does not lie within the camera frustum, so frustum culling all of these \n10.2. The Rendering Pipeline\n",
      "content_length": 2642,
      "extraction_method": "Direct"
    },
    {
      "page_number": 488,
      "chapter": null,
      "content": "466 \n10. The Rendering Engine\nobjects explicitly is usually incredibly wasteful. Instead, we would like to de-\nvise a data structure that manages all of the geometry in the scene and allows \nus to quickly discard large swaths of the world that are nowhere near the cam-\nera frustum prior to performing detailed frustum culling. Ideally, this data \nstructure should also help us to sort the geometry in the scene, either in front-\nto-back order for the z prepass or in material order for full-color rendering.\nSuch a data structure is oft en called a scene graph , in reference to the graph-\nlike data structures oft en used by ﬁ lm rendering engines and DCC tools like \nMaya. However, a game’s scene graph needn’t be a graph, and in fact the data \nstructure of choice is usually some kind of tree. The basic idea behind most \nof these data structures is to partition three-dimensional space in a way that \nmakes it easy to discard regions that do not intersect the frustum, without \nhaving to frustum cull all of the individual objects within them. Examples \ninclude quadtrees and octress, BSP trees, kd-trees, and spatial hashing tech-\nniques.\nQuadtrees and Octrees\nA quadtree divides space into quadrants recursively. Each level of recursion \nis represented by a node in the quadtree with four children, one for each \nquadrant. The quadrants are typically separated by vertically oriented, ax-\nis-aligned planes, so that the quadrants are square or rectangular. However, \nsome quadtrees subdivide space using arbitrarily-shaped regions.\nQuadtrees can be used to store and organize virtually any kind of spa-\ntially-distributed data. In the context of rendering engines, quadtrees are of-\nten used to store renderable primitives such as mesh instances, subregions of \nterrain geometry, or individual triangles of a large static mesh, for the pur-\nposes of eﬃ  cient frustum culling. The renderable primitives are stored at the \nFigure 10.46.  A top-down view of a space divided recursively into quadrants for storage in a \nquadtree, based on the criterion of one point per region.\n",
      "content_length": 2082,
      "extraction_method": "Direct"
    },
    {
      "page_number": 489,
      "chapter": null,
      "content": "467 \nleaves of the tree, and we usually aim to achieve a roughly uniform number of \nprimitives within each leaf region. This can be achieved by deciding whether \nto continue or terminate the subdivision based on the number of primitives \nwithin a region.\nTo determine which primitives are visible within the camera frustum, \nwe walk the tree from the root to the leaves, checking each region for inter-\nsection with the frustum. If a given quadrant does not intersect the frustum, \nthen we know that none of its child regions will do so either, and we can stop \ntraversing that branch of the tree. This allows us to search for potentially \nvisible primitives much more quickly than would be possible with a linear \nsearch (usually in O(log n) time). An example of a quadtree subdivision of \nspace is shown in Figure 10.46.\nAn octree is the three-dimensional equivalent of a quadtree, dividing space \ninto eight subregions at each level of the recursive subdivision. The regions of \nan octree are oft en cubes or rectangular prisms but can be arbitrarily-shaped \nthree-dimensional regions in general.\nBounding Sphere Trees\nIn the same way that a quadtree or octree subdivides space into (usually) \nrectangular regions, a bounding sphere tree divides space into spherical regions \nhierarchically. The leaves of the tree contain the bounding spheres of the ren-\nderable primitives in the scene. We collect these primitives into small logical \ngroups and calculate the net bounding sphere of each group. The groups are \nthemselves collected into larger groups, and this process continues until we \nhave a single group with a bounding sphere that encompasses the entire vir-\ntual world. To generate a list of potentially visible primitives, we walk the tree \nfrom the root to the leaves, testing each bounding sphere against the frustum, \nand only recursing down branches that intersect it.\nBSP Trees\nA binary space partitioning (BSP) tree divides space in half recursively until \nthe objects within each half-space meet some predeﬁ ned criteria (much as a \nquadtree divides space into quadrants). BSP trees have numerous uses, in-\ncluding collision detection and constructive solid geometry, as well as its most \nwell-known application as a method for increasing the performance of frus-\ntum culling and geometry sorting for 3D graphics. A kd-tree is a generaliza-\ntion of the BSP tree concept to k dimensions.\nIn the context of rendering, a BSP tree divides space with a single plane at \neach level of the recursion. The dividing planes can be axis-aligned, but more \ncommonly each subdivision corresponds to the plane of a single triangle in \nthe scene. All of the other triangles are then categorized as being either on \n10.2. The Rendering Pipeline\n",
      "content_length": 2749,
      "extraction_method": "Direct"
    },
    {
      "page_number": 490,
      "chapter": null,
      "content": "468 \n10. The Rendering Engine\nthe front side or the back side of the plane. Any triangles that intersect the \ndividing plane are themselves divided into three new triangles, so that every \ntriangle lies either entirely in front of or entirely behind the plane, or is copla-\nnar with it. The result is a binary tree with a dividing plane and one or more \ntriangles at each interior node and triangles at the leaves.\nA BSP tree can be used for frustum culling in much the same way a \nquadtree, octree, or bounding sphere tree can. However, when generated with \nindividual triangles as described above, a BSP tree can also be used to sort tri-\nangles into a strictly back-to-front or front-to-back order. This was particularly \nimportant for early 3D games like Doom, which did not have the beneﬁ t of a \nz-buﬀ er and so were forced to use the painter’s algorithm (i.e., to render the \nscene from back to front) to ensure proper inter-triangle occlusion.\nGiven a camera view point in 3D space, a back-to-front sorting algorithm \nwalks the tree from the root. At each node, we check whether the view point \nis in front of or behind that node’s dividing plane. If the camera is in front of \na node’s plane, we visit the node’s back children ﬁ rst, then draw any triangles \nthat are coplanar with its dividing plane, and ﬁ nally we visit its front chil-\ndren. Likewise, when the camera’s view point is found to be behind a node’s \ndividing plane, we visit the node’s front children ﬁ rst, then draw the triangles \ncoplanar with the node’s plane, and ﬁ nally we visit its back children. This \ntraversal scheme ensures that the triangles farthest from the camera will be \nvisited before those that are closer to it, and hence it yields a back-to-front \nFigure 10.47.  An example of back-to-front traversal of the triangles in a BSP tree. The tri-\nangles are shown edge-on in two dimensions for simplicity, but in a real BSP tree the triangles \nand dividing planes would be arbitrarily oriented in space.\nA\nB\nC\nD2\nD1\nCamera\nVisit A\nCam is in front\n    Visit B\n        Leaf node\nDraw B\nDraw A\n    Visit C\n        Cam is in front\n            V isit D 1\n                Leaf node\nDraw D1\nDraw C\n            V isit D 2\n                Leaf node\nDraw D2\nA\nD2\nD1\nC\nB\n",
      "content_length": 2252,
      "extraction_method": "Direct"
    },
    {
      "page_number": 491,
      "chapter": null,
      "content": "469 \nordering. Because this algorithm traverses all of the triangles in the scene, the \norder of the traversal is independent of the direction the camera is looking. A \nsecondary frustum culling step would be required in order to traverse only \nvisible triangles. A simple BSP tree is shown in Figure 10.47, along with the \ntree traversal that would be done for the camera position shown.\nFull coverage of BSP tree generation and usage algorithms is beyond our \nscope here. See htt p://www.ccs.neu.edu/home/donghui/teaching/slides/ge-\nometry/BSP2D.ppt and htt p://www.gamedev.net/reference/articles/article657.\nasp for more details on BSP trees.\n10.2.7.5. Choosing a Scene Graph\n Clearly there are many diﬀ erent kinds of scene graphs. Which data structure \nto select for your game will depend upon the nature of the scenes you expect \nto be rendering. To make the choice wisely, you must have a clear understand-\ning of what is required—and more importantly what is not required—when \nrendering scenes for your particular game.\nFor example, if you’re implementing a ﬁ ghting game, in which two char-\nacters batt le it out in a ring surrounded by a mostly static environment, you \nmay not need much of a scene graph at all. If your game takes place primarily \nin enclosed indoor environments, a BSP tree or portal system may serve you \nwell. If the action takes place outdoors on relatively ﬂ at terrain, and the scene \nis viewed primarily from above (as might be the case in a real-time strategy \ngame or god game), a simple quad tree might be all that’s required to achieve \nhigh rendering speeds. On the other hand, if an outdoor scene is viewed pri-\nmarily from the point of view of someone on the ground, we may need addi-\ntional culling mechanisms. Densely populated scenes can beneﬁ t from an oc-\nclusion volume (antiportal) system, because there will be plenty of occluders. \nOn the other hand, if your outdoor scene is very sparse, adding an antiportal \nsystem probably won’t pay dividends (and might even hurt your frame rate).\nUltimately, your choice of scene graph should be based on hard data ob-\ntained by actually measuring the performance of your rendering engine. You \nmay be surprised to learn where all your cycles are actually going! But once \nyou know, you can select scene graph data structures and/or other optimiza-\ntions to target the speciﬁ c problems at hand.\n10.3. Advanced Lighting and Global Illumination\nIn order to render photorealistic scenes, we need physically accurate global \nillumination algorithms. A complete coverage of these techniques is beyond \nour scope. In the following sections, we will brieﬂ y outline the most prevalent \n10.3. Advanced Lighting and Global Illumination\n",
      "content_length": 2719,
      "extraction_method": "Direct"
    },
    {
      "page_number": 492,
      "chapter": null,
      "content": "470 \n10. The Rendering Engine\ntechniques in use within the game industry today. Our goal here is to provide \nyou with an awareness of these techniques and a jumping oﬀ  point for further \ninvestigation. For an excellent in-depth coverage of this topic, see [8].\n10.3.1. Image-Based Lighting\nA number of advanced lighting and shading techniques make heavy use of \nimage data, usually in the form of two-dimensional texture maps. These are \ncalled image-based lighting algorithms.\n10.3.1.1. Normal Mapping\nA normal map speciﬁ es a surface normal direction vector at each texel. This al-\nlows a 3D modeler to provide the rendering engine with a highly detailed de-\nscription of a surface’s shape, without having to tessellate the model to a high \ndegree (as would be required if this same information were to be provided \nvia vertex normals). Using a normal map, a single ﬂ at triangle can be made to \nlook as though it were constructed from millions of tiny triangles. An example \nof normal mapping is shown in Figure 10.48.\nThe normal vectors are typically encoded in the RGB color channels of the \ntexture, with a suitable bias to overcome the fact that RGB channels are strictly \npositive while normal vector components can be negative. Sometimes only \ntwo coordinates are stored in the texture; the third can be easily calculated at \nruntime, given the assumption that the surface normals are unit vectors.\nFigure 10.48.  An example of a normal-mapped surface.\n10.3.1.2. Height Maps: Parallax and Relief Mapping\nAs its name implies, a height map encodes the height of the ideal surface above \nor below the surface of the triangle. Height maps are typically encoded as \ngrayscale images, since we only need a single height value per texel.\n",
      "content_length": 1741,
      "extraction_method": "Direct"
    },
    {
      "page_number": 493,
      "chapter": null,
      "content": "471 \nHeight maps are oft en used for parallax mapping and relief mapping —two \ntechniques that can make a planar surface appear to have rather extreme \nheight variation that properly self-occludes and self-shadows. Figure 10.49 \nshows an example of parallax occlusion mapping implemented in DirectX 9.\nA height map can also be used as a cheap way to generate surface normals. \nThis technique was used in the early days of bump mapping . Nowadays, most \ngame engines store surface normal information explicitly in a normal map, \nrather than calculating the normals from a height map.\n10.3.1.3. Specular/Gloss Maps\nWhen light reﬂ ects directly oﬀ  a shiny surface, we call this specular reﬂ ection. \nThe intensity of a specular reﬂ ection depends on the relative angles of the \nviewer, the light source, and the surface normal. As we saw in Section 10.1.3.2, \nthe specular intensity takes the form \n(\n) ,\nSk\nα\n⋅\nR V\n where R is the reﬂ ection of \nthe light’s direction vector about the surface normal, V is the direction to the \nviewer, kS is the overall specular reﬂ ectivity of the surface, and α is called the \nspecular power.\nMany surfaces aren’t uniformly glossy. For example, when a person’s face \nis sweaty and dirty, wet regions appear shiny, while dry or dirty areas appear \ndull. We can encode high-detail specularity information in a special texture \nmap known as a specular map .\nIf we store the value of kS in the texels of a specular map, we can control \nhow much specular reﬂ ection should be applied at each texel. This kind of \nspecular map is sometimes called a gloss map . It is also called a specular mask, \nbecause zero-valued texels can be used to “mask oﬀ ” regions of the surface \nwhere we do not want specular reﬂ ection applied. If we store the value of α \nin our specular map, we can control the amount of “focus” our specular high-\nFigure 10.49.  DirectX 9 parallax occlusion mapping. The surface is actually a ﬂ at disc; a height \nmap texture is used to deﬁ ne the surface details.\n10.3. Advanced Lighting and Global Illumination\n",
      "content_length": 2057,
      "extraction_method": "Direct"
    },
    {
      "page_number": 494,
      "chapter": null,
      "content": "472 \n10. The Rendering Engine\nlights will have at each texel. This kind of texture is called a specular power map . \nAn example of a gloss map is shown in Figure 10.50.\n10.3.1.4. Environment Mapping\nAn environment map looks like a panoramic photograph of the environment \ntaken from the point of view of an object in the scene, covering a full 360 \ndegrees horizontally and either 180 degrees or 360 degrees vertically. An envi-\nronment map acts like a description of the general lighting environment sur-\nrounding an object. It is generally used to inexpensively render reﬂ ections.\nThe two most common formats are spherical environment maps and cubic \nenvironment maps . A spherical map looks like a photograph taken through a \nﬁ sheye lens, and it is treated as though it were mapped onto the inside of a \nsphere whose radius is inﬁ nite, centered about the object being rendered. The \nproblem with sphere maps is that they are addressed using spherical coordi-\nnates. Around the equator, there is plenty of resolution both horizontally and \nvertically. However, as the vertical (azimuthal) angle approaches vertical, the \nresolution of the texture along the horizontal (zenith) axis decreases to a single \ntexel. Cube maps were devised to avoid this problem.\nA cube map looks like a composite photograph pieced together from pho-\ntos taken in the six primary directions (up, down, left , right, front, and back). \nDuring rendering, a cube map is treated as though it were mapped onto the \nsix inner surfaces of a box at inﬁ nity, centered on the object being rendered.\nTo read the environment map texel corresponding to a point P on the \nsurface of an object, we take the ray from the camera to the point P and reﬂ ect \nFigure 10.50.  This screen shot from EA’s Fight Night Round 3 shows how a gloss map can \nbe used to control the degree of specular reﬂ ection that should be applied to each texel of \na surface.\n",
      "content_length": 1918,
      "extraction_method": "Direct"
    },
    {
      "page_number": 495,
      "chapter": null,
      "content": "473 \nit about the surface normal at P. The reﬂ ected ray is followed until it intersects \nthe sphere or cube of the environment map. The value of the texel at this inter-\nsection point is used when shading the point P.\n10.3.1.5. Three-Dimensional Textures\nModern graphics harware also includes support for three-dimensional tex-\ntures. A 3D texture can be thought of as a stack of 2D textures. The GPU knows \nhow to address and ﬁ lter a 3D texture, given a three-dimensional texture co-\nordinate (u, v, w).\nThree-dimensional textures can be useful for describing the appearance \nor volumetric properties of an object. For example, we could render a marble \nsphere and allow it to be cut by an arbitrary plane. The texture would look \ncontinuous and correct across the cut no matt er where it was made, because \nthe texture is well-deﬁ ned and continuous throughout the entire volume of \nthe sphere.\n10.3.2. High Dynamic Range Lighting\nA display device like a television set or CRT monitor can only produce a lim-\nited range of intensities. This is why the color channels in the frame buﬀ er are \nlimited to a zero to one range. But in the real world, light intensities can grow \narbitrarily large. High dynamic range (HDR) lighting att empts to capture this \nwide range of light intensities.\nHDR lighting performs lighting calculations without clamping the result-\ning intensities arbitrarily. The resulting image is stored in a format that per-\nmits intensities to grow beyond one. The net eﬀ ect is an image in which ex-\ntreme dark and light regions can be represented without loss of detail within \neither type of region.\nPrior to display on-screen, a process called tone mapping is used to shift  \nand scale the image’s intensity range into the range supported by the display \ndevice. Doing this permits the rendering engine to reproduce many real-world \nvisual eﬀ ects, like the temporary blindness that occurs when you walk from \na dark room into a brightly lit area, or the way light seems to bleed out from \nbehind a brightly back-lit object (an eﬀ ect known as bloom ).\nOne way to represent an HDR image is to store the R, G, and B chan-\nnels using 32-bit ﬂ oating point numbers, instead of 8-bit integers. Another \nalternative is to employ an entirely diﬀ erent color model altogether. The log-\nLUV color model is a popular choice for HDR lighting. In this model, color \nis represented as an intensity channel (L) and two chromaticity channels \n(U and V). Because the human eye is more sensitive to changes in intensity \n10.3. Advanced Lighting and Global Illumination\n",
      "content_length": 2579,
      "extraction_method": "Direct"
    },
    {
      "page_number": 496,
      "chapter": null,
      "content": "474 \n10. The Rendering Engine\nthan it is to changes in chromaticity, the L channel is stored in 16 bits while \nU and V are given only eight bits each. In addition, L is represented using a \nlogarithmic scale (base two) in order to capture a very wide range of light \nintensities.\n10.3.3. Global Illumination\nAs we noted in Section 10.1.3.1, global illumination refers to a class of light-\ning algorithms that account for light’s interactions with multiple objects in the \nscene, on its way from the light source to the virtual camera . Global illumina-\ntion accounts for eﬀ ects like the shadows that arise when one surface occludes \nanother, reﬂ ections, caustics, and the way the color of one object can “bleed” \nonto the objects around it. In the following sections, we’ll take a brief look \nat some of the most common global illumination techniques. Some of these \nmethods aim to reproduce a single isolated eﬀ ect, like shadows or reﬂ ections. \nOthers like radiosity and ray tracing methods aim to provide a holistic model \nof global light transport.\n10.3.3.1. Shadow Rendering\nShadows are created when a surface blocks light’s path. The shadows caused \nby an ideal point light source would be sharp, but in the real world shadows \nhave blurry edges; this is called the penumbra . A penumbra arises because \nreal-world light sources cover some area and so produce light rays that graze \nthe edges of an object at diﬀ erent angles.\nThe two most prevalent shadow rendering techniques are shadow vol-\numes and shadow maps. We’ll brieﬂ y describe each in the sections below. In \nboth techniques, objects in the scene are generally divided into three catego-\nries: objects that cast shadows, objects that are to receive shadows, and ob-\njects that are entirely excluded from consideration when rendering shadows. \nLikewise, the lights are tagged to indicate whether or not they should gener-\nate shadows. This important optimization limits the number of light-object \ncombinations that need to be processed in order to produce the shadows in \na scene.\nShadow Volumes\n In the shadow volume technique, each shadow caster is viewed from the \nvantage point of a shadow-generating light source, and the shadow caster’s \nsilhouett e edges are identiﬁ ed. These edges are extruded in the direction of \nthe light rays emanating from the light source. The result is a new piece of \ngeometry that describes the volume of space in which the light is occluded by \nthe shadow caster in question. This is shown in Figure 10.51.\n",
      "content_length": 2514,
      "extraction_method": "Direct"
    },
    {
      "page_number": 497,
      "chapter": null,
      "content": "475 \nA shadow volume is used to generate a shadow by making use of a special \nfull-screen buﬀ er known as the stencil buﬀ er . This buﬀ er stores a single inte-\nger value corresponding to each pixel of the screen. Rendering can be masked \nby the values in the stencil buﬀ er—for example, we could conﬁ gure the GPU \nto only render fragments whose corresponding stencil values are non-zero. In \naddition, the GPU can be conﬁ gured so that rendered geometry updates the \nvalues in the stencil buﬀ er in various useful ways.\nTo render shadows, the scene is ﬁ rst drawn to generate an unshadowed \nimage in the frame buﬀ er, along with an accurate z-buﬀ er . The stencil buﬀ er \nis cleared so that it contains zeros at every pixel. Each shadow volume is then \nrendered from the point of view of the camera in such a way that front-facing \ntriangles increase the values in the stencil buﬀ er by one, while back-facing \ntriangles decrease them by one. In areas of the screen where the shadow vol-\nume does not appear at all, of course the stencil buﬀ er’s pixels will be left  \ncontaining zero. The stencil buﬀ er will also contain zeros where both the front \nand back faces of the shadow volume are visible, because the front face will \nincrease the stencil value but the back face will decrease it again. In areas \nwhere the back face of the shadow volume has been occluded by “real” scene \ngeometry, the stencil value will be one. This tells us which pixels of the screen \nare in shadow. So we can render shadows in a third pass, by simply darkening \nthose regions of the screen that contain a non-zero stencil buﬀ er value.\nShadow Maps\nThe shadow mapping technique is eﬀ ectively a per-fragment depth test per-\nformed from the point of view of the light instead of from the point of view \nof the camera. The scene is rendered in two steps: First, a shadow map texture \nFigure 10.51.  A shadow volume generated by extruding the silhouette edges of a shadow \ncasting object as seen from the point of view of the light source.\n10.3. Advanced Lighting and Global Illumination\n",
      "content_length": 2069,
      "extraction_method": "Direct"
    },
    {
      "page_number": 498,
      "chapter": null,
      "content": "476 \n10. The Rendering Engine\nis generated by rendering the scene from the point of view of the light source \nand saving oﬀ  the contents of the depth buﬀ er. Second, the scene is rendered \nas usual, and the shadow map is used to determine whether or not each frag-\nment is in shadow. At each fragment in the scene, the shadow map tells us \nwhether or not the light is being occluded by some geometry that is closer to \nthe light source, in just the same way that the z-buﬀ er tells us whether a frag-\nment is being occluded by a triangle that is closer to the camera.\nA shadow map contains only depth information—each texel records how \nfar away it is from the light source. Shadow maps are therefore typically ren-\ndered using the hardware’s double-speed z-only mode (since all we care about \nis the depth information). For a point light source, a perspective projection is \nused when rendering the shadow map; for a directional light source, an ortho-\ngraphic projection is used instead.\nTo render a scene using a shadow map, we draw the scene as usual from \nthe point of view of the camera. For each vertex of every triangle, we calculate \nits position in light space —i.e., in the same “view space” that was used when \ngenerating the shadow map in the ﬁ rst place. These light space coordinates \ncan be interpolated across the triangle, just like any other vertex att ribute. This \ngives us the position of each fragment in light space. To determine whether a \ngiven fragment is in shadow or not, we convert the fragment’s light-space (x, \ny)-coordinates into texture coordinates (u, v) within the shadow map. We then \ncompare the fragment’s light-space z-coordinate with the depth stored at the \ncorresponding texel in the shadow depth map. If the fragment’s light-space z \nis farther away from the light than the texel in the shadow map, then it must be \noccluded by some other piece of geometry that is closer to the light source—\nhence it is in shadow. Likewise, if the fragment’s light-space z is closer to the \nlight source than the texel in the shadow map, then it is not occluded and is \nnot in shadow. Based on this information, the fragment’s color can be adjusted \naccordingly. The shadow mapping process is illustrated in Figure 10.52.\nFigure 10.52.  The far left image is a shadow map—the contents of the z-buffer as rendered \nfrom the point of view of a particular light source. The pixels of the center image are black \nwhere the light-space depth test failed (fragment in shadow) and white where it succeeded \n(fragment not in shadow). The far right image shows the ﬁ nal scene rendered with shadows.\n",
      "content_length": 2622,
      "extraction_method": "Direct"
    },
    {
      "page_number": 499,
      "chapter": null,
      "content": "477 \n10.3.3.2. Ambient Occlusion\nAmbient occlusion is a technique for modeling contact shadows —the soft  shad-\nows that arise when a scene is illuminated by only ambient light. In eﬀ ect, am-\nbient occlusion describes how “accessible” each point on a surface is to light \nin general. For example, the interior of a section of pipe is less accessible to \nambient light than its exterior. If the pipe were placed outside on an overcast \nday, its interior would generally appear darker than its exterior.\nFigure 10.53 shows the level of ambient occlusion across an object’s sur-\nface. Ambient occlusion is measured at a point on a surface by constructing \na hemisphere with a very large radius centered on that point and determing \nwhat percentage of that hemisphere’s area is visible from the point in ques-\ntion. It can be precomputed oﬄ  ine for static objects, because ambient occlu-\nsion is independent of view direction and the direction of incident light. It is \ntypically stored in a texture map that records the level of ambient occlusion at \neach texel across the surface.\n10.3.3.3. Reﬂ ections\nReﬂ ections occur when light bounces oﬀ  a highly specular (shiny) surface pro-\nducing an image of another portion of the scene in the surface. Reﬂ ections \ncan be implemented in a number of ways. Environment maps are used to \nFigure 10.53.  A dragon \nrendered with ambient \nocclusion.\nFigure 10.54.  Mirror reﬂ ections in Luigi’s Mansion implemented by rendering the scene to a \ntexture that is subsequently applied to the mirror’s surface.\n10.3. Advanced Lighting and Global Illumination\n",
      "content_length": 1593,
      "extraction_method": "Direct"
    },
    {
      "page_number": 500,
      "chapter": null,
      "content": "478 \n10. The Rendering Engine\nproduce general reﬂ ections of the surrounding environment on the surfaces \nof shiny objects. Direct reﬂ ections in ﬂ at surfaces like mirrors can be produced \nby reﬂ ecting the camera’s position about the plane of the reﬂ ective surface and \nthen rendering the scene from that reﬂ ected point of view into a texture . The \ntexture is then applied to the reﬂ ective surface in a second pass.\n10.3.3.4. Caustics\nCaustics are the bright specular highlights arising from intense reﬂ ections or \nrefractions from very shiny surfaces like water or polished metal. When the \nreﬂ ective surface moves, as is the case for water, the caustic eﬀ ects glimmer \nand “swim” across the surfaces on which they fall. Caustic eﬀ ects can be pro-\nduced by projecting a (possibly animated) texture containing semi-random \nbright highlights onto the aﬀ ected surfaces. An example of this technique is \nshown in Figure 10.55.\nFigure 10.55.  Water caustics produced by projecting an animated texture onto the affected \nsurfaces.\n10.3.3.5. Subsurface Scattering\nWhen light enters a surface at one point, is scatt ered beneath the surface, \nand then reemerges at a diﬀ erent point on the surface, we call this subsurface \nscatt ering . This phenomenon is responsible for the “warm glow” of human \nskin, wax, and marble statues. Subsurface scatt ering is described by a more-\nadvanced variant of the BRDF (see Section 10.1.3.2) known as the BSSRDF \n(bidirectional surface scatt ering reﬂ ectance distribution function).\nSubsurface scatt ering can be simulated in a number of ways. Depth-map–\nbased subsurface scatt ering renders a shadow map (see Section 10.3.3.1), but \ninstead of using it to determine which pixels are in shadow, it is used to mea-\nsure how far a beam of light would have to travel in order to pass all the way \n",
      "content_length": 1836,
      "extraction_method": "Direct"
    },
    {
      "page_number": 501,
      "chapter": null,
      "content": "479 \nthrough the occluding object. The shadowed side of the object is then given \nan artiﬁ cial diﬀ use lighting term whose intensity is inversely proportional to \nthe distance the light had to travel in order to emerge on the opposite side of \nthe object. This causes objects to appear to be glowing slightly on the side op-\nposite to the light source but only where the object is relatively thin. For more \ninformation on subsurface scatt ering techniques, see htt p://htt p.developer.\nnvidia.com/GPUGems/gpugems_ch16.html.\n10.3.3.6. Precomputed Radiance Transfer (PRT)\nPrecomputed radiance transfer (PRT) is a relatively new technique that att empts \nto simulate the eﬀ ects of radiosity-based rendering methods in real time. It \ndoes so by precomputing and storing a complete description of how an inci-\ndent light ray would interact with a surface (reﬂ ect, refract, scatt er, etc.) when \napproaching from every possible direction. At runtime, the response to a par-\nticular incident light ray can be looked up and quickly converted into very \naccurate lighting results.\nIn general the light’s response at a point on the surface is a complex func-\ntion deﬁ ned on a hemisphere centered about the point. A compact repre-\nsentation of this function is required to make the PRT technique practical. A \ncommon approach is to approximate the function as a linear combination of \nspherical harmonic basis functions. This is essentially the three-dimensional \nequivalent of encoding a simple scalar function f(x) as a linear combination of \nshift ed and scaled sine waves.\nThe details of PRT are far beyond our scope. For more information, see \nhtt p://web4.cs.ucl.ac.uk/staﬀ /j.kautz/publications/prtSIG02.pdf. PRT lighting \nFigure 10.56.  On the left, a dragon rendered without subsurface scattering (i.e., using a BRDF \nlighting model). On the right, the same dragon rendered with subsurface scattering (i.e., using \na BSSRDF model). Images rendered by Rui Wang at the University of Virginia.\n10.3. Advanced Lighting and Global Illumination\n",
      "content_length": 2042,
      "extraction_method": "Direct"
    },
    {
      "page_number": 502,
      "chapter": null,
      "content": "480 \n10. The Rendering Engine\ntechniques are demonstrated in a DirectX sample program available in the \nDirectX SDK—see htt p://msdn.microsoft .com/en-us/library/bb147287.aspx \nfor more details.\n10.3.4. Deferred Rendering\nIn traditional triangle-rasterization–based rendering, all lighting and shad-\ning calculations are performed on the triangle fragments in view space. The \nproblem with this technique is that it is inherently ineﬃ  cient. For one thing, \nwe potentially do work that we don’t need to do. We shade the vertices of tri-\nangles, only to discover during the rasterization stage that the entire triangle \nis being depth-culled by the z test. Early z tests help eliminate unnecessary \npixel shader evaluations, but even this isn’t perfect. What’s more, in order to \nhandle a complex scene with lots of lights, we end up with a proliferation of \ndiﬀ erent versions of our vertex and pixel shaders—versions that handle dif-\nFigure 10.57.  Screenshots from Killzone 2, showing some of the typical components of the \nG-buffer used in deferred rendering. The upper image shows the ﬁ nal rendered image. Below \nit, clockwise from the upper left, are the albedo (diffuse) color, depth, view-space normal, \nscreen space 2D motion vector (for motion blurring), specular power, and specular intensity.\n",
      "content_length": 1306,
      "extraction_method": "Direct"
    },
    {
      "page_number": 503,
      "chapter": null,
      "content": "481 \nferent numbers of lights, diﬀ erent types of lights, diﬀ erent numbers of skin-\nning weights, etc.\nDeferred rendering is an alternative way to shade a scene that addresses \nmany of these problems. In deferred rendering, the majority of the lighting \ncalculations are done in screen space, not view space. We eﬃ  ciently render \nthe scene without worrying about lighting. During this phase, we store all \nthe information we’re going to need to light the pixels in a “deep” frame buf-\nfer known as the G-buﬀ er . Once the scene has been fully rendered, we use \nthe information in the G-buﬀ er to perform our lighting and shading calcula-\ntions. This is usually much more eﬃ  cient than view-space lighting, avoids the \nproliferation of shader variants, and permits some very pleasing eﬀ ects to be \nrendered relatively easily.\nThe G-buﬀ er may be physically implemented as a collection of buﬀ ers, \nbut conceptually it is a single frame buﬀ er containing a rich set of informa-\ntion about the lighting and surface properties of the objects in the scene at \nevery pixel on the screen. A typical G-buﬀ er might contain the following per-\npixel att ributes: depth, surface normal in clip space, diﬀ use color, specular \npower, even precomputed radiance transfer (PRT) coeﬃ  cients. The following \nsequence of screen shots from Guerrilla Games’ Killzone 2 shows some of the \ntypical components of the G-buﬀ er.\nAn in-depth discussion of deferred rendering is beyond our scope, but \nthe folks at Guerrilla Games have prepared an excellent presentation on the \ntopic, which is available at htt p://www.guerrilla-games.com/publications/\ndr_kz2_rsx_dev07.pdf.\n10.4. Visual Effects and Overlays\nThe rendering pipeline we’ve discussed to this point is responsible primarily \nfor rendering three-dimensional solid objects. A number of specialized render-\ning systems are typically layered on top of this pipeline, responsible for ren-\ndering visual elements like particle eﬀ ects, decals (small geometry overlays \nthat represent bullet holes, cracks, scratches, and other surface details), hair \nand fur, rain or falling snow, water , and other specialized visual eﬀ ects. Full-\nscreen post eﬀ ects may be applied, including vignett e (slight blur around the \nedges of the screen), motion blur, depth of ﬁ eld blurring, artiﬁ cial/enhanced \ncolorization, and the list goes on. Finally, the game’s menu system and heads-\nup display (HUD) are typically realized by rendering text and other two- or \nthree-dimensional graphics in screen space overlaid on top of the three-\ndimensional scene.\n10.4. Visual Effects and Overlays\n",
      "content_length": 2615,
      "extraction_method": "Direct"
    },
    {
      "page_number": 504,
      "chapter": null,
      "content": "482 \n10. The Rendering Engine\nAn in-depth coverage of these engine systems is beyond our scope. In the \nfollowing sections, we’ll provide a brief overview of these rendering systems, \nand point you in the direction of additional information.\n10.4.1. Particle Effects\nA particle rendering system is concerned with rendering amorphous objects \nlike clouds of smoke, sparks, ﬂ ame, and so on. These are called particle eﬀ ects. \nThe key features that diﬀ erentiate a particle eﬀ ect from other kinds of render-\nable geometry are as follows:\nz It is composed of a very large number of relatively simple pieces of geom-\netry—most oft en simple cards called quads, composed of two triangles \neach.\nz The geometry is oft en camera-facing (i.e., billboarded ), meaning that the \nengine must take steps to ensure that the face normals of each quad \nalways point directly at the camera’s focal point.\nz Its materials are almost always semi-transparent or translucent. As such, \nparticle eﬀ ects have some stringent rendering order constraints that do \nnot apply to the majority of opaque objects in a scene.\nz Particles animate in a rich variety of ways. Their positions, orientations, \nsizes (scales), texture coordinates, and many of their shader parameters \nvary from frame to frame. These changes are deﬁ ned either by hand-\nauthored animation curves or via procedural methods.\nz Particles are typically spawned and killed continually. A particle emitt er \nis a logical entity in the world that creates particles at some user-speci-\nﬁ ed rate; particles are killed when they hit a predeﬁ ned death plane, or \nFigure 10.58.  Some particle effects.\n",
      "content_length": 1641,
      "extraction_method": "Direct"
    },
    {
      "page_number": 505,
      "chapter": null,
      "content": "483 \nwhen they have lived for a user-deﬁ ned length of time, or as decided by \nsome other user-speciﬁ ed criteria.\nParticle eﬀ ects could be rendered using regular triangle mesh geometry \nwith appropriate shaders. However, because of the unique characteristics \nlisted above, a specialized particle eﬀ ect animation and rendering system is \nalways used to implement them in a real production game engine. A few ex-\nample particle eﬀ ects are shown in Figure 10.58.\nParticle system design and implementation is a rich topic that could oc-\ncupy many chapters all on its own. For more information on particle systems, \nsee [1] Section 10.7, [14] Section 20.5, [9] Section 13.7 and [10] Section 4.1.2.\n10.4.2. Decals\nA decal is a relatively small piece of geometry that is overlaid on top of the reg-\nular geometry in the scene, allowing the visual appearance of the surface to be \nmodiﬁ ed dynamically. Examples include bullet holes, foot prints, scratches, \ncracks, etc.\nThe approach most oft en used by modern engines is to model a decal as \na rectangular area that is to be projected along a ray into the scene. This gives \nrise to a rectangular prism in 3D space. Whatever surface the prism intersects \nﬁ rst becomes the surface of the decal. The triangles of the intersected geom-\netry are extracted and clipped against the four bounding planes of the decal’s \nprojected prism. The resulting triangles are texture-mapped with a desired \ndecal texture by generating appropriate texture coordinates for each vertex. \nThese texture-mapped triangles are then rendered over the top of the regular \nscene, oft en using parallax mapping to give them the illusion of depth and \nwith a slight z-bias (usually implemented by shift ing the near plane slightly) \nso they don’t experience z-ﬁ ghting with the geometry on which they are over-\nFigure 10.59.  Parallax-mapped decals from Uncharted: Drake’s Fortune.\n10.4. Visual Effects and Overlays\n",
      "content_length": 1936,
      "extraction_method": "Direct"
    },
    {
      "page_number": 506,
      "chapter": null,
      "content": "484 \n10. The Rendering Engine\nlaid. The result is the appearance of a bullet hole, scratch or other kind of sur-\nface modiﬁ cation. Some bullet-hole decals are depicted in Figure 10.59.\nFor more information on creating and rendering decals, see [7] Section \n4.8, and [28] Section 9.2.\n10.4.3. Environmental Effects\nAny game that takes place in a somewhat natural or realistic environment \nrequires some kind of environmental rendering eﬀ ects. These eﬀ ects are usu-\nally implemented via specialized rendering systems. We’ll take a brief look at \na few of the more common of these systems in the following sections.\n10.4.3.1. Skies\nThe sky in a game world needs to contain vivid detail, yet technically speak-\ning it lies an extremely long distance away from the camera. Therefore we \ncannot model it as it really is and must turn instead to various specialized \nrendering techniques.\nOne simple approach is to ﬁ ll the frame buﬀ er with the sky texture prior \nto rendering any 3D geometry. The sky texture should be rendered at an ap-\nproximate 1:1 texel-to-pixel ratio, so that the texture is roughly or exactly the \nresolution of the screen. The sky texture can be rotated and scrolled to corre-\nspond to the motions of the camera in-game. During rendering of the sky, we \nmake sure to set the depth of all pixels in the frame buﬀ er to the maximum \npossible depth value. This ensures that the 3D scene elements will always sort \non top of the sky. The arcade hit Hydro Thunder rendered its skies in exactly \nthis manner.\nFor games in which the player can look in any direction, we can use a sky \ndome or sky box . The dome or box is rendered with its center always at the cam-\nera’s current location, so that it appears to lie at inﬁ nity, no matt er where the \ncamera moves in the game world. As with the sky texture approach, the sky \nbox or dome is rendered before any other 3D geometry, and all of the pixels \nin the frame buﬀ er are set to the maximum z-value when the sky is rendered. \nThis means that the dome or box can actually be tiny, relative to other objects \nin the scene. Its size is irrelevant, as long as it ﬁ lls the entire frame buﬀ er when \nit is drawn. For more information on sky rendering, see [1] Section 10.3 and \n[38] page 253.\nClouds are oft en implemented with a specialized rendering and anima-\ntion system as well. In early games like Doom and Quake, the clouds were just \nplanes with scrolling semi-transparent cloud textures on them. More-recent \ncloud techniques include camera-facing cards (billboards), particle-eﬀ ect \nbased clouds, and volumetric cloud eﬀ ects.\n",
      "content_length": 2603,
      "extraction_method": "Direct"
    },
    {
      "page_number": 507,
      "chapter": null,
      "content": "485 \n10.4.3.2. Terrain\nThe goal of a terrain system is to model the surface of the earth and provide \na canvas of sorts upon which other static and dynamic elements can be laid \nout. Terrain is sometimes modeled explicitly in a package like Maya. But if the \nplayer can see far into the distance, we usually want some kind of dynamic \ntessellation or other level of detail (LOD) system. We may also need to limit \nthe amount of data required to represent very large outdoor areas.\nHeight ﬁ eld terrain is one popular choice for modeling large terrain areas. \nThe data size can be kept relatively small because a height ﬁ eld is typically \nstored in a grayscale texture map. In most height-ﬁ eld– based terrain systems, \nthe horizontal (y = 0) plane is tessellated in a regular grid patt ern, and the \nheights of the terrain vertices are determined by sampling the height ﬁ eld \ntexture. The number of triangles per unit area can be varied based on distance \nfrom the camera, thereby allowing large-scale features to be seen in the dis-\ntance, while still permitt ing a good deal of detail to be represented for nearby \nterrain. An example of a terrain deﬁ ned via a height ﬁ eld bitmap is shown in \nFigure 10.60.\nTerrain systems usually provide specialized tools for “painting” the height \nﬁ eld itself, carving out terrain features like roads, rivers, and so on. Texture \nmapping in a terrain system is oft en a blend between four or more textures. \nThis allows artists to “paint” in grass, dirt, gravel, and other terrain features \nby simply exposing one of the texture layers. The layers can be cross-blended \nfrom one to another to provide smooth textural transitions. Some terrain tools \nalso permit sections of the terrain to be cut out to permit buildings, trenches, \nand other specialized terrain features to be inserted in the form of regular \nmesh geometry. Terrain authoring tools are sometimes integrated directly into \nthe game world editor , while in other engines they may be stand-alone tools.\nFigure 10.60.  A grayscale height ﬁ eld bitmap (left) can be used to control the vertical posi-\ntions of the vertices in a terrain grid mesh (right). In this example, a water plane intersects the \nterrain mesh to create islands.\n10.4. Visual Effects and Overlays\n",
      "content_length": 2273,
      "extraction_method": "Direct"
    },
    {
      "page_number": 508,
      "chapter": null,
      "content": "486 \n10. The Rendering Engine\nOf course, height ﬁ eld terrain is just one of many options for modeling the \nsurface of the Earth in a game. For more information on terrain rendering, see \n[6] Sections 4.16 through 4.19 and [7] Section 4.2.\n10.4.3.3. Water\nWater renderers are commonplace in games nowadays. There are lots of dif-\nferent possible kinds of water, including oceans, pools, rivers, waterfalls, foun-\ntains, jets, puddles, and damp solid surfaces. Each type of water generally \nrequires some specialized rendering technology. Some also require dynamic \nmotion simulations . Large bodies of water may require dynamic tessellation \nor other LOD methodologies similar to those employed in a terrain system.\nWater systems sometimes interact with a game’s rigid body dynamics \nsystem (ﬂ otation, force from water jets, etc.) and with gameplay (slippery sur-\nfaces, swimming mechanics, diving mechanics, riding vertical jets of water, \nand so on). Water eﬀ ects are oft en created by combining disparate render-\ning technologies and subsystems. For example, a waterfall might make use \nof specialized water shaders, scrolling textures, particle eﬀ ects for mist at the \nbase, a decal-like overlay for foam, and the list goes on. Today’s games oﬀ er \nsome prett y amazing water eﬀ ects, and active research into technologies like \nreal-time ﬂ uid dynamics promises to make water simulations even richer and \nmore realistic in the years ahead. For more information on water rendering \nand simulation techniques, see [1] Sections 9.3, 9.5, and 9.6, [13], and [6] Sec-\ntions 2.6 and 5.11.\n10.4.4. Overlays\nMost games have heads-up displays, in-game graphical user interfaces, and \nmenu systems. These overlays are typically comprised of two- and three-di-\nmensional graphics rendered directly in view space or screen space .\nOverlays are generally rendered aft er the primary scene, with z testing \ndisabled to ensure that they appear on top of the three-dimensional scene. \nTwo-dimensional overlays are typically implemented by rendering quads (tri-\nangle pairs) in screen space using an orthographic projection. Three-dimen-\nsional overlays may be rendered using an orthographic projection or via the \nregular perspective projection with the geometry positioned in view space so \nthat it follows the camera around.\n10.4.4.1. Normalized Screen Coordinates\nThe coordinates of two-dimensional overlays can be measured in terms of \nscreen pixels. However, if your game is going to be expected to support mul-\ntiple screen resolutions (which is very common in PC games), it’s a far bett er \n",
      "content_length": 2590,
      "extraction_method": "Direct"
    },
    {
      "page_number": 509,
      "chapter": null,
      "content": "487 \nidea to use normalized screen coordinates . Normalized coordinates range from \nzero to one along one of the two axes (but not both—see below), and they can \neasily be scaled into pixel-based measurements corresponding to an arbitrary \nscreen resolution. This allows us to lay out our overlay elements without wor-\nrying about screen resolution at all (and only having to worry a litt le bit about \naspect ratio).\nIt’s easiest to deﬁ ne normalized coordinates so that they range from 0.0 \nto 1.0 along the y-axis. At a 4:3 aspect ratio, this means that the x-axis would \nrange from 0.0 to 1.333 (= 4 / 3), while at 16:9 the x-axis’ range would be from \n0.0 to 1.777 (= 16 / 9). It’s important not to deﬁ ne our coordinates so that they \nrange from zero to one along both axes. Doing this would cause square visual \nelements to have unequal x and y dimensions—or put another way, a visual \nelement with seemingly square dimensions would not look like a square on-\nscreen! Moreover, our “square” elements would stretch diﬀ erently at diﬀ erent \naspect ratios—deﬁ nitely not an acceptable state of aﬀ airs.\n10.4.4.2. Relative Screen Coordinates\nTo really make normalized coordinates work well, it should be possible to \nspecify coordinates in absolute or relative terms. For example, positive co-\nordinates might be interpreted as being relative to the top-left  corner of the \nscreen, while negative coordinates are relative to the bott om-right corner. That \nway, if I want a HUD element to be a certain distance from the right or bott om \nedges of the screen, I won’t have to change its normalized coordinates when \nthe aspect ratio changes. We might want to allow an even richer set of possible \nalignment choices, such as aligning to the center of the screen or aligning to \nanother visual element.\nThat said, you’ll probably have some overlay elements that simply cannot \nbe laid out using normalized coordinates in such a way that they look right at \nboth the 4:3 and 16:9 aspect ratios. You may want to consider having two distinct \nlayouts, one for each aspect ratio, so you can ﬁ ne-tune them independently.\n10.4.4.3. Text and Fonts\nA game engine’s text /font system is typically implemented as a special kind of \ntwo-dimensional (or sometimes three-dimensional) overlay. At its core, a text \nrendering system needs to be capable of displaying a sequence of character \nglyphs corresponding to a text string, arranged in various orientations on the \nscreen. A font is oft en implemented via a texture map containing the vari-\nous required glyphs. A font description ﬁ le provides information such as the \nbounding boxes of each glyph within the texture, and font layout information \nsuch as kerning, baseline oﬀ sets, and so on.\n10.4. Visual Effects and Overlays\n",
      "content_length": 2773,
      "extraction_method": "Direct"
    },
    {
      "page_number": 510,
      "chapter": null,
      "content": "488 \n10. The Rendering Engine\nA good text/font system must account for the diﬀ erences in character sets \nand reading directions inherent in various languages. Some text systems also \nprovide various fun features like the ability to animate characters across the \nscreen in various ways, the ability to animate individual characters, and so \non. Some game engines even go so far as to implement a subset of the Adobe \nFlash standard in order to support a rich set of two-dimensional eﬀ ects in \ntheir overlays and text. However, it’s important to remember when imple-\nmenting a game font system that only those features that are actually required \nby the game should be implemented. There’s no point in furnishing your en-\ngine with an advanced text animation if your game never needs to display \nanimated text!\n10.4.5. Gamma Correction\nCRT monitors tend to have a nonlinear response to luminance values. That is, \nif a linearly-increasing ramp of R, G, or B values were to be sent to a CRT, the \nimage that would result on-screen would be perceptually nonlinear to the hu-\nman eye. Visually, the dark regions of the image would look darker than they \nshould. This is illustrated in Figure 10.61.\nThe gamma response curve of a typical CRT display can be modeled quite \nsimply by the formula\nout\nin,\nV\nV γ\n=\n \nwhere γCRT > 1. To correct for this eﬀ ect, the colors sent to the CRT display \nare usually passed through an inverse transformation (i.e., using a gamma \nvalue γcorr < 1). The value of γCRT for a typical CRT monitor is 2.2, so the correc-\nFigure 10.61.  The effect of a CRT’s gamma response on image quality and how the effect can \nbe corrected for. Image courtesy of www.wikipedia.org.\n",
      "content_length": 1697,
      "extraction_method": "Direct"
    },
    {
      "page_number": 511,
      "chapter": null,
      "content": "489 \ntion value is usually γcorr = 1/2.2 = 0.455. These gamma encoding and decoding \ncurves are shown in Figure 10.62.\nGamma encoding can be performed by the 3D rendering engine to ensure \nthat the values in the ﬁ nal image are properly gamma-corrected. One problem \nthat is encountered, however, is that the bitmap images used to represent tex-\nture maps are oft en gamma-corrected themselves. A high-quality rendering \nengine takes this fact into account, by gamma-decoding the textures prior to \nrendering and then re-encoding the gamma of the ﬁ nal rendered scene so that \nits colors can be reproduced properly on-screen.\n10.4.6. Full-Screen Post Effects\nFull-screen post eﬀ ects are eﬀ ects applied to a rendered three-dimensional \nscene that provide additional realism or a stylized look. These eﬀ ects are of-\nten implemented by passing the entire contents of the screen through a pixel \nshader that applies the desired eﬀ ect(s). This can be accomplished by render-\ning a full-screen quad that has been mapped with a texture containing the \nunﬁ ltered scene. A few examples of full-screen post eﬀ ects are given below:\nz Motion blur . This is typically implemented by rendering a buﬀ er of \nscreen-space velocity vectors and using this vector ﬁ eld to selectively \nblur the rendered image. Blurring is accomplished by passing a con-\nvolution kernel over the image (see “Image Smoothing and Sharpening \nby Discrete Convolution” by Dale A. Schumacher, published in [4], for \ndetails).\nFigure 10.62.  Gamma encoding and decoding curves. Image courtesy of www.wikipedia.org.\n10.4. Visual Effects and Overlays\n",
      "content_length": 1613,
      "extraction_method": "Direct"
    },
    {
      "page_number": 512,
      "chapter": null,
      "content": "490 \n10. The Rendering Engine\nz Depth of ﬁ eld blur . This blur eﬀ ect can be produced by using the contents \nof the depth buﬀ er to adjust the degree of blur applied at each pixel.\nz Vignett e . In this ﬁ lmic eﬀ ect, the brightness or saturation of the image \nis reduced at the corners of the screen for dramatic eﬀ ect. It is some-\ntimes implemented by literally rendering a texture overlay on top of the \nscreen. A variation on this eﬀ ect is used to produce the classic circular \neﬀ ect used to indicate that the player is looking through a pair of bin-\noculars or a weapon scope.\nz Colorization . The colors of screen pixels can be altered in arbitrary ways \nas a post-processing eﬀ ect. For example, all colors except red could be \ndesaturated to grey to produce a striking eﬀ ect similar to the famous \nscene of the litt le girl in the red coat from Schindler’s List.\n10.5. Further Reading\nWe’ve covered a lot of material in a very short space in this chapter, but we’ve \nonly just scratched the surface. No doubt you’ll want to explore many of these \ntopics in much greater detail. For an excellent overview of the entire process of \ncreating three-dimensional computer graphics and animation for games and \nﬁ lm, I highly recommend [23]. The technology that underlies modern real-\ntime rendering is covered in excellent depth in [1], while [14] is well known as \nthe deﬁ nitive reference guide to all things related to computer graphics. Other \ngreat books on 3D rendering include [42], [9], and [10]. The mathematics of \n3D rendering is covered very well in [28]. No graphics programmer’s library \nwould be complete without one or more books from the Graphics Gems series \n([18], [4], [24], [19], and [36]) and/or the GPU Gems series ([13], [38], and [35]). \nOf course, this short reference list is only the beginning—you will undoubt-\nedly encounter a great many more excellent books on rendering and shaders \nover the course of your career as a game programmer.\n",
      "content_length": 1975,
      "extraction_method": "Direct"
    },
    {
      "page_number": 513,
      "chapter": null,
      "content": "491\n11\nAnimation Systems\nT\nhe majority of modern 3D games revolve around characters —oft en human \nor humanoid, sometimes animal or alien. Characters are unique because \nthey need to move in a ﬂ uid, organic way. This poses a host of new technical \nchallenges, over and above what is required to simulate and animate rigid \nobjects like vehicles, projectiles, soccer balls, and Tetris pieces. The task of im-\nbuing characters with natural-looking motion is handled by an engine compo-\nnent known as the character animation system.\nAs we’ll see, an animation system gives game designers a powerful suite \nof tools that can be applied to non-characters as well as characters. Any game \nobject that is not 100% rigid can take advantage of the animation system. So \nwhenever you see a vehicle with moving parts, a piece of articulated machin-\nery, trees waving gently in the breeze, or even an exploding building in a \ngame, chances are good that the object makes at least partial use of the game \nengine’s animation system.\n11.1. Types of Character Animation\nCharacter animation technology has come a long way since Donkey Kong. At \nﬁ rst, games employed very simple techniques to provide the illusion of life-\nlike movement. As game hardware improved, more-advanced techniques be-\n",
      "content_length": 1279,
      "extraction_method": "Direct"
    },
    {
      "page_number": 514,
      "chapter": null,
      "content": "492 \n11. Animation Systems\ncame feasible in real time. Today, game designers have a host of powerful \nanimation methods at their disposal. In this section, we’ll take a brief look \nat the evolution of character animation and outline the three most-common \ntechniques used in modern game engines.\n11.1.1. \nCel Animation\nThe precursor to all game animation techniques is known as traditional anima-\ntion, or hand-drawn animation . This is the technique used in the earliest animat-\ned cartoons. The illusion of motion is produced by displaying a sequence of \nstill pictures known as frames in rapid succession. Real-time 3D rendering can \nbe thought of as an electronic form of traditional animation, in that a sequence \nof still full-screen images is presented to the viewer over and over to produce \nthe illusion of motion.\nCel animation is a speciﬁ c type of traditional animation. A cel is a transpar-\nent sheet of plastic on which images can be painted or drawn. An animated \nsequence of cels can be placed on top of a ﬁ xed background painting or draw-\ning to produce the illusion of motion without having to redraw the static back-\nground over and over.\nThe electronic equivalent to cel animation is a technology known as sprite \nanimation. A sprite is a small bitmap that can be overlaid on top of a full-screen \nbackground image without disrupting it, oft en drawn with the aid of special-\nized graphics hardware. Hence, a sprite is to 2D game animation what a cel \nwas to traditional animation. This technique was a staple during the 2D game \nera. Figure 11.1 shows the famous sequence of sprite bitmaps that were used \nto produce the illusion of a running humanoid character in almost every Mat-\ntel Intellivision game ever made. The sequence of frames was designed so that \nit animates smoothly even when it is repeated indeﬁ nitely—this is known as \na looping animation . This particular animation would be called a run cycle in \nmodern parlance, because it makes the character appear to be running. Char-\nacters typically have a number of looping animation cycles, including various \nidle cycles, a walk cycle, and a run cycle.\nFigure 11.1.  The sequence of sprite bitmaps used in most Intellivision games.\n11.1.2. Rigid Hierarchical Animation\nWith the advent of 3D graphics, sprite techniques began to lose their appeal. \nDoom made use of a sprite-like animation system: Its monsters were nothing \n",
      "content_length": 2411,
      "extraction_method": "Direct"
    },
    {
      "page_number": 515,
      "chapter": null,
      "content": "493 \n11.1. Types of Character Animation\nmore than camera-facing quads, each of which displayed a sequence of texture \nbitmaps (known as an animated texture ) to produce the illusion of motion. And \nthis technique is still used today for low-resolution and/or distant objects—for \nexample crowds in a stadium, or hordes of soldiers ﬁ ghting a distant batt le \nin the background. But for high-quality foreground characters, 3D graphics \nbrought with it the need for improved character animation methods.\nThe earliest approach to 3D character animation is a technique known as \nrigid hierarchical animation. In this approach, a character is modeled as a col-\nlection of rigid pieces. A typical break-down for a humanoid character might \nbe pelvis, torso, upper arms, lower arms, upper legs, lower legs, hands, feet, \nand head. The rigid pieces are constrained to one another in a hierarchical \nfashion, analogous to the manner in which a mammal’s bones are connected \nat the joints. This allows the character to move naturally. For example, when \nthe upper arm is moved, the lower arm and hand will automatically follow it. \nA typical hierarchy has the pelvis at the root, with the torso and upper legs as \nits immediate children, and so on as shown below:\nPelvis\n Torso\n  UpperRightArm\n   LowerRightArm\n    RightHand\n  UpperLeftArm\n   UpperLeftArm\n    LeftHand\n  Head\n UpperRightLeg\n  LowerRightLeg\n   RightFoot\n UpperLeftLeg\n  UpperLeftLeg\n   LeftFoot\nThe big problem with the rigid hierarchy technique is that the behavior of \nthe character’s body is oft en not very pleasing due to “cracking” at the joints. \nThis is illustrated in Figure 11.2. Rigid hierarchical animation works well for \nFigure 11.2.  Cracking at the joints is a big problem in rigid hierarchical animation.\n",
      "content_length": 1778,
      "extraction_method": "Direct"
    },
    {
      "page_number": 516,
      "chapter": null,
      "content": "494 \n11. Animation Systems\nrobots and machinery that really is constructed of rigid parts, but it breaks \ndown under scrutiny when applied to “ﬂ eshy” characters.\n11.1.3. Per-Vertex Animation and Morph Targets\nRigid hierarchical animation tends to look unnatural because it is rigid. What \nwe really want is a way to move individual vertices so that triangles can stretch \nto produce more-natural looking motion.\nOne way to achieve this is to apply a brute-force technique known as \nper-vertex animation. In this approach, the vertices of the mesh are animated \nby an artist, and motion data is exported which tells the game engine how to \nmove each vertex at runtime. This technique can produce any mesh deforma-\ntion imaginable (limited only by the tessellation of the surface). However, it \nis a data-intensive technique, since time-varying motion information must be \nstored for each vertex of the mesh. For this reason, it has litt le application to \nreal-time games.\nA variation on this technique known as morph target animation is used in \nsome real-time games. In this approach, the vertices of a mesh are moved by \nan animator to create a relatively small set of ﬁ xed, extreme poses. Animations \nare produced by blending between two or more of these ﬁ xed poses at runtime. \nThe position of each vertex is calculated using a simple linear interpolation \n(LERP) between the vertex’s positions in each of the extreme poses.\nThe morph target technique is oft en used for facial animation, because \nthe human face is an extremely complex piece of anatomy, driven by roughly \n50 muscles. Morph target animation gives an animator full control over every \nvertex of a facial mesh, allowing him or her to produce both subtle and ex-\ntreme movements that approximate the musculature of the face well. Figure \n11.3 shows a set of facial morph targets.\nFigure 11.3.  A set of facial morph targets for NVIDIA’s Dawn character.\n",
      "content_length": 1925,
      "extraction_method": "Direct"
    },
    {
      "page_number": 517,
      "chapter": null,
      "content": "495 \n11.1.4. Skinned Animation\nAs the capabilities of game hardware improved further, an animation tech-\nnology known as skinned animation was developed. This technique has many \nof the beneﬁ ts of per-vertex and morph target animation—permitt ing the tri-\nangles of an animated mesh to deform. But it also enjoys the much more-\neﬃ  cient performance and memory usage characteristics of rigid hierarchical \nanimation. It is capable of producing reasonably realistic approximations to \nthe movement of skin and clothing.\nSkinned animation was ﬁ rst used by games like Super Mario 64, and it \nis still the most prevalent technique in use today, both by the game industry \nand the feature ﬁ lm industry. A host of famous modern game and movie char-\nacters, including the dinosaurs from Jurrassic Park, Solid Snake (Metal Gear \nSolid 4), Gollum (Lord of the Rings), Nathan Drake (Uncharted: Drake’s Fortune), \nBuzz Lightyear (Toy Story), and Marcus Fenix (Gears of War) were all animated, \nin whole or in part, using skinned animation techniques. The remainder of \nthis chapter will be devoted primarily to the study of skinned/skeletal anima-\ntion.\nIn skinned animation, a skeleton is constructed from rigid “bones ,” just as \nin rigid hierarchical animation. However, instead of rendering the rigid pieces \non-screen, they remain hidden. A smooth continuous triangle mesh called a \nskin is bound to the joints of the skeleton; its vertices track the movements of \nthe joints. Each vertex of the skin mesh can be weighted to multiple joints, so \nthe skin can stretch in a natural way as the joints move.\nFigure 11.4.  Eric Browning’s Crank the Weasel character, with internal skeletal structure.\n11.1. Types of Character Animation\n",
      "content_length": 1728,
      "extraction_method": "Direct"
    },
    {
      "page_number": 518,
      "chapter": null,
      "content": "496 \n11. Animation Systems\nIn Figure 11.4, we see Crank the Weasel, a game character designed by \nEric Browning for Midway Home Entertainment in 2001. Crank’s outer skin \nis composed of a mesh of triangles, just like any other 3D model. However, \ninside him we can see the rigid bones and joints that make his skin move.\n11.1.5. Animation Methods as Data Compression Techniques\nThe most ﬂ exible animation system conceivable would give the animator con-\ntrol over literally every inﬁ nitesimal point on an object’s surface. Of course, \nanimating like this would result in an animation that contains a potentially \ninﬁ nite amount of data! Animating the vertices of a triangle mesh is a simpli-\nﬁ cation of this ideal—in eﬀ ect, we are compressing the amount of information \nneeded to describe an animation by restricting ourselves to moving only the \nvertices. (Animating a set of control points is the analog of vertex animation \nfor models constructed out of higher-order patches.) Morph targets can be \nthought of as an additional level of compression, achieved by imposing addi-\ntional constraints on the system—vertices are constrained to move only along \nlinear paths between a ﬁ xed number of predeﬁ ned vertex positions. Skeletal \nanimation is just another way to compress vertex animation data by imposing \nconstraints. In this case, the motions of a relatively large number of vertices \nare constrained to follow the motions of a relatively small number of skeletal \njoints.\nWhen considering the trade-oﬀ s between various animation techniques, \nit can be helpful to think of them as compression methods, analogous in many \nrespects to video compression techniques. We should generally aim to select \nthe animation method that provides the best compression without producing \nunacceptable visual artifacts. Skeletal animation provides the best compres-\nsion when the motion of a single joint is magniﬁ ed into the motions of many \nvertices. A character’s limbs act like rigid bodies for the most part, so they can \nbe moved very eﬃ  ciently with a skeleton. However, the motion of a face tends \nto be much more complex, with the motions of individual vertices being more \nindependent. To convincingly animate a face using the skeletal approach, the \nrequired number of joints approaches the number of vertices in the mesh, thus \ndiminishing its eﬀ ectiveness as a compression technique. This is one reason \nwhy morph target techniques are oft en favored over the skeletal approach for \nfacial animation. (Another common reason is that morph targets tend to be a \nmore natural way for animators to work.)\n11.2. Skeletons\nA skeleton is comprised of a hierarchy of rigid pieces known as joints . In the \ngame industry, we oft en use the terms “joint” and “bone” interchangeably, \n",
      "content_length": 2787,
      "extraction_method": "Direct"
    },
    {
      "page_number": 519,
      "chapter": null,
      "content": "497 \nbut the term bone is actually a misnomer. Technically speaking, the joints are \nthe objects that are directly manipulated by the animator, while the bones \nare simply the empty spaces between the joints. As an example, consider the \npelvis joint in the Crank the Weasel character model. It is a single joint, but be-\ncause it connects to four other joints (the tail, the spine, and the left  and right \nhip joints), this one joint appears to have four bones sticking out of it. This is \nshown in more detail in Figure 11.5. Game engines don’t care a whip about \nbones—only the joints matt er. So whenever you hear the term “bone” being \nused in the industry, remember that 99% of the time we are actually speaking \nabout joints.\n11.2.1. The Skeleal Hierarchy\nAs we’ve mentioned, the joints in a skeleton form a hierarchy or tree structure. \nOne joint is selected as the root, and all other joints are its children, grandchil-\ndren, and so on. A typical joint hierarchy for skinned animation looks almost \nidentical to a typical rigid hierarchy. For example, a humanoid character’s \njoint hierarchy might look something like this:\nPelvis\n LowerSpine\n  MiddleSpine\n   UpperSpine\n    RightShoulder\n     RightElbow\n      RightHand\n       RightThumb\nFigure 11.5.  The pelvis joint of this character connects to four other joints (tail, spine, and two \nlegs), and so it produces four bones.\n11.2. Skeletons\n",
      "content_length": 1406,
      "extraction_method": "Direct"
    },
    {
      "page_number": 520,
      "chapter": null,
      "content": "498 \n11. Animation Systems\n       RightIndexFinger\n       RightMiddleFinger\n       RightRingFinger\n       RightPinkyFinger\n    LeftShoulder\n     LeftElbow\n      LeftHand\n       LeftThumb\n       LeftIndexFinger\n       LeftMiddleFinger\n       LeftRingFinger\n       LeftPinkyFinger\n    Neck\n     Head\n      LeftEye\n      RightEye\nvarious face joints\n RightThigh\n  RightKnee\n   RightAnkle\n LeftThigh\n  LeftKnee\n   LeftAnkle\nWe usually assign each joint an index from 0 to N – 1. Because each joint \nhas one and only one parent, the hierarchical structure of a skeleton can be \nfully described by storing the index of its parent with each joint. The root \njoint has no parent, so its parent index usually contains an invalid index such \nas –1.\n11.2.2. Representing a Skeleton in Memory\nA skeleton is usually represented by a small top-level data structure that \ncontains an array of data structures for the individual joints. The joints are \nusually listed in an order that ensures a child joint will always appear aft er \nits parent in the array. This implies that joint zero is always the root of the \nskeleton.\nJoint indices  are usually used to refer to joints within animation data struc-\ntures. For example, a child joint typically refers to its parent joint by specifying \nits index. Likewise, in a skinned triangle mesh, a vertex refers to the joint or \njoints to which it is bound by index. This is much more eﬃ  cient than referring \nto joints by name, both in terms of the amount of storage required (a joint in-\ndex can usually be 8 bits wide) and in terms of the amount of time it takes to \nlook up a referenced joint (we can use the joint index to jump immediately to \na desired joint in the array).\n",
      "content_length": 1709,
      "extraction_method": "Direct"
    },
    {
      "page_number": 521,
      "chapter": null,
      "content": "499 \nEach joint data structure typically contains the following information:\nThe \n• \nname of the joint, either as a string or a hashed 32-bit string id.\nThe \n• \nindex of the joint’s parent within the skeleton.\nThe \n• \ninverse bind pose transform of the joint. The bind pose of a joint is the \nposition, orientation, and scale of that joint at the time it was bound to \nthe vertices of the skin mesh. We usually store the inverse of this trans-\nformation for reasons we’ll explore in more depth below.\nA typical skeleton data structure might look something like this:\nstruct Joint\n{\n \nMatrix4x3   m_invBindPose; // inverse bind pose   \n \n  // \ntransform\n \nconst char* m_name;        // human-readable joint  \n \n  // \nname\n \nU8          m_iParent;      // parent index or 0xFF   \n  // \nif root\n};\nstruct Skeleton\n{\n U32 \n \n \n  m_jointCount;  // number of joints\nJoint*      m_aJoint;      // array of joints\n};\n11.3. Poses\n No matt er what technique is used to produce an animation, be it cel-based, \nrigid hierarchical, or skinned/skeletal, every animation takes place over time. \nA character is imbued with the illusion of motion by arranging the character’s \nbody into a sequence of discrete, still poses and then displaying those poses \nin rapid succession, usually at a rate of 30 or 60 poses per second. (Actually, as \nwe’ll see in Section 11.4.1.1, we oft en interpolate between adjacent poses rather \nthan displaying a single pose verbatim.) In skeletal animation, the pose of the \nskeleton directly controls the vertices of the mesh, and posing is the anima-\ntor’s primary tool for breathing life into her characters. So clearly, before we \ncan animate a skeleton, we must ﬁ rst understand how to pose it.\nA skeleton is posed by rotating, translating, and possibly scaling its joints \nin arbitrary ways. The pose of a joint is deﬁ ned as the joint’s position, orien-\ntation, and scale, relative to some frame of reference. A joint pose is usually \n11.3. Poses\n",
      "content_length": 1967,
      "extraction_method": "Direct"
    },
    {
      "page_number": 522,
      "chapter": null,
      "content": "500 \n11. Animation Systems\nrepresented by a 4 × 4 or 4 × 3 matrix, or by an SQT data structure (scale, \nquaternion rotation and vector translation). The pose of a skeleton is just the \nset of all of its joints’ poses and is normally represented as a simple array of \nmatrices or SQTs.\n11.3.1. Bind Pose\nTwo diﬀ erent poses of the same skeleton are shown in Figure 11.6. The pose \non the left  is a special pose known as the bind pose , also sometimes called the \nreference pose or the rest pose. This is the pose of the 3D mesh prior to being \nbound to the skeleton (hence the name). In other words, it is the pose that the \nmesh would assume if it were rendered as a regular, unskinned triangle mesh, \nwithout any skeleton at all. The bind pose is also called the T-pose because the \ncharacter is usually standing with his feet slightly apart and his arms out-\nstretched in the shape of the lett er T. This particular stance is chosen because \nit keeps the limbs away from the body and each other, making the process of \nbinding the vertices to the joints easier.\nFigure 11.6.  Two different poses of the same skeleton. The pose on the left is the special pose \nknown as bind pose.\n11.3.2. Local Poses\nA joint’s pose is most oft en speciﬁ ed relative to its parent joint. A parent-rela-\ntive pose allows a joint to move naturally. For example, if we rotate the shoul-\nder joint, but leave the parent-relative poses of the elbow, wrist and ﬁ ngers \n",
      "content_length": 1449,
      "extraction_method": "Direct"
    },
    {
      "page_number": 523,
      "chapter": null,
      "content": "501 \nunchanged, the entire arm will rotate about the shoulder in a rigid manner, as \nwe’d expect. We sometimes use the term local pose to describe a parent-relative \npose. Local poses are almost always stored in SQT format, for reasons we’ll \nexplore when we discuss animation blending.\nGraphically, many 3D authoring packages like Maya represent joints as \nsmall spheres. However, a joint has a rotation and a scale, not just a trans-\nlation, so this visualization can be a bit misleading. In fact, a joint actually \ndeﬁ nes a coordinate space, no diﬀ erent in principle from the other spaces \nwe’ve encountered (like model space, world space, or view space). So it is best \nto picture a joint as a set of Cartesian coordinate axes. Maya gives the user \nthe option of displaying a joint’s local coordinate axes —this is shown in Fig-\nure 11.7.\nMathematically, a joint pose is nothing more than an aﬃ  ne transformation. \nThe pose of joint j can be writt en as the 4 × 4 aﬃ  ne transformation matrix Pj , \nwhich is comprised of a translation vector Tj , a 3 × 3 diagonal scale matrix Sj \nand a 3 × 3 rotation matrix Rj. The pose of an entire skeleton Pskel can be writt en \nas the set of all poses Pj , where j ranges from 0 to N – 1 :\n \n{ }\n \n1\nskel\n0\n,\n1\n \n \n.\nj\nj\nj\nj\nN\nj\nj\n−\n=\n⎡\n⎤\n=⎢\n⎥\n⎣\n⎦\n=\nS R\n0\nP\nT\nP\nP\n \nFigure 11.7.  Every joint in a skeletal hierarchy deﬁ nes a set of local coordinate space axes, \nknown as joint space.\n11.3. Poses\n",
      "content_length": 1443,
      "extraction_method": "Direct"
    },
    {
      "page_number": 524,
      "chapter": null,
      "content": "502 \n11. Animation Systems\n11.3.2.1. Joint Scale\n Some game engines assume that joints will never be scaled, in which case Sj\nis simply omitt ed and assumed to be the identity matrix. Other engines make \nthe assumption that scale will be uniform if present, meaning it is the same in \nall three dimensions. In this case, scale can be represented using a single scalar \nvalue sj. Some engines even permit nonuniform scale, in which case scale can \nbe compactly represented by the three-element vector sj = [ sjx  sjy  sjz ]. The ele-\nments of the vector sj correspond to the three diagonal elements of the 3 × 3 \nscaling matrix Sj , so it is not really a vector per se. Game engines almost never \npermit shear, so Sj is almost never represented by a full 3 × 3 scale/shear ma-\ntrix, although it certainly could be.\nThere are a number of beneﬁ ts to omitt ing or constraining scale in a pose \nor animation. Clearly using a lower-dimensional scale representation can save \nmemory. (Uniform scale requires a single ﬂ oating-point scalar per joint per \nanimation frame, while nonuniform scale requires three ﬂ oats, and a full 3 × 3 \nscale-shear matrix requires nine.) Restricting our engine to uniform scale has \nthe added beneﬁ t of ensuring that the bounding sphere of a joint will never \nbe transformed into an ellipsoid, as it could be when scaled in a nonuniform \nmanner. This greatly simpliﬁ es the mathematics of frustum and collision tests \nin engines that perform such tests on a per-joint basis.\n11.3.2.2. Representing a Joint Pose in Memory\n As we mentioned above, joint poses are usually stored in SQT format. In C++ \nsuch a data structure might look like this, where Q is ﬁ rst to ensure proper \nalignment and optimal structure packing. (Can you see why?)\nstruct JointPose\n{\n Quaternion m_rot; \n // \nQ\n Vector3 \n m_trans; \n// T\n F32 \n \n \n \nm_scale; // S (uniform scale only)\n};\nIf nonuniform scale is permitt ed, we might deﬁ ne a joint pose like this \ninstead:\nstruct JointPose\n{\n Quaternion m_rot; \n // \nQ\n Vector3 \n m_trans; \n// T\n Vector3 \n m_scale; \n// S\n U8 \n   padding[8];\n};\n",
      "content_length": 2092,
      "extraction_method": "Direct"
    },
    {
      "page_number": 525,
      "chapter": null,
      "content": "503 \nThe local pose of an entire skeleton can be represented as follows, where \nit is understood that the array m_aLocalPose is dynamically allocated to con-\ntain just enough occurrences of JointPose to match the number of joints in \nthe skeleton.\nstruct SkeletonPose\n{\nSkeleton*  \nm_pSkeleton;  // skeleton + num joints\nJointPose* m_aLocalPose; \n // local joint poses\n};\n11.3.2.3. The Joint Pose as a Change of Basis\n It’s important to remember that a local joint pose is speciﬁ ed relative to the \njoint’s immediate parent. Any aﬃ  ne transformation can be thought of as trans-\nforming points and vectors from one coordinate space to another. So when \nthe joint pose transform Pj is applied to a point or vector that is expressed in \nthe coordinate system of the joint j, the result is that same point or vector ex-\npressed in the space of the parent joint.\nAs we’ve done in earlier chapters, we’ll adopt the convention of using \nsubscripts to denote the direction of a transformation. Since a joint pose takes \npoints and vectors from the child joint’s space (C) to that of its parent joint (P), \nwe can write it \nC\nP\n(\n)j\n→\nP\n. Alternatively, we can introduce the function p(j) which \nreturns the parent index of joint j, and write the local pose of joint j as \np(  )\nj\nj\n→\nP\n.\nOn occasion we will need to transform points and vectors in the opposite \ndirection—from parent space into the space of the child joint. This transformation \nis just the inverse of the local joint pose. Mathematically, \n(\n)\n1\np(  )\np(  )\nj\nj\nj\nj\n−\n→\n→\n=\nP\nP\n.\n11.3.3. Global Poses\nSometimes it is convenient to express a joint’s pose in model space or world \nspace. This is called a global pose . Some engines express global poses in matrix \nform, while others use the SQT format.\nMathematically, the model-space pose of a joint (j→M) can be found by \nwalking the skeletal hierarchy from the joint in question all the way to the \nroot, multiplying the local poses (j→p(j)) as we go. Consider the hierarchy \nshown in Figure 11.8. The parent space of the root joint is deﬁ ned to be model \nspace, so p(0)\nM\n≡\n. The model-space pose of joint J2 can therefore be writt en \nas follows:\n \n \n \n2\nM\n2\n1\n1\n0\n0\nM.\n→\n→\n→\n→\n=\nP\nP\nP\nP\n \nLikewise, the model-space pose of joint J5 is just \n \n \n \n5\nM\n5\n4\n4\n3\n3\n0\n0\nM.\n→\n→\n→\n→\n→\n=\nP\nP\nP\nP\nP\n11.3. Poses\n",
      "content_length": 2319,
      "extraction_method": "Direct"
    },
    {
      "page_number": 526,
      "chapter": null,
      "content": "504 \n11. Animation Systems\nIn general, the global pose (joint-to-model transform) of any joint j can be \nwritt en as follows:\n0\nM\np(  ),\nj\ni\ni\ni\n→\n→\n=\n=∏\nP\nP\nj\n \n(11.1) \nwhere it is implied that i becomes p(i) (the parent of joint i) aft er each iteration \nin the product, and p(0)\nM\n≡\n.\n11.3.3.1. Representing a Global Pose in Memory\n We can extend our SkeletonPose data structure to include the global pose \nas follows, where again we dynamically allocate the m_aGlobalPose array \nbased on the number of joints in the skeleton:\nstruct SkeletonPose\n{\nSkeleton*  \nm_pSkeleton;     // skeleton + num joints\nJointPose* m_aLocalPose; \n    // local joint poses\nMatrix44*  \nm_aGlobalPose; // global joint poses\n};\n11.4. Clips\nIn a ﬁ lm , every aspect of each scene is carefully planned out before any anima-\ntions are created. This includes the movements of every character and prop in \nthe scene, and even the movements of the camera. This means that an entire \nscene can be animated as one long, contiguous sequence of frames. And char-\nacters need not be animated at all whenever they are oﬀ -camera.\n0\n1\n2\n3\n4\n5\nxM\nyM\nFigure 11.8.  A global pose can be calculated by walking the hierarchy from the joint in \nquestion towards the root and model space origin, concatenating the child-to-parent (local) \ntransforms of each joint as we go.\n",
      "content_length": 1335,
      "extraction_method": "Direct"
    },
    {
      "page_number": 527,
      "chapter": null,
      "content": "505 \n11.4. Clips\nGame animation is diﬀ erent. A game is an interactive experience, so one \ncannot predict beforehand how the characters are going to move and behave. \nThe player has full control over his or her character and usually has partial \ncontrol over the camera as well. Even the decisions of the computer-driven \nnon-player characters are strongly inﬂ uenced by the unpredictable actions of \nthe human player. As such, game animations are almost never created as long, \ncontiguous sequences of frames. Instead, a game character’s movement must \nbe broken down into a large number of ﬁ ne-grained motions. We call these \nindividual motions animation clips , or sometimes just animations.\nEach clip causes the character to perform a single well-deﬁ ned action. \nSome clips are designed to be looped —for example, a walk cycle or run cycle . \nOthers are designed to be played once—for example, throwing an object, or \ntripping and falling to the ground. Some clips aﬀ ect the entire body of the \ncharacter—the character jumping into the air for instance. Other clips aﬀ ect \nonly a part of the body—perhaps the character waving his right arm. The \nmovements of any one game character are typically broken down into literally \nthousands of clips.\nThe only exception to this rule is when game characters are involved in \na noninteractive portion of the game, known as an in-game cinematic (IGC), \nnoninteractive sequence (NIS), or full-motion video (FMV). Noninteractive se-\nquences are typically used to communicate story elements that do not lend \nthemselves well to interactive gameplay, and they are created in much the \nsame way computer-generated ﬁ lms are made (although they oft en make use \nof in-game assets like character meshes, skeletons, and textures). The terms \nIGC and NIS typically refer to noninteractive sequences that are rendered in \nreal time by the game engine itself. The term FMV applies to sequences that \nhave been prerendered to an MP4, WMV, or other type of movie ﬁ le and are \nplayed back at runtime by the engine’s full-screen movie player.\nA variation on this style of animation is a semi-interactive sequence \nknown as a quick time event (QTE). In a QTE, the player must hit a butt on at \nthe right moment during an otherwise noninteractive sequence in order to see \nthe success animation and proceed; otherwise a failure animation is played, \nand the player must try again, possibly losing a life or suﬀ ering some other \nconsequence as a result.\n11.4.1. The Local Time Line\nWe can think of every animation clip as having a local time line , usually de-\nnoted by the independent variable t. At the start of a clip t = 0 and at the end \nt = T, where T is the duration of the clip. Each unique value of the variable t is \ncalled a time index . An example of this is shown in Figure 11.9.\n",
      "content_length": 2825,
      "extraction_method": "Direct"
    },
    {
      "page_number": 528,
      "chapter": null,
      "content": "506 \n11. Animation Systems\n11.4.1.1. \nPose Interpolation and Continuous Time\nIt’s important to realize that the rate at which frames are displayed to the \nviewer is not necessarily the same as the rate at which poses are created by the \nanimator. In both ﬁ lm and game animation, the animator almost never poses \nthe character every 1/30 or 1/60 of a second. Instead, the animator generates \nimportant poses known as key poses or key frames at speciﬁ c times within the \nclip, and the computer calculates the poses in between via linear or curve-\nbased interpolation. This is illustrated in Figure 11.10.\nBecause of the animation engine’s ability to interpolate poses (which we’ll \nexplore in depth later in this chapter), we can actually sample the pose of the \ncharacter at any time during the clip—not just on integer frame indices. In \nother words, an animation clip’s time line is continuous. In computer anima-\ntion, the time variable t is a real (ﬂ oating-point) number, not an integer.\nFilm animation doesn’t take full advantage of the continuous nature of \nthe animation time-line, because its frame rate is locked at exactly 24, 30, or \n60 frames per second. In ﬁ lm, the viewer sees the characters’ poses at frames \nAnimation A: Local Time\nt = 0\nt = (0.4)T\nt = T\nt = (0.8)T\nFigure 11.9.  The local time line of an animation showing poses at selected time indices.\ninterpolated\nposes\nkey pose 2\nkey pose 1\nFigure 11.10.  An animator creates a relatively small number of key poses, and the engine ﬁ lls \nin the rest of the poses via interpolation.\n",
      "content_length": 1557,
      "extraction_method": "Direct"
    },
    {
      "page_number": 529,
      "chapter": null,
      "content": "507 \n1, 2, 3, and so on—there’s never any need to ﬁ nd a character’s pose on frame \n3.7, for example. So in ﬁ lm animation, the animator doesn’t pay much (if any) \natt ention to how the character looks in between the integral frame indices.\nIn contrast, a real-time game’s frame rate always varies a litt le, depending \non how much load is currently being placed on the CPU and GPU. Also, game \nanimations are sometimes time-scaled in order to make the character appear to \nmove faster or slower than originally animated. So in a real-time game, an ani-\nmation clip is almost never sampled on integer frame numbers. In theory, with \na time scale of 1.0, a clip should be sampled at frames 1, 2, 3, and so on. But \nin practice, the player might actually see frames 1.1, 1.9, 3.2, and so on. And if \nthe time scale is 0.5, then the player might actually see frames 1.1, 1.4, 1.9, 2.6, \n3.2, and so on. A negative time scale can even be used to play an animation in \nreverse. So in game animation, time is both continuous and scalable.\n11.4.1.2. Time Units\n Because an animation’s time line is continuous, time is best measured in units \nof seconds. Time can also be measured in units of frames , presuming we deﬁ ne \nthe duration of a frame beforehand. Typical frame durations are 1/30 or 1/60 \nof a second for game animation. However, it’s important not to make the mis-\ntake of deﬁ ning your time variable t as an integer that counts whole frames. \nNo matt er which time units are selected, t should be a real (ﬂ oating-point) \nquantity, a ﬁ xed-point number, or an integer that measures subframe time \nintervals. The goal is to have suﬃ  cient resolution in your time measurements \nfor doing things like “tweening” between frames or scaling an animation’s \nplay-back speed.\n11.4.1.3. Frame versus Sample\nUnfortunately, the term frame has more than one common meaning in the \ngame industry. This can lead to a great deal of confusion. Sometimes a frame \nis taken to be a period of time that is 1/30 or 1/60 of a second in duration. But in \nother contexts, the term frame is applied to a single point in time (e.g., we might \nspeak of the pose of the character “at frame 42”).\nI personally prefer to use the term sample to refer to a single point in time, \nand I reserve the word frame to describe a time period that is 1/30 or 1/60 of a \nsecond in duration. So for example, a one-second animation created at a rate \nof 30 frames per second would consist of 31 samples and would be 30 frames in \nduration, as shown in Figure 11.11. The term “sample” comes from the ﬁ eld \nof signal processing. A continuous-time signal (i.e., a function f(t)) can be con-\nverted into a set of discrete data points by sampling that signal at uniformly-\nspaced time intervals. See htt p://en.wikipedia.org/wiki/Sampling_%28signal_\nprocessing%29 for more information on sampling.\n11.4. Clips\n",
      "content_length": 2871,
      "extraction_method": "Direct"
    },
    {
      "page_number": 530,
      "chapter": null,
      "content": "508 \n11. Animation Systems\n11.4.1.4. Frames, Samples and Looping Clips\nWhen a clip is designed to be played over and over repeatedly, we say it is a \nlooped animation . If we imagine two copies of a 1-second (30-frame/31-sample) \nclip laid back-to-front, then sample 31 of the ﬁ rst clip will coincide exactly in \ntime with sample 1 of the second clip, as shown in Figure 11.12. For a clip to \nloop properly, then, we can see that the pose of the character at the end of the \nclip must exactly match the pose at the beginning. This, in turn, implies that \nthe last sample of a looping clip (in our example, sample 31) is redundant. \nMany game engines therefore omit the last sample of a looping clip.\nThis leads us to the following rules governing the number of samples and \nframes in any animation clip:\nIf a clip is \n• \nnon-looping, an N-frame animation will have N + 1 unique \nsamples.\nIf a clip is \n• \nlooping, then the last sample is redundant, so an N-frame ani-\nmation will have N unique samples.\n26\n27\n28\n29\n30\n1\n2\n3\n4\n5\n...\n31\nSamples:\nFrames:\n30\n29\n28\n27\n26\n6\n5\n4\n3\n2\n1\nFigure 11.11.  A one-second animation sampled at 30 frames per second is 30 frames in duration \nand consists of 31 samples.\n30\n29\n28\n27\n26\n6\n5\n4\n3\n2\n30\n29\n28\n27\n26\n5\n4\n3\n2\n1\n31\n1\n...\n...\n...\n...\n...\n...\nFigure 11.12.  The last sample of a looping clip coincides in time with its ﬁ rst sample and is, \ntherefore, redundant.\n11.4.1.5. Normalized Time (Phase)\nIt is sometimes convenient to employ a normalized time unit u, such that u = 0 \nat the start of the animation, and u = 1 at the end, no matt er what its duration \nT may be. We sometimes refer to normalized time as the phase of the animation \nclip, because u acts like the phase of a sine wave when the animation is looped. \nThis is illustrated in Figure 11.13.\n",
      "content_length": 1798,
      "extraction_method": "Direct"
    },
    {
      "page_number": 531,
      "chapter": null,
      "content": "509 \nNormalized time is useful when synchronizing two or more animation \nclips that are not necessarily of the same absolute duration. For example, we \nmight want to smoothly cross-fade from a 2-second (60-frame) run cycle into \na 3-second (90-frame) walk cycle. To make the cross-fade look good, we want \nto ensure that the two animations remain synchronized at all times, so that the \nfeet line up properly in both clips. We can accomplish this by simply sett ing \nthe normalized start time of the walk clip, uwalk to match the normalized time \nindex of the run clip, urun. We then advance both clips at the same normalized \nrate, so that they remain in sync. This is quite a bit easier and less error-prone \nthan doing the synchronization using the absolute time indices twalk and trun.\n11.4.2. The Global Time Line\nJust as every animation clip has a local time line (whose clock starts at 0 at \nthe beginning of the clip), every character in a game has a global time line \n(whose clock starts when the character is ﬁ rst spawned into the game world, \nor perhaps at the start of the level or the entire game). In this book, we’ll use \nthe time variable τ to measure global time, so as not to confuse it with the local \ntime variable t.\nWe can think of playing an animation as simply mapping that clip’s local \ntime line onto the character’s global time line. For example, Figure 11.14 illus-\ntrates playing animation clip A starting at a global time of τstart = 102 seconds.\nA: Normalized Local Time\nu = 0\nu = 0.4\nu = 1\nu = 0.8\nFigure 11.13.  An animation clip, showing normalized time units.\nClip A\nt = 0 sec\n5 sec\nτstart = 102 sec\nτ = 105 sec\n110 sec\nFigure 11.14.  Playing animation clip A starting at a global time of 102 seconds.\n11.4. Clips\n",
      "content_length": 1750,
      "extraction_method": "Direct"
    },
    {
      "page_number": 532,
      "chapter": null,
      "content": "510 \n11. Animation Systems\nAs we saw above, playing a looping animation is like laying down an \ninﬁ nite number of back-to-front copies of the clip onto the global time line. \nWe can also imagine looping an animation a ﬁ nite number of times, which \ncorresponds to laying down a ﬁ nite number of copies of the clip. This is il-\nlustrated in Figure 11.15.\nTime-scaling a clip makes it appear to play back more quickly or more \nslowly than originally animated. To accomplish this, we simply scale the im-\nage of the clip when it is laid down onto the global time line. Time-scaling is \nmost naturally expressed as a playback rate , which we’ll denote R. For example, \nif an animation is to play back at twice the speed (R = 2), then we would scale \nthe clip’s local time line to one-half (1/R = 0.5) of its normal length when map-\nping it onto the global time line. This is shown in Figure 11.16.\nPlaying a clip in reverse corresponds to using a time scale of –1, as shown \nin Figure 11.17.\nClip A\n110 sec\nτstart = 102 sec\nClip A\n...\nτ = 105 sec\nFigure 11.15.  Playing a looping animation corresponds to laying down multiple back-to-back \ncopies of the clip.\nClip A\nτstart = 102 sec\nτ = 105 sec\nClip A\nR = 2\n(scale t by 1/R = 0.5)\nt = 0 sec\nt = 5 sec\nt = 0 sec\n5 sec\nFigure 11.16.  Playing an animation at twice the speed corresponds to scaling its local time line \nby a factor of ½.\nt = 5 sec\n0 sec\nτstart = 102 sec\nτ = 105 sec\n110 sec\n A pilC\nClip A\nt = 0 sec\n5 sec\nR = –1\n(ﬂip t)\nFigure 11.17.  Playing a clip in reverse corresponds to a time scale of –1.\n",
      "content_length": 1557,
      "extraction_method": "Direct"
    },
    {
      "page_number": 533,
      "chapter": null,
      "content": "511 \nIn order to map an animation clip onto a global time line, we need the fol-\nlowing pieces of information about the clip:\nits global start time\n• \n τstart ,\nits playback rate \n• \nR,\nits duration \n• \nT,\nand the number of times it should loop, which we’ll denote \n• \nN.\nGiven this information, we can map from any global time τ to the correspond-\ning local time t, and vice-versa, using the following two relations:\n \nstart\n(\n),\nt\nR\n=\nτ−τ\n \n(11.2)\n \nstart\n1 .t\nR\nτ=τ\n+\n \nIf the animation doesn’t loop (N = 1), then we should clamp t into the valid \nrange [0, T] before using it to sample a pose from the clip:\n \n[\n]\nstart\n0\nclamp\n(\n)  .\nT\nt\nR\n=\nτ−τ\n \nIf the animation loops forever (N = ∞), then we bring t into the valid range \nby taking the remainder of the result aft er dividing by the duration T. This is \naccomplished via the modulo operator (mod, or % in C/C++), as shown below:\n(\n)\nstart\n(\n)   mod .\nt\nR\nT\n=\nτ−τ\n \nIf the clip loops a ﬁ nite number of times (1 < N < ∞), we must ﬁ rst clamp t \ninto the range [0, NT] and then modulo that result by T in order to bring t into \na valid range for sampling the clip:\nMost game engines work directly with local animation time lines and don’t \nuse the global time line directly. However, working directly in terms of global \ntimes can have some incredibly useful beneﬁ ts. For one thing, it makes syn-\nchronizing animations trivial.\n11.4.3. Comparison of Local and Global Clocks\nThe animation system must keep track of the time indices of every animation \nthat is currently playing. To do so, we have two choices:\nLocal clocks\n• \n . In this approach, each clip has its own local clock, usually \nrepresented by a ﬂ oating-point time index stored in units of seconds or \nframes, or in normalized time units (in which case it is oft en called the \nphase of the animation). At the moment the clip begins to play, the local \n11.4. Clips\n[\n]\nstart\n0\n(clamp\n(\n)  \n)  mod .\nNT\nt\nR\nT\n=\nτ−τ\n",
      "content_length": 1934,
      "extraction_method": "Direct"
    },
    {
      "page_number": 534,
      "chapter": null,
      "content": "512 \n11. Animation Systems\ntime index t is usually taken to be zero. To advance the animations for-\nward in time, we advance the local clocks of each clip individually. If a \nclip has a non-unit playback rate R, the amount by which its local clock \nadvances must be scaled by R.\nGlobal clock\n• \n . In this approach, the character has a global clock, usually mea-\nsured in seconds, and each clip simply records the global time at which it \nstarted playing, τstart. The clips’ local clocks are calculated from this infor-\nmation using Equation (11.2), rather than being stored explicitly.\nThe local clock approach has the beneﬁ t of being simple, and it is the most \nobvious choice when designing an animation system. However, the global \nclock approach has some distinct advantages, especially when it comes to syn-\nchronizing animations, either within the context of a single character or across \nmultiple characters in a scene.\n11.4.3.1. Synchronizing Animations with a Local Clock\nWith a local clock approach, we said that the origin of a clip’s local time line \n(t = 0) is usually deﬁ ned to coincide with the moment at which the clip starts \nplaying. Thus, to synchronize two or more clips, they must be played at ex-\nactly the same moment in game time. This seems simple enough, but it can \nbecome quite tricky when the commands used to play the animations are \ncoming from disparate engine subsystems.\nFor example, let’s say we want to synchronize the player character’s punch \nanimation with a non-player character’s corresponding hit reaction anima-\ntion.  The problem is that the player’s punch is initiated by the player subsys-\ntem in response to detecting that a butt on was hit on the joy pad. Meanwhile, \nthe NPC ’s hit reaction animation is played by the artiﬁ cial intelligence (AI) \nsubsystem. If the AI code runs before the player code in the game loop, there \nwill be a one-frame delay between the start of the player’s punch and the start \nof the NPC’s reaction. And if the player code runs before the AI code, then the \nopposite problem occurs when an NPC tries to punch the player. If a message-\npassing (event) system is used to communicate between the two subsystems, \nadditional delays might be incurred (see Section 14.7 for more details). This \nproblem is illustrated in Figure 11.18.\nvoid GameLoop()\n{\n \nwhile (!quit)\n {\n \n \n// preliminary updates... \n \n \nUpdateAllNpcs(); // react to punch event  \n \n \n \n   // \nfrom last frame\n",
      "content_length": 2456,
      "extraction_method": "Direct"
    },
    {
      "page_number": 535,
      "chapter": null,
      "content": "513 \n \n \n// more updates...\n \n \nUpdatePlayer(); // punch button hit – start punch  \n       \n    // anim, and send event to NPC to  \n       \n    // \nreact\n \n \n// still more updates...\n }\n}\n11.4.3.2. Synchronizing Animations with a Global Clock\nA global clock approach helps to alleviate many of these synchronization \nproblems, because the origin of the time line (τ = 0) is common across all clips \nby deﬁ nition. If two or more animations’ global start times are numerically \nequal, the clips will start in perfect synchronization. If their play back rates \nare also equal, then they will remain in sync with no drift . It no longer matt ers \nwhen the code that plays each animation executes. Even if the AI code that \nplays the hit reaction ends up running a frame later than the player’s punch \ncode, it is still trivial to keep the two clips in sync by simply noting the global \nstart time of the punch and sett ing the global start time of the reaction anima-\ntion to match it. This is shown in Figure 11.19.\nOf course, we do need to ensure that the two character’s global clocks \nmatch, but this is trivial to do. We can either adjust the global start times to \ntake account of any diﬀ erences in the characters’ clocks, or we can simply have \nall characters in the game share a single master clock.\nFigure 11.18.  The order of execution of disparate gameplay systems can introduce animation synchro-\nnization problems when local clocks are used.\n11.4. Clips\n",
      "content_length": 1465,
      "extraction_method": "Direct"
    },
    {
      "page_number": 536,
      "chapter": null,
      "content": "514 \n11. Animation Systems\n11.4.4. A Simple Animation Data Format\nTypically, animation data is extracted from a Maya scene ﬁ le by sampling the \npose of the skeleton discretely at a rate of 30 or 60 samples per second. A sam-\nple comprises a full pose for each joint in the skeleton. The poses are usually \nstored in SQT format: For each joint j, the scale component is either a single \nﬂ oating-point scalar Sj , or a three-element vector Sj = [ Sjx  Sjy  Sjz ]. The rotation-\nal component is of course a four-element quaternion Qj = [ Qjx  Qjy  Qjz  Qjw ]. \nAnd the translational component is a three-element vector Tj = [ Tjx  Tjy  Tjz ]. \nWe sometimes say that an animation consists of up to 10 channels per joint, \nin reference to the 10 components of Sj , Qj , and Tj. This is illustrated in Fig-\nure 11.20. \nFigure 11.19.  A global clock approach can alleviate animation synchronization problems.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nSamples\nJoint 0\nT0\nQ0\nS0\nJoint 1\nT1\nQ1\nS1\n...\ny\nx\nz\ny\nx\nz\nw\ny\nx\nz\n...\n...\nFigure 11.20.  An uncompressed animation clip contains 10 channels of ﬂ oating-point data \nper sample, per joint.\n",
      "content_length": 1112,
      "extraction_method": "Direct"
    },
    {
      "page_number": 537,
      "chapter": null,
      "content": "515 \nIn C++, an animation clip can be represented in many diﬀ erent ways. Here \nis one possibility:\nstruct JointPose { ... }; // SQT, defined as above\nstruct AnimationSample\n{\nJointPose*       m_aJointPose; // array of joint   \n \n           \n// poses\n};\nstruct AnimationClip\n{\n \nSkeleton*        m_pSkeleton;\n \nF32              m_framesPerSecond;\n \nU32              m_frameCount;\nAnimationSample* m_aSamples; // array of samples\n \nbool             m_isLooping;\n};\nAn animation clip is authored for a speciﬁ c skeleton and generally won’t \nwork on any other skeleton. As such, our example AnimationClip data struc-\nture contains a reference to its skeleton, m_pSkeleton. (In a real engine, this \nmight be a unique skeleton id rather than a Skeleton* pointer. In this case, \nthe engine would presumably provide a way to quickly and conveniently look \nup a skeleton by its unique id.)\nThe number of JointPoses in the m_aJointPose array within each sam-\nple is presumed to match the number of joints in the skeleton. The number \nof samples in the m_aSamples array is dictated by the frame count and by \nwhether or not the clip is intended to loop. For a non-looping animation, the \nnumber of samples is (m_frameCount + 1). However, if the animation loops, \nthen the last sample is identical to the ﬁ rst sample and is usually omitt ed. In \nthis case, the sample count is equal to m_frameCount.\nIt’s important to realize that in a real game engine, animation data isn’t \nactually stored in this simplistic format. As we’ll see in Section 11.8, the data \nis usually compressed in various ways to save memory.\n11.4.4.1. Animation Retargeting\n We said above that an animation is typically only compatible with a single \nskeleton. An exception to this rule can be made for skeletons that are closely \nrelated. For example, if a group of skeletons are identical except for a number \nof optional leaf joints that do not aﬀ ect the fundamental hierarchy, then an an-\n11.4. Clips\n",
      "content_length": 1967,
      "extraction_method": "Direct"
    },
    {
      "page_number": 538,
      "chapter": null,
      "content": "516 \n11. Animation Systems\nimation authored for one of these skeletons should work on any of them. The \nonly requirement is that the engine be capable of ignoring animation channels \nfor joints that cannot be found in the skeleton being animated.\nOther more-advanced techniques exist for retargeting animations au-\nthored for one skeleton so that they work on a diﬀ erent skeleton. This is an \nactive area of research, and a full discussion of the topic is beyond the scope \nof this book. For more information, see for example htt p://portal.acm.org/cita-\ntion.cfm?id=1450621 and htt p://chrishecker.com/Real-time_Motion_Retarget-\ning_to_Highly_Varied_User-Created_Morphologies.\n11.4.5. Continuous Channel Functions\n The samples of an animation clip are really just deﬁ nitions of continuous func-\ntions over time. You can think of these as 10 scalar-valued functions of time \nper joint, or as two vector-valued functions and one quaternion-valued func-\ntion per joint. Theoretically, these channel functions are smooth and continu-\nous across the entire clip’s local time line, as shown in Figure 11.21 (with the \nexception of explicitly authored discontinuities like camera cuts). In practice, \nhowever, many game engines interpolate linearly between the samples, in \nwhich case the functions actually used are piece-wise linear approximations to \nthe underlying continuous functions. This is depicted in Figure 11.22.\nFigure 11.21.  The animation samples in a clip deﬁ ne continuous functions over time.\nt\nQy3\nSamples\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nFigure 11.22.  Many game engines use a piece-wise linear approximation when interpolating \nchannel functions.\n",
      "content_length": 1654,
      "extraction_method": "Direct"
    },
    {
      "page_number": 539,
      "chapter": null,
      "content": "517 \n11.4.6. Metachannels\n Many games permit additional “metachannels” of data to be deﬁ ned for an \nanimation. These channels can encode game-speciﬁ c information that doesn’t \nhave to do directly with posing the skeleton but which needs to be synchro-\nnized with the animation.\nIt is quite common to deﬁ ne a special channel that contains event triggers \nat various time indices, as shown in Figure 11.23. Whenever the animation’s \nlocal time index passes one of these triggers, an event is sent to the game en-\ngine, which can respond as it sees ﬁ t. (We’ll discuss events in detail in Chap-\nter 14.) One common use of event triggers is to denote at which points during \nthe animation certain sound or particle eﬀ ects should be played. For example, \nwhen the left  or right foot touches the ground, a footstep sound and a “cloud \nof dust” particle eﬀ ect could be initiated.\nAnother common practice is to permit special joints, known in Maya as \nlocators , to be animated along with the joints of the skeleton itself. Because a \njoint or locator is just an aﬃ  ne transform, these special joints can be used to \nencode the position and orientation of virtually any object in the game.\nA typical application of animated locators is to specify how the game’s \ncamera should be positioned and oriented during an animation. In Maya, a \nlocator is constrained to a camera, and the camera is then animated along with \nthe joints of the character(s) in the scene. The camera’s locator is exported and \nused in-game to move the game’s camera around during the animation. The \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nSamples\nJoint 0\nT0\nQ0\nS0\nJoint 1\nT1\nQ1\nS1\nOther\nFootstep\nLeft\nFootstep\nRight\nReload\nWeapon\nEvents\n...\n...\n...\n...\nFigure 11.23.  A special event trigger channel can be added to an animation clip in order to \nsynchronize sound effects, particle effects, and other game events with an animation.\n11.4. Clips\n",
      "content_length": 1898,
      "extraction_method": "Direct"
    },
    {
      "page_number": 540,
      "chapter": null,
      "content": "518 \n11. Animation Systems\nﬁ eld of view (focal length) of the camera, and possibly other camera att ributes, \ncan also be animated by placing the relevant data into one or more additional \nﬂ oating-point channels .\nOther examples of non-joint animation channels include:\ntexture coordinate scrolling,\n• \ntexture animation\n• \n (a special case of texture coordinate scrolling in which \nframes are arranged linearly within a texture, and the texture is scrolled \nby one complete frame at each iteration),\nanimated material parameters (color, specularity, transparency, etc.),\n• \nanimated lighting parameters (radius, cone angle, intensity, color, etc.),\n• \nany other parameters that need to change over time and are in some \n• \nway synchronized with an animation.\n11.5. Skinning and Matrix Palette Generation\nWe’ve seen how to pose a skeleton by rotating, translating, and possibly scal-\ning its joints. And we know that any skeletal pose can be represented math-\nematically as a set of local (\np( )\nj\nj\n→\nP\n) or global (\nM\nj→\nP\n) joint pose transformations, \none for each joint j. Next, we will explore the process of att aching the vertices \nof a 3D mesh to a posed skeleton. This process is known as skinning .\n11.5.1. Per-Vertex Skinning Information\nA skinned mesh is att ached to a skeleton by means of its vertices. Each vertex \ncan be bound to one or more joints. If bound to a single joint, the vertex tracks \nthat joint’s movement exactly. If bound to two or more joints, the vertex’s posi-\ntion becomes a weighted average of the positions it would have assumed had it \nbeen bound to each joint independently.\nTo skin a mesh to a skeleton, a 3D artist must supply the following ad-\nditional information at each vertex:\nthe \n• \nindex or indices of the joint(s) to which it is bound,\nfor each joint, a \n• \nweighting factor describing how much inﬂ uence that joint \nshould have on the ﬁ nal vertex position.\nThe weighting factors are assumed to add to one, as is customary when calcu-\nlating any weighted average.\nUsually a game engine imposes an upper limit on the number of joints \nto which a single vertex can be bound. A four-joint limit is typical for a num-\nber of reasons. First, four 8-bit joint indices can be packed into a 32-bit word, \n",
      "content_length": 2251,
      "extraction_method": "Direct"
    },
    {
      "page_number": 541,
      "chapter": null,
      "content": "519 \nwhich is convenient. Also, while it’s prett y easy to see a diﬀ erence in quality \nbetween a two-, three-, and even a four-joint-per-vertex model, most people \ncannot see a quality diﬀ erence as the number of joints per vertex is increased \nbeyond four.\nBecause the joint weights must sum to one, the last weight can be omitt ed \nand oft en is. (It can be calculated at runtime as \n3\n0\n1\n2\n1\n(\n)\nw\nw\nw\nw\n= −\n+\n+\n.) As \nsuch, a typical skinned vertex data structure might look as follows:\nstruct SkinnedVertex\n{\n    float m_position[3];    //   (Px, Py, Pz)\n    float m_normal[3];      //    (Nx, Ny, Nz)\n    float m_u, m_v;         // texture coordinates \n  \n \n       \n  //   (u, \nv)\n    U8    m_jointIndex[4];  // joint indices\n    float m_jointWeight[3]; // joint weights, last one   \n    \n       \n   // \nomitted\n};\n11.5.2. The Mathematics of Skinning\nThe vertices of a skinned mesh track the movements of the joint(s) to which \nthey are bound. To make this happen mathematically, we would like to ﬁ nd a \nmatrix that can transform the vertices of the mesh from their original positions \n(in bind pose) into new positions that correspond to the current pose of the \nskeleton. We shall call such a matrix a skinning matrix.\nLike all mesh vertices, the position of a skinned vertex is speciﬁ ed in mod-\nel space. This is true whether its skeleton is in bind pose, or in any other pose. \nSo the matrix we seek will transform vertices from model space (bind pose) \nto model space (current pose ). Unlike the other transforms we’ve seen thus \nfar, such as the model-to-world transform or the world-to-view transform, \na skinning matrix is not a change of basis transform. It morphs vertices into \nnew positions, but the vertices are in model space both before and aft er the \ntransformation.\n11.5.2.1. Simple Example: One-Jointed Skeleton\nLet us derive the basic equation for a skinning matrix. To keep things simple at \nﬁ rst, we’ll work with a skeleton consisting of a single joint. We therefore have \ntwo coordinate spaces to work with: model space, which we’ll denote with \nthe subscript M, and the joint space of our one and only joint, which will be \nindicated by the subscript J. The joint’s coordinate axes start out in bind pose, \n11.5. Skinning and Matrix Palette Generation\n",
      "content_length": 2287,
      "extraction_method": "Direct"
    },
    {
      "page_number": 542,
      "chapter": null,
      "content": "520 \n11. Animation Systems\nwhich we’ll denote with the superscript B. At any given moment during an \nanimation, the joint’s axes move to a new position and orientation in model \nspace—we’ll indicate this current pose with the superscript C.\nNow consider a single vertex that is skinned to our joint. In bind pose, \nits model-space position is \nB\nM\nv . The skinning process calculates the vertex’s \nnew model-space position in the current pose, \nC\nM\nv . This is illustrated in Fig-\nure 11.24.\nThe “trick” to ﬁ nding the skinning matrix for a given joint is to realize \nthat the position of a vertex bound to a joint is constant when expressed in that \njoint’s coordinate space. So we take the bind-pose position of the vertex in model \nspace, convert it into joint space, move the joint into its current pose, and ﬁ -\nnally convert the vertex back into model space. The net eﬀ ect of this round trip \nfrom model space to joint space and back again is to “morph” the vertex from \nbind pose into the current pose.\nReferring to the illustration in Figure 11.25, let’s assume that the coordi-\nnates of the vertex \nB\nM\nv\n are (4, 6) in model space (when the skeleton is in bind \npose). We convert this vertex into its equivalent joint space coordinates \nj\nv , \nwhich are roughly (1, 3) as shown in the diagram. Because the vertex is bound \nto the joint, its joint space coordinates will always be (1, 3) no matt er how the \njoint may move. Once we have the joint in the desired current pose, we con-\nvert the vertex’s coordinates back into model space, which we’ll denote with \nthe symbol \nC\nM\nv\n. In our diagram, these coordinates are roughly (18, 2). So the \nskinning transformation has morphed our vertex from (4, 6) to (18, 2) in model \nspace, due entirely to the motion of the joint from its bind pose to the current \npose shown in the diagram.\nLooking at the problem mathematically, we can denote the bind pose of the \njoint j in model space by the matrix \nM\nj→\nB\n. This matrix transforms a point or \nxM\nyM\nxB\nyB\nxC\nyC\nModel Space Axes\nBind pose \nvertex position, \nin model space\nBind Pose \nJoint Space \nAxes\nCurrent \nPose Joint \nSpace Axes\nCurrent pose \nvertex position, \nin model space\nvM\nB\nv M\nC\nFigure 11.24.  Bind pose and current pose of a simple, one-joint skeleton and a single vertex \nbound to that joint.\n",
      "content_length": 2315,
      "extraction_method": "Direct"
    },
    {
      "page_number": 543,
      "chapter": null,
      "content": "521 \nvector whose coordinates are expressed in joint j’s space into an equivalent set \nof model space coordinates . Now, consider a vertex \nB\nM\nv\n whose coordinates \nare expressed in model space with the skeleton in bind pose. To convert these \nvertex coordinates into the space of joint j, we simply multiply it by the inverse \nbind pose matrix, \n1\nM\nM\n(\n)\nj\nj\n−\n→\n→\n=\nB\nB\n:\n \n \n \nB\nB\n1\nM\nM\nM\nM\n(\n)\n.\nj\nj\nj\n−\n→\n→\n=\n=\nv\nv\nB\nv\nB\n \n(11.3)\nLikewise, we can denote the joint’s current pose (i.e., any pose that is not \nbind pose) by the matrix \nM\nj→\nC\n. To convert \nj\nv  from joint space back into mod-\nel space, we simply multiply it by the current pose matrix as follows:\n \n \nC\nM\nM.\nj\nj→\n=\nv\nv C\nIf we expand \nj\nv  using Equation (11.3), we obtain an equation that takes our \nvertex directly from its position in bind pose to its position in the current \npose:\n \n \n \n \n \nC\nM\nM\nB\n1\nM\nM\nM\nB\nM\n(\n)\n.\nj\nj\nj\nj\nj\n→\n−\n→\n→\n=\n=\n=\nv\nv C\nv\nB\nC\nv\nK\n (11.4) \n \nThe combined matrix \n1\nM\nM\n(\n)\nj\nj\nj\n−\n→\n→\n=\nK\nB\nC\n is known as a skinning matrix .\n11.5.2.2. Extension to Multijointed Skeletons\nIn the example above, we considered only a single joint. However, the math we \nderived above actually applies to any joint in any skeleton imaginable, because \nwe formulated everything in terms of global poses (i.e., joint space to model \nspace transforms). To extend the above formulation to a skeleton containing \nmultiple joints, we therefore need to make only two minor adjustments:\nxM\nyM\nxB\nyB\nxC\nyC\n1. Transform into\n    joint space\nv M\nB\nv M\nC\nv j\nv j\n3. Transform back\n    into model space\n2. Move joint into\n    current pose\nFigure 11.25.  By transforming a vertex’s position into joint space, it can be made to “track” \nthe joint’s movements.\n11.5. Skinning and Matrix Palette Generation\n",
      "content_length": 1774,
      "extraction_method": "Direct"
    },
    {
      "page_number": 544,
      "chapter": null,
      "content": "522 \n11. Animation Systems\nWe must make sure that our \n1. \nM\nj→\nB\n and \nM\nj→\nC\nmatrices are calculated \nproperly for the joint in question, using Equation (11.1). \nM\nj→\nB\n and \nM\nj→\nC\n are just the bind pose and current pose equivalents, respectively, \nof the matrix \nM\nj→\nP\n given in that equation.\nWe must calculate an array of skinning matrices \n2. \nj\nK , one for each joint j. \nThis array is known as a matrix palett e . The matrix palett e is passed to the \nrendering engine when rendering a skinned mesh. For each vertex, the \nrenderer looks up the appropriate joint’s skinning matrix in the palett e \nand uses it to transform the vertex from bind pose into current pose.\nWe should note here that the current pose matrix \nM\nj→\nC\n changes every \nframe as the character assumes diﬀ erent poses over time. However, the in-\nverse bind-pose matrix is constant throughout the entire game, because the \nbind pose of the skeleton is ﬁ xed when the model is created. Therefore, the \nmatrix \n1\nM\n(\n)\nj\n−\n→\nB\n is generally cached with the skeleton, and needn’t be calcu-\nlated at runtime. Animation engines generally calculate local poses for each \njoint (\np( )\nj\nj\n→\nC\n), then use Equation (11.1) to convert these into global poses \n(\nM\nj→\nC\n), and ﬁ nally multiply each global pose by the corresponding cached \ninverse bind pose matrix (\n1\nM\n(\n)\nj\n−\n→\nB\n) in order to generate a skinning matrix \n(\nj\nK ) for each joint.\n11.5.2.3. Incorporating the Model-to-World Transform\nEvery vertex must eventually be transformed from model space into world \nspace. Some engines therefore premultiply the palett e of skinning matrices by \nthe object’s model-to-world transform. This can be a useful optimization, as \nit saves the rendering engine one matrix multiply per vertex when rendering \nskinned geometry. (With hundreds of thousands of vertices to process, this \nsavings can really add up!)\nTo incorporate the model-to-world transform into our skinning matrices, \nwe simply concatenate it to the regular skinning matrix equation, as follows:\n \n \n1\nW\nM\nM\nM\nW\n(\n)\n(\n)\n.\nj\nj\nj\n−\n→\n→\n→\n=\nK\nB\nC\nM\nSome engines bake the model-to-world transform into the skinning ma-\ntrices like this, while others don’t. The choice is entirely up to the engineer-\ning team and is driven by all sorts of factors. For example, one situation in \nwhich we would deﬁ nitely not want to do this is when a single animation \nis being applied to multiple characters simultaneously—a technique known \nas animation instancing that is commonly used for animating large crowds of \ncharacters. In this case we need to keep the model-to-world transforms sepa-\nrate so that we can share a single matrix palett e across all characters in the \ncrowd.\n",
      "content_length": 2702,
      "extraction_method": "Direct"
    },
    {
      "page_number": 545,
      "chapter": null,
      "content": "523 \n11.5.2.4. Skinning a Vertex to Multiple Joints\n When a vertex is skinned to more than one joint, we calculate its ﬁ nal position \nby assuming it is skinned to each joint individually, calculating a model space \nposition for each joint and then taking a weighted average of the resulting posi-\ntions. The weights are provided by the character rigging artist, and they must \nalways sum to one. (If they do not sum to one, they should be re-normalized \nby the tools pipeline.)\nThe general formula for a weighted average of N quantities \n0a  through \n1\nN\na\n−, with weights \n0\nw  through \n1\nN\nw\n− and with \n \n \nThis works equally well for vector quantities ai. So, for a vertex skinned to N \njoints with indices \n0j  through \n1\nN\nj\n− and weights \n0\nw  through \n1\nN\nw\n−, we can \nextend Equation (11.4) as follows:\n \n \n1\nC\nB\nM\nM\n0\n,\ni\nN\nĳ\ni\nw\n−\n=\n=∑\nv\nv\nK\nwhere \nij\nK  is the skinning matrix for the joint \nij .\n11.6. Animation Blending\nThe term animation blending refers to any technique that allows more than one ani-\nmation clip to contribute the ﬁ nal pose of the character. To be more precise, blend-\ning combines two or more input poses to produce an output pose for the skeleton.\nBlending usually combines two or more poses at a single point in time, \nand generates an output at that same moment in time. In this context, blend-\ning is used to combine two or more animations into a host of new animations, \nwithout having to create them manually. For example, by blending an injured \nwalk animation with an uninjured walk, we can generate various intermedi-\nate levels of apparent injury for our character while he is walking. As another \nexample, we can blend between an animation in which the character is aim-\ning to the left  and one in which he’s aiming to the right, in order to make the \ncharacter aim along any desired angle between the two extremes. Blending \ncan be used to interpolate between extreme facial expressions, body stances, \nlocomotion modes, and so on.\nBlending can also be used to ﬁ nd an intermediate pose between two \nknown poses at diﬀ erent points in time. This is used when we want to ﬁ nd the \npose of a character at a point in time that does not correspond exactly to one of \n11.6. Animation Blending\n1\n0\n1\nN\ni\ni\nw\n−\n=\n=\n∑\n,  \nis\n  \n1\n0\nˆ\n.\nN\ni i\ni\na\nw a\n−\n=\n=∑\n",
      "content_length": 2298,
      "extraction_method": "Direct"
    },
    {
      "page_number": 546,
      "chapter": null,
      "content": "524 \n11. Animation Systems\nthe sampled frames available in the animation data. We can also use temporal \nanimation blending to smoothly transition from one animation to another, by \ngradually blending from the source animation to the destination over a short \nperiod of time.\n11.6.1. LERP Blending\nGiven two skeletal poses \n{\n}\n1\n0\n (\n)\n \nN\nskel\nA\nA j\nj\n−\n=\n=\nP\nP\n and \n{\n}\n1\n0\n (\n)\n \nN\nskel\nB\nB j\nj\n−\n=\n=\nP\nP\n, we wish \nto ﬁ nd an intermediate pose \nskel\nLERP\nP\n between these two extremes. This can be \ndone by performing a linear interpolation (LERP) between the local poses of \neach individual joint in each of the two source poses. This can be writt en as \nfollows:\n \nLERP\n(\n)\nLERP (\n) , (\n) , \n(1\n)(\n)\n(\n) .\nj\nA j\nB j\nA j\nB j\n⎡\n⎤\n=\nβ\n⎣\n⎦\n=\n−β\n+ β\nP\nP\nP\nP\nP\n \n(11.5)\nThe interpolated pose of the whole skeleton is simply the set of interpolated \nposes for all of the joints:\n \n{\n}\n1\nskel\nLERP\nLERP\n0\n (\n)\n \n.\nN\nj\nj\n−\n=\n=\nP\nP\n \n(11.6)\nIn these equations, β is called the blend percentage or blend factor. When \nβ = 0, the ﬁ nal pose of the skeleton will exactly match \nskel\nA\nP\n; when β = 1, the \nﬁ nal pose will match \nskel\nB\nP\n. When β is between zero and one, the ﬁ nal pose \nis an intermediate between the two extremes. This eﬀ ect is illustrated in Fig-\nure 11.10.\nWe’ve glossed over one small detail here: We are linearly interpolating \njoint poses , which means interpolating 4×4 transformation matrices. But, as we \nsaw in Chapter 4, interpolating matrices directly is not practical. This is one of \nthe reasons why local poses are usually expressed in SQT format—doing so \nallows us to apply the LERP operation deﬁ ned in Section 4.2.5 to each compo-\nnent of the SQT individually. The linear interpolation of the translation com-\nponent T of an SQT is just a straightforward vector LERP:\n \nLERP\n(\n)\nLERP[(\n) , (\n) , ]\n(1\n)(\n)\n(\n) .\nj\nA j\nB j\nA j\nB j\n=\nβ\n=\n−β\n+ β\nT\nT\nT\nT\nT\n \n(11.7)\nThe linear interpolation of the rotation component is a quaternion LERP or \nSLERP (spherical linear interpolation):\n \nLERP\n(q\n)\nLERP[(q ) , (q ) , ]\n(1\n)(q )\n(q )\nj\nA j\nB j\nA j\nB j\n=\nβ\n=\n−β\n+ β\n \n(11.8a)\nor\n",
      "content_length": 2102,
      "extraction_method": "Direct"
    },
    {
      "page_number": 547,
      "chapter": null,
      "content": "525 \n \nLERP\n(q\n)\nSLERP[(q ) , (q ) , ]\nsin((1\n) )\nsin(\n)\n(q )\n(q ) .\nsin( )\nsin( )\nj\nA j\nB j\nA j\nB j\n=\nβ\n−β θ\nβθ\n=\n+\nθ\nθ\n \n(11.8b)\nFinally, the linear interpolation of the scale component is either a scalar or vec-\ntor LERP, depending on the type of scale (uniform or nonuniform ) supported \nby the engine:\n \nLERP\n(\n)\nLERP[(\n) , (\n) , ]\n(1\n)(\n)\n(\n)\nj\nA j\nB j\nA j\nB j\n=\nβ\n=\n−β\n+ β\ns\ns\ns\ns\ns\n \n(11.9a)\nor\n \nLERP\n(\n)\nLERP[(\n) , (\n) , ]\n(1\n)(\n)\n(\n) .\nj\nA j\nB j\nA j\nB j\ns\ns\ns\ns\ns\n=\nβ\n=\n−β\n+ β\n \n(11.9b)\nWhen linearly interpolating between two skeletal poses, the most natural-\nlooking intermediate pose is generally one in which each joint pose is inter-\npolated independently of the others, in the space of that joint’s immediate \nparent. In other words, pose blending is generally performed on local poses. If \nwe were to blend global poses directly in model space, the results would tend \nto look biomechanically implausible.\nBecause pose blending is done on local poses, the linear interpolation of \nany one joint’s pose is totally independent of the interpolations of the other \njoints in the skeleton. This means that linear pose interpolation can be per-\nformed entirely in parallel on multiprocessor architectures.\n11.6.2. Applications of LERP Blending\nNow that we understand the basics of LERP blending, let’s have a look at \nsome typical gaming applications.\n11.6.2.1. Temporal Interpolation\n As we mentioned in Section 11.4.1.1, game animations are almost never sam-\npled exactly on integer frame indices. Because of variable frame rate, the play-\ner might actually see frames 0.9, 1.85, and 3.02, rather than frames 1, 2, and \n3 as one might expect. In addition, some animation compression techniques \ninvolve storing only disparate key frames, spaced at uneven intervals across \nthe clip’s local time line. In either case, we need a mechanism for ﬁ nding in-\ntermediate poses between the sampled poses that are actually present in the \nanimation clip.\nLERP blending is used to ﬁ nd these intermediate poses. As an example, \nlet’s imagine that our animation clip contains evenly-spaced pose samples at \n11.6. Animation Blending\n",
      "content_length": 2135,
      "extraction_method": "Direct"
    },
    {
      "page_number": 548,
      "chapter": null,
      "content": "526 \n11. Animation Systems\ntimes 0, Δt, 2Δt, 3Δt, and so on. To ﬁ nd a pose at time t = (2.18)Δt, we simply \nﬁ nd the linear interpolation between the poses at times 2Δt and 3Δt, using a \nblend percentage of β = 0.18.\nIn general, we can ﬁ nd the pose at time t given pose samples at any two \ntimes t1 and t2 that bracket t, as follows:\n \n1\n2\n1\n2\n( )\nLERP[\n( ), \n( ), ( )]\n(1\n( ))\n( )\n( )\n( ),\nj\nj\nj\nj\nj\nt =\nβ\n=\n−β\n+ β\nP\nP\nP\nP\nP\nt\nt\nt\nt\nt\nt\nt\n \n(11.10)\nwhere the blend factor β(t) is the ratio\n \n1\n2\n1\n( )\n.\nt\nt\nt\n−\nβ\n=\n−\nt\nt\n \n(11.11)\n11.6.2.2. Motion Continuity: Cross-Fading\n Game characters are animated by piecing together a large number of ﬁ ne-\ngrained animation clips. If your animators are any good, the character will ap-\npear to move in a natural and physically plausible way within each individual \nclip. However, it is notoriously diﬃ  cult to achieve the same level of quality \nwhen transitioning from one clip to the next. The vast majority of the “pops” \nwe see in game animations occur when the character transitions from one clip \nto the next.\nIdeally, we would like the movements of each part of a character’s body to \nbe perfectly smooth, even during transitions. In other words, the three-dimen-\nsional paths traced out by each joint in the skeleton as it moves should contain \nno sudden “jumps.” We call this C0 continuity ; it is illustrated in Figure 11.26.\nNot only should the paths themselves be continuous, but their ﬁ rst deriv-\natives (velocity curves) should be continuous as well. This is called C1 continu-\nity (or continuity of velocity and momentum). The perceived quality and real-\nism of an animated character’s movement improves as we move to higher and \nhigher order continuity. For example, we might want to achieve C2 continuity, \nin which the second derivatives of the motion paths (acceleration curves) are \nalso continuous.\nt\nTx7\nt\nTx7\ndiscontinuity\nC0 continuous\nnot C0 continuous\nFigure 11.26.  The channel function on the left has C0 continuity, while the path on the right \ndoes not.\n",
      "content_length": 2030,
      "extraction_method": "Direct"
    },
    {
      "page_number": 549,
      "chapter": null,
      "content": "527 \nStrict mathematical continuity up to C1 or higher is oft en infeasible to \nachieve. However, LERP-based animation blending can be applied to achieve \na reasonably pleasing form of C0 motion continuity. It usually also does a \nprett y good job of approximating C1 continuity. When applied to transitions \nbetween clips in this manner, LERP blending is sometimes called cross-fading . \nLERP blending can introduce unwanted artifacts, such as the dreaded “sliding \nfeet” problem, so it must be applied judiciously.\nTo cross-fade between two animations, we overlap the time lines of the \ntwo clips by some reasonable amount, and then blend the two clips together. \nThe blend percentage β starts at zero at time tstart  , meaning that we see only \nclip A when the cross-fade begins. We gradually increase β until it reaches a \nvalue of one at time tend . At this point only clip B will be visible, and we can \nretire clip A altogether. The time interval over which the cross-fade occurs \n(Δtblend = tend – tstart) is sometimes called the blend time .\nTypes of Cross-Fades\nThere are two common ways to perform a cross-blended transition:\nSmooth transition\n• \n . Clips A and B both play simultaneously as β increases \nfrom zero to one. For this to work well, the two clips must be looping \nanimations, and their time lines must be synchronized so that the posi-\ntions of the legs and arms in one clip match up roughly with their posi-\ntions in the other clip. (If this is not done, the cross-fade will oft en look \ntotally unnatural.) This technique is illustrated in Figure 11.27.\nFrozen transition\n• \n . The local clock of clip A is stopped at the moment clip \nB starts playing. Thus the pose of the skeleton from clip A is frozen \nwhile clip B gradually takes over the movement. This kind of transi-\ntional blend works well when the two clips are unrelated and cannot be \nClip A\nt\nClip B\nβ\n1\n0\ntstart\ntend\nFigure 11.27.  A smooth transition, in which the local clocks of both clips keep running during \nthe transition.\n11.6. Animation Blending\n",
      "content_length": 2045,
      "extraction_method": "Direct"
    },
    {
      "page_number": 550,
      "chapter": null,
      "content": "528 \n11. Animation Systems\ntime-synchronized, as they must be when performing a smooth transi-\ntion. This approach is depicted in Figure 11.28.\nWe can also control how the blend factor β varies during the transition. \nIn Figure 11.27 and Figure 11.28, the blend factor varied linearly with time. \nTo achieve an even smoother transition, we could vary β according to a cubic \nfunction of time, such as a one-dimensional Bézier. When such a curve is ap-\nplied to a currently-running clip that is being blended out, it is known as an \nease-out curve; when it is applied to a new clip that is being blended in, it is \nknown as an ease-in curve. This is shown in Figure 11.29.\nThe equation for a Bézier ease-in/ease-out curve is given below. It returns \nthe value of β at any time t within the blend interval. βstart is the blend factor \nat the start of the blend interval, tstart , and βend is the ﬁ nal blend factor at time \ntend. The parameter u is the normalized time between tstart and tend , and for con-\nvenience we’ll also deﬁ ne v = 1 – u (the inverse normalized time). Note that \nthe Bézier tangents Tstart and Tend are taken to be equal to the corresponding \nClip A\nt\nClip B\nβ\n1\n0\nA’s local timeline \nfreezes here\ntstart\ntend\nFigure 11.28.  A frozen transition, in which clip A’s local clock is stopped during the transi-\ntion.\nClip A\nt\nClip B\nβ\n1\n0\ntstart\ntend\nFigure 11.29.  A smooth transition, with a cubic ease-in/ease-out curve applied to the blend \nfactor.\n",
      "content_length": 1470,
      "extraction_method": "Direct"
    },
    {
      "page_number": 551,
      "chapter": null,
      "content": "529 \nblend factors βstart and βend , because this yields a well-behaved curve for our \npurposes:\n \nstart\nend\nstart\n3\n2\n2\n3\nstart\nstart\nend\nend\n3\n2\n2\n3\nstart\nend\nlet  \nand  \n1\n:\n( )\n(\n)\n(3\n)\n(3\n)\n(\n)\n(\n3\n)\n(3\n)\n.\nu\nt\nv\nu\nt\nv\nv u T\nvu T\nu\nv\nv u\nvu\nu\n⎛\n⎞\n−\n=⎜\n⎟\n⎝\n⎠\n−\n= −\nβ\n=\nβ\n+\n+\n+\nβ\n=\n+\nβ\n+\n+\nβ\nt\nt\nCore Poses\nThis is an appropriate time to mention that motion continuity can actually \nbe achieved without blending if the animator ensures that the last pose in any \ngiven clip matches the ﬁ rst pose of the clip that follows it. In practice, anima-\ntors oft en decide upon a set of core poses —for example, we might have a core \npose for standing upright, one for crouching, one for lying prone, and so on. \nBy making sure that the character starts in one of these core poses at the begin-\nning of every clip and returns to a core pose at the end, C0 continuity can be \nachieved by simply ensuring that the core poses match when animations are \nspliced together. C1 or higher-order motion continuity can also be achieved \nby ensuring that the character’s movement at the end of one clip smoothly \ntransitions into the motion at the start of the next clip. This is easily achieved \nby authoring a single smooth animation and then breaking it into two or more \nclips.\n11.6.2.3. Directional Locomotion\nLERP-based animation blending is oft en applied to character locomotion. \nWhen a real human being walks or runs, he can change the direction in which \nhe is moving in two basic ways: First, he can turn his entire body to change \ndirection, in which case he always faces in the direction he’s moving. I’ll call \nTargeted\nPivotal\nPath of \nMovement\nFigure 11.30.  In pivotal movement, the character faces the direction she is moving and pivots \nabout her vertical axis to turn. In targeted movement, the movement direction need not \nmatch the facing direction.\n11.6. Animation Blending\n",
      "content_length": 1882,
      "extraction_method": "Direct"
    },
    {
      "page_number": 552,
      "chapter": null,
      "content": "530 \n11. Animation Systems\nthis pivotal movement, because the person pivots about his vertical axis when \nhe turns. Second, he can keep facing in one direction, while walking forward, \nbackward, or sideways (known as straﬁ ng in the gaming world) in order to \nmove in a direction that is independent of his facing direction. I’ll call this \ntargeted movement, because it is oft en used in order to keep one’s eye—or one’s \nweapon—trained on a target while moving. These two movement styles are \nillustrated in Figure 11.30.\nTargeted Movement\nTo implement targeted movement, the animator authors three separate looping \nanimation clips—one moving forward, one straﬁ ng to the left , and one straf-\ning to the right. I’ll call these directional locomotion clips. The three directional \nclips are arranged around the circumference of a semicircle, with forward at \n0 degrees, left  at 90 degrees and right at –90 degrees. With the character’s fac-\ning direction ﬁ xed at 0 degrees, we ﬁ nd the desired movement direction on \nthe semicircle, select the two adjacent movement animations, and blend them \ntogether via LERP-based blending. The blend percentage β is determined by \nhow close the angle of movement is to the angles of two adjacent clips. This is \nillustrated in Figure 11.31.\nNote that we did not include backward movement in our blend, for a full \ncircular blend. This is because blending between a sideways strafe and a back-\nward run cannot be made to look natural in general. The problem is that when \nstraﬁ ng to the left , the character usually crosses its right foot in front of its left  \nso that the blend into the pure forward run animation looks correct. Likewise, \nthe right strafe is usually authored with the left  foot crossing in front of the \nright. When we try to blend such strafe animations directly into a backward \nrun, one leg will start to pass through the other, which looks extremely awk-\nFigure 11.31.  Targeted movement can be implemented by blending together looping locomo-\ntion clips that move in each of the four principal directions.\n",
      "content_length": 2075,
      "extraction_method": "Direct"
    },
    {
      "page_number": 553,
      "chapter": null,
      "content": "531 \nward and unnatural. There are a number of ways to solve this problem. One \nfeasible approach is to deﬁ ne two hemispherical blends, one for forward mo-\ntion and one for backward motion, each with strafe animations that have been \ncraft ed to work properly when blended with the corresponding straight run. \nWhen passing from one hemisphere to the other, we can play some kind of \nexplicit transition animation so that the character has a chance to adjust its gait \nand leg crossing appropriately.\nPivotal Movement\nTo implement pivotal movement, we can simply play the forward locomotion \nloop while rotating the entire character about its vertical axis to make it turn. \nPivotal movement looks more natural if the character’s body doesn’t remain \nbolt upright when it is turning—real humans tend to lean into their turns a \nlitt le bit. We could try slightly tilting the vertical axis of the character as a \nwhole, but that would cause problems with the inner foot sinking into the \nground while the outer foot comes oﬀ  the ground. A more natural-looking \nresult can be achieved by animating three variations on the basic forward \nwalk or run—one going perfectly straight, one making an extreme left  turn, \nand one making an extreme right turn. We can then LERP-blend between \nthe straight clip and the extreme left  turn clip to implement any desired lean \nangle.\n11.6.3. Complex LERP Blends\n In a real game engine, characters make use of a wide range of complex blends \nfor various purposes. It can be convenient to “prepackage” certain commonly \nused types of complex blends for ease of use. In the following sections, we’ll \ninvestigate a few popular types of prepackaged complex blends.\n11.6.3.1. Generalized One-Dimensional LERP Blending\nLERP blending can be easily extended to more than two animation clips, us-\ning a technique I call one-dimensional LERP blending. We deﬁ ne a new blend \nparameter b that lies in any linear range desired (e.g., –1 to +1, or from 0 to 1, \nor even from 27 to 136). Any number of clips can be positioned at arbitrary \npoints along this range, as shown in Figure 11.32. For any given value of b, we \nselect the two clips immediately adjacent to it and blend them together using \nEquation (11.5). If the two adjacent clips lie at points b1 and b2 , then the blend \npercentage β can be determined using a technique analogous to that used in \nEquation (11.10), as follows:\n \n \n \n(11.12)\n1\n2\n1\n.\nb\nb\nb\nb\n−\nβ=\n−\n11.6. Animation Blending\n",
      "content_length": 2477,
      "extraction_method": "Direct"
    },
    {
      "page_number": 554,
      "chapter": null,
      "content": "532 \n11. Animation Systems\nTargeted movement is just a special case of one-dimensional LERP blend-\ning. We simply straighten out the circle on which the directional animation \nclips were placed and use the movement direction angle θ as the param-\neter b (with a range of –90 to 90 degrees). Any number of animation clips \ncan be placed onto this blend range at arbitrary angles. This is shown in Fig-\nure 11.33.\n11.6.3.2. Simple Two-Dimensional LERP Blending\nSometimes we would like to smoothly vary two aspects of a character’s motion \nsimultaneously. For example, we might want the character to be capable of \naiming his weapon vertically and horizontally. Or we might want to allow our \ncharacter to vary her pace length and the separation of her feet as she moves. \nWe can extend one-dimensional LERP blending to two dimensions in order to \nachieve these kinds of eﬀ ects.\nClip A\nb0\nb1\nb2\nb3\nb4\nClip B\nClip C\nClip D Clip E\nb\n1\n2\n1\nb\nb\nb\nb\n−\n−\n=\nβ\nFigure 11.32.  A generalized linear blend between N animation clips.\nFigure 11.33.  The directional clips used in targeted movement can be thought of as a special \ncase of one-dimensional LERP blending.\n",
      "content_length": 1154,
      "extraction_method": "Direct"
    },
    {
      "page_number": 555,
      "chapter": null,
      "content": "533 \nIf we know that our 2D blend involves only four animation clips, and if \nthose clips are positioned at the four corners of a square region, then we can \nﬁ nd a blended pose by performing two 1D blends. Our generalized blend fac-\ntor b becomes a two-dimensional blend vector b = [ bx  by ]. If b lies within the \nsquare region bounded by our four clips, we can ﬁ nd the resulting pose by \nfollowing these steps:\nUsing the horizontal blend factor \n• \nbx , ﬁ nd two intermediate poses, one \nbetween the top two animation clips and one between the bott om two \nclips. These two poses can be found by performing two simple one-di-\nmensional LERP blends.\nThen, using the vertical blend factor \n• \nby , ﬁ nd the ﬁ nal pose by LERP-\nblending the two intermediate poses together.\nThis technique is illustrated in Figure 11.34.\n11.6.3.3. Triangular Two-Dimensional LERP Blending\nThe simple 2D blending technique we investigated above only works when \nthe animation clips we wish to blend lie at the corners of a square region. How \ncan we blend between an arbitrary number of clips positioned at arbitrary \nlocations in our 2D blend space?\n Let’s imagine that we have three animation clips that we wish to blend to-\ngether. Each clip, designated by the index i, corresponds to a particular blend \ncoordinate bi = [ bxi  byi ] in our two-dimensional blend space, and these three \nblend coordinates form a triangle in our two-dimensional blend space. Each \nclip i deﬁ nes a set of joint poses {\n}\n1\n0\n (\n)\n \nN\nĳ\nj\n−\n=\nP\n, where j is the joint index and N \nis the number of joints in the skeleton. We wish to ﬁ nd the interpolated pose \nClip A\nb x\nby\nClip B\nClip D\nClip C\nBlend \nAB\nBlend \nCD\nFinal \nBlend\nb\nFigure 11.34.  A simple formulation for 2D animation blending between four clips at the \ncorners of a square region.\n11.6. Animation Blending\n",
      "content_length": 1841,
      "extraction_method": "Direct"
    },
    {
      "page_number": 556,
      "chapter": null,
      "content": "534 \n11. Animation Systems\nof the skeleton corresponding to an arbitrary point b within the triangle, as \nillustrated in Figure 11.35.\nBut how can we calculate a LERP blend between three animation clips? \nThankfully, the answer is simple: the LERP function can actually operate on \nany number of inputs, because it is really just a weighted average . As with any \nweighted average, the weights must add to one. In the case of a two-input \nLERP blend, we used the weights β and (1 – β), which of course add to one. \nFor a three-input LERP, we simply use three weights, α, β, and γ = (1 – α – β). \nThen we calculate the LERP as follows:\nLERP\n0\n1\n2\n(\n)\n(\n)\n(\n)\n(1\n)(\n) .\nj\nj\nj\nj\n= α\n+ β\n+\n−α−β\nP\nP\nP\nP\n \n(11.13)\nGiven the two-dimensional blend vector b, we ﬁ nd the blend weights α, \nβ, and γ by ﬁ nding the barycentric coordinates of the point b relative to the \ntriangle formed by the three clips in two-dimensional blend space (htt p://\nen.wikipedia.org/wiki/Barycentric_coordinates_%28mathematics%29).  In \ngeneral, the barycentric coordinates of a point b within a triangle with vertices \nb1 , b2 , and b3 are three scalar values (α, β, γ) that satisfy the relations\n \n0\n1\n2\n \n \n \n= α\n+ β\n+γ\nb\nb\nb\nb  \n(11.14)\nand\n \n1.\nα+ β+γ=\nThese are exactly the weights we seek for our three-clip weighted average. \nBarycentric coordinates are illustrated in Figure 11.36.\nNote that plugging the barycentric coordinate (1, 0, 0) into Equation \n(11.14) yields b0 , while (0, 1, 0) gives us b1 and (0, 0, 1) produces b2. Likewise, \nplugging these blend weights into Equation (11.13) gives us poses \n0\n(\n)j\nP\n, \n1\n(\n)j\nP\n, \nClip A\nb0\nb y\nClip B\nClip C\nb\nb1\nb2\nbx\nFinal \nBlend\nFigure 11.35.  Two-dimensional animation blending between three animation clips.\n",
      "content_length": 1743,
      "extraction_method": "Direct"
    },
    {
      "page_number": 557,
      "chapter": null,
      "content": "535 \nand \n2\n(\n)j\nP\n, respectively. Furthermore, the barycentric coordinate (⅓, ⅓, ⅓) lies \nat the centroid of the triangle and gives us an equal blend between the three \nposes. This is exactly what we’d expect.\n11.6.3.4. Generalized Two-Dimensional LERP Blending\nThe barycentric coordinate technique can be extended to an arbitrary number \nof animation clips positioned at arbitrary locations within the two-dimension-\nal blend space. We won’t describe it in its entirety here, but the basic idea is \nto use a technique known as Delaunay triangulation (htt p://en.wikipedia.org/\nwiki/Delaunay_triangulation) to ﬁ nd a set of triangles given the positions of \nthe various animation clips bi . Once the triangles have been determined, we \ncan ﬁ nd the triangle that encloses the desired point b and then perform a \nthree-clip LERP blend as described above. This is shown in Figure 11.37.\nb0\nby\nb\nb1\nb2\nα\nβ\nγ\nbx\nFigure 11.36.  Various barycentric coordinates within a triangle.\nClip A\nb0\nClip B\nb1\nClip C\nClip D\nClip E\nClip F\nClip G\nClip H\nClip I\nClip J\nb2\nb3\nb4\nb5\nb6\nb7\nb8\nb9\nby\nbx\nFigure 11.37.  Delaunay triangulation between an arbitrary number of animation clips \npositioned at arbitrary locations in two-dimensional blend space.\n11.6. Animation Blending\n",
      "content_length": 1258,
      "extraction_method": "Direct"
    },
    {
      "page_number": 558,
      "chapter": null,
      "content": "536 \n11. Animation Systems\n11.6.4. Partial-Skeleton Blending\nA human being can control diﬀ erent parts of his or her body independently. \nFor example, I can wave my right arm while walking and pointing at some-\nthing with my left  arm. One way to implement this kind of movement in a \ngame is via a technique known as partial-skeleton blending .\nRecall from Equations (11.5) and (11.6) that when doing regular LERP \nblending, the same blend percentage β was used for every joint in the skeleton. \nPartial-skeleton blending extends this idea by permitt ing the blend percent-\nage to vary on a per-joint basis. In other words, for each joint j, we deﬁ ne a \nseparate blend percentage βj. The set of all blend percentages for the entire \nskeleton { }\n1\n0\n \n \nN\nj\nj\n−\n=\nβ\n is sometimes called a blend mask because it can be used to \n“mask out” certain joints by sett ing their blend percentages to zero.\nAs an example, let’s say we want our character to wave at someone using \nhis right arm and hand. Moreover, we want him to be able to wave whether \nhe’s walking, running, or standing still. To implement this using partial blend-\ning, the animator deﬁ nes three full-body animations: Walk, Run, and Stand. \nThe animator also creates a single waving animation, Wave. A blend mask is \ncreated in which the blend percentages are zero everywhere except for the \nright shoulder, elbow, wrist, and ﬁ nger joints, where they are equal to one:\n \n1,\n right arm,\n0,\notherwise.\nj\nj\n⎧\n∈\n⎨\nβ =\n⎩\n \nWhen Walk, Run, or Stand is LERP-blended with Wave using this blend mask, \nthe result is a character who appears to be walking, running, or standing while \nwaving his right arm.\nPartial blending is useful, but it has a tendency to make a character’s \nmovement look unnatural. This occurs for two basic reasons:\nAn abrupt change in the per-joint blend factors can cause the movements \n• \nof one part of the body to appear disconnected from the rest of the body. \nIn our example, the blend factors change abruptly at the right shoulder \njoint. Hence the animation of the upper spine, neck, and head are being \ndriven by one animation, while the right shoulder and arm joints are \nbeing entirely driven by a diﬀ erent animation. This can look odd. The \nproblem can be mitigated somewhat by gradually changing the blend \nfactors rather than doing it abruptly. (In our example, we might select a \nblend percentage of 0.9 at the right shoulder, 0.5 on the upper spine, and \n0.2 on the neck and mid-spine.)\nThe movements of a real human body are never totally independent. \n• \nFor example, one would expect a person’s wave to look more “bouncy” \n",
      "content_length": 2622,
      "extraction_method": "Direct"
    },
    {
      "page_number": 559,
      "chapter": null,
      "content": "537 \nand out of control when he or she is running than when he or she is \nstanding still. Yet with partial blending, the right arm’s animation will \nbe identical no matt er what the rest of the body is doing. This problem is \ndiﬃ  cult to overcome using partial blending. Instead, many game devel-\nopers have recently turned to a more natural-looking technique known \nas additive blending.\n11.6.5. Additive Blending \nAdditive blending approaches the problem of combining animations in a to-\ntally new way. It introduces a new kind of animation called a diﬀ erence clip, \nwhich, as its name implies, represents the diﬀ erence between two regular ani-\nmation clips. A diﬀ erence clip can be added onto a regular animation clip in \norder to produce interesting variations in the pose and movement of the char-\nacter. In essence, a diﬀ erence clip encodes the changes that need to be made to \none pose in order to transform it into another pose. Diﬀ erence clips are oft en \ncalled additive animation clips in the game industry. We’ll stick with the term \ndiﬀ erence clip in this book because it more accurately describes what is going \non.\nConsider two input clips called the source clip (S) and the reference clip (R). \nConceptually, the diﬀ erence clip is D = S – R. If a diﬀ erence clip D is added to \nits original reference clip, we get back the source clip (S = D + R). We can also \ngenerate animations that are partway between R and S by adding a percent-\nage of D to R, in much the same way that LERP blending ﬁ nds intermediate \nanimations between two extremes. However, the real beauty of the additive \nblending technique is that once a diﬀ erence clip has been created, it can be \nadded to other unrelated clips, not just to the original reference clip. We’ll call \nthese animations target clips and denote them with the symbol T.\nAs an example, if the reference clip has the character running normally \nand the source clip has him running in a tired manner, then the diﬀ erence clip \nwill contain only the changes necessary to make the character look tired while \nrunning. If this diﬀ erence clip is now applied to a clip of the character walk-\ning, the resulting animation can make the character look tired while walking. \nA whole host of interesting and very natural-looking animations can be cre-\nated by adding a single diﬀ erence clip onto various “regular” animation clips, \nor a collection of diﬀ erence clips can be created, each of which produces a \ndiﬀ erent eﬀ ect when added to a single target animation.\n11.6.5.1. Mathematical Formulation\nA diﬀ erence animation D is deﬁ ned as the diﬀ erence between some source \nanimation S and some reference animation R. So conceptually, the diﬀ erence \n11.6. Animation Blending\n",
      "content_length": 2737,
      "extraction_method": "Direct"
    },
    {
      "page_number": 560,
      "chapter": null,
      "content": "538 \n11. Animation Systems\npose (at a single point in time) is D = S – R. Of course, we’re dealing with joint \nposes, not scalar quantities, so we cannot simply subtract the poses. In gen-\neral, a joint pose is a 4 × 4 aﬃ  ne transformation matrix \nC\nP\n→\nP\n that transforms \npoints and vectors from the child joint’s local space to the space of its parent \njoint. The matrix equivalent of subtraction is multiplication by the inverse ma-\ntrix. So given the source pose Sj and the reference pose Rj for any joint j in the \nskeleton, we can deﬁ ne the diﬀ erence pose Dj at that joint as follows (for this \ndiscussion, we’ll drop the C→P or j→p(j) subscript, as it is understood that we \nare dealing with child-to-parent pose matrices):\n \n1.\nj\nj\nj\n−\n=\nD\nS R\n \n “Adding” a diﬀ erence pose Dj onto a target pose Tj yields a new additive \npose Aj. This is achieved by simply concatenating the diﬀ erence transform \nand the target transform as follows:\n \n1\n(\n)\n.\nj\nj\nj\nj\nj\nj\n−\n=\n=\nA\nD T\nS R\nT  \n(11.15)\nWe can verify that this is correct by looking at what happens when the diﬀ er-\nence pose is “added” back onto the original reference pose:\n \n1\n.\nj\nj\nj\nj\nj\nj\nj\n−\n=\n=\n=\nA\nD R\nS R\nR\nS\nIn other words, adding the diﬀ erence animation D back onto the original ref-\nerence animation R yields the source animation S, as we’d expect.\nTemporal Interpolation of Difference Clips\nAs we learned in Section 11.4.1.1, game animations are almost never sampled \non integer frame indices. To ﬁ nd a pose at an arbitrary time t, we must oft en \ntemporally interpolate between adjacent pose samples at times t1 and t2. Thank-\nfully, diﬀ erence clips can be temporally interpolated just like their non-addi-\ntive counterparts. We can simply apply Equations (11.10) and (11.11) directly \nto our diﬀ erence clips as if they were ordinary animations.\nNote that a diﬀ erence animation can only be found when the input clips \nS and R are of the same duration. Otherwise there would be a period of time \nduring which either S or R is undeﬁ ned, meaning D would be undeﬁ ned as \nwell.\nAdditive Blend Percentage\nIn games, we oft en wish to blend in only a percentage of a diﬀ erence anima-\ntion to achieve varying degrees of the eﬀ ect it produces. For example, if a \ndiﬀ erence clip causes the character to turn his head 80 degrees to the right, \n",
      "content_length": 2316,
      "extraction_method": "Direct"
    },
    {
      "page_number": 561,
      "chapter": null,
      "content": "539 \nblending in 50% of the diﬀ erence clip should make him turn his head only \n40 degrees to the right.\nTo accomplish this, we turn once again to our old friend LERP. We wish \nto interpolate between the unaltered target animation and the new animation \nthat would result from a full application of the diﬀ erence  animation. To do \nthis, we extend Equation (11.15) as follows: \nLERP(\n,  \n,  )\n(1\n)(\n)\n(\n).\nj\nj\nj\nj\nj\nj\nj\n=\nβ\n=\n−β\n+ β\nA\nT\nD T\nT\nD T\n \n(11.16)\nAs we saw in Chapter 4, we cannot LERP matrices directly. So Equation \n(11.16) must be broken down into three separate interpolations for S, Q, and T, \njust as we did in Equations (11.7), (11.8), and (11.9).\n11.6.5.2. Additive Blending Versus Partial Blending\nAdditive blending is similar in some ways to partial blending. For example, \nwe can take the diﬀ erence between a standing clip and a clip of standing while \nwaving the right arm. The result will be almost the same as using a partial \nblend to make the right arm wave. However, additive blends suﬀ er less from \nthe “disconnected” look of animations combined via partial blending. This \nis because, with an additive blend, we are not replacing the animation for \na subset of joints or interpolating between two potentially unrelated poses. \nRather, we are adding movement to the original animation—possibly across \nthe entire skeleton. In eﬀ ect, a diﬀ erence animation “knows” how to change a \ncharacter’s pose in order to get him to do something speciﬁ c, like being tired, \naiming his head in a certain direction, or waving his arm. These changes can \nbe applied to a wide variety of animations, and the result oft en looks very \nnatural.\n11.6.5.3. Limitations of Additive Blending\n Of course, additive animation is not a silver bullet. Because it adds movement \nto an existing animation, it can have a tendency to over-rotate the joints in the \nskeleton, especially when multiple diﬀ erence clips are applied simultaneous-\nly. As a simple example, imagine a target animation in which the character’s \nleft  arm is bent at a 90 degree angle. If we add a diﬀ erence animation that also \nrotates the elbow by 90 degrees, then the net eﬀ ect would be to rotate the arm \nby 90 + 90 = 180 degrees. This would cause the lower arm to interpenetrate the \nupper arm—not a comfortable position for most individuals!\nClearly we must be careful when selecting the reference clip and also \nwhen choosing the target clips to which to apply it. Here are some simple \nrules of thumb:\n11.6. Animation Blending\n",
      "content_length": 2514,
      "extraction_method": "Direct"
    },
    {
      "page_number": 562,
      "chapter": null,
      "content": "540 \n11. Animation Systems\nKeep hip rotations to a minimum in the reference clip.\n• \nThe shoulder and elbow joints should usually be in neutral poses in the \n• \nreference clip to minimize over-rotation of the arms when the diﬀ erence \nclip is added to other targets.\nAnimators should create a new diﬀ erence animation for each core pose \n• \n(e.g., standing upright, crouched down, lying prone, etc.). This allows \nthe animator to account for the way in which a real human would move \nwhen in each of these stances.\nThese rules of thumb can be a helpful starting point, but the only way to \nreally learn how to create and apply diﬀ erence clips is by trial and error or by \napprenticing with animators or engineers who have experience creating and \napplying diﬀ erence animations. If your team hasn’t used additive blending in \nthe past, expect to spend a signiﬁ cant amount of time learning the art of ad-\nditive blending.\n11.6.6. Applications of Additive Blending\n11.6.6.1. Stance Variation\nOne particularly striking application of additive blending is stance variation . \nFor each desired stance, the animator creates a one-frame diﬀ erence anima-\ntion. When one of these single-frame clips is additively blended with a base \nanimation, it causes the entire stance of the character to change drastically \nwhile he continues to perform the fundamental action he’s supposed to per-\nform. This idea is illustrated in Figure 11.38.\nTarget +\nDifference A\nTarget +\nDifference B\nTarget Clip\n(and Reference)\nFigure 11.38.  Two single-frame difference animations A and B can cause a target animation \nclip to assume two totally different stances. (Character from Naughty Dog’s Uncharted: \nDrake’s Fortune.)\n",
      "content_length": 1700,
      "extraction_method": "Direct"
    },
    {
      "page_number": 563,
      "chapter": null,
      "content": "541 \nTarget Clip\n(and Reference)\nTarget +\nDifference A\nTarget +\nDifference B\nTarget +\nDifference C\nFigure 11.39.  Additive blends can be used to add variation to a repetitive idle animation. \nImages courtesy of Naughty Dog Inc.\n11.6.6.2. Locomotion Noise\n Real humans don’t run exactly the same way with every footfall—there is \nvariation in their movement over time. This is especially true if the person \nis distracted (for example, by att acking enemies). Additive blending can be \nused to layer randomness, or reactions to distractions, on top of an otherwise \nentirely repetitive locomotion cycle. This is illustrated in Figure 11.39.\n11.6.6.3. Aim and Look-At\nAnother common use for additive blending is to permit the character to look \naround or to aim his weapon. To accomplish this, the character is ﬁ rst ani-\nmated doing some action, such as running, with his head or weapon facing \nstraight ahead. Then the animator changes the direction of the head or the \naim of the weapon to the extreme right and saves oﬀ  a one-frame or multi-\nframe diﬀ erence animation. This process is repeated for the extreme left , up, \nand down directions. These four diﬀ erence animations can then be additively \nblended onto the original straight ahead animation clip, causing the character \nto aim right, left , up, down, or anywhere in between.\nThe angle of the aim is governed by the additive blend factor of each clip. \nFor example, blending in 100 percent of the right additive causes the character \n11.6. Animation Blending\n",
      "content_length": 1522,
      "extraction_method": "Direct"
    },
    {
      "page_number": 564,
      "chapter": null,
      "content": "542 \n11. Animation Systems\nto aim as far right as possible. Blending 50 percent of the left  additive causes \nhim to aim at an angle that is one-half of his left most aim. We can also combine \nthis with an up or down additive to aim diagonally. This is demonstrated in \nFigure 11.40.\n11.6.6.4. Overloading the Time Axis\nIt’s interesting to note that the time axis of an animation clip needn’t be used \nto represent time. For example, a three-frame animation clip could be used to \nprovide three aim poses to the engine—a left  aim pose on frame 1, a forward \naim pose on frame 2, and a right aim pose on frame 3. To make the character \naim to the right, we can simply ﬁ x the local clock of the aim animation on \nframe 3. To perform a 50% blend between aiming forward and aiming right, \nwe can dial in frame 2.5. This is a great example of leveraging existing features \nof the engine for new purposes.\n11.7. Post-Processing\nOnce a skeleton as been posed by one or more animation clips and the results \nhave been blended together using linear interpolation or additive blending, it \nis oft en necessary to modify the pose prior to rendering the character. This is \ncalled animation post-processing . In this section, we’ll look at a few of the most \ncommon kinds of animation post-processing.\nTarget +\nDifference Right\nTarget +\nDifference Left\nTarget Clip\n(and Reference)\n0% Right\n0% Left\n100% Right\n100% Left\nFigure 11.40.  Additive blending can be used to aim a weapon. Screenshots courtesy of \nNaughty Dog Inc.\n",
      "content_length": 1513,
      "extraction_method": "Direct"
    },
    {
      "page_number": 565,
      "chapter": null,
      "content": "543 \n11.7.1. Procedural Animations\nA procedural animation is any animation generated at runtime rather than be-\ning driven by data exported from an animation tool such as Maya. Sometimes, \nhand-animated clips are used to pose the skeleton initially, and then the pose \nis modiﬁ ed in some way via procedural animation as a post-processing step. \nA procedural animation can also be used as an input to the system in place of \na hand-animated clip.\nFor example, imagine that a regular animation clip is used to make a ve-\nhicle appear to be bouncing up and down on the terrain as it moves. The \ndirection in which the vehicle travels is under player control. We would like \nto adjust the rotation of the front wheels and steering wheel so that they move \nconvincingly when the vehicle is turning. This can be done by post-processing \nthe pose generated by the animation. Let’s assume that the original animation \nhas the front tires pointing straight ahead and the steering wheel in a neutral \nposition. We can use the current angle of turn to create a quaternion about the \nvertical axis that will deﬂ ect the front tires by the desired amount. This quater-\nnion can be multiplied with the front tire joints’ Q channel to produce the ﬁ nal \npose of the tires. Likewise, we can generate a quaternion about the axis of the \nsteering column and multiply it in to the steering wheel joint’s Q channel to \ndeﬂ ect it. These adjustments are made to the local pose, prior to global pose \ncalculation and matrix palett e generation.\nAs another example, let’s say that we wish to make the trees and bushes in \nour game world sway naturally in the wind and get brushed aside when char-\nacters move through them. We can do this by modeling the trees and bushes as \nskinned meshes with simple skeletons. Procedural animation can be used, in \nplace of or in addition to hand-animated clips, to cause the joints to move in a \nnatural-looking way. We might apply one or more sinusoids to the rotation of \nvarious joints to make them sway in the breeze, and when a character moves \nthrough a region containing a bush or grass, we can deﬂ ect its root joint quater-\nnion radially outward to make it appear to be pushed over by the character.\n11.7.2. Inverse Kinematics\nLet’s say we have an animation clip in which a character leans over to pick up \nan object from the ground. In Maya, the clip looks great, but in our production \ngame level, the ground is not perfectly ﬂ at, so sometimes the character’s hand \nmisses the object or appears to pass through it. In this case, we would like to \nadjust the ﬁ nal pose of the skeleton so that the hand lines up exactly with the \ntarget object. A technique known as inverse kinematics (IK) can be used to make \nthis happen.\n11.7. Post-Processing\n",
      "content_length": 2772,
      "extraction_method": "Direct"
    },
    {
      "page_number": 566,
      "chapter": null,
      "content": "544 \n11. Animation Systems\nA regular animation clip is an example of forward kinematics (FK). In for-\nward kinematics, the input is a set of local joint poses, and the output is a \nglobal pose and a skinning matrix for each joint. Inverse kinematics goes in \nthe other direction: The input is the desired global pose of a single joint, which \nis known as the end eﬀ ector . We solve for the local poses of other joints in the \nskeleton that will bring the end eﬀ ector to the desired location.\nMathematically, IK boils down to an error minimization problem. As with \nmost minimization problems, there might be one solution, many, or none at \nall. This makes intuitive sense: If I try to reach a doorknob that is on the other \nside of the room, I won’t be able to reach it without walking over to it. IK \nworks best when the skeleton starts out in a pose that is reasonably close to the \ndesired target. This helps the algorithm to focus in on the “closest” solution \nand to do so in a reasonable amount of processing time. Figure 11.41 shows \nIK in action.\nImagine a two-joint skeleton, each of which can rotate only about a single \naxis. The rotation of these two joints can be described by a two-dimensional \nangle vector θ = [ θ1  θ2 ]. The set of all possible angles for our two joints forms \na two-dimensional space called conﬁ guration space . Obviously, for more-com-\nplex skeletons with more degrees of freedom per joint, conﬁ guration space be-\ncomes multidimensional, but the concepts described here work equally well \nno matt er how many dimensions we have.\nNow imagine plott ing a three-dimensional graph, where for each combi-\nnation of joint rotations (i.e., for each point in our two-dimensional conﬁ gura-\ntion space), we plot the distance from the end eﬀ ector to the desired target. \nAn example of this kind of plot is shown in Figure 11.42. The “valleys” in \nthis three-dimensional surface represent regions in which the end eﬀ ector is \nas close as possible to the target. When the height of the surface is zero, the \nend eﬀ ector has reached its target. Inverse kinematics, then, att empts to ﬁ nd \nminima (low points) on this surface.\nTarget\nPose \nAfter IK\nOriginal \nPose\nEnd \nEffector\nFigure 11.41.  Inverse kinematics attempts to bring an end effector joint into a target global \npose by minimizing the error between them.\n",
      "content_length": 2347,
      "extraction_method": "Direct"
    },
    {
      "page_number": 567,
      "chapter": null,
      "content": "545 \nWe won’t get into the details of solving the IK minimization problem here. \nYou can read more about IK at htt p://en.wikipedia.org/wiki/Inverse_kinemat-\nics and in Jason Weber’s article, “Constrained Inverse Kinematics,” in [40].\n11.7.3. Rag Dolls\nA character’s body goes limp when he dies or becomes unconscious. In such \nsituations, we want the body to react in a physically realistic way with its \nsurroundings. To do this, we can use a rag doll . A rag doll is a collection of \nphysically simulated rigid bodies , each one representing a semi-rigid part of \nthe character’s body, such as his lower arm or his upper leg. The rigid bodies \nare constrained to one another at the joints of the character in such a way as to \nproduce natural-looking “lifeless” body movement. The positions and orien-\ntations of the rigid bodies are determined by the physics system and are then \nused to drive the positions and orientations of certain key joints in the charac-\nter’s skeleton. The transfer of data from the physics system to the skeleton is \ntypically done as a post-processing step.\nTo really understand rag doll physics, we must ﬁ rst have an understand-\ning of how the collision and physics systems work. Rag dolls are covered in \nmore detail in Sections 12.4.8.7 and 12.5.3.8.\n11.8. Compression Techniques\nAnimation data can take up a lot of memory. A single joint pose might be \ncomposed of ten ﬂ oating-point channels (three for translation, four for rota-\ntion, and up to three more for scale). Assuming each channel contains a four-\nθ 1\nθ 2\ndtarget\nMinimum\nFigure 11.42.  A three-dimensional plot of the distance from the end effector to the target for \neach point in two-dimensional conﬁ guration space. IK ﬁ nds the local minimum.\n11.8. Compression Techniques\n",
      "content_length": 1775,
      "extraction_method": "Direct"
    },
    {
      "page_number": 568,
      "chapter": null,
      "content": "546 \n11. Animation Systems\nbyte ﬂ oating-point value, a one-second clip sampled at 30 samples per second \nwould occupy 4 bytes × 10 channels × 30 samples/second = 1200 bytes per joint \nper second, or a data rate of about 1.17 kB per joint per second. For a 100-joint \nskeleton (which is small by today’s standards), an uncompressed animation \nclip would occupy 117 kB per joint per second. If our game contained 1000 \nseconds of animation (which is on the low side for a modern game), the entire \ndata set would occupy a whopping 114.4 MB. That’s probably more than most \ngames can spare, considering that a PLAYSTATION 3 has only 256 MB of main \nRAM and 256 MB of video RAM. Therefore, game engineers invest a signiﬁ -\ncant amount of eﬀ ort into compressing animation data in order to permit the \nmaximum richness and variety of movement at the minimum memory cost.\n11.8.1. Channel Omission\nOne simple way to reduce the size of an animation clip is to omit channels \nthat are irrelevant. Many characters do not require nonuniform scaling, so the \nthree scale channels can be reduced to a single uniform scale channel. In some \ngames, the scale channel can actually be omitt ed altogether for all joints (ex-\ncept possibly the joints in the face). The bones of a humanoid character gener-\nally cannot stretch, so translation can oft en be omitt ed for all joints except the \nroot, the facial joints, and sometimes the collar bones. Finally, because quater-\nnions are always normalized, we can store only three components per quat \n(e.g., x, y, and z) and reconstruct the fourth component (e.g., w) at runtime.\nAs a further optimization, channels whose pose does not change over the \ncourse of the entire animation can be stored as a single sample at time t = 0 plus \na single bit indicating that the channel is constant for all other values of t.\nChannel omission can signiﬁ cantly reduce the size of an animation clip. \nA 100-joint character with no scale and no translation requires only 303 chan-\nnels—three channels for the quaternions at each joint, plus three channels for \nthe root joint’s translation. Compare this to the 1,000 channels that would be \nrequired if all ten channels were included for all 100 joints.\n11.8.2. Quantization\nAnother way to reduce the size of an animation is to reduce the size of each \nchannel. A ﬂ oating-point value is normally stored in 32-bit IEEE format. This \nformat provides 23 bits of precision in the mantissa and an 8-bit exponent. \nHowever, it’s oft en not necessary to retain that kind of precision and range in \nan animation clip. When storing a quaternion, the channel values are guaran-\nteed to lie in the range [–1, 1]. At a magnitude of 1, the exponent of a 32-bit \nIEEE ﬂ oat is zero, and 23 bits of precision give us accuracy down to the sev-\nenth decimal place. Experience shows that a quaternion can be encoded well \n",
      "content_length": 2874,
      "extraction_method": "Direct"
    },
    {
      "page_number": 569,
      "chapter": null,
      "content": "547 \nwith only 16 bits of precision, so we’re really wasting 16 bits per channel if we \nstore our quats using 32-bit ﬂ oats.\nConverting a 32-bit IEEE ﬂ oat into an n-bit integer representation is called \nquantization . There are actually two components to this operation: Encoding \nis the process of converting the original ﬂ oating-point value to a quantized \ninteger representation. Decoding is the process of recovering an approxima-\ntion to the original ﬂ oating-point value from the quantized integer. (We can \nonly recover an approximation to the original data—quantization is a lossy com-\npression method because it eﬀ ectively reduces the number of bits of precision \nused to represent the value.)\nTo encode a ﬂ oating-point value as an integer, we ﬁ rst divide the valid \nrange of possible input values into N equally sized intervals. We then deter-\nmine within which interval a particular ﬂ oating-point value lies and represent \nthat value by the integer index of its interval. To decode this quantized value, \nwe simply convert the integer index into ﬂ oating-point format and shift  and \nscale it back into the original range. N is usually chosen to correspond to the \nrange of possible integer values that can be represented by an n-bit integer. \nFor example, if we’re encoding a 32-bit ﬂ oating-point value as a 16-bit integer, \nthe number of intervals would be N = 216 = 65,536.\nJonathan Blow wrote an excellent article on the topic of ﬂ oating-point sca-\nlar quantization in the Inner Product column of Game Developer  Magazine, \navailable at htt p://number-none.com/product/Scalar%20Quantization/index.\nhtml. (Jonathan’s source code is also available at htt p://www.gdmag.com/\nsrc/jun02.zip.) The article presents two ways to map a ﬂ oating-point value \nto an interval during the encoding process: We can either truncate the ﬂ oat \nto the next lowest interval boundary (T encoding), or we can round the ﬂ oat \nto the center of the enclosing interval (R encoding). Likewise, it describes two \napproaches to reconstructing the ﬂ oating-point value from its integer repre-\nsentation: We can either return the value of the left hand side of the interval to \nwhich our original value was mapped (L reconstruction), or we can return the \nvalue of the center of the interval (C reconstruction). This gives us four possible \nencode/decode methods: TL, TC, RL, and RC. Of these, TL and RC are to be \navoided because they tend to remove or add energy to the data set, which can \noft en have disastrous eﬀ ects. TC has the beneﬁ t of being the most eﬃ  cient \nmethod in terms of bandwidth, but it suﬀ ers from a severe problem—there \nis no way to represent the value zero exactly. (If you encode 0.0f, it becomes \na small positive value when decoded.) RL is therefore usually the best choice \nand is the method we’ll demonstrate here.\nThe article only talks about quantizing positive ﬂ oating-point values, and \nin the examples, the input range is assumed to be [0, 1] for simplicity. Howev-\n11.8. Compression Techniques\n",
      "content_length": 3028,
      "extraction_method": "Direct"
    },
    {
      "page_number": 570,
      "chapter": null,
      "content": "548 \n11. Animation Systems\ner, we can always shift  and scale any ﬂ oating-point range into the range [0, 1]. \nFor example, the range of quaternion channels is [–1, 1], but we can convert \nthis to the range [0, 1] by adding one and then dividing by two.\nThe following pair of routines encode and decode an input ﬂ oating-point \nvalue lying in the range [0, 1] into an n-bit integer, according to Jonathan \nBlow’s RL method. The quantized value is always returned as a 32-bit un-\nsigned integer (U32), but only the least-signiﬁ cant n bits are actually used, as \nspeciﬁ ed by the nBits argument. For example, if you pass nBits==16, you \ncan safely cast the result to a U16.\nU32 CompressUnitFloatRL(F32 unitFloat, U32 nBits)\n{\n \n// Determine the number of intervals based on the  \n \n \n// number of output bits we’ve been asked to produce.\n U32 \nnIntervals = 1u << nBits;\n \n// Scale the input value from the range [0, 1] into   \n \n// the range [0, nIntervals – 1]. We subtract one  \n \n \n// interval because we want the largest output value  \n \n// to fit into nBits bits.\n F32 \nscaled = unitFloat * (F32)(nIntervals – 1u);\n \n// Finally, round to the nearest interval center. We   \n \n// do this by adding 0.5f, and then truncating to the  \n \n// next-lowest interval index (by casting to U32).\n U32 \nrounded = (U32)(scaled * 0.5f);\n \n// Guard against invalid input values.\nif (rounded > nIntervals – 1u)\n \n \nrounded = nIntervals – 1u;\n return \nrounded;\n}\nF32 DecompressUnitFloatRL(U32 quantized, U32 nBits)\n{\n \n// Determine the number of intervals based on the  \n \n \n// number of bits we used when we encoded the value.\n U32 \nnIntervals = 1u << nBits;\n \n// Decode by simply converting the U32 to an F32, and\n \n// scaling by the interval size.\n F32 \nintervalSize = 1.0f / (F32)(nIntervals – 1u);\n F32 \napproxUnitFloat = (F32)quantized * intervalSize;\n return \napproxUnitFloat;\n}\n",
      "content_length": 1872,
      "extraction_method": "Direct"
    },
    {
      "page_number": 571,
      "chapter": null,
      "content": "549 \nTo handle arbitrary input values in the range [min, max], we can use these \nroutines:\nU32 CompressFloatRL(F32 value, F32 min, F32 max,\n      U32 \nnBits)\n{\n \nF32 unitFloat = (value - min) / (max – min);\n \nU32 quantized = CompressUnitFloatRL(unitFloat,  \n \n \n  nBits);\n \nreturn quantized;\n}\nF32 DecompressFloatRL(U32 quantized, F32 min, F32 max,\n                      U32 nBits)\n{\n \nF32 unitFloat = DecompressUnitFloatRL(quantized,   \n \n  nBits);\n \nF32 value = min + (unitFloat * (max – min));\n \nreturn value;\n}\nLet’s return to our original problem of animation channel compression. \nTo compress and decompress a quaternion’s four components into 16 bits per \nchannel, we simply call CompressFloatRL() and DecompressFloatRL()\nwith min = –1, max = 1, and n = 16:\ninline U16 CompressRotationChannel(F32 qx)\n{\n \nreturn (U16)CompressFloatRL(qx, -1.0f, 1.0f, 16u);\n}\ninline F32 DecompressRotationChannel(U16 qx)\n{\n return \nDecompressFloatRL((U32)qx, -1.0f, 1.0f, 16u);\n}\nCompression of translation channels is a bit trickier than rotations, be-\ncause unlike quaternion channels, the range of a translation channel could \ntheoretically be unbounded. Thankfully, the joints of a character don’t move \nvery far in practice, so we can decide upon a reasonable range of motion and \nﬂ ag an error if we ever see an animation that contains translations outside the \nvalid range. In-game cinematics are an exception to this rule—when an IGC \nis animated in world space, the translations of the characters’ root joints can \ngrow very large. To address this, we can select the range of valid translations \non a per-animation or per-joint basis, depending on the maximum transla-\ntions actually achieved within each clip. Because the data range might diﬀ er \n11.8. Compression Techniques\n",
      "content_length": 1775,
      "extraction_method": "Direct"
    },
    {
      "page_number": 572,
      "chapter": null,
      "content": "550 \n11. Animation Systems\nfrom animation to animation, or from joint to joint, we must store the range \nwith the compressed clip data. This will add data to each animation, so it may \nor may not be worth the trade-oﬀ .\n// We’ll use a 2 meter range -- your mileage may vary.\nF32 MAX_TRANSLATION = 2.0f;\ninline U16 CompressTranslationChannel(F32 vx)\n{\n \n// Clamp to valid range...\n \nif (value < -MAX_TRANSLATION)\n \n  \nvalue = -MAX_TRANSLATION;\n \nif (value > MAX_TRANSLATION)\n \n  \nvalue = MAX_TRANSLATION;\n \nreturn (U16)CompressFloatRL(vx,\n \n  \n-MAX_TRANSLATION, MAX_TRANSLATION, 16);\n}\ninline F32 DecompressTranslationChannel(U16 vx)\n{\n return \nDecompressFloatRL((U32)vx,\n \n  \n-MAX_TRANSLATION, MAX_TRANSLATION, 16);\n}\n11.8.3. Sampling Frequency and Key Omission\nAnimation data tends to be large for three reasons: ﬁ rst, because the pose of \neach joint can contain upwards of ten channels of ﬂ oating-point data; second, \nbecause a skeleton contains a large number of joints (100 or more for a human-\noid character); third, because the pose of the character is typically sampled \nat a high rate (e.g., 30 frames per second). We’ve seen some ways to address \nthe ﬁ rst problem. We can’t really reduce the number of joints for our high-\nresolution characters, so we’re stuck with the second problem. To att ack the \nthird problem, we can do two things:\nReduce the sample rate overall.\n• \n Some animations look ﬁ ne when exported \nat 15 samples per second, and doing so cuts the animation data size in \nhalf.\nOmit some of the samples.\n• \n If a channel’s data varies in an approximately \nlinear fashion during some interval of time within the clip, we can omit \nall of the samples in this interval except the endpoints. Then, at runtime, \nwe can use linear interpolation to recover the dropped samples.\nThe latt er technique is a bit involved, and it requires us to store informa-\ntion about the time of each sample. This additional data can erode the savings \n",
      "content_length": 1957,
      "extraction_method": "Direct"
    },
    {
      "page_number": 573,
      "chapter": null,
      "content": "551 \nwe achieved by omitt ing samples in the ﬁ rst place. However, some game en-\ngines have used this technique successfully.\n11.8.4. Curve-Based Compression\n One of the most powerful, easiest-to-use, and best thought-out animation \nAPIs I’ve ever worked with is Granny , by Rad Game Tools. Granny stores \nanimations not as a regularly spaced sequence of pose samples but as a collec-\ntion of nth-order  nonuniform, nonrational B-splines, describing the paths of a \njoint’s S, Q, and T channels over time. Using B-splines allows channels with a \nlot of curvature to be encoded using only a few data points.\nGranny exports an animation by sampling the joint poses at regular in-\ntervals, much like traditional animation data. For each channel, Granny then \nﬁ ts a set of B-splines to the sampled data set to within a user-speciﬁ ed toler-\nance. The end result is an animation clip that is usually signiﬁ cantly smaller \nthan its uniformly sampled, linearly interpolated counterpart. This process is \nillustrated in Figure 11.43.\nB-spline \nsegment 1\nt\nQx1\nSamples\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nB-spline \nsegment 2\nFigure 11.43.  One form of animation compression ﬁ ts B-splines to the animation channel \ndata.\n11.8.5. Selective Loading and Streaming \n The cheapest animation clip is the one that isn’t in memory at all. Most games \ndon’t need every animation clip to be in memory simultaneously. Some clips \napply only to certain classes of character, so they needn’t be loaded during lev-\nels in which that class of character is never encountered. Other clips apply to \none-oﬀ  moments in the game. These can be loaded or streamed into memory \njust before being needed and dumped from memory once they have played.\nMost games load a core set of animation clips into memory when the game \nﬁ rst boots and keep them there for the duration of the game. These include \nthe player character’s core move set and animations that apply to objects that \nreappear over and over throughout the game, such as weapons or power-ups. \n11.8. Compression Techniques\n",
      "content_length": 2041,
      "extraction_method": "Direct"
    },
    {
      "page_number": 574,
      "chapter": null,
      "content": "552 \n11. Animation Systems\nAll other animations are usually loaded on an as-needed basis. Some game \nengines load animation clips individually, but many package them together \ninto logical groups that can be loaded and unloaded as a unit.\n11.9. Animation System Architecture\nNow that we understand the theory that underlies a game’s animation system, \nlet’s turn our att ention to how such a system is structured from a soft ware ar-\nchitecture standpoint. We’ll also investigate what kinds of interfaces exist be-\ntween the animation system and the other systems in a typical game engine.\nMost animation systems are comprised of up to three distinct layers:\nAnimation pipeline.\n• \n For each animating character and object in the game, \nthe animation pipeline takes one or more animation clips and corre-\nsponding blend factors as input, blends them together, and generates a \nsingle local skeletal pose as output. It also calculates a global pose for \nthe skeleton, and a palett e of skinning matrices for use by the rendering \nengine. Post-processing hooks are usually provided, which permit the \nlocal pose to be modiﬁ ed prior to ﬁ nal global pose and matrix palett e \ngeneration. This is where inverse kinematics (IK), rag doll physics, and \nother forms of procedural animation are applied to the skeleton.\nAction state machine (ASM).\n• \n The actions of a game character (standing, \nwalking, running, jumping, etc.) are usually best modeled via a ﬁ nite \nstate machine , commonly known as the action state machine (ASM). The \nASM subsystem sits atop the animation pipeline and provides a state-\ndriven animation interface for use by virtually all higher-level game \ncode. It ensures that characters can transition smoothly from state to \nstate. In addition, most animation engines permit diﬀ erent parts of the \ncharacter’s body to be doing diﬀ erent, independent actions simultane-\nously, such as aiming and ﬁ ring a weapon while running. This can be ac-\ncomplished by allowing multiple independent state machines to control \na single character via state layers.\nAnimation controllers.\n• \n In many game engines, the behaviors of a player \nor non-player character are ultimately controlled by a high-level sys-\ntem of animation controllers. Each controller is custom-tailored to man-\nage the character’s behavior when in a particular mode. There might \nbe one controller handling the character’s actions when he is ﬁ ghting \nand moving around out in the open (“run-and-gun” mode), one for \nwhen he is in cover, one for driving a vehicle, one for climbing a lad-\nder, and so on. These high-level animation controllers allow most if not \n",
      "content_length": 2641,
      "extraction_method": "Direct"
    },
    {
      "page_number": 575,
      "chapter": null,
      "content": "553 \nall of the animation-related code to be encapsulated, allowing top-level \nplayer control or AI logic to remain unclutt ered by animation micro-\nmanagement.\nSome game engines draw the lines between these layers diﬀ erently than \nwe do here. Other engines meld two or more of the layers into a single system. \nHowever, all animation engines need to perform these tasks in one form or an-\nother. In the following sections, we’ll explore animation architecture in terms \nof these three layers, noting in our examples when a particular game engine \ntakes a more or less uniﬁ ed approach.\n11.10. The Animation Pipeline\n The operations performed by the low-level animation engine form a pipeline \nthat transforms its inputs (animation clips and blend speciﬁ cations) into the \ndesired outputs (local and global poses, plus a matrix palett e for rendering). \nThe stages of this pipeline are:\n \n1. Clip decompression and pose extraction . In this stage, each individual clip’s \ndata is decompressed, and a static pose is extracted for the time index in \nquestion. The output of this phase is a local skeletal pose for each input \nclip. This pose might contain information for every joint in the skeleton \n(a full-body pose), for only a subset of joints (a partial pose), or it might be \na diﬀ erence pose for use in additive blending.\n \n2. Pose blending . In this stage, the input poses are combined via full-body \nLERP blending, partial-skeleton LERP blending, and/or additive blend-\ning. The output of this stage is a single local pose for all joints in the \nskeleton. This stage is of course only executed when blending more than \none animation clip together—otherwise the output pose from stage 1 \ncan be used directly.\n \n3. Global pose generation. In this stage, the skeletal hierarchy is walked, and \nlocal joint poses are concatenated in order to generate a global pose for \nthe skeleton.\n \n4. Post-processing . In this optional stage, the local and/or global poses of \nthe skeleton can be modiﬁ ed prior to ﬁ nalization of the pose. Post-pro-\ncessing is used for inverse kinematics , rag doll physics, and other forms \nof procedural animation adjustment.\n \n5. Recalculation of global poses. Many types of post-processing require glob-\nal pose information as input but generate local poses as output. Aft er \nsuch a post-processing step has run, we must recalculate the global pose \n11.10. The Animation Pipeline\n",
      "content_length": 2416,
      "extraction_method": "Direct"
    },
    {
      "page_number": 576,
      "chapter": null,
      "content": "554 \n11. Animation Systems\nfrom the modiﬁ ed local pose. Obviously, a post-processing operation \nthat does not require global pose information can be done between stag-\nes 2 and 3, thus avoiding the need for global pose recalculation.\n \n6. Matrix palett e generation. Once the ﬁ nal global pose has been generated, \neach joint’s global pose matrix is multiplied by the corresponding in-\nverse bind pose matrix. The output of this stage is a palett e of skinning \nmatrices suitable for input to the rendering engine.\nA typical animation pipeline is depicted in Figure 11.44.\n11.10.1. Data Structures\nEvery animation pipeline is architected diﬀ erently, but most operate in terms \nof data structures that are similar to the ones described in this section.\n11.10.1.1. Shared Resource Data\nAs with all game engine systems, a strong distinction must be made between \nshared resource data and per-instance state information. Each individual character \nor object in the game has its own per-instance data structures, but characters \nor objects of the same type typically share a single set of resource data. This \nshared data typically includes the following:\nSkeleton\n• \n . The skeleton describes the joint hierarchy and its bind pose.\nSkinned meshes\n• \n . One or more meshes can be skinned to a single skeleton. \nEach vertex within a skinned mesh contains the indices of one or more \nOutputs\nInputs\nDecompression\nand\nPose Extraction\nBlend\nSpecification\nPose \nBlending\nSkinning \nMatrix \nCalc.\nGlobal \nPose Calc.\nLocal \nPose\nRendering \nEngine\nMatrix \nPalette\nPost-\nProcessing\nSkeleton\nClip(s)\nLocal \nClock(s)\nGlobal \nPose\nGame Play \nSystems\nFigure 11.44.  A typical animation pipeline.\n",
      "content_length": 1679,
      "extraction_method": "Direct"
    },
    {
      "page_number": 577,
      "chapter": null,
      "content": "555 \njoints within the skeleton, plus weights governing how much inﬂ uence \neach joint should have on that vertex’s position.\nAnimation clips\n• \n . Many hundreds or even thousands of animation clips \nare created for a character’s skeleton. These may be full-body clips, par-\ntial-skeleton clips, or diﬀ erence clips for use in additive blending.\nA UML diagram of these data structures is shown in Figure 11.45. Pay \nparticular att ention to the cardinality and direction of the relationships between \nthese classes. The cardinality is shown just beside the tip or tail of the relation-\nship arrow between classes—a one represents a single instance of the class, \nwhile an asterisk indicates many instances. For any one type of character, there \nwill be one skeleton, one or more meshes, and one or more animation clips. \nThe skeleton is the central unifying element—the skins are att ached to the \nskeleton but don’t have any relationship with the animation clips. Likewise, \nthe clips are targeted at a particular skeleton, but they have no “knowledge” \nof the skin meshes. Figure 11.46 illustrates these relationships.\nGame designers oft en try to reduce the number of unique skeletons in \nthe game to one, or just a few, because each new skeleton generally requires \na whole new set of animation clips. To provide the illusion of many diﬀ erent \n1\n*\n1\n*\n1\n*\nSkeleton\n-uniqueId : int\n-jointCount : int\n-joints : SkeletonJoint\nSkeletonJoint\n-name : string\n-parentIndex : int\n-invBindPose : Matrix44\n1\n*\n1\n*\n1\n*\nMesh\n-indices : int\n-vertices : Vertex\n-skeletonId : int\nAnimationClip\n-nameId : int\n-duration : float\n-poseSamples : AnimationPose\nVertex\n-position : Vector3\n-normal : Vector3\n-uv : Vector2\n-jointIndices : int\n-jointWeights : float\nSQT\n-scale : Vector3\n-rotation : Quaternion\n-translation : Vector3\nAnimationPose\n-jointPoses : SQT\nFigure 11.45.  UML diagram of shared animation resources.\n11.10. The Animation Pipeline\n",
      "content_length": 1932,
      "extraction_method": "Direct"
    },
    {
      "page_number": 578,
      "chapter": null,
      "content": "556 \n11. Animation Systems\ntypes of characters, it is usually bett er to create multiple meshes skinned to the \nsame skeleton when possible, so that all of the characters can share a single \nset of animations.\n11.10.1.2. Per-Instance Data\n In most games, multiple instances of each character type can appear on-screen \nat the same time. Every instance of a particular character type needs its own \nprivate data structures, allowing it to keep track of its currently playing ani-\nmation clip(s), a speciﬁ cation of how the clips are to be blended together (if \nthere’s more than one), and its current skeletal pose.\nThere is no one universally accepted way to represent per-instance ani-\nmation data. However, virtually every animation engine keeps track of the \nfollowing pieces of information.\nClip state\n• \n . For each playing clip, the following information is main-\ntained:\nLocal clock\n \n□\n . A clip’s local clock describes the point along its lo-\ncal time line at which its current pose should be extracted. This \nmay be replaced by a global start time in some engines. (A com-\nparison between local and global clocks was provided in Sec-\ntion 11.4.3.)\nPlayback rate\n \n□\n . A clip can be played at an arbitrary rate, denoted R in \nSection 11.4.2.\nBlend speciﬁ cation\n• \n . The blend speciﬁ cation is a description of which ani-\nmation clips are currently playing and how these clips are to be blended \ntogether. The degree to which each clip contributes to the ﬁ nal pose is \nSkeleton\nClip N\n...\nSkin A\nSkin B\nSkin C\nClip 1\nClip 2\nClip 3\nother skeletons...\n...\n...\nFigure 11.46.  Many animation clips and one or more meshes target a single skeleton.\n",
      "content_length": 1655,
      "extraction_method": "Direct"
    },
    {
      "page_number": 579,
      "chapter": null,
      "content": "557 \ncontrolled by one or more blend weights. There are two primary meth-\nods of describing the set of clips that should be blended together: a ﬂ at \nweighted average approach and a tree of blend nodes. When the tree ap-\nproach is used, the structure of the blend tree is usually treated as a \nshared resource, while the blend weights are stored as part of the per-\ninstance state information.\nPartial-skeleton joint weights.\n• \n If a partial-skeleton blend is to be per-\nformed, the degrees to which each joint should contribute to the ﬁ nal \npose are speciﬁ ed via a set of joint weights . In some animation engines, \nthe joint weights are binary: either a joint contributes or it does not. In \nother engines, the weights can lie anywhere from zero (no contribution) \nto one (full contribution).\nLocal pose\n• \n . This is typically an array of SQT data structures, one per joint, \nholding the ﬁ nal pose of the skeleton in parent-relative format. This ar-\nray might also be reused to store an intermediate pose that serves both \nas the input to and the output of the post-processing stage of the pipe-\nline.\nGlobal pose\n• \n . This is an array of SQTs, or 4 × 4 or 4 × 3 matrices, one per \njoint, that holds the ﬁ nal pose of the skeleton in model-space or world-\nspace format. The global pose may serve as an input to the post-pro-\ncessing stage.\nMatrix palett e\n• \n . This is an array of 4 × 4 or 4 × 3 matrices, one per joint, \ncontaining skinning matrices for input to the rendering engine.\n11.10.2. The Flat Weighted Average Blend Representation\nAll but the most rudimentary game engines support animation blending in \nsome form. This means that at any given time, multiple animation clips may \nbe contributing to the ﬁ nal pose of a character’s skeleton. One simple way to \ndescribe how the currently active clips should be blended together is via a \nweighted a verage .\nIn this approach, every animation clip is associated with a blend weight \nindicating how much it should contribute to the ﬁ nal pose of the charac-\nter. A ﬂ at list of all active animation clips (i.e., clips whose blend weights \nare non-zero) is maintained. To calculate the ﬁ nal pose of the skeleton, we \nextract a pose at the appropriate time index for each of the N active clips. \nThen, for each joint of the skeleton, we calculate a simple N-point weighted \naverage of the translation vectors, rotation quaternions, and scale factors \nextracted from the N active animations. This yields the ﬁ nal pose of the \nskeleton.\n11.10. The Animation Pipeline\n",
      "content_length": 2532,
      "extraction_method": "Direct"
    },
    {
      "page_number": 580,
      "chapter": null,
      "content": "558 \n11. Animation Systems\nThe equation for the weighted average of a set of N vectors { vi } is as fol-\nlows:\n \n1\n0\navg\n1\n0\n.\nN\ni\ni\ni\nN\ni\ni\nw\nw\n−\n=\n−\n=\n=\n∑\n∑\nv\nv\n \nIf the weights are normalized, meaning they sum to one, then this equation can \nbe simpliﬁ ed to the following:\n \n1\n1\navg\n0\n0\n  when \n1 .\nN\nN\ni\ni\ni\ni\ni\nw\nw\n−\n−\n=\n=\n⎛\n⎞\n⎜\n⎟\n=\n=\n⎜\n⎟\n⎝\n⎠\n∑\n∑\nv\nv\n \nIn the case of N = 2, if we let w1 = β and w0 = (1 – β), the weighted average \nreduces to the familiar equation for the linear interpolation (LERP) between \ntwo vectors:\n \nLERP\nLERP[\n, \n, ]\n(1\n)\n.\nA\nB\nA\nB\n=\nβ\n=\n−β\n+ β\nv\nv\nv\nv\nv\n \nWe can apply this same weighted average formulation equally well to quater-\nnions by simply treating them as four-element vectors.\n11.10.2.1. Example: Ogre3D\nThe Ogre3D animation system works in exactly this way. An Ogre::Entity\nrepresents an instance of a 3D mesh (e.g., one particular character walk-\ning around in the game world). The Entity aggregates an object called \nan Ogre::AnimationStateSet, which in turn maintains a list of \nOgre::AnimationState objects, one for each active animation. The \nOgre::AnimationState class is shown in the code snippet below. (A few \nirrelevant details have been omitt ed for clarity.)\n/** Represents the state of an animation clip and the  \n \n \n weight of its influence on the overall pose of the   \n \n character.\n*/\nclass AnimationState\n{\nprotected:\n    String             mAnimationName; // reference to   \n            \n   // clip\n    Real               mTimePos;       // local clock\n    Real               mWeight;        // blend weight\n    bool               mEnabled;       // is this anim   \n            \n   // running?\n",
      "content_length": 1658,
      "extraction_method": "Direct"
    },
    {
      "page_number": 581,
      "chapter": null,
      "content": "559 \n    bool               mLoop;          // should the\n            \n    // \nanim loop?\npublic:\n    /// Gets the name of the animation.\n    const String& getAnimationName() const;\n    /// Gets the time position (local clock) for this  \n \n   \n/// anim.\n    Real getTimePosition(void) const;\n    /// Sets the time position (local clock) for this  \n \n  /// anim.\n    void setTimePosition(Real timePos);\n    /// Gets the weight (influence) of this animation\n    Real getWeight(void) const;\n    /// Sets the weight (influence) of this animation\n    void setWeight(Real weight);\n    /// Modifies the time position, adjusting for  \n \n \n     \n/// animation duration. This method loops if looping\n  /// \nis enabled.\n    void addTime(Real offset);\n    /// Returns true if the animation has reached the  \n \n /// end of local time line, and is not looping.\n    bool hasEnded(void) const;\n    /// Returns true if this animation is currently   \n \n  /// \nenabled.\n    bool getEnabled(void) const;\n    /// Sets whether or not this animation is enabled.\n    void setEnabled(bool enabled);\n    /// Sets whether or not this animation should loop.\n    void setLoop(bool loop) { mLoop = loop; }\n    /// Gets whether or not this animation loops.\n    bool getLoop(void) const { return mLoop; }\n};\nEach AnimationState keeps track of one animation clip’s local clock and \nits blend weight. When calculating the ﬁ nal pose of the skeleton for a particu-\nlar Ogre::Entity, Ogre’s animation system simply loops through each active \n11.10. The Animation Pipeline\n",
      "content_length": 1536,
      "extraction_method": "Direct"
    },
    {
      "page_number": 582,
      "chapter": null,
      "content": "560 \n11. Animation Systems\nAnimationState in its AnimationStateSet. A skeletal pose is extracted \nfrom the animation clip corresponding to each state at the time index speciﬁ ed \nby that state’s local clock. For each joint in the skeleton, an N-point weighted \naverage is then calculated for the translation vectors, rotation quaternions, \nand scales, yielding the ﬁ nal skeletal pose.\nOgre and the Playback Rate\nIt is interesting to note that Ogre has no concept of a playback rate (R). If \nit did, we would have expected to see a data member like this in the \nOgre::AnimationState class:\nReal    mPlaybackRate;\nOf course, we can still make animations play more slowly or more quickly in \nOgre by simply scaling the amount of time we pass to the addTime() func-\ntion, but unfortunately, Ogre does not support animation time scaling out of \nthe box.\n11.10.2.2. Example: Granny\nThe Granny animation system, by Rad Game Tools (htt p://www.radgame-\ntools.com/granny.html), provides a ﬂ at, weighted average animation blend-\ning system similar to Ogre’s. Granny permits any number of animations to be \nplayed on a single character simultaneously. The state of each active animation \nis maintained in a data structure known as a granny_control. Granny cal-\nculates a weighted average to determine the ﬁ nal pose, automatically normal-\nizing the weights of all active clips. In this sense, its architecture is virtually \nidentical to that of Ogre’s animation system. But where Granny really shines \nis in its handling of time. Granny uses the global clock approach discussed in \nSection 11.4.3. It allows each clip to be looped an arbitrary number of times or \ninﬁ nitely. Clips can also be time-scaled; a negative time scale allows an anima-\ntion to be played in reverse.\n11.10.3. Blend Trees\n For reasons we’ll explore below, some animation engines represent their blend \nspeciﬁ cations not as a ﬂ at weighted average but as a tree of blend operations. \nAn animation blend tree is an example of what is known in compiler theory \nas an expression tree or a syntax tree . The interior nodes of such a tree are opera-\ntors, and the leaf nodes serve as the inputs to those operators. (More correctly, \nthe interior nodes represent the non-terminals of the grammar , while the leaf \nnodes represent the terminals.) In the following sections, we’ll brieﬂ y revisit \nthe various kinds of animation blends we learned about in Sections 11.6.3 and \n11.6.5 and see how each can be represented by an expression tree.\n",
      "content_length": 2501,
      "extraction_method": "Direct"
    },
    {
      "page_number": 583,
      "chapter": null,
      "content": "561 \nLERP\nClip A\nClip B\nOutput Pose\nβ\nFigure 11.47.  A binary LERP blend, represented by a binary expression tree.\n11.10.3.1. Binary LERP Blend\nAs we saw in Section 11.6.1, a binary linear interpolation (LERP) blend takes \ntwo input poses and blends them together into a single output pose. A blend \nweight β controls the percentage of the second input pose that should appear \nat the output, while (1 – β) speciﬁ es the percentage of the ﬁ rst input pose. This \ncan be represented by the binary expression tree shown in Figure 11.47.\n11.10.3.2. Generalized One-Dimensional LERP Blend\nIn Section 11.6.3.1, we learned that it can be convenient to deﬁ ne a generalized \none-dimensional LERP blend by placing an arbitrary number of clips along a \nlinear scale. A blend factor b speciﬁ es the desired blend along this scale. Such \na blend can be pictured as an n-input operator, as shown in Figure 11.48.\nGiven a speciﬁ c value for b, such a linear blend can always be transformed \ninto a binary LERP blend. We simply use the two clips immediately adjacent \nto b as the inputs to the binary blend and calculate the blend weight β as speci-\nﬁ ed in Equation (11.12). This is illustrated in Figure 11.48.\nFor this specific value of \nb, this tree converts to...\nβ = 0\nβ = 1\nβ\nb\nbA\nbB\nbC\nbD\nLERP\nOutput Pose\nb\nClip A\nClip B\nClip C\nClip D\nLERP\nClip B\nClip C\nOutput Pose\nβ\nFigure 11.48.  A multi-input expression tree can be used to represent a generalized 1D blend. \nSuch a tree can always be transformed into a binary expression tree for any speciﬁ c value of \nthe blend factor b.\n11.10. The Animation Pipeline\n",
      "content_length": 1603,
      "extraction_method": "Direct"
    },
    {
      "page_number": 584,
      "chapter": null,
      "content": "562 \n11. Animation Systems\nbx\nLERP\nBottom Left\nBottom Right\nLERP\nTop Left\nTop Right\nOutput Pose\nLERP\nby\nFigure 11.49.  A simple 2D LERP blend, implemented as cascaded binary blends.\n11.10.3.3. Simple Two-Dimensional LERP Blend\nIn Section 11.6.3.2, we saw how a two-dimensional LERP blend can be realized \nby simply cascading the results of two binary LERP blends. Given a desired \ntwo-dimensional blend point b = [ bx  by ], Figure 11.49 shows how this kind of \nblend can be represented in tree form.\n11.10.3.4. Triangular LERP Blend\nSection 11.6.3.3 introduced us to triangular LERP blending, using the barycen-\ntric coordinates α, β, and γ = (1 – α – β) as the blend weights. To represent this \nkind of blend in tree form, we need a ternary (three-input) expression tree \nnode, as shown in Figure 11.50.\nTriangular \nLERP\nOutput Pose\nClip A\nClip B\nClip C\n(γ = 1 −α – β)\nα\nβ\nFigure 11.50.  A triangular 2D LERP blend, represented as a ternary expression tree.\n11.10.3.5. Generalized Triangular LERP Blend\nIn Section 11.6.3.4, we saw that a generalized two-dimensional LERP blend \ncan be speciﬁ ed by placing clips at arbitrary locations on a plane. A desired \noutput pose is speciﬁ ed by a point b = [ bx  by ] on the plane. This kind of \nblend can be represented as a tree node with an arbitrary number of inputs, \nas shown in Figure 11.51.\nA generalized triangular LERP blend can always be transformed into a \nternary tree by using  Delaunay triangulation to identify the triangle that sur-\nrounds the point b. The point is then converted into barycentric coordinates \nα, β, and γ = (1 – α – β), and these coordinates are used as the blend weights \n",
      "content_length": 1651,
      "extraction_method": "Direct"
    },
    {
      "page_number": 585,
      "chapter": null,
      "content": "563 \nof a ternary blend node with the three clips at the vertices of the triangle as its \ninputs. This is demonstrated in Figure 11.51.\n11.10.3.6. Additive Blend\nSection 11.6.5 described additive blending. This is a binary operation, so it can \nbe represented by a binary tree node, as shown in Figure 11.52. A single blend \nweight β controls the amount of the additive animation that should appear \nat the output—when β = 0, the additive clip does not aﬀ ect the output at all, \nwhile when β = 1, the additive clip has its maximum eﬀ ect on the output.\nAdditive blend nodes must be handled carefully, because the inputs are \nnot interchangeable (as they are with most types of blend operators). One of \nthe two inputs is a regular skeletal pose, while the other is a special kind of \npose known as a diﬀ erence pose (also known as an additive pose). A diﬀ erence \npose may only be applied to a regular pose, and the result of an additive blend \nis another regular pose. This implies that the additive input of a blend node \nmust always be a leaf node, while the regular input may be a leaf or an interior \nnode. If we want to apply more than one additive animation to our character, \nFor this specific value of \nb, this tree converts to...\nbE\nTriangular \nLERP\nOutput Pose\nClip C\nClip D\nClip E\n(γ = 1 −α – β)\nα\nβ\nDelaunay\nLERP\nOutput Pose\nb\nClip A\nClip B\nClip C\nClip D\nClip E\nbA\nb B\nbC\nbD\nb\nβ\nb\nα\nγ\nFigure 11.51.  A generalized 2D blend can be represented by a multi-input expression tree node, \nbut it can always be converted into a ternary tree via Delaunay triangulation.\nClip A\nOutput Pose\nβ\nDiff Clip B\n+\nFigure 11.52.  An additive blend represented as a binary tree.\n11.10. The Animation Pipeline\n",
      "content_length": 1703,
      "extraction_method": "Direct"
    },
    {
      "page_number": 586,
      "chapter": null,
      "content": "564 \n11. Animation Systems\nwe must use a cascaded binary tree with the additive clips always applied to \nthe additive inputs, as shown in Figure 11.53.\n11.10.4. Cross-Fading Architectures\nAs we saw in Section 11.6.2.2, cross-fading between animations is generally \naccomplished by LERP blending from the previous animation to the next one. \nCross-fades can be implemented in one of two ways, depending on whether \nyour animation engine uses the ﬂ at weighted average architecture or the ex-\npression tree architecture. In this section, we’ll take a look at both implemen-\ntations.\n11.10.4.1. Cross-Fades with a Flat Weighted Average\n In an animation engine that employs the ﬂ at weighted average architecture, \ncross-fades are implemented by adjusting the weights of the clips themselves. \nRecall that any clip whose weight wi = 0 will not contribute to the current pose \nof the character, while those whose weights are non-zero are averaged togeth-\ner to generate the ﬁ nal pose. If we wish to transition smoothly from clip A to \nclip B, we simply ramp up clip B’s weight, wB , while simultaneously ramping \ndown clip A’s weight, wA. This is illustrated in Figure 11.54.\nCross-fading in a weighted average architecture becomes a bit trickier \nwhen we wish to transition from one complex blend to another. As an ex-\nample, let’s say we wish to transition the character from walking to jumping. \nClip A\nβ1\nDiff Clip B\n+\nβ2\nDiff Clip C\n+\nOutput Pose\nβ3\nDiff Clip D\n+\nFigure 11.53.  In order to additively blend more than one difference pose onto a regular “base” \npose, a cascaded binary expression tree must be used.\nt\nw\n1\n0\ntstart\ntend\nwA\nwB\nFigure 11.54.  A simple cross-fade from clip A to clip B, as implemented in a weighted average \nanimation architecture.\n",
      "content_length": 1762,
      "extraction_method": "Direct"
    },
    {
      "page_number": 587,
      "chapter": null,
      "content": "565 \nLet’s assume that the walk movement is produced by a three-way average \nbetween clips A, B, and C, and that the jump movement is produced by a two-\nway average between clips D and E.\nWe want the character to look like he’s smoothly transitioning from walk-\ning to jumping, without aﬀ ecting how the walk or jump animations look indi-\nvidually. So during the transition, we want to ramp down the ABC clips and \nramp up the DE clips while keeping the relative weights of the ABC and DE clip \ngroups constant. If the cross-fade’s blend factor is denoted by λ, we can meet \nthis requirement by simply sett ing the weights of both clip groups to their de-\nsired values and then multiplying the weights of the source group by (1 – λ) \nand the weights of the destination group by λ.\nLet’s look at a concrete example to convince ourselves that this will work \nproperly. Imagine that before the transition from ABC to DE, the non-zero \nweights are as follows: wA = 0.2, wB = 0.3, and wC = 0.5. Aft er the transition, we \nwant the non-zero weights to be wD = 0.33, and wE = 0.66. So, we set the weights \nas follows:\n \n(1\n)(0.2),\n(0.33),\n(1\n)(0.3),\n(0.66).\n(1\n)(0.5),\nA\nD\nB\nE\nC\nw\nw\nw\nw\nw\n=\n−λ\n= λ\n=\n−λ\n= λ\n=\n−λ\n \n(11.17)\nFrom Equations (11.17), you should be able to convince yourself of the fol-\nlowing:\nWhen \n1. \nλ = 0, the output pose is the correct blend of clips A, B, and C, \nwith zero contribution from clips D and E.\nWhen \n2. \nλ = 1, the output pose is the correct blend of clips D and E, with \nno contribution from A, B ,or C.\nWhen 0 < \n3. \nλ < 1, the relative weights of both the ABC group and the DE \ngroup remain correct, although they no longer add to one. (In fact, group \nABC’s weights add to (1 – λ), and group DE’s weights add to λ.)\nFor this approach to work, the implementation must keep track of \nthe logical groupings between clips (even though, at the lowest level, all \nof the clips’ states are maintained in one big, ﬂ at array—for example, the \nOgre::AnimationStateSet in Ogre). In our example above, the system \nmust “know” that A, B, and C form a group, that D and E form another group, \nand that we wish to transition from group ABC to group DE. This requires ad-\nditional meta-data to be maintained, on top of the ﬂ at array of clip states.\n11.10.4.2. Cross-Fades with Expression Trees\nImplementing a cross-fade in an expression-tree -based animation engine is a \nbit more intuitive than it is in a weighted average architecture. Whether we’re \n11.10. The Animation Pipeline\n",
      "content_length": 2498,
      "extraction_method": "Direct"
    },
    {
      "page_number": 588,
      "chapter": null,
      "content": "566 \n11. Animation Systems\ntransitioning from one clip to another or from one complex blend to another, \nthe approach is always the same: We simply introduce a new, binary LERP \nnode at the root of the blend tree for the duration of the cross-fade.\nWe’ll denote the blend factor of the cross-fade node with the symbol λ as \nbefore. Its top input is the source tree (which can be a single clip or a complex \nblend), and its bott om input is the destination tree (again a clip or a complex \nblend). During the transition, λ is ramped from zero to one. Once λ = 1, the \ntransition is complete, and the cross-fade LERP node and its top input tree can \nbe retired. This leaves its bott om input tree as the root of the overall blend tree, \nthus completing the transition. This process is illustrated in Figure 11.55.\n11.10.5. Animation Pipeline Optimization\nOptimization is a crucial aspect of any animation pipeline. Some pipelines \nexpose all of their nitt y-gritt y optimization details, eﬀ ectively placing the re-\nsponsibility for proper optimization on the calling code. Others att empt to \nencapsulate most of the optimization details behind a convenient API, but \neven in these cases, the API still must be structured in a particular way so as to \npermit the desired optimizations to be implemented behind the scenes.\nAnimation pipeline optimizations are usually highly speciﬁ c to the archi-\ntecture of the hardware on which the game will run. For example, on mod-\nern hardware architectures, memory access patt erns can greatly aﬀ ect the \nperformance of the code. Cache misses and load-hit-store operations must be \navoided to ensure maximum speed. But on other hardware, ﬂ oating-point op-\nerations might be the bott leneck, in which case the code might be structured \nto take maximum advantage of SIMD vector math. Each hardware platform \nTree\nA\nOutput Pose\nLERP\nOutput Pose\nλ\nTree\nA\nTree\nB\nTree\nB\nOutput Pose\nBefore\nCross-Fade\nDuring\nCross-Fade\nAfter\nCross-Fade\nFigure 11.55.  A cross-fade between two arbitrary blend trees A and B.\n",
      "content_length": 2042,
      "extraction_method": "Direct"
    },
    {
      "page_number": 589,
      "chapter": null,
      "content": "567 \npresents a unique set of optimization challenges to the programmer. As a re-\nsult, some animation pipeline APIs are highly speciﬁ c to a particular platform. \nOther pipelines att empt to present an API that can be optimized in diﬀ erent \nways on diﬀ erent processors. Let’s take a look at a few examples.\n11.10.5.1. Optimization on the PlayStation 2\nThe PlayStation 2 has a region of ultra-fast memory known as the scratch pad. \nIt also has a fast direct memory access (DMA) controller, which is capable of \ncopying data to and from the scratch pad eﬃ  ciently. Some animation pipelines \ntake advantage of this hardware architecture by arranging for all animation \nblending to take place within the scratch pad. When two skeletal poses are to \nbe blended, they are DMA’d from main RAM to the scratch pad. The blend is \nperformed, and the result is writt en into another buﬀ er within the scratch pad . \nFinally, the resulting pose is DMA’d back into main RAM.\nThe PS2’s DMA controller can move memory around in parallel with the \nmain CPU. So, to maximize throughput, PS2 programmers are always look-\ning for ways to keep the CPU and the DMA controller busy simultaneously. \nOft en the best way to accomplish this is to use a batch-style API, where the \ngame queues up requests for animation blends in a big list and then kicks \neverything oﬀ  in one go. This permits the animation pipeline to maximize \nthe utilization of both the DMA controller and the CPU, because it can feed \na large number of pose requests through the pipeline with no “dead space” \nbetween them and even overlap the DMA of one request with the processing \nof an unrelated request.\n11.10.5.2. Optimization on the PLAYSTATION 3\nAs we saw in Section 7.6.1.2, the PLAYSTATION 3 has six specialized proces-\nsors known as synergistic processing units (SPU). The SPUs execute most code \nmuch more quickly than the main CPU (known as the power processing unit or \nPPU). Each SPU also has a 256 kB region of ultra-fast local store memory for its \nexclusive use. Like the PS2, the PS3 has a powerful DMA controller capable \nof moving memory back and forth between main RAM and the SPUs’ memo-\nries in parallel with computing tasks. If one could write an ideal animation \npipeline for the PS3, as much processing as possible would be executed on \nthe SPUs, and neither the PPU nor any SPU would ever be idle waiting for a \nDMA to complete.\nThis architecture leads to animation pipeline APIs that look similar in \nsome respects to their PlayStation 2 counterparts, in the sense that animation \nrequests are again batched so that they can be interleaved eﬃ  ciently. In ad-\ndition, a PLAYSTATION 3 animation API will usually expose the concept of \nanimation jobs, because a job is a fundamental unit of execution on the SPUs.\n11.10. The Animation Pipeline\n",
      "content_length": 2822,
      "extraction_method": "Direct"
    },
    {
      "page_number": 590,
      "chapter": null,
      "content": "568 \n11. Animation Systems\n11.10.5.3. Optimization on the Xbox and Xbox 360\nRather than having specialized memory regions and a DMA controller to \nmove data from region to region, the Xbox and the Xbox 360 both employ a \nuniﬁ ed memory architecture . All processors, including the main CPU (or in \nthe case of the 360, the three PowerPC cores), the GPU, and all other hardware \nsystems, tap into a single big block of main RAM.\nIn theory, the Xbox architecture requires a totally diﬀ erent set of optimi-\nzations than would be required on the PlayStation architectures, and so we \nmight expect to see very diﬀ erent animation APIs between these two plat-\nforms. However, the Xbox serves as an example of how optimizations for one \nplatform can sometimes be beneﬁ cial to other platforms as well. As it turns out, \nboth the Xbox and PlayStation platforms incur massive performance degra-\ndation in the presence of cache misses and load-hit-store memory access pat-\nterns. So, it is beneﬁ cial on both systems to keep animation data as localized as \npossible in physical RAM. An animation pipeline that processes animations in \nlarge batches and operates on data within relatively small regions of memory \n(such as the PS2’s scratch pad or PS3’s SPU memories) will also perform well \non a uniﬁ ed memory architecture like that of the Xbox. Achieving this kind of \nsynergy between platforms is not always possible, and every hardware plat-\nform requires its own speciﬁ c optimizations. However, when such an oppor-\ntunity does arise, it is wise to take advantage of it.\nA good rule of thumb is to optimize your engine for the platform with the \nmost stringent performance restrictions. When your optimized code is ported \nto other platforms with fewer restrictions, there’s a good chance that the opti-\nmizations you made will remain beneﬁ cial, or at worst will have few adverse \naﬀ ects on performance. Going in the other direction—porting from the least \nstringent platform to the more stringent ones—almost always results in less-\nthan-optimal performance on the most stringent platform.\n11.11. Action State Machines\nThe low-level pipeline is the equivalent of OpenGL or DirectX for animation—\nit is very powerful but can be rather inconvenient for direct use by game code. \nTherefore, it is usually convenient to introduce a layer between the low-level \npipeline and the game characters and other clients of the animation system. \nThis layer is usually implemented as a state machine, known as the action state \nmachine or the animation state machine ( ASM) .\nThe ASM sits on top of the animation pipeline, permitt ing the actions of \nthe characters in a game to be controlled in a straightforward, state-driven \n",
      "content_length": 2715,
      "extraction_method": "Direct"
    },
    {
      "page_number": 591,
      "chapter": null,
      "content": "569 \nmanner. The ASM is also responsible for ensuring that transitions from state \nto state are smooth and natural-looking. Some animation engines permit mul-\ntiple independent state machines to control diﬀ erent aspects of a character’s \nmovement, such as full-body locomotion, upper-body gestures, and facial \nanimations. This can be accomplished by introducing the concept of state lay-\nering. In this section, we’ll explore how a typical animation state machine is \narchitected.\n11.11.1. Animation States\nEach state in an ASM corresponds to an arbitrarily complex blend of simul-\ntaneous animation clips. In a blend tree architecture, each state corresponds \nto a particular predeﬁ ned blend tree. In a ﬂ at weighted average architecture, \na state represents a group of clips with a speciﬁ c set of relative weights. It is \nsomewhat more convenient and expressive to think in terms of blend trees, \nso we will do so for the remainder of this discussion. However, everything \nwe describe here can also be implemented using the ﬂ at weighted average \napproach, as long as additive blending or quaternion SLERP operations are \nnot involved.\nThe blend tree corresponding to a particular animation state can be as \nsimple or as complex as required by the game’s design (provided it remains \nwithin the memory and performance limitations of the engine). For example, \nan “idle” state might be comprised of a single full-body animation. A “run-\nning” state might correspond to a semicircular blend, with straﬁ ng left , run-\nning forward, and straﬁ ng right at the –90 degrees, 0 degrees, and +90 degrees \npoints, respectively. The blend tree for a “running while shooting” state might \ninclude a semicircular directional blend, plus additive or partial-skeleton \nblend nodes for aiming the character’s weapon up, down, left , and right, and \nadditional blends to permit the character to look around with its eyes, head, \nand shoulders. More additive animations might be included to control the \ncharacter’s overall stance, gait, and foot spacing while locomoting and to pro-\nvide a degree of “humanness” through random movement variations.\n11.11.1.1. State and Blend Tree Speciﬁ cations\n Animators, game designers, and programmers usually cooperate to create the \nanimation and control systems for the central characters in a game. These de-\nvelopers need a way to specify the states that make up a character’s ASM, to \nlay out the tree structure of each blend tree, and to select the clips that will \nserve as their inputs. Although the states and blend trees could be hard-coded, \nmost modern game engines provide a data-driven means of deﬁ ning animation \nstates. The goal of a data-driven approach is permit a user to create new ani-\n11.11. Action State Machines\n",
      "content_length": 2765,
      "extraction_method": "Direct"
    },
    {
      "page_number": 592,
      "chapter": null,
      "content": "570 \n11. Animation Systems\nmation states, remove unwanted states, ﬁ ne-tune existing states, and then see \nthe eﬀ ects of his or her changes reasonably quickly. In other words, the central \ngoal of a data-driven animation engine is to enable rapid iteration .\nThe means by which the users enter animation state data varies widely. \nSome game engines employ a simple, bare-bones approach, allowing anima-\ntion states to be speciﬁ ed in a text ﬁ le with a simple syntax. Other engines pro-\nvide a slick, graphical editor that permits animation states to be constructed \nby dragging atomic components such as clips and blend nodes onto a canvas \nand linking them together in arbitrary ways. Such editors usually provide a \nlive preview of the character so that the user can see immediately how the \ncharacter will look in the ﬁ nal game. In my opinion, the speciﬁ c method cho-\nsen has litt le bearing on the quality of the ﬁ nal game—what matt ers most is \nthat the user can make changes and see the results of those changes reason-\nably quickly and easily.\n11.11.1.2. Custom Blend Tree Node Types\nTo build an arbitrarily complex blend tree, we really only require four atomic \ntypes of blend nodes: clips, binary LERP blends, binary additive blends, and \nternary (triangular) LERP blends. Virtually any blend tree imaginable can be \ncreated as compositions of these atomic nodes.\nA blend tree built exclusively from atomic nodes can quickly become large \nand unwieldy. As a result, many game engines permit custom compound \nnode types to be predeﬁ ned for convenience. The N-dimensional linear blend \nnode discussed in Sections 11.6.3.4 and 11.10.3.2 is an example of a compound \nnode. One can imagine myriad complex blend node types, each one address-\ning a particular problem speciﬁ c to the particular game being made. A soccer \ngame might deﬁ ne a node that allows the character to dribble the ball. A war \ngame could deﬁ ne a special node that handles aiming and ﬁ ring a weapon. \nA brawler could deﬁ ne custom nodes for each ﬁ ght move the characters can \nperform. Once we have the ability to deﬁ ne custom node types, the sky’s the \nlimit.\n11.11.1.3. Example: Naughty Dog’s Uncharted Engine\n The animation engine used in Naughty Dog’s Uncharted: Drake’s Fortune \nand Uncharted 2: Among Thieves employs a simple, text-based approach to \nspecifying animation states. For reasons related to Naughty Dog’s rich his-\ntory with the Lisp language , state speciﬁ cations in the Uncharted engine \nare writt en in a customized version of the Scheme programming language \n(which itself is a Lisp variant). Two basic state types can be used: simple and \ncomplex.\n",
      "content_length": 2657,
      "extraction_method": "Direct"
    },
    {
      "page_number": 593,
      "chapter": null,
      "content": "571 \nSimple States\nA simple state contains a single animation clip. For example:\n(define-state simple\n    :name \"pirate-b-bump-back\"\n    :clip \"pirate-b-bump-back\"\n    :flags (anim-state-flag no-adjust-to-ground)\n)\nDon’t let the Lisp-style syntax throw you. All this block of code does is to de-\nﬁ ne a state named “pirate-b-bump-back” whose animation clip also happens \nto be named “pirate-b-bump-back.” The :f lags parameter allows users to \nspecify various Boolean options on the state.\nComplex States\nA complex state contains an arbitrary tree of LERP or additive blends. For ex-\nample, the following state deﬁ nes a tree that contains a single binary LERP \nblend node, with two clips (“walk-l-to-r” and “run-l-to-r”) as its inputs:\n(define-state complex\n    :name \"move-l-to-r\"\n:tree\n        (anim-node-lerp\n            (anim-node-clip \"walk-l-to-r\")\n            (anim-node-clip \"run-l-to-r\")\n        )\n)\nThe :tree argument allows the user to specify an arbitrary blend tree, com-\nposed of LERP or additive blend nodes and nodes that play individual anima-\ntion clips.\nFrom this, we can see how the (define-state simple ...) example \nshown above might really work under the hood—it probably deﬁ nes a com-\nplex blend tree containing a single “clip” node, like this:\n(define-state complex\n    :name \"pirate-b-unimog-bump-back\"\n:tree (anim-node-clip \"pirate-b-unimog-bump-back”)\n    :flags (anim-state-flag no-adjust-to-ground)\n)\nThe following complex state shows how blend nodes can be cascaded into \narbitrarily deep blend trees:\n(define-state complex\n    :name \"move-b-to-f\"\n11.11. Action State Machines\n",
      "content_length": 1610,
      "extraction_method": "Direct"
    },
    {
      "page_number": 594,
      "chapter": null,
      "content": "572 \n11. Animation Systems\n    :tree\n        (anim-node-lerp\n            (anim-node-additive\n                (anim-node-additive\n                    (anim-node-clip \"move-f\")\n                    (anim-node-clip \"move-f-look-lr\")\n                )\n                (anim-node-clip \"move-f-look-ud\")\n            )\n            (anim-node-additive\n                (anim-node-additive\n                    (anim-node-clip \"move-b\")\n                    (anim-node-clip \"move-b-look-lr\")\n                )\n                (anim-node-clip \"move-b-look-ud\")\n            )\n        )\n)\nThis corresponds to the tree shown in Figure 11.56.\nCustom Tree Syntax\nThanks to the powerful macro language in Scheme, custom blend trees can \nalso be deﬁ ned by the user in terms of the basic clip, LERP, and additive blend \nnodes. This allows us to deﬁ ne multiple states, each of which has a nearly \nidentical tree structure but with diﬀ erent input clips or any number of other \nvariations. For example, the complex blend tree used in the state “move-b-to-\nf” shown above could be partially deﬁ ned via a macro as follows:\n(define-syntax look-tree\n    (syntax-rules ()\nLERP\nmove-f\nmove-f-look-lr\n+\nmove-f-look-ud\nmove-b\nmove-b-look-lr\n+\nmove-b-look-ud\n+\n+\nFigure 11.56.  Blend tree corresponding to the example state “move-b-to-f.”\n",
      "content_length": 1309,
      "extraction_method": "Direct"
    },
    {
      "page_number": 595,
      "chapter": null,
      "content": "573 \n        ((look-tree base-clip look-lr-clip look-ud-clip)\n            ;; This means \"whenever the compiler sees  \n \n \n \n       ;; code of the form (look-tree b lr ud),\n    \n    ;; replace it with the following code...\"\n            (anim-node-additive\n                (anim-node-additive\n                    (anim-node-clip base-clip)\n                    (anim-node-clip look-lr-clip)\n                )\n                (anim-node-clip look-ud-clip)\n            )\n        )\n    )\n)\nThe original “move-b-to-f” state could then be redeﬁ ned in terms of this \nmacro as follows:\n(define-state complex\n    :name \"move-b-to-f\"\n    :tree\n        (anim-node-lerp\n            (look-tree \"move-f\"\n                \"move-f-look-lr\"\n                \"move-f-look-ud\")\n            (look-tree \"move-b\"\n                \"move-b-look-lr\"\n                \"move-b-look-ud\")\n        )\n)\nThe (look-tree ...) macro can be used to deﬁ ne any number of states that \nrequire this same basic tree structure but want diﬀ erent animation clips as \ninputs. They can also combine their “look trees” in any number of ways.\nRapid Iteration\nRapid iteration is achieved in Uncharted with the help of two important tools. \nAn in-game animation viewer allows a character to be spawned into the game \nand its animations controlled via an in-game menu. And a simple command-\nline tool allows animation scripts to be recompiled and reloaded into the run-\nning game on the ﬂ y. To tweak a character’s animations, the user can make \nchanges to the text ﬁ le containing the animation state speciﬁ cations, quick-\nly reload the animation states, and immediately see the eﬀ ects of his or her \nchanges on an animating character in the game.\n11.11. Action State Machines\n",
      "content_length": 1726,
      "extraction_method": "Direct"
    },
    {
      "page_number": 596,
      "chapter": null,
      "content": "574 \n11. Animation Systems\nFigure 11.57.  The Unreal Engine 3 graphical animation editor.\n11.11.1.4. Example: Unreal Engine 3\nUnreal Engine 3 (UE3) provides its users with a graphical interface to the ani-\nmation system. As shown in Figure 11.57, an animation blend tree in Unreal \nis comprised of a special root node called an AnimTree. This node takes three \nkinds of inputs: animations, morphs, and special nodes known as skel controls. \nThe animation input can be connected to the root of an arbitrarily complex \nblend tree (which happens to be drawn with poses ﬂ owing from right to left —\nopposite of the convention we use in this book). The “morph” input allows \nmorph-target-based animations to drive the character; this is most oft en used \nfor facial animation. The “skel control” inputs allow various kinds of proce-\ndural post-processing, such as inverse kinematics (IK), to be performed on the \npose generated by the animation and/or morph trees.\nThe UE3 Animation Tree\nThe Unreal animation tree is essentially a blend tree. Individual animation \nclips (called sequences in Unreal) are represented by nodes of type Anim\nSequence. A sequence node has a single output, which may either be con-\nnected directly to the “animation” input of the AnimTree node or to other \ncomplex node types. Unreal provides a wide selection of blend node types \nout of the box, including binary blends, four-way two-dimensional blends \n",
      "content_length": 1428,
      "extraction_method": "Direct"
    },
    {
      "page_number": 597,
      "chapter": null,
      "content": "575 \n(known as blend by aim), and so on. It also provides various special nodes that \nare capable of doing things like scaling the playback rate (R) of a clip, mirror-\ning the animation (which turns a right-handed motion into a left -handed one, \nfor example), and more.\nThe UE3 animation tree is also highly customizable. A programmer can \ncreate new types of nodes that perform arbitrarily complex operations. So the \nUnreal developer is not limited to simple binary and ternary LERP blends. At \nthe time this chapter was writt en, Unreal Engine 3 did not support additive \nanimation blending out of the box, although it’s certainly possible for a game \nteam to extend the Unreal engine to support it.\nIt is interesting to note that Unreal’s approach to character animation is \nnot explicitly state-based. Rather than deﬁ ning multiple states, each with its \nown local blend tree, the Unreal developer typically builds a single monolithic \ntree. The character can be put into diﬀ erent “states” by simply turning on or \noﬀ  certain parts of the tree. Some game teams implement a system for replac-\ning portions of the UE3 animation tree dynamically, so that a game’s mono-\nlithic tree can be broken into more manageable subtrees.\nThe UE3 Post-Processing Tree (Skel Controls)\nAs we have seen, animation post-processing involves procedurally modifying \nthe pose of the skeleton that has been generated by the blend tree. In UE3, \nskel control nodes are used for this purpose. To use a skel control, the user \nﬁ rst creates an input on the AnimTree node corresponding to the joint in the \nskeleton that he or she wishes to control procedurally. Then a suitable skel \ncontrol node is created, and its output is hooked up to the new input on the \nAnimTree node.\nUnreal provides a number of skel controls out of the box, to perform \nfoot IK (which ensures that the feet conform to ground contours), procedural \n“look-at” (which allows the character to look at arbitrary points in space), \nother forms of IK, and so on. As with animation nodes, it is quite easy for a \nprogrammer to create custom skel control nodes in order to meet the particu-\nlar needs of the game being developed.\n11.11.2. Transitions\nTo create a high-quality animating character, we must carefully manage the \ntransitions between states in the action state machine to ensure that the splices \nbetween animations do not have a jarring and unpolished appearance. Most \nmodern animation engines provide a data-driven mechanism for specifying \nexactly how transitions should be handled. In this section, we’ll explore how \nthis mechanism works.\n11.11. Action State Machines\n",
      "content_length": 2637,
      "extraction_method": "Direct"
    },
    {
      "page_number": 598,
      "chapter": null,
      "content": "576 \n11. Animation Systems\n11.11.2.1. Kinds of Transitions\nThere are many diﬀ erent ways to manage the transition between states. If we \nknow that the ﬁ nal pose of the source state exactly matches the ﬁ rst pose of \nthe destination state, we can simply “pop” from one state to another. Other-\nwise, we can cross-fade from one state to the next. Cross-fading is not always \na suitable choice when transitioning from state to state. For example, there \nis no way that a cross-fade can produce a realistic transition from lying on \nthe ground to standing upright. For this kind of state transition, we need one \nor more custom animations. This kind of transition is oft en implemented by \nintroducing special transitional states into the state machine. These states are \nintended for use only when going from one state to another—they are never \nused as a steady-state node. But because they are full-ﬂ edged states, they can \nbe comprised of arbitrarily complex blend trees. This provides maximum ﬂ ex-\nibility when authoring custom-animated transitions.\n11.11.2.2. Transition Parameters\nWhen describing a particular transition between two states, we generally need \nto specify various parameters, controlling exactly how the transition will oc-\ncur. These include but are not limited to the following.\nSource and destination states.\n• \n To which state(s) does this transition ap-\nply?\nTransition type.\n• \n Is the transition immediate, cross-faded, or performed \nvia a transitional state?\nDuration.\n• \n For cross-faded transitions, we need to specify how long the \ncross-fade should take.\nEase-in/ease-out curve\n• \n type. In a cross-faded transition, we may wish to \nspecify the type of ease-in/ease-out curve to use to vary the blend factor \nduring the fade.\nTransition window\n• \n . Certain transitions can only be taken when the source \nanimation is within a speciﬁ ed window of its local time line. For ex-\nample, a transition from a punch animation to an impact reaction might \nonly make sense when the arm is in the second half of its swing. If an \natt empt to perform the transition is made during the ﬁ rst half of the \nswing, the transition would be disallowed (or a diﬀ erent transition \nmight be selected instead).\n11.11.2.3. The Transition Matrix\nSpecifying transitions between states can be challenging, because the number \nof possible transitions is usually very large. In a state machine with n states, \n",
      "content_length": 2417,
      "extraction_method": "Direct"
    },
    {
      "page_number": 599,
      "chapter": null,
      "content": "577 \nthe worst-case number of possible transitions is n2. We can imagine a two-\ndimensional square matrix with every possible state listed along both the ver-\ntical and horizontal axes. Such a table can be used to specify all of the possible \ntransitions from any state along the vertical axis to any other state along the \nhorizontal axis.\nIn a real game, this transition matrix is usually quite sparse, because not \nall state-to-state transitions are possible. For example, transitions are usually \ndisallowed from a death state to any other state. Likewise, there is probably \nno way to go from a driving state to a swimming state (without going through \nat least one intermediate state that causes the character to jump out of his \nvehicle!). The number of unique transitions in the table may be signiﬁ cantly \nless even than the number of valid transitions between states. This is because \nwe can oft en re-use a single transition speciﬁ cation between many diﬀ erent \npairs of states.\n11.11.2.4. Implementing a Transition Matrix\nThere are all sorts of ways to implement a transition matrix. We could use a \nspreadsheet application to tabulate all the transitions in matrix form, or we \nmight permit transitions to be authored in the same text ﬁ le used to author our \naction states. If a graphical user interface is provided for state editing, transi-\ntions could be added to this GUI as well. In the following sections, we’ll take a \nbrief look at a few transition matrix implementations from real game engines.\nExample: Wild-Carded Transitions in Medal of Honor: Paciﬁ c Assault\nOn Medal of Honor: Paciﬁ c Assault (MOHPA), we used the sparseness of the \ntransition matrix to our advantage by supporting wild-carded transition spec-\niﬁ cations. For each transition speciﬁ cation, the names of both the source and \ndestination states could contain asterisks (*) as a wild-card character. This al-\nlowed us to specify a single default transition from any state to any other \nstate (via the syntax from=”*” to=”*” ) and then reﬁ ne this global default \neasily for entire categories of states. The reﬁ nement could be taken all the way \ndown to custom transitions between speciﬁ c state pairs when necessary. The \nMOHPA transition matrix looked something like this:\n<transitions>\n  // global default\n  <trans from=\"*\" to=\"*\" type=frozen duration=0.2>\n  ...\n  // default for any walk to any run\n  <trans from=\"walk*\" to=\"run*\" type=smooth  \n    \n  duration=0.15>\n11.11. Action State Machines\n",
      "content_length": 2494,
      "extraction_method": "Direct"
    },
    {
      "page_number": 600,
      "chapter": null,
      "content": "578 \n11. Animation Systems\n  ...\n  // special handling from any prone to any getting-up   \n  // action (only valid from 2 sec to 7.5 sec on the \n  // local timeline)\n  <trans from=\"*prone\" to=\"*get-up\" type=smooth   \n \n \n  duration=0.1\n  window-start=2.0 window-end=7.5>\n  ...\n  // special case between crouched walking and jumping\n  <trans from=\"walk-crouch\" to=\"jump\" type=frozen  \n \n \n  duration=0.3>\n  ...\n</transitions>\nExample: First-Class Transitions in Uncharted\nIn some animation engines, high-level game code requests transitions from \nthe current state to a new state by naming the destination state explicitly. The \nproblem with this approach is that the calling code must have intimate knowl-\nedge of the names of the states and of which transitions are valid when in a \nparticular state.\nIn Naughty Dog’s Uncharted engine, this problem is overcome by turn-\ning state transitions from secondary implementation details into ﬁ rst-class \nentities. Each state provides a list of valid transitions to other states, and each \ntransition is given a unique name. The names of the transitions are standard-\nized in order to make the eﬀ ect of each transition predictable. For example, \nif a transition is called “walk,” then it always goes from the current state to a \nwalking state of some kind, no matt er what the current state is. Whenever the \nhigh-level animation control code wants to transition from state A to state B, \nit asks for a transition by name (rather than requesting the destination state \nexplicitly). If such a transition can be found and is valid, it is taken; otherwise, \nthe request fails.\nThe following example state deﬁ nes four transitions named “reload,” \n“step-left ,” “step-right,” and “ﬁ re.” The (transition-group ...)  line \ninvokes a previously deﬁ ned group of transitions; it is useful when the \nsame set of transitions is to be used in multiple states. The (transition-\nend ...)  command speciﬁ es a transition that is taken upon reaching the \nend of the state’s local time line if no other transition has been taken before \nthen.\n(define-state complex\n    :name \"s_turret-idle\"\n",
      "content_length": 2121,
      "extraction_method": "Direct"
    },
    {
      "page_number": 601,
      "chapter": null,
      "content": "579 \n    :tree (aim-tree (anim-node-clip \n       \"turret-aim-all--base\")\n                    \"turret-aim-all--left-right\"\n                    \"turret-aim-all--up-down\")\n:transitions (\n        (transition \"reload\" \"s_turret-reload\"\n            (range - -) :fade-time 0.2)\n        (transition \"step-left\" \"s_turret-step-left\"\n            (range - -) :fade-time 0.2)\n        (transition \"step-right\" \"s_turret-step-right\"\n            (range - -) :fade-time 0.2)\n        (transition \"fire\" \"s_turret-fire\"\n            (range - -) :fade-time 0.1)\n        (transition-group \"combat-gunout-idle^move\")\n        (transition-end \"s_turret-idle\")\n    )\n)\nThe beauty of this approach may be diﬃ  cult to see at ﬁ rst. Its primary \npurpose is to allow transitions and states to be modiﬁ ed in a data-driven man-\nner, without requiring changes to the C++ source code in many cases. This \ndegree of ﬂ exibility is accomplished by shielding the animation control code \nfrom knowledge of the structure of the state graph. For example, let’s say that \nwe have ten diﬀ erent walking states (normal, scared, crouched, injured, and \nso on). All of them can transition into a jumping state, but diﬀ erent kinds \nof walks might require diﬀ erent jump animations (e.g., normal jump, scared \njump, jump from crouch, injured jump, etc.). For each of the ten walking states, \nwe deﬁ ne a transition simply called “jump.” At ﬁ rst, we can point all of these \ntransitions to a single generic “jump” state, just to get things up and running. \nLater, we can ﬁ ne-tune some of these transitions so that they point to custom \njump states. We can even introduce transitional states between some of the \n“walk” states and their corresponding “jump” states. All sorts of changes can \nbe made to the structure of the state graph and the parameters of the transi-\ntions without aﬀ ecting the C++ source code—as long as the names of the transi-\ntions don’t change.\n11.11.3. State Layers\nMost living creatures can do more than one thing at once with their bodies. \nFor example, a human can walk around with her lower body while looking at \n11.11. Action State Machines\n",
      "content_length": 2129,
      "extraction_method": "Direct"
    },
    {
      "page_number": 602,
      "chapter": null,
      "content": "580 \n11. Animation Systems\nsomething with her shoulders, head, and eyes and making a gesture with her \nhands and arms. The movements of diﬀ erent parts of the body aren’t gener-\nally in perfect sync—certain parts of the body tend to “lead” the movements \nof other parts (e.g., the head leads a turn, followed by the shoulders, the hips, \nand ﬁ nally the legs). In traditional animation, this well-known technique is \nknown as anticipation [44].\nThis kind of movement seems to be at odds with a state-machine-based \napproach to animation. Aft er all, we can only be in one state at a time. So how \ncan we get diﬀ erent parts of the body to operate independently? One solution \nto this problem is to introduce the concept of state layers . Each layer can be \nin only one state at a time, but the layers are temporally independent of one \nanother. The ﬁ nal pose of the skeleton is calculated by evaluating the blend \ntrees on each of the n layers, thus generating n skeletal poses, and then blend-\ning these poses together in a predeﬁ ned manner. This is illustrated in Fig-\nure 11.58.\nThe Uncharted engine uses a layered state architecture. The layers form \na stack, with the bott om-most layer (called the base layer) always producing \na full-body skeletal pose and each upper layer blending in a new full-body, \npartial-skeleton, or additive pose on top of the base pose. Two kinds of layers \nare supported: LERP and additive. A LERP layer blends its output pose with \nthe pose generated by the layer(s) below it. An additive layer assumes that its \noutput pose is always a diﬀ erence pose and uses additive blending to combine \nit with the pose generated by the layer(s) below it. In eﬀ ect, a layered state ma-\nBase Layer\nState A\nState B\nState C\nVariation Layer (Additive)\nD\nE\nG\nGesture Layer (Additive)\nH\nI\nGesture Layer (LERP)\nJ\nK\nF\nTime (τ)\nFigure 11.58.  A layered animation state machine, showing how each layer’s state transitions \nare temporally independent.\n",
      "content_length": 1969,
      "extraction_method": "Direct"
    },
    {
      "page_number": 603,
      "chapter": null,
      "content": "581 \nchine converts multiple, temporally independent blend trees (one per layer) \ninto a single uniﬁ ed blend tree. This is shown in Figure 11.59.\n11.11.4. Control Parameters\nFrom a soft ware engineering perspective, it can be challenging to orchestrate \nall of the blend weights, playback rates, and other control parameters of a \ncomplex animating character. Diﬀ erent blend weights have diﬀ erent eﬀ ects \non the way the character animates. For example, one weight might control \nthe character’s movement direction, while others control its movement speed, \nhorizontal and vertical weapon aim, head/eye look direction, and so on. We \nneed some way of exposing all of these blend weights to the code that is re-\nsponsible for controlling them.\nNet blend tree\nat time τ\nTime\nH\nF\nB\nτ\nK\nLERP\n+\nTree\nB\nTree\nF\nTree\nH\n+\nTree\nK\nFigure 11.59.  A layered state machine converts the blend trees from multiple states into a \nsingle, uniﬁ ed tree.\n11.11. Action State Machines\n",
      "content_length": 967,
      "extraction_method": "Direct"
    },
    {
      "page_number": 604,
      "chapter": null,
      "content": "582 \n11. Animation Systems\nIn a ﬂ at weighted average architecture, we have a ﬂ at list of all the ani-\nmation clips that could possibly be played on the character. Each clip state \nhas a blend weight, a playback rate, and possibly other control parameters. \nThe code that controls the character must look up individual clip states by \nname and adjust each one’s blend weight appropriately. This makes for a sim-\nple interface, but it shift s most of the responsibility for controlling the blend \nweights to the character control system. For example, to adjust the direction \nin which a character is running, the character control code must know that the \n“run” action is comprised of a group of animation clips, named something \nlike “StrafeLeft ,” “RunForward,” “StrafeRight,” and “RunBackward.” It must \nlook up these clip states by name and manually control all four blend weights \nin order to achieve a particular angled run animation. Needless to say, control-\nling animation parameters in such a ﬁ ne-grained way can be tedious and can \nlead to diﬃ  cult-to-understand source code.\nIn a blend tree , a diﬀ erent set of problems arise. Thanks to the tree struc-\nture, the clips are grouped naturally into functional units. Custom tree nodes \ncan encapsulate complex character motions. These are both helpful advantag-\nes over the ﬂ at weighted average approach. However, the control parameters \nare buried within the tree. Code that wishes to control the horizontal look-at \ndirection of the head and eyes needs a priori knowledge of the structure of \nthe blend tree so that it can ﬁ nd the appropriate nodes in the tree in order to \ncontrol their parameters.\nDiﬀ erent animation engines solve these problems in diﬀ erent ways. Here \nare some examples:\nNode search.\n• \n Some engines provide a way for higher-level code to ﬁ nd \nblend nodes in the tree. For example, relevant nodes in the tree can be \ngiven special names, such as “HorizAim” for the node that controls hor-\nizontal weapon aiming. The control code can simply search the tree for \na node of a particular name; if one is found, then we know what eﬀ ect \nadjusting its blend weight will have.\nNamed variables.\n• \n Some engines allow names to be assigned to the indi-\nvidual control parameters. The controlling code can look up a control \nparameter by name in order to adjust its value.\nControl structure.\n• \n In other engines, a simple data structure, such as an \narray of ﬂ oating-point values or a C struct, contains all of the control \nparameters for the entire character. The nodes in the blend tree(s) are \nconnected to particular control parameters, either by being hard-coded \nto use certain struct members or by looking up the parameters by \nname or index.\n",
      "content_length": 2733,
      "extraction_method": "Direct"
    },
    {
      "page_number": 605,
      "chapter": null,
      "content": "583 \nOf course, there are many other alternatives as well. Every animation en-\ngine tackles this problem in a slightly diﬀ erent way, but the net eﬀ ect is always \nroughly the same.\n11.11.5. Constraints\n We’ve seen how action state machines can be used to specify complex blend \ntrees and how a transition matrix can be used to control how transitions be-\ntween states should work. Another important aspect of character animation \ncontrol is to constrain the movement of the characters and/or objects in the \nscene in various ways. For example, we might want to constrain a weapon \nso that it always appears to be in the hand of the character who is carrying it. \nWe might wish to constrain two characters so that they line up properly when \nshaking hands. A character’s feet are oft en constrained so that they line up \nwith the ﬂ oor, and its hands might be constrained to line up with the rungs \non a ladder or the steering wheel of a vehicle. In this section, we’ll take a brief \nlook at how these constraints are handled in a typical animation system.\n11.11.5.1. Attachments\n Virtually all modern game engines permit objects to be att ached to one another. \nAt its simplest, object-to-object att achment involves constraining the position \nand/or orientation of a particular joint JA within the skeleton of object A so that \nit coincides with a joint JB in the skeleton of object B. An att achment is usually \na parent-child relationship. When the parent’s skeleton moves, the child object \nis adjusted to satisfy the constraint. However, when the child moves, the par-\nent’s skeleton is usually not aﬀ ected. This is illustrated in Figure 11.60.\nSometimes it can be convenient to introduce an oﬀ set between the parent \njoint and the child joint. For example, when placing a gun into a character’s \n… child \nskeleton \nfollows\nparent \nskeleton \nmoves…\nchild \nskeleton \nmoves…\n… parent \nskeleton \nunaffected\nFigure 11.60.  An attachment, showing how movement of the parent automatically produces \nmovement of the child but not vice-versa.\n11.11. Action State Machines\n",
      "content_length": 2072,
      "extraction_method": "Direct"
    },
    {
      "page_number": 606,
      "chapter": null,
      "content": "584 \n11. Animation Systems\nhand, we could constrain the “Grip” joint of the gun so that it coincides with \nthe “RightWrist” joint of the character. However, this might not produce the \ncorrect alignment of the gun with the hand. One solution to this problem is \nto introduce a special joint into one of the two skeletons. For example, we \ncould add a “RightGun” joint to the character’s skeleton, make it a child of the \n“RightWrist” joint, and position it so that when the “Grip” joint of the gun is \nconstrained to it, the gun looks like it is being held naturally by the character. \nThe problem with this approach, however, is that it increases the number of \njoints in the skeleton. Each joint has a processing cost associated with anima-\ntion blending and matrix palett e calculation and a memory cost for storing its \nanimation keys. So adding new joints is oft en not a viable option.\nWe know that an additional joint added for att achment purposes will not \ncontribute to the pose of the character—it merely introduces an additional \ntransform between the parent and child joint in an att achment. What we re-\nally want, then, is a way to mark certain joints so that they can be ignored by \nthe animation blending pipeline but can still be used for att achment purposes. \nSuch special joints are sometimes called att ach points. They are illustrated in \nFigure 11.61.\nAtt ach points might be modeled in Maya just like regular joints or loca-\ntors , although many game engines deﬁ ne att ach points in a more convenient \nmanner. For example, they might be speciﬁ ed as part of the action state ma-\nchine text ﬁ le or via a custom GUI within the animation authoring tool. This \nallows the animators to focus only on the joints that aﬀ ect the look of the \ncharacter, while the power to control att achments is put conveniently into the \nhands of the people who need it—the game designers and the engineers.\n11.11.5.2. Interobject Registration\n The interactions between game characters and their environments is growing \never more complex and nuanced with each new title. Hence, it is important \nAttachment  \nis equivalent \nto a joint\nFigure 11.61.  An attach point acts like an extra joint between the parent and the child.\n",
      "content_length": 2230,
      "extraction_method": "Direct"
    },
    {
      "page_number": 607,
      "chapter": null,
      "content": "585 \nto have a system that allows characters and objects to be aligned with one an-\nother when animating. Such a system can be used for in-game cinematics and \ninteractive gameplay elements alike.\nImagine that an animator, working in Maya or some other animation tool, \nsets up a scene involving two characters and a door object. The two charac-\nters shake hands, and then one of them opens the door and they both walk \nthrough it. The animator can ensure that all three actors in the scene line up \nperfectly. However, when the animations are exported, they become three \nseparate clips, to be played on three separate objects in the game world. The \ntwo characters might have been under AI or player control prior to the start of \nthis animated sequence. How, then, can we ensure that the three objects line \nup correctly with one another when the three clips are played back in-game?\nReference Locators\nOne good solution is to introduce a common reference point into all three \nanimation clips. In Maya, the animator can drop a locator (which is just a 3D \ntransform, much like a skeletal joint) into the scene, placing it anywhere that \nseems convenient. Its location and orientation are actually irrelevant, as we’ll \nsee. The locator is tagged in some way to tell the animation export tools that it \nis to be treated specially.\nWhen the three animation clips are exported, the tools store the position \nand orientation of the reference locator, expressed in coordinates that are rela-\ntive to the local object space of each actor , into all three clip’s data ﬁ les. Later, \nwhen the three clips are played back in-game, the animation engine can look \nup the relative position and orientation of the reference locator in all three \nclips. It can then transform the origins of the three objects in such a way as \nto make all three reference locators coincide in world space. The reference \nlocator acts much like an att ach point (Section 11.11.5.1) and, in fact, could be \nimplemented as one. The net eﬀ ect—all three actors now line up with one an-\nother, exactly as they had been aligned in the original Maya scene.\nyMaya\nxMaya\nReference \nLocator\nActor A\nActor B\nActor C\nFigure 11.62.  Original Maya scene containing three actors and a reference locator.\n11.11. Action State Machines\n",
      "content_length": 2291,
      "extraction_method": "Direct"
    },
    {
      "page_number": 608,
      "chapter": null,
      "content": "586 \n11. Animation Systems\nFigure 11.62 illustrates how the door and the two characters from the \nabove example might be set up in a Maya scene. As shown in Figure 11.63, the \nreference locator appears in each exported animation clip (expressed in that \nactor’s local space). In-game, these local-space reference locators are aligned \nto a ﬁ xed world-space locator in order to re-align the actors, as shown in Fig-\nure 11.64.\nFinding the World-Space Reference Location\nWe’ve glossed over one important detail here—who decides what the world-\nspace position and orientation of the reference locator should be? Each anima-\ntion clip provides the reference locator’s transform in the coordinate space of \nits actor. But we need some way to deﬁ ne where that reference locator should \nbe in world space.\nIn our example with the door and the two characters shaking hands, one \nof the actors is ﬁ xed in the world (the door). So one viable solution is to ask the \ndoor for the location of the reference locator and then align the two characters \nto it. The commands to do accomplish this might look similar to the following \npseudocode.\nvoid playShakingHandsDoorSequence(\n \nActor& door,\n \nActor& characterA,\n \nActor& characterB)\n{\nActor B’s \nClip\nActor C’s \nClip\nActor A’s \nClip\nyA\nxA\nyB\nxB\nxC\nyC\nFigure 11.63.  The reference locator is encoded in each actor’s animation ﬁ le.\nyworld\nx world\nFixed reference \nin world space\nFigure 11.64.  At runtime, the local-space reference transforms are aligned to a world-space \nreference locator, causing the actors to line up properly.\n",
      "content_length": 1572,
      "extraction_method": "Direct"
    },
    {
      "page_number": 609,
      "chapter": null,
      "content": "587 \n \n// Find the world-space transform of the reference  \n \n \n// locator as specified in the door’s animation.\n Transform \nrefLoc = getReferenceLocatorWs(door,\n  \"shake-hands-door\");\n \n// Play the door’s animation in-place. (It’s alread    \n \n// in the correct place in the world.)\nplayAnimation(\"shake-hands-door\", door);\n \n// Play the two characters’ animations relative to  \n \n \n// the world-space reference locator obtained from \n \n// the door.\nplayAnimationRelativeToReference\n(\"shake-hands-character-a\", characterA, refLoc);\nplayAnimationRelativeToReference\n(\"shake-hands-character-b\", characterB, refLoc);\n}\nAnother option is to deﬁ ne the world-space transform of the reference \nlocator independently of the three actors in the scene. We could place the ref-\nerence locator into the world using our world-building tool, for example (see \nSection 13.3). In this case, the pseudocode above should be changed to look \nsomething like this:\nvoid playShakingHandsDoorSequence(\n \nActor& door,\n \nActor& characterA,\n \nActor& characterB,\n \nActor& refLocatorActor)\n{\n \n// Find the world-space transform of the reference  \n \n \n// locator by simply querying the transform of an  \n \n \n// independent actor (presumably placed into the \n \n// world manually).\n Transform \nrefLoc = getActorTransformWs\n(refLocatorActor);\n \n// Play all animations relative to the world-space\n \n// reference locator obtained above.\nplayAnimationRelativeToReference(\"shake-hands-door\",\n  door, \nrefLoc);\nplayAnimationRelativeToReference\n(\"shake-hands-character-a\", characterA, refLoc);\nplayAnimationRelativeToReference\n(\"shake-hands-character-b\", characterB, refLoc);\n}\n11.11. Action State Machines\n",
      "content_length": 1671,
      "extraction_method": "Direct"
    },
    {
      "page_number": 610,
      "chapter": null,
      "content": "588 \n11. Animation Systems\n11.11.5.3. Grabbing and Hand IK\n Even aft er using an att achment to connect two objects, we sometimes ﬁ nd that \nthe alignment does not look exactly right in-game. For example, a character \nmight be holding a riﬂ e in her right hand, with her left  hand supporting the \nstock. As the character aims the weapon in various directions, we may no-\ntice that the left  hand no longer aligns properly with the stock at certain aim \nangles. This kind of joint misalignment is caused by LERP blending. Even if \nthe joints in question are aligned perfectly in clip A and in clip B, LERP blend-\ning does not guarantee that those joints will be in alignment when A and B are \nblended together.\nOne solution to this problem is to use inverse kinematics (IK) to correct \nthe position of the left  hand. The basic approach is to determine the desired \ntarget position for the joint in question. IK is then applied to a short chain of \njoints (usually two, three, or four joints), starting with the joint in question \nand progressing up the hierarchy to its parent, grandparent, and so on. The \njoint whose position we are trying to correct is known as the end eﬀ ector. The \nIK solver adjusts the orientations of the end eﬀ ector’s parent joint(s) in order \nto get the end eﬀ ector as close as possible to the target.\nThe API for an IK system usually takes the form of a request to enable or \ndisable IK on a particular chain of joints, plus a speciﬁ cation of the desired \ntarget point. The actual IK calculation is usually done internally by the low-\nlevel animation pipeline. This allows it to do the calculation at the proper \ntime—namely, aft er intermediate local and global skeletal poses have been \ncalculated but before the ﬁ nal matrix palett e calculation.\nSome animation engines allow IK chains to be deﬁ ned a priori. For ex-\nample, we might deﬁ ne one IK chain for the left  arm, one for the right arm, \nand two for the two legs. Let’s assume for the purposes of this example that \na particular IK chain is identiﬁ ed by the name of its end-eﬀ ector joint. (Other \nengines might use an index or handle or some other unique identiﬁ er, but the \nconcept remains the same.) The function to enable an IK calculation might \nlook something like this:\nvoid enableIkChain(\n \nActor& actor,\n \nconst char* endEffectorJointName,\n \nconst Vector3& targetLocationWs);\nand the function to disable an IK chain might look like this:\nvoid disableIkChain(\n \nActor& actor,\n \nconst char* endEffectorJointName);\n",
      "content_length": 2517,
      "extraction_method": "Direct"
    },
    {
      "page_number": 611,
      "chapter": null,
      "content": "589 \nIK is usually enabled and disabled relatively infrequently, but the world-\nspace target location must be kept up-to-date every frame (if the target is \nmoving). Therefore, the low-level animation pipeline always provides some \nmechanism for updating an active IK target point. For example, the pipeline \nmight allow us to call enableIkChain() multiple times. The ﬁ rst time it is \ncalled, the IK chain is enabled, and its target point is set. All subsequent calls \nsimply update the target point.\nIK is well-suited to making minor corrections to joint alignment when \nthe joint is already reasonably close to its target. It does not work nearly as \nwell when the error between a joint’s desired location and its actual location \nis large. Note also that most IK algorithms solve only for the position of a joint. \nYou may need to write additional code to ensure that the orientation of the end \neﬀ ector aligns properly with its target as well. IK is not a cure-all, and it may \nhave signiﬁ cant performance costs. So always use it judiciously.\n11.11.5.4. Motion Extraction and Foot IK\nIn games, we usually want the locomotion animations of our characters to \nlook realistic and “grounded.” One of the biggest factors contributing to the \nrealism of a locomotion animation is whether or not the feet slide around on \nthe ground. Foot sliding can be overcome in a number of ways, the most com-\nmon of which are motion extraction and foot IK.\nMotion Extraction\n Let’s imagine how we’d animate a character walking forward in a straight \nline. In Maya (or his or her animation package of choice), the animator makes \nFigure 11.65.  In the animation authoring package, the character moves forward in space, and \nits feet appear grounded.\n11.11. Action State Machines\n",
      "content_length": 1767,
      "extraction_method": "Direct"
    },
    {
      "page_number": 612,
      "chapter": null,
      "content": "590 \n11. Animation Systems\nthe character take one complete step forward, ﬁ rst with the left  foot and then \nwith the right foot. The resulting animation clip is known as a locomotion cycle , \nbecause it is intended to be looped indeﬁ nitely, for as long as the character \nis walking forward in-game. The animator takes care to ensure that the feet \nof the character appear grounded and don’t slide as it moves. The character \nmoves from its initial location on frame 0 to a new location at the end of the \ncycle. This is shown in Figure 11.65.\nNotice that the local-space origin of the character remains ﬁ xed during \nthe entire walk cycle. In eﬀ ect, the character is “leaving his origin behind him” \nas he takes his step forward. Now imagine playing this animation as a loop. \nWe would see the character take one complete step forward, and then pop \nback to where he was on the ﬁ rst frame of the animation. Clearly this won’t \nwork in-game.\nTo make this work, we need to remove the forward motion of the charac-\nter, so that his local-space origin remains roughly under the center of mass of \nthe character at all times. We could do this by zeroing out the forward transla-\ntion of the root joint of the character’s skeleton. The resulting animation clip \nwould make the character look like he’s “moonwalking,” as shown in Fig-\nure 11.66.\nIn order to get the feet to appear to “stick” to the ground the way they \ndid in the original Maya scene, we need the character to move forward \nby just the right amount each frame. We could look at the distance the \ncharacter moved, divide by the amount of time it took for him to get there, \nand hence find his average movement speed. But a character’s forward \nFigure 11.66.  Walk cycle after zeroing out the root joint’s forward motion.\n",
      "content_length": 1784,
      "extraction_method": "Direct"
    },
    {
      "page_number": 613,
      "chapter": null,
      "content": "591 \nspeed is not constant when walking. This is especially evident when a \ncharacter is limping (quick forward motion on the injured leg, followed \nby slower motion on the “good” leg), but it is true for all natural-looking \nwalk cycles.\nTherefore, before we zero out the forward motion of the root joint, we ﬁ rst \nsave the animation data in a special “extracted motion” channel. This data can \nbe used in-game to move the local-space origin of the character forward by the \nexact amount that the root joint had moved in Maya each frame. The net result \nis that the character will walk forward exactly as he was authored, but now \nhis local-space origin comes along for the ride, allowing the animation to loop \nproperly. This is shown in Figure 11.67.\nIf the character moves forward by 4 feet in the animation and the anima-\ntion takes one second to complete, then we know that the character is moving \nat an average speed of 4 feet/second. To make the character walk at a diﬀ erent \nspeed, we can simply scale the playback rate of the walk cycle animation. For \nexample, to make the character walk at 2 feet/second, we can simply play the \nanimation at half speed (R = 0.5).\nFoot IK\n Motion extraction does a good job of making a character’s feet appear ground-\ned when it is moving in a straight line (or, more correctly, when it moves in a \npath that exactly matches the path animated by the animator). However, a real \ngame character must be turned and moved in ways that don’t coincide with \nthe original hand-animated path of motion (e.g., when moving over uneven \nterrain). This results in additional foot sliding.\nFigure 11.67.  Walk cycle in-game, with extracted root motion data applied to the local-space \norigin of the character.\n11.11. Action State Machines\n",
      "content_length": 1774,
      "extraction_method": "Direct"
    },
    {
      "page_number": 614,
      "chapter": null,
      "content": "592 \n11. Animation Systems\nOne solution to this problem is to use IK to correct for any sliding in the \nfeet. The basic idea is to analyze the animations to determine during which \nperiods of time each foot is fully in contact with the ground. At the moment a \nfoot contacts the ground, we note its world-space location. For all subsequent \nframes while that foot remains on the ground, we use IK to adjust the pose \nof the leg so that the foot remains ﬁ xed to the proper location. This technique \nsounds easy enough, but gett ing it to look and feel right can be very challeng-\ning. It requires a lot of iteration and ﬁ ne-tuning. And some natural human \nmotions—like leading into a turn by increasing your stride—cannot be pro-\nduced by IK alone.\nIn addition, there is a big trade-oﬀ  between the look of the animations \nand the feel of the character, particularly for a human-controlled character. \nIt’s generally more important for the player character control system to feel \nresponsive and fun than it is for the character’s animations to look perfect. The \nupshot is this: Do not take the task of adding foot IK or motion extraction to \nyour game lightly. Budget time for a lot of trail and error, and be prepared to \nmake trade-oﬀ s to ensure that your player character not only looks good but \nfeels good as well.\n11.11.5.5. Other Kinds of Constraints\nThere are plenty of other possible kinds of constraint systems that can be add-\ned to a game animation engine. Some examples include:\nLook-at\n• \n . This is the ability for characters to look at points of interest in \nthe environment. A character might look at a point with only his or her \neyes, with eyes and head, or with eyes, head, and a twist of the entire \nupper body. Look-at constraints are sometimes implemented using IK \nor procedural joint oﬀ sets, although a more natural look can oft en be \nachieved via additive blending.\nCover registration\n• \n . This is the ability for a character to align perfectly with \nan object that is serving as cover. This is oft en implemented via the ref-\nerence locator technique described above.\nCover entry and departure\n• \n . If a character can take cover, animation blend-\ning and custom entry and departure animations must usually be used \nto get the character into and out of cover.\nTraversal aids\n• \n . The ability for a character to navigate over, under, around, \nor through obstacles in the environment can add a lot of life to a game. \nThis is oft en done by providing custom animations and using a refer-\nence locator to ensure proper registration with the obstacle being over-\ncome.\n",
      "content_length": 2600,
      "extraction_method": "Direct"
    },
    {
      "page_number": 615,
      "chapter": null,
      "content": "593 \n11.12. Animation Controllers\nThe animation pipeline provides high-speed animation posing and blending \nfacilities, but its interface is usually too cumbersome to be used directly by \ngameplay code. The action state machine provides a more convenient inter-\nface by allowing complex blend trees to be described, oft en in a data-driven \nmanner, and then encapsulated within easy-to-understand logical states. Tran-\nsitions between states can also be deﬁ ned, again oft en in a data-driven way, \nso that gameplay code can be writt en in a ﬁ re-and-forget manner, without \nhaving to micromanage every transition. The ASM system may also provide \na layering mechanism, allowing the motion of a character to be described by \nmultiple state machines running in parallel. But even given the relatively con-\nvenient interface provided by the action state machine, some game teams ﬁ nd \nit convenient to introduce a third layer of soft ware, aimed at providing higher-\nlevel control over how characters animate. As such, it is oft en implemented as \na collection of classes known as animation controllers .\nControllers tend to manage behaviors over relatively long periods of \ntime—on the order of a few seconds or more. Each animation controller is typ-\nically responsible for one type of gross character behavior, like how to behave \nwhen in cover, how to behave when locomoting from one place to another in \nthe game world, or how to drive a vehicle. A controller typically orchestrates \nall aspects of the character’s animation-related behavior. It adjusts blend fac-\ntors to control movement directions, aiming, and so on, manages state transi-\ntions, fades in and out layers, and does whatever else is needed to make the \ncharacter behave as desired.\nOne beneﬁ t of a controller-based design is that all of the code relating to a \nparticular behavioral category is localized in one place. This design also per-\nmits higher-level gameplay systems, like player mechanics or AI , to be writt en \nin a much simpler way, because all of the details of micromanaging the anima-\ntions can be extracted and hidden within the controllers.\nThe animation controller layer takes many diﬀ erent forms and is highly \ndependent upon the needs of the game and the soft ware design philosophies \nof the engineering team. Some teams don’t use animation controllers at all. \nOn other teams, the animation controllers may be tightly integrated into the \nAI and/or player mechanics systems. Still other teams implement a suite of \nrelatively general-purpose controllers that can be shared between the player \ncharacter and the NPCs . For bett er or for worse, there is no one standard way \nto implement animation controllers in the game industry (at least not yet).\n11.12. Animation Controllers\n",
      "content_length": 2775,
      "extraction_method": "Direct"
    },
    {
      "page_number": 616,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 617,
      "chapter": null,
      "content": "595\n12\nCollision and Rigid\nBody Dynamics\nI\nn the real world, solid objects are inherently, well… solid. They generally \navoid doing impossible things, like passing through one another, all by \nthemselves. But in a virtual game world, objects don’t do anything unless we \ntell them to, and game programmers must make an explicit eﬀ ort to ensure \nthat objects do not pass through one another. This is the role of one of the cen-\ntral components of any game engine—the collision detection system.\nA game engine’s collision system is oft en closely integrated with a physics \nengine . Of course, the ﬁ eld of physics is vast, and what most of today’s game \nengines call “physics” is more accurately described as a rigid body dynamics \nsimulation. A rigid body is an idealized, inﬁ nitely hard, non-deformable solid \nobject. The term dynamics refers to the process of determining how these rigid \nbodies move and interact over time under the inﬂ uence of forces. A rigid body \ndynamics simulation allows motion to be imparted to objects in the game \nin a highly interactive and naturally chaotic manner—an eﬀ ect that is much \nmore diﬃ  cult to achieve when using canned animation clips to move things \nabout.\nA dynamics simulation makes heavy use of the collision detection system \nin order to properly simulate various physical behaviors of the objects in the \nsimulation, including bouncing oﬀ  one another, sliding under friction, rolling, \nand coming to rest. Of course, a collision detection system can be used stand-\nalone, without a dynamics simulation—many games do not have a “physics” \n",
      "content_length": 1593,
      "extraction_method": "Direct"
    },
    {
      "page_number": 618,
      "chapter": null,
      "content": "596 \n12. Collision and Rigid Body Dynamics\nsystem at all. But all games that involve objects moving about in two- or three-\ndimensional space have some form of collision detection.\nIn this chapter, we’ll investigate the architecture of both a typical collision \ndetection system and a typical physics (rigid body dynamics) system. As we \ninvestigate the components of these two closely interrelated systems, we’ll \ntake a look at the mathematics and the theory that underlie them.\n12.1. Do You Want Physics in Your Game?\nNowadays, most game engines have some kind of physical simulation capa-\nbilities. Some physical eﬀ ects, like rag doll deaths, are simply expected by \ngamers. Other eﬀ ects, like ropes, cloth, hair, or complex physically driven ma-\nchinery can add that je ne sais quoi that sets a game apart from its competitors. \nIn recent years, some game studios have started experimenting with advanced \nphysical simulations, including approximate real-time ﬂ uid mechanics eﬀ ects \nand simulations of deformable bodies . But adding physics to a game is not \nwithout costs, and before we commit ourselves to implementing an exhaus-\ntive list of physics-driven features in our game, we should (at the very least) \nunderstand the trade-oﬀ s involved.\n12.1.1. Things You Can Do with a Physics System\n Here are just a few of the things you can do or have with a game physics \nsystem.\nz Detect collisions between dynamic objects and static world geometry.\nz Simulate free rigid bodies under the inﬂ uence of gravity and other forces.\nz Spring-mass systems.\nz Destructible buildings and structures.\nz Ray and shape casts (to determine line of sight, bullet impacts, etc.).\nz Trigger volumes (determine when objects enter, leave, or are inside pre-\ndeﬁ ned regions in the game world).\nz Allow characters to pick up rigid objects.\nz Complex machines (cranes, moving platform puzzles, and so on).\nz Traps (such as an avalanche of boulders).\nz Drivable vehicles with realistic suspensions.\nz Rag doll character deaths.\nz Powered rag doll: a realistic blend between traditional animation and \nrag doll physics.\n",
      "content_length": 2109,
      "extraction_method": "Direct"
    },
    {
      "page_number": 619,
      "chapter": null,
      "content": "597 \n12.1. Do You Want Physics in Your Game?\nz Dangling props (canteens, necklaces, swords), semi-realistic hair, cloth-\ning movements.\nz Cloth simulations.\nz Water surface simulations and buoyancy.\nz Audio propagation.\nAnd the list goes on.\nWe should note here that in addition to running a physics simulation at \nruntime in our game, we can also run a simulation as part of an oﬄ  ine pre-\nprocessing step in order to generate an animation clip. A number of physics \nplug-ins are available for animation tools like Maya. This is also the approach \ntaken by the Endorphin package by NaturalMotion Inc. (htt p://www.natu-\nralmotion.com/ endorphin.htm). In this chapter, we’ll restrict our discussion \nto runtime rigid body dynamics simulations, but oﬀ -line tools are a power-\nful option, of which we should always remain aware as we plan our game \nprojects.\n12.1.2. Is Physics Fun?\n The presence of a rigid body dynamics system in a game does not necessarily \nmake the game fun. More oft en than not, the inherently chaotic behavior of a \nphysics sim can actually detract from the gameplay experience rather than en-\nhancing it. The fun derived from physics depends on many factors, including \nthe quality of the simulation itself, the care with which it has been integrated \nwith other engine systems, the selection of physics-driven gameplay elements \nversus elements that are controlled in a more direct manner, how the physical \nelements interact with the goals of the player and the abilities of the player \ncharacter, and the genre of game being made.\nLet’s take a look at a few broad game genres and how a rigid body dy-\nnamics system might ﬁ t into each one.\n12.1.2.1. Simulations (Sims)\nThe primary goal of a sim is to accurately reproduce a real-life experience. Ex-\namples include the Flight Simulator, Gran Turismo, and NASCAR Racing series \nof games. Clearly, the realism provided by a rigid body dynamics system ﬁ ts \nextremely well into these kinds of games.\n12.1.2.2. Physics Puzzle Games\nThe whole idea of a physics puzzle is to let the user play around with dynami-\ncally simulated toys. So obviously this kind of game relies almost entirely on \nphysics for its core mechanic. Examples of this genre include Bridge Builder, \n",
      "content_length": 2244,
      "extraction_method": "Direct"
    },
    {
      "page_number": 620,
      "chapter": null,
      "content": "598 \n12. Collision and Rigid Body Dynamics\nThe Incredible Machine, the online game Fantastic Contraption, and Crayon Phys-\nics for the iPhone.\n12.1.2.3. Sandbox Games\nIn a sandbox game , there may be no objectives at all, or there may be a large \nnumber of optional objectives. The player’s primary objective is usually to \n“mess around” and explore what the objects in the game world can be made \nto do. Examples of sandbox games include Grand Theft  Auto, Spore, and Lit-\ntleBigPlanet.\nSandbox games can put a realistic dynamics simulation to good use, es-\npecially if much of the fun is derived from playing with realistic (or semi-\nrealistic) interactions between objects in the game world. So in these contexts, \nphysics can be fun in and of itself. However, many games trade realism for \nan increased fun factor (e.g., larger-than-life explosions, gravity that is stron-\nger or weaker than normal, etc.). So the dynamics simulation may need to be \ntweaked in various ways to achieve the right “feel.”\n12.1.2.4. Goal-Based and Story-Driven Games\nA goal-based game has rules and speciﬁ c objectives that the player must ac-\ncomplish in order to progress; in a story-driven game , telling a story is of par-\namount importance. Integrating a physics system into these kinds of games \ncan be tricky. We generally give away control in exchange for a realistic simula-\ntion, and this loss of control can inhibit the player’s ability to accomplish goals \nor the game’s ability to tell the story.\nFor example, in a character-based platformer game, we want the player \ncharacter to move in ways that are fun and easy to control but not necessar-\nily physically realistic. In a war game, we might want a bridge to explode \nin a realistic way, but we also may want to ensure that the debris doesn’t \nend up blocking the player’s only path forward. In these kinds of games, \nphysics is oft en not necessarily fun, and in fact it can oft en get in the way \nof fun when the player’s goals are at odds with the physically simulated \nbehaviors of the objects in the game world. Therefore, developers must be \ncareful to apply physics judiciously and take steps to control the behavior \nof the simulation in various ways to ensure it doesn’t get in the way of \ngameplay.\n12.1.3. Impact of Physics on a Game\n Adding a physics simulation to a game can have all sorts of impacts on the \nproject and the gameplay. Here are a few examples across various game de-\nvelopment disciplines.\n",
      "content_length": 2469,
      "extraction_method": "Direct"
    },
    {
      "page_number": 621,
      "chapter": null,
      "content": "599 \n12.1.3.1. Design Impacts\nz Predictability. The inherent chaos and variability that sets a physically \nsimulated behavior apart from an animated one is also a source of un-\npredictability. If something absolutely must happen a certain way every \ntime, it’s usually bett er to animate it than to try to coerce your dynamics \nsimulation into producing the motion reliably.\nz Tuning and control. The laws of physics (when modeled accurately) are \nﬁ xed. In a game, we can tweak the value of gravity or the coeﬃ  cient of \nrestitution of a rigid body, which gives back some degree of control. \nHowever, the results of tweaking physics parameters are oft en indirect \nand diﬃ  cult to visualize. It’s much harder to tweak a force in order to \nget a character to move in the desired direction than it is to tweak an \nanimation of a character walking.\nz Emergent behaviors . Sometimes physics introduces unexpected features \ninto a game—for example, the rocket-launcher jump trick in Team For-\ntress Classic, the high-ﬂ ying exploding Warthog in Halo, and the ﬂ ying \n“surfb oards” in PsyOps.\nIn general, the game design should usually drive the physics require-\nments of a game engine—not the other way around.\n12.1.3.2. Engineering Impacts\nz Tools pipeline. A good collision/physics pipeline takes time to build and \nmaintain.\nz User interface. How does the player control the physics objects in the \nworld? Does he or she shoot them? Walk into them? Pick them up? Us-\ning a virtual arm, as in Trespasser? Using a “gravity gun,” as in Half-\nLife 2? \nz Collision detection. Collision models intended for use within a dynamics \nsimulation may need to be more detailed and more carefully construct-\ned than their non-physics-driven counterparts.\nz AI . Pathing may not be predictable in the presence of physically simu-\nlated objects. The engine may need to handle dynamic cover points that \ncan move or blow up. Can the AI use the physics to its advantage?\nz Animation and character motion.  Animation-driven objects can clip slight-\nly through one another with few or no ill eﬀ ects, but when driven by a \ndynamics simulation, objects may bounce oﬀ  one another in unexpected \nways or jitt er badly. Collision ﬁ ltering may need to be applied to permit \nobjects to interpenetrate slightly. Mechanisms may need to be put in \nplace to ensure that objects sett le and go to sleep properly.\n12.1. Do You Want Physics in Your Game?\n",
      "content_length": 2425,
      "extraction_method": "Direct"
    },
    {
      "page_number": 622,
      "chapter": null,
      "content": "600 \n12. Collision and Rigid Body Dynamics\nz Rag doll physics. Rag dolls require a lot of ﬁ ne-tuning and oft en suﬀ er \nfrom instability in the simulation. An animation may drive parts of a \ncharacter’s body into penetration with other collision volumes—when \nthe character turns into a rag doll, these interpenetrations can cause \nenormous instability. Steps must be taken to avoid this.\nz Graphics. Physics-driven motion can have an eﬀ ect on renderable ob-\njects’ bounding volumes (where they would otherwise be static or more \npredictable). The presence of destructible buildings and objects can in-\nvalidate some kinds of precomputed lighting and shadow methods.\nz Networking and multiplayer . Physics eﬀ ects that do not aﬀ ect gameplay \nmay be simulated exclusively (and independently) on each client ma-\nchine. However, physics that has an eﬀ ect on gameplay (such as the \ntrajectory that a grenade follows) must be simulated on the server and \naccurately replicated on all clients.\nz Record and playback . The ability to record gameplay and play it back at \na later time is very useful as a debugging/testing aid, and it can also \nserve as a fun game feature. This feature is much more diﬃ  cult to imple-\nment in the presence of simulated dynamics because chaotic behavior \n(in which the simulation takes a very diﬀ erent path as a result of small \nchanges in initial conditions) and diﬀ erences in the timing of the phys-\nics updates can cause playbacks to fail to match the recorded original.\n12.1.3.3. Art Impacts\nz Additional tool and workﬂ ow complexity. The need to rig up objects with \nmass, friction, constraints, and other att ributes for consumption by the \ndynamics simulation makes the art department’s job more diﬃ  cult as \nwell.\nz More-complex content. We may need multiple visually identical versions of \nan object with diﬀ erent collision and dynamics conﬁ gurations for diﬀ er-\nent purposes—for example, a pristine version and a destructible version.\nz Loss of control . The unpredictability of physics-driven objects can make \nit diﬃ  cult to control the artistic composition of a scene.\n12.1.3.4. Other Impacts\nz Interdisciplinary impacts. The introduction of a dynamics simulation into \nyour game requires close cooperation between engineering, art, and de-\nsign.\nz Production impacts. Physics can add to a project’s development costs, \ntechnical and organizational complexity, and risk.\n",
      "content_length": 2420,
      "extraction_method": "Direct"
    },
    {
      "page_number": 623,
      "chapter": null,
      "content": "601 \n12.2. Collision/Physics Middleware\nHaving explored the impacts, most teams today do choose to integrate \na rigid body dynamics system into their games. With some careful planning \nand wise choices along the way, adding physics to your game can be reward-\ning and fruitful. And as we’ll see below, third-party middleware is making \nphysics more accessible than ever.\n12.2. Collision/Physics Middleware\n Writing a collision system and rigid body dynamics simulation is challeng-\ning and time-consuming work. The collision/physics system of a game engine \ncan account for a signiﬁ cant percentage of the source code in a typical game \nengine. That’s a lot of code to write and maintain!\nThankfully, a number of robust, high-quality collision/physics engines are \nnow available, either as commercial products or in open-source form. Some of \nthese are listed below. For a discussion of the pros and cons of various phys-\nics SDKs, check out the on-line game development forums (e.g., htt p://www.\ngamedev.net/community/forums/topic.asp?topic_id=463024).\n12.2.1. I-Collide, SWIFT, V-Collide, and RAPID\nI-Collide is an open-source collision detection library developed by the Uni-\nversity of North Carolina at Chapel Hill (UNC). It can detect intersections \nbetween convex volumes. I-Collide has been replaced by a faster, more fea-\nture-rich library called SWIFT . UNC has also developed collision detection \nlibraries that can handle complex non-convex shapes, called V-Collide and \nRAPID . None of these libraries can be used right out of the box in a game, but \nthey might provide a good basis upon which to build a fully functional game \ncollision detection engine. You can read more about I-Collide, SWIFT, and \nthe other UNC geometry libraries at htt p://www.cs.unc.edu/~geom/I_COL-\nLIDE/.\n12.2.2. ODE\nODE stands for “Open Dynamics Engine ” (htt p://www.ode.org). As its name \nimplies, ODE is an open-source collision and rigid body dynamics SDK. Its \nfeature set is similar to a commercial product like Havok. Its beneﬁ ts include \nbeing free (a big plus for small game studios and school projects!) and the \navailability of full source code (which makes debugging much easier and \nopens up the possibility of modifying the physics engine to meet the speciﬁ c \nneeds of a particular game).\n",
      "content_length": 2297,
      "extraction_method": "Direct"
    },
    {
      "page_number": 624,
      "chapter": null,
      "content": "602 \n12. Collision and Rigid Body Dynamics\n12.2.3. Bullet\nBullet is an open-source collision detection and physics library used by both \nthe game and ﬁ lm industries. Its collision engine is integrated with its dy-\nnamics simulation, but hooks are provided so that the collision system can \nbe used standalone or integrated with other physics engines. It supports con-\ntinuous collision detection (CCD)—also known as time of impact (TOI) collision \ndetection—which as we’ll see below can be extremely helpful when a simu-\nlation includes small, fast-moving objects. The Bullet SDK is available for \ndownload at htt p://code.google.com/p/bullet/, and the Bullet wiki is locat-\ned at htt p://www.bulletphysics.com/mediawiki-1.5.8/index.php?title=Main_\nPage.\n12.2.4. \nTrueAxis\nTrueAxis is another collision/physics SDK. It is free for non-commercial use. \nYou can learn more about TrueAxis at htt p://trueaxis.com.\n12.2.5. PhysX\nPhysX started out as a library called Novodex , produced and distributed by \nAgeia as part of their strategy to market their dedicated physics coprocessor. \nIt was bought by NVIDIA and is being retooled so that it can run using NVID-\nIA’s GPUs as a coprocessor. (It can also run entirely on a CPU, without GPU \nsupport.) It is available at htt p://www.nvidia.com/object/nvidia_physx.html. \nPart of Ageia ’s and NVIDIA’s marketing strategy has been to provide the CPU \nversion of the SDK entirely for free, in order to drive the physics coprocessor \nmarket forward. Developers can also pay a fee to obtain full source code and \nthe ability to customize the library as needed. PhysX is available for PC, Xbox \n360, PLAYSTATION 3, and Wii.\n12.2.6. Havok\nHavok is the gold standard in commercial physics SDKs, providing one of \nthe richest feature sets available and boasting excellent performance charac-\nteristics on all supported platforms. (It’s also the most expensive solution.) \nHavok is comprised of a core collision/physics engine, plus a number of \noptional add-on products including a vehicle physics system, a system for \nmodeling destructible environments, and a fully featured animation SDK \nwith direct integration into Havok’s rag doll physics system. It runs on PC, \nXbox 360, PLAYSTATION 3, and Wii and has been speciﬁ cally optimized \nfor each of these platforms. You can learn more about Havok at htt p://www.\nhavok.com.\n",
      "content_length": 2363,
      "extraction_method": "Direct"
    },
    {
      "page_number": 625,
      "chapter": null,
      "content": "603 \n12.3. The Collision Detection System\n12.2.7. Physics Abstraction Layer (PAL)\nThe Physics Abstraction Layer (PAL) is an open-source library that allows \ndevelopers to work with more than one physics SDK on a single project. It \nprovides hooks for PhysX (Novodex), Newton, ODE, OpenTissue , Tokamak , \nTrueAxis, and a few other SDKs. You can read more about PAL at htt p://www.\nadrianboeing.com/pal/index.html.\n12.2.8. Digital Molecular Matter (DMM)\nPixelux Entertainment S.A., located in Geneva, Switzerland, has produced a \nunique physics engine that uses ﬁ nite element methods to simulate the dy-\nnamics of deformable and breakable objects, called Digital Molecular Mat-\nter (DMM). The engine has both an oﬄ  ine and a runtime component. It was \nreleased in 2008 and can be seen in action in LucasArts’ Star Wars: The Force \nUnleashed . A discussion of deformable body mechanics is beyond our scope \nhere, but you can read more about DMM at htt p://www.pixeluxentertain-\nment.com.\n12.3. The Collision Detection System\nThe primary purpose of a game engine’s collision detection system is to deter-\nmine whether any of the objects in the game world have come into contact . To \nanswer this question, each logical object is represented by one or more geo-\nmetric shapes . These shapes are usually quite simple, such as spheres, boxes, \nand capsules. However, more-complex shapes can also be used. The collision \nsystem determines whether or not any of the shapes are intersecting (i.e., over-\nlapping) at any given moment in time. So a collision detection system is es-\nsentially a gloriﬁ ed geometric intersection tester.\nOf course, the collision system does more than answer yes/no questions \nabout shape intersection. It also provides relevant information about the na-\nture of each contact. Contact information can be used to prevent unrealistic \nvisual anomalies on-screen, such as objects interpenetrating one another. This \nis generally accomplished by moving all interpenetrating objects apart prior \nto rendering the next frame. Collisions can provide support for an object—one \nor more contacts that together allow the object to come to rest, in equilibrium \nwith gravity and/or any other forces acting on it. Collisions can also be used \nfor other purposes, such as to cause a missile to explode when it strikes its \ntarget or to give the player character a health boost when he passes through \na ﬂ oating health pack. A rigid body dynamics simulation is oft en the most \ndemanding client of the collision system, using it to mimic physically realistic \n",
      "content_length": 2569,
      "extraction_method": "Direct"
    },
    {
      "page_number": 626,
      "chapter": null,
      "content": "604 \n12. Collision and Rigid Body Dynamics\nbehaviors like bouncing, rolling, sliding, and coming to rest. But, of course, \neven games that have no physics system can still make heavy use of a collision \ndetection engine.\nIn this chapter, we’ll go on a brief high-level tour of how collision detec-\ntion engines work. For an in-depth treatment of this topic, a number of excel-\nlent books on real-time collision detection are available, including [12], [41], \nand [9].\n12.3.1. Collidable Entities\nIf we want a particular logical object in our game to be capable of colliding \nwith other objects, we need to provide it with a collision representation , describ-\ning the object’s shape and its position and orientation in the game world. This \nis a distinct data structure, separate from the object’s gameplay representation \n(the code and data that deﬁ ne its role and behavior in the game) and separate \nfrom its visual representation (which might be an instance of a triangle mesh, a \nsubdivision surface, a particle eﬀ ect, or some other visual representation).\nFrom the point of view of detecting intersections, we generally favor \nshapes that are geometrically and mathematically simple. For example, a rock \nmight be modeled as a sphere for collision purposes; the hood of a car might \nbe represented by a rectangular box ; a human body might be approximated \nby a collection of interconnected capsules (pill-shaped volumes). Ideally, we \nshould resort to a more-complex shape only when a simpler representation \nproves inadequate to achieve the desired behavior in the game. Figure 12.1 \nshows a few examples of using simple shapes to approximate object volumes \nfor collision detection purposes.\nHavok uses the term collidable to describe a distinct, rigid object that can \ntake part in collision detection. It represents each collidable with an instance \nof the C++ class hkpCollidable. PhysX calls its rigid objects actors and rep-\nresents them as instances of the class NxActor. In both of these libraries, a \ncollidable entity contains two basic pieces of information—a shape and a trans-\nFigure 12.1 Simple geometric shapes are often used to approximate the collision volumes of \nthe objects in a game.\n",
      "content_length": 2214,
      "extraction_method": "Direct"
    },
    {
      "page_number": 627,
      "chapter": null,
      "content": "605 \nform. The shape describes the collidable’s geometric form, and the transform \ndescribes the shape’s position and orientation in the game world. Collidables \nneed transforms for three reasons:\nTechnically speaking, a shape only describes the form of an object (i.e., \n1. \nwhether it is a sphere, a box, a capsule, or some other kind of volume). \nIt may also describe the object’s size (e.g., the radius of a sphere or the \ndimensions of a box). But a shape is usually deﬁ ned with its center \nat the origin and in some sort of canonical orientation relative to the \ncoordinate axes. To be useful, a shape must therefore be transformed in \norder to position and orient it appropriately in world space.\nMany of the objects in a game are dynamic. Moving an arbitrarily \n2. \ncomplex shape through space could be expensive if we had to move \nthe features of the shape (vertices, planes, etc.) individually. But with a \ntransform, any shape can be moved in space inexpensively, no matt er \nhow simple or complex the shape’s features may be.\nThe information describing some of the more-complex kinds of shapes \n3. \ncan take up a non-trivial amount of memory. So it can be beneﬁ cial to \npermit more than one collidable to share a single shape description. For \nexample, in a racing game, the shape information for many of the cars \nmight be identical. In that case, all of the car collidables in the game can \nshare a single car shape.\nAny particular object in the game may have no collidable at all (if it doesn’t \nrequire collision detection services), a single collidable (if the object is a simple \nrigid body), or multiple collidables (each representing one rigid component of \nan articulated robot arm, for example).\n12.3.2. The Collision/Physics World\nA collision system typically keeps track of all of its collidable entities via a \nsingleton data structure known as the collision world . The collision world is a \ncomplete representation of the game world designed explicitly for use by the \ncollision detection system. Havok’s collision world is an instance of the class \nhkpWorld. Likewise, the PhysX world is an instance of NxScene. ODE uses \nan instance of class dSpace to represent the collision world; it is actually the \nroot of a hierarchy of geometric volumes representing all the collidable shapes \nin the game.\nMaintaining all collision information in a private data structure has a \nnumber of advantages over att empting to store collision information with the \ngame objects themselves. For one thing, the collision world need only contain \ncollidables for those game objects that can potentially collide with one another. \n12.3. The Collision Detection System\n",
      "content_length": 2679,
      "extraction_method": "Direct"
    },
    {
      "page_number": 628,
      "chapter": null,
      "content": "606 \n12. Collision and Rigid Body Dynamics\nThis eliminates the need for the collision system to iterate over any irrelevant \ndata structures. This design also permits collision data to be organized in the \nmost eﬃ  cient manner possible. The collision system can take advantage of \ncache coherency to maximize performance, for example. The collision world \nis also an eﬀ ective encapsulation mechanism, which is generally a plus from \nthe perspectives of understandability, maintainability, testability, and the po-\ntential for soft ware reuse.\n12.3.2.1. The Physics World\nIf a game has a rigid body dynamics system, it is usually tightly integrated \nwith the collision system. It typically shares its “world” data structure with \nthe collision system, and each rigid body in the simulation is usually associat-\ned with a single collidable in the collision system. This design is commonplace \namong physics engines because of the frequent and detailed collision queries \nrequired by the physics system. It’s typical for the physics system to actually \ndrive the operation of the collision system, instructing it to run collision tests \nat least once, and sometimes multiple times, per simulation time step. For this \nreason, the collision world is oft en called the collision/physics world or some-\ntimes just the physics world.\nEach dynamic rigid body in the physics simulation is usually associated \nwith a single collidable object in the collision system (although not all collid-\nables need be dynamic rigid bodies). For example, in Havok, a rigid body is \nrepresented by an instance of the class hkpRigidBody, and each rigid body \nhas a pointer to exactly one hkpCollidable. In PhysX, the concepts of collid-\nable and rigid body are comingled—the NxActor class serves both purposes \n(although the physical properties of the rigid body are stored separately, in an \ninstance of NxBodyDesc). In both SDKs, it is possible to tell a rigid body that \nits location and orientation are to be ﬁ xed in space, meaning that it will be \nomitt ed from the dynamics simulation and will serve as a collidable only.\nDespite this tight integration, most physics SDKs do make at least some \natt empt to separate the collision library from the rigid body dynamics simu-\nlation. This permits the collision system to be used as a standalone library \n(which is important for games that don’t need physics but do need to detect \ncollisions). It also means that a game studio could theoretically replace a phys-\nics SDK’s collision system entirely, without having to rewrite the dynamics \nsimulation. (Practically speaking, this may be a bit harder than it sounds!)\n12.3.3. Shape Concepts\nA rich body of mathematical theory underlies the everyday concept of shape \n(see htt p://en.wikipedia.org/wiki/Shape). For our purposes, we can think of \n",
      "content_length": 2825,
      "extraction_method": "Direct"
    },
    {
      "page_number": 629,
      "chapter": null,
      "content": "607 \n12.3. The Collision Detection System\na shape simply as a region of space described by a boundary, with a deﬁ nite \ninside and outside. In two dimensions, a shape has area, and its boundary is \ndeﬁ ned either by a curved line or by three or more straight edges (in which \ncase it’s a polygon ). In three dimensions, a shape has volume, and its boundary \nis either a curved surface or is composed of polygons (in which case is it called \na polyhedron ).\nIt’s important to note that some kinds of game objects, like terrain, rivers, \nor thin walls, might be best represented by surfaces . In three-space, a surface \nis a two-dimensional geometric entity with a front and a back but no inside \nor outside. Examples include planes, triangles, subdivision surfaces, and sur-\nfaces constructed from a group of connected triangles or other polygons. Most \ncollision SDKs provide support for surface primitives and extend the term \nshape to encompass both closed volumes and open surfaces.\nIt’s commonplace for collision libraries to allow surfaces to be given vol-\nume via an optional extrusion parameter. Such a parameter speciﬁ es how \n“thick” a surface should be. Doing this helps reduce the occurrence of missed \ncollisions between small, fast-moving objects and inﬁ nitesimally thin surfaces \n(the so-called “bullet through paper” problem—see Section 12.3.5.7).\n12.3.3.1. Intersection\nWe all have an intuitive notion of what an intersection is. Technically speak-\ning, the term comes from set theory (htt p://en.wikipedia.org/wiki/Intersec-\ntion_(set_theory)). The intersection of two sets is comprised of the subset of \nmembers that are common to both sets. In geometrical terms, the intersection \nbetween two shapes is just the (inﬁ nitely large!) set of all points that lie inside \nboth shapes.\n12.3.3.2. Contact\nIn games, we’re not usually interested in ﬁ nding the intersection in the strict-\nest sense, as a set of points. Instead, we want to know simply whether or not \ntwo objects are intersecting. In the event of a collision, the collision system will \nusually provide additional information about the nature of the contact . This \ninformation allows us to separate the objects in a physically plausible and ef-\nﬁ cient way, for example.\nCollision systems usually package contact information into a convenient \ndata structure that can be instanced for each contact detected. For example, \nHavok returns contacts as instances of the class hkContactPoint. Contact \ninformation oft en includes a separating vector —a vector along which we can \nslide the objects in order to eﬃ  ciently move them out of collision. It also typi-\ncally contains information about which two collidables were in contact, in-\n",
      "content_length": 2713,
      "extraction_method": "Direct"
    },
    {
      "page_number": 630,
      "chapter": null,
      "content": "608 \n12. Collision and Rigid Body Dynamics\ncluding which individual shapes were intersecting and possibly even which \nindividual features of those shapes were in contact. The system may also re-\nturn additional information, such as the velocity of the bodies projected onto \nthe separating normal.\n12.3.3.3. Convexity\nOne of the most important concepts in the ﬁ eld of collision detection is the \ndistinction between convex and non-convex (i.e., concave ) shapes. Technically, a \nconvex shape is deﬁ ned as one for which no ray originating inside the shape \nwill pass through its surface more than once. A simple way to determine if a \nshape is convex is to imagine shrink-wrapping it with plastic ﬁ lm—if it’s con-\nvex, no air pockets will be left  under the ﬁ lm. So in two dimensions, circles, \nrectangles and triangles are all convex, but Pac Man is not. The concept ex-\ntends equally well to three dimensions.\nThe property of convexity is important because, as we’ll see, it’s generally \nsimpler and less computationally intensive to detect intersections between \nconvex shapes than concave ones. See htt p://en.wikipedia.org/wiki/Convex \nfor more information about convex shapes.\n12.3.4. Collision Primitives\nCollision detection systems can usually work with a relatively limited set of \nshape types. Some collision systems refer to these shapes as collision primitives \nbecause they are the fundamental building blocks out of which more-complex \nshapes can be constructed. In this section, we’ll take a brief look at some of the \nmost common types of collision primitives.\n12.3.4.1. Spheres\nThe simplest three-dimensional volume is a sphere . And as you might expect, \nspheres are the most eﬃ  cient kind of collision primitive. A sphere is repre-\nsented by a center point and a radius. This information can be conveniently \npacked into a four-element ﬂ oating-point vector—a format that works par-\nticularly well with SIMD math libraries.\n12.3.4.2. Capsules\nA capsule is a pill-shaped volume, composed of a cylinder and two hemispher-\nical end caps. It can be thought of as a swept sphere —the shape that is traced \nout as a sphere moves from point A to point B. (There are, however, some \nimportant diﬀ erences between a static capsule and a sphere that sweeps out a \ncapsule-shaped volume over time, so the two are not identical.) Capsules are \noft en represented by two points and a radius (Figure 12.2). Capsules are more \n",
      "content_length": 2435,
      "extraction_method": "Direct"
    },
    {
      "page_number": 631,
      "chapter": null,
      "content": "609 \n12.3. The Collision Detection System\neﬃ  cient to intersect than cylinders or boxes, so they are oft en used to model \nobjects that are roughly cylindrical, such as the limbs of a human body.\n12.3.4.3. Axis-Aligned Bounding Boxes\nAn axis-aligned bounding box (AABB) is a rectangular volume (technically \nknown as a cuboid) whose faces are parallel to the axes of the coordinate sys-\ntem. Of course, a box that is axis-aligned in one coordinate system will not \nnecessarily be axis-aligned in another. So we can only speak about an AABB in \nthe context of the particular coordinate frame(s) with which it aligns.\nAn AABB can be conveniently deﬁ ned by two points: one containing the \nminimum coordinates of the box along each of the three principal axes and the \nother containing its maximum coordinates. This is depicted in Figure 12.3.\nThe primary beneﬁ t of axis-aligned boxes is that they can be tested for \ninterpenetration with other axis-aligned boxes in a highly eﬃ  cient manner. \nThe big limitation of using AABBs is that they must remain axis-aligned at \nall times if their computational advantages are to be maintained. This means \nthat if an AABB is used to approximate the shape of an object in the game, \nthe AABB will have to be recalculated whenever that object rotates. Even if \nan object is roughly box-shaped, its AABB may degenerate into a very poor \napproximation to its shape when the object rotates oﬀ -axis. This is shown in \nFigure 12.4.\nr\nr\nP2\nP1\nFigure 12.2.  A capsule can be represented by two points and a radius.\ny\nx\nxmin\nxmax\nymin\nymax\nFigure 12.3.  An axis-aligned box. \n",
      "content_length": 1609,
      "extraction_method": "Direct"
    },
    {
      "page_number": 632,
      "chapter": null,
      "content": "610 \n12. Collision and Rigid Body Dynamics\n12.3.4.4. Oriented Bounding Boxes\nIf we permit an axis-aligned box to rotate relative to its coordinate system, \nwe have what is known as an oriented bounding box (OBB). It is oft en repre-\nsented by three half-dimensions (half-width, half-depth, and half-height) and \na transformation, which positions the center of the box and deﬁ nes its orien-\ntation relative to the coordinate axes. Oriented boxes are a commonly used \ncollision primitive because they do a bett er job at ﬁ tt ing arbitrarily oriented \nobjects, yet their representation is still quite simple.\n12.3.4.5. Discrete Oriented Polytopes (DOP)\nA discrete oriented polytope (DOP ) is a more-general case of the AABB and \nOBB. It is a convex polytope that approximates the shape of an object. A DOP \ncan be constructed by taking a number of planes at inﬁ nity and sliding them \nalong their normal vectors until they come into contact with the object whose \nshape is to be approximated. An AABB is a 6-DOP in which the plane normals \nare taken parallel to the coordinate axes. An OBB is also a 6-DOP in which \nthe plane normals are parallel to the object’s natural principal axes. A k-DOP \nis constructed from an arbitrary number of planes k. A common method of \nconstructing a DOP is to start with an OBB for the object in question and then \nbevel the edges and/or corners at 45 degrees with additional planes in an at-\ntempt to yield a tighter ﬁ t. An example of a k-DOP is shown in Figure 12.5.\ny\nx\ny\nx\nFigure 12.4.  An AABB is only a good approximation to a box-shaped object when the object’s \nprincipal axes are roughly aligned with the coorindate system’s axes.\nFigure 12.5.  An OBB that has been beveled on all eight corners is known as a 14-DOP.\n",
      "content_length": 1760,
      "extraction_method": "Direct"
    },
    {
      "page_number": 633,
      "chapter": null,
      "content": "611 \n12.3.4.6. Arbitrary Convex Volumes\nMost collision engines permit arbitrary convex volumes to be constructed by \na 3D artist in a package like Maya. The artist builds the shape out of polygons \n(triangles or quads). An oﬀ -line tool analyzes the triangles to ensure that they \nactually do form a convex polyhedron. If the shape passes the convexity test, \nits triangles are converted into a collection of planes (essentially a k-DOP), rep-\nresented by k plane equations, or k points and k normal vectors. (If it is found \nto be non-convex, it can still be represented by a polygon soup—described in \nthe next section.) This approach is depicted in Figure 12.6.\nConvex volumes are more expensive to intersection-test than the simpler \ngeometric primitives we’ve discussed thus far. However, as we’ll see in Sec-\ntion 12.3.5.5, certain highly eﬃ  cient intersection-ﬁ nding algorithms such as \nGJK are applicable to these shapes because they are convex.\n12.3.4.7. Poly Soup\nSome collision systems also support totally arbitrary, non-convex shapes. \nThese are usually constructed out of triangles or other simple polygons. For \nFigure 12.6.  An arbitrary convex volume can be represented by a collection of intersecting \nplanes.\nFigure 12.7.  A poly soup is often used to model complex static surfaces such as terrain or \nbuildings.\n12.3. The Collision Detection System\n",
      "content_length": 1371,
      "extraction_method": "Direct"
    },
    {
      "page_number": 634,
      "chapter": null,
      "content": "612 \n12. Collision and Rigid Body Dynamics\nthis reason, this type of shape is oft en called a polygon soup , or poly soup for \nshort. Poly soups are oft en used to model complex static geometry, such as \nterrain and buildings (Figure 12.7).\nAs you might imagine, detecting collisions with a poly soup is the most \nexpensive kind of collision test. In eﬀ ect, the collision engine must test every \nindividual triangle, and it must also properly handle spurious intersections \nwith triangle edges that are shared between adjacent triangles. As a result, \nmost games try to limit the use of poly soup shapes to objects that will not take \npart in the dynamics simulation.\nDoes a Poly Soup Have an Inside?\nUnlike convex and simple shapes, a poly soup does not necessarily represent \na volume—it can represent an open surface as well. Poly soup shapes oft en \ndon’t include enough information to allow the collision system to diﬀ erenti-\nate between a closed volume and an open surface. This can make it diﬃ  cult to \nknow in which direction to push an object that is interpenetrating a poly soup \nin order to bring the two objects out of collision.\nThankfully, this is by no means an intractable problem. Each triangle in a \npoly soup has a front and a back, as deﬁ ned by the winding order of its verti-\nces. Therefore, it is possible to carefully construct a poly soup shape so that all \nof the polygons’ vertex winding orders are consistent (i.e. adjacent triangles \nalways “face” in the same direction). This gives the entire poly soup a notion \nof “front” and “back.” If we also store information about whether a given poly \nsoup shape is open or closed (presuming that this fact can be ascertained by \noﬀ -line tools), then for closed shapes, we can interpret “front” and “back” to \nmean “outside” and “inside” (or vice-versa, depending on the conventions \nused when constructing the poly soup).\nWe can also “fake” an inside and outside for certain kinds of open poly \nsoup shapes (i.e., surfaces). For example, if the terrain in our game is repre-\nsented by an open poly soup, then we can decide arbitrarily that the front \nof the surface always points away from the Earth. This implies that “front” \nshould always correspond to “outside.” Practically speaking, to make this \nwork, we would probably need to customize the collision engine in some way \nin order to make it aware of our particular choice of conventions.\n12.3.4.8. Compound Shapes\n Some objects that cannot be adequately approximated by a single shape can \nbe approximated well by a collection of shapes. For example, a chair might be \nmodeled out of two boxes—one for the back of the chair and one enclosing the \nseat and all four legs. This is shown in Figure 12.8.\n",
      "content_length": 2735,
      "extraction_method": "Direct"
    },
    {
      "page_number": 635,
      "chapter": null,
      "content": "613 \nA compound shape can oft en be a more-eﬃ  cient alternative to a poly \nsoup for modeling non-convex objects; two or more convex volumes can oft en \nout-perform a single poly soup shape. What’s more, some collision systems \ncan take advantage of the convex bounding volume of the compound shape as \na whole when testing for collisions. In Havok, this is called midphase collision \ndetection. As the example in Figure 12.9 shows, the collision system ﬁ rst tests \nthe convex bounding volumes of the two compound shapes. If they do not \nintersect, the system needn’t test the subshapes for collisions at all.\n12.3.5. Collision Testing and Analytical Geometry\n A collision system makes use of analytical geometry —mathematical descrip-\ntions of three-dimensional volumes and surfaces—in order to detect inter-\nsections between shapes computationally. See htt p://en.wikipedia.org/wiki/\nAnalytic_geometry for more details on this profound and broad area of re-\nsearch. In this section, we’ll brieﬂ y introduce the concepts behind analytical \ngeometry, show a few common examples, and then discuss the generalized \nGJK intersection testing algorithm for arbitrary convex polyhedra.\nFigure 12.8.  A chair can be modeled using a pair of interconnected box shapes.\nB2\nB3\nB1\nB4\nA1\nA2\nSphere A\nSphere B\nA1\nA2\nB1\nB2\nB3\nB4\nBounding Volume \nHierarchies:\nSphere A\nSphere B\nFigure 12.9.  A collision system need only test the subshapes of a pair of compound shapes \nwhen their convex bounding volumes (in this case, Sphere A and Sphere B) are found to be \nintersecting.\n12.3. The Collision Detection System\n",
      "content_length": 1596,
      "extraction_method": "Direct"
    },
    {
      "page_number": 636,
      "chapter": null,
      "content": "614 \n12. Collision and Rigid Body Dynamics\n12.3.5.1. Point versus Sphere\n We can determine whether a point p lies within a sphere by simply forming \nthe separation vector s between the point and the sphere’s center c and then \nchecking its length. If it is greater than the radius of the sphere r, then the \npoint lies outside the sphere; otherwise, it lies inside:\n;\nif \n, then  is inside.\nr\n= −\n≤\ns\nc\np\ns\np\n \n12.3.5.2. Sphere versus Sphere\n Determining if two spheres intersect is almost as simple as testing a point \nagainst a sphere. Again, we form a vector s connecting the center points of the \ntwo spheres. We take its length, and compare it with the sum of the radii of the \ntwo spheres. If the length of the separating vector is less than or equal to the \nsum of the radii, the spheres intersect; otherwise, they do not:\n1\n2\n1\n2\n;\nif \n(\n), then spheres intersect.\nr\nr\n=\n−\n≤\n+\ns\nc\nc\ns\n \n(12.1)\nTo avoid the square root operation inherent in calculating the length of vec-\ntor s, we can simply square the entire equation. So Equation (12.1) becomes\n \n \n \n1\n2\n2\n2\n2\n1\n2\n;\n;\nif \n(\n) , then spheres intersect.\nr\nr\n=\n−\n= ⋅\n≤\n+\ns\nc\nc\ns\ns s\ns\n12.3.5.3. The Separating Axis Theorem\nMost collision detection systems make heavy use of a theorem known as \nthe separating axis theorem (htt p://en.wikipedia.org/wiki/Separating_axis_\ntheorem). It states that if an axis can be found along which the projection of \ntwo convex shapes do not overlap, then we can be certain that the two shapes \ndo not intersect at all. If such an axis does not exist and the shapes are convex, \nthen we know for certain that they do intersect. (If the shapes are concave, then \nthey may not be interpenetrating despite the lack of a separating axis. This is \none reason why we tend to favor convex shapes in collision detection.)\nThis theorem is easiest to visualize in two dimensions. Intuitively, it says \nthat if a line can be found, such that object A is entirely on one side of the line \nand object B is entirely on the other side, then objects A and B do not overlap. \nSuch a line is called a separating line, and it is always perpendicular to the sepa-\nrating axis. So once we’ve found a separating line, it’s a lot easier to convince \nourselves that the theory is in fact correct by looking at the projections of our \nshapes onto the axis that is perpendicular to the separating line.\n",
      "content_length": 2369,
      "extraction_method": "Direct"
    },
    {
      "page_number": 637,
      "chapter": null,
      "content": "615 \nThe projection of a two-dimensional convex shape onto an axis acts like \nthe shadow that the object would leave on a thin wire. It is always a line seg-\nment, lying on the axis, that represents the maximum extents of the object in \nthe direction of the axis. We can also think of a projection as a minimum and \nmaximum coordinate along the axis, which we can write as the fully closed \ninterval [ min\nc\n, max\nc\n]. As you can see in Figure 12.10, when a separating line ex-\nists between two shapes, their projections do not overlap along the separating \naxis. However, the projections may overlap along other, non-separating axes.\nIn three dimensions, the separating line becomes a separating plane, but \nthe separating axis is still an axis (i.e., an inﬁ nite line). Again, the projection of \na three-dimensional convex shape onto an axis is a line segment, which we can \nrepresent by the fully-closed interval [ min\nc\n, max\nc\n].\nSome types of shapes have properties that make the potential separating \naxes obvious. To detect intersections between two such shapes A and B, we can \nproject the shapes onto each potential separating axis in turn and then check \nwhether or not the two projection intervals, [ min\nA\nc\n, max\nA\nc\n] and [ min\nB\nc\n, max\nB\nc\n], are dis-\njoint (i.e., do not overlap). In math terms, the intervals are disjoint if max\nA\nc\n < min\nB\nc\nor if max\nB\nc\n < min\nA\nc\n. If the projection intervals along one of the potential separating \naxes are disjoint, then we’ve found a separating axis, and we know the two \nshapes do not intersect.\nOne example of this principle in action is the sphere-versus-sphere test. \nIf two spheres do not intersect, then the axis parallel to the line segment join-\ning the spheres’ center points will always be a valid separating axis (although \nother separating axes may exist, depending on how far apart the two spheres \nare). To visualize this, consider the limit when the two spheres are just about \nto touch but have not yet come into contact. In that case, the only separating \nA\nB\nNon-Separating Axis\nSeparating Axis\nSeparating \nLine/Plane\nProjection of A\nProjection of B\nA\nB\nFigure 12.10.  The projections of two shapes onto a separating axis are always two disjoint \nline segments. The projections of these same shapes onto a non-separating axis are not \nnecessarily disjoint. If no separating axis exists, the shapes intersect.\n12.3. The Collision Detection System\n",
      "content_length": 2425,
      "extraction_method": "Direct"
    },
    {
      "page_number": 638,
      "chapter": null,
      "content": "616 \n12. Collision and Rigid Body Dynamics\naxis is the one parallel to the center-to-center line segment. As the spheres \nmove apart, we can rotate the separating axis more and more in either direc-\ntion. This is shown in Figure 12.11.\n12.3.5.4. AABB versus AABB\n To determine whether two AABBs are intersecting, we can again apply the \nseparating axis theorem. The fact that the faces of both AABBs are guaranteed \nto lie parallel to a common set of coordinate axes tells us that if a separating \naxis exists, it will be one of these three coordinate axes.\nSo, to test for intersections between two AABBs, which we’ll call A and B, \nwe merely inspect the minimum and maximum coordinates of the two boxes \nalong each axis independently. Along the x-axis, we have the two intervals \n[ min\nA\nx\n, max\nA\nx\n] and [ min\nB\nx\n, max\nB\nx\n], and we have corresponding intervals for the y- and \nz-axes. If the intervals overlap along all three axes, then the two AABBs are in-\ntersecting—in all other cases, they are not. Examples of intersecting and non-\nintersecting AABBs are shown in Figure 12.12 (simpliﬁ ed to two dimensions \nfor the purposes of illustration). For an in-depth discussion of AABB collision, \nsee htt p://www.gamasutra.com/features/20000203/lander_01.htm.\nSeparating\nLine/Plane\nSeparating Axis\nMany\nSeparating Axes\nMany\nSeparating\nLines/Planes\nFigure 12.11.  When two spheres are an inﬁ nitesimal distance apart, the only separating axis \nlies parallel to the line segment formed by the two spheres’ center points.\ny\nx\ny\nx\nFigure 12.12.  A two-dimensional example of intersecting and non-intersecting AABBs. Notice \nthat even though the second pair of AABBs are intersecting along the x-axis, they are not \nintersecting along the y-axis.\n",
      "content_length": 1747,
      "extraction_method": "Direct"
    },
    {
      "page_number": 639,
      "chapter": null,
      "content": "617 \n12.3.5.5. Detecting Convex Collisions: The GJK Algorithm\nA very eﬃ  cient algorithm exists for detecting intersections between arbitrary \nconvex polytopes (i.e. convex polygons in two dimensions or convex polyhe-\ndra in three dimensions). It is known as the GJK algorithm, named aft er its \ninventors, E. G. Gilbert, D. W. Johnson, and S. S. Keerthi of the University \nof Michigan. Many papers have been writt en on the algorithm and its vari-\nants, including the original paper (htt p://ieeexplore.ieee.org/xpl/freeabs_all.\njsp?&arnumber=2083), an excellent SIGGRAPH PowerPoint presentation by \nChrister Ericson (htt p://realtimecollisiondetection.net/pubs/SIGGRAPH04_\nEricson_the_GJK_algorithm.ppt), and another great PowerPoint presentation \nby Gino van den Bergen (www.laas.fr/~nic/MOVIE/Workshop/Slides/Gino.\nvander.Bergen.ppt). However, the easiest-to-understand (and most entertain-\ning) description of the algorithm is probably Casey Muratori’s instructional \nvideo entitled, “Implementing GJK,” available online at htt p://mollyrocket.\ncom/353. Because these descriptions are so good, I’ll just give you a feel for the \nessence of the algorithm here and then direct you to the Molly Rocket website \nand the other references cited above for additional details.\nThe GJK algorithm relies on a geometric operation known as the Minkows-\nki diﬀ erence . This fancy-sounding operation is really quite simple: We take \nevery point that lies within shape B and subtract it pairwise from every point \ninside shape A. The resulting set of points { (Ai – Bj) } is the Minkowski dif-\nference.\nThe useful thing about the Minkowski diﬀ erence is that, when applied \nto two convex shapes, it will contain the origin if and only if those two shapes \nintersect. Proof of this statement is a bit beyond our scope, but we can intuit \nwhy it is true by remembering that when we say two shapes A and B intersect, \nwe really mean that there are points within A that are also within B. During the \nprocess of subtracting every point in B from every point in A, we would ex-\npect to eventually hit one of those shared points that lies within both shapes. \nA point minus itself is all zeros, so the Minkowski diﬀ erence will contain the \norigin if (and only if) sphere A and sphere B have points in common. This is \nillustrated in Figure 12.13.\nThe Minkowski diﬀ erence of two convex shapes is itself a convex shape. \nAll we care about is the convex hull of the Minkowski diﬀ erence, not all of the \ninterior points. The basic procedure of GJK is to try to ﬁ nd a tetrahedron (i.e., \na four-sided shape made out of triangles) that lies on the convex hull of the \nMinkwoski diﬀ erence and that encloses the origin. If one can be found, then \nthe shapes intersect; if one cannot be found, then they don’t.\nA tetrahedron is just one case of a geometrical object known as a simplex . \nBut don’t let that name scare you—a simplex is just a collection of points. A \n12.3. The Collision Detection System\n",
      "content_length": 2985,
      "extraction_method": "Direct"
    },
    {
      "page_number": 640,
      "chapter": null,
      "content": "618 \n12. Collision and Rigid Body Dynamics\nsingle-point simplex is a point, a two-point simplex is a line segment, a three-\npoint simplex is a triangle, and a four-point simplex is a tetrahedron (see Fig-\nure 12.14).\nGJK is an iterative algorithm that starts with a one-point simplex lying \nanywhere within the Minkowski diﬀ erence hull. It then att empts to build \nhigher-order simplexes that might potentially contain the origin. During each \niteration of the loop, we take a look at the simplex we currently have and \ndetermine in which direction the origin lies relative to it. We then ﬁ nd a sup-\nporting vertex of the Minkowski diﬀ erence in that direction—i.e., the vertex \nof the convex hull that is closest to the origin in the direction we’re currently \ngoing. We add that new point to the simplex, creating a higher-order simplex \n(i.e., a point becomes a line segment, a line segment becomes a triangle, and \na triangle becomes a tetrahedron). If the addition of this new point causes the \nsimplex to surround the origin, then we’re done—we know the two shapes \nintersect. On the other hand, if we are unable to ﬁ nd a supporting vertex that \nis closer to the origin than the current simplex, then we know that we can \nnever get there, which implies that the two shapes do not intersect. This idea \nis illustrated in Figure 12.15.\nContains the Origin\ny\nx\nA – B\nDoes not Contain \nthe Origin\ny\nA – B\nA\nB\nA\nB\nx\nFigure 12.13.  The Minkowski difference of two intersecting convex shapes contains the origin, \nbut the Minkowski difference of two non-intersecting shapes does not.\nLine Segment\nPoint\nTriangle\nTetrahedron\nFigure 12.14.  Simplexes containing one, two, three, and four points.\n",
      "content_length": 1696,
      "extraction_method": "Direct"
    },
    {
      "page_number": 641,
      "chapter": null,
      "content": "619 \nTo truly understand the GJK algorithm, you’ll need to check out the pa-\npers and video I refernce above. But hopefully this description will whet your \nappetite for deeper investigation. Or, at the very least, you can impress your \nfriends by dropping the name “GJK” at parties.\n12.3.5.6. Other Shape-Shape Combinations\nWe won’t cover any of the other shape-shape intersection combinations here, \nas they are covered well in other texts such as [12], [41], and [9]. The key point \nto recognize here, however, is that the number of shape-shape combinations is \nvery large. In fact, for N shape types, the number of pairwise tests required \nis O(N2). Much of the complexity of a collision engine arises because of the \nsheer number of intersection cases it must handle. This is one reason why \nthe authors of collision engines usually try to limit the number of primitive \ntypes—doing so drastically reduces the number of cases the collision detector \nmust handle. (This is also why GJK is popular—it handles collision detection \nbetween all convex shape types in one fell swoop. The only thing that diﬀ ers \nfrom shape type to shape type is the support function used in the algorithm.)\nThere’s also the practical matt er of how to implement the code that se-\nlects the appropriate collision-testing function given two arbitrary shapes that \nare to be tested. Many collision engines use a double dispatch method (htt p://\nen.wikipedia.org/wiki/Double_dispatch). In single dispatch (i.e., virtual func-\ntions), the type of a single object is used to determine which concrete imple-\nmentation of a particular abstract function should be called at runtime. Dou-\nble dispatch extends the virtual function concept to two object types. It can \nbe implemented via a two-dimensional function look-up table keyed by the \ntypes of the two objects being tested. It can also be implemented by arrang-\ning for a virtual function based on the type of object A to call a second virtual \nfunction based on the type of object B.\nLet’s take a look at a real-world example. Havok uses objects known as \ncollision agents (classes derived from hkCollisionAgent) to handle specif-\nNew Point\ny\nx\nNew Point\ny\nx\nSearch \nDirection\nSearch \nDirection\nFigure 12.15.  In the GJK algorithm, if adding a point to the current simplex creates a shape that \ncontains the origin, we know the shapes intersect; if there is no supporting vertex that will \nbring the simplex any closer to the origin, then the shapes do not intersect.\n12.3. The Collision Detection System\n",
      "content_length": 2536,
      "extraction_method": "Direct"
    },
    {
      "page_number": 642,
      "chapter": null,
      "content": "620 \n12. Collision and Rigid Body Dynamics\nic intersection test cases. Concrete agent classes include hkpSphereSphere\nAgent, hkpSphereCapsuleAgent, hkpGskConvexConvexAgent, and so on. \nThe agent types are referenced by what amounts to a two-dimensional dis-\npatch table, managed by the class hkpCollisionDispatcher. As you’d ex-\npect, the dispatcher’s job is to eﬃ  ciently look up the appropriate agent given \na pair of collidables that are to be collision-tested and then call it, passing the \ntwo collidables as arguments.\n12.3.5.7. Detecting Collisions Between Moving Bodies\n Thus far, we’ve considered only static intersection tests between stationary ob-\njects. When objects move, this introduces some additional complexity. Motion \nin games is usually simulated in discrete time steps. So one simple approach is \nto treat the positions and orientations of each rigid body as stationary at each \ntime step and use static intersection tests on each “snapshot” of the collision \nworld. This technique works as long as objects aren’t moving too fast relative \nto their sizes. In fact, it works so well that many collision/physics engines, \nincluding Havok, use this approach by default.\nHowever, this technique breaks down for small, fast-moving objects. \nImagine an object that is moving so fast that it covers a distance larger than \nits own size (measured in the direction of travel) between time steps. If we \nwere to overlay two consecutive snapshots of the collision world, we’d notice \nthat there is now a gap between the fast-moving object’s images in the two \nsnapshots. If another object happens to lie within this gap, we’ll miss the colli-\nsion with it entirely. This problem, illustrated in Figure 12.16, is known as the \n“bullet through paper” problem, also known as “tunneling.” The following \nsections describe a number of common ways to overcome this problem.\nFigure 12.16.  A small, fast-moving object can leave gaps in its motion path between consecutive \nsnapshots of the collision world, meaning that collisions might be missed entirely.\nSwept Shapes\nOne way to avoid tunneling is to make use of swept shapes . A swept shape is \na new shape formed by the motion of a shape from one point to another over \ntime. For example, a swept sphere is a capsule, and a swept triangle is a trian-\ngular prism (see Figure 12.17).\n",
      "content_length": 2342,
      "extraction_method": "Direct"
    },
    {
      "page_number": 643,
      "chapter": null,
      "content": "621 \nRather than testing static snapshots of the collision world for intersec-\ntions, we can test the swept shapes formed by moving the shapes from their \npositions and orientations in the previous snapshot to their positions and ori-\nentations in the current snapshot. This approach amounts to linearly interpo-\nlating the motion of the collidables between snapshots, because we generally \nsweep the shapes along line segments from snapshot to snapshot.\nOf course, linear interpolation may not be a good approximation of the \nmotion of a fast-moving collidable. If the collidable is following a curved path, \nthen theoretically we should sweep its shape along that curved path. Unfortu-\nnately, a convex shape that has been swept along a curve is not itself convex, \nso this can make our collision tests much more complex and computationally \nintensive.\nIn addition, if the convex shape we are sweeping is rotating, the resulting \nswept shape is not necessarily convex, even when it is swept along a line seg-\nment. As Figure 12.18 shows, we can always form a convex shape by linearly \nextrapolating the extreme features of the shapes from the previous and cur-\nrent snapshots—but the resulting convex shape is not necessarily an accurate \nrepresentation of what the shape really would have done over the time step. \nPut another way, a linear interpolation is not appropriate in general for ro-\ntating shapes. So unless our shapes are not permitt ed to rotate, intersection \nFigure 12.17.  A swept sphere is a capsule; a swept triangle is a triangular prism.\nFigure 12.18.  A rotating object swept along a line segment does not necessarily generate a \nconvex shape (left). A linear interpolation of the motion does form a convex shape (right), but \nit can be a fairly inaccurate approximation of what actually happened during the time step.\n12.3. The Collision Detection System\n",
      "content_length": 1879,
      "extraction_method": "Direct"
    },
    {
      "page_number": 644,
      "chapter": null,
      "content": "622 \n12. Collision and Rigid Body Dynamics\ntesting of swept shapes becomes much more complex and computationally \nintensive than its static snapshot-based counterpart.\nSwept shapes can be a useful technique for ensuring that collisions are \nnot missed between static snapshots of the collision world state. However, the \nresults are generally inaccurate when linearly interpolating curved paths or \nrotating collidables, so more-detailed techniques may be required depending \non the needs of the game.\nContinuous Collision Detection (CCD)\nAnother way to deal with the tunneling problem is to employ a technique \nknown as continuous collision detection (CCD). The goal of CCD is to ﬁ nd the \nearliest time of impact (TOI) between two moving objects over a given time in-\nterval.\nCCD algorithms are generally iterative in nature. For each collidable, we \nmaintain both its position and orientation at the previous time step and its \nposition and orientation at the current time. This information can be used \nto linearly interpolate the position and rotation independently, yielding an \napproximation of the collidable’s transform at any time between the previ-\nous and current time steps. The algorithm then searches for the earliest TOI \nalong the motion path. A number of search algorithms are commonly used, \nincluding Brian Mirtich’s conservative advancement method, performing a ray \ncast on the Minkowski sum, or considering the minimum TOI of individual \nfeature pairs. Erwin Coumans of Sony Computer Entertainment describes \nsome of these algorithms in htt p://www.continuousphysics.com/BulletCon-\ntinuousCollisionDetection.pdf along with his own novel variation on the con-\nservative advancement approach.\n12.3.6. Performance Optimizations\nCollision detection is a CPU-intensive task for two reasons:\nThe calculations required to determine whether two shapes intersect are \n1. \nthemselves non-trivial.\nMost game worlds contain a large number of objects, and the number of in-\n2. \ntersection tests required grows rapidly as the number of objects increases.\nTo detect intersections between n objects, the brute-force technique would \nbe to test every possible pair of objects, yielding an O(n2) algorithm. However, \nmuch more eﬃ  cient algorithms are used in practice. Collision engines typically \nemploy some form of spatial hashing (htt p://research.microsoft .com/~hoppe/\nperfecthash.pdf), spatial subdivision, or hierarchical bounding volumes in or-\nder to reduce the number of intersection tests that must be performed.\n",
      "content_length": 2531,
      "extraction_method": "Direct"
    },
    {
      "page_number": 645,
      "chapter": null,
      "content": "623 \n12.3.6.1. Temporal Coherency\nOne common optimization technique is to take advantage of temporal coher-\nency , also known as frame-to-frame coherency. When collidables are moving at \nreasonable speeds, their positions and orientations are usually quite similar \nfrom time step to time step. We can oft en avoid recalculating certain kinds \nof information every frame by caching the results across multiple time steps. \nFor example, in Havok, collision agents (hkpCollisionAgent) are usually \npersistent between frames, allowing them to reuse calculations from previous \ntime steps as long as the motion of the collidables in question hasn’t invali-\ndated those calculations.\n12.3.6.2. Spatial Partitioning\nThe basic idea of spatial partitioning is to greatly reduce the number of collid-\nables that need to be checked for intersection by dividing space into a number \nof smaller regions. If we can determine (in an inexpensive manner) that a pair \nof collidables do not occupy the same region, then we needn’t perform more-\ndetailed intersection tests on them.\nVarious hierarchical partitioning schemes, such as octrees , binary space \npartitioning (BSP) trees , kd-trees , or sphere trees , can be used to subdivide \nspace for the purposes of collision detection optimization. These trees subdi-\nvide space in diﬀ erent ways, but they all do so in a hierarchical fashion, start-\ning with a gross subdivision at the root of the tree and further subdividing \neach region until suﬃ  ciently ﬁ ne-grained regions have been obtained. The \ntree can then be walked in order to ﬁ nd and test groups of potentially collid-\ning objects for actual intersections. Because the tree partitions space, we know \nthat when we traverse down one branch of the tree, the objects in that branch \ncannot be colliding with objects in other sibling branches.\n12.3.6.3. Broad Phase, Midphase, and Narrow Phase\nHavok uses a three-tiered approach to prune the set of collidables that need to \nbe tested for collisions during each time step.\nz First, gross AABB tests are used to determine which collidables are po-\ntentially intersecting. This is known as broad phase collision detection.\nz Second, the coarse bounding volumes of compound shapes are tested. \nThis is known as midphase collision detection. For example, in a com-\npound shape composed of three spheres, the bounding volume might \nbe a fourth, larger sphere that encloses the other spheres. A compound \nshape may contain other compound shapes, so in general a compound \ncollidable has a bounding volume hierarchy. The midphase traverses \nthis hierarchy in search of subshapes that are potentially intersecting.\n12.3. The Collision Detection System\n",
      "content_length": 2690,
      "extraction_method": "Direct"
    },
    {
      "page_number": 646,
      "chapter": null,
      "content": "624 \n12. Collision and Rigid Body Dynamics\nz Finally, the collidables’ individual primitives are tested for intersection. \nThis is known as narrow phase collision detection.\nThe Sweep and Prune Algorithm\nIn all of the major collision/physics engines (e.g., Havok, ODE, PhysX), broad \nphase collision detection employs an algorithm known as sweep and prune \n(htt p://en.wikipedia.org/wiki/Sweep_and_prune). The basic idea is to sort \nthe minimum and maximum dimensions of the collidables’ AABBs along the \nthree principal axes, and then check for overlapping AABBs by traversing \nthe sorted lists. Sweep and prune algorithms can make use of frame-to-frame \ncoherency (see Section 12.3.6.1) to reduce an O(n log n) sort operation to an \nexpected O(n) running time. Frame coherency can also aid in the updating of \nAABBs when objects rotate.\n12.3.7. Collision Queries\nAnother responsibility of the collision detection system is to answer hypo-\nthetical questions about the collision volumes in the game world. Examples \ninclude the following:\nz If a bullet travels from the player’s weapon in a given direction, what is \nthe ﬁ rst target it will hit, if any?\nz Can a vehicle move from point A to point B without striking anything \nalong the way?\nz Find all enemy objects within a given radius of a character.\nIn general, such operations are known as collision queries .\nThe most common kind of query is a collision cast, sometimes just called a \ncast. (The terms trace and probe are other common synonyms for “cast.”) A cast \ndetermines what, if anything, a hypothetical object would hit if it were to be \nplaced into the collision world and moved along a ray or line segment. Casts \nare diﬀ erent from regular collision detection operations because the entity be-\ning cast is not really in the collision world—it cannot aﬀ ect the other objects \nin the world in any way. This is why we say that a collision cast answers hypo-\nthetical questions about the collidables in the world.\n12.3.7.1. Ray Casting\nThe simplest type of collision cast is a ray cast , although this name is actually a \nbit of a misnomer. What we’re really casting is a directed line segment —in other \nwords, our casts always have a start point (p0) and an end point (p1). (Most \ncollision systems do not support inﬁ nite rays, due to the parametric formula-\ntion used—see below.) The cast line segment is tested against the collidable \n",
      "content_length": 2405,
      "extraction_method": "Direct"
    },
    {
      "page_number": 647,
      "chapter": null,
      "content": "625 \nobjects in the collision world. If it intersects any of them, the contact point or \npoints are returned.\nRay casting systems typically describe the line segment via its start point \np0 and a delta vector d that, when added to p0 , yields the end point p1. Any \npoint on this line segment can be found via the following parametric equation , \nwhere the parameter t is permitt ed to vary between zero and one:\n \n \n \n0\n( )\n,   \n[0, 1].\nt =\n+\n∈\np\np\nd\nt\nt\n \nClearly, p0 = p(0) and p1 = p(1). In addition, any contact point along the seg-\nment can be uniquely described by specifying the value of the parameter t cor-\nresponding to the contact. Most ray casting APIs return their contact points as \n“t values,” or they permit a contact point to be converted into its correspond-\ning t by making an additional function call.\nMost collision detection systems are capable of returning the earliest con-\ntact —i.e., the contact point that lies closest to p0 and corresponds to the small-\nest value of t. Some systems are also capable of returning a complete list of all \ncollidables that were intersected by the ray or line segment. The information \nreturned for each contact typically includes the t value, some kind of unique \nidentiﬁ er for the collidable entity that was hit, and possibly other information \nsuch as the surface normal at the point of contact or other relevant properties \nof the shape or surface that was struck. One possible contact point data struc-\nture is shown below.\nstruct RayCastContact\n{\n F32 \nm_t;            // the t value for this  \n \n         \n    // \ncontact\n U32 \nm_collidableId; // which collidable did we   \n         \n   // hit?\n \nVector  \nm_normal;       // surface normal at  \n \n \n         \n   // contact pt.\n \n// other information...\n};\nApplications of Ray Casts\nRay casts are used heavily in games. For example, we might want to ask the \ncollision system whether character A has a direct line of sight to character B. \nTo determine this, we simply cast a directed line segment from the eyes of \ncharacter A to the chest of character B. If the ray hits character B, we know that \nA can “see” B. But if the ray strikes some other object before reaching character \nB, we know that the line of sight is being blocked by that object. Ray casts \n12.3. The Collision Detection System\n",
      "content_length": 2314,
      "extraction_method": "Direct"
    },
    {
      "page_number": 648,
      "chapter": null,
      "content": "626 \n12. Collision and Rigid Body Dynamics\nare used by weapon systems (e.g., to determine bullet hits), player mechanics \n(e.g., to determine whether or not there is solid ground beneath the character’s \nfeet), AI systems (e.g., line of sight checks, targeting , movement queries, etc.), \nvehicle systems (e.g., to locate and snap the vehicle’s tires to the terrain), and \nso on.\n12.3.7.2. Shape Casting\nAnother common query involves asking the collision system how far an imag-\ninary convex shape would be able to travel along a directed line segment be-\nfore it hits something solid. This is known as a sphere cast when the volume \nbeing cast is a sphere, or a shape cast in general. (Havok calls them linear casts.) \nAs with ray casts, a shape cast is usually described by specifying the start \npoint p0 , the distance to travel d, and of course the type, dimensions, and ori-\nentation of the shape we wish to cast.\nThere are two cases to consider when casting a convex shape.\nThe cast shape is already interpenetrating or contacting at least one other \n1. \ncollidable, preventing it from moving away from its starting location.\nThe cast shape is not intersecting with anything else at its starting loca-\n2. \ntion, so it is free to move a non-zero distance along its path.\nIn the ﬁ rst scenario, the collision system typically reports the contact (s) \nbetween the cast shape and all of the collidables with which it is initially in-\nterpenetrating. These contacts might be inside the cast shape or on its surface, \nas shown in Figure 12.19.\nIn the second case, the shape can move a non-zero distance along the line \nsegment before striking something. Presuming that it hits something, it will \nusually be a single collidable. However, it is possible for a cast shape to strike \nmore than one collidable simultaneously if its trajectory is just right. And of \ncourse, if the impacted collidable is a non-convex poly soup, the cast shape \nContacts\nd\nFigure 12.19.  A cast \nsphere that starts in \npenetration will be un-\nable to move, and the \npossibly many contact \npoints will lie inside the \ncast shape in general.\nContact\nContacts\nd\nd\nFigure 12.20.  If the starting location of a cast shape is not interpenetrating anything, then \nthe shape will move a non-zero distance along its line segment, and its contacts (if any) will \nalways be on its surface.\n",
      "content_length": 2357,
      "extraction_method": "Direct"
    },
    {
      "page_number": 649,
      "chapter": null,
      "content": "627 \nmay end up touching more than one part of the poly soup simultaneously. We \ncan safely say that no matt er what kind of convex shape is cast, it is possible \n(albeit unlikely) for the cast to generate multiple contact points. The contacts \nwill always be on the surface of the cast shape in this case, never inside it (be-\ncause we know that the cast shape was not interpenetrating anything when it \nstarted its journey). This case is illustrated in Figure 12.20.\nAs with ray casts, some shape casting APIs report only the earliest contact (s) \nexperienced by the cast shape, while others allow the shape to continue along \nits hypothetical path, returning all the contacts it experiences on its journey. \nThis is illustrated in Figure 12.21.\nThe contact information returned by a shape cast is necessarily a bit more \ncomplex than it is for a ray cast. We cannot simply return one or more t val-\nues, because a t value only describes the location of the center point of the \nshape along its path. It tells us nothing of where, on the surface or interior of \nthe shape, it came into contact with the impacted collidable. As a result, most \nshape casting APIs return both a t value and the actual contact point, along \nwith other relevant information (such as which collidable was struck, the sur-\nface normal at the contact point, etc.).\nUnlike ray casting APIs, a shape casting system must always be capable of \nreporting multiple contacts . This is because even if we only report the contact \nwith the earliest t value, the shape may have touched multiple distinct collid-\nables in the game world, or it may be touching a single non-convex collidable \nat more than one point. As a result, collision systems usually return an array \nor list of contact point data structures, each of which might look something \nlike this:\nstruct ShapeCastContact\n{\n F32 \n \n \nm_t;            // the t value for this  \n \n         \n    // \ncontact\n U32 \n \n \nm_collidableId; // which collidable did we   \n         \n    // \nhit?\nContact 1\nd\nContact 2\nContact 3\nFigure 12.21.  A shape casting API might return all contacts instead of only the earliest con-\ntact.\n12.3. The Collision Detection System\n",
      "content_length": 2183,
      "extraction_method": "Direct"
    },
    {
      "page_number": 650,
      "chapter": null,
      "content": "628 \n12. Collision and Rigid Body Dynamics\n \nPoint  \nm_contactPoint; // location of actual   \n \n         \n   // contact\n \nVector  \nm_normal;       // surface normal at  \n \n \n         \n   // contact pt.\n \n// other information...\n};\nGiven a list of contact points, we oft en want to distinguish between the \ngroups of contact points for each distinct t value. For example, the earliest \ncontact is actually described by the group of contact points that all share the \nminimum t in the list. It’s important to realize that collision systems may or \nmay not return their contact points sorted by t. If it does not, it’s almost always \na good idea to sort the results by t manually. This ensures that if one looks at \nthe ﬁ rst contact point in the list, it will be guaranteed to be among the earliest \ncontact points along the shape’s path.\nApplications of Shape Casts\nShape casts are extremely useful in games. Sphere casts can be used to de-\ntermine whether the virtual camera is in collision with objects in the game \nworld. Sphere or capsule casts are also commonly used to implement charac-\nter movement . For example, in order to slide the character forward on uneven \nterrain, we can cast a sphere or capsule that lies between the character’s feet \nin the direction of motion. We can adjust it up or down via a second cast, to \nensure that it remains in contact with the ground. If the sphere hits a very \nshort vertical obstruction, such as a street curb, it can “pop up” over the curb. \nIf the vertical obstruction is too tall, like a wall, the cast sphere can be slid \nhorizontally along the wall. The ﬁ nal resting place of the cast sphere becomes \nthe character’s new location next frame.\n12.3.7.3. Phantoms\nSometimes, games need to determine which collidable objects lie within \nsome speciﬁ c volume in the game world. For example, we might want the \nlist of all enemies that are within a certain radius of the player character. \nHavok supports a special kind of collidable object known as a phantom for \nthis purpose.\nA phantom acts much like a shape cast whose distance vector d is zero. \nAt any moment, we can ask the phantom for a list of its contacts with other \ncollidables in the world. It returns this data in essentially the same format that \nwould be returned by a zero-distance shape cast.\nHowever, unlike a shape cast, a phantom is persistent in the collision \nworld. This means that it can take full advantage of the temporal coherency \noptimizations used by the collision engine when detecting collisions between \n",
      "content_length": 2537,
      "extraction_method": "Direct"
    },
    {
      "page_number": 651,
      "chapter": null,
      "content": "629 \n“real” collidables. In fact, the only diﬀ erence between a phantom and a regu-\nlar collidable is that it is “invisible” to all other collidables in the collision \nworld (and it does not take part in the dynamics simulation). This allows it to \nanswer hypothetical questions about what objects it would collide with were \nit a “real” collidable, but it is guaranteed not to have any eﬀ ect of the other \ncollidables—including other phantoms—in the collision world.\n12.3.7.4. Other Types of Queries\nSome collision engines support other kinds of queries in addition to casts. For \nexample, Havok supports closest point queries, which are used to ﬁ nd the set \nof points on other collidables that are closest to a given collidable in the colli-\nsion world.\n12.3.8. Collision Filtering\nIt is quite common for game developers to want to enable or disable collisions \nbetween certain kinds of objects. For example, most objects are permitt ed to \npass through the surface of a body of water —we might employ a buoyancy \nsimulation to make them ﬂ oat, or they might just sink to the bott om, but in \neither case we do not want the water’s surface to appear solid. Most collision \nengines allow contacts between collidables to be accepted or rejected based on \ngame-speciﬁ c critiera. This is known as collision ﬁ ltering .\n12.3.8.1. Collision Masking and Layers\nOne common ﬁ ltering approach is to categorize the objects in the world and \nthen use a look-up table to determine whether certain categories are permitt ed \nto collide with one another or not. For example, in Havok, a collidable can be \na member of one (and only one) collision layer. The default collision ﬁ lter in \nHavok, represented by an instance of the class hkpGroupFilter, maintains \na 32-bit mask for each layer, each bit of which tells the system whether or not \nthat particular layer can collide with one of the other layers.\n12.3.8.2. Collision Callbacks\nAnother ﬁ ltering technique is to arrange for the collision library to invoke a \ncallback function whenever a collision is detected. The callback can inspect the \nspeciﬁ cs of the collision and make the decision to either allow or reject the \ncollision based on suitable criteria. Havok also supports this kind of ﬁ ltering. \nWhen contact points are ﬁ rst added to the world, the contactPointAdded()\ncallback is invoked. If the contact point is later determined to be valid (it may \nnot be if an earlier TOI contact was found), the contactPointConﬁ rmed()\ncallback is invoked. The application may reject contact points in these call-\nbacks if desired.\n12.3. The Collision Detection System\n",
      "content_length": 2616,
      "extraction_method": "Direct"
    },
    {
      "page_number": 652,
      "chapter": null,
      "content": "630 \n12. Collision and Rigid Body Dynamics\n12.3.8.3. Game-Speciﬁ c Collision Materials\nGame developers oft en need to categorize the collidable objects in the game \nworld, in part to control how they collide (as with collision ﬁ ltering) and in \npart to control other secondary eﬀ ects, such as the sound that is made or the \nparticle eﬀ ect that is generated when one type of object hits another. For ex-\nample, we might want to diﬀ erentiate between wood, stone, metal, mud, wa-\nter, and human ﬂ esh.\nTo accomplish this, many games implement a collision shape categoriza-\ntion mechanism similar in many respects to the material system used in the \nrendering engine. In fact, some game teams use the term collision material to \ndescribe this categorization. The basic idea is to associate with each collid-\nable surface a set of properties that deﬁ nes how that particular surface should \nbehave from a physical and collision standpoint. Collision properties can in-\nclude sound and particle eﬀ ects, physical properties like coeﬃ  cient of restitu-\ntion or friction coeﬃ  cients, collision ﬁ ltering information, and whatever other \ninformation the game might require.\nFor simple convex primitives, the collision properties are usually associ-\nated with the shape as a whole. For poly soup shapes, the properties might be \nspeciﬁ ed on a per-triangle basis. Because of this latt er usage, we usually try \nto keep the binding between the collision primitive and its collision material \nas compact as possible. A typical approach is to bind collision primitives to \ncollision materials via an 8-, 16-, or 32-bit integer. This integer indexes into \na global array of data structures containing the detailed collision properties \nthemselves.\n12.4. Rigid Body Dynamics\nMany game engines include a physics system for the purposes of simulating the \nmotion of the objects in the virtual game world in a somewhat physically real-\nistic way. Technically speaking, game physics engines are typically concerned \nwith a particular ﬁ eld of physics known as mechanics . This is the study of how \nforces aﬀ ect the behavior of objects. In a game engine, we are particularly \nconcerned with the dynamics of objects—how they move over time. Until very \nrecently, game physics systems have been focused almost exclusively on a \nspeciﬁ c subdiscipline of mechanics known as classical rigid body dynamics . This \nname implies that in a game’s physics simulation, two important simplifying \nassumptions are made:\nz Classical (Newtonian) mechanics . The objects in the simulation are as-\nsumed to obey Newton’s laws of motion . The objects are large enough \n",
      "content_length": 2639,
      "extraction_method": "Direct"
    },
    {
      "page_number": 653,
      "chapter": null,
      "content": "631 \nthat there are no quantum eﬀ ects, and their speeds are low enough that \nthere are no relativistic eﬀ ects.\nz Rigid bodies . All objects in the simulation are perfectly solid and cannot \nbe deformed. In other words, their shape is constant. This idea meshes \nwell with the assumptions made by the collision detection system. Fur-\nthermore, the assumption of rigidity greatly simpliﬁ es the mathematics \nrequired to simulate the dynamics of solid objects.\nGame physics engines are also capable of ensuring that the motions of \nthe rigid bodies in the game world conform to various constraints . The most \ncommon constraint is that of non-penetration—in other words, objects aren’t \nallowed to pass through one another. Hence the physics system att empts to \nprovide realistic collision responses whenever bodies are found to be interpen-\netrating. This is one of the primary reasons for the tight interconnection be-\ntween the physics engine and the collision detection system.\nMost physics systems also allow game developers to set up other kinds of \nconstraints in order to deﬁ ne realistic interactions between physically simulat-\ned rigid bodies. These may include hinges, prismatic joints (sliders), ball joints, \nwheels, “rag dolls” to emulate unconscious or dead characters, and so on.\nThe physics system usually shares the collision world data structure, and \nin fact it usually drives the execution of the collision detection algorithm as \npart of its time step update routine. There is typically a one-to-one mapping \nbetween the rigid bodies in the dynamics simulation and the collidables man-\naged by the collision engine. For example, in Havok, an hkpRigidBody object \nmaintains a reference to one and only one hkpCollidable (although it is \npossible to create a collidable that has no rigid body). In PhysX, the two con-\ncepts are a bit more tightly integrated—an NxActor serves both as a collidable \nobject and as a rigid body for the purposes of the dynamics simulation. These \nrigid bodies and their corresponding collidables are usually maintained in a \nsingleton data structure known as the collision/physics world, or sometimes just \nthe physics world .\nThe rigid bodies in the physics engine are typically distinct from the logi-\ncal objects that make up the virtual world from a gameplay perspective. The \npositions and orientations of game objects can be driven by the physics simu-\nlation. To accomplish this, we query the physics engine every frame for the \ntransform of each rigid body, and apply it in some way to the transform of \nthe corresponding game object. It’s also possible for a game object’s motion, \nas determined by some other engine system (such as the animation system or \nthe character control system) to drive the position and rotation of a rigid body \nin the physics world. As mentioned in Section 12.3.1, a single logical game \nobject may be represented by one rigid body in the physics world, or by many. \n12.4. Rigid Body Dynamics\n",
      "content_length": 2982,
      "extraction_method": "Direct"
    },
    {
      "page_number": 654,
      "chapter": null,
      "content": "632 \n12. Collision and Rigid Body Dynamics\nA simple object like a rock, weapon or barrel, might correspond to one rigid \nbody. But an articulated character or a complex machine might be composed \nof many interconnected rigid pieces.\nThe remainder of this chapter will be devoted to investigating how game \nphysics engines work. We’ll brieﬂ y introduce the theory that underlies rigid \nbody dynamics simulations. Then we’ll investigate some of the most common \nfeatures of a game physics system and have a look at how a physics engine \nmight be integrated into a game.\n12.4.1. Some Foundations\nA great many excellent books, articles, and slide presentations have been \nwritt en on the topic of classical rigid body dynamics . A solid foundation in \nanalytical mechanics theory can be obtained from [15]. Even more relevant \nto our discussion are texts like [34], [11], and [25], which have been writt en \nspeciﬁ cally about the kind of physics simulations done by games. Other texts, \nlike [1], [9], and [28], include chapters on rigid body dynamics for games. \nChris Hecker wrote a series of helpful articles on the topic of game physics for \nGame Developer Magazine; Chris has posted these and a variety of other useful \nresources at htt p://chrishecker.com/Rigid_Body_Dynamics. An informative \nslide presentation on dynamics simulation for games was produced by Rus-\nsell Smith, the primary author of ODE; it is available at htt p://www.ode.org/\nslides/parc/dynamics.pdf.\nIn this section, I’ll summarize the fundamental theoretical concepts that \nunderlie the majority of game physics engines. This will be a whirlwind tour \nonly, and by necessity I’ll have to omit some details. Once you’ve read this \nchapter, I strongly encourage you to read at least a few of the additional re-\nsources cited above.\n12.4.1.1. Units\nMost rigid body dynamics simulations operate in the MKS system of units . In \nthis system, distance is measured in meters (abbreviated “m”), mass is mea-\nsured in kilograms (abbreviated “kg”), and time is measured in seconds (ab-\nbreviated “s”). Hence the name MKS.\nYou could conﬁ gure your physics system to use other units if you wanted \nto, but if you do this, you need to make sure everything in the simulation \nis consistent. For example, constants like the acceleration due to gravity g, \nwhich is measured in m/s2 in the MKS system, would have to be re-expressed \nin whatever unit system you select. Most game teams just stick with MKS to \nkeep life simple.\n",
      "content_length": 2486,
      "extraction_method": "Direct"
    },
    {
      "page_number": 655,
      "chapter": null,
      "content": "633 \n12.4.1.2. Separability of Linear and Angular Dynamics\nAn unconstrained rigid body is one that can translate freely along all three \nCartesian axes and that can rotate freely about these three axes as well. We say \nthat such a body has six degrees of freedom (DOF).\nIt is perhaps somewhat surprising that the motion of an unconstrained \nrigid body can be separated into two independent components:\nz Linear dynamics . This is a description of the motion of the body when \nwe ignore all rotational eﬀ ects. (We can use linear dynamics alone to \ndescribe the motion of an idealized point mass—i.e., a mass that is inﬁ ni-\ntesimally small and cannot rotate.)\nz Angular dynamics . This is a description of the rotational motion of the body.\nAs you can well imagine, this ability to separate the linear and angular com-\nponents of a rigid body’s motion is extremely helpful when analyzing or sim-\nulating its behavior. It means that we can calculate a body’s linear motion \nwithout regard to rotation—as if it were an idealized point mass—and then \nlayer its angular motion on top in order to arrive at a complete description of \nthe body’s motion.\n12.4.1.3. Center of Mass\nFor the purposes of linear dynamics, an unconstrained rigid body acts as \nthough all of its mass were concentrated at a single point known as the center \nof mass (abbreviated CM, or sometimes COM). The center of mass is essen-\ntially the balancing point of the body for all possible orientations. In other \nwords, the mass of a rigid body is distributed evenly around its center of mass \nin all directions.\nFor a body with uniform density, the center of mass lies at the centroid of \nthe body. That is, if we were to divide the body up into N very small pieces, \nadd up the positions of all these pieces as a vector sum, and then divide by the \nnumber of pieces, we’d end up with a prett y good approximation to the loca-\ntion of the center of mass. If the body’s density is not uniform, the position of \neach litt le piece would need to be weighted by that piece’s mass, meaning that \nin general the center of mass is really a weighted average of the pieces’ positions. \nSo we have\n \n \n \n \n,\ni\ni\ni\ni\ni\ni\nCM\ni\ni\nm\nm\nm\nm\n∀\n∀\n∀\n=\n=\n∑\n∑\n∑\nr\nr\nr\n \nwhere the symbol r represents a radius vector or position vector —i.e., a vector \nextending from the world space origin to the point in question. (These sums \n12.4. Rigid Body Dynamics\n",
      "content_length": 2402,
      "extraction_method": "Direct"
    },
    {
      "page_number": 656,
      "chapter": null,
      "content": "634 \n12. Collision and Rigid Body Dynamics\nbecome integrals in the limit as the sizes and masses of the litt le pieces ap-\nproach zero.)\nThe center of mass always lies inside a convex body, although it may actu-\nally lie outside the body if it is concave. (For example, where would the center \nof mass of the lett er “C” lie?)\n12.4.2. Linear Dynamics\nFor the purposes of linear dynamics , the position of a rigid body can be fully \ndescribed by a position vector rCM that extends from the world space origin \nto the center of mass of the body, as shown in Figure 12.22. Since we’re using \nthe MKS system, position is measured in meters (m). For the remainder of \nthis discussion, we’ll drop the CM subscripts, as it is understood that we are \ndescribing the motion of the body’s center of mass.\ny\nx\nr CM\nFigure 12.22.  For the purposes of linear dynamics, the position of a rigid body can be fully \ndescribed by the position of its center of mass.\n12.4.2.1. Linear Velocity and Acceleration\nThe linear velocity of a rigid body deﬁ nes the speed and direction in which the \nbody’s CM is moving. It is a vector quantity, typically measured in meters per \nsecond (m/s). Velocity is the ﬁ rst time derivative of position, so we can write\n \nv\nr\nr\n( )\n( )\n( ),\nt\nd t\ndt\nt\n=\n= \u0002\n \nwhere the dot over the vector r denotes taking the derivative with respect to \ntime. Diﬀ erentiating a vector is the same as diﬀ erentiating each component \nindependently, so\n \nv t\ndr t\ndt\nr t\nx\nx\nx\n( )\n( )\n( ),\n=\n= \u0002\nand so on for the y- and z-components.\nLinear acceleration is the ﬁ rst derivative of linear velocity with respect to \ntime, or the second derivative of the position of a body’s CM versus time. Accel-\neration is a vector quantity, usually denoted by the symbol a. So we can write\n",
      "content_length": 1772,
      "extraction_method": "Direct"
    },
    {
      "page_number": 657,
      "chapter": null,
      "content": "635 \na\nv\nv\nr\nr\n( )\n( )\n( )\n( )\n( ).\nt\nd\nt\ndt\nt\nd\nt\ndt\nt\n=\n=\n=\n=\n\u0002\n\u0002\u0002\n2\n2\n12.4.2.2. Force and Momentum\nA force is deﬁ ned as anything that causes an object with mass to accelerate or \ndecelerate. A force has both a magnitude and a direction in space, so all forces \nare represented by vectors. A force is oft en denoted by the symbol F. When N \nforces are applied to a rigid body, their net eﬀ ect on the body’s linear motion \nis found by simply adding up the force vectors:\n \nnet\n1\n.\nN\ni\ni=\n=∑\nF\nF  \n Newton’s famous Second Law states that force is proportional to accelera-\ntion and mass:\n \nF\na\nr\n( )\n( )\n( ).\nt\nm\nt\nm\nt\n=\n=\n \n \u0002\u0002\n \n(12.2)\nAs Newton’s law implies, force is measured in units of kilogram-meters per \nsecond squared (kg-m/s2). This unit is also called the Newton.\nWhen we multiply a body’s linear velocity by its mass, the result is a \nquantity known as linear momentum . It is customary to denote linear momen-\ntum with the symbol p:\n \n \n( )\n( ).\nt\nm\nt\n=\np\nv\n \nWhen mass is constant, Equation (12.2) holds true. But if mass is not con-\nstant, as would be the case for a rocket whose fuel is being gradually used up \nand converted into energy, Equation (12.2) is not exactly correct. The proper \nformulation is actually as follows:\n \n \n( )\n(\n( ))\n( )\n,\nd\nt\nd m\nt\nt\ndt\ndt\n=\n=\np\nv\nF\nwhich of course reduces to the more familiar F = ma when the mass is constant \nand can be brought outside the derivative. Linear momentum is not of much \nconcern to us. However, the concept of momentum will become relevant when \nwe discuss angular dynamics.\n12.4.3. Solving the Equations of Motion\n The central problem in rigid body dynamics is to solve for the motion of \nthe body, given a set of known forces acting on it. For linear dynamics, this \n12.4. Rigid Body Dynamics\n",
      "content_length": 1774,
      "extraction_method": "Direct"
    },
    {
      "page_number": 658,
      "chapter": null,
      "content": "636 \n12. Collision and Rigid Body Dynamics\nmeans ﬁ nding v(t) and r(t) given knowledge of the net force Fnet(t) and pos-\nsibly other information, such as the position and velocity at some previous \ntime. As we’ll see below, this amounts to solving a pair of ordinary diﬀ er-\nential equations—one to ﬁ nd v(t) given a(t) and the other to ﬁ nd r(t) given \nv(t).\n12.4.3.1. Force as a Function\nA force can be constant, or it can be a function of time as shown above. A force \ncan also be a function of the position of the body, its velocity, or any number \nof other quantities. So in general, the expression for force should really be \nwritt en as follows:\nF\nr\nv\na\nt\nt\nm\nt\n, ( ),\n( ),\n( ).\n \n \n ...\n \n(\n)=\nt\n \n(12.3)\nThis can be rewritt en in terms of the position vector and its ﬁ rst and second \nderivatives as follows:\n \nF\nr\n r\nr\nt \nt\nm\nt \n, (  ), ( )  ,\n( ). \n \n \n ... \n  \n\u0002\n\u0002\u0002\n(\n)= \nt \nFor example, the force exerted by a spring is proportional to how far it has \nbeen stretched away from its natural resting position. In one dimension, with \nthe spring’s resting position at x = 0, we can write\n \n(\n)\n, ( )\n ( ),\nF t x t\nk x t\n=−\nwhere k is the spring constant , a measure of the spring’s stiﬀ ness.\nAs another example, the damping force exerted by a mechanical viscous \ndamper (a so-called dashpot) is proportional to the velocity of the damper’s \npiston. So in one dimension, we can write\n(\n)\n, ( )\n ( ),\nF t v t\nb v t\n=−\nwhere b is a viscous damping coeﬃ  cient.\n12.4.3.2. Ordinary Differential Equations\nIn general, an ordinary diﬀ erential equation (ODE) is an equation involving a \nfunction of one independent variable and various derivatives of that function. \nIf our independent variable is time and our function is x(t), then an ODE is a \nrelation of the form\nd x\ndt\nf\nx t\ndx t\ndt\nd x t\ndt\nd\nx t\ndt\nn\nn\nn\nn\n=\n⎛\n⎝\n−\n−\n,\n( ),\n( ) ,\n( ) ,\n,\n( )\n \n \n \n ...  \n2\n2\n1\n1\n⎜\n⎞\n⎠\n⎟.\nt\nPut another way, the nth derivative of x(t) is expressed as a function f whose \narguments can be time (t), position (x(t)), and any number of derivatives of \nx(t) as long as those derivatives are of lower order than n.\n",
      "content_length": 2104,
      "extraction_method": "Direct"
    },
    {
      "page_number": 659,
      "chapter": null,
      "content": "637 \nAs we saw in Equation (12.3), force is a function of time, position, and velocity \nin general:\n\u0002\u0002\n\u0002\nr\nF\nr\nr\n( )\n, ( ), ( ) .\nt\nm\nt\nt\n=\n(\n)\n1\n t\n \n(12.18)\nThis clearly qualiﬁ es as an ODE. We wish to solve this ODE in order to ﬁ nd \nv(t) and r(t).\n12.4.3.3. Analytical Solutions\nIn some rare situations, the diﬀ erential equations of motion can be solved ana-\nlytically , meaning that a simple, closed-form function can be found that de-\nscribes the body’s position for all possible values of time t. A common example \nis the vertical motion of a projectile under the inﬂ uence of a constant accelera-\ntion due to gravity, a(t) = [ 0, g, 0 ], where g = –9.8 m/s2. In this case, the ODE \nof motion boils down to\n \n\u0002\u0002y t\ng\n( )\n.\n=\nIntegrating once yields\n \n\u0002y t\ngt\nv\n( )\n,\n=\n+\n0\nwhere v0 is the vertical velocity at time t = 0. Integrating a second time yields \nthe familiar solution\n \ny t\ngt\nv t\ny\n( )\n,\n=\n+\n+\n1\n2\n2\n0\n0  \nwhere y0 is the initial vertical position of the object.\nHowever, analytical solutions are almost never possible in game physics. \nThis is due in part to the fact that closed-form solutions to some diﬀ erential \nequations are simply not known. Moreover, a game is an interactive simula-\ntion, so we cannot predict how the forces in a game will behave over time. This \nmakes it impossible to ﬁ nd simple, closed-form expressions for the positions \nand velocities of the objects in the game as functions of time.\n12.4.4. Numerical Integration\nFor the reasons cited above, game physics engines turn to a technique known \nas numerical integration . With this technique, we solve our diﬀ erential equa-\ntions in a time-stepped manner—using the solution from a previous time step \nto arrive at the solution for the next time step. The duration of the time step is \nusually taken to be (roughly) constant and is denoted by the symbol Δt . Given \nthat we know the body’s position and velocity at the current time t1 and that \nthe force is known as a function of time, position, and/or velocity, we wish to \nﬁ nd the position and velocity at the next time step t2 = t1 + Δt. In other words, \ngiven r(t1), v(t1), and F(t, r, v), the problem is to ﬁ nd r(t2) and v(t2).\n12.4. Rigid Body Dynamics\n",
      "content_length": 2210,
      "extraction_method": "Direct"
    },
    {
      "page_number": 660,
      "chapter": null,
      "content": "638 \n12. Collision and Rigid Body Dynamics\n12.4.4.1. Explicit Euler\nOne of the simplest numerical solutions to an ODE is known as the explicit \nEuler method. This is the intuitive approach oft en taken by new game program-\nmers. Let’s assume for the moment that we already know the current velocity \nand that we wish to solve the following ODE to ﬁ nd the body’s position on \nthe next frame:\n \nv\nr\n( )\n( ).\nt = \u0002 t  \n(12.4)\nUsing the explicit Euler method, we simply convert the velocity from meters \nper second into meters per frame by multiplying by the time delta, and then \nwe add “one frame’s worth” of velocity onto the current position in order to \nﬁ nd the new position on the next frame. This yields the following approxi-\nmate solution to the ODE given by Equation (12.4):\n \nr\nr\nv\n(\n)\n( )\n( )\n.\nt\nt\n2\n1\n1\n=\n+\n Δ\nt\nt  \n(12.5)\nWe can take an analogous approach to ﬁ nd the body’s velocity next frame \ngiven the net force acting this frame. Hence, the approximate explicit Euler \nsolution to the ODE\n \n \nis as follows:\n \n \n(12.6)\nInterpretations of Explicit Euler\nWhat we’re really doing in Equation (12.5) is assuming that the velocity of the \nbody is constant during the time step. Therefore, we can use the current velocity \nto predict the body’s position on the next frame. The change in position Δr be-\ntween times t1 and t2 is hence Δr = v(t1) Δt. Graphically, if we imagine a plot of the \nposition of the body versus time, we are taking the slope of the function at time \nΔr\nΔt\nt\nr(t1)\nrapprox(t2)\nr(t2)\nr(t)\nt1\nt2\nslope\n= v(t1)\nΔr\nΔt\nFigure 12.23.  In the explicit Euler method, the slope of r(t) at time t1 is used to linearly \nextrapolate from r(t1) to an estimate of the true value of r(t2).\na\nF\nv\n( )\n( )\n( )\nt\nt\nm\nt\n=\n=\nnet\n\u0002\nv\nv\nF\n( )\n( )\n( )\n.\nt\nt\nm\nt\n2\n1\n1\n=\n+\nnet\nΔ\nt\n",
      "content_length": 1792,
      "extraction_method": "Direct"
    },
    {
      "page_number": 661,
      "chapter": null,
      "content": "639 \nt1 (which is just v(t1)) and extrapolating it linearly to the next time step t2. As \nwe can see in Figure 12.23, linear extrapolation does not necessarily provide us \nwith a particularly good estimate of the true position at the next time step r(t2), \nbut it does work reasonably well as long as the velocity is roughly constant.\nFigure 12.23 suggests another way to interpret the explicit Euler method—\nas an approximation of a derivative . By deﬁ nition, any derivative is the quo-\ntient of two inﬁ nitesimally small diﬀ erences (in our case, dr/dt). The explicit \nEuler method approximates this using the quotient of two ﬁ nite diﬀ erences . In \nother words, dr becomes Δr and dt becomes Δt. This yields\nd\ndt\nt\nt\nt\nt\nt\nt\nr\nr\nv\nr\nr\nr\nr\n≈\n=\n−\n−\n=\n−\nΔ\nΔ\nΔ\n;\n( )\n( )\n( )\n( )\n( ).\n1\n2\n1\n2\n1\n2\n1\nt\nt\nt\nwhich again simpliﬁ es to Equation (12.5). This approximation is really only \nvalid when the velocity is constant over the time step. It is also valid in the \nlimit as Δt tends toward zero (at which point it becomes exactly right). Obvi-\nously, this same analysis can be applied to Equation (12.6) as well.\n12.4.4.2. Properties of Numerical Methods\nWe’ve implied that the explicit Euler method is not particularly accurate. Let’s \npin this idea down more concretely. A numerical solution to an ordinary dif-\nferential equation actually has three important and interrelated properties:\nz Convergence . As the time step Δt tends toward zero, does the approxi-\nmate solution get closer and closer to the real solution?\nz Order . Given a particular numerical approximation to the solution of an \nODE, how “bad” is the error? Errors in numerical ODE solutions are \ntypically proportional to some power of the time step duration Δt, so \nthey are oft en writt en using big “O” notation (e.g., O(Δt2)). We say that \na particular numerical method is of “order n” when its error term is \nO(Δt(n + 1)).\nz Stability . Does the numerical solution tend to “sett le down” over time? \nIf a numerical method adds energy into the system, object velocities will \neventually “explode,” and the system will become unstable. On the other \nhand, if a numerical method tends to remove energy from the system, it \nwill have an overall damping eﬀ ect, and the system will be stable.\nThe concept of order warrants a litt le more explanation. We usually mea-\nsure the error of a numerical method by comparing its approximate equa-\ntion with the inﬁ nite Taylor series expansion of the exact solution to the ODE. \nWe then cancel terms by subtracting the two equations. The remaining Taylor \n12.4. Rigid Body Dynamics\n",
      "content_length": 2592,
      "extraction_method": "Direct"
    },
    {
      "page_number": 662,
      "chapter": null,
      "content": "640 \n12. Collision and Rigid Body Dynamics\nterms represent the error inherent in the method. For example, the explicit \nEuler equation is\nr\nr\nr\n( )\n( )\n( )\n.\nt\nt\n2\n1\n1\n=\n+\u0002\n Δ\nt\nt  \nThe inﬁ nite Taylor series expansion of the exact solution is\n \nr\nr\nr\nr\nr\n( )\n( )\n( )\n( )\n( )\n....\nt\nt\nt\nt\n2\n1\n1\n1\n2\n1\n2\n1\n6\n1\n3\n=\n+\n+\n+\n+\n\u0002\n\u0002\u0002\n\u0002\u0002\u0002\n \n \n \nΔ\nΔ\nΔ\nt\nt\nt\nt\n \nTherefore, the error is represented by all of the terms aft er the v Δt term, which \nis of order O(Δt2) (because this term dwarfs the other higher-order terms):\n \nE\nr\nr\n=\n+\n+\n=\n1\n2\n1\n2\n1\n6\n1\n3\n2\n\u0002\u0002\n\u0002\u0002\u0002\n( )\n( )\n...\n(\n).\nt\nt\nO\nt\n \n \nΔ\nΔ\nΔ\nt\nt\n \nTo make the error of a method explicit, we’ll oft en write its equation with the \nerror term added in big “O” notation at the end. For example, the explicit Eu-\nler method’s equation is most accurately writt en as follows:\nWe say that the explicit Euler method is an “order one” method because it \nis accurate up to and including the Taylor series term involving Δt to the ﬁ rst \npower. In general, if a method’s error term is O(Δt(n + 1)), then it is said to be an \n“order n” method.\n12.4.4.3. Alternatives to Explicit Euler\nThe explicit Euler method sees quite a lot of use for simple integration tasks in \ngames, producing the best results when the velocity is nearly constant. How-\never, it is not used in general-purpose dynamics simulations because of its high \nerror and poor stability. There are all sorts of other numerical methods for solv-\ning ODEs, including backward Euler (another ﬁ rst-order method), midpoint \nEuler (a second-order method), and the family of Runge-Kutt a methods. (The \nfourth-order Runge-Kutt a, oft en abbreviated “RK4,” is particularly popular.) \nWe won’t describe these in any detail here, as you can ﬁ nd voluminous infor-\nmation about them online and in the literature. The Wikipedia page htt p://\nen.wikipedia.org/wiki/Numerical_ordinary_diﬀ erential_equations serves as \nan excellent jumping-oﬀ  point for learning these methods.\n12.4.4.4. Verlet Integration\nThe numerical ODE method most oft en used in interactive games these \ndays is probably the Verlet method, so I’ll take a moment to describe it in \nsome detail. There are actually two variants of this method: regular Verlet \nand the so-called velocity Verlet . I’ll present both methods here, but I’ll leave \nthe theory and deep explanations to the myriad papers and Web pages avail-\nr\nr\nr\n( )\n( )\n( )\n(\n).\nt\nt\nO\nt\n2\n1\n1\n2\n=\n+\n+\n\u0002\n Δ\nΔ\nt\nt\n",
      "content_length": 2434,
      "extraction_method": "Direct"
    },
    {
      "page_number": 663,
      "chapter": null,
      "content": "641 \nable on the topic. (For a start, check out htt p://en.wikipedia.org/wiki/Verlet_\nintegration.)\nThe regular Verlet method is att ractive because it achieves a high order \n(low error), is relatively simple and inexpensive to evaluate, and produces a \nsolution for position directly in terms of acceleration in one step (as opposed \nto the two steps normally required to go from acceleration to velocity and then \nfrom velocity to position). The formula is derived by adding two Taylor series \nexpansions, one going forward in time and one going backward in time:\n \nr\nr\nr\nr\nr\n(\n)\n( )\n( )\n( )\n( )\n(\nt\nt\nt\nt\nt\nO\nt\n1\n1\n1\n1\n2\n1\n2\n1\n6\n1\n3\n+\n=\n+\n+\n+\n+\nΔ\nΔ\nΔ\nΔ\nΔ\n\u0002\n\u0002\u0002\n\u0002\u0002\u0002\n \n \n \n4);\nt\nt\nt\nt\n \n \nr\nr\nr\nr\nr\n(\n)\n( )\n( )\n( )\n( )\n(\nt\nt\nt\nt\nt\nO\nt\n1\n1\n1\n1\n2\n1\n2\n1\n6\n1\n3\n−\n=\n−\n+\n−\n+\nΔ\nΔ\nΔ\nΔ\nΔ\n\u0002\n\u0002\u0002\n\u0002\u0002\u0002\n \n \n \n4).\nt\nt\nt\nt\n \nAdding these expressions causes the negative terms to cancel with the corre-\nsponding positive ones. The result gives us the position at the next time step \nin terms of the acceleration and the two (known) positions at the current and \nprevious time steps. This is the regular Verlet method:\n \nr\nr\nr\na\n(\n)\n( )\n(\n)\n( )\n(\n).\nt\nt\nt\nt\nO\nt\n1\n1\n1\n1\n2\n4\n2\n+\n=\n−\n−\n+\n+\nΔ\nΔ\nΔ\nΔ\n \nt\nt\nt\n \nIn terms of net force, the Verlet method becomes\n \nr\nr\nr\nF\n(\n)\n( )\n(\n)\n( )\n(\n).\nt\nt\nt\nt\nm\nt\nO\nt\n1\n1\n1\n1\n2\n4\n2\n+\n=\n−\n−\n+\n+\nΔ\nΔ\nΔ\nΔ\nnet\nt\nt\n \nThe velocity is conspicuously absent from this expression. However, it can \nbe found using the following somewhat inaccurate approximation (among \nother alternatives):\n \nv\nr\nr\n(\n)\n(\n)\n( )\n(\n).\nt\nt\nt\nt\nO\nt\n1\n1\n1\n+\n=\n+\n−\n+\nΔ\nΔ\nΔ\nΔ\nt\nt\n \n12.4.4.5. Velocity Verlet\nThe more commonly used velocity Verlet method is a four-step process in which \nthe time step is divided into two parts to facilitate the solution. Given that \na\nF\nr\nv\n( )\n( , ( ),\n( ))\nt\nt\nt\nm\n1\n1\n1\n1\n=\n \n \nt\n is known, we do the following:\n \n1. Calculate r\nr\nv\na\n(\n)\n( )\n( )\n( )\n.\nt\nt\nt\nt\n1\n1\n1\n1\n2\n1\n2\n+\n=\n+\n+\nΔ\nΔ\nΔ\n \n \nt\nt\nt\n \n \n2. Calculate v\nv\na\n(\n)\n( )\n( )\n.\nt\nt\nt\n1\n1\n2\n1\n1\n2\n1\n+\n=\n+\nΔ\nΔ\n \nt\nt\n \n \n3. Determine a\na\nF\nr\nv\n(\n)\n( )\n(\n, ( ),\n( )).\nt\nt\nt\nt\nm\n1\n2\n2\n2\n2\n+\n=\n=\nΔ\n \n \nt\nt\n \n \n4. Calculate \n \n1\n1\n1\n1\n1\n2\n2\n(\n)\n(\n)\n(\n)\n.\nt +Δ\n=\n+ Δ +\n+Δ\nΔ\nv\nv\na\nt\nt\nt\nt\nt\nt  \nNotice in the third step that the force function depends on the position \nand velocity on the next time step, r(t2) and v(t2). We already calculated r(t2) in \nstep 1, so we have all the information we need as long as the force is not ve-\n12.4. Rigid Body Dynamics\n",
      "content_length": 2425,
      "extraction_method": "Direct"
    },
    {
      "page_number": 664,
      "chapter": null,
      "content": "642 \n12. Collision and Rigid Body Dynamics\nlocity-dependent. If it is velocity-dependent, then we must approximate next \nframe’s velocity, perhaps using the explicit Euler method.\n12.4.5. Angular Dynamics in Two Dimensions\nUp until now, we’ve focused on analyzing the linear motion of a body’s center \nof mass (which acts as if it were a point mass). As I said earlier, an uncon-\nstrained rigid body will rotate about its center of mass. This means that we can \nlayer the angular motion of a body on top of the linear motion of its center of \nmass in order to arrive at a complete description of the body’s overall motion. \nThe study of a body’s rotational motion in response to applied forces is called \nangular dynamics .\nIn two dimensions, angular dynamics works almost identically to linear \ndynamics. For each linear quantity, there’s an angular analog, and the math-\nematics works out quite neatly. So let’s investigate two-dimensional angular \ndynamics ﬁ rst. As we’ll see, when we extend the discussion into three dimen-\nsions, things get a bit messier, but we’ll burn that bridge when we get to it!\n12.4.5.1. Orientation and Angular Speed\nIn two dimensions, every rigid body can be treated as a thin sheet of mate-\nrial. (Some physics texts refer to such a body as a plane lamina .) All linear mo-\ntion occurs in the xy-plane, and all rotations occur about the z-axis. (Visualize \nwooden puzzle pieces sliding about on an air hockey table.)\nThe orientation of a rigid body in 2D is fully described by an angle θ, \nmeasured in radians relative to some agreed-upon zero rotation. For example, \nwe might specify that θ = 0 when a race car is facing directly down the posi-\ntive x-axis in world space. This angle is of course a time-varying function, so \nwe denote it θ(t).\n12.4.5.2. Angular Speed and Acceleration\nAngular velocity measures the rate at which a body’s rotation angle chang-\nes over time. In two dimensions, angular velocity is a scalar, more correctly \ncalled angular speed , since the term “velocity” really only applies to vectors. \nIt is denoted by the scalar function ω(t) and measured in radians per second \n(rad/s). Angular speed is the derivative of the orientation angle θ(t) with re-\nspect to time:\n \nAngular: ω\nθ\nθ\n( )\n( )\n( );\nt\nd\nt\ndt\nt\n=\n= \u0002\n    Linear: v\nr\nr\n( )\n( )\n( ).\nt\nd t\ndt\nt\n=\n= \u0002\n \nAnd as we’d expect, angular acceleration , denoted α(t) and measured in \nradians per second squared (rad/s2), is the rate of change of angular speed:\n",
      "content_length": 2476,
      "extraction_method": "Direct"
    },
    {
      "page_number": 665,
      "chapter": null,
      "content": "643 \n \nAngular: \nα\nω\nω\nθ\n( )\n( )\n( )\n( );\nt\nd\nt\ndt\nt\n=\n=\n=\n\u0002\n\u0002\u0002\n  \nt\n Linear: \na\nv\nv\nr\n( )\n( )\n( )\n( ).\nt\nd\nt\ndt\nt\n=\n=\n=\n\u0002\n\u0002\u0002 t\n \n12.4.5.3. Moment of Inertia\nThe rotational equivalent of mass is a quantity known as the moment of inertia . \nJust as mass describes how easy or diﬃ  cult it is to change the linear velocity \nof a point mass, the moment of inertia measures how easy or diﬃ  cult it is to \nchange the angular speed of a rigid body about a particular axis. If a body’s \nmass is concentrated near an axis of rotation, it will be relatively easier to ro-\ntate about that axis, and it will hence have a smaller moment of inertia than a \nbody whose mass is spread out away from that axis. \nSince we’re focusing on two-dimensional angular dynamics right now, \nthe axis of rotation is always z, and a body’s moment of inertia is a simple \nscalar value. Moment of inertia is usually denoted by the symbol I. We won’t \nget into the details of how to calculate the moment of inertia here. For a full \nderivation, see [15].\n12.4.5.4. Torque\nUntil now, we’ve assumed that all forces are applied to the center of mass of a \nrigid body. However, in general, forces can be applied at arbitrary points on a \nbody. If the line of action of a force passes through the body’s center of mass, \nthen the force will produce linear motion only, as we’ve already seen. Other-\nwise, the force will introduce a rotational force known as a torque in addition \nto the linear motion it normally causes. This is illustrated in Figure 12.24.\nWe can calculate torque using a cross product . First, we express the loca-\ntion at which the force is applied as a vector r extending from the body’s center \nof mass to the point of application of the force. (In other words, the vector r \nis in body space , where the origin of body space is deﬁ ned to be the center of \nF1\nF2\nFigure 12.24.  On the left, a force applied to a body’s CM produces purely linear motion. On \nthe right, a force applied off-center will give rise to a torque, producing rotational motion as \nwell as linear motion.\n12.4. Rigid Body Dynamics\n",
      "content_length": 2092,
      "extraction_method": "Direct"
    },
    {
      "page_number": 666,
      "chapter": null,
      "content": "644 \n12. Collision and Rigid Body Dynamics\nmass .) This is illustrated in Figure 12.25. The torque N caused by a force F ap-\nplied at a location r is\n \n \n(12.7)\nEquation (12.7) implies that torque increases as the force is applied farther \nfrom the center of mass. This explains why a lever can help us to move a heavy \nobject. It also explains why a force applied directly through the center of mass \nproduces no torque and no rotation—the magnitude of the vector r is zero in \nthis case.\nWhen two or more forces are applied to a rigid body, the torque vectors \nproduced by each one can be summed, just as we can sum forces. So in general \nwe are interested in the net torque, Nnet.\nIn two dimensions, the vectors r and F must both lie in the xy-plane, so \nN will always be directed along the positive or negative z-axis. As such, we’ll \ndenote a two-dimensional torque via the scalar Nz , which is just the z-compo-\nnent of the vector N.\nTorque is related to angular acceleration and moment of inertia in much \nthe same way that force is related to linear acceleration and mass:\n \nAngular: \nN\nI\nt\nI\nt\nI\nt\nz =\n=\n=\nα\nθ\n( )\n( )\n( );\n\u0002\u0002\n  \nω\u0002\n Linear: \nF\na\nv\nr\n=\n=\n=\nm\nt\nm\nt\nm t\n( )\n( )\n( ).\n\u0002\n\u0002\u0002\n \n(12.8)\n12.4.5.5. Solving the Angular Equations of Motion in Two Dimensions\nFor the two-dimensional case, we can solve the angular equations of motion \nusing exactly the same numerical integration techniques we applied to the lin-\near dynamics problem. The pair of ODEs that we wish to solve is as follows:\n \nAngular: \nN\nt\nI\nt\nt\nnet\n  \n  \n( )\n( );\n( )\n( );\n=\n=\n\u0002\n\u0002\nω\nω\nθ t\n  Linear: \nF\nv\nv\nr\nnet\n \n( )\n( );\n( )\n( ),\nt\nm\nt\nt\n=\n=\n\u0002\n\u0002 t\n \nand their approximate explicit Euler solutions are\nF\nr\nr sin θ\nFigure 12.25.  Torque is calculated by taking the cross product between a force’s point of \napplication in body space (i.e., relative to the center of mass) and the force vector. The \nvectors are shown here in two dimensions for ease of illustration; if it could be drawn, the \ntorque vector would be directed into the page.\n.\n= ×\nN\nr\nF\n",
      "content_length": 2032,
      "extraction_method": "Direct"
    },
    {
      "page_number": 667,
      "chapter": null,
      "content": "645 \n \nAngular: \nω\nω\nθ\nθ\nω\n( )\n( )\n( )\n;\n( )\n( )\n( )\n;\nt\nN\nt\nI\nt\nt\nt\n2\n1\n1\n2\n1\n1\n=\n+\n=\n+\nnet\n  \n  \n \nΔ\nΔ\nt\nt\nt\n  Linear: \nv\nv\nF\nr\nr\nv\n( )\n( )\n( )\n;\n( )\n( )\n( )\n.\nt\nt\nm\nt\nt\nt\n2\n1\n1\n2\n1\n1\n=\n+\n=\n+\nnet\n \nΔ\nΔ\nt\nt\nt\n \nOf course, we could apply any of the other more-accurate numerical \nmethods as well, such as the velocity Verlet method (I’ve omitt ed the lin-\near case here for compactness, but compare this to the steps given in Section \n12.4.4.5):\n \n1. Calculate θ\nθ\nω\nα\n(\n)\n( )\n( )\n( )\n.\nt\nt\nt\nt\n1\n1\n1\n1\n2\n1\n2\n+\n=\n+\n+\nΔ\nΔ\nΔ\nt\nt\nt\n \n \n2. Calculate ω\nω\nα\n(\n)\n( )\n( )\n.\nt\nt\nt\n1\n1\n2\n1\n1\n2\n1\n+\n=\n+\nΔ\nΔ\nt\nt\n \n \n3. Determine α\nα\nθ\nω\n(\n)\n( )\n(\n,\n( ),\n( )).\nt\nt\nI\nN\nt\nt\n1\n2\n1\n2\n2\n2\n+\n=\n= −\nΔ\nnet\n \n \nt\nt\n \n \n4. Calculate ω\nω\nα\n(\n)\n(\n)\n(\n)\n.\nt\nt\nt\nt\n1\n1\n1\n2\n1\n2\n1\n+\n=\n+\n+\n+\nΔ\nΔ\nΔ Δ\nt\nt\nt\n \n12.4.6. Angular Dynamics in Three Dimensions\n Angular dynamics in three dimensions is a somewhat more complex topic \nthan its two-dimensional counterpart, although the basic concepts are of \ncourse very similar. In the following section, I’ll give a very brief overview of \nhow angular dynamics works in 3D, focusing primarily on the things that are \ntypically confusing to someone who is new to the topic. For further informa-\ntion, check out Glenn Fiedler’s series of articles on the topic, available at htt p://\ngaﬀ erongames.wordpress.com/game-physics. Another helpful resource is the \npaper entitled “An Introduction to Physically Based Modeling” by David Ba-\nraﬀ  of the Robotics Institute at Carnegie Mellon University, available at htt p://\nwww-2.cs.cmu.edu/~baraﬀ /sigcourse/notesd1.pdf.\n12.4.6.1. The Inertia Tensor\nA rigid body may have a very diﬀ erent distribution of mass about the three \ncoordinate axes. As such, we should expect a body to have diﬀ erent moments \nof inertia about diﬀ erent axes. For example, a long thin rod should be relative-\nly easy to make rotate about its long axis because all the mass is concentrated \nvery close to the axis of rotation. Likewise, the rod should be relatively more \ndiﬃ  cult to make rotate about its short axis because its mass is spread out far-\nther from the axis. This is indeed the case, and it is why a ﬁ gure skater spins \nfaster when she tucks her limbs in close to her body.\nIn three dimensions, the rotational mass of a rigid body is represented \nby a 3 × 3 matrix known as its inertia tensor . It is usually represented by the \nsymbol I (as before, we won’t describe how to calculate the inertia tensor here; \nsee [15] for details):\n12.4. Rigid Body Dynamics\n",
      "content_length": 2507,
      "extraction_method": "Direct"
    },
    {
      "page_number": 668,
      "chapter": null,
      "content": "646 \n12. Collision and Rigid Body Dynamics\n .\nxx\nxy\nxz\nyx\nyy\nyz\nzx\nzy\nzz\nI\nI\nI\nI\nI\nI\nI\nI\nI\n⎡\n⎤\n⎢\n⎥\n=⎢\n⎥\n⎢\n⎥\n⎣\n⎦\nI\nThe elements lying along the diagonal of this matrix are the moments of \ninertia of the body about its three principal axes, Ixx , Iyy , and Izz. The oﬀ -diago-\nnal elements are called products of inertia . They are zero when the body is sym-\nmetrical about all three principal axes (as would be the case for a rectangular \nbox). When they are non-zero, they tend to produce physically realistic yet \nsomewhat unintuitive motions that the average game player would probably \nthink were “wrong” anyway. Therefore, the inertia tensor is oft en simpliﬁ ed \ndown to the three-element vector [ Ixx  Iyy  Izz ] in game physics engines.\n12.4.6.2. Orientation in Three Dimensions\nIn two dimensions, we know that the orientation of a rigid body can be de-\nscribed by a single angle θ, which measures rotation about the z-axis (assum-\ning the motion is taking place in the xy-plane). In three dimensions, a body’s \norientation could be represented using three Euler angles [ θx  θy  θz ], each \nrepresenting the body’s rotation about one of the three Cartesian axes. How-\never, as we saw in Chapter 4, Euler angles suﬀ er from gimbal lock problems \nand can be diﬃ  cult to work with mathematically. Therefore, the orientation of \na body is more oft en represented using either a 3 × 3 matrix R or a unit quater-\nnion q. We’ll use the quaternion form exclusively in this chapter.\nRecall that a quaternion is a four-element vector whose x-, y-, and z-com-\nponents can be interpreted as a unit vector u lying along the axis of rotation, \nscaled by the sine of the half angle and whose w component is the cosine of \nthe half angle:\n \n( )\n( )\n \n \n2\n2\nq\n[\n]\n[\n]\nsin\ncos\n.\nx\ny\nz\nw\nw\nq\nq\nq\nq\nq\nθ\nθ\n=\n=\n⎡\n⎤\n=⎣\n⎦\nq\nu\n \nA body’s orientation is of course a function of time, so we should write it q(t).\nAgain, we need to select an arbitrary direction to be our zero rotation. For \nexample, we might say that by default, the front of every object will lie along \nthe positive z-axis in world space, with y up and x to the left . Any non-identity \nquaternion will serve to rotate the object away from this canonical world space \norientation. The choice of the canonical orientation is arbitrary, but of course \nit’s important to be consistent across all assets in the game.\n12.4.6.3. Angular Velocity and Momentum in Three Dimensions\nIn three dimensions, angular velocity is a vector quantity, denoted by ω(t). \nThe angular velocity vector can be visualized as a unit-length vector u that \n",
      "content_length": 2580,
      "extraction_method": "Direct"
    },
    {
      "page_number": 669,
      "chapter": null,
      "content": "647 \ndeﬁ nes the axis of rotation, scaled by the two-dimensional angular velocity \nω\nθ\nu\nu\n= \u0002  of the body about the u-axis. Hence,\n \nω\nω\nθ\n( )\n( ) ( )\n( ) ( ).\nt\nt\nt\nu\nu\n=\n=\n \n \nu\nu\n\u0002\nt\nt\n \nIn linear dynamics, we saw that if there are no forces acting on a body, \nthen the linear acceleration is zero, and linear velocity is constant. In two-\ndimensional angular dynamics, this again holds true: If there are no torques \nacting on a body in two dimensions, then the angular acceleration α is zero, \nand the angular speed ω about the z-axis is constant.\nUnfortunately, this is not the case in three dimensions. It turns out that \neven when a rigid body is rotating in the absence of all forces, its angular \nvelocity vector ω(t) may not be constant because the axis of rotation can con-\ntinually change direction. You can see this eﬀ ect in action when you try to \nspin a rectangular object, like a block of wood, in mid-air in front of you. If \nyou throw the block so that it is rotating about its shortest axis, it will spin in a \nstable way. The orientation of the axis stays roughly constant. The same thing \nhappens if you try to spin the block about its longest axis. But if you try to spin \nthe block around its medium-sized axis, the rotation will be utt erly unstable. \nThe axis of rotation itself changes direction wildly as the object spins. This is \nshown in Figure 12.26.\nThe fact that the angular velocity vector can change in the absence of \ntorques is another way of saying that angular velocity is not conserved. How-\never, a related quantity called the angular momentum does remain constant \nin the absence of forces and hence is conserved. Angular momentum is the \nrotational equivalent of linear momentum:\n \nAngular: L\nI\n( )\n( );\nt =  \n  \nt\nω\n  Linear: \n \n( )\n( ).\nt\nm\nt\n=\np\nv\n \nLike the linear case, angular momentum L(t) is a three-element vector. \nHowever, unlike the linear case, rotational mass (the inertia tensor) is not a \nscalar but rather a 3 × 3 matrix. As such, the expression Iω is computed via a \nmatrix multiplication:\nFigure 12.26.  A rectangular object that is spun about its shortest or longest axis has a \nconstant angular velocity vector. However, when spun about its medium-sized axis, the \ndirection of the angular velocity vector changes wildly.\n12.4. Rigid Body Dynamics\n",
      "content_length": 2316,
      "extraction_method": "Direct"
    },
    {
      "page_number": 670,
      "chapter": null,
      "content": "648 \n12. Collision and Rigid Body Dynamics\n \n \n( )\n( )\n( )\n( ) .\n( )\n( )\nx\nxx\nxy\nxz\nx\ny\nyx\nyy\nyz\ny\nz\nzx\nzy\nzz\nz\nL t\nI\nI\nI\nt\nL t\nI\nI\nI\nt\nL t\nI\nI\nI\nt\n⎡\n⎤⎡\n⎤⎡\n⎤\nω\n⎢\n⎥⎢\n⎥⎢\n⎥\n=\nω\n⎢\n⎥⎢\n⎥⎢\n⎥\n⎢\n⎥⎢\n⎥⎢\n⎥\nω\n⎣\n⎦⎣\n⎦⎣\n⎦\nBecause the angular velocity ω is not conserved, we do not treat it as \na primary quantity in our dynamics simulations the way we do the linear \nvelocity v. Instead, we treat angular momentum L as the primary quantity. \nThe angular velocity is a secondary quantity, determined only aft er we have \ndetermined the value of L at each time step of the simulation.\n12.4.6.4. Torque in Three Dimensions\nIn three dimensions, we still calculate torque as the cross product between \nthe radial position vector of the point of force application and the force vector \nitself (N = r × F). Equation (12.8) still holds, but we always write it in terms of \nthe angular momentum because angular velocity is not a conserved quantity:\n \nN\nI\nI\nI\nL\n=\n=\n=\n(\n)\n=\n ( )\n( )\n( )\n( ).\nt\nd\nt\ndt\nd\ndt\nt\nd\nt\ndt\nω\nα\nω\n12.4.6.5. Solving the Equations of Angular Motion in Three Dimensions\nWhen solving the equations of angular motion in three dimensions, we might \nbe tempted to take exactly the same approach we used for linear motion and \ntwo-dimensional angular motion. We might guess that the diﬀ erential equa-\ntions of motion should be writt en\n \nA3D(?): \nN\nI\nnet\n \n  \n  \n( )\n( );\n( )\n( );\nt\nt\n=\n=\n\u0002\n\u0002\nω t\nω\nt\nθ\n  L: \nF\nv\nv\nr\nnet\n \n( )\n( );\n( )\n( ),\nt\nm\nt\nt\n=\n=\n\u0002\n\u0002 t\n \nand using the explicit Euler method, we might guess that the approximate \nsolutions to these ODEs would look something like this:\n \nA3D(?): \nI\nN\n( )\n( )\n( )\n;\n( )\n( )\n( )\n;\nt\nt\nt\nt\n2\n1\n1\n1\n2\n1\n1\n=\n+\n=\n+\n−\n \n \n \nnet\n  \n  \nΔ\nΔ\nθ\nω\nω\nω\nt\nt\nθ\nt\n  L: \nv\nv\nF\nr\nr\nv\n( )\n( )\n( )\n;\n( )\n( )\n( )\n.\nt\nt\nm\nt\nt\nt\n2\n1\n1\n2\n1\n1\n=\n+\n=\n+\nnet\n \nΔ\nΔ\nt\nt\nt\n \nHowever, this is not actually correct. The diﬀ erential equations of angular mo-\ntion diﬀ er from their linear and two-dimensional angular counterparts in two \nimportant ways:\nInstead of solving for the angular velocity \n1. \nω, we solve for the angular \nmomentum L directly. We then calculate the angular velocity vector as a \nsecondary quantity using I and L. We do this because angular momen-\ntum is conserved , while angular velocity is not.\n",
      "content_length": 2234,
      "extraction_method": "Direct"
    },
    {
      "page_number": 671,
      "chapter": null,
      "content": "649 \nWhen solving for the orientation given the angular velocity, we have \n2. \na problem: The angular velocity is a three-element vector, while the \norientation is a four-element quaternion. How can we write an ODE \nrelating a quaternion to a vector? The answer is that we cannot, at \nleast not directly. But what we can do is convert the angular velocity \nvector into quaternion form and then apply a slightly odd-looking \nequation that relates the orientation quaternion to the angular veloc-\nity quaternion.\nIt turns out that when we express a rigid body’s orientation as a quater-\nnion, the derivative of this quaternion is related to the body’s angular velocity \nvector in the following way. First, we construct an angular velocity quaternion. \nThis quaternion contains the three components of the angular velocity vector \nin x, y, and z, with its w-component set to zero:\n \n=⎡⎣\n⎤⎦\nω\nω\nω\nx\ny\nz\n0\nω\n \nNow the diﬀ erential equation relating the orientation quaternion to the angu-\nlar velocity quaternion is (for reasons we won’t get into here) as follows:\n \nd\nt\ndt\nt\nt\nq( )\nq( )\n( ) q( ).\n=\n=\n\u0002\n1\n2\n \nω t\n \nIt’s important to remember here that ω(t) is the angular velocity quaternion as \ndescribed above and that the product ω(t)q(t) is a quaternion product (see Sec-\ntion 4.4.2.1 for details).\nSo, we actually need to write the ODEs of motion as follows (note that I’ve \nrecast the linear ODEs in terms of linear momentum as well, to underscore the \nsimilarities between the two cases):\n \nA3D: \nN\nL\nI\nL\nnet\n  \n  \n  \n \n( )\n( );\n( )\n( );\n( )\n[ ( )\n];\n( )q( )\nt\nt\nt\nt\n=\n=\n=\n=\n−\n\u0002\n1\n1\n2\n0\n\u0002q( );\nt   \nt\nt\nt\nt\nω\nω\nω\nω\n  L: \nF\np\nv\np\nv\nr\nnet( )\n( );\n( )\n( ) ;\n( )\n( ).\nt\nt\nt\nm\nt\n=\n=\n=\n\u0002\n\u0002\nt\nt\n \nUsing the explicit Euler method, the ﬁ nal approximate solution to the angular \nODEs in three dimensions is actually as follows:\n \nL\nL\nN\nL\nr\nF\n( )\n( )\n( )\n( )\n( ) ;\nt\nt\nt\nt\ni\n2\n1\n1\n1\n1\n=\n+\n=\n+\n×\n(\n)\n∑\nnet\nΔ\nΔ\nt\nt\nt\ni\n \n(vectors)\n \n \n \n( )\n[\n( )\n];\nt2\n1\n2\n0\n=\n−\nI\nL t\nω\n \n(quaternion)\nq( )\nq( )\n( ) q( )\n.\nt\nt\nt\n2\n1\n1\n2\n1\n1\n=\n+\n \n Δ\nt\nt\nω\n \n(quaternions)\n12.4. Rigid Body Dynamics\n",
      "content_length": 2073,
      "extraction_method": "Direct"
    },
    {
      "page_number": 672,
      "chapter": null,
      "content": "650 \n12. Collision and Rigid Body Dynamics\nThe orientation quaternion q(t) should be renormalized periodically to reverse \nthe eﬀ ects of the inevitable accumulation of ﬂ oating-point error.\nAs always, the explicit Euler method is being used here just as an ex-\nample. In a real engine, we would employ velocity Verlet, RK4, or some other \nmore-stable and more-accurate numerical method.\n12.4.7. Collision Response\nEverything we’ve discussed so far assumes that our rigid bodies are neither \ncolliding with anything, nor is their motion constrained in any other way. \nWhen bodies collide with one another, the dynamics simulation must take \nsteps to ensure that they respond realistically to the collision and that they \nare never left  in a state of interpenetration aft er the simulation step has been \ncompleted. This is known as collision response .\n12.4.7.1. Energy\nBefore we discuss collision response, we must understand the concept of en-\nergy . When a force moves a body over a distance, we say that the force does \nwork . Work represents a change in energy—that is, a force either adds energy \nto a system of rigid bodies (e.g., an explosion) or it removes energy from the \nsystem (e.g., friction). Energy comes in two forms. The potential energy V of a \nbody is the energy it has simply because of where it is relative to a force ﬁ eld \nsuch as a gravitational or a magnetic ﬁ eld. (For example, the higher up a body \nis above the surface of the Earth, the more gravitational potential energy it \nhas.) The kinetic energy of a body T represents the energy arising from the \nfact that it is moving relative to other bodies in a system. The total energy \nE = V + T of an isolated system of bodies is a conserved quantity, meaning that \nit remains constant unless energy is being drained from the system or added \nfrom outside the system.\nThe kinetic energy arising from linear motion can be writt en\n \nT\nmv\nlinear\n \n= 1\n2\n2,  \nor in terms of the linear momentum and velocity vectors:\n \nTlinear =\n⋅\n1\n2 p v.  \nAnalogously, the kinetic energy arising from a body’s rotational motion is as \nfollows:\n \n \n Energy and its conservation can be extremely useful concepts when solving \nall sorts of physics problems. We’ll see the role that energy plays in the deter-\nmination of collision responses in the following section.\nTangular =\n⋅\n1\n2 L\n.\nω\n",
      "content_length": 2349,
      "extraction_method": "Direct"
    },
    {
      "page_number": 673,
      "chapter": null,
      "content": "651 \n12.4.7.2. Impulsive Collision Response\n When two bodies collide in the real world, a complex set of events takes place. \nThe bodies compress slightly and then rebound, changing their velocities and \nlosing energy to sound and heat in the process. Most real-time rigid body \ndynamics simulations approximate all of these details with a simple model \nbased on an analysis of the momenta and kinetic energies of the colliding ob-\njects, called Newton’s law of restitution for instantaneous collisions with no friction. \nIt makes the following simplifying assumptions about the collision:\nz The collision force acts over an inﬁ nitesimally short period of time, turn-\ning it into what we call an idealized impulse . This causes the velocities of \nthe bodies to change instantaneously as a result of the collision.\nz There is no friction at the point of contact between the objects’ surfaces. \nThis is another way of saying that the impulse acting to separate the \nbodies during the collision is normal to both surfaces—there is no tan-\ngential component to the collision impulse. (This is just an idealization \nof course; we’ll get to friction in Section 12.4.7.5.)\nz The nature of the complex submolecular interactions between the bodies \nduring the collision can be approximated by a single quantity known \nas the coeﬃ  cient of restitution , customarily denoted by the symbol ε. This \ncoeﬃ  cient describes how much energy is lost during the collision. When \nε = 1, the collision is perfectly elastic , and no energy is lost. (Picture two \nbilliard balls colliding in mid air.) When ε = 0, the collision is perfectly in-\nelastic , also known as perfectly plastic , and the kinetic energy of both bod-\nies is lost. The bodies will stick together aft er the collision, continuing to \nmove in the direction that their mutual center of mass had been moving \nbefore the collision. (Picture pieces of putt y being slammed together.)\nAll collision analysis is based around the idea that linear momentum is \nconserved. So for two bodies 1 and 2, we can write\n \n \n \n \n \n1\n2\n1\n2\n1\n1\n2\n2\n1\n1\n2\n2\n,         or\n,\nm\nm\nm\nm\n′\n′\n+\n=\n+\n′\n′\n+\n=\n+\np\np\np\np\nv\nv\nv\nv\n \nwhere the primed symbols represent the momenta and velocities aft er the col-\nlision. The kinetic energy of the system is conserved as well, but we must ac-\ncount for the energy lost due to heat and sound by introducing an additional \nenergy loss term Tlost :\n \n \n \n \n \n2\n2\n2\n2\n1\n1\n1\n1\n1\n1\n2\n2\n1\n1\n2\n2\nlost\n2\n2\n2\n2\n.\nm v\nm v\nm v\nm v\nT\n′\n′\n+\n=\n+\n+\n \nIf the collision is perfectly elastic, the energy loss Tlost is zero. If it is perfectly \nplastic, the energy loss is equal to the original kinetic energy of the system, the \n12.4. Rigid Body Dynamics\n",
      "content_length": 2700,
      "extraction_method": "Direct"
    },
    {
      "page_number": 674,
      "chapter": null,
      "content": "652 \n12. Collision and Rigid Body Dynamics\nprimed kinetic energy sum becomes zero, and the bodies stick together aft er \nthe collision.\nTo resolve a collision using Newton’s law of restitution, we apply an ide-\nalized impulse to the two bodies. An impulse is like a force that acts over an in-\nﬁ nitesimally short period of time and thereby causes an instantaneous change \nin the velocity of the body to which it is applied. We could denote an impulse \nwith the symbol ∆p, since it is a change in momentum (∆p = m∆v). However, \nmost physics texts use the symbol ˆp  (pronounced “p-hat”) instead, so we’ll \ndo the same.\nBecause we assume that there is no friction involved in the collision, the \nimpulse vector must be normal to both surfaces at the point of contact. In oth-\ner words, \n \nˆ\nˆp\n=\np\nn , where n is the unit vector normal to both surfaces. This is \nillustrated in Figure 12.27. If we assume that the surface normal points toward \nbody 1, then body 1 experiences an impulse of ˆp, and body 2 experiences an \nequal but opposite impulse. Hence, the momenta of the two bodies aft er the \ncollision can be writt en in terms of their momenta prior to the collision and \nthe impulse ˆp  as follows:\n1\n1\n2\n2\nˆ\nˆ\n;        \n;\n′\n′\n=\n+\n=\n−\np\np\np\np\np\np\n \n1\n1\n1\n1\n2\n2\n2\n2\nˆ\nˆ\n;        \n;\nm\nm\nm\nm\n′\n′\n=\n+\n=\n−\nv\nv\np\nv\nv\np\n \n \n(12.9)\nThe coeﬃ  cient of restitution provides the key relationship between the rela-\ntive velocities of the bodies before and aft er the collision. Given that the cen-\nters of mass of the bodies have velocities \n1\nv  and \n2\nv  before the collision and \n1′\nv  and \n2′\nv  aft erward, the coeﬃ  cient of restitution ε is deﬁ ned as follows:\n \n2\n1\n2\n1\n(\n)\n(\n).\n′\n′\n−\n=ε\n−\nv\nv\nv\nv\n \n(12.10)\nSolving Equations (12.9) and (12.10) under the temporary assumption \nthat the bodies cannot rotate yields\nn\nBody 1\nBody 2\np^\nFigure 12.27.  In a frictionless collision, the impulse acts along a line normal to both surfaces \nat the point of contact. This line is deﬁ ned by the unit normal vector n.\n1\n1\n2\n2\n1\n2\nˆ\nˆ\n;        \n.\np\np\nm\nm\n′\n′\n=\n+\n=\n−\nv\nv\nn\nv\nv\nn\n",
      "content_length": 2069,
      "extraction_method": "Direct"
    },
    {
      "page_number": 675,
      "chapter": null,
      "content": "653 \n(\n)\n \n2\n1\n1\n2\n(\n1)\nˆ\nˆ\n .\n1\n1\np\nm\nm\nε+\n⋅−\n⋅\n=\n=\n+\nv\nn\nv\nn\np\nn\nn  \nNotice that if the coeﬃ  cient of restitution is one (perfectly elastic collision) and \nif the mass of body 2 is eﬀ ectively inﬁ nite (as it would be for, say, a concrete \ndriveway), then (1/m2) = 0, v2 = 0, and this expression reduces to a reﬂ ection of \nthe other body’s velocity vector about the contact normal, as we’d expect:\n(\n)\n(\n)\n(\n)\n \n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\nˆ\n2\n ;\nˆ\n2\n \n2\n .\nm\nm\nm\nm\nm\n=−\n⋅\n+\n−\n⋅\n′ =\n=\n=\n−\n⋅\np\nv\nn n\np\np\nv\nv\nn n\nv\nv\nv\nn n\nThe solution gets a bit hairier when we take the rotations of the bodies \ninto account. In this case, we need to look at the velocities of the points of \ncontact on the two bodies rather than the velocities of their centers of mass, \nand we need to calculate the impulse in such a way as to impart a realis-\ntic rotational eﬀ ect as a result of the collision. We won’t get into the details \nhere, but Chris Hecker’s article, available at htt p://chrishecker.com/images/e/\ne7/Gdmphys3.pdf, does an excellent job of describing both the linear and the \nrotational aspects of collision response. The theory behind collision response \nis explained more fully in [15].\n12.4.7.3. Penalty Forces\nAnother approach to collision response is to introduce imaginary forces called \npenalty forces into the simulation. A penalty force acts like a stiﬀ  damped spring \natt ached to the contact points between two bodies that have just interpenetrat-\ned. Such a force induces the desired collision response over a short but ﬁ nite \nperiod of time. Using this approach, the spring constant k eﬀ ectively controls \nthe duration of the interpenetration, and the damping coeﬃ  cient b acts a bit \nlike the restitution coeﬃ  cient. When b = 0, there is no damping—no energy is \nlost, and the collision is perfectly elastic. As b increases, the collision becomes \nmore plastic.\nLet’s take a brief look at some of the pros and cons of the penalty force ap-\nproach to resolving collisions. On the positive side, penalty forces are easy to \nimplement and understand. They also work well when three or more bodies \nare interpenetrating each other. This problem is very diﬃ  cult to solve when \nresolving collisions one pair at a time. A good example is the Sony PS3 demo \nin which a huge number of rubber duckies are poured into a bathtub—the \nsimulation was nice and stable despite the very large number of collisions. \nThe penalty force method is a great way to achieve this.\n12.4. Rigid Body Dynamics\n",
      "content_length": 2498,
      "extraction_method": "Direct"
    },
    {
      "page_number": 676,
      "chapter": null,
      "content": "654 \n12. Collision and Rigid Body Dynamics\nUnfortunately, because penalty forces respond to penetration (i.e., rela-\ntive position) rather than to relative velocity , the forces may not align with \nthe direction we would intuitively expect, especially during a high-speed col-\nlision. A classic example is a car driving head-on into a truck. The car is low \nwhile the truck is tall. Using only the penalty force method, it is easy to arrive \nat a situation in which the penalty force is vertical, rather than horizontal as \nwe would expect given the velocities of the two vehicles. This can cause the \ntruck to pop its nose up into the air while the car drives under it.\nIn general, the penalty force technique works well for low-speed impacts, \nbut it does not work well at all when objects are moving quickly. It is pos-\nsible to combine the penalty force method with other collision resolution ap-\nproaches in order to strike a balance between stability in the presence of large \nnumbers of interpenetrations and responsiveness and more-intuitive behav-\nior at high velocities.\n12.4.7.4. Using Constraints to Resolve Collisions\n As we’ll investigate in Section 12.4.8, most physics systems permit vari-\nous kinds of constraints to be imposed on the motion of the bodies in the \nsimulation. If collisions are treated as constraints that disallow object in-\nterpenetration, then they can be resolved by simply running the simula-\ntion’s general-purpose constraint solver. If the constraint solver is fast and \nproduces high-quality visual results, this can be an eﬀ ective way to resolve \ncollisions.\n12.4.7.5. Friction\nFriction is a force that arises between two bodies that are in continuous con-\ntact, resisting their movement relative to one another. There are a number of \ntypes of friction. Static friction is the resistance one feels when trying to start \na stationary object sliding along a surface. Dynamic friction is a resisting force \nthat arises when objects are actually moving relative to one another. Sliding \nfriction is a type of dynamic friction that resists movement when an object \nslides along a surface. Rolling friction is a type of static or dynamic friction \nthat acts at the point of contact between a wheel or other round object and the \nsurface it is rolling on. When the surface is very rough, the rolling friction is \nexactly strong enough to cause the wheel to roll without sliding, and it acts \nas a form of static friction. If the surface is somewhat smooth, the wheel may \nslip, and a dynamic form of rolling friction comes into play. Collision friction is \nthe friction that acts instantaneously at the point of contact when two bodies \ncollide while moving. (This is the friction force that we ignored when discuss-\ning Newton’s law of restitution in Section 12.4.7.1.) Various kinds of constraints \n",
      "content_length": 2838,
      "extraction_method": "Direct"
    },
    {
      "page_number": 677,
      "chapter": null,
      "content": "655 \ncan have friction as well. For example, a rusted hinge or axle might resist be-\ning turned by introducing a friction torque.\nLet’s look at an example to understand the essence of how friction works. \nLinear sliding friction is proportional to the component of an object’s weight \nthat is acting normal to the surface on which it is sliding. The weight of an \nobject is just the force due to gravity , G = mg, which is always directed down-\nward. The component of this force normal to an inclined surface that makes an \nangle θ with the horizontal is just GN = mg cos θ. The friction force f is then\n \n cos ,\nf\nmg\n= μ\nθ  \nwhere the constant of proportionality μ is called the coeﬃ  cient of friction . This \nforce acts tangentially to the surface, in a direction opposite to the att empted \nor actual motion of the object. This is illustrated in Figure 12.28.\nFigure 12.28 also shows the component of the gravitational force acting \ntangent to the surface, GT = mg sin θ. This force tends to make the object accel-\nerate down the plane, but in the presence of sliding friction, it is counteracted \nby f. Hence, the net force tangent to the surface is\n \n \nnet\n( in\ncos ).\nT\nF\nG\nf\nmg s\n=\n−\n=\nθ−μ\nθ\nIf the angle of inclination is such that the expression in parentheses is zero, \nthe object will slide at a constant speed (if already moving) or be at rest. If the \nexpression is greater than zero, the object will accelerate down the surface. If it \nis less than zero, the object will decelerate and eventually come to rest.\n12.4.7.6. Welding\nAn additional problem arises when an object is sliding across a polygon soup. \nRecall that a polygon soup is just what its name implies—a soup of essentially \nunrelated polygons (usually triangles). As an object slides from one triangle \nof this soup to the next, the collision detection system will generate additional \nG = mg\n|GN| =\nmg cos θ\n|G T| =\nmg sin θ\n|f| =\n μmg cos θ\nFigure 12.28.  The force of friction f is proportional to the normal component of the object’s \nweight. The proportionality constant μ is called the coefﬁ cient of friction.\n12.4. Rigid Body Dynamics\n",
      "content_length": 2122,
      "extraction_method": "Direct"
    },
    {
      "page_number": 678,
      "chapter": null,
      "content": "656 \n12. Collision and Rigid Body Dynamics\nspurious contacts because it will think that the object is about to hit the edge \nof the next triangle. This is illustrated in Figure 12.29.\nThere are a number of solutions to this problem. One is to analyze the \nset of contacts and discard ones that appear to be spurious, based on various \nheuristics and possibly some knowledge of the object’s contacts on a previous \nframe (e.g., if we know the object was sliding along a surface and a contact \nnormal arises that is due to the object being near the edge of its current tri-\nangle, then discard that contact normal). Versions of Havok prior to 4.5 em-\nployed this approach.\nStarting with Havok 4.5, a new technique was implemented that essen-\ntially annotates the mesh with triangle adjacency information. The collision \ndetection system therefore “knows” which edges are interior edges and can \ndiscard spurious collisions reliably and quickly. Havok describes this solution \nas welding , because in eﬀ ect the edges of the triangles in the poly soup are \nwelded to one another.\n12.4.7.7. Coming to Rest, Islands, and Sleeping\nWhen energy is removed from a simulated system via friction, damping, or \nother means, moving objects will eventually come to rest. This seems like a \nnatural consequence of the simulation—something that would just “fall out” \nof the diﬀ erential equations of motion. Unfortunately, in a real computerized \nsimulation, coming to rest is never quite that simple. Various factors such as \nﬂ oating-point error, inaccuracies in the calculation of restitution forces, and \nnumerical instability can cause objects to jitt er forever rather than coming to \nrest as they should. For this reason, most physics engines use various heuris-\ntic methods to detect when objects are oscillating instead of coming to rest \nas they should. Additional energy can be removed from the system to en-\nsure that such objects eventually sett le down, or they can simply be stopped \nabruptly once their average velocity drops below a threshold.\nFigure 12.29.  When an object slides between two adjacent triangles, spurious contacts with \nthe new triangle’s edge can be generated.\nSpurious Contacts \nwith Triangle Edge\n",
      "content_length": 2219,
      "extraction_method": "Direct"
    },
    {
      "page_number": 679,
      "chapter": null,
      "content": "657 \nWhen an object really does stop moving (ﬁ nds itself in a state of equilib-\nrium ), there is no reason to continue integrating its equations of motion every \nframe. To optimize performance, most physics engines allow dynamic objects \nin the simulation to be put to sleep . This excludes them from the simulation \ntemporarily, although sleeping objects are still active from a collision stand-\npoint. If any force or impulse begins acting on a sleeping object, or if the object \nloses one of the contacts that was holding it in equilibrium, it will be awoken \nso that its dynamic simulation can be resumed.\nSleep Criteria\nVarious criteria can be used to determine whether or not a body qualiﬁ es for \nsleep. It’s not always easy to make this determination in a robust manner for \nall situations. For example, a long pendulum might have very low angular \nmomentum and yet still be moving visibly on-screen.\nThe most commonly used criteria for equilibrium detection include:\nz The body is supported . This means it has three or more contact points \n(or one or more planar contacts) that allow it to att ain equilibrium with \ngravity and any other forces that might be aﬀ ecting it.\nz The body’s linear and angular momentum are below a predeﬁ ned thresh-\nold.\nz A running average of the linear and angular momentum are below a pre-\ndeﬁ ned threshold.\nz The total kinetic energy of the body (T =\n⋅+\n⋅\n1\n2\n1\n2\np v\nL ω) is below a pre-\ndeﬁ ned threshold. The kinetic energy is usually mass-normalized so that \na single threshold can be used for all bodies regardless of their masses.\nz The motion of a body that is about to go to sleep might be progressively \ndamped so that it comes to a smooth stop rather than stopping abruptly.\nSimulation Islands\nBoth Havok and PhysX further optimize their performance by automatically \ngrouping objects that either are interacting or have the potential to interact in \nthe near future into sets called simulation islands . Each simulation island can be \nsimulated independently of all the other islands—an approach that is highly \nconducive to cache coherency optimizations and parallel processing.\nHavok and PhysX both put entire islands to sleep rather than individu-\nal rigid bodies. This approach has its pros and cons. The performance boost \nis obviously larger when a whole group of interacting objects can be put to \nsleep. On the other hand, if even one object in an island is awake, the entire \nisland is awake. Overall, it seems that the pros tend to outweigh the cons, so \n12.4. Rigid Body Dynamics\n",
      "content_length": 2547,
      "extraction_method": "Direct"
    },
    {
      "page_number": 680,
      "chapter": null,
      "content": "658 \n12. Collision and Rigid Body Dynamics\nthe simulation island design is one we’re likely to continue to see in future \nversions of these SDKs.\n12.4.8. Constraints\nAn unconstrained rigid body has six degrees of freedom (DOF): It can trans-\nlate in three dimensions, and it can rotate about the three Cartesian axes. Con-\nstraints restrict an object’s motion, reducing its degrees of freedom either par-\ntially or completely. Constraints can be used to model all sorts of interesting \nbehaviors in a game. Here are a few examples:\nz a swinging chandelier (point-to-point constraint);\nz a door that can be kicked, slammed, blown of its hinges (hinge con-\nstraint);\nz a vehicle’s wheel assembly (axle constraint with damped springs for \nsuspension);\nz a train or a car pulling a trailer (stiﬀ  spring/rod constraint);\nz a rope or chain (chain of stiﬀ  springs or rods);\nz a rag doll (specialized constraints that mimic the behavior of various \njoints in the human skeleton).\nIn the sections that follow, we’ll brieﬂ y investigate these and some of the \nother most common kinds of constraints typically provided by a physics SDK.\n12.4.8.1. Point-to-Point Constraints\nA point-to-point constraint is the simplest type of constraint. It acts like a ball \nand socket joint—bodies can move in any way they like, as long as a speciﬁ ed \npoint on one body lines up with a speciﬁ ed point on the other body. This is \nillustrated in Figure 12.30.\n12.4.8.2. Stiff Springs\nA stiﬀ  spring constraint is a lot like a point-to-point constraint except that it \nkeeps the two points separated by a speciﬁ ed distance. This kind of constraint \nFigure 12.30.  A \npoint-to-point \nconstraint \nre-\nquires \nthat \na \npoint on body A \nalign with a point \non body B.\nFigure 12.31.  A stiff spring constraint requires that a point on body A be separated from a \npoint on body B by a user-speciﬁ ed distance.\n",
      "content_length": 1880,
      "extraction_method": "Direct"
    },
    {
      "page_number": 681,
      "chapter": null,
      "content": "659 \nacts like an invisible rod between the two constrained points. Figure 12.31 il-\nlustrates this constraint.\n12.4.8.3. Hinge Constraints\nA hinge constraint limits rotational motion to only a single degree of freedom, \nabout the hinge’s axis. An unlimited hinge acts like an axle, allowing the con-\nstrained object to complete an unlimited number of full rotations. It’s com-\nmon to deﬁ ne limited hinges that can only move through a predeﬁ ned range \nof angles about the one allowed axis. For example, a one-way door can only \nmove through a 180 degree arc, because otherwise it would pass through the \nadjacent wall. Likewise, a two-way door is constrained to move through a \n±180 degree arc. Hinge constraints may also be given a degree of friction in \nthe form of a torque that resists rotation about the hinge’s axis. A limited hinge \nconstraint is shown in Figure 12.32.\nFigure 12.32.  A limited hinge constraint mimics the behavior of a door.\nFigure 12.33.  A prismatic constraint acts like a piston.\n12.4.8.4. Prismatic Constraints\nPrismatic constraints act like a piston: A constrained body’s motion is restrict-\ned to a single translational degree of freedom. A prismatic constraint may or \nmay not permit rotation about the translation axis of the piston. Prismatic \nconstraints can of course be limited or unlimited and may or may not include \nfriction. A prismatic constraint is illustrated in Figure 12.33.\n12.4. Rigid Body Dynamics\n",
      "content_length": 1449,
      "extraction_method": "Direct"
    },
    {
      "page_number": 682,
      "chapter": null,
      "content": "660 \n12. Collision and Rigid Body Dynamics\n12.4.8.5. Other Common Constraint Types\nMany other types of constraints are possible, of course. Here are just a few \nexamples:\nz Planar . Objects are constrained to move in a two-dimensional plane.\nz Wheel . This is typically a hinge constraint with unlimited rotation, \ncoupled with some form of vertical suspension simulated via a spring-\ndamper as sembly.\nz Pulley . In this specialized constraint, an imaginary rope passes through \na pulley and is att ached to two bodies. The bodies move along the line \nof the rope via a leverage ratio.\nConstraints may be breakable , meaning that aft er enough force is ap-\nplied, they automatically come apart. Alternatively, the game can turn the \nconstraint on and oﬀ  at will, using its own criteria for when the constraint \nshould break.\n12.4.8.6. Constraint Chains\nLong chains of linked bodies are sometimes diﬃ  cult to simulate in a stable \nmanner because of the iterative nature of the constraint solver. A constraint \nchain is a specialized group of constraints with information that tells the \nconstraint solver how the objects are connected. This allows the solver to \ndeal with the chain in a more stable manner than would otherwise be pos-\nsible.\n12.4.8.7. Rag Dolls\nA rag doll is a physical simulation of the way a human body might move \nwhen it is dead or unconscious and hence entirely limp. Rag dolls are created \nby linking together a collection of rigid bodies, one for each semi-rigid part \nof the body. For example, we might have capsules for the feet, calves, thighs, \nhands, upper and lower arms, and head and possibly a few for the torso to \nsimulate the ﬂ exibility of the spine.\nThe rigid bodies in a rag doll are connected to one another via constraints. \nRag doll constraints are specialized to mimic the kinds of motions the joints in \na real human body can perform. We usually make use of constraint chains to \nimprove the stability of the simulation.\nA rag doll simulation is always tightly integrated with the animation \nsystem. As the rag doll moves in the physics world, we extract the positions \nand rotations of the rigid bodies, and use this information to drive the posi-\ntions and orientations of certain joints in the animated skeleton. So in eﬀ ect, a \nrag doll is really just a form of procedural animation that happens to be driven \n",
      "content_length": 2361,
      "extraction_method": "Direct"
    },
    {
      "page_number": 683,
      "chapter": null,
      "content": "661 \nby the physics system. (See Chapter 11 for more details on skeletal anima-\ntion.)\nOf course, implementing a rag doll is not quite as simple as I’ve made it \nsound here. For one thing, there’s usually not a one-to-one mapping between \nthe rigid bodies in the rag doll and the joints in the animated skeleton —the \nskeleton usually has more joints than the rag doll has bodies. Therefore, we \nneed a system that can map rigid bodies to joints (i.e., one that “knows” to \nwhich joint each rigid body in the rag doll corresponds). There may be addi-\ntional joints between those that are being driven by the rag doll bodies, so the \nmapping system must also be capable of determining the correct pose trans-\nforms for these intervening joints. This is not an exact science. We must apply \nartistic judgment and/or some knowledge of human biomechanics in order to \nachieve a natural-looking rag doll.\n12.4.8.8. Powered Constraints\nConstraints can also be “powered,” meaning that an external engine system \nsuch as the animation system can indirectly control the translations and orien-\ntations of the rigid bodies in the rag doll.\nLet’s take an elbow joint as an example. An elbow acts prett y much like \na limited hinge, with a litt le less than 180 degrees of free rotation. (Actual-\nly, an elbow can also rotate axially, but we’ll ignore that for the purposes of \nthis discussion.) To power this constraint, we model the elbow as a rotational \nspring . Such a spring exerts a torque proportional to the spring’s angle of de-\nﬂ ection away from some predeﬁ ned rest angle, N = –k(θ – θ rest). Now imagine \nchanging the rest angle externally, say by ensuring that it always matches the \nangle of the elbow joint in an animated skeleton. As the rest angle changes, the \nspring will ﬁ nd itself out of equilibrium , and it will exert a torque that tends \nBone\nCollision \nCapsule\nCapsule strikes \nan obstacle\nBone continues \nto move\nFigure 12.34.  Left: with a powered rag doll constraint, and in the absence of any additional \nforces or torques, a rigid body representing the lower arm can be made to exactly track the \nmovements of an animated elbow joint. Right: if an obstacle blocks the motion of the body, it \nwill diverge from that of the animated elbow joint in a realistic way.\n12.4. Rigid Body Dynamics\n",
      "content_length": 2310,
      "extraction_method": "Direct"
    },
    {
      "page_number": 684,
      "chapter": null,
      "content": "662 \n12. Collision and Rigid Body Dynamics\nto rotate the elbow back into alignment with θ rest. In the absence of any other \nforces or torques, the rigid bodies will exactly track the motion of the elbow \njoint in the animated skeleton. But if other forces are introduced (for example, \nthe lower arm comes in contact with an immovable object), then these forces \nwill play into the overall motion of the elbow joint, allowing it to diverge from \nthe animated motion in a realistic manner. As illustrated in Figure 12.34, this \nprovides the illusion of a human who is trying her best to move in a certain \nway (i.e., the “ideal” motion provided by the animation) but who is some-\ntimes unable to do so due to the limitations of the physical world (e.g., her arm \ngets caught on something as she tries to swing it forward).\n12.4.9. Controlling the Motions of Rigid Bodies\nMost game designs call for a degree of control over the way rigid bodies move \nover and above the way they would move naturally under the inﬂ uence of \ngravity and in response to collisions with other objects in the scene. For ex-\nample:\nz An air vent applies an upward force to any object that enters its shaft  of \ninﬂ uence.\nz A car is coupled to a trailer and exerts a pulling force on it as it moves.\nz A tractor beam exerts a force on an unwitt ing space craft .\nz An anti-gravity device causes objects to hover.\nz The ﬂ ow of a river creates a force ﬁ eld that causes objects ﬂ oating in the \nriver to move downstream.\nAnd the list goes on. Most physics engines typically provide their users with \na number of ways to exert control over the bodies in the simulation. We’ll out-\nline the most common of these mechanisms in the following sections.\n12.4.9.1. Gravity\nGravity is ubiquitous in most games that take place on the surface of the Earth \nor some other planet (or on a spacecraft  with simulated gravity!). Gravity is \ntechnically not a force but rather a (roughly) constant acceleration, so it af-\nfects all bodies equally regardless of their mass. Because of its ubiquitous and \nspecial nature, the magnitude and direction of the gravitational acceleration \nis speciﬁ ed via a global sett ing in most SDKs. (If you’re writing a space game, \nyou can always set gravity to zero to eliminate it from the simulation.)\n12.4.9.2. Applying Forces\nAny number of forces can be applied to the bodies in a game physics simula-\ntion. A force always acts over a ﬁ nite time interval. (If it acted instantaneous-\n",
      "content_length": 2485,
      "extraction_method": "Direct"
    },
    {
      "page_number": 685,
      "chapter": null,
      "content": "663 \nly, it would be called an impulse—more on that below.) The forces in a game \nare oft en dynamic in nature—they oft en change their directions and/or their \nmagnitudes every frame. So the force-application function in most physics \nSDKs is designed to be called once per frame for the duration of the force’s \ninﬂ uence. The signature of such a function usually looks something like this: \napplyForce(const Vector& forceInNewtons), where the duration of the \nforce is assumed to be Δt.\n12.4.9.3. Applying Torques\nWhen a force is applied such that its line of action passes through the center of \nmass of a body, no torque is generated, and only the body’s linear acceleration \nis aﬀ ected. If it is applied oﬀ -center, it will induce both a linear and a rotational \nacceleration. A pure torque can be applied to a body as well by applying two \nequal and opposite forces to points equidistant from the center of mass. The \nlinear motions induced by such a pair of forces will cancel each other out (since \nfor the purposes of linear dynamics, the forces both act at the center of mass). \nThis leaves only their rotational eﬀ ects. A pair of torque-inducing forces like \nthis is known as a couple (htt p://en.wikipedia.org/wiki/Couple_(mechanics)). \nA special function such as applyTorque(const Vector& torque) may be \nprovided for this purpose. However, if your physics SDK provides no apply\nTorque() function, you can always write one and have it generate a suitable \ncouple instead.\n12.4.9.4. Applying Impulses\nAs we saw in Section 12.4.7.2, an impulse is an instantaneous change in veloc-\nity (or actually, a change in momentum). Technically speaking, an impulse \nis a force that acts for an inﬁ nitesimal amount of time. However, the short-\nest possible duration of force application in a time-stepped dynamics simu-\nlation is Δt, which is not short enough to simulate an impulse adequately. \nAs such, most physics SDKs provide a function with a signature such as \napplyImpulse(const Vector& impulse)  for the purposes of applying \nimpulses to bodies. Of course, impulses come in two ﬂ avors—linear and an-\ngular—and a good SDK should provide functions for applying both types.\n12.4.10. The Collision/Physics Step\nNow that we’ve covered the theory and some of the technical details behind \nimplementing a collision and physics system, let’s take a brief look at how \nthese systems actually perform their updates every frame.\nEvery collision/physics engine performs the following basic tasks during \nits update step . Diﬀ erent physics SDKs may perform these phases in diﬀ erent \n12.4. Rigid Body Dynamics\n",
      "content_length": 2612,
      "extraction_method": "Direct"
    },
    {
      "page_number": 686,
      "chapter": null,
      "content": "664 \n12. Collision and Rigid Body Dynamics\norders. That said, the technique I’ve seen used most oft en goes something like \nthis:\nThe forces and torques acting on the bodies in the physics world are \n1. \nintegrated forward by Δt in order to determine their tentative positions \nand orientations next frame.\nThe collision detection library is called to determine if any new contacts \n2. \nhave been generated between any of the objects as a result of their \ntentative movement. (The bodies normally keep track of their contacts \nin order to take advantage of temporal coherency. Hence at each step of \nthe simulation, the collision engine need only determine whether any \nprevious contacts have been lost and whether any new contacts have \nbeen added.)\nCollisions are resolved, oft en by applying impulses or penalty forces \n3. \nor as part of the constraint solving step below. Depending on the SDK, \nthis phase may or may not include continuous collision detection (CCD, \notherwise known as time of impact detection or TOI).\nConstraints are satisﬁ ed by the constraint solver.\n4. \nAt the conclusion of step 4, some of the bodies may have moved away from \ntheir tentative positions as determined in step 1. This movement may cause \nadditional interpenetrations between objects or cause other previously sat-\nisﬁ ed constraints to be broken. Therefore, steps 1 through 4 (or sometimes \nonly 2 through 4, depending on how collisions and constraints are resolved) \nare repeated until either (a) all collisions have been successfully resolved \nand all constraints are satisﬁ ed, or (b) a predeﬁ ned maximum number of \niterations has been exceeded. In the latt er case, the solver eﬀ ectively “gives \nup,” with the hope that things will resolve themselves naturally during sub-\nsequent frames of the simulation. This helps to avoid performance spikes \nby amortizing the cost of collision and constraint resolution over multiple \nframes. However, it can lead to incorrect-looking behavior if the errors are \ntoo large or if the time step is too long or is inconsistent. Penalty forces can \nbe blended into the simulation in order to gradually resolve these problems \nover time.\n12.4.10.1. The Constraint Solver\nA constraint solver is essentially an iterative algorithm that att empts to satisfy \na large number of constraints simultaneously by minimizing the error between \nthe actual positions and rotations of the bodies in the physics world and their \nideal positions and rotations as deﬁ ned by the constraints. As such, constraint \nsolvers are essentially iterative error minimization algorithms.\n",
      "content_length": 2594,
      "extraction_method": "Direct"
    },
    {
      "page_number": 687,
      "chapter": null,
      "content": "665 \nLet’s take a look ﬁ rst at how a constraint solver works in the trivial case \nof a single pair of bodies connected by a single constraint. During each step of \nthe physics simulation, the numerical integrator will ﬁ nd new tentative trans-\nforms for the two bodies. The constraint solver then evaluates their relative \npositions and calculates the error between the positions and orientations of \ntheir shared axis of rotation. If any error is detected, the solver moves the \nbodies in such a way as to minimize or eliminate it. Since there are no other \nbodies in the system, the second iteration of the step should discover no new \ncontacts, and the constraint solver will ﬁ nd that the one hinge constraint is \nnow satisﬁ ed. Hence the loop can exit without further iterations.\nWhen more than one constraint must be satisﬁ ed simultaneously, more \niterations may be required. During each iteration, the numerical integrator \nwill sometimes tend to move the bodies out of alignment with their con-\nstraints, while the constraint solver tends to put them back into alignment. \nWith luck, and a carefully designed approach to minimizing error in the con-\nstraint solver, this feedback loop should eventually sett le into a valid solution. \nHowever, the solution may not always be exact. This is why, in games with \nphysics engines, you sometimes witness seemingly impossible behaviors, like \nchains that stretch (opening up litt le gaps between the links), objects that in-\nterpenetrate brieﬂ y, or hinges that momentarily move beyond their allowable \nranges. The goal of the constraint solver is to minimize error—it’s not always \npossible to eliminate it completely.\n12.4.10.2. Variations between Engines\nThe description given above is of course an over-simpliﬁ cation of what re-\nally goes on in a physics/collision engine every frame. The way in which the \nvarious phases of computation are performed, and their order relative to one \nanother, may vary from physics SDK to physics SDK. For example, some \nkinds of constraints are modeled as forces and torques that are taken care \nof by the numerical integration step rather than being resolved by the con-\nstraint solver. Collision may be run before the integration step rather than \naft er. Collisions may be resolved in any number of diﬀ erent ways. Our goal \nhere is merely to give you a taste of how these systems work. For a detailed \nunderstanding of how any one SDK operates, you’ll want to read its docu-\nmentation and probably also inspect its source code (presuming the relevant \nbits are available for you to read!). The curious and industrious reader can \nget a good start by downloading and experimenting with ODE and/or PhysX, \nas these two SDKs are available for free. You can also learn a great deal from \nODE’s wiki, which is available at htt p://opende.sourceforge.net/wiki/index.\nphp/Main_Page.\n12.4. Rigid Body Dynamics\n",
      "content_length": 2900,
      "extraction_method": "Direct"
    },
    {
      "page_number": 688,
      "chapter": null,
      "content": "666 \n12. Collision and Rigid Body Dynamics\n12.5. Integrating a Physics Engine \n \ninto Your Game\nObviously, a collision/physics engine is of litt le use by itself—it must be integrat-\ned into your game engine. In this section, we’ll discuss the most common inter-\nface points between the collision/physics engine and the rest of the game code.\n12.5.1. The Linkage between Game Objects and Rigid Bodies\nThe rigid bodies and collidables in the collision/physics world are nothing \nmore than abstract mathematical descriptions. In order for them to be useful \nin the context of a game, we need to link them in some way to their visual \nrepresentations on-screen. Usually, we don’t draw the rigid bodies directly \n(except for debugging purposes). Instead, the rigid bodies are used to describe \nthe shape, size, and physical behavior of the logical objects that make up the \nvirtual game world. We’ll discuss game objects in depth in Chapter 14, but for \nthe time being, we’ll rely on our intuitive notion of what a game object is—a \nlogical entity in the game world, such as a character, a vehicle, a weapon, \na ﬂ oating power-up, and so on. So the linkage between a rigid body in the \nphysics world and its visual representation on-screen is usually indirect, with \nthe logical game object serving as the hub that links the two together. This is \nillustrated in Figure 12.35.\nIn general, a game object is represented in the collision/physics world by \nzero or more rigid bodies. The following list describes three possible scenarios:\nz Zero rigid bodies. Game objects without any rigid bodies in the phys-\nics world act as though they are not solid, because they have no colli-\nsion representation at all. Decorative objects with which the player or \nRigid Body / \nCollidable\nGame \nObject\nMesh \nInstance\nRendering \nEngine\nDebug Draw\nDrive\nUpdate\nSubmit\nFigure 12.35.  Rigid bodies are linked to their visual representations by way of game objects. \nAn optional direct rendering path is usually provided so that the locations of the rigid bodies \ncan be visualized for debugging purposes.\n",
      "content_length": 2085,
      "extraction_method": "Direct"
    },
    {
      "page_number": 689,
      "chapter": null,
      "content": "667 \nnon-player characters cannot interact, such as birds ﬂ ying overhead or \nportions of the game world that can be seen but never reached, might \nhave no collision. This scenario can also apply to objects whose collision \ndetection is handled manually (without the help of the collision/physics \nengine) for some reason.\nz One rigid body. Most simple game objects need only be represented by a \nsingle rigid body. In this case, the shape of the rigid body’s collidable is \nchosen to closely approximate the shape of the game object’s visual rep-\nresentation, and the rigid body’s position and orientation exactly match \nthe position and orientation of the game object itself.\nz Multiple rigid bodies. Some complex game objects are represented by \nmultiple rigid bodies in the collision/physics world. Examples include \ncharacters, machinery, vehicles, or any object that is composed of multi-\nple solid pieces. Such game objects usually make use of a skeleton (i.e., a \nhierarchy of aﬃ  ne transforms) to track the locations of their component \npieces (although other means are certainly possible as well). The rigid \nbodies are usually linked to the joints of the skeleton in such a way that \nthe position and orientation of each rigid body corresponds to the posi-\ntion and orientation of one of the joints. The joints in the skeleton might \nbe driven by an animation, in which case the associated rigid bodies \nsimply come along for the ride. Alternatively, the physics system might \ndrive the locations of rigid bodies and hence indirectly control the loca-\ntions of the joints. The mapping from joints to rigid bodies may or may \nnot be one-to-one—some joints might be controlled entirely by anima-\ntion, while others are linked to rigid bodies.\nThe linkage between game objects and rigid bodies must be managed by \nthe engine, of course. Typically, each game object will manage its own rigid \nbodies, creating and destroying them when necessary, adding and removing \nthem from the physics world as needed, and maintaining the connection be-\ntween each rigid body’s location and the location of the game object and/or \none of its joints. For complex game objects consisting of multiple rigid bodies, \na wrapper class of some kind may be used to manage them. This insulates \nthe game objects from the nitt y-gritt y details of managing a collection of rigid \nbodies and allows diﬀ erent kinds of game objects to manage their rigid bodies \nin a consistent way.\n12.5.1.1. Physics-Driven Bodies\nIf our game has a rigid body dynamics system, then presumably we want \nthe motions of at least some of the objects in the game to be driven entirely \n12.5. Integrating a Physics Engine into Your Game\n",
      "content_length": 2698,
      "extraction_method": "Direct"
    },
    {
      "page_number": 690,
      "chapter": null,
      "content": "668 \n12. Collision and Rigid Body Dynamics\nby the simulation. Such game objects are called physics-driven objects. Bits of \ndebris, exploding buildings, rocks rolling down a hillside, empty magazines \nand shell casings—these are all examples of physics-driven objects.\nA physics-driven rigid body is linked to its game object by stepping the \nsimulation and then querying the physics system for the body’s position and \norientation. This transform is then applied either to the game object as a whole \nor to a joint or some other data structure within the game object.\nExample: Building a Safe with a Detachable Door\nWhen physics-driven rigid bodies are linked to the joints of a skeleton, the \nbodies are oft en constrained to produce a desired kind of motion. As an ex-\nample, let’s look at how a safe with a detachable door might be modeled.\nVisually, let’s assume that the safe consists of a single triangle mesh with \ntwo submeshes, one for the housing and one for the door. A two-joint skeleton \nis used to control the motions of these two pieces. The root joint is bound to \nthe housing of the safe, while the child joint is bound to the door in such a way \nthat rotating the door joint causes the door submesh to swing open and shut \nin a suitable way.\nThe collision geometry for the safe is broken into two independent pieces \nas well, one for the housing and one for the door. These two pieces are used \nto create two totally separate rigid bodies in the collision/physics world. The \nrigid body for the safe’s housing is att ached to the root joint in the skeleton, \nand the door’s rigid body is linked to the door joint. A hinge constraint is then \nadded to the physics world to ensure that the door body swings properly \nrelative to the housing when the dynamics of the two rigid bodies are simu-\nlated. The motions of the two rigid bodies representing the housing and the \ndoor are used to update the transforms of the two joints in the skeleton. Once \nthe skeleton’s matrix palett e has been generated by the animation system, the \nrendering engine will end up drawing the housing and door submeshes in the \nlocations of the rigid bodies within the physics world.\nIf the door needs to be blown oﬀ  at some point, the constraint can be \nbroken, and impulses can be applied to the rigid bodies to send them ﬂ y-\ning. Visibly, it will appear to the human player that the door and the housing \nhave become separate objects. But in reality, it’s still a single game object and \na single triangle mesh with two joints and two rigid bodies.\n12.5.1.2. Game-Driven Bodies\nIn most games, certain objects in the game world need to be moved about in \na non-physical way. The motions of such objects might be determined by an \nanimation or by following a spline path, or they might be under the control \n",
      "content_length": 2806,
      "extraction_method": "Direct"
    },
    {
      "page_number": 691,
      "chapter": null,
      "content": "669 \nof the human player. We oft en want these objects to participate in collision \ndetection—to be capable of pushing the physics-driven objects out of their \nway, for example—but we do not want the physics system to interfere with \ntheir motion in any way. To accommodate such objects, most physics SDKs \nprovide a special type of rigid body known as a game-driven body. (Havok calls \nthese “key framed” bodies.)\nGame-driven bodies do not experience the eﬀ ects of gravity. They are also \nconsidered to be inﬁ nitely massive by the physics system (usually denoted by \na mass of zero, since this is an invalid mass for a physics-driven body). The \nassumption of inﬁ nite mass ensures that forces and collision impulses within \nthe simulation can never change the velocity of a game-driven body.\nTo move a game-driven body around in the physics world, we cannot \nsimply set its position and orientation every frame to match the location of \nthe corresponding game object. Doing so would introduce discontinuities that \nwould be very diﬃ  cult for the physical simulation to resolve. (For example, \na physics-driven body might ﬁ nd itself suddenly interpenetrating a game-\ndriven body, but it would have no information about the game-driven body’s \nmomentum with which to resolve the collision.) As such, game-driven bodies \nare usually moved using impulses —instantaneous changes in velocity that, \nwhen integrated forward in time, will position the bodies in the desired places \nat the end of the time step. Most physics SDKs provide a convenience func-\ntion that will calculate the linear and angular impulses required in order to \nachieve a desired position and orientation on the next frame. When moving \na game-driven body, we do have to be careful to zero out its velocity when it \nis supposed to stop. Otherwise, the body will continue forever along its last \nnon-zero trajectory.\nExample: Animated Safe Door\nLet’s continue our example of the safe with a detachable door. Imagine that \nwe want a character to walk up to the safe, dial the combination, open the \ndoor, deposit some money, and close and lock the door again. Later, we want \na diﬀ erent character to get the money in a rather less-civilized manner—by \nblowing the door oﬀ  the safe. To do this, the safe would be modeled with an \nadditional submesh for the dial and an additional joint that allows the dial to \nbe rotated. No rigid body is required for the dial, however, unless of course \nwe want it to ﬂ y oﬀ  when the door explodes.\nDuring the animated sequence of the person opening and closing the safe, \nits rigid bodies can be put into game-driven mode. The animation now drives \nthe joints, which in turn drive the rigid bodies. Later, when the door is to be \nblown oﬀ , we can switch the rigid bodies into physics-driven mode, break the \nhinge constraint, apply the impulse, and watch the door ﬂ y.\n12.5. Integrating a Physics Engine into Your Game\n",
      "content_length": 2929,
      "extraction_method": "Direct"
    },
    {
      "page_number": 692,
      "chapter": null,
      "content": "670 \n12. Collision and Rigid Body Dynamics\nAs you’ve probably already noticed, the hinge constraint is not actually \nneeded in this particular example. It would only be required if the door is to \nbe left  open at some point and we want to see the door swinging naturally in \nresponse to the safe being moved or the door being bumped.\n12.5.1.3. Fixed Bodies\nMost game worlds are composed of both static geometry and dynamic objects. \nTo model the static components of the game world, most physics SDKs pro-\nvide a special kind of rigid body known as a ﬁ xed body. Fixed bodies act a bit \nlike game-driven bodies, but they do not take part in the dynamics simulation \nat all. They are, in eﬀ ect, collision-only bodies. This optimization can give a \nbig performance boost to most games, especially those whose worlds contain \nonly a small number of dynamic objects moving around within a large static \nworld.\n12.5.1.4. Havok’s Motion Type\nIn Havok, all types of rigid body are represented by instances of the class hkp\nRigidBody. Each instance contains a ﬁ eld that speciﬁ es its motion type . The \nmotion type tells the system whether the body is ﬁ xed, game-driven (what \nHavok calls “key framed”), or physics-driven (what Havok calls “dynamic”). \nIf a rigid body is created with the ﬁ xed motion type, its type can never be \nchanged. Otherwise, the motion type of a body can be changed dynamically at \nruntime. This feature can be incredibly useful. For example, an object that is in \na character’s hand would be game-driven. But as soon as the character drops \nor throws the object, it would be changed to physics-driven so the dynamics \nsimulation can take over its motion. This is easily accomplished in Havok by \nsimply changing the motion type at the moment of release.\nThe motion type also doubles as a way to give Havok some hints about \nthe inertia tensor of a dynamic body. As such, the “dynamic” motion type is \nbroken into subcategories such as “dynamic with sphere inertia,” “dynamic \nwith box inertia,” and so on. Using the body’s motion type, Havok can decide \nto apply various optimizations based on assumptions about the internal struc-\nture of the inertia tensor.\n12.5.2. Updating the Simulation\nThe physics simulation must of course be updated periodically, usually once \nper frame. This does not merely involve stepping the simulation (numerically \nintegrating, resolving collisions, and applying constraints). The linkages be-\ntween the game objects and their rigid bodies must be maintained as well. If \nthe game needs to apply any forces or impulses to any of the rigid bodies, this \n",
      "content_length": 2608,
      "extraction_method": "Direct"
    },
    {
      "page_number": 693,
      "chapter": null,
      "content": "671 \nmust also be done every frame. The following steps are required to completely \nupdate the physics simulation:\nz Update game-driven rigid bodies . The transforms of all game-driven rigid \nbodies in the physics world are updated so that they match the trans-\nforms of their counterparts (game objects or joints) in the game world.\nz Update phantoms . A phantom shape acts like a game-driven collidable \nwith no corresponding rigid body. It is used to perform certain kinds \nof collision queries. The locations of all phantoms are updated prior to \nthe physics step, so that they will be in the right places when collision \ndetection is run.\nz Update forces, apply impulses, and adjust constraints. Any forces being ap-\nplied by the game are updated. Any impulses caused by game events \nthat occurred this frame are applied. Constraints are adjusted if neces-\nsary. (For example, a breakable hinge might be checked to determine if \nit has been broken; if so, the physics engine is instructed to remove the \nconstraint.)\nz Step the simulation . We saw in Section 12.4.10 that the collision and phys-\nics engines must both be updated periodically. This involves numerically \nintegrating the equations of motion to ﬁ nd the physical state of all bodies \non the next frame, running the collision detection algorithm to add and \nremove contacts from all rigid bodies in the physics world, resolving col-\nlisions, and applying constraints. Depending on the SDK, these update \nphases may be hidden behind a single atomic step() function, or it \nmay be possible to run them individually.\nz Update physics-driven game objects . The transforms of all physics-driven \nobjects are extracted from the physics world, and the transforms of the \ncorresponding game objects or joints are updated to match.\nz Query phantoms. The contacts of each phantom shape are read aft er the \nphysics step and used to make decisions.\nz Perform collision cast queries . Ray casts and shape casts are kicked oﬀ , either \nsynchronously or asynchronously. When the results of these queries become \navailable, they are used by various engine systems to make decisions.\nThese tasks are usually performed in the order shown above, with the \nexception of ray and shape casts, which can theoretically be done at any time \nduring the game loop. Clearly it makes sense to update game-driven bod-\nies and apply forces and impulses prior to the step, so that the eﬀ ects will \nbe “seen” by the simulation. Likewise, physics-driven game objects should \nalways be updated aft er the step, to ensure that we’re using the most up-to-\ndate body transforms. Rendering typically happens aft er everything else in \n12.5. Integrating a Physics Engine into Your Game\n",
      "content_length": 2716,
      "extraction_method": "Direct"
    },
    {
      "page_number": 694,
      "chapter": null,
      "content": "672 \n12. Collision and Rigid Body Dynamics\nthe game loop. This ensures that we are rendering a consistent view of the \ngame world at a particular instant in time.\n12.5.2.1. Timing Collision Queries\nIn order to query the collision system for up-to-date information, we need to \nrun our collision queries (ray and shape casts) aft er the physics step has run \nduring the frame. However, the physics step is usually run toward the end of \nthe frame, aft er the game logic has made most of its decisions and the new lo-\ncations of any game-driven physics bodies have been determined. When then \nshould collision queries be run ?\nThis question does not have an easy answer. We have a number of op-\ntions, and most games end up using some or all of them:\nz Base decisions on last frame’s state. In many cases, decisions can be made \ncorrectly based on last frame’s collision information. For example, we \nmight want to know whether or not the player was standing on some-\nthing last frame, in order to decide whether or not he should start falling \nthis frame. In this case, we can safely run our collision queries prior to \nthe physics step.\nz Accept a one-frame lag. Even if we really want to know what is happen-\ning this frame, we may be able to tolerate a one-frame lag in our collision \nquery results. This is usually only true if the objects in question aren’t \nmoving too fast. For example, we might move one object forward in \ntime and then want to know whether or not that object is now in the \nplayer’s line of sight. A one-frame-oﬀ  error in this kind of query may \nnot be noticeable to the player. If this is the case, we can run the collision \nquery prior to the physics step (returning collision information from the \nprevious frame) and then use these results as if they were an approxima-\ntion to the collision state at the end of the current frame.\nz Run the query aft er the physics step. Another approach is to run certain \nqueries aft er the physics step. This is feasible when the decisions being \nmade based on the results of the query can be deferred until late in the \nframe. For example, a rendering eﬀ ect that depends on the results of a \ncollision query could be implemented this way.\n12.5.2.2. Single-Threaded Updating\nA very simple single-threaded game loop might look something like this:\nF32 dt = 1.0f/30.0f;\nfor (;;) // main game loop\n{\n g_hidManager->poll();\n",
      "content_length": 2388,
      "extraction_method": "Direct"
    },
    {
      "page_number": 695,
      "chapter": null,
      "content": "673 \n g_gameObjectManager->\npreAnimationUpdate(dt);\n g_animationEngine->updateAnimations(dt);\n g_gameObjectManager->\npostAnimationUpdate(dt);\ng_physicsWorld->step(dt);\n g_animationEngine->updateRagDolls(dt);\n g_gameObjectManager->\npostPhysicsUpdate(dt);\n g_animationEngine->finalize();\n g_effectManager->update(dt);\n g_audioEngine->udate(dt);\n \n// etc.\n g_renderManager->render();\n \ndt = calcDeltaTime();\n}\nIn this example, our game objects are updated in three phases: once before an-\nimation runs (during which they can queue up new animations, for example), \nonce aft er the animation system has calculated ﬁ nal local poses and a tenta-\ntive global pose (but before the ﬁ nal global pose and matrix palett e has been \ngenerated), and once aft er the physics system has been stepped.\nz The locations of all game-driven rigid bodies are generally updated in \npreAnimationUpdate() or postAnimationUpdate(). Each game-\ndriven body’s transform is set to match the location of either the game \nobject that owns it or a joint in the owner’s skeleton.\nz The location of each physics-driven rigid body is generally read in \npostPhysicsUpdate() and used to update the location of either the \ngame object or one of the joints in its skeleton.\nOne important concern is the frequency with which you are stepping \nthe physics simulation. Most numerical integrators, collision detection algo-\nrithms, and constraint solvers operate best when the time between steps (Δt) \nis constant. It’s usually a good idea to step your physics/collision SDK with an \nideal 1/30 second or 1/60 second time delta and then govern the frame rate of \nyour overall game loop.\n12.5.2.3. Multithreaded Updating\nThings get a bit more complicated when a physics engine is integrated into \na multiprocessor or multithreaded game engine. In Section 7.6, we saw that \nthere are many possible ways to structure the game loop to take advantage of \nmultiprocessor hardware. Let’s take a brief look at some of the physics-speciﬁ c \nissues that arise when applying these techniques.\n12.5. Integrating a Physics Engine into Your Game\n",
      "content_length": 2090,
      "extraction_method": "Direct"
    },
    {
      "page_number": 696,
      "chapter": null,
      "content": "674 \n12. Collision and Rigid Body Dynamics\nRunning Physics in a Separate Thread\nOne option is to run the physics/collision engine in a dedicated thread . \nAs you might guess, this kind of design can lead to race conditions. If a \ngame object doesn’t update its game-driven rigid bodies in time, the physics \nthread might end up using out-of-date locations in the simulation. Likewise, \nif the simulation isn’t quite done by the time we want to update our physics-\ndriven objects, the game objects might end up using out-of-date locations as \nwell.\nThis problem can be solved by arranging for the physics and main threads \nto wait for one another—a process known as thread synchronization . This is \ndone via mutexes, semaphores, or critical sections. Thread synchronization \nis usually a relatively expensive operation, so we generally aim to reduce the \nnumber of synchronization points between threads. In the case of the physics \nengine, we need two synchronization points at minimum—one that allows \nthe physics simulation to begin each frame (aft er all game-driven rigid bodies \nhave been updated) and one that notiﬁ es the main thread when the simulation \nis complete (thereby allowing physics-driven bodies to be queried).\nAs part of a strategy to reduce synchronization points, communication \nbetween threads is usually done via a command queue. The main thread locks \na critical section, writes some commands into the queue, and then quickly re-\nleases it. The physics thread picks up the next batch of commands whenever \nit gets the chance, again locking the critical section to ensure that the main \nthread isn’t overwriting the queue during the read.\nIn the presence of collision queries , things get even more complicated. \nTo manage access to the collision/physics world by multiple threads, phys-\nics engines like Havok allow the world to be locked and unlocked separately \nfor reading and for writing. This allows collision queries to be performed at \nany time during the game loop (during which the world is locked for read) \nexcept while the physics world is being updated (during which it is locked \nfor write).\nFork and Join\nThe nice thing about a fork and join architecture for physics is that it essen-\ntially eliminates all inter-thread synchronization issues. The main thread runs \nas usual until the physics system needs to be stepped. Then we fork the step \noﬀ  into separate threads (ideally one per processing core or hardware thread) \nin order to execute it as quickly as possible. When all threads have completed \ntheir work, the results can be collated, and the main thread can continue as in \nthe single-threaded case. Of course, for this to work, the physics system must \nbe designed to support fork and join. Most physics SDKs, including Havok \n",
      "content_length": 2780,
      "extraction_method": "Direct"
    },
    {
      "page_number": 697,
      "chapter": null,
      "content": "675 \nand PhysX, make use of collision islands—groups of rigid bodies that can be \nsimulated independently of one another. This design lends itself well to a fork \nand join architecture, as the islands can be dynamically distributed among the \navailable threads.\nJobs\nA job model can be particularly useful for physics processing if the physics \nSDK allows the individual phases of its update step (integration, collision de-\ntection, constraint solving, CCD, etc.) to be run independently. This allows \nus to kick oﬀ  each phase whenever it is most convenient and perform useful \nunrelated work in the main thread while we wait for the physics engine to do \nits thing.\nJobs are even more useful when doing collision queries (ray and shape \ncasts). This is because while a game engine typically only needs to step the \nphysics simulation once per frame, collision queries may be required at many \ndiﬀ erent points during the game loop. If lightweight jobs are used to run \nqueries, we can simply kick oﬀ  jobs whenever we need them. On the other \nhand, if collision queries can only be run at certain times during the frame \n(because they are being executed by a fork or a dedicated thread), this makes \nthe job of the game programmer more diﬃ  cult. He or she needs to collect all \nthe collision queries in a queue and then execute them as a batch the next time \nqueries are run during the frame. These two approaches are compared in Fig-\nure 12.36.\nPPU\nSPU\nJob\nJob\nJob\nMain \nThread\nCollision \nThread\nProcess Batch\nProcess Batch\nP...\nJob\nJob\nJob\nJ..\nJ\nGame Loop\nGame Loop\nFigure 12.36.  Collision queries are often batched, to be run at a few well-chosen points dur-\ning the game loop. However, with a job model, queries can be kicked off at any time, without \nthe need to batch them.\n12.5.3. Example Uses of Collision and Physics in a Game\nTo make our discussion of collision and physics more concrete, let’s take a \nhigh-level look at a few common examples of how collision and/or physics \nsimulations are commonly used in real games.\n12.5. Integrating a Physics Engine into Your Game\n",
      "content_length": 2087,
      "extraction_method": "Direct"
    },
    {
      "page_number": 698,
      "chapter": null,
      "content": "676 \n12. Collision and Rigid Body Dynamics\n12.5.3.1. Simple Rigid Body Game Objects\nMany games include simple physically simulated objects like weapons, rocks \nthat can be picked up and thrown, empty magazines, furniture, objects on \nshelves that can be shot, and so on. Such objects might be implemented by \ncreating a custom game object class and giving it a reference to a rigid body \nin the physics world (e.g., hkRigidBody if we’re using Havok). Or we might \ncreate an add-on component class that handles simple rigid body collision \nand physics, allowing this feature to be added to virtually any type of game \nobject in the engine.\nSimple physics objects usually change their motion type at runtime. They \nare game-driven when being held in a character’s hand and physics-driven \nwhen in free fall aft er having been dropped.\nImagine that a simple physics object is to be placed on a table or shelf, to \nbe knocked oﬀ  at some point by being struck by a bullet or other object. What \nmotion type should it be given initially? Should we make it physics-driven \nand let the simulation put it to sleep until it is struck? Or should we keep \nit game-driven when at rest and change it to physics-driven when hit? This \ndepends largely on the game design. If we require tight control over when \nthe object is allowed to be knocked down, then we might go the game-driven \nroute; otherwise, physics-driven may suﬃ  ce.\n12.5.3.2. Bullet Traces\nWhether or not you approve of game violence, the fact remains that bullets \nand projectiles of one form or another are a big part of most games. Let’s look \nat how these are typically implemented.\nSometimes bullets are implemented using ray casts. On the frame that the \nweapon is ﬁ red, we shoot oﬀ  a ray cast, determine what object was hit, and \nimmediately impart the impact to the aﬀ ected object.\nUnfortunately, the ray cast approach does not account for the travel time \nof the projectile. It also does not account for the slight downward trajectory \ncaused by the inﬂ uence of gravity. If these details are important to the game, \nwe can model our projectiles using real rigid bodies that move through the \ncollision/physics world over time. This is especially useful for slower-moving \nprojectiles, like thrown objects or rockets.\nThere are plenty of issues to consider and deal with when implementing \nbullets and projectiles. Here are a few of the most common ones:\nz Does the ray come from the camera focal point or from the tip of the gun \nin the player character’s hands? This is especially problematic in a third-\nperson shooter, where the ray coming out of the player’s gun usually \n",
      "content_length": 2641,
      "extraction_method": "Direct"
    },
    {
      "page_number": 699,
      "chapter": null,
      "content": "677 \ndoes not align with the ray coming from the camera focal point through \nthe reticle in the center of the screen. This can lead to situations in which \nthe reticle appears to be on top of a target yet the third-person character \nis clearly behind an obstacle and would not be able to shoot that target \nfrom his point of view. Various “tricks” must usually be employed to \nensure that the player feels like he or she is shooting what he or she is \naiming at while maintaining plausible visuals on the screen.\nz Mismatches between collision geometry and visible geometry can lead \nto situations in which the player can see the target through a small crack \nor just over the edge of some other object and yet the collision geometry \nis solid and hence the bullet cannot reach the target. (This is usually \nonly a problem for the player character.) One solution to this problem is \nto use a render query instead of a collision query to determine if the ray \nactually hit the target. For example, during one of the rendering passes, \nwe could generate a texture in which each pixel stores the unique identi-\nﬁ er of the game object to which it corresponds. We can then query this \ntexture to determine whether or not an enemy character or other suit-\nable target is beneath the weapon’s reticle.\nz AI characters may need to “lead” their shots if projectiles take a ﬁ nite \namount of time to reach their targets.\nz When bullets hit their targets, we may want to trigger a sound or a par-\nticle eﬀ ect, lay down a decal, or perform other tasks.\n12.5.3.3. Grenades\nGrenades in games are sometimes implemented as free-moving physics objects. \nHowever, this leads to a signiﬁ cant loss of control. Some control can be regained \nby imposing various artiﬁ cial forces or impulses on the grenade. For example, \nwe could apply an extreme air drag once the grenade bounces for the ﬁ rst time, \nin an att empt to limit the distance it can bounce away from its target.\nSome game teams actually go so far as to manage the grenade’s motion en-\ntirely manually. The arc of a grenade’s trajectory can be calculated beforehand, \nusing a series of ray casts to determine what target it would hit if released. The \ntrajectory can even be shown to the player via some kind of on-screen display. \nWhen the grenade is thrown, the game moves it along its arc and can then \ncarefully control the bounce so that it never goes too far away from its target, \nwhile still looking natural.\n12.5.3.4. Explosions\nIn a game, an explosion typically has a few components: some kind of visual \neﬀ ect like a ﬁ reball and smoke, audio eﬀ ects to mimic the sound of the explo-\n12.5. Integrating a Physics Engine into Your Game\n",
      "content_length": 2690,
      "extraction_method": "Direct"
    },
    {
      "page_number": 700,
      "chapter": null,
      "content": "678 \n12. Collision and Rigid Body Dynamics\nsion and its impacts with objects in the world, and a growing damage radius \nthat aﬀ ects any objects in its wake.\nWhen an object ﬁ nds itself in the radius of an explosion, its health is typi-\ncally reduced, and we oft en also want to impart some motion to mimic the ef-\nfect of the shock wave. This might be done via an animation. (For example, the \nreaction of character to an explosion might best be done this way.) We might \nalso wish to allow the impact reaction to be driven entirely by the dynamics \nsimulation. We can accomplish this by having the explosion apply impulses \nto any suitable objects within its radius. It’s prett y easy to calculate direction \nof these impulses—they are typically radial, calculated by normalizing the \nvector from the center of the explosion to the center of the impacted object \nand then scaling this vector by the magnitude of the explosion (and perhaps \nfalling oﬀ  as the distance from the epicenter increases).\nExplosions may interact with other engine systems as well. For example, \nwe might want to impart a “force” to the animated foliage system, causing \ngrass, plants and trees to momentarily bend as a result of the explosion’s \nshock wave.\n12.5.3.5. Destructible Objects\nDestructible objects are commonplace in many games. These objects are pecu-\nliar because they start out in an undamaged state in which they must appear \nto be a single cohesive object, and yet they must be capable of breaking into \nmany separate pieces. We may want the pieces to break oﬀ  one by one, al-\nlowing the object to be “whitt led down” gradually, or we may only require a \nsingle catastrophic explosion.\nDeformable body simulations like DMM can handle destruction naturally. \nHowever, we can also implement breakable objects using rigid body dynamics. \nThis is typically done by dividing a model into a number of breakable pieces \nand assigning a separate rigid body to each one. For reasons of performance op-\ntimization and/or visual quality, we might decide to use special “undamaged” \nversions of the visual and collision geometry, each of which is constructed as \na single solid piece. This model can be swapped out for the damaged version \nwhen the object needs to start breaking apart. In other cases, we may want to \nmodel the object as separate pieces at all times. This might be appropriate if the \nobject is a stack of bricks or a pile of pots and pans, for example.\nTo model a multi-piece object, we could simply stack a bunch of rigid \nbodies and let physics simulation take care of it. This can be made to work \nin good-quality physics engines (although it’s not always trivial to get right). \nHowever, we may want some Hollywood-style eﬀ ects that cannot be achieved \nwith a simple stack of rigid bodies.\n",
      "content_length": 2799,
      "extraction_method": "Direct"
    },
    {
      "page_number": 701,
      "chapter": null,
      "content": "679 \nFor example, we may want to deﬁ ne the structure of the object. Some \npieces might be indestructible, like the base of a wall or the chassis of a car. \nOthers might be non-structural—they just fall oﬀ  when hit by bullets or other \nobjects. Still other pieces might be structural—if they are hit, not only do they \nfall, but they also impart forces to other pieces lying on top of them. Some \npieces could be explosive—when they are hit, they create secondary explosions \nor propagate damage throughout the structure. We may want some pieces to \nact as valid cover points for characters but not others. This implies that our \nbreakable object system may have some connections to the cover system.\nWe might also want our breakable objects to have a notion of health. Dam-\nage might build up until eventually the whole thing collapses, or each piece \nmight have a health, requiring multiples shots or impacts before it is allowed \nto break. Constraints might also be employed to allow broken pieces to hang \noﬀ  the object rather than coming away from it completely.\nWe may also want our structures to take time to collapse completely. For \nexample, if a long bridge is hit by an explosion at one end, the collapse should \nslowly propagate from one end to the other so that the bridge looks massive. \nThis is another example of a feature the physics system won’t give you for \nfree—it would just wake up all rigid bodies in the simulation island simulta-\nneously. These kinds of eﬀ ects can be implemented through judicious use of \nthe game-driven motion type.\n12.5.3.6. Character Mechanics\nFor a game like bowling, pinball, or Marble Madness, the “main character” is \na ball that rolls around in an imaginary game world. For this kind of game, \nwe could very well model the ball as a free-moving rigid body in the physics \nsimulation and control its movements by applying forces and impulses to it \nduring gameplay.\nIn character-based games, however, we usually don’t take this kind of ap-\nproach. The movement of a humanoid or animal character is usually far too \ncomplex to be controlled adequately with forces and impulses. Instead, we \nusually model characters as a set of game-driven capsule-shaped rigid bodies, \neach one linked to a joint in the character’s animated skeleton. These bodies \nare primarily used for bullet hit detection or to generate secondary eﬀ ects \nsuch as when a character’s arm bumps an object oﬀ  a table. Because these \nbodies are game-driven , they won’t avoid interpenetrations with immovable \nobjects in the physics world, so it is up to the animator to ensure that the char-\nacter’s movements appear believable.\nTo move the character around in the game world, most games use sphere \nor capsule casts to probe in the direction of desired motion. Collisions are \nresolved manually. This allows us to do cool stuﬀ  like:\n12.5. Integrating a Physics Engine into Your Game\n",
      "content_length": 2907,
      "extraction_method": "Direct"
    },
    {
      "page_number": 702,
      "chapter": null,
      "content": "680 \n12. Collision and Rigid Body Dynamics\nz having the character slide along walls when he runs into them at an \noblique angle;\nz allowing the character to “pop up” over low curbs rather than gett ing \nstuck;\nz preventing the character from entering a “falling” state when he walks \noﬀ  a low curb;\nz preventing the character from walking up slopes that are too steep \n(most games have a cut-oﬀ  angle aft er which the character will slide \nback rather than being able to walk up the slope);\nz adjusting animations to accommodate collisions.\nAs an example of this last point, if the character is running directly into a \nwall at a roughly 90 degree angle, we can let the character “moonwalk” into \nthe wall forever, or we can slow down his animation. We can also do some-\nthing even more slick, like playing an animation in which the character sticks \nout his hand and touches the wall and then idles sensibly until the movement \ndirection changes.\nHavok provides a character controller system that handles many of these \nthings. In Havok’s system, illustrated in Figure 12.37, a character is modeled \nas a capsule phantom that is moved each frame to ﬁ nd a potential new loca-\ntion. A collision contact manifold (i.e., a collection of contact planes, cleaned \nup to eliminate noise) is maintained for the character. This manifold can be \nFigure 12.37.  Havok’s character controller models a character as a capsule-shaped phantom. \nThe phantom maintains a noise-reduced collision manifold (a collection of contact planes) \nthat can be used by the game to make movement decisions.\n",
      "content_length": 1581,
      "extraction_method": "Direct"
    },
    {
      "page_number": 703,
      "chapter": null,
      "content": "681 \nanalyzed each frame in order to determine how best to move the character, \nadjust animations, and so on.\n12.5.3.7. Camera Collision\nIn many games, the camera follows the player’s character or vehicle around \nin the game world, and it can oft en by rotated or controlled in limited ways \nby the person playing the game. It’s important in such games to never permit \nthe camera to interpenetrate geometry in the scene, as this would break the \nillusion of realism. The camera system is therefore another important client of \nthe collision engine in many games.\nThe basic idea behind most camera collision systems is to surround the \nvirtual camera with one or more sphere phantoms or sphere cast queries that \ncan detect when it is gett ing close to colliding with something. The system can \nrespond by adjusting the camera’s position and/or orientation in some way \nto avoid the potential collision before the camera actually passes through the \nobject in question.\nThis sounds simple enough, but it is actually an incredibly tricky problem \nrequiring a great deal of trial and error to get right. To give you a feel for how \nmuch eﬀ ort can be involved, many game teams have a dedicated engineer \nworking on the camera system for the entire duration of the project. We can’t \npossibly cover camera collision detection and resolution in any depth here, \nbut the following list should give you a sense of some of the most pertinent \nissues to be aware of:\nz Zooming the camera in to avoid collisions works well in a wide variety \nof situations. In a third-person game, you can zoom all the way in to a \nﬁ rst-person view without causing too much trouble (other than making \nsure the camera doesn’t interpenetrate the character’s head in the pro-\ncess).\nz It’s usually a bad idea to drastically change the horizontal angle of the \ncamera in response to collisions, as this tends to mess with camera-rel-\native player controls. However, some degree of horizontal adjustment \ncan work well, depending on what the player is expected to be doing \nat the time. If she is aiming at a target, she’ll be angry with you if you \nthrow oﬀ  her aim to bring the camera out of collision. But if she’s just \nlocomoting through the world, the change in camera orientation may \nfeel entirely natural.\nz You can adjust the vertical angle of the camera to some degree, but it’s \nimportant not to do too much of this or the player will lose track of the \nhorizon and end up looking down onto the top of the player character’s \nhead!\n12.5. Integrating a Physics Engine into Your Game\n",
      "content_length": 2566,
      "extraction_method": "Direct"
    },
    {
      "page_number": 704,
      "chapter": null,
      "content": "682 \n12. Collision and Rigid Body Dynamics\nz Some games allow the camera to move along an arc lying in a vertical \nplane, perhaps described by a spline . This permits a single HID control \nsuch as the vertical deﬂ ection of the left  thumb stick to control both the \nzoom and the vertical angle of the camera in an intuitive way. (The cam-\nera in Uncharted: Drake’s Fortune works this way.) When the camera comes \ninto collision with objects in the world, it can be automatically moved \nalong this same arc to avoid the collision, the arc might be compressed \nhorizontally, or any number of other approaches might be taken.\nz It’s important to consider not only what’s behind and beside the camera \nbut what is in front of it as well. For example, what should happen if \na pillar or another character comes between the camera and the player \ncharacter? In some games, the oﬀ ending object becomes translucent ; in \nothers, the camera zooms in or swings around to avoid the collision. \nThis may or may not feel good to the person playing the game! How \nyou handle these kinds of situations can make or break the perceived \nquality of your game.\nz You may want the camera to react to collisions diﬀ erently in diﬀ erent \nsituations. For example, when the main character is not engaged in a \nbatt le, it might be acceptable to swing the camera horizontally to avoid \ncollisions. But when the player is trying to ﬁ re at targets, both horizontal \nand vertical camera swings will throw oﬀ  his or her aim, so zoom may \nbe the only option.\nEven aft er taking account of these and many other problematic situations, \nyour camera may not look or feel right! Always budget plenty of time for trial \nand error when implementing a camera collision system.\n12.5.3.8. Rag Doll Integration\n In Section 12.4.8.7, we learned how special types of constraints can be used to \nlink a collection of rigid bodies together to mimic the behavior of a limp (dead \nor unconscious) human body. In this section, we’ll investigate a few of the is-\nsues that arise when integrating rag doll physics into your game.\nAs we saw in Section 12.5.3.6, the gross movements of a conscious char-\nacter are usually determined by performing shape casts or moving a phantom \nshape around in the game world. The detailed movements of the character’s \nbody are typically driven by animations. Game-driven rigid bodies are some-\ntimes att ached to the limbs for the purposes of weapons targeting or to allow \nthe character to knock over other objects in the world.\nWhen a character becomes unconscious, the rag doll system kicks in. \nThe character’s limbs are modeled as capsule-shaped rigid bodies connected \n",
      "content_length": 2666,
      "extraction_method": "Direct"
    },
    {
      "page_number": 705,
      "chapter": null,
      "content": "683 \nvia constraints and linked to joints in the character’s animated skeleton. The \nphysics system simulates the motions of these bodies, and we update the \nskeletal joints to match, thereby allowing physics to move the character’s \nbody .\nThe set of rigid bodies used for rag doll physics might not be the same \nones aﬃ  xed to the character’s limbs when it was alive. This is because the \ntwo collision models have very diﬀ erent requirements. When the character \nis alive, its rigid bodies are game-driven, so we don’t care if they interpen-\netrate. And in fact, we usually want them to overlap, so there aren’t any holes \nthrough which an enemy character might shoot. But when the character turns \ninto a rag doll, it’s important that the rigid bodies do not interpenetrate, as \nthis would cause the collision resolution system to impart large impulses that \nwould tend to make the limbs explode outward! For these reasons, it’s actually \nquite common for characters to have entirely diﬀ erent collision/physics repre-\nsentations depending on whether they’re conscious or unconscious.\nAnother issue is how to transition from the conscious state to the uncon-\nscious state. A simple LERP animation blend between animation-generated \nand physics-generated poses usually doesn’t work very well, because the phys-\nics pose very quickly diverges from the animation pose. (A blend between two \ntotally unrelated poses usually doesn’t look natural.) As such, we may want to \nuse powered constraints during the transition (see Section 12.4.8.8).\nCharacters oft en interpenetrate background geometry when they are con-\nscious (i.e., when their rigid bodies are game-driven). This means that the rigid \nbodies might be inside another solid object when the character transitions to \nrag doll (physics-driven) mode. This can give rise to huge impulses that cause \nrather wild-looking rag doll behavior in-game. To avoid these problems, it \nis best to author death animations carefully, so that the character’s limbs are \nkept out of collision as best as possible. It’s also important to detect collisions \nvia phantoms or collision callbacks during the game-driven mode so that you \ncan drop the character into rag doll mode the moment any part of his body \ntouches something solid.\nEven when these steps are taken, rag dolls have a tendency to get stuck \ninside other objects. Single-sided collision can be an incredibly important fea-\nture when trying to make rag dolls look good. If a limb is partly embedded \nin a wall, it will tend to be pushed out of the wall rather than staying stuck \ninside it. However, even single-sided collision doesn’t solve all problems. For \nexample, when the character is moving quickly or if the transition to rag doll \nisn’t executed properly, one rigid body in the rag doll can end up on the far \nside of a thin wall. This causes the character to hang in mid air rather than fall-\ning properly to the ground.\n12.5. Integrating a Physics Engine into Your Game\n",
      "content_length": 2988,
      "extraction_method": "Direct"
    },
    {
      "page_number": 706,
      "chapter": null,
      "content": "684 \n12. Collision and Rigid Body Dynamics\nAnother rag doll feature that is in vogue these days is the ability for un-\nconscious characters to regain consciousness and get back up . To implement \nthis, we need a way to search for a suitable “stand up” animation. We want \nto ﬁ nd an animation whose pose on frame zero most closely matches the rag \ndoll’s pose aft er it has come to rest (which is totally unpredictable in general). \nThis can be done by matching the poses of only a few key joints, like the up-\nper thighs and the upper arms. Another approach is to manually guide the \nrag doll into a pose suitable for gett ing up by the time it comes to rest, using \npowered constraints.\nAs a ﬁ nal note, we should mention that sett ing up a rag doll’s constraints \ncan be a tricky business. We generally want the limbs to move freely but with-\nout doing anything biomechanically impossible. This is one reason special-\nized types of constraints are oft en used when constructing rag dolls. None-\ntheless, you shouldn’t assume that your rag dolls will look great without \nsome eﬀ ort. High-quality physics engines like Havok provide a rich set of \ncontent creation tools that allow an artist to set up constraints within a DCC \npackage like Maya and then test them in real time to see how they might look \nin-game.\nAll in all, gett ing rag doll physics to work in your game isn’t particularly \ndiﬃ  cult, but gett ing it to look good can take a lot of work! As with many things \nin game programming, it’s a good idea to budget plenty of time for trial and \nerror, especially when it’s your ﬁ rst time working with rag dolls.\n12.6. A Look Ahead: Advanced Physics Features\nA rigid body dynamics simulation with constraints can cover an amazing \nrange of physics-driven eﬀ ects in a game. However, such a system clearly has \nits limitations. Recent research and development is seeking to expand physics \nengines beyond constrained rigid bodies. Here are just a few examples:\nz Deformable bodies. As hardware capabilities improve and more-eﬃ  cient \nalgorithms are developed, physics engines are beginning to provide \nsupport for deformable bodies . DMM is an excellent example of such an \nengine.\nz Cloth. Cloth can be modeled as a sheet of point masses, connected by \nstiﬀ  springs. However, cloth is notoriously diﬃ  cult to get right, as many \ndiﬃ  culties arise with respect to collision between cloth and other ob-\njects, numerical stability of the simulation, etc.\nz Hair. Hair can be modeled by a large number of small physically simu-\nlated ﬁ lments, or a simpler approach can be used to make a character’s \n",
      "content_length": 2614,
      "extraction_method": "Direct"
    },
    {
      "page_number": 707,
      "chapter": null,
      "content": "685 \nhair move as if it were a rope or deformable body. This is an active area \nof research, and the quality of hair in games continues to improve.\nz Water surface simulations and buoyancy. Games have been doing water \nsurface simulations and buoyancy for some time now. This can be done \nvia a special-case system (not part of the physics engine per se), or it can \nbe modeled via forces within the physics simulation. Organic move-\nment of the water surface is oft en a rendering eﬀ ect only and does not \naﬀ ect the physics simulation at all. From the point of view of physics, \nthe water surface is oft en modeled as a plane. For large displacements \nin the water surface, the entire plane might be moved. However, some \ngame teams and researchers are pushing the limits of these simulations, \nallowing for dynamic water surfaces, waves that crest, realistic current \nsimulations, and more.\nz General ﬂ uid dynamics simulations. Right now, ﬂ uid dynamics falls into \nthe realm of specialized simulation libraries. However, this is an active \narea of research, and it may well eventually ﬁ nd its way into mainstream \nphysics engines.\n12.6. A Look Ahead: Advanced Physics Features\n",
      "content_length": 1184,
      "extraction_method": "Direct"
    },
    {
      "page_number": 708,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 709,
      "chapter": null,
      "content": "Part IV\nGameplay\n",
      "content_length": 17,
      "extraction_method": "Direct"
    },
    {
      "page_number": 710,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 711,
      "chapter": null,
      "content": "13\n \nIntroduction to \nGameplay Systems\nU\np until now, everything we’ve talked about in this book has focused on \ntechnology. We’ve learned that a game engine is a complex, layered soft -\nware system built on top of the hardware, drivers, and operating system of the \ntarget machine. We’ve seen how low-level engine systems provide services \nthat are required by the rest of the engine; how human interface devices such \nas joypads, keyboards, mice, and other devices can allow a human player to \nprovide inputs to the engine; how the rendering engine produces 3D images \non-screen; how the collision system detects and resolves interpenetrations be-\ntween shapes; how the physics simulation causes objects to move in physi-\ncally realistic ways; how the animation system allows characters and objects \nto move naturally. But despite the wide range of powerful features provided \nby these components, if we were to put them all together, we still wouldn’t \nhave a game!\nA game is deﬁ ned not by its technology but by its gameplay . Gameplay can \nbe deﬁ ned as the overall experience of playing a game. The term game mechan-\nics pins down this idea a bit more concretely—it is usually deﬁ ned as the set \nof rules that govern the interactions between the various entities in the game. \nIt also deﬁ nes the objectives of the player(s), criteria for success and failure, \nthe player character’s abilities, the number and types of non-player entities that \nexist within the game’s virtual world, and the overall ﬂ ow of the gaming expe-\nrience as a whole. In many games, these elements are intertwined with a com-\n689\n",
      "content_length": 1613,
      "extraction_method": "Direct"
    },
    {
      "page_number": 712,
      "chapter": null,
      "content": "690 \n13. Introduction to Gameplay Systems\npelling story and a rich cast of characters. However, story and characters are \ndeﬁ nitely not a necessary part of every video game, as evidenced by wildly \nsuccessful puzzle games like Tetris. In their paper, “A Survey of ‘Game’ Por-\ntability” \n(htt p://www.dcs.shef.ac.uk/intranet/research/resmes/CS0705.pdf), \nAhmed BinSubaih, Steve Maddock, and Daniela Romano of the University of \nSheﬃ  eld refer to the collection of soft ware systems used to implement game-\nplay as a game’s G-factor . In the next three chapters, we’ll explore the crucial \ntools and engine systems that deﬁ ne and manage the game mechanics (a.k.a. \ngameplay, a.k.a. G-factor) of a game.\n13.1. Anatomy of a Game World\nGameplay designs vary widely from genre to genre and from game to game. \nThat said, most 3D games, and a good number of 2D games as well, conform \nmore or less to a few basic structural patt erns. We’ll discuss these patt erns \nin the following sections, but please keep in mind that there are bound to be \ngames out there that do not ﬁ t neatly into this mold.\n13.1.1. World Elements\nMost video games take place in a two- or three-dimensional virtual game \nworld . This world is typically comprised of numerous discrete elements . Gen-\nerally, these elements fall into two categories: static elements and dynamic \nelements. Static elements include terrain, buildings, roads, bridges, and prett y \nmuch anything that doesn’t move or interact with gameplay in an active way. \nDynamic elements include characters, vehicles, weaponry, ﬂ oating power-ups \nand health packs, collectible objects, particle emitt ers, dynamic lights, invis-\nible regions used to detect important events in the game, splines that deﬁ ne \nthe paths of objects, and so on. This breakdown of the game world is illus-\ntrated in Figure 13.1.\nGameplay is generally concentrated within the dynamic elements of a \ngame. Clearly, the layout of the static background plays a crucial role in how \nthe game plays out. For example, a cover-based shooter wouldn’t be very \nmuch fun if it were played in a big, empty, rectangular room. However, the \nsoft ware systems that implement gameplay are primarily concerned with up-\ndating the locations, orientations, and internal states of the dynamic elements, \nsince they are the elements that change over time. The term game state refers to \nthe current state of all dynamic game world elements, taken as a whole.\nThe ratio of dynamic to static elements also varies from game to game. \nMost 3D games consist of a relatively small number of dynamic elements mov-\n",
      "content_length": 2603,
      "extraction_method": "Direct"
    },
    {
      "page_number": 713,
      "chapter": null,
      "content": "691 \ning about within a relatively large static background area. Other games, like \nthe arcade classic Asteroids or the Xbox 360 retro hit Geometry Wars, have no \nstatic elements to speak of (other than a black screen). The dynamic elements \nof a game are usually more expensive than the static elements in terms of CPU \nresources, so most 3D games are constrained to a limited number of dynamic \nelements. However, the higher the ratio of dynamic to static elements, the \nmore “alive” the game world can seem to the player. As gaming hardware \nbecomes more and more powerful, games are achieving higher and higher \ndynamic-to-static ratios.\nIt’s important to note that the distinction between the dynamic and static \nelements in a game world is oft en a bit blurry. For example, in the arcade game \nHydro Thunder, the waterfalls were dynamic, in the sense that their textures \nanimated, they had dynamic mist eﬀ ects at their bases, and they could be \nplaced into the game world and positioned by a game designer independently \nof the terrain and water surface. However, from an engineering standpoint, \nwaterfalls were treated as static elements because they did not interact with \nthe boats in the race in any way (other than to obscure the player’s view of hid-\nFigure 13.1.  A typical game world is comprised of both static and dynamic elements.\n13.1. Anatomy of a Game World\n",
      "content_length": 1381,
      "extraction_method": "Direct"
    },
    {
      "page_number": 714,
      "chapter": null,
      "content": "692 \n13. Introduction to Gameplay Systems\nden boost power-ups and secret passageways). Diﬀ erent game engines draw \ndiﬀ erent lines between static and dynamic elements, and some don’t draw a \ndistinction at all (i.e., everything is potentially a dynamic element).\nThe distinction between static and dynamic serves primarily as an opti-\nmization tool—we can do less work when we know that the state of an object \nisn’t going to change. For example, the vertices of a static triangle mesh can \nbe speciﬁ ed in world space, thereby saving the per-vertex matrix multiplica-\ntion normally required to transform from model space to world space during \nrendering. Lighting can be precomputed, in the form of static vertex lighting, \nlight maps, shadow maps, static ambient occlusion information, or precom-\nputed radiance transfer (PRT) spherical harmonics coeﬃ  cients. Virtually any \ncomputation that must be done at runtime for a dynamic world element is a \ngood candidate for precomputation or omission when applied to a static ele-\nment.\nGames with destructible environments are an example of how the line \nbetween the static and dynamic elements in a game world can blur. For in-\nstance, we might deﬁ ne three versions of every static element—an undam-\naged version, a damaged version, and a fully destroyed version. These back-\nground elements act like static world elements most of the time, but they can \nbe swapped dynamically during an explosion to produce the illusion of be-\ncoming damaged. In reality, static and dynamic world elements are just two \nextremes along a gamut of possible optimizations. Where we draw the line \nbetween the two categories (if we draw one at all) shift s as our optimization \nmethodologies change and adapt to the needs of the game design.\n13.1.1.1. \nStatic Geometry\n The geometry of a static world element is oft en deﬁ ned in a tool like Maya. It \nmight be one giant triangle mesh, or it might be broken up into discrete pieces. \nThe static portions of the scene are sometimes built out of instanced geometry . \nInstancing is a memory conservation technique in which a relatively small \nnumber of unique triangle meshes are rendered multiple times throughout \nthe game world, at diﬀ erent locations and orientations, in order to provide the \nillusion of variety. For example, a 3D modeler might create ﬁ ve diﬀ erent kinds \nof short wall sections and then piece them together in random combinations \nin order to construct miles of unique-looking walls.\nStatic visual elements and collision data might also be constructed from \nbrush geometry . This kind of geometry originated with the Quake family of \nengines. A brush describes a shape as a collection of convex volumes, each \nbounded by a set of planes. Brush geometry is fast and easy to create and \nintegrates well into a BSP-tree -based rendering engine. Brushes can be real-\n",
      "content_length": 2869,
      "extraction_method": "Direct"
    },
    {
      "page_number": 715,
      "chapter": null,
      "content": "693 \nly useful for rapidly blocking out the contents of a game world. This allows \ngameplay to be tested early, when it is cheap to do so. If the layout proves its \nworth, the art team can either texture map and ﬁ ne-tune the brush geometry \nor replace it with more-detailed custom mesh assets. On the other hand, if \nthe level requires redesign, the brush geometry can be easily revised without \ncreating a lot of rework for the art team.\n13.1.2. World Chunks\nWhen a game takes place in a very large virtual world, it is typically divided \ninto discrete playable regions, which we’ll call world chunks . Chunks are also \nknown as levels, maps, stages, or areas. The player can usually see only one, or at \nmost a handful, of chunks at any given moment while playing the game, and \nhe or she progresses from chunk to chunk as the game unfolds.\nOriginally, the concept of “levels” was invented as a mechanism to pro-\nvide greater variety of gameplay within the memory limitations of early gam-\nFigure 13.2.  Many game worlds are divided into chunks for various reasons, including memory \nlimitations, the need to control the ﬂ ow of the game through the world, and as a division-of-\nlabor mechanism during development.\nChunk 2\nChunk 1\n13.1. Anatomy of a Game World\n",
      "content_length": 1264,
      "extraction_method": "Direct"
    },
    {
      "page_number": 716,
      "chapter": null,
      "content": "694 \n13. Introduction to Gameplay Systems\ning hardware. Only one level could exist in memory at a time, but the player \ncould progress from level to level for a much richer overall experience. Since \nthen, game designs have branched out in many directions, and linear level-\nbased games are much less common today. Some games are essentially still \nlinear, but the delineations between world chunks are usually not as obvious \nto the player as they once were. Other games use a star topology, in which the \nplayer starts in a central hub area and can access other areas at random from \nthe hub (perhaps only aft er they have been unlocked). Others use a graph-like \ntopology, where areas are connected to one another in arbitrary ways. Still \nothers provide the illusion of a vast, open world .\nDespite the richness of modern game designs, all but the smallest of game \nworlds are still divided into chunks of some kind. This is done for a number of \nreasons. First of all, memory limitations are still an important constraint (and \nwill be until game machines with inﬁ nite memory hit the market!). World \nchunks are also a convenient mechanism for controlling the overall ﬂ ow of the \ngame. Chunks can serve as a division-of-labor mechanism as well; each chunk \ncan be constructed and managed by a relatively small group of designers and \nartists. World chunks are illustrated in Figure 13.2.\n13.1.3. High-Level Game Flow\nA game’s high-level ﬂ ow deﬁ nes a sequence, tree, or graph of player objectives . \nObjectives are sometimes called tasks , stages, levels (a term that can also apply \nto world chunks), or waves (if the game is primarily about defeating hordes of \natt acking enemies). The high-level ﬂ ow also provides the deﬁ nition of success \nfor each objective (e.g., clear all the enemies and get the key) and the penalty \nfor failure (e.g., go back to the start of the current area, possibly losing a “life” \nin the process). In a story-driven game, this ﬂ ow might also include various \nin-game movies that serve to advance the player’s understanding of the story \nas it unfolds. These sequences are sometimes called cut-scenes, in-game cin-\nematics (IGC), or noninteractive sequences (NIS). When they are rendered oﬀ -\nline and played back as a full-screen movie, such sequences are usually called \nfull-motion videos (FMV).\nEarly games mapped the objectives of the player one-to-one to particular \nworld chunks (hence the dual meaning of the term “level”). For example, in \nDonkey Kong, each new level presents Mario with a new objective (namely to \nreach the top of the structure and progress to the next level). However, this \none-to-one mapping between world chunks and objectives is less popular in \nmodern game design. Each objective is associated with one or more world \nchunks, but the coupling between chunks and objectives remains deliberately \nloose. This kind of design oﬀ ers the ﬂ exibility to alter game objectives and \n",
      "content_length": 2951,
      "extraction_method": "Direct"
    },
    {
      "page_number": 717,
      "chapter": null,
      "content": "695 \nworld subdivision independently, which is extremely helpful from a logistic \nand practical standpoint when developing a game. Many games group their \nobjectives into coarser sections of gameplay, oft en called chapters or acts. A \ntypical gameplay architecture is shown in Figure 13.3.\n13.2. Implementing Dynamic Elements: \n \nGame Objects\nThe dynamic elements of a game are usually designed in an object-oriented \nfashion. This approach is intuitive and natural and maps well to the game de-\nsigner’s notion of how the world is constructed. He or she can visualize char-\nacters, vehicles, ﬂ oating health packs, exploding barrels, and myriad other \ndynamic objects moving about in the game. So it is only natural to want to \nbe able to create and manipulate these elements in the game world editor . \nLikewise, programmers usually ﬁ nd it natural to implement dynamic ele-\nments as largely autonomous agents at runtime. In this book, we’ll use the \nterm game object (GO) to refer to virtually any dynamic element within a game \nworld. However, this terminology is by no means standard within the indus-\ntry. Game objects are commonly referred to as entities, actors, or agents, and the \nlist of terms goes on.\nChapter 1\nChunk 1\nChunk 2\nChunk 3\nObjective 1 B\nObjective 1 A\nObjective 1 C\nOptional\nObjective 1D\nObjective 1 E\nObjective 1 G\nOptoinal\nObjective 1 F\nChapter 2\nChunk 4\nChunk 5\nChunk 6\nChunk 7\nObjective 2 B\nObjective 2 A\nObjective 2 C\nObjective 2 D\nObjective 2 G\nOptoinal\nObjective 2 H\nOptional\nObjective 2 F\nOptional\nObjective 2E\nObjective 2 I\nFigure 13.3.  Gameplay objectives are typically arranged in a sequence, tree, or graph, and \neach one maps to one or more game world chunks.\n13.2. Implementing Dynamic Elements: Game Objects\n",
      "content_length": 1749,
      "extraction_method": "Direct"
    },
    {
      "page_number": 718,
      "chapter": null,
      "content": "696 \n13. Introduction to Gameplay Systems\nAs is customary in object-oriented design, a game object is essentially a \ncollection of att ributes (the current state of the object) and behaviors (how the \nstate changes over time and in response to events). Game objects are usually \nclassiﬁ ed by type . Diﬀ erent types of objects have diﬀ erent att ribute schemas \nand diﬀ erent behaviors. All instances of a particular type share the same at-\ntribute schema and the same set of behaviors, but the values of the att ributes \ndiﬀ er from instance to instance. (Note that if a game object’s behavior is data-\ndriven, say through script code or via a set of data-driven rules governing the \nobject’s responses to events, then behavior too can vary on an instance-by-\ninstance basis.)\nThe distinction between a type and an instance of a type is a crucial one. For \nexample, the game of Pac-Man involves four game object types: ghosts, pellets, \npower pills, and Pac-Man. However, at any moment in time, there may be up \nto four instances of the type “ghost,” 50–100 instances of the type “pellet,” \nfour “power pill” instances, and one instance of the “Pac-Man” type.\nMost object-oriented systems provide some mechanism for the inheritance \nof att ributes, behavior, or both. Inheritance encourages code and design reuse. \nThe speciﬁ cs of how inheritance works varies widely from game to game, but \nmost game engines support it in some form.\n13.2.1. Game Object Models\nIn computer science, the term object model has two related but distinct mean-\nings. It can refer to the set of features provided by a particular programming \nlanguage or formal design language. For example, we might speak of the C++ \nobject model or the OMT object model . It can also refer to a speciﬁ c object-ori-\nented programming interface (i.e., a collection of classes, methods, and inter-\nrelationships designed to solve a particular problem). One example of this \nlatt er usage is the Microsoft  Excel object model , which allows external programs \nto control Excel in various ways. (See htt p://en.wikipedia.org/wiki/Object_\nmodel for further discussion of the term object model.)\nIn this book, we will use the term game object model to describe the facili-\nties provided by a game engine in order to permit the dynamic entities in the \nvirtual game world to be modeled and simulated. In this sense, the term game \nobject model has aspects of both of the deﬁ nitions given above:\nz A game’s object model is a speciﬁ c object-oriented programming inter-\nface intended to solve the particular problem of simulating the speciﬁ c \nset of entities that make up a particular game.\nz Additionally, a game’s object model oft en extends the programming \nlanguage in which the engine was writt en. If the game is implemented \n",
      "content_length": 2789,
      "extraction_method": "Direct"
    },
    {
      "page_number": 719,
      "chapter": null,
      "content": "697 \nin a non-object-oriented language like C, object-oriented facilities can \nbe added by the programmers. And even if the game is writt en in an \nobject-oriented language like C++, advanced features like reﬂ ection, per-\nsistence, and network replication are oft en added. A game object model \nsometimes melds the features of multiple languages. For example, a \ngame engine might combine a compiled programming language such as \nC or C++ with a scripting language like Python, Lua, or Pawn and pro-\nvide a uniﬁ ed object model that can be accessed from either language.\n13.2.2. Tool-Side Design versus Runtime Design\nThe object model presented to the designers via the world editor (discussed \nbelow) needn’t be the same object model used to implement the game at run-\ntime.\nz The tool-side game object model might be implemented at runtime us-\ning a language with no native object-oriented features at all, like C.\nz A single GO type on the tool side might be implemented as a collection \nof classes at runtime (rather than as a single class as one might at ﬁ rst \nexpect).\nz Each tool-side GO might be nothing more than a unique id at runtime, \nwith all of its state data stored in tables or collections of loosely coupled \nobjects.\nTherefore, a game really has two distinct but closely interrelated object models:\nz The tool-side object model is deﬁ ned by the set of game object types seen by \nthe designers within the world editor .\nz The runtime object model is deﬁ ned by whatever set of language con-\nstructs and soft ware systems the programmers have used to implement \nthe tool-side object model at runtime. The runtime object model might \nbe identical to the tool-side model or map directly to it, or it might be \nentirely diﬀ erent than the tool-side model under the hood.\nIn some game engines, the line between the tool-side and runtime designs \nis blurred or non-existent. In others, it is very well delineated. In some engines, \nthe implementation is actually shared between the tools and the runtime. In \nothers, the runtime implementation looks almost totally alien relative to the \ntool-side view of things. Some aspects of the implementation almost always \ncreep up into the tool-side design, and game designers must be cognizant of \nthe performance and memory consumption impacts of the game worlds they \nconstruct and the gameplay rules and object behaviors they design. That said, \n13.2. Implementing Dynamic Elements: Game Objects\n",
      "content_length": 2456,
      "extraction_method": "Direct"
    },
    {
      "page_number": 720,
      "chapter": null,
      "content": "698 \n13. Introduction to Gameplay Systems\nvirtually all game engines have some form of tool-side object model and a cor-\nresponding runtime implementation of that object model.\n13.3. Data-Driven Game Engines\nIn the early days of game development, games were largely hard-coded by \nprogrammers. Tools, if any, were primitive. This worked because the amount \nof content in a typical game was miniscule, and the bar wasn’t particularly \nhigh, thanks in part to the primitive graphics and sound of which early game \nhardware was capable.\nToday, games are orders of magnitude more complex, and the quality bar \nis so high that game content is oft en compared to the computer-generated ef-\nfects in Hollywood blockbusters. Game teams have grown much larger, but \nthe amount of game content is growing faster than team size. In the most \nrecent generation, deﬁ ned by the Wii, the Xbox 360, and the PLAYSTATION 3, \ngame teams routinely speak of the need to produce ten times the content, with \nteams that are at most 25% larger than in the previous generation. This trend \nmeans that a game team must be capable of producing very large amounts of \ncontent in an extremely eﬃ  cient manner.\nEngineering resources are oft en a production bott leneck because high-\nquality engineering talent is limited and expensive and because engineers \ntend to produce content much more slowly than artists and game designers \n(due to the complexities inherent in computer programming). Most teams \nnow believe that it’s a good idea to put at least some of the power to cre-\nate content directly into the hands of the folks responsible for producing that \ncontent—namely the designers and the artists. When the behavior of a game \ncan be controlled, in whole or in part, by data provided by artists and design-\ners rather than exclusively by soft ware produced by programmers, we say the \nengine is data-driven .\nData-driven architectures can improve team eﬃ  ciency by fully leveraging \nall staﬀ  members to their fullest potential and by taking some of the heat oﬀ  \nthe engineering team. It can also lead to improved iteration times. Whether a \ndeveloper wants to make a slight tweak to the game’s content or completely \nrevise an entire level, a data-driven design allows the developer to see the \neﬀ ects of the changes quickly, ideally with litt le or no help from an engineer. \nThis saves valuable time and can permit the team to polish their game to a \nvery high level of quality.\nThat being said, it’s important to realize that data-driven features oft en \ncome at a heavy cost. Tools must be provided to allow game designers and art-\nists to deﬁ ne game content in a data-driven manner. The runtime code must \n",
      "content_length": 2697,
      "extraction_method": "Direct"
    },
    {
      "page_number": 721,
      "chapter": null,
      "content": "699 \nbe changed to handle the wide range of possible inputs in a robust way. Tools \nmust also be provided in-game to allow artists and designers to preview their \nwork and troubleshoot problems. All of this soft ware requires signiﬁ cant time \nand eﬀ ort to write, test, and maintain.\nSadly, many teams make a mad rush into data-driven architectures with-\nout stopping to study the impacts of their eﬀ orts on their particular game de-\nsign and the speciﬁ c needs of their team members. In their haste, such teams \noft en dramatically overshoot the mark, producing overly complex tools and \nengine systems that are diﬃ  cult to use, bug-ridden, and virtually impossible \nto adapt to the changing requirements of the project. Ironically, in their eﬀ orts \nto realize the beneﬁ ts of a data-driven design, a team can easily end up with \nsigniﬁ cantly lower productivity than the old-fashioned hard-coded methods.\nEvery game engine should have some data-driven components, but a \ngame team must exercise extreme care when selecting which aspects of the \nengine to data-drive. It’s crucial to weigh the costs of creating a data-driven \nor rapid iteration feature against the amount of time the feature is expected to \nsave the team over the course of the project. It’s also incredibly important to \nkeep the KISS mantra (“keep it simple, stupid”) in mind when designing and \nimplementing data-driven tools and engine systems. To paraphrase Albert \nEinstein, everything in a game engine should be made as simple as possible, \nbut no simpler.\n13.4. The Game World Editor\nWe’ve already discussed data-driven asset-creation tools, such as Maya, Pho-\ntoshop, Havok content tools, and so on. These tools generate individual assets \nfor consumption by the rendering engine, animation system, audio system, \nphysics system, and so on. The analog to these tools in the gameplay space \nis the game world editor —a tool (or a suite of tools) that permits game world \nchunks to be deﬁ ned and populated with static and dynamic elements.\nAll commercial game engines have some kind of world editor tool. A well-\nknown tool called Radiant is used to create maps for the Quake and Doom fam-\nily of engines. A screen shot of Radiant is shown in Figure 13.4. Valve’s Source \nengine, the engine that drives Half-Life 2, The Orange Box  and Team Fortress 2, \nprovides an editor called Hammer (previously distributed under the names \nWorldcraft  and The Forge). Figure 13.5 shows a screen shot of Hammer.\nThe game world editor generally permits the initial states of game objects \n(i.e., the values of their att ributes) to be speciﬁ ed. Most game world editors \nalso give their users some sort of ability to control the behaviors of the dynamic \nobjects in the game world. This control might be via data-driven conﬁ guration \n13.4. The Game World Editor\n",
      "content_length": 2830,
      "extraction_method": "Direct"
    },
    {
      "page_number": 722,
      "chapter": null,
      "content": "700 \n13. Introduction to Gameplay Systems\nFigure 13.5.  Valve’s Hammer editor for the Source engine.\nFigure 13.4.  The Radiant world editor for the Quake and Doom family of engines.\n",
      "content_length": 182,
      "extraction_method": "Direct"
    },
    {
      "page_number": 723,
      "chapter": null,
      "content": "701 \nparameters (e.g., object A should start in an invisible state, object B should \nimmediately att ack the player when spawned, object C is ﬂ ammable, etc.), \nor behavioral control might be via a scripting language, thereby shift ing the \ngame designers’ tasks into the realm of programming. Some world editors \neven allow entirely new types of game objects to be deﬁ ned, with litt le or no \nprogrammer intervention.\n13.4.1. Typical Features of a Game World Editor\nThe design and layout of game world editors varies widely, but most editors \nprovide a reasonably standard set of features. These include, but are certainly \nnot limited to, the following.\n13.4.1.1. World Chunk Creation and Management\nThe unit of world creation is usually a chunk (also known as a level or map—\nsee Section 13.1.2). The game world editor typically allows new chunks to \nbe created and existing chunks to be renamed, broken up, combined, or de-\nstroyed. Each chunk can be linked to one or more static meshes and/or other \nstatic data elements such as AI navigation maps, descriptions of ledges that \ncan be grabbed by the player, cover point deﬁ nitions, and so on. In some en-\ngines, a chunk is deﬁ ned by a single background mesh and cannot exist with-\nout one. In other engines, a chunk may have an independent existence, per-\nhaps deﬁ ned by a bounding volume (e.g., AABB, OBB, or arbitrary polygonal \nregion), and can be populated by zero or more meshes and/or brush geometry \n(see Section 1.7.3.1).\nSome world editors provide dedicated tools for authoring terrain, water , \nand other specialized static elements. In other engines, these elements might \nbe authored using standard DCC applications but tagged in some way to indi-\ncate to the asset conditioning pipeline and/or the runtime engine that they are \nspecial. (For example, in Uncharted: Drake’s Fortune, the water was authored \nas a regular triangle mesh, but it was mapped with a special material that in-\ndicated that it was to be treated as water.) Sometimes, special world elements \nare created and edited in a separate, standalone tool. For example, the height \nﬁ eld terrain in Medal of Honor: Paciﬁ c Assault was authored using a customized \nversion of a tool obtained from another team within Electronic Arts because \nthis was more expedient than trying to integrate a terrain editor into Radiant, \nthe world editor being used on the project at the time.\n13.4.1.2. Game World Visualization\nIt’s important for the user of a game world editor to be able to visualize the \ncontents of the game world . As such, virtually all game world editors provide \n13.4. The Game World Editor\n",
      "content_length": 2636,
      "extraction_method": "Direct"
    },
    {
      "page_number": 724,
      "chapter": null,
      "content": "702 \n13. Introduction to Gameplay Systems\na three-dimensional perspective view of the world and/or a two-dimensional \northographic projection. It’s common to see the view pane divided into four \nsections, three for top, side, and front orthographic elevations and one for the \n3D perspective view.\nSome editors provide these world views via a custom rendering engine \nintegrated directly into the tool. Other editors are themselves integrated into \na 3D geometry editor like Maya or 3ds Max, so they can simply leverage the \ntool’s viewports. Still other editors are designed to communicate with the ac-\ntual game engine and use it to render the 3D perspective view. Some editors \nare even integrated into the engine itself.\n13.4.1.3. Navigation\nClearly, a world editor wouldn’t be of much use if the user weren’t able to \nmove around within the game world. In an orthographic view, it’s important \nto be able to scroll and zoom in and out. In a 3D view, various camera control \nschemes are used. It may be possible to focus on an individual object and \nrotate around it. It may also be possible to switch into a “ﬂ y through” mode \nwhere the camera rotates about its own focal point and can be moved forward, \nbackward, up, and down and panned left  and right.\nSome editors provide a host of convenience features for navigation. These \ninclude the ability to select an object and focus in on it with a single key press, \nthe ability to save various relevant camera locations and then jump between \nthem, various camera movement speed modes for coarse navigation and ﬁ ne \ncamera control, a Web-browser-like navigation history that can be used to \njump around the game world, and so on.\n13.4.1.4. Selection\nA game world editor is primarily designed to allow the user to populate a \ngame world with static and dynamic elements. As such, it’s important for the \nuser to be able to select individual elements for editing. Some editors only \nallow a single object to be selected at a time, while more-advanced editors \npermit multiobject selections. Objects might be selected via a rubber-band box \nin the orthographic view or by ray-cast style picking in the 3D view. Many \neditors also display a list of all world elements in a scrolling list or tree view so \nthat objects can be found and selected by name. Some world editors also allow \nselections to be named and saved for later retrieval.\nGame worlds are oft en quite densely populated. As such, it can some-\ntimes be diﬃ  cult to select a desired object because other objects are in the way. \nThis problem can be overcome in a number of ways. When using a ray cast \nto select objects in 3D, the editor might allow the user to cycle through all of \n",
      "content_length": 2701,
      "extraction_method": "Direct"
    },
    {
      "page_number": 725,
      "chapter": null,
      "content": "703 \nthe objects that the ray is currently intersecting rather than always selecting \nthe nearest one. Many editors allow the currently selected object(s) to be tem-\nporarily hidden from view. That way, if you don’t get the object you want the \nﬁ rst time, you can always hide it and try again. As we’ll see in the next section, \nlayers can also be an eﬀ ective way to reduce clutt er and improve the user’s \nability to select objects successfully.\n13.4.1.5. Layers\nSome editors also allow objects to be grouped into predeﬁ ned or user-deﬁ ned \nlayers. This can be an incredibly useful feature, allowing the contents of the \ngame world to be organized sensibly. Entire layers can be hidden or shown to \nreduce clutt er on-screen. Layers might be color-coded for easy identiﬁ cation. \nLayers can be an important part of a division-of-labor strategy, as well. For \nexample, when the lighting team is working on a world chunk, they can hide \nall of the elements in the scene that are not relevant to lighting.\nWhat’s more, if the game world editor is capable of loading and saving \nlayers individually, conﬂ icts can be avoided when multiple people are work-\ning on a single world chunk at the same time. For example, all of the lights \nmight be stored in one layer, all of the background geometry in another, and \nall AI characters in a third. Since each layer is totally independent, the light-\ning, background, and NPC teams can all work simultaneously on the same \nworld chunk.\n13.4.1.6. Property Grid\nThe static and dynamic elements that populate a game world chunk typically \nhave various properties (also known as att ributes) that can be edited by the \nuser. Properties might be simple key-value pairs and be limited to simple \natomic data types like Booleans, integers, ﬂ oating-point numbers, and strings. \nIn some editors, more-complex properties are supported, including arrays of \ndata and nested compound data structures.\nMost world editors display the att ributes of the currently selected object(s) \nin a scrollable property grid view. An example of a property grid is shown in \nFigure 13.6. The grid allows the user to see the current values of each att ribute \nand edit the values by typing, using check boxes or drop-down combo boxes, \ndragging spinner controls up and down, and so on.\nEditing Multiobject Selections\nIn editors that support multiobject selection , the property grid may support \nmultiobject editing as well. This advanced feature displays an amalgam of the \natt ributes of all objects in the selection. If a particular att ribute has the same \nvalue across all objects in the selection, the value is shown as-is, and editing \n13.4. The Game World Editor\n",
      "content_length": 2687,
      "extraction_method": "Direct"
    },
    {
      "page_number": 726,
      "chapter": null,
      "content": "704 \n13. Introduction to Gameplay Systems\nthe value in the grid causes the property value to be updated in all selected \nobjects. If the att ribute’s value diﬀ ers from object to object within the selection, \nthe property grid typically shows no value at all. In this case, if a new value is \ntyped into the ﬁ eld in the grid, it will overwrite the values in all selected ob-\njects, bringing them all into agreement. Another problem arises when the se-\nlection contains a heterogeneous collection of objects (i.e., objects whose types \ndiﬀ er). Each type of object can potentially have a diﬀ erent set of att ributes, so \nthe property grid must only display those att ributes that are common to all \nobject types in the selection. This can still be useful, however, because game \nobject types oft en inherit from a common base type. For example, most objects \nhave a position and orientation. In a heterogeneous selection, the user can still \nedit these shared att ributes even though more-speciﬁ c att ributes are tempo-\nrarily hidden from view.\nFree-Form Properties\nNormally, the set of properties associated with an object, and the data types of \nthose properties, are deﬁ ned on a per-object-type basis. For example, a render-\nFigure 13.6.  A typical property grid.\n",
      "content_length": 1270,
      "extraction_method": "Direct"
    },
    {
      "page_number": 727,
      "chapter": null,
      "content": "705 \nable object has a position, orientation, scale, and mesh, while a light has posi-\ntion, orientation, color, intensity, and light type. Some editors also allow addi-\ntional “free-form” properties to be deﬁ ned by the user on a per-instance basis. \nThese properties are usually implemented as a ﬂ at list of key-value pairs . The \nuser is free to choose the name (key) of each free-form property, along with \nits data type and its value. This can be incredibly useful for prototyping new \ngameplay features or implementing one-oﬀ  scenarios.\n13.4.1.7. Object Placement and Alignment Aids\n Some object properties are treated in a special way by the world editor. Typi-\ncally the position, orientation, and scale of an object can be controlled via spe-\ncial handles in the orthographic and perspective viewports, just like in Maya \nor Max. In addition, asset linkages oft en need to be handled in a special way. \nFor example, if we change the mesh associated with an object in the world, the \neditor should display this mesh in the orthographic and 3D perspective view-\nports. As such, the game world editor must have special knowledge of these \nproperties—it cannot treat them generically, as it can most object properties.\nMany world editors provide a host of object placement and alignment \naids in addition to the basic translation, rotation, and scale tools. Many of \nthese features borrow heavily from the feature sets of commercial graphics \nand 3D modeling tools like Photoshop, Maya, Visio, and others. Examples \ninclude snap to grid, snap to terrain, align to object, and many more.\n13.4.1.8. Special Object Types\nJust as some object properties must be handled in a special way by the world ed-\nitor, certain types of objects also require special handling. Examples include:\nz Lights. The world editor usually uses special icons to represent lights, \nsince they have no mesh. The editor may att empt to display the light’s \napproximate eﬀ ect on the geometry in the scene as well, so that design-\ners can move lights around in real time and get a reasonably good feel \nfor how the scene will ultimately look.\nz Particle emitt ers. Visualization of particle eﬀ ects can also be problematic \nin editors that are built on a standalone rendering engine. In this case, \nparticle emitt ers might be displayed using icons only, or some att empt \nmight be made to emulate the particle eﬀ ect in the editor. Of course, this \nis not a problem if the editor is in-game or can communicate with the \nrunning game for live tweaking.\nz Regions . A region is a volume of space that is used by the game to de-\ntect relevant events such as objects entering or leaving the volume or to \n13.4. The Game World Editor\n",
      "content_length": 2707,
      "extraction_method": "Direct"
    },
    {
      "page_number": 728,
      "chapter": null,
      "content": "706 \n13. Introduction to Gameplay Systems\ndemark areas for various purposes. Some game engines restrict regions \nto being modeled as spheres or oriented boxes, while others may per-\nmit arbitrary convex polygonal shapes when viewed from above, with \nstrictly horizontal sides. Still others might allow regions to be construct-\ned out of more-complex geometry, such as k-DOPs (see Section 12.3.4.5). \nIf regions are always spherical then the designers might be able to make \ndo with a “Radius” property in the property grid, but to deﬁ ne or mod-\nify the extents of an arbitrarily shaped region, a special-case editing tool \nis almost certainly required.\nz Splines. A spline is a three-dimensional curve deﬁ ned by a set of control \npoints and possibly tangent vectors at the points, depending on the type \nof mathematical curve used. Catmull-Rom splines are commonly used \nbecause they are fully deﬁ ned by a set of control points (without tan-\ngents) and the curve always passes through all of the control points. But \nno matt er what type of splines are supported, the world editor typically \nneeds to provide the ability to display the splines in its viewports, and \nthe user must be able to select and manipulate individual control points. \nSome world editors actually support two selection modes—a “coarse” \nmode for selecting objects in the scene and a “ﬁ ne” mode for select-\ning the individual components of a selected object, such as the control \npoints of a spline or the vertices of a region.\n13.4.1.9. Saving and Loading World Chunks\nOf course, no world editor would be complete if it were unable to load and \nsave world chunks . The granularity with which world chunks can be load-\ned and saved diﬀ ers widely from engine to engine. Some engines store each \nworld chunk in a single ﬁ le, while others allow individual layers to be loaded \nand saved independently. Data formats also vary across engines. Some use \ncustom binary formats, others text formats like XML. Each design has its pros \nand cons, but every editor provides the ability to load and save world chunks \nin some form—and every game engine is capable of loading world chunks so \nthat they can be played at runtime.\n13.4.1.10. Rapid Iteration\nA good game world editor usually supports some degree of dynamic tweak-\ning for rapid iteration. Some editors run within the game itself, allowing the \nuser to see the eﬀ ects of his or her changes immediately. Others provide a \nlive connection from the editor to the running game. Still other world editors \noperate entirely oﬀ -line, either as a standalone tool or as a plug-in to a DCC \napplication like Lightwave or Maya. These tools sometimes permit modiﬁ ed \n",
      "content_length": 2686,
      "extraction_method": "Direct"
    },
    {
      "page_number": 729,
      "chapter": null,
      "content": "707 \ndata to be reloaded dynamically into the running game. The speciﬁ c mecha-\nnism isn’t important—all that matt ers is that users have a reasonably short \nround-trip iteration time (i.e., the time between making a change to the game \nworld and seeing the eﬀ ects of that change in-game). It’s important to realize \nthat iterations don’t have to be instantaneous. Iteration times should be com-\nmensurate with the scope and frequency of the changes being made. For ex-\nample, we might expect tweaking a character’s maximum health to be a very \nfast operation, but when making major changes to the lighting environment \nfor an entire world chunk, a much longer iteration time might be acceptable.\n13.4.2. Integrated Asset Management Tools\nIn some engines, the game world editor is integrated with other aspects of \ngame asset database management, such as deﬁ ning mesh and material prop-\nerties, deﬁ ning animations, blend trees, animation state machines, sett ing up \ncollision and physical properties of objects, managing texture resources, and \nso on. (See Section 6.2.1.2 for a discussion of the game asset database.)\nPerhaps the best-known example of this design in action is UnrealEd , the \neditor used to create content for games built on the Unreal Engine. UnrealEd \nis integrated directly into the game engine, so any changes made in the editor \nare made directly to the dynamic elements in the running game. This makes \nrapid iteration very easy to achieve. But UnrealEd is much more than a game \nworld editor—it is actually a complete content-creation package. It manages \nFigure 13.7.  UnrealEd’s Generic Browser provides access to the entire game asset database.\n13.4. The Game World Editor\n",
      "content_length": 1705,
      "extraction_method": "Direct"
    },
    {
      "page_number": 730,
      "chapter": null,
      "content": "708 \n13. Introduction to Gameplay Systems\nthe entire database of game assets, from animations to audio clips to triangle \nmeshes to textures to materials and shaders and much more. UnrealEd pro-\nvides its user with a uniﬁ ed, real-time, WYSIWYG view into the entire asset \ndatabase, making it a powerful enabler of any rapid, eﬃ  cient game develop-\nment process. A few screen shots from UnrealEd are shown in Figure 13.7 and \nFigure 13.8.\n13.4.2.1. Data Processing Costs\nIn Section 6.2.1, we learned that the asset conditioning pipeline (ACP) con-\nverts game assets from their various source formats into the formats required \nby the game engine. This is typically a two-step process. First, the asset is \nexported from the DCC application to a platform-independent intermediate \nformat that only contains the data that is relevant to the game. Second, the \nasset is processed into a format that is optimized for a speciﬁ c platform. On a \nproject targeting multiple gaming platforms, a single platform-independent \nasset gives rise to multiple platform-speciﬁ c assets during this second phase.\nOne of the key diﬀ erences between tools pipelines is the point at which \nthis second platform-speciﬁ c optimization step is performed. UnrealEd per-\nFigure 13.8.  UnrealEd also provides a world editor.\n",
      "content_length": 1300,
      "extraction_method": "Direct"
    },
    {
      "page_number": 731,
      "chapter": null,
      "content": "709 \nforms it when assets are ﬁ rst imported into the editor. This approach pays oﬀ  \nin rapid iteration time when iterating on level design. However, it can make \nthe cost of changing source assets like meshes, animations, audio assets, and \nso on more painful. Other engines like the Source engine and the Quake engine \npay the asset optimization cost when baking out the level prior to running the \ngame. Halo gives the user the option to change raw assets at any time; they are \nconverted into optimized form when they are ﬁ rst loaded into the engine, and \nthe results are cached to prevent the optimization step from being performed \nneedlessly every time the game is run.\n13.4. The Game World Editor\n",
      "content_length": 707,
      "extraction_method": "Direct"
    },
    {
      "page_number": 732,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 733,
      "chapter": null,
      "content": "711\n14\nRuntime Gameplay\nFoundation Systems\n14.1. Components of the Gameplay \n \nFoundation System\nM\nost game engines provide a suite of runtime soft ware components that \ntogether provide a framework upon which a game’s unique rules, objec-\ntives, and dynamic world elements can be constructed. There is no standard \nname for these components within the game industry, but we will refer to them \ncollectively as the engine’s gameplay foundation system . If a line can reasonably \nbe drawn between the game engine and the game itself, then these systems \nlie just beneath this line. In theory, one can construct gameplay foundation \nsystems that are for the most part game-agnostic. However, in practice, these \nsystems almost always contain genre- or game-speciﬁ c details. In fact, the line \nbetween the engine and the game can probably be best visualized as one big \nblur—a gradient that arcs across these components as it links the engine to \nthe game. In some game engines, one might even go so far as to consider the \ngameplay foundation systems as lying entirely above the engine-game line. \nThe diﬀ erences between game engines are most acute when it comes to the \ndesign and implementation of their gameplay components. That said, there \nare a surprising number of common patt erns across engines, and those com-\nmonalities will be the topic of our discussions here.\n",
      "content_length": 1374,
      "extraction_method": "Direct"
    },
    {
      "page_number": 734,
      "chapter": null,
      "content": "712 \n14. Runtime Gameplay Foundation Systems\nEvery game engine approaches the problem of gameplay soft ware design \na bit diﬀ erently. However, most engines provide the following major subsys-\ntems in some form:\nz Runtime game object model . This is an implementation of the abstract game \nobject model advertised to the game designers via the world editor .\nz Level management and streaming . This system loads and unloads the \ncontents of the virtual worlds in which gameplay takes place. In many \nengines, level data is streamed into memory during gameplay, thus \nproviding the illusion of a large seamless world (when in fact it is broken \ninto discrete chunks).\nz Real-time object model updating . In order to permit the game objects \nin the world to behave autonomously, each object must be updated \nperiodically. This is where all of the disparate systems in a game engine \ntruly come together into a cohesive whole.\nz Messaging and event handling. Most game objects need to communicate \nwith one another. This is usually done via an abstract messaging system. \nInter-object messages oft en signal changes in the state of the game world \ncalled events. So the messaging system is referred to as the event system \nin many studios.\nz Scripting . Programming high-level game logic in a language like C or \nC++ can be cumbersome. To improve productivity, allow rapid iteration, \nand put more power into the hands of the non-programmers on the \nteam, a scripting language is oft en integrated into the game engine. \nThis language might be text-based, like Python or Lua, or it might be a \ngraphical language, like Unreal’s Kismet.\nz Objectives and game ﬂ ow management. This subsystem manages the play-\ner’s objectives and the overall ﬂ ow of the game. This is usually described \nby a sequence, tree, or graph of player objectives. Objectives are oft en \ngrouped into chapters, especially if the game is highly story-driven as \nmany modern games are. The game ﬂ ow management system manages \nthe overall ﬂ ow of the game, tracks the player’s accomplishment of ob-\njectives, and gates the player from one area of the game world to the \nnext as the objectives are accomplished. Some designers refer to this as \nthe “spine” of the game.\nOf these major systems, the runtime object model is probably the most \ncomplex. It typically provides most, if not all, of the following features:\nz Spawning and destroying game objects dynamically. The dynamic elements \nin a game world oft en need to come and go during gameplay. Health \n",
      "content_length": 2525,
      "extraction_method": "Direct"
    },
    {
      "page_number": 735,
      "chapter": null,
      "content": "713 \n14.1. Components of the Gameplay Foundation System\npacks disappear once they have been picked up, explosions appear \nand then dissipate, and enemy reinforcements mysteriously come from \naround a corner just when you think you’ve cleared the level. Many \ngame engines provide a system for managing the memory and other re-\nsources associated with dynamically spawned game objects. Other en-\ngines simply disallow dynamic creation or destruction of game objects \naltogether.\nz Linkage to low-level engine systems . Every game object has some kind of \nlinkage to one or more underlying engine systems. Most game objects are \nvisually represented by renderable triangle meshes. Some have particle \neﬀ ects. Many generate sounds. Some animate. Many have collision, \nand some are dynamically simulated by the physics engine. One of the \nprimary responsibilities of the gameplay foundation system is to ensure \nthat every game object has access to the services of the engine systems \nupon which it depends.\nz Real-time simulation of object behaviors . At its core, a game engine is a real-\ntime dynamic computer simulation of an agent-based model. This is just \na fancy way of saying that the game engine needs to update the states \nof all the game objects dynamically over time. The objects may need to \nbe updated in a very particular order, dictated in part by dependencies \nbetween the objects, in part by their dependencies on various engine \nsubsystems, and in part because of the interdependencies between those \nengine subsystems themselves.\nz Ability to deﬁ ne new game object types . Every game’s requirements change \nand evolve as the game is developed. It’s important that the game object \nmodel be ﬂ exible enough to permit new object types to be added easily \nand exposed to the world editor. In an ideal world, it should be possible \nto deﬁ ne a new type of object in an entirely data-driven manner. \nHowever, in many engines, the services of a programmer are required \nin order to add new game object types.\nz Unique object ids . Typical game worlds contain hundreds or even \nthousands of individual game objects of various types. At runtime, it’s \nimportant to be able to identify or search for a particular object. This \nmeans each object needs some kind of unique identiﬁ er. A human-\nreadable name is the most convenient kind of id, but we must be wary \nof the performance costs of using strings at runtime. Integer ids are \nthe most eﬃ  cient choice, but they are very diﬃ  cult for human game \ndevelopers to work with. Arguably the best solution is to use hashed \nstring ids (see Section 5.4.3.1) as our object identiﬁ ers, as they are as \n",
      "content_length": 2660,
      "extraction_method": "Direct"
    },
    {
      "page_number": 736,
      "chapter": null,
      "content": "714 \n14. Runtime Gameplay Foundation Systems\neﬃ  cient as integers but can be converted back into string form for ease \nof reading.\nz Game object queries . The gameplay foundation system must provide some \nmeans of ﬁ nding objects within the game world. We might want to ﬁ nd \na speciﬁ c object by its unique id, or all the objects of a particular type, or \nwe might want to perform advanced queries based on arbitrary criteria \n(e.g., ﬁ nd all enemies within a 20 meter radius of the player character).\nz Game object references . Once we’ve found the objects, we need some \nmechanism for holding references to them, either brieﬂ y within a single \nfunction or for much longer periods of time. An object reference might \nbe as simple as a pointer to a C++ class instance, or it might be something \nmore sophisticated, like a handle or a reference-counted smart pointer .\nz Finite state machine support. Many types of game objects are best modeled \nas ﬁ nite state machines. Some game engines provide the ability for a \ngame object to exist in one of many possible states, each with its own \natt ributes and behavioral characteristics.\nz Network replication . In a networked multiplayer game, multiple game \nmachines are connected together via a LAN or the Internet. The state of \na particular game object is usually owned and managed by one machine. \nHowever, that object’s state must also be replicated (communicated) to \nthe other machines involved in the multiplayer game so that all players \nhave a consistent view of the object.\nz Saving and loading games / object persistence. Many game engines allow \nthe current states of the game objects in the world to be saved to disk \nand later reloaded. This might be done to support a “save anywhere ” \nsave-game system or as a way of implementing network replication, or \nit might simply be the primary means of loading game world chunks \nthat were authored in the world editor tool. Object persistence usually \nrequires certain language features, such as runtime type identiﬁ cation \n(RTTI), reﬂ ection , and abstract construction . RTTI and reﬂ ection provide \nsoft ware with a means of determining an object’s type, and what att ri-\nbutes and methods its class provides, dynamically at runtime. Abstract \nconstruction allows instances of a class to be created without having \nto hard-code the name of the class—a very useful feature when serial-\nizing an object instance into memory from disk. If RTTI, reﬂ ection, and \nabstract construction are not natively supported in your language of \nchoice, these features can be added manually.\nWe’ll spend the remainder of this chapter delving into each of these subsys-\ntems in depth.\n",
      "content_length": 2680,
      "extraction_method": "Direct"
    },
    {
      "page_number": 737,
      "chapter": null,
      "content": "715 \n14.2. Runtime Object Model Architectures\n14.2. Runtime Object Model Architectures\nIn the world editor , the game designer is presented with an abstract game \nobject model, which deﬁ nes the various types of dynamic elements that can \nexist in the game, how they behave, and what kinds of att ributes they have. \nAt runtime, the gameplay foundation system must provide a concrete imple-\nmentation of this object model. This is by far the largest component of any \ngameplay foundation system.\nThe runtime object model implementation may or may not bear any re-\nsemblance to the abstract tool-side object model. For example, it might not be \nimplemented in an object-oriented programming language at all, or it might \nuse a collection of interconnected class instances to represent a single abstract \ngame object. Whatever its design, the runtime object model must provide a \nfaithful reproduction of the object types, att ributes, and behaviors advertised \nby the world editor.\nThe runtime object model is the in-game manifestation of the abstract \ntool-side object model presented to the designers in the world editor. Designs \nvary widely, but most game engines follow one of two basic architectural \nstyles:\nz Object-centric. In this style, each tool-side game object is represented at \nruntime by a single class instance or a small collection of interconnected \ninstances. Each object has a set of att ributes and behaviors that are encap-\nsulated within the class (or classes) of which the object is an instance. \nThe game world is just a collection of game objects.\nz Property-centric. In this style, each tool-side game object is represented \nonly by a unique id (implemented as an integer, hashed string id, or \nstring). The properties of each game object are distributed across many \ndata tables, one per property type, and keyed by object id (rather than \nbeing centralized within a single class instance or collection of inter-\nconnected instances). The properties themselves are oft en implemented \nas instances of hard-coded classes. The behavior of a game object is im-\nplicitly deﬁ ned by the collection of properties from which it is com-\nposed. For example, if an object has the “Health” property, then it can be \ndamaged, lose health, and eventually die. If an object has the “MeshIn-\nstance” property, then it can be rendered in 3D as an instance of a tri-\nangle mesh.\nThere are distinct advantages and disadvantages to each of these architec-\ntural styles. We’ll investigate each one in some detail and note where one style \nhas signiﬁ cant potential beneﬁ ts over the other as they arise.\n",
      "content_length": 2613,
      "extraction_method": "Direct"
    },
    {
      "page_number": 738,
      "chapter": null,
      "content": "716 \n14. Runtime Gameplay Foundation Systems\n14.2.1. Object-Centric Architectures\nIn an object-centric game world object architecture, each logical game object \nis implemented as an instance of a class, or possibly a collection of intercon-\nnected class instances. Under this broad umbrella, many diﬀ erent designs are \npossible. We’ll investigate a few of the most common designs in the following \nsections.\n14.2.1.1. A Simple Object-Based Model in C: Hydro Thunder\nGame object models needn’t be implemented in an object-oriented language \nlike C++ at all. For example, the arcade hit Hydro Thunder , by Midway Home \nEntertainment in San Diego, was writt en entirely in C. Hydro employed a very \nsimple game object model consisting of only a few object types:\nz boats (player- and AI -controlled),\nz ﬂ oating blue and red boost icons,\nz ambient animated objects (animals on the side of the track, etc.),\nz the water surface,\nz ramps,\nz waterfalls,\nz particle e ﬀ ects,\nz race track sectors (two-dimensional polygonal regions connected to one \nanother and together deﬁ ning the watery region in which boats could \nrace),\nz static geometry (terrain , foliage, buildings along the sides of the track, \netc.),\nz two-dimensional heads-up display (HUD) elements.\nA few screen shots of Hydro Thunder are shown in Figure 14.1. Notice the hov-\nering boost icons in both screen shots and the shark swimming by in the left  \nimage (an example of an ambient animated object).\nHydro had a C struct named World_t that stored and managed the con-\ntents of a game world (i.e., a single race track). The world contained pointers \nto arrays of various kinds of game objects. The static geometry was a single \nmesh instance. The water surface, waterfalls, and particle eﬀ ects were each \nrepresented by custom data structures. The boats, boost icons, and other dy-\nnamic objects in the game were represented by instances of a general-purpose \nstruct called WorldOb_t (i.e., a world object). This was Hydro’s equivalent of \na game object as we’ve deﬁ ned it in this chapter.\nThe WorldOb_t data structure contained data members encoding the po-\nsition and orientation of the object, the 3D mesh used to render it, a set of colli-\n",
      "content_length": 2210,
      "extraction_method": "Direct"
    },
    {
      "page_number": 739,
      "chapter": null,
      "content": "717 \nsion spheres, simple animation state information (Hydro only supported rigid \nhierarchical animation), physical properties like velocity, mass, and buoyancy, \nand other data common to all of the dynamic objects in the game. In addi-\ntion, each WorldOb_t contained three pointers: a void* “user data” pointer, \na pointer to a custom “update” function, and a pointer to a custom “draw” \nfunction. So while Hydro Thunder was not object-oriented in the strictest sense, \nthe Hydro engine did extend its non-object-oriented language (C) to support \nrudimentary implementations of two important OOP features: inheritance and \npolymorphism. The user data pointer permitt ed each type of game object to \nmaintain custom state information speciﬁ c to its type while inheriting the fea-\ntures common to all world objects. For example, the Banshee boat had a dif-\nferent booster mechanism than the Rad Hazard, and each booster mechanism \nrequired diﬀ erent state information to manage its deployment and stowing \nanimations. The two function pointers acted like  virtual functions, allowing \nworld objects to have polymorphic behaviors (via their “update” functions) \nand polymorphic visual appearances (via their “draw” functions).\nstruct WorldOb_s\n{\n \nOrient_t m_transform;  \n/* position/rotation */\n \nMesh3d* \nm_pMesh;  \n \n/* 3D mesh */\n \n/* ... */\n \nvoid*  \nm_pUserData;  \n/* custom state */\n \nvoid     (*m_pUpdate)(); /* polymorphic update */\n \nvoid \n   (*m_pDraw)();  \n/* polymorphic draw */\nFigure 14.1.  Screen shots from the arcade smash Hydro Thunder, developed by Midway Home \nEntertainment in San Diego.\n14.2. Runtime Object Model Architectures\n",
      "content_length": 1651,
      "extraction_method": "Direct"
    },
    {
      "page_number": 740,
      "chapter": null,
      "content": "718 \n14. Runtime Gameplay Foundation Systems\n};\ntypedef struct WorldOb_s WorldOb_t;\n14.2.1.2. Monolithic Class Hierarchies\nIt’s natural to want to classify game object types taxonomically. This tends to \nlead game programmers toward an object-oriented language that supports \ninheritance. A class hierarchy is the most intuitive and straightforward way to \nrepresent a collection of interrelated game object types. So it is not surprising \nthat the majority of commercial game engines employ a class hierarchy based \ntechnique.\nFigure 14.2 shows a simple class hierarchy that could be used to imple-\nment the game Pac-Man. This hierarchy is rooted (as many are) at a common \nclass called GameObject, which might provide some facilities needed by all \nobject types, such as RTTI or serialization. The MovableObject class repre-\nsents any object that has a position and orientation. RenderableObject gives \nthe object an ability to be rendered (in the case of traditional Pac-Man, via a \nsprite, or in the case of a modern 3D Pac-Man game, perhaps via a triangle \nmesh). From RenderableObject are derived classes for the ghosts, Pac-Man, \npellets, and power pills that make up the game. This is just a hypothetical \nexample, but it illustrates the basic ideas that underlie most game object class \nhierarchies—namely that common, generic functionality tends to exist at the \nroot of the hierarchy, while classes toward the leaves of the hierarchy tend to \nadd increasingly speciﬁ c functionality.\nFigure 14.2.  A hypothetical class hierarchy for the game Pac-Man.\nGameObject\nMovableObject\nRenderableObject\nPacMan\nGhost\nPowerPellet\nPellet\n...\n...\n...\n",
      "content_length": 1648,
      "extraction_method": "Direct"
    },
    {
      "page_number": 741,
      "chapter": null,
      "content": "719 \nA game object class hierarchy usually begins small and simple, and in \nthat form, it can be a powerful and intuitive way to describe a collection of \ngame object types. However, as class hierarchies grow, they have a tendency \nto deepen and widen simultaneously, leading to what I call a monolithic class \nhierarchy . This kind of hierarchy arises when virtually all classes in the game \nobject model inherit from a single, common base class. The Unreal Engine’s \ngame object model is a classic example, as Figure 14.3 illustrates.\n14.2.1.3. Problems with Deep, Wide Hierarchies\nMonolithic class hierarchies tend to cause problems for the game develop-\nment team for a wide range of reasons. The deeper and wider a class hierarchy \ngrows, the more extreme these problems can become. In the following sec-\ntions, we’ll explore some of the most common problems caused by wide, deep \nclass hierarchies.\nActor\nBrush\nController\nAIController\nPlayerController\nInfo\nGameInfo\nPawn\nVehicle\nUnrealPawn\nRedeemerWarhead\nScout\nLight\nInventory\nAmmunition\nPowerups\nWeapon\nHUD\nPickup\nAmmo\nArmorPickup\nWeaponPickup\n...\n...\n...\n...\n...\n...\n...\n...\n...\nFigure 14.3.  An excerpt from the game object class hierarchy from Unreal Tournament 2004.\n14.2. Runtime Object Model Architectures\n",
      "content_length": 1270,
      "extraction_method": "Direct"
    },
    {
      "page_number": 742,
      "chapter": null,
      "content": "720 \n14. Runtime Gameplay Foundation Systems\nUnderstanding, Maintaining, and Modifying Classes\nThe deeper a class lies within a class hierarchy, the harder it is to understand, \nmaintain, and modify. This is because to understand a class, you really need to \nunderstand all of its parent classes as well. For example, modifying the behav-\nior of an innocuous-looking virtual function in a derived class could violate \nthe assumptions made by any one of the many base classes, leading to subtle, \ndiﬃ  cult-to-ﬁ nd bugs.\nInability to Describe Multidimensional Taxonomies\nA hierarchy inherently classiﬁ es objects according to a particular system of \ncriteria known as a taxonomy . For example, biological taxonomy (also known \nas alpha taxonomy) classiﬁ es all living things according to genetic similarities, \nusing a tree with eight levels: domain, kingdom, phylum, class, order, family, \ngenus, and species. At each level of the tree, a diﬀ erent criterion is used to di-\nvide the myriad life forms on our planet into more and more reﬁ ned groups.\nOne of the biggest problems with any hierarchy is that it can only classify \nobjects along a single “axis”—according to one particular set of criteria—at \neach level of the tree. Once the criteria have been chosen for a particular hier-\narchy, it becomes diﬃ  cult or impossible to classify along an entirely diﬀ erent \nset of “axes.” For example, biological taxonomy classiﬁ es objects according to \ngenetic traits, but it says nothing about the colors of the organisms. In order to \nclassify organisms by color, we’d need an entirely diﬀ erent tree structure.\nIn object-oriented programming, this limitation of hierarchical classiﬁ ca-\ntion oft en manifests itself in the form of wide, deep, and confusing class hier-\narchies. When one analyzes a real game’s class hierarchy, one oft en ﬁ nds that \nits structure att empts to meld a number of diﬀ erent classiﬁ cation criteria into \na single class tree. In other cases, concessions are made in the class hierarchy \nto accommodate a new type of object whose characteristics were not antici-\npated when the hierarchy was ﬁ rst designed. For example, imagine the seem-\nVehicle\nMotorcycle\nSpeedBoat\nCar\nTruck\nHovercraft\nYacht\nLandVehicle\nWaterVehicle\nFigure 14.4.  A seemingly logical class hierarchy describing various kinds of vehicles.\n",
      "content_length": 2337,
      "extraction_method": "Direct"
    },
    {
      "page_number": 743,
      "chapter": null,
      "content": "721 \ningly logical class hierarchy describing diﬀ erent types of vehicles, depicted in \nFigure 14.4.\nWhat happens when the game designers announce to the programmers \nthat they now want the game to include an amphibious vehicle? Such a vehicle \ndoes not ﬁ t into the existing taxonomic system. This may cause the program-\nmers to panic or, more likely, to “hack” their class hierarchy in various ugly \nand error-prone ways.\nMultiple Inheritance: The Deadly Diamond\nOne solution to the amphibious vehicle problem is to utilize C++’s multiple \ninheritance (MI) features, as shown in Figure 14.5. At ﬁ rst glance, this seems \nlike a good solution. However, multiple inheritance in C++ poses a number \nof practical problems. For example, multiple inheritance can lead to an object \nthat contains multiple copies of its base class’s members—a condition known \nas the “deadly diamond ” or “diamond of death.” (See Section 3.1.1.3 for more \ndetails.)\nThe diﬃ  culties in building an MI class hierarchy that works and that is \nunderstandable and maintainable usually outweigh the beneﬁ ts. As a result, \nmost game studios prohibit or severely limit the use of multiple inheritance in \ntheir class hierarchies.\nVehicle\nAmphibiousVehicle\nLandVehicle\nWaterVehicle\nFigure 14.5.  A diamond-shaped class hierarchy for amphibious vehicles.\nMix-In Classes\nSome teams do permit a limited form of MI, in which a class may have any \nnumber of parent classes but only one grandparent. In other words, a class \nmay inherit from one and only one class in the main inheritance hierarchy, \nbut it may also inherit from any number of mix-in classes (stand-alone classes \nwith no base class). This permits common functionality to be factored out into \na mix-in class and then spot-patched into the main hierarchy wherever it is \nneeded. This is shown in Figure 14.6. However, as we’ll see below, it’s usually \nbett er to compose or aggregate such classes than to inherit from them.\n14.2. Runtime Object Model Architectures\n",
      "content_length": 1997,
      "extraction_method": "Direct"
    },
    {
      "page_number": 744,
      "chapter": null,
      "content": "722 \n14. Runtime Gameplay Foundation Systems\nThe Bubble-Up Effect\nWhen a monolithic class hierarchy is ﬁ rst designed, the root class(es) are usu-\nally very simple, each one exposing only a minimal feature set. However, as \nmore and more functionality is added to the game, the desire to share code \nbetween two or more unrelated classes begins to cause features to “bubble up ” \nthe hierarchy.\nFor example, we might start out with a design in which only wooden \ncrates can ﬂ oat in water. However, once our game designers see those cool \nﬂ oating crates, they begin to ask for other kinds of ﬂ oating objects, like charac-\nters, bits of paper, vehicles, and so on. Because “ﬂ oating versus non-ﬂ oating” \nwas not one of the original classiﬁ cation criteria when the hierarchy was de-\nsigned, the programmers quickly discover the need to add ﬂ otation to classes \nthat are totally unrelated within the class hierarchy. Multiple inheritance is \nfrowned upon, so the programmers decide to move the ﬂ otation code up the \nhierarchy, into a base class that is common to all objects that need to ﬂ oat. The \nfact that some of the classes that derive from this common base class cannot \nﬂ oat is seen as less of a problem than duplicating the ﬂ otation code across mul-\ntiple classes. (A Boolean member variable called something like m_bCanFloat\nmight even be added to make the distinction clear.) The ultimate result is that \nﬂ otation eventually becomes a feature of the root object in the class hierarchy \n(along with prett y much every other feature in the game).\nThe Actor class in Unreal is a classic example of this “bubble-up eﬀ ect.” It \ncontains data members and code for managing rendering, animation, physics, \nworld interaction, audio eﬀ ects, network replication for multiplayer games, \nGameObject\n+GetHealth()\n+ApplyDamage()\n+IsDead()\n+OnDeath()\nMHealth\n+PickUp()\n+Drop()\n+IsBeingCarried()\nMCarryable\nNPC\nPlayer\nTank\nJeep\nPistol\nMG\nCanteen\nAmmo\nCharacter\nVehicle\nWeapon\nItem\nFigure 14.6.  A class hierarchy with mix-in classes. The MHealth mix-in class adds the notion \nof health and the ability to be killed to any class that inherits it. The MCarryable mix-in class \nallows an object that inherits it to be carried by a Character.\n",
      "content_length": 2242,
      "extraction_method": "Direct"
    },
    {
      "page_number": 745,
      "chapter": null,
      "content": "723 \nobject creation and destruction, actor iteration (i.e., the ability to iterate over \nall actors meeting a certain criteria and perform some operation on them), \nand message broadcasting. Encapsulating the functionality of various engine \nsubsystems is diﬃ  cult when features are permitt ed to “bubble up” to the root-\nmost classes in a monolithic class hierarchy.\n14.2.1.4. Using Composition to Simplify the Hierarchy\nPerhaps the most prevalent cause of monolithic class hierarchies is over-use \nof the “is-a” relationship in object-oriented design. For example, in a game’s \nGUI, a programmer might decide to derive the class Window from a class \ncalled Rectangle, using the logic that GUI windows are always rectangular. \nHowever, a window is not a rectangle—it has a rectangle, which deﬁ nes its \nboundary. So a more workable solution to this particular design problem is to \nembed an instance of the Rectangle class inside the Window class, or to give \nthe Window a pointer or reference to a Rectangle.\nIn object-oriented design, the “has-a” relationship is known as composi-\ntion . In composition, a class A either contains an instance of class B directly, or \ncontains a pointer or reference to an instance of B. Strictly speaking, in order for \nthe term “composition” to be applicable, class A must own class B. This means \nthat when an instance of class A is created, it automatically creates an instance \nof class B as well; when that instance of A is destroyed, its instance of B is de-\nstroyed, too. We can also link classes to one another via a pointer or reference \nwithout having one of the classes manage the other’s lifetime . In that case, the \ntechnique is usually called aggregation .\nConverting Is-A to Has-A\nConverting “is-a” relationships into “has-a” relationships can be a useful tech-\nnique for reducing the width, depth, and complexity of a game’s class hier-\narchy. To illustrate, let’s take a look at the hypothetical monolithic hierarchy \nshown in Figure 14.7. The root GameObject class provides some basic func-\ntionality required by all game objects (e.g., RTTI, reﬂ ection, persistence via \nserialization, network replication, etc.). The MovableObject class represents \nany game object that has a transform (i.e., a position, orientation, and optional \nscale). RenderableObject adds the ability to be rendered on-screen. (Not all \ngame objects need to be rendered—for example, an invisible TriggerRegion\nclass could be derived directly from MovableObject.) The Collidable\nObject class provides collision information to its instances. The Animating\nObject class grants to its instances the ability to be animated via a skeletal \njoint hierarchy. Finally, the PhysicalObject gives its instances the ability to \nbe physically simulated (e.g., a rigid body falling under the inﬂ uence of grav-\nity and bouncing around in the game world).\n14.2. Runtime Object Model Architectures\n",
      "content_length": 2913,
      "extraction_method": "Direct"
    },
    {
      "page_number": 746,
      "chapter": null,
      "content": "724 \n14. Runtime Gameplay Foundation Systems\nOne big problem with this class hierarchy is that it limits our design \nchoices when creating new types of game objects. If we want to deﬁ ne an \nobject type that is physically simulated, we are forced to derive its class from \nPhysicalObject even though it may not require skeletal animation. If we \nwant a game object class with collision, it must inherit from Collidable even \nthough it may be invisible and hence not require the services of Renderable.\nA second problem with the hierarchy shown in Figure 14.7 is that it is \ndiﬃ  cult to extend the functionality of the existing classes. For example, let’s \nimagine we want to support morph target animation, so we derive two new \nclasses from AnimatingObject called SkeletalObject and MorphTarget\nObject. If we wanted both of these new classes to have the ability to be physi-\ncally simulated, we’d be forced to re-factor PhysicalObject into two nearly-\nidentical classes, one derived from SkeletalObject and one from Morph\nTargetObject, or turn to multiple inheritance.\nOne solution to these problems is to isolate the various features of a \nGameObject into independent classes, each of which provides a single, well-\ndeﬁ ned service. Such classes are sometimes called components or service objects. \nA componentized design allows us to select only those features we need for \neach type of game object we create. In addition, it permits each feature to be \nmaintained, extended, or re-factored without aﬀ ecting the others. The indi-\nvidual components are also easier to understand, and easier to test, because \nthey are decoupled from one another. Some component classes correspond \ndirectly to a single engine subsystem, such as rendering, animation, collision, \nphysics, audio, etc. This allows these subsystems to remain distinct and well-\nGameObject\nMovableObject\nRenderableObject\nCollidableObject\nAnimatingObject\nPhysicalObject\nFigure 14.7.  A hypo-\nthetical game object \nclass hierarchy us-\ning only inheritance \nto \nassociate \nthe \nclasses.\nFigure 14.8.  Our hypothetical game object class hierarchy, re-factored to favor class composi-\ntion over inheritance.\nGameObject\nTransform\nMeshInstance\nAnimationController\nRigidBody\n1\n1\n1\n1\n1\n1\n1\n1\n",
      "content_length": 2249,
      "extraction_method": "Direct"
    },
    {
      "page_number": 747,
      "chapter": null,
      "content": "725 \nencapsulated when they are integrated together for use by a particular game \nobject.\nFigure 14.8 shows how our class hierarchy might look aft er re-factoring \nit into components. In this revised design, the GameObject class acts like a \nhub, containing pointers to each of the optional components we’ve deﬁ ned. \nThe MeshInstance component is our replacement for the Renderable\nObject class—it represents an instance of a triangle mesh and encapsulates \nthe knowledge of how to render it. Likewise, the AnimationController\ncomponent replaces AnimatingObject, exposing skeletal animation services \nto the GameObject. Class Transform replaces MovableObject by maintain-\ning the position, orientation, and scale of the object. The RigidBody class rep-\nresents the collision geometry of a game object and provides its GameObject\nwith an interface into the low-level collision and physics systems, replacing \nCollidableObject and PhysicsObject.\nComponent Creation and Ownership\n In this kind of design, it is typical for the “hub” class to own its components, \nmeaning that it manages their lifetimes . But how should a GameObject “know” \nwhich components to create? There are numerous ways to solve this problem, \nbut one of the simplest is provide the root GameObject class with pointers \nto all possible components. Each unique type of game object is deﬁ ned as a \nderived class of GameObject. In the GameObject constructor, all of the com-\nponent pointers are initially set to NULL. Each derived class’s constructor is \nthen free to create whatever components it may need. For convenience, the \ndefault GameObject destructor can clean up all of the components automati-\ncally. In this design, the hierarchy of classes derived from GameObject serves \nas the primary taxonomy for the kinds of objects we want in our game, and the \ncomponent classes serve as optional add-on features.\nOne possible implementation of the component creation and destruction \nlogic for this kind of hierarchy is shown below. However, it’s important to \nrealize that this code is just an example—implementation details vary widely, \neven between engines that employ essentially the same kind of class hierarchy \ndesign.\nclass GameObject\n{\nprotected:\n \n// My transform (position, rotation, scale).\n Transform \n   m_transform;\n \n// Standard components:\n MeshInstance* \nm_pMeshInst;\n14.2. Runtime Object Model Architectures\n",
      "content_length": 2401,
      "extraction_method": "Direct"
    },
    {
      "page_number": 748,
      "chapter": null,
      "content": "726 \n14. Runtime Gameplay Foundation Systems\n AnimationController* \nm_pAnimController;\n RigidBody*\nm_pRigidBody;\npublic:\nGameObject()\n {\n \n \n// Assume no components by default. Derived  \n \n \n \n \n// classes will override.\n \n \nm_pMeshInst = NULL;\n \n \nm_pAnimController = NULL;\n \n \nm_pRigidBody = NULL;\n }\n~GameObject()\n {\n \n \n// Automatically delete any components created by\n \n \n// derived classes.\n  delete \nm_pMeshInst;\n  delete \nm_pAnimController;\n  delete \nm_pRigidBody;\n }\n \n// ...\n};\nclass Vehicle : public GameObject\n{\nprotected:\n \n// Add some more components specific to Vehicles...\n Chassis* \nm_pChassis;\n Engine* \nm_pEngine;\n \n// ...\npublic:\nVehicle()\n {\n \n \n// Construct standard GameObject components.\n  m_pMeshInst \n= new MeshInstance;\n  m_pRigidBody \n= new RigidBody;\n \n \n// NOTE: We’ll assume the animation controller  \n \n \n \n// must be provided with a reference to the mesh   \n \n \n// instance so that it can provide it with a \n \n \n// matrix palette.\n  m_pAnimController\n   = \nnew AnimationController(*m_pMeshInst);\n",
      "content_length": 1030,
      "extraction_method": "Direct"
    },
    {
      "page_number": 749,
      "chapter": null,
      "content": "727 \n \n \n// Construct vehicle-specific components.\n  m_pChassis \n= new Chassis(*this,\n      \n  *m_pAnimController);\n  m_pEngine \n= new Engine(*this);\n }\n~Vehicle()\n {\n \n \n// Only need to destroy vehicle-specific  \n \n \n \n \n \n// components, as GameObject cleans up the \n \n \n// standard components for us.\n  delete \nm_pChassis;\n  delete \nm_pEngine;\n }\n};\n14.2.1.5. Generic Components\n Another more-ﬂ exible (but also trickier to implement) alternative is to provide \nthe root game object class with a generic linked list of components. The com-\nponents in such a design usually all derive from a common base class—this \nallows us to iterate over the linked list and perform polymorphic operations, \nsuch as asking each component what type it is or passing an event to each \ncomponent in turn for possible handling. This design allows the root game \nobject class to be largely oblivious to the component types that are available \nand thereby permits new types of components to be created without modify-\ning the game object class in many cases. It also allows a particular game object \nto contain an arbitrary number of instances of each type of component. (The \nGameObject\nTransform\nMeshInstance\nAnimationController\nRigidBody\n+GetType()\n+IsType()\n+ReceiveEvent()\n+Update()\nComponent\n1\n*\nAsterisk indicates zero \nor more instances \n(e.g., linked list).\nFigure 14.9.  A linked list of components can provide ﬂ exibility by allowing the hub game ob-\nject to be unaware of the details of any particular component.\n14.2. Runtime Object Model Architectures\n",
      "content_length": 1548,
      "extraction_method": "Direct"
    },
    {
      "page_number": 750,
      "chapter": null,
      "content": "728 \n14. Runtime Gameplay Foundation Systems\nhard-coded design permitt ed only a ﬁ xed number, determined by how many \npointers to each component existed within the game object class.)\nThis kind of design is illustrated in Figure 14.9. It is trickier to implement \nthan a hard-coded component model because the game object code must be \nwritt en in a totally generic way. The component classes can likewise make no \nassumptions about what other components might or might not exist within \nthe context of a particular game object. The choice between hard-coding the \ncomponent pointers or using a generic linked list of components is not an easy \none to make. Neither design is clearly superior—they each have their pros and \ncons, and diﬀ erent game teams take diﬀ erent approaches.\n14.2.1.6. Pure Component Models\n What would happen if we were to take the componentization concept to \nits extreme? We would move literally all of the functionality out of our root \nGameObject class into various component classes. At this point, the game ob-\nject class would quite literally be a behavior-less container, with a unique id \nand a bunch of pointers to its components, but otherwise containing no logic \nof its own. So why not eliminate the class entirely? One way to do this is to \ngive each component a copy of the game object’s unique id. The components \nare now linked together into a logical grouping by id. Given a way to quickly \nlook up any component by id, we would no longer need the GameObject\n“hub” class at all. I will use the term pure component model to describe this kind \nof architecture. It is illustrated in Figure 14.10.\nA pure component model is not quite as simple as it ﬁ rst sounds, and it \nis not without its share of problems. For one thing, we still need some way \nof deﬁ ning the various concrete types of game objects our game needs and \nthen arranging for the correct component classes to be instantiated whenever \nFigure 14.10.  In a pure component model, a logical game object is comprised of many compo-\nnents, but the components are linked together only indirectly, by sharing a unique id.\n-m_uniqueId : int = 72\nGameObject\n-m_uniqueId : int = 72\nTransform\n-m_uniqueId : int = 72\nMeshInstance\n-m_uniqueId : int = 72\nAnimationController\n-m_uniqueId : int = 72\nRigidBody\n",
      "content_length": 2300,
      "extraction_method": "Direct"
    },
    {
      "page_number": 751,
      "chapter": null,
      "content": "729 \nan instance of the type is created. Our GameObject hierarchy used to handle \nconstruction of components for us. Instead, we might use a factory patt ern, \nin which we deﬁ ne factory classes, one per game object type, with a virtual \nconstruction function that is overridden to create the proper components for \neach game object type. Or we might turn to a data-driven model, where the \ngame object types are deﬁ ned in a text ﬁ le that can be parsed by the engine \nand consulted whenever a type is instantiated.\nAnother issue with a components-only design is inter-component com-\nmunication. Our central GameObject acted as a “hub,” marshalling commu-\nnications between the various components. In pure component architectures, \nwe need an eﬃ  cient way for the components making up a single game object \nto talk to one another. This could be done by having each component look up \nthe other components using the game object’s unique id. However, we prob-\nably want a much more eﬃ  cient mechanism—for example the components \ncould be prewired into a circular linked list.\nIn the same sense, sending messages from one game object to another \nis diﬃ  cult in a pure componentized model. We can no longer communicate \nwith the GameObject instance, so we either need to know a priori with which \ncomponent we wish to communicate, or we must multicast to all components \nthat make up the game object in question. Neither option is ideal.\nPure component models can and have been made to work on real game \nprojects. These kinds of models have their pros and cons, but again, they are \nnot clearly bett er than any of the alternative designs. Unless you’re part of a \nresearch and development eﬀ ort, you should probably choose the architecture \nwith which you are most comfortable and conﬁ dent, and which best ﬁ ts the \nneeds of the particular game you are building.\n14.2.2. Property-Centric Architectures\nProgrammers who work frequently in an object-oriented programming lan-\nguage tend to think naturally in terms of objects that contain att ributes (data \nmembers) and behaviors (methods, member functions). This is the object-cen-\ntric view:\nz Object1\nPosition = (0, 3, 15)\n \n□\nOrientation = (0, 43, 0)\n \n□\nz Object2\nPosition = (–12, 0, 8)\n \n□\nHealth = 15\n \n□\n14.2. Runtime Object Model Architectures\n",
      "content_length": 2305,
      "extraction_method": "Direct"
    },
    {
      "page_number": 752,
      "chapter": null,
      "content": "730 \n14. Runtime Gameplay Foundation Systems\nz Object3\nOrientation = (0, –87, 10)\n \n□\nHowever, it is possible to think primarily in terms of the att ributes, rather \nthan the objects. We deﬁ ne the set of all properties that a game object might \nhave. Then for each property, we build a table containing the values of that \nproperty corresponding to each game object that has it. The property values \nare keyed by the objects’ unique ids. This is what we will call the property-\ncentric vi ew:\nz Position\nObject1 = (0, 3, 15)\n \n□\nObject2 = (–12, 0, 8)\n \n□\nz Orientation\nObject1 = (0, 43, 0)\n \n□\nObject3 = (0, –87, 10)\n \n□\nz Health\nObject2 = 15\n \n□\nProperty-centric object models have been used very successfully on many \ncommercial games, including Deus Ex 2 and the Thief series of games. See Sec-\ntion 14.2.2.5 for more details on exactly how these projects designed their ob-\nject systems.\nA property-centric design is more akin to a relational database than an \nobject model. Each att ribute acts like a column in a database table (or a stand-\nalone table), with the game objects’ unique id as the primary key. Of course, in \nobject-oriented design, an object is deﬁ ned not only by its att ributes, but also \nby its behavior. If all we have are tables of properties, then where do we imple-\nment the behavior? The answer to this question varies somewhat from engine \nto engine, but most oft en the behaviors are implemented in one or both of the \nfollowing places:\nz in the properties themselves, and/or\nz via script code.\nLet’s explore each of these ideas further.\n14.2.2.1. Implementing Behavior via Property Classes\nEach type of property can be implemented as a property class. Properties can be \nas simple as a single Boolean or ﬂ oating-point value or as complex as a render-\nable triangle mesh or an AI “brain.” Each property class can provide behav-\niors via its hard-coded methods (member functions). The overall behavior of \n",
      "content_length": 1939,
      "extraction_method": "Direct"
    },
    {
      "page_number": 753,
      "chapter": null,
      "content": "731 \na particular game object is determined by the aggregation of the behaviors of \nall its properties.\nFor example, if a game object contains an instance of the Health property, \nit can be damaged and eventually destroyed or killed. The Health object can \nrespond to any att acks made on the game object by decrementing the object’s \nhealth level appropriately. A property object can also communicate with other \nproperty objects within the same game object to produce cooperative behav-\niors. For example, when the Health property detects and responds to an at-\ntack, it could possibly send a message to the AnimatedSkeleton property, \nthereby allowing the game object to play a suitable hit reaction animation. \nSimilarly, when the Health property detects that the game object is about to \ndie or be destroyed, it can talk to the RigidBodyDynamics property to acti-\nvate a physics-driven explosion or a “rag doll” dead body simulation.\n14.2.2.2. Implementing Behavior via Script\nAnother option is to store the property values as raw data in one or more data-\nbase-like tables and use script code to implement a game object’s behaviors. Ev-\nery game object could have a special property called something like ScriptId, \nwhich, if present, speciﬁ es the block of script code (script function, or script \nobject if the scripting language is itself object-oriented) that will manage the ob-\nject’s behavior. Script code could also be used to allow a game object to respond \nto events that occur within the game world. See Section 14.7 for more details on \nevent systems and Section 14.8 for a discussion of game scripting languages.\nIn some property-centric engines, a core set of hard-coded property classes \nare provided by the engineers, but a facility is provided allowing game design-\ners and programmers to implement new property types entirely in script. This \napproach was used successfully on the Dungeon Siege project, for example.\n14.2.2.3. Properties versus Components\nIt’s important to note that many of the authors cited in Section 14.2.2.5 use the \nterm “component” to refer to what I call a “property object ” here. In Section \n14.2.1.4, I used the term “component” to refer to a subobject in an object-cen-\ntric design, which isn’t quite the same as a property object.\nHowever, property objects are very closely related to components in many \nways. In both designs, a single logical game object is made up of multiple sub-\nobjects. The main distinction lies in the roles of the subobjects. In a property-\ncentric design, each subobject deﬁ nes a particular att ribute of the game object \nitself (e.g., health, visual representation, inventory, a particular magic power, \netc.), whereas in a component-based (object-centric) design, the subobjects of-\nten represent linkages to particular low-level engine subsystems (renderer, \nanimation, collision and dynamics, etc.) This distinction is so subtle as to be \n14.2. Runtime Object Model Architectures\n",
      "content_length": 2966,
      "extraction_method": "Direct"
    },
    {
      "page_number": 754,
      "chapter": null,
      "content": "732 \n14. Runtime Gameplay Foundation Systems\nvirtually irrelevant in many cases. You can call your design a pure component \nmodel (Section 14.2.1.6) or a property-centric design as you see ﬁ t, but at the end \nof the day, you’ll have essentially the same result—a logical game object that is \ncomprised of, and derives its behavior from, a collection of subobjects.\n14.2.2.4. Pros and Cons of Property-Centric Designs\n There are a number of potential beneﬁ ts to an att ribute-centric approach. \nIt tends to be more memory eﬃ  cient, because we need only store att ribute \ndata that is actually in use (i.e., there are never game objects with unused \ndata members). It is also easier to construct such a model in a data-driven \nmanner—designers can deﬁ ne new att ributes easily, without recompiling the \ngame, because there are no game object class deﬁ nitions to be changed. Pro-\ngrammers need only get involved when entirely new types of properties need \nto be added (presuming the property cannot be added via script).\nA property-centric design can also be more cache-friendly than an object-\ncentric model, because data of the same type is stored contiguously in memory. \nThis is a commonplace optimization technique on modern gaming hardware, \nwhere the cost of accessing memory is far higher than the cost of executing \ninstructions and performing calculations. (For example, on the PLAYSTA-\nTION 3, the cost of a single cache miss is equivalent to the cost of executing lit-\nerally thousands of CPU instructions.) By storing data contiguously in RAM, \nwe can reduce or eliminate cache misses, because when we access one element \nof a data array, a large number of its neighboring elements are loaded into the \nsame cache line. This approach to data design is sometimes called the struct of \narrays technique, in contrast to the more-traditional array of structs approach. \nThe diﬀ erences between these two memory layouts are illustrated by the code \nsnippet below. (Note that we wouldn’t really implement a game object model \nin exactly this way—this example is meant only to illustrate the way in which \na property-centric design tends to produce many contiguous arrays of like-\ntyped data, rather than a single array of complex objects.)\nstatic const U32 MAX_GAME_OBJECTS = 1024; \n// Traditional array-of-structs approach.\nstruct GameObject\n{\n U32 \n   m_uniqueId;\n Vector \n  m_pos;\n Quaternion m_rot;\n float \n  m_health;\n \n// ...\n};\nGameObject g_aAllGameObjects[MAX_GAME_OBJECTS];\n",
      "content_length": 2492,
      "extraction_method": "Direct"
    },
    {
      "page_number": 755,
      "chapter": null,
      "content": "733 \n// Cache-friendlier struct-of-arrays approach.\nstruct AllGameObjects\n{\n U32 \n   m_aUniqueId\n[MAX_GAME_OBJECTS];\n Vector \n  m_aPos\n[MAX_GAME_OBJECTS];\n Quaternion m_aRot\n[MAX_GAME_OBJECTS];\n float \n  m_aHealth\n[MAX_GAME_OBJECTS];\n \n// ...\n};\nAllGameObjects g_allGameObjects;\nAtt ribute-centric models have their share of problems as well. For ex-\nample, when a game object is just a grab bag of properties, it becomes much \nmore diﬃ  cult to enforce relationships between those properties. It can be hard \nto implement a desired large-scale behavior merely by cobbling together the \nﬁ ne-grained behaviors of a group of property objects. It’s also much trickier \nto debug such systems, as the programmer cannot slap a game object into \nthe watch window in the debugger in order to inspect all of its properties at \nonce.\n14.2.2.5. Further Reading\nA number of interesting PowerPoint presentations on the topic of property-\ncentric architectures have been given by prominent engineers in the game \nindustry at various game development conferences. You should be able to \naccess them by visiting the following URLs:\nz Rob Fermier, “Creating a Data Driven Engine,” Game Developer’s Con-\nference, 2002. htt p://www.gamasutra.com/features/gdcarchive/2002/\nrob_fermier.ppt.\nz Scott  Bilas, “A Data-Driven Game Object System,” Game Developer’s \nConference, 2002. htt p://www.drizzle.com/~scott b/gdc/game-objects.\nppt.\nz Alex Duran, “Building Object Systems: Features, Tradeoﬀ s, and \nPitfalls,” Game Developer’s Conference, 2003. htt p://www.gamasutra.\ncom/features/gdcarchive/2003/Duran_Alex.ppt.\nz Jeremy Chatelaine, “Enabling Data Driven Tuning via Existing Tools,” \nGame Developer’s Conference, 2003. htt p://www.gamasutra.com/\nfeatures/gdcarchive/2003/Chatelaine_Jeremy.ppt.\nz Doug Church, “Object Systems,” presented at a game development con-\nference in Seoul, Korea, 2003; conference organized by Chris Hecker, \nCasey Muratori, Jon Blow, and Doug Church. htt p://chrishecker.com/\nimages/6/6f/ObjSys.ppt.\n14.2. Runtime Object Model Architectures\n",
      "content_length": 2050,
      "extraction_method": "Direct"
    },
    {
      "page_number": 756,
      "chapter": null,
      "content": "734 \n14. Runtime Gameplay Foundation Systems\n14.3. World Chunk Data Formats\n As we’ve seen, a world chunk generally contains both static and dynamic \nworld elements . The static geometry might be represented by one big triangle \nmesh, or it might be comprised of many smaller meshes. Each mesh might be \ninstanced multiple times—for example, a single door mesh might be re-used \nfor all of the doorways in the chunk. The static data usually includes colli-\nsion information stored as a triangle soup , a collection of convex shapes, and/\nor other simpler geometric shapes like planes, boxes, capsules, or spheres. \nOther static elements include volumetric regions that can be used to detect \nevents or delineate areas within the game world, an AI navigation mesh, a set \nof line segments delineating edges within the background geometry that can \nbe grabbed by the player character, and so on. We won’t get into the details \nof these data formats here, because we’ve already discussed most of them in \nprevious chapters.\nThe dynamic portion of the world chunk contains some kind of repre-\nsentation of the game objects within that chunk. A game object is deﬁ ned by \nits att ributes and its behaviors, and an object’s behaviors are determined either \ndirectly or indirectly by its type. In an object-centric design, the object’s type \ndirectly determines which class(es) to instantiate in order to represent the ob-\nject at runtime. In a property-centric design, a game object’s behavior is deter-\nmined by the amalgamation of the behaviors of its properties, but the type still \ndetermines which properties the object should have (or one might say that an \nobject’s properties deﬁ ne its type). So, for each game object, a world chunk \ndata ﬁ le generally contains:\nz The initial values of the object’s att ributes. The world chunk deﬁ nes the \nstate of each game object as it should exist when ﬁ rst spawned into the \ngame world. An object’s att ribute data can be stored in a number of dif-\nferent formats. We’ll explore a few popular formats below.\nz Some kind of speciﬁ cation of the object’s type. In an object-centric engine, \nthis might be a string, a hashed string id, or some other unique type id. \nIn a property-centric design, the type might be stored explicitly, or it \nmight be deﬁ ned implicitly by the collection of properties/att ributes of \nwhich the object is comprised.\n14.3.1. Binary Object Images\nOne way to store a collection of game objects into a disk ﬁ le is to write a binary \nimage of each object into the ﬁ le, exactly as it looks in memory at runtime. \nThis makes spawning game objects trivial. Once the game world chunk has \n",
      "content_length": 2656,
      "extraction_method": "Direct"
    },
    {
      "page_number": 757,
      "chapter": null,
      "content": "735 \nbeen loaded into memory, we have ready-made images of all our objects, so \nwe simply let them ﬂ y.\nWell, not quite. Storing binary images of “live” C++ class instances is \nproblematic for a number of reasons, including the need to handle pointers \nand virtual tables in a special way, and the possibility of having to endian-\nswap the data within each class instance. (These techniques are described in \ndetail in Section 6.2.2.9.) Moreover, binary object images are inﬂ exible and not \nrobust to making changes. Gameplay is one of the most dynamic and unstable \naspects of any game project, so it is wise to select a data format that supports \nrapid development and is robust to frequent changes. As such, the binary ob-\nject image format is not usually a good choice for storing game object data \n(although this format can be suitable for more stable data structures, like mesh \ndata or collision geometry). \n14.3.2. Serialized Game Object Descriptions\nSerialization is another means of storing a representation of a game object’s in-\nternal state to a disk ﬁ le. This approach tends to be more portable and simpler \nto implement than the binary object image technique. To serialize an object out \nto disk, the object is asked to produce a stream of data that contains enough \ndetail to permit the original object to be reconstructed later. When an object is \nserialized back into memory from disk, an instance of the appropriate class is \ncreated, and then the stream of att ribute data is read in order to initialize the \nnew object’s internal state. If the original serialized data stream was complete, \nthe new object should be identical to the original for all intents and purposes.\nSerialization is supported natively by some programming languages. For \nexample, C# and Java both provide standardized mechanisms for serializing \nobject instances to and from an XML text format. The C++ language unfortu-\nnately does not provide a standardized serialization facility. However, many \nC++ serialization systems have been successfully built, both inside and out-\nside the game industry. We won’t get into all the details of how to write a C++ \nobject serialization system here, but we’ll describe the data format and a few \nof the main systems that need to be writt en in order to get serialization to \nwork in C++.\nSerialization data isn’t a binary image of the object. Instead, it is usually \nstored in a more-convenient and more-portable format. XML is a popular for-\nmat for object serialization because it is well-supported and standardized, it is \nsomewhat human-readable, and it has excellent support for hierarchical data \nstructures, which arise frequently when serializing collections of interrelated \ngame objects. Unfortunately, XML is notoriously slow to parse, which can \nincrease world chunk load times. For this reason, some game engines use a \n14.3. World Chunk Data Formats\n",
      "content_length": 2901,
      "extraction_method": "Direct"
    },
    {
      "page_number": 758,
      "chapter": null,
      "content": "736 \n14. Runtime Gameplay Foundation Systems\nproprietary binary format that is faster to parse and more compact than XML \ntext.\nThe mechanics of serializing an object to and from disk are usually imple-\nmented in one of two basic ways:\nz We can introduce a pair of virtual functions called something like \nSerializeOut() and SerializeIn() in our base class and arrange \nfor each derived class to provide custom implementations of them that \n“know” how to serialize the att ributes of that particular class.\nz We can implement a reﬂ ection system for our C++ classes. We can then \nwrite a generic system that can automatically serialize any C++ object \nfor which reﬂ ection information is available.\nReﬂ ection is a term used by the C# language, among others. In a nutshell, \nreﬂ ection data is a runtime description of the contents of a class. It stores infor-\nmation about the name of the class, what data members it contains, the types \nof each data member, and the oﬀ set of each member within the object’s mem-\nory image, and it also contains information about all of the class’s member \nfunctions. Given reﬂ ection information for an arbitrary C++ class, we could \nquite easily write a general-purpose object serialization system.\nThe tricky part of a C++ reﬂ ection system is generating the reﬂ ection data \nfor all of the relevant classes. This can be done by encapsulating a class’s data \nmembers in #def ine macros that extract relevant reﬂ ection information by \nproviding a virtual function that can be overridden by each derived class in \norder to return appropriate reﬂ ection data for that class, by hand-coding a \nreﬂ ection data structure for each class, or via some other inventive approach.\nIn addition to att ribute information, the serialization data stream invari-\nably includes the name or unique id of each object’s class or type. The class id \nis used to instantiate the appropriate class when the object is serialized into \nmemory from disk. A class id can be stored as a string, a hashed string id, or \nsome other kind of unique id.\nUnfortunately, C++ provides no way to instantiate a class given only its \nname as a string or id. The class name must be known at compile time, and \nso it must be hard-coded by a programmer (e.g., new ConcreteClass ). To \nwork around this limitation of the language, C++ object serialization systems \ninvariably include a class factory of some kind. A factory can be implemented \nin any number of ways, but the simplest approach is to create a data table that \nmaps each class name/id to some kind of function or functor object that has \nbeen hard-coded to instantiate that particular class. Given a class name or id, \nwe simply look up the corresponding function or functor in the table and call \nit to instantiate the class.\n",
      "content_length": 2786,
      "extraction_method": "Direct"
    },
    {
      "page_number": 759,
      "chapter": null,
      "content": "737 \n14.3.3. Spawners and Type Schemas\nBoth binary object images and serialization formats have an Achilles heel. They \nare both deﬁ ned by the runtime implementation of the game object types they \nstore, and hence they both require the world editor to contain intimate knowl-\nedge of the game engine’s runtime implementation. For example, in order for \nthe world editor to write out a binary image of a heterogeneous collection of \ngame objects, it must either link directly with the runtime game engine code, \nor it must be painstakingly hand-coded to produce blocks of bytes that exactly \nmatch the data layout of the game objects at runtime. Serialization data is less-\ntightly coupled to the game object’s implementation, but again, the world edi-\ntor either needs to link with runtime game object code in order to gain access \nto the classes’ SerializeIn() and SerializeOut() functions, or it needs \naccess to the classes’ reﬂ ection information in some way.\nThe coupling between the game world editor and the runtime engine \ncode can be broken by abstracting the descriptions of our game objects in an \nimplementation-independent way. For each game object in a world chunk \ndata ﬁ le, we store a litt le block of data, oft en called a spawner . A spawner is \na lightweight, data-only representation of a game object that can be used to \ninstantiate and initialize that game object at runtime. It contains the id of \nthe game object’s tool-side type. It also contains a table of simple key-value \npairs that describe the initial att ributes of the game object. These att ributes \noft en include a model-to-world transform, since most game objects have \na distinct position, orientation, and scale in world space. When the game \nobject is spawned, the appropriate class or classes are instantiated, as de-\ntermined by the spawner’s type. These runtime objects can then consult \nthe dictionary of key-value pairs in order to initialize their data members \nappropriately.\nA spawner can be conﬁ gured to spawn its game object immediately upon \nbeing loaded, or it can lie dormant until asked to spawn at some later time \nduring the game. Spawners can be implemented as ﬁ rst-class objects, so they \ncan have a convenient functional interface and can store useful meta-data in \naddition to object att ributes. A spawner can even be used for purposes other \nthan spawning game objects. For example, in Uncharted: Drake’s Fortune, de-\nsigners used spawners to deﬁ ne important points or coordinate axes in the \ngame world. These were called position spawners or locator spawners. Locators \nhave many uses in a game, such as:\nz deﬁ ning points of interest for an AI character,\nz deﬁ ning a set of coordinate axes relative to which a set of animations \ncan be played in perfect synchronization,\n14.3. World Chunk Data Formats\n",
      "content_length": 2823,
      "extraction_method": "Direct"
    },
    {
      "page_number": 760,
      "chapter": null,
      "content": "738 \n14. Runtime Gameplay Foundation Systems\nz deﬁ ning the location at which a particle eﬀ ect or audio eﬀ ect should \noriginate,\nz deﬁ ning waypoints along a race track,\nz and the list goes on.\n14.3.3.1. Object Type Schemas\nA game object’s att ributes and behaviors are deﬁ ned by its type. In a game \nworld editor that employs a spawner-based design, a game object type can be \nrepresented by a data-driven schema that deﬁ nes the collection of att ributes \nthat should be visible to the user when creating or editing an object of that \ntype. At runtime, the tool-side object type can be mapped in either a hard-\ncoded or data-driven way to a class or collection of classes that must be instan-\ntiated in order to spawn a game object of the given type.\nType schemas can be stored in a simple text ﬁ le for consumption by the \nworld editor and for inspection and editing by its users. For example, a sche-\nma ﬁ le might look something like this:\nenum LightType\n{\n \nAmbient, Directional, Point, Spot\n}\ntype Light\n{\n String \n   UniqueId;\nLightType   Type;\n Vector \n   Pos;\n Quaternion  Rot;\n Float \n   Intensity \n: min(0.0), max(1.0);\n ColorARGB \n  DiffuseColor;\n ColorARGB \n  SpecularColor;\n ...\n}\ntype Vehicle\n{\n String \n   UniqueId;\n Vector \n   Pos;\n Quaternion  Rot;\n MeshReference \n Mesh;\n Int \n    NumWheels \n: min(2), max(4);\n",
      "content_length": 1333,
      "extraction_method": "Direct"
    },
    {
      "page_number": 761,
      "chapter": null,
      "content": "739 \n Float \n   TurnRadius;\n Float \n   TopSpeed \n: min(0.0);\n ...\n}\n...\nThe above example brings a few important details to light. You’ll notice \nthat the data types of each att ribute are deﬁ ned, in addition to their names. \nThese can be simple types like strings, integers, and ﬂ oating-point values, or \nthey can be specialized types like vectors, quaternions, ARGB colors, or ref-\nerences to special asset types like meshes, collision data, and so on. In this \nexample, we’ve even provided a mechanism for deﬁ ning enumerated types, \nlike LightType. Another subtle point is that the object type schema provides \nadditional information to the world editor, such as what type of GUI element \nto use when editing the att ribute. Sometimes an att ribute’s GUI requirements \nare implied by its data type—strings are generally edited with a text ﬁ eld, \nBooleans via a check box, vectors via three text ﬁ elds for the x-, y-, and z-\ncoordinates or perhaps via a specialized GUI element designed for manipu-\nlating vectors in 3D. The schema can also specify meta-information for use \nby the GUI, such as minimum and maximum allowable values for integer \nand ﬂ oating-point att ributes, lists of available choices for drop-down combo \nboxes, and so on.\nSome game engines permit object type schemas to be inherited , much like \nclasses. For example, every game object needs to know its type and must have \na unique id so that it can be distinguished from all the other game objects at \nruntime. These att ributes could be speciﬁ ed in a top-level schema, from which \nall other schemas are derived.\n14.3.3.2. Default Attribute Values\nAs you can well imagine, the number of att ributes in a typical game object \nschema can grow quite large. This translates into a lot of data that must be \nspeciﬁ ed by the game designer for each instance of each game object type he \nor she places into the game world. It can be extremely helpful to deﬁ ne default \nvalues in the schema for many of the att ributes. This permits game designers \nto place “vanilla” instances of a game object type with litt le eﬀ ort but still \npermits him or her to ﬁ ne-tune the att ribute values on speciﬁ c instances as \nneeded.\nOne inherent problem with default values arises when the default val-\nue of a particular att ribute changes. For example, our game designers might \nhave originally wanted Orcs to have 20 hit points. Aft er many months of pro-\n14.3. World Chunk Data Formats\n",
      "content_length": 2450,
      "extraction_method": "Direct"
    },
    {
      "page_number": 762,
      "chapter": null,
      "content": "740 \n14. Runtime Gameplay Foundation Systems\nduction, it might be decided that Orcs should have a more powerful 30 hit \npoints by default. Any new Orcs placed into a game world will now have 30 \nhit points unless otherwise speciﬁ ed. But what about all the Orcs that were \nplaced into game world chunks prior to the change? Do we need to ﬁ nd all of \nthese previously-created Orcs and manually change their hit points to 30?\nIdeally, we’d like to design our spawner system so that changes in de-\nfault values automatically propagate to all preexisting instances that have not \nhad their default values overridden explicitly. One easy way to implement \nthis feature is to simply omit key-value pairs for att ributes whose value does \nnot diﬀ er from the default value. Whenever an att ribute is missing from the \nspawner, the appropriate default can be used. (This presumes that the game \nengine has access to the object type schema ﬁ le, so that it can read in the at-\ntributes’ default values.) In our example, most of the preexisting Orc spawners \nwould have had no HitPoints key-value pair at all (unless of course one of the \nspawner’s hit points had been changed from the default value manually). So \nwhen the default value changes from 20 to 30, these Orcs will automatically \nuse the new value.\nSome engines allow default values to be overridden in derived object \ntypes. For example, the schema for a type called Vehicle might deﬁ ne a de-\nfault TopSpeed of 80 miles per hour. A derived Motorcycle type schema could \noverride this TopSpeed to be 100 miles per hour.\n14.3.3.3. Some Beneifts of Spawners and Type Schemas\n The key beneﬁ ts of separating the spawner from the implementation of the \ngame object are simplicity, ﬂ exibility, and robustness. From a data management \npoint of view, it is much simpler to deal with a table of key-value pairs than it \nis to manage a binary object image with pointer ﬁ x-ups or a custom serialized \nobject format. The key-value pairs approach also makes the data format ex-\ntremely ﬂ exible and robust to changes. If a game object encounters key-value \npairs that it is not expecting to see, it can simply ignore them. Likewise, if the \ngame object is unable to ﬁ nd a key-value pair that it needs, it has the option \nof using a default value instead. This makes a key-value pair data format ex-\ntremely robust to changes made by both the designers and the programmers.\nSpawners also simplify the design and implementation of the game world \neditor, because it only needs to know how to manage lists of key-value pairs \nand object type schemas. It doesn’t need to share code with the runtime game \nengine in any way, and it is only very loosely coupled to the engine’s imple-\nmentation details.\nSpawners and archetypes give game designers and programmers a great \ndeal of ﬂ exibility and power. Designers can deﬁ ne new game object type sche-\n",
      "content_length": 2891,
      "extraction_method": "Direct"
    },
    {
      "page_number": 763,
      "chapter": null,
      "content": "741 \nmas within the world editor with litt le or no programmer intervention. The \nprogrammer can implement the runtime implementation of these new object \ntypes whenever his or her schedule allows it. The programmer does not need \nto immediately provide an implementation of each new object type as it is \nadded in order to avoid breaking the game. New object data can exist safely in \nthe world chunk ﬁ les with or without a runtime implementation, and runtime \nimplementations can exist with or without corresponding data in the world \nchunk ﬁ le.\n14.4. Loading and Streaming Game Worlds\nTo bridge the gap between the oﬀ -line world editor and our runtime game \nobject model, we need a way to load world chunks into memory and un-\nload them when they are no longer needed. The game world loading sys-\ntem has two main responsibilities: to manage the ﬁ le I/O necessary to load \ngame world chunks and other needed assets from disk into memory and to \nmanage the allocation and deallocation of memory for these resources. The \nengine also needs to manage the spawning and destruction of game objects as \nthey come and go in the game, both in terms of allocating and deallocating \nmemory for the objects and ensuring that the proper classes are instantiated \nfor each game object. In the following sections, we’ll investigate how game \nworlds are loaded and also have a look at how object spawning systems typi-\ncally work.\n14.4.1. Simple Level Loading\n The most straightforward game world loading approach, and the one used by \nall of the earliest games, is to allow one and only one game world chunk (a.k.a. \nlevel) to be loaded at a time. When the game is ﬁ rst started, and between pairs \nof levels, the player sees a static or simply animated two-dimensional loading \nscreen while he or she waits for the level to load.\nMemory management in this kind of design is quite straightforward. As \nwe mentioned in Section 6.2.2.7, a stack-based allocator is very well-suited \nto a one-level-at-a-time world loading design. When the game ﬁ rst runs, any \nresource data that is required across all game levels is loaded at the bott om \nof the stack. I will call this load-and-stay-resident (LSR) data. The location of \nthe stack pointer is recorded aft er the LSR data has been fully loaded. Each \ngame world chunk, along with its associated mesh, texture, audio, anima-\ntion, and other resource data, is loaded on top of the LSR data on the stack. \nWhen the level has been completed by the player, all of its memory can be \n14.4. Loading and Streaming Game Worlds\n",
      "content_length": 2560,
      "extraction_method": "Direct"
    },
    {
      "page_number": 764,
      "chapter": null,
      "content": "742 \n14. Runtime Gameplay Foundation Systems\nfreed by simply resett ing the stack pointer to the top of the LSR data block. \nAt this point, a new level can be loaded in its place. This is illustrated in \nFigure 14.11.\nWhile this design is very simple, it has a number of drawbacks. For one \nthing, the player only sees the game world in discrete chunks—there is no \nway to implement a vast, contiguous, seamless world using this technique. \nAnother problem is that during the time the level’s resource data is being \nloaded, there is no game world in memory. So the player is forced to watch a \ntwo-dimensional loading screen of some sort.\n14.4.2. Toward Seamless Loading: Air Locks\nThe best way to avoid boring level-loading screens is to permit the player \nto continue playing the game while the next world chunk and its associated \nFigure 14.11.  A stack-based memory allocator is extremely well-suited to a one-level-at-a-\ntime world loading system.\n",
      "content_length": 954,
      "extraction_method": "Direct"
    },
    {
      "page_number": 765,
      "chapter": null,
      "content": "743 \nresource data are being loaded. One simple approach would be to divide the \nmemory that we’ve set aside for game world assets into two equally sized \nblocks. We could load level A into one memory block, allow the player to start \nplaying level A, and then load level B into the other block using a streaming \nﬁ le I/O library (i.e., the loading code would run in a separate thread ). The big \nproblem with this technique is that it cuts the size of each level in half relative \nto what would be possible with a one-level-at-a-time approach.\nWe can achieve a similar eﬀ ect by dividing the game world memory into \ntwo unequally-sized blocks—a large block that can contain a “full” game \nworld chunk and a small block that is only large enough to contain a tiny \nworld chunk. The small chunk is sometimes known as an “air lock .”\nWhen the game starts, a “full” chunk and an “air lock” chunk are loaded. \nThe player progresses through the full chunk and into the air lock, at which \npoint some kind of gate or other impediment ensures that the player can nei-\nther see the previous full world area nor return to it. The full chunk can then \nbe un-loaded, and a new full-sized world chunk can be loaded. During the \nload, the player is kept busy doing some task within the air lock. The task \nmight be as simple as walking from one end of a hallway to the other, or it \ncould be something more engaging, like solving a puzzle or ﬁ ghting some \nenemies.\nAsynchronous ﬁ le I/O is what enables the full world chunk to be loaded \nwhile the player is simultaneously playing in the air lock region. See Section \n6.1.3 for more details. It’s important to note that an air lock system does not free \nus from displaying a loading screen whenever a new game is started, because \nduring the initial load there is no game world in memory in which to play. \nHowever, once the player is in the game world, he or she needn’t see a loading \nscreen ever again, thanks to air locks and asynchronous data loading.\nHalo for the Xbox used a technique similar to this. The large world areas \nwere invariably connected by smaller, more conﬁ ned areas. As you play Halo, \nwatch for conﬁ ned areas that prevent you from back-tracking—you’ll ﬁ nd one \nroughly every 5-10 minutes of gameplay. Jak 2 for the PlayStation 2 used the \nair lock technique as well. The game world was structured as a hub area (the \nmain city) with a number of oﬀ -shoot areas, each of which was connected to \nthe hub via a small, conﬁ ned air lock region.\n14.4.3. Game World Streaming\nMany game designs call for the player to feel like he or she is playing in a \nhuge, contiguous, seamless world. Ideally, the player should not be conﬁ ned \nto small air lock regions periodically—it would be best if the world simply \nunfolded in front of the player as naturally and believably as possible.\n14.4. Loading and Streaming Game Worlds\n",
      "content_length": 2882,
      "extraction_method": "Direct"
    },
    {
      "page_number": 766,
      "chapter": null,
      "content": "744 \n14. Runtime Gameplay Foundation Systems\nModern game engines support this kind of seamless world by using a \ntechnique known as streaming . World streaming can be accomplished in vari-\nous ways. The main goals are always (a) to load data while the player is en-\ngaged in regular gameplay tasks and (b) to manage the memory in such a way \nas to eliminate fragmentation while permitt ing data to be loaded and unloaded \nas needed as the player progresses through the game world.\nRecent consoles and PCs have a lot more memory than their predecessors, \nso it is now possible to keep multiple world chunks in memory simultane-\nously. We could imagine dividing our memory space into, say, three equally \nsized buﬀ ers. At ﬁ rst, we load world chunks A, B, and C into these three buf-\nfers and allow the player to start playing through chunk A. When he or she \nenters chunk B and is far enough along that chunk A can no longer be seen, \nwe can unload chunk A and start loading a new chunk D into the ﬁ rst buﬀ er. \nWhen B can no longer be seen, it can be dumped and chunk E loaded. This \nrecycling of buﬀ ers can continue until the player has reached the end of the \ncontiguous game world.\nThe problem with a coarse-grained approach to world streaming is that \nit places onerous restrictions on the size of a world chunk. All chunks in the \nentire game must be roughly the same size—large enough to ﬁ ll up the major-\nity of one of our three memory buﬀ ers but never any larger.\nOne way around this problem is to employ a much ﬁ ner-grained subdivi-\nsion of memory. Rather than streaming relatively large chunks of the world, we \ncan divide every game asset, from game world chunks to foreground meshes \nto textures to animation banks, into equally-sized blocks of data. We can then \nuse a chunky, pool-based memory allocation system like the one described in \nSection 6.2.2.7 to load and unload resource data as needed without having to \nworry about memory fragmentation. This is the technique employed by the \nUncharted: Drake’s Fortune engine.\n14.4.3.1. Determining Which Resources to Load\nOne question that arises when using a ﬁ ne-grained chunky memory allocator \nfor world streaming is how the engine will know what resources to load at \nany given moment during gameplay. In Uncharted: Drake’s Fortune (UDF), we \nused a relatively simple system of level load regions to control the loading and \nunloading of assets.\nUDF is set in two geographically distinct, contiguous game worlds—\nthe jungle and the island. Each of these worlds exists in a single, consistent \nworld space, but they are divided up into numerous geographically adjacent \nchunks. A simple convex volume known as a region encompasses each of the \nchunks; the regions overlap each other somewhat. Each region contains a list \n",
      "content_length": 2795,
      "extraction_method": "Direct"
    },
    {
      "page_number": 767,
      "chapter": null,
      "content": "745 \nof the world chunks that should be in memory when the player is in that \nregion.\nAt any given moment, the player is within one or more of these regions. \nTo determine the set of world chunks that should be in memory, we simply \ntake the union of the chunk lists from each of the regions enclosing the Na-\nthan Drake character. The level loading system periodically checks this master \nchunk list and compares it against the set of world chunks that are currently \nin memory. If a chunk disappears from the master list, it is unloaded, thereby \nfreeing up all of the allocation blocks it occupied. If a new chunk appears in \nthe list, it is loaded into any free allocation blocks that can be found. The level \nload regions and world chunks are designed in such a way as to ensure that \nthe player never sees a chunk disappear when it is unloaded and that there’s \nenough time between the moment at which a chunk starts loading and the \nmoment its contents are ﬁ rst seen by the player to permit the chunk to be fully \nstreamed into memory. This technique is illustrated in Figure 14.12.\n14.4.4. Memory Management for Object Spawning\nOnce a game world has been loaded into memory, we need to manage the pro-\ncess of spawning the dynamic game objects in the world. Most game engines \nhave some kind of game object spawning system that manages the instantia-\ntion of the class or classes that make up each game object and handles destruc-\ntion of game objects when they are no longer needed. One of the central jobs of \nany object spawning system is to manage the dynamic allocation of memory \nfor newly spawned game objects. Dynamic allocation can be slow, so steps \nmust be taken to ensure allocations are as eﬃ  cient as possible. And because \ngame objects come in a wide variety of sizes, dynamically allocating them can \ncause memory to become fragmented , leading to premature out-of-memory \nconditions. There are a number of diﬀ erent approaches to game object memo-\nry management. We’ll explore a few common ones in the following sections.\n14.4. Loading and Streaming Game Worlds\n1\n2\n3\n4\n1\n2\nLevel 1\nLevel 2\n3\nLevel 2\nLevel 3\n4\nLevel 3\nLevel 4\nFigure 14.12.  A game world divided into chunks. Level load regions, each with a requested \nchunk list, are arranged in such a way as to guarantee that the player never sees a chunk pop \nin or out of view.\n",
      "content_length": 2359,
      "extraction_method": "Direct"
    },
    {
      "page_number": 768,
      "chapter": null,
      "content": "746 \n14. Runtime Gameplay Foundation Systems\n14.4.4.1. Off-Line Memory Allocation for Object Spawning\nSome game engines solve the problems of allocation speed and memory frag-\nmentation in a rather draconian way, by simply disallowing dynamic mem-\nory allocation during gameplay altogether. Such engines permit game world \nchunks to be loaded and unloaded dynamically, but they spawn in all dy-\nnamic game objects immediately upon loading a chunk. Thereaft er, no game \nobjects can be created or destroyed. You can think of this technique as obeying \na “law of conservation of game objects.” No game objects are created or de-\nstroyed once a world chunk has been loaded. \nThis technique avoids memory fragmentation because the memory re-\nquirements of all the game objects in a world chunk are (a) known a priori \nand (b) bounded. This means that the memory for the game objects can be \nallocated oﬀ -line by the world editor and included as part of the world chunk \ndata itself. All game objects are therefore allocated out of the same memory \nused to load the game world and its resources, and they are no more prone \nto fragmentation than any other loaded resource data. This approach also has \nthe beneﬁ t of making the game’s memory usage patt erns highly predictable. \nThere’s no chance that a large group of game objects is going to spawn into the \nworld unexpectedly, and cause the game to run out of memory.\nOn the downside, this approach can be quite limiting for game designers. \nDynamic object spawning can be simulated by allocating a game object in the \nworld editor but instructing it to be invisible and dormant when the world \nis ﬁ rst loaded. Later, the object can “spawn” by simply activating itself and \nmaking itself visible. But the game designers have to predict the total number \nof game objects of each type that they’ll need when the game world is ﬁ rst \ncreated in the world editor. If they want to provide the player with an inﬁ nite \nsupply of health packs, weapons, enemies, or some other kind of game object, \nthey either need to work out a way to recycle their game objects, or they’re \nout of luck.\n14.4.4.2. Dynamic Memory Management for Object Spawning\nGame designers would probably prefer to work with a game engine that sup-\nports true dynamic object spawning. Although this is more diﬃ  cult to imple-\nment than a static game object spawning approach, it can be implemented in \na number of diﬀ erent ways. \nAgain, the primary problem is memory fragmentation. Because diﬀ erent \ntypes of game objects (and sometimes even diﬀ erent instances of the same \ntype of object) occupy diﬀ erent amounts of memory, we cannot use our fa-\nvorite fragmentation-free allocator—the pool allocator. And because game ob-\njects are generally destroyed in a diﬀ erent order than that in which they were \n",
      "content_length": 2823,
      "extraction_method": "Direct"
    },
    {
      "page_number": 769,
      "chapter": null,
      "content": "747 \nspawned, we cannot use a stack-based allocator either. Our only choice ap-\npears to be a fragmentation-prone heap allocator. Thankfully, there are many \nways to deal with the fragmentation problem. We’ll investigate a few common \nones in the following sections.\nOne Memory Pool per Object Type\nIf the individual instances of each game object type are guaranteed to all occu-\npy the same amount of memory, we could consider using a separate memory \npool for each object type. Actually, we only need one pool per unique game \nobject size, so object types of the same size can share a single pool.\nDoing this allows us to completely avoid memory fragmentation, but \none limitation of this approach is that we need to maintain lots of separate \npools. We also need to make educated guesses about how many of each type \nof object we’ll need. If a pool has too many elements, we end up wasting \nmemory; if it has too few, we won’t be able to satisfy all of the spawn requests \nat runtime, and game objects will fail to spawn. That said, many commer-\ncial game engines do successfully employ this kind of memory management \ntechnique.\nSmall Memory Allocators\nWe can transform the idea of one pool per game object type into something \nmore workable by allowing a game object to be allocated out of a pool whose \nelements are larger than the object itself. This can reduce the number of unique \nmemory pools we need signiﬁ cantly, at the cost of some potentially wasted \nmemory in each pool.\nFor example, we might create a set of pool allocators, each one with ele-\nments that are twice as large as those of its predecessor—perhaps 8, 16, 32, \n64, 128, 256, and 512 bytes. We can also use a sequence of element sizes that \nconforms to some other suitable patt ern or base the list of sizes on allocation \nstatistics collected from the running game.\nWhenever we try to allocate a game object, we search for the smallest pool \nwhose elements are larger than or equal to the size of the object we’re allocat-\ning. We accept that for some objects, we’ll be wasting space. In return, we al-\nleviate all of our memory fragmentation problems—a reasonably fair trade. If \nwe ever encounter a memory allocation request that is larger than our largest \npool, we can always turn it over to the general-purpose heap allocator, know-\ning that fragmentation of large memory blocks is not nearly as problematic as \nfragmentation involving tiny blocks.\nThis type of allocator is sometimes called a small memory allocator . It can \neliminate fragmentation (for allocations that ﬁ t into one of the pools). It can \nalso speed up memory allocations signiﬁ cantly for small chunks of data, be-\n14.4. Loading and Streaming Game Worlds\n",
      "content_length": 2710,
      "extraction_method": "Direct"
    },
    {
      "page_number": 770,
      "chapter": null,
      "content": "748 \n14. Runtime Gameplay Foundation Systems\ncause a pool allocation involves two pointer manipulations to remove the ele-\nment from the linked list of free elements—a much less-expensive operation \nthan a general-purpose heap allocation.\nMemory Relocation\nAnother way to eliminate fragmentation is to att ack the problem directly. This \napproach is known as memory relocation . It involves shift ing allocated memory \nblocks down into adjacent free holes to remove fragmentation. Moving the \nmemory is easy, but because we are moving “live” allocated objects, we need \nto be very careful about ﬁ xing up any pointers into the memory blocks we \nmove. See Section 5.2.2.2 for more details.\n14.4.5. Saved Games\n Many games allow the player to save his or her progress, quit the game, and \nthen load up the game at a later time in exactly the state he or she left  it. A saved \ngame system is similar to the world chunk loading system in that it is capable \nof loading the state of the game world from a disk ﬁ le or memory card. But the \nrequirements of this system diﬀ er somewhat from those of a world loading \nsystem, so the two are usually distinct (or overlap only partially).\nTo understand the diﬀ erences between the requirements of these two sys-\ntems, let’s brieﬂ y compare world chunks to saved game ﬁ les. World chunks \nspecify the initial conditions of all dynamic objects in the world, but they also \ncontain a full description of all static world elements. Much of the static infor-\nmation, such as background meshes and collision data, tends to take up a lot \nof disk space. As such, world chunks are sometimes comprised of multiple \ndisk ﬁ les, and the total amount of data associated with a world chunk is usu-\nally large.\nA saved game ﬁ le must also store the current state information of the \ngame objects in the world. However, it does not need to store a duplicate copy \nof any information that can be determined by reading the world chunk data. \nFor example, there’s no need to save out the static geometry in a saved game \nﬁ le. A saved game need not store every detail of every object’s state either. \nSome objects that have no impact on gameplay can be omitt ed altogether. For \nthe other game objects, we may only need to store partial state information. \nAs long as the player can’t tell the diﬀ erence between the state of the game \nworld before and aft er it has been saved and reloaded (or if the diﬀ erences \nare irrelevant to the player), then we have a successful saved game system. \nAs such, saved game ﬁ les tend to be much smaller than world chunk ﬁ les and \nmay place more of an emphasis on data compression and omission. Small ﬁ le \nsizes are especially important when numerous saved game ﬁ les must ﬁ t onto \n",
      "content_length": 2746,
      "extraction_method": "Direct"
    },
    {
      "page_number": 771,
      "chapter": null,
      "content": "749 \nthe tiny memory cards that were used on older consoles. But even today, with \nconsoles that are equipped with large hard drives, it’s still a good idea to keep \nthe size of a saved game ﬁ le as small as possible.\n14.4.5.1. Check Points\nOne approach to save games is to limit saves to speciﬁ c points in the game, \nknown as check points . The beneﬁ t of this approach is that most of the knowl-\nedge about the state of the game is saved in the current world chunk(s) in the \nvicinity of each check point. This data is always exactly the same, no matt er \nwhich player is playing the game, so it needn’t be stored in the saved game. \nAs a result, saved game ﬁ les based on check points can be extremely small. We \nmight need to store only the name of the last check point reached, plus per-\nhaps some information about the current state of the player character, such as \nthe player’s health, number of lives remaining, what items he has in his inven-\ntory, which weapon(s) he has, and how much ammo each one contains. Some \ngames based on check points don’t even store this information—they start the \nplayer oﬀ  in a known state at each check point. Of course, the downside of a \ngame based on check points is the possibility of user frustration, especially if \ncheck points are few and far between.\n14.4.5.2. Save Anywhere\nSome games support a feature known as save anywhere . As the name implies, \nsuch games permit the state of the game to be saved at literally any point dur-\ning play. To implement this feature, the size of the saved game data ﬁ le must \nincrease signiﬁ cantly. The current locations and internal states of every game \nobject whose state is relevant to gameplay must be saved and then restored \nwhen the game is loaded again later.\nIn a save anywhere design, a saved game data ﬁ le contains basically the \nsame information as a world chunk, minus the world’s static components. It \nis possible to utilize the same data format for both systems, although there \nmay be factors that make this infeasible. For example, the world chunk data \nformat might be designed for ﬂ exibility, but the saved game format might be \ncompressed to minimize the size of each saved game.\nAs we’ve mentioned, one way to reduce the amount of data that needs to \nbe stored in a saved game ﬁ le is to omit certain irrelevant game objects and to \nomit some irrelevant details of others. For example, we needn’t remember the \nexact time index within every animation that is currently playing or the exact \nmomentums and velocities of every physically simulated rigid body. We can \nrely on the imperfect memories of human gamers and save only a rough ap-\nproximation to the game’s state.\n14.4. Loading and Streaming Game Worlds\n",
      "content_length": 2726,
      "extraction_method": "Direct"
    },
    {
      "page_number": 772,
      "chapter": null,
      "content": "750 \n14. Runtime Gameplay Foundation Systems\n14.5. Object References and World Queries\nEvery game object generally requires some kind of unique id so that it can be \ndistinguished from the other objects in the game, found at runtime, serve as a \ntarget of inter-object communication, and so on. Unique object ids are equally \nhelpful on the tool side, as they can be used to identify and ﬁ nd game objects \nwithin the world editor .\nAt runtime, we invariably need various ways to ﬁ nd game objects. We \nmight want to ﬁ nd an object by its unique id, by its type, or by a set of arbi-\ntrary criteria. We oft en need to perform proximity-based queries, for example \nﬁ nding all enemy aliens within a 10 meter radius of the player character.\nOnce a game object has been found via a query , we need some way to re-\nfer to it. In a language like C or C++, object references might be implemented \nas pointers, or we might use something more sophisticated, like handles or \nsmart pointers . The lifetime of an object reference can vary widely, from the \nscope of a single function call to a period of many minutes.\nIn the following sections, we’ll ﬁ rst investigate various ways to implement \nobject references. Then we’ll explore the kinds of queries we oft en require when \nimplementing gameplay and how those queries might be implemented.\n14.5.1. Pointers\nIn C or C++, the most straightforward way to implement an object reference is \nvia a pointer (or a reference in C++). Pointers are powerful and are just about \nas simple and intuitive as you can get. However, pointers suﬀ er from a num-\nber of problems:\nz Orphaned objects. Ideally, every object should have an owner—another \nobject that is responsible for managing its lifetime—creating it and then \ndeleting it when it is no longer needed. But pointers don’t give the pro-\ngrammer any help in enforcing this rule. The result can be an orphaned \nobject—an object that still occupies memory but is no longer needed or \nreferenced by any other object in the system.\nz Stale pointers . If an object is deleted, ideally we should null-out any and \nall pointers to that object. If we forget to do so, however, we end up \nwith a stale pointer—a pointer to a block of memory that used to be \noccupied by a valid object but is now free memory. If anyone tries to \nread or write data through a stale pointer, the result can be a crash or \nincorrect program behavior. Stale pointers can be diﬃ  cult to track down \nbecause they may continue to work for some time aft er the object has \ndeleted. Only much later, when a new object is allocated on top of the \nstale memory block, does the data actually change and cause a crash.\n",
      "content_length": 2670,
      "extraction_method": "Direct"
    },
    {
      "page_number": 773,
      "chapter": null,
      "content": "751 \nz Invalid pointers . A programmer is free to store any address in a pointer, \nincluding a totally invalid address. A common problem is dereferencing \na null pointer. These problems can be guarded against by using asser-\ntion macros to check that pointers are never null prior to dereferencing \nthem. Even worse, if a piece of data is misinterpreted as a pointer, deref-\nerencing it can cause the program to read or write an essentially random \nmemory address. This usually results in a crash or other major problem \nthat can be very tough to debug.\nMany game engines make heavy use of pointers, because they are by far \nthe fastest, most eﬃ  cient, and easiest-to-work-with way to implement object \nreferences. However, experienced programmers are always wary of pointers, \nand some game teams turn to more sophisticated kinds of object references, \neither out of a desire to use safer programming practices or out of necessity. \nFor example, if a game engine relocates allocated data blocks at runtime to \neliminate memory fragmentation (see Section 5.2.2.2), simple pointers cannot \nbe used. We either need to use a type of object reference that is robust to mem-\nory relocation, or we need to manually ﬁ x up any pointers into every relocated \nmemory block at the time it is moved.\n14.5.2. Smart Pointers\nA smart pointer is a small object that acts like a pointer for most intents and pur-\nposes but avoids most of the problems inherent with native C/C++ pointers. At \nits simplest, a smart pointer contains a native pointer as a data member and \nprovides a set of overloaded operators that make it act like a pointer in most \nways. Pointers can be dereferenced, so the * and -> operators are overloaded \nto return the address as expected. Pointers can undergo arithmetic operations, \nso the +, -, ++, and -- operators are also overloaded appropriately.\nBecause a smart pointer is an object, it can contain additional meta-data \nand/or take additional steps not possible with a regular pointer. For example, a \nsmart pointer might contain information that allows it to recognize when the ob-\nject to which it points has been deleted and start returning a NULL address if so.\nSmart pointers can also help with object lifetime management by cooper-\nating with one another to determine the number of references to a particular \nobject. This is called reference counting. When the number of smart pointers \nthat reference a particular object drops to zero, we know that the object is no \nlonger needed, so it can be automatically deleted. This can free the program-\nmer from having to worry about object ownership and orphaned objects.\nSmart pointers have their share of problems. For one thing, they are rela-\ntively easy to implement, but they are extremely tough to get right. There are \na great many cases to handle, and the std::auto_ptr class provided by the \n14.5. Object References and World Queries\n",
      "content_length": 2912,
      "extraction_method": "Direct"
    },
    {
      "page_number": 774,
      "chapter": null,
      "content": "752 \n14. Runtime Gameplay Foundation Systems\nstandard C++ library is widely recognized to be inadequate in many situa-\ntions. The Boost C++ template library provides six diﬀ erent varieties of smart \npointers:\nz scoped_ptr. A pointer to a single object with one owner.\nz scoped_array. A pointer to an array of objects with one owner.\nz shared_ptr. A pointer to an object whose lifetime is shared by multiple \nowners.\nz shared_array. A pointer to an array of objects whose lifetimes are \nshared by multiple owners.\nz weak_ptr. A pointer that does not own or automatically destroy the \nobject it references (whose lifetime is assumed to be managed by a \nshared_ptr).\nz intrusive_ptr. A pointer that implements reference counting by as-\nsuming that the pointed-to object will maintain the reference count it-\nself. Intrusive pointers are useful because they are the same size as a na-\ntive C++ pointer (because no reference-counting apparatus is required) \nand because they can be constructed directly from native pointers.\nProperly implementing a smart pointer class can be a daunting task. Have \na glance at the Boost smart pointer documentation (htt p://www.boost.org/\ndoc/libs/1_36_0/libs/smart_ptr/smart_ptr.htm) to see what I mean. All sorts of \nissues come up, including:\nz type safety of smart pointers,\nz the ability for a smart pointer to be used with an incomplete type,\nz correct smart pointer behavior when an exception occurs,\nz runtime costs, which can be high.\nI have worked on a project that att empted to implement its own smart point-\ners, and we were ﬁ xing all sorts of nasty bugs with them up until the very end \nof the project. My personal recommendation is to stay away from smart point-\ners, or if you must use them, use a mature implementation such as Boost’s \nrather than trying to roll your own.\n14.5.3. Handles\nA handle acts like a smart pointer in many ways, but it is simpler to implement \nand tends to be less prone to problems. A handle is basically an integer index \ninto a global handle table. The handle table, in turn, contains pointers to the \nobjects to which the handles refer. To create a handle, we simply search the \nhandle table for the address of the object in question and store its index in the \nhandle. To dereference a handle, the calling code simply indexes the appropri-\n",
      "content_length": 2319,
      "extraction_method": "Direct"
    },
    {
      "page_number": 775,
      "chapter": null,
      "content": "753 \nate slot in the handle table and dereferences the pointer it ﬁ nds there. This is \nillustrated in Figure 14.13.\nBecause of the simple level of indirection aﬀ orded by the handle table, \nhandles are much safer and more ﬂ exible than raw pointers. If an object is \ndeleted, it can simply null out its entry in the handle table. This causes all \nexisting handles to the object to be immediately and automatically converted \nto null references. Handles also support memory relocation. When an object \nis relocated in memory, its address can be found in the handle table and up-\ndated appropriately. Again, all existing handles to the object are automatically \nupdated as a result.\nA handle can be implemented as a raw integer. However, the handle table \nindex is usually wrapped in a simple class so that a convenient interface for \ncreating and dereferencing the handle can be provided. \nHandles are prone to the possibility of referencing a stale object. For ex-\nample, let’s say we create a handle to object A, which occupies slot 17 in the \nhandle table. Later, object A is deleted, and slot 17 is nulled out. Later still, a \nnew object B is created, and it just happens to occupy slot 17 in the handle \ntable. If there are still any handles to object A lying around when object B is \ncreated, they will suddenly start referring to object B (instead of null). This is \nalmost certainly not desirable behavior.\nOne simple solution to the stale object problem is to include a unique \nobject id in each handle. That way, when a handle to object A is created, it con-\ntains not only slot index 17, but the object id “A.” When object B takes A’s place \nin the handle table, any left -over handles to A will agree on the handle index \nbut disagree on the object id. This allows stale object A handles to continue \n14.5. Object References and World Queries\nFigure 14.13.  A handle table contains raw object pointers. A handle is simply an index into \nthis table.\nNULL\nNULL\nObject1\nObject2\nObject3\nObject4\nObject5\nHandle Table\nm_handleIndex == 6\n0\n1\n2\n3\n4\n5\n6\nHandle to Object 5\n",
      "content_length": 2076,
      "extraction_method": "Direct"
    },
    {
      "page_number": 776,
      "chapter": null,
      "content": "754 \n14. Runtime Gameplay Foundation Systems\nto return null when dereferenced rather than returning a pointer to object B \nunexpectedly.\nThe following code snippet shows how a simple handle class might be \nimplemented. Notice that we’ve also included the handle index in the Game\nObject class itself—this allows us to create new handles to a GameObject\nvery quickly without having to search the handle table for its address to de-\ntermine its handle table index.\n// Within the GameObject class, we store a unique id, \n// and also the object’s handle index, for efficient \n// creation of new handles.\nclass GameObject\n{\nprivate:\n \n// ...\n GameObjectId \nm_uniqueId;       // object’s unique id\n \nU32  \n \n   m_handleIndex;    // speedier handle  \n \n   // \ncreation\n \nfriend class GameObjectHandle; // access to id and  \n \n           \n// index\n \n// ...\npublic:\nGameObject() // constructor\n {\n \n \n// The unique id might come from the world editor,  \n \n \n// or it might be assigned dynamically at runtime.\n  m_uniqueId \n= AssignUniqueObjectId();\n \n \n// The handle index is assigned by finding the  \n \n \n \n// first free slot in the handle table.\n  m_handleIndex \n= FindFreeSlotInHandleTable();\n  // \n...\n }\n \n// ...\n};\n// This constant defines the size of the handle table, \n// and hence the maximum number of game objects that can \n// exist at any one time.\nstatic const U32 MAX_GAME_OBJECTS = ...;\n// This is the global handle table -- a simple array of\n// pointers to GameObjects.\nstatic GameObject* g_apGameObject[MAX_GAME_OBJECTS];\n",
      "content_length": 1530,
      "extraction_method": "Direct"
    },
    {
      "page_number": 777,
      "chapter": null,
      "content": "755 \n// This is our simple game object handle class.\nclass GameObjectHandle\n{\nprivate:\n U32 \nm_handleIndex;        // index into the handle  \n \n         \n      // \ntable\n GameObjectId \nm_uniqueId;  // unique id avoids stale   \n         \n      // \nhandles\npublic:\n explicit \nGameObjectHandle(GameObject& object) :\n  m_handleIndex(object.m_handleIndex),\n  m_uniqueId(object.m_uniqueId)\n {\n }\n \n// This function dereferences the handle.\n GameObject* \nToObject() const\n {\n  GameObject* \npObject \n   = \ng_apGameObject[m_handleIndex];\n \n \nif (pObject != NULL\n \n \n&&  pObject->m_uniqueId == m_uniqueId)\n  {\n   return \npObject;\n  }\n  return \nNULL;\n }\n};\nThis example is functional but incomplete. We might want to implement copy \nsemantics, provide additional constructor variants, and so on. The entries in \nthe global handle table might contain additional information, not just a raw \npointer to each game object. And of course, a ﬁ xed-size handle table imple-\nmentation like this one isn’t the only possible design; handle systems vary \nsomewhat from engine to engine.\nWe should note that one fortunate side beneﬁ t of a global handle table is \nthat it gives us a ready-made list of all active game objects in the system. The \nglobal handle table can be used to quickly and eﬃ  ciently iterate over all game \nobjects in the world, for example. It can also make implementing other kinds \nof queries easier in some cases.\n14.5.4. Game Object Queries\nEvery game engine provides at least a few ways to ﬁ nd game objects at run-\ntime. We’ll call these searches game object queries . The simplest type of query is \nto ﬁ nd a particular game object by its unique id. However, a real game engine \n14.5. Object References and World Queries\n",
      "content_length": 1727,
      "extraction_method": "Direct"
    },
    {
      "page_number": 778,
      "chapter": null,
      "content": "756 \n14. Runtime Gameplay Foundation Systems\nmakes many other types of game object queries. Here are just a few examples \nof the kinds of queries a game developer might want to make:\nz Find all enemy characters with line of sight to the player.\nz Iterate over all game objects of a certain type.\nz Find all destructible game objects with more than 80% health.\nz Transmit damage to all game objects within the blast radius of an \nexplosion.\nz Iterate over all objects in the path of a bullet or other projectile, in near-\nest-to-farthest order.\nThis list could go on for many pages, and of course its contents are highly \ndependent upon the design of the particular game being made.\nFor maximum ﬂ exibility in performing game object queries, we could \nimagine a general-purpose game object database, complete with the ability to \nformulate arbitrary queries using arbitrary search criteria. Ideally, our game \nobject database would perform all of these queries extremely eﬃ  ciently and \nrapidly, making maximum use of whatever hardware and soft ware resources \nare available.\nIn reality, such an ideal combination of ﬂ exibility and blinding speed is \ngenerally not possible. Instead, game teams usually determine which types \nof queries are most likely to be needed during development of the game, and \nspecialized data structures are implemented to accelerate those particular \ntypes of queries. As new queries become necessary, the engineers either lever-\nage preexisting data structures to implement them, or they invent new ones \nif suﬃ  cient speed cannot be obtained. Here are a few examples of specialized \ndata structures that can accelerate speciﬁ c types of game object queries:\nz Finding game objects by unique id . Pointers or handles to the game objects \ncould be stored in a hash table or binary search tree keyed by unique \nid.\nz Iterating over all objects that meet a particular criterion. The game objects \ncould be presorted into linked lists based on various criteria (presuming \nthe criteria are known a priori). For example, we might construct a list of \nall game objects of a particular type, maintain a list of all objects within \na particular radius of the player, etc.\nz Finding all objects in the path of a projectile or with line of sight to some target \npoint. The collision system is usually leveraged to perform these kinds of \ngame object queries. Most collision systems provide ultra-fast ray casts, \nand some also provide the ability to cast other shapes such as spheres or \narbitrary convex volumes into the world to determine what they hit.\n",
      "content_length": 2577,
      "extraction_method": "Direct"
    },
    {
      "page_number": 779,
      "chapter": null,
      "content": "757 \nz Finding all objects within a given region or radius. We might consider stor-\ning our game objects in some kind of spatial hash data structure. This \ncould be as simple as a horizontal grid placed over the entire game \nworld or something more sophisticated, such as a quadtree, octt ree, kd-\ntree, or other data structure that encodes spatial proximity.\n14.6. Updating Game Objects in Real Time\n Every game engine, from the simplest to the most complex, requires some \nmeans of updating the internal state of every game object over time. The state \nof a game object can be deﬁ ned as the values of all its att ributes (sometimes \ncalled its properties, and called data members in the C++ language). For example, \nthe state of the ball in Pong is described by its (x, y) position on the screen \nand its velocity (speed and direction of travel). Because games are dynamic, \ntime-based simulations, a game object’s state describes its conﬁ guration at one \nspeciﬁ c instant in time. In other words, a game object’s notion of time is discrete \nrather than continuous. (However, as we’ll see, it’s helpful to think of the ob-\njects’ states as changing continuously and then being sampled discretely by \nthe engine, because it helps you to avoid some common pitfalls.)\nIn the following discussions, we’ll use the symbol Si(t) to denote the state \nof object i at an arbitrary time t. The use of vector notation here is not strictly \nmathematically correct, but it reminds us that a game object’s state acts like \na heterogeneous n-dimensional vector, containing all sorts of information of \nvarious data types. We should note that this usage of the term “state” is not \nthe same as the states in a ﬁ nite state machine . A game object may very well \nbe implemented in terms of one—or many—ﬁ nite state machines, but in that \ncase, a speciﬁ cation of the current state of each FSM would merely be a part of \nthe game object’s overall state vector S(t).\nMost low-level engine subsystems (rendering, animation, collision, \nphysics, audio, and so on) require periodic updating, and the game object \nsystem is no exception. As we saw in Chapter 7, updating is usually done via \na single master loop called the game loop (or possibly via multiple game loops , \neach running in a separate thread ). Virtually all game engines update game \nobject states as part of their main game loop—in other words, they treat the \ngame object model as just another engine subsystem that requires periodic \nservicing.\nGame object updating can therefore be thought of as the process of de-\ntermining the state of each object at the current time Si(t) given its state at a \nprevious time Si(t – Δt). Once all object states have been updated, the current \ntime t becomes the new previous time (t – Δt), and this process repeats for \n14.6. Updating Game Objects in Real Time\n",
      "content_length": 2848,
      "extraction_method": "Direct"
    },
    {
      "page_number": 780,
      "chapter": null,
      "content": "758 \n14. Runtime Gameplay Foundation Systems\nas long as the game is running. Usually, one or more clocks are maintained \nby the engine—one that tracks real time exactly and possibly others that \nmay or may not correspond to real time. These clocks provide the engine \nwith the absolute time t and/or with the change in time Δt from iteration \nto iteration of the game loop. The clock that drives the updating of game \nobject states is usually permitt ed to diverge from real time. This allows the \nbehaviors of the game objects to be paused, slowed down, sped up, or even \nrun in reverse—whatever is required in order to suit the needs of the game \ndesign. These features are also invaluable for debugging and development \nof the game.\nAs we mentioned in Chapter 1, a game object updating system is an ex-\nample of what is known as a dynamic, real-time, agent-based computer simulation \nin computer science. Game object updating systems also exhibit some aspects \nof discrete event simulations (see Section 14.7 for more details on events). These \nare well-researched areas of computer science, and they have many appli-\ncations outside the ﬁ eld of interactive entertainment. Games are one of the \nmore-complex kinds of agent-based simulation—as we’ll see, updating game \nobject states over time in a dynamic, interactive virtual environment can be \nsurprisingly diﬃ  cult to get right. Game programmers can learn a lot about \ngame object updating by studying the wider ﬁ eld of agent-based and discrete \nevent simulations. And researchers in those ﬁ elds can probably learn a thing \nor two from game engine design as well!\nAs with all high-level game engine systems, every engine takes a slightly \n(or sometimes radically) diﬀ erent approach. However, as before, most game \nteams encounter a common set of problems, and certain design patt erns tend \nto crop up again and again in virtually every engine. In this section, we’ll \ninvestigate these common problems and some common solutions to them. \nPlease bear in mind that game engines may exist that employ very diﬀ er-\nent solutions to the ones described here, and some game designs face unique \nproblems that we can’t possibly cover here.\n14.6.1. A Simple Approach (That Doesn’t Work)\nThe simplest way to update the states of a collection of game objects is to \niterate over the collection and call a virtual function, named something like \nUpdate(), on each object in turn. This is typically done once during each \niteration of the main game loop (i.e., once per frame). Game object classes can \nprovide custom implementations of the Update() function in order to per-\nform whatever tasks are required to advance the state of that type of object \nto the next discrete time index. The time delta from the previous frame can \nbe passed to the update function so that objects can take proper account of \n",
      "content_length": 2858,
      "extraction_method": "Direct"
    },
    {
      "page_number": 781,
      "chapter": null,
      "content": "759 \nthe passage of time. At its simplest, then, our Update() function’s signature \nmight look something like this:\nvirtual void Update(float dt);\nFor the purposes of the following discussions, we’ll assume that our en-\ngine employs a monolithic object hierarchy, in which each game object is rep-\nresented by a single instance of a single class. However, we can easily extend \nthe ideas here to virtually any object-centric design. For example, to update a \ncomponent-based object model, we could call Update() on every component \nthat makes up each game object, or we could call Update() on the “hub” \nobject and let it update its associated components as it sees ﬁ t. We can also ex-\ntend these ideas to property-centric designs, by calling some sort of Update()\nfunction on each property instance every frame.\nThey say that the devil is in the details, so let’s investigate two important \ndetails here. First, how should we maintain the collection of all game objects? \nAnd second, what kinds of things should the Update() function be respon-\nsible for doing?\n14.6.1.1. Maintaining a Collection of Active Game Objects\nThe collection of active game objects is oft en maintained by a singleton \nmanager class, perhaps named something like GameWorld or GameObject\nManager. The collection of game objects generally needs to be dynamic, be-\ncause game objects are spawned and destroyed as the game is played. Hence a \nlinked list of pointers, smart pointers, or handles to game objects is one simple \nand eﬀ ective approach. (Some game engines disallow dynamic spawning and \ndestroying of game objects; such engines can use a statically-sized array of \ngame object pointers, smart pointers, or handles rather than a linked list.) As \nwe’ll see below, most engines use more-complex data structures to keep track \nof their game objects rather than just a simple, ﬂ at linked list. But for the time \nbeing, we can visualize the data structure as a linked list for simplicity.\n14.6.1.2. Responsibilities of the Update() Function\nA game object’s Update() function is primarily responsible for determining \nthe state of that game object at the current discrete time index Si(t) given its \nprevious state Si(t – Δt). Doing this may involve applying a rigid body dynam-\nics simulation to the object, sampling a preauthored animation, reacting to \nevents that have occurred during the current time step, and so on.\nMost game objects interact with one or more engine subsystems. They \nmay need to animate , be rendered, emit particle eﬀ ects, play audio , collide \nwith other objects and static geometry, and so on. Each of these systems has \nan internal state that must also be updated over time, usually once or a few \n14.6. Updating Game Objects in Real Time\n",
      "content_length": 2751,
      "extraction_method": "Direct"
    },
    {
      "page_number": 782,
      "chapter": null,
      "content": "760 \n14. Runtime Gameplay Foundation Systems\ntimes per frame. It might seem reasonable and intuitive to simply update all of \nthese subsystems directly from within the game object’s Update() function. \nFor example, consider the following hypothetical update function for a Tank\nobject:\nvirtual void Tank::Update(float dt)\n{\n // \nUpdate the state of the tank itself.\n MoveTank(dt);\n DeflectTurret(dt);\n FireIfNecessary();\n \n// Now update low-level engine subsystems on behalf\n \n// of this tank. (NOT a good idea... see below!)\n m_pAnimationComponent->Update(dt);\n m_pCollisionComponent->Update(dt);\n m_pPhysicsComponent->Update(dt);\n m_pAudioComponent->Update(dt);\n m_pRenderingComponent->draw();\n}\nGiven that our Update() functions are structured like this, the game loop \ncould be driven almost entirely by the updating of the game objects, like this:\nwhile (true)\n{\n PollJoypad();\n \nfloat dt = g_gameClock.CalculateDeltaTime();\n \nfor (each gameObject)\n {\n \n \n// This hypothetical Update() function updates\n \n \n// all engine subsystems!\ngameObject.Update(dt);\n }\n g_renderingEngine.SwapBuffers();\n}\nHowever att ractive the simple approach to object updating shown above \nmay seem, it is usually not viable in a commercial-grade game engine. In the \nfollowing sections, we’ll explore some of the problems with this simplistic ap-\nproach and investigate common ways in which each problem can be solved.\n",
      "content_length": 1402,
      "extraction_method": "Direct"
    },
    {
      "page_number": 783,
      "chapter": null,
      "content": "761 \n14.6.2. Performance Constraints and Batched Updates\nMost low-level engine systems have extremely stringent performance con-\nstraints. They operate on a large quantity of data, and they must do a large \nnumber of calculations every frame as quickly as possible. As a result, most \nengine systems beneﬁ t from batched updating. For example, it is usually far \nmore eﬃ  cient to update a large number of animations in one batch than it is \nto update each object’s animation interleaved with other unrelated operations, \nsuch as collision detection, physical simulation, and rendering.\nIn most commercial game engines, each engine subsystem is updated di-\nrectly or indirectly by the main game loop rather than being updated on a \nper-game object basis from within each object’s Update() function. If a game \nobject requires the services of a particular engine subsystem, it asks that sub-\nsystem to allocate some subsystem-speciﬁ c state information on its behalf. For \nexample, a game object that wishes to be rendered via a triangle mesh might \nrequest the rendering subsystem to allocate a mesh instance for its use. (A mesh \ninstance represents a single instance of a triangle mesh—it keeps track of the \nposition, orientation, and scale of the instance in world space whether or not \nit is visible, per-instance material data, and any other per-instance information \nthat may be relevant.) The rendering engine maintains a collection of mesh in-\nstances internally. It can manage the mesh instances however it sees ﬁ t in order \nto maximize its own runtime performance. The game object controls how it is \nrendered by manipulating the properties of the mesh instance object, but the \ngame object does not control the rendering of the mesh instance directly. In-\nstead, aft er all game objects have had a chance to update themselves, the ren-\ndering engine draws all visible mesh instances in one eﬃ  cient batch update.\nWith batched updating, a particular game object’s Update() function, \nsuch as that of our hypothetical tank object, might look more like this:\nvirtual void Tank::Update(float dt)\n{\n // \nUpdate the state of the tank itself.\n MoveTank(dt);\n DeflectTurret(dt);\n FireIfNecessary();\n \n// Control the properties of my various engine  \n \n \n \n// subsystem components, but do NOT update \n \n// them here...\n \nif (justExploded)\n {\n  m_pAnimationComponent->PlayAnimation(\"explode\");\n }\n14.6. Updating Game Objects in Real Time\n",
      "content_length": 2442,
      "extraction_method": "Direct"
    },
    {
      "page_number": 784,
      "chapter": null,
      "content": "762 \n14. Runtime Gameplay Foundation Systems\n \nif (isVisible)\n {\n  m_pCollisionComponent->Activate();\n  m_pRenderingComponent->Show();\n }\n else\n {\n  m_pCollisionComponent->Deactivate();\n  m_pRenderingComponent->Hide();\n }\n \n// etc. \n}\nThe game loop then ends up looking more like this:\nwhile (true)\n{\n PollJoypad();\n \nfloat dt = g_gameClock.CalculateDeltaTime();\n \nfor (each gameObject)\n {\ngameObject.Update(dt);\n }\n g_animationEngine.\nUpdate(dt);\n g_physicsEngine.\nSimulate(dt);\n g_collisionEngine.\nDetectAndResolveCollisions(dt);\n g_audioEngine.\nUpdate(dt);\n g_renderingEngine.\nRenderFrameAndSwapBuffers();\n}\nBatched updating provides many performance beneﬁ ts, including but not \nlimited to:\nz Maximal cache coherency . Batched updating allows an engine subsys-\ntem to achieve maximum cache coherency because its per-object data \nis maintained internally and can be arranged in a single, contiguous \nregion of RAM.\nz Minimal duplication of computations. Global calculations can be done once \nand reused for many game objects rather than being redone for each \nobject.\n",
      "content_length": 1071,
      "extraction_method": "Direct"
    },
    {
      "page_number": 785,
      "chapter": null,
      "content": "763 \nz Reduced reallocation of resources. Engine subsystems oft en need to allocate \nand manage memory and/or other resources during their updates. If \nthe update of a particular subsystem is interleaved with those of other \nengine subsystems, these resources must be freed and reallocated for \neach game object that is processed. But if the updates are batched, the \nresources can be allocated once per frame and reused for all objects in \nthe batch.\nz Eﬃ  cient pipelining . Many engine subsystems perform a virtually identi-\ncal set of calculations on each and every object in the game world. When \nupdates are batched, new optimizations become possible, and special-\nized hardware resources can be leveraged. For example, the PLAY-\nSTATION 3 provides a batt ery of high-speed microprocessors known \nas SPUs, each of which has its own private high-speed memory area. \nWhen processing a batch of animations, the pose of one character can be \ncalculated while we simultaneously DMA the data for the next charac-\nter into SPU memory. This kind of parallelism cannot be achieved when \nprocessing each object in isolation.\nPerformance beneﬁ ts aren’t the only reason to favor a batch updating ap-\nproach. Some engine subsystems simply don’t work at all when updated on \na per-object basis. For example, if we are trying to resolve collisions within \na system of multiple dynamic rigid bodies, a satisfactory solution cannot be \nfound in general by considering each object in isolation. The interpenetrations \nbetween these objects must be resolved as a group, either via an iterative ap-\nproach or by solving a linear system.\n14.6.3. Object and Subsystem Interdependencies\nEven if we didn’t care about performance, a simplistic per-object updating ap-\nproach breaks down when game objects depend on one another. For example, \na human character might be holding a cat in her arms. In order to calculate \nthe world-space pose of the cat’s skeleton, we ﬁ rst need to calculate the world-\nspace pose of the human. This implies that the order in which objects are up-\ndated is important to the proper functioning of the game.\nAnother related problem arises when engine subsystems depend on one \nanother. For example, a rag doll physics simulation must be updated in con-\ncert with the animation engine. Typically, the animation system produces an \nintermediate, local-space skeletal pose. These joint transforms are converted \nto world space and applied to a system of connected rigid bodies that approxi-\nmate the skeleton within the physics system. The rigid bodies are simulated \nforward in time by the physics system, and then the ﬁ nal resting places of the \n14.6. Updating Game Objects in Real Time\n",
      "content_length": 2698,
      "extraction_method": "Direct"
    },
    {
      "page_number": 786,
      "chapter": null,
      "content": "764 \n14. Runtime Gameplay Foundation Systems\njoints are applied back to their corresponding joints in the skeleton. Finally, \nthe animation system calculates the ﬁ nal world-space pose and skinning ma-\ntrix palett e. So once again, the updating of the animation and physics systems \nmust occur in a particular order in order to produce correct results. These \nkinds of inter-subsystem dependencies are commonplace in game engine de-\nsign.\n14.6.3.1. Phased Updates\n To account for inter-subsystem dependencies, we can explicitly code our en-\ngine subsystem updates in the proper order within the main game loop. For \nexample, to handle the interplay between the animation system and rag doll \nphysics, we might write something like this:\nwhile (true) // main game loop\n{\n \n// ...\n g_animationEngine.\nCalculateIntermediatePoses(dt);\n g_ragdollSystem.\nApplySkeletonsToRagDolls();\n g_physicsEngine.\nSimulate(dt); // runs ragdolls too\n g_collisionEngine.\nDetectAndResolveCollisions(dt);\n g_ragdollSystem.\nApplyRagDollsToSkeletons();\n g_animationEngine.\nFinalizePoseAndMatrixPalette();\n \n// ...\n}\nWe must be careful to update the states of our game objects at the right \ntime during the game loop. This is oft en not as simple as calling a single Up-\ndate() function per game object per frame. Game objects may depend upon \nthe intermediate results of calculations performed by various engine subsys-\ntems. For example, a game object might request that animations be played \nprior to the animation system running its update. However, that same object \nmay also want to procedurally adjust the intermediate pose generated by the \nanimation system prior to that pose being used by the rag doll physics system \nand/or the ﬁ nal pose and matrix palett e being generated. This implies that the \nobject must be updated twice, once before the animation calculates its inter-\nmediate poses and once aft erward.\nMany game engines allow game objects to update at multiple points \nduring the frame. For example, an engine might update game objects three \ntimes—once before animation blending, once aft er  animation blending but \n",
      "content_length": 2113,
      "extraction_method": "Direct"
    },
    {
      "page_number": 787,
      "chapter": null,
      "content": "765 \nprior to ﬁ nal pose generation, and once aft er ﬁ nal pose generation. This can \nbe accomplished by providing each game object class with three virtual func-\ntions that act as “hooks.” In such a system, the game loop ends up looking \nsomething like this:\nwhile (true) // main game loop\n{\n \n// ...\n \nfor (each gameObject)\n {\n  gameObject.\nPreAnimUpdate(dt);\n }\n g_animationEngine.\nCalculateIntermediatePoses(dt);\n \nfor (each gameObject)\n {\n  gameObject.\nPostAnimUpdate(dt);\n }\n g_ragdollSystem.\nApplySkeletonsToRagDolls();\n g_physicsEngine.\nSimulate(dt); // runs ragdolls too\n g_collisionEngine.\nDetectAndResolveCollisions(dt);\n g_ragdollSystem.\nApplyRagDollsToSkeletons();\n g_animationEngine.\nFinalizePoseAndMatrixPalette();\n \nfor (each gameObject)\n {\n  gameObject.\nFinalUpdate(dt);\n }\n \n// ...\n}\nWe can provide our game objects with as many update phases as we see \nﬁ t. But we must be careful, because iterating over all game objects and calling \na virtual function on each one can be expensive. Also, not all game objects \nrequire all update phases—iterating over objects that don’t require a particu-\nlar phase is a pure waste of CPU bandwidth. One way to minimize the cost \nof iteration is to maintain multiple linked lists of game objects—one for each \nupdate phase. If a particular object wants to be included in one of the update \n14.6. Updating Game Objects in Real Time\n",
      "content_length": 1385,
      "extraction_method": "Direct"
    },
    {
      "page_number": 788,
      "chapter": null,
      "content": "766 \n14. Runtime Gameplay Foundation Systems\nphases, it adds itself to the corresponding linked list. This avoids having to \niterate over objects that are not interested in a particular update phase.\n14.6.3.2. Bucketed Updates\n In the presence of inter-object dependencies, the phased updates technique de-\nscribed above must be adjusted a litt le. This is because inter-object dependen-\ncies can lead to conﬂ icting rules governing the order of updating. For exam-\nple, let’s imagine that object B is being held by object A. Further, let’s assume \nthat we can only update object B aft er A has been fully updated, including the \ncalculation of its ﬁ nal world-space pose and matrix palett e. This conﬂ icts with \nthe need to batch animation updates of all game objects together in order to \nallow the animation system to achieve maximum throughput.\nInter-object dependencies can be visualized as a forest of dependency \ntrees. The game objects with no parents (no dependencies on any other object) \nrepresent the roots of the forest. An object that depends directly on one of \nthese root objects resides in the ﬁ rst tier of children in one of the trees in the \nforest. An object that depends on a ﬁ rst-tier child becomes a second-tier child, \nand so on. This is illustrated in Figure 14.14.\nOne solution to the problem of conﬂ icting update order requirements is \nto collect objects into independent groups, which we’ll call buckets here for \nlack of a bett er name. The ﬁ rst bucket consists of all root objects in the forest. \nThe second bucket is comprised of all ﬁ rst-tier children. The third bucket con-\ntains all second-tier children, and so on. For each bucket, we run a complete \nupdate of the game objects and the engine systems, complete with all update \nFigure 14.14.  Inter-object update order dependencies can be viewed as a forest of depen-\ndency trees.\n",
      "content_length": 1872,
      "extraction_method": "Direct"
    },
    {
      "page_number": 789,
      "chapter": null,
      "content": "767 \nphases. Then we repeat the entire process for each bucket until there are no \nmore buckets.\nIn theory, the depths of the trees in our dependency forest are unbounded. \nHowever, in practice, they are usually quite shallow. For example, we might \nhave characters holding weapons, and those characters might or might not be \nriding on a moving platform or a vehicle. To implement this, we only need \nthree tiers in our dependency forest, and hence only three buckets: one for \nplatforms/vehicles, one for characters, and one for the weapons in the charac-\nters’ hands. Many game engines explicitly limit the depth of their dependency \nforest so that they can use a ﬁ xed number of buckets (presuming they use a \nbucketed approach at all—there are of course many other ways to architect a \ngame loop).\nHere’s what a bucketed, phased, batched update loop might look like:\nvoid UpdateBucket(Bucket bucket)\n{\n \n// ...\n \nfor (each gameObject in bucket)\n {\n  gameObject.PreAnimUpdate(dt);\n }\n g_animationEngine.CalculateIntermediatePoses\n  (\nbucket, dt);\n \nfor (each gameObject in bucket)\n {\n  gameObject.PostAnimUpdate(dt);\n }\n g_ragdollSystem.ApplySkeletonsToRagDolls(\nbucket);\n g_physicsEngine.Simulate(\nbucket, dt); // runs   \n \n \n             \n   // \nragdolls too\n g_collisionEngine.DetectAndResolveCollisions\n  (\nbucket, dt);\n g_ragdollSystem.ApplyRagDollsToSkeletons(\nbucket);\n g_animationEngine.FinalizePoseAndMatrixPalette\n  (\nbucket);\n \nfor (each gameObject in bucket)\n {\n  gameObject.FinalUpdate(dt);\n }\n14.6. Updating Game Objects in Real Time\n",
      "content_length": 1552,
      "extraction_method": "Direct"
    },
    {
      "page_number": 790,
      "chapter": null,
      "content": "768 \n14. Runtime Gameplay Foundation Systems\n \n// ...\n}\nvoid RunGameLoop()\n{\n \nwhile (true)\n {\n  // \n...\nUpdateBucket(g_bucketVehiclesAndPlatforms);\nUpdateBucket(g_bucketCharacters);\nUpdateBucket(g_bucketAttachedObjects);\n  // \n...\n  g_renderingEngine.RenderSceneAndSwapBuffers();\n }\n}\nIn practice, things might a bit more complex than this. For example, some \nengine subsystems like the physics engine might not support the concept of \nbuckets, perhaps because they are third-party SDKs or because they cannot \nbe practically updated in a bucketed manner. However, this bucketed update \nis essentially what we used at Naughty Dog to implement Uncharted: Drake’s \nFortune and are using again for our upcoming title, Uncharted 2: Among Thieves. \nSo it’s a method that has proven to be practical and reasonably eﬃ  cient.\n14.6.3.3. Object State Inconsistencies and One-Frame-Off Lag\nLet’s revisit game object updating, but this time thinking in terms of each ob-\nject’s local notion of time. We said in Section 14.6 that the state of game object i \nat time t can be denoted by a state vector Si(t). When we update a game object, \nwe are converting its previous state vector Si(t1) into a new current state vector \nSi(t2) (where t2 = t1 + Δt).\nIn theory, the states of all game objects are updated from time t1 to time \nt2 instantaneously and in parallel, as depicted in Figure 14.15. However, in \npractice, we can only update the objects one by one—we must loop over each \ngame object and call some kind of update function on each one in turn. If \nwe were to stop the program half-way through this update loop, half of our \ngame objects’ states would have been updated to Si(t2), while the remaining \nhalf would still be in their previous states, Si(t1). This implies that if we were \nto ask two of our game objects what the current time is during the update \nloop, they may or may not agree! What’s more, depending on where exactly \nwe interrupt the update loop, the objects may all be in a partially updated \n",
      "content_length": 2009,
      "extraction_method": "Direct"
    },
    {
      "page_number": 791,
      "chapter": null,
      "content": "769 \nstate. For example, animation pose blending may have been run, but physics \nand collision resolution may not yet have been applied. This leads us to the \nfollowing rule:\nThe states of all game objects are consistent before and aft er the \nupdate loop, but they may be inconsistent during it.\nThis is illustrated in Figure 14.16.\nThe inconsistency of game object states during the update loop is a major \nsource of confusion and bugs, even among professionals within the game in-\ndustry. The problem rears its head most oft en when game objects query one \nt1\nt\nSA\nObjectA\nSA\nObjectB\nSB\nObjectC\nSC\nObjectD\nSD\nt2\nSB\nSC\nSD\nΔt\nFigure 14.15. In theory, the states of all game objects are updated instantaneously and in \nparallel during each iteration of the game loop.\n14.6. Updating Game Objects in Real Time\nFigure 14.16.  In practice, the states of the game objects are updated one by one. This means \nthat at some arbitrary moment during the update loop, some objects will think the current \ntime is t2 while others think it is still t1. Some objects may be only partially updated, so their \nstates will be internally inconsistent. In effect, the state of such an object lies at a point be-\ntween t1 and t2.\nt1\nt\nSA\nObjectA\nObjectB\nSA\nObjectC\nObjectD\nSC\nt2\nSB\nSD\nSB\nSC\n",
      "content_length": 1272,
      "extraction_method": "Direct"
    },
    {
      "page_number": 792,
      "chapter": null,
      "content": "770 \n14. Runtime Gameplay Foundation Systems\nanother for state information during the update loop (which implies that there \nis a dependency between them). For example, if object B looks at the velocity \nof object A in order to determine its own velocity at time t, then the program-\nmer must be clear about whether he or she wants to read the previous state of \nobject A, SA(t1), or the new state, SA(t2). If the new state is needed but object A \nhas not yet been updated, then we have an update order problem that can lead \nto a class of bugs known as one-frame-oﬀ  lags . In this type of bug, the state of \none object lags one frame behind the states of its peers, which manifests itself \non-screen as a lack of synchronization between game objects.\n14.6.3.4. Object State Caching\n As described above, one solution to this problem is to group the game ob-\njects into buckets (Section 14.6.3.2). One problem with a simple bucketed up-\ndate approach is that it imposes somewhat arbitrary limitations on the way in \nwhich game objects are permitt ed to query one another for state information. \nIf a game object A wants the updated state vector SB(t2) of another object B, then \nobject B must reside in a previously updated bucket. Likewise, if object A wants \nthe previous state vector SB(t1) of object B, then object B must reside in a yet-to-\nbe-updated bucket. Object A should never ask for the state vector of an object \nwithin its own bucket, because as we stated in the rule above, those state vec-\ntors may be only partially updated.\nOne way to improve consistency is to arrange for each game object to \ncache its previous state vector Si(t1) while it is calculating its new state vector \nSi(t2) rather than overwriting it in-place during its update. This has two im-\nmediate beneﬁ ts. First, it allows any object to safely query the previous state \nvector of any other object without regard to update order. Second, it guar-\nantees that a totally consistent state vector (Si(t1)) will always be available, \neven during the update of the new state vector. To my knowledge there is no \nstandard terminology for this technique, so I’ll call it state caching for lack of \na bett er name.\nAnother beneﬁ t of state caching is that we can linearly interpolate be-\ntween the previous and next states in order to approximate the state of an \nobject at any moment between these two points in time. The Havok physics \nengine maintains the previous and current state of every rigid body in the \nsimulation for just this purpose.\nThe downside of state caching is that it consumes twice the memory of the \nupdate-in-place approach. It also only solves half the problem, because while \nthe previous states at time t1 are fully consistent, the new states at time t2 still \nsuﬀ er from potential inconsistency. Nonetheless, the technique can be useful \nwhen applied judiciously.\n",
      "content_length": 2871,
      "extraction_method": "Direct"
    },
    {
      "page_number": 793,
      "chapter": null,
      "content": "771 \n14.6.3.5. Time-Stamping\n One easy and low-cost way to improve the consistency of game object states \nis to time-stamp them. It is then a trivial matt er to determine whether a game \nobject’s state vector corresponds to its conﬁ guration at a previous time or the \ncurrent time. Any code that queries the state of another game object during \nthe update loop can assert or explicitly check the time stamp to ensure that the \nproper state information is being obtained.\nTime-stamping does not address the inconsistency of states during the \nupdate of a bucket. However, we can set a global or static variable to reﬂ ect \nwhich bucket is currently being updated. Presumably every game object \n“knows” in which bucket it resides. So we can check the bucket of a queried \ngame object against the currently updating bucket and assert that they are not \nequal in order to guard against inconsistent state queries.\n14.6.4. Designing for Parallelism\nIn Section 7.6, we introduced a number of approaches that allow a game en-\ngine to take advantage of the parallel processing resources that have become \nthe norm in recent gaming hardware. How, then, does parallelism aﬀ ect the \nway in which game object states are updated?\n14.6.4.1. Parallelizing the Game Object Model Itself\n Game object models are notoriously diﬃ  cult to parallelize, for a few reasons. \nGame objects tend to be highly interdependent upon one another and upon \nthe data used and/or generated by numerous engine subsystems. Game ob-\njects communicate with one another, sometimes multiple times during the up-\ndate loop, and the patt ern of communication can be unpredictable and highly \nsensitive to the inputs of the player and the events that are occurring in the \ngame world. This makes it diﬃ  cult to process game object updates in mul-\ntiple threads , for example, because the amount of thread synchronization that \nwould be required to support inter-object communication is usually prohibi-\ntive from a performance standpoint. And the practice of peeking directly into \na foreign game object’s state vector makes it impossible to DMA a game object \nto the isolated memory of a coprocessor, such as the PLAYSTATION 3’s SPU, \nfor updating.\nThat said, game object updating can theoretically be done in parallel. \nTo make it practical, we’d need to carefully design the entire object model \nto ensure that game objects never peek directly into the state vectors of oth-\ner game objects. All inter-object communication would have to be done via \nmessage-passing, and we’d need an eﬃ  cient system for passing messages be-\n14.6. Updating Game Objects in Real Time\n",
      "content_length": 2630,
      "extraction_method": "Direct"
    },
    {
      "page_number": 794,
      "chapter": null,
      "content": "772 \n14. Runtime Gameplay Foundation Systems\ntween game objects even when those objects reside in totally separate memory \nspaces  or are being processed by diﬀ erent physical CPU cores. Some research \nhas been done into using a distributed programming language, such as Er-\nicsson’s Erlang (htt p://www.erlang.org), to code game object models. Such \nlanguages provide built-in support for parallel processing and message pass-\ning and handle context switching between threads much more eﬃ  ciently and \nquickly than in a language like C or C++, and their programming idioms help \nprogrammers to never “break the rules” that allow concurrent, distributed, \nmultiple agent designs to function properly and eﬃ  ciently.\n14.6.4.2. Interfacing with Concurrent Engine Subsystems\n Although sophisticated, concurrent, distributed object models are theoreti-\ncally feasible and are an area of extremely interesting research, at present \nmost game teams do not use them. Instead, most game teams leave the ob-\nject model in a single thread and use an old-fashioned game loop to update \nthem. They focus their att ention instead on parallelizing many of the lower-\nlevel engine systems upon which the game objects depend. This gives teams \nthe biggest “bang for their buck,” because low-level engine subsystems tend \nto be more performance-critical than the game object model. This is because \nlow-level subsystems must process huge volumes of data every frame, while \nthe amount of CPU power used by the game object model is oft en somewhat \nsmaller. This is an example of the 80-20 rule in action.\nOf course, using a single-threaded game object model does not mean that \ngame programmers can be totally oblivious to parallel programming issues. \nThe object model must still interact with engine subsystems that are them-\nselves running concurrently with the object model. This paradigm shift  re-\nquires game programmers to avoid certain programming paradigms that may \nhave served them well in the pre-parallel-processing era and adopt some new \nones in their place.\nProbably the most important shift  a game programmer must make is to \nbegin thinking asynchronously . As described in Section 7.6.5, this means that \nwhen a game object requires a time-consuming operation to be performed, it \nshould avoid calling a blocking function—a function that does its work direct-\nly in the context of the calling thread, thereby blocking that thread until the \nwork has been completed. Instead, whenever possible, large or expensive jobs \nshould be requested by calling a non-blocking function—a function that sends \nthe request to be executed by another thread, core, or processor and then im-\nmediately returns control to the calling function. The main game loop can \nproceed with other unrelated work, including updating other game objects, \nwhile the original object waits for the results of its request. Later in the same \n",
      "content_length": 2913,
      "extraction_method": "Direct"
    },
    {
      "page_number": 795,
      "chapter": null,
      "content": "773 \nframe, or next frame, that game object can pick up the results of its request and \nmake use of them.\nBatching is another shift  in thinking for game programmers. As we men-\ntioned in Section 14.6.2, it is more eﬃ  cient to collect similar tasks into batches \nand perform them en masse than it is to run each task independently. This \napplies to the process of updating game object states as well. For example, if \na game object needs to cast 100 rays into the collision world for various pur-\nposes, it is best if those ray cast requests can be queued up and executed as \none big batch. If an existing game engine is being retroﬁ tt ed for parallelism, \nthis oft en requires code to be rewritt en so that it batches requests rather than \ndoing them individually.\nOne particularly tricky aspect of converting synchronous, unbatched code \nto use an asynchronous, batched approach is determining when during the \ngame loop (a) to kick oﬀ  the request and (b) to wait for and utilize the results. \nIn doing this, it is oft en helpful to ask ourselves the following questions:\nz How early can we kick oﬀ  this request? The earlier we make the request, the \nmore likely it is to be done when we actually need the results—and this \nmaximizes CPU utilization by helping to ensure that the main thread \nis never idle waiting for an asynchronous request to complete. So for \nany given request, we should determine the earliest point during the \nframe at which we have enough information to kick it oﬀ , and kick it \nthere.\nz How long can we wait before we need the results of this request? Perhaps we \ncan wait until later in the update loop to do the second half of an opera-\ntion. Perhaps we can tolerate a one-frame lag and use last frame’s results \nto update the object’s state this frame. (Some subsystems like AI can \ntolerate even longer lag times because they update only every few sec-\nonds.) In many circumstances, code that uses the results of a request can \nin fact be deferred until later in the frame, given a litt le thought, some \ncode re-factoring, and possibly some additional caching of intermediate \ndata.\n14.7. Events and Message-Passing\nGames are inherently event-driven. An event is anything of interest that hap-\npens during gameplay. An explosion going oﬀ , the player being sighted by an \nenemy, a health pack gett ing picked up—these are all events. Games generally \nneed a way to (a) notify interested game objects when an event occurs and (b) \narrange for those objects to respond to interesting events in various ways—we \n14.6. Updating Game Objects in Real Time\n",
      "content_length": 2588,
      "extraction_method": "Direct"
    },
    {
      "page_number": 796,
      "chapter": null,
      "content": "774 \n14. Runtime Gameplay Foundation Systems\ncall this handling the event. Diﬀ erent types of game objects will respond in dif-\nferent ways to an event. The way in which a particular type of game object re-\nsponds to an event is a crucial aspect of its behavior, just as important as how \nthe object’s state changes over time in the absence of any external inputs. For \nexample, the behavior of the ball in Pong is governed in part by its velocity, in \npart by how it reacts to the event of striking a wall or paddle and bouncing oﬀ , \nand in part by what happens when the ball is missed by one of the players.\n14.7.1. The Problem with Statically Typed Function Binding\nOne simple way to notify a game object that an event has occurred is to sim-\nply call a method (member function) on the object. For example, when an \nexplosion goes oﬀ , we could query the game world for all objects within the \nexplosion’s damage radius and then call a virtual function named something \nlike OnExplosion() on each one. This is illustrated by the following pseudo-\ncode:\nvoid Explosion::Update()\n{\n \n// ...\n \nif (ExplosionJustWentOff())\n {\n  GameObjectCollection \ndamagedObjects;\n  g_world.\nQueryObjectsInSphere(GetDamageSphere(),\ndamagedObjects);\n  for \n(each object in damagedObjects)\n  {\n   object.\nOnExplosion(*this);\n  }\n }\n \n// ...\n}\nThe call to OnExplosion() is an example of statically typed late function \nbinding . Function binding is the process of determining which function im-\nplementation to invoke at a particular call location—the implementation is, \nin eﬀ ect, bound to the call. Virtual functions, such as our OnExplosion()\nevent-handling function, are said to be late-bound . This means that the com-\npiler doesn’t actually know which of the many possible implementations of \nthe function is going to be invoked at compile time—only at runtime, when \nthe type of the target object is known, will the appropriate implementation \nbe invoked. We say that a virtual function call is statically typed because the \n",
      "content_length": 2015,
      "extraction_method": "Direct"
    },
    {
      "page_number": 797,
      "chapter": null,
      "content": "775 \ncompiler does know which implementation to invoke given a particular object \ntype. It knows, for example, that Tank::OnExplosion() should be called \nwhen the target object is a Tank and that Crate::OnExplosion() should be \ncalled when the object is a Crate.\nThe problem with statically typed function binding is that it introduces \na degree of inﬂ exibility into our implementation. For one thing, the virtual \nOnExplosion() function requires all game objects to inherit from a common \nbase class. Moreover, it requires that base class to declare the virtual function\nOnExplosion(), even if not all game objects can respond to explosions. In \nfact, using statically typed virtual functions as event handlers would require \nour base GameObject class to declare virtual functions for all possible events \nin the game! This would make adding new events to the system diﬃ  cult. It \nprecludes events from being created in a data-driven manner—for example, \nwithin the world editing tool. It also provides no mechanism for certain types \nof objects, or certain individual object instances, to register interest in certain \nevents but not others. Every object in the game, in eﬀ ect, “knows” about every \npossible event, even if its response to the event is to do nothing (i.e., to imple-\nment an empty, do-nothing event handler function).\nWhat we really need for our event handlers, then, is dynamically typed \nlate function binding. Some programming languages support this feature na-\ntively (e.g., C#’s delegates ). In other languages, the engineers must implement \nit manually. There are many ways to approach this problem, but most boil \ndown to taking a data-driven approach. In other words, we encapsulate the \nnotion of a function call in an object and pass that object around at runtime in \norder to implement a dynamically typed late-bound function call.\n14.7.2. Encapsulating an Event in an Object\nAn event is really comprised of two components: its type (explosion, friend \ninjured, player spott ed, health pack picked up, etc.) and its arguments . The \narguments provide speciﬁ cs about the event (How much damage did the ex-\nplosion do? Which friend was injured? Where was the player spott ed? How \nmuch health was in the health pack?). We can encapsulate these two compo-\nnents in an object, as shown by the following pseudocode:\nstruct Event\n{\n \nconst U32 MAX_ARGS = 8;\n \nEventType m_type;\n U32 \n  m_numArgs;\n \nEventArg m_aArgs[MAX_ARGS];\n};\n14.7. Events and Message-Passing\n",
      "content_length": 2490,
      "extraction_method": "Direct"
    },
    {
      "page_number": 798,
      "chapter": null,
      "content": "776 \n14. Runtime Gameplay Foundation Systems\nSome game engines call these things messages or commands instead of events. \nThese names emphasize the idea that informing objects about an event is es-\nsentially equivalent to sending a message or command to those objects.\nPractically speaking, event objects are usually not quite this simple. We \nmight implement diﬀ erent types of events by deriving them from a root event \nclass, for example. The arguments might be implemented as a linked list or a \ndynamically allocated array capable of containing arbitrary numbers of argu-\nments, and the arguments might be of various data types.\nEncapsulating an event (or message) in an object has many beneﬁ ts:\nz Single event handler function. Because the event object encodes its type \ninternally, any number of diﬀ erent event types can be represented by an \ninstance of a single class (or the root class of an inheritance hierarchy). \nThis means that we only need one virtual function to handle all types of \nevents (e.g., virtual void OnEvent(Event& event);).\nz Persistence . Unlike a function call, whose arguments go out of scope \naft er the function returns, an event object stores both its type and its \narguments as data. An event object therefore has persistence. It can be \nstored in a queue for handling at a later time, copied and broadcast to \nmultiple receivers, and so on.\nz Blind event forwarding . An object can forward an event that it receives to \nanother object without having to “know” anything about the event. For \nexample, if a vehicle receives a Dismount event, it can forward it to all \nof its passengers, thereby allowing them to dismount the vehicle, even \nthough the vehicle itself knows nothing about dismounting.\nThis idea of encapsulating an event/message/command in an object is com-\nmonplace in many ﬁ elds of computer science. It is found not only in game \nengines but in other systems like graphical user interfaces, distributed com-\nmunication systems, and many others. The well-known “Gang of Four” de-\nsign patt erns book [17] calls this the Command patt ern.\n14.7.3. Event Types\n There are many ways to distinguish between diﬀ erent types of events. One \nsimple approach in C or C++ is to deﬁ ne a global enum that maps each event \ntype to a unique integer.\nenum EventType\n{\n EVENT_TYPE_LEVEL_STARTED,\n EVENT_TYPE_PLAYER_SPAWNED,\n",
      "content_length": 2362,
      "extraction_method": "Direct"
    },
    {
      "page_number": 799,
      "chapter": null,
      "content": "777 \n EVENT_TYPE_ENEMY_SPOTTED,\n EVENT_TYPE_EXPLOSION,\n EVENT_TYPE_BULLET_HIT,\n \n// ...\n}\nThis approach enjoys the beneﬁ ts of simplicity and eﬃ  ciency (since integers \nare usually extremely fast to read, write, and compare). However, it also suf-\nfers from two problems. First, knowledge of all event types in the entire game \nis centralized, which can be seen as a form of broken encapsulation (for bett er \nor for worse—opinions on this vary). Second, the event types are hard-coded, \nwhich means new event types cannot easily be deﬁ ned in a data-driven man-\nner. Third, enumerators are just indices, so they are order-dependent. If some-\none accidentally adds a new event type in the middle of the list, the indices \nof all subsequent event ids change, which can cause problems if event ids \nare stored in data ﬁ les. As such, an enumeration-based event typing system \nworks well for small demos and prototypes but does not scale very well at all \nto real games.\nAnother way to encode event types is via strings. This approach is totally \nfree-form, and it allows a new event type to be added to the system by merely \nthinking up a name for it. But it suﬀ ers from many problems, including a \nstrong potential for event name conﬂ icts, the possibility of events not work-\ning because of a simple typo, increased memory requirements for the strings \nthemselves, and the relatively high cost of comparing strings next to that of \ncomparing integers. Hashed string ids can be used instead of raw strings to \neliminate the performance problems and increased memory requirements, \nbut they do nothing to address event name conﬂ icts or typos. Nonetheless, \nthe extreme ﬂ exibility and data-driven nature of a string- or string-id-based \nevent system is considered worth the risks by some game teams.\nTools can be implemented to help avoid some of the risks involved in us-\ning strings to identify events. For example, a central database of all event type \nnames could be maintained. A user interface could be provided to permit new \nevent types to be added to the database. Naming conﬂ icts could be automati-\ncally detected when a new event is added, and the user could be disallowed \nfrom adding duplicate event types. When selecting a preexisting event, the \ntool could provide a sorted list in a drop-down combo box rather than requir-\ning the user to remember the name and type it manually. The event database \ncould also store meta-data about each type of event, including documentation \nabout its purpose and proper usage and information about the number and \ntypes of arguments it supports. This approach can work really well, but we \n14.7. Events and Message-Passing\n",
      "content_length": 2677,
      "extraction_method": "Direct"
    },
    {
      "page_number": 800,
      "chapter": null,
      "content": "778 \n14. Runtime Gameplay Foundation Systems\nshould not forget to account for the costs of sett ing up such a system, as they \nare not insigniﬁ cant.\n14.7.4. Event Arguments\nThe arguments of an event usually act like the argument list of a function, pro-\nviding information about the event that might be useful to the receiver. Event \narguments can be implemented in all sorts of ways.\nWe might derive a new type of Event class for each unique type of event. The \narguments can then be hard-coded as data members of the class. For example:\nclass ExplosionEvent : public Event\n{\n float \n m_damage;\n Point \n m_center;\n float \n m_radius;\n};\nAnother approach is to store the event’s arguments as a collection of vari-\nants . A variant is a data object that is capable of holding more than one type of \ndata. It usually stores information about the data type that is currently being \nstored, as well as the data itself. In an event system, we might want our argu-\nments to be integers, ﬂ oating-point values, Booleans, or hashed string ids. So \nin C or C++, we could deﬁ ne a variant class that looks something like this:\nstruct Variant\n{\n    enum Type\n    {\n        TYPE_INTEGER,\n        TYPE_FLOAT,\n        TYPE_BOOL,\n        TYPE_STRING_ID,\n        TYPE_COUNT // number of unique types\n    };\n    Type        m_type;\n    union\n    {\n        I32     m_asInteger;\n        F32     m_asFloat;\n        bool    m_asBool;\n        U32     m_asStringId;\n };\n};\n",
      "content_length": 1450,
      "extraction_method": "Direct"
    },
    {
      "page_number": 801,
      "chapter": null,
      "content": "779 \nThe collection of variants might be implemented as an array with a small, \nﬁ xed maximum size (say 4, 8, or 16 elements). This imposes an arbitrary limit \non the number of arguments that can be passed with an event, but it also side-\nsteps the problems of dynamically allocating memory for each event’s argu-\nment payload, which can be a big beneﬁ t, especially in memory-constrained \nconsole games.\nThe collection of variants might be implemented as a dynamically sized \ndata structure, like a dynamically sized array (like std::vector) or a linked \nlist (like std::list). This provides a great deal of additional ﬂ exibility over a \nﬁ xed-size design, but it incurs the cost of dynamic memory allocation. A pool \nallocator could be used to great eﬀ ect here, presuming that each Variant is \nthe same size.\n14.7.4.1. Event Arguments as Key-Value Pairs\nA fundamental problem with an indexed collection of event arguments is order \ndependency. Both the sender and the receiver of an event must “know” that \nthe arguments are listed in a speciﬁ c order. This can lead to confusion and \nbugs. For example, a required argument might be accidentally omitt ed or an \nextra one added.\nThis problem can be avoided by implementing event arguments as key-\nvalue pairs . Each argument is uniquely identiﬁ ed by its key, so the arguments \ncan appear in any order, and optional arguments can be omitt ed altogether. \nThe argument collection might be implemented as a closed or open hash table, \nwith the keys used to hash into the table, or it might be an array, linked list, or \nbinary search tree of key-value pairs. These ideas are illustrated in Table 14.1. \nThe possibilities are numerous, and the speciﬁ c choice of implementation is \nlargely unimportant as long as the game’s particular requirements have been \neﬀ ectively and eﬃ  ciently met.\n14.7. Events and Message-Passing\nfloat\nValue\n10.3\nint\n25\nbool\ntrue\n\"radius\"\n\"event\"\n\"damage\"\n\"grenade\"\nKey\nType\nstringid\n\"explosion\"\nTable 14.1.  The arguments of an event object can be implemented as a collection of key-value \npairs. The keys help to avoid order-dependency problems because each event argument is \nuniquely identiﬁ ed by its key.\n",
      "content_length": 2191,
      "extraction_method": "Direct"
    },
    {
      "page_number": 802,
      "chapter": null,
      "content": "780 \n14. Runtime Gameplay Foundation Systems\n14.7.5. Event Handlers\nWhen an event, message, or command is received by a game object, it needs to \nrespond to the event in some way. This is known as handling the event, and it \nis usually implemented by a function or a snippet of script code called an event \nhandler. (We’ll have more to say about game scripting later on.)\nOft en an event handler is a single native virtual function or script function \nthat is capable of handling all types of events (e.g., OnEvent(Event& event)). \nIn this case, the function usually contains some kind of switch statement or \ncascaded if/else-if clause to handle the various types of events that might be \nreceived. A typical event handler function might look something like this:\nvirtual void SomeObject::OnEvent(Event& event)\n{\n \nswitch (event.GetType())\n {\n case \nEVENT_ATTACK:\n  RespondToAttack(event.GetAttackInfo());\n  break;\n case \nEVENT_HEALTH_PACK:\n  AddHealth(event.GetHealthPack().GetHealth());\n  break;\n \n// ...\ndefault:\n \n \n// Unrecognized event.\n  break;\n }\n}\nAlternatively, we might implement a suite of handler functions, one for \neach type of event (e.g., OnThis(), OnThat(), …). However, as we discussed \nabove, a proliferation of event handler functions can be problematic.\nA Windows GUI toolkit called Microsoft  Foundation Classes (MFC) was \nwell-known for its message maps —a system that permitt ed any Windows mes-\nsage to be bound at runtime to an arbitrary non-virtual or virtual function. \nThis avoided the need to declare handlers for all possible Windows messages \nin a single root class, while at the same time avoiding the big switch statement \nthat is commonplace in non-MFC Windows message-handling functions. But \nsuch a system is probably not worth the hassle—a switch statement works re-\nally well and is simple and clear.\n14.7.6. Unpacking an Event’s Arguments\nThe example above glosses over one important detail—namely, how to ex-\ntract data from the event’s argument list in a type-safe manner. For example, \n",
      "content_length": 2031,
      "extraction_method": "Direct"
    },
    {
      "page_number": 803,
      "chapter": null,
      "content": "781 \nevent.GetHealthPack() presumably returns a HealthPack game object, \nwhich in turn we presume provides a member function called GetHealth(). \nThis implies that the root Event class “knows” about health packs (as well as, \nby extension, every other type of event argument in the game!) This is prob-\nably an impractical design. In a real engine, there might be derived Event\nclasses that provide convenient data-access APIs such as GetHealthPack(). \nOr the event handler might have to unpack the data manually and cast them \nto the appropriate types. This latt er approach raises type safety concerns, al-\nthough practically speaking it usually isn’t a huge problem because the type \nof the event is always known when the arguments are unpacked.\n14.7.7. Chains of Responsibility\nGame objects are almost always dependent upon one another in various ways. \nFor example, game objects usually reside in a transformation hierarchy, which \nallows an object to rest on another object or be held in the hand of a charac-\nter. Game objects might also be made up of multiple interacting components, \nleading to a star topology or a loosely connected “cloud” of component ob-\njects. A sports game might maintain a list of all the characters on each team. In \ngeneral, we can envision the interrelationships between game objects as one \nor more relationship graphs (remembering that a list and a tree are just special \ncases of a graph). A few examples of relationship graphs are shown in Fig-\nure 14.17.\n14.7. Events and Message-Passing\nFigure 14.17.  Game objects are interrelated in various ways, and we can draw graphs depicting \nthese relationships. Any such graph might serve as a distribution channel for events.\nAttachment \nGraph\nEvent1\nEvent3\nComponent \nGraph\nEvent2\nTeam \nGraph\nTeam\nCarter\nEvan\nQuinn\nCooper\nObjectA\nComponentA2\nComponentA1\nComponentA3\nClip\nWeapon\nCharacter\nVehicle\n",
      "content_length": 1883,
      "extraction_method": "Direct"
    },
    {
      "page_number": 804,
      "chapter": null,
      "content": "782 \n14. Runtime Gameplay Foundation Systems\nIt oft en makes sense to be able to pass events from one object to the next \nwithin these relationship graphs. For example, when a vehicle receives an \nevent, it may be convenient to pass the event to all of the passengers riding on \nthe vehicle, and those passengers may wish to forward the event to the objects \nin their inventories. When a multicomponent game object receives an event, it \nmay be necessary to pass the event to all of the components so that they all get \na crack at handling it. Or when an event is received by a character in a sports \ngame, we might want to pass it on to all of his or her teammates as well.\nThe technique of forwarding events within a graph of objects is a com-\nmon design patt ern in object-oriented, event-driven programming, sometimes \nreferred to as a chain of responsibility [17]. Usually, the order in which the event \nis passed around the system is predetermined by the engineers. The event is \npassed to the ﬁ rst object in the chain, and the event handler returns a Boolean \nor an enumerated code indicating whether or not it recognized and handled \nthe event. If the event is consumed by a receiver, the process of event for-\nwarding stops; otherwise, the event is forwarded on to the next receiver in \nthe chain. An event handler that supports chain-of-responsibility style event \nforwarding might look something like this:\nvirtual bool SomeObject::OnEvent(Event& event)\n{\n     // Call the base class’ handler first.\n     if (BaseClass::OnEvent(event))\n     {\n        return true;\n     }\n \n// Now try to handle the event myself.\n \nswitch (event.GetType())\n {\n case \nEVENT_ATTACK:\n  RespondToAttack(event.GetAttackInfo());\n \n \nreturn false; // OK to forward this event to others.\n case \nEVENT_HEALTH_PACK:\n  AddHealth(event.GetHealthPack().GetHealth());\n \n \nreturn true; // I consumed the event; don’t forward.\n \n// ...\ndefault:\n \n \nreturn false; // I didn’t recognize this event.\n }\n}\nWhen a derived class overrides an event handler, it can be appropriate to \ncall the base class’s implementation as well if the class is augmenting but not re-\n",
      "content_length": 2139,
      "extraction_method": "Direct"
    },
    {
      "page_number": 805,
      "chapter": null,
      "content": "783 \nplacing the base class’s response. In other situations, the derived class might be \nentirely replacing the response of the base class, in which case the base class’s \nhandler should not be called. This is another kind of responsibility chain.\nEvent forwarding has other applications as well. For example, we might \nwant to multicast an event to all objects within a radius of inﬂ uence (for an \nexplosion, for example). To implement this, we can leverage our game world’s \nobject query mechanism to ﬁ nd all objects within the relevant sphere and then \nforward the event to all of the returned objects.\n14.7.8. Registering Interest in Events\nIt’s reasonably safe to say that most objects in a game do not need to respond \nto every possible event. Most types of game objects have a relatively small set \nof events in which they are “interested.” This can lead to ineﬃ  ciencies when \nmulticasting or broadcasting events, because we need to iterate over a group \nof objects and call each one’s event handler, even if the object is not interested \nin that particular kind of event.\nOne way to overcome this ineﬃ  ciency is to permit game objects to regis-\nter interest in particular kinds of events. For example, we could maintain one \nlinked list of interested game objects for each distinct type of event, or each \ngame object could maintain a bit array, in which the sett ing of each bit corre-\nsponds to whether or not the object is interested in a particular type of event. \nBy doing this, we can avoid calling the event handlers of any objects that do \nnot care about the event. Calling virtual functions can incur a non-trivial per-\nformance hit, especially on consoles with relatively primitive RAM caches, so \nﬁ ltering objects by interest in an event can greatly improve the eﬃ  ciency of \nevent multicasting and broadcasting.\nEven bett er, we might be able to restrict our original game object query to \ninclude only those objects that are interested in the event we wish to multicast. \nFor example, when an explosion goes oﬀ , we can ask the collision system for \nall objects that are within the damage radius and that can respond to Explo-\nsion events. This can save time overall, because we avoid iterating over objects \nthat we know aren’t interested in the event we’re multicasting. Whether or not \nsuch an approach will produce a net gain depends on how the query mecha-\nnism is implemented and the relative costs of ﬁ ltering the objects during the \nquery versus ﬁ ltering them during the multicast iteration.\n14.7.9. To Queue or Not to Queue\n Most game engines provide a mechanism for handling events immediately \nwhen they are sent. In addition to this, some engines also permit events to be \nqueued for handling at an arbitrary future time. Event queuing has some at-\n14.7. Events and Message-Passing\n",
      "content_length": 2824,
      "extraction_method": "Direct"
    },
    {
      "page_number": 806,
      "chapter": null,
      "content": "784 \n14. Runtime Gameplay Foundation Systems\ntractive beneﬁ ts, but it also increases the complexity of the event system and \nposes some unique problems. We’ll investigate the pros and cons of event \nqueuing in the following sections and learn how such systems are implement-\ned in the process.\n14.7.9.1. Some Beneﬁ ts of Event Queuing\nControl Over When Events are Handled\nWe have seen that we must be careful to update engine subsystems and game \nobjects in a speciﬁ c order to ensure correct behavior and maximize runtime \nperformance. In the same sense, certain kinds of events may be highly sen-\nsitive to exactly when within the game loop they are handled. If all events \nare handled immediately upon being sent, the event handler functions end \nup being called in unpredictable and diﬃ  cult-to-control ways throughout the \ncourse of the game loop. By deferring events via an event queue, the engineers \ncan take steps to ensure that events are only handled when it is safe and ap-\npropriate to do so.\nAbility to Post Events into the Future\nWhen an event is sent, the sender can usually specify a delivery time—for \nexample, we might want the event to be handled later in the same frame, next \nframe, or some number of seconds aft er it was sent. This feature amounts to \nan ability to post events into the future, and it has all sorts of interesting uses. \nWe can implement a simple alarm clock by posting an event into the future. A \nperiodic task, such as blinking a light every two seconds, can be executed by \nposting an event whose handler performs the periodic task and then posts a \nnew event of the same type one period into the future .\nTo implement the ability to post events into the future, each event is \nstamped with a desired delivery time prior to being queued. An event is only \nhandled when the current game clock matches or exceeds its delivery time. An \neasy way to make this work is to sort the events in the queue in order of increas-\ning delivery time. Each frame, the ﬁ rst event on the queue can be inspected and \nits delivery time checked. If the delivery time is in the future, we abort imme-\ndiately because we know that all subsequent events are also in the future. But \nif we see an event whose delivery time is now or in the past, we extract it from \nthe queue and handle it. This continues until an event is found whose delivery \ntime is in the future. The following pseudocode illustrates this process:\n// This function is called at least once per frame. Its \n// job is to dispatch all events whose delivery time is \n// now or in the past.\nvoid EventQueue::DispatchEvents(F32 currentTime)\n{\n",
      "content_length": 2631,
      "extraction_method": "Direct"
    },
    {
      "page_number": 807,
      "chapter": null,
      "content": "785 \n \n// Look at, but don’t remove, the next event on the   \n \n// queue.\n \nEvent* pEvent = PeekNextEvent();\n \nwhile (pEvent && pEvent->GetDeliveryTime() <=   \n \n \n  currentTime\n)\n {\n \n \n// OK, now remove the event from the queue.\nRemoveNextEvent();\n \n \n// Dispatch it to its receiver’s event handler.\n  pEvent->\nDispatch();\n \n \n// Peek at the next event on the queue (again   \n \n \n \n// without removing it).\n  pEvent \n= PeekNextEvent();\n }\n}\nEvent Prioritization\nEven if our events are sorted by delivery time in the event queue, the order \nof delivery is still ambiguous when two or more events have exactly the same \ndelivery time. This can happen more oft en than you might think, because it is \nquite common for events’ delivery times to be quantized to an integral num-\nber of frames. For example, if two senders request that events be dispatched \n“this frame,” “next frame,” or “in seven frames from now,” then those events \nwill have identical delivery times.\nOne way to resolve these ambiguities is to assign priorities to events. \nWhenever two events have the same timestamp, the one with higher pri-\nority should always be serviced ﬁ rst. This is easily accomplished by ﬁ rst \nsorting the event queue by increasing delivery times and then sorting each \ngroup of events with identical delivery times in order of decreasing prior-\nity.\nWe could allow up to four billion unique priority levels by encoding our \npriorities in a raw, unsigned 32-bit integer, or we could limit ourselves to only \ntwo or three unique priority levels (e.g., low, medium, and high). In every \ngame engine, there exists some minimum number of priority levels that will \nresolve all real ambiguities in the system. It’s usually best to aim as close to \nthis minimum as possible. With a very large number of priority levels, it can \nbecome a small nightmare to ﬁ gure out which event will be handled ﬁ rst in \nany given situation. However, the needs of every game’s event system are dif-\nferent, and your mileage may vary.\n14.7. Events and Message-Passing\n",
      "content_length": 2039,
      "extraction_method": "Direct"
    },
    {
      "page_number": 808,
      "chapter": null,
      "content": "786 \n14. Runtime Gameplay Foundation Systems\n14.7.9.2. Some Problems with Event Queuing\nIncreased Event System Complexity\nIn order to implement a queued event system, we need more code, additional \ndata structures, and more-complex algorithms than would be necessary to \nimplement an immediate event system. Increased complexity usually trans-\nlates into longer development times and a higher cost to maintain and evolve \nthe system during development of the game.\nDeep-Copying Events and Their Arguments\nWith an immediate event handling approach, the data in an event’s arguments \nneed only persist for the duration of the event handling function (and any \nfunctions it may call). This means that the event and its argument data can \nreside literally anywhere in memory, including on the call stack. For example, \nwe could write a function that looks something like this:\nvoid SendExplosionEventToObject(GameObject& receiver)\n{\n \n// Allocate event args on the call stack.\n \nF32   damage = 5.0f;\n \nPoint centerPoint(-2.0f, 31.5f, 10.0f);\n \nF32   radius = 2.0f;\n \n// Allocate the event on the call stack.\nEvent event(\"Explosion\");\n \nevent.SetArgFloat(\"Damage\", damage);\n \nevent.SetArgPoint(\"Center\", &centerPoint);\n \nevent.SetArgFloat(\"Radius\", radius);\n \n// Send the event, which causes the receiver’s event\n \n// handler to be called immediately, as shown below.\n event.\nSend(receiver);\n //{\n \n//    receiver.OnEvent(event);\n //}\n}\nWhen an event is queued, its arguments must persist beyond the scope of \nthe sending function. This implies that we must copy the entire event object \nprior to storing the event in the queue. We must perform a deep-copy , meaning \nthat we copy not only the event object itself but its entire argument payload as \nwell, including any data to which it may be pointing. Deep-copying the event \nensures that there are no dangling references to data that exist only in the \n",
      "content_length": 1901,
      "extraction_method": "Direct"
    },
    {
      "page_number": 809,
      "chapter": null,
      "content": "787 \nsending function’s scope, and it permits the event to be stored indeﬁ nitely. The \nexample event-sending function shown above still looks basically the same \nwhen using a queued event system, but as you can see in the italicized code \nbelow, the implementation of the Event::Queue() function is a bit more \ncomplex than its Send() counterpart:\nvoid SendExplosionEventToObject(GameObject& receiver)\n{\n \n// We can still allocate event args on the call   \n \n \n// stack.\n \nF32   damage = 5.0f;\n \nPoint centerPoint(-2.0f, 31.5f, 10.0f);\n \nF32   radius = 2.0f;\n \n// Still OK to allocate the event on the call stack.\nEvent event(\"Explosion\");\n \nevent.SetArgFloat(\"Damage\", damage);\n \nevent.SetArgPoint(\"Center\", &centerPoint);\n \nevent.SetArgFloat(\"Radius\", radius);\n \n// This stores the event in the receiver’s queue for\n \n// handling at a future time. Note how the event   \n \n \n// must be deep-copied prior to being enqueued, since \n \n// the original event resides on the call stack and   \n \n// will go out of scope when this function returns.\n event.\nQueue(receiver);\n //{\n \n//    Event* pEventCopy = DeepCopy(event);\n \n//    receiver.EnqueueEvent(pEventCopy);\n //}\n}\nDynamic Memory Allocation for Queued Events\n Deep-copying of event objects implies a need for dynamic memory allocation, \nand as we’ve already noted many times, dynamic allocation is undesirable in \na game engine due to its potential cost and its tendency to fragment memory. \nNonetheless, if we want to queue events, we’ll need to dynamically allocate \nmemory for them.\nAs with all dynamic allocation in a game engine, it’s best if we can select \na fast and fragmentation-free allocator. We might be able to use a pool allocator, \nbut this will only work if all of our event objects are the same size and if their \nargument lists are comprised of data elements that are themselves all the same \nsize. This may well be the case—for example, the arguments might each be a \n14.7. Events and Message-Passing\n",
      "content_length": 1973,
      "extraction_method": "Direct"
    },
    {
      "page_number": 810,
      "chapter": null,
      "content": "788 \n14. Runtime Gameplay Foundation Systems\nVariant, as described above. If our event objects and/or their arguments can \nvary in size, a small memory allocator might be applicable. (Recall that a small \nmemory allocator maintains multiple pools, one for each of a few predeter-\nmined small allocation sizes.) When designing a queued event system, always \nbe careful to take dynamic allocation requirements into account.\nDebugging Difﬁ culties\n With queued events, the event handler is not called directly by the sender of \nthat event. So, unlike in immediate event handling, the call stack does not tell us \nwhere the event came from. We cannot walk up the call stack in the debugger \nto inspect the state of the sender or the circumstances under which the event \nwas sent. This can make debugging deferred events a bit tricky, and things get \neven more diﬃ  cult when events are forwarded from one object to another.\nSome engines store debugging information that forms a paper trail of the \nevent’s travels throughout the system, but no matt er how you slice it, event \ndebugging is usually much easier in the absence of queuing.\nEvent queuing also leads to interesting and hard-to-track-down race con-\ndition bugs. We may need to pepper multiple event dispatches throughout our \ngame loop, to ensure that events are delivered without incurring unwanted \none-frame delays yet still ensuring that game objects are updated in the proper \norder during the frame. For example, during the animation update, we might \ndetect that a particular animation has run to completion. This might cause an \nevent to be sent whose handler wants to play a new animation. Clearly, we \nwant to avoid a one-frame delay between the end of the ﬁ rst animation and \nthe start of the next. To make this work, we need to update animation clocks \nﬁ rst (so that the end of the animation can be detected and the event sent), then \nwe should dispatch events (so that the event handler has a chance to request \na new animation), and ﬁ nally we can start animation blending (so that the \nﬁ rst frame of the new animation can be processed and displayed). This is il-\nlustrated in the code snippet below:\nwhile (true) // main game loop\n{\n \n// ...\n \n// Update animation clocks. This may detect the end   \n \n// of a clip, and cause EndOfAnimation events to \n \n// be sent.\n g_animationEngine.\nUpdateLocalClocks(dt);\n \n// Next, dispatch events. This allows an  \n \n \n \n \n \n// EndOfAnimation event handler to start up a new \n \n// animation this frame if desired.\n g_eventSystem.\nDispatchEvents();\n",
      "content_length": 2561,
      "extraction_method": "Direct"
    },
    {
      "page_number": 811,
      "chapter": null,
      "content": "789 \n \n// Finally, start blending all currently-playing   \n \n \n// animations (including any new clips started \n \n// earlier this frame).\n g_animationEngine.\nStartAnimationBlending();\n \n// ...\n}\n14.7.10. Some Problems with Immediate Event Sending\nNot queuing events also has its share of issues. For example, immediate event \nhandling can lead to extremely deep call stacks. Object A might send object B \nan event, and in its event handler, B might send another event, which might \nsend another event, and another, and so on. In a game engine that supports \nimmediate event handling, it’s not uncommon to see a call stack that looks \nsomething like this:\n…\nShoulderAngel::OnEvent()\nEvent::Send()\nCharacer::OnEvent()\nEvent::Send()\nCar::OnEvent()\nEvent::Send()\nHandleSoundEffect()\nAnimationEngine::PlayAnimation()\nEvent::Send()\nCharacter::OnEvent()\nEvent::Send()\nCharacter::OnEvent()\nEvent::Send()\nCharacter::OnEvent()\nEvent::Send()\nCar::OnEvent()\nEvent::Send()\nCar::OnEvent()\nEvent::Send()\nCar::Update()\nGameWorld::UpdateObjectsInBucket()\nEngine::GameLoop()\nmain()\nA deep call stack like this can exhaust available stack space in extreme \ncases (especially if we have an inﬁ nite loop of event sending), but the real \ncrux of the problem here is that every event handler function must be writt en \nto be fully re-entrant. This means that the event handler can be called recur-\nsively without any ill side-eﬀ ects. As a contrived example, imagine a function \n14.7. Events and Message-Passing\n",
      "content_length": 1489,
      "extraction_method": "Direct"
    },
    {
      "page_number": 812,
      "chapter": null,
      "content": "790 \n14. Runtime Gameplay Foundation Systems\nthat increments the value of a global variable. If the global is supposed to be \nincremented only once per frame, then this function is not re-entrant, because \nmultiple recursive calls to the function will increment the variable multiple \ntimes.\n14.7.11. Data-Driven Event/Message-Passing Systems\n Event systems give the game programmer a great deal of ﬂ exibility over and \nabove what can be accomplished with the statically typed function calling \nmechanisms provided by languages like C and C++. However, we can do bet-\nter. In our discussions thus far, the logic for sending and receiving events is \nstill hard-coded and therefore under the exclusive control of the engineers. If \nwe could make our event system data-driven, we could extend its power into \nthe hands of our game designers.\nThere are many ways to make an event system data-driven. Starting with \nthe extreme of an entirely hard-coded (non-data-driven) event system, we \ncould imagine providing some simple data-driven conﬁ gurability. For exam-\nple, designers might be allowed to conﬁ gure how individual objects, or entire \nclasses of object, respond to certain events. In the world editor , we can imagine \nselecting an object and then bringing up a scrolling list of all possible events \nthat it might receive. For each one, the designer could use drop-down combo \nboxes and check boxes to control if, and how, the object responds, by selecting \nfrom a set of hard-coded, predeﬁ ned choices. For example, given the event \n“PlayerSpott ed,” AI characters might be conﬁ gured to do one of the following \nactions: run away, att ack, or ignore the event altogether. The event systems of \nsome real commercial game engines are implemented in essentially this way.\nAt the other end of the gamut, our engine might provide the game design-\ners with a simple scripting language (a topic we’ll explore in detail in Section \n14.8). In this case, the designer can literally write code that deﬁ nes how a partic-\nular kind of game object will respond to a particular kind of event. In a scripted \nmodel, the designers are really just programmers (working with a somewhat \nless powerful but also easier-to-use and hopefully less error-prone language \nthan the engineers), so anything is possible. Designers might deﬁ ne new types \nof events, send events, and receive and handle events in arbitrary ways.\nThe problem with a simple, conﬁ gurable event system is that it can se-\nverely limit what the game designers are capable of doing on their own, with-\nout the help of a programmer. On the other hand, a fully scripted solution \nhas its own share of problems: Many game designers are not professional \nsoft ware engineers by training, so some designers ﬁ nd learning and using a \nscripting language a daunting task. Designers are also probably more prone to \nintroducing bugs into the game than their engineer counterparts, unless they \n",
      "content_length": 2943,
      "extraction_method": "Direct"
    },
    {
      "page_number": 813,
      "chapter": null,
      "content": "791 \nhave practiced scripting or programming for some time. This can lead to some \nnasty surprises during alpha.\nAs a result, some game engines aim for a middle ground. They employ \nsophisticated graphical user interfaces to provide a great deal of ﬂ exibility \nwithout going so far as to provide users with a full-ﬂ edged, free-form scripting \nlanguage. One approach is to provide a ﬂ ow-chart-style graphical program-\nming language. The idea behind such a system is to provide the user with a \nlimited and controlled set of atomic operations from which to choose but with \nplenty of freedom to wire them up in arbitrary ways. For example, in response \nto an event like “PlayerSpott ed,” the designer could wire up a ﬂ ow chart that \ncauses a character to retreat to the nearest cover point, play an animation, wait \n5 seconds, and then att ack. A GUI can also provide error-checking and valida-\ntion to help ensure that bugs aren’t inadvertently introduced.\n14.7.11.1. Data Pathway Communication Systems\nOne of the problems with converting a function-call-like event system into \na data-driven system is that diﬀ erent types of events tend to be incompat-\nible. For example, let’s imagine a game in which the player has an electro-\nmagnetic pulse gun. This pulse causes lights and electronic devices to turn \noﬀ , scares small animals, and produces a shock wave that causes any nearby \nplants to sway. Each of these game object types may already have an event \nresponse that performs the desired behavior. A small animal might respond \nto the “Scare” event by scurrying away. An electronic device might respond \nto the “TurnOﬀ ” event by turning itself oﬀ . And plants might have an event \nhandler for a “Wind” event that causes them to sway. The problem is that our \nEMP gun is not compatible with any of these objects’ event handlers. As a re-\nsult, we end up having to implement a new event type, perhaps called “EMP,” \nand then write custom event handlers for every type of game object in order \nto respond to it.\n One solution to this problem is to take the event type out of the equation \nand to think solely in terms of sending streams of data from one game object to \nanother. In such a system, every game object has one or more input ports, to \nwhich a data stream can be connected, and one or more output ports, through \nwhich data can be sent to other objects. Provided we have some way of wir-\ning these ports together, such as a graphical user interface in which ports can \nbe connected to each other via rubber-band lines, then we can construct ar-\nbitrarily complex behaviors. Continuing our example, the EMP gun would \nhave an output port, perhaps named “Fire,” that sends a Boolean signal. Most \nof the time, the port produces the value 0 (false), but when the gun is ﬁ red, it \nsends a brief (one-frame) pulse of the value 1 (true). The other game objects in \n14.7. Events and Message-Passing\n",
      "content_length": 2913,
      "extraction_method": "Direct"
    },
    {
      "page_number": 814,
      "chapter": null,
      "content": "792 \n14. Runtime Gameplay Foundation Systems\nthe world have binary input ports that trigger various responses. The animals \nmight have a “Scare” input, the electronic devices a “TurnOn” input, and the \nfoliage objects a “Sway” input. If we connect the EMP gun’s “Fire” output port \nto the input ports of these game objects, we can cause the gun to trigger the de-\nsired behaviors. (Note that we’d have to pipe the gun’s “Fire” output through \na node that inverts its input, prior to connecting it to the “TurnOn” input of the \nelectronic devices. This is because we want them to turn oﬀ  when the gun is \nﬁ ring.) The wiring diagram for this example is shown in Figure 14.18.\nProgrammers decide what kinds of port(s) each type of game object will \nhave. Designers using the GUI can then wire these ports together in arbitrary \nways in order to construct arbitrary behaviors in the game. The programmers \nalso provide various other kinds of nodes for use within the graph, such as a \nnode that inverts its input, a node that produces a sine wave, or a node that \noutputs the current game time in seconds.\nVarious types of data might be sent along a data pathway. Some ports \nmight produce or expect Boolean data, while others might be coded to produce \nor expect data in the form of a unit ﬂ oat. Still others might operate on 3D vec-\ntors,  colors, integers, and so on. It’s important in such a system to ensure that \nconnections are only made between ports with compatible data types, or we \nmust provide some mechanism for automatically converting data types when \ntwo diﬀ erently typed ports are connected together. For example, connecting a \nunit-ﬂ oat output to a Boolean input might automatically cause any value less \nAnimal\nScare\nFoliage\nSway\nRadio\nTurnOn\nInvert\nIn\nOut\nEMP Gun\nFire\nFigure 14.18.  The EMP gun produces a 1 at its “Fire” output when ﬁ red. This can be connected \nto any input port that expects a Boolean value, in order to trigger the behavior associated \nwith that input.\n",
      "content_length": 1997,
      "extraction_method": "Direct"
    },
    {
      "page_number": 815,
      "chapter": null,
      "content": "793 \nthan 0.5 to be converted to false, and any value greater than or equal to 0.5 to \nbe converted to true. This is the essence of GUI-based event systems like Un-\nreal Engine 3’s Kismet. A screen shot of Kismet is shown in Figure 14.19.\n14.7.11.2. Some Pros and Cons of GUI-Based Programming\nThe beneﬁ ts of a  graphical user interface over a straightforward, text-ﬁ le-\nbased scripting language are probably prett y obvious: ease of use, a gradual \nlearning curve with the potential for in-tool help and tool tips to guide the \nuser, and plenty of error-checking. The downsides of a ﬂ ow-chart style GUI \ninclude the high cost to develop, debug, and maintain such a system, the addi-\ntional complexity, which can lead to annoying or sometimes schedule-killing \nbugs, and the fact that designers are sometimes limited in what they can do \nwith the tool. A text-ﬁ le based programming language has some distinct ad-\nvantages over a GUI-based programming system, including its relative sim-\nplicity (meaning that it is much less prone to bugs), the ability to easily search \nand replace within the source code, and the freedom of each user to choose the \ntext editor with which they are most comfortable.\n14.7. Events and Message-Passing\nFigure 14.19.  Unreal Engine 3’s Kismet.\n",
      "content_length": 1279,
      "extraction_method": "Direct"
    },
    {
      "page_number": 816,
      "chapter": null,
      "content": "794 \n14. Runtime Gameplay Foundation Systems\n14.8. Scripting\nA scripting language can be deﬁ ned as a programming language whose pri-\nmary purpose is to permit users to control and customize the behavior of a \nsoft ware application. For example, the Visual Basic language can be used to \ncustomize the behavior of Microsoft  Excel; both MEL and Python can be used \nto customize the behavior of Maya. In the context of game engines, a script-\ning language is a high-level, relatively easy-to-use programming language that \nprovides its users with convenient access to most of the commonly used fea-\ntures of the engine. As such, a scripting language can be used by program-\nmers and non-programmers alike to develop a new game or to customize—or \n“mod”—an existing game.\n14.8.1. Runtime versus Data Deﬁ nition\nWe should be careful to make an important distinction here. Game scripting \nlanguages generally come in two ﬂ avors:\nz Data-deﬁ nition languages. The primary purpose of a data-deﬁ nition lan-\nguage is to permit users to create and populate data structures that are \nlater consumed by the engine. Such languages are oft en declarative (see \nbelow) and are either executed or parsed oﬀ -line or at runtime when the \ndata is loaded into memory.\nz Runtime scripting languages . Runtime scripting languages are intended to \nbe executed within the context of the engine at runtime. These languag-\nes are usually used to extend or customize the hard-coded functionality \nof the engine’s game object model and/or other engine systems.\nIn this section, we’ll focus primarily on using a runtime scripting language for \nthe purpose of implementing gameplay features by extending and custom-\nizing the game’s object model.\n14.8.2. Programming Language Characteristics\nIn our discussion of scripting languages , it will be helpful for us all to be on \nthe same page with regard to programming language terminology. There are \nall sorts of programming languages out there, but they can be classiﬁ ed ap-\nproximately according to a relatively small number of criteria. Let’s take a \nbrief look at these criteria:\nz Interpreted versus compiled languages. The source code of a compiled lan-\nguage is translated by a program called a compiler into machine code, \nwhich can be executed directly by the CPU. In contrast, the source code \n",
      "content_length": 2327,
      "extraction_method": "Direct"
    },
    {
      "page_number": 817,
      "chapter": null,
      "content": "795 \nof an interpreted language is either parsed directly at runtime or is pre-\ncompiled into platform-independent byte code , which is then executed \nby a virtual machine at runtime. A virtual machine acts like an emulation \nof an imaginary CPU, and byte code acts like a list of machine language \ninstructions that are consumed by this CPU. The beneﬁ t of a virtual \nmachine is that it can be quite easily ported to almost any hardware \nplatform and embedded within a host application like a game engine. \nThe biggest cost we pay for this ﬂ exibility is execution speed—a virtual \nmachine usually executes its byte code instructions much more slowly \nthan the native CPU executes its machine language instructions.\nz Imperative langages. In an imperative language, a program is described \nby a sequence of instructions, each of which performs an operation and/\nor changes the state of data in memory. C and C++ are imperative lan-\nguages.\nz Declarative languages. A declarative language describes what is to be done \nbut does not specify exactly how the result should be obtained. That de-\ncision is left  up to the people implementing the language. Prolog is an \nexample of a declarative language. Mark-up languages like HTML and \nTeX can also be classiﬁ ed as declarative languages.\nz Functional languages. Functional languages, which are technically a sub-\nset of declarative languages, aim to avoid state altogether. In a func-\ntional language, programs are deﬁ ned by a collection of functions. Each \nfunction produces its results with no side-eﬀ ects (i.e., it causes no ob-\nservable changes to the system, other than to produce its output data). \nA program is constructed by passing input data from one function to the \nnext until the ﬁ nal desired result has been generated. These languages \ntend to be well-suited to implementing data-processing pipelines. Oca-\nml, Haskell, and F# are examples of functional languages.\nz Procedural versus object-oriented languages. In a procedural language, the \nprimary atom of program construction is the procedure (or function). \nThese procedures and functions perform operations, calculate results, \nand/or change the state of various data structures in memory. In con-\nstrast, an object-oriented language’s primary unit of program construc-\ntion is the class, a data structure that is tightly coupled with a set of \nprocedures/functions that “know” how to manage and manipulate the \ndata within that data structure.\nz Reﬂ ective languages. In a reﬂ ective language, information about the data \ntypes, data member layouts, functions, and hierarchical class relation-\nships in the system is available for inspection at runtime. In a non-reﬂ ec-\n14.8. Scripting\n",
      "content_length": 2710,
      "extraction_method": "Direct"
    },
    {
      "page_number": 818,
      "chapter": null,
      "content": "796 \n14. Runtime Gameplay Foundation Systems\ntive language, the majority of this meta-information is known only at \ncompile time; only a very limited amount of it is exposed to the runtime \ncode. C# is an example of a reﬂ ective language, while C and C++ are \nexamples of non-reﬂ ective languages.\n14.8.2.1. Typical Characteristics of Game Scripting Languages\nThe characteristics that set a game scripting language apart from its native pro-\ngramming language brethren include:\nz Interpreted. Most game scripting languages are interpreted by a virtual \nmachine, not compiled. This choice is made in the interest of ﬂ exibility, \nportability, and rapid iteration (see below). When code is represented \nas platform-independent byte code, it can easily be treated like data by \nthe engine. It can be loaded into memory just like any other asset rather \nthan requiring help from the operating system (as is necessary with a \nDLL on a PC platform or an IRX on the PLAYSTATION 3, for example). \nBecause the code is executed by a virtual machine rather than directly \nby the CPU, the game engine is aﬀ orded a great deal of ﬂ exibility re-\ngarding how and when script code will be run.\nz Lightweight. Most game scripting languages have been designed for \nuse in an embedded system. As such, their virtual machines tend to be \nsimple, and their memory footprints tend to be quite small.\nz Support for rapid iteration. Whenever native code is changed, the program \nmust be recompiled and relinked, and the game must be shut down \nand rerun in order to see the eﬀ ects of the changes. On the other hand, \nwhen script code is changed, the eﬀ ects of the changes can usually be \nseen very rapidly. Some game engines permit script code to be reloaded \non the ﬂ y, without shutt ing down the game at all. Others require the \ngame to be shut down and rerun. But either way, the turnaround time \nbetween making a change and seeing its eﬀ ects in-game is usually much \nfaster than when making changes to the native language source code.\nz Convenience and ease of use. Scripting languages are oft en customized to \nsuit the needs of a particular game. Features can be provided that make \ncommon tasks simple, intuitive, and less error-prone. For example, a \ngame scripting language might provide functions for ﬁ nding game ob-\njects by name, sending and handling events, pausing or manipulating \nthe passage of time, waiting for a speciﬁ ed amount of time to pass, im-\nplementing ﬁ nite state machines, exposing tweakable parameters to the \nworld editor for use by the game designers, or even handling network \nreplication for multiplayer games.\n",
      "content_length": 2628,
      "extraction_method": "Direct"
    },
    {
      "page_number": 819,
      "chapter": null,
      "content": "797 \n14.8.3. Some Common Game Scripting Languages\nWhen implementing a runtime game scripting system, we have one funda-\nmental choice to make: Do we select a third-party commercial or open-source \nlanguage and customize it to suit our needs, or do we design and implement \na custom language from scratch?\nCreating a custom language from scratch is usually not worth the hassle \nand the cost of maintenance throughout the project. It can also be diﬃ  cult or \nimpossible to hire game designers and programmers who are already familiar \nwith a custom, in-house language, so there’s usually a training cost as well. \nHowever, this is clearly the most ﬂ exible and customizable approach, and that \nﬂ exibility can be worth the investment.\nFor many studios, it is more convenient to select a reasonably well-known \nand mature scripting language and extend it with features speciﬁ c to your \ngame engine. There are a great many third-party scripting languages from \nwhich to choose, and many are mature and robust, having been used in a \ngreat many projects both within and outside the game industry.\nIn the following sections, we’ll explore a number of custom game script-\ning languages and a number of game-agnostic languages that are commonly \nadapted for use in game engines.\n14.8.3.1. QuakeC\nId Soft ware’s John Carmack implemented a custom scripting language for \nQuake, known as QuakeC (QC). This language was essentially a simpliﬁ ed \nvariant of the C programming language with direct hooks into the Quake en-\ngine. It had no support for pointers or deﬁ ning arbitrary structs, but it could \nmanipulate entities (Quake’s name for game objects) in a convenient manner, \nand it could be used to send and receive/handle game events. QuakeC is an \ninterpreted, imperative, procedural programming language.\nThe power that QuakeC put into the hands of gamers is one of the fac-\ntors that gave birth to what is now known as the mod community. Scripting \nlanguages and other forms of data-driven customization allow gamers to \nturn many commercial games into all sorts of new gaming experiences—from \nslight modiﬁ cations on the original theme to entirely new games.\n14.8.3.2. UnrealScript\nProbably the best-known example of an entirely custom scripting language \nis Unreal Engine’s UnrealScript . This language is based on a C++-like syntacti-\ncal style, and it supports most of the concepts that C and C++ programmers \nhave become accustomed to, including classes, local variables, looping, arrays \nand structs for data organization, strings, hashed string ids (called FName in \n14.8. Scripting\n",
      "content_length": 2591,
      "extraction_method": "Direct"
    },
    {
      "page_number": 820,
      "chapter": null,
      "content": "798 \n14. Runtime Gameplay Foundation Systems\nUnreal), and object references (but not free-form pointers). In addition, Un-\nrealScript provides a number of extremely powerful game-speciﬁ c features, \nwhich we’ll explore brieﬂ y below. UnrealScript is an interpreted, imperative, \nobject-oriented language.\nAbility to Extend the Class Hierarchy\nThis is perhaps UnrealScript’s biggest claim to fame. The Unreal object model \nis essentially a monolithic class hierarchy, with add-on components provid-\ning interfaces to various engine systems. The root classes in the hierarchy are \nknown as native classes, because they are implemented in the native C++ lan-\nguage. But UnrealScript’s real power comes from the fact that it can be used to \nderive new classes that are implemented entirely in script.\nThis may not sound like a big deal until you try to imagine how you \nwould implement such a thing! In eﬀ ect, UnrealScript redeﬁ nes and extends \nC++’s native object model, which is really quite astounding. For native Unreal \nclasses, the UnrealScript source ﬁ les (normally named with the extension .uc) \ntake the place of C++’s header ﬁ les (.h ﬁ les) as the primary deﬁ nition of each \nclass—the UnrealScript compiler actually generates the C++ .h ﬁ les from the .uc \nﬁ les, and the programmer implements the classes in regular .cpp source ﬁ les. \nDoing this allows the UnrealScript compiler to introduce additional features \ninto every Unreal class, and these features permit new script-only classes to be \ndeﬁ ned by users that inherit from native classes or other script-only classes.\nLatent Functions\nLatent functions are functions whose execution may span multiple frames of \ngameplay. A latent function can execute some instructions and then “go to \nsleep” waiting for an event or for a speciﬁ ed amount of time to pass. When the \nrelevant event occurs or the time period elapses, the function is “woken up” by \nthe engine, and it continues executing where it left  oﬀ . This feature is highly use-\nful for managing behaviors in the game that depend upon the passage of time.\nConvenient Linkage to UnrealEd\nThe data members of any UnrealScript-based class can be optionally marked with \na simple annotation, indicating that that data member is to be made available for \nviewing and editing in Unreal’s world editor , UnrealEd . No GUI programming \nis required. This makes data-driven game design extremely easy (as long as Un-\nrealEd’s built-in data member editing GUI suits your needs, of course).\nNetwork Replication for Multiplayer Games\nIndividual data elements in UnrealScript can be marked for replication. In \nUnreal networked games, each game object exists in its full form on one \n",
      "content_length": 2696,
      "extraction_method": "Direct"
    },
    {
      "page_number": 821,
      "chapter": null,
      "content": "799 \nparticular machine; all the other machines have a lightweight version of the \nobject known as a remote proxy . When you mark a data member for replica-\ntion, you are telling the engine that you want that data to be replicated from \nthe master object to all of the remote proxies. This allows a programmer or \ndesigner to easily control which data should be made available across the \nnetwork. This indirectly controls the amount of network bandwidth required \nby the game.\n14.8.3.3. Lua\nLua is a well-known and popular scripting language that is easy to integrate \ninto an application such as a game engine. The Lua website (htt p://www.lua.\norg/about.html) calls the language the “leading scripting language in games.”\nAccording to the Lua website, Lua’s key beneﬁ ts are:\nz Robust and mature. Lua has been used on numerous commercial prod-\nucts, including Adobe’s Photoshop Lightroom, and many games, includ-\ning World of Warcraft .\nz Good documentation. Lua’s reference manual [21] is complete and \nunderstandable and is available in online and book formats. A number \nof books have been writt en about Lua, including [22] and [43].\nz Excellent runtime performance. Lua executes its byte code more quickly \nand eﬃ  ciently than many other scripting languages.\nz Portable. Out of the box, Lua runs on all ﬂ avors of Windows and \nUNIX, mobile devices, and embedded microprocessors. Lua is writt en \nin a portable manner, making it easy to adapt to new hardware plat-\nforms.\nz Designed for embedded systems. Lua’s memory footprint is very small \n(approximately 350 kB for the interpreter and all libraries).\nz Simple, powerful, and extensible. The core Lua language is very small and \nsimple, but it is designed to support meta-mechanisms that extend its \ncore functionality in virtually limitless ways. For example, Lua itself \nis not an object-oriented language, but OOP support can and has been \nadded via a meta-mechanism.\nz Free. Lua is open source and is distributed under the very liberal MIT \nlicense.\nLua is a dynamically typed language, meaning that variables don’t have \ntypes—only values do. (Every value carries its type information along with it.) \nLua’s primary data structure is the table, also known as an associative array. A \ntable is essentially a list of key-value pairs with an optimized ability to index \ninto the array by key.\n14.8. Scripting\n",
      "content_length": 2372,
      "extraction_method": "Direct"
    },
    {
      "page_number": 822,
      "chapter": null,
      "content": "800 \n14. Runtime Gameplay Foundation Systems\nLua provides a convenient interface to the C language—the Lua virtual \nmachine can call and manipulate functions writt en in C as easily as it can \nthose writt en in Lua itself.\nLua treats blocks of code, called chunks, as ﬁ rst-class objects that can be \nmanipulated by the Lua program itself. Code can be executed in source code \nformat or in precompiled byte code format. This allows the virtual machine \nto execute a string that contains Lua code, just as if the code were compiled \ninto the original program. Lua also supports some powerful advanced pro-\ngramming constructs, including coroutines. This is a simple form of coopera-\ntive multitasking , in which each thread must yield the CPU to other threads \nexplicitly (rather than being time-sliced as in a preemptive multithreading \nsystem).\nLua does have some pitfalls. For example, its ﬂ exible function binding \nmechanism makes it possible (and quite easy) to redeﬁ ne an important global \nfunction like sin() to perform a totally diﬀ erent task (which is usually not \nsomething one intends to do). But all in all, Lua has proven itself to be an ex-\ncellent choice for use as a game scripting language.\n14.8.3.4. Python\nPython is a procedural, object-oriented, dynamically typed scripting lan-\nguage, designed with ease of use, integration with other programming lan-\nguages, and ﬂ exibility in mind. Like Lua, Python is a common choice for use \nas a game scripting language. According to the oﬃ  cial Python website (htt p://\nwww.python.org), some of Python’s best features include:\nz Clear and readable syntax. Python code is easy to read, in part because \nthe syntax enforces a speciﬁ c indentation style. (It actually parses the \nwhitespace used for intentation in order to determine the scope of each \nline of code.)\nz Reﬂ ective language. Python includes powerful runtime introspection \nfacilities. Classes in Python are ﬁ rst-class objects, meaning they can be \nmanipulated and queried at runtime, just like any other object.\nz Object-oriented. One advantage of Python over Lua is that OOP is built \ninto the core language. This makes integrating Python with a game’s \nobject model a litt le easier.\nz Modular. Python supports hierarchical packages, encouraging clean \nsystem design and good encapsulation.\nz Exception-based error handling. Exceptions make error-handling code in \nPython simpler, more elegant, and more localized than similar code in a \nnon-exception based language.\n",
      "content_length": 2498,
      "extraction_method": "Direct"
    },
    {
      "page_number": 823,
      "chapter": null,
      "content": "801 \nz Extensive standard libraries and third-party modules. Python libraries exist \nfor virtually every task imaginable. (Really!)\nz Embeddable. Python can be easily embedded into an application, such as \na game engine.\nz Extensive documentation. There’s plenty of documentation and tutorials \non Python, both online and in book form. A good place to start is the \nPython website, htt p://www.python.org.\nPython syntax is reminiscent of C in many respects (for example, its use \nof the = operator for assignment and == for equality testing). However, in \nPython, code indentation serves as the only means of deﬁ ning scope (as opposed \nto C’s opening and closing braces). Python’s primary data structures are the \nlist—a linearly indexed sequence of atomic values or other nested lists—and \nthe dictionary—a table of key-value pairs. Each of these two data structures \ncan hold instances of the other, allowing arbitrarily complex data structures to \nbe constructed easily. In addition, classes—uniﬁ ed collections of data elements \nand functions—are built right into the language.\nPython supports duck typing , which is a style of dynamic typing in which \nthe functional interface of an object determines its type (rather than being de-\nﬁ ned by a static inheritance hierarchy). In other words, any class that supports \na particular interface (i.e., a collection of functions with speciﬁ c signatures) \ncan be used interchangeably with any other class that supports that same \ninterface. This is a powerful paradigm: In eﬀ ect, Python supports polymor-\nphism without requiring the use of inheritance. Duck typing is similar in some \nrespects to C++ template meta-programming, although it is arguably more \nﬂ exible because the bindings between caller and callee are formed dynami-\ncally, at runtime. Duck typing gets its name from the well-known phrase (at-\ntributed to James Whitcomb Riley), “If it walks like a duck and quacks like a \nduck, I would call it a duck.” See htt p://en.wikipedia.org/wiki/Duck_typing \nfor more information on duck typing.\nIn summary, Python is easy to use and learn, embeds easily into a game \nengine, integrates well with a game’s object model, and can be an excellent \nand powerful choice as a game scripting language.\n14.8.3.5. Pawn / Small / Small-C\nPawn is a lightweight, dynamically typed, C-like scripting language created \nby Marc Peter. The language was formerly known as Small, which itself was \nan evolution of an earlier subset of the C language called Small-C, writt en by \nRon Cain and James Hendrix. It is an interpreted language—the source code \nis compiled into byte code (also known as P-code), which is interpreted by a \nvirtual machine at runtime.\n14.8. Scripting\n",
      "content_length": 2716,
      "extraction_method": "Direct"
    },
    {
      "page_number": 824,
      "chapter": null,
      "content": "802 \n14. Runtime Gameplay Foundation Systems\nPawn was designed to have a small memory footprint and to execute its \nbyte code very quickly. Unlike C, Pawn’s variables are dynamically typed. \nPawn also supports ﬁ nite state machines, including state-local variables. This \nunique feature makes it a good ﬁ t for many game applications. Good online \ndocumentation is available for Pawn (htt p://www.compuphase.com/pawn/\npawn.htm). Pawn is open source and can be used free of charge under the \nZlib/libpng license (htt p://www.opensource.org/licenses/zlib-license.php).\nPawn’s C-like syntax makes it easy to learn for any C/C++ programmer \nand easy to integrate with a game engine writt en in C. Its ﬁ nite state machine \nsupport can be very useful for game programming. It has been used success-\nfully on a number of game projects, including Freaky Flyers by Midway. Pawn \nhas shown itself to be a viable game scripting language.\n14.8.4. Architectures for Scripting\nScript code can play all sorts of roles within a game engine. There’s a gamut of \npossible architectures, from tiny snippets of script code that perform simple \nfunctions on behalf of an object or engine system to high-level scripts that \nmanage the operation of the game. Here are just a few of the possible archi-\ntectures:\nz Scripted callbacks . In this approach, the engine’s functionality is largely \nhard-coded in the native programming language, but certain key bits of \nfunctionality are designed to be customizable. This is oft en implement-\ned via a hook function or callback—a user-supplied function that is called \nby the engine for the purpose of allowing customization. Hook func-\ntions can be writt en in the native language, of course, but they can also \nbe writt en in a scripting language. For example, when updating game \nobjects during the game loop, the engine might call an optional callback \nfunction that can be writt en in script. This gives users the opportunity to \ncustomize the way in which the game object updates itself over time.\nz Scripted event handlers . An event handler is really just a special type of \nhook function whose purpose is to allow a game object to respond to \nsome relevant occurrence within the game world (e.g., responding to \nan explosion going oﬀ ) or within the engine itself (e.g., responding to \nan out-of-memory condition). Many game engines allow users to write \nevent handler hooks in script as well as in the native language.\nz Extending game object types, or deﬁ ning new ones, with script . Some script-\ning languages allow game object types that have been implemented in \nthe native language to be extended via script. In fact, callbacks and event \nhandlers are examples of this on a small scale, but the idea can be ex-\n",
      "content_length": 2749,
      "extraction_method": "Direct"
    },
    {
      "page_number": 825,
      "chapter": null,
      "content": "803 \ntended even to the point of allowing entirely new types of game objects \nto be deﬁ ned in script. This might be done via inheritance (i.e., deriving a \nclass writt en in script from a class writt en in the native language) or via \ncomposition /\n \naggregation (i.e., att aching an instance of a scripted class to a \nnative game object).\nz Scripted components or properties . In a component- or property-based \ngame object model, it only makes sense to permit new components or \nproperty objects to be constructed partially or entirely in script. This ap-\nproach was used by Gas Powered Games for Dungeon Siege (htt p://www.\ndrizzle.com /~scott b /gdc /game-objects.ppt). The game object model was \nproperty-based, and it was possible to implement properties in either \nC++ or Gas Powered Games’ custom scripting language, Skrit (htt p://\nds.heavengames.com/library/dstk/skrit/skrit). By the end of the project, \nthey had approximately 148 scripted property types and 21 native C++ \nproperty types.\nz Script-driven engine systems . Script might be used to drive an entire \nengine system. For example, the game object model could conceiv-\nably be written entirely in script, calling into the native engine code \nonly when it requires the services of lower-level engine compo-\nnents.\nz Script-driven game. Some game engines actually ﬂ ip the relationship \nbetween the native language and the scripting language on its head. \nIn these engines, the script code runs the whole show, and the native \nengine code acts merely as a library that is called to access certain \nhigh-speed features of the engine. The Panda3D engine (htt p://www.\npanda3d.org) is an example of this kind of architecture. Panda3D games \ncan be writt en entirely in the Python language, and the native engine \n(implemented in C++) acts like a library that is called by script code. \n(Panda3D games can also be writt en entirely in C++.)\n14.8.5. Features of a Runtime Game Scripting Language\nThe primary purpose of many game scripting languages is to implement \ngameplay features, and this is oft en accomplished by augmenting and cus-\ntomizing a game’s object model. In this section, we’ll explore some of the most \ncommon requirements and features of such a scripting system.\n14.8.5.1. Interface with the Native Programming Language\nIn order for a scripting language to be useful, it must not operate in a vacuum. \nIt’s imperative for the game engine to be able to execute script code, and it’s \n14.8. Scripting\n",
      "content_length": 2483,
      "extraction_method": "Direct"
    },
    {
      "page_number": 826,
      "chapter": null,
      "content": "804 \n14. Runtime Gameplay Foundation Systems\nusually equally important for script code to be capable of initiating operations \nwithin the engine as well. \nA runtime scripting language’s virtual machine is generally embedded \nwithin the game engine. The engine initializes the virtual machine, runs script \ncode whenever required, and manages those scripts’ execution. The unit of \nexecution varies depending on the speciﬁ cs of the language and the game’s \nimplementation.\nz In a functional scripting language, the function is oft en the primary unit of \nexecution. In order for the engine to call a script function, it must look up \nthe byte code corresponding to the name of the desired function and spawn \na virtual machine to execute it (or instruct an existing VM to do  so).\nz In an object-oriented scripting language, classes are typically the prima-\nry unit of execution. In such a system, objects can be spawned and de-\nstroyed, and methods (member functions) can be invoked on individual \nclass instances.\nIt’s usually beneﬁ cial to allow two-way communication between script \nand native code. Therefore, most scripting languages allowing native code to \nbe invoked from script as well. The details are language- and implementation-\nspeciﬁ c, but the basic approach is usually to allow certain script functions to \nbe implemented in the native language rather than in the scripting language. \nTo call an engine function, script code simply makes an ordinary function call. \nThe virtual machine detects that the function has a native implementation, \nlooks up the corresponding native function’s address (perhaps by name or via \nsome other kind of unique function identiﬁ er), and calls it. For example, some \nor all of the member functions of a Python class or module can be implement-\ned using C functions. Python maintains a data structure, known as a method \ntable , that maps the name of each Python function (represented as a string) to \nthe address of the C function that implements it.\nCase Study: Naughty Dog’s DC Language\nAs an example, let’s have a brief look at how Naughty Dog’s runtime scripting \nlanguage, a language called DC, was integrated into the engine.\nDC is a variant of the Scheme language (which is itself a variant of Lisp ). \nChunks of executable code in DC are known as script lambdas , which are the \napproximate equivalent of functions or code blocks in the Lisp family of lan-\nguages. A DC programmer writes script lambdas and identiﬁ es them by giving \nthem globally unique names. The DC compiler converts these script lambdas \ninto chunks of byte code, which are loaded into memory when the game runs \nand can be looked up by name using a simple functional interface in C++.\n",
      "content_length": 2717,
      "extraction_method": "Direct"
    },
    {
      "page_number": 827,
      "chapter": null,
      "content": "805 \nOnce the engine has a pointer to a chunk of script lambda byte code , it can \nexecute the code by calling a function in the engine and passing the pointer to \nthe byte code to it. The function itself is surprisingly simple. It spins in a loop, \nreading byte code instructions one-by-one, and executing each instruction. \nWhen all instructions have been executed, the function returns.\nThe virtual machine contains a bank of registers, which can hold any kind \nof data the script may want to deal with. This is implemented using a variant \ndata type—a union of all the data types (see 14.7.4 for a discussion of vari-\nants). Some instructions cause data to be loaded into a register; others cause \nthe data held in a register to be looked up and used. There are instructions for \nperforming all of the mathematical operations available in the language, as \nwell as instructions for performing conditional checks—implementations of \nDC’s (if …), (when …), and (cond …) instructions and so on.\nThe virtual machine also supports a function call stack. Script lambdas in \nDC can call other script lambdas (i.e., functions) that have been deﬁ ned by \na script programmer via DC’s (defun …) syntax. Just like any procedural \nprogramming language, a stack is needed to keep track of the states of the \nregisters and the return address when one function calls another. In the DC \nvirtual machine, the call stack is literally a stack of register banks—each new \nfunction gets its own private bank of registers. This prevents us from having \nto save oﬀ  the state of the registers, call the function, and then restore the reg-\nisters when the called function returns. When the virtual machine encounters \na byte code instruction that tells it to call another script lambda, the byte code \nfor that script lambda is looked up by name, a new stack frame is pushed, and \nexecution continues at the ﬁ rst instruction of that script lambda. When the vir-\ntual machine encounters a return instruction, the stack frame is popped from \nthe stack, along with the return “address” (which is really just the index of the \nbyte code instruction in the calling script lambda aft er the one that called the \nfunction in the ﬁ rst place).\nThe following pseudocode should give you a feel for what the core in-\nstruction-processing loop of the DC virtual machine looks like:\nvoid DcExecuteScript(DCByteCode* pCode)\n{\n DCStackFrame* \npCurStackFrame =  \n       \nDcPushStackFrame(pCode);\n \n// Keep going until we run out of stack frames (i.e.,  \n \n// the top-level script lambda \"function\" returns).\n  while (pCurStackFrame != NULL)\n  {\n14.8. Scripting\n",
      "content_length": 2627,
      "extraction_method": "Direct"
    },
    {
      "page_number": 828,
      "chapter": null,
      "content": "806 \n14. Runtime Gameplay Foundation Systems\n \n \n// Get the next instruction. We will never run    \n \n \n// out, because the return instruction is always \n \n \n// last, and it will pop the current stack frame   \n  // \nbelow.\n  DCInstruction& \ninstr\n   = \npCurStackFrame->GetNextInstruction();\n \n \n// Perform the operation of the instruction.\n  switch \n(instr.GetOperation())\n  {\n  case \nDC_LOAD_REGISTER_IMMEDIATE:\n   {\n \n \n \n \n// Grab the immediate value to be loaded  \n \n    // \nfrom the instruction.\n    Variant& \ndata = instr.GetImmediateValue();\n \n \n \n \n// Also determine into which register to  \n \n    // \nput it.\n    U32 \niReg = instr.GetDestRegisterIndex();\n \n \n \n \n// Grab the register from the stack frame.\n    Variant& \nreg\n     \n= pCurStackFrame->GetRegister(iReg);\n \n \n \n \n// Store the immediate data into the  \n \n \n    // \nregister.\nreg = data;\n   }\n   break;\n \n \n// Other load and store register operations...\n  case \nDC_ADD_REGISTERS:\n   {\n \n \n \n \n// Determine the two registers to add. The\n \n \n \n \n// result will be stored in register A.\n    U32 \niRegA = instr.GetDestRegisterIndex();\n    U32 \niRegB = instr.GetSrcRegisterIndex();\n \n \n \n \n// Grab the 2 register variants from the  \n \n    // \nstack.\n    Variant& \ndataA\n     = \npCurStackFrame->GetRegister(iRegA);\n    Variant& \ndataB\n     = \npCurStackFrame->GetRegister(iRegB);\n \n \n \n \n// Add the registers and store in \n    // \nregister A.\ndataA = dataA + dataB;\n   }\n   break;\n",
      "content_length": 1443,
      "extraction_method": "Direct"
    },
    {
      "page_number": 829,
      "chapter": null,
      "content": "807 \n \n \n// Other math operations...\n  case \nDC_CALL_SCRIPT_LAMBDA:\n   {\n \n \n \n \n// Determine in which register the name of   \n \n \n \n \n// the script lambda to call is stored. \n \n \n \n \n// (Presumably it was loaded by a previous   \n    // \nload instr.)\n    U32 \niReg = instr.GetSrcRegisterIndex();\n \n \n \n \n// Grab the appropriate register, which  \n \n \n \n \n \n// contains the name of the lambda to call.\n    Variant& \nlambda\n     = \npCurStackFrame->GetRegister(iReg);\n \n \n \n \n// Look up the byte code of the lambda by   \n    // \nname.\n    DCByteCode* \npCalledCode\n     = \nDcLookUpByteCode(lambda.AsStringId());\n \n \n \n \n// Now \"call\" the lambda by pushing a new   \n    // \nstack frame.\n    if \n(pCalledCode)\n    {\npCurStackFrame\n      = \nDcPushStackFrame(pCalledCode);\n    }\n   }\n   break;\n  case \nDC_RETURN:\n   {\n \n \n \n \n// Just pop the stack frame. If we’re in  \n \n \n \n \n \n// the top lambda on the stack, this \n \n \n \n \n// function will return NULL, and the loop   \n    // \nwill terminate.\npCurStackFrame = DcPopStackFrame();\n   }\n   break;\n \n \n// Other instructions...\n  // \n...\n \n \n} // end switch\n \n} // end for\n}\nIn the above example, we assume that the global functions DcPushStack\nFrame() and DcPopStackFrame() manage the stack of register banks for us \n14.8. Scripting\n",
      "content_length": 1272,
      "extraction_method": "Direct"
    },
    {
      "page_number": 830,
      "chapter": null,
      "content": "808 \n14. Runtime Gameplay Foundation Systems\nin some suitable way and that the global function DcLookUpByteCode() is \ncapable of looking up any script lambda by name. We won’t show implemen-\ntations of those functions here, because the purpose of this example is simply \nto show how the inner loop of a script virtual machine might work, not to \nprovide a complete functional implementation.\nDC script lambdas can also call native functions—i.e., global functions \nwritt en in C++ that serve as hooks into the engine itself. When the virtual ma-\nchine comes across an instruction that calls a native function, the address of \nthe C++ function is looked up by name using a global table that has been hard-\ncoded by the engine programmers. If a suitable C++ function is found, the \narguments to the function are taken from registers in the current stack frame, \nand the function is called. This implies that the C++ function’s arguments are \nalways of type Variant. If the C++ function returns a value, it too must be a \nVariant, and its value will be stored into a register in the current stack frame \nfor possible use by subsequent instructions.\nThe global function table might look something like this:\ntypedef Variant DcNativeFunction(U32 argCount, \n \nVariant* aArgs);\nstruct DcNativeFunctionEntry\n{\n StringId \n   m_name;\n \nDcNativeFunction* m_pFunc;\n};\nDcNativeFunctionEntry g_aNativeFunctionLookupTable[] = {\n \n{ SID(\"get-object-pos\"), DcGetObjectPos },\n \n{ SID(\"animate-object\"), DcAnimateObject },\n \n// etc.\n \n// ...\n};\nA native DC function implementation might look something like the fol-\nlowing. Notice how the Variant arguments are passed to the function as an \narray. The function must verify that the number of arguments passed to it \nequals the number of arguments it expects. It must also verify that the types of \nthe argument(s) are as expected and be prepared to handle errors that the DC \nscript programmer may have made when calling the function.\nVariant DcGetObjectPos(U32 argCount, Variant* aArgs)\n{\n \n// Set up a default return value.\n Variant \nresult;\n result.\nSetAsVector(Vector(0.0f, 0.0f, 0.0f));\n",
      "content_length": 2123,
      "extraction_method": "Direct"
    },
    {
      "page_number": 831,
      "chapter": null,
      "content": "809 \n \nif (argCount != 1)\n {\n  DcErrorMessage(\"get-object-pos: \n   Invalid \narg count.\\n\");\nreturn result;\n }\n \nif (aArgs[0].GetType() != Variant::TYPE_STRING_ID)\n {\n  DcErrorMessage(\"get-object-pos: \nExpected \n   string \nid.\\n\");\nreturn result;\n }\n StringId \nobjectName = aArgs[0].AsStringId();\n GameObject* \npObject = GameObject::LookUpByName\n(objectName);\n \nif (pObject == NULL)\n {\n  DcErrorMessage(\n \n \n \n\"get-object-pos: Object ‘%s’ not found.\\n\",\n   objectName.ToString());\nreturn result;\n }\n result.\nSetAsVector(pObject->GetPosition());\nreturn result;\n}\n14.8.5.2. Game Object Handles\nScript functions oft en need to interact with game objects, which themselves \nmay be implemented partially or entirely in the engine’s native language. The \nnative language’s mechanisms for referencing objects (e.g., pointers or refer-\nences in C++) won’t necessarily be valid in the scripting language. (It may not \nsupport pointers at all, for example.) Therefore, we need to come up with \nsome reliable way for script code to reference game objects.\nThere are a number of ways to accomplish this. One approach is to refer to \nobjects in script via opaque numeric handles . The script code can obtain object \nhandles in various ways. It might be passed a handle by the engine, or it might \nperform some kind of query, such as asking for the handles of all game objects \nwithin a radius of the player or looking up the handle that corresponds to a \nparticular object name. The script can then perform operations on the game \n14.8. Scripting\n",
      "content_length": 1533,
      "extraction_method": "Direct"
    },
    {
      "page_number": 832,
      "chapter": null,
      "content": "810 \n14. Runtime Gameplay Foundation Systems\nobject by calling native functions and passing the object’s handle as an argu-\nment. On the native language side, the handle is converted back into a pointer \nto the native object, and then the object can be manipulated as appropriate.\nNumeric handles have the beneﬁ t of simplicity and should be easy to sup-\nport in any scripting language that supports integer data. However, they can \nbe unintuitive and diﬃ  cult to work with. Another alternative is to use the \nnames of the objects, represented as strings, as our handles. This has some \ninteresting beneﬁ ts over the numeric handle technique. For one thing, strings \nare human-readable and intuitive to work with. There is a direct correspon-\ndence to the names of the objects in the game’s world editor. In addition, we \ncan choose to reserve certain special object names and give them “magic” \nmeanings. For example, in Naughty Dog’s scripting language, the reserved \nname “self” always refers to the object to which the currently-running script is \natt ached. This allows game designers to write a script, att ach it to an object in \nthe game, and then use the script to play an animation on the object by simply \nwriting (animate \"self\" name-of-animation).\nUsing strings as object handles has its pitfalls, of course. Strings oft en \noccupy more memory than integer ids. And because strings vary in length, \ndynamic memory allocation is required in order to copy them. String com-\nparisons are slow. Script programmers are apt to make mistakes when typing \nthe names of game objects, which can lead to bugs. In addition, script code can \nbe broken if someone changes the name of an object in the game world editor \nbut forgets to update the name of the object in script.\nHashed string ids overcome most of these problems by converting any \nstrings (regardless of length) into an integer. In theory, hashed string ids enjoy \nthe best of both worlds—they can be read by users just like strings, but they \nhave the runtime performance characteristics of an integer. However, for this \nto work, your scripting language needs to support hashed string ids in some \nway. Ideally, we’d like the script compiler to convert our strings into hashed \nids for us. That way, the runtime code doesn’t have to deal with the strings at \nall, only the hashed ids (except possibly for debugging purposes—it’s nice to \nbe able to see the string corresponding to a hashed id in the debugger). How-\never, this isn’t always possible in all scripting languages. Another approach is \nto allow the user to use strings in script and convert them into hashed ids at \nruntime, whenever a native function is called.\n14.8.5.3. Receiving and Handling Events\nEvents are a ubiquitous communication mechanism in most game engines. By \npermitt ing event handler functions to be writt en in script, we open up a pow-\nerful avenue for customizing the hard-coded behavior of our game.\n",
      "content_length": 2951,
      "extraction_method": "Direct"
    },
    {
      "page_number": 833,
      "chapter": null,
      "content": "811 \nEvents are usually sent to individual objects and handled within the con-\ntext of that object. Hence scripted event handlers need to be associated with \nan object in some way. Some engines use the game object type system for this \npurpose—scripted event handlers can be registered on a per-object-type basis. \nThis allows diﬀ erent types of game objects to respond in diﬀ erent ways to the \nsame event but ensures that all instances of each type respond in a consis-\ntent and uniform way. The event handler functions themselves can be simple \nscript functions, or they can be members of a class if the scripting language is \nobject-oriented. In either case, the event handler is typically passed a handle \nto the particular object to which the event was sent, much as C++ member \nfunctions are passed the this pointer.\nIn other engines, scripted event handlers are associated with individual \nobject instances rather than with object types. In this approach, diﬀ erent in-\nstances of the same type might respond diﬀ erently to the same event.\nThere are all sorts of other possibilities, of course. For example, in Naughty \nDog’s Uncharted engine, scripts are objects in their own right. They can be as-\nsociated with individual game objects, they can be att ached to regions (convex \nvolumes that are used to trigger game events), or they can exist as standalone \nobjects in the game world. Each script can have multiple states (that is, scripts \nare ﬁ nite state machines in the Uncharted engine). In turn, each state can have \none or more event handler code blocks. When a game object receives an event, \nit has the option of handling the event in native C++. It also checks for an at-\ntached script object, and if one is found, the event is sent to that script’s current \nstate. If the state has an event handler for the event, it is called. Otherwise, the \nscript simply ignores the event.\n14.8.5.4. Sending Events\nAllowing scripts to handle game events that are generated by the engine is \ncertainly a powerful feature. Even more powerful is the ability to generate and \nsend events from script code either back to the engine or to other scripts.\nIdeally, we’d like to be able not only to send predeﬁ ned types of events \nfrom script but to deﬁ ne entirely new event types in script. Implementing this \nis trivial if event types are strings. To deﬁ ne a new event type, the script pro-\ngrammer simply comes up with a new event type name and types it into his \nor her script code. This can be a highly ﬂ exible way for scripts to communicate \nwith one another. Script A can deﬁ ne a new event type and send it to Script B. \nIf Script B deﬁ nes an event handler for this type of event, we’ve implemented \na simple way for Script A to “talk” to Script B. In some game engines, event- or \nmessage-passing is the only supported means of inter-object communication \nin script. This can be an elegant yet powerful and ﬂ exible solution.\n14.8. Scripting\n",
      "content_length": 2956,
      "extraction_method": "Direct"
    },
    {
      "page_number": 834,
      "chapter": null,
      "content": "812 \n14. Runtime Gameplay Foundation Systems\n14.8.5.5. Object-Oriented Scripting Languages\nSome scripting languages are inherently object-oriented. Others do not sup-\nport objects directly but provide mechanisms that can be used to implement \nclasses and objects. In many engines, gameplay is implemented via an object-\noriented game object model of some kind. So it makes sense to permit some \nform of object-oriented programming in script as well.\nDeﬁ ning Classes in Scripts\nA class is really just a bunch of data with some associated functions. So any \nscripting language that permits new data structures to be deﬁ ned, and pro-\nvides some way to store and manipulate functions, can be used to implement \nclasses. For example, in Lua, a class can be built out of a table that stores data \nmembers and member functions.\nInheritance in Script\nObject-oriented languages do not necessarily support inheritance . However, \nif this feature is available, it can be extremely useful, just as it is in native pro-\ngramming languages like C++.\nIn the context of game scripting languages, there are two kinds of in-\nheritance: deriving scripted classes from other scripted classes and deriving \nscripted classes from native classes. If your scripting language is object-orient-\ned, chances are the former is supported out of the box. However, the latt er is \ntough to implement even if the scripting language supports inheritance. The \nproblem is bridging the gap between two languages and two low-level object \nmodels. We won’t get into the details of how this might be implemented here, \nas the implementation is bound to be speciﬁ c to the pair of languages being \nintegrated. UnrealScript is the only scripting language I’ve seen that allows \nscripted classes to derive from native classes in a seamless way.\nComposition/Aggregation in Script\nWe don’t need to rely on inheritance to extend a hierarchy of classes—we can \nalso use composition or aggregation to similar eﬀ ect. In script, then, all we \nreally need is a way to deﬁ ne classes and associate instances of those classes \nwith objects that have been deﬁ ned in the native programming language. For \nexample, a game object could have a pointer to an optional component writ-\nten entirely in script. We can delegate certain key functionality to the script \ncomponent, if it exists. The script component might have an Update() function \nthat is called whenever the game object is updated, and the scripted compo-\nnent might also be permitt ed to register some of its member functions/meth-\nods as event handlers. When an event is sent to the game object, it calls the \n",
      "content_length": 2623,
      "extraction_method": "Direct"
    },
    {
      "page_number": 835,
      "chapter": null,
      "content": "813 \nappropriate event handler on the scripted component, thus giving the script \nprogrammer an opportunity to modify or extend the behavior of the natively \nimplemented game object.\n14.8.5.6. Scripted Finite State Machines\nMany problems in game programming can be solved naturally using ﬁ nite \nstate machines (FSM). For this reason, some engines build the concept of ﬁ nite \nstate machines right into the core game object model. In such engines, every \ngame object can have one or more states, and it is the states—not the game \nobject itself—that contain the update function, event handler functions, and \nso on. Simple game objects can be created by deﬁ ning a single state, but more-\ncomplex game objects have the freedom to deﬁ ne multiple states, each with a \ndiﬀ erent update and event-handling behavior.\nIf your engine supports a state-driven game object model, it makes a lot of \nsense to provide ﬁ nite state machine support in the scripting language as well. \nAnd of course, even if the core game object model doesn’t support ﬁ nite state \nmachines natively, one can still provide state-driven behavior by using a state \nmachine on the script side. An FSM can be implemented in any programming \nlanguage by using class instances to represent states, but some scripting lan-\nguages provide tools especially for this purpose. An object-oriented scripting \nlanguage might provide custom syntax that allows a class to contains multiple \nstates, or it might provide tools that help the script programmer easily aggre-\ngate state objects together within a central hub object and then delegate the \nupdate and event-handling functions to it in a straightforward way. But even \nif your scripting language provides no such features, you can always adopt a \nmethodology for implementing FSMs and follow those conventions in every \nscript you write.\n14.8.5.7. Multithreaded Scripts\nIt’s  oft en useful to be able to execute multiple scripts in parallel. This is espe-\ncially true on today’s highly parallelized hardware architectures. If multiple \nscripts can run at the same time, we are in eﬀ ect providing parallel threads of \nexecution in script code, much like the threads provided by most multitasking \noperating systems. Of course, the scripts may not actually run in parallel—if \nthey are all running on a single CPU, the CPU must take turns executing each \none. However, from the point of view of the script programmer, the paradigm \nis one of parallel multithreading.\nMost scripting systems that provide parallelism do so via cooperative \nmultitasking . This means that a script will execute until it explicitly yields to \nanother script. This is in contrast with a preemptive multitasking approach, in \n14.8. Scripting\n",
      "content_length": 2731,
      "extraction_method": "Direct"
    },
    {
      "page_number": 836,
      "chapter": null,
      "content": "814 \n14. Runtime Gameplay Foundation Systems\nwhich the execution of any script could be interrupted at any time to permit \nanother script to execute.\nOne simple approach to cooperative multitasking in script is to permit \nscripts to explicitly go to sleep, waiting for something relevant to happen. A \nscript might wait for a speciﬁ ed number of seconds to elapse, or it might wait \nuntil a particular event is received. It might wait until another thread of execu-\ntion has reached a predeﬁ ned synchronization point. Whatever the reason, \nwhenever a script goes to sleep, it puts itself on a list of sleeping script threads \nand tells the virtual machine that it can start executing another eligible script. \nThe system keeps track of the conditions that will wake up each sleeping \nscript—when one of these conditions becomes true, the script(s) waiting on \nthe condition are woken up and allowed to continue executing.\nTo see how this works in practice, let’s look at an example of a multi-\nthreaded script. This script manages the animations of two characters and \na door. The two characters are instructed to walk up to the door—each one \nmight take a diﬀ erent, and unpredictable, amount of time to reach it. We’ll \nput the script’s threads to sleep while they wait for the characters to reach the \ndoor. Once they both arrive at the door, one of the two characters opens the \ndoor, which it does by playing an “open door” animation. Note that we don’t \nwant to hard-code the duration of the animation into the script itself. That \nway, if the animators change the animation, we won’t have to go back and \nmodify our script. So we’ll put the threads to sleep again while the wait for the \nanimation to complete. A script that accomplishes this is shown below, using \na simple C-like pseudocode syntax.\nfunction DoorCinematic\n{\n \nthread Guy1\n {\n \n \n// Ask guy1 to walk to the door.\n  CharacterWalkToPoint(guy1, \ndoorPosition);\n \n \nWaitUntil(ARRIVAL); // go to sleep until he gets   \n        // \nthere\n \n \n// OK, we’re there. Tell the other threads via a   \n  // \nsignal.\n  RaiseSignal(\"Guy1Arrived\");\n \n \n// Wait for the other guy to arrive as well.\n  WaitUntil(SIGNAL, \n\"Guy2Arrived\");\n \n \n// Now tell guy1 to play the \"open door\"  \n \n \n \n  // \nanimation.\n  CharacterAnimate(guy1, \n\"OpenDoor\");\n  WaitUntil(ANIMATION_DONE);\n",
      "content_length": 2331,
      "extraction_method": "Direct"
    },
    {
      "page_number": 837,
      "chapter": null,
      "content": "815 \n \n \n// OK, the door is open. Tell the other threads.\n  RaiseSignal(\"DoorOpen\");\n \n \n// Now walk thru the door.\n  CharacterWalkToPoint(guy1, \nbeyondDoorPosition);\n }\n \nthread Guy2\n {\n \n \n// Ask guy2 to walk to the door.\n  CharacterWalkToPoint(guy2, \ndoorPosition);\n \n \nWaitUntil(ARRIVAL); // go to sleep until he gets   \n        // \nthere\n \n \n// OK, we’re there. Tell the other threads via a   \n  // \nsignal.\n  RaiseSignal(\"Guy2Arrived\");\n \n \n// Wait for the other guy to arrive as well.\n  WaitUntil(SIGNAL, \n\"Guy1Arrived\");\n \n \n// Now wait until guy1 opens the door for me.\n  WaitUntil(SIGNAL, \n\"DoorOpen\");\n \n \n// OK, the door is open. Now walk thru the door.\n  CharacterWalkToPoint(guy2, \nbeyondDoorPosition);\n }\n}\nIn the above, we assume that our hypothetical scripting language pro-\nvides a simple syntax for deﬁ ning threads of execution within a single func-\ntion. We deﬁ ne two threads, one for Guy1 and one for Guy2.\nThe thread for Guy1 tells the character to walk to the door and then goes \nto sleep waiting for his arrival. We’re hand-waving a bit here, but let’s imagine \nthat the scripting language magically allows a thread to go to sleep, wait-\ning until a character in the game arrives at a target point to which he was \nrequested to walk. In reality, this might be implemented by arranging for the \ncharacter to send an event back to the script and then waking the thread up \nwhen the event arrives.\nOnce Guy1 arrives at the door, his thread does two things that warrant \nfurther explanation. First, it raises a signal called “Guy1Arrived.” Second, it \ngoes to sleep waiting for another signal called “Guy2Arrived.” If we look at \nthe thread for Guy2, we see a similar patt ern, only reversed. The purpose of \nthis patt ern of raising a signal and then waiting for another signal is used to \nsynchronize the two threads.\nIn our hypothetical scripting language, a signal is just a Boolean ﬂ ag with \na name. The ﬂ ag starts out false, but when a thread calls RaiseSignal(name), \n14.8. Scripting\n",
      "content_length": 2015,
      "extraction_method": "Direct"
    },
    {
      "page_number": 838,
      "chapter": null,
      "content": "816 \n14. Runtime Gameplay Foundation Systems\nthe named ﬂ ag’s value changes to true. Other threads can go to sleep, wait-\ning for a particular named signal to become true. When it does, the sleeping \nthread(s) wake up and continue executing. In this example, the two threads \nare using the “Guy1Arrived” and “Guy2Arrived” signals to synchronize with \none another. Each thread raises its signal and then waits for the other thread’s \nsignal. It does not matt er which signal is raised ﬁ rst—only when both signals \nhave been raised will the two threads wake up. And when they do, they will \nbe in perfect synchronization. Two possible scenarios are illustrated in Fig-\nure 14.20, one in which Guy1 arrives ﬁ rst, the other in which Guy2 arrives \nﬁ rst. As you can see, the order in which the signals are raised is irrelevant, and \nthe threads always end up in sync aft er both signals have been raised.\n14.9. High-Level Game Flow\n A game object model provides the foundations upon which a rich and en-\ntertaining collection of game object types can be implemented with which to \npopulate our game worlds. However, by itself, a game object model only per-\nmits us to deﬁ ne the kinds of objects that exist in our game world and how \nthey behave individually. It says nothing of the player’s objectives, what hap-\npens if he or she completes them, and what fate should befall the player if he \nor she fails.\nFor this, we need some kind of system to control high-level game ﬂ ow. \nThis is oft en implemented as a ﬁ nite state machine . Each state usually repre-\nWalk\nSignal\nWait\nWalk\nSignal\n(No Wait)\nGuy1\nGuy2\nSync\nWalk\nSignal\nWait\nWalk\nSignal\n(No Wait)\nGuy1\nGuy2\nSync\nFigure 14.20.  Two examples showing how a simple pattern of raising one signal and then \nwaiting on another can be used to synchronize a pair of script threads.\n",
      "content_length": 1827,
      "extraction_method": "Direct"
    },
    {
      "page_number": 839,
      "chapter": null,
      "content": "817 \nsents a single player objective or encounter and is associated with a particular \nlocale within the virtual game world. As the player completes each task, the \nstate machine advances to the next state, and the player is presented with a \nnew set of goals. The state machine also deﬁ nes what should happen in the \nevent of the player’s failure to accomplish the necessary tasks or objectives . \nOft en, failure sends the player back to the beginning of the current state, so he \nor she can try again. Sometimes aft er enough failures, the player has run out \nof “lives” and will be sent back to the main menu, where he or she can choose \nto play a new game. The ﬂ ow of the entire game, from the menus to the ﬁ rst \n“level” to the last, can be controlled through this high-level state machine.\nThe task system used in Naughty Dog’s Jak and Daxter and Uncharted fran-\nchises is an example of such a state-machine-based system. It allows for linear \nsequences of states (called tasks at Naughty Dog). It also permits parallel tasks, \nwhere one task branches out into two or more parallel tasks, which eventu-\nally merge back into the main task sequence. This parallel task feature sets \nthe Naughty Dog task graph apart from a regular state machine, since state \nmachines typically can only be in one state at a time.\n14.9. High-Level Game Flow\n",
      "content_length": 1348,
      "extraction_method": "Direct"
    },
    {
      "page_number": 840,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 841,
      "chapter": null,
      "content": "Part V\nConclusion\n",
      "content_length": 18,
      "extraction_method": "Direct"
    },
    {
      "page_number": 842,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 843,
      "chapter": null,
      "content": "15\nYou Mean There’s More?\nC\nongratulations! You’ve reached the end of your journey through the \nlandscape of game engine architecture in one piece (and hopefully none \nthe worse for wear). With any luck, you’ve learned a great deal about the \nmajor components that comprise a typical game engine. But of course, every \njourney’s end is another’s beginning. There’s a great deal more to be learned \nwithin each and every topic area covered within these pages. As technology \nand computing hardware continue to improve, more things will become pos-\nsible in games—and more engine systems will be invented to support them. \nWhat’s more, this book’s focus was on the game engine itself. We haven’t even \nbegun to discuss the rich world of gameplay programming, a topic that could \nﬁ ll many more volumes.\nIn the following brief sections, I’ll identify a few of the engine and game-\nplay systems we didn’t have room to cover in any depth in this book, and I’ll \nsuggest some resources for those who wish to learn more about them.\n15.1. Some Engine Systems We Didn’t Cover\n15.1.1. Audio\nI mentioned in Section 1.6.13 that audio oft en takes a back seat to other aspects \nof game development, much to the chagrin of the audio engineers, sound de-\n821\n",
      "content_length": 1244,
      "extraction_method": "Direct"
    },
    {
      "page_number": 844,
      "chapter": null,
      "content": "822 \n15. You Mean There’s More?\nsigners, voice actors, and composers who work so hard to add that all-too-\ncritical fourth dimension to the virtual game world. And yet, sadly, the same \nthing has happened in this book—I am out of room and out of time, so a full \ntreatment of audio will have to wait until the second edition. (In keeping with \na long and painfully unfortunate tradition in game development, once again \naudio gets the shaft !)\nThankfully, a number of other books and online resources do provide a \nwealth of information on audio development. First oﬀ , I recommend reading \nthe documentation for Microsoft ’s XACT sound authoring tool and runtime \nAPI, located on the MSDN website under XACT: Tutorials and Samples (htt p://\nmsdn.microsoft .com/en-us/library/bb172329(VS.85).aspx). XACT supports \nvirtually every audio feature the average game programmer would want, and \nits documentation is quite easy to digest. The Game Programming Gems book \nseries also includes a plethora of articles on audio—see [7] Section 6 and [40] \nSection 6.\n15.1.2. Movie Player\nMost games include a movie player for displaying prerendered movies, also \nknown as full-motion video (FMV). The basic components of the movie player \nare an interface to the streaming ﬁ le I/O system (see Section 6.1.3), a codec to \ndecode the compressed video stream, and some form of synchronization with \nthe audio playback system for the sound track.\nA number of diﬀ erent video encoding standards and corresponding \ncodecs are available, each one suited to a particular type of application. For \nexample, video CDs (VCD) and DVDs use  MPEG-1 and MPEG-2 (H.262) \ncodecs, respectively. The H.261 and H.263 standards are designed primar-\nily for online video conferencing applications. Games oft en use standards \nlike MPEG-4 part 2 (e.g., DivX ), MPEG-4 Part 10 / H.264, Windows Media \nVideo (WMV ), or Bink Video (a standard designed speciﬁ cally for games by \nRad Game Tools, Inc.). See htt p://en.wikipedia.org/wiki/Video_codec and \nhtt p://www.radgametools.com/bnkmain.htm for more information on video \ncodecs.\n15.1.3. Multiplayer Networking\nAlthough we have touched on a number of aspects of multiplayer game ar-\nchitecture and networking (e.g., Sections 1.6.14, 7.7, and 14.8.3.2), this book’s \ncoverage of the topic is far from complete. For an in-depth treatment of multi-\nplayer networking, see [3].\n",
      "content_length": 2391,
      "extraction_method": "Direct"
    },
    {
      "page_number": 845,
      "chapter": null,
      "content": "823 \n15.2. Gameplay Systems\n15.2. Gameplay Systems\nA game is of course much more than just its engine. On top of the gameplay \nfoundation layer (discussed in Chapter 14), you’ll ﬁ nd a rich assortment of \ngenre- and game-speciﬁ c gameplay systems. These systems tie the myriad \ngame engine technologies described in this book together into a cohesive \nwhole, breathing life into the game.\n15.2.1. Player Mechanics\nPlayer mechanics are of course the most important gameplay system. Each \ngenre is deﬁ ned by a general style of player mechanics and gameplay, and \nof course every game within a genre has its own speciﬁ c designs. As such, \nplayer mechanics is a huge topic. It involves the integration of human inter-\nface device systems, motion simulation, collision detection, animation, and \naudio, not to mention integration with other gameplay systems like the game \ncamera, weapons, cover, specialized traversal mechanics (ladders, swinging \nvines, etc.), vehicle systems, puzzle mechanics, and so on.\nClearly player mechanics are as varied as the games themselves, so there’s \nno one place you can go to learn all about them. It’s best to tackle this topic \nby studying a single genre at a time. Play games and try to reverse-engineer \ntheir player mechanics. Then try to implement them yourself! And as a very \nmodest start on reading, you can check out [7] Section 4.11 for a discussion of \nMario-style platformer player mechanics.\n15.2.2. Cameras\nA game’s camera system is almost as important as the player mechanics. In \nfact, the camera can make or break the gameplay experience. Each genre tends \nto have its own camera control style, although of course every game within a \nparticular genre does it a litt le bit diﬀ erently (and some very diﬀ erently!). See \n[6] Section 4.3 for some basic game camera control techniques. In the follow-\ning paragraphs, I’ll brieﬂ y outline some of the most prevalent kinds of cam-\neras in 3D games, but please note that this is far from a complete list.\nLook-at cameras.\n• \n This type of camera rotates about a target point and can \nbe moved in and out relative to this point.\nFollow cameras.\n• \n This type of camera is prevalent in platformer, third-\nperson shooter, and vehicle-based games. It acts much like a look-at \ncamera focused on the player character/avatar/vehicle, but its motion \ntypically lags the player. A follow camera alos includes advanced \ncollision detection and avoidance logic and provides the human player \n",
      "content_length": 2478,
      "extraction_method": "Direct"
    },
    {
      "page_number": 846,
      "chapter": null,
      "content": "824 \n15. You Mean There’s More?\nwith some degree of control over the camera’s orientation relative to the \nplayer avatar.\nFirst-person cameras.\n• \n As the player character moves about in the game \nworld, a ﬁ rst-person camera remains aﬃ  xed to the character’s virtual \neyes. The player typically has full control over the direction in which \nthe camera should be pointed, either via mouse or joypad control. \nThe look direction of the camera also translates directly into the aim \ndirection of the player’s weapon, which is typically indicated by a set of \ndisembodied arms and a gun att ached to the bott om of the screen, and a \nreticle at the center of the screen.\nRTS cameras.\n• \n Real-time strategy and god games tend to employ a camera \nthat ﬂ oats above the terrain, looking down at an angle. The camera can \nbe panned about over the terrain, but the pitch and yaw of the camera \nare usually not under direct player control.\nCinematic cameras.\n• \n Most three-dimensional games have at least some \ncinematic moments in which the camera ﬂ ies about within the scene \nin a more ﬁ lmic manner rather than being tethered to an object in the \ngame.\n15.2.3. Artiﬁ cial Intelligence\nAnother major component of most character-based games is artiﬁ cial intelli-\ngence (AI ). At its lowest level, an AI system is usually founded in technologies \nlike basic path ﬁ nding (which commonly makes use of the well-known A* \nalgorithm ), perception systems (line of sight, vision cones, knowledge of the \nenvironment, etc.) and some form of memory.\nOn top of these foundations, character control logic is implemented. \nA character control system determines how to make the character perform \nspeciﬁ c actions like locomoting, navigating unusual terrain features, using \nweapons, driving vehicles, taking cover, and so on. It typically involves com-\nplex interfaces to the collision, physics, and animation systems within the \nengine. Character control is discussed in detail in Sections 11.11 and 11.12.\nAbove the character control layer, an AI system typically has goal sett ing \nand decision making logic, emotional state, group behaviors (coordination, \nﬂ anking, crowd and ﬂ ocking behaviors, etc.), and possibly some advanced \nfeatures like an ability to learn from past mistakes or adapt to a changing \nenvironment.\nOf course, the term “artiﬁ cial intelligence” is a bit of a misnomer when \napplied to games. Game AI is usually more smoke and mirrors than it is an \natt empt at true artiﬁ cial intelligence. It’s important to realize that, in a game, \n",
      "content_length": 2548,
      "extraction_method": "Direct"
    },
    {
      "page_number": 847,
      "chapter": null,
      "content": "825 \nall that really matt ers is the player’s perception of what is going on. A classic \nexample comes from the game Halo . When Bungie ﬁ rst implemented their AI \nsystem, they included a simple rule that stated that the small “grunt” aliens \nwould all run away when their leader had died. In play test aft er play test, \nno one realized that this was why the litt le guys were running away. Even af-\nter the Bungie team had made various adjustments to the animations and AI \nbehaviors in the game, still no one got the connection. Finally, the developers \nresorted to having one of the grunts say, “Leader dead! Run away!” This just \ngoes to show that all the AI logic in the world doesn’t amount to anything if \nthe player doesn’t perceive the meaning behind it.\nAI programming is a rich topic, and we certainly have not done it justice \nin this book. For more information, see [16], [6] Section 3, [7] Section 3, and \n[40] Section 3.\n15.2.4. Other Gameplay Systems\nClearly there’s a lot more to a game than just player mechanics, cameras, and \nAI. Some games have drivable vehicles, implement specialized types of weap-\nonry, allow the player to destroy the environment with the help of a dynam-\nic physics simulation, let the player create his or her own characters, build \ncustom levels, require the player to solve puzzles, or… Of course, the list of \ngenre- and game-speciﬁ c features, and all of the specialized soft ware systems \nthat implement them, could go on forever. Gameplay systems are as rich and \nvaried as games are. Perhaps this is where your next journey as a game pro-\ngrammer will begin!\n15.2. Gameplay Systems\n",
      "content_length": 1634,
      "extraction_method": "Direct"
    },
    {
      "page_number": 848,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 849,
      "chapter": null,
      "content": "827\n \nReferences\n[1] \nTomas Akenine-Moller, Eric Haines, and Naty Hoﬀ man. Real-Time Rendering \n(3rd Edition). Wellesley, MA: A K Peters, 2008.\n[2] \nAndrei Alexandrescu. Modern C++ Design: Generic Programming and Design \nPatt erns Applied. Resding, MA: Addison-Wesley, 2001.\n[3] \nGrenville Armitage, Mark Claypool and Philip Branch. Networking and Online \nGames: Understanding and Engineering Multiplayer Internet Games. New York, \nNY: John Wiley and Sons, 2006.\n[4] \nJames Arvo (editor). Graphics Gems II. San Diego, CA: Academic Press, \n1991.\n[5] \nGrady Booch, Robert A. Maksimchuk, Michael W. Engel, Bobbi J. Young, \nJim Conallen, and Kelli A. Houston. Object-Oriented Analysis and Design with \nApplications, third edition. Reading, MA: Addison-Wesley, 2007.\n[6] \nMark DeLoura (editor). Game Programming Gems. Hingham, MA: Charles \nRiver Media, 2000.\n[7] \nMark DeLoura (editor). Game Programming Gems 2. Hingham, MA: Charles \nRiver Media, 2001.\n[8] \nPhilip Dutré, Kavita Bala and Philippe Bekaert. Advanced Global Illumination \n(Second Edition). Wellesley, MA: A K Peters, 2006.\n",
      "content_length": 1082,
      "extraction_method": "Direct"
    },
    {
      "page_number": 850,
      "chapter": null,
      "content": "828 \nReferences\n[9] \nDavid H. Eberly. 3D Game Engine Design: A Practical Approach to Real-Time \nComputer Graphics. San Francisco, CA: Morgan Kaufmann, 2001.\n[10] David H. Eberly. 3D Game Engine Architecture: Engineering Real-Time \nApplications with Wild Magic. San Francisco, CA: Morgan Kaufmann, 2005.\n[11] David H. Eberly. Game Physics. San Francisco, CA: Morgan Kaufmann, \n2003.\n[12] Christer Ericson. Real-Time Collision Detection. San Francisco, CA: Morgan \nKaufmann, 2005.\n[13] Randima Fernando (editor). GPU Gems: Programming Techniques, Tips and \nTricks for Real-Time Graphics. Reading, MA: Addison-Wesley, 2004.\n[14] James D. Foley, Andries van Dam, Steven K. Feiner, and John F. Hughes. \nComputer Graphics: Principles and Practice in C, second edition. Reading, MA: \nAddison-Wesley, 1995.\n[15] Grant R. Fowles and George L. Cassiday. Analytical Mechanics (7th Edition). \nPaciﬁ c Grove, CA: Brooks Cole, 2005.\n[16] John David Funge. AI for Games and Animation: A Cognitive Modeling Approach. \nWellesley, MA: A K Peters, 1999.\n[17] Erich Gamma, Richard Helm, Ralph Johnson, and John M. Vlissiddes. \nDesign Patt erns: Elements of Reusable Object-Oriented Soft ware. Reading, MA: \nAddison-Wesley, 1994.\n[18] Andrew S. Glassner (editor). Graphics Gems I. San Francisco, CA: Morgan \nKaufmann, 1990.\n[19] Paul S. Heckbert (editor). Graphics Gems IV. San Diego, CA: Academic Press, \n1994.\n[20] Maurice Herlihy, Nir Shavit. The Art of Multiprocessor Programming. San \nFrancisco, CA: Morgan Kaufmann, 2008.\n[21] Roberto Ierusalimschy, Luiz Henrique de Figueiredo and Waldemar Celes. \nLua 5.1 Reference Manual. Lua.org, 2006.\n[22] Roberto Ierusalimschy. Programming in Lua, 2nd Edition. Lua.org, 2006.\n[23] Isaac Victor Kerlow. The Art of 3-D Computer Animation and Imaging (Second \nEdition). New York, NY: John Wiley and Sons, 2000.\n[24] David Kirk (editor). Graphics Gems III. San Francisco, CA: Morgan Kaufmann, \n1994.\n[25] Danny Kodicek. Mathematics and Physics for Game Programmers. Hingham, \nMA: Charles River Media, 2005.\n",
      "content_length": 2028,
      "extraction_method": "Direct"
    },
    {
      "page_number": 851,
      "chapter": null,
      "content": "829 \nReferences\n[26] Raph Koster. A Theory of Fun for Game Design. Phoenix, AZ: Paraglyph, \n2004.\n[27] John Lakos. Large-Scale C++ Soft ware Design. Reading, MA: Addison-Wesley, \n1995.\n[28] Eric Lengyel. Mathematics for 3D Game Programming and Computer Graphics, \n2nd Edition. Hingham, MA: Charles River Media, 2003.\n[29] Tuoc V. Luong, James S. H. Lok, David J. Taylor and Kevin Driscoll. \nInternationalization: Developing Soft ware for Global Markets. New York, NY: \nJohn Wiley & Sons, 1995.\n[30] Steve Maguire. Writing Solid Code: Microsoft ’s Techniques for Developing Bug-\nFree C Programs. Bellevue, WA: Microsoft  Press, 1993.\n[31] Scott  Meyers. Eﬀ ective C++: 55 Speciﬁ c Ways to Improve Your Programs and \nDesigns (3rd Edition). Reading, MA: Addison-Wesley, 2005.\n[32] Scott  Meyers. More Eﬀ ective C++: 35 New Ways to Improve Your Programs and \nDesigns. Reading, MA: Addison-Wesley, 1996.\n[33] Scott  Meyers. Eﬀ ective STL: 50 Speciﬁ c Ways to Improve Your Use of the Standard \nTemplate Library. Reading, MA: Addison-Wesley, 2001.\n[34] Ian Millington. Game Physics Engine Development. San Francisco, CA: Morgan \nKaufmann, 2007.\n[35] Hubert Nguyen (editor). GPU Gems 3. Reading, MA: Addison-Wesley, 2007.\n[36] Alan W. Paeth (editor). Graphics Gems V. San Francisco, CA: Morgan \nKaufmann, 1995.\n[37] C. Michael Pilato, Ben Collins-Sussman, and Brian W. Fitzpatrick. Version \nControl with Subversion, second edition. Sebastopol , CA: O’Reilly Media, \n2008. (Commonly known as “The Subversion Book.” Available online at \nhtt p://svnbook.red-bean.com.)\n[38] Matt  Pharr (editor). GPU Gems 2: Programming Techniques for High-Performance \nGraphics and General-Purpose Computation. Reading, MA: Addison-Wesley, \n2005.\n[39] Bjarne Stroustrup. The C++ Programming Language, special edition (3rd \nedition). Reading, MA: Addison-Wesley, 2000.\n[40] Dante Treglia (editor). Game Programming Gems 3. Hingham, MA: Charles \nRiver Media, 2002.\n[41] Gino van den Bergen. Collision Detection in Interactive 3D Environments. San \nFrancisco, CA: Morgan Kaufmann, 2003.\n",
      "content_length": 2057,
      "extraction_method": "Direct"
    },
    {
      "page_number": 852,
      "chapter": null,
      "content": "830 \nReferences\n[42] Alan Watt . 3D Computer Graphics (3rd Edition). Reading, MA: Addison \nWesley, 1999.\n[43] James Whitehead II, Bryan McLemore and Matt hew Orlando. World of \nWarcraft  Programming: A Guide and Reference for Creating WoW Addons. New \nYork, NY: John Wiley & Sons, 2008.\n[44] Richard Williams. The Animator’s Survival Kit. London, England: Faber & \nFaber, 2002.\n",
      "content_length": 378,
      "extraction_method": "Direct"
    },
    {
      "page_number": 853,
      "chapter": null,
      "content": "Game Engine Architecture\n\nJason Gregory\n\nForeword by Jeff Lander and Matt Whiting\n\nThis book covers both the theory and practice of game engine software development, bringing\ntogether complete coverage of a wide range of topics. The concepts and techniques described\nare the actual ones used by real game studios like Electronic Arts and Naughty Dog. The\nexamples are often grounded in specific technologies, but the discussion extends way beyond\nany particular engine or API. The references and citations make it a great jumping off point for\nthose who wish to dig deeper into any particular aspect of the game development process.\nIntended as the text for a college level series in game programming, this book can also be used by\namateur software engineers, hobbyists, self-taught game programmers, and existing members\nof the game industry. Junior game engineers can use it to solidify their understanding of game\ntechnology and engine architecture. Even senior engineers who specialize in one particular field\nof game development can benefit from the bigger picture presented in these pages.\nTopics include:\n\ne large-scale C++ software architecture in a games context\n\ne mathematics for game programming\n\ne game development tools for debugging, source control, and profiling\n\ne engine subsystems including engine foundation systems, rendering, collision,\nphysics, character animation, and game world object models\n\ne multiplatform game engines\n© game programming in multiprocessor environments\n\ne tool pipelines and the game asset database\n\nJason Gregory has worked as a software engineer in the games industry since March 1999 and\nas a professional software engineer since 1994. He got his start in game programming at Midway\nHome Entertainment in San Diego. He also wrote the Playstation 2/Xbox animation system for\nFreaky Flyers and Crank the Weasel. In 2003, Jason moved to Electronic Arts Los Angeles, where he\nworked on engine and game play technology for Medal of Honor: Pacific Assault and served as a\nlead engineer on the Medal of Honor: Airborne project. Jason is currently a Generalist Programmer\nat Naughty Dog Inc., where he developed engine and gameplay software for Uncharted: Drake's\nFortune and is now working on Uncharted 2: Among Thieves. Jason also teaches courses in game\ntechnology at the University of Southern California.\n\nISBN 978-1-56881-413-1\n\n9 \\| |\n\n131",
      "content_length": 2386,
      "extraction_method": "OCR"
    }
  ],
  "enrichment": {
    "version": "1.0.0",
    "generated_by": "generate_chapter_metadata.py",
    "contains": [
      "keywords",
      "concepts",
      "summary"
    ]
  }
}