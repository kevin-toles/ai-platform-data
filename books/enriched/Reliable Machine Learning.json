{
  "metadata": {
    "title": "Reliable Machine Learning",
    "source_file": "Reliable Machine Learning_metadata.json"
  },
  "chapters": [
    {
      "chapter_number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "summary": "Reliable Machine Learning\nReliable Machine Learning Whether you’re part of a small startup or a multinational corporation, this practical book shows data scientists, software and site reliability engineers, product managers, and business owners how to run and establish ML reliably, effectively, and accountably within your organization.\nBy applying an SRE mindset to machine learning, authors and engineering professionals Cathy Chen, Kranti Parisa, Niall Richard Murphy, D.\nSculley, Todd Underwood, and featured guest authors show you how to run an efficient and reliable ML system.\nNiall Richard Murphy is a CEO of a startup in the ML & SRE space, and has worked for Amazon, Google, and Microsoft.\nTodd Underwood is a senior director and founder of machine learning SRE at Google.\nPraise for Reliable Machine Learning\nI don’t care how much data science work you’ve done in the past, or how expert you are on the statistical foundations of machine learning.\nBefore you ever put a real system based on machine learning into deployment, you will benefit from reading this book.\n—Chip Huyen, author of Designing Machine Learning Systems\nReliable Machine Learning is a must-read for people building real-world machine learning systems.\nReliable Machine Learning by Cathy Chen, Niall Richard Murphy, Kranti Parisa, D.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Reliable Machine Learning, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\n. 1 The ML Lifecycle 1 Data Collection and Analysis 3 ML Training Pipelines 3 Build and Validate Applications 5 Quality and Performance Evaluation 6 Defining and Measuring SLOs 7 Launch 8 Monitoring and Feedback Loops 11 Lessons from the Loop 12\n47 Training Data 48 Labels 50 Training Methods 51 Infrastructure and Pipelines 54 Platforms 55 Feature Generation 55 Upgrades and Fixes 56 A Set of Useful Questions to Ask About Any Model 57 An Example ML System 59 Yarn Product Click-Prediction Model 59 Features 59 Labels for Features 61 Model Updating 61 Model Serving 62 Common Failures 63 Conclusion 64\nAn Annotation Platform 80 Active Learning and AI-Assisted Labeling 81 Documentation and Training for Labelers 81 Metadata 82 Metadata Systems Overview 82 Dataset Metadata 83 Feature Metadata 84 Label Metadata 85 Pipeline Metadata 85 Data Privacy and Fairness 86 Privacy 86 Fairness 87 Conclusion 87\n107 Fairness (a.k.a. Fighting Bias) 108 Definitions of Fairness 112 Reaching Fairness 117 Fairness as a Process Rather than an Endpoint 120 A Quick Legal Note 121 Privacy 121 Methods to Preserve Privacy 124 A Quick Legal Note 126 Responsible AI 127 Explanation 128 Effectiveness 130 Social and Cultural Appropriateness 131 Responsible AI Along the ML Pipeline 132 Use Case Brainstorming 132 Data Collection and Cleaning 132 Model Creation and Training 133 Model Validation and Quality Assessment 133 vii\n. 137 Requirements 138 Basic Training System Implementation 140 Features 141 Feature Store 141 Model Management System 142 Orchestration 143 Quality Evaluation 144 Monitoring 144 General Reliability Principles 145 Most Failures Will Not Be ML Failures 145 Models Will Be Retrained 146 Models Will Have Multiple Versions (at the Same Time!) 146 Good Models Will Become Bad 147 Data Will Be Unavailable 148 Models Should Be Improvable 149 Features Will Be Added and Changed 149 Models Can Train Too Fast 150 Resource Utilization Matters 151 Utilization != Efficiency 152 Outages Include Recovery 154 Common Training Reliability Problems 154 Data Sensitivity 154 Example Data Problem at YarnIt 155 Reproducibility 155 Example Reproducibility Problem at YarnIt 157 Compute Resource Capacity 159 Example Capacity Problem at YarnIt 159 Structural Reliability 160 Organizational Challenges 160 Ethics and Fairness Considerations 161 Conclusion 162",
      "keywords": [
        "Niall Richard Murphy",
        "Machine Learning",
        "Reliable Machine Learning",
        "Sam Charrington",
        "Machine Learning Systems",
        "Richard Murphy",
        "Niall Richard",
        "Kranti Parisa",
        "machine learning SRE",
        "Reliable Machine",
        "Machine",
        "Learning",
        "model",
        "Todd Underwood",
        "Machine Learning Applying"
      ],
      "concepts": [
        "models",
        "data",
        "reliable",
        "reliability",
        "reliably",
        "production",
        "products",
        "featured",
        "feature",
        "training"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 3,
          "title": "",
          "score": 0.872,
          "base_score": 0.722,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 4,
          "title": "",
          "score": 0.73,
          "base_score": 0.58,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 39,
          "title": "",
          "score": 0.665,
          "base_score": 0.515,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 37,
          "title": "",
          "score": 0.636,
          "base_score": 0.486,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 9,
          "title": "",
          "score": 0.633,
          "base_score": 0.483,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "machine",
          "machine learning",
          "reliable machine",
          "learning",
          "richard"
        ],
        "semantic": [],
        "merged": [
          "machine",
          "machine learning",
          "reliable machine",
          "learning",
          "richard"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.39401153133741385,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.824480+00:00"
      }
    },
    {
      "chapter_number": 2,
      "title": "Segment 2 (pages 9-18)",
      "start_page": 9,
      "end_page": 18,
      "summary": "190 The Concerns That ML Brings to Monitoring 192 Reasons for Continual ML Observability—in Production 193 Problems with ML Production Monitoring 193 Difficulties of Development Versus Serving 194 A Mindset Change Is Required 196 Best Practices for ML Model Monitoring 196 Generic Pre-serving Model Recommendations 197 Training and Retraining 198 Model Validation (Before Rollout) 202 Serving 205 Other Things to Consider 216 High-Level Recommendations for Monitoring Strategy 221 Conclusion 223\nFiltering Out Bad Data 227 Feature Stores and Data Management 228 Updating the Model 228 Pushing Updated Models to Serving 229 Observations About Continuous ML Systems 230 External World Events May Influence Our Systems 230 Models Can Influence Their Own Training Data 232 Temporal Effects Can Arise at Several Timescales 234 Emergency Response Must Be Done in Real Time 235 New Launches Require Staged Ramp-ups and Stable Baselines 239 Models Must Be Managed Rather Than Shipped 241 Continuous Organizations 242 Rethinking Noncontinuous ML Systems 245 Conclusion 246\n247 Incident Management Basics 248 Life of an Incident 249 Incident Response Roles 250 Anatomy of an ML-Centric Outage 251 Terminology Reminder: Model 252 Story Time 252 Story 1: Searching but Not Finding 252 Story 2: Suddenly Useless Partners 257 Story 3: Recommend You Find New Suppliers 264 ML Incident Management Principles 274 Guiding Principles 274 Model Developer or Data Scientist 275 Software Engineer 277 ML SRE or Production Engineer 278 Product Manager or Business Leader 281 Special Topics 282 Production Engineers and ML Engineering Versus Modeling 282 The Ethical On-Call Engineer Manifesto 284 Conclusion 286\nBusiness Goal Setting 292 MVP Construction and Validation 295 Model and Product Development 296 Deployment 296 Support and Maintenance 297 Build Versus Buy 298 Models 298 Data Processing Infrastructure 299 End-to-End Platforms 300 Scoring Approach for Making the Decision 301 Making the Decision 301 Sample YarnIt Store Features Powered by ML 302 Showcasing Popular Yarns by Total Sales 302 Recommendations Based on Browsing History 303 Cross-selling and Upselling 303 Content-Based Filtering 303 Collaborative Filtering 304 Conclusion 305\nChapter Assumptions 308 Leader-Based Viewpoint 308 Detail Matters 308 ML Needs to Know About the Business 309 The Most Important Assumption You Make 310 The Value of ML 311 Significant Organizational Risks 312 ML Is Not Magic 312 Mental (Way of Thinking) Model Inertia 312 Surfacing Risk Correctly in Different Cultures 313 Siloed Teams Don’t Solve All Problems 314 Implementation Models 314 Remembering the Goal 315 Greenfield Versus Brownfield 316 ML Roles and Responsibilities 316 How to Hire ML Folks 317 Organizational Design and Incentives 318 Strategy 319 Structure 320 Processes 321 Rewards 321\nContinuous ML Model Impacting Traffic 341 Background 341 Problem and Resolution 341 Takeaways 342 3.\nTesting and Measuring Dependencies in ML Workflow 356 Background 356 Problem and Resolution 357 Takeaways 361\nPicking up where the “data-driven” wave of the 2000s left off, ML enables a new era of model-driven decision making that promises to improve organizational performance and enhance customer experiences by allowing machines to make near-instantaneous, high-fidelity decisions, at the point of interac‐ tion, based on the most current information available.\nTo support the productive use of ML models, the practice of machine learning has had to evolve rapidly from a primarily academic pursuit to a fully fledged engineering discipline.\nPart of what we see in the evolution of machine learning roles is a healthy shift in focus from simply trying to get models to work to ensuring that they work in a way that meets the needs of the organization.\nThe first wave of MLOps focused on the application of technology and process disci‐ pline to the development and deployment of models, resulting in a greater ability for organizations to move models from “the lab” to “the factory,” as well as an explosion of tools and platforms for supporting those stages of the ML lifecycle.\nRather than leaving it to each individual or team to identify how to apply SRE principles to their machine learning workflow, the authors of this book aim to give you a head start by sharing what has worked for them at Google, Apple, Microsoft, and other organizations.\nIn the fall of 2019, I organized the first TWIMLcon: AI Platforms conference to provide a venue for the then-nascent MLOps community to share experiences and advance the practice of building processes, tooling, and platforms for supporting the end-to-end machine learning workflow.\nAt our second conference, in 2021, Todd Underwood joined us to present “When Good Models Go Bad: The Damage Caused by Wayward Models and How to Prevent It.”2 The talk shared the results of a hand analysis of approximately 100 incidents tracked over 10 years in which bad ML models made it, or nearly made it, into production.\n(Though I’ve not previously come across Cathy and Kranti’s work, it is clear that their experience structuring SRE organizations and driving large-scale consumer-facing applications of ML informs many aspects of the book, particularly the chapters on implementing ML organizations and integrating ML into products.)\nThis book provides a valuable lens into the authors’ experiences building, operating, and scaling some of the largest machine learning systems around.\nIf we’ve learned anything as a community over the past several years it’s that the ability to create, deliver, and operate ML models in an efficient, repeatable, and scalable manner is far from easy.\nI’m grateful to Cathy, Niall, Kranti, D., and Todd for allowing us all to benefit from their hard won lessons and for helping to advance the state of machine learning in production in the process.",
      "keywords": [
        "Machine learning",
        "Model Serving Architectures",
        "Models",
        "Machine Learning Systems",
        "Serving",
        "Machine",
        "learning",
        "Problem and Resolution",
        "Serving Model",
        "Table of Contents",
        "Background",
        "Development Versus Serving",
        "Systems",
        "Data",
        "Conclusion"
      ],
      "concepts": [
        "model",
        "data",
        "conclusion",
        "serving",
        "engineer",
        "engineering",
        "processing",
        "processes",
        "process",
        "systems"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 32,
          "title": "",
          "score": 0.827,
          "base_score": 0.677,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 35,
          "title": "",
          "score": 0.826,
          "base_score": 0.676,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 36,
          "title": "",
          "score": 0.805,
          "base_score": 0.655,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 26,
          "title": "",
          "score": 0.8,
          "base_score": 0.65,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 30,
          "title": "",
          "score": 0.74,
          "base_score": 0.59,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "machine learning",
          "machine",
          "models",
          "conclusion",
          "learning"
        ],
        "semantic": [],
        "merged": [
          "machine learning",
          "machine",
          "models",
          "conclusion",
          "learning"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4740514490329024,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.824558+00:00"
      }
    },
    {
      "chapter_number": 3,
      "title": "Segment 3 (pages 19-26)",
      "start_page": 19,
      "end_page": 26,
      "summary": "The way that machine learning (ML) works is fascinating.\nThis means that we talk about the messy, complicated, and occasionally frustrating work involved in shepherding data correctly and responsibly; reliable model build‐ ing; ensuring a smooth (and reversible) path to production; safety in updating; and concerns about cost, performance, business goals, and organizational structure.\nWe attempt to cover everything involved in having ML happen reliably in your organization.\nThere is simultaneously both completely unjustified ML/AI hype and way more real, working applications to more industries than most people are aware of.\nSRE as the Lens on ML A plethora of ML books exist already, many of which promise to make your ML journey better in some way, often focusing on making it easier, faster, or more productive.\nFew, however, talk about how to make ML more reliable, an attribute often overlooked or undervalued.3 That is what we focus on, since looking at how to do ML well through that lens has specific benefits you don’t get in other ways.\n3 Or to put it another way, there is plenty of material on how to build an ML model, but not much on how to\nWe believe, in short, that ML systems being reliable captures the essence of what customers, business owners, and staff really want from them.\nIntended Audience We are writing for anyone who wants to take ML into the real world and make a difference in their organization.\nAccordingly, this book is for data scientists and ML engineers, for software engineers and site reliability engineers, and for organiza‐ tional decision makers—even nontechnical ones, although parts of the book are quite technical:\nData scientists and ML engineers\nSoftware engineering building ML infrastructure or integrating ML into existing products\nAn improved understanding of how the ML lifecycle works helps with developing functionality, designing application programming interfaces (APIs), and supporting customers.\nWe’ll also explore the implications of ML model quality not being something a reliability engineer can entirely ignore.\nOrganizational leaders who want to add ML to their existing products or services\nWe will help you understand how best to integrate ML into your existing products and services, and the structures and organizational patterns required.\nFor example, Chapter 2 can certainly be read by data scientists and ML engineers.\nOur Approach Engineers need to employ specific approaches and techniques to make ML systems work well.\nIn some ways, it’s quite a simple business, but the complexity will show up almost immediately as we try to add ML.\nML has the potential to positively or negatively transform the business fundamentally, changing the way products are created and selected, customers are identified and served, and commercial opportunities are uncovered.\nML-adopting businesses that successfully deploy these technologies will outperform their competitors in the long run; a recent survey of over 2,000 executives conducted by McKinsey indicated that 63% of execs had ML/AI projects that improved the bottom line, though organizations are often cagey about precisely how much.\nOur website, yarnit.ai, has many sources of data to implement these initial, concrete improvements and many potential applications for ML.\nConcretely, we’ll work through several examples of areas where ML can improve our operations, although, of course, not limited to these requirements:\nWe might want to use ML to predict how to order replacement products from our suppliers based on a prediction of future sales and the predicted delivery delays.\nOrganizations like our yarn store should approach ML with an open mind but a willingness to experiment, measure, and possibly cancel the applications if they do not work out.\nFor completeness, we need to say that obviously this is only a single example of the kind of organization and application that might find uses for ML.\nHere, relative newcomers to ML (or ML in production) can orient themselves to the problem space as a whole.\nIt is also where we cover critical topics that impact all of the rest of the chapters, such as data management, what an ML model is, how to evaluate its quality, what a feature is (and why you would care), and fairness and privacy.\nWe start with a concrete illustration of the complexities of ML in the incident response domain—something we expect basically every engineer with production responsibilities can relate to.\nWe look at some of the critical questions about how organizations could integrate ML into existing (and emerging) products, often a process best done with some thought put into it in advance.\nAfter all of that, well, first of all, you deserve a break—but second of all, you should be well equipped to understand everything you’re likely to come across when doing ML for the first time, or even when refining how it works inside your organization when you already have experience.\nAbout the Authors The authors of this book collectively have decades of experience building and run‐ ning various kinds of ML systems in production.\nad-targeting systems; built large search-and-discovery systems; published ground‐ breaking research on ML in production; and constructed and run the critical data ingestion, processing, and storage systems wrapped around them all.",
      "keywords": [
        "machine learning",
        "book",
        "machine learning works",
        "products",
        "make machine learning",
        "systems",
        "business",
        "organization",
        "data",
        "model",
        "customers",
        "learning",
        "machine",
        "Preface",
        "engineers"
      ],
      "concepts": [
        "production",
        "productive",
        "data",
        "business",
        "businesses",
        "customers",
        "works",
        "systems",
        "preface",
        "concrete"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 1,
          "title": "",
          "score": 0.872,
          "base_score": 0.722,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 39,
          "title": "",
          "score": 0.851,
          "base_score": 0.701,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 4,
          "title": "",
          "score": 0.82,
          "base_score": 0.67,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 37,
          "title": "",
          "score": 0.808,
          "base_score": 0.658,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 9,
          "title": "",
          "score": 0.792,
          "base_score": 0.642,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ml existing",
          "machine",
          "products",
          "engineers",
          "existing products"
        ],
        "semantic": [],
        "merged": [
          "ml existing",
          "machine",
          "products",
          "engineers",
          "existing products"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.43583388837789144,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.824610+00:00"
      }
    },
    {
      "chapter_number": 4,
      "title": "Segment 4 (pages 27-35)",
      "start_page": 27,
      "end_page": 35,
      "summary": "We would like to collectively thank the following contributors to the book: Ely M.\nI’d like to thank my friend, Dave Rensin, for looping me into this amazing opportunity and motivating me to share my knowl‐ edge and experiences building ML systems at scale with the rest of the world.\nWe begin with a model, or framework, for adding machine learning (ML) to a website, widely applicable across a number of domains—not just this example.\nThis model we call the ML loop.\nML model developers often hope their lives will be simple, and they’ll have to collect data and train a model only once, but it rarely happens that way.\nIf it doesn’t work well enough, data scientists, business analysts, and ML engineers will typically collaborate on how to understand the failures and improve upon them.\nThis involves, as you might expect, a lot of work: perhaps modifying the existing training pipeline to change some features, adding or removing some data, and restructuring the model in order to iterate on what has already been done.\nThis typically involves—you guessed it—modifying the existing training pipeline, changing features, adding or removing data, and possibly even restructuring the model.\nML systems start with data, so let’s start on the left side of the diagram and go through this loop in more detail.\nFor example, business analysts could live in the finance, accounting, or product teams, and use platform-provided data every day.\nProd‐ uct and ML engineers will also be involved, thinking about what to do with all of this data, and site reliability engineers (SREs) will make recommendations and decisions about the overall pipeline in order to make it more monitorable, manageable, and reliable.\nManaging data for ML is a sufficiently involved topic that we’ve devoted Chapter 2 to data management principles and later discuss training data in Chapters 4 and 10.\nFor now, it is useful to assume that the proper design and management of a data collection and processing system is at the core of any good ML system.\nOnce we have the data in a suitable place and format, we will begin to train a model.\nML Training Pipelines ML training pipelines are specified, designed, built, and used by data engineers, data scientists, ML engineers, and SREs. They are the special-purpose extract, transform, load (ETL) data processing pipelines that read the unprocessed data and apply the\nML algorithm and structure of our model to the data.1 Their job is to consume training data and produce completed models, ready for evaluation and use.\nThese models are either produced complete at once or incrementally in a variety of ways— some models are incomplete in that they cover only some of the available data, and others are incomplete in scope as they are designed to cover only part of the ML learning as a whole.\nTraining pipelines are one of the only parts of our ML system that directly and explicitly use ML-specific algorithms, although even here these are most commonly packaged up in relatively mature platforms and frameworks such as TensorFlow and PyTorch.\nAfter ML engineers have built and validated a training pipeline, probably by relying on relatively mature libraries, the pipeline is safe to reuse and operate by others without as much need for direct statistical expertise.2\nTraining pipelines have all the reliability challenges of any other data transformation pipeline, plus a few ML-specific ones.\nThe most common ML training pipeline failures are as follows:\n• Software bugs or errors implementing the data parsing or ML algorithm\nAll of these failures are also characteristic of the failure modes for a regular (non-ML) ETL data pipeline.\nBut ML models can fail silently for reasons related to data distri‐ bution, missing data, undersampling, or a whole host of problems unknown in the\nregular ETL world.3 One concrete example, covered in more detail in Chapter 2, hinges on the idea that missing, misprocessing, or otherwise not being able to use subsets of data is a common cause of failure for ML training pipelines.\nFor now, let’s just remember that ML pipelines really are somewhat more difficult to operate reliably than other data pipelines, because of these kinds of subtle failure modes.\nIn case it’s not already clear, ML training pipelines are absolutely and completely a production system, worthy of the same care and attention as serving binaries or data analysis.",
      "keywords": [
        "data",
        "Training Pipelines",
        "model",
        "training",
        "book",
        "Pipelines",
        "ETL data pipeline",
        "Online Learning",
        "training data",
        "online learning platform",
        "O’Reilly Online Learning",
        "existing training pipeline",
        "O’Reilly Media",
        "engineers",
        "training pipeline failures"
      ],
      "concepts": [
        "data",
        "training",
        "model",
        "pipeline",
        "learning",
        "thank",
        "chapters",
        "engineers",
        "engineering",
        "useful"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 30,
          "title": "",
          "score": 0.835,
          "base_score": 0.685,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 3,
          "title": "",
          "score": 0.82,
          "base_score": 0.67,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 9,
          "title": "",
          "score": 0.82,
          "base_score": 0.67,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 41,
          "title": "",
          "score": 0.778,
          "base_score": 0.628,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 38,
          "title": "",
          "score": 0.774,
          "base_score": 0.624,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "training pipeline",
          "pipelines",
          "pipeline",
          "training pipelines",
          "training"
        ],
        "semantic": [],
        "merged": [
          "training pipeline",
          "pipelines",
          "pipeline",
          "training pipelines",
          "training"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.47382879895347646,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.824660+00:00"
      }
    },
    {
      "chapter_number": 5,
      "title": "Segment 5 (pages 36-44)",
      "start_page": 36,
      "end_page": 44,
      "summary": "By logging such events, this integration will provide new feedback for our model, so that it can train on the quality of its own recommenda‐ tions and begin improving.4 At this stage, though, we will simply validate that it works at all: in other words, that the model loads into our serving system, the queries are issued by our web server application, the results are shown to users, the predictions are logged, and the logs are stored for future model training.\nNext up is the process of evaluating the model quality and performance.\nQuality and Performance Evaluation ML models are useful only if they work, of course.\nIn a live launch, the model takes live production traffic, affects the website and dependent systems, and so on.\nA dark launch involves consulting the model and logging the result, but not using it actively in the website as users see it.\nOnce we gain confidence that the model is not causing harm and is helping our users (and our revenue, hopefully!), we are almost ready to launch.\nA concrete example is “99.99% of HTTP requests completing successfully (with a 20x code) within 150 ms.” SLOs are the natural domain of SREs, but they are also critical for product managers who specify what the product needs to do, and how it treats its users, as well as data scientists, ML engineers, and software engineers.\nSpecifying SLOs in general is challenging, but specifying them for ML systems is doubly so because of the way that subtle changes in data, or even in the world around us, can significantly degrade the performance of the system.\nHaving said that, we can use obvious separations of concern to get started when thinking about SLOs for ML systems.\nThis is unlikely to help us figure out all aspects of whether the model works and might generate genuinely bad user experiences.\nSo then, for a web application, we might select 1% of all logged-in users to get the model-generated results or perhaps 1% of all cookies.\nIn that case, we will not easily be able to tell the impact of model-generated results on users, and there might be bias in the selection of current users versus new users.\nFor training, we should probably look at throughput (examples per second trained or perhaps bytes of data trained if our models are all of comparable complexity).\nAnd in the application, we should probably monitor metrics such as number of shown recommendations, and successful calls to the model servers (from the perspective of the application, which may or may not match the error rate reported by the model serving system).\nNotice, however, that none of these examples is about the ML performance of the models.\nWe examine this in more detail in Chapter 9, but for the moment we ask you to accept there are reasonable ways to arrive at SLOs for an ML context, and they involve many of the same techniques that are used in non-ML SLO conversations elsewhere (though the details of how ML works are likely to make such conversations longer).\nOnce we have gathered the data, built the model, integrated it into our application, measured its quality, and specified the SLOs, we’re ready for the exciting phase of launching!\nHere product software engineers, ML engineers, and SREs all work together to ship an updated version of our application to our end users.\nIn our case, though, we’re releasing a new version of the website that will include the recommendations and results driven by our ML models.\nRemember that models are code every bit as much as your training system binaries, serving path, and data processing code are.\nDeploying new models can even impact training in some systems (for example, if you are using transfer learning to start training with another model).\nIt is important to treat code and model launches similarly: even though some organizations ship new models over (say) the holiday season, it’s entirely possible for the models to go wrong, and we’ve seen this happen in a way that required code fixes shortly thereafter.\nWhen deploying a new version of an online system, we are often able to do so progressively, starting with a fraction of all servers or users and scaling up over time only as we gain confidence in our system behaving correctly and the quality of our ML improvements.\nSpecifically, if a new model or serving system logs output that is consumed by older versions of the code or model, diagnosing problems can be long and tricky.\nThis can happen to any system that processes data produced by a different element of the system, although the failures in ML systems tend to be subtler and harder to detect.\nBasic model health, or generic ML signals\nChecking on basic model health metrics is the ML equivalent of systems health: it is not particularly sophisticated, or tightly coupled to the domain, yet includes basic and representative facts about the modeling system.\nModel quality, or domain-specific signals\nFor example, if our model has poor recommendations for people shopping for needles but not yarn on our site, that could be an opportunity to improve our model (if we chose to launch with this level of quality), or it could be an urgent incident that requires immediate response (if this is a recent regression).7 The difference is context.\nThis is also the most difficult aspect of ML systems for most SREs to come to terms with: there is no objective measure of “good enough” for model quality, and, worse yet, it’s a multidimensional space that is hard to measure.\nUltimately, product and business leaders will have to establish real-world metrics that indicate whether models are performing according to their requirements, and the ML engineers and SREs will need to work together to determine which quality measures are most directly correlated with those outcomes.\nAs a final step in the loop, we need to ensure that the ways that our end users interact with the models make it back into the next round of data collection and are ready to travel the loop again.\nML serving systems should log anything they think will be useful so they can improve in the future.\nBy this point, yarnit.ai should have at least minimal ML functionality added, and we should be in a position to start continuously improving it, either by making the first models better or by identifying other aspects of the site that could be improved with ML.\nSuccessfully, reliably integrating ML into any business or application is not possible without understanding the data that you have and the information you can extract from it.\nEven for organizations with a lot of experience with ML and the ability to evaluate the quality and value of models, most new ML ideas should be trialed first, because most new ML ideas don’t work out.",
      "keywords": [
        "model",
        "system",
        "model quality",
        "data",
        "users",
        "model serving system",
        "serving system",
        "SLOs",
        "quality",
        "application",
        "launch",
        "Basic model",
        "Basic model health",
        "serving",
        "model health"
      ],
      "concepts": [
        "model",
        "data",
        "launching",
        "launches",
        "systems",
        "errors",
        "user",
        "recommendations",
        "recommends",
        "products"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 29,
          "title": "",
          "score": 0.797,
          "base_score": 0.647,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 26,
          "title": "",
          "score": 0.743,
          "base_score": 0.593,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 32,
          "title": "",
          "score": 0.726,
          "base_score": 0.576,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "",
          "score": 0.701,
          "base_score": 0.551,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 10,
          "title": "",
          "score": 0.683,
          "base_score": 0.533,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "users",
          "slos",
          "application",
          "quality",
          "health"
        ],
        "semantic": [],
        "merged": [
          "users",
          "slos",
          "application",
          "quality",
          "health"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4476697603264036,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.824710+00:00"
      }
    },
    {
      "chapter_number": 6,
      "title": "Segment 6 (pages 45-52)",
      "start_page": 45,
      "end_page": 52,
      "summary": "CHAPTER 2 Data Management Principles\nWhere data comes from •\nHow to interpret data •\n• Data quality\nUpdating data sources (which we use and how we use them) •\nAssembling data into an appropriate form for use •\nWe’ll cover the production requirements of data and show that, just like models, data in production has a lifecycle:\n• Cleaning and data consistency\n2 For the data to be useful, it has to be of high quality (accurate, sufficiently detailed, representative of things in the world that our model cares about).\nChapter 2: Data Management Principles\nThe intent of this short section is not to be the authoritative work on data collection, storage, reporting, and deletion practices.\nData as Liability\nin some jurisdictions include prohibition against collecting personally identifiable information (PII) about an individual without their explicit written consent, along with the requirement to delete that data upon request by the data subject.\nVery few good things happen to organizations as a result of revealing their users’ private data.\nIn other words, if it is important that a data field be similar in a certain way under particular circumstances (think of postal codes, for example, which are prefix- identical when they are in the same town or the same part of the same city), then our pseudonymization might need to preserve that.\nExample Applications for Different Levels of Anonymization Different controls are required for data, depending on the sensitivity of the data that is being accessed:\nRaw data\nPseudonymized data\nChapter 2: Data Management Principles\nAnonymized data\nData as Liability\nFinally, we will ultimately need to be able to delete data.\nDeleting data is also trivial: if we lose the key to a user’s data, we can no longer read that data.\nChapter 2: Data Management Principles\nA pipeline can easily go from mostly right to significantly wrong simply by omitting a small fraction of the data, provided that small fraction is not random, or is somehow not evenly sampled in the range of characteristics our model is sensitive to.\nIn many of these cases, losing a small amount of data that turns out to be systematically biased results in significant confusion in the understanding and predictions of our models.\nAs a result of this sensitivity, the ability to aggregate, process, and monitor data, rather than only the live systems, is critical to successfully managing ML data pipe‐ lines.\nThe Data Sensitivity of ML Pipelines\nIf we train on this data, our model will probably have terrible results for Spanish-language searches.\nWe’ll have to think about the data as under our care from the moment of its creation until we delete it.\nOur next operations on the data will be to train ML models, anonymize certain sensitive items of data, and delete data when we no longer need it or are asked to do so.\nChapter 2: Data Management Principles",
      "keywords": [
        "Data",
        "Data Management Principles",
        "Data Management",
        "data processing pipelines",
        "data processing",
        "pipelines",
        "private data",
        "model",
        "Management Principles",
        "deleting data",
        "delete data",
        "system",
        "Management",
        "Spanish",
        "results"
      ],
      "concepts": [
        "data",
        "models",
        "different",
        "difference",
        "storage",
        "access",
        "accessed",
        "reliability",
        "reliable",
        "reliably"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 18,
          "title": "",
          "score": 0.729,
          "base_score": 0.579,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 8,
          "title": "",
          "score": 0.677,
          "base_score": 0.527,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 46,
          "title": "",
          "score": 0.673,
          "base_score": 0.523,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 19,
          "title": "",
          "score": 0.564,
          "base_score": 0.414,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 7,
          "title": "",
          "score": 0.519,
          "base_score": 0.519,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data management",
          "management principles",
          "chapter data",
          "management",
          "delete"
        ],
        "semantic": [],
        "merged": [
          "data management",
          "management principles",
          "chapter data",
          "management",
          "delete"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3040183752204197,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.824750+00:00"
      }
    },
    {
      "chapter_number": 7,
      "title": "Segment 7 (pages 53-61)",
      "start_page": 53,
      "end_page": 61,
      "summary": "a basic understanding of model training does directly inform what we do with data as we get it ready.\nData management in modern ML environments consists of multiple phases before feeding the data into model-training pipelines, as illustrated in Figure 2-2:\n• Post-processing (which includes data management, storage, and analysis)\nImplicit in this process is that we will want to design new systems and adapt existing systems to generate more data than we might otherwise, so that our ML systems have something to work with.\nPhases of Data\nOn the other hand, unstructured data is qualitative without a standard data model/ schema, so it cannot be processed and analyzed using conventional data methods and tools.\nThe character of the internal structure of the data will have significant implications for the way we process, store, and use it.\nML training data categories\nOne final note on dataset creation, or rather, dataset augmentation: if we have a small amount of training data but not enough to train a high-quality model on, we may need to augment that data.\nPhases of Data\nIngestion The data needs to be received into the system and written to storage for further processing.\nWe may filter data by type at this stage (data fields or elements that we do not believe will be useful for our model).\nWe may also simply sample at this stage if we have so much data we do not believe we can afford to process all of it, as ML training and other data processing is often extremely computationally expensive.\nSampling data can be an effective way to save money on intermediate processing and training costs, but it is important to measure the quality cost of sampling and compare that to the savings.\nIn general, ML training systems perform better with more data.\nA date- or time-oriented bucketing system in storage combined with an off-by-one error in the ingestion process could end up with every day’s data stored in the previous day’s directory.\nProcessing Once we have successfully loaded (or ingested) the data into a reasonable feature storage system, most data scientists or modelers will go through a set of common operations to make the data ready for training.\nNo matter how efficient and powerful our ML models are, they can never do what we want them to do with bad data.\nThe more data we have, the more likely that cleaning and data consistency will be its own stage of processing.\nPhases of Data\nNormalization generally refers to a set of techniques used to transform the input data into a similar scale, which is useful for methods like deep learning that rely on gradient descent or similar numerical optimization methods for training.\nPhases of Data\nWe might use many external data sources to extend our training data.\nStorage Finally, we need to store the data somewhere.\nHow and where we store the data is mostly driven by how we tend to use it, which is really a set of questions about training and serving systems.\n• Are we training models once over this data or many times?\nWill each model read all of the data or only parts of it?\nOn the subject of reuse of data, it turns out that almost all data is read multiple times and the storage system should be built for that, even if model owners assert that they will train only one model on the data once.\nAn ML engineer makes a model (reading the data necessary to do so), measures how well the model performs at its designed task, and then deploys it.\nEvery system we build, from data to training all the way to serving, should be built with the assumption that model developers will semi-continuously retrain the same models in order to improve them.\nGiven this, a column-oriented storage scheme with one column per feature is a common design architecture, especially for models training on structured data.7 Most readers will be familiar with row-oriented storage, in which every fetch of data from the database retrieves all of the fields of a matching row.\nThis is much more useful for a collection of applications (ML training pipelines in this case) that each use a given subset of the data.\nIn other words, column-oriented data storage allows different models that efficiently read different subsets of features and do so without reading in the whole row of data every time.\nThe more data we collect in one place, the more likely it is that we will have different models using very different subsets of that data.\nWhen multiple people work on building models on the same data (or when the same person works on this over time), metadata about the stored features provides huge value.\nPhases of Data",
      "keywords": [
        "data",
        "Data management",
        "Data Management Principles",
        "model",
        "training data",
        "training",
        "system",
        "data storage system",
        "data storage",
        "data model",
        "storage",
        "process",
        "Structured data",
        "storage system",
        "data creation"
      ],
      "concepts": [
        "data",
        "models",
        "processing",
        "process",
        "validation",
        "validate",
        "validating",
        "storage",
        "different",
        "differing"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 12,
          "title": "",
          "score": 0.771,
          "base_score": 0.621,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 20,
          "title": "",
          "score": 0.769,
          "base_score": 0.619,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 9,
          "title": "",
          "score": 0.718,
          "base_score": 0.568,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 21,
          "title": "",
          "score": 0.716,
          "base_score": 0.566,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 30,
          "title": "",
          "score": 0.675,
          "base_score": 0.525,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "storage",
          "training",
          "phases data",
          "phases",
          "storage data"
        ],
        "semantic": [],
        "merged": [
          "storage",
          "training",
          "phases data",
          "phases",
          "storage data"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.39727774679286154,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.824800+00:00"
      }
    },
    {
      "chapter_number": 8,
      "title": "Segment 8 (pages 62-69)",
      "start_page": 62,
      "end_page": 69,
      "summary": "And if data management is about how and why we write data, ML training pipelines are about how and why we read it.\nManagement Typically, data storage systems implement credential-based access controls to restrict unauthorized users from accessing the data.\nFor example, we might want to allow only model developers to access the features that they directly work on or restrict their access to a subset of the data in some other way (perhaps only recent data).\nFinally, we might allow production engineers to access all of the data, but only after demonstrating that they need to during an incident and having their access carefully logged and monitored by a separate team.\nSREs can configure data access restrictions on the storage system in production to allow data scientists to read data securely via authorized networks like virtual private networks (VPNs), implement audit logging to track which users and training jobs are accessing what data, generate reports, and monitor usage patterns.\nAnalysis and Visualization Data analysis and visualization is the process of transforming large amounts of data into an easy-to-navigate representation using statistical and/or graphical techniques and tools.8 It is an essential task of ML architectures and knowledge-discovery techni‐ ques to make data less confusing and more accessible.\nThis section covers just the basics of making sure that the data is not lost (durability), is the same for all copies (consistency), and is tracked carefully as it changes over time (version control).\nThis might include cases where the data needs to be recovered from another slower storage system (say, a tape drive) or copied from off-site over a slow network connection.\nData Reliability\nFor an ML storage system with many data transformations, we need to be careful about how those transformations are written and monitored.\nConsistency We may want to guarantee that, as we access data from multiple computers, the data is the same with every read; this is the property of consistency.\nIt is difficult to guarantee that data is replicated, available, and consistent everywhere.\nWhether the model-training system cares about consistency is actually a property of the model and the data.\nNot all training systems are sensitive to inconsistency in the data.\nThe first is to build models that are resilient to inconsistent data.\nIf we can tolerate inconsistent data, especially when the data is recently written, we might be able to train our models significantly faster and operate our storage system more cheaply.\nThe most common way to do that for a replicated storage system is for the system itself to provide information about what data is completely and consistently replicated.\nIf we want to use the data quickly after ingestion and transformation, we may need to have lots of resources provisioned for networking (to copy the data) and storage I/O capacity (to write the copies).\nVersion Control ML dataset versioning is, in many ways, similar to traditional data and/or source code versioning used to bookmark the state of the data so that we can apply a specific version of the dataset for future experiments.\nData Reliability\nPerformance The storage system needs fast-enough write throughput to rapidly ingest the data and not slow transformations.\nAvailability The data we write needs to be there when we read it.\nIf the data is in our storage system and is consistently replicated, and we are able to read it with reasonable performance, then the data will count as available.\nThis means respecting provenance, security, and integrity.10 Our data management system will need to be designed for these properties from the beginning to be able to make appropriate guarantees about the kind of access controls and other data integrity we can offer.\n9 Some readers might read “version control” and think “Git.” A content-indexed software version control system like Git is not really appropriate or necessary to track versions of ML data.\nThe version control we need tracks what the data refers to, who created/updated it, and when it was created.\nSince this entire chapter is about data management, durability has been grouped with reliability concepts, and integrity here refers to properties we can assert about the data, beyond its mere existence and accessibility.\nReasonable use of the datastore will restrict access to certain data to the team most likely to need and use that data.\nThoughtful restriction of access will actually increase productivity if model developers can easily access (and only access) the data they are most likely to use to build models.\nPrivacy When ML data is about individuals, the storage system will need to have privacy- preserving characteristics.\nData Integrity\nTo make those kinds of recommendations, however, we need private data.\nIf we decide that our models can achieve their goals only by having access to private data, we will need to have a serious conversation about the architecture of storing, using, and eventually deleting that private data.\nlearning—an advanced topic that’s beyond the scope of this book.12 (See Figure 2-5 for a list of the types of data and access control implications.)\nChoices and processing as data moves through an ML system\nData Integrity",
      "keywords": [
        "data",
        "storage system",
        "system",
        "data management",
        "Data Management Principles",
        "data management system",
        "private data",
        "Data Integrity",
        "storage",
        "access",
        "data storage systems",
        "Data Reliability",
        "management",
        "data storage",
        "PII"
      ],
      "concepts": [
        "version",
        "versions",
        "accessing",
        "accessibility",
        "model",
        "consistency",
        "consistent",
        "durability",
        "durable",
        "private"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 46,
          "title": "",
          "score": 0.686,
          "base_score": 0.536,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 6,
          "title": "",
          "score": 0.677,
          "base_score": 0.527,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 18,
          "title": "",
          "score": 0.602,
          "base_score": 0.452,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 16,
          "title": "",
          "score": 0.558,
          "base_score": 0.408,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 19,
          "title": "",
          "score": 0.547,
          "base_score": 0.397,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "access",
          "storage",
          "version control",
          "control",
          "integrity"
        ],
        "semantic": [],
        "merged": [
          "access",
          "storage",
          "version control",
          "control",
          "integrity"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.40075855979693137,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.824846+00:00"
      }
    },
    {
      "chapter_number": 9,
      "title": "Segment 9 (pages 70-77)",
      "start_page": 70,
      "end_page": 77,
      "summary": "Conclusion This has been a rapid and superficial introduction (though it probably doesn’t feel like that) to thinking about data systems for ML.\nIf we have started using ML at all, it is likely that our data science teams are already building bespoke data transformation pipelines in various places around the organization.\nCleaning, normalizing, and transforming the data are normal operations that are required to do ML.\nThis involves work that is quite different from the work often performed by many data scientists and ML researchers, who try to spend their days developing new predictive models and methods that can squeeze out another percentage point of accuracy.\nInstead, in this book, we focus on ensuring that a system that includes an ML model exhibits consistent, robust, and reliable system-level behavior.\nWe will say at the outset that our goal here is not to teach you everything about how to build ML models, which models might be good for what problems, or how to become a data scientist.\nInstead of going too deep into the minutia, in this chapter our goal is to give a quick reminder about what ML models are and how they work.\nWhat Is a Model?\nThe models we typically deal with in ML are similar in some ways.\nOne key difference is that the models we typically use for ML are ones that we cannot write down within a neat little rule like E = mc2, no matter how smart we are.\nSome examples of data that can be processed as features are atmospheric readings from thousands of locations, the color values of thousands of pixels in an image, or the purchase history of all users who have recently visited an online store.\nIn using data to train a model, we hope the resulting model will both fit our past data well and generalize to predict well on new, previously unseen data in the future.\nA Basic Model Creation Workflow The basic process that is most widely used for creating ML models right now—for‐ mally called supervised machine learning—looks like this.\nFeatures represent key qualities of the data in a way that ML models can easily digest.\nSome examples are input features that might represent things like the atmospheric pressure for each of 1,000 sensor locations, or the specific color and size values for a given yarn ball, or a set of features corresponding to each possible product, with a value of 1 if a given user has viewed that product and 0 if they have not.\nFor supervised ML, we also require a label of some kind, showing the historical outcome that we would like our model to predict, if it were to see a similar situation in the future.\nWe’ll then train a model on this historical data, using a chosen model type and a chosen ML platform or service.\nCurrently, many folks choose to use model types based on deep learning, also known as neural networks, which are especially effec‐ tive when given very large amounts of data (think millions or billions of labeled examples).1 Neural networks build connections among layers of nodes based on the examples that are used in training.\nIn other settings, methods like random forests or gradient boosted decision trees work well when presented with fewer examples.2 And more complex, larger models are not always preferred, either for predictive power or for maintainability and understandability.\nThose who are not sure which model type might work best often use methods like automated machine learning (AutoML), which trains many model versions, and try to automatically pick the best one.\nRegardless, at the end, our training process will produce a model, which will take the feature inputs for new examples and produce a prediction as an output.\nInstead of using this for training, we wait and use it to stress-test our model, reasoning that because the model has never seen this held-out data in training, it can serve well as the kind of previously unseen data for which we hope it will make useful predictions.\nIt is critical that this validation data is indeed held out from training, because it is all too easy for a model to experience overfitting, which happens when the model memorizes its training data perfectly but cannot predict well on new unseen data.\nThis means finding a way to serve the model predictions as they are required.\nAnother common method is to create a flow of inputs from the production system and transform them into features so that they can be fed to a copy of our model on demand, and then feed the resulting predictions from the model back into the larger system for use.\nThe configuration of the model plus the training environment, and the type and definition of data we will train on.\nIt is reasonable to think of this as the closure of the entire training environment,4 although many people don’t think through the reliability or systems implications of this (in particular, that a very large set of software and data must be carefully and mutually versioned in order to get reasonable amounts of reproducibility).\nTrained model\nA specific snapshot or instantiated representation of the configured model trained on specific data at a point in time, containing a specific set of trained model parameters such as weights and thresholds.\nAs a result, the exact same configured model trained twice on the exact same data may or may not produce a significantly different trained model.",
      "keywords": [
        "model",
        "data",
        "Trained Model",
        "Basic Model Creation",
        "Versus Trained Model",
        "system",
        "Versus Model Definition",
        "Model Architecture Versus",
        "Basic Model",
        "Model Creation Workflow",
        "Model Definition",
        "Data Management Principles",
        "Model Architecture",
        "requirements",
        "work"
      ],
      "concepts": [
        "models",
        "data",
        "working",
        "likely",
        "requirements",
        "requirement",
        "requires",
        "compliance",
        "settings",
        "train"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 4,
          "title": "",
          "score": 0.82,
          "base_score": 0.67,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 3,
          "title": "",
          "score": 0.792,
          "base_score": 0.642,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 30,
          "title": "",
          "score": 0.778,
          "base_score": 0.628,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 32,
          "title": "",
          "score": 0.768,
          "base_score": 0.618,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 39,
          "title": "",
          "score": 0.759,
          "base_score": 0.609,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "trained model",
          "examples",
          "trained",
          "unseen data",
          "unseen"
        ],
        "semantic": [],
        "merged": [
          "trained model",
          "examples",
          "trained",
          "unseen data",
          "unseen"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.47492628749432364,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.824923+00:00"
      }
    },
    {
      "chapter_number": 10,
      "title": "Segment 10 (pages 78-85)",
      "start_page": 78,
      "end_page": 85,
      "summary": "The qualities of the training data establish the qualities and behavior of our models and systems, and imperfections in training data can be amplified in surprising ways.\nTraining on this data would likely result in a model that shows very high accuracy on held-out test data, but that is essentially just a white-wall detector.\nDeploying this model on real data could have terrible results, even while showing excellent performance on held-out test data.\nAgain, uncovering such issues requires careful consideration of the training data, and targeted probing of the model with well-chosen examples to stress-test its behavior in a variety of situations.\nMany production modeling systems collect additional data over time in deployment and are retrained or updated to incorporate that data.\nThese systems have little or no training data at the very start of their lifecycle, and can also encounter cold-start problems for individual items when new products are released over time.\nMany models are used within feedback loops, in which they filter data, recommend items, or select actions that then create or influence the model’s training data in the future.\nThis can cause a situation in which data that is not selected never gets positive feedback, and thus never rises to prominence in the model’s estimation.\nSolving this often requires some amount of intentional exploration of lower-ranked data, occasionally choosing to show or try data or actions that the model currently thinks are not as good, in order to ensure a reasonable flow of training data across the full spectrum of possibilities.\nFor example, imagine that our model helps recommend hotels to users, based on its learning on feedback from previous user bookings.\nIt is easy to imagine a scenario in which a COVID-style lockdown creates a sudden sharp drop in hotel bookings, meaning that a model trained on pre-lockdown data is now extremely overly optimis‐ tic.\nThe introduction of a new product quickly causes a spike in user interest on • a certain kind of wool, but our model doesn’t have any previous information about it.\nLabels In supervised ML, the training labels provide the “correct answer,” showing the model what it should be trying to predict for a given example.\nBecause labels are so important to model training, it is easy to see that problems with labels can be the root cause for many downstream model problems.\nThis might lead to a model that over-fixates on purchases, perhaps learning over time to promote products that appear to be good deals but in fact are of disappointing quality.\nFor example, some email spam systems allow users to label messages as “spam” or “not spam.” It is easy to imagine that a motivated spammer may try to fool such a system by sending many spam emails to accounts under their own control and try to label them as “not spam” in an attempt to poison the overall model.\nIn addition to problems with developing a complete and representative dataset, or labeling examples correctly, we can encounter threats to the model during the model- training process.\nTraining Methods Some models are trained once and then rarely or never updated.\nas another batch of wet lab data comes in from antibody testing, or it might happen every week to incorporate a new set of image data and associated object labels, or it might happen every few minutes in a streaming setting to update with new data based on users browsing and purchasing various yarn products.\nAs we discussed in the brief overview of a typical model lifecycle, a good model will generalize well to new, previously unseen data and not just narrowly memorize its training data.\nEach time a model is retrained or updated, we need to check for overfitting by using held-out validation data.\nFor example, if our validation dataset about purchases on yarnit.ai is never updated, while our customers’ behavior changes over time to favor brighter wools, our models will fail to track this change in purchase preferences because we will score models that learn this behavior as being of “lower quality” than models that do not.\nWhen deep learning models are trained from scratch—with no prior information— they begin from a randomized initial state and are fed a huge stream of training data in randomized order, most often in small batches.\nThe model makes its best current prediction (which early on is terrible, since the model has not learned very much yet) and then is shown the correct label.\nWe stop training after we decide that the model shows good performance on held-out validation data.\nTherefore, repeating the process of training the same model with the same settings on the same data can lead to substantially different final models.\nThe model performance on held-out validation data generally improves with additional training steps, but sometimes bounces around early on, and later can often get significantly worse if the model starts to overfit the training data by memorizing it too closely.\nWe stop training and choose the best model that we can when performance converges to a good level, often choosing a checkpoint version that shows good behavior at an intermediate point.\nDeep learning models sometimes explode in training\nDeep learning models are notoriously sensitive to such settings, which means that significant experimentation and testing is required.\nDeep learning methods extrapolate from their training data, which means that the more unfamiliar a new previously unseen data point is, the more likely we are to have an extreme prediction that might be completely off base or out of the range of typical behavior.\nInfrastructure and Pipelines Models are just one component in larger ML systems, which are typically supported by significant infrastructure to support model training, validation, serving, and mon‐ itoring in one or more pipelines.\nAn additional consideration is that because these platforms are typically created as general-purpose tools, we usually need to create a significant number of adapter components, or glue code, that can help us transform our raw data or features into the correct formats to be used by the platform, and to interface with the models at serving time.",
      "keywords": [
        "model",
        "Data",
        "Training Data",
        "deep learning models",
        "Training",
        "deep learning",
        "systems",
        "validation data",
        "learning models",
        "learning",
        "Labels",
        "deep",
        "Deep learning methods",
        "time",
        "model training"
      ],
      "concepts": [
        "model",
        "data",
        "training",
        "users",
        "labels",
        "systemic",
        "systems",
        "given",
        "settings",
        "performance"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 30,
          "title": "",
          "score": 0.825,
          "base_score": 0.675,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 21,
          "title": "",
          "score": 0.797,
          "base_score": 0.647,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 14,
          "title": "",
          "score": 0.788,
          "base_score": 0.638,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 11,
          "title": "",
          "score": 0.775,
          "base_score": 0.625,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 33,
          "title": "",
          "score": 0.741,
          "base_score": 0.591,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "training",
          "training data",
          "deep",
          "deep learning",
          "learning models"
        ],
        "semantic": [],
        "merged": [
          "training",
          "training data",
          "deep",
          "deep learning",
          "learning models"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.453169601430758,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.824976+00:00"
      }
    },
    {
      "chapter_number": 11,
      "title": "Segment 11 (pages 86-93)",
      "start_page": 86,
      "end_page": 93,
      "summary": "The first is that errors in feature generation are often not visible by aggregate model performance metrics, such as accuracy on held-out test data.\nFor example, if our model relies on a localized time of day for an on-device model, that may be computed via a global batch-processing job when training data is computed, but it may be queried directly from the device at serving time.\nFor example, if our model for generating yarn-purchase predictions depends on a lookup query to another service that reports user reviews and satisfaction ratings, our model would have serious problems if that service suddenly went offline or stopped returning sensible responses.\nThis is because any change to the distribution of feature values that our model expects to see associated with certain data may cause erroneous behavior.\nIf we are modeling user interactions with yarn products, what order are they displayed in, and how does the user move through the page?\nFeatures, the information we extract from raw data to enable easy digestion for ML models, are often added by model developers with a “more is always better” approach.\nAs suggested previously, most insidious are issues that occur when features are computed by one code path at training time—for exam‐ ple, to optimize for memory efficiency—and by another code path at serving time for real deployment—for example, to optimize for latency.\nIn such cases, the model’s predictions can go awry, but we may have no ground-truth validation data to use to detect this.\nNearly all ML models are imperfect and will make mistakes in their predictions on at least some kinds of examples.\nIt is important to spend time looking at data that our model makes errors on, understanding any commonalities or trends, so that we can identify whether these failure modes will impact the downstream use cases in important ways.\nHow is the model updated over time?\nWe need to know the full set of upstream dependencies that provide data to our model, both at training time and serving time, and know how they might change or fail and how we might be alerted if this happens.\nSimilarly, we need to know all of the downstream consumers of our model’s predictions, so that we can appropriately alert them if our model should experience issues.\nWe also need to know how the model’s predictions impact the end use case, if the model is part of any feedback loops (either direct or indirect), and if there are any cyclic dependencies such as time-of-day, day-of-week, or time-of-year effects.\nPerhaps most importantly, we need to know what happens to the larger system if the ML model fails in any way, or if it gives the worst possible prediction for a given input.\nAn Example ML System To help ground our introduction to basic models, we will walk through some of the structure of an example production system.\nYarn Product Click-Prediction Model In our imaginary yarnit.ai site, ML models are applied to many areas.\nBecause these characteristics are expressed in a wide variety of ways in product descrip‐ tions from different manufacturers, each characteristic is predicted by a separate component model specially trained to identify that characteristic from product\nBecause products that appear higher in the listed results are more likely to be viewed and clicked than products listed farther down, it is important at training time to have features that show where the product was listed at the time the data was collected.\nNote, however, that this introduces a tricky dependency—we cannot know at serving time the value of these features, because the ranking and ordering of the results on the page depends on our model’s output.\nLabels for Features The training labels for our model are a straightforward mapping of 1 if the user clicks the product and 0 if the user does not click the product.\nOther, more nuanced but equally ill-intentioned manufacturers attempt to lower their competitors’ listings by issuing many queries that place their competitors listings near the top without clicking them.7 Both of these are attempts at fraud or spam and need to be filtered out before the model is trained on this data.\n7 This works because every time a product is shown but not clicked, the model learns that the product was not a good result for that customer in that context, or at least learns that the product was not the best result.\nOf course, this happens frequently for all products, but if a competitor is able to swamp the system with millions (or more!) of instances of a product being shown but not clicked, the model will learn that customers, in general, just don’t like that product.\nIn practice, then, our model is updated about 12 hours after a given user has viewed or clicked a given product.\nThis is done from time to time by model developers when new model types or additional features are added to the system.\nFor every query, our system needs to quickly score candidate products so that a set of product listings can be returned in the two- to three-tenths of a second before a user begins to perceive waiting time.\nBecause the model is refreshed on the go and serves live, our system has several areas that need to be monitored to ensure that performance in real time continues to be good.\nIf our model were to score 0 for all products, none would be shown to users.\nThis would, of course, indicate an issue with the model, but we need to monitor average predictions and fall back to another system if things go awry.\nNaively, this might create a setting in which those other products, or new products over time, never get a chance to be shown to users and thus never get click information that would help them rise in the rankings.\nA small amount of randomization can be helpful in these situations to ensure that the model gets some amount of exploration data, which will allow the model to learn about products that had not previously been shown.\nThis form of exploration data is also useful for monitoring, as it allows us to reality-check the model’s predictions and ensure that when it says a product is unlikely to be clicked, this holds true in reality as well.",
      "keywords": [
        "model",
        "system",
        "data",
        "Product",
        "time",
        "features",
        "predictions",
        "user",
        "feature generation",
        "serving time",
        "serving",
        "training",
        "training data",
        "Model Serving",
        "Basic Introduction"
      ],
      "concepts": [
        "model",
        "production",
        "products",
        "prediction",
        "predictive",
        "data",
        "user",
        "timing",
        "feature",
        "values"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 10,
          "title": "",
          "score": 0.775,
          "base_score": 0.625,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 34,
          "title": "",
          "score": 0.716,
          "base_score": 0.566,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 20,
          "title": "",
          "score": 0.714,
          "base_score": 0.564,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 14,
          "title": "",
          "score": 0.708,
          "base_score": 0.558,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 21,
          "title": "",
          "score": 0.691,
          "base_score": 0.541,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "time",
          "product",
          "clicked",
          "serving time",
          "predictions"
        ],
        "semantic": [],
        "merged": [
          "time",
          "product",
          "clicked",
          "serving time",
          "predictions"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4129292550579192,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.825027+00:00"
      }
    },
    {
      "chapter_number": 12,
      "title": "Segment 12 (pages 94-106)",
      "start_page": 94,
      "end_page": 106,
      "summary": "• Features and training labels are critical inputs to our models.\nCHAPTER 4 Feature and Training Data\nFeatures The data is just the data.\nFeatures are what make it useful for ML.1 A feature is any aspect of the data that we determine is useful in accomplishing the goals of the model.\nIn other words, a feature is a specific, measurable aspect of the data, or a function of it.\nBut a trained model is very much a formula for combining a collection of features (essentially, feature definitions used to extract feature values from real data—we cover this definition more completely later in this chapter).\nTypically, features contain smaller portions of structured data extracted from the underlying training data.\nAs modeling techniques continue to develop, it seems likely that more features will start to resemble raw data rather than these extracted elements.\n1 We are aware that one of the promises of deep learning is that it is sometimes possible to use raw data to train a model without specifically identifying features.\nIn those cases, we may not need to extract specific features from the underlying data, although we may still get good value from metadata features.\nChapter 4: Feature and Training Data\nIt is important to note that, absent any actual data, we have no idea whether any of these features is useful.\nFeatures\nFor example, “Source country of the current customer obtained by taking the customer’s source IP address and looking it up in the geolocation data service” is a feature definition.\nFeature value\nThis is a specific output of a feature definition applied to incoming data.\nFeature Selection and Engineering Feature selection is the process by which we identify the features in the data that we will use in our model.\nChapter 4: Feature and Training Data\nWithout data, we have no features.\nWe need to collect or create data in order to create features.\nAlthough even the process of feature creation could be considered some kind of data normalization, here we’re referring to coarser preprocessing: eliminating obviously malformed examples, scaling input samples to a common set of val‐ ues, and possibly even deleting specific data that we should not train on for policy reasons.\nFeatures\nWe need to write code that reads the input data and extracts the features that we need from the data.\nBut if we expect to train on the same data more than a few times, it’s probably sensible to extract the feature from the raw data and store it for later efficient and consistent reading.\nIt is important to remember that if our application involves online serving, we need a version of this code to extract the same features from the values that we have available at serving in order to use them to perform inference in our model.\nA feature store is just a place to write extracted feature values so that they can be quickly and consistently read during training a model.\n7. Model training and serving using feature values\nChapter 4: Feature and Training Data\noptionally, reprocess older data to create a new set of feature values for the new version.\nWe will need to remove any serving code that refers to the feature, remove the feature values in the feature store, cancel the code that extracts the feature from the data, and proceed to delete the feature code.\nFeature Systems To successfully manage the flow of data through our systems, and to turn data into features that are usable by our training system and manageable by our modelers, we need to decompose the work into several subsystems.\nAs was mentioned in the introduction to this chapter, one of these systems will be a metadata system that tracks information about the data, datasets, feature generation, and labels.\nFor now, let’s walk through the feature systems starting with raw data and ending up with features stored in a format ready to be read by a training system.\nWe will have to write software that reads raw data, applies our feature extraction code to that data, and stores the resulting feature values in the feature store.\nFeatures\nThis helps them write reliable feature-extraction code so that we can run that code reliably in our data ingestion system.\nIn cases like this, the data ingestion system will also generate labels for those features as well as the features themselves, and both can be stored together in a common datastore.\nFeature store\nA feature store is a storage system designed to store extracted feature (and label) values so that they can be quickly and consistently read during training a model and even during inference.\nMost ML training and serving systems have some kind of a feature store even if it is not called that.2 They are most useful, however, in larger, centrally managed services, especially when the features (definitions and val‐ ues both) are shared among multiple models.\na structured, managed feature store, since there’s no need to mediate that data for other on-device users.\nChapter 4: Feature and Training Data\nour feature and label data in a single, well-managed place has significantly improved the production readiness of ML in the industry.\nUsually these are stored as code that extracts the feature in a raw data format and outputs the feature data in the desired format.\nStore feature values themselves\nServe feature data\nTo get the most out of our feature store, we should keep information about the data we store in it in the metadata system.\nMany feature stores also provide basic normalization of data in ingestion as well as more sophisticated transformations on data in the store.\nFeatures\n• Is the feature data ingested once, never appended to, and seldom updated?\nOnce we are clear on the requirements for an API and have a clearer understanding of our data access needs, in general we will find that our feature store falls into one of two buckets:4\nChapter 4: Feature and Training Data\nLifecycle Access Patterns As we make choices about our feature store, it is critical to keep in mind how, and how often, we will use the data that we are storing.\nMany feature stores will need to be replicated, partially or completely, in order to store data near the training and serving stacks.\nFor those of us who have worked on relational database systems, transforming features are the stored procedures of feature stores—they are fixed transformations of the data, written in code, but stored in the storage system.\nFeatures\nIn all cases, the key is to remember that transforming features offer a consistent and deterministic value that is the programmatic result of the code we have written being applied to the data in the feature store.\nIf a transforming feature is computationally intensive and we intend to read that column frequently, a feature store might choose to materialize that feature, by processing new data as it arrives and writing out the result of the transforming feature into a new column.\nFirst, we may change the definition of the feature, which would require recomputing the column over all of the data.\nChapter 4: Feature and Training Data",
      "keywords": [
        "Feature",
        "data",
        "feature store",
        "Model",
        "feature data",
        "Training Data",
        "transforming features",
        "Feature definition",
        "training",
        "store",
        "system",
        "Feature engineering",
        "Data ingestion system",
        "feature data Provide",
        "source feature stores"
      ],
      "concepts": [
        "features",
        "data",
        "model",
        "stored",
        "store",
        "production",
        "products",
        "human",
        "likely",
        "customer"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 20,
          "title": "",
          "score": 0.896,
          "base_score": 0.746,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 7,
          "title": "",
          "score": 0.771,
          "base_score": 0.621,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 21,
          "title": "",
          "score": 0.734,
          "base_score": 0.584,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 13,
          "title": "",
          "score": 0.727,
          "base_score": 0.577,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 14,
          "title": "",
          "score": 0.624,
          "base_score": 0.474,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "feature",
          "features",
          "store",
          "chapter feature",
          "feature store"
        ],
        "semantic": [],
        "merged": [
          "feature",
          "features",
          "store",
          "chapter feature",
          "feature store"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.34038373696796803,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.825075+00:00"
      }
    },
    {
      "chapter_number": 13,
      "title": "Segment 13 (pages 107-114)",
      "start_page": 107,
      "end_page": 114,
      "summary": "Labels Although features seem like the most important aspects of the data, one thing is more important: labels.\nLabels are the other main category of data used in training an ML model.\nWhile features serve as the input to the model, labels are examples of the correct model output.\nHeld-out examples and labels (labels not used in training) are also used in model evaluation to understand model quality.\nAs we discussed earlier, for some classes of problems, like our recommendation system, the labels can be generated algorithmically from the system’s log data.\nSince this data is generated from the system’s log data often with the feature data, these labels are most commonly stored in the feature system for model training and evaluation.\nIn fact, all labels can be stored in the feature store, although labels generated by humans need additional systems to generate, validate, and correct these labels before storing them for model training and evaluation.\nHuman-Generated Labels So let’s turn our attention to the large classes of problems requiring human annota‐ tions to provide the model training data.\nHumans need to be trained on the task, and each image may need to be labeled multiple times to ensure that we have trustworthy results.\nBecause of the large cost associated with acquiring human-generated labels, training systems should be designed to get as much mileage from them as possible.\nOne technique commonly used is data augmentation: the feature data is “fuzzed” in a way that changes the features but doesn’t change the correctness of the label.5 For example, consider our stitch classification problem.\nThis is a question of scale and equity.6 For simpler models, for which a small amount of data is sufficient to train the model, typically the engineer building the model will do their own labeling, often with hacky, homebuilt tools (and a significant chance of biased labels).\nSome of these services use paid volunteers, and others use teams of employees to label the data.\nMeasuring Human Annotation Quality As the quality of any model is only as good as the data used to train the model, quality must be designed into the system from the start.\nTrusted labelers (or the model builder) produce a set of test questions that are randomly included in the unlabeled data to evaluate the quality of the produced labels.\nAn Annotation Platform A labeling platform organizes the flow of data to be annotated and the results of the annotations while providing quality and throughput metrics of the overall process.\nThe actual labeling tool that allows the labelers to view the data and provide their annotations should be flexible to support any arbitrary annotation task.\nQuality measurement using the techniques discussed previously should be designed into the system from the start, so project owners can understand the labeling throughput and quality of all their annotation tasks.\nPublicly available tools are moving beyond simple queuing systems and are starting to provide dedicated tools for common tasks, including advanced features like AI-assisted labeling (see the following section).\nActive Learning and AI-Assisted Labeling Active learning techniques can focus the annotation effort on the cases in which the model and the human annotators disagree or the model is most uncertain, and thereby improve overall label quality.\nSuch active learning techniques can actually increase overall label quality since the model and humans will often have their best performance on different kinds of input data.\nA semi-supervised system allows the modeler to bootstrap the system with weak heuristic functions that imperfectly predict the labels of some data, and then use humans to train a model that takes these imperfect heuristics to high-quality training data.\nDocumentation and Training for Labelers Documentation and labeler training systems are some of the most commonly over‐ looked parts of an annotation platform.\nLabeling definitions and directions should be updated as new corner cases are discov‐ ered, and the annotation and modeling teams should be notified about the changes.\nIf the changes are significant, previously labeled data might have to be re-annotated to correct data labeled with old instructions.\nAnnotation teams often have significant turnover, so investing in training for using annotation tools and for understanding labeling instructions will almost always give big wins in label quality and throughput.\nMetadata Feature systems and labeling systems both benefit from efficient tracking of metadata.\nNow that you have a relatively complete understanding of the kinds of data that will be provided by a feature or labeling system, you can start to think about what metadata is produced during those operations.\nIn the case of features and labels, it should minimally keep track of the feature definitions we have and the versions used in each model’s definitions and trained models.\nEven within this very chapter, you’re about to see that we’re going to need to store metadata about labels, including their specification and when particular labels were applied to particular feature values.\nLater, we’re going to need a system for mapping model definitions to trained models, along with data about the engineers or teams responsible for those models.\nBut it’s difficult to imagine how feature engineering or labeling can take place without the metadata system being functional, so it can still cause production problems for our humans working on those tasks.\nPerhaps we would have one for features and labels, one for training, one for serving, and one for quality monitoring.\nWe will have processes that need to join data across the features, labeling, training, and serving systems.\nDataset Metadata For metadata about features and labels, here are a few specific elements that we should ensure are included:",
      "keywords": [
        "data",
        "feature",
        "model",
        "system",
        "annotation",
        "Labels",
        "training data",
        "training",
        "metadata system",
        "Metadata",
        "quality",
        "model training",
        "model training data",
        "annotation teams",
        "Metadata Feature systems"
      ],
      "concepts": [
        "labels",
        "data",
        "annotation",
        "annotations",
        "annotators",
        "annotate",
        "models",
        "feature",
        "quality",
        "human"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 20,
          "title": "",
          "score": 0.756,
          "base_score": 0.606,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 12,
          "title": "",
          "score": 0.727,
          "base_score": 0.577,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 14,
          "title": "",
          "score": 0.721,
          "base_score": 0.571,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 7,
          "title": "",
          "score": 0.628,
          "base_score": 0.478,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 30,
          "title": "",
          "score": 0.627,
          "base_score": 0.477,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "labels",
          "annotation",
          "labeling",
          "metadata",
          "quality"
        ],
        "semantic": [],
        "merged": [
          "labels",
          "annotation",
          "labeling",
          "metadata",
          "quality"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.38019637752641716,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.825134+00:00"
      }
    },
    {
      "chapter_number": 14,
      "title": "Segment 14 (pages 115-127)",
      "start_page": 115,
      "end_page": 127,
      "summary": "Users of these labels might choose different thresholds to decide which labels to use in training their models.\nCHAPTER 5 Evaluating Model Validity and Quality\nSimilarly, a model might be shown to have wonderful predictive performance in offline tests, but rely on a particular feature version that is not currently available in the production stack, or use an incompatible version of some ML package, or rarely give values of NaN that cause downstream consumers to crash.\nChapter 5: Evaluating Model Validity and Quality\nAs you’ll learn in Chapter 7, we are likely to take a trained version of a model and copy it to another location where it will be used either for offline scoring by a large batch process, or for online serving of live traffic on demand.\nIn either case, the model is likely to be stored as a file or set of files in a particular format that are then moved, copied, or replicated for serving.\nEvaluating Model Validity\nIt is possible for model files to become corrupted in one way or another, either through error at write time or by having NaN values written to disk if there were not sufficient sanity checks in training.\nImagine that a model developer creates a new version of a feature in training, but neglects to implement or hook up a pipeline that allows that feature to be used in the serving stack.\nIn these cases, loading in a version of the model that relies on that feature will result in crashes or undesirable behavior.\nChapter 5: Evaluating Model Validity and Quality\nInstead, our collective stress level will be reduced if we first ask the model to serve just a tiny trickle of data, and then gradually increase the amount after we have assurance that the model is performing as expected in serving.\nEvaluating Model Quality Ensuring that a model passes validation tests is important, but by itself does not answer whether the model is good enough to do its job well.\nOffline Evaluations As discussed in the whirlwind tour of the model development lifecycle in Chapter 2, model developers typically rely on offline evaluations, such as looking at accuracy on a held-out test set as a way to judge how good a model is.\nData distributions are things like “a held-out test set that was randomly sampled from the same source of data as the training data” that we have talked about before, but held-out test data is not the only distribution that might be important to look at.\nEvaluating Model Quality\nAn evaluation is always composed of both a metric and a distribution together— the evaluation shows the model’s performance on the data in that distribution, as computed by the chosen metric.\nIndeed, many of the issues around fairness and bias that emerged in the late 2010s can likely be tracked down to not giving sufficient consideration to the specifics of the data distribution used at test time during model evaluations.\nThe most common evaluation distribution used is held-out test data, which we cov‐ ered in Chapter 3 when reviewing the typical model lifecycle.\nOn the surface, this seems like an easy thing to think about—we randomly select some of our training data to be set aside and used only for evaluation.\nWhen each example in the training data has an equal and independent chance of being put into the held-out test data, we call this an IID test set.\nWe can think of the IID test set process as basically flipping a (maybe biased) coin or rolling a die for each example in the training data, and holding each one out for the IID test set based on the result.\nAs an example, imagine we have a large set of stock-price data, and we want to train a model to predict these prices.\nIf we create a purely IID test set, we might have training data from 12:01 and 12:03 from a given day in the training data, while data from 12:02 ends up in the test data.\nChapter 5: Evaluating Model Validity and Quality\nthis kind of information, so we would need to be careful to create our evaluations in a way that does not allow the model to train on “future” data.\nThe basic idea is to simulate how training and prediction would work in the real world, but playing the data through to the model in the same order that it originally appeared.\nThat prediction is then recorded and incorporated to the aggregate evaluation metric, and only then is the example shown to the model for training.\nThe drawback is that this setup is somewhat awkward to adapt if our models require many passes over the data to train well.\nA second drawback is that we must be careful to make comparisons between models based on evaluation data from exactly the same time range.\nIn models that continually retrain and evaluate using some form of progressive validation, it can be difficult to know whether the model performance is changing or whether the data is getting easier or harder to predict on.\nOne way to control this is to create a golden set of data that models are not ever allowed to train on, but that is from a specific point in time.\nWhen we set aside the golden set of data, we also keep with it either the results of running that set of data through our model or, in some cases, the result of having humans evaluate the golden set.\nPerformance on golden set data like this can reveal any sudden changes in model quality, which can aid debugging greatly.\nNote that golden set evaluations are not particularly useful for judging absolute model quality, because their relevance to current performance diminishes as their time period recedes into the past.\nThe primary benefit of golden sets is to identify changes or bugs, because, typically, model performance on golden set data changes only gradually as new training data is incorporated into the model.\nWhen deploying models in the real world, one worry is that the data they may encounter in reality may differ substantially from the data they were shown in training.\nFor example, we might have a model trained largely on image data from North America and Western Europe, but that is then later applied in countries across the globe.\nFirst, the model may not perform well on the new kinds of data.\nSecond, and even more important, we may not know that the model would not perform well because the data was not represented in the source that supplied the (supposedly) IID test data.\nIf our training data does not include portrait images with a wide range of skin tones, an IID test set might not have sufficient representation to uncover problems if the model does not do well for images of people with especially dark skin tones.\n(This example harkens back to seminal work by Buolamwini and Gebru.)1 In cases like this, it’s important to create specific stress-test distributions in which carefully constructed test sets each probe for model performance on different skin tones.\nChapter 5: Evaluating Model Validity and Quality\nOne way to understand a model’s performance at a deeper level involves learning what the model would have predicted if the data had been different.\nThis is some‐ times called counterfactual testing because the data that we end up feeding to the model runs counter to the actual data in some way.\nFor example, accuracy was, for years, the one way that models were evaluated on ImageNet held-out test data.",
      "keywords": [
        "model",
        "data",
        "Evaluating Model Validity",
        "Evaluating Model Quality",
        "Evaluating Model",
        "model quality",
        "PII data",
        "Model Validity",
        "IID test set",
        "Training Data",
        "test data",
        "IID test",
        "IID test data",
        "held-out test data",
        "training"
      ],
      "concepts": [
        "model",
        "data",
        "label",
        "feature",
        "evaluating",
        "evaluation",
        "evaluations",
        "evaluate",
        "time",
        "version"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 10,
          "title": "",
          "score": 0.788,
          "base_score": 0.638,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 13,
          "title": "",
          "score": 0.721,
          "base_score": 0.571,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 11,
          "title": "",
          "score": 0.708,
          "base_score": 0.558,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 27,
          "title": "",
          "score": 0.704,
          "base_score": 0.554,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 30,
          "title": "",
          "score": 0.69,
          "base_score": 0.54,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "test",
          "evaluating model",
          "evaluating",
          "iid test",
          "iid"
        ],
        "semantic": [],
        "merged": [
          "test",
          "evaluating model",
          "evaluating",
          "iid test",
          "iid"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.425111541648944,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.825190+00:00"
      }
    },
    {
      "chapter_number": 15,
      "title": "Segment 15 (pages 128-135)",
      "start_page": 128,
      "end_page": 135,
      "summary": "Just as there is no one best place to watch the sun rise, there is no one best metric to evaluate a given model, and the most effective approach is often to consider a diverse range of metrics and evaluations, each of which has its own strengths, weaknesses, blind spots, and peculiarities.\nAs we’ve noted, this set of metrics offers a useful way to tell when something is horribly wrong with our model.\nOn the flip side, if these metrics look good, that does not necessarily mean that all is well or that our model is perfect.\nIn an ideal world, we would, and typically a well-functioning model will show very low bias, meaning a very low difference between the total expected and observed values for a given class of predictions.\nOne of the nice qualities of bias as a metric is that unlike most other metrics, there is a “correct” value of 0.00 that we do expect most models to achieve.\nThe drawback of bias as a metric is that it is trivial to create a model with perfectly zero bias, but that is a terrible model.\nBias as a metric is a great canary, but just having zero bias is not by itself indicative of a high-quality model.\nWhen we have a model that predicts a probability value or a regression estimate, like a probability that a user will click a given product or a numerical pre‐ diction of tomorrow’s temperature, creating a calibration plot can provide significant insight into the overall quality of the model.\nCalibration plots can show systemic effects such as overprediction or underpre‐ diction in different areas, and can be an especially useful visualization to help understand how a model performs near the limits of its output range.\nIn general, calibration plots can help show areas where a model may systematically overpredict or underpredict, by plotting the observed rates of occurrence versus their predicted probabilities.\nFor example, the plot in Figure 5-2 shows a model that gives good calibration in the middle ranges, but does not do as well at the extremes, overpredicting on actual low probability examples and underpredicting on actual high-probability examples.\nWhen we think of model evaluation metrics, classification metrics like accuracy are often the first ones that come to mind.\nBroadly speaking, a classification metric helps measure whether we’ve correctly identified that a given example belongs to a specific category (or class).\nIn conversation, many folks use the term accuracy to mean a general sense of goodness, but accuracy also has a formal definition that shows the fraction of predictions for which the model was correct.\nTo place an accuracy metric into context, we need to have some understanding of how good a naive model that always predicts the most prevalent class would be, and also to understand relative costs of different types of errors.\nBoth metrics have a notion of a positive, which we can think of as “the thing we are trying hard to find.” This can be finding spam for a spam classifier, or yarn products that match the user’s interest for a yarn store model.\nIf our model does not have sufficient precision, we may be able to increase its precision by increasing the threshold it uses to make a decision.\nHowever, this would also mean that the model refrains from saying “positive” more often, meaning that it identifies fewer of the total possible number of positives and results in lower recall because of the increased precision.\nThe choice of threshold can impact the value of the metric substantially, making comparisons between models tricky.\nConceptually, AUC ROC is computed by creating a plot showing the true-positive rate and the false-positive rate for a given model at every possible classification threshold, and then finding the area under that plotted curved line; see Figure 5-3 for an example.\n(This sounds expensive, but efficient algorithms can be used for this computation that don’t involve actually running a lot of evaluations with different thresholds.) When the area under this curve is scaled to a range from 0 to 1, this value also ends up giving the answer to the following question: “If we randomly choose one positive example and one negative from our data, what is the probability that our model gives a higher prediction score to the positive example rather than the negative?”\nInstead, they look at the raw numerical output that represents a model’s prediction, like predicted price for a given skein of yarn, number of seconds a user might spend reading a description, or the probability that a given picture contains a puppy.\nWhen comparing predictions from a model to a ground-truth value, the first metric we might look at is the difference between our prediction and reality.\nBoth metrics have the useful quality that a value of 0.0 shows a perfect model.\nIt can be especially useful to compute both metrics and see if they yield qualitatively different results for a comparison between two models, which can provide clues into a deeper level of understanding their differences.\nFor example, if we are creating a risk-prediction model predicting the chance of an accident, there is an enormous difference between an operation being 99% reliable and 99.99% reliable—and we could end up making very bad pricing decisions if our metrics did not highlight these differences.\nObviously, it is highly useful for MLOps folks to have a working knowledge of the various distributions and metrics that are most critical for assessing model quality for our system.\nFor example, the main problems in developing the first version of a yarn store product recommen‐ dation model are much more likely to be around creating a data pipeline and a serving stack, and model developers might not have bandwidth to choose carefully between varying classification or ranking metrics.",
      "keywords": [
        "model",
        "Evaluating Model Validity",
        "Evaluating Model Quality",
        "Model Quality",
        "Evaluating Model",
        "metrics",
        "Model Validity",
        "AUC ROC",
        "Quality",
        "model evaluation metrics",
        "ROC",
        "Bias",
        "Precision",
        "model quality evaluations",
        "Classification metrics"
      ],
      "concepts": [
        "model",
        "metric",
        "different",
        "difference",
        "values",
        "valued",
        "predictions",
        "predicts",
        "prediction",
        "evaluate"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 17,
          "title": "",
          "score": 0.584,
          "base_score": 0.434,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 16,
          "title": "",
          "score": 0.532,
          "base_score": 0.382,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 19,
          "title": "",
          "score": 0.419,
          "base_score": 0.269,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 28,
          "title": "",
          "score": 0.398,
          "base_score": 0.398,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 14,
          "title": "",
          "score": 0.395,
          "base_score": 0.395,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "metrics",
          "positive",
          "metric",
          "probability",
          "bias"
        ],
        "semantic": [],
        "merged": [
          "metrics",
          "positive",
          "metric",
          "probability",
          "bias"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2737843012927797,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.825230+00:00"
      }
    },
    {
      "chapter_number": 16,
      "title": "Segment 16 (pages 136-144)",
      "start_page": 136,
      "end_page": 144,
      "summary": "For example, we might notice a cold-start problem in which new products are not represented in the held-out set, or we might decide to look at calibration and bias metrics across a range of slices by country or product listing type to understand more about our model’s performance.\nTogether, these two forms of testing establish a decent level of trust in a model and will be a reasonable starting point for many statically trained ML systems.\nCHAPTER 6 Fairness, Privacy, and Ethical ML Systems\nThis chapter is devoted to topics related to ethical considerations and legal obliga‐ tions when creating or deploying ML systems.\nEditor’s note: When we put together the list of topics for what MLOps folks truly need to know, issues of fairness, privacy, and ethical concerns in AI and ML systems were right at the top of the list.\nIndeed, one reasonable position right now is that a quite viable approach to promoting fairness in computing systems is not to use AI/ML.\nTherefore, when we think about fairness and ethics in AI and ML, we also have to recognize that they may very well provide improve‐ ments in some cases relative to human decision makers.\nSo, despite the gloom and doom you will find in this chapter, we also recognize from the outset that some uses of algorithms have been tremendously successful in terms of increasing overall fairness, even if there isn’t yet a clear solution to making AI and ML fair and just in a guaranteed or global sense.\nWithin this chapter, we also provide notes on how you might consider refactoring work at your organization, in practical ways, so as to enhance fairness and ethics in your own AI/ML work.\nFairness (a.k.a. Fighting Bias) Algorithmic fairness, and other variations of this term, are a hot topic in ML and have been for many years.\nIn framing such discussions, many have emphasized that this could happen because data used to train ML systems could be taken from biased systems or collected in a biased way.\nIn this section, we focus on the mainstream use of fairness as exclusively related to bias.\nChapter 6: Fairness, Privacy, and Ethical ML Systems\nThis is a key point: it’s important to realize that the fundamental source of bias can come from many steps in the ML pipeline, including bad data, but also including bad modeling choices.\nYet, this data, sampled in a biased fashion, then creates new biased inputs for more ML modeling to allocate future police patrols.\nAmazon famously developed but did not deploy an in-house hiring algorithm that applied a strong negative parameter for attending a woman’s college.4 Thus we see a case of an algorithm using irrelevant factors to make a decision in a way that directly discriminates against a group (in this case, women going to women’s colleges).5 If the algorithm had been used, this would have looked like a case of disparate treatment.\nBroadly, we can think of systemic bias as a host of factors that likely cannot be identified in individual cases as explanatory features but that, on an aggregate level, clearly influence differences in outcomes for individuals due to the structural limitations (limitations that are baked into our social, educational, and employment systems, among others).\nIndeed, even the particular problems to which AI is applied are often highlighted as themselves manifestations of systemic bias.\nChapter 6: Fairness, Privacy, and Ethical ML Systems\nTherefore, modeling human behavior should require paying particular attention to make sure that the model is giving everyone a fair shake rather than modeling everyone according to a prototype that in fact is reasonably accurate only for someone in the majority class.\nTragically, it remains indisputable that these forms of bias (and many others) con‐ tinue to pop up in a wide range of ML applications, even with increasing media attention about algorithmic fairness.\nThe ML/AI research community has been developing methods and techniques to systematically identify and remediate some of these biases.\nPractitioners who are trying to develop responsible and fair AI systems should be aware of these emerging tools that might help.\nWhat’s more, ML/AI may very well provide a way forward to make systems fairer than they have been with the use of unaccountable humans, so long as ML development is done carefully and with appropriate safeguards, when aimed at appropriate problems.\nDefinitions of Fairness In the ML community, we have not cohered around a single definition of fairness, but common categories exist that have intuitive and appealing descriptions.\nChapter 6: Fairness, Privacy, and Ethical ML Systems\nThese notions of fairness argue that, for two individuals who are “the same” other than irrelevant factors (race, gender, etc.), these individuals should be treated the same.6 Other definitions of fair‐ ness emphasize that ML should look to fairness at the level of groups.\nStill other definitions of fairness might get more complicated and look to establish causal mechanisms to understand what might drive individuals to success or failure, before looking to categorize them algorithmically.\nAs we will describe, different use cases will have different reasonable definitions of fairness.\nThus, ordinary people in fact seem to commonly apply different notions of fairness to different domains of life, and we propose that the same could plausibly be true in different contexts or use cases of ML.\nIt might seem that the ML ethics community should be attempting to converge on a single definition of fairness.\nWe have many useful definitions of fairness to select from, and picking one or several to work toward provides us with the opportunity for rapid progress right away without needing to await further theoretical or legal developments.\nThese are the very people most likely to miss the ways that AI and ML systems can negatively impact disempowered or underrepresented groups.\nGroup parity fairness definitions require that relevant rates of algorithmic perfor‐ mance be the same across various groups.\nChapter 6: Fairness, Privacy, and Ethical ML Systems",
      "keywords": [
        "Fairness",
        "bias",
        "systems",
        "model",
        "Fighting Bias",
        "decision",
        "Systemic bias",
        "Definitions of Fairness",
        "decision makers",
        "Sampling bias",
        "groups",
        "algorithm",
        "human decision makers",
        "Ethical",
        "Definitions"
      ],
      "concepts": [
        "fairness",
        "bias",
        "biases",
        "different",
        "differences",
        "algorithmic",
        "algorithms",
        "systems",
        "systemic",
        "decisions"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 17,
          "title": "",
          "score": 0.692,
          "base_score": 0.542,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 19,
          "title": "",
          "score": 0.68,
          "base_score": 0.53,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 18,
          "title": "",
          "score": 0.651,
          "base_score": 0.501,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 46,
          "title": "",
          "score": 0.562,
          "base_score": 0.412,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 8,
          "title": "",
          "score": 0.558,
          "base_score": 0.408,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "fairness",
          "bias",
          "ethical",
          "definitions",
          "ai ml"
        ],
        "semantic": [],
        "merged": [
          "fairness",
          "bias",
          "ethical",
          "definitions",
          "ai ml"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.37953899357906684,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.825286+00:00"
      }
    },
    {
      "chapter_number": 17,
      "title": "Segment 17 (pages 145-152)",
      "start_page": 145,
      "end_page": 152,
      "summary": "In contrast to group parity, calibration fairness definitions require that a ML model work equally well for all individuals.\nSo a calibration-oriented definition of fairness would require that for any group, the ML score means the same thing.\nBecause this algorithm has been shown to be calibrated correctly across racial categories, the same COMPAS score (say, 0.5) means the same thing for both Black and white data subjects: that is, a white offender and a Black offender with the same risk score have the same probability of reoffending.8 When understood in plain English, this seems intuitive as well—that a model score should mean the same thing for everyone, no matter what group they belong to.\nAfter all, there are many correct and intuitive ways of describing the world in other contexts, so why not in the case of fairness?\nThe algorithm passed calibration checks that were considered to be the gold standard in ensuring fairness and nondiscrimination.\nMathematically, not all fairness definitions can be satisfied at the same time, given real world conditions.\nWe have to decide which fairness goals to pursue in an ML context, and likewise decide whether focusing on a specific fairness metric might even tend to reduce the tendency toward a holistic viewpoint that might otherwise be more helpful in enhancing fairness and other ethical values.\nFor now, we consider that for a specific ML tool, it may make sense to choose and emphasize a particular fairness metric, while recognizing that it will not be possible to satisfy all intuitive and normatively desirable notions of fairness at the same time.\nWhat’s more, some researchers have recognized that different situations may call for different fairness metrics, given the relative harms or policy goals of a particular use case of an algorithm, and so interested readers can find useful guidance as to how to choose a fairness metric for a specific task given specific concerns about the likely consequences of various kinds of mistakes.\n9 If we look for and interpret questions about algorithmic fairness broadly, other impossibility theorems are related to fairness concerns.\nFor example, Arrow’s impossibility theorem demonstrates that three intuitive fairness criteria for a specific form of voting cannot all be met at the same time.\nChapter 6: Fairness, Privacy, and Ethical ML Systems\nIn contrast, an example normative statement could be that algorithmic bias is wrong because all equally qualified people should have an equal chance at a job as a matter of funda‐ mental fairness.\nAnd so, we can see how different sets of similar but not identical normative values can lead to conflicts that will continuously surface in real-world debates and decisions about the best way to enhance fairness in society.\nReaching Fairness Concretely, a classic ML setup has three modes of working toward fairer (less biased) outcomes.\nThese methods intervene in data rather than in models.\nFor example, some approaches, such as that of Kamiran and Calders (2011), offer ways of identifying data that should be relabeled because the data suggests a biased outcome.10\nAlso it can be difficult to identify exactly what about the model changes as a result of changing the data, since in most cases it will not be possible to know all that clearly affects the relabeling a few data points.\nThis can manifest in any way in which the actual step-by-step training of a model is affected by fairness considerations.\nAnother method is similar to that described in preprocessing (learned fair repre‐ sentations) in terms of motivation to remove identifying information from the model’s knowledge about the sensitive attribute.\nAnother example of post-processing is to set different thresholds—say, for a credit score or college admissions scores—for different groups such that the predictions or decisions made with the score can be equally accurate for different\nChapter 6: Fairness, Privacy, and Ethical ML Systems\nFor example, people may be uncomfortable with the idea of having explicitly different score cutoffs for different groups, and they also might be uncomfortable with the notion of eliminating, and replacing with randomized numbers, those outputs from an algorithm that are, in fact, likely to be correct.\nAlgorithms will help increase efficiency and uniformity, but there will never be a single go-to algorithmic fairness solution for all scenarios.\nAlgorithmic Fallbacks for Fairness?\nThere is no “perfectly” fair solution to any algorithmic fairness challenge (at least none has been identified so far).\nLikewise, there is no perfect way to enforce fairness even if you do settle on a • particular definition of fairness (that is, defining a metric doesn’t guarantee you can make a model perfectly conform to that metric or indicate how you should attempt to do so).\n• Any process to work toward fairness is far better than a world in which we ignore these issues—because ignoring hasn’t led to good outcomes.\nFairness is no different.\nIf you are doing ML work that touches on these core areas, you likely need to be deeply concerned about fairness—and, specifically, about fairness as it is implemented in your nation’s antidiscrimination laws.\nWith the advent of big data, many datasets were compiled, often about overlapping groups of people, and it then became possible to use different datasets together to identify people from de-identified information.\nFor example, for most Americans, it turned out to be possible to identify them in a de-identified dataset merely from knowing their birthday, zip code, and gender.13 And, the world is increasingly full of new datasets, sometimes resulting from data breaches but other times resulting from people voluntarily sharing information about themselves—information that becomes very easy to access by any casual creepy stalker.\nSo, for example, we could apply k-anonymity to a dataset listing individuals in a town by requiring that data be bucketed such that for any zip code / birth information / gender category, there were at least 10 individuals.",
      "keywords": [
        "fairness",
        "data",
        "model",
        "algorithmic fairness",
        "fairness metric",
        "algorithm",
        "fairness definitions",
        "Privacy",
        "algorithmic",
        "information",
        "group",
        "world",
        "score",
        "methods",
        "fairness definitions require"
      ],
      "concepts": [
        "fair",
        "data",
        "different",
        "examples",
        "score",
        "scoring",
        "given",
        "algorithm",
        "algorithmic",
        "individuals"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 16,
          "title": "",
          "score": 0.692,
          "base_score": 0.542,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 15,
          "title": "",
          "score": 0.584,
          "base_score": 0.434,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 18,
          "title": "",
          "score": 0.472,
          "base_score": 0.322,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 19,
          "title": "",
          "score": 0.414,
          "base_score": 0.264,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 46,
          "title": "",
          "score": 0.341,
          "base_score": 0.191,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "fairness",
          "score",
          "algorithmic",
          "fairness metric",
          "different"
        ],
        "semantic": [],
        "merged": [
          "fairness",
          "score",
          "algorithmic",
          "fairness metric",
          "different"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.20624081245692258,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.825320+00:00"
      }
    },
    {
      "chapter_number": 18,
      "title": "Segment 18 (pages 153-161)",
      "start_page": 153,
      "end_page": 161,
      "summary": "This mathematical method adds noise to data such that probabilistic guarantees can be made regarding the possibility (or more importantly, lack of possibility) to make inferences about a specific individual when given access to that noisified data.\nHowever, if differential privacy is applied, it would, in probability, not be possible to infer that individ‐ ual’s age at a pre-specified level of precision and given a pre-specified query budget.14 Differential privacy can apply quite broadly, not just to the computation of aggregate statistics but also to the training of ML models, with methods to ensure that a model’s outputs or training is not contingent on inclusion of a particular data point.\nHowever, if an ML model has already been trained, that right to deletion may not be entirely meaningful.\nFor example, does an individual’s right to delete their data mean that they can also force a company to prove that its ML models have also “forgotten” their data?15\nSome have explored how training models with differential privacy might address this concern.\nHowever, technical challenges remain because producing ML models with differential privacy guarantees is quite a technical challenge.\nAny data about people in a database should be treated as a “need to know” resource, with ML engineers requesting access for specific purposes rather than being able to freely access or browse data.\nThis makes it possible to understand data use patterns, see when someone might be inap‐ propriately accessing data, and preserve evidence in case allegations of inappro‐ priate use are made later.\nAn analysis of such logs may also indicate ways in which data storage schema could be refactored to reduce the extent of data that different use patterns can access.\nFor example, if an ML model calls for access to a sensitive table of data merely to access one column, consider splitting off that column of data rather than granting access to a full table of additional but unnecessary pieces of information.\n17 One recommended starting point is TensorFlow Privacy, which includes training algorithms for differentially\nChapter 6: Fairness, Privacy, and Ethical ML Systems\nThe data needed for legitimate business uses should be separated from sensitive data (such as names and addresses) that is unlikely to be relevant to creating an ML model.\nFor example, to predict users’ clicks, there doesn’t seem to be a justification for knowing a user’s name or address.18 Therefore, there is no reason for that information to be stored with information that might be useful for that particular prediction task, such as past browsing history or demographic information.19 As noted previously, studying your data access logs can help you identify ways in which data storage can be refactored to minimize exposure of data to ML applications.\nML engineers should be given—but usually are not —a thorough review not only of general ethics training (such as the possibility of bias in data) but also domain-specific training when building algorithms for a specific use case.\nIn addition to technical measures already described, it makes sense to have explicit rules that are readily available regarding what constitutes appropriate access to data and use of that data and what use cases are expressly prohibited.\nPrivacy by design can provide a flexible but holistic way to ensure privacy in all elements of ML development.\nHere we highlight a few key categories of laws that are related to privacy and ML:\nChapter 6: Fairness, Privacy, and Ethical ML Systems\nNonetheless, such laws are useful in providing notice to consumers as to when they may be most at risk because of exposure of their personal data.\nData protection and personal privacy laws\nThis category of more general consumer protection law is important in the US context, since the US otherwise lacks a comprehensive national personal data privacy regime.\nThus it is—at the least—essential to ensure that, as your organization builds out data collection and ML modeling capabilities, you ensure that such practices are consistent with public-facing privacy policies and terms of service.\nExplanation ML model explanation is the process of analyzing and presenting information about an ML system to describe how that system works.\nThis process and the goal of making the model amenable to human understanding is often discussed in shorthand as explainability, and it’s a key area of interest with respect to ML.\nFor both technical and ethical reasons, it is desirable to have methods that can “explain” how an ML model works or why it reached a particular outcome in a particular case.\nThe technical motivation for explainable AI is related to controlling model quality and possibly learning about the data through the model.\nTechnologists who develop explanations of their models can gain insights into why their model is working well or poorly, and may even learn something about the underlying domain of the data.\nIn this way, model explanation can feel a lot like thinking through the problems that highly trained professions, such as doctors or lawyers, face in trying to give advice or a diagnosis of a real-world situation to a particular audience.\nIf the purpose of an ML explanation is to inspect model quality, such as to make sure the model is making the right decisions for the right reasons, then a global explanation might be preferred.\nChapter 6: Fairness, Privacy, and Ethical ML Systems\nContemplate whether you can offer global (explaining the model overall) or • local (explaining a particular ML model decision/classification) explanations and explore a few techniques for doing so.\nprovides a great and accessible example of considerations that go into what kind of explanation is likely to be useful.\nBut if a variety of ML algorithms are all using the same logic to not hire a particular candidate, that candidate never gets the chance at a job, and we never actually know whether that candidate could have done a good job (systematically missing data).\nIf someone is labeled by a ML algorithm in a particular way, sometimes that label is trusted to settle the issue, even if humans are supposed to be exercising some level of supervision.23 In the employment scenario, someone could be labeled a poor candidate and not be hired by the human who is using the algorithmic assistance.\nAt some point, their extended period of unemployment will itself become another factor that an ML algorithm is likely to use as a flag against hiring someone.\nChapter 6: Fairness, Privacy, and Ethical ML Systems\nWhile some celebrate ML on the basis that such models can “find patterns that humans can’t see,” sometimes the patterns identified don’t make any sense, or make sense for the wrong reason.\nSocial and Cultural Appropriateness Another element of consideration for the responsible use of ML relates to the accept‐ able use of technology more generally in social situations or with social ramifications.",
      "keywords": [
        "data",
        "privacy",
        "Differential privacy",
        "Explanation",
        "data access",
        "model",
        "Data access guidelines",
        "access",
        "differential privacy guarantees",
        "algorithm",
        "model explanation",
        "laws",
        "Data breach",
        "responsible",
        "Differential"
      ],
      "concepts": [
        "data",
        "privacy",
        "models",
        "algorithms",
        "algorithmic",
        "useful",
        "uses",
        "access",
        "accessing",
        "explanation"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 6,
          "title": "",
          "score": 0.729,
          "base_score": 0.579,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 16,
          "title": "",
          "score": 0.651,
          "base_score": 0.501,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 19,
          "title": "",
          "score": 0.641,
          "base_score": 0.491,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 8,
          "title": "",
          "score": 0.602,
          "base_score": 0.452,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 46,
          "title": "",
          "score": 0.598,
          "base_score": 0.448,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "privacy",
          "explanation",
          "access",
          "differential",
          "differential privacy"
        ],
        "semantic": [],
        "merged": [
          "privacy",
          "explanation",
          "access",
          "differential",
          "differential privacy"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3233684739966824,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.825387+00:00"
      }
    },
    {
      "chapter_number": 19,
      "title": "Segment 19 (pages 162-169)",
      "start_page": 162,
      "end_page": 169,
      "summary": "Data Collection and Cleaning At this point in the pipeline, you have decided on a use case and are looking for data and preparing data for modeling.\nChapter 6: Fairness, Privacy, and Ethical ML Systems\nWill data be acquired in a way that respects informed consent?25 Have you dis‐ • closed the purposes for which you will use the data to the subjects in a reasonably informative and transparent way?\nModel Creation and Training Now you are in receipt of data, and it’s time to do some modeling:\n• Are you training in a manner that will reduce data leaks from the trained model and enhance robustness against malicious attacks?\nYour job is to kick the tires in a reasoned way and decide whether to give this model approval to go forward, or send it back to training:\n• Have you asked whether the model was trained on a proxy, and if so, what data is available to justify the use of that proxy?\nHave you tested the model robustly, with a fair selection of held-out data in • realistically challenging situations?\n• Have you established ex ante criteria to assess whether the model is working as expected?26\nHave you tried running the model in an online mode so that you can watch how • it performs, counterfactually, in advance of an actual product launch?\nProducts for the Market Whether models are intended for internal or external use, they need to meet the same standards for fairness and privacy protections.\nChapter 6: Fairness, Privacy, and Ethical ML Systems\nConclusion We have reviewed many fairness, privacy, and other ethical considerations that affect the design, training, and deployment of real-world ML systems.\nMost fairness, privacy, and other ethical problems in ML originate at the level of • product conception and creation.\nThe more you learn and the longer you keep Responsible AI concerns in mind, • the more your ML pipeline and ML offerings will reflect good fairness practices.\nML training is the process by which we transform input data into models.\nA training algorithm describes the specific steps by which software reads data and updates a model to try to represent that data.\nThe simplest imple‐ mentation of an ML training system is on a single computer running in a single process that reads data, performs some cleaning and imposes some consistency on that data, applies an ML algorithm to it, and creates a representation of the data in a model with new values as a result of what it learns from the data.\nTraining on a single computer is by far the simplest way to build a model, and the large cloud providers do rent powerful configurations of individual machines.\nIn part, because of our broad conception of what an ML training system is, ML train‐ ing systems may have less in common with one another across different organizations and model builders than any other part of the end-to-end ML system.\nIn Chapter 8, you will see that even across different use cases, many of the basic requirements of a serving system are broadly similar: they take a representation of the model, load it into RAM, and answer queries about the contents of that model sent from an application.\nAdditional differences appear when we look at the way that training systems maintain and represent the state of the model.\nBecause of this significant variety of differences across legitimate and well-structured ML training systems, it is not reasonable to cover all of the ways that organizations train models.\nWe’ll describe a system that lives in a distinct part of the ML loop, next to the data and producing artifacts bound for the model quality evaluation sys‐ tem and serving system.\nMany training systems have a means of representing the configuration of an individual model separate from the configuration of the training system as a whole.1 These should store model configurations in a versioned system with some metadata about the teams creating the models and the data used by the models.\nModel-training framework\nMost model creators will not be writing model-training frameworks by hand.\nIt seems likely that most ML engineers and modelers will eventually be exclu‐ sively using a training systems framework and customizing it as necessary.\nTraining or model development software",
      "keywords": [
        "Model",
        "Training Systems",
        "data",
        "Training",
        "system",
        "fairness",
        "serving system",
        "privacy",
        "model quality",
        "Ethical",
        "Model configuration system",
        "model quality evaluation",
        "training systems framework",
        "Responsible",
        "case"
      ],
      "concepts": [
        "models",
        "data",
        "different",
        "differences",
        "product",
        "production",
        "training",
        "fair",
        "ethical",
        "ethics"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 16,
          "title": "",
          "score": 0.68,
          "base_score": 0.53,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 18,
          "title": "",
          "score": 0.641,
          "base_score": 0.491,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 6,
          "title": "",
          "score": 0.564,
          "base_score": 0.414,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 46,
          "title": "",
          "score": 0.554,
          "base_score": 0.404,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 8,
          "title": "",
          "score": 0.547,
          "base_score": 0.397,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "training",
          "fairness",
          "training systems",
          "fairness privacy",
          "ethical"
        ],
        "semantic": [],
        "merged": [
          "training",
          "fairness",
          "training systems",
          "fairness privacy",
          "ethical"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3667218969378264,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.825435+00:00"
      }
    },
    {
      "chapter_number": 20,
      "title": "Segment 20 (pages 170-177)",
      "start_page": 170,
      "end_page": 177,
      "summary": "In this simplified training system, the data flows in from the left, and models emerge on the right.\nWe use an ML framework that applies a training algorithm to turn the data into a model.\nAll the while, we keep track of our models and data in a\nFeatures Training data is data about events or facts in the world that we think will be relevant to our model.\nSpecifically, features are those aspects of the data that we believe are most likely to be useful in modeling, categorizing, and predicting future events given similar circumstances.\nTo be useful, features need a consistent definition, consistent normalization, and con‐ sistent semantics across the whole ML system, including the feature store, training, quality evaluation, and serving.\nFeatures and feature development are a critical part of how we experiment when we are making a model.\nIn other words, we want to know which features make our model better.\nAs we develop the model, we need easy ways to add new features, produce new features on old data when we get a new idea for what to look for in existing logs, and remove features that turned out to not be important.\nThe characteristics of a feature store will exist, even if our model training system reads raw data and extracts features each time.\nA feature store gives a consistent place to understand the definition and authorship of features and can significantly improve innovation in model development.\nSnapshots of trained models\nAuthorship and usage of each feature by specific models\nJust as with feature stores, everyone has a rudimentary form of a model management system, but if it amounts to “whatever configuration files and scripts are in the lead model developer’s home directory,” it may be appropriate to check whether that level of flexibility is still appropriate for the needs of the organization.\nThis typically includes scheduling and configuring the various jobs associated with training the model as well and tracking when training is complete.\nThe point of ML training is to transform the input data into a representation of that data, called a model.\nThe ML framework we use will provide an API to build the model that we need and will take care of all of the boilerplate code to read the features and convert them into the data structures appropriate for the model.\nQuality Evaluation The ML model development process can be thought of as continuous partial failure followed by modest success.\nIt necessarily follows, therefore, that one of the essential elements of a model-training environment is a systematic way to evaluate the model that we just trained.\nMost Failures Will Not Be ML Failures ML training systems are complex data processing pipelines that happen to be tremen‐ dously sensitive to the data that they are processing.\nAmusing examples of these failures include such straightforward things as “the train‐ ing system lost permission to read the data so the model trained on nothing,” and “the version that we copied to serving wasn’t the version we thought it was.” Most of them are of the form of incorrectly monitored and managed data pipelines.\nSome model developers will train a model from a dataset once, check the results, deploy the model into production, and claim to be done.\nEventually, whether in a day or a year, that model developer or their successor will get a new idea and want to train a different version of the same model.\nPerhaps a better dataset covering similar cases will be identified or created, and then the model developers will want to train on that one.\nFor all of these reasons, assume every model will be retrained and plan accordingly— store configs and version them, store snapshots, and keep versioned data and meta‐ data.\nModels Will Have Multiple Versions (at the Same Time!) Models are almost always developed in cohorts, most obviously because we will want to train different versions with different hyperparameters.\nMultiple very large language models are being trained by large ML-centric organizations in order to provide answers to complex queries across a variety of languages and data types.\nThese models are so expensive to train that the production model for them is explicitly to train them once and use them (either directly or via transfer learning) “forever.” Of course, if the cost of training these models is significantly reduced or other algorithmic advances arise, these organizations may find themselves training new versions of these models anyway.\nIn many environments, we will want to serve two or more versions of the same model at the same time in order to determine how the different versions of the model work for different conditions (for those familiar with traditional web development for user experience, this is essentially A/B testing for models).\nWe need to use our model management infrastructure to keep track of model metadata (including things like the model family, model name, model version, and model creation date).\nThis is going to be a heuristic or algorithm or default that ensures that at least some basic functionality is provided by our application when the ML model is unable to provide sophisticated predictions, categorizations, and insights.\nThis approach has a tremendous problem, however: it limits how good you can let your ML models be.\nUltimately, the second backup plan, combined with the ability to serve multiple models at the same time and quickly develop new variations of existing models, provides a path to understanding and resolving future model quality problems when",
      "keywords": [
        "model",
        "Training System",
        "system",
        "Training",
        "data",
        "Training System Implementation",
        "Model Management System",
        "Basic Training System",
        "model training system",
        "features",
        "Model Management",
        "Features Training data",
        "Systems metadata system",
        "Training Systems metadata",
        "feature store"
      ],
      "concepts": [
        "model",
        "data",
        "features",
        "training",
        "systems",
        "version",
        "versions",
        "specifically",
        "reliability",
        "reliable"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 12,
          "title": "",
          "score": 0.896,
          "base_score": 0.746,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 21,
          "title": "",
          "score": 0.856,
          "base_score": 0.706,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 30,
          "title": "",
          "score": 0.775,
          "base_score": 0.625,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 7,
          "title": "",
          "score": 0.769,
          "base_score": 0.619,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 13,
          "title": "",
          "score": 0.756,
          "base_score": 0.606,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "training",
          "features",
          "models",
          "model management",
          "train"
        ],
        "semantic": [],
        "merged": [
          "training",
          "features",
          "models",
          "model management",
          "train"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4507337675007115,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.825497+00:00"
      }
    },
    {
      "chapter_number": 21,
      "title": "Segment 21 (pages 178-185)",
      "start_page": 178,
      "end_page": 185,
      "summary": "Data Will Be Unavailable Some of the data used to train new models will not be available when we try to read it.\nFor very high-volume systems (web-browsing logs, for example), many organizations subsample this data before training automatically as well, simply to reduce the cost of data processing and model training.\nIf we were to drop one out of every 1,000 training records in a completely random way, this is almost certainly safe to ignore for the model.\nModels Should Be Improvable Models will change over time, not just by adding new data.\nProcedurally, the most challenging change to model training in the training system we’re describing here is adding a completely new feature.\nFeatures Will Be Added and Changed Most production ML training systems will have some form of a feature store to orga‐ nize the ML training data.\n(Feature stores offer many advantages and are discussed in more detail in Chapter 4.) From the perspective of a training system, what we need to note is that a significant part of model development over time is often adding new features to the model.\nThis happens when a model developer has a new idea about some data that might, in combination with our existing data, usefully improve the model.\nAdding new columns to the feature store and changing to schema and content of data in the past are both activities that can significantly impact reliability of all our models in the training system if the changes are not managed and coordinated carefully.\nModels Can Train Too Fast ML production engineers are sometimes surprised to learn that in some learning systems, models can train too fast.8 This can depend a bit on the exact ML algorithm, model structure, and parallelism of the system in which it’s being implemented.\nThe problem is that multiple race conditions can exist, where two or more learner tasks consult the model, read some data, and queue and update to the model at the same time.\nThe next time a bunch of learner tasks consult the model, they find it skewed in one direction by a lot, compared to the data that they are reading, so they queue up changes to move it substantially in the other direction.\nUnfortunately for the discipline of ML production engineering, there is no simple way to determine when a model is being trained “too fast.” There’s a real-world test that is inconvenient and frustrating: if you train the same model faster and slower (typically, with more and fewer learner tasks), and the slower model is “better” in some set of quality metrics, then you might be training too fast.\nWhile the biggest costs at the beginning are people and opportunity costs, as we collect more data, train more models, and use them more, computer infrastructure costs will grow to an increasingly large share of our expenditure.\nThe cheaper models are to train,\nEarly-stage ML training systems often rebuild models from scratch on all of the data when new data arrives.\n• Use online learning, whereby the model is incrementally updated as each new data point arrives.\nSimple steps such as these can significantly impact an organization’s computational costs for training and retraining models.\nMost commonly in production ML environments, transfer learning involves starting learning with the snapshot of an already trained, earlier version of our model.\nWe don’t need to measure the actual business impact of every single trained model.\nIn that case, we need a metric that helps us compare the value being created across different implementations training the same model.\nnumber of experimental models trained\nThis will help us easily see efforts to make reading and training more efficient without requiring us to know anything at all about what the model actually does.\nConversely, overall value attempts to measure value across the entire program of ML model training, considering how much it costs us to add value to the organization as a whole.\nThis will include the cost of the staff, the test models, and the production model training and serving.\nFor example, if a system can tolerate a 24-hour outage of your training system, but it takes you 18 hours to detect any problems and 12 hours to train a new model after the problems are detected, we can‐ not reliably stay within the 24 hours.\nMany people modeling production-engineering response to training-system outages utterly neglect to include model recovery time.\nThis section covers three of the most common reliability problems for ML training systems: data sensitivity, reproducibility, and capacity shortfalls.\nData Sensitivity As has been repeatedly mentioned, ML training systems can be extremely sensitive to small changes in the input data and to changes in the distribution of that data.\nLack of representativeness in the input data is one common source of bias in ML models; here, we are using bias in both the technical sense of the difference between the predicted value and the correct value in a model, but also in the social sense of being prejudiced against or damaging for a population in society.\nExample Data Problem at YarnIt YarnIt uses an ML model to rank the results of end-user searches.\nUntil they notice this, the model will train entirely on full-price purchases.\nAs a result, the model will eventually stop recommending the discounted products, since there is no longer any evidence from our logging, data, and training system that anyone is ever purchasing them!\nThis kind of very small error in data handling during training can lead to significant errors in the model.\nReproducibility ML training is often not strictly reproducible; it is almost impossible to use exactly the same binaries on exactly the same training data and produce exactly the same model, given the way most modern ML training frameworks work.",
      "keywords": [
        "Data",
        "model",
        "training",
        "Training Systems",
        "training data",
        "model training",
        "systems",
        "Training Reliability Problems",
        "training system reliability",
        "Common Training Reliability",
        "learning",
        "Data storage systems",
        "production model training",
        "General Reliability Principles",
        "time"
      ],
      "concepts": [
        "model",
        "data",
        "trained",
        "systems",
        "value",
        "cost",
        "usefully",
        "uses",
        "production",
        "products"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 20,
          "title": "",
          "score": 0.856,
          "base_score": 0.706,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 30,
          "title": "",
          "score": 0.835,
          "base_score": 0.685,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 22,
          "title": "",
          "score": 0.822,
          "base_score": 0.672,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 10,
          "title": "",
          "score": 0.797,
          "base_score": 0.647,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 4,
          "title": "",
          "score": 0.755,
          "base_score": 0.605,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "training",
          "training systems",
          "ml training",
          "train",
          "models"
        ],
        "semantic": [],
        "merged": [
          "training",
          "training systems",
          "ml training",
          "train",
          "models"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.46241590685346223,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.825559+00:00"
      }
    },
    {
      "chapter_number": 22,
      "title": "Segment 22 (pages 186-194)",
      "start_page": 186,
      "end_page": 194,
      "summary": "As obvious as it may sound, most ML training feature storage systems are frequently updated, and it is difficult to guarantee that there are no changes at all to data between two runs of the same model.\nEven minor version updates to your ML training framework, learning binaries, or orchestration or scheduling system can result in changes to the resulting models.\nAnother way to read this is “my models could change any time I happen to update TensorFlow or PyTorch, even for a new minor version.” This is essentially true but not common, and the differences often aren’t pronounced.\nSome level of inability to reproduce precisely the same model is a neces‐ sary feature of the ML training process.\nExample Reproducibility Problem at YarnIt At YarnIt, we retrain our search and recommendations models nightly to ensure that we regularly adjust them for changes in products and customer behavior.\nTypically, we take a snapshot of the previous day’s model and then train the new events since then on top of that model.\nThis is cheaper to do but ultimately does mean that each model is really dozens or hundreds of incremental training runs on top of a model trained quite some time ago.\nDetecting that a transaction is fraudulent may take up to several days, and by that point we may have already trained a new model with that transaction included as an authorized purchase.\nWhen processors update a portion of the model based on input data that they have learned from, there can always be other processors using an older version of those keys in the model.\nin most ML models.13 We can approximate the change by treating the fraud detected as a new negative event, but the resulting model won’t be quite the same.14\nWhen they develop a new model, they train it from scratch on all the data with the new model structure and then compare it to the existing model to see if it is materially better or worse.\nIt may be obvious where this is going: the problem is that if we retrain the current production model from scratch on the current data, that model may well be significantly different from the current production model that is in production (which was trained iteratively on the same data over time).\nThe situation is actually even less deterministic than that: even if we train the exact same model on the exact same data with no changes, we might have nontrivial differences, with one model trained all at once and another trained incrementally over several updates.15\nThe only real reliability solution to this problem is to treat each model as it is trained as a slightly different variant of the Platonic ideal of that model and fully renounce the idea of equality between trained models, even when they are the same model configuration trained by the same computers on the same data twice in a row.\n13 A large and growing set of work exists on the topic of deleting data from ML models.\n14 Chapter 6 covers some cases where we want to delete private data from an existing model trained on that data.\nThis, alone, is a powerful argument for ensuring that our models do not include private data.\n15 Details of why this is the case are really specific to the model and ML framework, and beyond the scope of this book.\nThe basic capacity that we need to train a new model includes the following:\nThe state of the model at any given time is stored somewhere, but most com‐ monly in RAM, so when the training system updates the state, the system requires memory bandwidth to do so.\nOne of the tricky and troubling aspects of ML training system capacity problems is that changes in the distribution of input data, and not just its size, can create compute and storage capacity problems.\nExample Capacity Problem at YarnIt YarnIt updates many models each day.\nAt the very least, we will need to read the logs produced by the serving system, since the models that YarnIt trains daily read the searches, purchases, and browsing history from the website the day before.\nAs with most ML models, some types of events are less computationally compli‐ cated to process than others.\nTheir most common use is to combine multiple very different models for a single purpose.\nAdditionally, some models might be more important than others, and we probably want a system for prioritization of training jobs in those cases where we are resource constrained and need to focus more resources on those important models.\nOrganizational Challenges Many organizations adding ML capabilities start by hiring someone to develop a model.\nThis is reason‐ able to a point, but to be fully productive, model developers need a stable, efficient, reliable, and well-instrumented environment to run in.\nThe ML training system is one place where we can have visibility into problems (model quality monitoring) and can enforce governance standards.\nFor organizations that are newer to implementing ML, the model developers and ML training system engineers may be jointly responsible for implementing minimal privacy, fairness, and ethics checks.\nWith this perspective on how trained models are created, we can now turn our attention to the following steps in the ML lifecycle.\nThat’s a common shorthand for “creating a structure to ensure our system can ask the model to make predictions on new examples, and return those predictions to the people or systems that need them” (so you can see why the shorthand was invented).\nHave different versions of the model with different trade-offs of computational • cost and accuracy.\nAlong the way, we will also discuss critical practicalities like ensuring that the feature pipeline used in serving is compatible with that used in training, and strategies for updating a model in serving.\nKey Questions for Model Serving There are a lot of ways that we can think about creating structures around a model that support serving, each with very different sets of trade-offs.",
      "keywords": [
        "model",
        "training system",
        "training",
        "system",
        "Data",
        "Training Reliability Problems",
        "training system capacity",
        "model trained",
        "Common Training Reliability",
        "Capacity",
        "problems",
        "training data",
        "time",
        "training system engineers",
        "Reliability"
      ],
      "concepts": [
        "model",
        "training",
        "difference",
        "different",
        "chapters",
        "problems",
        "capacity",
        "computing",
        "compute",
        "computationally"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 30,
          "title": "",
          "score": 0.832,
          "base_score": 0.682,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 21,
          "title": "",
          "score": 0.822,
          "base_score": 0.672,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 10,
          "title": "",
          "score": 0.728,
          "base_score": 0.578,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 27,
          "title": "",
          "score": 0.716,
          "base_score": 0.566,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 33,
          "title": "",
          "score": 0.7,
          "base_score": 0.55,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "training",
          "trained",
          "capacity",
          "models",
          "capacity problems"
        ],
        "semantic": [],
        "merged": [
          "training",
          "trained",
          "capacity",
          "models",
          "capacity problems"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.40527076088031433,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.825609+00:00"
      }
    },
    {
      "chapter_number": 23,
      "title": "Segment 23 (pages 195-202)",
      "start_page": 195,
      "end_page": 202,
      "summary": "What Are the Prediction Latency Needs of Our Model?\nFactoring in network delays and other processing necessary to build and load the page, this might mean that we have only a few milliseconds for the model to make all of its predictions on candidate products.\nIf prediction latency is too high, we can mitigate the issue by using more powerful hardware, or by making our model less expensive to compute.\nHowever, it is important to note that parallelization by creating a larger number of model replicas is usually not a solution to prediction latency, as the end-to-end time it takes to send a single example through the model isn’t affected by simply having more versions of the model available.\nIn our modern world, defined as it is by flows of information and concepts like virtual machines and cloud computing, it can be easy to forget that computers are physical devices, and that a model needs to be stored on a physical device in a specific location.\nWe need to determine the home (or homes) for our model, and this choice has significant implications on the overall serving system architecture.\nAlthough this is not really a production-level solution, in some cases a model devel‐ oper may have a model running on their local machine, perhaps invoking small batch jobs to process data when needed.\nIt may also be the right option if latency is a hypercritical concern, or if specialty hardware is needed to run our models.\nServing our models by using a cloud-based provider can allow for easily scaling our overall computational footprint up or down, and may also allow us to choose from several hardware options.\nIn a managed inference service, some monitoring needs may be automatically addressed—although we will still likely need to independently verify and monitor overall model quality and predictive performance.\nWhen a model is needed in these settings, it is much more likely that it will need to be stored on the device itself, because the alternative is to access a model in the cloud, which requires constant network connection and may also have complicated privacy concerns.\nWhat Are the Hardware Needs for Our Model?\nIn recent years, a range of computational hardware and chip options has emerged that has enabled dramatic improvements in serving efficiency for various model\nThe main thing to know about deep models in serving is that they rely on dense matrix multiplications, which basically means that we need to do a lot of multiplica‐ tion and addition operations in a way that is compute intensive, but that is also highly predictable in terms of memory access patterns.3 The little multiplication and addition operations that make up one dense matrix multiplication operation parallelize beautifully.\nThis typically means that either we need to invest organizationally in serving deep models using GPUs, or we are using a cloud service that supplies GPUs (and may charge a premium accordingly), or that we are serving in an on-device setting where a GPU is locally available.\nHow Will the Serving Model Be Stored, Loaded, Versioned, and Updated?\nAs a physical object, our serving model has a specific size that needs to be stored.\nA model that is serving offline in an environment might be stored on disk and loaded in by specific binaries in batch jobs whenever a new set of predictions needs to be made.\nThe main storage requirements are thus the disk space needed to keep the model, as well as the I/O capacity to load the model from disk, and the RAM needed to load the model into memory for use—and of these costs, the RAM is likely more expensive or more limited in capacity.\nA model that is used in live online serving needs to be stored in RAM in dedicated machines, and for high-throughput services in latency-critical settings, copies of this model likely will be stored and served in many replica machines in parallel.\nThis means that we will need to swap the version of a model currently used in serving on a given machine with a new version.\nAny feature processing or other data manipulation that is done to our data at training time will almost certainly need to be repeated for all examples sent to our model at serving time, and the computational requirements for this may be considerable.\nFor example, for our yarnit.ai store, we might need to supply a product recommenda‐ tion model with the following:\nIn many cases, this means that the actual code used to turn these pieces of information into features for our ML model to use may be different at serving time from the code used for similar tasks at training time.\nIt is also worth noting that creating features for our model to use at serving time is a key source of latency, and in many systems will be the dominating factor.\nBatch inference is a way to avoid the problem of hosting a model to be reachable for predictions on demand when you don’t need that.\nModel Serving Architectures\nOffline model serving via data store\nIf the use case is less demanding, we might even be able to avoid the complexity of storing and serving model predictions via a database and write the predictions to a flat file, or in-memory data structure, and use them directly within the application (Figure 8-2).\nOffline model serving via in-memory data structures",
      "keywords": [
        "model",
        "serving",
        "Model Serving",
        "Offline model serving",
        "Model Serving Architectures",
        "serving model predictions",
        "model predictions",
        "time",
        "serving time",
        "Latency",
        "serving deep models",
        "Prediction Latency",
        "predictions",
        "data",
        "Serving Architectures"
      ],
      "concepts": [
        "model",
        "served",
        "products",
        "production",
        "latency",
        "latencies",
        "needs",
        "likely",
        "computational",
        "compute"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 24,
          "title": "",
          "score": 0.715,
          "base_score": 0.565,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 22,
          "title": "",
          "score": 0.642,
          "base_score": 0.492,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 11,
          "title": "",
          "score": 0.622,
          "base_score": 0.472,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 21,
          "title": "",
          "score": 0.614,
          "base_score": 0.464,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 38,
          "title": "",
          "score": 0.606,
          "base_score": 0.456,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "serving",
          "latency",
          "model serving",
          "hardware",
          "stored"
        ],
        "semantic": [],
        "merged": [
          "serving",
          "latency",
          "model serving",
          "hardware",
          "stored"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3537535645586909,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.825659+00:00"
      }
    },
    {
      "chapter_number": 24,
      "title": "Segment 24 (pages 203-211)",
      "start_page": 203,
      "end_page": 211,
      "summary": "Model Serving Architectures\nStoring multiple model outputs in memory or in application databases will have storage limitations and/or cause performance problems.\nIn our web store example, we could build a more personalized shopping experience by having the model constantly learn the real-time user behavior by using the current context along with historical information to make the predictions.\nApplications powered by inferences generated by offline models plus training the supplemental models for additional parameters in real time (Figure 8-3) provides huge benefits and significant business impacts.\nHybrid online model serving in combination with predictions generated offline\nInstead of adapting to concept drift at deployment time, the model adapts to concept drift at inference time, improving the performance of the model for customers.\nInstead of training and changing one global model, we can tune more situation- specific models with a small subset of real-time data (for example, user- or location-specific models).\nAs the predictions are made in real time, rolling out the model changes is highly challenging, especially in a container-orchestration environment like Kubernetes.\nInstead, we might need to build a cluster of single-model instances that can consume new data as quickly as possible, and return the sets of learned parameters as part of the API response.\nThis approach needs more advanced monitoring and adjustment/rollback mech‐ anisms in place, since real-time changes could well include fraudulent behaviors caused by the bad actors in the ecosystem, and they could interact with, or influence, model behavior in some way.\nModel Serving Architectures\nServing models online is more powerful when it’s combined with the model-as- a-service approach we discuss in the following section.\nModel as a Service The Model-as-a-Service (MaaS) approach is similar to software as a service and inherently favors a microservice architecture.\nWith MaaS, models are stored in a dedicated cluster and served results via well-defined APIs. Regardless of the transport or serialization methods (e.g., gRPC or REST),6 because models are served as a microservice, they’re relatively isolated from the main application (Figure 8-4).\nModels served as a separate microservice\nGiven the wide popularity of X-as-a-service approaches throughout the industry, we will focus on this particular method more than others, and will examine in detail various aspects of serving model predictions via APIs later in the chapter.\nA separate service approach allows ML engineering to make model adjustments in a stabler way, and apply well-known techniques for managing operational problems.\nIn these cases, adding a new model-serving capacity is as simple as adding new instances to the serving architecture, also known as horizontal scaling.\nAs per any development architecture in which RPCs are the sole method of communication, the choice of technical stack could vary between application and model service layers, allowing respective teams to develop very differently if required.\nVersioning is easy to proliferate, since we can store multiple versions of the models in the same cluster and point to them as required; this is extremely con‐ venient for A/B testing, for example.\nThe version-identifying information about the model being used can often be designed as a part of the service’s response data as well.\nAmong other benefits, this allows for rolling redeployments because stakeholder systems can rely on a model identifier to track, route, and collate any event data that may be generated as a result of using the ML model, such as tracking which model was used to serve a particular result in an A/B test.\nModel Serving Architectures\nServing at the Edge A slightly less commonly understood serving architecture is used when a model is deployed onto edge devices (Figure 8-5).\nModels served at the edge and as a separate microservice on the server\nRunning models on edge devices is essentially compulsory here.\n7 Federated learning is an approach that trains a model across multiple disconnected edge devices.\nModel Serving Architectures\nNon- edge infrastructure should still handle training, building, and serving of large models, while edge devices can perform local inferences with smaller models.\nML models can consume lots of RAM and be computationally expensive; fitting these on memory-constrained edge devices can be difficult or impossible.\nWe might push out a critical improvement to an on-device ML model in an iOS app, but that doesn’t mean that millions of existing users have to update the version on their iPhones.\nBecause any ML model deployed to the edge device might need to robustly keep operating and have its prediction and on-device learning setup continue working for a long time, it’s a huge architectural commitment that might carry a lot of future-looking tech debt and legacy support with it, and should be chosen carefully.\nOne of the important attributes you need to track when serving ML models in production is versioning.\nOur recommended approach is to first consider the amount of data and speed of the data required for your application: if extremely low latency is the priority, use offline/in-memory serving.\nOtherwise, use MaaS, except when you’re running on an edge device, in which case serving at the edge is (obviously) the most appropriate.",
      "keywords": [
        "Model Serving Architectures",
        "model",
        "serving",
        "Model Serving",
        "data",
        "Serving Architectures",
        "Edge",
        "edge devices",
        "serving model predictions",
        "online model serving",
        "Predictions",
        "Model API Design",
        "time",
        "Online Serving",
        "architecture"
      ],
      "concepts": [
        "models",
        "serving",
        "serve",
        "data",
        "required",
        "require",
        "architectures",
        "device",
        "applications",
        "application"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 30,
          "title": "",
          "score": 0.722,
          "base_score": 0.572,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 23,
          "title": "",
          "score": 0.715,
          "base_score": 0.565,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 21,
          "title": "",
          "score": 0.705,
          "base_score": 0.555,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 11,
          "title": "",
          "score": 0.679,
          "base_score": 0.529,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 5,
          "title": "",
          "score": 0.667,
          "base_score": 0.517,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "edge",
          "serving",
          "model serving",
          "edge devices",
          "serving architectures"
        ],
        "semantic": [],
        "merged": [
          "edge",
          "serving",
          "model serving",
          "edge devices",
          "serving architectures"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.39554515620659647,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.825715+00:00"
      }
    },
    {
      "chapter_number": 25,
      "title": "Segment 25 (pages 212-220)",
      "start_page": 212,
      "end_page": 220,
      "summary": "Resource-oriented architectures (ROAs) in the REST style appear well suited to this task, given the natural alignment of REST’s design phi‐ losophy with the desire to hide implementation-specific details.8 Partially in support of this view, we’ve seen rapid growth in the area of ML web services in recent years: for example, Google Prediction/Vision APIs, Microsoft Azure Machine Learning, and many more.\nMost service-oriented architecture (SOA) best practices apply to ML model/inference APIs too.9 But you’ll want to take note of the following points for models:\nTo gain all the benefits of DevOps, however, you will want to empower the data science team to take full ownership of releasing its models to production.\nA fun‐ damental tension exists between requiring a data science team to fully own all end-to-end components of a production deployment and fully separating production concerns away from the data science team so it may focus fully on its domain specialization of model training and model optimization.\nIf the data science team owns too little, it may be disconnected from the constraints or realities of the production system its models must fit into, and will be unable to remediate errors, assist in critical bug fixes, or contribute to architectural planning.\nSo, when we are ready to deploy models in production, we actually deploy two different things: the model itself and the APIs that go and query the model to fetch the predictions for a given input.\nThose two things also generate a lot of telemetry and a lot of information that’ll later be used to help us monitor the models in production, try to detect drift or other anomalies, and feed back into the training phase of the ML lifecycle.\nTesting Testing model APIs, before deploying and serving in production, is extremely critical because models can be have a significant memory footprint and require significant computational resources to provide fast answers.\nData scientists and ML engineers need to work closely with the software and QA engineers, and product and business teams, to estimate API usage.\nWhen serving ML models, a performance increase doesn’t always mean business growth.\nMonitoring and correlating the model metrics with the business key perfor‐ mance indicators (KPIs) help bridge the gap between performance analysis and busi‐ ness impact, integrating the whole organization to function more efficiently toward a common goal.\nModel performance is an assessment of the model’s ability to perform a task accu‐ rately, not only with sample data but also with actual user data in real time in a production setup.\nDetection is followed by mitigation of these errors by debugging, based on its behavior to ensure that the deployed model is making accurate predictions at the user’s end and is resil‐ ient to data fluctuations.\nML model metrics are measured and evaluated based on the type of model that the users are served by (for example, binary classification, linear regression, etc.), to yield a statistical report that enlists all the KPIs and becomes the basis of model performance.\nEven though improvements in these metrics, such as minimizing log loss or improv‐ ing recall, will lead to better statistical performance for the model, we find that business owners tend to care less about these statistical metrics and more about business KPIs. We will be looking for KPIs that provide a detailed view of how well a particular organization is performing, and create an analytical basis for optimized decision making.\nA resilient model, while not the best model with respect to data science measures like accuracy or AUC, will perform well on a wide range of datasets beyond just the training set.\nThis means that we don’t need to constantly monitor and retrain the model, which can disrupt model use in production and potentially even create losses for the organization.\n• Similar error rates for longer times in production models\nWe discuss more details about model quality and evaluation in Chapter 5, and API/ system-level KPIs like latencies and resource utilization in Chapter 9.\nML models deployed as API endpoints need to respond to such changes in demand.\nThe number of API instances serving the models should increase when requests rise.\nSeparate from these standard service failure considerations, the deep reliance of ML systems on training and data pipelines (whether online or offline) creates additional require‐ ments, including accommodating data schema changes and database upgrades, onboarding new data sources, durable recovery of stateful data resources (like the state of online learning, or the state of on-device retraining in an edge serving use case after an app crashes), and graceful failure in the face of missing data or upstream data ETL job outages—to name but a few.\nData is constantly changing and growing in data warehouses, data lakes, and stream‐ ing data sources: adding new and/or enhancing existing features in the product/ser‐ vice creates new telemetry, a new data source may be added to supplement a new model, an existing database goes through a migration, someone accidentally begins initializing a counter at 1 instead of 0 in the last version of the model, and the list can go on.\nWithout proper care for failure recovery, ML models that experience unexplained data changes or data disruptions may need to be taken out of production and iterated offline, some‐ times for months or longer.\nIt is critical to factor in this data extensibility as an early architectural consideration to avoid failure scenarios where we are blocked from being able to ingest a new feature for the model because of the logistics of accommodating the new data in production.\nWhen it comes to ethics and fairness while serving the ML models in produc‐ tion, we need to establish checks and balances as part of the development and deployment framework and be transparent with both internal stakeholders and customers about the data being collected and how it will be used.\nWhen we process a request through the model APIs, request and response schemas should try to avoid or at least minimize the need for user personal, dem‐ ographic information.\nAlong with data privacy, especially when dealing with PII, product/business owners and ML/software engineers should invest more time and resources to secure the model API endpoints even though they are accessible only within the internal network (i.e., user requests are first processed by the application server before calling model APIs).\nThere are multiple ways to measure the ML model and product impact over the business, including input from key stakeholders, customers, and employees, and actual ROI as measured in revenue, or some other organizationally relevant metric.\nCHAPTER 9 Monitoring and Observability for Models\nDespite that, this chapter outlines what we know about how to monitor, observe, and alert for ML production systems, and makes suggestions for developing the practice within your own organization.\nChapter 9: Monitoring and Observability for Models",
      "keywords": [
        "model",
        "Data",
        "data science team",
        "Data science",
        "model APIs",
        "Serving",
        "production",
        "Monitoring",
        "model API",
        "API",
        "systems",
        "Testing model APIs",
        "science team",
        "model API endpoints",
        "Production Systems"
      ],
      "concepts": [
        "data",
        "model",
        "production",
        "product",
        "services",
        "monitor",
        "serving",
        "serve",
        "apis",
        "api"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 42,
          "title": "",
          "score": 0.703,
          "base_score": 0.553,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 38,
          "title": "",
          "score": 0.688,
          "base_score": 0.538,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 26,
          "title": "",
          "score": 0.64,
          "base_score": 0.49,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 4,
          "title": "",
          "score": 0.623,
          "base_score": 0.473,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 24,
          "title": "",
          "score": 0.616,
          "base_score": 0.466,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "apis",
          "model apis",
          "api",
          "science",
          "data science"
        ],
        "semantic": [],
        "merged": [
          "apis",
          "model apis",
          "api",
          "science",
          "data science"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3901338881131315,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.825764+00:00"
      }
    },
    {
      "chapter_number": 26,
      "title": "Segment 26 (pages 221-228)",
      "start_page": 221,
      "end_page": 228,
      "summary": "The Concerns That ML Brings to Monitoring One important concern is not necessarily the task of monitoring ML itself, but the perception of the act of monitoring by the model development community.\nChapter 9: Monitoring and Observability for Models\nmean inspection activities applying to model development, or it can mean continual observation of systems in production.\nThis is partially because of the nature of ML, partially a function of the way models are developed today, partially the nature of production operation, and partially a reflection of the fact that tools for inspectability are generally aimed just at model development.\nReasons for Continual ML Observability—in Production Observability data from your models is absolutely fundamental to business—both tactical operations and strategic insights.\nProblems with ML Production Monitoring ML model development is still in its infancy.\nProblems with ML Production Monitoring\nThis leads to two important observations we would make about the difference between model development and production serving.\nDifficulties of Development Versus Serving The first problem is that effectively simulating production in development is extremely hard, even in separate environments dedicated to that task (like test, staging, and so on.) This is not just because of the wide variety of possible serving architectures (model pools, shared libraries, edge devices, etc., with the associated infrastructure that you might or might not be running on) but also because in development you often invoke prediction methods directly or with a relatively small amount of code between you and the model for velocity reasons.\nFinally—and crucially—the data you have in testing is not necessarily distributed like the data the model encounters in production, and as always for ML, data distribution really matters.\nChapter 9: Monitoring and Observability for Models\nThe skew most likely to cause production problems is training-serving skew, which describes any difference between the performance of your model in training and in serving.\nAstute readers will therefore note a problem: the techniques for monitoring and detecting skew are model specific (or at least specific to a particular model architecture, config‐ uration, and purpose).\nIn terms of impact on monitoring best practices, this implies that the monitoring system has to be general-purpose and flexible, but that individual model and model- family monitoring has to be implemented by production engineers and ML engineers working closely together.\nIn particular, model developers don’t generally think in terms of detection of issues post- deployment, but instead think in terms of modeling KPI performance pre-deploy‐ ment—and modeling KPIs are not necessarily directly connected to business KPIs!6\nSince the special case tools that today are used for model development (TensorBoard, Weights & Biases, and so on) don’t usually naturally translate into production itself, the particular monitoring system in use, and so on, at the moment we will necessarily have to make some of this up ourselves.\nGiven that, the overall goal for this chapter is to recommend a whole-lifecycle approach to monitoring, and in particular, suggest a default set of things to monitor other than the specific business metrics the model is intended to improve, since they are already well understood.\nBest Practices for ML Model Monitoring Let’s start off with a few framing assumptions: for the purposes of this chapter, model development is generally done in a loop.\nChapter 9: Monitoring and Observability for Models\nFrom the more practical perspective of model monitoring only, however, explainabil‐ ity is important to understand in preproduction and, increasingly, production phases.\nGeneric Pre-serving Model Recommendations We talk about this more in Chapter 3, but from a monitoring point of view, it’s most important to keep in mind the business goal attached to the development of the model, and connect its KPIs to exported metrics for monitoring purposes.\nThe most important recommendation is that your business KPIs should correlate with model metrics; you should be able to trace these continuously from development to production.\nWhen monitoring for data integrity, the most important features of the model should therefore be given high priority.\nPeople responsible for ML models want explainability for a few reasons: establishing which features should be prioritized for data integrity, investigating a specific predic‐ tion or specific slices of predictions, and responding to business requirements for\nBest Practices for ML Model Monitoring\nChapter 9: Monitoring and Observability for Models",
      "keywords": [
        "model",
        "monitoring",
        "model development",
        "Production",
        "Model Monitoring",
        "Production Monitoring",
        "development",
        "data",
        "important",
        "Observability",
        "metrics",
        "Production Observability data",
        "model development community",
        "business",
        "monitoring system"
      ],
      "concepts": [
        "model",
        "monitor",
        "production",
        "data",
        "business",
        "generally",
        "general",
        "problem",
        "likely",
        "serving"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 27,
          "title": "",
          "score": 0.825,
          "base_score": 0.675,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "",
          "score": 0.8,
          "base_score": 0.65,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 38,
          "title": "",
          "score": 0.789,
          "base_score": 0.639,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 36,
          "title": "",
          "score": 0.765,
          "base_score": 0.615,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 32,
          "title": "",
          "score": 0.759,
          "base_score": 0.609,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "monitoring",
          "development",
          "model development",
          "observability",
          "production"
        ],
        "semantic": [],
        "merged": [
          "monitoring",
          "development",
          "model development",
          "observability",
          "production"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4593533498926349,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.825814+00:00"
      }
    },
    {
      "chapter_number": 27,
      "title": "Segment 27 (pages 229-236)",
      "start_page": 229,
      "end_page": 236,
      "summary": "In the first, retraining would execute over the exact same data you used to train the old model (in which case, you would broadly expect the same behavior, since it’s the same input).\nBest Practices for ML Model Monitoring\nCompare the — training dataset to either the last time we trained this model or use another exogenous metric that can indicate rough size.\n— If the data is copied from somewhere else to go into, say, a feature store, and — the model is built from a feature store, does that copying happen correctly?\nChapter 9: Monitoring and Observability for Models\n— — What is the size of the output model?\n— — Can the model be successfully loaded and make simple predictions?\n— — For those who do testing in a separate environment, does the model pass the\n— — What is the time to get the model into production and serving queries?\n— For those who do testing in production, does the model pass exposure to users?\n— Are the business or use metrics that you track in production affected in an unexpected way by the new model?\nBest Practices for ML Model Monitoring\nBusiness validation can therefore be understood as attempting to understand the business impact of a model,12 for example, looking at how it affects profit and loss (also sometimes known as the profit/loss curve), or using a confusion matrix, which tries to understand where the model classified no and the answer should have been yes, and so on.\nTo do that, we not only have to test versus historical data, which we are probably doing anyway, but also compare two models against each other.13 We can choose from at least two good approaches:\n• Test in production, with the model getting a small subset of real user traffic (typically, between 1% and 5%), sometimes called a canary test.\nThe second approach also has the good effect of exposing any problem with the model reacting to user traffic before a full rollout.\nA hybrid approach is to send some data to the model locally, and the same data to the model running in production (though with this approach, the local model is not enabled to take full production traffic; it is just taking your test traffic).\nFinally, some folks send production traffic to a model, but don’t serve the results to end users, which tests many components of the serving path\nIn that case, there are good arguments for doing two opposite things: (1) preferring the model currently running on the basis that it changes the least and is well understood, and (2) preferring a new model on the basis that something built over more recent data is probably going to survive change better and will be less painful to transition from.\nChapter 9: Monitoring and Observability for Models\nFor all kinds of reasons, it’s highly advisable to have a fallback plan, which is a set of steps you take if the new model fails, the rollout fails, or even the old model is found to have some weird behavior in a particular subset of circumstances.\nUse an older version of the model (roll back)\nThis implies it’s a good idea to keep the actual binaries of these around, versioned correctly, as well as monitoring the actual versions in production, since stressing your training infrastructure to build a model from old data at a time of produc‐ tion outage is generally the opposite of a good idea.\nBest Practices for ML Model Monitoring\nChapter 9: Monitoring and Observability for Models\nWhat are good metrics to measure my model in serving?\n• Is my model performing as expected?\nTo choose good metrics for measuring model performance, we need to properly understand how a model can fail.\nModel\nThe model making predictions.\nThe data that is flowing in and out of the model.\nThis includes the features that the model uses to make a prediction and the prediction itself.\nBest Practices for ML Model Monitoring\nModel\nGiven those difficulties, we prefer to use the same metrics to evaluate the model in serving as we do in training/validation, since that will give us more confidence that we’re actually measuring in some sense “the same thing.” Of course, doing this requires being able to match the prediction with the corresponding observed reality.\nWhile evaluating this model in validation, you care a lot about minimizing the error (usually referred to as root mean squared error, or RMSE16) so that the model’s prediction is close to the actual ETA: in this context, it turns out that customers tend to get more upset if you overpromise and underdeliver than the other way around.\nA similar example is food delivery; as soon as the pizza has arrived at the hungry customer’s house, you have real measurements you can compare your model predictions with, and pizza delivery as a business has a strong time limit baked into it (as well as other ingredients)—\nChapter 9: Monitoring and Observability for Models",
      "keywords": [
        "model",
        "data",
        "Model Monitoring",
        "monitoring",
        "Input data",
        "training",
        "production",
        "time",
        "n’t",
        "serving",
        "version",
        "good",
        "feature",
        "model performance",
        "input"
      ],
      "concepts": [
        "model",
        "data",
        "process",
        "processed",
        "processes",
        "different",
        "difference",
        "differ",
        "time",
        "particular"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 26,
          "title": "",
          "score": 0.825,
          "base_score": 0.675,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 29,
          "title": "",
          "score": 0.754,
          "base_score": 0.604,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 22,
          "title": "",
          "score": 0.716,
          "base_score": 0.566,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "",
          "score": 0.704,
          "base_score": 0.554,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 14,
          "title": "",
          "score": 0.704,
          "base_score": 0.554,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "monitoring",
          "model monitoring",
          "traffic",
          "good",
          "chapter monitoring"
        ],
        "semantic": [],
        "merged": [
          "monitoring",
          "model monitoring",
          "traffic",
          "good",
          "chapter monitoring"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.429023185803154,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.825866+00:00"
      }
    },
    {
      "chapter_number": 28,
      "title": "Segment 28 (pages 237-245)",
      "start_page": 237,
      "end_page": 245,
      "summary": "The key thing this quick feedback loop enables is the ability to measure the effective‐ ness of your models essentially instantaneously (or at least very quickly); of course, once you have this latent ground truth linked back to your prediction event, no matter how long it took to get it, model performance metrics can easily be calculated and tracked.19 Tracking such metrics on a regular cadence allows you to make certain that performance has not degraded drastically from when a model was trained, or when it was initially promoted to production.\nHowever, many real-world environments change the way you get access to ground- truth data, as well as the tools you have at your disposal to monitor your models.\nUltimately, you can still make the first approach work if you get “enough” semi- real-time data, the data arrives reliably enough, and so on, but if you can’t make that approach work, teams may need to turn to proxy metrics.\n19 The best model metric to use primarily depends on the type of model and the distribution of the data it’s\nAs a result, we will never know whether someone the model predicted will default could have actually paid the loan back in full.\n• The time window of getting actuals is so delayed it cannot meaningfully inform the modeler that the model’s performance should be looked at.\nIn these scenarios, it’s not uncommon for ML teams to once again use proxy metrics to give signals of model performance.\nQuite apart from how a model might handle the relationship with actuals, it is possible to use generic measurements of the behavior of a model that can be useful for figuring out whether things have gone truly awry.\nThe most common causes are typically undersampling in training data, drift, and data integrity issues impacting the quality of data the model uses to make predictions.\nBy doing so, you can surface where the missed classifications happen, and what slices of data to upsample when retraining the model.\nData\nAs features are added or dropped, monitoring must be adapted to the schema of the model.\nDrift is better for catching slow changes to the distribution of the data, while data quality checks are better for catching sudden, large changes in the data.\nModels do not perform equally well on every possible input: they are highly dependent on the data they were trained on, perform well when they see data that resembles that, and perform less well when they don’t.\nEspecially in hyper-growth businesses where data is constantly evolving, accounting for drift is important to ensure that your models stay relevant.\nSome models are resilient to minor changes in input distributions, but accepting infinite resilience does not exist; at some point data distributions will stray far from\nIt would be great if the only things that could change were the inputs to your model, but unfortunately, that’s not the case.\nAssuming your model is deterministic, and nothing in your feature pipelines has changed, it should give the same results if it sees the same inputs.\nEspecially when actuals are not available, drift is used in the real world to identify changes in model predictions, features, and actuals.\nIn many ML use cases where performance metrics cannot be calculated directly, drift often becomes the primary way to monitor changes in model\nWhile drift is focused on slow failures, data quality monitoring for models is focused on the hard failures.\nModels rely on the input features coming in to make a predic‐ tion, and these input features can come from a variety of data sources.\nLet’s say, for example, your hypothetical model predicting which pet food to buy for your pet supply store sees data suggesting that people own only cats now.\nThis is, quite simply, a bug in your data stream, and a violation of the contract (semantic expectations) you have set up between the data and the model.\nAt this point, whatever comes out of your model is undefined behavior, and you need to make sure to protect yourself against type mismatches like this in categorical data streams.\nWhile these techniques help you compensate for this problem, it’s not really a sustainable solution: if you have hundreds, thousands, or tens of thousands of data streams used to compute one feature vector for your model, the chance that one of these streams is missing can be very high!\nThough this is more to do with robustness than monitoring, it is possible to compensate for missing values in categorical data in a number of ways, a process commonly referred to as imputation.\nYou could choose the most common category that you have histor‐ ically seen in your data,20 or you could use the values that are present to predict what this missing value likely is.\nIt’s not surprising to ML practitioners today that many models rely on very large numbers of features to perform their task.\nThe reality is that this data schema will inevitably change often as the team experiments to improve the model.\n— Missing values: Is this feature missing data?\n— — Missing values: Is this feature missing data?\nFor an ML system to be successful, you need to understand not just the data going in and out of the ML system, and the performance of the model itself, but the overall service performance in rendering or serving the model—making its predictions or classifications available.\nEven if the model performance improves business outcomes and data integrity is maintained, but it takes several minutes for a single prediction, it might not be performant enough to be deployed in a real-time serving system.\nRegardless of what is used for model serving, it is important (especially in real-time services) to monitor the prediction latency of the service because the expected prediction should happen immediately after the request is sent.\nReduce the time it takes for the model to make a prediction.\nThis is not just about the model, but also gathering the input features (sometimes precomputing or caching them), and quickly catching the predictions to serve.\nTo optimize the model for lower prediction latencies, the best approach is to reduce the complexity of the model.\nFor example, if there are more levels in a decision tree, more-complex relationships can be captured from the data and therefore increase the overall effectiveness of the model.\nBefore the model can even make a prediction, all of the input features must be gathered, and this is often accomplished by the service layer of the ML system.\nFor example, a model predicting the likelihood of a customer responding to an ad might take in the historical purchase information of this customer.\nThe customer wouldn’t provide this when they view the page themselves, but the model service would query a feature store or a real-time database to fetch this information.",
      "keywords": [
        "model",
        "data",
        "Model Monitoring",
        "model performance",
        "feature",
        "Monitoring",
        "Categorical data",
        "numerical data",
        "model performance metrics",
        "feature missing data",
        "drift",
        "data quality",
        "predictions",
        "distribution",
        "Models model"
      ],
      "concepts": [
        "data",
        "models",
        "change",
        "changed",
        "features",
        "prediction",
        "predict",
        "value",
        "service",
        "monitor"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 10,
          "title": "",
          "score": 0.722,
          "base_score": 0.572,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 5,
          "title": "",
          "score": 0.666,
          "base_score": 0.516,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 29,
          "title": "",
          "score": 0.654,
          "base_score": 0.504,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 32,
          "title": "",
          "score": 0.614,
          "base_score": 0.464,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 27,
          "title": "",
          "score": 0.608,
          "base_score": 0.458,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "missing",
          "drift",
          "performance",
          "input features",
          "actuals"
        ],
        "semantic": [],
        "merged": [
          "missing",
          "drift",
          "performance",
          "input features",
          "actuals"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.37302708553199737,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.825914+00:00"
      }
    },
    {
      "chapter_number": 29,
      "title": "Segment 29 (pages 246-254)",
      "start_page": 246,
      "end_page": 254,
      "summary": "SLOs in ML monitoring.\nChapter 9: Monitoring and Observability for Models\nFor environments with lots of models, or lots of model churn, a promising • approach is a self-service infrastructure so ML engineers can define and enforce per model or per class of model SLOs (generally by comparison to a golden dataset); SREs could develop, offer, and support such a service, thus helping the overall SLO approach scale for everyone.\nBehaviors or metrics to monitor per ML context\nBest Practices for ML Model Monitoring\nTraining Pretraining: Source data size (i.e., hasn’t grown or shrunk a lot since last build) Training: Training time as a function of model type and input data size Training configurations (e.g., hyperparameters) Post-training: Model quality metrics (accuracy, precision, recall, other model-specific metrics) Built model passes validation tests (not only that aggregate filters work, but results on golden set and held-out set tests are of equal or better quality).\nServing Model serving latency (as a fraction of overall serving latency or compared over time) Model serving throughput Model serving error rate (time-out/ empty value) Serving resource use (RAM in particular) Age/version of model in serving\nApplication (YarnIt) Model-specific metrics (visible on individual page load) Number of recommendations per page Estimated quality (predicted click-through) of each recommendation Similar metrics for search model\n(Though we’ve been talking about distributions a lot in this chapter, we are specifically referring to monitoring in this section, rather than ML.)\nChapter 9: Monitoring and Observability for Models\nOur experience suggests that deduplication is potentially the most important capability here—if your system can’t perform that, the next time you have a serious outage, you potentially have N × <number of pages you would receive>, where N is the number of monitoring instances you have.\nIn this chapter, we have covered important metrics for moni‐ toring the service health, model efficacy, and data integrity of the model.\nHowever, the topic of monitoring includes other things to consider that we will not be able to cover in this chapter.\nFrom a monitoring point of view, our major concern is the requirement to facilitate such visibility into model decisions, while not facilitating inappropriate visibility—see the next section for more detail.\nBest Practices for ML Model Monitoring\nAnother important category we don’t cover is relating the model performance metrics to the business impact of the model.\nAs ML increasingly uses images, video, and so on as inputs into its models, it’s necessary to monitor data integrity for these nontabular data types too.\nDetermining the correlation between available monitoring metrics and business value is almost certainly the most important thing for the organization to do in order to extract the most value out of its ML efforts in production.\nChapter 9: Monitoring and Observability for Models\nHigh-Level Recommendations for Monitoring Strategy “Serving” on page 205 made many detailed recommendations for how to start mon‐ itoring, but we’d like to cover a few high-level recommendations for your overall monitoring strategy here:\nIf your model is able to get back actuals in a near real-time way, monitoring model/KPI performance is the best signal, since that corresponds most closely to your concept of what the model is doing.\nThere’s also some value in looking at how the model performance is working at a pure service level.\nThink of it as a simple request-response service; if the model is unable to make predictions/recommendations, if the model produces predictions with a drastically lower confidence, and if any algorithmic fallback path is being invoked more than expected, these are all good things to know (and you should therefore monitor them).\nBest Practices for ML Model Monitoring\nWe haven’t said much specifically about alerting in this chapter, since in general, once you’re monitoring a metric and have established some kind of threshold, alerting is (supposed to be) a fairly mechanical achievement.\nThe major challenge here is that since the model building duration can be a large number of minutes, hours, or even days, it doesn’t make much sense to alert on every fluctuation in training performance; otherwise, you’ll be alerting too much.\nAn important monitoring and/or debugging technique is to log predictions (ID, value) to a table, or some otherwise easily searchable format—if you’re concerned about speed, head-of-line blocking, and so on, then sampling is also a perfectly tractable approach (though you will then lose guaranteed explainability).\nChapter 9: Monitoring and Observability for Models\nAs per the preceding “Generic recommendations” paragraph, success here really looks like being able to tie the specific state of the model at the time of specific prediction request with a specific state.\nConclusion We hope this chapter has provided a useful overview to monitoring your ML systems from birth to happy life in production.\nTo reiterate: the main battle is to realize that you need to monitor at as high a level of fidelity as you can—for explainability, production debugging, and just generally knowing how your business is doing.",
      "keywords": [
        "model",
        "monitoring",
        "data",
        "Model Monitoring",
        "system",
        "serving",
        "model performance",
        "Model serving",
        "metrics",
        "model performance metrics",
        "n’t",
        "SLOs",
        "training",
        "Service Level Objectives",
        "Implementing Service Level"
      ],
      "concepts": [
        "model",
        "monitoring",
        "data",
        "systems",
        "systemic",
        "served",
        "training",
        "metrics",
        "useful",
        "uses"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 5,
          "title": "",
          "score": 0.797,
          "base_score": 0.647,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 26,
          "title": "",
          "score": 0.756,
          "base_score": 0.606,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 27,
          "title": "",
          "score": 0.754,
          "base_score": 0.604,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "",
          "score": 0.71,
          "base_score": 0.56,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 28,
          "title": "",
          "score": 0.654,
          "base_score": 0.504,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "monitoring",
          "metrics",
          "chapter monitoring",
          "service",
          "model monitoring"
        ],
        "semantic": [],
        "merged": [
          "monitoring",
          "metrics",
          "chapter monitoring",
          "service",
          "model monitoring"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.41524591986911974,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.825963+00:00"
      }
    },
    {
      "chapter_number": 30,
      "title": "Segment 30 (pages 255-262)",
      "start_page": 255,
      "end_page": 262,
      "summary": "Up until now, our discussions of ML systems have sometimes centered on the idea that a model is something we train and then deploy, almost as though this is some‐ thing that happens once and only once.\nA slightly deeper view is to draw a distinction between models that are trained once and deployed versus those that are trained in a more continuous fashion, which we will refer to as continuous ML systems.\nTypical continuous ML systems receive new data in a streaming or periodic batch fashion and use this to trigger training an updated model version that is pushed to serving.\nManaging continuous data streams, responding to model failures and corruptions, and even seemingly trivial tasks like introducing new features for the model to train on all increase system complexity.\nIf we remember that in ML, data is code, the idea of continuous ML is to accept the equivalent of a steady stream of new code that can change the behavior of our production system.\n• Models may influence their own future training data through feedback loops.\nAt a high level, a continuous ML system regularly takes data in from the world in a steady stream, uses it to update the model, and then after appropriate validation, pushes out an updated version of the model to serve new data.\nTraining Examples Rather than existing as a fixed set of immutable data, training data in a continu‐ ous ML system comes in a steady stream.\nIn all cases, it is important to remove such forms of bad data from the pipeline before training, so that the model training is not impacted.\nFeature Stores and Data Management In typical production ML systems, raw data is converted into features, which in addition to being useful for learning are also more compact for storage.\nMany pro‐ duction systems use a feature store for storing data in this way, which is essentially an augmented database that manages input streams, knows how to convert raw data into features, stores them efficiently, allows for sharing among projects, and supports both model training and serving.2 For high-volume applications, it is often necessary to do some amount of sampling from the overall stream of data to reduce storage and processing costs.\nUpdating the Model In continuous ML systems, it is often preferable to use a training methodology that allows for incremental updates.\n(Recall that SGD forms the basis of most deep learning training platforms.) To use SGD in a continuous setting, we essentially just close our eyes and pretend that the stream of data shown to the model comes in a stochastic (random) order.\nHowever, more data- starved applications may need to visit individual examples many times to converge to a good model, so this strategy of visiting each example exactly once in order cannot always be followed.\nPushing Updated Models to Serving In most continuous ML settings, we refer to major changes to a model as a launch.\nMinor changes such as small modifications to internal model weights based on new incoming data are referred to as updates.\nThe model may be corrupted somehow— perhaps by bugs, perhaps by having been trained on bad data.\nOf course, lots of things can go wrong with our system if we do not regularly push model updates based on new data, so the point is not to avoid updating models, but rather to point out that validation of model checkpoints is a critical step before they are pushed to serving.\nAfter a short delay, the model is updated on this new influx of search and purchase data, and learns to predict much higher values for brown wool products.\nOur model is trained with a form of SGD, which ends up overconfident and making scores extremely high for these products.\nThe next influx of data shows that no users are purchasing any products, and the model overcompensates, but because the brown_wool products have been shown on such a broad range of queries to such a broad range of users, the model now learns to give lower scores for nearly all products, resulting in no results or junk results for all user queries.\nThis reinforces the trend that no users are purchasing anything, and the system spirals down, until our MLOps team identifies the issue, rolls the model back to a previous well-behaved version, and filters the abnormal data from our store of training data before re-enabling training.\nOne way to do this would be to create a model to do propensity scoring, which shows how likely a given example is to occur in our training data at a given time.\nwe could use extensive randomization to ensure that no example has too low a probability of being included, but this may well mean exposing users to random or irrelevant data or having our models suggest random actions that may be undesirable.\nModels Can Influence Their Own Training Data One of the most important questions to answer for our continuous ML system is whether a feedback loop exists between a model and its training data.\nClearly, all trained models are influenced by the stream of data that comes in for training, but some models also, in turn, influence the data that is collected.\nTo help understand the issues, consider that some model systems have no influence over the stream of data that is collected for retraining.\nOther models do influence the collection of their training data, especially when those models make recommendations to users or decide on actions that impact what they can learn about next.\nAs a more concrete example, consider a model that helps recommend wool products to show to users; the model may then be trained on the user response to those selected products, such as clicks, page views, or purchases.",
      "keywords": [
        "model",
        "data",
        "Continuous",
        "training data",
        "system",
        "training",
        "continuous ML systems",
        "continuous data streams",
        "Bad Data",
        "stream",
        "world",
        "products",
        "system behavior",
        "model training",
        "data streams"
      ],
      "concepts": [
        "model",
        "train",
        "production",
        "product",
        "setting",
        "sets",
        "examples",
        "continuous",
        "likely",
        "updated"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 32,
          "title": "",
          "score": 0.861,
          "base_score": 0.711,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 21,
          "title": "",
          "score": 0.835,
          "base_score": 0.685,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 4,
          "title": "",
          "score": 0.835,
          "base_score": 0.685,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 22,
          "title": "",
          "score": 0.832,
          "base_score": 0.682,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 10,
          "title": "",
          "score": 0.825,
          "base_score": 0.675,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "continuous",
          "stream",
          "training",
          "continuous ml",
          "stream data"
        ],
        "semantic": [],
        "merged": [
          "continuous",
          "stream",
          "training",
          "continuous ml",
          "stream data"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.48766142507806676,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.826014+00:00"
      }
    },
    {
      "chapter_number": 31,
      "title": "Segment 31 (pages 263-271)",
      "start_page": 263,
      "end_page": 271,
      "summary": "products that were not selected.4 It is easy to imagine that a new wool product, such as a new color of organic alpaca yarn, might be something that users would love to purchase, but for which the model has no previous data.\nIf the product recommendation relies on purchase behavior as a signal in training, and the presence of discounts influences purchase behavior, then there is a feedback loop that links these two models, and changes or updates to one model can influence the other.\nThe effect of feedback loops can be lessened to some degree by logging model version information along with other training data, and using this information as a feature in the model.\nTemporal Effects Can Arise at Several Timescales We create continuous ML systems when we care about the ways that data changes over time.\nThe most effective way to deal with seasonality effects is to make sure that our model has trained on data from more than one full year in the past—if we are fortunate enough to have it—and that time-of-year information is included as a feature signal in the training data.\nThe subtlety for daily cycles comes in when we consider that most continuous ML systems actually run continuously behind reality, because of the inherent need for delays in pipelines and data streams waiting for training labels—such as click or pur‐ chase behavior that may take some time for a user to decide on—as well as filtering out bad data, updating models, validating checkpoints, and pushing checkpoints to serving and ramping them up fully.\nSecond, our model may have a feedback loop with itself, meaning that if we do not address issues quickly, the stream of input data may also be corrupted and require care to fix as well.\nReal-time crisis response requires first detecting issues quickly, which means that from an organizational standpoint, a good litmus test for determining whether we are ready for continuous ML is to examine the thoroughness and timeliness of our moni‐ toring and alerting strategies.\nSimilarly, when we find that our data stream is corrupted in some way, perhaps by a bad model, or an outage, or a code-level bug somewhere in the system, a useful response can be to stop model training and halt pushing any new model versions out to serving.\nIt makes sense to ensure that there is an easy way for MLOps folks to stop training on any model that is their responsibility.\nAutomated systems are helpful here, but of course need to alert sufficiently so we do not discover that a model has silently stopped training three weeks ago.\nIn continuous ML systems, it is important to have a fallback strategy that can be used in place of our production model that provides acceptable (even if nonoptimal) results.\nFallback strategies are typically less reliable in overall performance than our main model (otherwise, we would not use an ML model in the first place), so fallback strategies are very much intended to be short-term responses that allow for emergency responses to take place in other parts of the system.\nIf we believe that the root cause of our issue is bad data that has caused the model to train itself into a bad state, it makes sense to roll back the model version to a previously known-good version.\nAgain, it is important to keep checkpoints of our trained production model on hand so that we have a set of previous versions to choose from.\nWhen we have bad data in our system, we need to have an easy way to remove it so that our model will not be corrupted by it.\nOtherwise, when we re-enable training to proceed, this data will be encountered by our rolled-back model as it moves forward in time through the training data, and it will be corrupted by the bad data again.\nRemoving bad data is a useful strategy whenever we believe that the data itself is highly unrepresentative of typical data and is unlikely to give the model useful new information, and that the root cause of the\nHowever, if a crisis is detected due to an external world event, sometimes the best response is to just cross our fingers and roll through it, allowing the model to train on the atypical data and then to recover itself as the world event ends.\nIndeed, it is unfortunately true that this world has few days with no political event, major sporting event, or other major newsworthy disaster happening somewhere, and making sure that our model has enough exposure to atypical data like this from different global regions can be an important way to ensure that our model is generally robust.\nTo answer that question, we need to observe our model’s response to similar historical events, which is most easily done when we have trained the model on historical data in sequential temporal order.\nNew Launches Require Staged Ramp-ups and Stable Baselines When we have had a model running as part of our continuous ML system for a period of time, we will eventually want to launch a new version of that model that creates improvements in various ways.\nMaybe we want to use a larger version of the model for improved quality and now have serving capacity to handle it, or perhaps a model developer has created several new features that significantly improve predic‐ tive performance, or maybe we have discovered a more efficient model architecture that reduces serving costs in an important way.\nAs we describe in Chapter 11, offline testing and validation can give useful guidance on whether a new version of a model is likely to perform well in production, but often cannot give a complete picture.\nThis is especially true when our continuous ML model is part of a feedback loop, because the data that we have previously trained on was most likely chosen by a previous model version, and evaluation on offline data is limited to data that has been collected based on actions or recommendations made by that previous model.\nInstead, we will most often use a staged ramp-up, first allowing the model to serve only a fraction of the overall data, and increasing that amount only as we observe good performance over time.\nThis strategy is commonly known as an A/B test: we test out our new model A against the performance of our old model B, in a format that resembles a controlled scientific experiment and helps verify that the new model will show the appropriate perfor‐ mance on our final business metrics (which may be distinct from offline evaluation metrics like accuracy).\nFor A/B experiments comparing continuous ML models, it turns out that A and B may well influence each other when our models are part of a feedback loop.\nFor example, imagine that our new model A does a great job of recommending organic wool products to yarnit.ai users, whereas our previous model B had never done so.\nAn A/B experiment might initially show that the A model is much better in this regard, but then as training data is produced by A that includes many more organic wool recommendations and purchases, the B model (which is also continuously updating) may then also learn that these products are liked by users and begin to recommend them as well, making the two models appear identical over time.\nThis strategy can work well when each model serves the same amount of data, such as 50% of the overall traffic each, but can make for flawed comparisons in other cases.\nIf A is looking bad early on, is that because the model is bad, or because it has only 1% of training data while B has 99%?\nIf we expect our overall launch process to take, say, two weeks, then a reasonable alternative can be to run a copy of our production model that is set to update continuously, but at a two-week delay.\nAn approach that maintains strict independence from A and B but does not have a limited shelf life is a parallel universe model that is allowed to serve a small fraction of overall data and learns only on the data that it serves itself.\nParallel universe models often take time to stabilize after being set up because of the restricted amount of training data and the overall distribution shift.",
      "keywords": [
        "model",
        "data",
        "Continuous",
        "Continuous ML Systems",
        "systems",
        "bad data",
        "production model",
        "training data",
        "time",
        "training",
        "bad",
        "Response",
        "model version",
        "version",
        "crisis"
      ],
      "concepts": [
        "model",
        "data",
        "likely",
        "liked",
        "issues",
        "training",
        "response",
        "responsibility",
        "products",
        "production"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 10,
          "title": "",
          "score": 0.698,
          "base_score": 0.548,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 22,
          "title": "",
          "score": 0.688,
          "base_score": 0.538,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 11,
          "title": "",
          "score": 0.633,
          "base_score": 0.483,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 30,
          "title": "",
          "score": 0.621,
          "base_score": 0.471,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 34,
          "title": "",
          "score": 0.603,
          "base_score": 0.453,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "bad",
          "continuous",
          "continuous ml",
          "bad data",
          "training"
        ],
        "semantic": [],
        "merged": [
          "bad",
          "continuous",
          "continuous ml",
          "bad data",
          "training"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30008608285327354,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.826062+00:00"
      }
    },
    {
      "chapter_number": 32,
      "title": "Segment 32 (pages 272-281)",
      "start_page": 272,
      "end_page": 281,
      "summary": "continuous ML model may be difficult to solve in a complete or permanent way.\nIn this way, a continuous ML system is something that requires regular management.\nContinuous Organizations At this point, it should be clear that an organization that is taking on a continuous ML system is committing to a long-term responsibility.\nStructuring our organizations to be well equipped to handle the responsibilities of managing a con‐ tinuous ML system requires numerous structures to do well.\nDecision making in continuous ML settings often requires some amount of counterfactual reasoning, thinking through the impact of feedback loops, or wrestling with noise and uncertainty that makes effective decision making challenging.\nThe first is to ensure that we have organizational leadership with enough scope and context to effectively weigh the differing needs of, for example, improving model monitoring and crisis response handling with that of improving model accuracy.\nOrganizationally, understanding that a continuous ML system relies on a steady stream of incoming data to determine system behavior makes clear that the data pipeline itself requires serious, dedicated oversight and management.\nAs we’ve noted, the launch process for ML systems and further improvements neces‐ sarily requires a staged ramp-up procedure in the continuous ML setting.\nThe process for assessing the wider impact of various launch stages and ensuring stability before proceeding must be well established and rigorously followed for a continuous ML organization to be effective in the long run.\nFor continuous ML systems, we have an advantage and a disadvantage in handling incidents.\nThe most important prework for an organization running a continuous ML system is to pre-negotiate outage consequences and handling.\nML production engineers shouldn’t be determining the urgency of resolving a particular incident while in the middle of it, and model developers shouldn’t be guessing at the costs and consequences of a given outage while it is ongoing.\nMany organizations tend not to work out these decisions in advance until they experience a few outages, but after that, it becomes quite reasonable to pre-decide what to do in the case of really bad ML problems.\nRethinking Noncontinuous ML Systems We have talked about a range of issues for continuous ML systems in this chapter.\nAll production ML systems should be treated as continuous ML systems.\nAfter all, continuous ML systems are full of complexity and vectors for failure.\nOne reason is that if we apply the standards and best practices from continuous ML systems to all production-grade ML systems, we will definitely be ensuring that our technical infrastructure, model development, and MLOps or crisis response teams are set up to meet challenges as they arise.\nIf a model is trained only once, applying the standards and best practices from continuous ML may be seen as a waste of resources.\nIn our experience, we have seen that every production-level ML model will eventually be retrained or have a new version launched—maybe in a few months, or next year, as new data becomes available or models are developed.\nFrom this standpoint, then, the recommendations for continuous ML systems are applicable to every ML system.\nConclusion In this chapter, we have laid out sets of procedures and practices that can form the foundation of an organizational playbook for the ongoing care and oversight of continuous ML systems.\nThese systems offer a remarkable range of benefits, enabling models that adapt to new data over time and allow for responsive learned systems that interact with users, marketplaces, environments, and the world.\nAny organization managing a continuous ML system needs to think of this as an ongoing high-priority mission, with special care at times of launches or major updates, but also with continual monitoring and contingencies in place to allow fast response to emergent crises.\nAnd finally, we ended the chapter with the idea that all ML systems should likely best be thought of as continuous ML systems, as all models are eventually retrained, and having strong standards in place will benefit any organization in the long run.\nSpecifically, this chapter is about how to respond when bad, urgent things happen to ML systems.\nYou may already be familiar with how teams handle systems going down or otherwise having a problem: this is known as incident man‐ agement, and best practices exist for managing incidents that are common across lots of computer systems.1\nWe cover these generally applicable practices, but our focus is on how to manage outages for ML systems, and in particular how those outages and their management differ from other distributed computing system outages.\nThe main thing to remember is that ML systems have attributes that make resolving their incidents potentially very different from the incidents of non-ML production systems.\nThis means that we can see unintuitive effects where there is a disconnect among the ML system, the world, or the user behavior we are trying to model.\nto understand now is that troubleshooting ML incidents can involve very much more of the organization than standard production incidents do, including finance, supplier and vendor management, PR, legal, and so on.\nML incident resolution is not necessarily something that only engineering does.\nA final serious point we would like to make here at the beginning is that, as with other aspects of ML systems, incident management has serious implications for ethics in general and very commonly for privacy.\nPrivacy and ethics will make an appearance in several parts of the chapter and are addressed directly toward the end because by then we will be in a better place to draw some clear conclusions about how ML ethics principles interact with incident management.\nIndeed, if you’ve worked with incidents for long enough, you’ve probably seen one already, and it probably starts something like this: an engineer becomes aware of a problem; they troubleshoot the problem alone, hoping to figure out the cause; they fail to assess the impact of the problem on end users; and they don’t communicate the state of the problem, either to other members of their team or the rest of the organization.\nWe’ll briefly cover the roles in a typical incident and then will try to understand what differs in handling an ML incident.\nIncident Response Roles Some companies have thousands of engineers working on systems infrastructure, and others might be lucky to have a single person.\nThese roles are invariant, whether or not you are dealing with an ML incident.\nML incidents usually involve a broader range of staff during troubleshooting and resolution, including business/product and management/leadership.\nML outages often impact multiple systems because of their role in integrating with and modifying other parts of your infrastructure.\nMany ML incidents involve impact to quality metrics that themselves already vary over time.\nTo develop a more intuitive and concrete understanding of why these differences show up in this context, let’s consider a few example outages of ML systems.",
      "keywords": [
        "systems",
        "incident",
        "model",
        "incident management",
        "continuous",
        "continuous ML systems",
        "Incident Management System",
        "Incident Response",
        "organization",
        "management",
        "Incident Management Basics",
        "data",
        "roles",
        "response",
        "outage"
      ],
      "concepts": [
        "incidents",
        "models",
        "systems",
        "management",
        "manage",
        "outage",
        "organizational",
        "organizationally",
        "product",
        "responsibility"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 30,
          "title": "",
          "score": 0.861,
          "base_score": 0.711,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 38,
          "title": "",
          "score": 0.832,
          "base_score": 0.682,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "",
          "score": 0.827,
          "base_score": 0.677,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 39,
          "title": "",
          "score": 0.77,
          "base_score": 0.62,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 9,
          "title": "",
          "score": 0.768,
          "base_score": 0.618,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "continuous",
          "continuous ml",
          "incident",
          "systems",
          "ml systems"
        ],
        "semantic": [],
        "merged": [
          "continuous",
          "continuous ml",
          "incident",
          "systems",
          "ml systems"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4642030344889237,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.826124+00:00"
      }
    },
    {
      "chapter_number": 33,
      "title": "Segment 33 (pages 282-291)",
      "start_page": 282,
      "end_page": 291,
      "summary": "The specific configuration of an individual model plus the learning environment, and the structure of the data we will train on\nA specific instance of one configured model trained on one set of data at a point in time\nan ML model that tries to predict how to order those results, given everything we know about the search at the time it’s performed.\nAriel declares an incident and notifies the search model team since it might be a problem with the model.\nAriel also notifies the retail team, just to check that we’re not suddenly making less money from customers who are searching for products (as opposed to browsing for them) and also asks the team to check for recent changes to the website that would change the way results are rendered.\nAriel finds—and the search model team confirms—that no changes have been made to the model configuration in the past two months.\nInstead, one of the search model team members notes something interesting: they use a golden set of queries to test new models daily, and they’ve noticed that in the past three weeks the golden set is producing incredibly consistent results—consistent enough to be suspicious.\nAriel goes to look at the trained model deployed in production.\nSo Ariel looks at the search model training system, which schedules the search model training every night.\nIt has not completed a training run in over three weeks, which would definitely explain why there isn’t a new trained model in production.\nWe have a proximal cause for the outage, but at this point we don’t know the underlying cause, and there’s no obvious simple mitigation: without a new trained model in production, we cannot improve the situation.\nA scheduler loads a set of processes to store the state of the model, and another set of processes to read the last day’s search logs and update the model with the new expressed preferences of the users.\nThe team agrees, Ariel makes the change, the log-feeder jobs stop crashing, and the search training run completes a few hours later.\nThe outage is mitigated as soon as the training run completes and the new trained model is put into production.\nAriel works with the team to double-check that the new trained model loads auto‐ matically into the serving system.\nThen they all wait a few hours to accumulate enough logs to generate the data they need to make sure that the updated trained model is really performing well for customers.\nAriel and the team work on a review of the incident, accumulating some post-outage work they’d like to perform, including the following:\nDetermine our requirements for having a fresh model and then distribute the • available time to the subcomponents of the training process.\nFor example, if we need to get a model updated in production every 48 hours at the most, we might give ourselves 12 hours or so to troubleshoot problems and train a new model, so then we can allocate the remainder of the 36 hours to the log processing, log-feeding, training, evaluation, and copying to serving portions of the pipeline.\nassumptions about the kind of outage we’re experiencing—it could be a systems problem, a model problem, or just a drift in our ability to correctly predict the world.3 Sometimes it’s the world changing faster than we can keep up with— more on this in story 3.\nThe fastest and least risky steps to mitigate the problem, in this case, involved training a new model and successfully deploying it to our production search serving system.\nFor ML systems, especially those that train on lots of data or that produce large models, no such quick resolution might be available.\nWe can see that outages can present as quality problems of models not quite doing what we expect or need them to do.\ntrain a separate model per partner, and extract partner-specific data into isolated repositories, though we can still have a common feature store for shared data.\nSam, a production engineer at YarnIt, works on the partner training system.\nWhile preparing the report, Sam notices that the partner in question has zero recent conversions (sales) recorded in the ML training data but that the account‐ ing system reports that it’s selling products every day.\nThe logs of the partner model training system clearly report that the partner models are successfully training every day, and there are no recent changes to either the binaries that carry out the training or the structure and features of the models themselves.\nLooking at the metrics coming from the models, Sam can see that the predicted value of every product in the CrochetStuff catalog has declined significantly every day for the past two weeks.\nOne of the things they notice is what Sam noticed originally: there are no sales for any partners in the last two weeks in the ML training data.\nThe data extraction team resurrects Sam’s bug from a few days before and starts looking at it.\nSam, who needs to find a fast mitigation for the problem, notes that the team stores older copies of trained models for as long as several months.\nThe team confirms that while the old trained model versions won’t have any information about new products or big changes in consumer behavior, they will have the expected recommendation behavior for all existing products.\nSince the scope of the outage is so significant, the partner team decides it is worth the risk to roll back the models.\nIn consultation with the partner team, Sam rolls back all of the partner trained models to versions that were created two weeks earlier, since that seems to be before the impact of the outage began.\nThe ML engineers do a quick check of aggregate metrics on the old models and confirm that recommendations should be back to where they were two weeks ago.5\nFurther investigation reveals that two weeks ago, in order to facilitate other data analysis projects, the data management team changed the unique partner key, used to identify each partner in its log entries.\nSam requests that a single partner’s data be re-extracted and that a model be trained on the new data in order to quickly verify that the system will work correctly end to end.\nOnce this is done, Sam and the team are able to verify that the newly extracted data contains the expected number of conversions and that the models are now, again, predicting that these products are good recommendations for many customers.\nSam and the data extraction engineers do some quick estimations on how long it will take to re-extract all of the data, and Sam then consults with the ML engineers on how long it will take to retrain all of the models.\nSam requests that all partner data be re-extracted and that all partner models be retrained, preferably from scratch, starting with the beginning of data we have.\nThey monitor the process for three days, and once it is done, verify that the new models are recommending not only the older products but also newer products that didn’t exist two weeks prior.\nAfter careful checking, the new models are deemed to be good by the ML engineers and are put into production.\nSam brings the team together to go over the outage and file some follow-up bugs so that they can avoid this kind of outage in the future and detect it more quickly than they did this time.\nThe data extraction team settles on a strategy of storing the count of merged log lines by partner by day and comparing today’s successes to the trailing average of the last n days.\nIt has prominent differences though, and the best way to see those with some context and nuance is to walk through the partner training outage and look at the ML-salient features that occur during each section:",
      "keywords": [
        "Model",
        "data",
        "search model team",
        "Trained model",
        "partner",
        "team",
        "model training system",
        "search model",
        "system",
        "Sam",
        "search model training",
        "Ariel",
        "time",
        "training",
        "Incident Response"
      ],
      "concepts": [
        "model",
        "sam",
        "data",
        "team",
        "partners",
        "train",
        "logs",
        "logging",
        "log",
        "production"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 10,
          "title": "",
          "score": 0.741,
          "base_score": 0.591,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 21,
          "title": "",
          "score": 0.703,
          "base_score": 0.553,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 22,
          "title": "",
          "score": 0.7,
          "base_score": 0.55,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 34,
          "title": "",
          "score": 0.686,
          "base_score": 0.536,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 30,
          "title": "",
          "score": 0.661,
          "base_score": 0.511,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "partner",
          "sam",
          "ariel",
          "team",
          "search"
        ],
        "semantic": [],
        "merged": [
          "partner",
          "sam",
          "ariel",
          "team",
          "search"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4044302198997834,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.826184+00:00"
      }
    },
    {
      "chapter_number": 34,
      "title": "Segment 34 (pages 292-300)",
      "start_page": 292,
      "end_page": 300,
      "summary": "We have commercial and product staff (the partner team), ML engineers who build the model, data extraction engineers who get the data out of the feature store and logs store and ship it to our partner model training environment, and production engineers like Sam coordinating the whole effort.\nTroubleshooting ML outages really has to start not with the data but with the outside world: what is our model saying, and why is that wrong?\nML outages can only sometimes be mitigated by restoring an older version of the model, because their job is to help computer systems adapt to the world, and there’s no way to restore a snapshot of the world as it used to be.\nStory 3: Recommend You Find New Suppliers We have models for several aspects of our business at YarnIt. The recommendations model in particular has an important signal: purchases.\nGabi is a production engineer who works on the discovery modeling system.\nGabi then asks the payments team to verify its numbers about recommendations added to carts as well as to provide any breakdowns it can.\nAre we showing recommendations as often as we have in the past, and for all the queries and users and products that we did in the past, and in the same proportions across user subpopulations?\nGabi also starts doing the normal production investigation, focusing particular atten‐ tion on what changed in the recommendations stack recently.\nThe results are not promising for finding an obvious culprit: the recommendations models and binaries\nGabi needs to continue troubleshooting but takes time to compose a quick message to the finance and payments teams that asked for help with this issue.\nGabi confirms what is known so far: the recommendations system is running and producing results, and there are no recent changes to be found, but the quality of the results has not been verified.\nNo obvious software, modeling, or data updates correlate with the outage, so Gabi decides that it’s time to dig into the recommendations model itself.\nGabi sends a quick message to Imani, who built the model, asking for help.\nAs Gabi is explaining to Imani what they know so far (fewer products purchased, fewer recommendations purchased per checkout, no system changes to speak of), the note from customer support comes to mind.\nImani thinks this may be worth investigating and asks Gabi to grab enough data to trend some basic metrics on the recommendations system: number of recommendations per browse page, average hourly delta between expected “value” of all recommendations (probability that a customer will purchase a recommended product times the purchase price), and the observed value (total value of recommended products ultimately purchased).\nImani grabs a copy of recent customer queries and product results in order to use them as a repeatable test of the recommendation system.\nThe recommendation system uses the query that a user made, the page that they are on, and their purchase history (if we know it) to make recommendations, so this is the information that Imani will need to query the recommendation model directly.\nSearch queries may contain protected information like user IP addresses, and any collection of search queries contains the additional prob‐ lem that when correlated with each other for a given user, they reveal even more private information.7 Imani definitely should have consulted with privacy and data protection professionals at YarnIt, or better yet, not even had direct, unmonitored access to the queries to make this kind of a mistake.\nImani extracts out about 100,000 queries and page views and sets up a test environ‐ ment where they can be played against the recommendation model.\nAfter a test run through the system, Imani has recommendations for all of the results and has stored a copy of the whole run so that it can be compared to future runs if they need to modify or fix the model itself.\nEven more strangely, though, the gap between the expected value and observed value of the recommendations started narrowing a week ago, but at the same time the number of recommendations shown began falling again, so that now we seem to be showing very few recommendations at all, but those that we do show seem to be relatively accurately valued.\nThe number of expected and shown recommendations as well as their average expected value as they change over the period of the incident\nOn a hunch, Gabi and Imani grab another 100,000 queries and page views from a month ago (before there was any evidence of a problem) as well as a snapshot of a model from every week in the last six weeks.\nImani plans to run the old and new queries against each of the models and see what can be learned.\nGabi pushes for a quick test first: today’s queries versus a month-old model.\nThe old model makes different recommendations than the new model and does seem to make slightly more of them.\nBut the old model still makes many fewer recommendations against today’s queries than it did against the queries a month ago.\nWithout something quite a bit more concrete, Gabi isn’t comfortable that changing the model to an older one will help.\nGabi decides to leave the recommendation system in its current state.\nImani and Gabi finish running the full sweep of old and new queries against older and newer models.\nImani wants to figure out how they have changed in the last month, thinking maybe the problem is with the model’s ability to handle a shift in user behavior rather than something being wrong with the model itself.\nWhile that’s running, Gabi takes the product pages the customers ended up on and assigns each to a large category (yarn, pattern, needles, accessories, gear), and then to subcategories within those, according to the product ontology (built by another team).\nGabi lines up the two pairs of reports and looks for the biggest differences between the user behavior four weeks ago and today.\nImani and Gabi stare at the results, and the problem suddenly seems so obvious.\nImani points out, however, that that doesn’t explain the decrease in recommenda‐ tions, only the change in what the recommendations should be.\nGabi walks through a few queries to the recommendation engine by hand, using a command-line tool built for troubleshooting like this, and notices something.\nImani and Gabi have a solid hypothesis on the weird recommendations.\nIt recommends few products for most customers on most page views since we don’t have much of what most of our customers want right now.\nThere is no obvious change to the recommendations model that can improve the situation, given our supply shortfalls and the way that the weather has impacted our customers’ preferences.",
      "keywords": [
        "Gabi",
        "recommendations",
        "model",
        "recommendations system",
        "system",
        "Imani",
        "data",
        "recommendations model",
        "queries",
        "Incident Response",
        "Time",
        "outage",
        "products",
        "Incident",
        "weeks ago"
      ],
      "concepts": [
        "recommendations",
        "recommending",
        "model",
        "product",
        "production",
        "users",
        "customers",
        "data",
        "teams",
        "incident"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 35,
          "title": "",
          "score": 0.838,
          "base_score": 0.688,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 38,
          "title": "",
          "score": 0.722,
          "base_score": 0.572,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "",
          "score": 0.721,
          "base_score": 0.571,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 11,
          "title": "",
          "score": 0.716,
          "base_score": 0.566,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 33,
          "title": "",
          "score": 0.686,
          "base_score": 0.536,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "gabi",
          "imani",
          "recommendations",
          "queries",
          "month"
        ],
        "semantic": [],
        "merged": [
          "gabi",
          "imani",
          "recommendations",
          "queries",
          "month"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.42571227332703804,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.826235+00:00"
      }
    },
    {
      "chapter_number": 35,
      "title": "Segment 35 (pages 301-308)",
      "start_page": 301,
      "end_page": 308,
      "summary": "At this point, the outage is probably over, since we’ve decided not to change the system or model.\nThe overall stability of the model has been perceived to be of value, but in this case it ended up showing bad recommendations to users for many days and making it harder for the production team to troubleshoot problems with it.\nImani wants to find a way to improve the responsiveness to new situations without making the model overly unstable.\nThis is fundamentally a product and business problem rather than an ML engineering problem: we need to figure out the behavior we want the model to exhibit and the kinds of recom‐ mendations we think we should surface to users under these circumstances.\nThere may be a model that could produce better recommendations under these circumstances (rapid change in demand combined with an inventory problem), but that falls more under the heading of continuous model improvement rather than incident avoidance.\nThe process of investigating this outage includes some of the hallmarks of many ML-centric outage investigations: detailed probing at a particular model (or set of models or modeling infrastructure) coupled with broad investigation of changes in the world around us.\nMoreover, the only way to actually resolve the core outage, and get our revenue back on track, is to change what our users want or to fix the products that we have available to sell.\nOne thing the team didn’t think about, probably in part because it was focused on troubleshooting the model and resolving the ML portion of the outage, was that there may have been other, non-ML ways of mitigating the outage: what if our system showed out-of-stock recommendations and invited customers to be notified when we had those (or similar) products available?\nIn many cases, follow-up from an ML-centric incident evolves into a phase that doesn’t resemble “fix the problem” so much as “improve the performance on the model.” Post-incident follow-up often devolves into longer-term projects, even for non-ML-related systems and outages.\nBut the boundary between a “fix” and “ongoing model improvement” is particularly fuzzy for ML systems.\nrecommendation: first define your model improvement process clearly.\nTrack efforts that are underway and define the metrics you plan to use to guide model quality improvement.\nOnce an incident occurs, take input from the incident to add, update, or reprioritize existing model improvement work.\nThese three stories, however different in detail, demonstrate common patterns for ML incidents in their detection, troubleshooting, mitigation, resolution, and ulti‐ mately post-incident follow-up actions.\nKeeping these in mind, it is useful to take a broader view of what is happening to make these incidents somewhat different from other outages in distributed computing systems.\nML Incident Management Principles While each of these stories is specific, many of their lessons remain useful across various events.\nML outages are often detected first by end users, or at least at the very end of the pipeline, all the way out in serving or integrated into an application.\nThis is partly true because ML model performance (quality) monitoring is difficult.\nML outages are also unclear in impact: it can be hard to see whether a particular condition of an ML system is a significant outage or just a model that is not yet as sophisticated or effective as we would like it to be.\nwork is effective, the model gets better over time as we refine our understanding of how to model the world and improve the data the model uses to do so.\nML outage troubleshooting and resolution involve a broad range of systems and portions of the organization.\nModel Developer or Data Scientist People working at the beginning of the ML system pipeline sometimes don’t like to think about incidents.\nIf ML ends up mattering in an application or organization, however, the data and modeling staff will absolutely be involved in incident manage‐ ment in the end.\nOrganize and version all models and data\nThis is the most important step that data and modeling staff can take to get ready for forthcoming incidents.\nFinally, it is useful to store historical versions of the models in ready-to-serve format.\nThe final bit of preparation that is most useful is to think carefully about model quality and performance metrics.\nUltimately, we want a set of metrics that detect when the model stops working well that are independent of how it is implemented.\nModel developers and data scientists play an important role during incidents: they explain the models as they currently are built.\nFinally, during incident handling and triage, model and data staff may be called upon to do custom data analysis and even to generate variants of the current model to test hypotheses.\nModel and data staff should work to shorten the model quality evaluation loop as a valuable but not dominant priority.\nSoftware Engineer Some, but not all, organizations have software engineers who implement the systems software to make ML work, glue the parts together, and move the data around.\nThe binaries that do the inference in serving, for example, and the model that they are reading from, should be pushed to production independently, with quality evaluations conducted each time.\nWe should have tools to show the versions of the data (reading from the metadata) in every environment, and tools for customer support staff or production engineers (SREs) to read data directly for troubleshooting purposes (with appropriate logging and audit trails to respect privacy and data integrity guarantees).\nBut as our system gets more mature, we will be able to treat this as a few large systems that can be well managed: a data system (feature store), a data pipeline (training), an analytics system (model quality), and a serving system (serving).\nSoftware engineers should work regularly with model developers, with SREs/produc‐ tion engineers, and with customer support in order to understand what is missing and how the software should be improved.",
      "keywords": [
        "model",
        "data",
        "incident",
        "outage",
        "system",
        "Incident Management Principles",
        "model quality",
        "incident response",
        "recommendations",
        "model improvement",
        "Time",
        "Response recommendations model",
        "users",
        "work",
        "engineers"
      ],
      "concepts": [
        "model",
        "recommendations",
        "recommendation",
        "data",
        "product",
        "production",
        "incidents",
        "outage",
        "working",
        "problem"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 34,
          "title": "",
          "score": 0.838,
          "base_score": 0.688,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "",
          "score": 0.826,
          "base_score": 0.676,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 32,
          "title": "",
          "score": 0.747,
          "base_score": 0.597,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 36,
          "title": "",
          "score": 0.72,
          "base_score": 0.57,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 10,
          "title": "",
          "score": 0.717,
          "base_score": 0.567,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "incident",
          "improvement",
          "outage",
          "model improvement",
          "incidents"
        ],
        "semantic": [],
        "merged": [
          "incident",
          "improvement",
          "outage",
          "model improvement",
          "incidents"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.41187781124569306,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.826286+00:00"
      }
    },
    {
      "chapter_number": 36,
      "title": "Segment 36 (pages 309-316)",
      "start_page": 309,
      "end_page": 316,
      "summary": "In ML, these are generally slices (subsets) of the data or model.\nProduction engineering teams should educate themselves about the business that they are in.\nML systems that work make a difference for the organizations that deploy them.\nTo successfully navigate incidents, SREs or production engineers should understand what matters to the business and how ML interacts with that.\nML Incident Management Principles\nAnswering those questions ahead of time prepares a production engineer for the necessary work of prioritizing, troubleshooting, and mitigating ML outages.\nThis is a fairly different practice than production engineers normally employ, but it is required for ML systems outages.\nExtensive experience interacting with customers or business leaders is not a typical requirement for production engineers.\nML production engineers tend to get over that preference quickly.\nML production engineers will collect many ideas about how the incident could have gone better.\nProduct Manager or Business Leader Business and product leaders often think that following and tracking incidents is not their problem, but rather one for the technical teams.\nBusiness and product leaders can report on the real-world impact of ML problems, and can also suggest which causes are most likely and which mitigations are least costly.\nIf ML systems matter, business and product leaders should and will care about them.\nTo the extent possible, business and product leaders should educate themselves about the ML technologies that are being deployed in their organization and products, including, and especially, the need to responsibly use these technologies.\nBusiness and product leaders who take a basic interest in the way ML works will be astoundingly more useful during a serious incident than those who do not.\nML Incident Management Principles\nProduction Engineers and ML Engineering Versus Modeling Given that many ML systems problems present as model quality problems, a mini‐ mum level of ML modeling skill and experience seems required by ML production engineers.\nWithout knowing something about the structure and functioning of the model, it may be difficult for those engineers to effectively and independently troubleshoot problems and evaluate potential solutions.\nThe converse problem also appears: if there is no robust production engineering group, we might well end up with modelers responsible for the production serving system indefinitely.\nSpecifi‐ cally, in smaller organizations, it will be common to have the model developer, system developer, and production engineer be a single person or the same small team.\nThis is somewhat analogous to the model in which the developer of a service is also responsible for the production deployment, reliability, and incident response for that service.\nAs the organization and services get larger, though, the requirement that production engineers be model developers vanishes entirely.\nIn fact, most SREs doing production engineering on ML systems at large employers never or rarely train models on their own.\nML SREs or ML production engineers do need certain ML-related skills and knowl‐ edge to be effective.\nThey need basic familiarity with what ML models are, how they are constructed, and above all, the flavor and structure of the interconnected systems that build them.\nIn this case, the ML production engineer needs to know something about what TensorFlow is and how it works, how the data is updated in the feature store, how the model training processes are scheduled, how they read the data, what a saved model file looks like, how big it is, and how to validate it.\nOn the other side of the same coin, suppose we have settled on a delivery pipeline in which an ML modeling engineer packages their model into a Docker container, annotates a few configuration details in an appropriate config system, and submits the model for deployment as a microservice running in Kubernetes.\nThe ML model‐ ing engineer may need to understand the implications of how the Docker container is built and how large the container is, how the configuration choices will affect the container (particularly if there are config errors), and how to follow the container to its deployment location and do some cursory log checking or system inspection to verify basic health checks.\nThe ML modeling engineer probably does not, however, need to know about low-level Kubernetes choices like pod-disruption budget settings, DNS resolution of the container’s pod, or the network connectivity details between the Docker container registry and Kubernetes.\nespecially in the case where infrastructure components are part of a failure, the ML modeling engineer won’t be well suited to address them and may need to rely on handing off those types of errors to an SRE specialist familiar with that part of the infrastructure.\nThe Ethical On-Call Engineer Manifesto We’ve written a lot in this chapter about how performing incident response is differ‐ ent and more difficult when ML is involved.\nAnother way in which ML incident response is hard is how to handle customer data when you’re on call and actively resolving a problem, a constraint we call privacy-preserving incident management.\nThis is a difficult change for some to make, since today (and decades previous), on- call engineers are accustomed to having prompt and unmediated access to systems, configuration, and data in order to resolve problems.\nTo help us understand why this should be the case, let’s consider the four incident dimensions in which ethical considerations for ML can arise: the impact (severity and type), the cause (or contributing factors), the troubleshooting process itself, and the call to action.\nIn ideal circumstances, no organization would employ ML without undergoing at least a cursory Responsible AI evaluation as part of the design of the system and the model.12 This evaluation would provide clear guidelines for metrics and tools to be used in identifying and mitigating bias that might appear in the model.\nTeams need to adopt the use of Responsible AI practices during the model and system design phase in order to create consistent monitoring of fairness metrics and to provide incident responders a framework to evaluate against.",
      "keywords": [
        "incident",
        "production engineers",
        "model",
        "system",
        "incident management",
        "Incident Response",
        "business leaders",
        "business",
        "Production",
        "data",
        "engineers",
        "leaders",
        "Incident Management Principles",
        "product leaders",
        "organization"
      ],
      "concepts": [
        "incidents",
        "model",
        "production",
        "product",
        "engineering",
        "engineers",
        "problem",
        "teams",
        "data",
        "systems"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "",
          "score": 0.805,
          "base_score": 0.655,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 39,
          "title": "",
          "score": 0.799,
          "base_score": 0.649,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 37,
          "title": "",
          "score": 0.793,
          "base_score": 0.643,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 40,
          "title": "",
          "score": 0.771,
          "base_score": 0.621,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 26,
          "title": "",
          "score": 0.765,
          "base_score": 0.615,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "incident",
          "container",
          "production engineers",
          "leaders",
          "production"
        ],
        "semantic": [],
        "merged": [
          "incident",
          "container",
          "production engineers",
          "leaders",
          "production"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.40415490977406415,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.826333+00:00"
      }
    },
    {
      "chapter_number": 37,
      "title": "Segment 37 (pages 317-325)",
      "start_page": 317,
      "end_page": 325,
      "summary": "ML production engineers who hope to get their organizations ready to manage these kinds of outages would do well to make sure that the engineers understand the business, and the business and product leaders understand the technology.\nCHAPTER 12 How Product and ML Interact\nMany of the product teams and business managers, still anchored in traditional software product development methodologies, find themselves in a new and unfamiliar terri‐ tory: building ML products.\nBuilding your first ML product can be overwhelming.\nIt’s not just a question of getting ML right, difficult enough in itself; rather, the integration of the ML into the rest of the product (and the rest of the business) requires many things that need to work together.\nAmong these, data collection practices and governance, the quality of data, definition of product behavior, UI/UX, and business goals all contribute to the success of an ML-based product or feature.\nDifferent Types of Products One of the important and useful features of ML is that it can be applied to many types of products.\nAs a result of this huge diversity of use cases, organizations focused on integrating ML into their existing or new products face an extremely steep learning curve and numerous choices about their implementation.\nSpecifically, it is not feasible for this chapter to seriously consider each of the many common types of ML-product integrations that exist.\nML models are never “done,” so ML product integration has limited determinism.\nChapter 12: How Product and ML Interact\nML product development phases\nML Product Development Phases\nIf the problem is expected to scale to thousands of users or more, it could be a good use case for ML.\nFor example, if users of a web store are writing product reviews constantly, the algorithm for recommending relevant products needs to adapt in real time and is amenable to an ML solution.\nProduct managers (PMs) need to understand the end user and the long-term business goals to drive value.\nChapter 12: How Product and ML Interact\nWe need to acknowledge that getting it wrong could carry huge costs and that understanding the cost of getting it wrong is a significant part of building an ML product.\nOn the other hand, in the case of a recommender system to suggest similar products on the product details page, for a user with no purchase history, the impact of bad recommendations might be just low conversion rates, and perhaps a vague sense that our web store is not very trustworthy or useful.\nWe must then think about how to improve the model, but the consequences of getting it wrong are not catastrophic in our use case (although they clearly are in many other ML products).\nWe need to use the business purpose of ML in order to think about the desired precision (and recall) of the model.\nAs described in the following list, defining the safety nets and business performance metrics that are relevant to the product we are building is a critical step before introducing ML into the product:\nML Product Development Phases\nAlong with the general ML model performance metrics that we’ve discussed in Chapter 8, it is extremely important for PMs to clearly define the business performance metrics to measure the success of the ML systems in production.\nFor example, the following are a few important business metrics to track for ML-based recommendations on an ecommerce store like yarnit.ai:\nFor example, on the shopping cart page, this metric would help determine the success of the ML model powering the “Frequently bought together” list.\nFor instance, on the product details page, this metric would help determine the success of the ML model powering the “Compare with similar items” list.\nChapter 12: How Product and ML Interact\nMVP Construction and Validation Investing in ML models in order to integrate with our product is likely to be expen‐ sive.\nTo figure out whether the ML integration into our product will work, we need to answer two questions: (1) can we make a model that works (or works well enough), and (2) can we integrate that model into our product in a compelling and useful way?\nSometimes this is referred to as launching a minimal viable product (MVP) with a fixed set of rules or heuristics (without real ML models in place) to prove the point that the feature will really solve the customer needs.\nML Product Development Phases",
      "keywords": [
        "product",
        "Product Development Phases",
        "product development",
        "business",
        "Model",
        "user",
        "Interact product development",
        "Development Phases",
        "development",
        "metrics",
        "list",
        "systems",
        "product teams",
        "business goals",
        "order"
      ],
      "concepts": [
        "product",
        "business",
        "businesses",
        "customer",
        "user",
        "recommending",
        "recommendations",
        "lists",
        "metrics",
        "teams"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 39,
          "title": "",
          "score": 0.96,
          "base_score": 0.81,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 38,
          "title": "",
          "score": 0.883,
          "base_score": 0.733,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 40,
          "title": "",
          "score": 0.847,
          "base_score": 0.697,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 3,
          "title": "",
          "score": 0.808,
          "base_score": 0.658,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 36,
          "title": "",
          "score": 0.793,
          "base_score": 0.643,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "product",
          "ml product",
          "product development",
          "development phases",
          "development"
        ],
        "semantic": [],
        "merged": [
          "product",
          "ml product",
          "product development",
          "development phases",
          "development"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.42409854881767006,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.826392+00:00"
      }
    },
    {
      "chapter_number": 38,
      "title": "Segment 38 (pages 326-333)",
      "start_page": 326,
      "end_page": 333,
      "summary": "Model and Product Development With a clear set of goals and targets developed through previous stages, the next step is to build the models and integrate them with customer-facing features.\nDeployment In the production deployment stage, the ML system is introduced to the infrastruc‐ ture, where it will serve the live customer traffic, and gather feedback data that is fed into the ML training pipelines to improve the models.\nIn the context of an ML system, feedback is also important for a model to learn and become better.\nChapter 12: How Product and ML Interact\nIn particular, one challenge is that the specific initiatives or projects within a large company that ultimately contain the day-to-day work of building an ML model may have many layers of separation from the higher-level business goals.\nSo it’s extremely important for product teams to have a plan for how an ML system will be deployed incrementally.\nSupport and Maintenance ML systems designed to integrate into products are not complete on their first ship‐ ping version.\nOrganizations that are serious about integrating ML into their products need to be serious about continuing the maintenance of those ML models and the infrastructure that produces them.\nAs our product changes, as the needs of our customers change, as our understanding of the business changes, and as the world changes, we will need to keep developing and shipping models.\nWhile the work of a functional ML product\nML Product Development Phases\nIn general, models must come from your learning on your own data.3 But the tools and infrastructure we use to create our models are a different story.\nHowever, in the age of ML, we have more dimensions to this traditional question to consider, includ‐ ing make-or-buy models, data processing infrastructure/tools, and the end-to-end platform that holds it all together.\nVendor-provided solutions may present a significant time- and effort- saving potential, but the key aspects to assess build versus buy for models are dis‐ cussed next.\nIf a use case’s details, data, or processes are fairly specific to the organization, the effort spent adjusting internal systems and processes to match the vendor-provided solution’s specifications may wipe away the benefit from buying something ready-made.\nChapter 12: How Product and ML Interact\nAs a result, ML models and/or application build-or-buy decisions should also take into account the company’s data strategy: sometimes the acquisition of expertise and technology are actual ends in and of themselves.\nOn the other hand, open source solutions, whether for ML or general-purpose software, may be widely adopted, and in many cases have a longer track record than\nOpen source ML solutions are generally great point solutions within a broader context but still require that we do the work necessary to integrate them into our environment and assemble a complete solution.\nEnd-to-End Platforms At the present state of maturity, essentially no ML platforms are available that provide relatively well-integrated solutions starting with data and ending with a model that is available in a serving system.\nModels, data processing infrastructure, human resources, and business value all evolve at dif‐ ferent paces in various ways.\nWhile this has sometimes been a significant concern with distributed computing or data storage platforms, it is particularly difficult in the ML space, where tools and technolo‐ gies may become obsolete in the time it takes to properly implement them.4 If the initiative is meant to be long-term, acquiring a full platform able to accom‐ modate the fast-evolving data technologies may be relevant, while for short-term initiatives, it may be possible to assemble based on today’s components.\nIt is true that algorithms change quickly (certainly yearly), as new approaches are tried and found to be more successful at some types of problems, Some ML platforms, such as TensorFlow and PyTorch most notably, are maintained over longer periods of time (those two since 2015 and 2016, respectively).\nHowever, even in that time, both have changed significantly, and other ML platforms have arisen that may be more effective ways to solve the same problems.\nChapter 12: How Product and ML Interact\nFor example, commercial ML platforms not only allow teams to complete one data project from start to finish one time, but also introduce efficiencies everywhere to scale.\n• Spending less time cleaning data and doing other parts of the data processing flow that don’t provide direct business value\nScoring Approach for Making the Decision Once we are clear on the problem we’re trying to solve and that we need ML specifically to solve the problem, at a minimum we need to consider each of the following factors to evaluate whether building or buying ML is the right choice for the business:\nMaking the Decision Whether building or buying, incorporating ML technologies as a key business tool is a strategic decision that should not be made quickly or without all the components in\nProduct recommendations is an area in which a lot of features can be powered by ML models.\nJust as humans get to know someone and are then able to choose what the best birthday gift for them would be, ML models can leverage data including the product catalog, search queries, viewing history, past purchases, items placed in the shopping cart, products recommended on social media, location, customer segments/buyer personas, and so on.\nChapter 12: How Product and ML Interact\nFor example, when the customer is looking for “baby yarns” on the product page, showing cross-selling recommendations like “Patterns featuring this yarn” and/or “Popular baby clothes featuring this yarn” can not only help increase the average order value but also save a lot of time for customers.",
      "keywords": [
        "data",
        "product",
        "models",
        "Build Versus Buy",
        "data processing",
        "data processing infrastructure",
        "time",
        "Build Versus",
        "Versus Buy",
        "build",
        "user",
        "business",
        "Buy",
        "infrastructure",
        "decision"
      ],
      "concepts": [
        "product",
        "production",
        "data",
        "model",
        "customer",
        "solutions",
        "solution",
        "user",
        "business",
        "based"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 37,
          "title": "",
          "score": 0.883,
          "base_score": 0.733,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 39,
          "title": "",
          "score": 0.855,
          "base_score": 0.705,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 32,
          "title": "",
          "score": 0.832,
          "base_score": 0.682,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 42,
          "title": "",
          "score": 0.807,
          "base_score": 0.657,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 26,
          "title": "",
          "score": 0.789,
          "base_score": 0.639,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "product",
          "buy",
          "product ml",
          "ml platforms",
          "solutions"
        ],
        "semantic": [],
        "merged": [
          "product",
          "buy",
          "product ml",
          "ml platforms",
          "solutions"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4771758873823963,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.826441+00:00"
      }
    },
    {
      "chapter_number": 39,
      "title": "Segment 39 (pages 334-342)",
      "start_page": 334,
      "end_page": 342,
      "summary": "Chapter 12: How Product and ML Interact\nConclusion For ML projects to have a massive business impact, PMs and business owners need to be relentlessly focused on asking the right questions at every stage.\nRather than dive directly into technical details and ML implementations, teams must ensure they understand the business problems and goals as specifically as possible.\nIn either case, a lot of planning and coordination is needed to integrate the ML solutions into customer-facing products.\nClearly defining business goals and measurable product metrics, choosing appropri‐ ate measures of success, and deploying solutions iteratively are each important steps toward building great ML products.\nCHAPTER 13 Integrating ML into Your Organization\nEven trying to enumerate all the areas of the business that have or process data in some way helps to make this point—data is everywhere, and ML follows too.\nFor ML to be successful, leaders need a holistic view of what’s going on, and a way to influence what’s being done with it—at every level.\nThough there are points of relevance to data scientists, ML engineers, SREs, and so on, this chapter is most urgently addressed to those responsible for the health, structure, and outcomes of their organization, on a scale from a team (two or more people) to a business unit or company (hundreds or thousands of people).\nAs per the preceding assumption, since doing ML well involves understanding the principles of how it works, what use it makes of data, what counts as data, and so on, leaders need to know this before the decisions they make are going to be sensible.\nOur main observation here is that by default, leaders are not going to pick up ML-relevant knowledge as part of their regular management activity, and so there needs to be an explicit mechanism for doing so.\nWe currently believe that ML is sufficiently complex, new, and potentially impactful that being aware of the details matters—though we expect this will change as time goes on.1 For the moment, though, leaders need to understand ML basics and\nAt the very least, organizational leaders need to know the business metric being optimized and need to have a means of measuring whether the ML system is optimizing that metric effectively.\nChapter 13: Integrating ML into Your Organization\nML Needs to Know About the Business Our third assumption is that the complexity of the business is a direct input to how ML is conceived and implemented.\nML practitioners need to be more aware of broad business-level concerns and state than the average product developer.\nAn ML developer at YarnIt wants to make a business impact, specifically on the web sales part of the business.\nAs a result, leaders need to be aware of how ML functions in their organization, so they can provide the vital coordination and broad oversight that would otherwise be missing.\nHere is one structural way to think about it: you need to be able to centralize the portions of the ML work where oversight and control are most important, to liberate those portions of the work where domain-specific concerns are most important, and to provide an integration point where these workstreams can meet.\nChapter 13: Integrating ML into Your Organization\nThe Value of ML ML can do more for your business than just make you more money.2 Implementing ML could mean that you could improve civic engagement, raise more funds for disaster relief, or figure out which bridges most urgently need maintenance.3 But for business leaders, it usually means making more money and making customers happier (and happy customers generally lead to more money for the business), or sometimes reducing costs via automation.\nML can model the supply chain constraints and inventory • levels and propose optimal reordering for products to ensure that YarnIt has stock of the appropriate mix of products, given financial, sales, storage, and supply constraints.\nLeaders looking to implement ML often hope for it to improve the thing they already do: make more money, give out more food, pay for more housing.\nML can do these things, but it can also transform the way you think about running the organization as a whole.\nIt is the kind of once-in- a-generation technological change that really can transform the way organizations function—hence the necessity to examine carefully what you think ML can do for you, figuring out what subset of that you want to achieve, and writing all that down before you start.\nML Is Not Magic While most business leaders have some appreciation of the value and potential of ML, they do not necessarily understand the risks equally well.\nHere, we confine ourselves to saying that implementing ML is just like any other change, in that it requires stakeholder management and obtaining buy-in from those affected, but also unlike other changes in that the total set of stakeholders is likely to be much larger.\nChapter 13: Integrating ML into Your Organization",
      "keywords": [
        "business",
        "Organization",
        "products",
        "leaders",
        "model",
        "make",
        "Chapter Assumptions",
        "assumptions",
        "change",
        "YarnIt",
        "business leaders",
        "customers",
        "n’t",
        "organizational leaders",
        "Collaborative Filtering"
      ],
      "concepts": [
        "products",
        "business",
        "businesses",
        "customer",
        "leaders",
        "teams",
        "models",
        "managed",
        "manage",
        "changing"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 37,
          "title": "",
          "score": 0.96,
          "base_score": 0.81,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 38,
          "title": "",
          "score": 0.855,
          "base_score": 0.705,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 3,
          "title": "",
          "score": 0.851,
          "base_score": 0.701,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 40,
          "title": "",
          "score": 0.84,
          "base_score": 0.69,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 36,
          "title": "",
          "score": 0.799,
          "base_score": 0.649,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "leaders",
          "business",
          "leaders need",
          "chapter 13",
          "13 integrating"
        ],
        "semantic": [],
        "merged": [
          "leaders",
          "business",
          "leaders need",
          "chapter 13",
          "13 integrating"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4294005758036052,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.826489+00:00"
      }
    },
    {
      "chapter_number": 40,
      "title": "Segment 40 (pages 343-351)",
      "start_page": 343,
      "end_page": 351,
      "summary": "Ultimately, for most practical concerns, implementing ML requires serious stake‐ holder management and a large concerted effort to shift mental (way-of-thinking) models.\nto implement ML on their own in any serious way.\nSiloed Teams Don’t Solve All Problems Another common risk is for ML teams and projects to be treated equivalently to the way other new kinds of work are treated, and a common instinct is to start a new siloed team to do that work, leading to its separation in the organization.\nBut more importantly, because of the broad scope of impact that ML projects can have, success‐ fully deploying ML requires organizational change to support structure, processes, and the people needed to keep it reliable.\nImplementation Models Having discussed some risks involved in introducing ML to an organization, let’s focus on the nuts and bolts—how to actually get it done.\nA small implementation project probably starts with applying ML to something that is integral to your organization’s success.\nChapter 13: Integrating ML into Your Organization\nThere may also be ML-specific product managers who guide what we implement as well.\nChapter 13: Integrating ML into Your Organization\nThis affects all employers, but the most prestigious of ML companies (generally large tech organizations) continue to hire most of the new graduates and experienced staff.\nBut the staff needed to manage the data, integrate ML into the product, and maintain the models\nNow that we’ve considered some of the concrete challenges that organizations face adapting ML specifically, let’s take a step back and consider the problem from the perspective of traditional organizational design.\nOrganizational Design and Incentives Making an organization function well, given what it is supposed to do—often called organizational design—is a difficult art that involves a mixture of strategy, structure, and process.\nTalk of strategy and process and structure can be difficult to map onto the main actual tasks: hiring the right people and getting ML added to your application.\nUltimately, though, the main lesson is that thinking about the way your organization currently works, and how that will change, hugely improves your chances of doing ML successfully.\nGalbraith, and apply it specifically to the challenge of implementing ML in an organization (Figure 13-1).\nChapter 13: Integrating ML into Your Organization\nIn this model, strategy, structure, processes, rewards, and people are all design poli‐ cies or choices that can be set by management and that influence the behavior of the employees in the organization.\nThis model is useful because it goes beyond the reporting structure or organization chart, where most leaders tend to start and end their change efforts.\nGalbraith points out that “most design efforts invest far too much time drawing the organization chart and far too little on processes and rewards.” This model allows you to take that observation and then think about whether all of the interconnected aspects are affected or can be changed to support the requirements better.\nLet’s review each of these in the context of an organization trying to implement ML.\nlearning in all aspects of the product” might mean the organization funding new and innovative ways of using ML everywhere, with more tolerance for lower-quality results to start with.\nOne way to think about choices for organizational structure, and the one that Gal‐ braith identifies, is that it includes functional, product, market, geographic, and process structures:\nThis structure organizes the company around a specific function or specialty (for example, centralizing ML implementation in a single team).\nThis is probably not how we would structure an ML implementation.\nThis may be a good model for ML teams that work across various product lines but need to create standards and processes for the organization.\nLeaders will generally have a mental model for the way the organization works and the approach they should use to effectuate change.\nChapter 13: Integrating ML into Your Organization\nmental model, a leader will tend to centralize ML functions around a specific senior leader—and this has obvious drawbacks if the right leader is not to be found, or centralization doesn’t fit well with (say) the existing engineering culture.\nSimilarly, a siloed ML function might work better for senior leaders to maintain control of, but would inhibit progress of ML on other engineering teams.\nUltimately, leaders will probably need to shift their mental model of the way things work, depending on the chosen ML strategy.\nProcesses Processes constrain the flow of information and decisions through an organization, and hence are critical to the way ML will work.\nOne potential way to begin adding ML to your organization is to treat the introduc‐ tion as a vertical process, with decisions made centrally but implemented throughout the organization.\nFor example, if we fund an ML training and serving team to add a new ML feature to our application, do we also fund teams to curate all of the data, or to handle model quality measurement over time or fairness?\nOnce the organization has several ML projects implemented, centralizing the infra‐ structure from those projects to fulfill specific workflows may add robustness and reliability.\nAt that point, we could centralize serving for some of those models, think about building a central feature store, and so start establishing common aspects of the ML organizational infrastructure regardless of the model team.",
      "keywords": [
        "organization",
        "models",
        "structure",
        "Organization mental model",
        "teams",
        "mental models",
        "product",
        "organizational design",
        "organizational",
        "Implementation Models",
        "work",
        "processes",
        "start",
        "Implementation",
        "mental"
      ],
      "concepts": [
        "models",
        "organization",
        "organizes",
        "organized",
        "teams",
        "structure",
        "product",
        "production",
        "processes",
        "process"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 41,
          "title": "",
          "score": 0.848,
          "base_score": 0.698,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 37,
          "title": "",
          "score": 0.847,
          "base_score": 0.697,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 39,
          "title": "",
          "score": 0.84,
          "base_score": 0.69,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 36,
          "title": "",
          "score": 0.771,
          "base_score": 0.621,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 38,
          "title": "",
          "score": 0.76,
          "base_score": 0.61,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "organization",
          "mental",
          "ml organization",
          "structure",
          "organizational"
        ],
        "semantic": [],
        "merged": [
          "organization",
          "mental",
          "ml organization",
          "structure",
          "organizational"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.35313869001537534,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.826537+00:00"
      }
    },
    {
      "chapter_number": 41,
      "title": "Segment 41 (pages 352-359)",
      "start_page": 352,
      "end_page": 359,
      "summary": "One thing that should be considered is rewarding staff throughout the organization for learning more about ML.\nIf the sales staff, accounting staff, buyers, and product managers all have a basic education in ML, the organization may well be much more effective in the long run.\nGiven how rare ML education and skills are at present, most organizations should consider hiring staff who can learn on the job rather than only those already quali‐ fied.\nFinally, the organization will need people who can work through the ambiguity of problems caused by ML without stopping at a root cause of “the ML model said so.” That’s a fine place to start, but people will need to be able to think creatively about the way ML models are built, how changes in the world and in the data impact them, as well as how those models impact the rest of their organization.\nOrganizational leaders will face new challenges and changes in their organization as a result of adopting ML.\nFor each of these scenarios, we will describe how the organizational leader has chosen to integrate ML into the organization and the impact of that choice.\nScenario 1: A New Centralized ML Team Let’s say that YarnIt decides to incorporate ML into its stack by hiring a single ML expert who develops a model to produce shopping recommendations.\nThe YarnIt CEO decides to hire a new VP to build and run the ML Center of Excellence team as a new, centralized capability for the organization.\nThe centralization also creates a significant nexus of influence: the leaders of the ML organization have more standing to advocate for their priorities across all of YarnIt.\nAs the group grows and the projects diversify, more of YarnIt will need to interact with the ML organization.\nThe ML team cannot be too distant from the rest of the business, as it will take the team longer to see opportunity, to deeply understand the raw data, and to build good models.\nEven worse, placing these two functions (ML and product development) completely separately in the organizational chart might encourage the teams to be competitive instead of cooperative.\nFinally, a centralized organization may not be usefully responsive to the needs of the business units requesting help to add ML to their products.\nWhen it comes to productionizing ML, the business units likely will not understand the reliability needs of the ML teams and not understand why reliability processes are being followed (thus slowing delivery).\nWhile these pitfalls exist for a solely centralized ML team, the organization can always evolve.\nAnother possible evolution is that the centralized team educates and enables others to increase ML literacy within the rest of the organization.\nThese reviews, presented by the ML team on a regular basis, should ensure approval of the current modeling results as well as an understanding by business leaders of the way the system is adapting to the business.\nOne issue that can crop up in a centralized ML team is that all the changes become dependent on one another and may be held up by other changes.\nWhile this is easier in a centralized ML model team, introduce processes so that peo‐ ple from the various business units can also evaluate the changes.\nFor the centralized ML team, this may be the product/business team that requested a change or feature, or the support team that may be affected by the changes.\nRewards To ensure a successful implementation of the ML program, we need to reward interaction between business and model builders.\nScenario 1: A New Centralized ML Team\nProduct leaders need to understand that tuning the ML model for the organizational objectives may take some experimentation and that negative impacts will almost certainly occur along the way.\nBigger Isn’t Always Better The YarnIt ML team begins work on a project whose success definition is to create bigger carts (shopping carts containing more products worth more money).\nSales can generate an acceptable target for cart abandonment for the ML team to include in its model optimization, the web UI team can think of ways to make it easy to check out parts of a cart rather than a whole cart, and the product team can think about remarketing to users who abandoned carts, asking whether they might want to purchase just some of the products.\nYarnIt, and all organizations implementing ML, need to hire for a mindset of nuance in order to be successful.\n• Hire a new leader with ML modeling and production skills.\n• Plan compensation by successful implementation and compensate both ML staff and product area staff.\nScenario 2: Decentralized ML Infrastructure and Expertise YarnIt might decide to invest in several experts across the organization, rather than a single senior leader.",
      "keywords": [
        "Team",
        "organization",
        "model",
        "centralized team",
        "People",
        "Centralized",
        "organizational",
        "product teams",
        "business",
        "staff",
        "Implementation",
        "centralized organization",
        "Hire",
        "product",
        "YarnIt"
      ],
      "concepts": [
        "teams",
        "model",
        "organizational",
        "organizations",
        "organ",
        "changes",
        "changing",
        "product",
        "production",
        "carts"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 40,
          "title": "",
          "score": 0.848,
          "base_score": 0.698,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 39,
          "title": "",
          "score": 0.797,
          "base_score": 0.647,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 3,
          "title": "",
          "score": 0.788,
          "base_score": 0.638,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 42,
          "title": "",
          "score": 0.786,
          "base_score": 0.636,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 4,
          "title": "",
          "score": 0.778,
          "base_score": 0.628,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "centralized",
          "team",
          "ml team",
          "organization",
          "centralized ml"
        ],
        "semantic": [],
        "merged": [
          "centralized",
          "team",
          "ml team",
          "organization",
          "centralized ml"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4175345728160948,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.826588+00:00"
      }
    },
    {
      "chapter_number": 42,
      "title": "Segment 42 (pages 360-367)",
      "start_page": 360,
      "end_page": 367,
      "summary": "Without a central place for ML expertise, especially in management, developing a deeper understanding of what YarnIt needs to do to be successful at ML will be harder.\nIt will be hard to understand when the ML team is advocating for something it really needs (model-specific quality-tracking tools like TensorBoard), as opposed to something that might be nice to have but may not be required (GPUs for some model types and sizes or cloud training services that offer huge scale but also large costs).\nYarnIt should create a template for these reports, possibly generated by collaboration among some of the first groups to start using ML, and a standard schedule for reviews by a small group with representatives beyond just the organization implementing ML.\nML model developers should meet weekly with production engineering staff and stakeholders from the product development group to review any changes or unexpected effects of the ML deployments.\nScenario 2: Decentralized ML Infrastructure and Expertise\n— — Hire ML engineering staff to build models directly with the product teams.\n— — Hire ML staff or shift production engineering staff to run the infrastructure.\n• Run weekly triage or ML production meetings to review changes.\nScenario 3: Hybrid with Centralized Infrastructure/ Decentralized Modeling YarnIt started its implementation via the centralized models, but as the organization matures and ML adoption spreads throughout the company, the company decides to revisit that model and consider a hybrid structure.\nIn this case, the organization will maintain some centralized infrastructure teams and some ML model consulting teams in the central organization, but individual business units are free to hire and develop their own ML modeling experts as well.\nBut decentralizing at least some of the ML expertise will increase the speed of adoption and improve alignment between the ML models and the business needs.\nMeanwhile, the centralized infrastructure might create friction for the decentralized modeling teams.\n— — ML team(s) findings documentation and review\nRewards In the hybrid scenario, YarnIt senior management should reward business units for utilizing the centralized infrastructure, to prevent them from developing their own, duplicative infrastructure.\nCentralized infrastructure teams should be rewarded for meeting the needs of the other business units.\nCentral infrastructure teams should have a plan to identify key technology developed in the business units and extend its use to the rest of the company.\nAnd from a career development perspective, ML modelers from the business units should be able to rotate onto the central infrastructure team for a period of time to understand the services available and their constraints, as well as to provide an end-user perspective to those teams.\nThe ML teams embedded with the business need to have a mindset that cooperation is best, so they should be looking for opportunities to collaborate across divisions.\nHire a centralized team (leader) with ML infrastructure and production skills: •\n— — Hire ML production engineering staff to run the infrastructure.\n— — Hire ML engineering staff to build models directly with the product teams.\nSelect processes that will aid in cross-organizational collaboration such as cross- • team ML findings reviews.\nSome teams, such as the production engineering or software engineering teams, will not require significant ML skills to start being effective.\nWe also need to build ML skills among business leaders.\nCentralized ML infrastructure and expertise\nTeams need to be the champions of ML quality, fairness, ethics, and privacy across the company.\nTeams across all product areas need to gain expertise on ML quality, fairness, ethics, and privacy.\nNeeds a lot of documentation around best practices, knowledge, evaluation, and launch criteria to maintain consistency, or a deliberate decision not to maintain any outside of local team scope (which is problematic for ML).\nmeeting business goals, individual/ team performance needs to be measured based on the effectiveness of cross-functional collaboration.\nOn top of overall quality and meeting business goals, individual/ team performance needs to be measured based on consistency, published quality standards, and operating internal ML communities.\nEstablish mechanisms to compensate both ML and product teams together for successful AI feature launches.\nHybrid with centralized infrastructure and decentralized modeling Centralize ML infrastructure and modeling for common/core business use cases but encourage individual model development for specific needs.\nNeeds cross-functional collaboration and decent documentation between infrastructure and individual product teams on a project/ program basis.\nOn top of overall quality and meeting business goals, individual/team performance needs to be measured based on reusability, evolution of common infrastructure, and speed of execution.",
      "keywords": [
        "infrastructure",
        "Centralized Infrastructure",
        "teams",
        "business",
        "model",
        "centralized infrastructure teams",
        "Implementation",
        "business units",
        "infrastructure teams",
        "Centralized",
        "organization",
        "review",
        "quality",
        "Org Implementation",
        "decentralized"
      ],
      "concepts": [
        "team",
        "model",
        "developing",
        "needed",
        "infrastructure",
        "organizational",
        "process",
        "processes",
        "processing",
        "business"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 38,
          "title": "",
          "score": 0.807,
          "base_score": 0.657,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 41,
          "title": "",
          "score": 0.786,
          "base_score": 0.636,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 26,
          "title": "",
          "score": 0.747,
          "base_score": 0.597,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 39,
          "title": "",
          "score": 0.746,
          "base_score": 0.596,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 32,
          "title": "",
          "score": 0.738,
          "base_score": 0.588,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "centralized",
          "infrastructure",
          "centralized infrastructure",
          "teams",
          "infrastructure teams"
        ],
        "semantic": [],
        "merged": [
          "centralized",
          "infrastructure",
          "centralized infrastructure",
          "teams",
          "infrastructure teams"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.43827123384249506,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.826651+00:00"
      }
    },
    {
      "chapter_number": 43,
      "title": "Segment 43 (pages 368-375)",
      "start_page": 368,
      "end_page": 375,
      "summary": "Therefore, we continually aspire to improve the ASR models in our ML pipelines.\nIn this case study, we discuss a couple of challenges we’ve come across and the solutions we’ve implemented as we worked toward deploying a model for multiple dialects while respecting user privacy.\nAt Dialpad, we value user privacy, and to power various AI features, we need massive amounts of data for model training.\nThe ASR model did well with North American dialects because we had been feeding it North Ameri‐ can speech, so we could also improve this existing model by adding undersampled data, building a model agnostic of dialects.\nWe manually transcribed this underrepresented data and trained a new model with this dataset plus the original training dataset.\nWithin a few rounds of model tuning and evaluations, the ASR models started performing better on the underrepresented dialects test set that we manually curated, without any changes to the training techniques or model architecture.\nThese substantial privacy wins, however, require equally substantial cleverness across the entire ASR system in terms of model testing and experimentation, and especially so for the Dialects ML pipeline that consists of multiple steps: collection of audio, transcription, data preparation, experimentation, and final productionization.\nThat means training data and test sets are not constant, making it difficult to reproduce experimental results, sometimes leading to delays in training the models and launching the desired improvements for customers.\nLate in the process of rolling out the new Dialects model, we saw that it performed well on multiple test sets, but performed significantly worse with one single test set across multiple internal trials (compared to the model in production, released six months earlier).\nWe used multiple methods, including training the new Dialects model from scratch and checking data partitioning (after a previous misadventure inadvertently mispartition‐ ing between training data and test data).\nWe also wanted to reproduce the results from the production model by using the same process to train a model, but 11 months later, the data subject to retention policies had begun expiring, and we didn’t have the exact training dataset anymore.\n1. Accommodating Privacy and Data Retention Policies in ML Pipelines\nUltimately, the key insight to resolving the discrepancy was that the previous model that had performed well on the test set was actually in use during the time the data from production was taken to make the test set.\nWhile difficult, this challenge suggests a way in which privacy-promoting, data-minimizing techniques could secure much more robust access to ML training data.\nTakeaways Integrating with privacy and data retention policies undoubtedly introduces chal‐ lenges in ML pipelines, especially those powering the primary use cases of a customer-facing product/service.\nIn our use case, working toward a more inclusive ASR model for Dialects, we first learned that even a little diversity in our training data makes the model more robust.\nThe system in question included a continuous ML model that helped predict the likelihood of clicks on certain kinds of results in a search engine setting, continually updating on new data as it came in.\nThis data with many more queries but no additional clicks was staged for retraining of the continuous ML model.\nAt this point, the continuous ML model was now happily training on all the corrup‐ ted data.\nBecause the corrupted data included a lot of traffic with no associated clicks, the model was getting a signal that the overall click-through rate in the world was now about half of what it had been just a few hours before.\nBy this time, the ops folks had pushed the stop button on the continuous ML model training, and all model training was stopped.\nThe most up-to-date version of the model was therefore one that has been impacted by the corrupted data.\nThe behavior that an app update caused a duplicate query was not widely known, and those who knew about it on the app side did not make the connection to the way that it could impact training data in a continuous ML model.\nThe ML model eventually saw clean data with the appropriate click-through rates.\nI am an ML engineer at Landing AI and wrote this case study to show some data-centric techniques we used to develop deep learning models for visual inspection tasks.\nThe customers had been working on this problem for almost 10 years, and the best performance their model was able to achieve was only 80% accuracy, which was not sufficient for the client’s needs.",
      "keywords": [
        "North American dialects",
        "data",
        "model",
        "training data",
        "dialects",
        "Dialects model",
        "North American",
        "American dialects",
        "training",
        "model training",
        "ASR models",
        "Data Retention Policies",
        "ASR",
        "Data Retention",
        "data team"
      ],
      "concepts": [
        "recognition",
        "kingdom",
        "years",
        "ago",
        "rely",
        "industries",
        "batches",
        "small",
        "phrasal",
        "united"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 45,
          "title": "",
          "score": 0.701,
          "base_score": 0.551,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 44,
          "title": "",
          "score": 0.629,
          "base_score": 0.479,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 18,
          "title": "",
          "score": 0.423,
          "base_score": 0.423,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 19,
          "title": "",
          "score": 0.397,
          "base_score": 0.397,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 9,
          "title": "",
          "score": 0.396,
          "base_score": 0.396,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "dialects",
          "asr",
          "training",
          "retention",
          "american"
        ],
        "semantic": [],
        "merged": [
          "dialects",
          "asr",
          "training",
          "retention",
          "american"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.28297368760981706,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.826690+00:00"
      }
    },
    {
      "chapter_number": 44,
      "title": "Segment 44 (pages 376-385)",
      "start_page": 376,
      "end_page": 385,
      "summary": "Timeline for data labeling, label review, and model training\nIf we allowed this label to be added to our training set, it would mislead the model when calculating the losses, and we should avoid that.\nAfter each time we trained a model, I spent most of my time reviewing falsely predicted examples and identified root causes of errors.\nIn 2019, the large language model BERT achieved state-of-the-art NLP performance.1 We planned to leverage it to provide more accurate NLP capabilities, including punctuation restoration and date, time, and currency detection.\nTo reduce cloud cost, however, our real-time production environment has very limited resources assigned to it (GPU is not an option, and we have one CPU at most for many models).\nProblem and Resolution Our team needed to perform local profiling in order to benchmark various NLP models, which ran model inference over a large number of randomly sampled utter‐ ances and calculated average inference time per utterance.\nOnce the average inference speed met a fixed threshold, the packaged model would be handed over to our data engineering (DE) team, which would then do canary deployment in our Google Kubernetes Engine (GKE) cluster, and monitor a dash‐ board of various real-time metrics (Figure 15-7) with an ability to drill down into specific metrics (Figure 15-8).\nFor a large language model based on BERT, often DE teams discovered that the latency or queue time bumped up significantly and they had to roll back the deployment.\nOur QA team made test calls to test various call features, and applied scientists leveraged this environment to ensure that the model ran properly with the product UI.\nThe DE team proposed a comprehensive and self-serve load-test tool to help applied scientists benchmark model inference in the staging environment.\n• Load-test data should contain trigger phrases for the model.\nLoad-test data should trigger model inference and not get short-circuited by • optimizations/caches that would lead to misleadingly low runtime latencies.\n• Text-based (model specific) load test\nmodel).\nWhen our testing methods started failing on the more resource-intensive BERT model, we reached into our staging environment to give our scientists a more representative environment to test against and made it self-serve so they could iterate rapidly.\nBoth teams are now relieved as applied scientists are able to obtain close-to-production estimates on model inference with great confidence.\nThey do so in part by using models to predict the probability that a given ad will be clicked.\nWhen there are opportunities to show ads, an auction is conducted, and as part of this auction a server uses a model to predict the probability that certain ads will be clicked.\nThe row in the database corresponds to an ad being shown to the user, with columns associated with the features used for model training.\nThe model was trained by looking at the rows in this database as examples, and using the click bit as the label.\nThe rows in the data were the raw inputs to the model and the label for each event recording either “resulted in a click” or “was not clicked.” If we were to never update the models, they would work well for some\nTo improve the overall behavior, we automated the retraining and deployment of the click prediction model.\nNewly trained models would be sent to this validation system, which generated inferences on recent events with the test_set bit.\nModels would be pushed to production only if the new models performed better than the old model over the recent test set events.\nThe model was acting as if clicks were far more rare, but the data in our database didn’t reflect that.\nThe data science team started by looking at the validation system: this was supposed to keep us from pushing out models that performed worse than the versions we were replacing.\nThe logs from Sunday’s validation run indicated we processed the test events as expected, and that the loss statistic was lower for the new model than our old model.\nWith a hunch, someone decided to rerun the validation system with the same pair of models across the test set.\nWhen the production engineer and data scientist teams conferred with each other and shared their findings, they realized the model was underpredicting because of a failure of the infrastructure responsible for processing the raw click logs and distrib‐ uting the clean click feed to consumers.\nThe clean clicks arrived to the ML training system late—not until early Monday morning, after the model had last trained.\nWithout any evidence to the contrary, the model believed that every ad shown in this period didn’t result in a click.\nThe model that we trained concluded that the ads were awful and no one would click on any of them, which was accurate given the data that we had when we trained the model.\nWhen the click-processing feed caught up, the validation data was relabeled so that ads that resulted in clicks were labeled that way.\nOur models built using supervised learning techniques predict labels in our training set, and it’s of critical importance that our labels reflect reality.",
      "keywords": [
        "model",
        "Load Test",
        "Click",
        "model inference",
        "Staging Load Test",
        "test set",
        "NLP models",
        "data",
        "NLP models systems",
        "NLP",
        "system",
        "production",
        "Case Studies",
        "defect",
        "Click Prediction"
      ],
      "concepts": [
        "team",
        "click",
        "nlp",
        "time",
        "data",
        "added",
        "adding",
        "ads",
        "trained",
        "figured"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 45,
          "title": "",
          "score": 0.71,
          "base_score": 0.56,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 43,
          "title": "",
          "score": 0.629,
          "base_score": 0.479,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 24,
          "title": "",
          "score": 0.496,
          "base_score": 0.496,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 47,
          "title": "",
          "score": 0.47,
          "base_score": 0.47,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 23,
          "title": "",
          "score": 0.459,
          "base_score": 0.459,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "test",
          "load test",
          "click",
          "nlp",
          "inference"
        ],
        "semantic": [],
        "merged": [
          "test",
          "load test",
          "click",
          "nlp",
          "inference"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3571498680952784,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.826736+00:00"
      }
    },
    {
      "chapter_number": 45,
      "title": "Segment 45 (pages 386-394)",
      "start_page": 386,
      "end_page": 394,
      "summary": "Our follow-ups included establishing an availability target for the click log processing systems, expanding our validation system to check that the test set’s positive ratio wasn’t suspiciously low or high, and establishing a process for the click log team to communicate outages and pause training if serious problems with model health occurred.\nThe output is then fed into our language-specific NLP models like NER and sentiment analysis.\nMultiple speech-recognition models may be used, depending on the user’s location and/or the product line they are using within Dialpad.\nAdditionally, for the NLP, several task-specific models run in parallel to perform tasks such as sentiment analysis, question detection, and action-item identification.\nthis in mind, the simplified flowchart can be extended to highlight the diversity of models in Dialpad’s production ML pipeline.\nProblem and Resolution Figure 15-11 highlights some of the ML dependencies that exist for NLP task-specific models with regards to the upstream speech-recognition models.\nNLP model perfor‐ mance is sensitive to ASR model output artifacts.\nWhile most NLP models are not overly sensitive to minor changes to ASR model outputs, over time the data changes significantly enough, and NLP models experience a degradation in performance due to regression and data drift.\nA few common updates to ASR that result in a change in input data distribution for NLP models are as follows:\nSome of the ML dependencies that exist for NLP task-specific models\nTo combat this phenomenon, the DE team at Dialpad, in collaboration with the data science team, built an offline testing pipeline that could measure NLP model performance for a given ASR model.\n• Ensure that monitoring of NLP model performance would happen automatically whenever newer ASR models are released.\n• Allow for ad hoc evaluation by data science teams when they wish to evaluate a model prerelease.\nBe a scalable system so that multiple evaluations could occur simultaneously, and • also not be bottlenecked as we increase either the dataset sizes or the number of models being tested.\nThe pipelines in KFP will build the correct infrastructure for evaluation by • selecting the correct model deployment artifacts, given the evaluation criteria.\nDatasets for every task-specific NLP model would be versioned so as to track • changes in evaluation data.\n• Metrics will be collected for every combination of ASR model version, NLP model version, and dataset version.\nThe input to the testing pipeline is raw audio recordings of conversations, since • the idea was to capture whether an ASR model has changed in such a way that it alters the output enough that the downstream NLP model has varied performance.\nAnd within KFP, a pipeline would simulate evaluation for a single combination of ASR model version, NLP model version, and dataset version.\nOnce the pipelines were built on KFP, the next part of the project was to automat‐ ically perform regression tests whenever dependencies changed for NLP models.\nLuckily, at Dialpad we have mature CI/CD workflows managed by engineering, and they were updated to trigger KFP pipelines whenever ASR models were updated in the transcription service.\nThe CI/CD workflow would send a signal to KFP with information about the ASR models, NLP models, etc., and the evaluation would then commence on KFP.\nOnce operational, this process captures performance evaluation data for all NLP task-specific models that have testing data available on the platform.\nFor example, the F1-score of the NLP sentiment analysis model degraded by ~25% over the course of a year, as shown in Figure 15-14; the graph highlights the absolute difference from a baseline.\nAnother tangential benefit of this process is that it allows for ad hoc evaluation of NLP models against different ASR models prior to production release.\nFor instance, it is possible to measure the accuracy of a sentiment analysis model, prior to release, against an ASR model trained on new English dialects, such as Australian or New Zealand English.\nF1-score of the NLP sentiment analysis model\nTakeaways This ML regression-testing platform developed at Dialpad has provided data scien‐ tists and engineers with much improved visibility on the impact of new model relea‐ ses on all dependent components in our production stack.\nexamples, 227 labels, 227 correctness of data, 26 counterfactual testing, 97 CPUs, 168 creation phase of data management, 23-25 crime-prediction algorithms, 109 cultural appropriateness, Responsible AI, 131",
      "keywords": [
        "NLP models",
        "ASR model",
        "NLP",
        "model",
        "NLP task-specific models",
        "ASR",
        "NLP model version",
        "NLP model performance",
        "ASR model output",
        "data",
        "ASR model version",
        "KFP",
        "Dialpad",
        "click log processing",
        "NLP sentiment analysis"
      ],
      "concepts": [
        "model",
        "data",
        "tested",
        "product",
        "production",
        "nlp",
        "training",
        "changes",
        "changed",
        "metrics"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 44,
          "title": "",
          "score": 0.71,
          "base_score": 0.56,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 43,
          "title": "",
          "score": 0.701,
          "base_score": 0.551,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 13,
          "title": "",
          "score": 0.485,
          "base_score": 0.485,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 5,
          "title": "",
          "score": 0.462,
          "base_score": 0.462,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 47,
          "title": "",
          "score": 0.444,
          "base_score": 0.444,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "nlp",
          "asr",
          "nlp model",
          "asr model",
          "nlp models"
        ],
        "semantic": [],
        "merged": [
          "nlp",
          "asr",
          "nlp model",
          "asr model",
          "nlp models"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.36138759519949093,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.826783+00:00"
      }
    },
    {
      "chapter_number": 46,
      "title": "Segment 46 (pages 395-403)",
      "start_page": 395,
      "end_page": 403,
      "summary": "pseudonymized data, 18 raw data, 18 as liability, 16-20 deletion, 20 distribution, 21 encryption, 20 filtering, continuous ML systems, 227 golden sets, 95-96 input, monitoring, 200-200 numerical, 212 pseudonymization, 18 pseudonymized data, 18 raw data, 18 rewriting, 20 semi-structured, 24 sparse, 34 structured, 24 unstructured, 24\ndata access guidelines, 126 data access, privacy preservation, 125 data analysis, 3-3 data availability data availability, training systems, 148-149 data breach notification laws, 126 Data Cards, 25 data cleaning, responsible AI, 132 data collection, 3\nbusiness purpose and, 31 continuous ML systems, 228 data integrity, 36\npolicy and compliance, 40-41 privacy, 37-39 security, 37-37 data reliability, 33 availability, 36 consistency, 34-35 durability, 33 performance, 36 version control, 35 model structure and, 32 phases of data, 22\ndata separation, privacy preservation, 125 data types, monitoring, 220 databases, case study, 353-356 datasets\ndecoration, monitoring systems, 190 deep learning models, 45 vulnerabilities, 52-54\ndeployment, Responsible AI, 134 development versus model serving, 194-195 DevOps, data science team, 182 Dialpad AI, 337 differential privacy, 123 disaster recovery, model serving, 186-187 discovery and definition phase, product devel‐\necommerce, metrics, 294 edge devices, model serving, 178-181 efficiency, training systems, 152-154 emergency response, continuous ML systems,\nmodel serving, 187\nethics training, privacy preservation, 125 evaluation strategies, continuous ML systems,\nAPI, 73-74 continuous ML systems, 228 data, 73 data normalization, 73 definitions, 73 lifecycle, 75 metadata, 73 training systems, 141-142, 149 values, 73\nmodel building, 66 models, 44 quality evaluation, 76-77 training systems, 141 transforming, 75-76 value, 68\nfeatures overview, 66 feedback loops, model training data, 232-233 feedback, deployment phase, 296 filtering, continuous ML systems, 227 fixes, vulnerabilities, 56 functional organizational structure, 320\nmodels, 314\nimplementation models, 315 in-processing methods, fairness and, 118 incentives, 318 incident management, 247\nML engineering versus modeling, 282-284 ML-centric outages, 251-274 model serving, 282-284 organization, 275 outage start phase, 249, 256 YarnIt case, 262, 272 pre-incident phase, 249\nbusiness leaders, 281-282 communications lead, 250 incident commander, 250 model developers, 275-277 on-call engineers, ethics, 284 operations lead, 250 planning lead, 250 product managers, 281-282 production engineers, 278-280, 282-284\nincomplete coverage, 48 ingestion phase of data management, 26 initializations, reproducibility, 156 input data, monitoring, 200-200 input feature lookup, 215 integration\nassumptions, 310 brownfield, 316 business complexity, 309-310 detail, 308-309 goals, YarnIt example, 315 greenfield, 316 hiring tasks, 317-318 implementation models, 314, 315 integration point, 310 leader-based viewpoint, 308 organizational risks, 312-314 roles and responsibilities, 316-317 value of ML, 311-312\nmonitoring systems, 191 training, continuous ML systems, 227 vulnerabilities, 50 fraud, 51 label noise, 50 malicious feedback, 51 objectives, wrong, 51 latency, prediction latency, 165-166 launching new models, continuous ML sys‐\nM MaaS (model-as-a-service), 176-178 management phase, data management, 32-32 market organizational structure, 320 market products, Responsible AI, 134 mean absolute error, 104 mean squared error, 104 metadata\nMicrosoft Azure SQL Data Warehouse, 31 ML echo chambers, training data, 49 ML engineers, integration and, 317 ML framework, training systems, 144 ML lifecycle, 1-3 applications\nbuilding, 5-6 validating, 5-6 basic model health, 11 data analysis, 3-3 data collection, 3-3 launch, 8-9\nad click prediction, 353-356 continuous ML model and traffic, 341-342 data retention policies and, 337-341 databases, 353-356 dependencies, workflow and, 356-361 NLP applications, 348-353 privacy policies, 337-341 steel inspection, 343-348 visual inspection, 343-348\nmodel and product development phase, 296 model architecture, 47, 252 model definition, 47 model developers, incident management and,\nmodel management system, 142-143 model performance monitoring, 221 model quality, 89\nmodel validity, 89\nbackups, 147 bias in, 25 build versus buy, 298-299 configuration, 156 configured models, 47 corrupt, 92 deep learning and, 45 fail-safe implementation, 147 feature store, training systems and, 149 features, 44 hardware, 167-169 infrastructure crashes, 91 loading, 169 location\npost-deployment, 196 predictions, 44 production environment, loading, 90 provability, training and, 149 resource utilitization, 151-152 Responsible AI, 133 deployment, 134 market products, 134 quality assessment, 133 validation, 133\ndefined, 189 delayed actuals, 207 dense data types, 220 fairness in, 219 high-level point, 221 holistic view, 201 infrastructure performance, 221 input data and, 200-200 metrics, proxy metrics, 207 model performance monitoring, 221 model serving, 205 models, 206-209 performance, 205\nno/few actuals, 208 perception of, 192 privacy in, 219-220 processing and, 200-201 production monitoring, 193 model serving, 194-195\nmodel level, 214 model performance optimization, 214 service performance optimization, 215 serving level, 214 service performance, 221 SLOs and, 216-218 training and, 198-201 training systems, 144 monitoring systems, 190 aggregation, 190 decoration, 190 labeled data and, 191 pull notifications, 191\n(see also data normalization) training systems, 141 normative values, 116-117 numerical data\nO observability, 189 observability data, 193 on-call engineers, ethics, 284 online inference (see online serving) online serving, 174-176 orchestration, training systems, 143\nP parallel universe models, 241 parallelism, reproducibility and, 156 performance metrics, 93 performance monitoring, serving and, 205 performance-oriented cultures, 314 personal privacy laws, 127 personalization, discovery and definition phase,\nanonymization, 18 data collection and, 18 excluding, 37, 37 features and, 86 labeling and, 87 monitoring and, 220-220 privacy and, 86 pseudonymized data and, 18 serving on edge and, 180\ndata sensitivity, 21-22 launching, 9 metadata, 85 model serving and, 170 Responsible AI\ndata collection and cleaning, 132 deployment, 134 market products, 134 modeling, 133 quality assessment, 133 use case brainstorming, 132 validation, 133\nplatforms, build versus buy, 300-301 policy and compliance, 40-41 post-deployment models, 196 post-processing methods, fairness and, 118-119 PR (precision and recall) curve, validation and,\npreprocessing, fairness and, 117-118 privacy, 37-39, 107, 121 big data and, 122 differential privacy, 123 in monitoring, 219-220 k-anonymity, 122 preserving, 124\naccess controls, 124 access logging, 124 data access guidelines, 125 data minimization, 124 data separation, 125 ethics training, 125 institutional measures, 125-126 privacy by design, 126 technical measures, 124-125\nprivacy-preserving incident management, 284 process organizational structure, 320 process, fairness as, 120-121 processes, organizational design and, 321 processing phase of data management, 26-30 processing, monitoring, 200-201 product development, 290\nbusiness goal setting phase, 292-295 deployment phase, 296-297 discovery and definition phase, 291-292 model and product development phase, 296 MVP construction and validation phase,\nproduction monitoring, 189-193 model serving and, 194-195\nproducts, 289 progressive validation, 95 propensity scores, small, 231 proxy metrics, 207 pseudonymization, 18 pseudonymized data, 18\nR race conditions, training speed and, 150 raw data, 18 real-time actuals, 206 real-world events, training data, 49 recall, validation and, 204 regression metrics\nbinary changes and, 156 data differences, 156 initializations, random, 156 model configuration and, 156 parallelism, data, 157 parallelism, system, 156 training systems, 155-159\ndata collection and cleaning, 132 deployment, 134 market products, 134 model validation, 133 modeling, 133 quality assessment, 133 use case brainstorming, 132",
      "keywords": [
        "data ingestion system",
        "model performance monitoring",
        "model training data",
        "product development phase",
        "model management system",
        "data protection laws",
        "Index incident management",
        "data quality monitoring",
        "build versus buy",
        "data sensitivity Index",
        "SQL Data Warehouse",
        "Azure SQL Data",
        "training data availability",
        "training data feedback",
        "data access guidelines"
      ],
      "concepts": [
        "data",
        "models",
        "phases",
        "training",
        "features",
        "systems",
        "monitoring",
        "serving",
        "index",
        "validation"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 47,
          "title": "",
          "score": 0.742,
          "base_score": 0.742,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 8,
          "title": "",
          "score": 0.686,
          "base_score": 0.536,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 6,
          "title": "",
          "score": 0.673,
          "base_score": 0.523,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 18,
          "title": "",
          "score": 0.598,
          "base_score": 0.448,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 16,
          "title": "",
          "score": 0.562,
          "base_score": 0.412,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "phase",
          "18",
          "134",
          "200",
          "156"
        ],
        "semantic": [],
        "merged": [
          "phase",
          "18",
          "134",
          "200",
          "156"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3371900925658832,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:45.826826+00:00"
      }
    },
    {
      "chapter_number": 47,
      "title": "Segment 47 (pages 404-409)",
      "start_page": 404,
      "end_page": 409,
      "summary": "scikit-learn, 55 self-fulfilling algorithms, 131 self-fulfilling prophecies, training data, 49 semi-structured data, 24 sequencing, organizational design and, 322-323 service\nmodel level, 214 model performance optimization, 214 service performance optimization, 215 serving level, 214\nservice-level indicators (SLIs), 7 services, monitoring across, 218-219 serving the model (see model serving) serving, offline, 171-174 shifts in distribution, 5 signals\nsolutionism, 112 sparse data, 34 sprints, 290 spurious correlations, training data, 48 SREs (site reliability engineers), 3\nAmazon Redshift, 31 feature store, 72-76 Google Cloud, 31 Microsoft Azure SQL Data Warehouse, 31 model serving and, 169\nstress-test distributions, 96 structured data, 24 supervised learning, 44\ncontinuous ML systems, case study, 341-342 model serving, 164-165 queries per second, 164\ntrained models, 252 training\nethics training, privacy and, 125 human labelers, 81 model retraining, 146 monitoring and, 198-201 Responsible AI, 133 roll forward and, 199 vulnerabilities, 51\ndeep learning, 52-54 overfitting, 52 stability, 52 training algorithms, 137 training data source, 57 training systems, 141 vulnerabilities, 48 cold start, 49 incomplete coverage, 48 labels, 50-51 ML echo chambers, 49 real-world events, 49 self-fulfilling prophecies, 49 spurious correlations, 48 training methods, 51-54\nbatch processing, 152 compute resource capacity, 159-160 data sensitivity and, 154 feature development, 141 feature store and, 141-142 features, 141 ML training pipelines, 5, 143 model management system, 142-143 money-indexed cost, 153 monitoring, 144 normalization, 141\ncommon problems, 154-160 data availability, 148-149 efficiency, 152-154 failures, 145 feature changes, 149 feature store, 149 model backups, 147 model provability, 149 model retraining, 146 model versions, 146 resource utilization, 151-152 training too fast, 150-151\ndata to train on, 138 framework, 139 model configuration system, 138 quality evaluation system, 139 synching models to serving, 139\nresource-indexed cost, 153 training data, 141 training pipelines, 3-5 training-serving skew, 91, 195\nfeature generation, 56 fixes and, 56 training data and, 48 cold start, 49 incomplete coverage, 48 labels, 50-51 ML echo chambers, 49 real-world events, 49 self-fulfilling prophecies, 49 spurious correlations, 48 training methods, 51-54\nfreshness, 62 labels, 61 latency, 63 prediction failures, 63 prediction stability, 62 previous user behavior, 60 product placement, 61 raw product image, 60 serving, 62-63 updates, 61-62 cross-selling, 303 data problem example, 155 features, options, 67 filtering\njurisdictional rules, 40 KPIs, 184 labels, human-generated, 78 latency, prediction latency, 165-166 lost data, 21 pipeline, 170 recommendation system, privacy and, 38 recommendations, 303 recommendations model, 67 reproducibility problem, 157-159 searches, training on, 21 SLOs, 8 temporal effects, 234 upselling, 303 yarn by total sales, 302-303\nShe has led teams in large tech companies and startups launching product features, internal tools, and operating large systems.\nHe has worked with all of the major cloud providers from their Dublin, Ireland offices, and most recently at Microsoft, where he was global head of Azure site reliability engineering (SRE).\nHis first exposure to machine learning came with managing the Ads ML teams in Google’s Dublin office and working with Todd Underwood in Pittsburgh, though it has continued to fascinate him since.\nHis teams build large-scale, cloud native, real-time business communications and collab‐ oration software with industry-leading in-house AI/ML and telephony technology.\nBefore Dialpad, he led teams responsible for search and personalization platforms, products, and services at Apple.\nD. Sculley is the CEO of Kaggle and GM of Third-Party ML Ecosystems at Google, and previously was a director of the Google Brain Team and the lead of some of Google’s most critical production machine learning pipelines.\nHe has focused on issues of technical debt in machine learning, along with robustness and reliability of models and pipelines, and has led teams applying machine learning to problems as diverse as ad click-through prediction and abuse prevention to protein design and scientific discovery.\nML SRE teams build and scale internal and external ML services, and are critical to almost every significant product at Google.\nThe insect on the cover of Reliable Machine Learning is the honeypot ant (Myrmeco‐ cystus mimicus).",
      "keywords": [
        "SQL Data Warehouse",
        "IID test data",
        "training data",
        "training data source",
        "Azure SQL Data",
        "storage Amazon Redshift",
        "test data held-out",
        "management detection phase",
        "outage start phase",
        "51-54 training systems",
        "incident management",
        "troubleshooting data drift",
        "incident management detection",
        "service level objectives",
        "data"
      ],
      "concepts": [
        "model",
        "data",
        "product",
        "production",
        "training",
        "phase",
        "learning",
        "feature",
        "systemic",
        "systems"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 46,
          "title": "",
          "score": 0.742,
          "base_score": 0.742,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "",
          "score": 0.591,
          "base_score": 0.591,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 1,
          "title": "",
          "score": 0.505,
          "base_score": 0.505,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 29,
          "title": "",
          "score": 0.492,
          "base_score": 0.492,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 44,
          "title": "",
          "score": 0.47,
          "base_score": 0.47,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "49",
          "48",
          "training",
          "141",
          "51"
        ],
        "semantic": [],
        "merged": [
          "49",
          "48",
          "training",
          "141",
          "51"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.38852935238462966,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:08:45.826875+00:00"
      }
    }
  ],
  "total_chapters": 47,
  "enrichment_provenance": {
    "taxonomy_id": "none",
    "taxonomy_version": "none",
    "taxonomy_path": "none",
    "taxonomy_checksum": "sha256:none",
    "source_metadata_file": "Reliable Machine Learning_metadata.json",
    "enrichment_date": "2025-12-17T23:08:45.834715+00:00",
    "enrichment_method": "msep",
    "model_version": "ai-agents-msep-v1",
    "processing_time_ms": 4228.474043999086,
    "total_similar_chapters": 235
  }
}