{
  "metadata": {
    "title": "Software Architecture",
    "source_file": "Software Architecture_metadata.json"
  },
  "chapters": [
    {
      "chapter_number": 1,
      "title": "Segment 1 (pages 1-9)",
      "start_page": 1,
      "end_page": 9,
      "summary": "Software Architecture: The Hard Parts\nPraise for Software Architecture: The Hard Parts\n“This book provides the missing manual around building microservices and analyzing the nuances of architectural decisions throughout the whole tech stack.\nIn this book, you get a catalog of architectural decisions you can make when building your distributed system and what are the pros and cons associated with each decision.\n“Software Architecture: The Hard Parts provides the reader with valuable insight, practices, and real-world examples on pulling apart highly coupled systems and building them back up again.\n“This book will equip you with the theoretical background and with a practical framework to help answer the most difficult questions faced in modern software architecture.”\nSoftware Architecture: The Hard Parts Modern Trade-Off Analysis for Distributed Architectures\nSoftware Architecture: The Hard Parts by Neal Ford, Mark Richards, Pramod Sadalage, and Zhamak Dehghani\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Software Architecture: The Hard Parts, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\n2 Giving Timeless Advice About Software Architecture 3 The Importance of Data in Architecture 4 Architectural Decision Records 5 Architecture Fitness Functions 6 Using Fitness Functions 7 Architecture Versus Design: Keeping Definitions Simple 13 Introducing the Sysops Squad Saga 15 Nonticketing Workflow 16 Ticketing Workflow 17 A Bad Scenario 17 Sysops Squad Architectural Components 18 Sysops Squad Data Model 19\n2. Discerning Coupling in Software Architecture.\n25 Architecture (Quantum | Quanta) 28 Independently Deployable 29 High Functional Cohesion 30 High Static Coupling 30 Dynamic Quantum Coupling 38 Sysops Squad Saga: Understanding Quanta 42\n131 Data Decomposition Drivers 132 Data Disintegrators 133 Data Integrators 146 Sysops Squad Saga: Justifying Database Decomposition 150 Decomposing Monolithic Data 151 Step 1: Analyze Database and Create Data Domains 156 Step 2: Assign Tables to Data Domains 156 Step 3: Separate Database Connections to Data Domains 158 Step 4: Move Schemas to Separate Database Servers 159 Step 5: Switch Over to Independent Database Servers 161 Selecting a Database Type 161 Relational Databases 163 Key-Value Databases 165 Document Databases 167 Column Family Databases 169 Graph Databases 171 NewSQL Databases 173 Cloud Native Databases 175 Time-Series Databases 177 Sysops Squad Saga: Polyglot Databases 179\n185 Granularity Disintegrators 188 Service Scope and Function 189 Code Volatility 191 Scalability and Throughput 192 Fault Tolerance 193 Security 195 Extensibility 196 Granularity Integrators 197 Database Transactions 198 Workflow and Choreography 200 Shared Code 203 Data Relationships 205 Finding the Right Balance 208 Sysops Squad Saga: Ticket Assignment Granularity 209 Sysops Squad Saga: Customer Registration Granularity 212 vii",
      "keywords": [
        "Sysops Squad Saga",
        "Squad Saga",
        "Sysops Squad",
        "Pattern Description",
        "Fitness Functions",
        "Domain Components Pattern",
        "Software Architecture",
        "Components Pattern",
        "Functions for Governance",
        "Pattern",
        "Squad",
        "Hard Parts",
        "Saga",
        "Component Dependencies Pattern",
        "Sysops"
      ],
      "concepts": [
        "database",
        "architecture",
        "architectural",
        "components",
        "saga",
        "data",
        "coupling",
        "software",
        "decomposition",
        "editor"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 3,
          "title": "",
          "score": 0.786,
          "base_score": 0.636,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 11,
          "title": "",
          "score": 0.719,
          "base_score": 0.569,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 43,
          "title": "",
          "score": 0.642,
          "base_score": 0.492,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 12,
          "title": "",
          "score": 0.637,
          "base_score": 0.487,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 9,
          "title": "",
          "score": 0.626,
          "base_score": 0.476,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "software architecture",
          "databases",
          "hard parts",
          "software",
          "architecture hard"
        ],
        "semantic": [],
        "merged": [
          "software architecture",
          "databases",
          "hard parts",
          "software",
          "architecture hard"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3925128746169275,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.167133+00:00"
      }
    },
    {
      "chapter_number": 2,
      "title": "Segment 2 (pages 10-18)",
      "start_page": 10,
      "end_page": 18,
      "summary": ". 219 Code Replication 221 When to Use 223 Shared Library 223 Dependency Management and Change Control 224 Versioning Strategies 225 When To Use 227 Shared Service 228 Change Risk 229 Performance 231 Scalability 232 Fault Tolerance 232 When to Use 234 Sidecars and Service Mesh 234 When to Use 239 Sysops Squad Saga: Common Infrastructure Logic 239 Code Reuse: When Does It Add Value?\n. 249 Assigning Data Ownership 250 Single Ownership Scenario 251 Common Ownership Scenario 252 Joint Ownership Scenario 253 Table Split Technique 254 Data Domain Technique 256 Delegate Technique 258 Service Consolidation Technique 261 Data Ownership Summary 262 Distributed Transactions 263 Eventual Consistency Patterns 267 Background Synchronization Pattern 269 Orchestrated Request-Based Pattern 272 Event-Based Pattern 277 Sysops Squad Saga: Data Ownership for Ticket Processing 279\nReplicated Caching Pattern 288 Data Domain Pattern 293 Sysops Squad Saga: Data Access for Ticket Assignment 295\n299 Orchestration Communication Style 301 Choreography Communication Style 306 Workflow State Management 311 Trade-Offs Between Orchestration and Choreography 315 State Owner and Coupling 315 Sysops Squad Saga: Managing Workflows 317\n365 Strict Versus Loose Contracts 367 Trade-Offs Between Strict and Loose Contracts 370 Contracts in Microservices 372 Stamp Coupling 376 Over-Coupling via Stamp Coupling 376 Bandwidth 377 Stamp Coupling for Workflow Management 378 Sysops Squad Saga: Managing Ticketing Contracts 379\nWhen two of your authors, Neal and Mark, were writing the book Fundamentals of Software Architecture, we kept coming across complex examples in architecture that we wanted to cover but that were too difficult.\nTo that end, we asked experts in those fields to join us, which allows this book to fully incorporate decision making from both angles: architecture to data and data to architecture.\nThe result is this book: a collection of difficult problems in modern software architec‐ ture, the trade-offs that make the decisions hard, and ultimately an illustrated guide to show you how to apply the same trade-off analysis to your own unique problems.\nConstant width italic\nIn general, if example code is offered with this book, you may use it in your programs and documentation.\nFor example, writing a program that uses several chunks of code from this book does not require permission.\nSelling or distributing examples from O’Reilly books does require permission.\nAnswering a question by citing this book and quoting example code does not require permission.\nIncorporating a significant amount of example code from this book into your product’s documentation does require permission.\nFor example: “Software Architecture: The Hard Parts by Neal Ford, Mark Richards, Pramod Sadalage, and Zhamak Deh‐ ghani (O’Reilly).\nEmail bookquestions@oreilly.com to comment or ask technical questions about this book.\nFor news and information about our books and courses, visit http://oreilly.com.\nAcknowledgments Mark and Neal would like to thank all the people who attended our (almost exclu‐ sively online) classes, workshops, conference sessions, and user group meetings, as well as all the other people who listened to versions of this material and provided invaluable feedback.\nWe thank the publishing team at O’Reilly, who made this as painless an experience as writing a book can be.\nAcknowledgments from Mark Richards In addition to the preceding acknowledgments, I once again thank my lovely wife, Rebecca, for putting up with me through yet another book project.\nAcknowledgments from Neal Ford I would like to thank my extended family, Thoughtworks as a collective, and Rebecca Parsons and Martin Fowler as individual parts of it.\nLastly, I thank my wife, Candy, who continues to support this lifestyle that has me staring at things like book writing rather than our cats too much.\nI also thank Thoughtworks for its continued support in my life, and Neal Ford, Rebecca Parsons, and Martin Fowler for being amazing mentors; you all make me a better person.",
      "keywords": [
        "Sysops Squad Saga",
        "Squad Saga",
        "Sysops Squad",
        "Data",
        "Saga",
        "Pattern",
        "book",
        "Data Mesh",
        "Data Ownership",
        "Data Domain Pattern",
        "Sysops",
        "Squad",
        "Neal Ford",
        "Transactional Saga Patterns",
        "Code"
      ],
      "concepts": [
        "patterns",
        "saga",
        "data",
        "book",
        "examples",
        "preface",
        "management",
        "manage",
        "time",
        "permission"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 45,
          "title": "",
          "score": 0.881,
          "base_score": 0.731,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 44,
          "title": "",
          "score": 0.821,
          "base_score": 0.671,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 5,
          "title": "",
          "score": 0.729,
          "base_score": 0.579,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 43,
          "title": "",
          "score": 0.685,
          "base_score": 0.535,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 28,
          "title": "",
          "score": 0.684,
          "base_score": 0.534,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "book",
          "thank",
          "neal",
          "data",
          "require permission"
        ],
        "semantic": [],
        "merged": [
          "book",
          "thank",
          "neal",
          "data",
          "require permission"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.44536748324404957,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.167201+00:00"
      }
    },
    {
      "chapter_number": 3,
      "title": "Segment 3 (pages 19-26)",
      "start_page": 19,
      "end_page": 26,
      "summary": "Entire classes of problems exist in software architecture that have no general good solutions, but rather present one messy set of trade-offs cast against an (almost) equally messy set.\nArchitects may have wondered why so few books exist about architecture compared to technical topics like frameworks, APIs, and so on.\nDon’t try to find the best design in software architecture; instead, strive for the least worst combination of trade-offs.\nOften, the best design an architect can create is the least worst collection of trade-offs —no single architecture characteristics excels as it would alone, but the balance of all the competing architecture characteristics promote project success.\nWhich begs the question: “How can an architect find the least worst combination of trade-offs (and document them effectively)?” This book is primarily about decision making, enabling architects to make better decisions when confronted with novel situations.\nWhy did we call this book Software Architecture: The Hard Parts?\nSimilarly, architects discuss the distinction between architecture and design, where the former is structural and the latter is more easily changed.\nThis presents an age-old problem for authors of books about technology generally and software architecture specifically—how can we write something that isn’t old immediately?\nThe Importance of Data in Architecture\nFor many in architecture, data is everything.\nFor example, architects and DBAs must ensure that business data sur‐ vives the breaking apart of monolith systems and that the business can still derive value from its data regardless of architecture undulations.\nThis reliance on data means that all software architecture is in the service of data, ensuring the right data is available and usable by all parts of the enterprise.\nHowever, in microservices and the philosophical adherence to a bounded context from Domain-Driven Design, as a way of limiting the scope of implementation detail coupling, data has moved to an architectural concern, along with transactionality.\nWe devoted an entire chapter to ADRs in our previous book, Fundamentals of Software Architecture (O’Reilly).\nWe will be leveraging ADRs as a way of documenting various architecture decisions made throughout the book.\nFortunately, modern engineering practices allow automating many common governance concerns by using architecture fitness functions.\nArchitecture Fitness Functions Once an architect has identified the relationship between components and codified that into a design, how can they make sure that the implementers will adhere to that design?\nAs this book primarily covers architecture structure, we cover how to automate design and quality principles via fitness functions in many places.\nFor example, if an architect chooses a particu‐ lar architecture style or communication medium, how can they make sure that a\nWhen done manually, architects perform code reviews or perhaps hold architecture review boards to assess the state of governance.\nUsing Fitness Functions In the 2017 book Building Evolutionary Architectures (O’Reilly), the authors (Neal Ford, Rebecca Parsons, and Patrick Kua) defined the concept of an architectural fit‐ ness function: any mechanism that performs an objective integrity assessment of some architecture characteristic or combination of architecture characteristics.\nArchitects can use a wide variety of tools to implement fitness functions; we will show numerous examples throughout the book.\nFor example, dedicated testing libraries exist to test architecture structure, architects can use monitors to test operational architecture characteristics such as performance or scalability, and chaos engineering frameworks test reliability and resiliency.\nFor example, an architect can’t specify that they want a “high performance” website; they must provide an object value that can be measured by a test, monitor, or other fitness function.\nSome architecture characteristic or combination of architecture characteristics This characteristic describes the two scopes for fitness functions:\nThese fitness functions handle a single architecture characteristic in isola‐ tion.\nArchitecture Fitness Functions\nHolistic fitness functions validate a combination of architecture characteris‐ tics.\nHolistic fitness functions exercise a combination of interlocking architecture characteristics to ensure that the combined effect won’t negatively affect the architecture.\nAn architect implements fitness functions to build protections around unexpected change in architecture characteristics.\nFitness functions validate architecture characteristics, not domain criteria; unit tests are the opposite.\nThus, elasticity is an architectural concern and within the scope of a fitness function.",
      "keywords": [
        "architecture",
        "software architecture",
        "architecture characteristics",
        "architecture fitness functions",
        "fitness functions",
        "Data",
        "software",
        "software development",
        "architects",
        "architecture decision",
        "decision",
        "fitness",
        "Architectural Decision Records",
        "book Software Architecture",
        "architecture fitness"
      ],
      "concepts": [
        "architecture",
        "architectural",
        "architects",
        "data",
        "development",
        "operating",
        "operationally",
        "operational",
        "integration",
        "integrate"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 1,
          "title": "",
          "score": 0.786,
          "base_score": 0.636,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 41,
          "title": "",
          "score": 0.667,
          "base_score": 0.517,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 9,
          "title": "",
          "score": 0.613,
          "base_score": 0.463,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 8,
          "title": "",
          "score": 0.587,
          "base_score": 0.437,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 4,
          "title": "",
          "score": 0.577,
          "base_score": 0.427,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "architecture",
          "fitness",
          "fitness functions",
          "functions",
          "architecture characteristics"
        ],
        "semantic": [],
        "merged": [
          "architecture",
          "fitness",
          "fitness functions",
          "functions",
          "architecture characteristics"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2851454134822072,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.167244+00:00"
      }
    },
    {
      "chapter_number": 4,
      "title": "Segment 4 (pages 27-35)",
      "start_page": 27,
      "end_page": 35,
      "summary": "The solution to this problem is to write a fitness function to avoid component cycles, as shown in Example 1-1.\nFitness function to detect component cycles\nArchitecture Fitness Functions\nExample 1-1 shows a very low-level, code-centric fitness function.\nWhen designing a layered architecture such as the one in Figure 1-2, the architect defines the layers to ensure separation of concerns.\nArchUnit allows architects to address this problem via a fitness function, shown in Example 1-2.\nArchUnit fitness function to govern layers\nIn Example 1-2, the architect defines the desirable relationship between layers and writes a verification fitness function to govern it.\nThus, for these systems, architects design perfor‐ mance fitness functions that take into account the number of concurrent users.\nAs long as the measure of an architecture characteristic is objective, architects can test it.\nMost deployment pipelines support manual stages, allowing teams to accommodate manual fitness functions.\nArchitecture Fitness Functions\nContinuity is important, as illustrated in this example of enterprise-level governance using fitness functions.\nFitness functions provide many benefits for architects, not the least of which is the chance to do some coding again!\nOne of the universal complaints among architects is that they don’t get to code much anymore—but fitness functions are often code!\nBy building an executable specification of the architecture, which anyone can validate anytime by running the project’s build, architects must understand the system and its ongoing evolution well, which overlaps with the core goal of keeping up with the code of the project as it grows.\nHowever powerful fitness functions are, architects should avoid overusing them.\nArchitects should not form a cabal and retreat to an ivory tower to build an impossi‐ bly complex, interlocking set of fitness functions that merely frustrate developers and teams.\nBy codifying rules about code quality, structure, and other safeguards against decay into fitness functions that run continually, architects build a quality checklist that developers can’t skip.\nFitness func‐ tions represent a checklist of important principles defined by architects and run as part of the build to make sure developers don’t accidentally (or purposefully, because of external forces like schedule pressure) skip them.\nWe utilize fitness functions throughout the book when an opportunity arises to illus‐ trate governing an architectural solution as well as the initial design.\nWe use the term contract broadly to define the interface between two software parts, which may encompass method or function calls, integration architecture remote calls, dependencies, and so on.\nTo that end, we need a problem to illustrate architecture con‐ cepts against—which leads us to the Sysops Squad.\nWhile many books on software architecture cover new development efforts, many real-world problems exist within existing systems.\nTherefore, our story starts with the existing Sysops Squad architecture highlighted here.\noccur, customer-facing technology experts (the Sysops Squad) come to the customer’s residence (or work office) to fix problems with the electronic device.\nThe customer registers for the Sysops Squad service and maintains their cus‐ tomer profile, support contracts, and billing information.\nThe manager keeps track of problem ticket operations and receives operational and analytical reports about the overall Sysops Squad problem ticket system.\nNonticketing Workflow The nonticketing workflows include those actions that administrators, managers, and customers perform that do not relate to a problem ticket.\nTicketing Workflow The ticketing workflow starts when a customer enters a problem ticket into the sys‐ tem, and ends when the customer completes the survey after the repair is done.\n1. Customers who have purchased the support plan enter a problem ticket by using the Sysops Squad website.\n2. Once a problem ticket is entered in the system, the system then determines which Sysops Squad expert would be the best fit for the job based on skills, cur‐ rent location, service area, and availability.\n3. Once assigned, the problem ticket is uploaded to a dedicated custom mobile app on the Sysops Squad expert’s mobile device.\nThe Sysops Squad expert can also access a knowledge base through the mobile app to find out what has been done in the past to fix the problem.\n6. Once the expert fixes the problem, they mark the ticket as “complete.” The sysops squad expert can then add information about the problem and repair the knowl‐ edge base.\nA Bad Scenario Things have not been good with the Sysops Squad problem ticket application lately.\nBecause of reliability issues, the Sysops Squad system frequently “freezes up,” or crashes, resulting in all application functionality not being available anywhere from five minutes to two hours while the problem is identified and the application restarted.",
      "keywords": [
        "Sysops Squad",
        "Sysops Squad expert",
        "Fitness Functions",
        "Sysops Squad Saga",
        "Sysops Squad problem",
        "Sysops Squad system",
        "Architecture Fitness Functions",
        "Sysops Squad architecture",
        "fitness",
        "Squad",
        "architecture",
        "Sysops",
        "Squad problem ticket",
        "Sysops Squad service",
        "Functions"
      ],
      "concepts": [
        "architecture",
        "architectural",
        "functions",
        "functionality",
        "customers",
        "fit",
        "code",
        "coding",
        "ticketing",
        "developer"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 13,
          "title": "",
          "score": 0.815,
          "base_score": 0.665,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 9,
          "title": "",
          "score": 0.698,
          "base_score": 0.548,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 41,
          "title": "",
          "score": 0.591,
          "base_score": 0.441,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 12,
          "title": "",
          "score": 0.591,
          "base_score": 0.441,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 3,
          "title": "",
          "score": 0.577,
          "base_score": 0.427,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "fitness",
          "problem",
          "fitness functions",
          "squad",
          "problem ticket"
        ],
        "semantic": [],
        "merged": [
          "fitness",
          "problem",
          "fitness functions",
          "squad",
          "problem ticket"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3246662025584423,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.167292+00:00"
      }
    },
    {
      "chapter_number": 5,
      "title": "Segment 5 (pages 36-45)",
      "start_page": 36,
      "end_page": 45,
      "summary": "If something isn’t done soon, Penultimate Electronics will be forced to abandon the very lucrative support contract business line and lay off all the Sysops Squad adminis‐ trators, experts, managers, and IT development staff—including the architects.\nSysops Squad Architectural Components The monolithic system for the Sysops Squad application handles ticket management, operational reporting, customer registration, and billing, as well as general adminis‐ trative functions such as user maintenance, login, and expert skills and profile main‐ tenance.\nFigure 1-3 and the corresponding Table 1-1 illustrate and describe the components of the existing monolithic application (the ss.\nss.billing.payment\nss.customer.profile\nss.expert.profile\nTicket\nss.ticket\nss.ticket.assign\nss.ticket.notify\nss.ticket.route\nSurvey\nss.survey\nss.survey.notify\nss.survey.templates\nThese components will be used in subsequent chapters to illustrate various techni‐ ques and trade-offs when dealing with breaking applications into distributed architectures.\nSysops Squad Data Model The Sysops Squad application with its various components listed in Table 1-1 uses a single schema in the database to host all its tables and related database code.\nThe database is used to persist customers, users, contracts, billing, payments, knowledge base, and customer surveys; the tables are listed in Table 1-2, and the ER model is illustrated in Figure 1-4.\nSurvey\nA survey for after-support customer satisfaction\nSurvey question is assigned to customer\nA customer’s response to the survey\nTicket\nSupport tickets raised by customers\nTo understand complex subjects (such as trade-offs in distributed architectures), an architect must figure out where to start untangling.\nStatic coupling refers to the way architectural parts (classes, components, services, and so on) are wired together: dependencies, coupling degree, connection points, and so on.\nAn architect can often measure static coupling at compile time as it represents the static depen‐ dencies within the architecture.\nDynamic coupling refers to how architecture parts call one another: what kind of communication, what information is passed, strictness of contracts, and so on.\nOur goal is to investigate how to do trade-off analysis in distributed architectures; to do that, we must pull the moving pieces apart so that we can discuss them in isolation to understand them fully before putting them back together.\nIn Chapter 2, we tackle the problem of defining the scope of static and dynamic coupling in architectures, and present the entire picture that we must pull apart to understand.\nData and transactions have become increasingly important in architecture, driving many trade-off decisions by architects and DBAs. Chapter 6 addresses the architec‐ tural impacts of data, including how to reconcile service and data boundaries.\nCHAPTER 2 Discerning Coupling in Software Architecture\nDistributed architectures like microservices are difficult, especially if architects cannot untangle all the forces at play.\nWhat we need is an approach or framework that helps us figure out the hard problems in our architecture.”\nOne of the most difficult tasks an architect will face is untangling the various forces and trade-offs at play in distibuted architectures.\nWhy have architects struggled with decisions in distributed architectures?\nBuilding services that model bounded contexts required a subtle but important change to the way architects designed distributed systems because now transactionality is a first-class architectural concern.\nWhile several frameworks have existed for decades (such as Architecture Trade-off Analysis Method, or ATAM), they lack focus on real problems architects face on a daily basis.\nChapter 2: Discerning Coupling in Software Architecture\nAs in many things in architecture, the advice is simple; the hard parts lie in the details, particularly how difficult parts become entan‐ gled, making it difficult to see and understand the individual parts, as illustrated in Figure 2-1.",
      "keywords": [
        "Sysops Squad",
        "Sysops Squad application",
        "existing Sysops Squad",
        "Sysops Squad Saga",
        "Sysops Squad components",
        "Sysops",
        "Sysops Squad adminis",
        "Sysops Squad database",
        "Sysops Squad Data",
        "Sysops support Customer",
        "Sysops Squad Architectural",
        "Squad application",
        "Squad",
        "customer",
        "Sysops users Expert"
      ],
      "concepts": [
        "architectural",
        "architectures",
        "customer",
        "chapters",
        "survey",
        "architects",
        "coupling",
        "ticket",
        "data",
        "billing"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 45,
          "title": "",
          "score": 0.761,
          "base_score": 0.611,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 14,
          "title": "",
          "score": 0.747,
          "base_score": 0.597,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 15,
          "title": "",
          "score": 0.729,
          "base_score": 0.579,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 2,
          "title": "",
          "score": 0.729,
          "base_score": 0.579,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 19,
          "title": "",
          "score": 0.722,
          "base_score": 0.572,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ss",
          "sysops",
          "squad",
          "sysops squad",
          "survey"
        ],
        "semantic": [],
        "merged": [
          "ss",
          "sysops",
          "squad",
          "sysops squad",
          "survey"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4267201816447281,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.167339+00:00"
      }
    },
    {
      "chapter_number": 6,
      "title": "Segment 6 (pages 46-53)",
      "start_page": 46,
      "end_page": 53,
      "summary": "We start with our first great untangling of forces in dis‐ tributed architectures: defining architecture quantum along with the two types of coupling, static and dynamic.\nArchitecture quantum\nAn architecture quantum is an independently deployable artifact with high func‐ tional cohesion, high static coupling, and synchronous dynamic coupling.\nFor example, in a microservices architecture, a service must contain dependent components such as a database, repre‐ senting static coupling—the service isn’t operational without the necessary data.\nThus, from an independently deployable standpoint, a service within a microservices architecture represents an architecture quantum (contingent on cou‐ pling—as discussed next).\nSecond, the architecture quantum represents one of the forces (static coupling) archi‐ tects must consider when striving for proper granularity of services within a dis‐ tributed architecture.\nThird, independent deployability forces the architecture quantum to include common coupling points such as databases.\nHigh Static Coupling High static coupling implies that the elements inside the architecture quantum are tightly wired together, which is really an aspect of contracts.\nAn architecture quantum is, in part, a measure of static coupling, and the measure is quite simple for most architecture topologies.\nFor example, the following diagrams show the architecture styles featured in Fundamentals of Software Architecture, with the architecture quantum static coupling illustrated.\nThe architecture quantum measure of static coupling includes the database, and a system that relies on a single database cannot have more than a single quantum.\nThus, the static coupling measure of an architecture quantum helps identify coupling points in architecture, not just within\nMost monolithic architectures contain a single coupling point (typically, a database) that makes its quantum measure one.\nWhile this individual services model shows the isolation common in microservices, the architecture still utilizes a single relational database, rendering its architecture quantum score to one.\nSo far, the static coupling measurement of architecture quantum has evaluated all the topologies to one.\nFor example, the mediator style of event- driven architecture will always be evaluated to an single architecture quantum, as illustrated in Figure 2-4.\nEven though this style represents a distributed architecture, two coupling points push it toward a single architecture quantum: the database, as common with the previous monolithic architectures, but also the Request Orchestrator itself—any holistic coupling point necessary for the architecture to function forms an architecture quan‐ tum around it.\nThis broker-style event driven architecture (without a central mediator) is neverthe‐ less a single architecture quantum because all the services utilize a single relational database, which acts as a common coupling point.\nThe question answered by the static analysis for an architecture quantum is, “Is this dependent of the architecture necessary to bootstrap this service?” Even in the case of an event-driven architecture where some of the services don’t access the database, if they rely on services that do access the database, then they become part of the static coupling of the architecture quantum.\nThe static coupling measure of an architecture quantum assesses the coupling depen‐ dencies between architectural and operational components.\nThus, the operating sys‐ tem, data store, message broker, container orchestration, and all other operational dependencies form the static coupling points of an architecture quantum, using the strictest possible contracts, operational dependencies (more about the role of con‐ tracts in architecture quanta in Chapter 13).\nArchitects in these architectures favor high degrees of decoupling and take care not to create coupling points between services, allowing each individual service to each form its own quanta, as shown in Figure 2-7.",
      "keywords": [
        "architecture quantum",
        "architecture",
        "quantum",
        "single architecture quantum",
        "coupling",
        "static coupling",
        "software architecture",
        "static",
        "single architecture",
        "services",
        "high static coupling",
        "coupling points",
        "single",
        "architecture quantum static",
        "database"
      ],
      "concepts": [
        "architectures",
        "architectural",
        "service",
        "coupled",
        "deployed",
        "high",
        "dependencies",
        "dependency",
        "depending",
        "database"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 7,
          "title": "",
          "score": 0.857,
          "base_score": 0.707,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 32,
          "title": "",
          "score": 0.696,
          "base_score": 0.546,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 35,
          "title": "",
          "score": 0.673,
          "base_score": 0.523,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 33,
          "title": "",
          "score": 0.603,
          "base_score": 0.453,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 30,
          "title": "",
          "score": 0.566,
          "base_score": 0.416,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "quantum",
          "architecture quantum",
          "architecture",
          "static",
          "coupling"
        ],
        "semantic": [],
        "merged": [
          "quantum",
          "architecture quantum",
          "architecture",
          "static",
          "coupling"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.40591172968514305,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.167387+00:00"
      }
    },
    {
      "chapter_number": 7,
      "title": "Segment 7 (pages 54-62)",
      "start_page": 54,
      "end_page": 62,
      "summary": "However, if the system is tightly coupled to a user interface, the architecture forms a single architecture quantum, as illustrated in Figure 2-8.\nA tightly coupled user interface can reduce a microservices architecture quantum to one\nAdditionally, it will be difficult for an architect to design different levels of opera‐ tional architecture characteristics (performance, scale, elasticity, reliability, and so on) for each service if they all must cooperate together in a single user interface (particu‐ larly in the case of synchronous calls, covered in “Dynamic Quantum Coupling” on page 38).\nArchitects design user interfaces utilizing asynchronicity that doesn’t create coupling between front and back.\nIn such an architecture, the user interface elements that interact on behalf of the services are emitted from the services themselves.\nChapter 2: Discerning Coupling in Software Architecture\nIn a micro-frontend architecture, each service + user interface component forms an architecture quantum\nIn this example, the four tinted services along with their corresponding micro- frontends form architecture quanta: each of these services may have different archi‐ tecture characteristics.\nAny coupling point in an architecture can create static coupling points from a quan‐ tum standpoint.\nThe static coupling of a system provides valuable insight, even in complex systems involving integration architecture.\nIncreasingly, a common architect technique for understanding legacy architecture involves creating a static quantum diagram of how things are “wired” together, which helps determine what systems will be impacted by change and offers a way of understanding (and potentially decoupling) the architecture.\nStatic coupling is only one-half of the forces at play in distributed architectures.\nDynamic Quantum Coupling The last portion of the architecture quantum definition concerns synchronous cou‐ pling at runtime—in other words, the behavior of architecture quanta as they interact with one another to form workflows within a distributed architecture.\nChapter 2: Discerning Coupling in Software Architecture\nAsynchronous communication occurs between two services when the caller posts a message to the receiver (usually via a mechanism such as a message queue) and, once the caller gets acknowledgment that the message will be processed, it returns to work.\nGenerally, architects use message queues (illustrated via the gray cylindrical tube in the top dia‐ gram in Figure 2-12) to implement asynchronous communication, but queues are common and create noise on diagrams, so many architects leave them off, as shown in the lower diagram.\nFor example, transactionality is easier in synchronous architectures with mediation, whereas higher levels of scale are possible with eventually consistent asynchronous choreographed systems.\nIn the remaining chapters in Part I, we focus on static coupling and understanding the various dimensions at play in distributed architectures, including data ownership,\nIn Part II, Putting Things Back Together, we focus on dynamic coupling and understanding communication patterns in microservices.\n“I’ve been reading about this architecture quantum stuff, and I just…don’t…get…it!”\n“Well,” said Addison, “the architecture quantum basically defines a DDD bounded context in archi- tectural terms.”\nBut the architecture quantum definition goes further by identifying types of cou- pling—that’s where the static and dynamic stuff comes in.”\nAnother way to think about it: consider one of the services we’re building in our target architecture.\nWhat is all the wiring required to bootstrap that service?”\nWhat else?”\nIf the service (or, more broadly, architecture quantum) I want to bootstrap utilizes a message broker to function, the broker must be present.\nWhen the service calls another service via the broker, we get into the dynamic side.”\n“If I think about what it would take to bootstrap it from scratch, that’s the static quantum coupling.”\nWe recently built a diagram of the static quan- tum coupling for each of our services defensively.”\nThey’re trying to do risk mitigation —if we change a service, they want to know what must be tested.”\nWhat about dynamic coupling?”\n“Dynamic coupling concerns how quanta communicate with each other, particularly synchronous versus asynchronous calls and their impact on operational architecture characteristics—things like performance, scale, elasticity, reliability, and so on.\nThat’s the other side effect of static coupling, by the way—it identifies the scope of things like operational architecture characteristics.\nIf on the other hand we make an asynchronous call, using the message queue as a buffer, we can allow the two services to execute operationally independently, allowing the caller to add messages to the queue and continue working, receiving notification when the workflow is complete.”\nThe architecture quantum defines the scope of architecture characteristics—it’s obvious how the static coupling can affect that.\nBut I see now that, depending on the type of call you make, you might temporarily couple two services together.”\n“The architecture quanta can entangle one another temporarily, during the course of a call, if the nature of the call ties things like performance, responsiveness, scale, and a bunch of others.”\n“OK, I think I understand what an architecture quantum is, and how the coupling definitions work.\nBut I’m never going to get that quantum/quanta thing straight!”\n“You’ll see a lot more of the impact of dynamic coupling on workflows and transactional sagas as you keep digging into our architecture.”",
      "keywords": [
        "architecture",
        "architecture quantum",
        "coupling",
        "Dynamic Quantum Coupling",
        "Quantum Coupling",
        "static quantum coupling",
        "quantum",
        "user interface",
        "dynamic coupling",
        "Software Architecture",
        "static coupling",
        "Discerning Coupling",
        "services",
        "communication",
        "user"
      ],
      "concepts": [
        "architecture",
        "saga",
        "communication",
        "communicate",
        "coupled",
        "couple",
        "service",
        "chapters",
        "asynchronous",
        "architect"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 6,
          "title": "",
          "score": 0.857,
          "base_score": 0.707,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 32,
          "title": "",
          "score": 0.729,
          "base_score": 0.579,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 33,
          "title": "",
          "score": 0.664,
          "base_score": 0.514,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 35,
          "title": "",
          "score": 0.657,
          "base_score": 0.507,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 36,
          "title": "",
          "score": 0.61,
          "base_score": 0.46,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "quantum",
          "architecture",
          "coupling",
          "architecture quantum",
          "static"
        ],
        "semantic": [],
        "merged": [
          "quantum",
          "architecture",
          "coupling",
          "architecture quantum",
          "static"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.35427029625902645,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.167431+00:00"
      }
    },
    {
      "chapter_number": 8,
      "title": "Segment 8 (pages 63-71)",
      "start_page": 63,
      "end_page": 71,
      "summary": "“Without a working application,” they had said, “we cannot possibly continue to support this business line.”\n“I agree with you,” said Addison, “but how do we convince the business to spend more money and time to refactor the architecture?\n“But if we both agree that we need to break apart the application to keep it alive, how in the world are we going to convince the business and get the funding and time we need to completely restructure the Sysops Squad application?” asked Addison.\nAddison sent a message explaining that they wanted to break apart the existing monolithic application, but weren’t sure how to convince the business that this approach would work.\n“What makes you so sure that breaking apart the Sysops Squad application will solve all of the issues?” asked Logan.\n“Then how do you know breaking apart the application is the right approach?” asked Logan.\n“Sorry,” said Logan, “but you know as well as I do that’s not a reasonable justification for the business.\n“Well,” said Logan, “to build a good business case for something of this magnitude, you first need to understand the benefits of architectural modularity, match those benefits to the issues you are fac- ing with the current system, and finally analyze and document the trade-offs involved with breaking apart the application.”\nBusiness drivers (such as mergers and acquisitions), increased competition in the marketplace, increased consumer demand, and increased innova‐ tion (such as automation through machine learning and artificial intelligence) neces‐ sarily require changes to underlying computer systems.\nHowever, unlike the structural architecture of a building, software architecture must constantly change and adapt to meet the new demands of today’s business and technology environment.\nOne aspect of architectural modularity is breaking large monolithic applications into separate and smaller parts to provide more capacity for further scalability and growth, while at the same time facilitating constant and rapid change.\nThe water glass analogy is a great way of explaining architectural modularity (the breaking up of monolithic applications) to business stakeholders and C-level executives, who will inevitably be paying for the architecture-refactoring effort.\nEnabling technology to move as fast as the business (or, conversely, preventing technology from slowing the business) requires a certain level of architectural agility.\nThe primary business drivers for breaking applications into smaller parts include speed-to-market (sometimes called time-to-market) and achieving a level of competitive advantage in the marketplace.\nBusinesses must be agile to survive in today’s fast-paced and ever-changing volatile market, meaning the underlying architectures must be agile as well.\nAs illustrated in Figure 3-3, the five key architectural characteristics to support agility, speed-to- market, and, ultimately, competitive advantage in today’s marketplace are availability (fault tolerance), scalability, deployability, testability, maintainability.\nMaintainability, testability, and deployability (defined in the following sections) can also be achieved through monolithic architectures such as a modular monolith or even a microkernel architecture (see Appendix B for a list of references providing more information about these architecture styles).\nBoth of these architec‐ ture styles offer a level of architectural modularity based on the way the components are structured.\nFor example, with a modular monolith, components are grouped into well-formed domains, providing for what is known as a domain partitioned architec‐ ture (see Fundamentals of Software Architecture, Chapter 8, page 103).\nWithin the context of architecture, we are defining a component as an architectural building block of the application that does some sort of business or infrastructure function, usually manifested through a package structure (Java), namespace (C#), or physical grouping of files (classes) within some sort of directory structure.\nLarge monolithic architectures generally have low levels of maintainability due to the technical partitioning of functionality into layers, the tight coupling between compo‐ nents, and weak component cohesion from a domain perspective.\nFor example, con‐ sider a new requirement within a traditional monolithic layered architecture to add an expiration date to items contained in a customer’s wish list (items in a list to maybe purchase at a later time).\nNotice in Figure 3-4 that the change scope of the new requirement is at an application level since the change is propagated to all of the layers within the application.\nWith monolithic layered architectures, change is at an application level\nDepending on the team structure, implementing this simple change to add an expira‐ tion date to wish list items in a monolithic layered architecture could possibly require the coordination of at least three teams:\nNotice that with a distributed service-based architecture, as shown in Figure 3-5, the change scope of the new requirement is at a domain level within a par‐ ticular domain service, making it easier to isolate the specific deployment unit requir‐ ing the change.\nMoving to even more architectural modularity such as a microservices architecture, as illustrated in Figure 3-6, places the new requirement at a function-level change\nWith service-based architectures, change is at a domain level\nWith microservices architectures, change is at a function level",
      "keywords": [
        "Architectural Modularity Tuesday",
        "Architectural Modularity",
        "application",
        "business",
        "Modularity",
        "change",
        "Architectural",
        "architecture",
        "Modularity Tuesday",
        "Modularity Drivers",
        "Sysops Squad application",
        "Austen",
        "Addison",
        "level",
        "Business drivers"
      ],
      "concepts": [
        "business",
        "businesses",
        "architectural",
        "architecture",
        "changing",
        "application",
        "applications",
        "level",
        "components",
        "domains"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 10,
          "title": "",
          "score": 0.821,
          "base_score": 0.671,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 16,
          "title": "",
          "score": 0.766,
          "base_score": 0.616,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 5,
          "title": "",
          "score": 0.683,
          "base_score": 0.533,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 12,
          "title": "",
          "score": 0.671,
          "base_score": 0.521,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 41,
          "title": "",
          "score": 0.664,
          "base_score": 0.514,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "business",
          "modularity",
          "level",
          "application",
          "architectural modularity"
        ],
        "semantic": [],
        "merged": [
          "business",
          "modularity",
          "level",
          "application",
          "architectural modularity"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3605555014484831,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.167478+00:00"
      }
    },
    {
      "chapter_number": 9,
      "title": "Segment 9 (pages 72-79)",
      "start_page": 72,
      "end_page": 79,
      "summary": "Large monolithic architecture styles like the layered architecture support relatively low levels of testability (and hence agility) due to the difficulty in achieving full and complete regression testing of all features within the large deploy‐ ment unit.\nArchitectural modularity—the breaking apart of applications into smaller deploy‐ ment units—significantly reduces the overall testing scope for changes made to a ser‐ vice, allowing for better completeness of testing as well as ease of testing.\nWhile architectural modularity generally improves testability, it can sometimes lead to the same problems that exist with monolithic, single-deployment applications.\nDeploying software every two weeks (or more) not only increases the overall risk of deployment (due to grouping multiple changes together), but in most cases unnecessarily delays new features or bug fixes that are ready to be pushed out to customers.\nMonolithic architectures generally support low levels of deployability due to the amount of ceremony involved in deploying the application (such as code freezes, mock deployments, and so on), the increased risk that something else might break once new features or bug fixes are deployed, and a long time frame between deploy‐ ments (weeks to months).\nApplications having a certain level of architectural modu‐ larity in terms of separately deployed units of software have less deployment ceremony, less risk of deployment, and can be deployed more frequently than a large, single monolithic application.\nDeploy‐ ment risk is increased, and it becomes more difficult to deploy a simple change for fear of breaking other services.\nAlthough both scalability and elasticity improve with finer-grained services, elasticity is more a function of granularity (the size of a deployment unit), whereas scalability is more a function of modularity (the breaking apart of applications into separate deployment units).\nLarge monolithic layered architectures are both difficult and expensive to scale because all of the application functionality must scale to the same degree (application-level scalability and poor MTTS).\nHowever, with service-based architecture, notice that scalability improves, but not as much as elasticity.\nThis is because domain services in a service-based architecture are coarse grained and usually contain the entire domain in one deployment unit (such as order processing or warehouse management), and generally have too long of a mean time to startup (MTTS) to respond fast enough to immediate demand for elasticity due to their large size (domain-level scalability and fair MTTS).\nNotice that with microservices, both scalability and elasticity are maxi‐ mized because of the small, single-purpose, fine-grained nature of each separately deployed service (function-level scalability and excellent MTTS).\nLike testability and deployability, the more services communicate with one other to complete a single business transaction, the greater the negative impact on scalability and elasticity.\nArmed with a better understanding of what is meant by architectural modularity and the corresponding drivers for breaking apart a system, Addison and Austen met to discuss the Sysops Squad issues and try to match them to modularity drivers in order to build a solid business justification to present to the business sponsors.\n“Let’s take each of the issues we are facing and see if we can match them to some of the modularity drivers,” said Addison.\n“That way, we can demonstrate to the business that breaking apart the application will in fact address the issues we are facing.”\n“And the developers are constantly complaining that the codebase is too large, and it’s difficult to find the right place to apply changes to new features or bug fixes,” said Addison.\n“So, by breaking apart the application, it would not only decouple the code, but it would isolate and partition the functionality into separately deployed services, making it easier for developers to apply changes.”\n“Testability is another key characteristic related to this problem, but we have that covered already because of all our automated unit tests,” said Austen.\nAddison also explained that the developers were continually complaining that the entire unit test suite had to be run for any change (big or small), which not only took a long time, but developers were faced with having to fix issues not related to their change.\n“Testability is about the ease of testing, but also the completeness of testing,” said Addison.\nchanges made to the application, group relevant automated unit tests together, and get better completeness of testing—hence fewer bugs.”\n“I agree,” said Austen, “and besides, the mock deployments and code freezes we do for each release take up valuable time—time we don’t have.\n“I see what you mean,” said Austen, “and while I agree with you, I still maintain that at some point we will have to modify our current deployment pipeline as well.”\nSatisfied that breaking apart the Sysops Squad application and moving to a distributed architecture would address the change issues, Addison and Austen moved on to the other business sponsor concerns.\n“So, we are in agreement then that overall availability through fault tolerance will address the application not always being available for the customers since they only interact with the ticketing portion of the system.”\n“So,” said Addison, “it appears we have both a scalability and a database load issue here.”\n“And get this—by breaking up the application and the monolithic database, we can segregate reporting into its own system and also provide the added scalability for the customer-facing ticketing functionality.”\nContext The Sysops Squad is currently a monolithic problem ticket application that supports many different business functions related to problem tickets, including customer registration, problem ticket entry and processing, operations and analytical reporting, billing and pay- ment processing, and various administrative maintenance functions.",
      "keywords": [
        "Architectural modularity",
        "Sysops Squad",
        "system",
        "Sysops Squad application",
        "Addison",
        "Scalability",
        "application",
        "modularity",
        "services",
        "Sysops Squad Saga",
        "architecture",
        "Austen",
        "deployment",
        "architectural",
        "business"
      ],
      "concepts": [
        "deployment",
        "deployments",
        "architectural",
        "architectures",
        "architecturally",
        "application",
        "applications",
        "tickets",
        "tested",
        "change"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 11,
          "title": "",
          "score": 0.712,
          "base_score": 0.562,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 4,
          "title": "",
          "score": 0.698,
          "base_score": 0.548,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 13,
          "title": "",
          "score": 0.646,
          "base_score": 0.496,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 10,
          "title": "",
          "score": 0.644,
          "base_score": 0.494,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 1,
          "title": "",
          "score": 0.626,
          "base_score": 0.476,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "application",
          "modularity",
          "deployment",
          "testing",
          "scalability"
        ],
        "semantic": [],
        "merged": [
          "application",
          "modularity",
          "deployment",
          "testing",
          "scalability"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.36380086338316264,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.167524+00:00"
      }
    },
    {
      "chapter_number": 10,
      "title": "Segment 10 (pages 80-87)",
      "start_page": 80,
      "end_page": 87,
      "summary": "Now that Addison and Austen had the go-ahead to move to a distributed architecture and break apart the monolithic Sysops Squad application, they needed to determine the best approach for how to get started.\nSo let’s use the same principle with the Sysops Squad application,” said Austen.\n“That might be a good start,” said Addison, “but what about the data?\nAddison and Austen met with Logan to discuss some of the approaches they were considering for how to break apart the application.\n“The approach you’re suggesting,” said Logan, “is what is known as the Elephant Migration Anti- Pattern.\nEating the elephant one bite at a time may seem like a good approach at the start, but in most cases it leads to an unstructured approach that results in a big ball of distributed mud, what some people also call a distributed monolith.\nAre there patterns we can use to break apart the application?” asked Addison.\n“You need to take a holistic view of the application and apply either tactical forking or component- based decomposition,” said Logan.\nWhereas architectural modularity describes the why for breaking apart a monolithic application, architectural decomposition describes the how.\nBreaking apart large, complex monolithic applications can be a complex and time-consuming undertaking, and it’s important to know whether it is even feasible to begin such an effort and how to approach it.\nComponent-based decomposition and tactical forking are two common approaches for breaking apart monolithic applications.\nComponent-based decomposition is an extraction approach that applies various refactoring patterns for refining and extract‐ ing components (the logical building blocks of an application) to form a distributed architecture in an incremental and controlled fashion.\nOne of the main factors in selecting a decomposition approach is how well the existing monolithic application code is structured.\nDo clear components and component boundaries exist within the codebase, or is the codebase largely an unstructured big ball of mud?\nAs the flowchart in Figure 4-1 illustrates, the first step in an architecture decomposi‐ tion effort is to first determine whether the codebase is even decomposable.\nIf the codebase is decomposable, the next step is to determine if the source code is largely an unstructured mess with no clearly definable components.\nloosely defined) components, then a component-based decomposition approach (see “Component-Based Decomposition” on page 71) is the way to go.\nWe describe both of these approaches in this chapter, and then devote an entire chap‐ ter (Chapter 5) to describing each of the component-based decomposition patterns in detail.\nHowever, archi‐ tects do have tools to help determine macro characteristics of a codebase, particularly coupling metrics, to help evaluate internal structure.\nAbstractness and Instability Robert Martin, a well-known figure in the software architecture world, created some derived metrics for a C++ book in the late 1990s that are applicable to any object- oriented language.\nA component with an instability value near one is highly unstable, a value close to zero may be either stable or rigid: it is stable if the module or component contains mostly abstract elements, and rigid if it comprises mostly concrete elements.\nDistance from the Main Sequence One of the few holistic metrics architects have for architectural structure is distance from the main sequence, a derived metric based on instability and abstractness, shown in Equation 4-3.\nIn the equation, A = abstractness and I = instability.\nThe distance-from-the-main-sequence metric imagines an ideal relationship between abstractness and instability; components that fall near this idealized line exhibit a healthy mixture of these two competing concerns.\nFor example, graphing a particular component allows developers to calculate the distance-from-the-main-sequence met‐ ric, illustrated in Figure 4-3.\nComponents that fall too far into the upper-right corner enter into what architects call the zone of uselessness: code that is too abstract becomes difficult to use.",
      "keywords": [
        "codebase",
        "Austen",
        "Architectural Decomposition",
        "Decomposition",
        "Sysops Squad application",
        "Addison",
        "approach",
        "Codebase Decomposable",
        "component",
        "Instability",
        "application",
        "Addison and Austen",
        "Sysops Squad",
        "coupling",
        "decomposition approach"
      ],
      "concepts": [
        "approach",
        "approaches",
        "components",
        "architecture",
        "architectural",
        "metrics",
        "coupling",
        "abstractness",
        "abstraction",
        "decomposition"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 12,
          "title": "",
          "score": 0.85,
          "base_score": 0.7,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 16,
          "title": "",
          "score": 0.843,
          "base_score": 0.693,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 8,
          "title": "",
          "score": 0.821,
          "base_score": 0.671,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 11,
          "title": "",
          "score": 0.775,
          "base_score": 0.625,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 5,
          "title": "",
          "score": 0.681,
          "base_score": 0.531,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "decomposition",
          "instability",
          "approach",
          "component",
          "codebase"
        ],
        "semantic": [],
        "merged": [
          "decomposition",
          "instability",
          "approach",
          "component",
          "codebase"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3986053674711053,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.167571+00:00"
      }
    },
    {
      "chapter_number": 11,
      "title": "Segment 11 (pages 88-95)",
      "start_page": 88,
      "end_page": 95,
      "summary": "Following the flowchart in Figure 4-1, once an architect decides that the codebase is decomposable, the next step is to determine what approach to take to decompose the application.\nThe following sections describe the two approaches for decomposing an application: component-based decomposition and tactical forking.\nComponent-Based Decomposition It has been our experience that most of the difficulty and complexity involved with migrating monolithic applications to highly distributed architecture like microservi‐ ces comes from poorly defined architectural components.\nThroughout many collective years of migrating monolithic applications to distributed architectures (such as microservices), we’ve developed a set of component-based decomposition patterns described in Chapter 5 that help prepare a monolithic appli‐ cation for migration.\nThese patterns involve the refactoring of source code to arrive at a set of well-defined components that can eventually become services, easing the effort needed to migrate applications to distributed architectures.\nThese component-based decomposition patterns essentially enable the migration of a monolithic architecture to a service-based architecture, which is defined in Chapter 2 and described in more detail in Fundamentals of Software Architecture.\nService-based architecture is a hybrid of the microservices architecture style where an application is broken into domain services, which are coarse-grained, separately deployed services containing all of the business logic for a particular domain.\nService-based architecture does not require the database to be broken apart, therefore allowing architects to focus on the domain and functional partitioning prior to tackling database decomposition (discussed in detail in Chapter 6).\nWhen migrating monolithic applications to microservices, con‐ sider moving to a service-based architecture first as a stepping- stone to microservices.\nIn Figure 4-6, developers have to constantly deal with the exuberant strands of cou‐ pling that define this architecture; as they extract pieces, they discover that more and more of the monolith must come along because of dependencies.\nFor this decomposition approach, the system starts as a single monolithic application, as shown in Figure 4-8.\nThe first step in tactical forking involves cloning the entire monolith, and giving each team a copy of the entire codebase, as illustrated in Figure 4-9.\nEach team receives a copy of the entire codebase, and they start deleting (as illustrated previously in Figure 4-7) the code they don’t need rather than extract the desirable code.\nAt the completion of the tactical forking pattern, teams have split the original mono‐ lithic application into two parts, preserving the coarse-grained structure of the behav‐ ior in each part, as illustrated in Figure 4-11.",
      "keywords": [
        "tactical forking",
        "decomposition",
        "Architectural Decomposition",
        "service-based architecture",
        "architecture",
        "forking",
        "tactical forking pattern",
        "component-based decomposition",
        "tactical",
        "application",
        "monolithic applications",
        "structure",
        "codebase",
        "internal structure",
        "code"
      ],
      "concepts": [
        "architectural",
        "architecture",
        "architects",
        "monolithic",
        "monolith",
        "components",
        "code",
        "domain",
        "services",
        "teams"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 12,
          "title": "",
          "score": 0.858,
          "base_score": 0.708,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 10,
          "title": "",
          "score": 0.775,
          "base_score": 0.625,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 16,
          "title": "",
          "score": 0.737,
          "base_score": 0.587,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 1,
          "title": "",
          "score": 0.719,
          "base_score": 0.569,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 9,
          "title": "",
          "score": 0.712,
          "base_score": 0.562,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "decomposition",
          "tactical",
          "forking",
          "based",
          "based decomposition"
        ],
        "semantic": [],
        "merged": [
          "decomposition",
          "tactical",
          "forking",
          "based",
          "based decomposition"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.40774062191023,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.167619+00:00"
      }
    },
    {
      "chapter_number": 12,
      "title": "Segment 12 (pages 96-104)",
      "start_page": 96,
      "end_page": 104,
      "summary": "Since we have good component boundaries already defined, I’m OK with doing the slower component-based decomposition approach and giving up my sculpting career.\nAddison and Austen came to an agreement that the component decomposition approach would be the appropriate one for the Sysops Squad application.\nAddison wrote an ADR for this decision, outlining the trade-offs and justification for the component-based decomposition approach.\nADR: Migration Using the Component-Based Decomposition Approach\nThe two approaches we considered for the migration to a distributed architecture were tactical forking and component-based decomposition.\nDecision We will use the component-based decomposition approach to migrate the existing mon- olithic Sysops Squad application to a distributed architecture.\nThe application has well-defined component boundaries, component-based decomposition approach.\nWith the component-based decomposition approach, the service definitions will naturally emerge through compo- nent grouping.\nGiven the nature of the problems we are facing with the current application with regard to reliability, availability, scalability, and workflow, using the component-based decomposi- tion approach provides a safer and more controlled incremental migration than the tacti- cal forking approach does.\nConsequences The migration effort will likely take longer with the component-based decomposition approach than with tactical forking.\nCHAPTER 5 Component-Based Decomposition Patterns\nAddison and Austen chose to use the component-based decomposition approach, but were unsure about the details of each decomposition pattern.\nYou talked earlier about component-based decomposition, and we chose that approach, but we aren’t able to find much about it on the internet.”\nComponent-based decomposition (introduced in Chapter 4) is a highly effective technique for breaking apart a monolithic application when the codebase is some‐ what structured and grouped by namespaces (or directories).\nThis chapter introduces a set of patterns, known as component-based decomposition patterns, that describe the refactoring of monolithic source code to arrive at a set of well-defined components that can eventually become services.\nFigure 5-1 shows the road map for the component-based decomposition patterns described in this chapter and how they are used together to break apart a monolithic application.\nThis pattern is used to identify, manage, and properly size components.\nChapter 5: Component-Based Decomposition Patterns\nComponent-based decomposition pattern flow and usage\nComponent-Based Decomposition Patterns\nIdentify and Size Components Pattern The first step in any monolithic migration is to apply the Identify and Size Compo‐ nents pattern.\nThe purpose of this pattern is to identify and catalog the architectural components (logical building blocks) of the application and then properly size the components.\nPattern Description Because services are built from components, it is critical to not only identify the com‐ ponents within an application, but to properly size them as well.\nOne metric we’ve found useful for component sizing is calculating the total number of statements within a given component (the sum of statements within all source files contained within a namespace or directory).\nChapter 5: Component-Based Decomposition Patterns\nFor example, the component Billing History shown in Table 5-1 is clearly a component that contains source code files used to manage a customer’s billing history.\nIdentify and Size Components Pattern\nFor example, the component namespace for source code files in the ss/customer/notification directory structure would have the namespace value ss.customer.notification.\nThe relative size of the component based on its percentage of the overall source code containing that component.\nThe percent metric is helpful in identifying components that appear too large or too small in the overall application.\nThis metric is calculated by taking the total number of statements within the source code files representing that component and dividing that number by the total number of statements in the entire codebase of the application.\nChapter 5: Component-Based Decomposition Patterns",
      "keywords": [
        "Component-Based Decomposition Patterns",
        "Sysops Squad application",
        "component-based decomposition approach",
        "component",
        "component-based decomposition",
        "Decomposition Patterns",
        "Size Components Pattern",
        "Decomposition Approach",
        "Sysops Squad",
        "pattern",
        "Decomposition",
        "Components Pattern",
        "application",
        "Sysops Squad Saga",
        "Squad application"
      ],
      "concepts": [
        "component",
        "pattern",
        "code",
        "architectures",
        "architectural",
        "approach",
        "approaches",
        "services",
        "files",
        "identify"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 11,
          "title": "",
          "score": 0.858,
          "base_score": 0.708,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 10,
          "title": "",
          "score": 0.85,
          "base_score": 0.7,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 5,
          "title": "",
          "score": 0.712,
          "base_score": 0.562,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 16,
          "title": "",
          "score": 0.704,
          "base_score": 0.554,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 2,
          "title": "",
          "score": 0.671,
          "base_score": 0.521,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "component based",
          "component",
          "based decomposition",
          "decomposition",
          "based"
        ],
        "semantic": [],
        "merged": [
          "component based",
          "component",
          "based decomposition",
          "decomposition",
          "based"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.41974791917904347,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.167666+00:00"
      }
    },
    {
      "chapter_number": 13,
      "title": "Segment 13 (pages 105-114)",
      "start_page": 105,
      "end_page": 114,
      "summary": "Fitness Functions for Governance Once this decomposition pattern has been applied and components have been identi‐ fied and sized correctly, it’s important to apply some sort of automated governance to identify new components and to ensure components don’t get too large during nor‐ mal application maintenance and create unwanted or unintended dependencies.\nAutomated holistic fitness functions can be triggered during deployment to alert the architect if specified constraints are exceeded (such as the percent metric discussed previously or use of standard deviations to identify outliers).\nFitness function: Maintain component inventory\nThis automated holistic fitness function, usually triggered on deployment through a CI/CD pipeline, helps keep the inventory of components current.\nIdentify and Size Components Pattern\nFitness function: No component shall exceed <some percent> of the overall codebase\nThis automated holistic fitness function, usually triggered on deployment through a CI/CD pipeline, identifies components that exceed a given threshold in terms of the percentage of overall source code represented by that component, and alerts the architect if any component exceeds that threshold.\nPseudocode for maintaining component size based on percent of code\nFitness function: No component shall exceed <some number of standard deviations> from the mean component size\nThis automated holistic fitness function, usually triggered on deployment through a CI/CD pipeline, identifies components that exceed a given threshold in terms of the number of standard deviations from the mean of all component sizes (based on the total number of statements in the component), and alerts the architect if any compo‐ nent exceeds that threshold.\nPseudocode for maintaining component size based on number of standard deviations\n# Walk through all of the source code to accumulate total statements and number # of statements per component SET total_statements TO 0 MAP component_size_map FOREACH component IN component_list { num_statements = accumulate_statements(component) ADD num_statements TO total_statements ADD component,num_statements TO component_size_map }\n# Calculate the standard deviation SET square_diff_sum TO 0 num_components = get_num_entries(component_list) mean = total_statements / num_components FOREACH component,size IN component_size_map { diff = size - mean ADD square(diff) TO square_diff_sum } std_dev = square_root(square_diff_sum / (num_components - 1))\nIdentify and Size Components Pattern\nAfter the discussion with Logan (the lead architect) about component-based decompo- sition patterns, Addison decided to apply the Identify and Size Components pattern to identify all of the components in the Sysops Squad ticketing application and calculate the size of each component based on the total number of statements in each component.\nss.reporting\nss.ticket\nAddison noticed that most of the components listed in Table 5-2 are about the same size, with the exception of the Reporting component (ss.reporting) which consisted of 33% of the codebase.\nSince the Reporting component was significantly larger than the other components (illustrated in Figure 5-2), Addison chose to break this component apart to reduce its overall size.\nAfter doing some analysis, Addison found that the reporting component contained source code that implemented three categories of reports:\nIdentify and Size Components Pattern\nSydney, one of the Sysops Squad developers assigned the architecture story, refactored the code to break apart the single Reporting component into four separate compo- nents—a Reporting Shared component containing the common code and three other components (Ticket Reports, Expert Reports, and Financial Reports), each representing a functional reporting area, as illustrated in Figure 5-3.\nss.reporting.shared\nss.reporting.tickets\nss.reporting.experts\nss.reporting.financial\nss.ticket\nAlthough the namespace still exists (ss.report ing), it is no longer considered a component, but rather a subdomain.\nThe refactored components listed in Table 5-3 will be used when applying the next decomposition pattern, Gather Common Domain Components.\nIdentify and Size Components Pattern\nGather Common Domain Components Pattern When moving from a monolithic architecture to a distributed one, it is often benefi‐ cial to identify and consolidate common domain functionality to make common services easier to identify and create.\nOne hint that common domain processing exists in the application is the use of shared classes across components or a common inheritance structure used by multiple components.\nAnother way of identifying common domain functionality is through the name of a logical component or its corresponding namespace.\nNotice how each of these components (Ticket Auditing, Billing Auditing, and Survey Auditing) all have the same thing in common—writing the action performed and the user requesting the action to an audit table.\nThis common domain into a new component called penulti functionality can be consolidated mate.ss.shared.audit, resulting in less duplication of code and also fewer services in the resulting distributed architecture.\nFitness function: Find common names in leaf nodes of component namespace\nThis automated holistic fitness function can be triggered on deployment through a CI/CD pipeline to locate common names within the namespace of a component.\nGather Common Domain Components Pattern\nFitness function: Find common code across components\nThis automated holistic fitness function can be triggered on deployment through a CI/CD pipeline to locate common classes used between namespaces.\nLike the previous fitness function, an exclusion file is used to reduce the number of “false positives” for known common code that is not considered duplicate domain logic.\nPseudocode for finding common source files between components",
      "keywords": [
        "component",
        "Size Components Pattern",
        "LIST",
        "Fitness function",
        "common",
        "Components Pattern",
        "Common Domain Components",
        "Common Domain",
        "component size",
        "LIST component",
        "Reporting component",
        "Size",
        "Domain Components Pattern",
        "Fitness",
        "common domain functionality"
      ],
      "concepts": [
        "component",
        "common",
        "reporting",
        "list",
        "functions",
        "function",
        "functional",
        "shared",
        "ticketing",
        "identify"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 4,
          "title": "",
          "score": 0.815,
          "base_score": 0.665,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 9,
          "title": "",
          "score": 0.646,
          "base_score": 0.496,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 12,
          "title": "",
          "score": 0.627,
          "base_score": 0.477,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 2,
          "title": "",
          "score": 0.603,
          "base_score": 0.453,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 45,
          "title": "",
          "score": 0.585,
          "base_score": 0.435,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "component",
          "components",
          "size",
          "fitness",
          "fitness function"
        ],
        "semantic": [],
        "merged": [
          "component",
          "components",
          "size",
          "fitness",
          "fitness function"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30806702883656395,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.167706+00:00"
      }
    },
    {
      "chapter_number": 14,
      "title": "Segment 14 (pages 115-125)",
      "start_page": 115,
      "end_page": 125,
      "summary": "Figure 5-4 illustrates these common notification components within the Sysops Squad application.\nAddison then found that if the customer notification functionality was consolidated into a single component, the coupling level for the resulting single component increased to an incoming cou- pling level of 5, as shown in Table 5-6.\nIn some cases, combining common domain functionality into a single consolidated component increased the incoming coupling level of that component, thus resulting in too many dependencies on a single shared component within the application.\nAddison wrote an architecture story to combine all of the notification functionality into a single namespace representing a common Notification component.\nSydney, assigned to the architecture story, refactored the source code, creating a single component for customer notification, as illustra- ted in Figure 5-5.\nNotice that the Customer Notification component (ss.customer.notification), Ticket Notify component (ss.ticket.notify), and Survey Notify components (ss.survey.notify) were removed, and the source code moved to the new consolidated Notification component ( ss.notification).\nss.reporting.tickets\nss.ticket\nss.survey\nss.survey.templates\nFlatten Components Pattern As mentioned previously, components—the building blocks of an application—are usually identified through namespaces, package structures, or directory structures and are implemented through class files (or source code files) contained within these structures.\nThe Flatten Components pattern is used to ensure that components are not built on top of one another, but rather flattened and represented as leaf nodes in a directory structure or namespace.\nTo illustrate this point, consider the customer survey functionality within the Sysops Squad application represented by two components: Survey (ss.survey) and Survey Templates (ss.survey.templates).\nNotice in Table 5-8 how the ss.survey name‐ space, which contains five class files used to manage and collect the surveys, is exten‐ ded with the ss.survey.templates namespace to include seven classes representing each survey type send out to customers.\nThe Survey component contains orphaned classes and should be flattened\nComponent namespace\nss.survey\nWith this definition, ss.survey.templates is a component, whereas ss.survey would be considered a subdomain, not a compo‐ nent.\nWe further define namespaces such as ss.survey as root namespaces because they are extended with other namespace nodes (in this case, .templates).\nNotice how the ss.survey root namespace in Table 5-8 contains five class files.\nRecall that a component is identified by a leaf node namespace containing source code.\nBecause the ss.survey namespace was extended to include .templates, ss.survey is no longer considered a component and therefore should not contain any class files.\nA collection of classes grouped within a leaf node namespace that performs some sort of specific functionality in the application (such as payment processing or customer survey functionality).\nFor example, given the namespaces ss.survey and ss.survey.templates, ss.sur vey would be considered a root namespace because it is extended by .templates.\nComponents, root namespaces, and orphaned classes (C box denotes source code)\nNotice that since both ss.survey and ss.ticket are extended through other name‐ space nodes, those namespaces are considered root namespaces, and the classes con‐ tained in those root namespaces are hence orphaned classes (belonging to no defined\nThus, the only components denoted in Figure 5-6 are ss.survey.tem plates, ss.login, ss.ticket.assign, and ss.ticket.route.\nThe Flatten Components decomposition pattern is used to move orphaned classes to create well-defined components that exist only as leaf nodes of a directory or name‐ space, creating well-defined subdomains (root namespaces) in the process.\nWe refer to the flattening of components as the breaking down (or building up) of namespaces within an application to remove orphaned classes.\nFor example, one way of flattening the ss.survey root namespace in Figure 5-6 and remove orphaned classes is to move the source code contained in the ss.survey.templates namespace down to the ss.survey namespace, thereby making ss.survey a single component (.survey is now the leaf node of that namespace).\nAlternatively, flattening could also be applied by taking the source code in ss.survey and applying functional decomposition or domain-driven design to identify separate functional areas within the root namespace, thus forming components from those functional areas.\nFor example, suppose the functionality within the ss.survey name‐ space creates and sends a survey to a customer, and then processes a completed sur‐ vey received from the customer.\nTwo components could be created from the ss.survey namespace: ss.survey.create, which creates and sends the survey, and ss.survey.process, which processes a survey received from a customer.\nSurvey is flattened by moving the orphaned classes to new leaf nodes ( components)\nRegardless of the direction of flattening, make sure source code files reside only in leaf node namespaces or directories so that source code can always be identified within a specific component.\nAnother common scenario where orphaned source code might reside in a root name‐ space is when code is shared by other components within that namespace.\nConsider the example in Figure 5-9 where customer survey functionality resides in three com‐ ponents (ss.survey.templates, ss.survey.create, and ss.survey.process), but common code (such as interfaces, abstract classes, common utilities) resides in the root namespace ss.survey.\nThe shared classes in ss.survey would still be considered orphaned classes, even though they represent shared code.\nApplying the Flatten Components pattern would move those shared orphaned classes to a new component called ss.survey.shared, therefore removing all orphaned classes from the ss.survey subdomain, as illustra‐ ted in Figure 5-10.\nShared survey code is moved into its own component\nAfter applying the “Gather Common Domain Components Pattern” on page 94, Addison analyzed the results in Table 5-7 and observed that the Survey and Ticket components contained orphaned classes.",
      "keywords": [
        "Flatten Components Pattern",
        "Sysops Squad",
        "components",
        "Domain Components pattern",
        "Components pattern",
        "Sysops Squad components",
        "Common Domain Components",
        "Gathering Common Components",
        "Common Components Friday",
        "Survey",
        "Flatten Components",
        "Components decomposition pattern",
        "Namespace",
        "Flatten Components decomposition",
        "Sysops Squad application"
      ],
      "concepts": [
        "components",
        "survey",
        "functionality",
        "functions",
        "function",
        "notification",
        "shared",
        "classes",
        "common",
        "customer"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 5,
          "title": "",
          "score": 0.747,
          "base_score": 0.597,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 15,
          "title": "",
          "score": 0.734,
          "base_score": 0.584,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 45,
          "title": "",
          "score": 0.622,
          "base_score": 0.472,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 12,
          "title": "",
          "score": 0.611,
          "base_score": 0.461,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 2,
          "title": "",
          "score": 0.599,
          "base_score": 0.449,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ss",
          "survey",
          "ss survey",
          "components",
          "namespace"
        ],
        "semantic": [],
        "merged": [
          "ss",
          "survey",
          "ss survey",
          "components",
          "namespace"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3596343024300405,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.167761+00:00"
      }
    },
    {
      "chapter_number": 15,
      "title": "Segment 15 (pages 126-147)",
      "start_page": 126,
      "end_page": 147,
      "summary": "The Survey and Ticket components contain orphaned classes and should be flattened\nSysops Squad Ticket and Survey components should be flattened\nAddison decided to address the ticketing components first.\nKnowing that flattening components meant getting rid of source code in nonleaf nodes, Addison had two choices: consolidate the code contained in the ticket assignment and ticket routing components into the ss.ticket component, or break up the 45 classes in the ss.ticket component into separate components, thus making ss.ticket a subdomain.\nAddison discussed these options with Sydney (one of the Sysops Squad developers), and based on the complexity and frequent changes in the ticket assignment function- ality, decided to keep those components separate and move the orphaned code from the ss.ticket root namespace into other namespaces, thus forming new components.\nWith help from Sydney, Addison found that the 45 orphaned classes contained in the ss.ticket namespace implemented the following ticketing functionality:\nSince ticket assignment and ticket routing functionality were already in their own components (ss.ticket.assign and ss.ticket.route, respectively), Addison created an architecture story to move the source code contained in the ss.ticket namespace to three new components, as shown in Table 5-10.\nThe prior Sysops Squad Ticket component broken into three new components\nWith this information, Addison created an architec- ture story to move the seven class files from ss.survey.templates into the ss.survey namespace and removed the ss.survey.template component, as shown in Table 5-11.\nAfter applying the Flatten Components pattern (illustrated in Figure 5-12), Addison observed that there were no “hills” (component upon component) or orphaned classes and that all of the compo- nents were contained only in the leaf nodes of the corresponding namespace.\nDetermine Component Dependencies Pattern Three of the most common questions asked when considering a migration from a monolithic application to a distributed architecture are as follows:\nDetermine Component Dependencies Pattern\nPattern Description The purpose of the Determine Component Dependencies pattern is to analyze the incoming and outgoing dependencies (coupling) between components to determine what the resulting service dependency graph might look like after breaking up the monolithic application.\nFor example, suppose the CustomerSurvey class in the ss.survey component invokes a method in the CustomerNotification class in the ss.notifica tion component to send out the customer survey, as illustrated in the pseudocode in Example 5-7.\nNotice the dependency between the Survey and Notification components, because the CustomerNotification class used by the CustomerSurvey class resides outside the ss.survey namespace.\nNotice there is only a single dependency between the com‐ ponents in this diagram, making this application a good candidate for breaking apart since the components are functionally independent from one another.\nA monolithic application with minimal component dependencies takes less effort to break apart (golf ball sizing)\nDetermine Component Dependencies Pattern\nA monolithic application with a high number of component dependencies takes more effort to break apart (basketball sizing)\nA monolithic application with too many component dependencies is not feasible to break apart (airliner sizing)\nIn essence these diagrams form a radar from which to determine where the enemy (high component coupling) is located, and also paint a picture of what the resulting service dependency matrix might look like if the monolithic application were to be broken into a highly distributed architecture.\nThis pattern is useful not only for identifying the overall level of component coupling in an application, but also for determining dependency refactoring opportunities prior to breaking apart the application.\nFor example, assume component A has an afferent coupling level of 20 (meaning, 20 other components are dependent on the functionality of the compo‐ nent).\nFitness function: No component shall have more than <some number> of total dependencies\nFitness function: <some component> should not have a dependency on <another component>\nExample 5-9 shows an exam‐ ple using ArchUnit for ensuring that the Ticket Maintenance component (ss.ticket.maintenance) does not have a dependency on the Expert Profile compo‐ nent (ss.expert.profile).\nAfter reading about the Determine Component Dependencies pattern, Addison won- dered what the Sysops Squad application dependency matrix looked like and whether it was feasible to even break the application apart.\nAddison used an IDE plug-in to gener- ate a component dependency diagram of the current Sysops Squad application.\nInitially, Addison felt a bit discouraged because Figure 5-16 showed a lot of dependencies between the Sysops Squad application components.\nComponent dependencies in the Sysops Squad application\nHowever, Addison also saw lots of dependencies within the Ticketing and Reporting components.\nBoth of these domain areas have a specific component for shared code (interfaces, helper classes, entity classes, and so on).\nReal- izing that both the ticketing and reporting shared code contains mostly compile-based class refer- ences and would likely be implemented as shared libraries rather than services, Addison filtered out these components to get a better view of the dependencies between the core functionality of the application, which is illustrated in Figure 5-17.\nComponent dependencies in the Sysops Squad application without shared library dependencies\nAddison showed these results to Austen, and they both agreed that most of the components were relatively self-contained and it appeared that the Sysops Squad application was a good candidate for breaking apart into a distributed architecture.\nCreate Component Domains Pattern While each component identified within a monolithic application can be considered a possible candidate for a separate service, in most cases the relationship between a service and components is a one-to-many relationship—that is, a single service may contain one or more components.\nThe purpose of the Create Component Domains pattern is to logically group components together so that more coarse-grained domain services can be created when breaking up an application.\nCreating component domains is an effective way of determining what will eventually become domain services in a service-based architecture.\nComponent domains are physically manifested in an application through component namespaces (or directories).\nThis technique is illustrated in Figure 5-18, where the second node in the namespace (.customer) refers to the domain, the third node represents a subdomain under the customer domain (.billing), and the leaf node (.payment) refers to the component.\nComponent domains are identified through the namespace nodes\nFor example, consider the components listed in Table 5-13 that make up the Customer domain within the Sysops Squad application.\nComponents related to the Customer domain before refactoring\nCreate Component Domains Pattern\nTo properly identify the Customer domain (manifested through the namespace ss.customer), the namespaces for the Billing Payment, Billing History, and Support Contract components would have to be modified to add the .customer node at the beginning of the namespace, as shown in Table 5-14.\nComponents related to the Customer domain after refactoring\nNotice in the prior table that all of the customer-related functionality (billing, profile maintenance, and support contract maintenance) is now grouped under .customer, aligning each component with that particular domain.\nFitness Functions for Governance Once refactored, it’s important to govern the component domains to ensure that namespace rules are enforced and that no code exists outside the context of a compo‐ nent domain or subdomain.\nThe following automated fitness function can be used to help govern component domains once they are established within the monolithic application.\nSysops Squad Saga: Creating Component Domains Thursday, November 18, 13:15\nAddison and Austen consulted with Parker, the Sysops Squad product owner, and together they identified five main domains within the application: a Ticketing domain (ss.ticket) containing all ticket-related functionality, including ticket processing, cus- tomer surveys, and knowledge base (KB) functionality; a Reporting domain (ss.report ing) containing all reporting (ss.customer) containing customer profile, billing, and support contracts; an Admin domain (ss.admin) containing maintenance of users and Sysops Squad experts; and finally, a Shared domain (ss.shared) containing login and notification functionality used by the other domains.\nAddison created a domain diagram (see Figure 5-19) showing the various domains and correspond- ing groups of components within each domain, and was satisfied with this grouping as no compo- nent was left out, and there was good cohesion between the components within each domain.\nThe exercise Addison did in diagramming and grouping the components was an important one as it validated the identified domain candidates and also demonstrated the need for collaboration with business stakeholders (such as the product owner or business application sponsor).\nSatisfied that all of the components fit nicely into these domains, Addison then looked at the various component namespaces in Table 5-12 after applying the “Flatten Components Pattern” on page 101 and identified the component domain refactoring that needed to take place.\nCreate Component Domains Pattern\nAddison started with the Ticket domain and saw that while the core ticket functionality started with the namespace ss.ticket, the survey and knowledge base components did not.\nTherefore, Addi- son wrote an architecture story to refactor the components listed in Table 5-15 to align with the ticketing domain.\nSysops Squad component refactoring for the Ticket domain\nNext Addison considered the customer-related components, and found that the billing and survey components needed to be refactored to include them under the Customer domain, creating a Bill- ing subdomain in the process.\nSysops Squad component refactoring for the Customer domain\nBy applying the “Identify and Size Components Pattern” on page 84, Addison found that the report- ing domain was already aligned, and no further action was needed with the reporting components listed in Table 5-17.\nSysops Squad Reporting components are already aligned with the Reporting domain\nCreate Component Domains Pattern\nAddison saw that both the Admin and Shared domains needed alignment as well, and decided to create a single architecture story for this refactoring effort and listed these components in Table 5-18.\nSysops Squad component refactoring for the Admin and Shared domains\nWith this pattern complete, Addison realized they were now prepared to structurally break apart the monolithic application and move to the first stage of a distributed architecture by applying the Cre- ate Domain Services pattern (described in the next section).\nCreate Domain Services Pattern Once components have been properly sized, flattened, and grouped into domains, those domains can then be moved to separately deployed domain services, creating what is known as a service-based architecture (see Appendix A).\nPattern Description The previous “Create Component Domains Pattern” on page 120 forms well-defined component domains within a monolithic application and manifests those domains through the component namespaces (or directory structures).\nThis pattern takes those well-defined component domains and extracts those component groups into separately deployed services, known as a domain services, thus creating a service- based architecture.\nNotice in the diagram how the Reporting component domain defined in the “Create Component Domains Pattern” on page 120 is extracted from of the monolithic application, form‐ ing its own separately deployed Reporting service.\nComponent domains are moved to external domain services\nA word of advice, however: don’t apply this pattern until all of the component domains have been identified and refactored.\nThis helps reduce the amount of modi‐ fication needed to each domain service when moving components (and hence source code) around.\nFor example, suppose all of the ticketing and knowledge base function‐ ality in the Sysops Squad application was grouped and refactored into a Ticket domain, and a new Ticket service created from that domain.\nNow suppose that the customer survey component (identified through the ss.customer.survey name‐ space) was deemed part of the Ticket domain.\nSince the Ticket domain had already been migrated, the Ticket service would now have to be modified to include the Sur‐ vey component.\nBetter to align and refactor all of the components into component domains first, then start migrating those component domains to domain services.\nFitness Functions for Governance It is important to keep the components within each domain service aligned with the domain, particularly if the domain service will be broken into smaller microservices.\nThe following fitness function ensures that the namespace (and hence components) are kept consistent within a domain service.\nFitness function: All components in <some domain service> should start with the same namespace\nThis automated holistic fitness function can be triggered on deployment through a CI/CD pipeline to make sure the namespaces for components within a domain ser‐ vice remain consistent.\nFor example, all components within the Ticket domain ser‐ vice should start with ss.ticket.\nArchUnit code for governing components within the Ticket domain service\npublic void restrict_domain_within_ticket_service() { classes().should().resideInAPackage(\"..ss.ticket..\") .check(myClasses); }\nAddison and Austen worked closely with the Sysops Squad development team to develop a migration plan to stage the migration from component domains to domain services.\nThey realized this effort not only required the code within each component domain to be extracted from the monolith and moved to a new project workspace, but also for the user interface to now remotely access the functionality within that domain.\nWorking from the component domains identified previously in Figure 5-19, the team migrated each component, one at a time, eventually arriving at a service-based architec- ture, as shown in Figure 5-22.",
      "keywords": [
        "Component Domains Pattern",
        "Component Dependencies Pattern",
        "Component",
        "Component Domains",
        "Sysops Squad",
        "Determine Component Dependencies",
        "Create Component Domains",
        "Sysops Squad application",
        "Sysops Squad component",
        "Domain Services pattern",
        "Ticket",
        "Component Dependencies",
        "Sysops Squad Ticket",
        "domain",
        "Flatten Components Pattern"
      ],
      "concepts": [
        "component",
        "domain",
        "ticketing",
        "function",
        "functionality",
        "functionally",
        "dependencies",
        "depending",
        "dependent",
        "services"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 14,
          "title": "",
          "score": 0.734,
          "base_score": 0.584,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 5,
          "title": "",
          "score": 0.729,
          "base_score": 0.579,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 12,
          "title": "",
          "score": 0.601,
          "base_score": 0.451,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 2,
          "title": "",
          "score": 0.593,
          "base_score": 0.443,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 45,
          "title": "",
          "score": 0.559,
          "base_score": 0.409,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "component",
          "components",
          "ss",
          "component domains",
          "domain"
        ],
        "semantic": [],
        "merged": [
          "component",
          "components",
          "ss",
          "component domains",
          "domain"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30143401273326054,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.167814+00:00"
      }
    },
    {
      "chapter_number": 16,
      "title": "Segment 16 (pages 148-155)",
      "start_page": 148,
      "end_page": 155,
      "summary": "Once these patterns are applied, teams can now work to decompose monolithic data (see Chapter 6) and begin breaking apart domain services into more fine-grained microservices (see Chapter 7) as needed.\nNow that the Sysops Squad application was successfully broken into separately deployed domain services, Addison and Austen both realized that it was time to start thinking about breaking apart the monolithic Sysops Squad database.\n“Who said anything about breaking apart the database?”\n“Addison and I agreed last week that we needed to break up the Sysops Squad database,” said Devon.\nBesides, do you know how hard it would be to break apart that database?”\n“Of course it will be difficult,” said Devon, “but I know of a five-step process leveraging what are known as data domains that would work really well on this database.\nSome architecture styles, such as microservices, require data to be broken apart to form well-defined bounded contexts (where each service owns its own data), whereas other distributed architectures, such as service-based architecture, allow services to share a single database.\nIn this chapter, we explore some of the drivers for decomposing data and show tech‐ niques for how to effectively break apart monolithic data into separate data domains, schemas, and even separate databases in an iterative and controlled fashion.\nData Decomposition Drivers Breaking apart a monolithic database can be a daunting task, and as such it’s impor‐ tant to understand if (and when) a database should be decomposed, as illustrated in Figure 6-1.\nHow many services are impacted by a database table change?\nOne of the primary data disintegration drivers is controlling changes in the database table schemas.\nAs illustrated in Figure 6-2, when breaking changes occur to a database, multiple services must be updated, tested, and deployed together with the database changes.\nImagine trying to coordinate 42 separately deployed services for a single breaking database change!\nServices impacted by the database change must be deployed together with the database\nThe real danger of changing a shared database in any distributed architecture is forgetting about services that access the table just changed.\nBreaking apart a database into well-defined bounded contexts significantly helps con‐ trol breaking database changes.\nAs illustrated in Figure 6-4, well-formed bounded con‐ texts around services and their corresponding data helps control change, because change is isolated to just those services within that bounded context.\nDatabase changes are isolated to only those services within the associated bounded context\nNotice in Figure 6-4 that Service C needs access to some of the data in Database D that is contained in a bounded context with Service D.\nSince Database D is in a differ‐ ent bounded context, Service C cannot directly access the data.\nOne important aspect of a bounded context related to the scenario between Service C needing data and Service D owning that data within its bounded context is that of database abstraction.\nThe advantage of the bounded context is that the data sent to Service C can be a dif‐ ferent contract than the schema for Database D.\nThis means that a breaking change to some table in Database D impacts only Service D and not necessarily the contract of the data sent to Service C.",
      "keywords": [
        "Sysops Squad database",
        "Sysops Squad",
        "database",
        "data",
        "Sysops Squad data",
        "Sysops Squad application",
        "Squad application Summary",
        "services",
        "Squad database",
        "Data Decomposition Drivers",
        "breaking",
        "Sysops",
        "Squad",
        "monolithic Sysops Squad",
        "Data Decomposition"
      ],
      "concepts": [
        "data",
        "database",
        "service",
        "change",
        "changing",
        "schemas",
        "context",
        "types",
        "architectures",
        "architectural"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 10,
          "title": "",
          "score": 0.843,
          "base_score": 0.693,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 19,
          "title": "",
          "score": 0.818,
          "base_score": 0.668,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 18,
          "title": "",
          "score": 0.812,
          "base_score": 0.662,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 8,
          "title": "",
          "score": 0.766,
          "base_score": 0.616,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 45,
          "title": "",
          "score": 0.765,
          "base_score": 0.615,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "database",
          "data",
          "bounded",
          "breaking",
          "squad"
        ],
        "semantic": [],
        "merged": [
          "database",
          "data",
          "bounded",
          "breaking",
          "squad"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4557225062872365,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.167867+00:00"
      }
    },
    {
      "chapter_number": 17,
      "title": "Segment 17 (pages 156-163)",
      "start_page": 156,
      "end_page": 163,
      "summary": "As illustrated in Figure 6-6, when multiple services share the same database, the number of connections can quickly become saturated, particularly as the number of services or service instances increase.\nDatabase connections can quickly get saturated with multiple service instances\nReaching (or exceeding) the maximum number of available database connections is yet another driver to consider when deciding whether to break apart a database.\nTo illustrate the issues associated with database connections and distributed architec‐ ture, consider the following example: a monolithic application with 200 database con‐ nections is broken into a distributed architecture consisting of 50 services, each with 10 database connections in its connection pool.\nConnections per service\nTotal service connections\nNotice how the number of database connections within the same application context grew from 200 to 1,000, and the services haven’t even started scaling yet!\nAssuming half of the services scale to an average of 5 instances each, the number of database connections quickly grows to 1,700.\nOne effective approach is to assign each service a connection quota to govern the distribution of available database connec‐ tions across services.\nA connection quota specifies the maximum number of database connections a service is allowed to use or make available in its connection pool.\nBy specifying a connection quota, services are not allowed to create more database connections than are allocated to it.\nIf a service reaches the maximum number of database connections in its quota, it must wait for one of the connections it’s using to become available.\nThis method can be implemented using two approaches: evenly distributing the same connection quota to every service, or assigning a different con‐ nection quota to each service based on its needs.\nThe advantage of this approach is that it optimizes the use of available database connections across distributed services, making sure those services that require more database connections have them available for use.\nNotice that Service A has only ever needed a maximum of 5 connections, Service C only 15 con‐ nections, and Service E only 14 connections, whereas Service B and Service D have reached their max connection quota and have experienced connection waits.\nMoving five database connections to Service B and five database connections to Service D yields the results shown in Table 6-2.\nThis is better, but Service B is still experiencing connection waits, indicating that it requires more connections than it has in its connection quota.\nAs illustrated in Figure 6-7, service scalability can put a tremendous strain on the database, not only in terms of database connections (as discussed in the prior sec‐ tion), but also on throughput and database capacity.\nDatabase connections, capacity, throughput, and perfor‐ mance are all factors in determining whether a shared database can meet the demands of multiple services within a distributed architecture.\nWhen services scale by adding multiple instances, the picture changes dra‐ matically, as shown in Table 6-4, where the total number of database connections is 100.\nWhen services scale, more connection are used than are available\nNotice that even though the connection quota is distributed to match the 100 data‐ base connections available, once services start to scale, the quota is no longer valid because the total number of connections used increases to 242, which is 142 more connections than are available in the database.\nBreaking data into separate data domains or even a database-per-service, as illustra‐ ted in Figure 6-8, requires fewer connections to each database, hence providing better database scalability and performance as the services scale.\nThus, all five services, along with the database, form a single architectural quantum.\nNotice in Figure 6-12 that since the database is broken apart, Service A and Service B, along with the corresponding data, are now a separate quantum from the one formed with services C, D, and E.",
      "keywords": [
        "Service",
        "database",
        "Database connections",
        "connections",
        "connection quota",
        "quota",
        "Data",
        "connection waits",
        "Data Decomposition Drivers",
        "services scale",
        "connection waits Service",
        "connections Distributed services",
        "database connection quotas",
        "Distributed services",
        "Total service connections"
      ],
      "concepts": [
        "service",
        "connection",
        "connections",
        "database",
        "data",
        "quota",
        "distributed",
        "distribution",
        "distributions",
        "architectures"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 24,
          "title": "",
          "score": 0.668,
          "base_score": 0.518,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 23,
          "title": "",
          "score": 0.638,
          "base_score": 0.488,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 26,
          "title": "",
          "score": 0.625,
          "base_score": 0.475,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 22,
          "title": "",
          "score": 0.581,
          "base_score": 0.431,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 25,
          "title": "",
          "score": 0.571,
          "base_score": 0.421,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "connections",
          "database connections",
          "database",
          "quota",
          "connection"
        ],
        "semantic": [],
        "merged": [
          "connections",
          "database connections",
          "database",
          "quota",
          "connection"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.32176820769781983,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.167907+00:00"
      }
    },
    {
      "chapter_number": 18,
      "title": "Segment 18 (pages 164-172)",
      "start_page": 164,
      "end_page": 172,
      "summary": "Breaking apart monolithic data allows the architect to move certain data to a more optimal database type.\nFor example, suppose a monolithic relational database stored application-related transactional data, including reference data in the form of key- value pairs (such as country codes, product codes, warehouse codes, and so on).\nThese drivers provide answers and justifications for the question “when should I consider putting data back together?” Along with data disintegrators, data integrators provide the balance and trade-offs for analyzing when to break apart data and when not to.\nArtifacts like foreign keys, triggers, views, and stored procedures tie tables together, making it difficult to pull data apart; see Figure 6-13.\nImagine walking up to your DBA or data architect and telling them that since the database must be broken apart to support tightly formed bounded contexts within a microservices ecosystem, every foreign key and view in the database needs to be removed!\nThese artifacts are necessary in most relational databases to support data consistency and data integrity.\nHowever, as illustrated in Figure 6-14, these artifacts must be removed when moving data to another schema or database to form bounded contexts.\nNotice that the foreign key (FK) relationship between the tables in Service A can be preserved because the data is in the same bounded context, schema, or database.\nAnother data integrator is that of database transactions, something we discuss in detail in “Distributed Transactions” on page 263.\nHowever, when data is broken apart into either separate schemas or databases, as illustrated in Figure 6-16, a single transactional unit of work no longer exists because of the remote calls between services.\nWhile we dive into the details of distributed transaction management and transac‐ tional sagas in Chapter 12, the point here is to emphasize that database transactions are yet another data integration driver, and should be taken into account when con‐ sidering breaking apart a database.\nArmed with their justifications, Addison and Devon met to convince Dana that it was necessary to break apart the monolithic Sysops Squad database.\n“We think we have enough evidence to convince you that it’s necessary to break apart the Sysops Squad database.”\nBut that still doesn’t convince me that the nonreporting data should be broken apart,” said Dana.\n“Speaking of database connections,” said Devon, “look at this connection pool estimate as we start breaking apart the domain services.”\n“So you see, Dana,” said Devon, “with these projected estimates, we will need an additional 2,000 connections to the database to provide the scalability we need to handle the ticket load, and we simply do not have them with a single database.”\n“What Addison is saying,” added Devon, “is that by breaking apart the database, we can provide bet- ter fault tolerance by creating domain silos for the data.\nIn other words, if the survey database were to go down, ticketing functionality would still be available.”\n“In other words, since the database is part of the static coupling of a system, breaking it apart would make the core ticketing functionality stand- alone and not synchronously dependent on other parts of the system.”\n“Listen,” said Dana, “you’ve convinced me that there’s good reasons to break apart the Sysops Squad database, but explain to me how you can even think about doing that.\nThat’s where data domains and the five- step process come into play,” said Devon.\nDecomposing Monolithic Data Decomposing a monolithic database is hard, and requires an architect to collaborate closely with the database team to safely and effectively break apart the data.\nAs illustrated in Figure 6-17, this evolutionary and iterative pro‐ cess leverages the concept of a data domain as a vehicle for methodically migrating data into separate schemas, and consequently different physical databases.\nA data domain is a collection of coupled database artifacts—tables, views, foreign keys, and triggers—that are all related to a particular domain and frequently used together within a limited functional scope.\nExisting Sysops Squad database tables assigned to data domains\narticle_keyword\nTable 6-5 lists six data domains within the Sysops Squad application: Customer, Sur‐ vey, Payment, Profile, Knowledge base, and Ticketing.\nThe billing table belongs to the Payment data domain, ticket and ticket_type tables belong to the Ticketing data domain, and so on.\nOne way to conceptually think about data domains is to think about the database as a soccer ball, where each white hexagon represents a separate data domain.\nDatabase objects in a hexagon belong in a data domain\nVisualizing the database this way allows the architect and database team to clearly see data domain boundaries and also the cross-domain dependencies (such as foreign keys, views, stored procedures, and so on) that need to be broken.\nThis means removing foreign-key constraints, views, triggers, functions, and stored procedures between data domains.\nSince the customer table belongs to a different data domain than the v_customer_contract, the customer table must be removed from the view in the Payment domain.",
      "keywords": [
        "data",
        "data domain",
        "database",
        "Sysops Squad database",
        "Sysops Squad",
        "Operational Data",
        "monolithic data",
        "Decomposing Monolithic Data",
        "domain",
        "tables",
        "Squad database",
        "Dana",
        "Knowledge Base",
        "Addison",
        "Data Decomposition Drivers"
      ],
      "concepts": [
        "data",
        "database",
        "tables",
        "domain",
        "said",
        "dana",
        "service",
        "reports",
        "views",
        "transactions"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 16,
          "title": "",
          "score": 0.812,
          "base_score": 0.662,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 19,
          "title": "",
          "score": 0.748,
          "base_score": 0.598,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 40,
          "title": "",
          "score": 0.729,
          "base_score": 0.579,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 43,
          "title": "",
          "score": 0.665,
          "base_score": 0.515,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 20,
          "title": "",
          "score": 0.662,
          "base_score": 0.512,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data",
          "database",
          "apart",
          "foreign",
          "data domain"
        ],
        "semantic": [],
        "merged": [
          "data",
          "database",
          "apart",
          "foreign",
          "data domain"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3883249360383411,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.167953+00:00"
      }
    },
    {
      "chapter_number": 19,
      "title": "Segment 19 (pages 173-180)",
      "start_page": 173,
      "end_page": 180,
      "summary": "Once architects and database teams understand the concept of a data domain, they can apply the five-step process for decomposing a monolithic database.\nStep 1: Analyze Database and Create Data Domains As illustrated in Figure 6-20, all services have access to all data in the database.\nAs illustrated in Figure 6-21, we have created schemas for each data domain and moved tables to the schemas to which they belong.\nServices use the primary schema according to their data domain needs\nData Domain Versus Database Schema A data domain is an architectural concept, whereas a schema is a database construct that holds the database objects belonging to a particular data domain.\nTo illustrate the assignment of tables to schemas, consider the Sysops Squad example where the billing table must be moved from its original schema to another data domain schema called payment:\nAlternatively, a database team can create synonyms for tables that do not belong in their schema.\nStep 3: Separate Database Connections to Data Domains In this step, the database connection logic within each service is refactored to ensure services connect to a specific schema and have read and write access to the tables belonging only to their data domain.\nNotice that the database configuration has been changed so that all data access is done strictly via services and their connected schemas.\nThere is no cross-schema access; all synonyms created in “Step 2: Assign Tables to Data Domains” on page 156 are removed.\nWhen data from other domains is needed, do not reach into their databases.\nTeams can change the database schema without worrying about affecting changes in other domains.\nAll database code (stored procedures, functions) that access tables belonging to other domains must be moved to the service layer.\nStep 4: Move Schemas to Separate Database Servers Once database teams have created and separated data domains, and have isolated services so that they access their own data, they can now move the data domains to separate physical databases.\nservices access their own schemas, accessing a single database creates a single archi‐ tecture quantum, as discussed in Chapter 2, which might have adverse effects for operational characteristics, such as scalability, fault tolerance, and performance.\nWith this option, teams first back up each schema with data domains, then set up database servers for each data domain.\nThey then restore the schemas, connect services to schemas in the new database servers, and finally remove schemas from the original database server.\nUsing the replicate option, teams first set up database servers for each data domain.\nFigure 6-23 shows an example of the replication option, where the database team sets up multiple database servers so that there is one database server for each data domain.\nReplicate schemas (data domains) to their own database servers\nStep 5: Switch Over to Independent Database Servers Once the schemas are fully replicated, the service connections can be switched.\nThe last step in getting the data domains and services to act as their own independent deployable units is to remove the connection to the old database servers and remove the schemas from the old database servers as well.\nIndependent database servers for each data domain\nOnce the database team has separated the data domains, isolated the database con‐ nections, and finally moved the data domains to their own database servers, they can optimize the individual database servers for availability and scalability.\nThis characteristic refers to the ease with which new developers, data architects, data modelers, operational DBAs, and other users of the databases can learn and adopt.",
      "keywords": [
        "data domains",
        "data",
        "Database",
        "database servers",
        "domains",
        "schema",
        "data domain schema",
        "Create Data Domains",
        "specific data domain",
        "tables",
        "Operational Data",
        "Decomposing Monolithic Data",
        "servers",
        "service",
        "joins CREATE VIEW"
      ],
      "concepts": [
        "databases",
        "schema",
        "service",
        "tables",
        "payment",
        "supports",
        "create",
        "created",
        "figurations",
        "steps"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 16,
          "title": "",
          "score": 0.818,
          "base_score": 0.668,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 45,
          "title": "",
          "score": 0.771,
          "base_score": 0.621,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 43,
          "title": "",
          "score": 0.764,
          "base_score": 0.614,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 44,
          "title": "",
          "score": 0.758,
          "base_score": 0.608,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 18,
          "title": "",
          "score": 0.748,
          "base_score": 0.598,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "database",
          "data",
          "database servers",
          "servers",
          "domains"
        ],
        "semantic": [],
        "merged": [
          "database",
          "data",
          "database servers",
          "servers",
          "domains"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4165781089895097,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.167999+00:00"
      }
    },
    {
      "chapter_number": 20,
      "title": "Segment 20 (pages 181-194)",
      "start_page": 181,
      "end_page": 194,
      "summary": "The star ratings for relational databases appear in Figure 6-25.\nRelational databases rated for various adoption characteristics\nRelational databases have been around for many years.\nRelational databases allow for flexible data modeling.\nRelational databases organize data into tables and rows (similar to spreadsheets), something that is natural for most database modelers.\nRelational databases favor consistency over availability and partition tolerance, discussed in “Table Split Technique” on page 254.\nRelational databases have been dominant for years because of their support for ACID properties.\nSince relational databases have been around for many years, well-known design, implementation, and operational patterns can be applied to them, thus making them easy to adopt, develop, and integrate within an architecture.\nIn relational databases, the data model can be designed in such a way that either reads become more efficient or writes become more efficient.\nThe same database can handle different types of workloads, allowing for balanced read-write prior‐ ity.\nKey-Value Databases Key-value databases are similar to a hash table data structure, something like tables in an RDBMS with an ID column as the key and a blob column as the value, which can consequently store any type of data.\nKey-value databases are part of a family known as NoSQL databases.\nKey-value databases are easiest to understand among the NoSQL databases.\nUnlike relational databases, key-value databases should be picked based on needs.\nOther relational database constructs like joins, where, and order by are not supported, but rather the operations get, put, and delete.\nThe ratings for key-value databases appear in Figure 6-26.\nKey-value databases rated for various adoption characteristics\nKey-value databases are easy to understand.\nSince key-value databases are aggregate oriented, they can use memory structures like arrays, maps, or any other type of data, including big blob.\nThe data can be queried only by key or ID, which means the client should have access to the key outside of the database.\nSince there are many types of key-value databases and each has different proper‐ ties, even the same database can be configured to act in different ways either for an installation or for each read.\nKey-value databases have good programming language support, and many open source databases have an active community to help learn and understand them.\nSince key-value databases are aggregate oriented, access to data via a key or ID is geared toward read priority.\nKey-value databases can be used for session storage, and can be used to cache user properties and preferences as well.\nSharding in Databases The concept of partitioning is well-known in relational databases: the table data is partitioned into sets based on a schema on the same database server.\nThe word shard means horizontal partition of data in a database.\nDocument data‐ bases are another type of NoSQL database, whose ratings appear in Figure 6-27.\nThese databases understand the structure of the data and can index multiple attributes of the documents, allowing for better query flexibility.\nDocument databases are like key-value databases where the value is human read‐ able.\nJust like key-value databases, data modeling involves modeling aggregates such as orders, tickets, and other domain objects.\nLike key-value databases, document databases can be configured for higher avail‐ ability.\nJust like key-value data‐ bases, document databases provide the ability to tune the read and write opera‐ tions using the quorum mechanism.\nCol‐ umn family databases are another type of NoSQL database that group related data that is accessed at the same time, and whose ratings appear in Figure 6-28.\nData modeling with column family databases takes some getting used to.\nSome column family databases like Apache Cassandra have introduced a SQL-like query language known as Cassandra Query Language (CQL) that makes data modeling accessible.\nAll column family databases are highly scalable and suit use cases where high write or read throughput is needed.\nColumn family databases scale horizontally for read and write operations.\nSimilar to key-value and document databases, column family databases can tune writes and reads based on quorum needs.\nColumn family databases use the concepts of SSTables, commit logs, and memta‐ bles, and since the name-value pairs are populated when data is present, they can handle sparse data much better than relational databases.\nHaving aggregates improves read and write performance, and also allows for higher availabil‐ ity and partition tolerance when the databases are run as a cluster.\nGraph Databases Unlike relational databases, where relations are implied based on references, graph databases use nodes to store entities and their properties.\nSince it’s difficult to split or shard graphs, write throughput is constrained with the type of graph database picked.\nSome graph databases, such as Neo4j, support transactions, so that data is always consistent.\nIn graph databases, data storage is optimized for relationship traversal as opposed to relational databases, where we have to query the relationships and derive them at query time.\nGraph databases allow the same node to have various types of relationships.\nNewSQL Databases Matthew Aslett first used the term NewSQL to define new databases that aimed to provide the scalability of NoSQL databases while supporting the features of relational databases like ACID.\nNewSQL databases use different types of storage mechanisms, and all of them support SQL.\nNewSQL databases, whose ratings appear in Figure 6-31, improve upon relational databases by providing automated data partitioning or sharding, allowing for\nSince NewSQL databases are just like relational databases (with SQL interface, added features of horizontal scaling, ACID compliant), the learning curve is much easier.\nSince NewSQL databases are like relational databases, data modeling is familiar to many and easier to pick up.\nNewSQL databases are designed to support horizontal scaling for distributed sys‐ tems, allowing for multiple active nodes, unlike relational databases that have only one active leader, and the rest of the nodes are followers.\nThe multiple active nodes allow NewSQL databases to be highly scalable and to have better throughput.\nThe data is always consistent, and this allows for relational database users to easily transition to NewSQL databases.\nNewSQL databases are used just like relational databases, with added benefits of indexing and distributing geographically either to improve read performance or write performance.\nUnderstanding the type of modeling provided by the data‐ base is critical in selecting the database to use.",
      "keywords": [
        "Databases",
        "Relational Databases",
        "Column Family Databases",
        "Key-Value Databases",
        "data",
        "Document Databases",
        "Graph Databases",
        "Family Databases",
        "Database Type",
        "Relational",
        "NewSQL Databases",
        "NoSQL databases",
        "Unlike relational databases",
        "Type",
        "data modeling"
      ],
      "concepts": [
        "databases",
        "key",
        "keys",
        "relational",
        "related",
        "relations",
        "relates",
        "reads",
        "consistency",
        "consistent"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 43,
          "title": "",
          "score": 0.741,
          "base_score": 0.591,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 18,
          "title": "",
          "score": 0.662,
          "base_score": 0.512,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 21,
          "title": "",
          "score": 0.605,
          "base_score": 0.455,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 19,
          "title": "",
          "score": 0.601,
          "base_score": 0.451,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 44,
          "title": "",
          "score": 0.597,
          "base_score": 0.447,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "databases",
          "relational databases",
          "relational",
          "key value",
          "value databases"
        ],
        "semantic": [],
        "merged": [
          "databases",
          "relational databases",
          "relational",
          "key value",
          "value databases"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.25629427086918377,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.168041+00:00"
      }
    },
    {
      "chapter_number": 21,
      "title": "Segment 21 (pages 195-202)",
      "start_page": 195,
      "end_page": 202,
      "summary": "The underlying concept with time-series databases is to analyze changes in data over time.\nFor example, with the Sysops Squad example, changes done to a ticket object can be stored in a time-series database, where the timestamp of change and ticket_id are tagged.\nWhen using time-series databases, the database automatically attaches a timestamp to every datum creation, and the data contains tags or attributes of information.\nNow that the team had formed data domains from the monolithic Sysops Squad data- base, Devon noticed that the Survey data domain would be a great candidate for migrating from a traditional relational database to a document database using JSON.\n“Actually,” said Skyler, “if you had originally talked with us about this when the system was first being developed, you would understand that from a user interface perspective, it’s really hard to deal with relational data for something like a customer survey.\nIt may work out good for you, but from a user interface development standpoint, dealing with relational data for the survey stuff has been a major pain point.”\n“This is why we need to change it to a document database.”\nYou can’t just start adding different database types to the system,” said Dana.\n“Wait,” said Skyler, “didn’t we all agree that part of the problem of the current monolithic Sysops Squad application was that the development teams didn’t work close enough with the database teams?”\n“OK,” said Dana, “but what I’m going to need from you and Devon is a good solid justification for introducing another type of database into the mix.”\nDevon and Skyler knew that a document database would be a much better solution for the cus- tomer survey data, but they weren’t sure how to build the right justifications for Dana to agree to migrate the data.\nAddison agreed to help, and set up a meeting with Parker (the Sysops Squad product owner) to validate whether there was any business justification to migrating the customer survey tables to a document database.\n“As I mentioned to you before, we are thinking of changing the way the customer survey data is stored, and have a few questions for you.”\nYou see, the customer survey part of the system has been a major pain point for the marketing department, as well as for me.”\n“How long does it take you to apply even the smallest of change requests to the customer surveys?” asked Parker.\n“Well,” said Devon, “it’s not too bad from the database side.\nYou have no idea how hard it is to query all of that relational data and render a customer survey in the user interface.\n“So Parker, what you’re saying is that the customer survey changes frequently, and it is taking too long to make the changes?”\n“What if I were to tell you that the lack of flexibility and responsiveness to change requests has everything to do with the technology used to store customer surveys, and that by changing the way we store data, we could significantly improve flexibility as well as response time for change requests?” asked Addison.\n“So, essentially we have two options for modeling the survey questions in a document database,” said Devon.\n“How to we know which one to use?” asked Skyler, happy that the development teams were now finally working with the database teams to arrive at a unified solution.\nDevon showed the team that with the single aggregate option, as shown in Figure 6-36, with the corresponding source code listing in Example 6-3, both the survey data and all related question data were stored as one document.\nTherefore, the entire customer survey could be retrieved from the database by using a single get operation, making it easy for Skyler and others on the development team to work with the data.\n“Yeah,” said Devon, “but it would require additional work on the database side as questions would be replicated in each survey document.\nADR: Use of Document Database for Customer Survey\nThe survey is currently stored in a relational database, but the team wants to migrate the sur- vey to a document database using JSON.\nDecision We will use a document database for the customer survey.\nMoving to a document database would not only provide better flexibility, but also better timeliness for changes needed to the customer surveys.\nUsing a document database would simplify the customer survey user interface and better facilitate changes to the surveys.\nConsequences Since we will be using a single aggregate, multiple documents would need to be changed when a common survey question is updated, added, or removed.\nSurvey functionality will need to be shut down during the data migration from the rela- tional database to the document database.",
      "keywords": [
        "database",
        "Survey",
        "data",
        "document database",
        "customer survey",
        "Time-Series Databases",
        "Survey data",
        "customer survey data",
        "Sysops Squad Saga",
        "Sysops Squad",
        "database type",
        "customer",
        "Skyler",
        "question",
        "Document"
      ],
      "concepts": [
        "data",
        "databases",
        "survey",
        "said",
        "changes",
        "changing",
        "time",
        "product",
        "questions",
        "question"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 20,
          "title": "",
          "score": 0.605,
          "base_score": 0.455,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 40,
          "title": "",
          "score": 0.594,
          "base_score": 0.444,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 19,
          "title": "",
          "score": 0.576,
          "base_score": 0.426,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 43,
          "title": "",
          "score": 0.548,
          "base_score": 0.398,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 18,
          "title": "",
          "score": 0.541,
          "base_score": 0.391,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "survey",
          "document database",
          "database",
          "document",
          "customer survey"
        ],
        "semantic": [],
        "merged": [
          "survey",
          "document database",
          "database",
          "document",
          "customer survey"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.22074895101772196,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.168081+00:00"
      }
    },
    {
      "chapter_number": 22,
      "title": "Segment 22 (pages 203-212)",
      "start_page": 203,
      "end_page": 212,
      "summary": "CHAPTER 7 Service Granularity\n“Speaking of customer functionality, did you ever figure out if the customer login functionality is going to be a separate service?”\n“I’m telling you,” said Taylen, “we need to break up the domain services into smaller services.\n“If that’s the case, then how do you determine what services should and shouldn’t be broken apart?” asked Taylen.\nSo tell me everyone, one service or three services?”\nThat’s what micro- services is all about.”\n“The key to getting service granularity right,” said Logan, “is to remove opinion and gut feeling, and use granularity disintegrators and integrators to objectively analyze the trade-offs and form solid justifications for whether or not to break apart a service.”\nChapter 7: Service Granularity\nGranularity dis‐ integrators address the question “When should I consider breaking apart a service into smaller parts?”, whereas Granularity integrators address the question “When should I consider putting services back together?” One common mistake many devel‐ opment teams make is focusing too much on granularity disintegrators while ignor‐ ing granularity integrators.\nService Granularity\nChapter 7: Service Granularity\nService Scope and Function The service scope and function is the first and most common driver for breaking up a single service into smaller ones, particularly with regard to microservices.\nConsider a typical Notification Service that does three things: notifies a customer through SMS (Short Message Service), email, or a printed postal letter that is mailed to the customer.\nAlthough it is very tempting to break this service into three separate single-purpose services (one for SMS, one for email, and one for postal letters) as illustrated in Figure 7-2, this alone is not enough to justify breaking the service apart because it already has relatively strong cohesion—all of these functions relate to one thing, notifying the customer.\nAll of that module, class or function’s services should be narrowly aligned with that responsibility.” While the single-responsibility principle was origi‐ nally scoped within the context of classes, in later years it has expanded to include components and services.\nChapter 7: Service Granularity\nThe subjectiveness related to what is and isn’t a single responsibility is where most developers get into trouble with regard to service granularity.\nAs a single service, any change to the postal letter code would require the developer to test and redeploy the entire service, including SMS and email functionality.\nHowever, by breaking this service into two separate services (Electronic Notification and Postal Letter Notification), as illustrated in Fig‐ ure 7-4, frequent changes are now isolated into a single, smaller service.\nConsider once again the Notification Service example, where a single service notifies customers through SMS, email, and printed postal letter.\nAs a single service, email and postal letter functionality must unneces‐ sarily scale to meet the demands of SMS notifications, impacting cost and also elastic‐ ity in terms of mean time to startup (MTTS).\nBreaking the Notification Service into three separate services (SMS, Email, and Letter), as illustrated in Figure 7-5, allows each of these services to scale independently to meet their varying demands of throughput.\nChapter 7: Service Granularity\nConsider the same consolidated Notification Service example that notifies customers through SMS, email, and postal letter (Figure 7-6).\nIf the email functionality contin‐ ues to have problems with out-of-memory conditions and fatally crashes, the entire service comes down, including SMS and postal letter processing.\nSeparating this single consolidated Notification Service into three separate services provides a level of fault tolerance for the domain of customer notification.\nNow, a fatal error in the functionality of the email service doesn’t impact SMS or postal letters.\nNotice in this example that the Notification Service is split into three separate services (SMS, Email, and Postal Letter), even though email functionality is the only issue with regard to frequent crashes (the other two are very stable).\nSince email function‐ ality is the only issue, why not combine the SMS and postal letter functionality into a single service?\nservice into only two services made sense because Postal Letter was the offending functionality, but Email and SMS are related—they both have to do with electronically notifying the customer.\nMoving the email functionality to a separate service disrupts the overall domain cohe‐ sion because the resulting cohesion between SMS and postal letter functionality is weak.\nEmail Service and…SMS-Letter Notification Service?\nThis naming problem relates back to the service scope and function granularity disintegrator—if a service is too hard to name because it’s doing multiple things, then consider breaking apart the service.\nNotification Service → Email Service, SMS-Letter Service (poor name)\nNotification Service → Email Service, SMS Service, Letter Service (good names)",
      "keywords": [
        "Service",
        "notification service",
        "Service Granularity",
        "Service Granularity Thursday",
        "postal letter",
        "Granularity",
        "Postal letter notification",
        "email service",
        "notification",
        "SMS",
        "email",
        "single service",
        "granularity disintegrators",
        "postal",
        "letter"
      ],
      "concepts": [
        "service",
        "function",
        "functions",
        "granularity",
        "notification",
        "notifications",
        "changes",
        "code",
        "coding",
        "letters"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 24,
          "title": "",
          "score": 0.713,
          "base_score": 0.563,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 23,
          "title": "",
          "score": 0.675,
          "base_score": 0.525,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 27,
          "title": "",
          "score": 0.619,
          "base_score": 0.469,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 25,
          "title": "",
          "score": 0.616,
          "base_score": 0.466,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 17,
          "title": "",
          "score": 0.581,
          "base_score": 0.431,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "letter",
          "postal",
          "email",
          "sms",
          "service"
        ],
        "semantic": [],
        "merged": [
          "letter",
          "postal",
          "email",
          "sms",
          "service"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3715888138433111,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.168123+00:00"
      }
    },
    {
      "chapter_number": 23,
      "title": "Segment 23 (pages 213-220)",
      "start_page": 213,
      "end_page": 220,
      "summary": "Although the API entry points into the consolidated customer profile service may differ, nevertheless there is risk that someone entering into the service to retrieve the customer name might also have access to credit card functionality.\nBy breaking this service into two separate services, access to the functionality used to maintain credit\nExtensibility Another primary driver for granularity disintegration is_ extensibility_—the ability to add additional functionality as the service context grows.\nThus, with the single consolidated payment service, the testing scope is increased and deployment risk is higher, making it more difficult to add additional payment types.\nNow consider breaking up the existing consolidated service into three separate serv‐ ices (Credit Card Processing, Gift Card Processing, and PayPal Transaction Process‐ ing), as illustrated in Figure 7-8.\nIs an ACID transaction required between separate services?\nDatabase Transactions Most monolithic systems and course-grained domain services using relational data‐ bases rely on single-unit-of-work database transactions to maintain data integrity and consistency; see “Distributed Transactions” on page 263 for the details of ACID (database) transactions and how they differ from BASE (distributed) transactions.\nTo understand how database transactions impact service granularity, consider the situa‐ tion illustrated in Figure 7-9 where customer functionality has been split into a Cus‐ tomer Profile Service that maintains customer profile information and a Password Service that maintains password and other security-related information and functionality.\nNotice that having two separate services provides a good level of security access con‐ trol to password information since access is at a service level rather than at a request level.\nSeparate services with combined operations do not support database (ACID) transactions\nWhile separating the services provides better security access control to the password information, the trade-off is that there is no ACID transaction for actions such as reg‐ istering a new customer or unsubscribing (deleting) a customer from the system.\nThus, if having a single-unit-of-work ACID transaction is required from a business perspective, these services should be consolidated into a sin‐ gle service, as illustrated in Figure 7-11.\nA single service supports database (ACID) transactions\nInterestingly enough, fault tolerance is one of the granularity disintegration drivers from the previous section—yet when those services need to talk to one another, noth‐ ing is really gained from a fault-tolerance perspective.\nOverall performance and responsiveness is another driver for granularity integration (putting services back together).\nIn terms of overall performance, the trade-off for this integration driver is balancing the need to break apart a service with the corresponding performance loss if those services need to communicate with one another.\nFor example, if 30% of the requests require a workflow between services to complete the request and 70% are purely atomic (dedicated to only one service without the need for any additional communication), then it might be OK to keep the services separate.\nConsider the previous example, where 30% of the requests require a workflow between services to complete the request, and 70% are purely atomic.\nConsider the example in Figure 7-14: customer information is separated into five separate customer services.",
      "keywords": [
        "service",
        "Service Granularity",
        "separate services",
        "Granularity",
        "payment service",
        "Payment",
        "credit card",
        "profile service",
        "single service",
        "Granularity Integrators",
        "Customer",
        "separate",
        "Card",
        "Profile",
        "consolidated payment service"
      ],
      "concepts": [
        "service",
        "payment",
        "payments",
        "functionality",
        "functions",
        "functionalities",
        "security",
        "secure",
        "transactions",
        "transaction"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 25,
          "title": "",
          "score": 0.675,
          "base_score": 0.525,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 22,
          "title": "",
          "score": 0.675,
          "base_score": 0.525,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 24,
          "title": "",
          "score": 0.675,
          "base_score": 0.525,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 17,
          "title": "",
          "score": 0.638,
          "base_score": 0.488,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 29,
          "title": "",
          "score": 0.619,
          "base_score": 0.619,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "service",
          "acid",
          "services",
          "granularity",
          "transactions"
        ],
        "semantic": [],
        "merged": [
          "service",
          "acid",
          "services",
          "granularity",
          "transactions"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.39418610854783764,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.168168+00:00"
      }
    },
    {
      "chapter_number": 24,
      "title": "Segment 24 (pages 221-230)",
      "start_page": 221,
      "end_page": 230,
      "summary": "While there may have been a good disintegrator driver for breaking apart these services, they all share a common codebase of domain functionality (as opposed to common utilities or infrastructure functionality).\nIn these cases, it might be wise to consolidate these five services into a single service to avoid multiple deployments, as well as having the service functionality be out of sync based on the use of different versions of a library.\nA change in shared code requires a coordinated change to all services\nBreaking up the collective functionality into separate services would mean that almost half of the source code is in a shared library used only by those three services.\nIn this example it might be wise to consider keeping the collective customer-related functionality in a single consolidated ser‐ vice along with the shared code (particularly if the shared code changes fre‐ quently, as discussed next).\nRegardless of the size of the shared library, frequent changes to shared function‐ ality require frequent coordinated changes to the services using that shared domain functionality.\nWhile versioning can sometimes be used to help mitigate coordinated changes, eventually services using that shared functionality will need to adopt the latest version.\nIf the shared code changes frequently, it might be wise to consider consolidating the services using that shared code to help mitigate the complex change coordination of multiple deployment units.\nWhile versioning can help mitigate coordinated changes and allow for backward compatibility and agility (the ability to respond quickly to change), at times cer‐ tain business functionality must be applied to all services at the same time (such as a defect or a change in business rules).\nData Relationships Another trade-off in the balance between granularity disintegrators and integrators is the relationship between the data that a single consolidated service uses as opposed to the data that separate services would use.\nThis integrator driver assumes that the data resulting from breaking apart a service is not shared, but rather formed into tight bounded contexts within each service to facilitate change control and support overall availability and reliability.\nConsider the example in Figure 7-16: a single consolidated service has three functions (A, B, and C) and corresponding data table relationships.\nAssume that based on some of the disintegration drivers outlined in the prior section, this service was broken into three separate services (one for each of the functions in the consolidated service); see Figure 7-17.\nHowever, breaking apart the single consoli‐ dated service into three separate services now requires the corresponding data tables to be associated with each service in a bounded context.\nTo better understand the bounded context and why Service C cannot simply access table 3, say Service B (which owns table 3) decides to make a change to its business rules that requires a column to be removed from table 3.\nBased on the dependency of the data between services B and C, it would be wise to consolidate those services into a single service to avoid the latency, fault tolerance, and scalability issues associated with the interservice communication between these services, demonstrating that relationships between tables can influence service granu‐ larity.\nArchitect: “We want to break apart our service to isolate frequent code changes, but in doing so we won’t be able to maintain a database transaction.\nWhich is more important based on our business needs—better overall agility (maintainability, testability, and deployability), which translates to faster time-to-market, or stronger data integrity and consistency?”\nProject Sponsor: “Based on our business needs, I’d rather sacrifice a little bit slower time-to-market to have better data integrity and consistency, so let’s leave it as a single service for right now.”\nArchitect: “We need to keep the service together to support a database transaction between two operations to ensure data consistency, but that means sensitive function‐ ality in the combined single service will be less secure.\nIn this case, it’s more important to secure sensitive data, so let’s keep the services separate and work out how we can mitigate some of the issues with data consistency.”\nThe Sysops Squad development team was having trouble deciding whether these two components (assignment and routing) should be implemented as a single consolidated service or two separate services, as illustrated in Figure 7-18.\n“So you see,” said Taylen, “the ticket assignment algorithms are very complex, and therefore should be isolated from the ticket routing functionality.\nThat way, when those algorithms change, I don’t have to worry about all of the routing functionality.”\n“But if we separated the assignment and routing functionality into two services, there would need to be constant communication between them,” said Skyler.\n“Furthermore, assignment and routing are really one function, not two.”\n“See,” said Skyler, “you cannot make a ticket assignment without routing it to the expert.\n“What happens in the current functionality if a ticket can’t be routed to the expert?” asked Addison.\n“Yes, but they are still two separate functions, not one as Skyler is suggesting,” said Taylen.\nWhich is more important—isolating the assignment functionality for change control purposes, or combining assignment and routing into a single service for better performance, error handling, and workflow control?”\n“Well,” said Taylen, “when you put it that way, obviously the single service.\n“OK,” said Addison, “in that case, how about we make three distinct architectural components in the single service.\nLet’s go with a single service then.”\nADR: Consolidated Service for Ticket Assignment and Routing\nThis can be done through a single consolida- ted ticket assignment service or separate services for ticket assignment and ticket routing.\nDecision We will create a single consolidated ticket assignment service for the assignment and routing functions of the ticket.\nMaking these functions separate services would require workflow between them, result- ing in performance, fault tolerance, and possible reliability issues.",
      "keywords": [
        "Service",
        "service granularity",
        "Shared Code",
        "Sysops Squad",
        "single service",
        "Shared",
        "Ticket Assignment",
        "Sysops Squad Saga",
        "ticket assignment service",
        "data",
        "granularity",
        "Ticket Assignment Granularity",
        "separate services",
        "Assignment",
        "Code"
      ],
      "concepts": [
        "services",
        "functions",
        "functionality",
        "function",
        "data",
        "granularity",
        "change",
        "changed",
        "tables",
        "code"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 26,
          "title": "",
          "score": 0.855,
          "base_score": 0.705,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 25,
          "title": "",
          "score": 0.838,
          "base_score": 0.688,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 27,
          "title": "",
          "score": 0.776,
          "base_score": 0.626,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 22,
          "title": "",
          "score": 0.713,
          "base_score": 0.563,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 23,
          "title": "",
          "score": 0.675,
          "base_score": 0.525,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "assignment",
          "routing",
          "service",
          "assignment routing",
          "services"
        ],
        "semantic": [],
        "merged": [
          "assignment",
          "routing",
          "service",
          "assignment routing",
          "services"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.39678247614326057,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.168211+00:00"
      }
    },
    {
      "chapter_number": 25,
      "title": "Segment 25 (pages 231-248)",
      "start_page": 231,
      "end_page": 248,
      "summary": "“Well,” said Austen, “we are struggling with how many services to create for registering customers and maintaining customer-related information, You see, there are four main pieces of data we are dealing with here: profile info, credit card info, password info, and purchased product info.”\nBecause it’s a single service, changes to source code for profile info, credit card, password, or products purchased will increase testing scope and increase deployment risk.\nAs the development team members worked on breaking apart the domain services, they started running into disagreements about what to do with all the shared code and shared functionality.\n“Everyone knows you should have multiple shared libraries in a distributed architecture!”\n“Seems to me it’s much easier to manage a single shared library DLL rather than dozens of them.”\n“The authorization code has to be a shared service, you know——not in a shared library.\"”\n“That code should be in a shared DLL.”\n“It’s got to be in its own separate shared service.”\n“And,” said Skyler, “Taylen is insisting on having multiple shared libraries for the shared functionality rather than a single shared library.”\n“Let’s go over the trade-offs of shared library granularity, and also go over the trade-offs between a shared library and a shared service to see if we can resolve these issues in a more reasonable and thoughtful manner.”\nIn this chapter, we introduce sev‐ eral techniques for managing code reuse within a distributed architecture, including replicating code, shared libraries, shared services, and sidecars within a service mesh.\nWhile it might sound crazy, this technique became popular in the early days of microservices when a lot of confusion and misunderstanding arose about the bounded context concept, hence the drive to create a “share nothing architecture.” In theory, code replication seemed like a good approach at that time to reduce code sharing, but in practice it quickly fell apart.\nIf this were a unique one-off class, it might be worth copying it into each service code repository rather than creating a shared library for it.\nShared Library One of the most common techniques for sharing code is to use a shared library.\nA shared library is an external artifact (such as a JAR file, DLL, and so on) containing source code that is used by multiple services which is typically bound to the service at compile time (see Figure 8-3).\nAlthough the shared library technique seems simple and straightforward, it has its share of complexities and trade-offs, not the least of which is shared library granularity and versioning.\nWith the shared library technique, common code is consolidated and shared at compile time\nDependency Management and Change Control Similar to service granularity (discussed in Chapter 7), there are trade-offs associated with the granularity of a shared library.\nThe two opposing forces that form trade-offs with shared libraries are dependency management and change control.\nNote that while the dependency management is relatively straightforward (each service uses the sin‐ gle shared library), change control is not.\nIf a change occurs to any of the class files in the coarse-grained shared library, every service, whether it cares about the change or not, must eventually adopt the change because of a version deprecation of the shared library.\nThis forces unnecessary retesting and redeployment of all the services using that library, therefore significantly increasing the overall testing scope of a shared library change.\nChanges to coarse-grained shared libraries impact multiple services but keep dependencies low\nBreaking shared code into smaller functionality-based shared libraries (such as secu‐ rity, formatters, annotations, calculators, and so on) is better for change control and overall maintainability, but unfortunately creates a mess in terms of dependency management.\nAs shown in Figure 8-5, a change in shared class C7 impacts only Ser‐ vice D and Service E, but managing the dependency matrix between shared libraries and services quickly starts looking like a big ball of distributed mud (or what some people refer to as a distributed monolith).\nChanges to fine-grained shared libraries impact fewer services but increase dependencies\nThe choice of shared library granularity may not matter much with only a few serv‐ ices, but as the number of services increases, so do the issues associated with change control and dependency management.\nFor example, carving off relatively static functionality such as formatters and security (authentication and authorization) into their own shared libraries isolates this static code, therefore reducing the testing scope and unnecessary version deprecation deployments for other shared functionality.\nWithout versioning, all 10 services would have to be tested and redeployed when making the shared library change, thereby increasing the amount of time and coordination for the shared library change (hence less agility).\nOne of the first complexities of shared library versioning is communicating a version change.\nIn a highly distributed architecture with multiple teams, it is often difficult to communicate a version change to a shared library.\nFor example, if a Security.jar shared library doesn’t change often, maintaining only two or three versions is a rea‐ sonable strategy.\nHowever, if the Calculators.jar shared library changes weekly, main‐ taining only two or three versions means that all services using that shared library will be incorporating a newer version on a monthly (or even weekly) basis—causing a lot of unnecessary frequent retesting and redeployment.\nThe global deprecation strategy dictates that all shared libraries, regardless of the rate of change, will not support more than a certain number of backward versions (for example, four).\nRegardless of the deprecation strategy used, serious defects or breaking changes to shared code invalidate any sort of deprecation strategy, causing all services to adopt the latest version of a shared library at once (or within a very short period of time).\nWhile the shared library technique allows changes to be versioned (therefore provid‐ ing a good level of agility for shared code changes), dependency management can be difficult and messy.\nWhen To Use The shared library technique is a good approach for homogeneous environments where shared code change is low to moderate.\nThe ability to version (although some‐ times complex) allows for good levels of agility when making shared code changes.\nBecause shared libraries are usually bound to the service at compile time, operational characteristics such as performance, scalability, and fault tolerance are not impacted, and the risk of breaking other services with a change to common code is low because of versioning.\nShared Service The primary alternative to using a shared library for common functionality is to use a shared service instead.\nThe shared service technique, illustrated in Figure 8-6, avoids reuse by placing shared functionality in a separately deployed service.\nWith the shared service technique, common functionality is made available at runtime through separate services\nChanges to shared functionality no longer require redeployment of services; rather, since changes are isolated to a separate ser‐ vice, they can be deployed without redeploying other services needing the shared functionality.\nHowever, like everything in software architecture, many trade-offs are associated with using shared services, including change risk, performance, scalability, and fault tolerance.\nChange Risk Changing shared functionality using the shared service technique turns out to be a double-edged sword.\nAs illustrated in Figure 8-7, changing shared functionality is simply a matter of modifying the shared code contained in a separate service (such as a discount calculator), redeploying the service, and voila—the changes are now avail‐ able to all services, without having to retest and redeploy any other service needing that shared functionality.\nShared functionality changes are isolated to only the shared service\nChanges to a shared service can break other services at runtime\nShared Service\nIn the shared library technique, versioning is managed through compile-time bindings, significantly reducing risk associated with a change in a shared library.\nHowever, how does one version a simple shared service change?\nThe immediate response, of course, is to use API endpoint versioning—in other words, create a new endpoint containing each shared service change, as shown in Example 8-3.\nUsing this approach, each time a shared service changes, the team would create a new API endpoint containing a new version of the URI.\nThe bottom line is that with the shared service technique, changes to a shared service are generally runtime in nature, and therefore carry much more risk than with shared libraries.",
      "keywords": [
        "shared library",
        "shared",
        "shared service",
        "service",
        "shared library technique",
        "shared service technique",
        "shared code",
        "shared libraries",
        "code",
        "shared functionality",
        "library",
        "shared library granularity",
        "services Shared Library",
        "functionality",
        "technique"
      ],
      "concepts": [
        "services",
        "code",
        "version",
        "versions",
        "shared",
        "share",
        "changes",
        "changed",
        "said",
        "security"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 24,
          "title": "",
          "score": 0.838,
          "base_score": 0.688,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 26,
          "title": "",
          "score": 0.729,
          "base_score": 0.579,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 27,
          "title": "",
          "score": 0.716,
          "base_score": 0.566,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 23,
          "title": "",
          "score": 0.675,
          "base_score": 0.525,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 22,
          "title": "",
          "score": 0.616,
          "base_score": 0.466,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "shared",
          "library",
          "shared library",
          "shared service",
          "change"
        ],
        "semantic": [],
        "merged": [
          "shared",
          "library",
          "shared library",
          "shared service",
          "change"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3241923812887854,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.168251+00:00"
      }
    },
    {
      "chapter_number": 26,
      "title": "Segment 26 (pages 249-256)",
      "start_page": 249,
      "end_page": 256,
      "summary": "Shared Service\nHowever, as illustrated in Figure 8-10, the shared library technique does not have this issue because the shared functionality is contained within the service at compile time.\nFault Tolerance While fault-tolerance issues can usually be mitigated through multiple instances of a service, nevertheless it is a trade-off to consider when using the shared service techni‐ que.\nThe shared library technique does not have this issue since the shared functionality is contained in the service at compile time, and therefore accessed through standard method or function calls.\nShared services introduce fault-tolerance issues\nWhile the shared service technique preserves the bounded context and is good for shared code that changes frequently, operational characteristics such as performance, scalability, and availability suffer.\nTrade-offs for the shared service technique\nShared Service\nWhen to Use The shared service technique is good to use in highly polyglot environments (those with multiple heterogeneous languages and platforms), and also when shared func‐ tionality tends to change often.\nWhile changes in a shared service tend to be much more agile overall than with the shared library technique, be careful of runtime side- effects and risks to services needing the shared functionality.\nSidecars and Service Mesh Perhaps the most common response to any question posed by an architect is “It depends!” No issue in distributed architectures better illustrates this ambiguity better than operational coupling.\nOne of the design goals of microservices architectures is a high degree of decoupling, often manifested in the advice “Duplication is preferable to coupling.” For example, let’s say that two Sysops Squad services need to pass customer information, yet the domain-driven design bounded context insists that implementation details remain private to the service.\nThus, in microservices architecture, the answer to the question of “should we duplicate or couple to some capability?” is likely duplicate, whereas in another architecture style such as a service-based architecture, the correct answer is likely couple.\nHere, each service includes a split between operational concerns (the larger compo‐ nents toward the bottom of the service) and domain concerns, pictured in the boxes toward the top of the service labeled “domain.” If architects desire consistency in operational capabilities, the separable parts go into a sidecar component, metaphori‐ cally named for the sidecar that attaches to motorcycles, whose implementation is either a shared responsibility across teams or managed by a centralized infrastructure group.\nIf architects can assume that every service includes the sidecar, it forms a con‐ sistent operational interface across services, typically attached via a service plane, shown in Figure 8-14.\nIf architects and operations can safely assume that every service includes the sidecar component (governed by fitness functions), it forms a service mesh, illustrated in Fig‐ ure 8-15.\nThe Sidecar pattern allows governance groups like enterprise architects a reasonable restraint over too many polyglot environments: one of the advantages of microservi‐ ces is a reliance on integration rather than a common platform, allowing teams to choose the correct level of complexity and capabilities on a service-by-service basis.\nFor example, without a service mesh, if enterprise architects want to unify around a common monitoring solution, then teams must build a sidecar per platform that supports that solution.",
      "keywords": [
        "shared service",
        "shared service technique",
        "service",
        "shared",
        "Service Mesh",
        "shared library technique",
        "shared functionality",
        "service technique",
        "shared service scale",
        "Sidecar pattern",
        "pattern",
        "shared library",
        "Sidecar",
        "Shared service introduces",
        "Java Message Service"
      ],
      "concepts": [
        "service",
        "operational",
        "operations",
        "architectures",
        "likely",
        "shared",
        "share",
        "patterns",
        "microservices",
        "architect"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 24,
          "title": "",
          "score": 0.855,
          "base_score": 0.705,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 27,
          "title": "",
          "score": 0.764,
          "base_score": 0.614,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 25,
          "title": "",
          "score": 0.729,
          "base_score": 0.579,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 17,
          "title": "",
          "score": 0.625,
          "base_score": 0.475,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 23,
          "title": "",
          "score": 0.6,
          "base_score": 0.45,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "shared",
          "service",
          "shared service",
          "sidecar",
          "technique"
        ],
        "semantic": [],
        "merged": [
          "shared",
          "service",
          "shared service",
          "sidecar",
          "technique"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3616323489972684,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.168295+00:00"
      }
    },
    {
      "chapter_number": 27,
      "title": "Segment 27 (pages 257-266)",
      "start_page": 257,
      "end_page": 266,
      "summary": "What about monitoring, service discovery, circuit breakers, even some of our utility functions, like the JSON toXML library that a few teams are sharing?\nThat’s why we’re in the process of implementing a service mesh with this common behavior in a sidecar component.”\nSydney said, “I’ve read about sidecars and service mesh—it’s a way to share things across a bunch of microservices, right?”\nIf we consolidate logging code into the common sidecar that every service imple- ments, we can enforce consistency.”\nSydney said, “So if we need to share libraries between services, just ask them to put it in the sidecar?”\nAddison said, “Be careful—the sidecar isn’t meant to be used for just anything, only operational coupling.”\nBut you should never put domain shared compo- nents, like the Address or Customer class, in the sidecar.”\nIn most architectures, a single implementation of that service would be shared across the teams that need it.\n“That’s the key trade-off for shared utility code—how many teams need it versus how much over- head does it add to every service, particularly ones that don’t need it.”\nDecision We will use a sidecar component in conjunction with a service mesh to consolidate shared operational coupling.\nThe shared infrastructure team will own and maintain the sidecar for service teams; service teams act as their customers.\nHowever, all three services used common database logic (queries and updates) and shared a set of database tables in the ticketing data domain.\nTaylen wanted to create a shared data service that would contain the common database logic, thus forming a database abstraction layer, as shown in Figure 8-18.\nOption using a shared Ticket Data service for common database logic for the Sysops Squad ticketing services\nSkyler hated the idea and wanted to use a single shared library (DLL) that each service would include as part of the build and deployment, as illustrated in Figure 8-19.\nOption using a shared library for common database logic for the Sysops Squad ticketing services\nShould the shared database logic be in a shared data service or a shared library?” asked Taylen.\nLet’s do a hypothesis-based approach and hypothesize that the most appropriate solution is to use the shared data service.”\n“First of all,” said Skyler, “all three services would need to make an interservice call to the shared data service for every database query or update.\nFurthermore, if the shared data service goes down, all three of those services become nonoperational.”\nDon’t forget, the Ticket Creation service is customer facing, and it would be using the same shared data service as the backend ticketing functionality.”\n“So far,” said Addison, “it looks like the trade-off for using the shared data service is performance and fault tolerance for the ticketing services.”\n“Let’s also not forget that any changes made to the shared data service are runtime changes.\nIn other words,” said Skyler, “if we make a change and deploy the shared data service, we could possi- bly break something.”\n“Yeah, but if you want to reduce risk you would have to test all of the ticketing services for every change to the shared data service, which increases testing time significantly.\nWith a shared DLL, we could version the shared library to provide backward compatibility,” said Skyler.\n“OK, we will add increased risk for changes and increased testing effort to the trade-offs as well,” said Addison.\nEvery time we create more instances of the ticket creation service, we would have to make sure we create more instances of the shared data service as well.”\n“How about the positives of using a shared data service?”\n“OK,” said Addison, “let’s talk about the benefits of using a shared data service.”\nAll they would have to do is make a remote service call to the shared data service.”\n“Well,” said Taylen, “I was going to say centralized connection pooling, but we would need multiple instances anyway to support the customer ticket creation service.\nHowever, change control would be so much easier with a shared data service.\nWe wouldn’t have to redeploy any of the ticketing services for database logic changes.”\n“Let’s take a look at those shared class files in the repository and see historically how much change there really is for that code,” said Addison.\nOK, so I guess the changes are fairly minimal for the shared database logic after all.”\nThrough the conversation of discussing trade-offs, Taylen started to realize that the negatives of a shared service seemed to outweigh the positives, and there was no real compelling justification for putting the shared database logic in a shared service.\nThe two options are to use a shared library or create a shared data service.",
      "keywords": [
        "shared data service",
        "shared",
        "shared data",
        "service",
        "data service",
        "common database logic",
        "Addison",
        "Ticket Creation service",
        "database logic",
        "shared database logic",
        "shared library",
        "Sidecar",
        "Sysops Squad Saga",
        "Taylen",
        "shared infrastructure team"
      ],
      "concepts": [
        "service",
        "said",
        "shared",
        "share",
        "change",
        "team",
        "architectural",
        "architecture",
        "coupling",
        "reuse"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 24,
          "title": "",
          "score": 0.776,
          "base_score": 0.626,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 26,
          "title": "",
          "score": 0.764,
          "base_score": 0.614,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 25,
          "title": "",
          "score": 0.716,
          "base_score": 0.566,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 22,
          "title": "",
          "score": 0.619,
          "base_score": 0.469,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 23,
          "title": "",
          "score": 0.587,
          "base_score": 0.437,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "shared",
          "data service",
          "shared data",
          "service",
          "database logic"
        ],
        "semantic": [],
        "merged": [
          "shared",
          "data service",
          "shared data",
          "service",
          "database logic"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3890166595746318,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.168341+00:00"
      }
    },
    {
      "chapter_number": 28,
      "title": "Segment 28 (pages 267-278)",
      "start_page": 267,
      "end_page": 278,
      "summary": "While the database team worked on decomposing the monolithic Sysops Squad data- base, the Sysops Squad development team, along with Addison, the Sysops Squad archi- tect, started to work on forming bounded contexts between the services and the data, assigning table ownership to services in the process.\nThe general rule of thumb for assigning table ownership states that services that per‐ form write operations to a table own that table.\nWhile this general rule of thumb works well for single ownership (only one service ever writes to a table), it gets messy when teams have joint ownership (multiple services do writes to the same table) or even worse, common ownership (most or all services write to the table).\nThe general rule of thumb for data ownership is that the service that performs write operations to a table is the owner of that table.\nTo illustrate some of the complexities with data ownership, consider the example illustrated in Figure 9-1 showing three services: a Wishlist Service that manages all of the customer wish lists, a Catalog Service that maintains the product catalog, and an Inventory Service that maintains the inventory and restocking functionality for all products in the product catalog.\nOnce data is broken apart, tables must be assigned to services that own them\nIn this chapter, we unravel this complexity by discussing the three scenarios encoun‐ tered when assigning data ownership to services (single ownership, common owner‐ ship, and joint ownership), and exploring techniques for resolving these scenarios, using Figure 9-1 as a common reference point.\nSingle Ownership Scenario Single table ownership occurs when only one service writes to a table.\nThis diagramming tech‐ nique is an effective way to indicate table ownership and the bounded context formed between the service and its corresponding data.\nWith single ownership, the service that writes to the table becomes the table owner\nCommon Ownership Scenario Common table ownership occurs when most (or all) of the services need to write to the same table.\nFor example, Figure 9-1 shows that all services (Wishlist, Catalog, and Inventory) need to write to the Audit table to record the action performed by the user.\nThe solution of simply putting the Audit table in a shared database or shared schema that is used by all services unfortunately reintroduces all of the data-sharing issues described at the beginning of Chapter 6, including change control, connection starva‐ tion, scalability, and fault tolerance.\nA popular technique for addressing common table ownership is to assign a dedicated single service as the primary (and only) owner of that data, meaning only one service is responsible for writing data to the table.\nComing back to the Audit table example, notice in Figure 9-3 that the architect cre‐ ated a new Audit Service and assigned it ownership of the Audit table, meaning it is the only service that performs read or write actions on the table.\nJoint Ownership Scenario One of the more common (and complex) scenarios involving data ownership is joint ownership, which occurs when multiple services perform write actions on the same table.\nThis scenario differs from the prior common ownership scenario in that with joint ownership, only a couple of services within the same domain write to the same table, whereas with common ownership, most or all of the services perform write\nFor example, notice in Figure 9-1 that all services per‐ form write operations on the Audit table (common ownership), whereas only the Catalog and Inventory services perform write operations on the Product table (joint ownership).\nThe Catalog Service inserts new products into the table, removes products no longer offered, and updates static product information as it changes, whereas the Inventory Service is responsible for reading and updating the current inventory for each product as prod‐ ucts are queried, sold, or returned.\nJoint ownership occurs when multiple services within the same domain per‐ form write operations on the same table\nFortunately, several techniques exist to address this type of ownership scenario—the table split technique, the data domain technique, the delegation technique, and the service consolidation technique.\nTable Split Technique The table split technique breaks a single table into multiple tables so that each service owns a part of the data it’s responsible for.\nSplitting the database table moves the joint ownership to a single table ownership sce‐ nario: the Catalog Service owns the data in the Product table, and the Inventory Ser‐ vice owns the data in the Inventory table.\nHowever, as shown in Figure 9-5, this technique requires communication between the Catalog Service and Inventory Ser‐ vice when products are created or removed to ensure the data remains consistent between the two tables.\nThis is formed when data ownership is shared between the services, thus creating multiple owners for the table.\nWith this technique, the tables shared by the same services are put into the same schema or database, therefore forming a broader bounded context between the services and the data.\nNotice that Figure 9-6 looks close to the original diagram in Figure 9-4 with one noticeable difference—the data domain diagram has the Product table in a separate box outside the context of each owning service.\nBecause a broader bounded context is formed between the services and the data, changes to the shared table structures may require those changes to be coordinated among multiple services.\nAnother issue with the data domain technique with regard to data ownership is con‐ trolling which services have write responsibility to what data.\nTable 9-2 summarizes the trade-offs associated with the data domain technique for the joint ownership scenario.\nWith this technique, one service is assigned single ownership of the table and becomes the delegate, and the other service (or services) communicates with the delegate to perform updates on its behalf.\nThe second option, called operational characteristics priority, assigns table ownership to the service need‐ ing higher operational architecture characteristics, such as performance, scalability, availability, and throughput.\nTo illustrate these two options and the corresponding trade-offs associated with each, consider the Catalog Service and Inventory Service joint ownership scenario shown in Figure 9-4.\nAs illustrated in Figure 9-7, since the Catalog Service performs most of the CRUD operations on product information, the Catalog Service would be assigned as the single owner of the table.\nTable ownership is assigned to the Catalog service because of domain priority\nLike the common ownership scenario described earlier, the delegate technique always forces interservice communication between the other services needing to update the data.\nIn this case, table ownership would be assigned to the Inventory Service, the jus‐ tification being that updating product inventory is a part of the frequent real-time transactional processing of purchasing products as opposed to the more infrequent administrative task of updating product information or adding and removing prod‐ ucts (see Figure 9-8).\nTable ownership is assigned to the Inventory Service because of operational characteristics priority\nRegardless of which service is assigned as the delegate (sole table owner), the delegate technique has some disadvantages, the biggest being service coupling and the need for interservice communication.",
      "keywords": [
        "Sysops Squad data",
        "Catalog Service",
        "Inventory Service",
        "Sysops Squad",
        "service",
        "Sysops Squad archi",
        "monolithic Sysops Squad",
        "Data Ownership",
        "Ownership",
        "Inventory",
        "Data",
        "table ownership",
        "joint ownership",
        "Sysops Squad development",
        "User Maintenance service"
      ],
      "concepts": [
        "tables",
        "data",
        "services",
        "inventory",
        "ownership",
        "product",
        "writes",
        "writing",
        "database",
        "techniques"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 31,
          "title": "",
          "score": 0.76,
          "base_score": 0.61,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 16,
          "title": "",
          "score": 0.691,
          "base_score": 0.541,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 2,
          "title": "",
          "score": 0.684,
          "base_score": 0.534,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 5,
          "title": "",
          "score": 0.671,
          "base_score": 0.521,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 19,
          "title": "",
          "score": 0.656,
          "base_score": 0.506,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ownership",
          "table",
          "inventory",
          "catalog",
          "table ownership"
        ],
        "semantic": [],
        "merged": [
          "ownership",
          "table",
          "inventory",
          "catalog",
          "table ownership"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.36694086716929586,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.168389+00:00"
      }
    },
    {
      "chapter_number": 29,
      "title": "Segment 29 (pages 279-289)",
      "start_page": 279,
      "end_page": 289,
      "summary": "Once table ownership has been assigned to services, an architect must then validate the table ownership assignments by analyzing business workflows and their corre‐ sponding transaction requirements.\nFor example, during the course of an ACID transaction, when the customer profile information is inserted into the Customer Profile table, no other services outside of the ACID transaction scope can access the newly inserted information until the entire transaction is committed.\nThis information is then sent to the single Customer Service, as shown in Figure 9-11, which then performs all of the database activity associated with the customer registration business request.\nFirst, notice that with an ACID transaction, because an error occurred when trying to insert the billing information, both the profile information and support contract information that were previously inserted are now rolled back (that’s the atomicity and consistency parts of ACID).\nWhile not illustrated in the diagram, data inserted into each table during the course of the transaction is not visible to other requests (that’s the isolation part of ACID).\nNote that ACID transactions can exist within the context of each service in a dis‐ tributed architecture, but only if the corresponding database supports ACID proper‐ ties as well.\nEach service can perform its own commits and rollbacks to the tables it owns within the scope of the atomic business transaction.\nHowever, if the business request spans multiple services, the entire business request itself cannot be an ACID transaction—rather, it becomes a distributed transaction.\nDistributed transactions occur when an atomic business request containing multiple database updates is performed by separately deployed remote services.\nAtomicity is not supported because each separately deployed service commits its own data and performs only one part of the overall atomic business request.\nIn a dis‐ tributed transaction, atomicity is bound to the service, not the business request (such as customer registration).\nConsistency is not supported because a failure in one service causes the data to be out of sync between the tables responsible for the business request.\nAs shown in Figure 9-12, since the Billing Payment Service insert failed, the Profile table and Contract table are now out of sync with the Billing table (we’ll show how to address these issues later in this section).\nIsolation is not supported because once the Customer Profile Service inserts the pro‐ file data in the course of a distributed transaction to register a customer, that profile information is available to any other service or request, even though the customer registration process (the current transaction) hasn’t completed.\nWhile asynchronous communication can help decouple services and address availability issues associated with the distributed transaction participants, it unfortunately impacts how long it will take the data to become consistent for the atomic business transaction (see eventual consistency later in this section).\nIn the customer registration example shown in Figure 9-12, soft state occurs when the customer profile information is inserted (and com‐ mitted) in the Profile table, but the support contract and billing information are not.\nThe unknown part of soft state can occur if, using the same example, all three services work in parallel to insert their corresponding data—the exact state of the atomic busi‐ ness request is not known at any point in time until all three services report back that the data has been successfully processed.\nEventual consistency (the E part of BASE) means that given enough time, all parts of the distributed transaction will complete successfully and all of the data is in sync with one another.\nThe type of eventual consistency pattern used and the way errors are handled dictates how long it will take for all of the data sources involved in the distributed transaction to become consistent.\nIn this example, three separate services are involved in the customer reg‐ istration process: a Customer Profile Service that maintains basic profile information, a Support Contract Service that maintains products covered under the Sysops Squad repair plan for each customer, and a Billing Payment Service that charges the cus‐ tomer for the support plan.\nNotice in the figure that customer 123 is a subscriber to the Sysops Squad service, and therefore has data in each of the corresponding tables owned by each service.\nAs shown in Figure 9-14, the Customer Profile Service receives this request from the user interface, removes the customer from the Profile table, and returns a confirmation to the customer that they are successfully unsubscribed and will no longer be billed.\nHowever, data for that customer still exists in the Contract table owned by the Support Contract Service and the Billing table owned by the Billing Payment Service.\nWe will use this scenario to describe each of the eventual consistency patterns for get‐ ting all of the data in sync for this atomic business request.\nBackground Synchronization Pattern The background synchronization pattern uses a separate external service or process to periodically check data sources and keep them in sync with one another.\nThe length of time for data sources to become eventually consistent using this pattern can vary based on whether the background process is implemented as a batch job running sometime in the middle of the night, or a service that wakes up periodically (say, every hour) to check the consistency of the data sources.\nRegardless of the technique used to identify changes, the background process must have knowledge of all the tables and data sources involved in the transaction.\nThe Customer Profile Service receives the request, removes the data, and one second later (11:23:01) responds back to the customer that they have been successfully unsubscribed from the system.\nThe biggest disadvantage of the background synchronization pattern is that it couples all of the data sources together, thus breaking every bounded context between the data and the services.\nNotice in Figure 9-16 that the background batch synchroniza‐ tion process must have write access to each of the tables owned by the corresponding services, meaning that all of the tables effectively have shared ownership between the services and the background synchronization process.\nThis shared data ownership between the services and the background synchroniza‐ tion process is riddled with issues, and emphasizes the need for tight bounded con‐ texts within a distributed architecture.",
      "keywords": [
        "Customer Profile Service",
        "service",
        "Distributed Transactions",
        "ACID transaction",
        "data",
        "Billing Payment Service",
        "transaction",
        "Service Consolidation Technique",
        "customer profile",
        "Data Ownership",
        "customer",
        "Support Contract Service",
        "Profile Service",
        "ACID",
        "Customer Profile table"
      ],
      "concepts": [
        "service",
        "tables",
        "transactions",
        "data",
        "process",
        "processed",
        "consistency",
        "consistent",
        "business",
        "inserted"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 28,
          "title": "",
          "score": 0.653,
          "base_score": 0.503,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 40,
          "title": "",
          "score": 0.65,
          "base_score": 0.5,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 31,
          "title": "",
          "score": 0.631,
          "base_score": 0.481,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 19,
          "title": "",
          "score": 0.631,
          "base_score": 0.481,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 23,
          "title": "",
          "score": 0.619,
          "base_score": 0.619,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "transaction",
          "profile",
          "customer",
          "customer profile",
          "acid"
        ],
        "semantic": [],
        "merged": [
          "transaction",
          "profile",
          "customer",
          "customer profile",
          "acid"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3302646597164889,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.168434+00:00"
      }
    },
    {
      "chapter_number": 30,
      "title": "Segment 30 (pages 290-300)",
      "start_page": 290,
      "end_page": 300,
      "summary": "Orchestrated Request-Based Pattern A common approach for managing distributed transactions is to make sure all of the data sources are synchronized during the course of the business request (in other words, while the end user is waiting).\nThe orches‐ trator, which can be a designated existing service or a new separate service, is respon‐ sible for managing all of the work needed to process the request, including knowledge of the business process, knowledge of the participants involved, multicast‐ ing logic, error handling, and contract ownership.\nThis technique, illus‐ trated in Figure 9-17, designates one of the services to take on the role as orchestrator in addition to its other responsibilities, which in this case is the Customer Profile Service.\nIn addition to the role of an orchestrator, the designated service manag‐ ing the distributed transaction must perform its own responsibilities as well.\nThe approach we generally prefer when using the orchestrated request-based pattern is to use a dedicated orchestration service for the business request.\nThis approach, illustrated in Figure 9-18, frees up the Customer Profile Service from the responsibil‐ ity of managing the distributed transaction and places that responsibility on a sepa‐ rate orchestration service.\nWe will use this separate orchestration service approach to describe how this eventual consistency pattern works and the corresponding trade-offs with this pattern.\nThe request is received by the Unsubscribe Orchestrator Service, which then forwards the request synchronously to the Customer Profile Service to remove the customer from the Profile table.\nOne second later, the Customer Profile Service sends back an acknowledgment to the Unsubscribe Orchestrator Service, which then sends parallel requests (either through threads or some sort of asynchro‐ nous protocol) to both the Support Contract and Billing Payment Services.\nNow that all data is in sync, the Unsubscribe Orchestra‐ tor Service responds back to the client at 11:23:02 (two seconds after the initial request was made), letting the customer know they were successfully unsubscribed.\nResponse time could be improved in Figure 9-18 by executing the Customer Profile request at the same time as the other services, but we chose to do that operation syn‐ chronously for error handling and consistency reasons.\nFor example, if the customer could not be deleted from the Profile table because of an outstanding billing charge, no other action is needed to reverse the operations in the Support Contract and Bill‐ ing Payment Services.\nWhile the orchestrated request-based pattern might seem straightforward, con‐ sider what happens when the customer is removed from the Profile table and Contract table, but an error occurs when trying to remove the billing information from the Billing table, as illustrated in Figure 9-19.\nSince the Profile and Support Contract Services individually committed their operations, the Unsubscribe Orches‐ trator Service must now decide what action to take while the customer is waiting for the request to be processed:\n2. Should the orchestrator perform a compensating transaction and have the Sup‐ port Contract and Customer Profile Services reverse their update operations?\nTable 9-6 summarizes the trade-offs for the orchestrated request-based pattern for eventual consistency.\nServ‐ ices are highly decoupled from one another with this pattern, and responsiveness is good because the service triggering the eventual consistency event doesn’t have to wait for the data synchronization to occur before returning information to the customer.\nThe Customer Profile Service receives the request, removes the cus‐ tomer from the Profile table, publishes a message to a message topic or event stream, and returns information one second later letting the customer know they were suc‐ cessfully unsubscribed.\nAt around the same time this happens, both the Support Con‐ tract and Billing Payment Services receive the unsubscribe event and perform whatever functionality is needed to unsubscribe the customer, making all the data sources eventually consistent.\nThe advantages of the event-based pattern are responsiveness, timeliness of data con‐ sistency, and service decoupling.\nTable 9-7 lists the trade-offs for the event-based pattern for eventual consistency.\nSo, from what Dana said, the service that performs write actions on the data table owns the table, regardless of what other services need to access the data in a read-only manner.\nContext When forming bounded contexts between services and data, tables must be assigned ownership to a particular service or group of services.\nTherefore, for single table ownership scenarios, regardless of how many other services need to access the table, only one service is ever assigned an owner, and that owner is the service that maintains the data.\nConsequences Depending on the technique used, services requiring read-only access to a table in another bounded context may incur performance and fault-tolerance issues when access- ing data in a different bounded context.\nNow that Sydney and Addison better understood table ownership and how to form bounded con- texts between the service and the data, they started to work on the survey functionality.\n“Both the Ticket Completion Service and the Survey Service write to the Survey table.”\nLet both services write to the table and share a common schema,” said Sydney.\nAddison and Sydney agreed that the Survey Service would own the Survey table, and would use the delegation technique to pass data when the table notifies the Survey Service to kick off the survey process as illustrated in Figure 9-21.\nContext Both the Ticket Completion Service and the Survey Service write to the Survey table.\nOnce a ticket is marked as complete and is accepted by the system, the Ticket Completion Service needs to send a message to the Survey Service to kick off the customer survey processing.\nthe necessary ticket information can be passed along with that event, thus eliminating the need for the Ticket Completion Service to have any access to the Survey table.\nConsequences All of the necessary data that the Ticket Completion Service needs to insert into the Sur- vey table will need to be sent as part of the payload when triggering the customer survey process.",
      "keywords": [
        "Customer Profile Service",
        "Ticket Completion Service",
        "Survey Service",
        "service",
        "Profile Service",
        "Completion Service",
        "Customer Profile",
        "Unsubscribe Orchestrator Service",
        "Billing Payment Service",
        "Data",
        "survey table",
        "Data Ownership",
        "survey",
        "Ticket Completion",
        "Customer"
      ],
      "concepts": [
        "services",
        "tables",
        "data",
        "process",
        "processing",
        "orchestrated",
        "orchestrator",
        "orchestration",
        "transaction",
        "pattern"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 32,
          "title": "",
          "score": 0.744,
          "base_score": 0.594,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 38,
          "title": "",
          "score": 0.735,
          "base_score": 0.585,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 34,
          "title": "",
          "score": 0.706,
          "base_score": 0.556,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 33,
          "title": "",
          "score": 0.69,
          "base_score": 0.54,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 35,
          "title": "",
          "score": 0.671,
          "base_score": 0.521,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "service",
          "survey",
          "profile",
          "table",
          "request"
        ],
        "semantic": [],
        "merged": [
          "service",
          "survey",
          "profile",
          "table",
          "request"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4047905841629409,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.168479+00:00"
      }
    },
    {
      "chapter_number": 31,
      "title": "Segment 31 (pages 301-316)",
      "start_page": 301,
      "end_page": 316,
      "summary": "“Now that we’ve assigned ownership of the expert profile table to the User Management Service,” said Sydney, “how should the Ticket Assignment Service get to the expert loca- tion and skills data?\n“But our only other option is to make remote calls to the User Management Service every time the assignment algorithm needs expert data,” said Addison.\n“There’s got to be another solution to access data we no longer own,” said Addison.\nThe four patterns of data access we discuss in this chapter include the Inter- service Communication pattern, Column Schema Replication pattern, Replicated Cache pattern, and the Data Domain pattern.\nHowever, the Wishlist Service does not have the item description in its table; that data is owned by the Catalog Service in a tightly formed bounded context provid‐ ing change control and data ownership.\nTherefore, the architect must use one of the data access patterns outlined in this chapter to ensure the Wishlist Service can obtain the product descriptions from the Catalog Service.\nWishlist Service needs item descriptions but doesn’t have access to the product table containing the data\nTable 10-1 summarizes the trade-offs associated with the interservice communication data access pattern.\nTrade-offs for the Interservice Communication data access pattern\nAs shown in Figure 10-3, the item_desc column is added to the Wishlist table, making that data available to the Wishlist Service without having to ask the Catalog Service for the data.\nWith the Column Schema Replication data access pattern, data is replicated to other tables\nData synchronization and data consistency are the two biggest issues associated with the Column Schema Replication data access pattern.\nWhenever a product is created, removed from the catalog, or a product description changed, the Catalog Service must somehow let the Wishlist Service (and any other services replicating the data) know about the change.\nEven though the services are still coupled because of data synchronization, the service requiring read access has immediate access to the data, and can do simple SQL joins or queries to its own table to get the data.\nWhile in general we caution against use of this data access pattern for scenarios such as the Wishlist Service and Catalog Service example, some situations where it might be a consideration are data aggregation, reporting, or situations where the other data access patterns are not a good fit because of large data volumes, high responsiveness requirements, or high-fault tolerance requirements.\nTable 10-2 summarizes the trade-offs associated with the Column Schema Replica‐ tion data access pattern.\nTrade-offs for the Column Schema Replication data access pattern\nThis pattern leverages replicated in-memory caching so that data needed by other services is made available to each service without them having to ask for it.\nA replicated cache differs from other caching models in that data is held in-memory within each service and is con‐ tinuously synchronized so that all services have the same exact data at all times.\nWith this caching model (illustrated in Figure 10-4), in-memory data is not synchronized between the caches, meaning each service has its own unique data specific to that service.\nWhile this caching model does help increase responsive‐ ness and scalability within each service, it’s not useful for sharing data between serv‐ ices because of the lack of cache synchronization between the services.\nWith a single in-memory cache, each service contains its own unique data\nAs illustrated in Figure 10-5, with this caching model, data is not held in-memory within each service, but rather held externally within a caching server.\nNote that unlike the single in-memory caching model, data can be shared among the services.\nThe distributed cache model is not an effective caching model to use for the replica‐ ted caching data access pattern for several reasons.\nBecause the cache data is centralized and shared, the distributed cache model allows other services to update data, thereby breaking the bounded context regarding data ownership.\nLastly, since access to the centralized distributed cache is through a remote call, net‐ work latency adds additional retrieval time for the data, thus impacting overall responsiveness as compared to an in-memory replicated cache.\nWith replicated caching, each service has its own in-memory data that is kept in sync between the services, allowing the same data to be shared across multiple services.\nWith a replicated cache, each service contains the same in-memory data\nTo see how replicated caching can address distributed data access, we’ll return to our Wishlist Service and Catalog Service example.\nReplicated caching data access pattern\nWhen updates are made to the product description by the Catalog Service, the caching product will update the cache in the Wishlist Service to make the data consistent.\nThe first trade-off with this pattern is a service dependency with regard to the cache data and startup timing.\nNotice that only the initial Wishlist Service instance is impacted by this startup dependency; if the Catalog Service is down, other Wishlist instances can be started up, with the cache data transferred from one of the other Wishlist instances.\nIt’s also important to note that once the Wishlist Service starts and has the data in the cache, it is not necessary for the Catalog Service to be available.\nArchitects must analyze both the size of the cache and the total number of services instances needing the cached data to determine the total memory requirements for the replicated cache.\nA third trade-off is that the replicated caching model usually cannot keep the data fully in sync between services if the rate of change of the data (update rate) is too high.\nTable 10-3 lists the trade-offs associated with the replicated cache data access pattern.\nTrade-offs associated with the replicated caching data access pattern\nData Domain Pattern In the previous chapter, we discussed the use of a data domain to resolve joint owner‐ ship, where multiple services both need to write data to the same table.\nThat same pattern can be used for data access as well.\nSuppose the Interservice Communication pattern is not a feasible solution because of reliability issues with the Catalog Service as well as the performance issues with network latency and the additional data retrieval.\nFinally, suppose that the Replicated Cache pattern isn’t an option because of the high data volumes.\nThe only other solution is to create a data domain, combining the Wishlist and Product tables in the same shared schema, accessible to both the Wishlist Service and the Catalog Service.\nResponsiveness is very good with this pattern because the data is available using a normal SQL call, removing the need to do additional data aggregations within the functionality of the service (as is required with the Replicated Cache pattern).\nSince multiple services access the same data tables, data does not need to be transferred, replicated, or synchronized.\nThe contracts used with the interservice communication pattern and the Replicated Cache pattern form an abstraction layer over the table schema, allowing changes to the table structures to remain within a tight bounded context and not impact other services.\nHowever, this pattern forms a broader boun‐ ded context, requiring multiple services to possibly change when the structure to any of the tables in the data domain changes.\nFor example, in Figure 10-8 the Wishlist Service has com‐ plete access to all the data within the data domain.\nWhile this is OK in the Wishlist and Catalog Service example, there might be times when services accessing the data domain shouldn’t have access to certain data.\nTable 10-4 lists trade-offs associated with the data domain data access pattern.\nTrade-offs associated with the data domain data access pattern\n“Unless we start consolidating all of these services, I guess we are stuck with the fact that the Ticket Assignment needs to somehow get to the expert profile data, and fast,” said Taylen.\n“So service consolidation is out because these services are in entirely different domains, and the shared data domain option is out for the same reasons we talked about before— we cannot have the Ticket Assignment Service connecting to two different databases.”\nWhat data does the Ticket Assignment Service need from the expert profile table?”\n“Let’s not forget that if we used a replicated cache, we would have to take into account how many instances we would have for the User Management Service as well as the Ticket Assignment Ser- vice,” said Addison.\nI suggest that we should go with the in-memory replicated cache option to cache only the data necessary for the Ticket Assignment Service.\n“As long as the cache is populated, then the Ticket Assignment Service would be fine,” said Addison.\n“Wait, you mean to tell me that the data would be in-memory, even if the User Management Service is unavailable?” asked Taylen.\nAccess to the expert profile information can be done through interservice communication, in-memory replicated caching, or a common data domain.",
      "keywords": [
        "Wishlist Service",
        "Ticket Assignment Service",
        "Catalog Service",
        "User Management Service",
        "data access pattern",
        "Data Access",
        "Service",
        "Data",
        "Distributed Data Access",
        "Assignment Service",
        "Management Service",
        "Data Access Monday",
        "initial Wishlist Service",
        "data domain",
        "access pattern"
      ],
      "concepts": [
        "data",
        "services",
        "cache",
        "caching",
        "tables",
        "access",
        "accessing",
        "patterns",
        "issue",
        "sydney"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 28,
          "title": "",
          "score": 0.76,
          "base_score": 0.61,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 40,
          "title": "",
          "score": 0.687,
          "base_score": 0.537,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 29,
          "title": "",
          "score": 0.631,
          "base_score": 0.481,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 44,
          "title": "",
          "score": 0.598,
          "base_score": 0.448,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 45,
          "title": "",
          "score": 0.592,
          "base_score": 0.442,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data",
          "cache",
          "wishlist",
          "service",
          "access"
        ],
        "semantic": [],
        "merged": [
          "data",
          "cache",
          "wishlist",
          "service",
          "access"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.32803653545345635,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.168525+00:00"
      }
    },
    {
      "chapter_number": 32,
      "title": "Segment 32 (pages 317-326)",
      "start_page": 317,
      "end_page": 326,
      "summary": "I’d be happy to write up the ADR that states that we’re only going to use choreography in the new architecture to keep things decoupled.”\nIn Chapter 2, we identified three coupling forces when considering interaction mod‐ els in distributed architectures: communication, consistency, and coordination, shown in Figure 11-1.\nOrchestration versus choreography in distributed architectures\nOrchestration Communication Style The orchestration pattern uses an orchestrator (sometimes called a mediator) compo‐ nent to manage workflow state, optional behavior, error handling, notification, and a host of other workflow maintenance.\nNotice that micro‐ services architectures have an orchestrator per workflow, not a global orchestrator such as an enterprise service bus (ESB).\nOrchestration is useful when an architect must model a complex workflow that includes more than just the single “happy path,” but also alternate paths and error conditions.\nThis system passes the Place Order request to the Order Placement Orchestrator, which makes a synchronous call to the Order Placement Service, which records the order and returns a status message.\nNext, the orchestrator makes an asynchronous call to the Fulfillment Service to handle the order.\nSimilarly, the orchestrator then calls the Email Service to notify the user of a successful electronics order.\nHere, the Order Placement Orchestrator updates the order via the Order Placement Service as before.\nIn that case, the Pay‐ ment Service notifies the orchestrator, which then places a (typically) asynchronous call to send a message to the Email Service to notify the customer of the failed order.\nAdditionally, the orchestrator updates the state of the Order Placement Service, which still thinks this is an active order.\nIn the second error scenario, the workflow has progressed further along: what hap‐ pens when the Fulfillment Service reports a back order?\nAs you can see, the workflow preceeds as normal until the Fulfillment Service notifies the orchestrator that the current item is out of stock, necessitating a back order.\nIn that case, the orchestrator must refund the payment (this is why many online services don’t charge until shipment, not at order time) and update the state of the Order Placement Service.\nOne interesting characteristic to note in Figure 11-6: even in the most elaborate error scenarios, the architect wasn’t required to add additional communication paths that weren’t already there to facilitate the normal workflow, which differs from the “Chor‐ eography Communication Style” on page 306.\nGeneral advantages of the orchestration communication style include the following:\nBecause an orchestrator monitors the state of the workflow, an architect may add logic to retry if one or more domain services suffers from a short-term outage.\nGeneral disadvantages of the orchestration communication style include the following:\nWhile orchestration enhances recoverability for domain services, it creates a potential single point of failure for the workflow, which can be addressed with redundancy but adds more complexity.\nThis communication style doesn’t scale as well as choreography because it has more coordination points (the orchestrator), which cuts down on potential paral‐ lelism.\nThe orchestration communication style’s trade-offs appear in Table 11-1.\nChoreography Communication Style Whereas the Orchestration Communication Style was named for the metaphorical central coordination offered by an orchestrator, the choreography pattern visually illustrates intent of the communication style that has no central coordination.\nFigure 11-4 described the orchestrated workflow for a customer purchasing electron‐ ics from Penultimate Electronics; the same workflow modeled in the choreography communication style appears in Figure 11-7.\nOnce the Fulfillment Service realizes the error condition, it should generate events suited to its bounded context, perhaps a broadcast message subscribed to by the Email, Payment, and Order Placement services.\nWhile the initial workflow in choreography illustrated in Figure 11-7 seemed simpler than Figure 11-4, the error case (and others) keeps adding more complexity to the choreographed solution.\nIn Figure 11-10, each error scenario forces domain services to interact with each other, adding communication links that weren’t necessary for the happy path.",
      "keywords": [
        "Order Placement Service",
        "Orchestration Communication Style",
        "Managing Distributed Workflows",
        "Communication Style",
        "service",
        "Distributed Workflows Tuesday",
        "Order Placement",
        "Order",
        "Fulfillment Service",
        "Placement Service",
        "Orchestration Communication",
        "Order Placement Orchestrator",
        "workflow",
        "communication",
        "Choreography Communication Style"
      ],
      "concepts": [
        "services",
        "workflows",
        "orchestration",
        "orchestrator",
        "orchestrated",
        "error",
        "communication",
        "order",
        "patterns",
        "payment"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 33,
          "title": "",
          "score": 0.897,
          "base_score": 0.747,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 34,
          "title": "",
          "score": 0.782,
          "base_score": 0.632,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 30,
          "title": "",
          "score": 0.744,
          "base_score": 0.594,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 35,
          "title": "",
          "score": 0.743,
          "base_score": 0.593,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 7,
          "title": "",
          "score": 0.729,
          "base_score": 0.579,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "order",
          "communication style",
          "placement",
          "order placement",
          "orchestrator"
        ],
        "semantic": [],
        "merged": [
          "order",
          "communication style",
          "placement",
          "order placement",
          "orchestrator"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.40158969191280375,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.168568+00:00"
      }
    },
    {
      "chapter_number": 33,
      "title": "Segment 33 (pages 327-334)",
      "start_page": 327,
      "end_page": 334,
      "summary": "Every workflow that architects need to model in software has a certain amount of semantic coupling—the inherent coupling that exists in the problem domain.\nThe semantic coupling of a workflow is mandated by the domain requirements of the solution and must be modeled somehow.\nIf the architect has organized their architecture the same as the domains, the implementation of the workflow should have similar complexity.\nWorkflow State Management Most workflows include transient state about the status of the workflow: what ele‐ ments have executed, which ones are left, ordering, error conditions, retries, and so on.\nFor orchestrated solutions, the obvious workflow state owner is the orchestrator (although some architectural solutions create stateless orchestrators for higher scale).\nHowever, for choreography, no obvious owner for workflow state exists.\nIf that service contains information about both orders and the state of the workflow, some of the domain services must have a communication link to query and update the order state, as illustrated in Figure 11-13.\nIn choreography, a Front Controller is a domain service that owns workflow state in addition to domain behavior\nWhile this simplifies the workflow, it increases communication overhead and makes the Order Placement Ser‐ vice more complex than one that handled only domain behavior.\nAdds additional workflow state to a domain service\nA second way for an architect to manage the transactional state is to keep no transi‐ ent workflow state at all, relying on querying the individual services to build a real- time snapshot.\nFor example, consider a workflow like the simple choreography happy path in Figure 11-7 with no extra state.\nIf a customer wants to know the state of their order, the architect must build a workflow that queries the state of each domain service to determine the most up-to-date order status.\nWhile this makes for a highly flexible solution, rebuilding state can be complex and costly in terms of operational architecture characteristics like scalability and performance.\nStateless choreography trades high performance for workflow control, as illustrated in Table 11-3.\nA third solution utilizes stamp coupling (described in more detail in “Stamp Coupling for Workflow Management” on page 378), storing extra workflow state in the mes‐ sage contract sent between services.\nThis is a partial solution, as it still does not provide a single place for users to query the state of the ongoing workflow.\nHowever, it does provide a way to pass the state between services as part of the workflow, providing each service with additional potentially useful context.\nAllows domain services to pass workflow state without additional queries to a state owner\nError handling becomes more difficult without an orchestrator because the domain services must have more workflow knowledge.\nTrade-offs for the choreography communication style\nTrade-Offs Between Orchestration and Choreography As with all things in software architecture, neither orchestration nor choreography represent the perfect solution for all possibilities.\nState Owner and Coupling As illustrated in Figure 11-13, state ownership typically resides somewhere, either in a formal mediator acting as an orchestrator, or a front controller in a choreographed solution.\nFor example, if an architect has a workflow that needs higher scale and typically has few error conditions, it might be worth trading the higher scale of choreography with the complexity of error handling.\nHowever, as workflow complexity goes up, the need for an orchestrator rises propor‐ tionally, as illustrated in Figure 11-14.\nIn addition, the more semantic complexity contained in a workflow, the more utili‐ tarian an orchestrator is.\nUltimately, the sweet spot for choreography lies with workflows that need responsive‐ ness and scalability, and either don’t have complex error scenarios or they are infre‐ quent.\nOn the other hand, orchestration is best suited for complex workflows that include boundary and error conditions.",
      "keywords": [
        "workflow",
        "Workflow State",
        "State",
        "Choreography Communication Style",
        "Managing Distributed Workflows",
        "Communication Style",
        "choreography",
        "domain",
        "Front Controller pattern",
        "communication",
        "Choreography Communication",
        "Distributed Workflows",
        "coupling",
        "Front Controller",
        "service"
      ],
      "concepts": [
        "workflow",
        "state",
        "architecture",
        "architectural",
        "saga",
        "patterns",
        "domain",
        "service",
        "choreography",
        "solution"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 32,
          "title": "",
          "score": 0.897,
          "base_score": 0.747,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 34,
          "title": "",
          "score": 0.819,
          "base_score": 0.669,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 37,
          "title": "",
          "score": 0.739,
          "base_score": 0.589,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 35,
          "title": "",
          "score": 0.728,
          "base_score": 0.578,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 30,
          "title": "",
          "score": 0.69,
          "base_score": 0.54,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "state",
          "workflow",
          "choreography",
          "workflow state",
          "error"
        ],
        "semantic": [],
        "merged": [
          "state",
          "workflow",
          "choreography",
          "workflow state",
          "error"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3907745316340201,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.168612+00:00"
      }
    },
    {
      "chapter_number": 34,
      "title": "Segment 34 (pages 335-344)",
      "start_page": 335,
      "end_page": 344,
      "summary": "Sysops Squad Saga: Managing Workflows Thursday, March 15, 11:00\n“Sure,” said Logan.\nAre y’all ready to talk about workflow options for the primary ticket flow?”\n“Yes!” said Austen.\n“Give me an overview of the workflow we’re looking at.”\n“It’s the primary ticket workflow,” said Addison.\n5. The Ticket Management Service communicates with the Survey Service to tell the customer to\n“Have you modeled both solutions?” asked Logan.\n“Yes. The drawing for choreography is in Figure 11-15.”\n“…and the model for orchestration is in Figure 11-16.”\nPrimary ticket workflow modeled as orchestration\n“OK, which handles that problem better—orchestration or choreography?”\n“Easier control of the workflow sounds like the orchestrator version is better—we can handle all the workflow issues there,” volunteered Austen.\nTrade-off between orchestration and choreography for ticket workflow\n“What’s the next issue we should model?” Addison asked.\nThat implies we need an orchestrator so that we can query the state of the workflow.”\n“But you don’t have to have an orchestrator for that—we can query any given service to see if it has handled a particular part of the workflow, or use stamp coupling,” said Addison.\nUpdated trade-offs between orchestration and choreography for ticket workflow\n“OK, what else?”\n“Just one more that I can think of,” Addison said.\nThat means orchestration?”\nFinal trade-offs between orchestration and choreography for ticket workflow\nAny more?”\nADR: Use Orchestration for Primary Ticket Workflow\nDecision We will use orchestration for the primary ticketing workflow.\nWhen I showed Addison my design for the Ticketing workflow, I was immedi- ately instructed to come to you and tell you I’ve created a horror story.”\nYou designed a workflow with asynchronous communication, atomic transactionality, and choreography, right?”\nThere are eight generic saga patterns we start from, so it’s good to know what they are, because each has a different balance of trade-offs.”\nFor example, the Epic Saga(sao) pattern indicates the values of synchronous, atomic, and orchestrated for com‐ munication, consistency, and coordination.\nWe illustrate each possible communication combination with both a three- dimensional representation of the intersection of the three forces in space along with an example workflow using generic distributed services, which we refer to as isomor‐ phic diagrams.\nEpic Saga(sao) Pattern This type of communication is the “traditional” saga pattern as many architects understand it, also called an Orchestrated Saga because of its coordination type.\nThe Epic Saga(sao) pattern’s dynamic coupling (communication, consistency, coordination) relationships\nThe isomorphic representation of the Epic Saga(sao) pattern appears in Figure 12-3.\nThe isomorphic communication illustration of the Epic Saga(sao) pattern",
      "keywords": [
        "Saga",
        "ticket workflow Orchestration",
        "primary ticket workflow",
        "ticket workflow",
        "Sysops Squad Saga",
        "ticket",
        "workflow",
        "workflow Orchestration Choreography",
        "Epic Saga",
        "saga pattern",
        "Transactional Saga Patterns",
        "Ticket Assignment Service",
        "Ticket Management Service",
        "Squad Saga",
        "primary ticket"
      ],
      "concepts": [
        "saga",
        "workflows",
        "pattern",
        "orchestration",
        "orchestrator",
        "orchestrated",
        "ticket",
        "services",
        "said",
        "communicates"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 33,
          "title": "",
          "score": 0.819,
          "base_score": 0.669,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 32,
          "title": "",
          "score": 0.782,
          "base_score": 0.632,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 30,
          "title": "",
          "score": 0.706,
          "base_score": 0.556,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 38,
          "title": "",
          "score": 0.695,
          "base_score": 0.545,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 37,
          "title": "",
          "score": 0.666,
          "base_score": 0.516,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "workflow",
          "ticket workflow",
          "ticket",
          "orchestration",
          "saga"
        ],
        "semantic": [],
        "merged": [
          "workflow",
          "ticket workflow",
          "ticket",
          "orchestration",
          "saga"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3674806157421286,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.168654+00:00"
      }
    },
    {
      "chapter_number": 35,
      "title": "Segment 35 (pages 345-357)",
      "start_page": 345,
      "end_page": 357,
      "summary": "Consider a common implementation of the Epic Saga(sao) pattern, utilizing compen‐ sating transactions.\nA compensating transaction pattern assigns a service to monitor the transactional completeness of a request, as shown in Figure 12-4.\nTransactional Saga Patterns\nBecause the goal of the Epic Saga(sao) is atomic consistency, the mediator must utilize compensating transactions and request that the other two services undo the operation from before, returning the overall state to what it was before the transac‐ tion started.\nMany architects default to the Epic Saga(sao) pattern because it feels familiar to monolithic architectures, combined with a request (sometimes demand) from stakeholders that state changes must synchronize, regardless of technical con‐ straints.\nThe clear advantage of the Epic Saga(sao) is the transactional coordination that mimics monolithic systems, coupled with the clear workflow owner represented via an orchestrator.\nThe Epic Saga(sao) pattern features the following characteristics:\nThis pattern exhibits extremely high levels of coupling across all possible dimen‐ sions: synchronous communication, atomic consistency, and orchestrated coor‐ dination—it is in fact the most highly coupled pattern in the list.\nRatings for the Epic Saga(sao) Epic Saga(sao) pattern Communication\nTransactional Saga Patterns\nPhone Tag Saga(sac) Pattern The Phone Tag Saga(sac) pattern changes one of the dimensions of the Epic Saga(sao), changing coordination from orchestrated to choreographed; this change is illustrated in Figure 12-6.\nThe Phone Tag Saga(sac) pattern features atomicity but also choreography, meaning that the architect designates no formal orchestrator.\nTransactional Saga Patterns\nHowever, this pattern also features lower performance for error conditions and other workflow complexities—without a mediator, the workflow must be resolved via com‐ munication between services, which impacts performance.\nEven though this pattern utilizes synchronous requests, fewer wait conditions for happy path workflows exist, allowing for higher scale.\nWith the improved scalability brought about because of a lack of orchestration comes the increased complexity of the domain services to manage the workflow concerns in addition to their nominal responsibility.\nIf error conditions are easy to resolve, or domain services can utilize idempotence and retries, then architects can build higher parallel scale using this pattern compared to an Epic Saga(sao).\nThis pattern relaxes one of the coupling dimensions of the Epic Saga(sao) pattern, utilizing a choreographed rather than orchestrated workflow.\nThus, this pattern is slightly less coupled, but with the same transactional requirement, meaning that the complexity of the workflow must be distributed between the domain services.\nThis pattern is significantly more complex than the Epic Saga(sao); complexity in this pattern rises linearly proportionally to the semantic complexity of the work‐ flow: the more complex the workflow, the more logic must appear in each service to compensate for lack of orchestrator.\nLess orchestration generally leads to better responsiveness, but error conditions in this pattern become more difficult to model without an orchestrator, requiring more coordination via callbacks and other time-consuming activities.\nThe Phone Tag Saga(sac) pattern is better for simple workflows that don’t have many common error conditions.\nFairy Tale Saga(seo) Pattern Typical fairy tales provide happy stories with easy-to-follow plots, thus the name Fairy Tale Saga(seo), which utilizes synchronous communication, eventual consistency, and orchestration, as shown in Figure 12-8.\nTransactional Saga Patterns\nIn this pattern, an orchestrator exists to coordinate request, response, and error han‐ dling.\nHowever, the orchestrator isn’t responsible for managing transactions, which each domain service retains responsibility for (for examples of common workflows, see Chapter 11).\nHaving a mediator makes managing workflows easier, synchronous communication is the easier of the two choices, and eventual consistency removes the most difficult coordination challenge, especially for error handling.\nThe Fairy Tale Saga(seo) features high coupling, with two of the three coupling drivers maximized in this pattern (synchronous communication and orchestra‐ ted coordination).\nComplexity for the Fairy Tale Saga(seo) is quite low; it includes the most conve‐ nient options (orchestrated, synchronicity) with the loosest restriction (eventual consistency).\nTransactional Saga Patterns\nTime Travel Saga(sec) Pattern The Time Travel Saga(sec) pattern features synchronous communication, and eventual consistency, but choreographed workflow.\nIn other words, this pattern avoids a cen‐ tral mediator, placing the workflow responsibilities entirely on the participating domain services, as illustrated in Figure 12-10.\nTransactional Saga Patterns\nEach service in this pattern “owns” its own trans‐ actionality, so architects must design workflow error conditions into the domain design.\nIt is called Time Travel Saga(sec) because everything is decoupled from a time standpoint: each service owns its own transactional context, making workflow consistency temporally gradual—the state will become consistent over time based on the design of the interaction.\nThe lack of transactions in the Time Travel Saga(sec) pattern makes workflows easier to model; however, the lack of an orchestrator means that each domain service must include most workflow state and information.\nLack of coupling increases scalability with this pattern; only adding asynchronicity would make it more scalable (as in the Anthology Saga(aec) pattern).\nHowever, because this pattern lacks holistic transactional coordination, architects must take extra effort to synchronize data.\nThe coupling level falls in the medium range with the Time Travel Saga(sec), with the decreased coupling brought on by the absence of an orchestrator balanced by the still remaining coupling of synchronous communication.\nAs with all eventual consistency patterns, the absence of transactional coupling eases many data concerns.\nResponsiveness scores a medium with this architectural pattern: it is quite high for built-to-purpose systems, as described previously, and quite low for complex error handling.\nBecause no orchestrator exists in this pattern, each domain ser‐ vice must handle the scenario to restore eventual consistency in the case of an error condition, which will cause a lot of overhead with synchronous calls, impacting responsiveness and performance.\nThe ratings for the Time Travel Saga(sec) pattern appear in Table 12-5.\nThe Time Travel Saga(sec) pattern provides an on-ramp to the more complex but ulti‐ mately scalable Anthology Saga(aec) pattern.\nTransactional Saga Patterns",
      "keywords": [
        "Fairy Tale Saga",
        "Phone Tag Saga",
        "Time Travel Saga",
        "Transactional Saga Patterns",
        "Epic Saga",
        "Saga",
        "Tale Saga",
        "Tag Saga",
        "Travel Saga",
        "Saga Patterns",
        "pattern",
        "Transactional Saga",
        "transactional Epic Saga",
        "Phone Tag",
        "Fairy Tale"
      ],
      "concepts": [
        "patterns",
        "transactions",
        "transaction",
        "coupling",
        "couples",
        "orchestrated",
        "orchestration",
        "workflow",
        "complex",
        "complexities"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 37,
          "title": "",
          "score": 0.951,
          "base_score": 0.801,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 36,
          "title": "",
          "score": 0.922,
          "base_score": 0.772,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 38,
          "title": "",
          "score": 0.748,
          "base_score": 0.598,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 32,
          "title": "",
          "score": 0.743,
          "base_score": 0.593,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 33,
          "title": "",
          "score": 0.728,
          "base_score": 0.578,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "saga",
          "pattern",
          "epic",
          "epic saga",
          "sao"
        ],
        "semantic": [],
        "merged": [
          "saga",
          "pattern",
          "epic",
          "epic saga",
          "sao"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4022568195562418,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.168698+00:00"
      }
    },
    {
      "chapter_number": 36,
      "title": "Segment 36 (pages 358-366)",
      "start_page": 358,
      "end_page": 366,
      "summary": "Fantasy Fiction Saga(aao) Pattern The Fantasy Fiction Saga(aao) uses atomic consistency, asynchronous communication, and orchestrated coordination, as shown in Figure 12-12.\nThis pattern resembles the Epic Saga(sao) in all aspects except for communication—this pattern uses asynchronous rather than synchronous communication.\nThe Fantasy Fiction Saga(aao) pattern is far-fetched because transaction coordination for asynchronous communication presents difficulties\nAdding asynchronicity to orchestrated workflows adds asynchronous transactional state to the equation, removing serial assumptions about ordering and adding the possibilities of deadlocks, race conditions, and a host of other parallel system challenges.\nTransactional Saga Patterns\nThe coupling level is extremely high in this pattern, using an orchestrator and atomicity but with asynchronous communication, which makes coordination more difficult because architects and developers must deal with race conditions and other out-of-order problems imposed by asynchronous communication.\nBecause this pattern attempts transactional coordination across calls, responsive‐ ness will be impacted overall and be extremely bad if one or more of the services isn’t available.\nScale is much better in the similar pattern Parallel Saga(aeo), which switches atomic to eventual consistency.\nThe ratings for the Fantasy Fiction Saga(aao) pattern appear in Table 12-6.\nThis pattern is unfortunately more popular than it should be, mostly from the mis- guided attempt to improve the performance of Epic Saga(sao) while maintaining trans‐ actionality; a better option is usually Parallel Saga(aeo).\nHorror Story(aac) Pattern One of the patterns must be the worst possible combination; it is the aptly named Horror Story(aac) pattern, characterized by asynchronous communication, atomic con‐ sistency, and choreographed coordination, illustrated in Figure 12-14.\nTransactional Saga Patterns\nThis pattern requires a lot of interservice communication because of required transactionality and the lack of a mediator\nIn this pattern, no mediator exists to manage transactional consistency across multi‐ ple services—while using asynchronous communication.\nInstead, an architect would be better off choosing the Anthology Saga(aec) pattern, which removes holistic transactionality.\nWhile this pattern does attempt the worst kind of single coupling (transactionality), it relieves the other two, lacking both a media‐ tor and the coupling—increasing synchronous communication.\nJust as the name implies, the complexity of this pattern is truly horrific, the worst of any because it requires the most stringent requirement (transactionality) with the most difficult combination of other factors to achieve that (asynchronicity and choreography).\nThis pattern does scale better than ones with a mediator, and asynchronicity also adds the ability to perform more work in parallel.\nResponsiveness is low for this pattern, similar to the other patterns that require holistic transactions: coordination for the workflow requires a large amount of interservice “chatter,” hurting performance and responsiveness.\nThe aptly named Horror Story(aac) pattern is often the result of a well-meaning archi‐ tect starting with an Epic Saga(sao) pattern, noticing slow performance because of complex workflows, and realizing that techniques to improve performance include asynchronous communication and choreography.\nTransactional Saga Patterns\nParallel Saga(aeo) Pattern The Parallel Saga(aeo) pattern is named after the “traditional” Epic Saga(sao) pattern with two key differences that ease restrictions and therefore make it an easier pattern to implement: asynchronous communication and eventual consistency.\nThe dimen‐ sional diagram of the Parallel Saga(aeo) pattern appears in Figure 12-16.\nThe most difficult goals in the Epic Saga(sao) pattern revolve around transactions and synchronous communication, both of which cause bottlenecks and performance deg‐ radation.\nThis pattern has a low coupling level, isolating the coupling-intensifying force of transactions to the scope of the individual domain services.\nTransactional Saga Patterns\nUsing asynchronous communication and smaller transaction boundaries allows this architecture to scale quite nicely, and with good levels of isolation between services.\nBecause of lack of coordinated transactions and asynchronous communication, the responsiveness of this architecture is high.\nThe ratings associated with the Parallel Saga(aeo) pattern appear in Table 12-8.\nOverall, the Parallel Saga(aeo) pattern offers an attractive set of trade-offs for many scenarios, especially with complex workflows that need high scale.",
      "keywords": [
        "Fantasy Fiction Saga",
        "Parallel Saga",
        "Transactional Saga Patterns",
        "Saga",
        "Epic Saga",
        "Pattern",
        "Fiction Saga",
        "Saga Patterns",
        "pattern Parallel Saga",
        "asynchronous communication",
        "Transactional Sagas",
        "asynchronous",
        "communication",
        "parallel",
        "Horror Story"
      ],
      "concepts": [
        "transaction",
        "transactions",
        "pattern",
        "communication",
        "services",
        "saga",
        "complexity",
        "architectural",
        "architecture",
        "workflow"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 35,
          "title": "",
          "score": 0.922,
          "base_score": 0.772,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 37,
          "title": "",
          "score": 0.873,
          "base_score": 0.723,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 32,
          "title": "",
          "score": 0.69,
          "base_score": 0.54,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 33,
          "title": "",
          "score": 0.663,
          "base_score": 0.513,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 38,
          "title": "",
          "score": 0.662,
          "base_score": 0.512,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "saga",
          "pattern",
          "parallel",
          "asynchronous",
          "asynchronous communication"
        ],
        "semantic": [],
        "merged": [
          "saga",
          "pattern",
          "parallel",
          "asynchronous",
          "asynchronous communication"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3478311592548232,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.168737+00:00"
      }
    },
    {
      "chapter_number": 37,
      "title": "Segment 37 (pages 367-376)",
      "start_page": 367,
      "end_page": 376,
      "summary": "Anthology Saga(aec) Pattern The Anthology Saga(aec) pattern provides the exact opposite set of characteristics to the traditional Epic Saga(sao) pattern: it utilizes asynchronous communication, even‐ tual consistency, and choreographed coordination, providing the least coupled exem‐ plar among all these patterns.\nTransactional Saga Patterns\nWhile it may not seem possible without an orchestrator, stamp coupling (“Stamp Coupling for Workflow Manage‐ ment” on page 378) may be used to carry workflow state, as described in the similar Phone Tag Saga(sac) pattern.\nFor example, architects can manage transactional sagas through atomic transactions by using compensating updates or by managing transactional state with eventual con‐ sistency.\nThis section showed the advantages and disadvantages of each approach, which will help an architect decide which transactional saga pattern to use.\nState Management and Eventual Consistency State management and eventual consistency leverage finite state machines (see “Saga State Machines” on page 352) to always know the current state of the transactional saga, and to also eventually correct the error condition through retries or some sort of automated or manual corrective action.\nHowever, with this type of saga, rather than issue a compensating update, the state of the saga is changed to NO_SURVEY and a successful response is sent to the Sysops Expert (step 7 in the diagram).\nBy managing the state of the saga rather than issuing compensating updates, the end user (in this case, the Sysops Squad expert) doesn’t need to be concerned that the sur‐ vey was not sent to the customer—that responsibility is for the Ticket Orchestrator Service to worry about.\nSaga State Machines A state machine is a pattern that describes all of the possible paths that can exist within a distributed architecture.\nA state machine always starts with a beginning state that launches the transactional saga, then contains transition states and correspond‐ ing action that should occur when the transition state happens.\nTo illustrate how a saga state machine works, consider the following workflow of a new problem ticket created by a customer in the Sysops Squad system:\nThe various states that can exist within this transactional saga, as well as the corre‐ sponding transition actions, are illustrated in Figure 12-21.\nThe following items describe in more detail this transactional saga and the corre‐ sponding states and transition actions that happen within each state:\nThe transactional saga starts with a customer entering a new problem ticket into the system.\nOnce the ticket is inserted into the ticket table in the database, the transac‐ tional saga state moves to CREATED and the customer is notified that the ticket has been successfully created.\nIf no expert is available to service the ticket, it is held in a wait state until an expert is available.\nOnce an expert is assigned, the saga state moves to the ASSIGNED state.\nIf the ticket cannot be routed because the expert cannot be located or is unavailable, the saga stays in this state until it can be routed.\nOnce this happens, the transactional saga state moves to ACCEPTED.\nThere are two possible states once a ticket has been accepted by a Sysops Squad expert: COMPLETED or REASSIGN.\nOnce the expert finishes the repair and marks the ticket as “complete,” the state of the saga moves to COMPLETED.\nOnce in this saga state, the system will reassign the ticket to a different expert.\nLike the CREATED state, if an expert is not available, the transactional saga will remain in the REASSIGN state until an expert is assigned.\nOnce a different expert is found and the ticket is once again assigned, the state moves into the ASSIGNED state, waiting to be accepted by the other expert.\nThis is the only pos‐ sible outcome for this state transition, and the saga remains in this state until an expert is assigned to the ticket.\nThe two possible states once an expert completes a ticket are CLOSED or NO_SURVEY.\nWhen the ticket is in this state, a survey is sent to the customer to rate the expert and the service, and the saga state is moved to CLOSED, thus end‐ ing the transaction saga.\nOnce successfully sent, the state moves to CLOSED, marking the end of the transactional saga.\nSaga state machine for a new problem ticket in the Sysops Squad system\nTransaction action Assign ticket to expert\nTicket saga done\nTicket saga done\nThe choice between using compensating updates or state management for distributed transaction workflows depends on the situation as well as trade-off analysis between responsiveness and consistency.\nNotice that in both imple‐ mentations, the transactional sagas (NEW_TICKET, CANCEL_TICKET, and so on) are con‐ tained within the Transaction enum, providing a single place within the source code for listing and documenting the various sagas that exist within an application context.\nFor example, the source code listing in Example 12-3 shows that the Survey Service (identified by the SurveyServiceAPI class as the service entry point) is involved in the NEW_TICKET saga, whereas the Ticket Service (identified by the TicketServiceAPI class as the service entry point) is involved in two sagas: the NEW_TICKET and the CANCEL_TICKET.\n@ServiceEntrypoint @Saga(Transaction.NEW_TICKET) public class SurveyServiceAPI { ...\n@ServiceEntrypoint @Saga({Transaction.NEW_TICKET,) Transaction.CANCEL_TICKET}) public class TicketServiceAPI { ...\nNotice how the NEW_TICKET saga includes the Survey Service and the Ticket Service.\nFor example, using a simple custom code-walk tool, a developer, architect, or even a business analyst can query what services are involved for the NEW_TICKET saga:",
      "keywords": [
        "Saga",
        "Transactional Saga",
        "ticket",
        "Anthology Saga",
        "Saga State",
        "state",
        "CLOSED Ticket saga",
        "Ticket saga",
        "Ticket Orchestrator Service",
        "Sysops Squad expert",
        "saga state moves",
        "Saga State Machines",
        "transactional saga state",
        "Expert",
        "transactional"
      ],
      "concepts": [
        "saga",
        "state",
        "transactions",
        "transaction",
        "pattern",
        "services",
        "expert",
        "highly",
        "workflows",
        "error"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 35,
          "title": "",
          "score": 0.951,
          "base_score": 0.801,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 36,
          "title": "",
          "score": 0.873,
          "base_score": 0.723,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 33,
          "title": "",
          "score": 0.739,
          "base_score": 0.589,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 32,
          "title": "",
          "score": 0.688,
          "base_score": 0.538,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 38,
          "title": "",
          "score": 0.678,
          "base_score": 0.528,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "state",
          "saga",
          "saga state",
          "ticket",
          "expert"
        ],
        "semantic": [],
        "merged": [
          "state",
          "saga",
          "saga state",
          "ticket",
          "expert"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3577833820975942,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.168784+00:00"
      }
    },
    {
      "chapter_number": 38,
      "title": "Segment 38 (pages 377-384)",
      "start_page": 377,
      "end_page": 384,
      "summary": "Once sent, the Ticket Service sends an acknowledgment to the Ticket Orchestrator Service that the update is complete.\nSysops Squad Saga: Atomic Transactions and Compensating Updates\n9. Finally, the Ticket Orchestrator Service sends a response back to the Sysops Squad expert’s mobile device stating that the ticket completion processing is done.\nTo a sea of nods, Logan continued, “One of the first issues that occurs with compensating updates is that since there’s no transactional isolation within a distributed transaction (see “Distributed Transac- tions” on page 263), other services may have taken action on the data updated within the scope of the distributed transaction before the distributed transaction is complete.\nTo illustrate this issue, consider the same Epic Saga example appearing in Figure 12-23: the Sysops Squad expert marks a ticket as complete, but this time the Survey Service is not available.\nIn this case, a compensating update (step 7 in the diagram) is sent to the Ticket Service to reverse the update, changing the ticket state from completed back to in-progress (step 8 in the diagram).”\nThe expert just wants to get on to the next ticket assigned to them.”\nNotice that as part of the original update to mark the ticket as complete, the Ticket Service asynchronously sent the ticket information to a queue (step 4 in the diagram) to be processed by the Analytics Service (step 5).\nHowever, when the compensating update is issued to the Ticket Service (step 7), the ticket informa- tion has already been processed by the Analytics Service in step 5.”\nBy reversing the transaction in the Ticket Service, actions performed by other services using data from the prior update may have already taken place and might not be able to be reversed.\nTo address this issue, the Ticket Service could send another request through the data pump to the Analytics Service, tell- ing that service to ignore the prior ticket information, but just imagine the amount of complex code and timing logic that would be required in the Analytics Service to address this compensating change.\nWith distributed architectures and distributed transactions, it really is sometimes turtles all the way down.”\nKeeping with the same Epic Saga example for completing a ticket, notice in Figure 12-24 that in step 7 a compensating update is issued to the Ticket Service to change the state from completed back to in- progress.\nHowever, in this case, the Ticket Service generates an error when trying to change the state of the ticket (step 8).”\nSysops Squad Saga: Atomic Transactions and Compensating Updates\n“Architects and developers tend to assume that compensating updates will always work,” Logan said.\n“Yeah, I can imagine the developers coming to us to ask us how to resolve this issue,” Addison said.\nIf an error occurs, the end user must wait until all corrective action is taken (through compensating updates) before a response is sent telling the user about the error.”\n“Yes, while responsiveness can sometimes be resolved by asynchronously issuing compensating updates through eventual consistency (such as with the Parallel Saga and the Anthology Saga pat- tern), nevertheless most atomic distributed transactions have worse responsiveness when compen- sating updates are involved.”\nLet’s build a table to summarize some of the trade-offs associated with atomic distributed transactions and compensating updates.” (See Table 12-12.)\nTrade-offs associated with atomic distributed transactions and compensating updates\n“I know: a service cannot perform a rollback,” said Austen.\nAlternatively, the mediator could insist that other services don’t accept calls during the course of a workflow, which guarantees a valid transaction but destroys performance and scalability.”\nSysops Squad Saga: Atomic Transactions and Compensating Updates\nTransactional coordination is one of the hard- est parts of architecture, and the broader the scope, the worse it becomes.”\n“Yes!” Logan said.\nThese sagas rely on asynchronous eventual consistency and state management rather than atomic distributed transactions with compensating updates when errors occur.\n“Thanks—that’s a lot of material, but now I see why the architects made some of the decisions in the new architecture,” Addison said.\n“Well, that’s an implementation, not an architecture,” Addison said.",
      "keywords": [
        "Ticket Orchestrator Service",
        "Ticket Service",
        "ticket",
        "Ticket Orchestrator",
        "Service",
        "Ticket Service updates",
        "Sysops Squad Saga",
        "Orchestrator Service",
        "Analytics Service",
        "Ticket Service asynchronously",
        "Sysops Squad expert",
        "Ticket Service sends",
        "Orchestrator Service sends",
        "Sysops Squad",
        "Compensating Updates"
      ],
      "concepts": [
        "ticket",
        "saga",
        "service",
        "transactions",
        "transaction",
        "logan",
        "updated",
        "updates",
        "architecture",
        "contracts"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 35,
          "title": "",
          "score": 0.748,
          "base_score": 0.598,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 30,
          "title": "",
          "score": 0.735,
          "base_score": 0.585,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 34,
          "title": "",
          "score": 0.695,
          "base_score": 0.545,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 37,
          "title": "",
          "score": 0.678,
          "base_score": 0.528,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 36,
          "title": "",
          "score": 0.662,
          "base_score": 0.512,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ticket",
          "ticket service",
          "compensating",
          "compensating updates",
          "updates"
        ],
        "semantic": [],
        "merged": [
          "ticket",
          "ticket service",
          "compensating",
          "compensating updates",
          "updates"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3347174479956604,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.168823+00:00"
      }
    },
    {
      "chapter_number": 39,
      "title": "Segment 39 (pages 385-393)",
      "start_page": 385,
      "end_page": 393,
      "summary": "The spectrum of contract types, from strict to loose\nExample 13-1 shows a strict JSON contract with schema information attached.\nStrict Versus Loose Contracts\nThis creates a strict contract, with required fields and types specified.\nAt the far end of the spectrum of contract coupling lie extremely loose contracts, often expressed as name-value pairs in formats like YAML or JSON, as illustrated in Example 13-4.\nUsing such loose contracts allows for extremely decoupled systems, often one of the goals in architectures, such as microservices.\nStrict Versus Loose Contracts\nTrade-Offs Between Strict and Loose Contracts When should an architect use strict contracts and when should they use looser ones?\nStrict contracts\nMany schema tools provide mechanisms to verify contracts at build time, adding a level of type checking for integration points.\nStrict contracts also have a few disadvantages:\nLoose contracts\nLoose contracts, such as name-value pairs, offer the least coupled integration points, but they too have trade-offs, as summarized in Table 13-2.\nThese are some advantages of loose contracts:\nMany architects have a stated goal for microservices architectures that includes high levels of decoupling, and loose contracts provide the most flexibility.\nOf course, semantic coupling changes still require coordination across all interested parties—implementation cannot reduce semantic coupling—but loose contracts allow easier implementation evolution.\nLoose contracts also have a few disadvantages:\nLoose contracts by definition don’t have strict contract features, which may cause problems such as misspelled names, missing name-value pairs, and other defi‐ ciencies that schemas would fix.\nTo solve the contract issues just described, many teams use consumer-driven contracts as an architecture fitness function to make sure that loose contracts still contain sufficient information for the contract to function.\nStrict Versus Loose Contracts\nTrade-offs for loose contracts\nFor an example of the common trade-offs encountered by architects, consider the example of contracts in microservice architectures.\nContracts in Microservices Architects must constantly make decisions about how services interact with one another, what information to pass (the semantics), how to pass it (the implementa‐ tion), and how tightly to couple the services.\nThe architect could implement both services in the same technology stack and use a strictly typed contract, either a platform-specific remote procedure protocol (such as RMI) or an implementation-independent one like gRPC, and pass the customer information from one to another with high confidence of contract fidelity.\nWhen pass‐ ing information, the architect utilizes name-value pairs in JSON to pass the relevant information in a loose contract.\nSome protocols, such as JSON, include schema tools to allow architects to overlay loose contracts with more metadata.\nA common problem in microservices architectures is the seemingly contradictory goals of loose coupling yet contract fidelity.\nStrict Versus Loose Contracts\nEach consumer creates a contract specify‐ ing required information and passes it to the provider, who includes their tests as part of a continuous integration or deployment pipeline.\nThis allows each team to specify the contract as strictly or loosely as needed while guaranteeing contract fidelity as part of the build process.\nConsumer-driven contracts are quite common in microservices architecture because they allow architects to solve the dual problems of loose coupling and governed inte‐ gration.\nAllow loose contract coupling between services\nFor example, if all teams run continuous integration that includes contract tests, then fitness functions provide a good verification mechanism.\nThus, many architects use the combination of name-value pairs and consumer-driven contracts to validate contracts.\nAllows loose contract coupling between services\nStrict Versus Loose Contracts",
      "keywords": [
        "Loose Contracts",
        "Contracts",
        "Versus Loose Contracts",
        "Strict Versus Loose",
        "strict contracts",
        "Loose",
        "Strict",
        "strict JSON contract",
        "Versus Loose",
        "consumer-driven contracts",
        "loose contract coupling",
        "Strict Versus",
        "Profile",
        "architects",
        "Customer"
      ],
      "concepts": [
        "contracts",
        "types",
        "coupling",
        "couple",
        "allows",
        "architects",
        "architecture",
        "architectural",
        "requires",
        "required"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 18,
          "title": "",
          "score": 0.461,
          "base_score": 0.461,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 6,
          "title": "",
          "score": 0.447,
          "base_score": 0.447,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 42,
          "title": "",
          "score": 0.427,
          "base_score": 0.427,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 40,
          "title": "",
          "score": 0.401,
          "base_score": 0.401,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 19,
          "title": "",
          "score": 0.383,
          "base_score": 0.383,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "loose",
          "contracts",
          "loose contracts",
          "strict",
          "contract"
        ],
        "semantic": [],
        "merged": [
          "loose",
          "contracts",
          "loose contracts",
          "strict",
          "contract"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3152926636516022,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:08:52.168862+00:00"
      }
    },
    {
      "chapter_number": 40,
      "title": "Segment 40 (pages 394-416)",
      "start_page": 394,
      "end_page": 416,
      "summary": "Stamp Coupling A common pattern and sometimes anti-pattern in distributed architectures is stamp coupling, which describes passing a large data structure between services, but each service interacts with only a small part of the data structure.\nIn this example, even though the Wishlist Service needs only the name (accessed via a unique ID), the architect has coupled Profile’s entire data structure as the contract, perhaps in a misguided effort for future proofing.\nCHAPTER 14 Managing Analytical Data\n“How are we going to handle analytical data in this new architecture?” asked Dana.\nAre we going to have a data warehouse?”\nLogan said, “We looked into creating a data warehouse, and while it solved the consolidation prob- lem, it had a bunch of issues for us.”\nThe separation of application and data processing allowed better transactional management, coordination, and numerous other benefits, including the ability to start utilizing historical data for new purposes, such as analytics.\nArchitects made an early attempt to provide queriable analytical data with the Data Warehouse pattern.\nThe basic problem they tried to address goes to the core of the separation of operational and analytical data: the formats and schemas of one don’t necessarily fit (or even allow the use of) the other.\nHere are the main characteristics of the Data Warehouse pattern:\nIt wasn’t practical to query across all the various databases in the organization to build reports, so the data was extracted into the warehouse solely for analytical purposes.\nChapter 14: Managing Analytical Data\nFor example, an operational system needs to structure schemas and behavior around transactions, whereas an analytical system is rarely OLTP data (see Chapter 1) but typically deals with large amounts of data, for reporting, aggregations, and so on.\nThus, most data warehouses utilized a Star Schema to implement dimensional modelling, transforming data from operational systems in differing formats into the warehouse schema.\nHowever, building useful reports requires domain understanding, meaning that domain expertise must reside in both the operational data system and the analytical systems, where query designers must use the same data in a transformed schema to build meaningful reports and busi‐ ness intelligence.\nThe output of the data warehouse included business intelligence reports, dash‐ boards that provide analytical data, reports, and any other information to allow the company to make better decisions.\nThe Data Warehouse pattern provides a good example of technical partitioning in software architecture: warehouse designers transform the data into a schema that facilitates queries and analysis but loses any domain partitioning, which must be re- created in queries where required.\nHowever, the major failings of the Data Warehouse pattern included integration brit‐ tleness, extreme partitioning of domain knowledge, complexity, and limited function‐ ality for intended purpose:\nThe requirement built into this pattern to transform the data during the injection phase creates crippling brittleness in systems.\nA database schema for a particular problem domain is highly coupled to the semantics of that problem; changes to the domain require schema changes, which in turn require data import logic changes.\nChapter 14: Managing Analytical Data\nBuilding an alternate schema to allow advanced analytics adds complexity to the system, along with the ongoing mechanisms required to injest and transform data.\nA data warehouse is a separate project outside the normal operational systems for an organization, so must be maintained as a wholly separate ecosys‐ tem, yet highly coupled to the domains embedded inside the operational systems.\nTable 14-1 shows the trade-offs for the data warehouse pattern.\nTrade-offs for the Data Warehouse pattern\n“We looked at creating a data warehouse, but realized that it fit better with older, mono- lithic kinds of architectures than modern distributed ones,” said Logan.\n“I read a blog post on Martin Fowler’s site.1 It seems like it addresses a bunch of the issues with the data warehouse, and it is more suitable for ML use cases.”\nThe data lake was one of the early answers, mostly as a counter to the data warehouse, which definitely won’t work in something like microservices.”\nRather than do the immense work of transformation, the philosophy of the Data Lake pattern holds that, rather than do useless transformations that may never be used, do no transformations, allowing business users access to analytical data in its natural format, which typically required transformation and massaging for their purpose.\nChapter 14: Managing Analytical Data\nOperational data is still extracted in this pattern, but less transformation into another schema takes place—rather, the data is often stored in its “raw,” or native, form.\nThe Data Lake pattern, while an improvement in many ways to the Data Warehouse pattern, still suffered many limitations.\nWhile the Data Lake pattern avoided the transformation-induced problems from the Data Warehouse pattern, it also either didn’t address or created new problems.\nThe current trend in software architecture shifts focus from partitioning a system based on technical capabilities into ones based on domains, whereas both the Data Warehouse and Data Lake patterns focus on technical partitioning.\nFor example, the microservices architecture attempts to separate services by domain rather than technical capabilities, encapsulating domain knowledge, including data, inside the service boundary.\nHowever, both the Data Warehouse and Data Lake patterns try to separate data as a separate entity, losing or obscuring important domain perspectives (such as PII data) in the process.\nThe last point is critical—increasingly, architects design around domain rather than technical partitioning in architecture, and both previous approaches exemplify sepa‐ rating data from its context.\nTable 14-2 lists the trade-offs for the Data Lake pattern.\nTrade-offs for the Data Lake pattern\nThe Data Lake pattern pushes data integrity testing, data quality, and other quality issues to downstream lake pipelines, which can create some of the same operational bottlenecks that manifest in the Data Warehouse pattern.\nChapter 14: Managing Analytical Data\n“OK, so we can’t use the data lake either!” exclaimed Dana.\n“Fortunately, some recent research has found a way to solve the problem of analytical data with distributed architectures like microservices,” replied Logan.\n“It adheres to the domain boundaries we’re trying to achieve, but also allows us to project analytical data in a way that the data scientists can use.\nThe Data Mesh Observing other trends in distributed architectures, Zhamak Dehghani and several other innovators derived the core idea of the Data Mesh pattern from domain- oriented decoupling of microservices, service mesh, and sidecars (see “Sidecars and Service Mesh” on page 234), and applied it to analytical data, with modifications.\nAs we mentioned in Chapter 8, the Sidecar Pattern provides a nonentangling way to organize orthogonal coupling (see “Orthogonal Coupling” on page 238); the separa‐ tion between operational and analytical data is another excellent example of just such a coupling, but with more complexity than simple operational coupling.\nDomain ownership of data\nThis architecture allows for distributed sharing and accessing the data from multiple domains and in a peer-to-peer fashion without\nData as a product\nTo empower the domain teams to build and maintain their data products, data mesh introduces a new set of self-serve platform capabilities.\nData Product Quantum The core tenet of the data mesh overlays modern distributed architectures such as microservices.\nJust as in the service mesh, teams build a data product quantum (DPQ) adjacent but coupled to their service, as illustrated in Figure 14-1.\nChapter 14: Managing Analytical Data\nThe domain includes a data product quantum, which also contains code and data, and which acts as an interface to the overall analytical and reporting por‐ tion of the system.\nThe DPQ acts as an operationally independent but highly coupled set of behaviors and data.\nProvides analytical data on behalf of the collaborating architecture quantum, typ‐ ically a microservice, acting as a cooperative quantum.\nThe data product quantum acts as a separate but highly coupled adjunct to a service\nThe data\nChapter 14: Managing Analytical Data\nproduct quantum also likely has behavior as well as data for the purposes of analytics and business intelligence.\nTo operate, this analytical quantum has static quantum coupling to the individual data product quanta it needs for information.\nData Mesh, Coupling, and Architecture Quantum Because analytical reporting is probably a required feature of a solution, the DPQ and its communication implementation belong to the static coupling of an architecture quantum.\nHowever, like the Sidecar pattern in a service mesh, the DPQ should be orthogonal to implementation changes within the service, and maintain a separate contract with the data plane.\nIn other words, a data sidecar should never include a transactional requirement to keep operational and analytical data in sync, which would defeat the purpose of using a DPQ for orthogonal decoupling.\nSimilarly, com‐ munication to the data plane should genearlly be asynchronous, so as to have mini‐ mal impact on the operational architecture characteristics of the domain service.\nWhen to Use Data Mesh Like all things in architecture, this pattern has trade-offs associated with it, as shown in Table 14-3.\nTrade-offs for the Data Mesh pattern\nRequires contract coordination with data product quantum\nAllows excellent decoupling between analytical and operational data\nData mesh is an outstanding example of the constant incremental evolution that occurs in the software development ecosystem; new capabilities create new perspec‐ tives, which in turn help address some persistent headaches from the past, such as the artificial separation of domain from data, both operational and analytical.\nChapter 14: Managing Analytical Data\nThat allows any team that wants to build a new analytical use case to search and find the data products of choice within existing architecture quanta, directly connect to them, and start using them.\nThe platform also supports domains that want to create new data products.\nThe platform continuously monitors the mesh for any data prod- uct downtimes, or incompatibility with the governance policies and informs the domain teams to take actions.”\nLogan said, “The domain data product owners in collaboration with security, legal, risk, and compli- ance SMEs, as well as the platform product owners, have formed a global federated governance group, which decides on aspects of the DPQs that must be standardized, such as their data-sharing contracts, modes of asynchronous transport of data, access control, and so on.\n“What data do we need in order to supply the information for the expert supply problem?”\nIts first product can be called supply recommendations, which uses an ML model trained using data aggregated from DPQs in sur- veys, tickets, and maintenance domains.\nChapter 14: Managing Analytical Data",
      "keywords": [
        "data",
        "data warehouse",
        "Data Warehouse pattern",
        "Data Lake pattern",
        "Managing Analytical Data",
        "data mesh",
        "Analytical Data",
        "data product quantum",
        "data lake",
        "data product",
        "operational data",
        "Expert Supply DPQ",
        "Data Mesh pattern",
        "warehouse",
        "Analytical Data warehouse"
      ],
      "concepts": [
        "data",
        "domain",
        "architectures",
        "architectural",
        "analytical",
        "coupling",
        "couple",
        "required",
        "requirements",
        "require"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 18,
          "title": "",
          "score": 0.729,
          "base_score": 0.579,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 44,
          "title": "",
          "score": 0.7,
          "base_score": 0.55,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 31,
          "title": "",
          "score": 0.687,
          "base_score": 0.537,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 43,
          "title": "",
          "score": 0.663,
          "base_score": 0.513,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 19,
          "title": "",
          "score": 0.66,
          "base_score": 0.51,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data",
          "analytical",
          "analytical data",
          "warehouse",
          "data warehouse"
        ],
        "semantic": [],
        "merged": [
          "data",
          "analytical",
          "analytical data",
          "warehouse",
          "data warehouse"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4087681158946628,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.168911+00:00"
      }
    },
    {
      "chapter_number": 41,
      "title": "Segment 41 (pages 417-424)",
      "start_page": 417,
      "end_page": 424,
      "summary": "CHAPTER 15 Build Your Own Trade-Off Analysis\n“Well,” said Bailey, the main business sponsor and head of the Sysops Squad ticketing application, “I suppose we should get things started.\n“That was one part of it,” said Dana, “but one of the things that turned things around for me and the database team was starting to work together more with the application teams to solve problems.\n“For me it was learning how to properly analyze trade-offs,” said Addison.\n“We continue our new habit of creating trade-off tables for all our decisions, continue documenting and communicating our decisions through architecture decision records, and continue collaborating with other teams on problems and solutions.”\nThroughout this book, the unifying example illustrates how to generically perform trade-off analysis in distributed architectures.\nTo that end, this chapter provides some advice on how to build your own trade-off analysis, using many of the same techniques we used to derive the conclusions pre‐ sented in this book.\nChapter 15: Build Your Own Trade-Off Analysis\nThis is unique within a particular architecture but discoverable by experienced developers, architects, operations folks, and other roles familiar with the existing overall ecosystem and its capabilities and constraints.\nCoupling The first part of the analysis answers this question for an architect: how are parts within an architecture coupled to one another?\nIn Chapter 2, we described the concept of the static coupling between architecture quanta, which provides a comprehensive structural diagram of technical coupling.\nFor example, to create a static coupling diagram for a microservice within an archi‐ tecture, an architect needs to gather the following details:\nFor example, if an AssignTicket Service cooperates with the ManageTicket within a workflow but has no other cou‐ pling points, they are statically independent (but dynamically coupled during the actual workflow).\nTeams that already have most of their environments built via automation can build into that generative mechanism an extra capability to document the coupling points as the system builds.\nFor this book, our goal was to measure the trade-offs in distributed architecture cou‐ pling and communication.\nTo determine what became our three dimensions of dynamical quantum coupling, we looked at hundreds of examples of distributed\narchitectures (both microservices and others) to determine the common coupling points.\nBuilding sample topologies for workflows (much as we do in this book) allows an architect or team to build a matrix view of trade-offs, allowing quicker and more thorough analysis than ad hoc approaches.\nAnalyze Coupling Points Once an architect or team has identified the coupling points they want to analyze, the next step is to model the possible combinations in a lightweight way.\nThe goal of the analysis is to determine what forces the architect needs to study—in other words, which forces require trade-off analysis?\nFor example, for our architecture quantum dynamic coupling analysis, we chose coupling, complexity, responsiveness/availability, and scale/elasticity as our primary trade-off concerns, in addition to analyzing the three forces of communication, consistency, and coordina‐ tion, as shown in the ratings table for the “Parallel Saga(aeo) Pattern” on page 346, appearing again in Table 15-1.\nWhen building these ratings lists, we considered each design solution (our named patterns) in isolation, combining them only at the end to see the differences, shown in Table 15-2.\nChapter 15: Build Your Own Trade-Off Analysis\nFor example, we focused on synchro‐ nous versus asynchronous communication, a choice that creates a host of possibilities and restrictions—everything in software architecture is a trade-off.\nAn architect team can iterate on this process until they have solved the difficult decisions—in other words, decisions with entan‐ gled dimensions.\nQualitative Versus Quantative Analysis You may have noticed that virtually none of our trade-off tables are quantitative— based on numbers—but are rather qualitative—measuring the quality of something rather than the quantity, which is necessary because two architectures will always dif‐ fer enough to prevent true quantitative comparisons.\nFor example, when comparing the scalability of patterns, we looked at multiple differ‐ ent implementations of communication, consistency, and coordination combinations, assessing scalability in each case, and allowing us to build the comparative scale shown in Table 15-2.\nSimilarly, architects within a particular organization can carry out the same exercise, building a dimensional matrix of coupled concerns, and look at representative exam‐ ples (either within the existing organization or localized spikes to test theories).\nMECE Lists It is important for architects to be sure they are comparing the same things rather than wildly different ones.\nA useful concept borrowed from the technology strategy world to help architects get the correct match of things to compare is a MECE list, an acronym for mutually exclu‐ sive, collectively exhaustive:\nChapter 15: Build Your Own Trade-Off Analysis\nThe “Out-of-Context” Trap When assessing trade-offs, architects must make sure to keep the decision in context; otherwise, external factors will unduly affect their analysis.\nArchitects need to make sure they balance the correct set of trade-offs, not all available ones.\nFor example, perhaps an architect is trying to decide whether to use a shared service or shared library for common functionality within a distributed architecture, as illus‐ trated in Figure 15-2.\nTrade-off analysis for two solutions\nChapter 15: Build Your Own Trade-Off Analysis",
      "keywords": [
        "Trade-Off Analysis Monday",
        "Trade-Off Analysis",
        "low Medium High",
        "high Low",
        "Analysis",
        "Sysops Squad",
        "High",
        "Coupling",
        "Low",
        "Trade-Off",
        "Saga High"
      ],
      "concepts": [
        "coupled",
        "said",
        "analysis",
        "architectures",
        "business",
        "trade",
        "architect",
        "service",
        "things",
        "saga"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 5,
          "title": "",
          "score": 0.687,
          "base_score": 0.537,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 43,
          "title": "",
          "score": 0.687,
          "base_score": 0.537,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 10,
          "title": "",
          "score": 0.672,
          "base_score": 0.522,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 3,
          "title": "",
          "score": 0.667,
          "base_score": 0.517,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 8,
          "title": "",
          "score": 0.664,
          "base_score": 0.514,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "analysis",
          "trade",
          "trade analysis",
          "build trade",
          "15"
        ],
        "semantic": [],
        "merged": [
          "analysis",
          "trade",
          "trade analysis",
          "build trade",
          "15"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3810590509312028,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.168955+00:00"
      }
    },
    {
      "chapter_number": 42,
      "title": "Segment 42 (pages 425-432)",
      "start_page": 425,
      "end_page": 432,
      "summary": "First, finding the best context for a decision allows the architect to consider fewer options, greatly simplifying the decision process.\nFinding the correct narrow context for decisions allows architects to think about less, in many cases sim‐ plifying design.\nModel Relevant Domain Cases Architects shouldn’t make decisions in a vacuum, without relevant drivers that add value to the specific solution.\nAdding those domain drivers back to the decision pro‐ cess can help the architect filter the available options and focus on the really impor‐ tant trade-offs.\nFor example, consider this decision by an architect as to whether to create a single payment service or a separate service for each payment type, as illustrated in Figure 15-5.\nHowever, those forces are generic—an architect may add more nuance to the decision by modeling some likely scenarios.\nFor example, consider the first scenario, illustrated in Figure 15-6, to update a credit card processing service.\nIn the second scenario, the architect models what happens when the system adds a new payment type, as shown in Figure 15-7.\nThe architect adds a reward points payment type to see what impact it has on the architecture characteristics of interest, highlighting extensibility as a benefit of sepa‐ rate services.\nIn this scenario, the architect starts gaining insight into the real trade-offs involved in this decision.\nHaving modeled these three scenarios, the architect realizes that the real trade-off analysis comes down to which is more important: performance and data consistency (a single payment service) or extensibility and agility (separate services).\nAs architecture generally evades generic solutions, it is important for architects to build their skills in modeling relevant domain scenarios to home in on better trade-off analysis and decisions.\nPrefer Bottom Line over Overwhelming Evidence It’s easy for architects to build up an enormous amount of information in pursuit of learning all the facets of a particular trade-off analysis.\nRather than show all the information they have gathered, an architect should reduce the trade-off analysis to a few key points, which are sometimes aggregates of individ‐ ual trade-offs.\nConsider the common problem an architect might face in a microservices architec‐ ture about the choice of synchronous or asynchronous communication, illustrated in Figure 15-9.\nAfter considering the generic factors that point to one versus the other, the architect next thinks about specific domain scenarios of interest to nontechnical stakeholders.\nTrade-offs between synchronous and asynchronous communication for credit card processing\nAfter modeling these scenarios, the architect can create a bottom-line decision for the stakeholders: which is more important, a guarantee that the credit approval process starts immediately or responsiveness and fault-tolerance?\nThis architect has likely worked on problems in the past where extensibility was a key driving architecture characteristic and believes that capability will always drive the decision process.\nWhile experience is useful, scenario analysis is one of an architect’s most powerful tools to allow iterative design without building whole systems.\nBy modeling likely scenarios, an architect can discover if a particular solution will, in fact, work well.\nThe architect’s goal is to add bid history to the workflow—should the team keep the existing publish-and-subscribe approach or move to point-to-point messaging for each consumer?\nTo discover the trade-offs for this specific problem, the architect should model likely domain scenarios using the two topologies.",
      "keywords": [
        "architect",
        "Trade-Off Analysis",
        "decision",
        "Trade-Off",
        "Trade-Off Techniques",
        "separate services",
        "services",
        "architecture",
        "Analysis",
        "scenario",
        "single payment service",
        "illustrated in Figure",
        "Build",
        "context",
        "bid"
      ],
      "concepts": [
        "decision",
        "decisions",
        "solution",
        "architectures",
        "architect",
        "trade",
        "service",
        "problem",
        "bid",
        "workflows"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 34,
          "title": "",
          "score": 0.572,
          "base_score": 0.422,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 33,
          "title": "",
          "score": 0.57,
          "base_score": 0.42,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 32,
          "title": "",
          "score": 0.565,
          "base_score": 0.415,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 35,
          "title": "",
          "score": 0.563,
          "base_score": 0.413,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 37,
          "title": "",
          "score": 0.554,
          "base_score": 0.404,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "architect",
          "decision",
          "scenarios",
          "trade",
          "analysis"
        ],
        "semantic": [],
        "merged": [
          "architect",
          "decision",
          "scenarios",
          "trade",
          "analysis"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.35936334890849525,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.169000+00:00"
      }
    },
    {
      "chapter_number": 43,
      "title": "Segment 43 (pages 433-447)",
      "start_page": 433,
      "end_page": 447,
      "summary": "Component coupling: Chapter 7, page 92\nTechnical versus domain partitioning: Chapter 8, page 103\nService-based architecture: Chapter 13, page 163\nMicroservices architecture: Chapter 12, page 151\n“ADR: Migrate Sysops Squad Application to a Distributed Architecture” on page 61\n“ADR: Use of a Shared Library for Common Ticketing Database Logic” on page 247\nThe primary focus of this book is trade-off analysis; to that end, we created a number of trade-off tables and figures in Part II to summarize trade-offs around a particular architecture concern.\nFigure 6-26, “Key-value databases rated for various adoption characteristics” on page 166\nFigure 6-28, “Column family databases rated for various adoption characteristics” on page 170\nFigure 6-32, “Cloud native databases rated for various adoption characteristics” on page 175\nFigure 6-33, “Time-series databases rated for various adoption characteristics” on page 177\nTable 8-1, “Trade-offs for the code replication technique” on page 223\nTable 8-2, “Trade-offs for the shared library technique” on page 227\nTable 8-3, “Trade-offs for the shared service technique” on page 233\nTable 8-4, “Trade-offs for the Sidecar pattern / service mesh technique” on page 239\nTable 9-1, “Joint ownership table split technique trade-offs” on page 256\nTable 9-2, “Joint ownership data-domain technique trade-offs” on page 258\nTable 9-3, “Joint ownership delegate technique trade-offs” on page 261\nTable 9-4, “Joint ownership service consolidation technique trade-offs” on page 262\nTable 9-5, “Background synchronization pattern trade-offs” on page 272\nTable 9-6, “Orchestrated request-based pattern trade-offs” on page 277\nTable 9-7, “Event-based pattern trade-offs” on page 279\nTable 10-1, “Trade-offs for the Interservice Communication data access pattern” on page 286\nTable 10-2, “Trade-offs for the Column Schema Replication data access pattern” on page 288\nTable 10-3, “Trade-offs associated with the replicated caching data access pattern” on page 293\nTable 10-4, “Trade-offs associated with the data domain data access pattern” on page 295\nTable 11-1, “Trade-offs for orchestration” on page 306\nTable 11-2, “Trade-offs for the Front Controller pattern” on page 312\nTable 11-3, “Stateless choreography trade-offs” on page 313\nTable 11-4, “Stamp coupling trade-offs” on page 314\nTable 11-5, “Trade-offs for the choreography communication style” on page 315\nTable 11-6, “Trade-off between orchestration and choreography for ticket workflow” on page 319\nTable 11-7, “Updated trade-offs between orchestration and choreography for ticket workflow” on page 320\nTable 11-8, “Final trade-offs between orchestration and choreography for ticket workflow” on page 320\nTable 12-11, “Trade-offs associated with state management rather than atomic dis‐ tributed transactions with compensating updates” on page 356\nTable 12-12, “Trade-offs associated with atomic distributed transactions and compen‐ sating updates” on page 363\nTable 13-1, “Trade-offs for strict contracts” on page 371\nTable 13-2, “Trade-offs for loose contracts” on page 372\nTable 13-3, “Trade-offs for consumer-driven contracts” on page 375\nTable 13-4, “Trade-offs for stamp coupling” on page 379\nTable 14-1, “Trade-offs for the Data Warehouse pattern” on page 385\nTable 14-2, “Trade-offs for the Data Lake pattern” on page 388\nTable 14-3, “Trade-offs for the Data Mesh pattern” on page 394\nTable 15-2, “Consolidated comparison of dynamic coupling patterns” on page 403\nTable 15-3, “Trade-offs between synchronous and asynchronous communication for credit card processing” on page 411\nTable 15-4, “Trade-offs between point-to-point versus publish-and-subscribe messag‐ ing” on page 414\nabout, 64, 82 about patterns, 71, 82 architecture stories, 84 create component domains, 120-122 create domain services, 126-129 determine component dependencies,\nabout, 132 about decomposing monolithic, 151 assign tables to data domains, 156-158 create data domains, 156 drivers of decomposition, 132-149 Refactoring Databases (Ambler and\narchitecture quantum about, 28, 144 coupling, static versus dynamic, 28\ndata disintegration driver, 144 separate databases, 159 Sysops Squad saga, 151 data product quantum, 390-393 high functional cohesion, 30\nbasic availability of BASE transactions, 267 coupling relationship, 403 database types\ndata requirement in some architectures, 132 data sovereignty per service, 159 granularity and, 206 high functional cohesion, 30 ownership of data (see ownership of data)\ndefinition, 15 service granularity and, 200-203 stamp coupling for management, 378 workflow state management, 311-315 Front Controller pattern, 311 stamp coupling, 313 stateless choreography, 313\ndatabase connection pool, 138 Sysops Squad saga, 150 distributed data access, 287 dynamic coupling as, 23, 29\ndatabase per service, 143 Sysops Squad saga, 150\nabout choreographed versus, 300, 331 definition, 14 workflow state management, 311 stamp coupling for management, 378 Sysops Squad saga, 299, 317-321\nabout static versus dynamic, 29 dynamic coupling, 23, 29 static coupling, 23, 28 Sysops Squad saga, 42 architectural decomposition afferent coupling, 66, 68 common domain functionality consoli‐\navailability relationship, 403 data relationships, 147 definition, 14, 27 interservice communication pattern, 286 loose contracts for decoupling, 369, 371 consumer-driven contracts, 373 microservices, 372 microservices and contract fidelity, 373 strict contract tight coupling, 370\nfitness functions, 122 Sysops Squad saga, 123-126, 129 Create Domain Services pattern, 126-129",
      "keywords": [
        "Sysops Squad Saga",
        "Sysops Squad",
        "Squad Saga",
        "trade-offs",
        "Data",
        "Squad",
        "Sysops",
        "coupling",
        "databases",
        "architecture",
        "trade-off analysis",
        "Sysops Squad decision",
        "pattern",
        "234-239 Sysops Squad",
        "Sysops Squad Expert"
      ],
      "concepts": [
        "database",
        "data",
        "coupling",
        "architecture",
        "architectural",
        "patterns",
        "service",
        "saga",
        "trade",
        "tables"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 44,
          "title": "",
          "score": 0.805,
          "base_score": 0.655,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 45,
          "title": "",
          "score": 0.783,
          "base_score": 0.633,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 19,
          "title": "",
          "score": 0.764,
          "base_score": 0.614,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 20,
          "title": "",
          "score": 0.741,
          "base_score": 0.591,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 16,
          "title": "",
          "score": 0.702,
          "base_score": 0.552,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "page",
          "offs",
          "trade offs",
          "trade",
          "table"
        ],
        "semantic": [],
        "merged": [
          "page",
          "offs",
          "trade offs",
          "trade",
          "table"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4312977811612774,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.169046+00:00"
      }
    },
    {
      "chapter_number": 44,
      "title": "Segment 44 (pages 448-455)",
      "start_page": 448,
      "end_page": 455,
      "summary": "about, 4, 381 data lakes, 386-389 data meshes, 389-394 data meshes coupled to reporting, 393 data warehouses, 382-385 definition, 5 domain over technical partitioning, 388 Sysops Squad saga, 381, 386, 389,\nrelationships among data, 147 data domains (see data domains) database-per-service, 143\nabout, 132 about decomposing monolithic, 151 assign tables to data domains, 156-158 create data domains, 156 data access in other domains, 158 database-per-service requiring, 147 dependencies, 153-155 drivers of, 132-149 Refactoring Databases (Ambler and\nabout, 133 architecture quantum, 144 connection management, 138-141 database change control, 134-138 database type optimization, 146 fault tolerance, 143 scalability, 141\ngranularity and data relationships, 205-207 integration drivers about, 146 database transactions, 148\nabout, 249 assigning, 250 common ownership, 252 data mesh domain ownership, 389 data sovereignty per service, 159 distributed data access, 287, 289 joint ownership, 253-260 service consolidation technique, 261 single ownership, 251 summary, 262 Sysops Squad saga, 249, 279-282\nSysops Squad saga, 151 Sysops Squad data model, 19\ndata domains, 152\ndata domain technique of joint ownership,\n256-258 data domains about, 152 about decomposing monolithic data, 151 assign tables to data domains, 156-158 bounded context rules, 155 create data domains, 156 schemas to separate servers, 159 separate connections to data domains,\nservice consolidation, 261 tables tightly coupled, 157 data access in other domains, 158 data schemas versus, 157\nsynonyms for tables, 157 distributed data access, 293-295 joint ownership of data, 256-258 soccer ball visualization, 153 Sysops Squad data model, 152\nabout, 389 Data Mesh (Dehghani), 390 data product quantum, 390-393 Sysops Squad saga, 394-397 when to use, 393\nNewSQL databases, 174 database refactoring book, 154 database structure Sysops Squad saga, 180-184 database transactions and granularity, 198 database types, 178\nabout characteristics rated, 161 aggregate orientation, 165 cloud native databases, 175 column family databases, 169 database type optimization, 146 document databases, 167 graph databases, 171-173 key-value databases, 165-167 NewSQL databases, 173 NoSQL databases, 165 relational databases, 163-164 schema-less databases, 169 Sysops Squad saga, 179 time series databases, 177-178\ndatabase-per-service, 143\nmonolithic data domains, 153-155\n111-118 fitness functions, 117 Sysops Squad saga, 118-120\ndata sovereignty per service, 159 database connection pool, 138\nconnection management, 139 Sysops Squad saga, 150\nsharing data, 257 side effects, 361 state machines, 352-355 static coupling, 32\nColumn Schema Replication pattern, 287 Data Domain pattern, 293-295 Interservice Communication pattern, 285 Replicated Caching pattern, 288-293 Sysops Squad saga, 283, 295-298\nBASE transactions, 267 compensating updates, 275, 327 Sysops Squad saga, 358-364\n(see also transactional saga patterns)\nSysops Squad saga, 299, 317-321 transactional saga patterns, 323\n(see also transactional saga patterns)\ndata domains, 152 data lakes losing relationships, 387 data mesh domain ownership, 389 granularity and shared domain functional‐\nCreate Domain Services pattern, 126-129 fitness function for namespace, 129 domain cohesion and granularity, 194 Front Controller in choreography, 311 service-based architecture definition, 72,\nEpic Saga pattern, 325-330 Equifax data breach, 12 Evans, Erik, 165 event-based eventual consistency, 277-279 publish-and-subscribe messaging, 277\nchoreography, 314 data disintegration driver, 143 definition, 58, 143, 193 granularity disintegration driver, 193 granularity integration driver, 200-203 modularity driver, 49, 58 orchestration, 305\nshared libraries, 232 shared services, 232 single point of failure, 143 Sysops Squad saga, 151\ntion create component domains, 122 create domain services, 129 determine component dependencies,\nfitness functions, 107 metrics for shared components, 106 Sysops Squad saga, 107-110\ntern, 94-96 fitness functions, 95 Sysops Squad saga, 97-99\nabout, 187, 197 data relationships, 205-207 database transactions, 198 shared code, 203-205 workflow and choreography, 200-203\nmetrics, 187 microservice single responsibility, 186, 190 modularity versus, 186 shared data and, 257 shared libraries, 224 Sysops Squad saga, 185, 209-216\nH hard parts of architecture, 2 hash table data structure, 165 Hello2morrow, 50 Henney, Kevlin, 222 Hexagonal Architecture, 235 high functional cohesion, 30\ndatabase architecture quantum, 145\nfitness functions, 87-89 Sysops Squad saga, 81 implementation coupling, 309 independent deployability, 29 InfluxDB time series database, 177 infrastructure versus domain shared function‐\nabout, 253 data domain technique, 256-258 delegate technique, 258-261 service consolidation technique, 261 table split technique, 254-256\narchitecture quantum, 28, 29 decoupled services, 35 high functional cohesion, 30 user interface coupling, 36\narchitecture quanta, 35 architecture quantum versus, 42 breaking database changes controlled,\ndata domains, 155 data domains combined, 157 data requirement, 132 database abstraction, 136-138 granularity and, 206 high functional cohesion, 30 ownership of data (see ownership of\ndeployability, 56 scalability and elasticity, 57 operational coupling, 234-239 orchestrator per workflow, 301 saga pattern, 323 service-based architecture as stepping-\nMicroservices Patterns (Richardson), 323 Microsoft SQL Server relational database, 164 modularity\ngranularity versus, 186 monolithic architectures about modularity, 50 about need for modularity, 47 deployability, 55 fault tolerance, 58 maintainability, 51 modular monoliths, 50 scalability and elasticity, 57 Sysops Squad saga, 45, 59-62 testability, 54 water glass analogy, 47\ndata decomposition about, 151 assign tables to data domains, 156-158 create data domains, 156 Refactoring Databases (Ambler and\ndata importance, 4 database connection pool, 138 database type optimization, 146 Epic Saga mimicking, 325-330 modular architecture\nabout need for, 47 deployability, 55 fault tolerance, 58 maintainability, 51 modular monoliths, 50 scalability and elasticity, 57 Sysops Squad saga, 45, 59-62 testability, 54 water glass analogy, 47\nabout, 71, 82 architecture stories, 84 create component domains, 120-122 create domain services, 126-129 determine component dependencies,\nsingle architecture quantum, 29 functional cohesion, 30 static coupling, 31\nO online resources (see resources online) Online Transactional Processing (OLTP), 4 operational coupling, 234-239 Sysops Squad saga, 239 operational data definition, 4 Oracle Coherence replicated caching, 290 Oracle relational database, 164 orchestrated coordination, 301-306\nOrchestrated Saga pattern, 325-330 orphaned classes definition, 103 orthogonal code reuse pattern, 238 orthogonal coupling, 238 out-of-context trap, 405-407 ownership of data about, 249 assigning, 250 common ownership, 252 data mesh domain ownership, 389 data sovereignty per service, 159 distributed data access, 287, 289 joint ownership about, 253 data domain technique, 256-258 delegate technique, 258-261 table split technique, 254-256 service consolidation technique, 261 single ownership, 251 summary, 262 Sysops Squad saga, 249, 279-282\nP Page-Jones, Meilir, 23 Parallel Saga pattern, 346-348 PCI (Payment Card Industry) data security and granularity, 195\nPersonally Identifiable Information (PII), 387 Phone Tag Saga pattern, 330-333 PII (Personally Identifiable Information), 387 Pipes and Filters design pattern, 338, 351 platforms for code reuse, 244 Ports and Adaptors Pattern, 235 PostgreSQL relational database, 164 publish-and-subscribe messaging, 277\nSaga Pattern (Richardson), 323 stamp coupling information, 376 static code analysis tools, 85\nabout, 323 definition, 15 dynamic coupling matrix, 41, 324, 402 microservice saga pattern, 323 state machines, 352-355 Sysops Squad saga background, 15 (see also Sysops Squad sagas)\ntechniques for managing, 356 transactional saga patterns\nchoreography, 314, 331 connection management, 141 database-per-service, 143 Sysops Squad saga, 150 coupling relationship, 332, 403 data disintegration driver, 141 database types",
      "keywords": [
        "Sysops Squad saga",
        "Sysops Squad",
        "Squad saga",
        "Sysops Squad data",
        "Index Sysops Squad",
        "390-393 Sysops Squad",
        "transactional saga patterns",
        "common domain components",
        "data mesh domain",
        "Determine Component Dependencies",
        "Decorator Design Pattern",
        "mesh domain ownership",
        "distributed data access",
        "create data domains",
        "structure Sysops Squad"
      ],
      "concepts": [
        "database",
        "saga",
        "data",
        "pattern",
        "architecture",
        "architectural",
        "coupled",
        "service",
        "domain",
        "definition"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 45,
          "title": "",
          "score": 1.031,
          "base_score": 0.881,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 2,
          "title": "",
          "score": 0.821,
          "base_score": 0.671,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 43,
          "title": "",
          "score": 0.805,
          "base_score": 0.655,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 19,
          "title": "",
          "score": 0.758,
          "base_score": 0.608,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 16,
          "title": "",
          "score": 0.737,
          "base_score": 0.587,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data",
          "saga",
          "squad saga",
          "squad",
          "sysops"
        ],
        "semantic": [],
        "merged": [
          "data",
          "saga",
          "squad saga",
          "squad",
          "sysops"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4048166808054399,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.169092+00:00"
      }
    },
    {
      "chapter_number": 45,
      "title": "Segment 45 (pages 456-462)",
      "start_page": 456,
      "end_page": 462,
      "summary": "cross-schema access resolved, 158 data domain versus data schema, 157 databases separated physically, 159 schema-less databases, 169 synonyms for tables, 157\ngranularity disintegration driver, 195 PCI data Sysops Squad saga, 212-216 security latency, 285 semantic coupling, 309\nimplementation coupling, 309 stamp coupling to manage workflow, 378 service consolidation technique of joint owner‐\nSysops Squad saga, 239 service-based architecture about, 32, 72, 126 data access in other domains, 158 data shared by services, 132 changes to database, 134 connection management, 138-141 single point of failure, 143\n121 Create Domain Services pattern, 126-129\nabout, 71, 82 architecture stories, 84 create component domains, 120-122 create domain services, 126-129 determine component dependencies,\nCreate Component Domains pattern,\ndata access in other domains, 158 data shared by services, 132 changes to database, 134 connection management, 138-141 single point of failure, 143 database connection pool, 138 Sysops Squad saga, 150\ndatabase-per-service, 143 definition, 14 distributed data access\nData Domain pattern, 293-295 Interservice Communication Pattern,\nReplicated Caching pattern, 288-293 Sysops Squad saga, 283, 295-298\norchestrated coordination definition, 14 ownership of data assigning, 250 common ownership, 252 data sovereignty per service, 159 distributed data access, 287, 289 joint ownership, 253-260 service consolidation technique, 261 single ownership, 251 summary, 262 Sysops Squad saga, 249, 279-282\nservice mesh, 237-239 shared services for code reuse, 228-234 Sidecar pattern, 234-239\nshared code and granularity, 203-205 (see also code reuse patterns) shared libraries, 224 shared domain functionality granularity and, 204 shared infrastructure functionality versus,\nshared libraries about, 223 fault tolerance, 232 granularity and, 224 versioning strategies, 225-227 shared services for code reuse, 228-234\nSMS information link, 189 Snowflake cloud native database, 175 soft state of BASE transactions, 267 software architecture (see architecture) SOLID principles (Martin), 190 SonarQube code hygiene tool, 10 speed-to-market as modularity driver, 49 stamp coupling about, 376 bandwidth, 377 choreography state management, 313, 332 over-coupling via, 376 workflow management, 378 standard deviation calculation, 88 Star Schema of data warehouses, 383, 384 Starbucks Does Not Use Two-Phase Commit\nhigh static coupling, 30-37 independently deployable, 29 Sysops Squad saga, 42\nasynchronous versus, 38, 43 definition, 14 delegate technique of data ownership, 259 fault tolerance, 59 granularity and, 200-203 scalability and elasticity versus, 57\narchitectural components, 18 architecture quantum, 42 contracts, 365, 379 data model, 19\nData Domain pattern, 293-295 Interservice Communication Pattern,\n6. create domain services, 127 business case for, 59-62 code reuse patterns, 219, 239, 244 create component domains, 123-126 create domain services, 129 data pulled apart, 131, 150 database structure, 180-184 database types, 179 decomposition method, 63, 78 decomposition pattern, 81 determine component dependencies, 97-99\ngranularity, 185, 209-216 identify and size components, 90-93 ownership of data, 249, 279-282\nstate machines, 352-355 trade-off analysis, 399, 416 transactional sagas, 323, 358-364\ntime series databases, 177-178 Time Travel Saga pattern, 336-339 TimeScale time series database, 177 tools\nabout importance of, 116 afferent and efferent coupling, 66 data domains via soccer ball, 153 dependencies, 66, 113, 116 distance from main sequence, 69 domain diagramming exercise, 123 graph databases, 171-173 isomorphic diagrams, 325 static coupling diagram, 401\nbuilding your own, 400-403 consistency and availability, 178 coordination, 315 coupling, 26-28, 401-403 coupling points, 402 dynamic quantum coupling factors, 41 static coupling diagram, 401 documenting versus governing, 5 granularity, 208 iterative nature of, 403 loose contracts, 371 strict contracts, 370 Sysops Squad saga, 399, 416 tactical forking, 77 techniques of\ntransactional saga patterns\ndata integration driver, 148 database transactions and granularity, 198 distributed transactions about, 265, 327 ACID transactions, 263-267 BASE transactions, 267 compensating updates, 275, 327 eventual consistency, 267\nafferent and efferent coupling, 66 data domains via soccer ball, 153 dependencies, 113\nimportance of visualization, 116 JDepend Eclipse plug-in, 66 distance from main sequence, 69 domain diagramming exercise, 123 graph databases, 171-173 isomorphic diagrams, 325 static coupling diagram, 401 volatility-based decomposition, 191\nwhy more important than how, 13 wide column databases, 169 Woolf, Bobby, 156 workflow and service granularity, 200-203\nMark Richards is an experienced, hands-on software architect involved in the archi‐ tecture, design, and implementation of microservices architectures, service-oriented architectures, and distributed systems in a variety of technologies.\nThe animal on the cover of Software Architecture: The Hard Parts is a black-rumped golden flameback woodpecker (Dinopium benghalense), a striking species of wood‐ pecker found throughout the plains, foothills, forests, and urban areas of the Indian subcontinent.",
      "keywords": [
        "Sysops Squad saga",
        "Sysops Squad",
        "Squad saga",
        "Schema Replication pattern",
        "Replicated Caching pattern",
        "Interservice Communication Pattern",
        "Column Schema Replication",
        "Create Domain Services",
        "237-239 Sysops Squad",
        "288-293 Sysops Squad",
        "data Sysops Squad",
        "293-295 Interservice Communication",
        "create component domains",
        "Domain Services pattern",
        "static coupling diagram"
      ],
      "concepts": [
        "database",
        "saga",
        "data",
        "pattern",
        "architecture",
        "architectural",
        "service",
        "software",
        "domain",
        "coupling"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 44,
          "title": "",
          "score": 1.031,
          "base_score": 0.881,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 2,
          "title": "",
          "score": 0.881,
          "base_score": 0.731,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 43,
          "title": "",
          "score": 0.783,
          "base_score": 0.633,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 19,
          "title": "",
          "score": 0.771,
          "base_score": 0.621,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Software Architecture",
          "chapter": 16,
          "title": "",
          "score": 0.765,
          "base_score": 0.615,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data",
          "pattern",
          "coupling",
          "granularity",
          "saga"
        ],
        "semantic": [],
        "merged": [
          "data",
          "pattern",
          "coupling",
          "granularity",
          "saga"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4361974308661555,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:52.169141+00:00"
      }
    }
  ],
  "total_chapters": 45,
  "enrichment_provenance": {
    "taxonomy_id": "none",
    "taxonomy_version": "none",
    "taxonomy_path": "none",
    "taxonomy_checksum": "sha256:none",
    "source_metadata_file": "Software Architecture_metadata.json",
    "enrichment_date": "2025-12-17T23:08:52.174112+00:00",
    "enrichment_method": "msep",
    "model_version": "ai-agents-msep-v1",
    "processing_time_ms": 3945.545458998822,
    "total_similar_chapters": 225
  }
}