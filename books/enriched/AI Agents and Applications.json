{
  "metadata": {
    "title": "AI Agents and Applications",
    "source_file": "AI Agents and Applications_metadata.json"
  },
  "chapters": [
    {
      "chapter_number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "summary": "With LangChain, LangGraph, and MCP\nAI Agents and Applications\n2. 1_Introduction_to_AI_Agents_and_Applications\n4. 3_Summarizing_text_using_LangChain\n11_Building_Tool-based_Agents_with_LangGraph\nAppendix_A._Trying_out_LangChain\nAppendix_C._Choosing_an_LLM\nAppendix_E._Open-source_LLMs\nThank you so much for purchasing \"AI Agents and Applications\".\nOpenAI APIs, delve into prompt engineering, and design applications using\nan open-source LLM application framework, significantly accelerated my\n\"AI Agents and Applications\" is structured to cater both to beginners and\ntopics from running open source LLMs locally to advanced RAG techniques\napplications.\nwelcome 1 Introduction to AI Agents and Applications 2 Executing prompts\nprogrammatically 3 Summarizing text using LangChain 4 Building a research\nsummarization engine 5 Agentic Workflows with LangGraph 6 RAG\ngeneration, routing and retrieval post-processing 11 Building Tool-based\nAgents with LangGraph 12 Multi-agent Systems 13 Building and consuming\nTrying out LangChain Appendix B.\nChoosing an LLM Appendix D.\nApplications\nCore challenges in building LLM-powered applications\nLangChain’s modular architecture and components\nPatterns for engines, chatbots, and agents\nexperimenting with applications powered by large language models (LLMs).\nunlocked a new class of applications: AI agents.\nLangChain, LangGraph, and\npromote best practices, and let you focus on application logic instead of low-\nscale real LLM-based applications and agents.\nthe main problems LLM applications aim to solve, explore the architecture\nfamilies of LLM-powered applications: engines, chatbots, and AI agents.\nchallenges in building LLM applications, the patterns that solve them, and the\n1.1 Introducing LangChain\nretrievers, and prompt templates, you can focus on application logic instead\nLangChain has also evolved rapidly, fueled by an active open-source\nshared best practices for building and deploying LLM-based systems.\nBy learning LangChain, you not only gain the ability to build production-\ngrade LLM applications, but also acquire transferable skills.\n1.1.1 LangChain architecture\nFigure 1.1 LangChain architecture: The Document Loader imports data, which the Text Splitter",
      "keywords": [
        "Roberto Infante MEAP",
        "MCP Roberto Infante",
        "andy Roberto Infante",
        "Roberto Infante",
        "Infante MEAP",
        "Agents",
        "Applications",
        "LLM applications",
        "LangChain",
        "LLM",
        "building LLM applications",
        "LLMs",
        "Appendix",
        "Building",
        "open-source LLM application"
      ],
      "concepts": [
        "langchain",
        "applications",
        "developer",
        "appendix",
        "building",
        "data",
        "framework",
        "open",
        "model",
        "retrieval"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 62,
          "title": "",
          "score": 0.706,
          "base_score": 0.556,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 49,
          "title": "",
          "score": 0.686,
          "base_score": 0.536,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "",
          "score": 0.684,
          "base_score": 0.534,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 4,
          "title": "",
          "score": 0.678,
          "base_score": 0.678,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 57,
          "title": "",
          "score": 0.625,
          "base_score": 0.475,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "applications",
          "agents",
          "infante",
          "langchain",
          "ai agents"
        ],
        "semantic": [],
        "merged": [
          "applications",
          "agents",
          "infante",
          "langchain",
          "ai agents"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.37015700606861857,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.530934+00:00"
      }
    },
    {
      "chapter_number": 2,
      "title": "Segment 2 (pages 9-16)",
      "start_page": 9,
      "end_page": 16,
      "summary": "such as document and text files,\nCalled chunks transforms the LLM\nDocument object\nModel (LLM) or\nDocument\ncontext usually comes from document chunks pulled out of a vector store.\nDocument loaders (1): in LangChain, document loaders play a pivotal\nDocument objects.\nwith embeddings and stored in the vector database.\nDocument: In LangChain, a Document is a fundamental data structure that\nDocument object, with metadata storing details like the source file name\nDocument, making it easier to process them in chunks.\nEmbedding models (3): LangChain provides support for the most\nVector stores (4): vector stores operate as specialized databases designed\nfor the efficient retrieval of Document objects.\nrepresent fragments or \"chunks\" of the original document, indexed based\nBy ingesting Documents\nKnowledge Graph databases: Although not a key component of the\nRetrievers (5): In LangChain, retrievers efficiently fetch data, often a list\nof Documents containing unstructured text, from indexed databases like\nvector stores.\nLangChain provides support for various retrievers,\ncatering not only to vector stores but also to relational databases and\nmore complex data stores, such as knowledge graph databases like\nPrompts (6): LangChain provides tools for defining prompt templates—\nsuch as vector stores.\nsent to the Language Model (LLM) for processing.\nAdditionally, LangChain supports a Fake LLM for unit\nOutput Parser (9): This component transforms an LLM's natural\n1.2 LangChain core object model\nloaders generate Document objects, how splitters divide them into smaller\nDocument\nand sharing reusable components such as prompts, chains, and tools.\nFigure 1.2 Object model of classes associated with the Document core entity, including Document\nloaders (which create Document objects), splitters (which create a list of Document objects),\nvector stores (which store Document objects in vector stores) and retrievers (which retrieve\nDocument objects from vector stores and other sources)\nDocument loaders\nDocument object\nDocument transformers\ntransform a Document\nDocument objects.\nsmaller documents called\nDocuments (and text\nDocuments containing\nvector stores, but also\nIn figure 1.3, you can see the object model related to Language Models,\nFigure 1.3 Object model of classes associated with Language Models, including Prompt",
      "keywords": [
        "document",
        "Document objects",
        "LLM",
        "Vector stores",
        "LangChain",
        "text",
        "vector",
        "object model",
        "objects",
        "documents",
        "Model",
        "stores",
        "Document loaders",
        "Document object Large",
        "databases"
      ],
      "concepts": [
        "document",
        "documents",
        "langchain",
        "llm",
        "text",
        "components",
        "prompt",
        "object",
        "databases",
        "stores"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 21,
          "title": "",
          "score": 0.867,
          "base_score": 0.717,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 24,
          "title": "",
          "score": 0.861,
          "base_score": 0.711,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 27,
          "title": "",
          "score": 0.812,
          "base_score": 0.662,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 36,
          "title": "",
          "score": 0.799,
          "base_score": 0.649,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 23,
          "title": "",
          "score": 0.734,
          "base_score": 0.584,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "document",
          "objects",
          "stores",
          "document objects",
          "vector stores"
        ],
        "semantic": [],
        "merged": [
          "document",
          "objects",
          "stores",
          "document objects",
          "vector stores"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.32400998637156736,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.531071+00:00"
      }
    },
    {
      "chapter_number": 3,
      "title": "Segment 3 (pages 17-24)",
      "start_page": 17,
      "end_page": 24,
      "summary": "1.3 Building LLM applications and AI agents\nLLM-based applications or engines – Systems that provide a specific\nLangChain supports their development—starting with LLM-based\napplications and engines.\n1.3.1 LLM-based applications: summarization and Q&A\nengines\nAn LLM-based engine acts as a backend tool that handles specific natural\nFor example, a summarization engine\nFigure 1.4 A summarization engine efficiently summarizes and stores content from large volumes\nAnother common type is the Question & Answer (Q&A) engine, which\nanswers user queries against a knowledge base.\nA Q&A engine works in two\nIn the ingestion phase, the engine builds its knowledge base by pulling in\nEmbeddings are vector representations of words, tokens, or larger text units\nallowing language models to understand meaning, context, and similarity.\nIn the query phase, the engine takes a user’s question, turns it into an\nuses both the question and the retrieved context to generate an accurate,\nRetrieval-Augmented Generation (RAG) is a design pattern where the LLM’s\nFigure 1.5 A Q&A engine implemented with RAG design: an LLM query engine stores domain-\nspecific document information in a vector store.\nconverts the natural language question into its embeddings (or vector) representation, retrieves\nthe related documents from the vector store, and then gives the LLM the information it needs to\nLangChain makes it straightforward to build engines like question-answering\nloaders, text splitters, embedding models, vector stores, and retrievers.\ndifferent embedding models and vector stores.\nFor example, an engine\nhandling a user request might convert natural language instructions into API\ncalls, pull data from outside systems, and then use an LLM to interpret and\n1.3.2 LLM-based chatbots\nAn LLM-based chatbot acts as an intelligent assistant, enabling ongoing,\nnatural conversations with a language model.\nLLM-based chatbots are usually specialized for tasks such as summarization,\nsummarization chatbot (figure 1.6) builds on a basic summarization engine,\nFigure 1.6 A summarization chatbot has some similarities with a summarization engine, but it\nThe crucial difference between a summarization engine and a summarization\nFigure 1.7 Sequence diagram that outlines how a user interacts with an LLM through a chatbot",
      "keywords": [
        "LLM",
        "engine",
        "summarization",
        "summarization engine",
        "vector store",
        "vector",
        "model",
        "natural language",
        "language",
        "applications",
        "Systems",
        "text",
        "LLM applications",
        "summarization chatbot",
        "Building LLM applications"
      ],
      "concepts": [
        "engines",
        "text",
        "llm",
        "context",
        "conversational",
        "conversations",
        "summarization",
        "summarizes",
        "model",
        "language"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 23,
          "title": "",
          "score": 0.827,
          "base_score": 0.677,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 36,
          "title": "",
          "score": 0.793,
          "base_score": 0.643,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 24,
          "title": "",
          "score": 0.776,
          "base_score": 0.626,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 35,
          "title": "",
          "score": 0.772,
          "base_score": 0.622,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 27,
          "title": "",
          "score": 0.759,
          "base_score": 0.609,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "engine",
          "summarization",
          "llm based",
          "llm",
          "summarization engine"
        ],
        "semantic": [],
        "merged": [
          "engine",
          "summarization",
          "llm based",
          "llm",
          "summarization engine"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.40313074697862733,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.531152+00:00"
      }
    },
    {
      "chapter_number": 4,
      "title": "Segment 4 (pages 25-33)",
      "start_page": 25,
      "end_page": 33,
      "summary": "techniques like role instructions, few-shot examples, and advanced prompt\n1.3.3 AI agents\nAn AI agent is a system that works with a large language model (LLM) to\nAt each step, the agent consults the LLM to decide which tools to use, runs\nConsider this example: a tour operator uses an AI agent to generate holiday\n1. The agent sends a prompt to the LLM asking it to choose the most\n2. Guided by a developer-crafted prompt listing available tools and their\ndescriptions, the LLM selects the appropriate ones and generates the\n3. The agent executes the queries, gathers the results, and sends another\nprompt to the LLM containing both the original holiday request and the\n4. The LLM responds with a summarized holiday plan that includes all\nFigure 1.8 Workflow of an AI agent tasked with assembling holiday packages: An external client\nThe agent prompts the LLM to\nFinally, the agent forwards the summarized package\nThe workflow can involve multiple iterations between the agent and the LLM\nalternative design could be based on a set of more granular agents\nIn the holiday planning example, the agent could be\nLangChain’s agent framework also allows developers to incorporate human\nThere’s been a surge of interest in AI agents, and major players like OpenAI,\nLangChain’s agent framework—\nadvanced agents with LangGraph, learning how to design, orchestrate, and\nIn many ways, an AI agent represents the most advanced form of LLM\nAgents dynamically select and use multiple tools,\nguided by prompts you design, making them powerful across industries like\nInterest in agents has accelerated with the introduction of the Model Context\nexpose tools through MCP servers, which agents can then access via MCP\nby major LLM providers like OpenAI and Google—and thousands of tools\nWe’ll dive into MCP in Chapter 13, where you’ll learn the protocol, explore\ndirectly into an agent application.\n1.4 Typical LLM use cases\nLanguage Models (LLMs).” We’ll explore this extensively in the\nLLM-powered agents using LangGraph in Chapter 12.\nThese use cases assume the LLM can competently handle user requests.\nPrompts for large language models (LLMs) can be as simple as a single\nPrompt engineering is the practice of designing these inputs so\nA common technique here is in-context learning, where the model\ndomain-aware output from an LLM without needing extra training data.\nprompts consistently, which we’ll explore in the next chapter.\nprompt engineering alone has limits—especially when applications need to\nusing an embedding model.\nefficiently, and generate embeddings, then store everything in a vector\nFigure 1.9 A collection of documents is split into text chunks and transformed into vector-based\nBoth text chunks and related embeddings are then stored in a vector store.\n2. request LLM provider to calculate the\nText ingestion script d Embeddings model\nLLM provider",
      "keywords": [
        "LLM",
        "agent",
        "LLMs",
        "prompt",
        "Embeddings model LLM",
        "model",
        "holiday",
        "MCP",
        "prompt engineering",
        "generation",
        "model LLM provider",
        "tools",
        "LLM provider",
        "text",
        "’ll"
      ],
      "concepts": [
        "agent",
        "llm",
        "prompt",
        "uses",
        "useful",
        "coding",
        "tools",
        "providers",
        "provides",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 17,
          "title": "",
          "score": 0.82,
          "base_score": 0.67,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 45,
          "title": "",
          "score": 0.713,
          "base_score": 0.563,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 1,
          "title": "",
          "score": 0.678,
          "base_score": 0.678,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 42,
          "title": "",
          "score": 0.673,
          "base_score": 0.523,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 47,
          "title": "",
          "score": 0.556,
          "base_score": 0.556,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "agent",
          "holiday",
          "llm",
          "agents",
          "llm provider"
        ],
        "semantic": [],
        "merged": [
          "agent",
          "holiday",
          "llm",
          "agents",
          "llm provider"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.35481299717892567,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.531220+00:00"
      }
    },
    {
      "chapter_number": 5,
      "title": "Segment 5 (pages 34-41)",
      "start_page": 34,
      "end_page": 41,
      "summary": "In Chapter 6, you’ll learn techniques for having the LLM\n3. Flexibility: By swapping embedding models, retrievers, or vector stores,\n\"Grounding\" an LLM involves crafting prompts that include context pulled\nthat the LLM generates its response based on verified facts rather than relying\nA “hallucination” is when a large language model generates an incorrect,\nTo make RAG reliable, prompts should explicitly instruct the LLM to rely\nIn short, RAG bridges the gap between static pretrained models and dynamic,\nRAG still don’t meet your needs, the next step is fine-tuning the model,\nFine-tuning is the process of adapting a pre-trained LLM to perform better in\nThis is done by training the model on a curated\nThe main benefit of fine-tuning is efficiency: once a model has absorbed\nefficient fine-tuning methods have lowered both cost and complexity, making\nIt allows models to capture domain-\nspecific jargon and workflows in ways generic models struggle to match.\nIn summary, fine-tuning customizes an LLM for domain expertise and\nWhen developing LLM-based applications, you’ll find a wide range of\nmodels to choose from.\nstandardized interface, you can switch models with minimal code changes—\nconsiderations for choosing the right LLM for a specific task.\nModel Purpose:\nsentiment analysis, most large model families (GPT, Gemini,\nFor specialized tasks—such as code generation—seek out models\nFor example, some models support up to 2\nIf your app needs to handle multiple languages, opt for models with\nSome models may\nModel size:\nSmaller models can be more cost-effective and faster,\nA key distinction in LLM behavior is between instruction models\nand reasoning models.\nInstruction models, such as OpenAI’s GPT-4 series or Google\ntask should be performed and want the LLM to follow your\nReasoning models, such as OpenAI’s o-series or Google Gemini\nThese models not only execute work but also\nThe trade-offs are cost and speed: reasoning models are generally\nslower and more expensive, while instruction models are faster and\nwant the model to simply follow a plan you provide or to figure out\nOpen-source models (Llama, Mistral, Qwen, Falcon) provide\nProprietary models are easier to set up via API and may deliver\nyour system to use different models for different tasks.\napplications powered by large language models.\nadvanced class of LLM-powered systems, where engines and tools come\nAlthough the examples reference OpenAI models for accessibility and quick\nwins, you’ll also learn how to use open-source models through the inference\nBeyond building, you’ll explore the entire lifecycle of LLM applications:\nand large language models.\nIf you're new to LangChain, LLMs, and LLM applications, you'll encounter\nTable 1.1 LLM Application Glossary\nLLM\nA Large Language Model trained on vast text datasets to\nModel\nModels that convert text into vectors, capturing the\nLLM in generating accurate responses, often through\nThe response generated by an LLM when given a\nprompt, based on the model's predictive text abilities.\nLLM-Based\nLLM-Based\nA chatbot using an LLM, optimized with prompts and\nRetrieval Augmented Generation combines LLM\nThe model's tendency to generate incorrect or fabricated\nLLM-Based\nwith a language model.\nAdapting a pre-trained LLM for specific tasks by\nembedding models, retrievers, and vector stores, making it\nLLM-based",
      "keywords": [
        "LLM",
        "models",
        "LLMs",
        "RAG",
        "applications",
        "’ll",
        "fine-tuning",
        "tasks",
        "LLM applications",
        "context",
        "language model",
        "large language model",
        "cost",
        "LLM Application Glossary",
        "reasoning models"
      ],
      "concepts": [
        "model",
        "llm",
        "applications",
        "application",
        "task",
        "context",
        "fine",
        "domains",
        "base",
        "generates"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 56,
          "title": "",
          "score": 0.75,
          "base_score": 0.6,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 62,
          "title": "",
          "score": 0.63,
          "base_score": 0.48,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "",
          "score": 0.612,
          "base_score": 0.462,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 1,
          "title": "",
          "score": 0.578,
          "base_score": 0.428,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 57,
          "title": "",
          "score": 0.568,
          "base_score": 0.418,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "models",
          "llm",
          "model",
          "fine tuning",
          "fine"
        ],
        "semantic": [],
        "merged": [
          "models",
          "llm",
          "model",
          "fine tuning",
          "fine"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.28620690740106236,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.531289+00:00"
      }
    },
    {
      "chapter_number": 6,
      "title": "Segment 6 (pages 42-49)",
      "start_page": 42,
      "end_page": 49,
      "summary": "Prompt engineering remains a critical skill for shaping LLM behavior,\n2 Executing prompts\nEnhancing prompt responses using one, two, or a few-shot learning\nExamples of using prompts with ChatGPT and the OpenAI API\ngenerate accurate and relevant responses, your prompts must be well-crafted\nIn LangChain, prompt design plays a central\nLangChain's suite of prompt engineering tools, including PromptTemplate\n2.1 Running prompts programmatically\nLangChain applications rely on well-crafted prompts to generate\nUnlike prompts entered manually in interfaces like ChatGPT, LangChain\nprompts are typically constructed and sent to the LLM programmatically as\nand executing prompts using the OpenAI API directly, and then explore how\nto run and manage prompts within LangChain.\n\"Creating an OpenAI key\" in Section 1.7 and follow the instructions in\n2.1.1 Setting up an OpenAI Jupyter Notebook environment\nC:\\Github\\building-llm-applications\\ch02>\nC:\\Github\\building-llm-applications\\ch02>python -m venv env_ch02\nC:\\Github\\building-llm-applications\\ch02>.\\env_ch02\\Scripts\\activ\n(env_ch02) C:\\Github\\building-llm-applications\\ch02>\na Jupyter notebook for executing prompts with OpenAI models.\n(env_ch02) C:\\Github\\building-llm-applications\\ch02>pip install -\n(env_ch02) C:\\Github\\building-llm-applications\\ch02>jupyter noteb\n(env_ch02) C:\\Github\\building-llm-applications\\ch02>pip install n\nAfter about a minute, the installation of the notebook and OpenAI packages\n(env_ch02) C:\\Github\\building-llm-applications\\ch02>jupyter noteb\nRename … and name the notebook file: 02-prompt_examples.ipynb.\nNotebook and then rename the file to prompt_examples.ipynb\nenvironment, installed the OpenAI library, and launched a Jupyter notebook\n2.1.2 Minimal prompt execution\nIn the next notebook cell, enter and execute the following code.\nprompt_input = \"\"\"Write a short message to remind users to be vig\n{\"role\": \"user\", \"content\": prompt_input}\ncapturing prompts.\n{\"role\": \"user\", \"content\": prompt_input}\nsame prompt) to 2 (the output will vary considerably at each execution\nof the same prompt).\n2.2 Running prompts with LangChain\n(env_ch02) c:\\Github\\building-llm-applications\\ch02>pip install l",
      "keywords": [
        "Jupyter notebook",
        "prompts",
        "OpenAI",
        "OpenAI API key",
        "notebook",
        "OpenAI API",
        "Github",
        "Jupyter",
        "OpenAI Jupyter Notebook",
        "LangChain",
        "Jupyter notebook environment",
        "API",
        "LLM",
        "Prompt engineering",
        "API key"
      ],
      "concepts": [
        "prompts",
        "langchain",
        "openai",
        "application",
        "applications",
        "llm",
        "model",
        "creating",
        "create",
        "responses"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 53,
          "title": "",
          "score": 0.883,
          "base_score": 0.733,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 54,
          "title": "",
          "score": 0.666,
          "base_score": 0.516,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 10,
          "title": "",
          "score": 0.614,
          "base_score": 0.614,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 1,
          "title": "",
          "score": 0.594,
          "base_score": 0.594,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 7,
          "title": "",
          "score": 0.58,
          "base_score": 0.58,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ch02",
          "applications ch02",
          "notebook",
          "env_ch02",
          "jupyter"
        ],
        "semantic": [],
        "merged": [
          "ch02",
          "applications ch02",
          "notebook",
          "env_ch02",
          "jupyter"
        ]
      },
      "topic_id": 6,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3085857887426232,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.531342+00:00"
      }
    },
    {
      "chapter_number": 7,
      "title": "Segment 7 (pages 50-59)",
      "start_page": 50,
      "end_page": 59,
      "summary": "prompt_input = \"\"\"Write a coincise message to remind users to be \nresponse = llm.invoke(prompt_input)\n2.3 Prompt templates\nWhen building LLM applications, creating flexible prompt templates that\nparametrized prompts without the need for custom functions.\n2.3.1 Implementing a prompt template with a Python function\nTo illustrate the concept of a template, let's create a text summarization\ntemplate that requests the text, desired summary length, and preferred tone.\ndef generate_text_summary_prompt(text, num_words, tone):\nLet’s use the prompt template to generate a prompt and then execute it\ninput_prompt = generate_text_summary_prompt(text=segovia_aqueduct_\nresponse = llm.invoke(input_prompt)\nWith LangChain, you don't need to implement a prompt template function\nfrom langchain_core.prompts import PromptTemplate\nprompt_template = PromptTemplate.from_template(\"You are an experi\nTo use the prompt template, create a prompt instance and format it with your\nprompt = prompt_template.format(text=segovia_aqueduct_text, num_w\nresponse = llm.invoke(prompt)\ninto prompt engineering.\n2.4 Types of prompt\nAlthough these prompting techniques are key to developing LangChain\n2.4.1 Text classification\npredefined categories, as demonstrated in the following prompt:\nInstruction: Classify the following text into one of these categories: history,\nInstruction: Classify the following text into one of these categories: history,\nInstruction: Classify the following text into one of these categories: history,\nIn summary, in a standard text classification prompt, you find three\ncomponents: an Instruction, the input Text, and an Output specification.\nNow, let's delve into a slightly specialized text classification: sentiment\nSentiment analysis is a specific type of text classification that aims to\ndetermine whether a given text is perceived as positive, neutral, or negative.\nFirst prompt example:\nInstruction: Classify the following text as positive, neutral or negative\nInstruction: Classify the following text as positive, neutral or negative\nText: this is the worst movie I have watched this month\nThird prompt example:\nInstruction: Classify the following text as positive, neutral or negative\nyou can input them all into a single prompt:\nNow, let's create a prompt for one of the most common LLM use cases: text\n2.4.3 Text summarization\nCreating a text summarization prompt is simple: you only need to ask for a\nsummary of an input text with your preferred length.\nInstruction: write a 30 word summary for the following text\nText: Home PCs from the 1980s were iconic pioneers of personal computing.\nWrite a piece on the diver watches, mentioning the following facts:\ndiver watch was the Seamaster\nDiver watches have a storied history, encapsulating the spirit of adventure\npioneering role in the evolution of diver watches.\nOmega, a respected Swiss watchmaker, made its mark on the diver watch\nInstruction: Write a piece on the diver watches\ndiver watch was the Seamaster\n**Diver Watches: Timepieces of Exploration and Style**\nDiver watches have long been cherished for their blend of functionality and\nworld's first purpose-designed diver watch.\nRolex didn't lag behind, introducing the Submariner in 1953, a watch that",
      "keywords": [
        "text",
        "prompt",
        "Seamaster",
        "output",
        "Rolex Oyster Perpetual",
        "diver watches",
        "prompt template",
        "text classification",
        "diver",
        "Submariner",
        "text classification prompt",
        "James Bond",
        "watches",
        "Omega Seamaster",
        "Rolex"
      ],
      "concepts": [
        "prompt",
        "text",
        "watched",
        "watches",
        "diver",
        "llm",
        "stock",
        "functions",
        "function",
        "functionality"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 10,
          "title": "",
          "score": 0.625,
          "base_score": 0.625,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 8,
          "title": "",
          "score": 0.609,
          "base_score": 0.459,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 6,
          "title": "",
          "score": 0.58,
          "base_score": 0.58,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 11,
          "title": "",
          "score": 0.534,
          "base_score": 0.534,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 9,
          "title": "",
          "score": 0.52,
          "base_score": 0.37,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "diver",
          "text",
          "watches",
          "prompt",
          "diver watches"
        ],
        "semantic": [],
        "merged": [
          "diver",
          "text",
          "watches",
          "prompt",
          "diver watches"
        ]
      },
      "topic_id": 7,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.29246615226471334,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.531393+00:00"
      }
    },
    {
      "chapter_number": 8,
      "title": "Segment 8 (pages 60-67)",
      "start_page": 60,
      "end_page": 67,
      "summary": "Now, let's go back to the same text we used in the summarization example\nInstruction: read the following text, and tell me which home computers were\nText: Home PCs from the 1980s were iconic pioneers of personal computing.\nComputers like the Apple II, IBM PC, and Commodore 64 revolutionized\nSum the palindromes in this sequence: 13, 1331, 121, 73, 99, 56, 232, 7\nThe palindromes in the sequence 13, 1331, 121, 73, 99, 56, 232, and 7 are\nSo, the sum of the palindromes in this sequence is 1691.\nTable 2.1 Prompt types and examples\nPrompt example\nText\nWrite a 30 word summary for the following text\nRead the following text, and tell me which home\nI'll demonstrate how to go beyond zero-shot learning to refine LLM prompts\nWhen you provide just one example in the prompt, it's called \"one-shot\nInstruction: Sum the palindromes in this sequence: 13, 1331, 121, 73, 99, 56,\nExample: 33 is a palindrome\npalindromes in this sequence: 13, 1331, 121, 73, 99, 56, 232, and 7.\nThe palindromes in the sequence are 1331, 121, 232, and 7.\nSo, the sum of the palindromes in this sequence is indeed 1691.\nInstruction: Sum the palindromes in this sequence: 13, 1331, 121, 73, 99, 56,\n33 is a palindrome\n44 is a palindrome\nCertainly, let's identify and sum the palindromes in the sequence: 13, 1331,\nThe palindromes in this sequence are 1331, 121, 232, and 7.\nSo, the sum of the palindromes in this sequence is 1691.\nInstruction: Sum the palindromes in this sequence: 13, 1331, 121, 73, 99, 56\n33 is a palindrome\n44 is a palindrome\nSteps: 1) identify the palindromes; 2) add them up\nStep 1: Identify the palindromes in the sequence.\nSo, the sum of the palindromes in the sequence is 1790.\nSum the palindromes in this sequence: 13, 1331, 121, 73, 99, 56, 232, 7\nLet's identify the palindromes in the given sequence:\n1331 is a palindrome\n121 is a palindrome\n99 is a palindrome\n232 is a palindrome\n7 is a palindrome\nTherefore, the sum of the palindromes in the sequence is 1790.\nreasoning tasks with minimal prompts, which previously required more\ncomplex prompts like the few-shot techniques you're about to learn.",
      "keywords": [
        "palindromes",
        "sequence",
        "sum",
        "text",
        "prompt",
        "Output",
        "Sum the palindromes",
        "LLM",
        "Reasoning",
        "LLMs",
        "Java Virtual Machine",
        "James Bond",
        "Instruction",
        "Seamaster",
        "Question answering"
      ],
      "concepts": [
        "prompt",
        "text",
        "instruction",
        "instructions",
        "output",
        "examples",
        "reasoning",
        "steps",
        "palindromes",
        "user"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 7,
          "title": "",
          "score": 0.609,
          "base_score": 0.459,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 9,
          "title": "",
          "score": 0.54,
          "base_score": 0.39,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 11,
          "title": "",
          "score": 0.407,
          "base_score": 0.407,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 12,
          "title": "",
          "score": 0.406,
          "base_score": 0.406,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 10,
          "title": "",
          "score": 0.38,
          "base_score": 0.38,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "palindromes",
          "palindromes sequence",
          "sequence",
          "sum palindromes",
          "sum"
        ],
        "semantic": [],
        "merged": [
          "palindromes",
          "palindromes sequence",
          "sequence",
          "sum palindromes",
          "sum"
        ]
      },
      "topic_id": 7,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2430828604678799,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.531455+00:00"
      }
    },
    {
      "chapter_number": 9,
      "title": "Segment 9 (pages 68-75)",
      "start_page": 68,
      "end_page": 75,
      "summary": "examples, known as \"few-shot learning.\" To demonstrate this, let me create a\ngame where a number can be classified as Abra, Kadabra, or Abra Kadabra.\nClassify the following numbers as Abra, Kadabra or Abra Kadabra:\nThe classification of numbers into \"Abra,\" \"Kadabra,\" or \"Abra Kadabra\"\nClassify the following numbers as Abra, Kadabra or Abra Kadabra:\nLet's classify the numbers as Abra, Kadabra, or Abra Kadabra based on\ncorrectly classify numbers without examples, using \"zero-shot\" learning.\nprompt_input = \"\"\"Classify the following numbers as Abra, Kadabra \nexamples are hardcoded in the prompt.\nfrom langchain_core.prompts.few_shot import FewShotPromptTemplate\n\"number\": 6,\n\"number\": 15,\n\"number\": 12,\n\"number\": 21,\n\"number\": 70,\nexample_prompt = PromptTemplate(input_variables=[\"number\", \"reaso\nfew_shot_prompt = FewShotPromptTemplate(\nsuffix=\"Classify the following numbers as Abra, Kadabra or Ab\n35 \\ divisible by 5 and by 7 \\ Abra Kadabra\nas \"chain of thought,\" which blends \"providing logical steps\" with \"few-shot\nodd numbers is divisible by 3.\nIs the following sequence \"Strange\"?\ndefine what a \"strange\" sequence is, please provide more details for a more\nI will now create a prompt using the Chain of Thought technique, as\nThis prompt, an\nextension of a few-shot learning prompt, will include:\n1. Several sample sequences for few-shot learning\nA: 5 and 7 are odd numbers; the sum of 5 and 7 is 12; 12 is divisible by 3 //\nA: 1, 5 and 7 are odd numbers; the sum of 1, 5 and 7 is 13; 13 is not divisible\nA: 5, 7, 9 are odd numbers; the sum of 5, 7 and 9 is 21; 21 is divisible by 3 //\nThe sequence \"3, 4, 5, 7, 10, 18, 22, 24\" has 3 odd numbers: 3, 5, and 7.\ncriteria, this sequence is **Strange**.\nof numbers.\nusing information provided within the context of a prompt.\nthe prompt context.\nshot learning, as well as providing step-by-step guidance, often referred to as\nThis section covered a range of in-context learning prompts, which I have\nTable 2.2 In-context learning prompts\nNo example is provided in the prompt\nOne example is provided in the prompt\nTwo examples are provided in the prompt\nA number of examples are provided in the\nprompt\nA number of examples are provided in the\nprompt, and for each example, all the logical",
      "keywords": [
        "Abra Kadabra",
        "divisible",
        "Abra",
        "Kadabra",
        "Abra Kadabra based",
        "numbers",
        "Strange",
        "number divisible",
        "prompt",
        "learning",
        "sequence",
        "odd numbers",
        "numbers as Abra",
        "thought",
        "chain of thought"
      ],
      "concepts": [
        "number",
        "providing",
        "provide",
        "prompt",
        "examples",
        "strange",
        "technique",
        "sequence",
        "mathematical",
        "thought"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 8,
          "title": "",
          "score": 0.54,
          "base_score": 0.39,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 7,
          "title": "",
          "score": 0.52,
          "base_score": 0.37,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 10,
          "title": "",
          "score": 0.311,
          "base_score": 0.311,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "abra",
          "kadabra",
          "abra kadabra",
          "numbers",
          "numbers abra"
        ],
        "semantic": [],
        "merged": [
          "abra",
          "kadabra",
          "abra kadabra",
          "numbers",
          "numbers abra"
        ]
      },
      "topic_id": 7,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.20442446399393244,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.531495+00:00"
      }
    },
    {
      "chapter_number": 10,
      "title": "Segment 10 (pages 76-83)",
      "start_page": 76,
      "end_page": 83,
      "summary": "Chain of Thought improves how language models handle complex reasoning\nintegrates with various models and prompts.\n2.6 Prompt structure\nCombining all the prompt elements from earlier sections results in the\nPersona: Specify the role you want the language model (LLM) to\nContext: Provide detailed background information to help the LLM\nspecified, the LLM assumes a text answer.\nExamples: In cases where the LLM may lack sufficient training for\nPersona: You are an experienced Large Language Model (LLM) developer\nContext: You have been invited to give a keynote speech for a LLM event.\nmany popular LLMs and LLM based chatbots have been launched since\nStudies (for example “The Prompt Report: A Systematic Survey of\nPrompting Techniques”, https://arxiv.org/abs/2406.06608) have found that\nexplicitly naming different parts of a prompt tends to improve results.\nthe purpose of the text in the prompt on their own.\nothers (like Context or Tone).\nIf you want to delve deeper into prompt engineering, I highly recommend the\nhttps://github.com/dair-ai/Prompt-Engineering-Guide\nhttps://github.com/promptslab/Awesome-Prompt-Engineering\nA prompt is a specific request that guides the LLM, giving it instructions\nDifferent types of prompts are tailored for specific tasks, such as text\nTraining involves enhancing the prompt with examples, categorized as\nstructure with sections like persona, context, instruction, input, steps,\nYou should adapt the prompt structure to your use case by choosing\nLLM UI like ChatGPT, software developers typically automate prompt\nLangChain supports prompt engineering with a range of classes, from\n3 Summarizing text using\nSummarization of large documents exceeding the LLM’s context\nSummarization across multiple documents\nSummarization of structured data\nIn Chapter 1, you explored three major LLM application types:\nautomating tasks like summarization.\nconstructing a more advanced summarization engine in the next chapter.\nsummarization engine is a practical entry point for developing LLM\neach suited to specific scenarios like large documents, content consolidation,\nSince you’ve already worked with summarizing\n3.1 Summarizing a document bigger than context\nAs mentioned in chapter 2, each LLM has a maximum prompt size, also\nThe \"LLM context window\" represents the maximum size of the prompt\nprovided to an LLM, comprising instructions and context.\nAs the context window for popular LLMs continues to grow, you may still\nFigure 3.1 Summarizing a document bigger than the LLM’s context window: this involves\nsplitting the document into smaller chunks, summarizing each chunk, and then summarizing the\nsummarize\ndocument chunks |",
      "keywords": [
        "LLM",
        "LLMs",
        "prompt",
        "Thought",
        "Context",
        "Tree of Thought",
        "context window",
        "Chain of Thought",
        "Thread of Thought",
        "summarization",
        "text",
        "LLM context window",
        "language models",
        "models",
        "language"
      ],
      "concepts": [
        "prompts",
        "summarization",
        "summarize",
        "llm",
        "contexts",
        "documents",
        "document",
        "chunk",
        "improves",
        "improving"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 7,
          "title": "",
          "score": 0.625,
          "base_score": 0.625,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 6,
          "title": "",
          "score": 0.614,
          "base_score": 0.614,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 4,
          "title": "",
          "score": 0.549,
          "base_score": 0.549,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 56,
          "title": "",
          "score": 0.518,
          "base_score": 0.518,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "",
          "score": 0.467,
          "base_score": 0.467,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "prompt",
          "context",
          "window",
          "llm context",
          "context window"
        ],
        "semantic": [],
        "merged": [
          "prompt",
          "context",
          "window",
          "llm context",
          "context window"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3016211749077734,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:00:35.531545+00:00"
      }
    },
    {
      "chapter_number": 11,
      "title": "Segment 11 (pages 84-95)",
      "start_page": 84,
      "end_page": 95,
      "summary": "summarizing each one, and then summarizing the combined summaries.\nTo start, you need to split the text into chunks using a tokenizer.\n(env_ch03) C:\\Github\\building-llm-applications\\ch03>pip install t\n3.1.1 Chunking the text into Document objects\nLet's summarize the book \"Moby Dick\" using its text file, Moby-Dick.txt,\nNow you're ready to set up the first chain, which will break the document into\ntext_chunks_chain = (\nThe next step is to set up the \"map\" chain, which will run a summarization\nprompt for each document chunk.\nsummarize_chunk_prompt_template = \"\"\"\nsummarize_chunk_prompt = PromptTemplate.from_template(summarize_c\nsummarize_chunk_chain = summarize_chunk_prompt | llm\nsummarize_map_chain = (\n'summary': summarize_chunk_chain | StrOutputParser()  \nsummarize_map_chain, and each chunk will be summarized in parallel by the\ninner summarize_map_chain.\nSetting up the reduce chain, which summarizes the summaries from each\ndocument chunk, follows a process similar to the map chain but requires a bit\nsummarize_summaries_prompt_template = \"\"\"\nText: {summaries}\nsummarize_summaries_prompt = PromptTemplate.from_template(summari\nsummarize_reduce_chain = (\n| summarize_summaries_prompt \nThe reduce chain includes a lambda function that combines the summaries\nsummarize_summaries_prompt prompt, which generates a final summary of\n3.1.5 Map-reduce combined chain\nFinally, we combine the document-splitting chain, the map chain, and the\nmap_reduce_chain = (\ntext_chunks_chain  #A\n| summarize_map_chain.map()  #B\n| summarize_reduce_chain  #C\nThis setup efficiently splits the input document in chunks, summarizes each\nfunction on summarize_map_chain is essential to enable parallel processing\nStart the map-reduce summarization of the large\nsummary = map_reduce_chain.invoke(moby_dick_book)\nAs previously mentioned, running the map_reduce_chain will incur costs; the\nLet's now proceed to the next use case: summarizing across documents.\n3.2 Summarizing across documents\nsources, such as Wikipedia or local files in Microsoft Word, PDF, and text\nFigure 3.2 Summarizing across documents using the Map-Reduce technique seen earlier: in this\nmethod, each document chunk undergoes a map operation to generate a summary.\nthese Document objects are converted into individual summaries, which are\nFigure 3.3 Summarizing across documents using the Refine technique: with this approach, a final\nfinal summary and one of the document chunks.\nThis process continues until all document\nchunks have been processed, resulting in the completion of the final summary.\nsummarize !\nsummarized\ndocument = Summary\ncontent from the PDF Summary + tet document\nand txt Documents | fe\nEach document is sent to the LLM for summarization, along with\nsummarizing large volumes of text, where some content loss is acceptable to\nWhen summarizing a large document, you typically start by breaking it into\nWhile you can create a document\n(env_ch03) C:\\Github\\Building-llm-applications\\ch03>pip install w\nNow, import the content from the Paestum Wikipedia page:\nfrom langchain.document_loaders import WikipediaLoader\nDocument object.\nBelow is the process to load these files into corresponding documents:\nfrom langchain.document_loaders import Docx2txtLoader\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.document_loaders import TextLoader\nThe document variables (word_docs, pdf_docs, txt_docs) are in plural mode\nbecause a loader always returns a list of documents, even if the list contains\nDocument for each.\n3.2.4 Creating the Document list\nDocument list:",
      "keywords": [
        "document",
        "text",
        "chain",
        "summary",
        "summarize",
        "Github",
        "documents",
        "content",
        "chunk",
        "final summary",
        "Map",
        "document chunk",
        "Paestum",
        "reduce",
        "OpenAI"
      ],
      "concepts": [
        "summaries",
        "summary",
        "document",
        "documents",
        "content",
        "important",
        "summarizing",
        "summarize",
        "summarized",
        "chunks"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 30,
          "title": "",
          "score": 0.795,
          "base_score": 0.645,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 12,
          "title": "",
          "score": 0.684,
          "base_score": 0.684,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 28,
          "title": "",
          "score": 0.674,
          "base_score": 0.524,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 31,
          "title": "",
          "score": 0.66,
          "base_score": 0.51,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 32,
          "title": "",
          "score": 0.658,
          "base_score": 0.508,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "document",
          "map",
          "summary",
          "summarizing",
          "summarize_map_chain"
        ],
        "semantic": [],
        "merged": [
          "document",
          "map",
          "summary",
          "summarizing",
          "summarize_map_chain"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30789575494215754,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.531601+00:00"
      }
    },
    {
      "chapter_number": 12,
      "title": "Segment 12 (pages 96-103)",
      "start_page": 96,
      "end_page": 103,
      "summary": "3.2.5 Progressively refining the final summary\nDOC SUMMARY:\"\"\"\ndoc_summary_chain = doc_summary_prompt | llm\nNext, set up the chain for refining the summary by iteratively combining the\ncurrent summary with the summary of an additional document:\nrefine_summary_template = \"\"\"\nYour must produce a final summary from the current refined summar\nrefine_summary_prompt = PromptTemplate.from_template(refine_summa\nrefine_chain = refine_summary_prompt | llm | StrOutputParser()\nFinally, define a function that loops over each document, summarizes it using\nthe doc_summary_chain, and refines the overall summary using the\ndef refine_summary(docs):\ncurrent_refined_summary = ''\ncurrent_refined_summary = refine_chain.invoke(intermediat\nreturn {\"final_summary\": current_refined_summary,\nYou can now start the summarization process by calling refine_summary()\nfull_summary = refine_summary(all_docs)\nsummarization approach based on whether you need to summarize multiple unrelated documents\nto summarize one or\nyou're summarizing one or multiple documents.\nsingle prompt for summarization.\nof each document is included in the final summary.\n3.4 Summary\nmethod summarizes each document or chunk individually in the \"map\"\nfinal summary.\nWhen summarizing multiple documents, load each document with the\nrefine the summary by integrating each document’s summary until all\nMap-reduce is suited for large text summaries where some content loss\nUsing prompt engineering for creating web searches and summarizing\nBuilding on the content summarization techniques from chapter 3, this\nsummary.\nsummaries into a final prompt for a consolidated summary (see figure 4.1).",
      "keywords": [
        "summary",
        "final summary",
        "summarization",
        "Document",
        "final",
        "documents",
        "refine",
        "LLM",
        "research summarization engine",
        "prompt",
        "research summarization",
        "content",
        "summarization engine",
        "current",
        "multiple documents"
      ],
      "concepts": [
        "summarizes",
        "summarized",
        "document",
        "documents",
        "summaries",
        "text",
        "llm",
        "langchain",
        "prompt",
        "step"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 11,
          "title": "",
          "score": 0.684,
          "base_score": 0.684,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 30,
          "title": "",
          "score": 0.612,
          "base_score": 0.612,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 32,
          "title": "",
          "score": 0.5,
          "base_score": 0.5,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 16,
          "title": "",
          "score": 0.482,
          "base_score": 0.482,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 41,
          "title": "",
          "score": 0.461,
          "base_score": 0.461,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "summary",
          "summarization",
          "final summary",
          "final",
          "document"
        ],
        "semantic": [],
        "merged": [
          "summary",
          "summarization",
          "final summary",
          "final",
          "document"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2756235191966295,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:00:35.531649+00:00"
      }
    },
    {
      "chapter_number": 13,
      "title": "Segment 13 (pages 104-111)",
      "start_page": 104,
      "end_page": 111,
      "summary": "searching and scraping, then set up the OpenAI LLM model for\nI am assuming you're using Visual Studio Code with the free Python\nIf you're new to Visual Studio Code, get it set up\nInstalling Visual Studio Code and the Python extension\nDownload and install the appropriate version of Visual Studio Code for your\nenter python in the search box; 3) Select Python (from Microsoft) and click Install.\nOnce installed, open Visual Studio Code, and click the Extensions icon on\nI'll briefly guide you through setting up a Python project in Visual Studio\nCode, creating a virtual environment, activating it, and installing necessary\nOpen Visual Studio Code, then choose File > Open Folder, navigate to the\nOpen a terminal within Visual Studio Code by selecting Terminal > New\nIf you've just installed Visual Studio Code or you're new to it, enable the\ninstall the required Python packages (I'm omitting the full path to ch04 for\nconfiguration to ensure you're running and debugging your code within the\nsearches and another for extracting text from related web pages.\n4.3.1 Implementing web searching\nperform web searches.\nAdd a new empty file named web_searching.py to the project and fill it with\ndef web_search(web_query: str, num_results: int) -> List[str]:\nCreate a separate Python file, such as web_searching_try.py, to test the\nfrom web_searching import web_search\nresult = web_search(web_query=\"How many titles did Michael Jordan \nOther web search engine wrappers provided by LangChain are\n4.3.2 Implementing web scraping\nWe'll scrape the web pages from the result list using Beautiful Soup, which is\nPlace the code shown in the listing below in a file\nnamed web_scraping.py:\nListing 4.1 web_scraping.py\nLet's try out the function by placing the code below in a file named\nweb_scraping_try.py:\nresult = web_scrape('https://en.wikipedia.org/wiki/List_of_career_\nin a file named llm_models.py (replace 'YOUR_OPENAI_API_KEY' with\nTo recap: we've set up a Visual Studio Code project and implemented the\nsearching, web scraping, and summarization requests to the LLM, let's step\nweb search, returns URLs, scrapes and summarizes web pages, and compiles a research report",
      "keywords": [
        "Visual Studio Code",
        "Visual Studio",
        "Studio Code",
        "open Visual Studio",
        "Code",
        "Python",
        "web",
        "Studio",
        "Visual",
        "Python extension",
        "installed Visual Studio",
        "file",
        "OpenAI",
        "Studio Code project",
        "LLM"
      ],
      "concepts": [
        "python",
        "code",
        "coding",
        "returns",
        "file",
        "installing",
        "creating",
        "create",
        "llm",
        "search"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 15,
          "title": "",
          "score": 0.66,
          "base_score": 0.51,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 14,
          "title": "",
          "score": 0.636,
          "base_score": 0.486,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 16,
          "title": "",
          "score": 0.628,
          "base_score": 0.478,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 19,
          "title": "",
          "score": 0.583,
          "base_score": 0.433,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 44,
          "title": "",
          "score": 0.445,
          "base_score": 0.445,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "visual",
          "visual studio",
          "studio code",
          "studio",
          "code"
        ],
        "semantic": [],
        "merged": [
          "visual",
          "visual studio",
          "studio code",
          "studio",
          "code"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2871794627491158,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.531696+00:00"
      }
    },
    {
      "chapter_number": 14,
      "title": "Segment 14 (pages 112-119)",
      "start_page": 112,
      "end_page": 119,
      "summary": "directly to the web search engine, you can use the LLM to create multiple\nsearch queries.\ncontext to short queries to improve search results.\nsearches.\nFor instance, rather than querying the search engine \"How many titles did\nGenerate three web search queries to research the following topic, aiming for\nweb searches and gathering results from each.\nFigure 4.5 Revised system architecture diagram, incorporating query rewriting: The process\nbegins by tasking the LLM to generate a specified number of queries based on the user's research\nThese queries are then submitted to the search engine.\n4. Web searches\nsearches\nmultiple web searches\nWe’ll create prompts for generating web search\n4.5.1 Crafting Web Search Prompts\nstocks?\" When guiding the LLM to generate web search queries based on this\nTo generate these instructions dynamically based on the research question,\nsuitable research assistant and provides instructions tailored to the user's\nListing 4.2 prompts.py: Prompts to select and generate assistant instructions\nASSISTANT_SELECTION_PROMPT_TEMPLATE = PromptTemplate.from_templat\nWith this prompt in place to select the appropriate assistant type and provide\ninstructions, you can now proceed to create the prompt for generating web\nsearches based on the user's question, as shown in the following listing.\nListing 4.3 prompts.py: Prompt to rewrite the user query into multiple web searches\nWEB_SEARCH_INSTRUCTIONS = \"\"\"\nWrite {num_search_queries} web search queries to gather as much i\n{{\"search_query\": \"query1\", \"user_question\": \"{user_question}\n{{\"search_query\": \"query2\", \"user_question\": \"{user_question}\n{{\"search_query\": \"query3\", \"user_question\": \"{user_question}\nWEB_SEARCH_PROMPT_TEMPLATE = PromptTemplate.from_template(\ntemplate=WEB_SEARCH_INSTRUCTIONS\nWith the web search prompt completed, let's move on to creating the\nQuestion: {search_query}\n4.5.3 Research Report prompt\nSimilarly, the prompt for generating the research report is straightforward:\nListing 4.5 prompts.py: Prompt for generating the research report\nRESEARCH_REPORT_INSTRUCTIONS = \"\"\"\nRESEARCH_REPORT_PROMPT_TEMPLATE = PromptTemplate.from_template(\ntemplate=RESEARCH_REPORT_INSTRUCTIONS\nASSISTANT_SELECTION_PROMPT_TEMPLATE,\nWEB_SEARCH_PROMPT_TEMPLATE,\nRESEARCH_REPORT_PROMPT_TEMPLATE\nsearches with 5 results per search would entail summarizing 20 web pages,\nNUM_SEARCH_QUERIES = 2\nNUM_SEARCH_RESULTS_PER_QUERY = 3\n4.6.4 Generating the web searches and collecting the results\nIn the process of generating web searches and collecting results, the first step\nis to execute the LLM prompt to determine the correct research assistant and\nrelated instructions based on the user's research question:\nassistant_selection_prompt = ASSISTANT_SELECTION_PROMPT_TEMPLATE.\nassistant_instructions = llm.invoke(assistant_selection_prompt)",
      "keywords": [
        "prompt",
        "web search queries",
        "search",
        "web",
        "web searches",
        "research",
        "question",
        "assistant",
        "web search",
        "user",
        "LLM",
        "instructions",
        "research report",
        "TEMPLATE",
        "Michael Jordan"
      ],
      "concepts": [
        "queries",
        "query",
        "prompt",
        "searches",
        "assistant",
        "research",
        "important",
        "importing",
        "summarization",
        "summarize"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 15,
          "title": "",
          "score": 0.829,
          "base_score": 0.679,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 16,
          "title": "",
          "score": 0.78,
          "base_score": 0.63,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 19,
          "title": "",
          "score": 0.753,
          "base_score": 0.603,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 13,
          "title": "",
          "score": 0.636,
          "base_score": 0.486,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 34,
          "title": "",
          "score": 0.603,
          "base_score": 0.603,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "web",
          "web search",
          "research",
          "search",
          "searches"
        ],
        "semantic": [],
        "merged": [
          "web",
          "web search",
          "research",
          "search",
          "searches"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.36618100176907925,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.531762+00:00"
      }
    },
    {
      "chapter_number": 15,
      "title": "Segment 15 (pages 120-128)",
      "start_page": 120,
      "end_page": 128,
      "summary": "Now, you can execute the prompt to generate web searches based on the\nweb_search_prompt = WEB_SEARCH_PROMPT_TEMPLATE.format(assistant_i\nweb_search_queries = llm.invoke(web_search_prompt)\nupon printing web_search_queries_list, you would obtain a list of search\nsearches_and_result_urls = [{'result_urls': web_search(web_query=w\nfor wq in web_search_queries_list]\nExecuting up to this point, the searches_and_result_urls variable would\nEach dictionary shows a search query and its corresponding result URLs (3\ncontains a search query and just one result URL:\nsearch_query_and_result_url_list = []\nfor qr in searches_and_result_urls:\nsearch_query_and_result_url_list.extend([{'search_query': qr[\nNow, search_query_and_result_url_list has 9 dictionaries, just as expected:\n[{'search_query': 'Astorga attractions', 'result_url': 'https://ww\nWith the web search queries and all related result URLs ready, the next move\nyour search results:\nresult_text_list = [ {'result_text': web_scrape(url=re['result_ur\nfor re in search_query_and_result_url_list]\nsummary will also keep the original search queries and URLs, which you'll\nresult_text_summary_list = []\nThis process results in result_text_summary_list, a list of 9 dictionaries.\nstringified_summary_list = [f'Source URL: {sr[\"result_url\"]}\\nSum\nfor sr in result_text_summary_list]\nresearch_summary=appended_result_summaries,\ncomplete research report like the one below, based on web summaries after\nit up to handle 10 web searches with 10 results each, the time required would\nsearch, page scraping, and summarization—into an efficient chain or\nmaster Web Research chain.\nFigure 4.6 Architecture of the chain-based research summarization engine: Each step of the\nThis master Web Research chain handles the entire process, as shown in\nall integrated into the master Web Research chain:\nWeb Searches chain: This chain generates multiple web searches based\nSearch and Summarization chain: This chain performs web searches,\nretrieves URLs from search results, scrapes the relevant web pages, and\noriginal question and the summaries generated from the search results.\nThe Search and Summarization chain, detailed in figure 4.7, is itself a\ncomposite of three chains: one for web search, one for scraping and\nsummarizing web pages, and one for compiling summaries into a single text\nFigure 4.7 The Search and Summarization chain is made up of three chains: a web search chain,\nFigure 4.7 shows the Search and Summarization chain, which combines three\nseparate chains: one for web searches, another for scraping and summarizing\n1. A separate instance of the Search and Summarization chain is created\nSearches chain.\n2. For each search result generated by the Search Result URLs chain, an\nindividual Search Result Text and Summary chain instance is launched",
      "keywords": [
        "search",
        "web",
        "result",
        "research",
        "web search",
        "chain",
        "SUMMARY",
        "URL",
        "text",
        "Web Research chain",
        "assistant",
        "list",
        "query",
        "urls",
        "Search Result URLs"
      ],
      "concepts": [
        "chain",
        "research",
        "likely",
        "result",
        "execute",
        "executing",
        "execution",
        "spain",
        "search",
        "summary"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 14,
          "title": "",
          "score": 0.829,
          "base_score": 0.679,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 19,
          "title": "",
          "score": 0.819,
          "base_score": 0.669,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 16,
          "title": "",
          "score": 0.777,
          "base_score": 0.627,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 13,
          "title": "",
          "score": 0.66,
          "base_score": 0.51,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 44,
          "title": "",
          "score": 0.54,
          "base_score": 0.54,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "web",
          "search",
          "chain",
          "urls",
          "result"
        ],
        "semantic": [],
        "merged": [
          "web",
          "search",
          "chain",
          "urls",
          "result"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.31611797553217297,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.531812+00:00"
      }
    },
    {
      "chapter_number": 16,
      "title": "Segment 16 (pages 129-139)",
      "start_page": 129,
      "end_page": 139,
      "summary": "4.7.2 Web Searches chain\nListing 4.6 Web Searches chain for rewriting the user query into web searches\nweb_searches_chain = (\nListing 4.7 Script to test the Web Searches chain\nfrom chain_2_1 import web_searches_chain\nweb_searches_list = web_searches_chain.invoke(assistant_instructi\n4.7.3 Search and Summarization chain\nThe Search and Summarization chain is designed to perform a web search\nbased on a query from the previous chain, retrieve URLs from the search\nSearch result URLs chain\nSearch result Text and Summary chain\nWe'll begin by building these sub-chains, starting with the Search Result\nURLs chain.\nSearch result URLs chain\nThis sub-chain carries out a web search, retrieving a specific number of\nListing 4.8 Search and Summarization chain\nsearch_result_urls_chain = (\nprevious chain, such as the search query and the original user question.\nchain, specifically the Web Searches chain.\ncome from the Web Searches chain.\nListing 4.9 Script to test the Search and Summarization chain\nfrom chain_3_1 import search_result_urls_chain\nresult_urls_list = search_result_urls_chain.invoke(web_search_dic\nSearch Result Text and Summary chain\nThe Search Result Text and Summary sub-chain handles a URL from the\nListing 4.10 Search Result Text and Summary chain\nsearch_result_text_and_summary_chain = (\n'search_result_text': web_scrape(url=x['result_url'])\nListing 4.11 Script to test the Search Result Text and Summary chain\nfrom chain_4_1 import search_result_text_and_summary_chain\nsearch_text_summary = search_result_text_and_summary_chain.invoke\nAssembling the Search and Summarization chain\nFigure 4.9 Enhanced Search and Summarization chain diagram: The Search Result URLs chain\ngenerates multiple URLs; each URL initiates an instance of the Result Text and Summary chain,\nAs illustrated in figure 4.9, the Search Result URLs chain generates multiple\nchain.\nListing 4.12 Search and Summarization chain\nfrom chain_2_1 import web_searches_chain\nfrom chain_3_1 import search_result_urls_chain\nfrom chain_4_1 import search_result_text_and_summary_chain\nsearch_and_summarization_chain = (\nsearch_result_urls_chain \n| search_result_text_and_summary_chain.map() # parallelize fo\nSummary chain, one for each dictionary from the Search Result URLs chain\nThis sub-chain merges summaries from each instance of the Result Text and\nchain.\n4.7.4 Web Research chain\nFigure 4.10 Web Research chain: This illustrates how each web search initiated by the Web\nAs shown in Figure 4.10, web searches from the Web Searches chain trigger\nmultiple instances of the Search and Summarization chains to run in parallel.\nEach chain generates a summary for a specific web search, and these\nListing 4.13 Web Research chain\nweb_research_chain = (\n| web_searches_chain \n| search_and_summarization_chain.map() # parallelize for each \nchain.\nTo test the Web Research chain, use the following script, saving it as\nListing 4.14 Script to test Web Research chain\nfrom chain_5_1 import web_research_chain\nweb_research_report = web_research_chain.invoke(question)",
      "keywords": [
        "SEARCH",
        "Search result Text",
        "chain",
        "Search result URLs",
        "Web Research chain",
        "Search result",
        "Web",
        "Web Searches chain",
        "result",
        "result URLs chain",
        "Summarization chain",
        "result Text",
        "Summary",
        "Summary chain",
        "Web Searches"
      ],
      "concepts": [
        "chain",
        "important",
        "web",
        "searches",
        "search",
        "urls",
        "url",
        "llm",
        "output",
        "research"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 19,
          "title": "",
          "score": 0.781,
          "base_score": 0.631,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 14,
          "title": "",
          "score": 0.78,
          "base_score": 0.63,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 15,
          "title": "",
          "score": 0.777,
          "base_score": 0.627,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 13,
          "title": "",
          "score": 0.628,
          "base_score": 0.478,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 40,
          "title": "",
          "score": 0.606,
          "base_score": 0.606,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "chain",
          "web",
          "search",
          "search result",
          "result"
        ],
        "semantic": [],
        "merged": [
          "chain",
          "web",
          "search",
          "search result",
          "result"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.36010421430726997,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.531867+00:00"
      }
    },
    {
      "chapter_number": 17,
      "title": "Segment 17 (pages 140-148)",
      "start_page": 140,
      "end_page": 148,
      "summary": "A research summarization engine is built around an LLM for efficient\n5 Agentic Workflows with\nOverview of agentic workflows and agents\nTransition from LangChain chains to an agentic workflow\nbecome more complex, agentic workflows have become essential—a pattern\nagentic workflows follow a predictable sequence of steps and do not\n5.1 Understanding Agentic Workflows and Agents\nLLM-powered agent-based systems typically follow one of two core design\npatterns: agentic workflows and agents.\nFigure 5.1 Workflows and agents: workflows use the LLM to choose the next step from a fixed set\nAgents, however, dynamically select and combine tools to achieve\nWorkflow\nAgent\nIn contrast, agents use language models for more than just task\nthe LLM to drive application behavior, workflows maintain a structured and\npredictable path, whereas agents can adapt in real time based on new\n5.1.1 Workflows\nWorkflows use the LLM to pick the next step from a limited set of choices.\nFigure 5.2 Common Workflows patterns: the Controller-Worker pattern uses the LLM in the\nIn the Router pattern, the LLM simply directs the task to the appropriate worker\nWorkflow\n5.1.2 Agents\nLLM agents use language models to perceive data, reason about it, decide on\nmemory of past interactions, and build dynamic workflows with branching\nUnlike fixed prompt-response systems, agents generate new flows\n5.1.3 When to Use Agent-Based Architectures\nThe concepts of LLM-based workflows and agents are closely related and\nfocus is on agentic workflows; we’ll explore agents in greater depth later in\nbreak complex tasks into smaller steps, make decisions based on previous\nIt’s best to adopt agentic workflows or agents when your use\ndeeper understanding of workflows, agents, and when to use them.\nagents\nA variety of frameworks are available for building agent-based systems, each\npersistent agentic workflows using graph-based execution, making it\nLangGraph builds on LangChain to manage more complex agentic\nworkflows with branching paths, stateful processing, and clear transitions\nmanage state, or handle complex agentic workflows.\nyou organize those components into a structured, stateful workflow.\nneed to repeat steps based on new information, or when you want to manage\nagents to take different paths based on previous results, making decision-\nCyclical workflows let agents repeat tasks until they meet\nLangGraph also makes it easier to understand and debug complex workflows.\nmulti-step reasoning, task planning, managing context in long conversations,\napplications get more complex, the benefits of using LangGraph’s agent-\nneeded to build smart, multi-step systems that can adapt and make decisions",
      "keywords": [
        "Agentic Workflows",
        "Workflows",
        "LLM",
        "agents",
        "Agentic",
        "LangGraph",
        "complex agentic workflows",
        "tasks",
        "GPT Researcher",
        "application",
        "state",
        "complex",
        "LangChain",
        "based",
        "chains"
      ],
      "concepts": [
        "agents",
        "workflow",
        "based",
        "tasks",
        "llm",
        "complex",
        "steps",
        "chains",
        "data",
        "result"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 4,
          "title": "",
          "score": 0.82,
          "base_score": 0.67,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 45,
          "title": "",
          "score": 0.697,
          "base_score": 0.547,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 42,
          "title": "",
          "score": 0.643,
          "base_score": 0.493,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 3,
          "title": "",
          "score": 0.617,
          "base_score": 0.617,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 1,
          "title": "",
          "score": 0.611,
          "base_score": 0.611,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "workflows",
          "agentic",
          "agentic workflows",
          "agents",
          "workflows agents"
        ],
        "semantic": [],
        "merged": [
          "workflows",
          "agentic",
          "agentic workflows",
          "agents",
          "workflows agents"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3337350274695836,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.531920+00:00"
      }
    },
    {
      "chapter_number": 18,
      "title": "Segment 18 (pages 149-157)",
      "start_page": 149,
      "end_page": 157,
      "summary": "Figure 5.3 LangGraph core Components: a strongly typed state (in this example, modelled with\nselect_assistant), perform tasks, and edges create directed data flows between nodes, in some\nResearch State\ngenerate_search_queries\nResearch State\ndef Research State\nResearch State\nResearch State\nAt the heart of every LangGraph application is a state object — in our\nexample, ResearchState — which defines a clear and strongly typed state\nIn a LangGraph, each node functions as a processing unit.\nState management is central to LangGraph applications.\nsearch_queries: Optional[List[dict]]\nsearch_results: Optional[List[dict]]\nresearch_summary: Optional[str]\nEach node receives the current state and returns updates that merge into the\ndef process_node(state: dict) -> dict:\nresult = do_something(state[\"input_data\"]) #A\ndef generate_search_queries(state: dict) -> dict:\n\"\"\"Generate search queries based on user question.\"\"\"\ngraph.add_node(\"generate_queries\", generate_search_queries) #A\ngraph.add_edge(\"generate_queries\", \"perform_searches\")\nA conditional edge uses a function to choose the next node based on the state:\ndef should_refine_queries(state: dict) -> str:\nif len(state[\"search_results\"]) < 2:\ngraph.add_conditional_edge(\"perform_searches\", should_refine_quer\nare relevant, redirect the flow back to generating new search queries.\n1. Choose the appropriate research assistant based on the user's question.\n2. Generate search queries.\nListing 5.1 Original LangChain implementation of the Web Research Assistant\nredirects the application to generate new search queries if less than 50%\nTo convert the web research assistant to LangGraph, I first identify the key\nQuery Generator: Creates search queries derived from the user's input.\nGenerator to create new search queries.\nroute_based_on_relevance, which checks the relevance of the search results\ngenerates new queries and repeats the search and evaluation steps.\nThe input state: The data each node requires to function.\nThe state updates: The information each node returns to update the\nStep 1: Define the State\nThe first step is to design the state structure that will flow through the graph.\nA well-defined state helps you keep track of data across all nodes.\nListing 5.2 State type of the LangGraph based Research assistant\nsearch_query: str\nsearch_query: str\nsearch_queries: Optional[List[SearchQuery]]\nsearch_results: Optional[List[SearchResult]]\nsearch_summaries: Optional[List[SearchSummary]]\nresearch_summary: Optional[str]\nStep 2: Convert Components to Node Functions\ndef select_assistant(state: dict) -> dict:",
      "keywords": [
        "state",
        "Optional",
        "Web Research Assistant",
        "Research State def",
        "Research",
        "search",
        "str",
        "Research Assistant",
        "Research State",
        "queries",
        "assistant",
        "search queries",
        "node",
        "Web Research",
        "question"
      ],
      "concepts": [
        "steps",
        "optional",
        "nodes",
        "graph",
        "based",
        "results",
        "queries",
        "query",
        "processing",
        "process"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 19,
          "title": "",
          "score": 0.717,
          "base_score": 0.717,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 44,
          "title": "",
          "score": 0.592,
          "base_score": 0.442,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 50,
          "title": "",
          "score": 0.514,
          "base_score": 0.514,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 15,
          "title": "",
          "score": 0.484,
          "base_score": 0.484,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 16,
          "title": "",
          "score": 0.482,
          "base_score": 0.482,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "state",
          "research",
          "optional",
          "dict",
          "research state"
        ],
        "semantic": [],
        "merged": [
          "state",
          "research",
          "optional",
          "dict",
          "research state"
        ]
      },
      "topic_id": 9,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3566079340723282,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.531976+00:00"
      }
    },
    {
      "chapter_number": 19,
      "title": "Segment 19 (pages 158-165)",
      "start_page": 158,
      "end_page": 165,
      "summary": "def generate_search_queries(state: dict) -> dict:\n\"\"\"Generate search queries based on the question.\"\"\"\nnum_search_queries=3\nsearch_queries = parse_search_queries(response.content) #D\nreturn {\"search_queries\": search_queries} #E\nnode for relevance evaluation and a conditional edge that dynamically alters\nthe flow based on the relevance of the search results.\ngraph.add_node(\"select_assistant\", select_assistant)  #B\ngraph.add_node(\"generate_search_queries\", generate_search_queries\ngraph.add_node(\"perform_web_searches\", perform_web_searches)  #B\ngraph.add_node(\"summarize_search_results\", summarize_search_resul\ngraph.add_node(\"evaluate_search_relevance\", evaluate_search_relev\ngraph.add_node(\"write_research_report\", write_research_report)  #\nreturn \"generate_search_queries\"\ngraph.add_edge(\"select_assistant\", \"generate_search_queries\")  #D\ngraph.add_edge(\"generate_search_queries\", \"perform_web_searches\") \ngraph.add_edge(\"perform_web_searches\", \"summarize_search_results\"\ngraph.add_edge(\"summarize_search_results\", \"evaluate_search_relev\ngraph.add_edge(\"write_research_report\", END)  #D\n\"evaluate_search_relevance\",\n\"generate_search_queries\": \"generate_search_queries\",\nredirects the flow back to the Query Generator to refine the search.\nAfter defining the graph, I compile it and run it using an initial state, as\n\"search_queries\": None,\n\"search_results\": None,\n\"search_summaries\": None,\nrefining search queries if needed, and ensure that the final report is based on\npath—either refining search queries if results are insufficient or\nConditional flows based on runtime evaluations let applications adapt,\nsuch as re-running searches when results are insufficient.\nImplementing semantic search using the RAG architecture\nIn this chapter, you’ll dive into two essential concepts: semantic search and\nmodels (LLMs) are used for semantic search through a chatbot, enabling you\nQ&A chatbot that searches across multiple documents.\nThis chapter focuses on RAG, the design pattern that powers semantic search\nRAG systems and understand how terms like \"semantic search\" and \"Q&A\"\n6.1 Semantic Search\nSemantic search differs from traditional keyword-based searches, which fail\nsemantic search chatbot's architecture.\nDocument: Contains the text for semantic search or information\nPrompt: Encapsulates the user’s question (semantic search) and the",
      "keywords": [
        "search",
        "semantic search",
        "queries",
        "graph.add",
        "state",
        "Graph",
        "assistant",
        "node",
        "semantic",
        "report",
        "generate",
        "results",
        "chatbot",
        "search queries",
        "flow"
      ],
      "concepts": [
        "graph",
        "node",
        "based",
        "base",
        "state",
        "searches",
        "searching",
        "langchain",
        "relevant",
        "step"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 15,
          "title": "",
          "score": 0.819,
          "base_score": 0.669,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 16,
          "title": "",
          "score": 0.781,
          "base_score": 0.631,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 14,
          "title": "",
          "score": 0.753,
          "base_score": 0.603,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 18,
          "title": "",
          "score": 0.717,
          "base_score": 0.717,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 13,
          "title": "",
          "score": 0.583,
          "base_score": 0.433,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "semantic search",
          "semantic",
          "search",
          "graph",
          "generate_search_queries"
        ],
        "semantic": [],
        "merged": [
          "semantic search",
          "semantic",
          "search",
          "graph",
          "generate_search_queries"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.323801851614458,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.532036+00:00"
      }
    },
    {
      "chapter_number": 20,
      "title": "Segment 20 (pages 166-173)",
      "start_page": 166,
      "end_page": 173,
      "summary": "Context is the text or information in the prompt, along with the user’s\nquestion, used to formulate an answer.\nSynthesize means to generate an answer from the question and context\nChatGPT or an alternative LLM-based chatbot like Gemini or Claude, and\nRead the following text and let me know how many temples are in Paestum,\nGreek temples.\ndate from the 6th century BC, while the Temple of Hera II (the so-called\nTemple of Neptune) was probably built about 460 BC and is the best\nThere are three temples in Paestum, and they are constructed in the Doric\nThese temples are:\nThese temples are remarkable for their well-preserved Doric architecture and\nreceived any answer about who built the temples.\nquestions so you can refine the answer through interaction.\nwho built the temples?\nThe temples in Paestum were built by Greek\nSo, the Greek colonists constructed these temples in the\nrounds of questions, ChatGPT answered all my questions about the temples\nA chatbot like ChatGPT creates a stateful session that remembers the\nallowing the user to refine the answer without resending the initial text.\nChatGPT answered:\npreserved Greek temples and ancient ruins in Paestum.\nHow many columns do the three temples have in total?\nChatGPT answered:\nThe three temples in Paestum have a total of 94 columns:\nThe Temple of Hera I (Basilica) has 6 columns on its shorter sides and 9\nChatGPT tried to answer, but the answer is wrong.\nThe Temple of Hera I has\nWhen a chatbot can't find the information in the provided text, it might rely\nthe information in the prompt and admit when it doesn't know the answer.\nIn one instance, ChatGPT provided accurate information about UNESCO, yet\nHow many columns do the three temples have in total?\ndoesn’t mention the total number of columns in the three temples in Paestum.\nDesigning safe prompts for Q&A chatbots reduces the chance of\nLet’s summarize the design of the basic LLM-based Q&A chatbot, like\n1. You send a prompt to the chatbot with the text you want to search for\n2. The prompt should instruct the chatbot to formulate an answer using\nBut what if your chatbot needs to answer questions about company\nWhen designing an enterprise Q&A chatbot, one of the main obstacles is that\nbase and retrieve only the specific content needed to answer a given question.\nFigure 6.2 Hypothetical design for an enterprise Q&A chatbot: the knowledge of the chatbot is\nNow, you might wonder: how can you connect the ChatGPT chatbot (or\nmore control over how the chatbot interacts with text sources and the LLM,\nGeneration: This refers to generating the answer to your question.\nand then use it to augment the LLM's generated answer.\n2. Question-Answering Stage (Retrieval and Generation Stage): The\nchatbot takes a user's question, retrieves relevant information from the\nBefore users can query the Q&A chatbot, you need to store relevant content,",
      "keywords": [
        "temples",
        "Paestum",
        "answer",
        "Temple of Hera",
        "chatbot",
        "Greek temples",
        "ChatGPT",
        "columns",
        "Greek",
        "text",
        "preserved Greek temples",
        "temples in Paestum",
        "information",
        "question",
        "prompt"
      ],
      "concepts": [
        "greek",
        "temples",
        "text",
        "answer",
        "answered",
        "question",
        "questions",
        "context",
        "content",
        "retrieve"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 22,
          "title": "",
          "score": 0.426,
          "base_score": 0.426,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 59,
          "title": "",
          "score": 0.421,
          "base_score": 0.421,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 23,
          "title": "",
          "score": 0.376,
          "base_score": 0.376,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 25,
          "title": "",
          "score": 0.368,
          "base_score": 0.368,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 26,
          "title": "",
          "score": 0.333,
          "base_score": 0.333,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "temples",
          "chatbot",
          "chatgpt",
          "answer",
          "paestum"
        ],
        "semantic": [],
        "merged": [
          "temples",
          "chatbot",
          "chatgpt",
          "answer",
          "paestum"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2062098204173029,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:00:35.532078+00:00"
      }
    },
    {
      "chapter_number": 21,
      "title": "Segment 21 (pages 174-182)",
      "start_page": 174,
      "end_page": 182,
      "summary": "converted into embeddings while being stored in a vector database, which stores a copy of the\noriginal chunks and their embeddings (vector form).\nVector\nYou can create embeddings using the vector store’s\nThe embeddings and content chunks are then stored in a\nvector database.\nFor example, querying the vector store for \"feline animals\"\nembeddings, and stored in a vector store, users can query your Q&A chatbot.\nquestion into embeddings; 2) then the retriever uses the embedding to perform a similarity\nsearch in the vector store.\n3) The vector store returns several relevant text chunks; 4) The\n‘Nn vector DB returns text\n} Vector ' __ and the retrieved chunks (the : (the response)\nlanguage query into its vector representation using an embedding model.\nthen uses the vectorized question to query the vector store, which understands\nthe query's semantic meaning and returns document chunks with embeddings\nvectorized question and the items in the vector index stored in the database.\nvector store.\nthe vector store provides the information for the answer.\n6.2 Vector Stores\nI have mentioned vector stores several times, but only briefly.\n6.2.1 What’s a Vector Store?\nA vector store is a storage system designed to efficiently store and query\nIn short, embeddings are vectors that capture the meaning of words in\nThe main use of vector stores in LLM and ML applications is to store\nSearches in vector stores are \"similarity searches,\" which measure the\ndistance between the embeddings of the query and those of the stored chunks.\n6.2.2 How Do Vector Stores Work?\nCardenas: https://weaviate.io/blog/distance-metrics-in-vector-search.\nThe first vector stores, like Milvus, appeared in 2019 to support dense vector\nthe vectors, or embeddings, have most dimensions with non-zero values.\nMilvus was initially built for image-based embeddings, where the vectors\n(LLMs), new vector stores emerged specializing in text-based semantic\nVector Databases\nThe first vector stores, known as \"vector libraries,\" like Faiss (developed by\nHandling Underlying Text: Vector libraries only stored embeddings,\nHandling Text and Embeddings: Vector databases store both the text\nQuerying During Import: Vector databases allow similarity searches\nFor the rest of the book, I will use \"vector store\" and \"vector database\"\n6.2.4 Most Popular Vector Stores\nTable 6.1 Most popular vector stores and related characteristics\nVector\nVector\nVector\nVector\nVector\nVector\nVector\nVector\nVector\nlearn how to store text in a vector store and perform semantic searches.\nWe'll use Chroma, a vector database that's easy to set up and use.",
      "keywords": [
        "vector",
        "vector store",
        "vector database",
        "embeddings",
        "chunks",
        "Popular Vector Stores",
        "store",
        "text",
        "Vector databases store",
        "Vector Libraries",
        "question",
        "LLM",
        "text chunks",
        "user question",
        "database"
      ],
      "concepts": [
        "vector",
        "embeddings",
        "stored",
        "stores",
        "chunks",
        "search",
        "searches",
        "text",
        "data",
        "database"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 24,
          "title": "",
          "score": 0.924,
          "base_score": 0.774,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 2,
          "title": "",
          "score": 0.867,
          "base_score": 0.717,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 36,
          "title": "",
          "score": 0.863,
          "base_score": 0.713,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 27,
          "title": "",
          "score": 0.807,
          "base_score": 0.657,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 23,
          "title": "",
          "score": 0.788,
          "base_score": 0.638,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "vector",
          "vector vector",
          "store",
          "embeddings",
          "stores"
        ],
        "semantic": [],
        "merged": [
          "vector",
          "vector vector",
          "store",
          "embeddings",
          "stores"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3110186124261251,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.532127+00:00"
      }
    },
    {
      "chapter_number": 22,
      "title": "Segment 22 (pages 183-190)",
      "start_page": 183,
      "end_page": 190,
      "summary": "pip install notebook chromadb==0.5.3 openai\nnotebook 06-chromadb-ingestion-and-querying.ipynb, otherwise create it\nOn In your notebook, import the chromadb module and create an in-memory\nclient for the vector database.\nchroma_client = chromadb.Client()\nNext, create a collection to store the content on Paestum from the Britannica\ntourism_collection = chroma_client.create_collection(name=\"touris\nquery_texts=[\"How many Doric temples are in Paestum\"],\nChroma understands the query's meaning and returns the correct text chunk\nbetween the query and answer embeddings.\nUnlike querying ChatGPT, where you had to provide the question and the full\ntext, querying Chroma only requires sending the question, as the content is\nTo see how close the returned text chunk (paestum-br-03) is to the question\ncompared to the other text chunks (paestum-br-01 and paestum-br-02),\nquery_texts=[\"How many Doric temples are in Paestum\"],\nThe embeddings for paestum-br-03 are the closest to the question’s\nthe semantically closest text chunks to your query.\nanswer, you still need an LLM model to process the original question and the\nNext, instantiate the HTTP client in your notebook or application like this:\nLet’s implement RAG by building a chatbot that uses the gpt-4o-mini model\nWe will then ask it the same question about Paestum's\nhad to send a prompt with both the question and the full text on Paestum from\nquery Chroma, retrieve the content, and feed it to GPT-4o-mini with the\noriginal question to get the full answer.\ndef query_vector_database(question):\nquery_texts=[question],\nresults_text = results['documents'][0][0]\nreturn results_text\nresults_text = query_vector_database(\"How many Doric temples are \nprint(results_text)\nWe need to craft a prompt that combines the user’s question with the context\nretrieved from the vector database, and then submit it to the LLM.\ndef prompt_template(question, context):\nreturn f'Read the following text and answer this question: {q\nprompt_response = openai_client.chat.completions.create(\nLet’s test the functions with the question that made ChatGPT hallucinate:\ntq_result_text = query_vector_database(trick_question)\ntq_prompt = prompt_template(trick_question, tq_result_text)\nhave enough information to answer the question.\nUse the following pieces of retrieved context to answer the question.\ndef prompt_template(question, text):\ntq_result_text = query_vector_database(trick_question)\ntq_prompt = prompt_template(trick_question, tq_result_text)",
      "keywords": [
        "question",
        "prompt",
        "text",
        "chroma",
        "Github",
        "Paestum",
        "LLM",
        "Activate the virtual",
        "openai",
        "vector",
        "results",
        "notebook",
        "vector database",
        "answer",
        "Github repo"
      ],
      "concepts": [
        "chromadb",
        "question",
        "chroma",
        "openai",
        "prompt",
        "client",
        "llm",
        "query",
        "model",
        "answer"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 33,
          "title": "",
          "score": 0.725,
          "base_score": 0.575,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 41,
          "title": "",
          "score": 0.569,
          "base_score": 0.419,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 37,
          "title": "",
          "score": 0.551,
          "base_score": 0.551,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 31,
          "title": "",
          "score": 0.503,
          "base_score": 0.503,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 23,
          "title": "",
          "score": 0.497,
          "base_score": 0.497,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "paestum",
          "question",
          "br",
          "trick_question",
          "paestum br"
        ],
        "semantic": [],
        "merged": [
          "paestum",
          "question",
          "br",
          "trick_question",
          "paestum br"
        ]
      },
      "topic_id": 8,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3458085199828273,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.532183+00:00"
      }
    },
    {
      "chapter_number": 23,
      "title": "Segment 23 (pages 191-199)",
      "start_page": 191,
      "end_page": 199,
      "summary": "results_text = query_vector_database(question)  #A\na basic chatbot that can answer questions based on text imported into the\nBefore re-implementing RAG with LangChain, let’s recap the RAG\ntext (typically an answer) augmented\nsearches, typically a vector store\nText chunk\nA fragment of text from a document.\ntext stores like vector stores\nchunk, text\nVector\nPhase in the RAG design where text is\nIn a vector\nstore, text is broken into chunks and\nText indexing,\ntext\nvectorization\nVector store\nVector\nembeddings of the search question and\nthe text chunks in a vector store\nQ&A, vector\nbe a full document or a list of text\nchunks retrieved from a vector store\nRAG Question\ncontent store (typically a vector store),\nRAG Question-\nYou are now ready to re-implement RAG with LangChain, which you will do\nImplement a minimalistic Q&A chatbot by feeding a question and\nFor answering questions over a knowledge base, use RAG, which\nincludes a vector store, a retriever, and an LLM to synthesize responses.\nThe RAG design pattern has two stages: ingestion (populating the vector\nstore with text and embeddings) and Q&A (retrieving relevant\nretrieving text through related embeddings.\nthe LLM (such as the OpenAI API) and the vector store (such as the\n7 Q&A chatbots with LangChain\nImplementing RAG with LangChain\nAlternative implementation using LangChain Q&A specialized\ndocuments, the vector store, and the LLM.\nalternative chatbot implementations using LangChain’s specialized Q&A\nLangChain classes that support the Q&A chatbot use case.\n7.1 LangChain object model for Q&A chatbots\nsource text, retrieving relevant context from a vector store, generating\nDocuments (and text\nembeddings, vector\nDocument: Models the text content and related metadata.\nBaseLoader: Loads text from external sources into the document model.\nVector Store: Stores text chunks and related embeddings for efficient\nEmbedding Model: Converts text into embeddings (vector\nLoader to import text, which is parsed into a Document.\nand both the chunks and embeddings are stored in the vector store.\n2. The Loader parses the text and converts it into a Document object.",
      "keywords": [
        "vector store",
        "text",
        "RAG",
        "vector",
        "store",
        "document",
        "Embeddings",
        "LangChain",
        "content ingestion",
        "chunks",
        "question",
        "RAG design",
        "text chunks",
        "content",
        "llm"
      ],
      "concepts": [
        "text",
        "rag",
        "document",
        "documents",
        "vectorization",
        "question",
        "questions",
        "langchain",
        "stored",
        "stores"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 27,
          "title": "",
          "score": 0.895,
          "base_score": 0.745,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 3,
          "title": "",
          "score": 0.827,
          "base_score": 0.677,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 24,
          "title": "",
          "score": 0.819,
          "base_score": 0.669,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 21,
          "title": "",
          "score": 0.788,
          "base_score": 0.638,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 2,
          "title": "",
          "score": 0.734,
          "base_score": 0.584,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "text",
          "vector",
          "vector store",
          "store",
          "rag"
        ],
        "semantic": [],
        "merged": [
          "text",
          "vector",
          "vector store",
          "store",
          "rag"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.39882793980075665,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.532244+00:00"
      }
    },
    {
      "chapter_number": 24,
      "title": "Segment 24 (pages 200-208)",
      "start_page": 200,
      "end_page": 208,
      "summary": "Figure 7.3 Object model associated with the retrieval and generation stage.\nembeddings, vector\nchunks) and the related embedding models «abstract»\nDocuments into their\nVectorStore: Stores and retrieves relevant text chunks.\nRetriever: Retrieves relevant text chunks from the Vector Store based\non the similarity between the query's embedding and the stored text\nqueries and documents.\nvector store Retriever, which generates an embedding using an Embeddings model.\nRetriever searches for similar embeddings in the vector store and returns relevant documents.\nThe Q&A orchestrator combines the documents and the question into a prompt with a\n1. The Q&A orchestrator sends the user's question to the vector store\n3. The Retriever searches the vector store for documents with similar\n4. The Q&A orchestrator combines the retrieved documents and the user’s\nBefore querying your documents, you must store them in a vector database.\nFirst, install the required packages for loading, splitting documents, and\nfrom langchain_community.document_loaders import WikipediaLoader, \n7.2.1 Splitting and Storing the Documents\n1. Split each document into chunks of about 500 characters.\n2. calculate the embeddings of the document chunks and store them in the\nInstantiate the splitter, embeddings model, and vector database client as\nvector_db = Chroma(\"tourist_info\", embeddings_model) \nProcess each document by loading it, splitting it into chunks, and storing the\nchunks with the related embeddings into the vector database:\nwikipedia_chunks = text_splitter.split_documents(wikipedia_loader\nvector_db.add_documents(wikipedia_chunks)\nOnce the content has been split and stored in the vector database, you will see\nword_chunks = text_splitter.split_documents(word_loader.load())\nvector_db.add_documents(word_chunks)\npdf_chunks = text_splitter.split_documents(pdf_loader.load())\nvector_db.add_documents(pdf_chunks)\ntxt_chunks = text_splitter.split_documents(txt_loader.load())\nvector_db.add_documents(txt_chunks)\ndef split_and_import(loader):\nchunks = text_splitter.split_documents(loader.load())\nvector_db.add_documents(chunks)\nprint(f\"Ingested chunks created by {loader}\")\nsplit_and_import(wikipedia_loader)\nsplit_and_import(word_loader)\nsplit_and_import(pdf_loader)\nsplit_and_import(txt_loader)\nIngested chunks created by <langchain_community.document_loaders.w\nWord, PDF, txt) using a specialized document loader for each type.\nYou can ingest all the files located in a folder into a vector store by iterating\nand load them into the vector store using get_loader() and\nQA_across_documents.ipynb on GitHub. Ingesting all files with DirectoryLoader\nAn alternative method for loading all files in a folder into the vector store is",
      "keywords": [
        "Documents",
        "vector store",
        "vector",
        "loader",
        "embeddings",
        "chunks",
        "Embeddings model",
        "Paestum",
        "model",
        "store",
        "text",
        "object",
        "vector database",
        "vector store Retriever",
        "folder"
      ],
      "concepts": [
        "documents",
        "document",
        "embeddings",
        "models",
        "loader",
        "ingestion",
        "ingested",
        "ingest",
        "retrieval",
        "retriever"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 21,
          "title": "",
          "score": 0.924,
          "base_score": 0.774,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 2,
          "title": "",
          "score": 0.861,
          "base_score": 0.711,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 27,
          "title": "",
          "score": 0.843,
          "base_score": 0.693,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 23,
          "title": "",
          "score": 0.819,
          "base_score": 0.669,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 36,
          "title": "",
          "score": 0.811,
          "base_score": 0.661,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "vector",
          "vector_db",
          "chunks",
          "store",
          "split_and_import"
        ],
        "semantic": [],
        "merged": [
          "vector",
          "vector_db",
          "chunks",
          "store",
          "split_and_import"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.370816778317088,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.532298+00:00"
      }
    },
    {
      "chapter_number": 25,
      "title": "Segment 25 (pages 209-219)",
      "start_page": 209,
      "end_page": 219,
      "summary": "Let's query the vector store to see what documents are retrieved for a question\n7.3.2 Asking a Question through a LangChain Chain\nFigure 7.5 LangChain RAG chain, LangChain RAG chain, packaging the prompt parameters\nrag_prompt_template = \"\"\"Use the following pieces of context to a\nChat Model: Processes the prompt to generate the answer.\nInstantiate the retriever, question feeder, and chat model:\nrag_chain = {\"context\": retriever, \"question\": question_feeder} | \nanswer = chain.invoke(question)\nanswer = execute_chain(rag_chain, question)\nanswer = execute_chain(rag_chain, question)\nquestions and answers.\n7.4 Chatbot memory of message history\nLet's see how to incorporate message history into the RAG setup we finalized\nListing 7.3 Initial RAG Setup Before Incorporating Message History\nrag_prompt_template = \"\"\"Use the following pieces of context to a\nrag_chain = {\"context\": retriever, \"question\": question_feeder} | \nanswer = rag_chain.invoke(question)\nFirst, we should amend the prompt to include message history.\nThe original RAG prompt doesn't account for message history, so we need to\nSince message history is a core feature of the memory-enabled\na prompt from a list of chat messages.\nChat Messages\nassociated with chat messages.\nTable 7.1 Chat Message Roles\nA chat history is a list of such messages:\nchat_history = [\nNow that you understand how to model chat messages, you can create a\nmessage-based prompt:\nrag_prompt = ChatPromptTemplate.from_messages(\n(\"chat_history\", \"{chat_history_messages}\"),\nchat_history, you will feed the new {question}, the newly\n{retrieved_context}, and the accumulated {chat_history_memory}.\nIn the next section, I will show you how to update the message history at\n7.4.2 Updating the Chat Message History\nLangChain provides the ChatMessageHistory class to model chat message\nfrom langchain_community.chat_message_histories import ChatMessag\nchat_history_memory = ChatMessageHistory()\nYou can add messages for Human (user question) and AI (LLM response) to\nthe chat history using the ChatMessageHistory convenience methods listed\nadd_user_message(user_question)\nto the chat message history, as they are already part of the prompt and won't\nUpdate the chat history in the execute_chain() function as follows:\nchat_history_memory.add_user_message(question)\nanswer = chain.invoke(question)\nchat_history_memory.add_ai_message(answer)\nprint(f'Full chat message history: {chat_history_memory.messa\nWhen the chat_history_memory object is updated with the latest Human or\nAI messages, you can retrieve the entire message history using the messages\nfull_message_history = chat_history_memory.messages\n7.4.3 Feeding the Chat History to the RAG Chain\nAfter updating your code to include message history in the prompt and\nupdated message history to the RAG chain.\n\"chat_history_messages\": chat_history_memory.messages\nIn this setup, the chat_history_messages prompt parameter is fed through\nthe corresponding chat_history_messages property above.\nListing 7.4 RAG Chain with Chatbot Memory\nfrom langchain_community.chat_message_histories import ChatMessag\nrag_prompt = ChatPromptTemplate.from_messages(\n(\"placeholder\", \"{chat_history_messages}\"),\nchat_history_memory = ChatMessageHistory()\nreturn chat_history_memory.messages",
      "keywords": [
        "RAG chain",
        "RAG",
        "question",
        "history",
        "message history",
        "chat",
        "Chat Message History",
        "Chain",
        "message",
        "prompt",
        "LangChain RAG chain",
        "LangChain",
        "chat message",
        "chat history",
        "context"
      ],
      "concepts": [
        "messages",
        "question",
        "questions",
        "answer",
        "prompt",
        "langchain",
        "chain",
        "context",
        "retrieved",
        "retrieves"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 40,
          "title": "",
          "score": 0.697,
          "base_score": 0.697,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 23,
          "title": "",
          "score": 0.68,
          "base_score": 0.68,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 50,
          "title": "",
          "score": 0.623,
          "base_score": 0.623,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 35,
          "title": "",
          "score": 0.587,
          "base_score": 0.587,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 34,
          "title": "",
          "score": 0.577,
          "base_score": 0.577,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "history",
          "message history",
          "message",
          "chat",
          "chat_history_memory"
        ],
        "semantic": [],
        "merged": [
          "history",
          "message history",
          "message",
          "chat",
          "chat_history_memory"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4111440042966962,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:00:35.532358+00:00"
      }
    },
    {
      "chapter_number": 26,
      "title": "Segment 26 (pages 220-229)",
      "start_page": 220,
      "end_page": 229,
      "summary": "answer = execute_chain_with_memory(rag_chain, question)\nanswer = execute_chain_with_memory(rag_chain, question)\nimportant topic: tracing its chain execution with LangSmith.\n7.5 Tracing Execution with LangSmith\nDevelopment and Debugging: LangSmith's tracing feature ensures your\nTo enable LangSmith's tracing, you just need to set up the LangSmith API\nkey and some tracing configurations at the top of your application.\ndetails on LangSmith and how to set up the API key, refer to the sidebar\nSidebar: Setting up LangSmith’s API Key\nSet up a LangSmith API key as follows:\nWith the LangSmith API key, you’ll be able to use it in your projects.\nThe easiest way to enable tracing through LangSmith is by setting a few\ntracing through LangSmith (I assume you are in C:\\Github\\building-llm-\n(env_ch07) C:\\...\\ch07>set LANGSMITH_TRACING=true\n(env_ch07) C:\\...\\ch07>set LANGSMITH_PROJECT=Q & A chatbot\n(env_ch07) C:\\...\\ch07>set LANGSMITH_API_KEY=<YOUR_LANGSMITH_API_\nOnce you’ve completed these steps, LangSmith will have captured tracing\ntrace data directly in the LangSmith dashboard.\n7.5.1 Inspecting the LangSmith Traces\nFigure 7.7 LangSmith high-level trace: click the latest trace to get high-level details of the chain\nThe current view shows all the traces associated with your Q&A Chatbot\nWhen you click any trace,\nFigure 7.8 LangSmith trace details obtained by clicking one of the traces, for example, the latest\nYou will see three panels: 1) The left panel shows all traces of the Chatbot Q&A\nproject; 2) the middle panel shows the runs of the selected trace.\nFigure 7.9 Middle and right-hand panels you get when clicking the latest trace; 1) the middle\npanel shows the runs of the selected trace; 2) the right panel shows the input to the selected trace\nwhen clicking the latest trace on the Q&A Chatbot project webpage:\nLeft Panel: Shows the list of all traces associated with your project\ntrace, which is the top one.\nRight Panel: Shows the trace input (the user question) and its output (the\nFor instance, clicking the Retriever run in the middle panel shows its details\nThis is just a glimpse of LangSmith's tracing capabilities using a simple",
      "keywords": [
        "LangSmith API key",
        "LangSmith",
        "API key",
        "chat message history",
        "API",
        "Create API Key",
        "LangSmith API",
        "panel",
        "Api Keys",
        "trace",
        "key",
        "middle panel",
        "LangSmith tracing",
        "chatbot",
        "panel shows"
      ],
      "concepts": [
        "tracing",
        "trace",
        "question",
        "questions",
        "setting",
        "applications",
        "keys",
        "answers",
        "panels",
        "chain"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 25,
          "title": "",
          "score": 0.527,
          "base_score": 0.527,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 45,
          "title": "",
          "score": 0.508,
          "base_score": 0.508,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 23,
          "title": "",
          "score": 0.496,
          "base_score": 0.496,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 49,
          "title": "",
          "score": 0.458,
          "base_score": 0.458,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 6,
          "title": "",
          "score": 0.453,
          "base_score": 0.453,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "langsmith",
          "trace",
          "tracing",
          "panel",
          "panel shows"
        ],
        "semantic": [],
        "merged": [
          "langsmith",
          "trace",
          "tracing",
          "panel",
          "panel shows"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30962203210628697,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:00:35.532421+00:00"
      }
    },
    {
      "chapter_number": 27,
      "title": "Segment 27 (pages 230-240)",
      "start_page": 230,
      "end_page": 240,
      "summary": "answer = execute_chain(rag_chain, question)\nsearch_type: Specifies the search algorithm to retrieve from the vector\nBy default, the retriever is configured for a plain Similarity Search (with the\nEmbedding Models: Index Document objects with their\nembeddings (vector representations) when storing them in the\nThe Q&A or retrieval stage of the RAG solution is supported by\nBaseRetriever: Abstracts the retrieval process from the vector store.\nBaseLanguageModel: Abstracts clients to LLMs. A typical RAG chain includes a passthrough component, a vector store\nthe RAG chain for each interaction between user and LLM.\nAdvanced RAG techniques for more effective retrieval\nSelecting the optimal chunk splitting strategy for your use case\nUsing multiple embeddings to enhance coarse chunk retrieval\nExpanding granular chunks to add context during retrieval\nIn chapter 7, you learned about the basics of the RAG (Retrieval-Augmented\nwhen your content store (usually a vector store) has relevant data.\nembeddings for larger text chunks stored in the vector database.\nTo improve RAG (Retrieval-Augmented Generation) accuracy, analyze each\nRetrieval accuracy can be improved through an optimized content ingestion\nembeddings are linked to related text chunks in the vector store.\nFigure 8.1 Common accuracy issues in the ingestion stage of a simple RAG architecture are often\ndue to inadequate indexing that only uses basic embeddings for each text chunk.\nindexing techniques involve generating multiple embeddings for each chunk, enhancing\nthe embeddings of the document chunks\n3, store documents chunks and related\nembeddings into Vector store\nEven if a question is clear, retrieval can fail with overly simple indexing\nIn vector indexing, chunk size and overlap length are crucial:\nsmaller chunks may work well for precise questions but fail with broader\nqueries, while larger chunks may lack detail for specific questions.\n8.1.2 Question Answering Stage\nHandle poorly formulated questions with question transformation; 2) Enhance retrieval accuracy\nby transforming the original user questions into more suitable vector DB search queries; 3)\nInclude relevant data sources by adding structured data content stores, such as relational DBs; 4)\nGenerate DB queries for structured data content stores; 5) Filter out irrelevant context retrieved\nfrom the content stores.\nThe context retrieved from the\n1. User question and context (Search results)\nanswer the question.\nqueries from the original user question.\n2) Query generation for retrieval Vector DB\nstore may return weak context, leading to poor results.\nIneffective question for retrieval: Using the original question for both\nretrieval and generation can fail, especially when the query is broad or\nBroad questions may not pinpoint relevant content, resulting in\nquestions into specific sub-questions to retrieve more precise\nLimited data relevance in the content store: Most RAG systems rely\nonly on vector stores, but adding structured data sources, like relational\nto the appropriate content store based on the type of data needed to\nIrrelevant search results fed to the LLM: Even with clearer questions\nindexing, question transformations, and multi-store routing—into an\nchunks\nIneffective question for retrieval\nContent store query generation\nFor an LLM to generate high-quality answers, the relevance and accuracy of\nthe text chunks retrieved from the vector store (or any document store) are\nSplitting Strategy: The granularity of document chunks impacts retrieval\nSmaller chunks yield more precise results for specific queries\nEmbedding Strategy: How you index each chunk is equally important.\nIndexing Structured and Semi-Structured Data: Retrieving structured\nDuring the ingestion phase of RAG, documents are split into chunks before\nbeing stored in a vector database or document store.\nEach chunk is indexed\nVector similarity searches rely on the embeddings index, while\nmetadata searches use the keyword index.\nThe easiest way to improve the relevance of document chunk retrieval is to\ndocument store should return all relevant chunks that provide the LLM with\nFigure 8.3 Impact of Chunk Size on Answer Accuracy.\nGranular chunks provide less context but",
      "keywords": [
        "rag",
        "question",
        "search",
        "vector store",
        "vector",
        "content",
        "chunks",
        "store",
        "Document",
        "chain",
        "retrieval",
        "content stores",
        "llm",
        "embeddings",
        "context"
      ],
      "concepts": [
        "question",
        "questions",
        "retriever",
        "retrieve",
        "retrieval",
        "chunks",
        "query",
        "queries",
        "index",
        "indexes"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 23,
          "title": "",
          "score": 0.895,
          "base_score": 0.745,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 24,
          "title": "",
          "score": 0.843,
          "base_score": 0.693,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 36,
          "title": "",
          "score": 0.825,
          "base_score": 0.675,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 2,
          "title": "",
          "score": 0.812,
          "base_score": 0.662,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 21,
          "title": "",
          "score": 0.807,
          "base_score": 0.657,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "retrieval",
          "store",
          "vector",
          "chunks",
          "embeddings"
        ],
        "semantic": [],
        "merged": [
          "retrieval",
          "store",
          "vector",
          "chunks",
          "embeddings"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3775724926531805,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.532476+00:00"
      }
    },
    {
      "chapter_number": 28,
      "title": "Segment 28 (pages 241-248)",
      "start_page": 241,
      "end_page": 248,
      "summary": "chunks.\nFinding the right balance between granular and coarse chunks depends on\nsplit the document.\nHowever, chunk sizes can vary\n2. Splitting by Absolute Size: You can define chunk size by characters,\nThis results in more consistent chunk sizes,\nbut context might be lost if chunks split mid-sentence.\nbe refined depending on the chunk granularity.\nTable 8.2 Splitting Strategies, Pros and Cons, and Related LangChain Classes\nChunk size\nchunk size\nchunk size\nchunks may\nNow, create a Chroma DB collection to store the more granular chunks:\ncornwall_granular_collection = Chroma(  #A\ncollection_name=\"cornwall_granular\",\ncornwall_granular_collection.\nNext, set up a second collection for coarser chunks:\nboth granular and coarse chunks.\nSplitting Content into Granular Chunks Using HTMLSectionSplitter\nwill generate more granular chunks, as shown in listing 8.1.\ndef split_docs_into_granular_chunks(docs):\nall_chunks = []\ntemp_chunks = html_section_splitter.split_text(html_strin\nYou can now generate the granular chunks:\ngranular_chunks = split_docs_into_granular_chunks(docs)\nNow, insert the granular chunks into the Chroma collection:\ncornwall_granular_collection.add_documents(documents=granular_chu\nSearching Granular Chunks\nYou can now run a search for specific content within the granular chunks:\nresults = cornwall_granular_collection.similarity_search(query=\"E\nSplitting Content into Coarse Chunks Using\nNext, define a function to split the content into coarse chunks:\ndef split_docs_into_coarse_chunks(docs):\ncoarse_chunks = text_splitter.split_documents(text_docs)  #B\nreturn coarse_chunks\nThen generate the coarse chunks:\ncoarse_chunks = split_docs_into_coarse_chunks(docs)\nInsert these chunks into the corresponding Chroma collection:\ncornwall_coarse_collection.add_documents(documents=coarse_chunks)\nSearching Coarse Chunks\nYou can now search for more general content within the coarse chunks:\ncollections for various UK destinations and ingest the related content chunks.\nuk_granular_collection = Chroma(  #A\ncollection_name=\"uk_granular\",\ngranular_chunks = split_docs_into_granular_chunks(docs)\nuk_granular_collection.add_documents(documents=granular_chunk\ncoarse_chunks = split_docs_into_coarse_chunks(docs)\nuk_coarse_collection.add_documents(documents=coarse_chunks)\ngranular_results = uk_granular_collection.similarity_search(query\nsee how the results vary between granular and coarse chunks.",
      "keywords": [
        "chunks",
        "granular",
        "coarse",
        "granular chunks",
        "content",
        "coarse chunks",
        "docs",
        "cornwall",
        "collection",
        "size",
        "chunk size",
        "chroma",
        "HTML",
        "Splitting",
        "API"
      ],
      "concepts": [
        "docs",
        "doc",
        "split",
        "sizes",
        "html",
        "collections",
        "collection",
        "document",
        "documents",
        "content"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 31,
          "title": "",
          "score": 0.929,
          "base_score": 0.779,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 29,
          "title": "",
          "score": 0.864,
          "base_score": 0.714,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 30,
          "title": "",
          "score": 0.787,
          "base_score": 0.637,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 32,
          "title": "",
          "score": 0.74,
          "base_score": 0.59,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 11,
          "title": "",
          "score": 0.674,
          "base_score": 0.524,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "chunks",
          "granular",
          "coarse",
          "coarse chunks",
          "granular chunks"
        ],
        "semantic": [],
        "merged": [
          "chunks",
          "granular",
          "coarse",
          "coarse chunks",
          "granular chunks"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.28102194346549614,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.532531+00:00"
      }
    },
    {
      "chapter_number": 29,
      "title": "Segment 29 (pages 249-256)",
      "start_page": 249,
      "end_page": 256,
      "summary": "precise embeddings for retrieving the synthesis chunks.\n8.4.1 Embedding Child Chunks with ParentDocumentRetriever\nTo solve this, split the document into larger parent chunks and create smaller\nchild chunks within each parent.\nUse the child chunks solely for generating\nmore granular embeddings, which are then stored against the parent chunk.\nFigure 8.4 Child Chunk embeddings—A coarse document chunk is indexed with its own\nembedding and the embeddings generated from its smaller child chunks.\nCoarse chunks\nCoarse chunk\nChild chunk #1 embedding\nCoarse chunk embedding Child chunk #4\nChild chunk #2 embedding\nx Child chunk #3 embedding\nChild chunk #3\nthe embedding of a child chunk of\na coarse chunk\nchunk\nchunk is retrieved, providing rich context.\nsynthesis, then further dividing each into smaller child chunks for retrieval.\nListing 8.3 Setting Up Parent and Child Splitters for Coarse and Granular Chunks\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=500)  \ncollection_name=\"uk_child_chunks\",\nvectorstore=child_chunks_collection,\nNow, generate both coarse and granular chunks using the configured splitters\nNow perform a search on the child chunks using the\nNow, compare the results by directly searching only the child chunks:\nchild_docs_only = child_chunks_collection.similarity_search(\"Cornw\n8.4.2 Embedding Child Chunks with MultiVectorRetriever\nAn alternative method for embedding child chunks and linking them to the\nlarger parent chunks used in synthesis is to use the MultiVectorRetriever.\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=500)  \ncollection_name=\"uk_child_chunks\",\nvectorstore=child_chunks_collection,\ncoarse_chunks = parent_splitter.split_documents(text_docs)  #\ngranular_chunks = child_splitter.split_documents([coarse_\ngranular_chunk.metadata[doc_key] = coarse_chunk_id  #\nmulti_vector_retriever.docstore.mset(list(zip(coarse_chunks_i\nchild_docs_only = child_chunks_collection.similarity_search(\"Cornw\nThe first document retrieved from the child collection (child_docs_only[0])",
      "keywords": [
        "Child",
        "chunks",
        "Child Chunks",
        "Embedding Child Chunks",
        "coarse",
        "document",
        "parent",
        "Embedding Child",
        "Embedding",
        "smaller child chunks",
        "doc",
        "docs",
        "Coarse chunk",
        "Coarse chunk Child",
        "Child Splitters"
      ],
      "concepts": [
        "chunk",
        "embedding",
        "importing",
        "retrieval",
        "retrieving",
        "store",
        "stored",
        "listing",
        "document",
        "documents"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 31,
          "title": "",
          "score": 0.956,
          "base_score": 0.806,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 28,
          "title": "",
          "score": 0.864,
          "base_score": 0.714,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 30,
          "title": "",
          "score": 0.845,
          "base_score": 0.695,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 32,
          "title": "",
          "score": 0.821,
          "base_score": 0.671,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 24,
          "title": "",
          "score": 0.614,
          "base_score": 0.614,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "child",
          "child chunks",
          "chunks",
          "embedding child",
          "chunk"
        ],
        "semantic": [],
        "merged": [
          "child",
          "child chunks",
          "chunks",
          "embedding child",
          "chunk"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.32443215843018075,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.532580+00:00"
      }
    },
    {
      "chapter_number": 30,
      "title": "Segment 30 (pages 257-264)",
      "start_page": 257,
      "end_page": 264,
      "summary": "8.4.3 Embedding Document Summaries\nEmbeddings from coarse chunks are often ineffective because they capture\nTo address this, you can create a summary of the coarse chunk and generate\nThese summary embeddings are then stored alongside\nFigure 8.5 Chunk Summary Embedding—A coarse chunk is indexed with its own embedding and\nan additional embedding from its summary.\nCoarse chunk summary\nCoarse chunk embedding\nSummary embedding\nthe embedding of the summary\nfrom langchain.retrievers.multi_vector import MultiVectorRetrieve\nFirst, create a collection to store the summaries and set up the document store\nListing 8.6 Injecting summary collection and document store into MultiVectorRetriever\nUse an LLM to generate summaries of the coarse chunks.\nIngesting Coarse Chunks and Summaries into Stores\nNext, load the content, split it into coarse chunks, generate summaries, and\nListing 8.7 Ingesting Coarse Chunks and Their Summaries\ncoarse_chunks = parent_splitter.split_documents(text_docs)  #\nsummary_text =  summarization_chain.invoke(coarse_chunk)  \nsummary_doc = Document(page_content=summary_text, metadat\nmulti_vector_retriever.vectorstore.add_documents(all_summarie\nmulti_vector_retriever.docstore.mset(list(zip(coarse_chunks_i\nchunk similar to those retrieved when using child embeddings.\nsummaries or child chunks retrieves focused but context-limited information,\n8.4.4 Embedding Hypothetical Questions\nTo address this, you can generate hypothetical questions that each chunk is\nlikely to answer, and then store the chunk using embeddings derived from\nFigure 8.6 Hypothetical Question embeddings—A document chunk is indexed with its own\nembedding and additional embeddings generated from hypothetical questions that it can answer.\nHypothetical question #1 embedding\nHypothetical question #2 embedding\nCoarse chunk embedding\nA tion is likely to hit of the the chunk the embedding of an hypothetical question\nMultiVectorRetriever with hypothetical question embeddings:\nfrom langchain.retrievers.multi_vector import MultiVectorRetrieve\nsummary embeddings, but this time use a vector store specifically for storing\nListing 8.8 Configuring MultiVectorRetriever with Hypothetical Question Collection",
      "keywords": [
        "coarse chunk",
        "coarse",
        "chunk",
        "Coarse chunk embedding",
        "question",
        "Hypothetical Question",
        "Embedding",
        "Hypothetical",
        "summary",
        "Summaries",
        "langchain",
        "Embedding Hypothetical Questions",
        "Hypothetical Question embeddings",
        "Ingesting Coarse Chunks"
      ],
      "concepts": [
        "embedding",
        "chunk",
        "document",
        "documents",
        "retrieval",
        "retrieved",
        "retrieves",
        "summaries",
        "summary",
        "questions"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 31,
          "title": "",
          "score": 0.893,
          "base_score": 0.743,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 32,
          "title": "",
          "score": 0.856,
          "base_score": 0.706,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 29,
          "title": "",
          "score": 0.845,
          "base_score": 0.695,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 11,
          "title": "",
          "score": 0.795,
          "base_score": 0.645,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 28,
          "title": "",
          "score": 0.787,
          "base_score": 0.637,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "coarse",
          "hypothetical",
          "embedding",
          "chunk",
          "hypothetical question"
        ],
        "semantic": [],
        "merged": [
          "coarse",
          "hypothetical",
          "embedding",
          "chunk",
          "hypothetical question"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3426653799327705,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.532632+00:00"
      }
    },
    {
      "chapter_number": 31,
      "title": "Segment 31 (pages 265-272)",
      "start_page": 265,
      "end_page": 272,
      "summary": "Create a chain to generate hypothetical questions for each document chunk.\nIngesting Coarse Chunks and Related Hypothetical Questions\nNow, generate coarse chunks, create the hypothetical questions for each, and\nListing 8.10 Ingesting Coarse Chunks and Related Hypothetical Questions\ncoarse_chunks = parent_splitter.split_documents(text_docs)  #\n8.5 Granular Chunk Expansion\nGranular chunks\nThe granular chunk associated with\nGranular chunks the embedding hit by the question\nA detailed question its the subsequent granular chunk\nembedding of a granular chunk\ngranular chunks and an in-memory document store for the expanded chunks,\nListing 8.11 Setting up a multi vector retriever for granular chunk expansion\ngranular_chunks_collection = Chroma(  #B\ncollection_name=\"uk_granular_chunks\",\nvectorstore=granular_chunks_collection,\nIngesting Granular and Expanded Chunks\nNow, generate expanded chunks by including the content from adjacent\nListing 8.12 Generating and Storing Expanded Chunks\ngranular_chunks = granular_chunk_splitter.split_documents(tex\nexpanded_chunk_store_items = []\nexpanded_chunk_text = \"\"  #G\nexpanded_chunk_text += granular_chunks[previous_chunk_\nexpanded_chunk_text += \"\\n\"\nexpanded_chunk_text += granular_chunks[this_chunk_num].pa\nexpanded_chunk_text += \"\\n\"\nexpanded_chunk_text += granular_chunks[next_chunk_num\nexpanded_chunk_text += \"\\n\"\nexpanded_chunk_doc = Document(page_content=expanded_chunk_\ngranular_chunk.metadata[doc_key] = expanded_chunk_id  #J\nmulti_vector_retriever.docstore.mset(expanded_chunk_store_ite\nwhich now uses expanded chunks for a more complete context:\nThe first document retrieved will include content from surrounding chunks,\nFor comparison, run a search directly on the granular chunks without\nchild_docs_only = granular_chunks_collection.similarity_search(\"C\nStore the coarse text chunks and the full tables in a document store and place",
      "keywords": [
        "hypothetical questions",
        "chunk",
        "Hypothetical",
        "Granular",
        "questions",
        "Related Hypothetical Questions",
        "expanded",
        "Question Generation Chain",
        "generate hypothetical questions",
        "Granular Chunk",
        "hypothetical question embeddings",
        "document",
        "Hypothetical Question Generation",
        "Text",
        "Coarse"
      ],
      "concepts": [
        "chunk",
        "document",
        "documents",
        "embeddings",
        "granular",
        "context",
        "structured",
        "generation",
        "generate",
        "generated"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 29,
          "title": "",
          "score": 0.956,
          "base_score": 0.806,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 28,
          "title": "",
          "score": 0.929,
          "base_score": 0.779,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 30,
          "title": "",
          "score": 0.893,
          "base_score": 0.743,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 32,
          "title": "",
          "score": 0.818,
          "base_score": 0.668,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 35,
          "title": "",
          "score": 0.666,
          "base_score": 0.666,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "granular",
          "hypothetical",
          "expanded_chunk_text",
          "chunks",
          "granular chunk"
        ],
        "semantic": [],
        "merged": [
          "granular",
          "hypothetical",
          "expanded_chunk_text",
          "chunks",
          "granular chunk"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.35318820427035896,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.532691+00:00"
      }
    },
    {
      "chapter_number": 32,
      "title": "Segment 32 (pages 273-281)",
      "start_page": 273,
      "end_page": 281,
      "summary": "Coarse chunk embedding\nText chunk summary\nTable chunk summary\npreparation stage, you can use a multi-modal LLM to generate a summary of\nFigure 8.9 Multi-Modal RAG Workflow—1) Data Ingestion: Use a multi-modal LLM to generate\nan image summary, store the summary embeddings in a vector store, and keep the raw image in\n2) Retrieval: If the summary embeddings match a query, the raw image is\nDuring retrieval, if the summary’s embeddings match a user query, the\nspecific queries, retrieval post-processing (e.g., result reranking), and\nmultiple embeddings for coarse chunks, and expanded context for\nFor coarse chunks, enhance retrieval accuracy by using multiple\nembeddings generated from child chunks, summaries, or hypothetical\nquestions, or child chunks.\nFor granular chunks, expand retrieval context by including adjacent\nFor semi-structured content (e.g., text with tables) or multi-modal\nRewrite user questions with \"Rewrite-Retrieve-Read\" for better\nGenerate hypothetical documents to align questions with embeddings.\ndocuments, splitting them into chunks, and generating embeddings for\nrelevant content in the vector store, but from issues in the user's question\nvector store and the LLM, leading to weaker retrieval results.\nthe question.\nFigure 9.1 In the standard Retrieve-and-Read setup, the retriever processes the user question\nan initial Rewrite step uses an LLM to rephrase the query before it reaches the retriever,\noriginal question directly and sends results to the LLM for synthesis.\nquestion before passing it to the retriever.\nstore, keeping the original question in the RAG prompt.\nthe vector database, while preserving the original question for the LLM to\nFigure 9.2 Using query rewriting to generate an optimized vector store query, while preserving",
      "keywords": [
        "LLM",
        "question",
        "summary",
        "Summary embedding Table",
        "Table chunk summary",
        "multi-modal LLM",
        "table chunk Broad",
        "RAG",
        "chunks",
        "query",
        "embedding Table coarse",
        "embeddings",
        "Multi-Modal RAG",
        "Table coarse chunks"
      ],
      "concepts": [
        "retrieval",
        "retrieve",
        "retriever",
        "question",
        "questions",
        "chunks",
        "query",
        "queries",
        "rag",
        "likely"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 30,
          "title": "",
          "score": 0.856,
          "base_score": 0.706,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 29,
          "title": "",
          "score": 0.821,
          "base_score": 0.671,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 31,
          "title": "",
          "score": 0.818,
          "base_score": 0.668,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 28,
          "title": "",
          "score": 0.74,
          "base_score": 0.59,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 27,
          "title": "",
          "score": 0.677,
          "base_score": 0.677,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "multi modal",
          "modal",
          "summary",
          "embeddings",
          "chunks"
        ],
        "semantic": [],
        "merged": [
          "multi modal",
          "modal",
          "summary",
          "embeddings",
          "chunks"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3434908656714248,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.532744+00:00"
      }
    },
    {
      "chapter_number": 33,
      "title": "Segment 33 (pages 282-289)",
      "start_page": 282,
      "end_page": 289,
      "summary": "The user's question is also included in the\nuser question\nThe user's question is\nthe user question into the chunks\nquestion and the retrieved\nthe user question prompt and synthetizes a\nA prompt for rewriting the query can be as simple as the one below, adapted\nOriginal question: {user_question}  \nrewrite the original user question, open a new OS shell, navigate to the\nThis setup prepares your data for effective query rewriting and retrieval using\nsearch queries, improving retrieval results and the overall quality of the\n9.1.1 Retrieving Content Using the Original User Question\nLet’s start by performing a search with the original user question:\nuser_question = \"Tell me some fun things I can enjoy in Cornwall\"\ninitial_results = uk_granular_collection.similarity_search(query=\nThe  Camel Trail  is an  18-mile (29   km)  [REDUCED…]\nThe  Cornish Film Festival  is held annually [REDUCED…]\nThe  Cornish Film Festival  is held annually [REDUCED…]  \n9.1.2 Setting Up the Query Rewriter Chain\nTo refine the user question into a more effective query for Chroma DB, we’ll\nSetting up a query rewriter chain will help us automate this transformation.\nNext, define the prompt that instructs the LLM to rewrite the user question:\nGenerate search query for the Chroma DB vector store from a user \nUser question: {user_question}\nrewriter_chain = rewriter_prompt | llm | StrOutputParser()\nThis setup allows you to pass a user question to the rewriter chain, which\n9.1.3 Retrieving Content with the Rewritten Query\nNow, let’s use the rewriter chain to create a more targeted query and see if it\nreturns more accurate results compared to the original question.\nuser_question =\"Tell me some fun things I can do in Cornwall\"\nsearch_query = rewriter_chain.invoke({\"user_question\": user_quest\nimproved_results = uk_granular_collection.similarity_search(query\nThe  Cornish Film Festival  is held annually each November aroun\n' Obby 'Oss  is held annually on May Day (1 May), [REDUCED…]\nThe  Cornish Film Festival  is held annually each November aroun\nUsing the rewritten question, the vector store retriever provides a\nquestion.\nquestion into a search query for vector retrieval.\nThe original question is\nRAG chain, including the query rewriting step.\nListing 9.2 Combined RAG Chain with Query Rewriting\nrewrite_retrieve_read_rag_chain = (\n\"context\": {\"user_question\": RunnablePassthrough()} | rew\nuser_question = \"Tell me some fun things I can do in Cornwall\"\nanswer = rewrite_retrieve_read_rag_chain.invoke(user_question)\n4. Attending the Cornish Film Festival held annually in November \nThe “Rewrite-Retrieve-Read” approach assumes the original user question\nimplicit questions, rewriting it as a single improved query may not be\nFigure 9.3 Workflow for Multiple Query Generation: The LLM application reformulates the\noriginal question into multiple explicit questions, which are executed against the vector store to\nproduce the context, and synthesizes the answer from a prompt with the original question and the",
      "keywords": [
        "South West Coast",
        "West Coast Path",
        "user question",
        "question",
        "Cornish Film Festival",
        "original user question",
        "reduced",
        "South West",
        "user",
        "Cornwall",
        "Coast Path runs",
        "West Coast",
        "query",
        "original question",
        "Coast Path"
      ],
      "concepts": [
        "reduced",
        "question",
        "questions",
        "retrieved",
        "retrieval",
        "query",
        "queries",
        "importing",
        "cornwall",
        "chunks"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 22,
          "title": "",
          "score": 0.725,
          "base_score": 0.575,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 34,
          "title": "",
          "score": 0.64,
          "base_score": 0.64,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 41,
          "title": "",
          "score": 0.636,
          "base_score": 0.486,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 35,
          "title": "",
          "score": 0.56,
          "base_score": 0.56,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 36,
          "title": "",
          "score": 0.558,
          "base_score": 0.558,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "question",
          "user question",
          "query",
          "festival",
          "annually"
        ],
        "semantic": [],
        "merged": [
          "question",
          "user question",
          "query",
          "festival",
          "annually"
        ]
      },
      "topic_id": 8,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.28301250007140505,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.532793+00:00"
      }
    },
    {
      "chapter_number": 34,
      "title": "Segment 34 (pages 290-300)",
      "start_page": 290,
      "end_page": 300,
      "summary": "User generate 3 queries from the original question\nIn the figure 9.3, the LLM application reformulates the original question into\nthe original question and the retrieved context.\nYou can automatically generate multiple queries for any user question using a\ndifferent versions of the given user question to retrieve rel\ndifferent versions of the given user question to retrieve relevan\nSince the LLM is generating five alternative questions, it’s useful to format\nmulti_query_gen_chain = multi_query_gen_prompt | llm | questions_\nuser_question = \" Tell me some fun things I can do in Cornwall.\"\nmultiple_queries = multi_query_gen_chain.invoke(user_question)\nWhen you print multiple_queries, you should get a list of questions similar\nretriever=basic_retriever, llm_chain=multi_query_gen_chain, \nuser_question = \"Tell me some fun things I can do in Cornwall\"\nretrieved_docs = multi_query_retriever.invoke(user_question)\nuser_question = \" Tell me some fun things I can do in Cornwall\"\nretrieved_docs = multi_query_retriever.invoke(user_question)\n9.3 Step-Back Question\nIn this approach, you start with the user’s detailed question but then create a\nbroader question to retrieve a more generalized context.\nFigure 9.4 Step-Back Question Workflow: The LLM application first sends the detailed question\nThe LLM application sends the original detailed question to the vector store\nto retrieve detailed context, then generates and executes a broader question to\nIt combines both contexts with the original question\nquestion:\nGenerate a less specific question (aka Step-back question) for th\nStep-back question:\nFor example, if you input the prompt with the detailed question, “Can you\nThis broader question helps retrieve more general information, which,\n9.3.1 Setting Up the Chain to Generate a Step-Back Question\nImplementing the step-back question technique is straightforward: it involves\ncrafting an effective prompt to generate a broader question and then\nGenerate a less specific question (aka Step-back question) for th\nStep-back question:\nstep_back_question_gen_chain = step_back_prompt | llm | StrOutput\nuser_question = \"Can you give me some tips for a trip to Brighton\nstep_back_question = step_back_question_gen_chain.invoke(user_que\nWhen you print step_back_question, you should get a response like:\nThis generated step-back question can then be used within a RAG\n9.3.2 Incorporating Step-Back Question Generation into the\nYou can integrate the step-back question generation chain into a RAG\nListing 9.3 Integrating Step-Back Question Generation within a RAG Architecture\nGiven a question and some context, answer the question.\nstep_back_question_rag_chain = (\n\"context\": {\"detailed_question\": RunnablePassthrough()} | \nuser_question = \"Can you give me some tips for a trip to Brighton\nanswer = step_back_question_rag_chain.invoke(user_question)",
      "keywords": [
        "question",
        "LLM",
        "Step-Back Question",
        "User",
        "detailed question",
        "prompt",
        "context",
        "Step-Back",
        "user question",
        "original question",
        "detailed",
        "retriever",
        "Chain",
        "questions",
        "generate"
      ],
      "concepts": [
        "question",
        "questions",
        "retrieved",
        "retrieve",
        "retrieval",
        "context",
        "llm",
        "answer",
        "step",
        "prompt"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 41,
          "title": "",
          "score": 0.678,
          "base_score": 0.678,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 33,
          "title": "",
          "score": 0.64,
          "base_score": 0.64,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 14,
          "title": "",
          "score": 0.603,
          "base_score": 0.603,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 40,
          "title": "",
          "score": 0.598,
          "base_score": 0.598,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 25,
          "title": "",
          "score": 0.577,
          "base_score": 0.577,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "question",
          "step question",
          "user_question",
          "step",
          "detailed"
        ],
        "semantic": [],
        "merged": [
          "question",
          "step question",
          "user_question",
          "step",
          "detailed"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3675006888243602,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:00:35.532848+00:00"
      }
    },
    {
      "chapter_number": 35,
      "title": "Segment 35 (pages 301-308)",
      "start_page": 301,
      "end_page": 308,
      "summary": "enhancing retrieval by broadening the question itself to improve context.\nAs discussed in section 7.2.2, embedding hypothetical questions can enhance\nrepresent questions answerable by the content in each chunk.\nto the user’s question than embeddings of the raw chunk text alone.\nwhile generating hypothetical documents based on the user’s question, as\nFigure 9.5 Hypothetical Document Embeddings (HyDE): In this approach, the LLM generates\nhypothetical documents that would answer the user’s question.\ndocument store with the user’s original question, these generated documents are used.\nsemantic similarity between the user’s question and the document chunk\nQuestion\ngenerate a hypothetical document that could answer the user’s question, and\nWrite one sentence that could answer the provided question.\nuser_question = \"What are the best beaches in Cornwall?\"\nhypotetical_document = hyde_chain.invoke(user_question)\nquestion and the document chunks in the vector store.\nGiven a question and some context, answer the question.\nOnly use the provided context to answer the question.\n\"context\": {\"question\": RunnablePassthrough()} | hyde_cha\nuser_question = \"What are the best beaches in Cornwall?\"\nanswer = hyde_rag_chain.invoke(user_question)\nsequence of dependent queries, where each answer informs the next question.\nFigure 9.6 Multi-Step Decomposition Workflow: In this workflow, the original complex question\nis sent to the LLM, which generates a sequence of parameterized questions.\nEach question is then\nOnce all questions\nAfter all questions are answered, the LLM\nTo prompt the LLM to generate this question sequence, use a template like\nReplacing the user question with\nprocessing the final question, you will have the context needed to answer the\noriginal question with the LLM.",
      "keywords": [
        "question",
        "Hypothetical Document Embeddings",
        "Hypothetical Document",
        "SSS",
        "questions",
        "Document",
        "RAG",
        "LLM",
        "Hypothetical",
        "user",
        "original question",
        "answer",
        "HyDE",
        "User Question",
        "Document Embeddings"
      ],
      "concepts": [
        "question",
        "questions",
        "retrieval",
        "retrieved",
        "retrieve",
        "parameter",
        "answerable",
        "answer",
        "context",
        "embeddings"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 36,
          "title": "",
          "score": 0.808,
          "base_score": 0.658,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 3,
          "title": "",
          "score": 0.772,
          "base_score": 0.622,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 24,
          "title": "",
          "score": 0.768,
          "base_score": 0.618,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 27,
          "title": "",
          "score": 0.741,
          "base_score": 0.591,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 21,
          "title": "",
          "score": 0.736,
          "base_score": 0.586,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "question",
          "hypothetical",
          "document",
          "answer",
          "hypothetical document"
        ],
        "semantic": [],
        "merged": [
          "question",
          "hypothetical",
          "document",
          "answer",
          "hypothetical document"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3734378877624951,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.532902+00:00"
      }
    },
    {
      "chapter_number": 36,
      "title": "Segment 36 (pages 309-316)",
      "start_page": 309,
      "end_page": 316,
      "summary": "Routing a natural language question to the relevant content store.\nGenerating a query for specific content stores, like SQL databases, from\nThe quality of LLM answers depends on well-structured questions, as\nvague or complex queries can mislead the vector store and LLM,\nthe original question, helping retrieve better context and enhancing LLM\nEmbedding hypothetical questions alongside document chunks improves\nretrieval by aligning them with potential user questions.\naligned content based on user questions without altering chunk\nGenerate metadata queries directly from user questions\nConvert user questions into database-specific queries (e.g., SQL,\neffectiveness for broader chunks, adding richer context, while query\ntransformations boost the precision of vector store retrieval.\nlearn to generate queries specific to the type of content store in use.\nquestion to retrieve data from a relational database.\nseveral types of content stores—such as vector stores, a relational database,\n10.1 Content Database Query Generation\nTo give an LLM the best information possible for answering user questions,\nthese databases hold structured data and only accept structured queries.\nstructured queries required for these databases poses a problem.\nYou’re already familiar with vector stores, which hold document chunks\nSimilarity between a user query and stored\nDuring querying, the user’s question is tokenized in the same\nchunk align with the query.\ntypically retrieved using SQL queries.\naccurately convert user questions into JSON-based queries that align\ndata is stored in a graph database, it can be queried using graph-specific\ngraph structures from raw text, generating SPARQL or Cypher queries,\nstructured query for different databases, starting with retrieving document\nchunks from a vector store using metadata.\n10.2 Self-Querying (Metadata Query Enrichment)\nA vector store typically indexes document chunks by embedding for dense\ndirectly—such as with dropdown options—your vector store query can\nthe user’s question.\nmetadata querying,” enables your application to automatically generate a\nquery enriched with metadata filters based on the user’s question.\nIn a self-querying flow, the user’s original question is transformed into an\nenriched query with both a metadata filter and a semantic component,\nFigure 10.1 Self-querying workflow: the original question turns into a semantic query with an\nWhen this enriched query runs on the vector store, it first selects\nchunks matching the metadata filter, then applies semantic search on this refined set.\nThe user's question is also included in\nLLM to generate a Rewritten query +] Y;\nthe rewritten query into the Retriever |\nembeddi i question and the retrieved\nyou’ll tag each chunk with relevant metadata keywords.\nphase, I’ll show you two methods for generating self-metadata queries: one\ndata into a new collection, this time storing metadata for each chunk.",
      "keywords": [
        "vector store",
        "LLM",
        "user questions",
        "vector",
        "question",
        "user",
        "query",
        "questions",
        "metadata",
        "store",
        "databases",
        "chunks",
        "SQL",
        "queries",
        "’ll"
      ],
      "concepts": [
        "query",
        "queries",
        "queried",
        "question",
        "questions",
        "retrievals",
        "retrieve",
        "retrieved",
        "database",
        "chunks"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 21,
          "title": "",
          "score": 0.863,
          "base_score": 0.713,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 27,
          "title": "",
          "score": 0.825,
          "base_score": 0.675,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 24,
          "title": "",
          "score": 0.811,
          "base_score": 0.661,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 35,
          "title": "",
          "score": 0.808,
          "base_score": 0.658,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 2,
          "title": "",
          "score": 0.799,
          "base_score": 0.649,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "metadata",
          "query",
          "questions",
          "user questions",
          "user"
        ],
        "semantic": [],
        "merged": [
          "metadata",
          "query",
          "questions",
          "user questions",
          "user"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.33417134635576295,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.532953+00:00"
      }
    },
    {
      "chapter_number": 37,
      "title": "Segment 37 (pages 317-324)",
      "start_page": 317,
      "end_page": 324,
      "summary": "uk_with_metadata_collection = Chroma(\nuk_destination_url_with_metadata = [  #C\nEnrich the content with metadata by processing each document chunk as\nListing 10.3 Enriching chunks with related metadata\nuk_with_metadata_collection.add_documents(documents=chunks)\nYou can query this content and apply metadata filters to refine\nThere are three ways to query metadata-enriched content:\n2. SelfQueryRetriever: Automatically generate the metadata filter using the\n3. Structured LLM Function Call: Infer the metadata filter with a structured\nQuerying with an Explicit Metadata Filter\nresult_docs = metadata_retriever.invoke(question)\n[Document(metadata={'destination': 'Newquay', 'region': 'Cornwall\nDocument(metadata={'destination': 'Newquay', 'region': 'Cornwall\nAutomatically Generating Metadata Filters with SelfQueryRetriever\nYou can also generate metadata filters automatically with\nListing 10.4 Setting up metadata field information\nfrom langchain.chains.query_constructor.base import AttributeInfo\nfrom langchain.retrievers.self_query.base import SelfQueryRetriev\nNext, define the metadata attributes to infer from the question:\nself_query_retriever = SelfQueryRetriever.from_llm(\nllm, uk_with_metadata_collection, question, metadata_field_in\nresult_docs = self_query_retriever.invoke(question)\n[Document(metadata={'destination': 'Newquay', 'region': 'Cornwall\nDocument(metadata={'destination': 'Newquay', 'region': 'Cornwall\nDocument(metadata={'destination': 'Newquay', 'region': 'Cornwall\nGenerating Metadata Filters with an LLM Function Call\nYou can also infer metadata filters by having the LLM map the question to a\nfrom langchain.retrievers.self_query.chroma import ChromaTranslat\nobject with a content_search field containing the question (minus filtering\nListing 10.6 Strongly-typed structured question including inferred filter attributes\nBuild a Chroma DB Filter Statement from the Structured Query\nListing 10.7 Building a ChromaDB specific filter statement from a structured query object\ndef build_filter(destination_search: DestinationSearch):\nregion = destination_search.region  #A\nBuild a Query Chain to Convert the Question into a Structured Query\nNow, define the query generator chain to convert the user question into a\nstructured query with metadata filters:\nquery_generator = prompt | structured_llm\nstructured_query =query_generator.invoke(question)\nPrinting structured_query shows the question converted into a structured\nWith the structured query created, generate a Chroma DB-compatible search\nsearch_filter = build_filter(structured_query)\nPerform the vector search using the generated structured query and Chroma\nsearch_query = structured_query.content_search\nanswer = metadata_retriever.invoke(search_query)\n[Document(metadata={'destination': 'Newquay', 'region': 'Cornwall",
      "keywords": [
        "metadata",
        "Cornwall",
        "filter",
        "destination",
        "metadata filters",
        "query",
        "Structured",
        "question",
        "East",
        "region",
        "structured query",
        "Newquay",
        "Sussex",
        "Content",
        "Chroma"
      ],
      "concepts": [
        "importing",
        "cornwall",
        "question",
        "questions",
        "filters",
        "filtering",
        "destinations",
        "metadata",
        "query",
        "queries"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 31,
          "title": "",
          "score": 0.611,
          "base_score": 0.611,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 28,
          "title": "",
          "score": 0.563,
          "base_score": 0.563,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 22,
          "title": "",
          "score": 0.551,
          "base_score": 0.551,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 35,
          "title": "",
          "score": 0.523,
          "base_score": 0.523,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 33,
          "title": "",
          "score": 0.521,
          "base_score": 0.521,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "metadata",
          "region",
          "structured",
          "destination",
          "newquay"
        ],
        "semantic": [],
        "merged": [
          "metadata",
          "region",
          "structured",
          "destination",
          "newquay"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.32898630966028247,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:00:35.533004+00:00"
      }
    },
    {
      "chapter_number": 38,
      "title": "Segment 38 (pages 325-336)",
      "start_page": 325,
      "end_page": 336,
      "summary": "In the SQLite terminal, load the SQL scripts to create and populate the\nsqlite> .read CreateUkBooking.sql\nsqlite> .read PopulateUkBooking.sql\nReturn to the Jupyter notebook and import the libraries needed for SQL\nfrom langchain.chains import create_sql_query_chain\nUse the following code to connect to the database and list available tables:\n10.3.3 Generating SQL Queries from Natural Language\nNow that the setup is complete, you can start generating SQL queries directly\nsql_query_gen_chain = create_sql_query_chain(llm, db)\nresponse = sql_query_gen_chain.invoke({\"question\": \"Give me some \nPrinting response will show the generated SQL query:\n'```sql\\nSELECT \"Offer\".\"OfferDescription\", \"Offer\".\"DiscountRate\nHowever, if you attempt to execute this SQL directly against the database,\n'Error: (sqlite3.OperationalError) near \"```sql\\nSELECT \"Offer\".\"\nTo clean up the SQL formatting, you can use the LLM to strip unnecessary\ncharacters and output a properly formatted SQL statement.\nListing 10.9 Chain to fix the formatting of the generated SQL\nOnly return an executable SQL statement which terminates with a s\nclean_sql_chain = clean_sql_prompt | llm\nfull_sql_gen_chain = sql_query_gen_chain | clean_sql_chain | StrO\nresponse = full_sql_gen_chain.invoke({\"question\": question})\nThe output should be a clean SQL statement:\n10.3.4 Executing the SQL Query\nNow, let’s create a chain to generate and execute SQL queries.\nsql_query_exec_chain = QuerySQLDataBaseTool(db=db)\nsql_query_gen_and_exec_chain = full_sql_gen_chain | sql_query_exe\nresponse = sql_query_gen_and_exec_chain.invoke({\"question\":questi\ncombined chain (sql_query_gen_and_exec_chain) that handles both SQL\nFigure 10.3 RAG with SQL Workflow: The LLM converts the natural language question into a\nSQL query, which is executed on the SQL database.\ngenerate SQL from CREATE + examples |\nReturn SQL|SELECT ...\nSQL records:\nContext: SQL records\nfull RAG with SQL workflow would look like.\ngenerate SQL queries directly from user questions.\n10.4 Generating a Semantic SQL Query\nIn the previous section, you learned how to generate SQL queries from\nHowever, these queries rely on “strict” SQL, meaning they\nBut what if you want to expand the SQL search to include results that are\nstandard SQL to a semantic SQL search.\noverview of how to implement semantic SQL search, a topic that continues to\n10.4.1 Standard SQL Query\nA standard SQL query filters based on exact matches.\nThis query returns only users named “Roberto.” It won’t return records for\n10.4.2 Semantic SQL Query\nIn this section, I’ll refer to this approach as semantic SQL search or SQL\nALTER TABLE user  ADD COLUMN first_name_embedding VECTOR\nembeddings in-place with SQL:\nsql = f'UPDATE user SET first_name_embeddings = ARRAY{emb} WH\ndb.run(sql)\n10.4.4 Performing a Semantic SQL Search\n10.4.5 Automating Semantic SQL Search\nsearches in a SQL database, the final step is to create a prompt that can\nautomatically generate SQL similarity queries.\nwe covered for generating traditional SQL queries.\nsearches on PGVector or any SQL database that supports ARRAY (or\n10.4.6 Benefits of Semantic SQL Search\nThe simple example here only scratches the surface of semantic SQL’s\nmultiple semantic filters, especially powerful in multi-table queries using\nsemantic filters in SQL offers greater flexibility, particularly for complex\n10.5 Generating Queries for a Graph Database",
      "keywords": [
        "SQL",
        "semantic SQL search",
        "Semantic SQL",
        "SQL query",
        "SQL search",
        "Semantic SQL Query",
        "DOUBLE DEFAULT NULL",
        "SQL Queries",
        "query",
        "SQL database",
        "Database",
        "generate SQL",
        "SQL statement",
        "DEFAULT NULL",
        "Semantic"
      ],
      "concepts": [
        "tables",
        "database",
        "embeddings",
        "imports",
        "query",
        "queries",
        "offers",
        "generating",
        "generate",
        "generation"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 34,
          "title": "",
          "score": 0.432,
          "base_score": 0.432,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 54,
          "title": "",
          "score": 0.414,
          "base_score": 0.414,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 22,
          "title": "",
          "score": 0.413,
          "base_score": 0.413,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 53,
          "title": "",
          "score": 0.412,
          "base_score": 0.412,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 6,
          "title": "",
          "score": 0.411,
          "base_score": 0.411,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "sql",
          "semantic sql",
          "semantic",
          "sql query",
          "sql search"
        ],
        "semantic": [],
        "merged": [
          "sql",
          "semantic sql",
          "semantic",
          "sql query",
          "sql search"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.25998896278235234,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:00:35.533049+00:00"
      }
    },
    {
      "chapter_number": 39,
      "title": "Segment 39 (pages 337-346)",
      "start_page": 337,
      "end_page": 346,
      "summary": "However, only some graph DBs use RDF.\nto store this data directly in a graph DB.\nSPARQL queries from natural language questions, easing a task\ninitial question, the generated Cypher or SPARQL query, and the results\nFigure 10.5 below shows the resulting Knowledge Graph RAG architecture,\nFigure 10.5 Knowledge Graph RAG Architecture (KG-RAG): Similar to vector-store-based RAG\nThe generator converts a natural language question into\nSPARQL, which is executed on the Knowledge Graph database.\nThe retrieved graph data is then\nThe user question is also fed to the\nuser question\n1a, The user question is\nknowledge graph query sag\nquestion and the retrieved\nbranches of the graph prompt and synthetizes a\nWhile this book does not cover Knowledge Graph RAG in detail, I\nNeo4j on building DevOps RAG applications with knowledge graphs:\nThe question is:\n{question}\"\"\"\nknowledge graph databases for entity relationships.\ntasks, such as answering questions about tourist Destinations (from a vector store) or\nThe user question is also fed to the\nQuestion\nThe user question is\nroute this query to a RAG chain based on a vector store.\nIf the user’s question\nTo route each question to the correct chain, you use a routing chain.\nThese retriever chains direct questions to the appropriate data source.\nwe’ll build a router to direct user questions to one of these retriever chains.\nWe’ll implement a question router using an LLM.\neach question and determine the best retriever chain based on its content.\nprompt will specify the function of each retriever: the vector store for general\n\"tourist_info_store\" or \"uk_booking_db\" depending on the question’s\n\"\"\"Route a user question to the most relevant datasource.\"\"\"\ndescription=\"Given a user question, route it either to a \nUse the vectorstore for general tourist information questions on \nFor questions about accommodation availability or booking, use th\nquestion_router = route_prompt | structured_llm_router\nThis setup enables the LLM to intelligently route each question to the\nLet’s test the router chain with a question about tourist information and\nselected_data_source = question_router.invoke(\nselected_data_source = question_router.invoke(\nchosen data source ('uk_booking_db' or 'tourist_info_store').\n'tourist_info_store': tourist_info_retriever_chain,\n'uk_booking_db': uk_accommodation_retriever_chain\ndef retriever_chooser(question):\nselected_data_source = question_router.invoke(\nreturn retriever_chains[selected_data_source.datasource]\nLet’s test the retriever chooser function with a sample question:",
      "keywords": [
        "Knowledge Graph RAG",
        "Knowledge Graph",
        "question",
        "Graph",
        "user question",
        "LLM",
        "retriever",
        "SPARQL knowledge graph",
        "Graph RAG",
        "RAG",
        "Graph RAG architecture",
        "Knowledge",
        "Chain",
        "graph DBs",
        "data"
      ],
      "concepts": [
        "questions",
        "question",
        "retrieved",
        "retrieval",
        "graph",
        "chain",
        "generation",
        "generate",
        "generated",
        "generator"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 36,
          "title": "",
          "score": 0.782,
          "base_score": 0.632,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 21,
          "title": "",
          "score": 0.72,
          "base_score": 0.57,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 23,
          "title": "",
          "score": 0.695,
          "base_score": 0.545,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 3,
          "title": "",
          "score": 0.68,
          "base_score": 0.53,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 24,
          "title": "",
          "score": 0.662,
          "base_score": 0.512,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "graph",
          "question",
          "knowledge graph",
          "knowledge",
          "graph rag"
        ],
        "semantic": [],
        "merged": [
          "graph",
          "question",
          "knowledge graph",
          "knowledge",
          "graph rag"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.32117984313832926,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.533102+00:00"
      }
    },
    {
      "chapter_number": 40,
      "title": "Segment 40 (pages 347-354)",
      "start_page": 347,
      "end_page": 354,
      "summary": "The output confirms that the correct retriever chain instance is selected based\nenabling the workflow of retriever selection, query execution, and answer\nListing 10.13 Full RAG Chain for Routing, Retrieval, and Answer Synthesis\ndef execute_rag_chain(question, chosen_retriever):\nreturn full_rag_chain.invoke(question)\nchosen_retriever = retriever_chooser(question)\nanswer = execute_rag_chain(question, chosen_retriever)\nchosen_retriever_2 = retriever_chooser(question_2)\nanswer2 = execute_rag_chain(question_2, chosen_retriever_2)\nIn both cases, the RAG chain correctly routes each question to the\nanswer by feeding the context and original question to the LLM.\n10.7 Retrieval Post-Processing\naccuracy of RAG retrieval, you’ll likely have a list of document chunks (or\nFigure 10.7 Retrieval Post-processing: Retrieved chunks from the vector store are filtered to\nremove irrelevant content, ensuring only high-quality chunks are sent to the LLM for answering\nscore_threshold_similartity_retriever = vector_store.as_retriever\ndoc_chunks = score_threshold_similartity_retriever.get_relevant_d\nThis retrieves only documents with similarity scores above the specified\nAnother post-processing approach is to filter retrieved document chunks by\nrank chunks by both similarity score and recency:\nThis adjusted score allows recent, relevant content to rank higher, ensuring\nmultiple queries are generated from a user’s question, and a subset of relevant\nretrieved documents based on a specific scoring formula:\nrank: the document's current rank based on similarity or relevance.\ntheir cumulative RRF scores, and the top-ranked results are sent to the LLM\nFigure 10.8 Reciprocal Rank Fusion Workflow: Multiple queries are generated from the initial\nEach query retrieves a set of results, such as from a vector store.\nthen reranked using RRF scores, with only the top results sent to the LLM for final answer\nqueries, as shown in chapter 9, and ranking the results with the RRF",
      "keywords": [
        "Reciprocal Rank Fusion",
        "RAG",
        "RAG Chain",
        "Full RAG Chain",
        "question",
        "Rank Fusion",
        "chain",
        "retriever",
        "Reciprocal Rank",
        "llm",
        "rank",
        "Similarity",
        "chunks",
        "score",
        "RAG Fusion"
      ],
      "concepts": [
        "retrieval",
        "retrieved",
        "retrieves",
        "question",
        "rank",
        "chunks",
        "scores",
        "scoring",
        "rag",
        "fusion"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 27,
          "title": "",
          "score": 0.72,
          "base_score": 0.72,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 25,
          "title": "",
          "score": 0.697,
          "base_score": 0.697,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 41,
          "title": "",
          "score": 0.64,
          "base_score": 0.64,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 35,
          "title": "",
          "score": 0.623,
          "base_score": 0.623,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 23,
          "title": "",
          "score": 0.608,
          "base_score": 0.608,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "rank",
          "fusion",
          "rag",
          "chosen_retriever",
          "question chosen_retriever"
        ],
        "semantic": [],
        "merged": [
          "rank",
          "fusion",
          "rag",
          "chosen_retriever",
          "question chosen_retriever"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.39394041835214605,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:00:35.533158+00:00"
      }
    },
    {
      "chapter_number": 41,
      "title": "Segment 41 (pages 355-362)",
      "start_page": 355,
      "end_page": 362,
      "summary": "You can generate multiple queries from an initial question using a chain, as\nListing 10.14 Multi-Query Generation\ndifferent versions of the given user question to retrieve relevan\nmulti_query_gen_chain = multi_query_gen_prompt | llm | questions_\nThe core of this workflow is the Reciprocal Rank Fusion (RRF) algorithm,\nwhich assigns scores to documents retrieved by multiple queries.\nListing 10.15 Reciprocal Rank Fusion (RRF) Algorithm\ndef reciprocal_rank_fusion(results_groups: list[list], k=60):  #A\nindexed_results[(group_id, local_rank)] = doc\nfused_scores[key] += 1 / (local_rank + k)  #G\n(indexed_results[key], score)\nWith the RRF algorithm in place, let’s create a RAG Fusion retrieval chain,\nrag_fusion_retrieval_chain = multi_query_gen_chain | retriever.ma\ndocs = rag_fusion_retrieval_chain.invoke({\"question\": question})  \nThe final step is to integrate this RAG Fusion retrieval chain into a larger\nRAG chain for end-to-end question routing, retrieval, and answer synthesis.\nthe RAG Fusion retrieval chain within a RAG chain.\nListing 10.16 Integrating the RAG Fusion Retrieval Chain into a RAG Chain\n\"context\": {\"question\": RunnablePassthrough()} | rag_fusi\nNow, let’s test the complete RAG chain with an example question:\nuser_question = \"Can you give me some tips for a trip to Brighton\nanswer = rag_chain.invoke(user_question)\nLLMs can also generate metadata queries directly from user questions.\nMany LLMs can translate user questions into SQL queries, enabling\ngraphs with the help of LLMs or answer questions based on knowledge\n- Reciprocal Rank Fusion (RRF) improves query relevance by reranking\n11 Building Tool-based Agents with\nBuilding LLM-powered agents using LangGraph\nDebugging agent execution and tool calls\nIn chapter 5, you explored the distinction between agentic workflows and\nagents.\nhelp of a language model (LLM), an agent chooses which tools to use—and\nit into a true multi-tool agent, able to answer questions about both travel\nThis chapter’s multi-tool agent will serve as the foundation for the more\nadvanced, multi-agent systems you’ll build in the chapters ahead.\nbuilding a straightforward travel information agent.\nThis first agent will use\njust one tool: a vector store retriever that answers questions about Cornwall’s\nIf you’ve followed the advanced RAG (Retrieval-Augmented Generation)\nPS C:\\Github\\building-llm-applications\\ch11> python -m venv env_c\nPS C:\\Github\\building-llm-applications\\ch11> .\\env_ch11\\Scripts\\a\n(env_ch11) PS C:\\Github\\building-llm-applications\\ch11>\n(env_ch11) PS C:\\Github\\building-llm-applications\\ch11> pip insta\nCreate a .env file in your project root and add your OpenAI API key:",
      "keywords": [
        "Reciprocal Rank Fusion",
        "RAG Fusion retrieval",
        "Fusion retrieval chain",
        "RAG",
        "RAG Fusion",
        "RAG chain",
        "Rank Fusion",
        "Reciprocal Rank",
        "Fusion",
        "chain",
        "Rank",
        "question",
        "Fusion retrieval",
        "agent",
        "retrieval chain"
      ],
      "concepts": [
        "agents",
        "question",
        "questions",
        "retrieve",
        "retrieval",
        "retrieved",
        "llm",
        "listing",
        "based",
        "rag"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 34,
          "title": "",
          "score": 0.678,
          "base_score": 0.678,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 40,
          "title": "",
          "score": 0.64,
          "base_score": 0.64,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 33,
          "title": "",
          "score": 0.636,
          "base_score": 0.486,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 16,
          "title": "",
          "score": 0.578,
          "base_score": 0.578,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 22,
          "title": "",
          "score": 0.569,
          "base_score": 0.419,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "fusion",
          "fusion retrieval",
          "rag",
          "rank",
          "retrieval chain"
        ],
        "semantic": [],
        "merged": [
          "fusion",
          "fusion retrieval",
          "rag",
          "rank",
          "retrieval chain"
        ]
      },
      "topic_id": 8,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3375332292876876,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.533226+00:00"
      }
    },
    {
      "chapter_number": 42,
      "title": "Segment 42 (pages 363-371)",
      "start_page": 363,
      "end_page": 371,
      "summary": "To enable our travel information agent to answer queries about Cornwall\nretrieve relevant travel information as the agent operates.\nListing 11.2 Preparing the Travel Information Vector Store\nembedded using OpenAI’s embedding models and stored in a Chroma vector\nget_travel_info_vectorstore() function ensures that the vector store is\n11.2 Enabling Agents to Call Tools\nthis retrieval capability as a tool the agent can use.\n11.2.1 From Function Calling to Tool Calling\nWith tool calling, models can invoke not\nthe agent to process a user question by thinking, calling tools as needed, and following up with\nAfter the tool call is completed, the agent returns to another Reasoning phase\nThis may lead to further tool calls, or\nfunction calling to tool calling has accelerated.\n11.2.2 How Tool Calling Works with LLMs\nOpenAI’s tool calling supports several types of tools—including user-defined\nhigh level, when you register a tool with the LLM, you expose both the\nmodel then decides, at runtime, when and how to use each tool based on the\nIn LangGraph, tools can be registered using either a class-based or decorator-\nLet’s implement our semantic search tool as a function decorated with @tool,\nmaking it available to the agent for tool calling, as shown in listing 11.3:\nListing 11.3 LangGraph attribute-based tool definition\n@tool #A\nThis decorated function, search_travel_info(), is now recognized as a\ntool: it takes a user query, searches the vector store for relevant WikiVoyage\nThe @tool\nschema are all available to the LLM for tool calling.\n11.2.3 Registering Tools with the LLM\nTo enable the agent to use our semantic search tool, we must register it with\nWith the introduction of the OpenAI Responses API, the way tools (and\n(https://platform.openai.com/docs/guides/function-calling?api-\nExample: Manual Tool Registration with OpenAI API\nYou would define the tool’s\nListing 11.4 Manual tool registration with OpenAI API\nsearch_travel_info_tool = {\n# Register the tool (function) when making a chat completion requ\ntools=[search_travel_info_tool],  #K\ntool_choice=\"auto\",  # let the model decide when to use the t\nthen pass it in the tools argument of your API call.\nneeds to call search_travel_info, it will return a structured tool call in its\nRegistering Tools in LangChain\nListing 11.5 Registering tools in LangChain\nTOOLS = [search_travel_info] #A\nllm_with_tools = llm_model.bind_tools(TOOLS) #C\nHere, we list the available tools (currently just search_travel_info),\nuse .bind_tools(TOOLS) to expose those tools to the model for tool calling.\nOpenAI function/tool-calling protocol, including automatically generating the\nresponses according to the tool calling protocol.\nResponses API, tool calling can still work—though the capabilities and",
      "keywords": [
        "tool",
        "tool calling",
        "Vector Store",
        "API",
        "Chroma vector store",
        "Responses API",
        "OpenAI Responses API",
        "agent",
        "Travel",
        "function",
        "calling",
        "OpenAI API",
        "Vector",
        "Store",
        "Information Vector Store"
      ],
      "concepts": [
        "tools",
        "functionality",
        "function",
        "functions",
        "agent",
        "models",
        "api",
        "queries",
        "query",
        "calling"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 45,
          "title": "",
          "score": 0.707,
          "base_score": 0.557,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 4,
          "title": "",
          "score": 0.673,
          "base_score": 0.523,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 17,
          "title": "",
          "score": 0.643,
          "base_score": 0.493,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 1,
          "title": "",
          "score": 0.536,
          "base_score": 0.536,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 3,
          "title": "",
          "score": 0.529,
          "base_score": 0.529,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "tool",
          "calling",
          "tool calling",
          "tools",
          "11"
        ],
        "semantic": [],
        "merged": [
          "tool",
          "calling",
          "tool calling",
          "tools",
          "11"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3792410070207144,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.533287+00:00"
      }
    },
    {
      "chapter_number": 43,
      "title": "Segment 43 (pages 372-379)",
      "start_page": 372,
      "end_page": 379,
      "summary": "between the user, the agent, and any tool responses.\n11.2.5 Executing Tool Calls\nThe core of the tool execution logic is implemented in a node that examines\nthe LLM’s most recent message, extracts any tool calls requested by the\n\"\"\"Execute tools requested by the LLM in the last AIMessage.\"\ntool_messages: list[ToolMessage] = [] #E\ntool_messages.append(\nname=tool_name,\ntools_execution_node = ToolsExecutionNode(TOOLS) #N\nThis pattern allows the agent to handle multiple tool calls in a single step.\nThe tools execution node inspects the LLM’s latest response, invokes each\nIn a typical agent workflow, these tool results are\ntools_execution_node = ToolNode(TOOLS)\nthe agent, guided by the LLM, determines which tools to call—or whether it\n\"\"\"LLM node that decides whether to call the search tool.\"\"\"\nrespose_message = llm_with_tools.invoke(current_messages) #C\nThis node takes the agent state (a list of messages) and forwards them to the\nBecause we are using an LLM with tool calling enabled\nIf the last message is a user question, the LLM can decide to request a\nIf the last message(s) are tool results, the LLM integrates those\nWith our LLM node and tool execution node ready, the next step is to\ntravel information agent as a graph.\neither the LLM (language model) or the tool execution logic.\nListing 11.8 Graph of the tool-based agent\nbuilder.add_node(\"tools\", tools_execution_node) #B\nbuilder.add_conditional_edges(\"llm_node\", tools_condition) #C\nbuilder.add_edge(\"tools\", \"llm_node\") #D\nOur agent graph consists of two main nodes, as shown in figure 11.2: the\nLLM node, responsible for reasoning and generating tool calls, and the tools\nnode, responsible for executing the requested tools and returning results.\nFigure 11.2 Conditional graph logic routes each user query through the LLM node, then\ndynamically directs flow to the tools node or ends the process, depending on whether tool calls\nIf the message contains the tool_calls property (meaning the LLM is\nthe node named \"tools\".\nIf there are no tool calls present, the flow is directed to the END node,\nSTART node to \"llm_node\" with graph_builder.add_edge(START,\ninformation agent, ready to receive queries and intelligently use its tool to\nLLM and the tool, helping you see the LangGraph framework in motion.",
      "keywords": [
        "LLM",
        "tool",
        "LLM Node",
        "node",
        "Tool Calls",
        "agent",
        "travel information agent",
        "Calls",
        "Graph",
        "Agent Graph",
        "user",
        "tool execution",
        "messages",
        "loop",
        "information agent"
      ],
      "concepts": [
        "tool",
        "agent",
        "messages",
        "node",
        "loop",
        "listing",
        "user",
        "state",
        "builder",
        "graph"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 44,
          "title": "",
          "score": 0.738,
          "base_score": 0.738,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 45,
          "title": "",
          "score": 0.536,
          "base_score": 0.536,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 17,
          "title": "",
          "score": 0.529,
          "base_score": 0.529,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 4,
          "title": "",
          "score": 0.526,
          "base_score": 0.526,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 42,
          "title": "",
          "score": 0.522,
          "base_score": 0.522,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "tool",
          "node",
          "agent",
          "tools",
          "calls"
        ],
        "semantic": [],
        "merged": [
          "tool",
          "node",
          "agent",
          "tools",
          "calls"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3432910256869667,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:00:35.533342+00:00"
      }
    },
    {
      "chapter_number": 44,
      "title": "Segment 44 (pages 380-388)",
      "start_page": 380,
      "end_page": 388,
      "summary": "LLM is configured for tool calling, examine the resulting response_message:\ntool_calls=[\nHere, the LLM has generated three tool calls—each targeting your semantic\nTools Execution Node\nThe list of tool calls is added to the message list.\ntool_calls=[\nyet because it needs more information), but it contains the tool calls that must\nTool Call Results Passed Back to LLM\ncontains your original question, the LLM’s tool call instructions, and the\ntool\\_calls=\\[{'name': 'search\\_travel\\_info', 'args': {'query\nToolMessage(content='...', name='search\\_travel\\_info', tool\\_\nToolMessage(content='...', name='search\\_travel\\_info', tool\\_\nToolMessage(content='...', name='search\\_travel\\_info', tool\\_\nThe content of each tool message gives the LLM the facts it needs to\nresponse\\_message = llm\\_with\\_tools.invoke(current\\_messages)\ntool_calls field is gone—the LLM no longer needs external tools and has\nAs execution leaves llm_node(), the tools_condition on the conditional\nAIMessage(content=[],  tool_calls=[...]),\nForecast Tool\ntime weather data for any given town, returning both a weather condition\n11.6.2 Creating the Weather Forecast Tool\n@tool\n\"\"\"Get a mock weather forecast for a given town.\nreturn {\"error\": f\"No weather data available for '{town}'\nThe weather_forecast tool provides a mock weather forecast for a given\nThis tool allows the agent to incorporate simulated real-time weather\nFinally, make sure your LLM model is set up to use tool calling and\nTOOLS = [search_travel_info, weather_forecast] #A\nllm_with_tools = llm_model.bind_tools(TOOLS) #C\nboth tools effectively, and observe the agent’s new capabilities in action.\n11.7 Executing the Multi-Tool Agent\nWith your weather forecast tool registered, your agent can now synthesize\nLLM and the tool calling protocol.\n—especially in the tool execution and LLM nodes—and add a breakpoint at\nresponse_message.tool_calls:\n[{'name': 'weather_forecast', 'args': {'town': 'Newquay'}, 'id': \n{'name': 'weather_forecast', 'args': {'town': 'Falmouth'}, 'id': \nsearch tool at all.\n11.7.2 Improving LLM Tool Usage with System Guidance\nTo nudge the LLM towards tool use and away from hallucinations, let’s make\n@tool(description=\"Get the weather forecast, given a town name.\")\n\"\"\"LLM node that decides whether to call the search tool.\"\"\"\nrespose_message = llm_with_tools.invoke(current_messages) #E\nThe LLM is now forced to use your semantic search tool for candidate beach\nAIMessage(content=[], ..., tool_calls=[{'name': 'search_travel_in\ncalls for the weather forecast in two of these towns:\nAIMessage(content=[], ..., tool_calls=[\n{'name': 'weather_forecast', 'args': {'town': 'Newquay'}, 'id': '\nStep through the weather tool calls—each ToolMessage you inspect should\ntool_calls=[\n{'name': 'weather_forecast', 'args': {'town': 'Perranporth'}, 'id\n{'name': 'weather_forecast', 'args': {'town': 'Falmouth'}, 'id': \nHere, the LLM is asking for the weather in two more towns, likely because\nAfter the final tool calls, the LLM generates a synthesized, fact-based\ninstructions via system prompts, you can guide the LLM to chain tool use in a\nTry replacing the mock weather tool with a real API, like OpenWeatherMap,\nthe graph, orchestrating tool calls, and stepping through every detail in the",
      "keywords": [
        "LLM",
        "tool",
        "Weather",
        "towns",
        "Cornwall beach towns",
        "beach towns",
        "content",
        "tool calls",
        "search",
        "agent",
        "Forecast",
        "travel",
        "calls",
        "Weather Forecast",
        "LLM Node Activation"
      ],
      "concepts": [
        "tool",
        "weather",
        "message",
        "towns",
        "agent",
        "llm",
        "travel",
        "query",
        "queries",
        "calling"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 43,
          "title": "",
          "score": 0.738,
          "base_score": 0.738,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 18,
          "title": "",
          "score": 0.592,
          "base_score": 0.442,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 40,
          "title": "",
          "score": 0.542,
          "base_score": 0.542,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 15,
          "title": "",
          "score": 0.54,
          "base_score": 0.54,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 34,
          "title": "",
          "score": 0.533,
          "base_score": 0.533,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "tool",
          "weather",
          "town",
          "forecast",
          "tool_calls"
        ],
        "semantic": [],
        "merged": [
          "tool",
          "weather",
          "town",
          "forecast",
          "tool_calls"
        ]
      },
      "topic_id": 9,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3595225408848133,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.534009+00:00"
      }
    },
    {
      "chapter_number": 45,
      "title": "Segment 45 (pages 389-396)",
      "start_page": 389,
      "end_page": 396,
      "summary": "11.8.1 Refactoring to Use the LangGraph React Agent\nfield allows the agent to manage how many tool-calling rounds are left in a\nagent:\ntravel_info_agent = create_react_agent(\nThe React agent now orchestrates the flow, tool calling, and\n11.8.2 Running the Pre-Built Agent\nWhile you can still debug tool functions directly, the flow inside the agent\ninspection of agent behavior, including tool calls, LLM reasoning, and\nLANGSMITH_PROJECT=\"langchain-in-action-react-agent\"\nFigure 11.4 LangSmith agent execution trace: this trace visualizes each step of the agent’s\nBD agent 282s\nBD agent 222s v\nThe LangSmith graphical trace shows every tool call, every LLM step, and\nIn summary, by combining pre-built LangGraph agent components with\ndynamic LLM agents using node-based and tool-based architectures.\nBy stepping through code with breakpoints, you can trace agent\nMulti-tool agents can chain together tool outputs (e.g., finding towns,\nUsing pre-built agent components, like the LangGraph React agent,\nLangSmith provides rich observability, showing each LLM step and tool\n12 Multi-agent Systems\nDebugging, testing, and tracing multi-agent interactions\nIn chapter 11, we explored the foundations of building AI agents by creating\na travel information agent capable of answering user queries about\nwe’ll embark on that journey—transforming our travel information agent into\na robust, multi-agent travel assistant system.\nenhanced multi-agent travel assistant will do just that: it will be able to\nTo achieve this, we’ll begin by building a new agent\n—the accommodation booking agent.\nThe accommodation booking agent will empower users to book lodgings\nOnce we have our new agent in place, we’ll combine it with the travel\ninformation agent from the previous chapter.\nmulti-agent travel assistant capable of fielding a wide variety of travel-related\nLet’s begin by constructing our new accommodation booking agent.\n12.1 Building an Accommodation Booking Agent\ndevelop an accommodation booking agent from the ground up, starting by\nbuilding the tools it needs: one for hotel bookings based on a local room\n12.1.1 Hotel Booking Tool\nLet’s start by creating the hotel booking tool.\nTo enable our agent to retrieve\nhotel offers and availability, we’ll use the LangChain SQL Database Toolkit,\nwhich exposes a SQL database as a set of agent tools.\nstraightforward for an agent to run queries, retrieve hotel details, and check\nfrom langchain_community.agent_toolkits import SQLDatabaseToolkit",
      "keywords": [
        "agent",
        "accommodation booking agent",
        "Tool",
        "Cornwall",
        "LLM",
        "hotel",
        "LangSmith",
        "booking agent",
        "React Agent",
        "LangGraph React Agent",
        "travel",
        "booking",
        "Cornwall beach towns",
        "database",
        "Booking Tool"
      ],
      "concepts": [
        "agent",
        "tool",
        "tracing",
        "trace",
        "book",
        "hotels",
        "llm",
        "travel",
        "weather",
        "database"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 4,
          "title": "",
          "score": 0.713,
          "base_score": 0.563,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 42,
          "title": "",
          "score": 0.707,
          "base_score": 0.557,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 17,
          "title": "",
          "score": 0.697,
          "base_score": 0.547,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 1,
          "title": "",
          "score": 0.583,
          "base_score": 0.583,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 43,
          "title": "",
          "score": 0.536,
          "base_score": 0.536,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "agent",
          "booking",
          "tool",
          "hotel",
          "react agent"
        ],
        "semantic": [],
        "merged": [
          "agent",
          "booking",
          "tool",
          "hotel",
          "react agent"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2925687468012873,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.534079+00:00"
      }
    },
    {
      "chapter_number": 46,
      "title": "Segment 46 (pages 397-404)",
      "start_page": 397,
      "end_page": 404,
      "summary": "12.1.3 ReAct Accommodation Booking Agent\nReAct accommodation booking agent.\nThis agent will use both tools in\naccommodation_booking_agent = create_react_agent( #B\nresult = accommodation_booking_agent.invoke(state) \nAt this point, your accommodation booking agent is working as expected.\nAlthough you now have both a travel information agent and an\naccommodation booking agent, they are still disconnected.\nand an accommodation booking agent, each with specialized capabilities.\nThis agent\n12.2.1 Designing the Router Agent\nTo implement our multi-agent travel assistant, begin by copying your\n\"travel_info_agent\" or \"accommodation_booking_agent\".\nWith this system prompt, the router agent evaluates each user input and\nthe entry node of our LangGraph workflow, with the travel information agent\nand the accommodation booking agent as subsequent nodes.\nListing 12.3 Router Agent node\ndef router_agent_node(state: AgentState) -> Command[AgentType]:\n\"\"\"Router node: decides which agent should handle the user qu\nagent_name = router_response.agent.value #G\nreturn Command(update=state, goto=AgentType.travel_info_agent\n12.2.3 Building the Multi-Agent Graph\nListing 12.4 Router-based multi-agent travel assistant graph\ngraph.add_node(\"router_agent\", router_agent_node) #B\ngraph.add_node(\"travel_info_agent\", travel_info_agent) #C\ngraph.add_node(\"accommodation_booking_agent\", accommodation_booki\ngraph.add_edge(\"travel_info_agent\", END) #E\ngraph.add_edge(\"accommodation_booking_agent\", END) #F\ngraph.set_entry_point(\"router_agent\") #G\nThe workflow graph connects the router agent to the two specialized agents.\ninformation agent and the accommodation booking agent to the end of the\nThe connection from the router to the specialized agents is\nFigure 12.1 Router-based multi-agent Travel assistant: the router agent dispatches user queries\nto either the travel information agent or the accommodation booking agent, each equipped with\nlower level, each specialist agent uses its own set of tools (such as travel data\nAPIs or accommodation booking interfaces) and follows an agentic, tool-\n12.2.4 Trying Out the Router Agent\nthe request is routed to the travel information agent.\n( router_agent 9.295\n(QB travel_info_agent 3.32s\n(2B agent 0.96s\nagent 1.18s",
      "keywords": [
        "Agent",
        "Accommodation Booking Agent",
        "BnB",
        "router agent",
        "Accommodation Booking",
        "travel information agent",
        "Travel",
        "router",
        "Booking Agent",
        "Booking",
        "Accommodation",
        "Travel Assistant",
        "travel information",
        "multi-agent travel assistant",
        "information agent"
      ],
      "concepts": [
        "agent",
        "graph",
        "user",
        "tool",
        "router",
        "book",
        "travel",
        "state",
        "question",
        "returns"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 47,
          "title": "",
          "score": 0.962,
          "base_score": 0.812,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 52,
          "title": "",
          "score": 0.754,
          "base_score": 0.604,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 51,
          "title": "",
          "score": 0.669,
          "base_score": 0.519,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 45,
          "title": "",
          "score": 0.496,
          "base_score": 0.496,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 4,
          "title": "",
          "score": 0.458,
          "base_score": 0.458,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "agent",
          "booking",
          "accommodation",
          "accommodation booking",
          "router"
        ],
        "semantic": [],
        "merged": [
          "agent",
          "booking",
          "accommodation",
          "accommodation booking",
          "router"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2061941656060891,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.534125+00:00"
      }
    },
    {
      "chapter_number": 47,
      "title": "Segment 47 (pages 405-412)",
      "start_page": 405,
      "end_page": 412,
      "summary": "the system dispatches the query to the accommodation booking agent.\n12.3 Handling Multi-Agent Requests with a\nThe workflow-based multi-agent architecture we developed in the previous\nclearly routed to either the travel information agent or the accommodation\nbooking agent.\n12.3.1 The Supervisor Pattern: An Agent of Agents\nEach agent acts as a specialized tool that the Supervisor can\nLet’s see how to set up this pattern in your multi-agent travel assistant.\nWhen defining agents to be managed by the Supervisor, it’s important to\ntravel_info_agent = create_react_agent(\nname=\"travel_info_agent\",\nname=\"accommodation_booking_agent\",\nListing 12.6 Setting up the Supervisor agent\ntravel_assistant = create_supervisor( #A\nagents=[travel_info_agent, accommodation_booking_agent], #B\nsupervisor_name=\"travel_assistant\",\n\"You are a supervisor that manages two agents: a travel i\nsupervisor-based travel assistant:\nonce and only once to a specific agent (a “one-way ticket”)—the Supervisor\nThe Supervisor can invoke each agent as\nFigure 12.3 Router-based multi-agent Travel assistant: the router agent dispatches user queries\nto either the travel information agent or the accommodation booking agent, each equipped with\nThis Supervisor-driven architecture unlocks a new level of multi-agent\n12.3.3 Trying out the Supervisor agent\ntravel_assistant is the supervisor agent.\ne& travel_info_agent 76<s\nGs travel_info_agent 76<=s\nGa agent oss\nGa agent 353s\nGay agent 2-245\nagent 2.61s\nagent 193s\ntransfer_to_travel_info_agent\ntravel_info_agent\naccommodation_booking_agent\nboth agents, using each one and its underlying tools as needed to fully answer\nTechnically, agents are now orchestrated as tools, and the",
      "keywords": [
        "agent",
        "Supervisor",
        "travel",
        "tools",
        "travel assistant",
        "assistant",
        "Supervisor agent",
        "Supervisor agent travel",
        "booking",
        "booking agent",
        "accommodation booking agent",
        "Supervisor Pattern",
        "info",
        "accommodation",
        "agent travel"
      ],
      "concepts": [
        "agent",
        "tools",
        "supervisor",
        "travel",
        "assistant",
        "model",
        "level",
        "design",
        "single",
        "user"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 46,
          "title": "",
          "score": 0.962,
          "base_score": 0.812,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 52,
          "title": "",
          "score": 0.76,
          "base_score": 0.61,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 51,
          "title": "",
          "score": 0.684,
          "base_score": 0.534,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 4,
          "title": "",
          "score": 0.556,
          "base_score": 0.556,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 45,
          "title": "",
          "score": 0.441,
          "base_score": 0.441,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "agent",
          "supervisor",
          "travel",
          "supervisor agent",
          "travel_info_agent"
        ],
        "semantic": [],
        "merged": [
          "agent",
          "supervisor",
          "travel",
          "supervisor agent",
          "travel_info_agent"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.21815666506894416,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.534170+00:00"
      }
    },
    {
      "chapter_number": 48,
      "title": "Segment 48 (pages 413-424)",
      "start_page": 413,
      "end_page": 424,
      "summary": "13 Building and consuming MCP\nThe purpose and architecture behind the Model Context Protocol (MCP)\nBuilding and exposing your own MCP server, with a practical weather\nTesting and consuming MCP servers and related tools in applications\nIntegrating remote MCP tools into agents alongside local tools\nThis is the promise of the Model Context Protocol (MCP),\nMCP servers.\nMCP clients, discovering and using remote tools as easily as local ones.\ntools from MCP servers work seamlessly with existing agent architectures.\nmaking services available as MCP servers.\nlisted on public MCP server portals, ready to be integrated into new or\necosystem, and show you where to find both official and community MCP\nAs the MCP ecosystem grows, understanding how to build and consume\nMCP servers is becoming essential for developers and service providers\n13.1 Introduction to MCP Servers\nFigure 13.1 MCP host process connecting to multiple local and remote MCP servers via MCP\nme ce Local MCP Server 1 (on\nMCP client 3\nRemote MCP Server 3 fen\nRemote MCP Server 2\n13.1.2 The Solution: The Model Context Protocol (MCP)\nservice providers to expose tools via MCP servers.\nto MCP servers and use the tools they expose, with minimal additional work.\nHere, the MCP host process‚Äîthe agent or application‚Äîconnects to one or\nmore MCP servers using an MCP client.\nEach MCP server can expose a\nAs you can also see on figure 13.1, MCP servers can be either remote\nworking with remote MCP servers.\nOnce configured, tools from MCP servers integrate with your agent in\nremote MCP tools.\n13.1.3 The MCP Ecosystem\nLLM providers like OpenAI and Google have both adopted MCP, integrating\nincreasingly wrapping their services and data with MCP servers, making\nA number of public MCP server portals have emerged, making it easy to find\nTable 13.1 MCP server portals\nhttps://github.com/modelcontextprotocol/servers Anthropic's official MCP\nhttps://mcp.so/\ntools, mostly MCP-\nMCP servers, we are ready to look at how to build, expose, and consume\nhow to create your own MCP server and how to integrate MCP tools\n13.2 How to Build MCP Servers\nWith a clear understanding of what MCP servers are and their role in the\ndeploy, and integrate MCP servers.\nkey resources, official tools, and best practices to help you create robust MCP\nservers and make them available to agents and applications.\n13.2.1 Essential Resources for MCP Server Development\nfoundational documentation and tools for MCP development.\n13.2.2 Official Language-Specific MCP SDKs\nMCP servers in Python.\nBuilding on the experience with FastMCP 1, the MCP community released\n13.2.3 Consuming MCP Servers in LLM Applications and\nWhile the previous sections focused on building MCP servers, it‚Äôs equally\nThanks to wide adoption, integrating MCP tools has\nOpenAI‚Äôs API natively supports tools provided by public MCP servers via\nReview the OpenAI documentation on remote MCP tool integration.\nIn many enterprise environments, MCP servers might be available only\nfacilities to connect, authenticate, and consume tools from MCP servers\nfrom multiple MCP servers through a simple configuration interface.\nYou‚Äôll learn how to build a practical MCP server, test it, and integrate it\n13.3 Building a Weather MCP Server\nAfter learning about what MCP Servers are, their purpose, and the available\nwith a real-world weather MCP server based on the AccuWeather REST API.\nWe‚Äôll also integrate this server into one of the agent-based solutions built\nserver to your agent.\n13.3.1 Implementing the MCP Server\nWe begin by replacing our previous mock weather tool with a proper MCP\nApp name: accu-mcp-server\nYou are now ready to implement a real MCP server that exposes the weather\nYou can see the MCP server implementation in listing 14.1, adapted from\nListing 13.1 Accuweather MCP server\n@mcp.tool(description=\"Get weather conditions for a location.\") #\nport=8020, path=\"/accu-mcp-server\")",
      "keywords": [
        "MCP servers",
        "MCP",
        "Weather MCP Server",
        "remote MCP",
        "remote MCP servers",
        "consuming MCP servers",
        "MCP server portals",
        "servers",
        "remote MCP tools",
        "MCP tools",
        "public MCP server",
        "tools",
        "building MCP servers",
        "Local MCP Server",
        "Accuweather MCP server"
      ],
      "concepts": [
        "tool",
        "agent",
        "server",
        "locate",
        "location",
        "locations",
        "weather",
        "clients",
        "current",
        "important"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 61,
          "title": "",
          "score": 0.563,
          "base_score": 0.413,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 49,
          "title": "",
          "score": 0.515,
          "base_score": 0.365,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 57,
          "title": "",
          "score": 0.501,
          "base_score": 0.351,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 1,
          "title": "",
          "score": 0.464,
          "base_score": 0.314,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 62,
          "title": "",
          "score": 0.448,
          "base_score": 0.298,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "mcp",
          "servers",
          "mcp servers",
          "mcp server",
          "server"
        ],
        "semantic": [],
        "merged": [
          "mcp",
          "servers",
          "mcp servers",
          "mcp server",
          "server"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.16776969260240165,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.534215+00:00"
      }
    },
    {
      "chapter_number": 49,
      "title": "Segment 49 (pages 425-432)",
      "start_page": 425,
      "end_page": 432,
      "summary": "(env_ch11) C:\\Github\\building-llm-applications\\ch11> pip install \n(env_ch11) C:\\Github\\building-llm-applications\\ch11>cd mcp\n(env_ch11) C:\\Github\\building-llm-applications\\ch11\\mcp>python ac\nInstalling MCP Inspector\nTo get started, you‚Äôll need to install the MCP Inspector on your computer.\nMCP Inspector is a Node.js application, so ensure you have Node.js installed.\nc:\\Github\\building-llm-applications\\ch11\\mcp-inspector>npx @model\nAfter confirmation, the installation will proceed, and MCP Inspector will\nAdding short-term memory with LangGraph checkpoints\nmaking AI agents production-ready: memory and guardrails.\nconversation state using checkpoints, enforce scope restrictions at both the\n14.1 Memory\nMemory allows the system to maintain context between user\nWithout memory, each interaction between the user and the LLM starts fresh,\nmemory: checkpoints.\nIn AI agents, memory can exist at different scopes:\nShort-term memory — Context retained during a single session between\nTypically stored in-memory or in a session-\nLong-term user memory — Persistent across multiple sessions for the\nLong-term application-level memory — Persistent across all users and\nLong-term user and application memory tend to be highly system-specific, so\nmemory.\n14.1.2 Why Short-Term Memory is Needed\nIn a conversational application that uses the tool-calling protocol (as\n3. The application executes these tools and returns results to the LLM.\n14.1.3 Checkpoints in LangGraph\nA checkpoint is a snapshot of the graph’s execution state taken at a specific\nyou can resume from the last successful checkpoint without re-running\nMulti-turn conversational context — For chatbots, checkpoints ensure\nWhat is a Checkpoint?\nA checkpoint represents the saved state of the graph at a given super-step,\nBy saving a checkpoint at each super-step, we can:\nHow Checkpoints Work in LangGraph\n1. Checkpointer — The component responsible for capturing and storing\ncheckpointer records the current graph state, as illustrated in figure 14.1.\nFigure 14.1 Sequence diagram of the travel assistant with checkpoints saved after each node\nexecution, allowing state restoration across turns using a shared thread_id.\nRouter Agent Agents Checkpointer\nSave checkpoint\nSave checkpoint\n(after agent execution)\nSave checkpoint\nSave checkpoint",
      "keywords": [
        "MCP Inspector",
        "mcp",
        "MCP server",
        "memory",
        "Inspector",
        "Äôs API",
        "Github",
        "checkpoint",
        "agent",
        "LLM",
        "launch MCP Inspector",
        "user",
        "Installing MCP Inspector",
        "state",
        "server"
      ],
      "concepts": [
        "checkpoint",
        "memory",
        "agents",
        "conversational",
        "user",
        "application",
        "applications",
        "executes",
        "execution",
        "turns"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 1,
          "title": "",
          "score": 0.686,
          "base_score": 0.536,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 61,
          "title": "",
          "score": 0.552,
          "base_score": 0.402,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "",
          "score": 0.53,
          "base_score": 0.38,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 48,
          "title": "",
          "score": 0.515,
          "base_score": 0.365,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 62,
          "title": "",
          "score": 0.509,
          "base_score": 0.359,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "memory",
          "mcp",
          "checkpoint",
          "inspector",
          "mcp inspector"
        ],
        "semantic": [],
        "merged": [
          "memory",
          "mcp",
          "checkpoint",
          "inspector",
          "mcp inspector"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2964688920425444,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.534266+00:00"
      }
    },
    {
      "chapter_number": 50,
      "title": "Segment 50 (pages 433-440)",
      "start_page": 433,
      "end_page": 440,
      "summary": "LangGraph retrieves the stored state from the last checkpoint in that thread\ninteracts, we only pass the new message as the state to travel_assistant.\nstate = {\"messages\": [HumanMessage(content=user_input)]} \nresult = travel_assistant.invoke(state) #E\nprint(f\"Assistant: {response_msg.content}\\n\") #G\nstate = {\"messages\": [HumanMessage(content=user_input)]}\nresult = travel_assistant.invoke(state)\nListing 14.2 shows the revised chat loop implementation, where the thread ID\nListing 14.2 Revised chat loop implementation with thread_id\nconfig={\"configurable\": {\"thread_id\": thread_id}} #B\nstate = {\"messages\": [HumanMessage(content=user_input)]} \nresult = travel_assistant.invoke(state, config=config) #F\nprint(f\"Assistant: {response_msg.content}\\n\") #H\nresult = travel_assistant.invoke(state, config)\nThe config ensures that all checkpoints and state belong to this specific\nWith the thread ID in place, we can add an in-memory checkpointer to store\nfrom langgraph.checkpoint.memory import InMemorySaver\ntravel_assistant = graph.compile(checkpointer=checkpointer) #I\nuse_previous_response_id=True) #C\n14.1.5 Executing the Checkpointer-Enabled Assistant\nWith the checkpointer integrated into our travel_assistant, we can now\n14.1.6 Rewinding the State to a Past Checkpoint\n3. Rehydrate the graph state to that checkpoint.\nStep 1 — Updating the Chat Loop for State Inspection\nconfig={\"configurable\": {\"thread_id\": thread_id}} #B\nresult = travel_assistant.invoke(question, config=config) #E\nprint(f\"Assistant: {response_msg.content}\\n\") #G\nstate_history = travel_assistant.get_state_history(config) #H\nprint(f'State history: {state_history_list}') #I\nthread_id = last_snapshot.config[\"configurable\"][\"thread_id\"] \nlast_checkpoint_id = last_snapshot.config[\"configurable\"][\"ch\nretrieved_snapshot = travel_assistant.get_state(new_config) #\ntravel_assistant.invoke(None, config=new_config) #P\nresult = travel_assistant.invoke(new_question, config=new_con\nprint(f\"Assistant: {response_msg.content}\\n\") #S\nstate_history = travel_assistant.get_state_history(config)\nprint(f'State history: {state_history_list}')\nlast_snapshot = list(state_history_list)[0]\nExtract the thread_id and checkpoint_id:\nthread_id = last_snapshot.config[\"configurable\"][\"thread_id\"]\nlast_checkpoint_id = last_snapshot.config[\"configurable\"][\"checkp\nretrieved_snapshot = travel_assistant.get_state(new_config)\nYou should see the full conversation history up to that checkpoint, including\nuser messages, tool outputs, and assistant responses.\ntravel_assistant.invoke(None, config=new_config)\nresult = travel_assistant.invoke(new_question, config=new_config)",
      "keywords": [
        "state",
        "thread",
        "Travel",
        "Assistant",
        "history",
        "Chat Loop",
        "config",
        "response",
        "Travel Assistant",
        "checkpoint",
        "Chat",
        "Loop",
        "result",
        "messages",
        "snapshot"
      ],
      "concepts": [
        "checkpoint",
        "state",
        "step",
        "stepping",
        "graph",
        "message",
        "conversational",
        "assistant",
        "listing",
        "history"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 25,
          "title": "",
          "score": 0.623,
          "base_score": 0.623,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 18,
          "title": "",
          "score": 0.514,
          "base_score": 0.514,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 44,
          "title": "",
          "score": 0.441,
          "base_score": 0.441,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 43,
          "title": "",
          "score": 0.433,
          "base_score": 0.433,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 22,
          "title": "",
          "score": 0.409,
          "base_score": 0.409,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "config",
          "travel_assistant",
          "state",
          "thread_id",
          "travel_assistant invoke"
        ],
        "semantic": [],
        "merged": [
          "config",
          "travel_assistant",
          "state",
          "thread_id",
          "travel_assistant invoke"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30224411655362265,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:00:35.534319+00:00"
      }
    },
    {
      "chapter_number": 51,
      "title": "Segment 51 (pages 441-449)",
      "start_page": 441,
      "end_page": 449,
      "summary": "In this section, we’ll integrate custom guardrails into the router node of the\ntravel queries.\nenforce guardrails before any LLM call, ensuring that both the travel and\n14.2.1 Implementing Guardrails to Reject Non-Travel-Related\nThe first and most obvious guardrail for our UK travel information assistant\nListing 14.5 Guardrail policy to restrict questions to travel-related topics\nto determine whether a user’s question is travel-related.\ndedicated guardrail_refusal node in our LangGraph routing structure.\nFigure 14.2 Updated travel assistant workflow: A guardrail checks if the user’s question is in\nIrrelevant queries are routed to guardrail_refusal, which sends them directly to END,\nThe router agent now checks whether a question is in scope.\nis routed to either the travel information agent or the weather agent as before.\nIf it isn’t, the router sends it to the new guardrail_refusal node, which is\nListing 14.6 Adding a guardrail_refusal node to the router graph\ndef guardrail_refusal_node(state: AgentState): #A\ngraph.add_node(\"router_agent\", router_agent_node) \ngraph.add_node(\"travel_info_agent\", travel_info_agent) \ngraph.add_node(\"guardrail_refusal\", guardrail_refusal_node) #B\ngraph.add_edge(\"travel_info_agent\", END) \ngraph.add_edge(\"guardrail_refusal\", END) #C\nAs you can see, the guardrail_refusal node is intentionally a no-op—its\nUpdating the Router Agent\ndecided whether to send a query to the travel or weather agent.\nFigure 14.3 Flowchart of updated router logic with guardrail check: queries first pass a relevance\nfilter, and irrelevant ones are sent with a refusal message directly to the guardrail_refusal node.\nRouter guardrail\nRouter agent\nguardrail_refusal node\ntravel assistant—the router generates a refusal message and routes execution\ndirectly to the guardrail_refusal node, as shown in Figure 14.3.\nListing 14.7 Router agent with guardrail enforcement\ndef router_agent_node(state: AgentState) -> Command[AgentType]:\n\"\"\"Router node: decides which agent should handle the user qu\ngoto=\"guardrail_refusal\",\nreturn Command(update=state, goto=AgentType.travel_info_agent\nThe router first invokes llm_guardrail with the latest user query.\nIf the classification result indicates the question is not travel-related, the\nAssistant: Sorry, I can only help with travel-related questions (\n14.2.2 Implementing More Restrictive Guardrails at Agent\nagent-based systems: each agent should enforce its own input guardrails,\nThese agent-level guardrails are often more restrictive than system-wide\nThe travel information agent can only handle queries about Cornwall,\nWhy have two levels of guardrails?\nRouter-level guardrail — Acts as an early fail-fast filter before any\nAgent-level guardrails — Provide a “belt-and-suspenders” safeguard to\nAGENT_GUARDRAIL_SYSTEM_PROMPT = ( ",
      "keywords": [
        "guardrail",
        "agent",
        "router",
        "travel",
        "REFUSAL",
        "refusal node",
        "node",
        "router agent",
        "Guardrail Policy",
        "user",
        "LLM",
        "Cornwall",
        "travel-related",
        "question",
        "weather"
      ],
      "concepts": [
        "agent",
        "travel",
        "guardrails",
        "graph",
        "checks",
        "query",
        "decision",
        "routing",
        "message",
        "defining"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 52,
          "title": "",
          "score": 0.868,
          "base_score": 0.718,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 47,
          "title": "",
          "score": 0.684,
          "base_score": 0.534,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 46,
          "title": "",
          "score": 0.669,
          "base_score": 0.519,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 42,
          "title": "",
          "score": 0.439,
          "base_score": 0.439,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 18,
          "title": "",
          "score": 0.408,
          "base_score": 0.408,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "guardrail_refusal",
          "router",
          "travel",
          "agent",
          "guardrail"
        ],
        "semantic": [],
        "merged": [
          "guardrail_refusal",
          "router",
          "travel",
          "agent",
          "guardrail"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2395090686071325,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.534365+00:00"
      }
    },
    {
      "chapter_number": 52,
      "title": "Segment 52 (pages 450-457)",
      "start_page": 450,
      "end_page": 457,
      "summary": "Creating the Agent-Level Guardrail Function\nThe agent guardrail is implemented as a Python function that takes the\nListing 14.9 Agent-level guardrail function\ndef pre_model_guardrail(state: dict):\nSystemMessage(content=AGENT_GUARDRAIL_SYSTEM_PROMPT),\ndecision = llm_guardrail.invoke(classifier_messages)\nThe pre_model_guardrail() function works as a pre-processing filter before\nthe LLM sees the user’s query:\nthe guardrail LLM.\nIf the query is in-scope (travel-related and Cornwall-specific), it passes\nOtherwise, the function prepends a refusal instruction so the agent\nInjecting the Guardrail into the Agents\nLangGraph’s ReAct agents support pre-model hooks (pre_model_hook) and\ninput-side guardrails.\nTo enable the Cornwall restriction, we simply pass pre_model_guardrail to\nboth the travel information agent and the accommodation booking agent.\nListing 14.10 Travel information agent with Cornwall guardrail\ntravel_info_agent = create_react_agent(\nprompt=\"You are a helpful assistant that can search travel in\npre_model_hook=pre_model_guardrail, #A\nListing 14.11 Accommodation booking agent with Cornwall guardrail\naccommodation_booking_agent = create_react_agent( #A\npre_model_hook=pre_model_guardrail,\nTesting the Cornwall Guardrail\nllm_guardrail invocation inside pre_model_guardrail().\nUK Travel Assistant (type 'exit' to quit)\n1. A router-level guardrail that quickly rejects any non-travel queries.\n2. Agent-level guardrails that enforce Cornwall-specific scope for travel\nBy this point, you’ve seen how to equip AI agents with memory and\naddress before deploying to real users.\n14.3.1 Long-term user and application memory\nHowever, production agents often benefit from persistent, long-term memory\nacross weeks, months, or even years.\nTable 14.1 summarizes the various types of memory your AI agent-based\nsystem might need to accommodate user needs.\nTable 14.1 Type of memory in AI agents\n(user)\none user\nusers and\nEven a well-designed travel information assistant will encounter situations\nrequests to be escalated to a human travel expert for review before a response\nFor our Cornwall-focused travel assistant, common HITL scenarios could\n14.3.3 Post-model guardrails\nproduction travel information assistant, you may also want post-model\nguardrails that inspect the model’s output before it is shown to the user or\nFor our Cornwall-focused assistant, post-model guardrails could include:\nPost-model guardrails act as a final safety net, catching cases where the\n14.3.4 Evaluation of AI agents and applications\nagent for production is systematic evaluation.\nFor our Cornwall-focused travel information assistant, evaluation might\nFunctional testing – Verifying that the assistant provides correct,\nand maintain the trustworthiness of your travel assistant over time.\nEquipping your agents with memory and guardrails is an important\n14.3.5 Deployment on LangGraph Platform and Open Agent\nThe final step in preparing agents for production is deployment.\nmanaged hosting solution for agentic applications.\nGuardrails maintain scope and safety — They filter or redirect requests\nrejection and agent-level checks for specific data or domain constraints.\nPost-model guardrails act as a final safety net — They verify output",
      "keywords": [
        "Guardrail",
        "agent",
        "travel",
        "assistant",
        "Agent-Level Guardrail Function",
        "LLM",
        "model",
        "Travel Assistant",
        "travel information assistant",
        "Cornwall guardrail",
        "user",
        "Cornwall guardrail travel",
        "Post-model guardrails",
        "travel information agent",
        "memory"
      ],
      "concepts": [
        "agent",
        "travel",
        "guardrail",
        "user",
        "assistant",
        "information",
        "production",
        "state",
        "memory",
        "evaluation"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 51,
          "title": "",
          "score": 0.868,
          "base_score": 0.718,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 47,
          "title": "",
          "score": 0.76,
          "base_score": 0.61,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 46,
          "title": "",
          "score": 0.754,
          "base_score": 0.604,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 42,
          "title": "",
          "score": 0.526,
          "base_score": 0.526,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 45,
          "title": "",
          "score": 0.489,
          "base_score": 0.489,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "guardrail",
          "travel",
          "agent",
          "guardrails",
          "assistant"
        ],
        "semantic": [],
        "merged": [
          "guardrail",
          "travel",
          "agent",
          "guardrails",
          "assistant"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2722312011203671,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.534436+00:00"
      }
    },
    {
      "chapter_number": 53,
      "title": "Segment 53 (pages 458-465)",
      "start_page": 458,
      "end_page": 465,
      "summary": "prompt engineering in a Jupyter Notebook.\nOpenAI model and refining its output through prompt engineering.\n1. Already own or generate an OpenAI key.\n\"Creating an OpenAI key\" and refer to Appendix B for instructions on setting\nrun these examples in an online notebook environment like Google Colab, as\nCreating an OpenAI key\nlike c:\\Github\\building-llm-applications\\ch01 navigate into it, and\nC:\\>cd Github\\building-llm-applications\\ch01\nC:\\Github\\building-llm-applications\\ch01>python -m venv env_ch01\nC:\\Github\\building-llm-applications\\ch01>.\\env_ch01\\Scripts\\activ\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>\nNow install the notebook, langchain (and indirectly openai) packages:\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>pip install n\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>pip install -r\nOnce the installation is complete, start up a Jupyter notebook:\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>jupyter noteb\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>jupyter noteb\nNow, you are ready to execute LangChain code in the notebook.\nimporting the LangChain library and configuring an OpenAI Language\nfrom langchain_openai import ChatOpenAI\nNow add and execute a cell to grab your OpenAI API key (just hit Enter after\nSetting the OpenAI API key with an environment variable\nAnother way to set the OpenAI key is by using an environment variable.\nset OPENAI_API_KEY=your_openai_api_key\nOnce you have entered your OpenAI key, add and execute this cell:\nllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,\nA.1.2 Prompt engineering examples\nA prompt is the instruction you give the LLM to complete a task and\nIt's a key part of any LLM application, so much so that\nresponse = llm.invoke(prompt_input)\nIn Chapter 2 on prompt engineering, you'll learn about prompt templates.\nexample of how to create and execute a prompt from a template, which you\nfrom langchain_core.prompts import PromptTemplate\nresponse = llm.invoke(prompt_input)\nimplemented in LangChain using the plain OpenAI REST API, so you can\nchain = web_scraping | prompt | llm_model | email_text\nFirst, set up the chain in a new notebook cell as follows:\nllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,\nchain = prompt_template | llm",
      "keywords": [
        "OpenAI API key",
        "OpenAI",
        "API",
        "OpenAI REST API",
        "key",
        "Jupyter Notebook environment",
        "OpenAI key",
        "Jupyter Notebook",
        "prompt",
        "Notebook",
        "API key",
        "Github",
        "LLM",
        "REST API",
        "OpenAI API"
      ],
      "concepts": [
        "prompt",
        "llm",
        "notebook",
        "key",
        "keys",
        "chains",
        "langchain",
        "local",
        "locally",
        "content"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 6,
          "title": "",
          "score": 0.883,
          "base_score": 0.733,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 54,
          "title": "",
          "score": 0.817,
          "base_score": 0.667,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 1,
          "title": "",
          "score": 0.551,
          "base_score": 0.551,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 61,
          "title": "",
          "score": 0.503,
          "base_score": 0.503,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 49,
          "title": "",
          "score": 0.466,
          "base_score": 0.466,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "applications ch01",
          "ch01",
          "openai",
          "key",
          "notebook"
        ],
        "semantic": [],
        "merged": [
          "applications ch01",
          "ch01",
          "openai",
          "key",
          "notebook"
        ]
      },
      "topic_id": 6,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2773754857669874,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.534485+00:00"
      }
    },
    {
      "chapter_number": 54,
      "title": "Segment 54 (pages 466-473)",
      "start_page": 466,
      "end_page": 473,
      "summary": "quickly show you what LangChain is, its object model, and how to code with\nNotebook environment\nIf you're just starting with Python Jupyter notebooks, think of it as an\nInstalling the Python interpreter or a Python distribution\nPython installation without the additional libraries and tools bundled\nto manage virtual environments for specific applications without taking\nFor the remainder of this appendix, let's assume you have Python installed (as\nNow, you're ready to set up a virtual environment using venv, which is a tool\nfor managing virtual environments, and to update pip, the Python package\nCreating a virtual environment with venv and upgrading pip\nC:\\Github\\building-llm-applications\\ch01>\nCreate a virtual environment with venv.\nself-contained Python installation, specifically for the ch01 folder you've\nCreate a virtual environment for ch01\nC:\\Github\\building-llm-applications\\ch01>python -m venv env_ch01\nYou've successfully set up a virtual environment named \"env_ch01.\" Now,\nC:\\Github\\building-llm-applications\\ch01>.\\env_ch01\\Scripts\\activ\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>\nBefore proceeding further, it is useful to upgrade pip, the Python package\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>python -m pip \nhttps://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-\nJupyter notebook for executing prompts with OpenAI models.\ninstall Jupyter and the LangChain packages using the following steps:\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>pip install n\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>pip list\nYou can start the Jupyter notebook by executing the following command:\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>jupyter noteb\nC.1 Popular Large Language Models\nC.1.1 OpenAI GPT-4o and GPT-4.1 series\nThe GPT-4o series succeeded GPT-4, offering specialized options for\nGPT-4o was optimized for complex, high-accuracy tasks like\nmulti-step reasoning, while GPT-4o-mini targeted simpler applications such\nThe GPT-4.1 series, released in mid-2025, followed GPT-4o and introduced\nLangChain integrates seamlessly with OpenAI models through the\nlangchain-openai package and also supports open-source models using\nC.1.2 OpenAI o1 and o3 series\nIn September 2024, OpenAI introduced o1, designed for advanced reasoning\nC.1.3 Gemini\nmodels capable of processing text, audio, images, and code.\nGemini 2.0 Flash, offers fast, high-performance capabilities with support for\nDesigned for both advanced and real-time applications, Gemini 2.5 Pro offers\nLangChain offers seamless access to Gemini models through the langchain-\nmodels into their applications effortlessly.\nresearch and tech-stack as Gemini, Gemma offers model weight files at 2B",
      "keywords": [
        "Python",
        "Gemini",
        "virtual environment",
        "Jupyter notebook",
        "Python Jupyter notebooks",
        "Jupyter",
        "Github",
        "Notebook",
        "models",
        "virtual",
        "environment",
        "Anaconda",
        "Gemini Ultra",
        "env",
        "Python interpreter"
      ],
      "concepts": [
        "model",
        "python",
        "gemini",
        "openai",
        "installing",
        "offering",
        "offers",
        "notebook",
        "package",
        "reasoning"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 53,
          "title": "",
          "score": 0.817,
          "base_score": 0.667,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 6,
          "title": "",
          "score": 0.666,
          "base_score": 0.516,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 49,
          "title": "",
          "score": 0.44,
          "base_score": 0.44,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 38,
          "title": "",
          "score": 0.414,
          "base_score": 0.414,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 26,
          "title": "",
          "score": 0.371,
          "base_score": 0.371,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ch01",
          "python",
          "virtual",
          "applications ch01",
          "env_ch01"
        ],
        "semantic": [],
        "merged": [
          "ch01",
          "python",
          "virtual",
          "applications ch01",
          "env_ch01"
        ]
      },
      "topic_id": 6,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.24197953392303675,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.534530+00:00"
      }
    },
    {
      "chapter_number": 55,
      "title": "Segment 55 (pages 474-483)",
      "start_page": 474,
      "end_page": 483,
      "summary": "enhanced accuracy, safety, and versatility across diverse language tasks.\nAnthropic models, including the Claude family, are accessible by installing\nCohere offers models ranging from 6 billion to 52 billion parameters,\nCohere has achieved the highest accuracy among large language models at\nrecognized models from OpenAI.\nYou can access Cohere models from\nMeta's Llama series of large language models has played a major role in\nadvancing open-access AI.\nmodel, Llama quickly evolved into a flexible, open-source platform with\nmodels like Vicuna and Orca.\nand Llama-3, including a 405B-parameter model and the introduction of\nparameters, multimodal, 10M token context), Maverick (400B parameters,\noptimized for reasoning), and Behemoth (a 2T-parameter model still in\nthese models come with licensing constraints for commercial use.\nbased, causal decoder-only model designed for natural language processing\nLLMs. C.1.9 Mistral\nFounded in 2023, Mistral AI has rapidly become a leader in open-source\nlarge language models, known for its efficient and high-performing designs.\nIt introduced the Mistral 7B model, followed by the Mixtral 8x7B and 8x22B\nactivate only a subset of parameters per token to improve cost-efficiency and\nsized model optimized for enterprise use, offering strong performance at\nand open-source approach position it as a major competitor in the AI space,\nQwen, open-sourced on GitHub in August 2023, is a family of LLMs\nThe models range from 1.8 billion to 72 billion\nWhile Qwen models are designed for general purposes, there are fine-tuned\nThe models\nAzure AI Foundry, bringing the models to a broader enterprise audience.\nThe Phi-3 family comprises small language models (SLMs) designed to\nprovide many capabilities of large language models while being more\nThese models outperform others of the same and next size\nup in benchmarks for language, coding, and math tasks, thanks to Microsoft’s\nexceptional performance, rivaling models twice its size, with future releases\nThe models are accessible through Microsoft Azure AI Model\nthe-art open model optimized for reasoning-intensive tasks such as code,\ndeveloped open-source large language models (LLMs) that rival leading\nIts DeepSeek-V3 model, a\nIt outperforms other open-source models and matches top\nmodels are open-source, they avoid politically sensitive topics, raising\nTable C.1 Comparison among LLM models (*sizes with an asterisk are estimated)\nModel\nC.2 How to choose a model\nspecific needs can be a complex task.\nmodels to identify the most suitable one.\nModel Purpose\nModel size\nC.2.1 Model Purpose\nPro are versatile and adaptable models, suitable for a range of tasks, while\nSpecialized models are usually smaller than their more general counterparts.\nMany language models are proprietary, meaning their developers keep the\nProprietary models from providers like OpenAI, Gemini, Cohere, Anthropic\naccessed through REST APIs. Pricing depends on the accuracy of the model\nIn contrast, open-source models offer full transparency, providing access to\ndomain-specific datasets, giving you the flexibility to adapt the model to your\nLangChain supports both proprietary models, such as OpenAI, Gemini, and\nCohere, and open-source models through inference engine wrappers like\nC.2.3 Model size (Number of Parameters)\nThe A model's parameters represent its internal variables, specifically the\ntraining and enable the model to learn patterns from data.\nallow a model to capture greater complexity and nuance, potentially\nHowever, larger models with more parameters require\nCompression techniques can reduce a model's size without major loss of\naccuracy, which we’ll discuss in the chapter on open-source LLMs. Language models vary widely in parameter count, from trillions in GPT-4\nform the foundation of a model’s ability to process text tasks.\nThe number of input tokens allowed in Large Language Models (LLMs)\nlimits vary across models, influencing the complexity of prompts they can\nModels with smaller token limits are well-suited for concise prompts and\nchoices or computational constraints, and such models are typically\ndetailed interactions or extended context, models with larger token limits are\nuse of the model’s capabilities.\nLLMs excel with languages that have extensive training data, like English,\nLarger models with more parameters deliver higher\nSmaller models, with fewer parameters, are faster and better suited for real-\nThe decision between a large or small model\nfavors larger models, while speed-sensitive tasks benefit from smaller ones.\nsource models, have bridged this gap.\nCompact models with lower parameter\ncounts now achieve accuracy comparable to much larger models, making the\nC.2.7 Cost and Hardware Requirements\nCost and hardware requirements are key factors when deploying Large\nLanguage Models (LLMs).\ntechnical considerations to ensure effective use of these models.\ncapabilities result in higher costs, requiring organizations to balance accuracy\nDeploying these models demands\nThe choice between proprietary and open-source LLMs depends on\nProprietary models offer convenience and\nperformance for a fee, while open-source models provide flexibility and cost\nC.2.8 Task suitability (standard benchmarks)\nThe effectiveness of a language model for specific tasks depends on its\narchitecture, size, training data, and fine-tuning.\nDifferent models are\nChatbot Arena Leaderboard offer a centralized way to compare models across\nbenchmark for Large Language Models (LLMs) designed to assess their",
      "keywords": [
        "models",
        "large language models",
        "language models",
        "LLMs",
        "language",
        "Claude",
        "tasks",
        "large language",
        "LLM",
        "parameters",
        "Cohere",
        "open-source",
        "open-source models",
        "token",
        "Proprietary models"
      ],
      "concepts": [
        "models",
        "tasks",
        "tokens",
        "parameters",
        "open",
        "required",
        "requirements",
        "require",
        "llms",
        "language"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 1,
          "title": "",
          "score": 0.684,
          "base_score": 0.534,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 56,
          "title": "",
          "score": 0.683,
          "base_score": 0.533,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 62,
          "title": "",
          "score": 0.651,
          "base_score": 0.501,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 57,
          "title": "",
          "score": 0.632,
          "base_score": 0.482,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "",
          "score": 0.612,
          "base_score": 0.462,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "models",
          "model",
          "language",
          "parameters",
          "open"
        ],
        "semantic": [],
        "merged": [
          "models",
          "model",
          "language",
          "parameters",
          "open"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2960968780059675,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.534582+00:00"
      }
    },
    {
      "chapter_number": 56,
      "title": "Segment 56 (pages 484-491)",
      "start_page": 484,
      "end_page": 491,
      "summary": "designed to test areas where language models have historically\nrequire multi-step reasoning and highlight the limitations of language\nmodels without advanced prompting techniques.\n(CoT) prompting, models like PaLM and Codex demonstrated\nalone underestimates the potential of language models, and CoT\nprompting reveals their advanced reasoning capabilities, particularly as\nmodel scale increases.\nstep-by-step solution, enabling models to learn answer derivations and\nprovided to help models grasp fundamental mathematical concepts.\nDespite improvements, model accuracy on MATH remains low,\nevaluate LLMs on multistep reasoning tasks framed within natural\nmodels and can scale to match future LLM advancements.\nquestions, resulting in a dataset that better evaluates complex reasoning\nCompared to MMLU, MMLU-Pro decreases model\nModels using Chain of Thought (CoT) reasoning\nTable C.2 Benchmark scores (%) of the most popular LLMs\nModel\n72B-\n32B\nMath-7B\n3.3-70B-\n27b-it\nEnsuring the safety and fairness of Large Language Models (LLMs) is\nTo improve safety, LLMs must avoid generating content that is sexist, racist,\nRegularly refining training data and fine-tuning models are necessary steps to\nIn summary, promoting safety and reducing bias in LLMs requires\ncode assistant chatbot to generate Python snippets for stock trading strategies.\nThe code generator must utilize proprietary in-house trading libraries, making\nFigure C.1 Flowchart for selecting an LLM for a Python coding assistant designed to generate\nspecialized model, with a preference for LLMs tailored for code generation, particularly in\nThe next step focuses on selecting open-source models to comply with strict IT policies\nrequiring all proprietary code to remain confidential.\naccuracy, leading to the selection of Qwen2.5 Coder 32b based on its strong performance,\nincluding a high Python HumanEval score for accurate Python code generation.\ncode for proprietary stock trading models\ncoding in Python\nCode Llama 7b Python\nCode Llama 70b Instruct\nCode Llama 70b Python\nQwen2.5 Coder 32b\nmedium model\nSpecialized Models: The focus is on specialized\nLLMs optimized for code generation, particularly for Python.\nmodels are better suited to the task than general-purpose models.\nThis requires selecting an open-source model to\n3. Model Size: While cost is not a concern, the model must balance\nseven LLMs, including general-purpose code generators and Python-\nspecific models.\n4. Accuracy Evaluation: The shortlisted models are evaluated using the\nmodels-leaderboard), which measures Python code generation accuracy.\nQwen2.5-Coder-32b is chosen for its superior balance of accuracy and\nModel\nQwen2.5-Coder-32B-Instruct\nCodeLlama-70b-Python\nCodeLlama-7b-Python\n34b, Deepseek Coder 7b, and Code Llama 7b Python.\nLLMs Large Language Models (LLMs) have revolutionized natural language\nPrivacy and Security: Proprietary LLMs may use prompt data for model\nmitigate this risk, consider deploying open-source models in a private\nSome models\nUnderstanding these challenges ensures you deploy LLMs responsibly",
      "keywords": [
        "models",
        "LLMs",
        "Python",
        "code",
        "language models",
        "reasoning",
        "language",
        "Python code",
        "Code Llama",
        "LLM",
        "Large Language Models",
        "Multitask Language Understanding",
        "accuracy",
        "Python code generation",
        "benchmark"
      ],
      "concepts": [
        "model",
        "llms",
        "reasoning",
        "python",
        "prompt",
        "require",
        "requiring",
        "requirements",
        "biased",
        "biases"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "",
          "score": 0.75,
          "base_score": 0.6,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "",
          "score": 0.683,
          "base_score": 0.533,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 1,
          "title": "",
          "score": 0.571,
          "base_score": 0.421,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 57,
          "title": "",
          "score": 0.555,
          "base_score": 0.405,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 62,
          "title": "",
          "score": 0.543,
          "base_score": 0.393,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "models",
          "code",
          "python",
          "python code",
          "llms"
        ],
        "semantic": [],
        "merged": [
          "models",
          "code",
          "python",
          "python code",
          "llms"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3015822715912434,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.534637+00:00"
      }
    },
    {
      "chapter_number": 57,
      "title": "Segment 57 (pages 492-499)",
      "start_page": 492,
      "end_page": 499,
      "summary": "Open a new command shell and type sqlite3.\nOpen-source LLMs\nAdvantages of open-source LLMs: flexibility, transparency, and control.\nPerformance benchmarks and key features of leading open-source\nquick setup lets you work with state-of-the-art models like GPT-4o, 4o-mini,\nThis chapter introduces open-source LLMs, a practical solution for reducing\nI’ll guide you through the most popular open-source LLM families,\nthese models, ranging from high-performance, advanced setups to user-\nyou built earlier to a local open-source LLM.\nyou’ll understand open-source LLMs well and feel confident using them\nE.1 Benefits of open-source LLMs\nOpen-source Large Language Models (LLMs) offer clear advantages in cost,\ndevelopment fuels innovation, making these models competitive with\nThis section explores the key benefits of open-source\nClosed-source models often function as \"black boxes,\" making them difficult\nOpen-source LLMs, by contrast, are transparent in their architecture, training\nOpen-source LLMs mitigate these concerns by enabling on-premises or\nThese models can be customized to meet privacy needs,\nAs privacy regulations grow stricter, open-source LLMs become increasingly\nOpen-source LLMs, however, benefit from contributions by a\nThe open-source community produces a wide variety of models and training\nDespite these risks, open-source\nChoosing between proprietary and open-source LLMs depends on your\nopen-source models are ideal.\ncritical, proprietary models might be better suited.\nOpen-source LLMs are typically more cost-effective than proprietary models\nownership (TCO) for open-source models is usually lower over the medium\ninfrastructure, transitioning to an open-source model can save money.\nalready have the skills to deploy and manage an open-source LLM, starting\nopen-source model may reduce costs.\nminimal text processing, proprietary models might be more economical.\nAn additional advantage of open-source LLMs is the ability to fine-tune them\nthrough specialized training, creating a custom model.\nIn cases where no proprietary model suits\nyour specific domain, building and fine-tuning an open-source LLM may be\nE.2 Popular open-source LLMs\nThe world of open-source Large Language Models (LLMs) is moving fast,\ntogether key details about popular open-source LLMs into a series of tables,\nof the most interesting open-source LLMs available just before this book was\nTable E.1 Most popular open-source LLMs at the time of publication\nModel\nModel\n72B-\n72B-Instruct\n72B\n32B\n32B\nMath-7B\nMath-7B\n7B\n3.3-70B-\n70B-Instruct\n70B\n141B\n7B-v0.3\n7B-v0.3\n27b-it\n27b-it\nTable E.2 shows the performance of these models based on standard\nTable E.2 Extract from the HuggingFace Open LLM Leaderboard, showing standard\nperformance benchmarks on the most popular open-source LLMs at the time of publication:\nModel\n72B-\nMath-7B\n70B-\nMistral-7B-\n27b-it\nIn that case, Qwen2.5-72B-Instruct is a strong option with an\nNow that you know about popular open-source LLMs and their features, I’ll\nshow you how to run these models on your own computer.\nE.3 Considerations on running open-source LLMs\nLarge language models operate in two main phases: training and inference.\nOpen-source LLM weights are usually\ntransitioning to an open-source LLM later can reduce costs or address privacy",
      "keywords": [
        "Open-source LLMs",
        "Open-source",
        "Popular open-source LLMs",
        "LLMs",
        "models",
        "LLM",
        "popular open-source",
        "open-source models",
        "Installing SQLite",
        "SQLite",
        "Instruct",
        "proprietary",
        "proprietary models",
        "Large Language Models",
        "Savings Open-source LLMs"
      ],
      "concepts": [
        "model",
        "open",
        "data",
        "llm",
        "llms",
        "cost",
        "privacy",
        "proprietary",
        "base",
        "based"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "",
          "score": 0.632,
          "base_score": 0.482,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 1,
          "title": "",
          "score": 0.625,
          "base_score": 0.475,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "",
          "score": 0.568,
          "base_score": 0.418,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 56,
          "title": "",
          "score": 0.555,
          "base_score": 0.405,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 58,
          "title": "",
          "score": 0.548,
          "base_score": 0.398,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "open source",
          "open",
          "source",
          "source llms",
          "llms"
        ],
        "semantic": [],
        "merged": [
          "open source",
          "open",
          "source",
          "source llms",
          "llms"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2490145556241177,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.534684+00:00"
      }
    },
    {
      "chapter_number": 58,
      "title": "Segment 58 (pages 500-510)",
      "start_page": 500,
      "end_page": 510,
      "summary": "If the inference engine supports an OpenAI-compatible API, you\nTechniques like quantization and specialized inference engines reduce the\nFor example, LLAMA-2-7B, originally 13.5GB with 16-bit precision, can be\nGGML or GGUF, or directly through inference engines such as Ollama or\nsignificantly lowered the barrier to running open-source LLMs locally,\nE.3.3 OpenAI REST API compatibility\nMany inference engines include an embedded HTTP server that accepts\nrequests through endpoints compatible with the OpenAI REST API.\nto make requests using raw HTTP (e.g., curl), the OpenAI Python library\n$ curl https://api.openai.com/v1/chat/completions   -H \"Content-T\n\"logprobs\": null,\nTo make the same request to a local open-source LLM, adjust the command\nFor local inference engines, you do not need to include an OpenAI key in the\nadjust two key details in the `curl` request based on the inference engine and\n1. Port Number: Each inference engine’s local HTTP server is set to a\n2. Model Name: Inference engines use specific naming conventions for\nPython openai library\nTo use the OpenAI library for requests, create a virtual environment, install\nListing E.1 Calling the OpenAI completions endpoint\nIn order to run the code above against a local open-source LLM, you need to\nTo adapt the example above for your inference engine and local open-source\nLLM, update the port number to match the engine's local HTTP server and\nFor LangChain, direct its OpenAI wrapper to the local inference engine by\nllm = ChatOpenAI(openai_api_base=f'http://localhost:{port_number}\nEnsure the port matches the local engine configuration.\nmodels on regular consumer hardware: local inference engines.\nE.4 Local inference engines\nRunning an open-source LLM on consumer hardware is most practical with\nan inference engine.\nThese engines host the local model and handle requests\nMany inference engines also\ninclude a local HTTP server with OpenAI REST API-compatible endpoints,\nallowing you to use familiar OpenAI libraries or frameworks like LangChain\nopen-source LLMs and OpenAI’s public service with minimal effort.\nIn the following sections, I will introduce several inference engines,\nbeginning with foundational tools like llama.cpp, optimized for high\nAs shown in Figure E.1, which serves as a guide for this section, llama.cpp\nengines such as GPT4All and LM Studio, are built on llama.cpp.\nFigure E.1 Lineage and functionality of local inference engines\nE.4.1 Llama.cpp\nllama.cpp was one of the first engines designed to run open-source models\nSetting up llama.cpp\nTo use llama.cpp, follow these steps:\n./main -m ./models/mistral-7b-instruct-v0.2.Q2_K.gguf -p \"How \nhttps://github.com/ggerganov/llama.cpp.\nthe llama.cpp GitHub page.\nBefore installation, review both the llama.cpp\npip install llama-cpp-python\nllama.cpp from source using CMake and your system's C compiler.\nAfter setup, you can use Python to interact with your local quantized LLM\nListing E.2 shows an adapted example from the official llama-cpp-\nListing E.2 Executing prompts against a local quantized Mistral-7B-Instruct instance\n### https://github.com/abetlen/llama-cpp-python\nfrom llama_cpp import Llama\nllm = Llama(\nmodel_path=\"./models/mistral-7b-instruct-v0.2.Q2_K.gguf\"\n\"model\": \"./models/mistral-7b-instruct-v0.2.Q2_K.gguf\",\ninstance of Llama-based models, provided you have installed the llama-cpp-\npython library (https://github.com/abetlen/llama-cpp-python):\nllm = LlamaCpp(model_path=\"./models/llama-2-7b-chat.ggmlv3.q2_K.b\nOpenAI API compatible endpoints\nllama.cpp includes a local HTTP server that provides OpenAI REST API-\nWhile detailed installation and setup instructions for llama.cpp are not\ninference engines discussed later are built on llama.cpp or draw inspiration\nUnlike llama.cpp, Ollama does not require",
      "keywords": [
        "inference engines",
        "local inference engines",
        "OpenAI",
        "OpenAI REST API",
        "inference",
        "model",
        "engines",
        "local",
        "API",
        "local inference",
        "LLM",
        "tokens",
        "REST API",
        "quantization",
        "llama.cpp"
      ],
      "concepts": [
        "models",
        "engines",
        "openai",
        "llm",
        "uses",
        "hardware",
        "local",
        "locally",
        "include",
        "including"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 61,
          "title": "",
          "score": 0.82,
          "base_score": 0.67,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 62,
          "title": "",
          "score": 0.696,
          "base_score": 0.546,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 59,
          "title": "",
          "score": 0.611,
          "base_score": 0.461,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 1,
          "title": "",
          "score": 0.587,
          "base_score": 0.437,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "",
          "score": 0.568,
          "base_score": 0.418,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "llama cpp",
          "cpp",
          "llama",
          "inference",
          "local"
        ],
        "semantic": [],
        "merged": [
          "llama cpp",
          "cpp",
          "llama",
          "inference",
          "local"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2925386695547032,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.534732+00:00"
      }
    },
    {
      "chapter_number": 59,
      "title": "Segment 59 (pages 511-519)",
      "start_page": 511,
      "end_page": 519,
      "summary": "Figure 11.2 Screenshot of ollama terminal while installing the Mistral LLM\nOllama natively supports several popular open-source LLMs, including\nOllama’s REST API endpoints are proprietary and not compatible with\n\"model\": \"mistral\", \n\"prompt\":\"How many Greek temples are in Paestum?\"\n\"model\": \"mistral\",\n\"model\": \"mistral\",\n\"model\": \"mistral\",\n\"model\": \"mistral\", \n\"prompt\":\"How many Greek temples are in Paestum?\",\n\"model\": \"mistral\",\n\"response\": \" There are three Doric temples in Paestum, locat\n\"model\": \"mistral\",\nTo get started, install the Ollama Python library:\nYou can then interact with the LLM's /chat endpoint as shown:\nresponse = ollama.chat(model='mistral', messages=[\nollama.generate(model='mistral', prompt=’How many Greek temples a\nTo enable streaming responses, set stream=True in the chat or generate\nresponse = client.chat(model='mistral', messages=[\nmessage = {'role': 'user', 'content': ’How many Greek temples \nresponse = await AsyncClient().chat(model='mistral', messages\nLangChain provides a wrapper for the Ollama Python library, integrating it\nfrom langchain_community.llms import Ollama\nollama = Ollama(model='mistral')\nresponse  = llm.invoke(query)\nOllama is designed to help you explore open-source models locally.\nof inference engines designed to host open-source models of any size on\nE.4.3 vLLM\nvLLM is a high-performance Python library for LLM inference, built on a\nTensor Parallelism: Enables distributed inference across multiple GPUs. Streaming Outputs: Provides real-time response generation.\nmodels, vLLM is optimized for larger model versions, such as LLaMA-2-\nSupported Models section of the vLLM documentation.\n$ python -m vllm.entrypoints.openai.api_server --model mistralai/\nthat you can use with vLLM.\nthe LLM and SamplingParams modules, create a list of prompts, and process\nListing E.3 vLLM offline batched inference\nfrom vllm import LLM, SamplingParams\nllm = LLM(model=\"mistralai/Mistral-7B-v0.1\") #C\noutputs = llm.generate(prompts, sampling_params) #D\nllm_response = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, LLM response: {llm_response!r}\")\nOne of the simplest ways to run an LLM locally is with llamafile.\nto its GitHub description, llamafile allows you to \"distribute and run an LLM\napproach sets an incredibly low barrier for experimenting with local LLMs. The llamafile GitHub page offers a variety of prebuilt LLMs, including recent\nRunning an LLM with llamafile is straightforward.\nchmod +x mistral-7b-instruct-v0.2.Q5_K_M.llamafile\nmistral-7b-instruct-v0.2.Q5_K_M.llamafile.exe\nc:\\temp>mistral-7b-instruct-v0.2.Q5_K_M.llamafile.exe -ngl 999\nFigure E.3 llamafile chat web UI pointing to the local web server communicating with the local",
      "keywords": [
        "Greek temples",
        "LLM",
        "ollama",
        "model",
        "temples",
        "Greek",
        "response",
        "vLLM",
        "REST API",
        "API",
        "Mistral",
        "Greek temples response",
        "Ollama Python library",
        "Paestum",
        "Python"
      ],
      "concepts": [
        "models",
        "llm",
        "vllm",
        "response",
        "included",
        "includes",
        "execution",
        "executable",
        "execute",
        "chat"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 58,
          "title": "",
          "score": 0.611,
          "base_score": 0.461,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 62,
          "title": "",
          "score": 0.592,
          "base_score": 0.442,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 61,
          "title": "",
          "score": 0.566,
          "base_score": 0.416,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "",
          "score": 0.556,
          "base_score": 0.406,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 1,
          "title": "",
          "score": 0.519,
          "base_score": 0.369,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "model mistral",
          "mistral",
          "ollama",
          "vllm",
          "llamafile"
        ],
        "semantic": [],
        "merged": [
          "model mistral",
          "mistral",
          "ollama",
          "vllm",
          "llamafile"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2596300074746744,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.534819+00:00"
      }
    },
    {
      "chapter_number": 60,
      "title": "Segment 60 (pages 520-527)",
      "start_page": 520,
      "end_page": 527,
      "summary": "E.4.5 LM Studio\nFigure E.4 GGUF Search Screen: 1) search for a model using the search box (in our case,\nMistral), 2) select a model variant from the left panel, and download a specific quantized version\nFigure E.5 Chat Screen: Select the chat menu on the left, choose the model from the dropdown at\nFigure E.6 Prompt Response: The time to receive an answer depends on the model, its\n3. Click the Start Server button, as shown in Figure E.7.\nFigure E.7 Activating the Local HTTP Server: click the Server menu on the left, enter (or accept)\n[2024-12-30 13:42:41.709] [INFO] [LM STUDIO SERVER] Verbose serve\n[2024-12-30 13:42:41.721] [INFO] [LM STUDIO SERVER] Success!\n[2024-12-30 13:42:41.721] [INFO] [LM STUDIO SERVER] Supported end\n[2024-12-30 13:42:41.721] [INFO] [LM STUDIO SERVER] ->    GET  ht\n[2024-12-30 13:42:41.722] [INFO] [LM STUDIO SERVER] ->    POST ht\n[2024-12-30 13:42:41.722] [INFO] [LM STUDIO SERVER] ->    POST ht\n[2024-12-30 13:42:41.722] [INFO] [LM STUDIO SERVER] ->    POST ht\n[2024-12-30 13:42:41.723] [INFO] [LM STUDIO SERVER] Model loaded: \n[2024-12-30 13:42:41.723] [INFO] [LM STUDIO SERVER] Logs are save\n[2024-12-30 13:46:12.783] [INFO] [LM STUDIO SERVER] Context Overf\n[2024-12-30 13:46:12.783] [INFO] [LM STUDIO SERVER] Streaming res\n[2024-12-30 13:46:20.630] [INFO] [LM STUDIO SERVER] First token g",
      "keywords": [
        "STUDIO SERVER",
        "server",
        "Studio",
        "INFO",
        "Ollama previously",
        "entered into Ollama",
        "Greek temples",
        "model",
        "local HTTP server",
        "chat screen",
        "Start Server button",
        "HTTP server",
        "chat",
        "REST API",
        "Start Server"
      ],
      "concepts": [
        "server",
        "info",
        "models",
        "panel",
        "chat",
        "post",
        "stream",
        "openai",
        "port",
        "searching"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 59,
          "title": "",
          "score": 0.502,
          "base_score": 0.352,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 61,
          "title": "",
          "score": 0.473,
          "base_score": 0.323,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 58,
          "title": "",
          "score": 0.441,
          "base_score": 0.291,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 49,
          "title": "",
          "score": 0.414,
          "base_score": 0.264,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 62,
          "title": "",
          "score": 0.401,
          "base_score": 0.251,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "server",
          "studio server",
          "12 30",
          "info lm",
          "30 13"
        ],
        "semantic": [],
        "merged": [
          "server",
          "studio server",
          "12 30",
          "info lm",
          "30 13"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.15205093200985365,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.534863+00:00"
      }
    },
    {
      "chapter_number": 61,
      "title": "Segment 61 (pages 528-535)",
      "start_page": 528,
      "end_page": 535,
      "summary": "OpenAI-like REST API calls to back-end engines like llama.cpp or vLLM.\nFigure E.1: LocalAI Architecture: LocalAI routes OpenAI-like REST API calls to inference\nengines like llama.cpp, vLLM, or other custom backends.\nmodels are automatically downloaded when starting the container.\nmodel and hardware configuration.\nlocal folder (e.g., local_models) and reference it when starting the container:\ndocker run -p 8080:8080 -v $PWD/local_models:/local_models -ti --\nWhen a model is started using Docker, LocalAI launches an HTTP server on\nexamples of curl, Python, and LangChain code you can use with LocalAI.\nEnsure the port is set to 8080 and verify the model name in the LocalAI\nE.4.7 GPT4All\nGPT4All is an inference engine built on llama.cpp that allows you to run\nGGUF quantized LLM models on consumer hardware, including CPUs and\nInstallation packages for Windows, macOS, and Ubuntu Linux are available\ndownload a model, chat with it and also upload your documents to enable\nFigure E.2 GPT4All desktop application home screen\n1. Backend Inference Engine: Built on llama.cpp, the engine supports\nGGUF quantized LLM models (typically under 8GB) for architectures\nthe inference engine.\ndropdown or added from the Model Explorer on the GPT4All homepage\nTo enable the local HTTP server for REST API access (port 4891), navigate\nto GPT4All > Settings and enable the web server option.\nport is set to 4891, and verify the correct model name in the GPT4All\nGPT4All Python bindings\nYou can create a Python client using the GPT4All Python generation API.\npip install gpt4all\nmodel = GPT4All('mistral-7b-instruct-v0.1.Q4_0.gguf')\nLangChain GPT4all python library\nLangChain integrates with the native GPT4All Python bindings, providing an\nFirst, install the gpt4all package:\npip install gpt4all\nNext, test the GPT4All LangChain integration by running the code in Listing\nBefore running the code, ensure you download a GGUF quantized model of\nyour choice into the specified local directory: open the GPT4All application\nListing E.1 Connecting to a local open-source model via LangChain’s GPT4All Wrapper\nfrom langchain_community.llms import GPT4All\nllm = GPT4All(model=model_path)\nThis code demonstrates how to access a local open-source model through\nGPT4All using LangChain’s wrapper.\nE.4.8 Comparing local inference engines\nTable E.3 Summary of the characteristics of local LLM inference engines\nGPT4All\nengines to run open-source LLMs. However, inference engines are not the\nE.4.9 Choosing a local inference engine\nIf you are new to local LLM inference engines, start with a simple option like",
      "keywords": [
        "REST API",
        "local",
        "Python",
        "model",
        "API",
        "LocalAI",
        "inference",
        "inference engines",
        "REST API calls",
        "OpenAI-like REST API",
        "LLM",
        "local inference engines",
        "OpenAI REST API",
        "Docker",
        "REST"
      ],
      "concepts": [
        "models",
        "local",
        "python",
        "localai",
        "docker",
        "llm",
        "api",
        "llama",
        "option",
        "optional"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 58,
          "title": "",
          "score": 0.82,
          "base_score": 0.67,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 62,
          "title": "",
          "score": 0.69,
          "base_score": 0.54,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 59,
          "title": "",
          "score": 0.566,
          "base_score": 0.416,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 48,
          "title": "",
          "score": 0.563,
          "base_score": 0.413,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 1,
          "title": "",
          "score": 0.555,
          "base_score": 0.405,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "gpt4all",
          "inference",
          "local",
          "engines",
          "localai"
        ],
        "semantic": [],
        "merged": [
          "gpt4all",
          "inference",
          "local",
          "engines",
          "localai"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.24853267692279207,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.534927+00:00"
      }
    },
    {
      "chapter_number": 62,
      "title": "Segment 62 (pages 536-543)",
      "start_page": 536,
      "end_page": 543,
      "summary": "E.5 Inference via the HuggingFace Transformers\ncan run a pre-trained model using the Hugging Face Transformers library.\nE.5.1 Hugging Face Transformers library\nOnce installed, you can interact with pre-trained models stored locally.\nListing E.4 Performing inference through the Hugging Face transformers library\noutputs = model.generate(**inputs, max_new_tokens=20) #D\nE.5.2 LangChain’s HuggingFace Pipeline\nLangChain offers a wrapper for the Hugging Face Transformers pipeline,\nHugging Face with LangChain, you can implement the code shown in listing\nListing E.5 HuggingFace transformers via LangChain\nfrom langchain_community.llms.huggingface_pipeline import Hugging\nllm_chain = prompt | hf #E\nNow that I’ve covered how to run an open-source LLM locally, the next step\nOpenAI endpoints to a local open-source LLM like Mistral.\nE.6 Building a local summarization engine\noriginal OpenAI-based solution with a local open-source LLM.\nKeep in mind that processing with a local LLM will take significantly longer\nusing a local open-source LLM.\nE.6.1 Choosing the inference engine\ncompatible REST API endpoints provided by both GPT4All and LM Studio.\nE.6.2 Starting up the OpenAI compatible server\nLM Studio will now accept OpenAI-compatible requests.\nonly need to make a small change in the llm_models.py file, shown in\nListing E.6 Original llm_models.py code\nfrom langchain.llms.openai import OpenAI\nreturn OpenAI(openai_api_key=openai_api_key, model=\"gpt-4o-mi\nListing E.7 Modified llm_models.py code, using a local open-source LLM\nyou could make get_llm() configurable, as shown in Listing E.8.\nListing E.8 configurable get_llm()\nif is_llm_local:\nclient = OpenAI(openai_api_key=openai_api_key, model=\"gpt\nThis version allows you to switch between OpenAI and local LLMs. Keep in\nmind that some inference engines have fixed port numbers or specific model\nE.6.4 Running the summarization engine through the local\nLLM\nE.6.5 Comparison between OpenAI and local LLM\nWhen running the summarization engine with a local LLM, you’ll notice:\n2. Performance: Local LLM inference is slower, especially on CPUs. Without a GPU, processing times can be significantly longer.\nThis suggests that while a quantized local open-source LLM is sufficient for\nOpen-source LLMs provide potential cost savings, privacy control, and\nfeatures make open-source LLMs an appealing option for both\nComparative tables simplify the evaluation of open-source LLMs,\nOpen-source LLM weights enable local inference with accessible tools\nmodels, progress to tools like Ollama or LM Studio for ease of use, and\nApplications built for OpenAI can be adapted to open-source LLMs\nOpenAI APIs, delve into prompt engineering, and design applications using\nan open-source LLM application framework, significantly accelerated my\ntopics from running open source LLMs locally to advanced RAG techniques\nOpen-source LLMs",
      "keywords": [
        "Hugging Face Transformers",
        "Hugging Face",
        "llm",
        "local open-source LLM",
        "Face Transformers library",
        "open-source LLM",
        "Face Transformers",
        "OpenAI",
        "Transformers",
        "LLMs",
        "Transformers library",
        "local",
        "local LLM",
        "open-source",
        "Hugging"
      ],
      "concepts": [
        "model",
        "llm",
        "openai",
        "llms",
        "inference",
        "open",
        "code",
        "applications",
        "application",
        "transformers"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 1,
          "title": "",
          "score": 0.706,
          "base_score": 0.556,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 58,
          "title": "",
          "score": 0.696,
          "base_score": 0.546,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 61,
          "title": "",
          "score": 0.69,
          "base_score": 0.54,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 55,
          "title": "",
          "score": 0.651,
          "base_score": 0.501,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "",
          "score": 0.63,
          "base_score": 0.48,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "open source",
          "local",
          "transformers",
          "open",
          "source"
        ],
        "semantic": [],
        "merged": [
          "open source",
          "local",
          "transformers",
          "open",
          "source"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30276576703716424,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:35.534979+00:00"
      }
    }
  ],
  "total_chapters": 62,
  "enrichment_provenance": {
    "taxonomy_id": "none",
    "taxonomy_version": "none",
    "taxonomy_path": "none",
    "taxonomy_checksum": "sha256:none",
    "source_metadata_file": "AI Agents and Applications_metadata.json",
    "enrichment_date": "2025-12-17T23:00:35.554582+00:00",
    "enrichment_method": "msep",
    "model_version": "ai-agents-msep-v1",
    "processing_time_ms": 5273.332585999015,
    "total_similar_chapters": 308
  }
}