{
  "metadata": {
    "title": "AI Agents and Applications",
    "author": "welcome.html",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 543,
    "conversion_date": "2025-11-28T12:01:53.474530",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "AI Agents and Applications.pdf",
    "extraction_method": "PyMuPDF (Direct: 498, OCR: 45)"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "detection_method": "topic_boundary",
      "content": "tS andy\n\nRoberto Infante\n\nMEAP.\n\nEd MHANWING\n\nWith LangChain, LangGraph, and MCP\n\nRoberto Infante\n\ni HANHIHG\n\nAI Agents and Applications\n1. welcome\n2. 1_Introduction_to_AI_Agents_and_Applications\n3. 2_Executing_prompts_programmatically\n4. 3_Summarizing_text_using_LangChain\n5. 4_Building_a_research_summarization_engine\n6. 5_Agentic_Workflows_with_LangGraph\n7. 6_RAG_fundamentals_with_Chroma_DB\n8. 7_Q&A_chatbots_with_LangChain_and_LangSmith\n9. 8_Advanced_indexing\n10. 9_Question_transformations\n11. 10_Query_generation,_routing_and_retrieval_post-processing\n12. 11_Building_Tool-based_Agents_with_LangGraph\n13. 12_Multi-agent_Systems\n14. 13_Building_and_consuming_MCP_servers\n15. 14_Productionizing_AI_Agents:_memory,_guardrails,_and_beyond\n16. Appendix_A._Trying_out_LangChain\n17. Appendix_B._Setting_up_a_Jupyter_Notebook_environment\n18. Appendix_C._Choosing_an_LLM\n19. Appendix_D._Installing_SQLite_on_Windows\n20. Appendix_E._Open-source_LLMs\n21. index\n\n\nwelcome\nDear Reader,\nThank you so much for purchasing \"AI Agents and Applications\". Your\ninterest in this cutting-edge topic means a great deal to me, and I am excited\nto guide you through the expansive world of large language models (LLMs).\nThis book emerged from my professional journey as a software developer,\nwhere my main focus has been quantitative development in finance. Over the\nyears, I occasionally delved into AI projects aimed at tackling problems\nwhere traditional mathematical computations fell short. My fascination with\nLLMs began when I encountered ChatGPT in November 2022. Its\ncapabilities inspired me to explore further, leading me to experiment with the\nOpenAI APIs, delve into prompt engineering, and design applications using\nRetrieval Augmented Generation (RAG). My involvement with LangChain,\nan open-source LLM application framework, significantly accelerated my\nlearning and allowed me to witness its rapid adoption within the community.\n\"AI Agents and Applications\" is structured to cater both to beginners and\nseasoned professionals. It offers a comprehensive look into the foundational\ntechnologies and advanced techniques in the realm of LLMs. Whether you\nare new to programming or an experienced developer, you will find the\ncontent approachable yet enriching, especially with practical code examples\nto enhance your engagement.\nThe insights in this book are drawn from my real-world experiences and\ncontinuous learning in a field that evolves almost daily. I cover a range of\ntopics from running open source LLMs locally to advanced RAG techniques\nand fine-tuning methods. My goal is to provide a resource that I wish had\nbeen available to me—an accessible, practical guide that empowers you to\nnavigate and excel in this emerging domain.\nAs this field is still in its infancy, your feedback is incredibly valuable. I\nencourage you to actively participate in our online discussion forum at\n\n\nliveBook. Your questions, comments, and suggestions not only help improve\nthis book but also contribute to the broader community exploring LLM\napplications.\nThank you once again for joining me on this journey. I am eager to hear\nabout your experiences and look forward to any insights you might share.\nWarm regards,\n— Roberto Infante\nIn this book\nwelcome 1 Introduction to AI Agents and Applications 2 Executing prompts\nprogrammatically 3 Summarizing text using LangChain 4 Building a research\nsummarization engine 5 Agentic Workflows with LangGraph 6 RAG\nfundamentals with Chroma DB 7 Q&A chatbots with LangChain and\nLangSmith 8 Advanced indexing 9 Question transformations 10 Query\ngeneration, routing and retrieval post-processing 11 Building Tool-based\nAgents with LangGraph 12 Multi-agent Systems 13 Building and consuming\nMCP servers 14 Productionizing AI Agents: memory, guardrails, and beyond\nAppendix A. Trying out LangChain Appendix B. Setting up a Jupyter\nNotebook environment Appendix C. Choosing an LLM Appendix D.\nInstalling SQLite on Windows Appendix E. Open-source LLMs\n\n\n1 Introduction to AI Agents and\nApplications\nThis chapter covers\nCore challenges in building LLM-powered applications\nLangChain’s modular architecture and components\nPatterns for engines, chatbots, and agents\nFoundations of prompt engineering and RAG\nAfter the launch of ChatGPT in late 2022, developers quickly began\nexperimenting with applications powered by large language models (LLMs).\nSince then, LLMs have moved from novelty to necessity, becoming a new\nstaple in the application development toolbox—much like databases or web\ninterfaces in earlier eras. They give applications the ability to answer\ncomplex questions, generate tailored content, summarize long documents,\nand coordinate actions across systems. More importantly, LLMs have\nunlocked a new class of applications: AI agents. Agents take input in natural\nlanguage, decide which tools or services to call, orchestrate multi-step\nworkflows, and return results in a clear, human-friendly format.\nYou could build such systems entirely from scratch, but you would quickly\nencounter recurring challenges: how to ingest and manage data, how to\nstructure prompts, how to chain model calls reliably, and how to integrate\nexternal APIs and services. Reinventing these patterns each time is slow and\nerror-prone. This is where frameworks come in. LangChain, LangGraph, and\nLangSmith provide modular building blocks that eliminate boilerplate,\npromote best practices, and let you focus on application logic instead of low-\nlevel wiring. Throughout this book, we will use these frameworks as practical\ntools—not as the goal, but as the means—to learn how to design, build, and\nscale real LLM-based applications and agents.\nIn this opening chapter, we will set the foundation. You’ll get an overview of\nthe main problems LLM applications aim to solve, explore the architecture\n\n\nand object model that LangChain provides, and examine the three primary\nfamilies of LLM-powered applications: engines, chatbots, and AI agents. We\nwill also introduce the key patterns—such as Retrieval-Augmented\nGeneration (RAG) and prompt engineering—that you’ll return to throughout\nthe book. By the end of this chapter, you will have a clear picture of the\nchallenges in building LLM applications, the patterns that solve them, and the\nframeworks we will use to bring those solutions to life.\n1.1 Introducing LangChain\nImagine you’ve been asked to build a chatbot that can answer customer\nquestions from your company’s documentation, or a search engine that\nretrieves precise answers from thousands of internal reports. You quickly\ndiscover the same pain points:\nHow do you get your own data into the model reliably, without pasting\nentire documents into prompts?\nHow do you keep prompts, chains, and integrations maintainable as your\nfeatures grow?\nHow do you handle context limits and costs while still preserving\naccuracy?\nHow do you orchestrate multi-step workflows and API calls without\nfragile glue code?\nAnd once your app is live, how do you evaluate, debug, and monitor its\nbehavior?\nWithout a framework, developers end up re-implementing the same plumbing\n—load data, split it, embed it, store it, retrieve it, prompt it—again and again.\nIt works for a demo, but it doesn’t scale. LangChain addresses these\nproblems by standardizing common patterns into modular, composable\ncomponents. With loaders, splitters, embeddings, retrievers, vector store\nretrievers, and prompt templates, you can focus on application logic instead\nof boilerplate. The LangChain Expression Language (LCEL) and the\nRunnable interface then let you chain these pieces together consistently,\nmaking pipelines easier to build, debug, and maintain.\nLangChain has also evolved rapidly, fueled by an active open-source\n\n\ncommunity. It continues to keep pace with advances in LLM architectures,\nnew data sources, and retrieval technologies, while helping to establish\nshared best practices for building and deploying LLM-based systems.\nThree principles guide its design: modularity, composability, and\nextensibility. Components follow standard interfaces, so you can swap an\nLLM, change a vector store, or add a new data connector without rewriting\nyour entire application. Real-world tasks can be composed from multiple\ncomponents, forming chains or agent workflows that dynamically select the\nright tools for the job. And while default implementations exist, you can\nalways extend or replace them with custom logic or third-party integrations,\navoiding lock-in and promoting interoperability.\nBy learning LangChain, you not only gain the ability to build production-\ngrade LLM applications, but also acquire transferable skills. Competing\nframeworks solve similar problems in similar ways, so once you understand\nthese patterns, you can adapt them to whatever stack you choose in the future.\n1.1.1 LangChain architecture\nLangChain’s documentation is thorough, but the best way to really\nunderstand how it works is by building with it. This section gives you a high-\nlevel overview that we’ll keep coming back to in later chapters. Think of it as\nthe map you’ll use while we dive into the details and code examples later on.\nMost LLM frameworks follow a similar overall pattern, but each one defines\nits own components. In LangChain, the workflow looks roughly like what\nyou see in Figure 1.1. You start by pulling in text from different sources—\nfiles, databases, or websites—and wrapping it into Document objects. Those\ndocuments are often split into smaller chunks so they’re easier to handle.\nNext, each chunk is passed through an embedding model, which turns the\ntext into vectors that capture its meaning. Both the raw chunks and their\nembeddings are stored in a vector store, which lets you quickly retrieve the\nmost relevant pieces of text based on similarity search.\nFigure 1.1 LangChain architecture: The Document Loader imports data, which the Text Splitter\ndivides into chunks. These are vectorized by an Embedding Model, stored in a Vector Store, and\nretrieved through a Retriever for the LLM. The LLM Cache checks for prior requests to return\n",
      "page_number": 1,
      "chapter_number": 1,
      "summary": "This chapter covers segment 1 (pages 1-8). Key topics include langchain, applications, and developer.",
      "keywords": [
        "Roberto Infante MEAP",
        "MCP Roberto Infante",
        "andy Roberto Infante",
        "Roberto Infante",
        "Infante MEAP",
        "Agents",
        "Applications",
        "LLM applications",
        "LangChain",
        "LLM",
        "building LLM applications",
        "LLMs",
        "Appendix",
        "Building",
        "open-source LLM application"
      ],
      "concepts": [
        "langchain",
        "applications",
        "developer",
        "appendix",
        "building",
        "data",
        "framework",
        "open",
        "model",
        "retrieval"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 55,
          "title": "Segment 55 (pages 465-472)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 57,
          "title": "Segment 57 (pages 483-490)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "Segment 28 (pages 232-240)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.61,
          "method": "api"
        }
      ]
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 9-16)",
      "start_page": 9,
      "end_page": 16,
      "detection_method": "topic_boundary",
      "content": "cached responses, while the Output Parser formats the LLM's final response.\n\n\ndatabases and web pages\n\nLLM applications process unstructured\ntext coming from a variety of sources,\nsuch as document and text files,\n\nA Splitter splits\nthe raw text into txt PDF. docx, em!\nalist of smaller unstructured text fles\n\nDatabases Websites CSV, JSON, XML\nstructured files\n\ndocuments An Output Parser\nCalled chunks transforms the LLM\nmo _—— response into a\neasily than the The Document say CY mt\noriginal text Loader parses SON\nextemal text content\ninto a typed\n\nDocument object\n\nLarge Language\nModel (LLM) or\nchat model\n\nDocument\nchunks\n\nA prompt encapsulates\nthe question asked to\nthe LLM and the context\nrepresented by chunks\nretrieved from a vector\nstore or a knowledge\ngraph database\n\nAn embedding model\ntransforms chunks into\nvectors representing the\nsemantic meaning of the\nunderlying information\n\nWhen an LLM app runs a task—say summarization or semantic search—it\nbuilds a prompt that combines the user’s question with extra context. That\ncontext usually comes from document chunks pulled out of a vector store.\nSometimes, though, you’ll also want to bring in information from a graph\ndatabase. Vector stores are still the backbone of most retrieval-augmented\ngeneration (RAG) workflows, but graph databases are becoming more\ncommon in apps that need to represent and reason about relationships\nbetween entities. LangChain already integrates with popular options like\nNeo4j, and it lets you use graph-based memory or planning components—\nfeatures that are showing up more and more in advanced agent architectures.\nTo make all of this easier to wire together, LangChain introduced the\nRunnable interface and the LangChain Expression Language (LCEL). These\ngive you a clean, consistent way to chain components without writing piles of\nglue code. We’ll go deeper into both later in this chapter.\nIt’s also worth keeping in mind that LangChain’s workflow isn’t locked into\na simple pipeline. You can arrange components as graphs to handle more\ncomplex, branching flows—a capability formalized in the LangGraph\nmodule. We’ll dig into these patterns and architectural variations in the next\nsection, where the bigger picture of LLM application design comes into\nfocus.\nA detailed description of each component follows below (it references the\nnumbering in figure 1.1):\nDocument loaders (1): in LangChain, document loaders play a pivotal\nrole by extracting data from various sources and transforming it into\nDocument objects. These Documents serve as the fundamental entities\nprocessed within the system.\nText splitters (2): As we'll discuss in upcoming chapters, text splitters\nare crucial for breaking down text from a data source into smaller\nDocument instances or \"chunks.\" This approach helps overcome the\nlimitation of the maximum prompt size, also known as “context\nwindow”. It's especially relevant in the ingestion phase of the Question\n& Answer use case, where the original text is split before being indexed\nwith embeddings and stored in the vector database.\n\n\nDocument: In LangChain, a Document is a fundamental data structure that\nencapsulates both content and metadata. It represents a unit of text—\nstructured or unstructured—along with relevant contextual information.\nFor example, the text extracted from a PDF file can be wrapped in a\nDocument object, with metadata storing details like the source file name\nor page number. When working with larger texts, a Text Splitter can\ndivide them into smaller segments, each represented as an individual\nDocument, making it easier to process them in chunks.\nEmbedding models (3): LangChain provides support for the most\npopular embedding models through convenient wrappers. These models\nare designed to capture the semantic meaning of text and convert it into\nnumerical vectors.\nVector stores (4): vector stores operate as specialized databases designed\nfor the efficient retrieval of Document objects. These objects typically\nrepresent fragments or \"chunks\" of the original document, indexed based\non their associated embeddings or vector-based semantic\nrepresentations. This indexing allows for search queries to match the\nembeddings of the chunks against those of the query, retrieving the most\nsimilar embeddings and their related chunks. By ingesting Documents\nalong with their associated embeddings, the vector store can serve as an\noffline knowledge base for proprietary data. LangChain is compatible\nwith various popular vector stores.\nKnowledge Graph databases: Although not a key component of the\narchitecture, LangChain offers client wrappers for leading graph\ndatabases to facilitate Knowledge Graph functionality. These databases\nstore entities and their relationships in a graph form.\nRetrievers (5): In LangChain, retrievers efficiently fetch data, often a list\nof Documents containing unstructured text, from indexed databases like\nvector stores. LangChain provides support for various retrievers,\ncatering not only to vector stores but also to relational databases and\nmore complex data stores, such as knowledge graph databases like\nNeo4j.\nPrompts (6): LangChain provides tools for defining prompt templates—\nreusable structures that can be dynamically filled with input to create\nprompt instances. These inputs may come from users or external sources\nsuch as vector stores. Once a prompt instance is fully constructed, it is\nsent to the Language Model (LLM) for processing. To improve prompt\n\n\nquality and guide the model’s behavior, you can enhance templates by\nincluding examples, a technique often used in few-shot learning.\nLLM Cache (7): An optional component that improves performance and\nreduces costs by minimizing calls to the underlying LLM. It provides\nimmediate answers to previously asked questions.\nLLM / ChatModel (8): This component acts as an interface to an LLM\nor a chat model built on an LLM. LangChain supports multiple LLMs\nand associated chat models, including those from OpenAI, Cohere, and\nHuggingFace. Additionally, LangChain supports a Fake LLM for unit\ntesting purposes.\nOutput Parser (9): This component transforms an LLM's natural\nlanguage response into a structured format, such as JSON, making it\neasier for the LLM application to process.\nThe components above can be organized into a chain or structured around\nagents:\nChain: A composite arrangement guiding LangChain's processing\nworkflow, customized for specific use cases and based on a sequence of\nthe described components.\nAgent: This component manages a dynamic workflow, extending a\nsequential chain. The agent's processing is flexible and can adapt based\non user input or component output. Resources in the dynamic workflow\nare called \"tools\" or \"plugins\" in most frameworks' documentation, and\nthe collection of all tools is referred to as the \"toolkit.\"\nLangChain's comprehensive design supports three primary application types,\nwhich we’ll examine in detail shortly: summarization and query services,\nchatbots, and agents. Although the framework can seem complex at first\nglance, the detailed explanations in upcoming chapters will break down each\ncomponent and clear up any uncertainties.\n1.2 LangChain core object model\nWith a good understanding of LangChain’s high-level architecture, we can\nnow turn to its core object model for a clearer picture of how the framework\noperates. Gaining familiarity with the key objects and their interactions will\n\n\nsignificantly improve your ability to use LangChain effectively.\nThe object model is organized into class hierarchies, beginning with abstract\nbase classes from which multiple concrete implementations are derived.\nFigure 1.2 provides a visual overview of the main class families used across\nvarious tasks, all centered around the Document entity. It illustrates how\nloaders generate Document objects, how splitters divide them into smaller\nsegments, and how these are then passed into vector stores and retrievers for\ndownstream processing.\nAs covered in section 1.1.1 on LangChain architecture, the primary classes\ninclude:\nDocument\nDocumentLoader\nTextSplitter\nVectorStore\nRetriever\nLangChain integrates with a wide variety of third-party tools and services\nacross these components, offering great flexibility. You can find a full list of\nsupported integrations in the official documentation. In addition, LangChain\nprovides the LangChain Hub, a community-driven repository for discovering\nand sharing reusable components such as prompts, chains, and tools.\nA key architectural feature shared by many of these components—from\nloaders to LLMs—is the Runnable interface. This common interface allows\nobjects to be composed and chained together consistently, enabling highly\nmodular workflows. We'll dive deeper into this feature, and into the\nLangChain Expression Language (LCEL), in a later section focused on\nbuilding composable and expressive LLM pipelines.\nFigure 1.2 Object model of classes associated with the Document core entity, including Document\nloaders (which create Document objects), splitters (which create a list of Document objects),\nvector stores (which store Document objects in vector stores) and retrievers (which retrieve\nDocument objects from vector stores and other sources)\n\n\nDocument loaders\nparse external text\ncontent into a typed\nDocument object\n\nDocument transformers\ntransform a Document\ninto one or more\nDocument objects. For\nexample, a Splitter splits :\nthe raw text into a list of +\nsmaller documents called\nchunks thatcanbe\nprocessed more easily\nthan the original text\n\nA Document, considered\nthe core entity in\nLangChain, encapsulates\nboth the content and\nmetadata of unstructured\nor structured data\n\nA vector store contains\n\nDocuments (and text\nchunks) and the related\nvector representations\nused for semantic\nsearches\n\n: listof\nobjects»\n\nceeeeee\n\nA Retriever efficiently fetch\ndata, often a list of\nDocuments containing\nunstructured text, from\nindexed databases like\nvector stores, but also\nfrom other sources\n\n«create\n\neeeeeee\n\nIn figure 1.3, you can see the object model related to Language Models,\nincluding PromptTemplate and PromptValue. This figure illustrates how\nthese classes connect to the LLM interface, exposing a somewhat more\ncomplex hierarchy than the classes presented earlier.\nFigure 1.3 Object model of classes associated with Language Models, including Prompt\nTemplates and Prompt Values\n",
      "page_number": 9,
      "chapter_number": 2,
      "summary": "We’ll go deeper into both later in this chapter Key topics include document, documents, and langchain. databases and web pages\n\nLLM applications process unstructured\ntext coming from a variety of sources,\nsuch as document and text files,\n\nA Splitter splits\nthe raw text into txt PDF.",
      "keywords": [
        "document",
        "Document objects",
        "LLM",
        "Vector stores",
        "LangChain",
        "text",
        "vector",
        "object model",
        "objects",
        "documents",
        "Model",
        "stores",
        "Document loaders",
        "Document object Large",
        "databases"
      ],
      "concepts": [
        "document",
        "documents",
        "langchain",
        "llm",
        "text",
        "components",
        "prompt",
        "object",
        "databases",
        "stores"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 7,
          "title": "Segment 7 (pages 52-61)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 19,
          "title": "Segment 19 (pages 157-164)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 25,
          "title": "Segment 25 (pages 216-223)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.62,
          "method": "api"
        }
      ]
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 17-24)",
      "start_page": 17,
      "end_page": 24,
      "detection_method": "topic_boundary",
      "content": "A PromptTemplate is A MessagePromptTempiate\ndesigned to generate a iS designed to generate a\nPromptValue instance PromptValue instance from a\n\nfrom template structures template structure including\nand input parameters messages from various roles\n(human, system, Al) and\ninput parameters\n«abstract»\nBaseLanguageModel\nALLM model is suitable 1 AChat model is suitable\nfor one-off instructions from a chatbox which\ninvoked by a client query abstract» «abstracts accepts interactive\nengine BaseChatModel instructions and keeps an\n: . history for reference\n\n«abstracts\nLLM\n\nChatOpenAl\n\n(many others ...) SimpleChatModel\n\n(many others ...)\n\ni\n\n(many others ...)\n\nNow that you've reviewed LangChain's architecture and object model, let's\npause and explore the types of applications you can build with it.\n1.3 Building LLM applications and AI agents\nLLMs are great at handling natural language—they can understand text,\ngenerate it, and pull out information when you need it. That makes them\nuseful for all kinds of applications: summarization, translation, sentiment\nanalysis, semantic search, chatbots and\ncode generation. Because of this range, you’ll now see LLMs showing up in\nfields as varied as education, healthcare, law, and finance.\nEven with all these different use cases, most LLM apps end up looking pretty\nsimilar under the hood. They take in natural language input, work with\nunstructured data, pull in extra context from one or more sources, and then\npackage everything into a prompt for the model to process.\nFrom a high level, these applications generally fall into three main categories:\nLLM-based applications or engines – Systems that provide a specific\ncapability such as summarization, search, or content generation.\nChatbots – Conversational interfaces that maintain context over multiple\nexchanges.\nAI agents – Autonomous or semi-autonomous systems that use LLMs to\nplan and execute multi-step tasks, often interacting with external tools or\nAPIs.\nWe’ll examine each of these categories in turn before exploring how\nLangChain supports their development—starting with LLM-based\napplications and engines.\n1.3.1 LLM-based applications: summarization and Q&A\nengines\nAn LLM-based engine acts as a backend tool that handles specific natural\n\n\nlanguage requests for other systems. For example, a summarization engine\ncondenses lengthy text passages into concise summaries. These summaries\ncan be returned immediately to the client or stored in a database for later use\nby other applications, as shown in Figure 1.4.\nFigure 1.4 A summarization engine efficiently summarizes and stores content from large volumes\nof text and can be invoked by other systems through REST API.\nSummarization engines are often deployed as shared services, typically\nexposed through a REST API so multiple systems can call them on demand.\nAnother common type is the Question & Answer (Q&A) engine, which\nanswers user queries against a knowledge base. A Q&A engine works in two\nphases: ingestion and query.\nIn the ingestion phase, the engine builds its knowledge base by pulling in\n\n\ntext, splitting it into chunks, and converting those chunks into embeddings—\nmathematical vectors that capture meaning. Both the embeddings and the\noriginal chunks are stored in a vector store for efficient retrieval. Don’t worry\nif “embedding model” or “vector store” sound unfamiliar; we’ll cover them\nin detail later. For now, just think of this step as transforming raw text into a\nsearchable representation.\nDefinition\nEmbeddings are vector representations of words, tokens, or larger text units\n—such as sentences, paragraphs, or document chunks—mapped into a\ncontinuous, high-dimensional space (typically with hundreds or thousands of\ndimensions). These vectors capture semantic and syntactic relationships,\nallowing language models to understand meaning, context, and similarity.\nEmbeddings are learned during the pre-training phase, where models are\ntrained on large-scale text corpora to predict tokens based on surrounding\ncontext. By encoding both individual words and broader semantic meaning,\nembeddings enable more effective reasoning, retrieval, and language\nunderstanding.\nIn the query phase, the engine takes a user’s question, turns it into an\nembedding using the same model, and performs a semantic search over the\nvector store. The most relevant chunks are retrieved and combined with the\noriginal question to form a prompt, which is then sent to the LLM. The model\nuses both the question and the retrieved context to generate an accurate,\ngrounded answer.\nThis workflow is known as Retrieval-Augmented Generation (RAG), shown\nin Figure 1.5. RAG has quickly become a cornerstone of modern LLM\napplications, and we’ll spend three full chapters (8, 9, and 10) diving into\nadvanced techniques for making it more powerful and reliable.\nDefinition\nRetrieval-Augmented Generation (RAG) is a design pattern where the LLM’s\ntext “generation” is “augmented” by incorporating additional context,\n“retrieved” from a local knowledge base—often stored in a vector store—at\n\n\nquery time.\nFigure 1.5 A Q&A engine implemented with RAG design: an LLM query engine stores domain-\nspecific document information in a vector store. When an external system sends a query, it\nconverts the natural language question into its embeddings (or vector) representation, retrieves\nthe related documents from the vector store, and then gives the LLM the information it needs to\ncraft a natural language response.\nLangChain makes it straightforward to build engines like question-answering\nsystems by giving you modular components out of the box: document\nloaders, text splitters, embedding models, vector stores, and retrievers. You\n\n\ncan snap these together with minimal boilerplate, which means you spend\nmore time on your application’s logic and less time wrestling with low-level\ndetails. Later in the book, we’ll look at how to evaluate and choose among\ndifferent embedding models and vector stores. LangChain supports many\npopular options, so you can mix and match without getting locked into a\nsingle vendor.\nEngines aren’t limited to Q&A. They can also call external tools by running\npredefined sequences of steps, often called chains. For example, an engine\nhandling a user request might convert natural language instructions into API\ncalls, pull data from outside systems, and then use an LLM to interpret and\npresent the results in a clean, human-readable format.\nAt their core, these engines are designed for system-level work: automating\nprocesses, processing data intelligently, and stitching together different\nplatforms. They simplify workflows that involve natural language by\nhandling the messy parts—retrieval, transformation, orchestration—so you\ndon’t have to.\nBut once a human is directly interacting with the system in real time, the\ngame changes. At that point, the engine takes on a conversational role,\nshifting from pure automation into dialogue. This is where chatbots come\ninto play.\n1.3.2 LLM-based chatbots\nAn LLM-based chatbot acts as an intelligent assistant, enabling ongoing,\nnatural conversations with a language model. Unlike simple question–answer\nscripts, these systems aim to keep interactions both useful and safe. They rely\nheavily on prompt design: clear instructions shape the model’s behavior and\nhelp prevent irrelevant, inaccurate, or unsafe replies. Modern chat APIs—\nsuch as those from OpenAI—support role-based messaging formats (system,\nuser, assistant), which let you define an assistant’s persona and enforce\nconsistent behavior across a conversation.\nTo improve accuracy, chatbots often pull in factual context from local\nknowledge sources like vector stores. This lets them blend conversational\n\n\nfluency with domain-specific grounding, so the answers aren’t just smooth\nbut also relevant and reliable.\nA key strength of chatbots is conversation memory. By tracking earlier turns,\nthey can keep responses coherent and personalized. This memory is limited\nby the model’s context window. Larger windows help, but many systems still\ncompress or summarize conversation history to stay within limits—and to\nmanage cost.\nLLM-based chatbots are usually specialized for tasks such as summarization,\nquestion answering, or translation. They can either respond directly to user\ninput or combine it with stored knowledge. For example, the architecture of a\nsummarization chatbot (figure 1.6) builds on a basic summarization engine,\nbut adds dialogue management and context-awareness layers.\nFigure 1.6 A summarization chatbot has some similarities with a summarization engine, but it\noffers an interactive experience where the LLM and the user can work together to fine-tune and\nimprove the results.\n\n\nThe crucial difference between a summarization engine and a summarization\nchatbot is interactivity. A chatbot lets you refine responses in real time: if you\nwant a shorter or more casual summary, you can just ask, as illustrated in the\nsequence diagram in Figure 1.7.\nFigure 1.7 Sequence diagram that outlines how a user interacts with an LLM through a chatbot\nto create a more concise summary.\n",
      "page_number": 17,
      "chapter_number": 3,
      "summary": "This chapter covers segment 3 (pages 17-24). Key topics include engines, text. That makes them\nuseful for all kinds of applications: summarization, translation, sentiment\nanalysis, semantic search, chatbots and\ncode generation.",
      "keywords": [
        "LLM",
        "engine",
        "summarization",
        "summarization engine",
        "vector store",
        "vector",
        "model",
        "natural language",
        "language",
        "applications",
        "Systems",
        "text",
        "LLM applications",
        "summarization chatbot",
        "Building LLM applications"
      ],
      "concepts": [
        "engines",
        "text",
        "llm",
        "context",
        "conversational",
        "conversations",
        "summarization",
        "summarizes",
        "model",
        "language"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 56,
          "title": "Segment 56 (pages 475-482)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 27,
          "title": "Segment 27 (pages 222-231)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "Segment 28 (pages 232-240)",
          "relevance_score": 0.64,
          "method": "api"
        }
      ]
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 25-33)",
      "start_page": 25,
      "end_page": 33,
      "detection_method": "topic_boundary",
      "content": "pom}\n\nChatbot\nREST API\n\n—— — em mm a a ee — ———\n\nThis back-and-forth makes the process collaborative—producing answers\nthat feel more tailored and context-aware. In the next chapter, we’ll explore\ntechniques like role instructions, few-shot examples, and advanced prompt\nengineering to give you more control over chatbot behavior and output.\n1.3.3 AI agents\nAn AI agent is a system that works with a large language model (LLM) to\ncarry out multi-step tasks—often involving multiple data sources, branching\nlogic, and adaptive decision-making. Unlike simple pipelines, agents can\noperate with a degree of independence: they follow the constraints you set,\nbut they make choices about what to do next.\nAt each step, the agent consults the LLM to decide which tools to use, runs\nthose tools, and processes the results before moving on. This loop continues\nuntil the agent produces a complete solution. In practice, that might mean\npulling information from both structured sources (like databases or APIs) and\nunstructured ones (like documents or web pages), combining the results, and\npresenting them in a coherent format.\nConsider this example: a tour operator uses an AI agent to generate holiday\npackages based on natural language requests from a booking website. As\nshown in Figure 1.8, the process could look like this:\n1. The agent sends a prompt to the LLM asking it to choose the most\nrelevant tools for the request—for example, flight, hotel, and car rental\nproviders; weather forecast services; and internal holiday deals\ndatabases.\n2. Guided by a developer-crafted prompt listing available tools and their\ndescriptions, the LLM selects the appropriate ones and generates the\nrequired queries—such as SQL for the holiday deals database or REST\nAPI calls for external providers.\n3. The agent executes the queries, gathers the results, and sends another\nprompt to the LLM containing both the original holiday request and the\ncollected data.\n4. The LLM responds with a summarized holiday plan that includes all\n\n\nbookings, which the agent then returns to the booking website.\nFigure 1.8 Workflow of an AI agent tasked with assembling holiday packages: An external client\nsystem sends a customized holiday request in natural language. The agent prompts the LLM to\nselect tools and formulate queries in technology-specific formats. The agent executes the queries,\ngathers the results, and sends them back to the LLM, along with the original request, to obtain a\ncomprehensive holiday package summary. Finally, the agent forwards the summarized package\nto the client system.\nNote\n\n\nThe workflow can involve multiple iterations between the agent and the LLM\nbefore producing a final output—such as a complete holiday plan. Also, the\narchitecture shown in figure 1.8 is only one of the possible ones. An\nalternative design could be based on a set of more granular agents\ncoordinated by a supervisor agent at the top. We’ll explore these designs in\nchapters 11, 12 and 13. In high-stakes domains like finance or healthcare, it's\ncommon to include a human-in-the-loop step. This ensures that a human can\nreview and validate critical actions—such as approving a financial\ntransaction or confirming a complex medical recommendation—before\nfinalizing them. In the holiday planning example, the agent could be\nprogrammed to pause and request human approval of the proposed itinerary\nbefore sending it to the client, adding an extra layer of oversight and trust.\nLangChain’s agent framework also allows developers to incorporate human\nvalidation steps as part of the toolchain.\nThere’s been a surge of interest in AI agents, and major players like OpenAI,\nGoogle, and Amazon have all released agent SDKs to encourage developers\nand companies to adopt the approach. LangChain’s agent framework—\nespecially with the introduction of LangGraph—is built to handle these\nmulti-step, multi-tool workflows in a modular, controlled way.\nLangGraph provides pre-built agent and orchestrator classes, along with\nready-to-use tool integrations, so you don’t have to reinvent the wheel.\nBetween Chapters 11 and 13, you’ll get hands-on experience building\nadvanced agents with LangGraph, learning how to design, orchestrate, and\nrefine them in practice.\nIn many ways, an AI agent represents the most advanced form of LLM\napplication. It leverages the full spectrum of LLM capabilities—\nunderstanding, generating, and reasoning over text—to drive complex,\nautomated workflows. Agents dynamically select and use multiple tools,\nguided by prompts you design, making them powerful across industries like\nfinance, healthcare, and logistics where multi-step reasoning and data\nintegration are essential.\nInterest in agents has accelerated with the introduction of the Model Context\nProtocol (MCP) by Anthropic. MCP defines a standard way for services to\nexpose tools through MCP servers, which agents can then access via MCP\n\n\nclients as easily as if they were local components. This shifts the integration\nburden to the service itself, letting developers focus on building capable\nagents rather than writing and maintaining custom connectors. Since its\nrelease in late 2024, MCP has quickly become a de facto standard—adopted\nby major LLM providers like OpenAI and Google—and thousands of tools\nare now available through public MCP portals.\nWe’ll dive into MCP in Chapter 13, where you’ll learn the protocol, explore\nthe ecosystem in practice, and build your own MCP server that can plug\ndirectly into an agent application.\nNow that you’ve seen LangChain’s purpose, architecture, and the core\napplication types it supports, I encourage you to try out LangChain using the\nJupyter notebook described in Appendix A. It’s a quick way to get a feel for\nthe framework before we dive deeper!\n1.4 Typical LLM use cases\nLLMs are applied across a wide range of tasks, from text classification to\ncode generation and logical reasoning. Below are some of the most common\nuse cases, along with real-world examples for further exploration:\nText Classification and Sentiment Analysis: This includes categorizing\nnews articles or recommending stocks based on sentiment analysis. For\nexample, GoDaddy uses LLMs to automatically classify support tickets,\nas described in the article “LLM From the Trenches: 10 Lessons\nLearned Operationalizing Models at GoDaddy.”\nNatural Language Understanding and Generation: LLMs can identify\nmain topics in a text and generate summaries tailored by length, tone, or\nterminology. Duolingo uses AI to accelerate lesson creation, detailed in\ntheir blog post “How Duolingo Uses AI to Create Lessons Faster.”\nSummarization is explored in depth in Chapters 3 and 4.\nSemantic Search: This involves querying a knowledge base based on the\nintent and context of a question, rather than relying on simple keywords.\nThe Picnic supermarket app uses LLMs to enhance recipe search, as\nexplained in the Medium post “Enhancing Search Retrieval with Large\nLanguage Models (LLMs).” We’ll explore this extensively in the\n\n\ncontext of Q&A chatbot development in Chapters 6 and 7, with deeper\ndives into advanced techniques in Chapters 8 to 10.\nAutonomous Reasoning and Workflow Execution: LLMs can handle\ntasks like planning a complete holiday package by understanding\nrequests and managing each step of the process. We’ll discuss building\nLLM-powered agents using LangGraph in Chapter 12.\nStructured Data Extraction: This involves pulling structured data—like\nentities and their relationships—from unstructured text, such as financial\nreports or news articles.\nCode Understanding and Generation: LLMs can analyze code to identify\nissues, suggest improvements, or generate new code components—\nranging from simple functions and classes to entire applications—based\non user instructions. This capability powers popular IDE extensions like\nGitHub Copilot and Cline AI, and emerging AI-driven coding assistants\nsuch as Cursor and Windsurf, which provide real-time code suggestions\nand error detection as you work. In addition, CLI-based coding tools like\nAnthropic’s Claude Code and OpenAI Codex allow developers to\ninteract with AI through the command line, enabling code generation,\nrefactoring, and debugging directly from terminal environments.\nPersonalized Education and Tutoring: LLMs are increasingly used as\ninteractive tutors, providing personalized help and feedback. For\ninstance, Khan Academy’s “Khanmigo” uses an LLM to assist students\nwith interactive learning.\nThese use cases assume the LLM can competently handle user requests.\nHowever, real-world tasks often involve specific domains or scenarios that\nextend beyond the LLM’s initial training. How can you ensure your LLM\nmeets user needs effectively in these cases? That’s exactly what you’ll learn\nin the next section.\n1.5 How to adapt an LLM to your needs\nThere are several techniques to enhance the LLM's ability to respond to user\nrequests, even without prior knowledge or training in a specific domain.\nThese include:\nPrompt engineering\n\n\nRetrieval Augmented Generation\nFine-tuning\nLet's begin with the most basic approach: prompt-engineering.\n1.5.1 Prompt engineering\nPrompts for large language models (LLMs) can be as simple as a single\ncommand or as complex as a block of instructions enriched with examples\nand context. Prompt engineering is the practice of designing these inputs so\nthat the model understands the task and produces useful, accurate responses.\nDone well, it allows developers to guide the model’s behavior, adapt it to\ndomain-specific needs, and even handle problems the model wasn’t explicitly\ntrained on. A common technique here is in-context learning, where the model\ninfers patterns from examples embedded directly in the prompt—no fine-\ntuning required.\nIn practice, prompts are often organized as templates: a fixed instruction with\nvariable fields that accept dynamic input. This makes prompts reusable and\neasier to manage across different parts of an application. A widely used\npattern is few-shot prompting, where you include a handful of examples to\nteach the model how to generalize to similar inputs.\nWell-crafted prompts are surprisingly powerful. They can coax high-quality,\ndomain-aware output from an LLM without needing extra training data. For\ninstance, chatbots often embed recent conversation turns into the prompt so\nthe model maintains context and produces coherent, multi-turn replies.\nPrompt engineering can often take you very far, making it a lightweight yet\neffective tool for many real-world applications.\nLangChain builds on this idea with abstractions for managing and reusing\nprompts consistently, which we’ll explore in the next chapter. That said,\nprompt engineering alone has limits—especially when applications need to\nground answers in user-specific or enterprise data. In those cases, the natural\nnext step is Retrieval-Augmented Generation (RAG), which augments\nprompts with knowledge pulled dynamically from external sources.\n\n\n1.5.2 Retrieval Augmented Generation (RAG)\nOne of the most effective ways to improve LLM responses is to ground them\nin your own data. Instead of relying only on what the model learned during\npretraining, you can retrieve relevant context from a local knowledge base—\nusually stored in a vector database—and add it to the prompt. This approach,\ncalled Retrieval-Augmented Generation (RAG), has become a core pattern in\nmodern LLM applications.\nThe workflow begins with building a knowledge base. Documents are\ningested, split into smaller chunks, and converted into vector representations\nusing an embedding model. These embeddings capture semantic meaning,\nmaking it possible to compare text by similarity rather than exact wording.\nLangChain provides tools to load documents in many formats, split them\nefficiently, and generate embeddings, then store everything in a vector\ndatabase.\nFigure 1.9 A collection of documents is split into text chunks and transformed into vector-based\nembeddings. Both text chunks and related embeddings are then stored in a vector store.\n\n\n1. feed a text document\nto the ingestion script\n\n2. request LLM provider to calculate the\nembeddings of the document chunks\n\n7 ; 3, return embeddings\nText ingestion script d Embeddings model\n\nLLM provider\n\n4. store documents chunks and related\nembeddings into Vector store\n\nVector store",
      "page_number": 25,
      "chapter_number": 4,
      "summary": "This chapter covers segment 4 (pages 25-33). Key topics include agent, prompt. 1.3.3 AI agents\nAn AI agent is a system that works with a large language model (LLM) to\ncarry out multi-step tasks—often involving multiple data sources, branching\nlogic, and adaptive decision-making.",
      "keywords": [
        "LLM",
        "agent",
        "LLMs",
        "prompt",
        "Embeddings model LLM",
        "model",
        "holiday",
        "MCP",
        "prompt engineering",
        "generation",
        "model LLM provider",
        "tools",
        "LLM provider",
        "text",
        "’ll"
      ],
      "concepts": [
        "agent",
        "llm",
        "prompt",
        "uses",
        "useful",
        "coding",
        "tools",
        "providers",
        "provides",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents In Action",
          "chapter": 5,
          "title": "Segment 5 (pages 33-40)",
          "relevance_score": 0.76,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.72,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 34,
          "title": "Segment 34 (pages 296-303)",
          "relevance_score": 0.68,
          "method": "api"
        }
      ]
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 34-41)",
      "start_page": 34,
      "end_page": 41,
      "detection_method": "topic_boundary",
      "content": "RAG offers multiple benefits:\n1. Efficiency: Instead of passing an entire document to the model, you\nretrieve only the key chunks—keeping inputs concise, reducing token\ncosts, and working within context limits.\n2. Accuracy: Responses are grounded on real data, reducing the risk of\nhallucinations. In Chapter 6, you’ll learn techniques for having the LLM\ncite its sources, further improving transparency and trust.\n3. Flexibility: By swapping embedding models, retrievers, or vector stores,\nyou can adapt the same pattern to different domains and requirements.\nDefinition\n\"Grounding\" an LLM involves crafting prompts that include context pulled\nfrom a trusted knowledge source—often stored in a vector store. This ensures\nthat the LLM generates its response based on verified facts rather than relying\nsolely on its pre-trained knowledge, which may include outdated or\nunreliable data.\nDefinition\nA “hallucination” is when a large language model generates an incorrect,\nmisleading, or fabricated response. This happens when the model draws from\npoor-quality data during training or lacks sufficient information to answer\naccurately. Due to their auto-regressive nature, LLMs will try to generate a\nresponse even when relevant content is missing—leading to hallucinations as\nthey “fill in the gaps” with plausible-sounding but incorrect information.\nTo make RAG reliable, prompts should explicitly instruct the LLM to rely\nonly on the retrieved context. LangChain also supports guardrails and\nvalidators to help enforce safe behavior. In high-stakes cases, human-in-the-\nloop review remains the best safeguard.\nIn short, RAG bridges the gap between static pretrained models and dynamic,\ndomain-specific applications. By teaching your app to “speak the same\nlanguage” as the LLM—vectors—you unlock a practical way to deliver\ngrounded, verifiable, and cost-efficient answers. If prompt engineering and\n\n\nRAG still don’t meet your needs, the next step is fine-tuning the model,\nwhich we’ll revisit later in the book.\n1.5.3 Fine-tuning\nFine-tuning is the process of adapting a pre-trained LLM to perform better in\na specific task or domain. This is done by training the model on a curated\ndataset of examples that capture the style, terminology, and reasoning\npatterns you want it to master. Traditionally, fine-tuning required specialized\nML frameworks and access to powerful hardware, but today many platforms\n—including OpenAI’s fine-tuning API and several open-source toolkits—\nmake it possible with just a dataset upload and minimal configuration.\nThe main benefit of fine-tuning is efficiency: once a model has absorbed\ndomain-specific knowledge, you don’t need to stuff every prompt with long\ninstructions or examples. Instead, the model “knows” how to respond in your\ncontext. The trade-offs, however, are real. Preparing high-quality datasets\ntakes time and expertise, and training runs can be costly since they often\nrequire GPUs.\nRecent advances like Low-Rank Adaptation (LoRA) and other parameter-\nefficient fine-tuning methods have lowered both cost and complexity, making\nfine-tuning more accessible. Related techniques, such as instruction tuning\nand reinforcement learning from human feedback (RLHF), push models to\nfollow directions more reliably, though they typically demand more\nengineering effort and infrastructure.\nWhether fine-tuning is truly necessary is an ongoing debate. General-purpose\nLLMs are surprisingly strong out-of-the-box, especially when paired with\nRetrieval-Augmented Generation (RAG). In fact, research by Heydar\nSoudani, Evangelos Kanoulas, and Faegheh Hasibi (“Fine-Tuning vs.\nRetrieval-Augmented Generation for Less Popular Knowledge”) shows that\nRAG often outperforms fine-tuning by letting you provide context\ndynamically at runtime, reducing both costs and retraining needs.\nThat said, in highly specialized domains—such as medicine, law, or finance\n—fine-tuning remains invaluable. It allows models to capture domain-\n\n\nspecific jargon and workflows in ways generic models struggle to match.\nWell-known examples include:\nBioMistral (biology and life sciences)\nLexisNexis’s legal-domain LLM (“LexiGPT”)\nBloombergGPT (finance)\nAnthropic’s Claude Code and similar code-focused models\nIn summary, fine-tuning customizes an LLM for domain expertise and\nspecialized accuracy, but it comes at a cost in time, money, and complexity.\nAs a developer, you’ll need to weigh when it’s truly worth it versus when\nRAG or prompt engineering will get you there faster.\n1.6 Which LLMs to choose\nWhen developing LLM-based applications, you’ll find a wide range of\nmodels to choose from. Some are proprietary, accessible via subscription or\npay-as-you-go APIs, while others are open source and can be self-hosted.\nMost modern LLMs offer REST APIs for easy integration and user-friendly\nchat interfaces. Many also come in different size variants, letting you balance\nperformance, speed, and cost.\nLangChain makes it simple to integrate with a variety of LLMs. Thanks to its\nstandardized interface, you can switch models with minimal code changes—\nan essential feature in today’s fast-evolving LLM landscape.Below are key\nconsiderations for choosing the right LLM for a specific task.\nModel Purpose:\nFor general tasks like summarization, translation, classification, or\nsentiment analysis, most large model families (GPT, Gemini,\nClaude, Llama, Mistral) handle these as standard capabilities.\nFor specialized tasks—such as code generation—seek out models\nthat are optimized or fine-tuned for that purpose. For instance,\nClaude Sonnet and Meta’s Code Llama variants are well-regarded\nfor coding tasks.\nContext Window size:\nLarger A larger context window lets your application handle longer\n\n\nprompts and documents. For example, some models support up to 2\nmillion tokens, while many others offer 128K–256K tokens.\nKeep in mind: larger windows can increase both performance\noverhead and cost, especially for commercial APIs that charge per\ntoken.\nMultilingual Support:\nIf your app needs to handle multiple languages, opt for models with\nstrong multilingual training. Qwen and Llama are especially\ncapable in both Western and Asian languages. Some models may\nspecialize—for example, certain Gemma releases are optimized for\nJapanese.\nModel size:\nLLMs range from small (1B parameters) to very large (trillions of\nparameters). Smaller models can be more cost-effective and faster,\noften sufficient for simpler tasks.\nFor example, OpenAI mini and nano versions offer impressive\naccuracy at a lower cost and latency than its full-size counterpart.\nSpeed:\nSmaller and mid-size models typically respond faster. If\nresponsiveness is critical (e.g., in chatbots), benchmark both\nperformance and accuracy.\nInstruction vs Reasoning:\nA key distinction in LLM behavior is between instruction models\nand reasoning models.\nInstruction models, such as OpenAI’s GPT-4 series or Google\nGemini Pro, are best suited when you already know exactly how a\ntask should be performed and want the LLM to follow your\ndetailed directions. They excel at accurately executing well-\ndescribed instructions. You can think of them as reliable junior\nassistants: fast, precise, and effective when the steps are clear.\nReasoning models, such as OpenAI’s o-series or Google Gemini\nThinking, are designed for situations where the task is defined but\nthe method is not. These models not only execute work but also\ndecide how to approach it. They are closer to senior problem-\nsolvers—capable of planning, adapting, and filling in missing\ndetails.\nThe trade-offs are cost and speed: reasoning models are generally\n\n\nslower and more expensive, while instruction models are faster and\nmore economical. Choosing between them depends on whether you\nwant the model to simply follow a plan you provide or to figure out\nthe plan itself.\nOpen-Source vs. Proprietary:\nOpen-source models (Llama, Mistral, Qwen, Falcon) provide\nstronger data privacy and allow on-premise or private cloud\ndeployment.\nProprietary models are easier to set up via API and may deliver\nbest-in-class performance out of the box—but long-term costs can\nbe high. Many teams start with commercial APIs, then migrate to\nopen-source hosting for cost or compliance reasons.\nBy weighing these factors, you can select an LLM that aligns with your\nproject’s goals, technical requirements, and budget. Keep in mind that your\napplication may involve multiple tasks, each with its own demands for\naccuracy, speed, and cost-efficiency. In many cases, it’s wise to configure\nyour system to use different models for different tasks. For example, in one\nrecent project, I used GPT-4o Mini for summarization and sentiment\nanalysis, o3 to interpret and route user queries to the appropriate agents, and\nGPT-4.1 for answer synthesis—balancing performance and cost across the\nworkflow.\n1.7 What You'll Learn from this Book\nIf you’ve read this far, you already recognize the value of building\napplications powered by large language models. LLM interaction is rapidly\nbecoming a core feature of modern software—similar to how websites, in the\nearly 2000s, opened up a new communication channel for server-based\napplications. And this shift is only accelerating.\nWe’ll begin with prompt engineering, the foundational skill for effective\ninteraction with LLMs. You’ll start by experimenting interactively with\nChatGPT, then move to programmatic access via REST APIs. These\nexercises will establish the backbone for many projects in this book.\nFrom there, we’ll use LangChain to build two categories of applications:\n\n\ncustom engines (such as summarization and Q&A systems) and chatbots that\ncombine conversational fluency with knowledge retrieval. Each project will\nbe small and self-contained, but all will share a common background theme\nin the travel industry, so you can see how the ideas fit together in a coherent\ndomain.\nOnce those foundations are in place, we’ll move to LangGraph to build AI\nagents—applications that can orchestrate multi-step workflows, coordinate\ntools, and make adaptive decisions. This will be your introduction to the most\nadvanced class of LLM-powered systems, where engines and tools come\ntogether into dynamic, autonomous applications. To make this approachable,\nI’ll start with a simple Python script and then extend it step by step. Each\nextension will branch into a different capability—such as tool use, planning,\nor memory—so you can see how agents grow in sophistication without being\noverwhelmed all at once.\nWe’ll also take a deep dive into Retrieval-Augmented Generation (RAG), the\narchitectural pattern behind many real-world systems. RAG concepts will be\nintroduced through short, focused scripts—some independent, others\nprogressively refined into more advanced workflows—so you can master\nboth fundamentals and cutting-edge techniques.\nAlthough the examples reference OpenAI models for accessibility and quick\nwins, you’ll also learn how to use open-source models through the inference\nengines listed in Appendix E. This way, you’ll gain the skills to build and\ndeploy applications that are cost-effective, privacy-conscious, and fully under\nyour control.\nBeyond building, you’ll explore the entire lifecycle of LLM applications:\ndebugging, monitoring, and refining with LangSmith; orchestrating complex\nworkflows with LangGraph; and applying best practices for production\ndeployment to ensure scalability and maintainability.\nBy the end of this book, you’ll have built a portfolio of working applications,\nlearned the core architectural patterns, and developed the skills to design and\nimplement LLM-powered systems with confidence. You won’t just\nunderstand how these applications work—you’ll be ready to keep innovating\nand pushing the boundaries of what’s possible with LangChain, LangGraph,\n\n\nand large language models.\n1.8 Recap on LLM terminology\nIf you're new to LangChain, LLMs, and LLM applications, you'll encounter\nmany terms in this chapter. The following table will help clarify these\nconcepts.\nTable 1.1 LLM Application Glossary\nTerm\nDefinition\nLLM\nA Large Language Model trained on vast text datasets to\nperform tasks like understanding, generating, and\nsummarizing text.\nEmbedding\nNumerical vector representations of words or tokens in a\nhigh-dimensional space, capturing their semantic and\nsyntactic meaning.\nEmbedding\nModel\nModels that convert text into vectors, capturing the\nmeaning for efficient processing and retrieval in\nLangChain.\nVector Store\nA specialized database for storing and retrieving vector-\nbased representations of documents, supporting efficient\nsearch and retrieval.\nPrompt\nA request to guide the LLM, often created from\ntemplates and user inputs, for processing specific tasks.\nPrompt\nEngineering\nThe craft of designing effective prompts to guide the\nLLM in generating accurate responses, often through\ntrial and refinement.\nCompletion\nThe response generated by an LLM when given a\nprompt, based on the model's predictive text abilities.\nLLM-Based\nEngine\nA backend service using LLMs for tasks like\nsummarization or analysis, powering other applications\nwith language processing.\nLLM-Based\nChatbot\nA chatbot using an LLM, optimized with prompts and\ninstructions to guide interactions and maintain safety\nand relevance.\n\n\nRAG\nRetrieval Augmented Generation combines LLM\noutputs with context retrieved from a local knowledge\nbase for better accuracy.\nGrounding\nSupplying the LLM with verified context from a trusted\nsource to ensure reliable responses.\nHallucination\nThe model's tendency to generate incorrect or fabricated\ninformation when lacking sufficient context or relying\non low-quality data.\nLLM-Based\nAgent\nAn autonomous system that orchestrates complex\nworkflows by coordinating across multiple tools,\nincluding remote tools hosted by MCP servers, and data\nsources, executing multi-step tasks through interactions\nwith a language model.\nFine-Tuning\nAdapting a pre-trained LLM for specific tasks by\ntraining it on examples, enhancing its domain-specific\nknowledge.\nEach term has been introduced in the chapter to support your understanding\nof LangChain and its applications.\n1.9 Summary\nLLMs have rapidly evolved into core building blocks for modern\napplications, enabling tasks like summarization, semantic search, and\nconversational assistants.\nWithout frameworks, teams often reinvent the wheel—managing\ningestion, embeddings, retrieval, and orchestration with brittle, one-off\ncode. LangChain addresses this by standardizing these patterns into\nmodular, reusable components.\nLangChain’s modular architecture builds on loaders, splitters,\nembedding models, retrievers, and vector stores, making it\nstraightforward to build engines such as summarization and Q&A\nsystems.\nConversational use cases demand more than static pipelines. LLM-based\nchatbots extend engines with dialogue management and memory,\nallowing adaptive, multi-turn interactions.\nBeyond chatbots, AI agents represent the most advanced type of LLM\n",
      "page_number": 34,
      "chapter_number": 5,
      "summary": "This chapter covers segment 5 (pages 34-41). Key topics include model, applications. Accuracy: Responses are grounded on real data, reducing the risk of\nhallucinations.",
      "keywords": [
        "LLM",
        "models",
        "LLMs",
        "RAG",
        "applications",
        "’ll",
        "fine-tuning",
        "tasks",
        "LLM applications",
        "context",
        "language model",
        "large language model",
        "cost",
        "LLM Application Glossary",
        "reasoning models"
      ],
      "concepts": [
        "model",
        "llm",
        "applications",
        "application",
        "task",
        "context",
        "fine",
        "domains",
        "base",
        "generates"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 8,
          "title": "Segment 8 (pages 59-66)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 30,
          "title": "Segment 30 (pages 595-616)",
          "relevance_score": 0.68,
          "method": "api"
        }
      ]
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 42-49)",
      "start_page": 42,
      "end_page": 49,
      "detection_method": "topic_boundary",
      "content": "application. Agents orchestrate multi-step workflows and tools under\nLLM guidance, with frameworks like LangGraph designed to make this\npractical and maintainable.\nRetrieval-Augmented Generation (RAG) is a foundational pattern that\ngrounds LLM outputs in external knowledge, improving accuracy while\nreducing hallucinations and token costs.\nPrompt engineering remains a critical skill for shaping LLM behavior,\nbut when prompts alone aren’t enough, RAG or even fine-tuning can\nextend capabilities further.\n\n\n2 Executing prompts\nprogrammatically\nThis chapter covers\nUnderstanding prompts and prompt engineering\nDifferent kinds of prompts and how they're structured\nEnhancing prompt responses using one, two, or a few-shot learning\nExamples of using prompts with ChatGPT and the OpenAI API\nApplications built with LangChain interact with LLMs primarily through\nprompts—structured inputs that guide the model’s behavior. Think of it as\ngiving instructions to a highly capable but inexperienced colleague: the\nclearer and more specific your instructions, the better the outcome. To\ngenerate accurate and relevant responses, your prompts must be well-crafted\nand tailored to the task at hand. In LangChain, prompt design plays a central\nrole in determining how your application performs.\nPrompt engineering—the practice of designing and refining prompts to steer\nthe LLM’s output—is a core skill in LLM application development. You’ll\nspend a significant amount of time creating, testing, and iterating on prompts\nto ensure your system consistently delivers high-quality results. LangChain\nrecognizes this and places prompt engineering at the heart of its design,\noffering a suite of tools to support the process. Whether you're writing simple\ninstructions or building sophisticated templates that require advanced\nreasoning and dynamic input handling, LangChain provides the infrastructure\nto manage and scale your prompt-driven workflows effectively.\nIn this chapter, you'll begin with the basics of prompt design and gradually\nmove to more sophisticated techniques, such as Chain of Thought.\nLangChain's suite of prompt engineering tools, including PromptTemplate\nand FewShotPromptTemplate, will be your key resources as you learn to\nharness the full power of LLMs in your applications.\n\n\n2.1 Running prompts programmatically\nLangChain applications rely on well-crafted prompts to generate\ncompletions, which are then passed to the next component in the chain.\nUnlike prompts entered manually in interfaces like ChatGPT, LangChain\nprompts are typically constructed and sent to the LLM programmatically as\npart of a larger workflow. In the following sections, we’ll start by setting up\nand executing prompts using the OpenAI API directly, and then explore how\nto run and manage prompts within LangChain.\nBefore diving in, make sure you’ve covered these basics:\n1. You have an OpenAI API key.\n2. You know how to create a Python Jupyter notebook environment.\nIf you're not familiar with these tasks, check the sidebar for guidance on\n\"Creating an OpenAI key\" in Section 1.7 and follow the instructions in\nAppendix B titled \"Setting up a Jupyter notebook environment.\".\n2.1.1 Setting up an OpenAI Jupyter Notebook environment\nAssuming you have cleared the prerequisites, open the operating system\nterminal shell (e.g., cmd on Windows), create a project folder, and navigate\ninto it:\nC:\\Github\\building-llm-applications\\ch02>\nCreate and activate a virtual environment with venv:\nC:\\Github\\building-llm-applications\\ch02>python -m venv env_ch02\nC:\\Github\\building-llm-applications\\ch02>.\\env_ch02\\Scripts\\activ\nYou should now observe the updated operating system prompt, displaying the\nenvironment name in front as (env_ch02):\n(env_ch02) C:\\Github\\building-llm-applications\\ch02>\nHaving activated the virtual environment, you're now prepared to implement\na Jupyter notebook for executing prompts with OpenAI models. If you have\n\n\ncloned the repository from Github, you can install Jupyter and the OpenAI\npackages as follows:\n(env_ch02) C:\\Github\\building-llm-applications\\ch02>pip install -\nThen you can start the notebook:\n(env_ch02) C:\\Github\\building-llm-applications\\ch02>jupyter noteb\nOtherwise, if you are creating everything from scratch locally, install the\npackages in this way:\n(env_ch02) C:\\Github\\building-llm-applications\\ch02>pip install n\nAfter about a minute, the installation of the notebook and OpenAI packages\nshould be finished. You can start the Jupyter notebook by executing the\nfollowing command:\n(env_ch02) C:\\Github\\building-llm-applications\\ch02>jupyter noteb\nAfter a few seconds, you should see some output in the terminal.\nSubsequently a browser window will open up at this URL:\nhttp://localhost:8888/tree.\nOnce you have created the notebook, go to the Jupyter menu, select: File >\nRename … and name the notebook file: 02-prompt_examples.ipynb.\nFigure 2.1 Creating a new Jupyter notebook: create the notebook with the menu File > New >\nNotebook and then rename the file to prompt_examples.ipynb\n\n\nNow, you're prepared to input code into the notebook cells (or simply execute\nthem if you got the notebook from Github).\nTIP\nIf you are unfamiliar with Jupyter notebook, remember to press Shift + Enter\nto execute the code in each cell.\nThroughout the remaining chapter, I assume you've set up a virtual\nenvironment, installed the OpenAI library, and launched a Jupyter notebook\ninstance, as outlined in the previous section.\n\n\n2.1.2 Minimal prompt execution\nIn the first cell of your notebook, import the required libraries and grab the\nOpenAI API key in a secure way (never hardcode it as it might get misused):\nfrom openai import OpenAI\nimport getpass\nOPENAI_API_KEY = getpass.getpass('Enter your OPENAI_API_KEY')\nAfter entering your OpenAI API key (you only need to hit Enter, without\nShift), set up the OpenAI client as follows:\nclient = OpenAI(api_key=OPENAI_API_KEY)\nIn the next notebook cell, enter and execute the following code. It's based on\none of the prompts discussed in the \"prompt basics\" section, and you can find\ndetails about the chat.completions.create function in the accompanying\nsidebar.\nprompt_input = \"\"\"Write a short message to remind users to be vig\nresponse = client.chat.completions.create(\n model=\"gpt-4o-mini\",\n messages=[\n   {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n   {\"role\": \"user\", \"content\": prompt_input}\n ],\n temperature= 0.7,\n max_tokens= 400  \n)\nprint(response)\nThe output will look like this (I have shortened it for convenience):\nChatCompletion(id='chatcmpl-8Wr77Lor0Xusp7V7iM2NURV8dbwVj', choic\nFor a clearer output, explore the attributes and properties of the response\nobject:\nprint(response.choices[0].message.content)\n\n\nYou will see an output similar to the following, though it may vary slightly\ndue to the non-deterministic nature of LLMs—especially with the\ntemperature parameter set above zero, which increases randomness:\nAttention users,\nPlease be vigilant and stay alert regarding phishing attacks. Cyb\nStay safe and secure online!\nBest regards,\n[Your Name/Company Name]\nNote\nYou may not be familiar with the three double quotes (\"\"\") I have used to\nformulate the prompt and may wonder about their purpose. This allows you\nto enter a block of text spanning multiple lines in a very readable way,\nwithout having to introduce newline characters (\\n), which is ideal for\ncapturing prompts.\nchat.completions.create\nThe chat.completions.create function is essential for interacting with the\nOpenAI LLM, so it's crucial to understand its signature in detail:\nclient.chat.completions.create(\n model=\"gpt-4o-mini\",\n messages=[\n   {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n   {\"role\": \"user\", \"content\": prompt_input}\n ],\n temperature= 0.7,\n max_tokens= 400  \n)\nHere is a description of the parameters:\nmodel: This refers to the OpenAI model to be used. Some model\nfamilies, such as GPT-4o and GPT-4o-mini, have generic capabilities,\nunderstanding and generating text. Other models, like the \"moderation\"\n\n\nfamily, are more specialized. Within each family, there are variants that\ncan accept bigger or smaller prompts or are newer, and therefore\nrecommended over more obsolete variants. Learn more on this page on\nOpenAI Models: https://platform.openai.com/docs/models.\nmessages: A prompt can be composed of a list of messages (when\nfollowing the OpenAI convention), each impersonated by a “role” and\nhaving some “content” with the instructions to be followed.\ntemperature: This is a \"creativity\" or \"entropy\" coefficient, ranging from\n0 (no creativity, meaning the output will always be the same given the\nsame prompt) to 2 (the output will vary considerably at each execution\nof the same prompt).\nmax_tokens: This limits the number of output tokens, generally to\nreduce costs, as you are charged for the number of tokens read and\ngenerated.\nNow that you've grasped the fundamentals of programmatically submitting\nprompts, you can proceed to learn and work with more intricate prompts.\n2.2 Running prompts with LangChain\nBefore moving to more advanced prompts, let me show you how to replicate\nthe same example in LangChain. This will help you familiarize yourself with\nits object model. In the following sections, I'll demonstrate how LangChain\nsimplifies the implementation of more complex prompts that would be more\nchallenging to implement from scratch.\nBefore adding LangChain code to your notebook, open a new OS shell and\nimport the LangChain packages as you did in chapter 1 to avoid stopping the\nrunning OS shell (you do not need to do this if you installed the dependencies\nfrom requirements.txt, as explained earlier):\n(env_ch02) c:\\Github\\building-llm-applications\\ch02>pip install l\nOnce the installation is complete, add a new cell in the notebook and import\nthe LangChain OpenAI wrapper library. Then, instantiate a connection to the\nLLM:\nfrom langchain_openai import ChatOpenAI\n",
      "page_number": 42,
      "chapter_number": 6,
      "summary": "Whether you're writing simple\ninstructions or building sophisticated templates that require advanced\nreasoning and dynamic input handling, LangChain provides the infrastructure\nto manage and scale your prompt-driven workflows effectively Key topics include prompts, langchain, and openai.",
      "keywords": [
        "Jupyter notebook",
        "prompts",
        "OpenAI",
        "OpenAI API key",
        "notebook",
        "OpenAI API",
        "Github",
        "Jupyter",
        "OpenAI Jupyter Notebook",
        "LangChain",
        "Jupyter notebook environment",
        "API",
        "LLM",
        "Prompt engineering",
        "API key"
      ],
      "concepts": [
        "prompts",
        "langchain",
        "openai",
        "application",
        "applications",
        "llm",
        "model",
        "creating",
        "create",
        "responses"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 21,
          "title": "Segment 21 (pages 168-175)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 22,
          "title": "Segment 22 (pages 176-183)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 35,
          "title": "Segment 35 (pages 292-299)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 24,
          "title": "Segment 24 (pages 193-201)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 8,
          "title": "Segment 8 (pages 61-70)",
          "relevance_score": 0.57,
          "method": "api"
        }
      ]
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 50-59)",
      "start_page": 50,
      "end_page": 59,
      "detection_method": "topic_boundary",
      "content": "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,\n                model_name=\"gpt-4o-mini\")\nYou can now instantiate and execute the prompt you saw earlier as follows:\nprompt_input = \"\"\"Write a coincise message to remind users to be \nresponse = llm.invoke(prompt_input)\nprint(response.content)\nThis will return the following output similar to:\n\"Stay alert and protect your information - beware of phishing attacks.\"\n2.3 Prompt templates\nWhen building LLM applications, creating flexible prompt templates that\nincorporate user input through parameters is crucial. LangChain makes this\neasier with its PromptTemplate class, which lets you manage and reuse\nparametrized prompts without the need for custom functions. This not only\nstreamlines the creation of dynamic prompts but also enhances the efficiency\nand adaptability of your LLM interactions, as I’ll demonstrate shortly.\n2.3.1 Implementing a prompt template with a Python function\nTo illustrate the concept of a template, let's create a text summarization\ntemplate that requests the text, desired summary length, and preferred tone.\nYou could implement this using a simple Python function:\ndef generate_text_summary_prompt(text, num_words, tone):\n   return f\"You are an experienced copywriter. Write a {num_word\nLet’s use the prompt template to generate a prompt and then execute it\nthrough LangChain’s ChatOpenAI wrapper as usual:\nsegovia_aqueduct_text = \"The Aqueduct of Segovia (Spanish: Acuedu\n\n\ninput_prompt = generate_text_summary_prompt(text=segovia_aqueduct_\nresponse = llm.invoke(input_prompt)\nprint(response.content) \nYou should be output similar to:\nThe Aqueduct of Segovia, built in the 1st century AD, is a well-p\n2.3.2 Using LangChain’s PromptTemplate\nWith LangChain, you don't need to implement a prompt template function\nmanually. Instead, you can use the convenient PromptTemplate class to\nhandle parametrized templates, as I showed in chapter 1. Here's how you can\nuse it:\nfrom langchain_core.prompts import PromptTemplate\nprompt_template = PromptTemplate.from_template(\"You are an experi\nTo use the prompt template, create a prompt instance and format it with your\nparameters:\nprompt = prompt_template.format(text=segovia_aqueduct_text, num_w\nThen, invoke the ChatOpenAI client with the formatted prompt:\nresponse = llm.invoke(prompt)\nprint(response.content)\nThis will generate output similar to what’s shown here (as you now\nunderstand, the exact output may differ from what’s printed in the book, so I\nwon’t repeat this point on the next page):\nThe Aqueduct of Segovia, a Roman marvel built in the 1st century A\nLet’s take a short break from coding with LangChain. I want to delve deeper\ninto prompt engineering. In the upcoming pages, you'll see that a good\nprompt can have various elements, depending on your goal, task complexity,\nand desired accuracy. By learning from different prompt variations, you'll be\n\n\nable to handle both simple and complex prompts, adjusting their complexity\nto fit specific cases.\n2.4 Types of prompt\nCreating effective prompts is crucial for getting the best results in LangChain\napplications. Whether your app is focused on text classification, sentiment\nanalysis, summarization, text generation, or question answering, each task\nrequires a carefully designed prompt. In the following sections, we'll dive\ninto how to craft prompts tailored to these specific tasks, ensuring your\napplication functions as intended.\nAlthough these prompting techniques are key to developing LangChain\napplications, we'll use ChatGPT for now to keep things simple and explain\nprompt engineering concepts more clearly. The patterns you learn here will\ndirectly apply to the LLM apps we build with LangChain in the upcoming\nchapters.\n2.4.1 Text classification\nIn classification, the aim is to place an input text into one of several\npredefined categories, as demonstrated in the following prompt:\nInstruction: Classify the following text into one of these categories: history,\ntech, gardening.\nText: Headphones provide immersive audio experiences for music lovers and\ngamers alike.\nOutput:\nThe text \"Headphones provide immersive audio experiences for music lovers\nand gamers alike.\" should be classified as \"tech\" because it discusses\ntechnology-related products and their functionality.\n\n\nThe output is overly detailed. Let's revise the prompt and specify to only\ndisplay the category.\nInstruction: Classify the following text into one of these categories: history,\ntech, gardening.\nText: Headphones provide immersive audio experiences for music lovers and\ngamers alike.\nOutput: only the category\nOutput:\nTech\nNow, categorize another sentence by applying the same prompt and only\naltering the text:\nInstruction: Classify the following text into one of these categories: history,\ntech, gardening.\nText: Julius Caesar was a Roman general and statesman who played a pivotal\nrole in the demise of the Roman Republic and the rise of the Roman Empire.\nOutput: only the category\nOutput:\nHistory\nIn summary, in a standard text classification prompt, you find three\ncomponents: an Instruction, the input Text, and an Output specification.\nNow, let's delve into a slightly specialized text classification: sentiment\nanalysis.\n\n\n2.4.2 Sentiment analysis\nSentiment analysis is a specific type of text classification that aims to\ndetermine whether a given text is perceived as positive, neutral, or negative.\nBelow are three sample prompts you can experiment with independently.\nFirst prompt example:\nInstruction: Classify the following text as positive, neutral or negative\nText: Apple have announced stellar quarterly results\nExpected output:\nPositive\nSecond prompt example:\nInstruction: Classify the following text as positive, neutral or negative\nText: this is the worst movie I have watched this month\nExpected output:\nNegative\nThird prompt example:\nInstruction: Classify the following text as positive, neutral or negative\nText: The customer bought a green dress\nExpected output:\n\n\nNeutral\nIf you have a list of texts and wish to analyze their sentiments collectively,\nyou can input them all into a single prompt:\nInstruction: classify the sentiment of following stock reports as positive,\nneutral or negative\nStock 1: Apple: the launch of the new iPhone has been a success\nStock 2: Nvidia: sales are being propelled by consumer demand on LLMs\nStock 3: GX oil: demand of carbon based energy is dropping thanks to the\nincrease of renewable energy\nOutput: output a table with the columns \"stock name\", \"sentiment\"\nExpected output:\n| Stock Name | Sentiment |\n|-------------|-----------|\n| Apple | positive |\n| Nvidia | positive |\n| GX oil | negative |\nAs you observe, in this scenario, the output is formatted in a table and can be\nreadily processed programmatically.\nNow, let's create a prompt for one of the most common LLM use cases: text\nsummarization.\n2.4.3 Text summarization\nCreating a text summarization prompt is simple: you only need to ask for a\n\n\nsummary of an input text with your preferred length.\nInstruction: write a 30 word summary for the following text\nText: Home PCs from the 1980s were iconic pioneers of personal computing.\nComputers like the Apple II, IBM PC, and Commodore 64 revolutionized\nhow we lived and worked. They introduced us to word processing, gaming,\nand the possibilities of home computing. The graphical user interface, as seen\nin the Apple Macintosh, changed the way we interacted with machines. These\nPCs, though less powerful by today's standards, ignited a digital revolution,\nshaping the technology landscape we know today. They hold a special place\nin history, symbolizing the dawn of a new era in computing that laid the\nfoundation for the modern digital age.\nOutput:\n1980s home PCs like Apple II, IBM PC, Commodore 64 pioneered personal\ncomputing, introducing word processing, gaming, and GUIs. They shaped\ntoday's tech, symbolizing a digital era's dawn.\nYou can also ask the LLM to provide a summary for a text it's already\nfamiliar with:\nInstruction: Write a one sentence summary of the Lord of the Rings\nOutput:\n\"The Lord of the Rings\" follows the epic journey of a diverse group of\ncharacters as they seek to destroy a powerful ring and save Middle-earth from\nthe dark lord Sauron.\n2.4.4 Composing text\nLLMs are part of the AI category called \"generative AI.\" While LLMs can\n\n\nhandle audio and video, with text they generate new content, like a paragraph\nor sentence, based on a list of facts, for example:\nWrite a piece on the diver watches, mentioning the following facts:\nthe Rolex Oyster Perpetual was among the first to be water resistant\nOmega produced the Seamaster in 1948 as the first purpose designed\ndiver watch was the Seamaster\nthe Seamaster set a world record of immersion of 62.5m in 1955.\nRolex produced the first Submariner in 1953\nthe Submariner became popular also because of the association with\nJames Bond until 1995\nJames Bond started to wear the Seamaster in 1995\nOutput:\nDiver Watches: A Journey into Time's Abyss\nDiver watches have a storied history, encapsulating the spirit of adventure\nand resilience against the unforgiving depths of the oceans. These timepieces\nnot only serve as reliable companions for professional divers but have also\nbecome iconic symbols of style and innovation. Here, we delve into the\nfascinating world of diver watches, highlighting key milestones and\nlegendary timepieces that have defined this genre.\nThe Rolex Oyster Perpetual, introduced in the early 20th century, played a\npioneering role in the evolution of diver watches. Among its notable features\nwas its remarkable water-resistant design, which set the stage for the\ndevelopment of truly waterproof timepieces. This innovation marked a\ncrucial step in ensuring that watches could withstand the rigors of underwater\nexploration.\nOmega, a respected Swiss watchmaker, made its mark on the diver watch\nscene in 1948 with the introduction of the Seamaster. This timepiece was a\ngame-changer as it was purposefully designed for divers, combining style and\nfunctionality. It was a clear indication that dive watches were evolving\n\n\nbeyond mere timekeeping instruments into specialized tools for underwater\nadventurers.\nThe Seamaster didn't stop at design; it also showcased its capabilities in the\ndepths. In 1955, it set a remarkable world record for immersion, descending\nto an astonishing 62.5 meters underwater. This achievement solidified the\nSeamaster's reputation as a reliable and robust diver's companion.\nNot to be outdone, Rolex joined the fray by producing the first Submariner in\n1953. This iconic watch quickly became synonymous with underwater\nexploration and adventure. Its success was further boosted by its association\nwith the fictional British spy, James Bond. The Submariner's appearance on\nthe wrist of the suave secret agent added a layer of mystique and\nsophistication to this already legendary timepiece. It's a partnership that\nendured until 1995, firmly establishing the Submariner as a symbol of\ntimeless style and rugged functionality.\nHowever, the tides of popularity shifted in 1995 when James Bond, portrayed\nby Pierce Brosnan, made a bold move by strapping on an Omega Seamaster\nin \"GoldenEye.\" This marked the beginning of a new era for the Seamaster,\nas it became the watch of choice for the world's most famous secret agent.\nThe Seamaster seamlessly blended style, innovation, and precision, making it\na perfect match for 007's daring missions.\nIn conclusion, diver watches have come a long way since their inception. The\nRolex Oyster Perpetual and Omega Seamaster stand as pioneers, shaping the\npath for future innovations. These watches not only broke records but also\ncaptured the imaginations of generations. The Rolex Submariner and the\nOmega Seamaster, each with its unique charm, continue to be revered in the\nworld of horology, leaving an indelible mark on the wrists of adventurers and\nstyle enthusiasts alike.\nYou may notice that ChatGPT has used the limited information to create a\nquite lengthy article. On the other hand, you might have expected a shorter\npiece and may not be a fan of the complex vocabulary or somewhat formal\ntone it used. In such a situation, try restructuring the prompt more\nsystematically to get the generated text to match your preferred length,\nvocabulary, and tone:\n\n\nPersona: you are a watch enthusiast, experienced in writing high quality\narticles for watch magazines for a general audience\nInstruction: Write a piece on the diver watches\nFacts to mention:\nthe Rolex Oyster Perpetual was among the first to be water resistant\nOmega produced the Seamaster in 1948 as the first purpose designed\ndiver watch was the Seamaster\nthe Seamaster set a world record of immersion of 62.5m in 1955.\nRolex produced the first Submariner in 1953\nthe Submariner became popular also because of the association with\nJames Bond until 1995\nJames Bond started to wear the Seamaster in 1995\nTone: use a sober, knowledgeable but not too formal tone and use everyday\nvocabulary\nOutput: the length of the article should be 150 words at most\nOutput:\n**Diver Watches: Timepieces of Exploration and Style**\nDiver watches have long been cherished for their blend of functionality and\naesthetics, a perfect companion for both the adventurous spirit and the style-\nconscious.\nRolex deserves recognition for pioneering water resistance with the Oyster\nPerpetual, setting the stage for a new era in watchmaking.\nOmega, however, made a significant splash in 1948 with the Seamaster, the\nworld's first purpose-designed diver watch. Its 1955 record dive of 62.5\nmeters underlines its prowess in the deep.\nRolex didn't lag behind, introducing the Submariner in 1953, a watch that\n",
      "page_number": 50,
      "chapter_number": 7,
      "summary": "Output:\nThe text \"Headphones provide immersive audio experiences for music lovers\nand gamers alike.\" should be classified as \"tech\" because it discusses\ntechnology-related products and their functionality Key topics include prompt, text, and watched.",
      "keywords": [
        "text",
        "prompt",
        "Seamaster",
        "output",
        "Rolex Oyster Perpetual",
        "diver watches",
        "prompt template",
        "text classification",
        "diver",
        "Submariner",
        "text classification prompt",
        "James Bond",
        "watches",
        "Omega Seamaster",
        "Rolex"
      ],
      "concepts": [
        "prompt",
        "text",
        "watched",
        "watches",
        "diver",
        "llm",
        "stock",
        "functions",
        "function",
        "functionality"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 55,
          "title": "Segment 55 (pages 536-543)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 7,
          "title": "Segment 7 (pages 50-57)",
          "relevance_score": 0.45,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 38,
          "title": "Segment 38 (pages 353-360)",
          "relevance_score": 0.44,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 16,
          "title": "Segment 16 (pages 131-138)",
          "relevance_score": 0.42,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 14,
          "title": "Segment 14 (pages 111-118)",
          "relevance_score": 0.41,
          "method": "api"
        }
      ]
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 60-67)",
      "start_page": 60,
      "end_page": 67,
      "detection_method": "topic_boundary",
      "content": "became synonymous with exploration, partly due to its affiliation with James\nBond on-screen.\nSpeaking of James Bond, it's worth noting that in 1995, the iconic secret\nagent started sporting the Omega Seamaster, a move that reinforced the\nSeamaster's reputation as a sophisticated yet rugged timepiece for those who\ndare to explore both land and sea. These diver watches are more than\ninstruments; they're symbols of adventure and style.\nBy adding more details to the prompt, the article has been generated exactly\nas you wanted it.\nnote\nCreating a well-structured prompt with defined sections (Persona, Instruction,\nFacts, Tone, Output Format) helps the LLM understand your requirements\nbetter and generate the expected text.\n2.4.5 Question answering\nThe ability of LLMs to \"understand\" text makes them suitable not only for\nsummarizing but also for retrieving specific information, known as the\n\"Question answering\" use case.\nNow, let's go back to the same text we used in the summarization example\nand ask focused questions about its content.\nInstruction: read the following text, and tell me which home computers were\npopular in the 80s.\nText: Home PCs from the 1980s were iconic pioneers of personal computing.\nComputers like the Apple II, IBM PC, and Commodore 64 revolutionized\nhow we lived and worked. They introduced us to word processing, gaming,\nand the possibilities of home computing. The graphical user interface, as seen\nin the Apple Macintosh, changed the way we interacted with machines. These\nPCs, though less powerful by today's standards, ignited a digital revolution,\nshaping the technology landscape we know today. They hold a special place\nin history, symbolizing the dawn of a new era in computing that laid the\n\n\nfoundation for the modern digital age.\nOutput: just output the names of the computers in a list\nOutput:\nThe popular home computers in the 1980s were:\n1. Apple II\n2. IBM PC\n3. Commodore 64\n4. Apple Macintosh\nQuestion answering is such a common use case most LLMs understand Q\nand A as shorthand for Question and Answer. For example:\nText: Java is a popular programming language that compiles code into\nbytecode, which is executed by the Java Virtual Machine (JVM) for\nplatform-independent application development.\nQ: Where is Java code executed?\nA:\nOutput:\nJava code is executed by the Java Virtual Machine (JVM).\nLLMs are skilled not just at finding requested information in a text but also at\nmaking logical deductions. This will become apparent in the next section.\n2.4.6 Reasoning\n\n\nWhile reasoning is a powerful capability, it's an area where LLMs aren't\nexceptional yet. Therefore, you might need more advanced prompt\nengineering techniques to get the results you want.\nHere's a simple example involving mathematical reasoning:\nInstruction: Add the square numbers in this sequence: 19, 13, 1, 17, 4, 64,\n900\nOutput:\nThe square numbers in the sequence 19, 13, 1, 17, 4, 64, and 900 are 1, 4, 64,\nand 900. To find their sum:\n1 + 4 + 64 + 900 = 969\nSo, the sum of the square numbers in this sequence is 969.\nChatGPT identified the square numbers, picked them out, and then figured\nout their total. The next example is more challenging, involving palindromes\n—numbers that read the same forward and backward, such as 1661 or 27872:\nInstruction:\nSum the palindromes in this sequence: 13, 1331, 121, 73, 99, 56, 232, 7\nI got this output:\nThe palindromes in the sequence 13, 1331, 121, 73, 99, 56, 232, and 7 are\n1331, 121, 232, and 7. To find their sum:\n1331 + 121 + 232 + 7 = 1691\nSo, the sum of the palindromes in this sequence is 1691.\n\n\nThis is incorrect! A palindrome is a number that reads the same forward and\nbackward. Therefore, 99 should have been considered a palindrome, but it\nwasn't.\nI'll be frank with you: the output above is what I encountered in the early\nstages of writing this book. However, you should now see a different and,\nmore importantly, accurate output. It's still valuable to explain how I\ncorrected ChatGPT's mistake before OpenAI addressed this in a later version\nof the model.\nSo, how did I fix it? Since the LLM didn’t fully grasp the concept of a\npalindrome, I “taught” ChatGPT to correctly identify all relevant cases, as I'll\nexplain in the next section. Before moving on, take a moment to review the\ntypes of prompts you've learned. I've summarized them in Table 2.1 below\nfor your reference.\nTable 2.1 Prompt types and examples\nPrompt type\nPrompt example\nText classification\nClassify the following text into one of these categories:\nhistory, tech, gardening.\nSentiment\nanalysis\nClassify the following text as positive, neutral or\nnegative\nText\nsummarization\nWrite a 30 word summary for the following text\nComposing Text\nWrite a piece on the diver watches, mentioning the\nfollowing facts:\n- the Rolex Oyster Perpetual was among the first to be\nwater resistant\n- Omega produced the Seamaster in 1948 as the first\npurpose designed diver watch was the Seamaster\n…\nQuestion\nanswering\nRead the following text, and tell me which home\ncomputers were popular in the 80s.\nReasoning\nAdd the square numbers in this sequence: 19, 13, 1, 17,\n4, 64, 900\n\n\n2.5 Reasoning in detail\nWhen developing LLM applications, particularly those requiring complex\nreasoning, it's important to understand the limitations of different prompt\ntechniques. For instance, \"zero-shot learning\" is a method where you provide\nno examples, expecting the LLM to grasp and perform the task based purely\non the instructions given. While this approach can work well in some\nscenarios, it doesn't always yield accurate results for more intricate tasks,\nsuch as teaching an LLM to identify palindromes. In the following sections,\nI'll demonstrate how to go beyond zero-shot learning to refine LLM prompts\nfor more reliable performance in reasoning tasks.\n2.5.1 One-shot learning\nWhen you provide just one example in the prompt, it's called \"one-shot\nlearning.\" Here's how I attempted to address the palindrome problem by\ngiving a single example:\nInstruction: Sum the palindromes in this sequence: 13, 1331, 121, 73, 99, 56,\n232, 7\nExample: 33 is a palindrome\nI got this output:\nI apologize for the previous oversight. Let's correctly identify and sum the\npalindromes in this sequence: 13, 1331, 121, 73, 99, 56, 232, and 7.\nThe palindromes in the sequence are 1331, 121, 232, and 7. To find their\nsum:\n1331 + 121 + 232 + 7 = 1691\nSo, the sum of the palindromes in this sequence is indeed 1691.\nUnfortunately, even after using one-shot learning, the issue persisted. So, I\n\n\nmade adjustments to the prompt and included two examples.\n2.5.2 Two-shot learning\nHere is the amended prompt I used:\nInstruction: Sum the palindromes in this sequence: 13, 1331, 121, 73, 99, 56,\n232, 7\nExamples:\n33 is a palindrome\n44 is a palindrome\nThis is the output I got:\nCertainly, let's identify and sum the palindromes in the sequence: 13, 1331,\n121, 73, 99, 56, 232, and 7.\nThe palindromes in this sequence are 1331, 121, 232, and 7. To find their\nsum:\n1331 + 121 + 232 + 7 = 1691\nSo, the sum of the palindromes in this sequence is 1691.\nStill incorrect: 99 got overlooked once again.\nFollowing another unsuccessful attempt, I explored different approaches and\nfound a standard solution. It suggested breaking down the problem into\n\"steps\" and providing clear instructions for each step. This is similar to\nguiding a colleague who may be struggling with a specific step in a\nprocedure.\n2.5.3 Providing steps\n\n\nThis is the amended prompt I entered:\nInstruction: Sum the palindromes in this sequence: 13, 1331, 121, 73, 99, 56\n232, 7\nExamples:\n33 is a palindrome\n44 is a palindrome\nSteps: 1) identify the palindromes; 2) add them up\nI got this output:\nCertainly, let's follow your instructions step by step:\nStep 1: Identify the palindromes in the sequence.\nPalindromes in the sequence:\n1331\n121\n99\n232\n7\nStep 2: Add them up.\n1331 + 121 + 99 + 232 + 7 = 1790\nSo, the sum of the palindromes in the sequence is 1790.\nFinally, I nailed it!\nInterestingly, it appears that ChatGPT has undergone a similar training\nprocess (on a much larger scale, of course) to fix this issue. It's likely that\nother users encountered the same problem.\n\n\nIf you enter the original prompt:\nInstruction:\nSum the palindromes in this sequence: 13, 1331, 121, 73, 99, 56, 232, 7\nYou'll now see the output below.\nLet's identify the palindromes in the given sequence:\n1331 is a palindrome\n121 is a palindrome\n99 is a palindrome\n232 is a palindrome\n7 is a palindrome\nNow, let's add these palindromes together:\n1331 + 121 + 99 + 232 + 7 = 1790\nTherefore, the sum of the palindromes in the sequence is 1790.\nIt appears ChatGPT is addressing the problem step by step, much like how I\nresolved the issue.\nI found it interesting to share an example that's no longer relevant. This is\nsomething you might experience while working with LLMs: you could invest\ntime in teaching the LLM certain tasks using advanced prompt engineering\ntechniques, only to find that a newer model version can handle the task\nwithout any additional training. For instance, OpenAI's o1 model performed\nreasoning tasks with minimal prompts, which previously required more\ncomplex prompts like the few-shot techniques you're about to learn.\n",
      "page_number": 60,
      "chapter_number": 8,
      "summary": "This chapter covers segment 8 (pages 60-67). Key topics include prompt, text, and instruction.",
      "keywords": [
        "palindromes",
        "sequence",
        "sum",
        "text",
        "prompt",
        "Output",
        "Sum the palindromes",
        "LLM",
        "Reasoning",
        "LLMs",
        "Java Virtual Machine",
        "James Bond",
        "Instruction",
        "Seamaster",
        "Question answering"
      ],
      "concepts": [
        "prompt",
        "text",
        "instruction",
        "instructions",
        "output",
        "examples",
        "reasoning",
        "steps",
        "palindromes",
        "user"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents In Action",
          "chapter": 7,
          "title": "Segment 7 (pages 50-57)",
          "relevance_score": 0.74,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 14,
          "title": "Segment 14 (pages 113-121)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 23,
          "title": "Segment 23 (pages 457-476)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 4,
          "title": "Segment 4 (pages 25-32)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 30,
          "title": "Segment 30 (pages 261-269)",
          "relevance_score": 0.62,
          "method": "api"
        }
      ]
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 68-75)",
      "start_page": 68,
      "end_page": 75,
      "detection_method": "topic_boundary",
      "content": "2.5.4 Few-shot learning\nIn some cases, providing only two examples may not be enough to teach the\nLLM a new concept. In such instances, you need to provide several\nexamples, known as \"few-shot learning.\" To demonstrate this, let me create a\ngame where a number can be classified as Abra, Kadabra, or Abra Kadabra.\nLet's see what happens when no information about the classification rules is\nprovided.\nClassify the following numbers as Abra, Kadabra or Abra Kadabra:\n3, 5, 8, 2, 7, 12, 13\nOutput:\nThe classification of numbers into \"Abra,\" \"Kadabra,\" or \"Abra Kadabra\"\ndoesn't follow a conventional mathematical pattern. These are just words, not\nrelated to numbers. Therefore, it's not possible to classify the numbers you\nprovided as \"Abra,\" \"Kadabra,\" or \"Abra Kadabra\" based on any\nmathematical or logical criteria.\nChatGPT hasn't experienced any hallucinations but rightfully pointed out that\nyou haven't clarified the classification rules.\nI'll show you a prompt that, instead of directly explaining the rules, offers an\ninterpretation based on a specific list of numbers, different from the ones I\noriginally asked to be addressed:\nClassify the following numbers as Abra, Kadabra or Abra Kadabra:\n3, 4, 5, 7, 8, 10, 11, 13, 35\nExamples:\n6 // not divisible by 5, not divisible by 7 // None\n\n\n15 // divisible by 5, not divisible by 7 // Abra\n12 // not divisible by 5, not divisible by 7 // None\n21 // not divisible by 5, divisible by 7 // Kadabra\n70 // divisible by 5, divisible by 7 // Abra Kadabra\nObserve the format I've employed for every example: I've delineated each\nfield with a double forward slash (//), although I haven't explicitly outlined\nthe meaning of each field. Trusting the LLM to decipher it.\nHere is the output:\nLet's classify the numbers as Abra, Kadabra, or Abra Kadabra based on\nwhether they are divisible by 5 and/or 7:\n3: Not divisible by 5, not divisible by 7 - None\n4: Not divisible by 5, not divisible by 7 - None\n5: Divisible by 5, not divisible by 7 - Abra\n7: Not divisible by 5, divisible by 7 - Kadabra\n8: Not divisible by 5, not divisible by 7 - None\n10: Divisible by 5, not divisible by 7 - Abra\n11: Not divisible by 5, not divisible by 7 - None\n13: Not divisible by 5, not divisible by 7 - None\n35: Divisible by 5, divisible by 7 - Abra Kadabra\nSo, here's the classification:\n\n\nAbra: 5, 10, 35\nKadabra: 7\nAbra Kadabra: 35\nNone: 3, 4, 8, 11, 13\nChatGPT not only provided accurate results but also explained the reasoning\nbehind them. It successfully deduced the general rules from the given\nexamples and articulated them well. Quite impressive, isn't it?\nNote\nYou may be familiar with a similar \"number classification game\" or\n“Divisibility Test Algorithm”\n(https://stackoverflow.com/questions/20486231/divisibility-test-algorithm)\nwhere a number divisible by 3 is labeled \"Foo,\" a number divisible by 5 is\nlabeled \"Bar,\" and a number divisible by both 3 and 5 is labeled \"Foo Bar.\" I\nhaven't used this as an example because ChatGPT already knows it and can\ncorrectly classify numbers without examples, using \"zero-shot\" learning.\n2.5.5 Implementing few-shot learning with LangChain\nLet’s implement the AbraKadabra game with LangChain. First, establish a\nconnection to the LLM, as you did previously:\nfrom langchain_openai import ChatOpenAI\nllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,\n                model_name=\"gpt-4o-mini\")\nNow, define and run the same few-shot prompt you executed previously\ndirectly against ChatGPT:\nprompt_input = \"\"\"Classify the following numbers as Abra, Kadabra \n3, 4, 5, 7, 8, 10, 11, 13, 35\nExamples: \n\n\n6 // not divisible by 5, not divisible by 7 // None\n15 // divisible by 5, not divisible by 7 // Abra\n12 // not divisible by 5, not divisible by 7 // None\n21 // not divisible by 5, divisible by 7 // Kadabra\n70 // divisible by 5, divisible by 7 // Abra Kadabra\n\"\"\"\nresponse = llm.invoke(prompt_input)\nprint(response.content)\nThe output, is, as expected:\n3 // not divisible by 5, not divisible by 7 // None\n4 // not divisible by 5, not divisible by 7 // None\n5 // divisible by 5, not divisible by 7 // Abra\n7 // not divisible by 5, divisible by 7 // Kadabra\n8 // not divisible by 5, not divisible by 7 // None\n10 // divisible by 5, not divisible by 7 // Abra\n11 // not divisible by 5, not divisible by 7 // None\n13 // not divisible by 5, not divisible by 7 // None\n35 // divisible by 5, divisible by 7 // Abra Kadabra\nThe output is accurate, but the implementation isn't ideal because the\nexamples are hardcoded in the prompt. LangChain provides a cleaner\nsolution for creating a few-shot prompt. It lets you separate the training\nexamples from the prompt template and inject them later, as shown in listing\n2.1.\nListing 2.1 Few-shot prompt using FewShotPromptTemplate\nfrom langchain_core.prompts.few_shot import FewShotPromptTemplate\nfrom langchain_core.prompts.prompt import PromptTemplate\nexamples = [\n {\n     \"number\": 6,\n     \"reasoning\": \"not divisible by 5 nor by 7\",\n     \"result\": \"None\"\n },\n {\n     \"number\": 15,\n     \"reasoning\": \"divisible by 5 but not by 7\",\n     \"result\": \"Abra\"\n },\n {\n\n\n     \"number\": 12,\n     \"reasoning\": \"not divisible by 5 nor by 7\",\n     \"result\": \"None\"\n },\n {\n     \"number\": 21,\n     \"reasoning\": \"divisible by 7 but not by 5\",\n     \"result\": \"Kadabra\"\n },\n {\n     \"number\": 70,\n     \"reasoning\": \"divisible by 5 and by 7\",\n     \"result\": \"Abra Kadabra\"\n } ]\nexample_prompt = PromptTemplate(input_variables=[\"number\", \"reaso\nfew_shot_prompt = FewShotPromptTemplate(\n   examples=examples,\n   example_prompt=example_prompt,\n   suffix=\"Classify the following numbers as Abra, Kadabra or Ab\n   input_variables=[\"comma_delimited_input_numbers\"]\n)\nprompt_input = few_shot_prompt.format(comma_delimited_input_numbe\nresponse = llm.invoke(prompt_input)\nprint(response.content)\nAnd the result is still correct:\n3 \\ not divisible by 5 nor by 7 \\ None\n4 \\ not divisible by 5 nor by 7 \\ None\n5 \\ divisible by 5 but not by 7 \\ Abra\n7 \\ divisible by 7 but not by 5 \\ Kadabra\n8 \\ not divisible by 5 nor by 7 \\ None\n10 \\ divisible by 5 but not by 7 \\ Abra\n11 \\ not divisible by 5 nor by 7 \\ None\n13 \\ not divisible by 5 nor by 7 \\ None\n35 \\ divisible by 5 and by 7 \\ Abra Kadabra\nBefore concluding this section, I want to introduce you to a technique known\nas \"chain of thought,\" which blends \"providing logical steps\" with \"few-shot\nlearning.\"\n2.5.6 Chain of Thought (CoT)\n\n\nI'm making up a new mathematical definition: I'll name a sequence of\nnumbers \"Strange\" if it contains at least two odd numbers, and the sum of all\nodd numbers is divisible by 3.\nInitially, I check whether the LLM is unfamiliar with the rules of \"Strange\"\nsequences:\nIs the following sequence \"Strange\"?\n1, 3, 4, 5, 7, 10, 15, 18, 22, 24\nOutput:\nThe sequence you provided appears to be increasing, but it doesn't follow a\ncommon mathematical pattern like arithmetic or geometric progressions.\nWhether it's considered \"strange\" depends on the context or criteria you're\nusing to define \"strange.\" In a mathematical sense, it might be considered an\nunusual or non-standard sequence. If you have specific criteria or rules that\ndefine what a \"strange\" sequence is, please provide more details for a more\nprecise classification.\nAs expected, ChatGPT doesn't seem to understand the concept of a \"Strange\nsequence,\" reaffirming that, luckily, it hasn't developed psychic abilities…\nyet.\nI will now create a prompt using the Chain of Thought technique, as\nintroduced by Wei et al (https://arxiv.org/abs/2201.11903). This prompt, an\nextension of a few-shot learning prompt, will include:\n1. Several sample sequences for few-shot learning\n2. For each sample sequence, an explanation of the logical steps to\ndetermine if a sequence is Strange\nQ: Is the following sequence \"Strange\"?\n3, 4, 5, 7, 10, 18, 22, 24\n\n\nA:\nExamples:\nQ: is the following sequence strange: 1, 4, 6, 8, 20\nA: 1 is an odd number; I need at least two odd numbers // Not Strange\nQ: is the following sequence strange: 5, 6, 7, 8, 20\nA: 5 and 7 are odd numbers; the sum of 5 and 7 is 12; 12 is divisible by 3 //\nStrange\nQ: is the following sequence strange: 1, 5, 6, 7, 8, 20\nA: 1, 5 and 7 are odd numbers; the sum of 1, 5 and 7 is 13; 13 is not divisible\nby 3 // Not Strange\nQ: is the following sequence strange: 5, 6, 7, 8, 9, 20\nA: 5, 7, 9 are odd numbers; the sum of 5, 7 and 9 is 21; 21 is divisible by 3 //\nStrange\nOutput:\nThank you for providing the examples and clarifications. Based on the\ncriteria you've presented:\nThe sequence \"3, 4, 5, 7, 10, 18, 22, 24\" has 3 odd numbers: 3, 5, and 7. The\nsum of these odd numbers is 15. 15 is divisible by 3. So, according to the\ncriteria, this sequence is **Strange**.\nChatGPT gave the right answer and explained its thought process. To\nreinforce your understanding, try using a similar prompt with a different set\nof numbers. Also, observe how it responds when the sequence contains just\none odd number.\n\n\nIn essence, in this section you've discovered how to introduce ChatGPT to a\nnew task by offering various examples or outlining the necessary steps. This\napproach is known as \"in-context\" learning, as you're instructing the LLM\nusing information provided within the context of a prompt.\nDefinition\n“In-context learning” refers to any method that instructs an LLM on an\nunfamiliar task or instruction requested by the user, utilizing examples within\nthe prompt context. Common techniques include one-shot, two-shot, or few-\nshot learning, as well as providing step-by-step guidance, often referred to as\na \"chain of thought.\" Compared to fine-tuning, in-context learning is a more\ncost-effective and less resource-intensive approach, as it doesn't demand\nhigh-end hardware resources like GPUs or in-depth knowledge of\ntransformer architectures.\nThis section covered a range of in-context learning prompts, which I have\nsummarized, for convenience, in table 2.2.\nTable 2.2 In-context learning prompts\nIn-Context learning\ntechnique\nExplanation\nZero-shot learning\nNo example is provided in the prompt\nOne-shot learning\nOne example is provided in the prompt\nTwo-shot learning\nTwo examples are provided in the prompt\nFew-shot learning\nA number of examples are provided in the\nprompt\nChain of Thought\nA number of examples are provided in the\nprompt, and for each example, all the logical\nsteps to achieve an objective are clearly\nexplained\nIf you want to explore advanced techniques beyond Chain of Thought, check\nthe related sidebar. It includes detailed pointers on Tree of Thought and\nThread of Thought, two powerful methods that enhance reasoning and\nproblem-solving in language models.\n",
      "page_number": 68,
      "chapter_number": 9,
      "summary": "LangChain provides a cleaner\nsolution for creating a few-shot prompt Key topics include number, providing, and provide. In such instances, you need to provide several\nexamples, known as \"few-shot learning.\" To demonstrate this, let me create a\ngame where a number can be classified as Abra, Kadabra, or Abra Kadabra.",
      "keywords": [
        "Abra Kadabra",
        "divisible",
        "Abra",
        "Kadabra",
        "Abra Kadabra based",
        "numbers",
        "Strange",
        "number divisible",
        "prompt",
        "learning",
        "sequence",
        "odd numbers",
        "numbers as Abra",
        "thought",
        "chain of thought"
      ],
      "concepts": [
        "number",
        "providing",
        "provide",
        "prompt",
        "examples",
        "strange",
        "technique",
        "sequence",
        "mathematical",
        "thought"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 14,
          "title": "Segment 14 (pages 113-121)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 8,
          "title": "Segment 8 (pages 61-70)",
          "relevance_score": 0.45,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 7,
          "title": "Segment 7 (pages 50-57)",
          "relevance_score": 0.44,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 21,
          "title": "Segment 21 (pages 417-434)",
          "relevance_score": 0.44,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 35,
          "title": "Segment 35 (pages 292-299)",
          "relevance_score": 0.43,
          "method": "api"
        }
      ]
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 76-83)",
      "start_page": 76,
      "end_page": 83,
      "detection_method": "topic_boundary",
      "content": "Beyond Chain of Thought\nChain of Thought improves how language models handle complex reasoning\nby breaking problems into smaller, logical steps. However, it has limitations,\nsuch as missing deeper exploration or struggling with messy contexts. Two\nadvanced techniques address these gaps: Tree of Thought (ToT) and Thread\nof Thought (ThoT):\nTree of Thought (ToT): ToT improves problem-solving by allowing\nlanguage models to explore multiple reasoning paths. Traditional models\nmake token-by-token decisions, which limits their ability to plan ahead\nor revisit earlier choices. ToT structures the process into coherent steps,\nenabling models to evaluate and refine options as they go. This method\n(https://arxiv.org/abs/2305.10601) is highly effective in tasks requiring\nstrategic thinking, such as the Game of 24, creative writing, and mini\ncrosswords. For example, ToT helped GPT-4 solve 74% of Game of 24\nproblems, compared to just 4% using Chain of Thought. This approach\nimproves deliberate decision-making, backtracking, and strategic\nforesight.\nThread of Thought (ThoT): ThoT addresses challenges with chaotic\ncontexts, where irrelevant details distract models or lead to errors.\nInspired by human cognition, ThoT (https://arxiv.org/abs/2311.08734)\nsystematically breaks down and analyzes large chunks of information\nwhile focusing on relevant details. It works as a modular add-on that\nintegrates with various models and prompts. Experiments on datasets\nlike PopQA, EntityQ, and a Multi-Turn Conversation Response dataset\nshow ThoT significantly enhances reasoning accuracy. It excels in\nfiltering noise and maintaining focus in complex contexts.\nBoth techniques push language models beyond simple response generation,\nimproving reasoning, planning, and decision-making in structured and\nchaotic scenarios. For technical details and examples, refer to the published\nresearch and code repositories.\nBefore closing this chapter, let's try to extract a standard prompt format from\nour examination of various use cases up to this point.\n\n\n2.6 Prompt structure\nCombining all the prompt elements from earlier sections results in the\nfollowing generalized structure:\nPersona: Specify the role you want the language model (LLM) to\nembody. Whether a skilled marketing manager, an award-winning sci-fi\nauthor, or a proficient copy editor for a computing magazine, set the\nexpectations high.\nContext: Provide detailed background information to help the LLM\ncomprehend the context surrounding your request. Precision enhances\nthe likelihood of aligning with your expectations.\nInstruction: Clearly define the action you want the LLM to take with\nyour input text. Whether it's translation, writing, rewriting, improving,\nclassifying, ordering, or finding, be explicit.\nInput: This refers to specific context details, which could take the form\nof a paragraph, a query, or a list of key points. You may choose to label\nit accordingly, such as \"Text\" or \"Question.\", etc\nSteps: Outline the processing steps the LLM should follow to generate\nthe output.\nTone: Specify the desired tone of the LLM's answer—formal, informal,\nwitty, enthusiastic, sober, friendly, etc. Combinations are possible.\nOutput Format: Optionally specify the desired output format. If not\nspecified, the LLM assumes a text answer. However, you can request\nformats like a table, CSV file (with column names), JSON (with\nattribute names), or other structured formats.\nExamples: In cases where the LLM may lack sufficient training for\ncertain tasks, especially in text classification and reasoning, provide\nmini-inline training through examples. Even a few examples\nsignificantly enhance answer quality.\nThis list acts as a guide or checklist for organizing prompts. Customize it\nbased on your requirements, choosing relevant sections or making\nmodifications. Here's a prompt that includes many of these elements,\nstructured with clear sections:\nPersona: You are an experienced Large Language Model (LLM) developer\n\n\nand renowned speaker.\nContext: You have been invited to give a keynote speech for a LLM event.\nInstruction: Write the punch lines for the speech.\nInput: Include the following facts:\nLLMs have become mainstream with the launch of ChatGPT in\nNovember 2022\nmany popular LLMs and LLM based chatbots have been launched since\nthen, such as LLAMA-`2, Falcon180B, Bard.\nLLMs becoming as popular as search engines\nmany companies want to integrate LLMs in their applications\nTone: Use a witty but entertaining tone.\nOutput format: Present the text in two paragraphs of 5 lines each.\nOutput:\nLadies and gentlemen, as we embark on this linguistic odyssey, remember\nthis: In November 2022, ChatGPT didn't just enter the scene; it was the\nlinguistic equivalent of a rockstar's encore, setting the stage for an epic word-\ntastic journey! Today, with LLAMA-2, Falcon180B, and Bard, it's safe to say\nthat LLMs have become the Beatles of the digital era, making search engines\nfeel like yesterday's news.\nBut here's the kicker – companies are now falling over themselves,\ndesperately trying to get these LLMs into their applications. It's like the gold\nrush, but instead of panning for precious metal, we're sifting through bytes of\nbrilliant language. So, my friends, in this age where words wield the power,\nand LLMs are the mighty pens, we're not just scripting the future; we're\npenning a linguistic saga that's bound to be a best-seller! Welcome to the age\nof LLMs, where we're rewriting the rules, one sentence at a time, and words\nare the currency of change!\n\n\nNOTE\nStudies (for example “The Prompt Report: A Systematic Survey of\nPrompting Techniques”, https://arxiv.org/abs/2406.06608) have found that\nexplicitly naming different parts of a prompt tends to improve results.\nHowever, you don't have to name every section. Most LLMs can figure out\nthe purpose of the text in the prompt on their own. So, you can mix things up\nby naming some parts (like Question or Examples) and leaving out names for\nothers (like Context or Tone). Experiment and see what works best for your\ncase and the results you're getting.\nIf you want to delve deeper into prompt engineering, I highly recommend the\nfollowing resources:\nhttps://www.promptingguide.ai/techniques\nhttps://github.com/dair-ai/Prompt-Engineering-Guide\nhttps://github.com/promptslab/Awesome-Prompt-Engineering\n2.7 Summary\nA prompt is a specific request that guides the LLM, giving it instructions\nbased on provided background information.\nDifferent types of prompts are tailored for specific tasks, such as text\nclassification, sentiment analysis, text summarization, text generation, or\nquestion and answering.\nIf a prompt doesn't produce the expected result, it means the underlying\nLLM isn't familiar with the requested task and needs training.\nTraining involves enhancing the prompt with examples, categorized as\none-shot, two-shot, or few-shot learning based on the number of\nexamples given.\nAs you create various prompts, you might notice a common prompt\nstructure with sections like persona, context, instruction, input, steps,\nexamples, etc.\nYou should adapt the prompt structure to your use case by choosing\nrelevant sections or adding custom ones. You can also drop some\nexplicit section names\nWhile users often create and execute prompts directly through a chat\n\n\nLLM UI like ChatGPT, software developers typically automate prompt\ngeneration programmatically using directly the LLM API or LangChain.\nLangChain supports prompt engineering with a range of classes, from\nPromptTemplate to the more advanced FewShotPromptTemplate\nIn LLM application development, it's crucial to design parameterized\nprompts, also known as prompt templates.\n\n\n3 Summarizing text using\nLangChain\nThis chapter covers\nSummarization of large documents exceeding the LLM’s context\nwindow\nSummarization across multiple documents\nSummarization of structured data\nIn Chapter 1, you explored three major LLM application types:\nsummarization engines, chatbots, and autonomous agents. In this chapter,\nyou'll begin building practical summarization chains using LangChain, with a\nparticular focus on the LangChain Expression Language (LCEL) to handle\nvarious real-world scenarios. A chain is a sequence of connected operations\nwhere the output of one step becomes the input for the next—ideal for\nautomating tasks like summarization. This work lays the foundation for\nconstructing a more advanced summarization engine in the next chapter.\nSummarization engines are essential for automating the summarization of\nlarge document volumes, a task that would be impractical and costly to\nhandle manually, even with tools like ChatGPT. Starting with a\nsummarization engine is a practical entry point for developing LLM\napplications, providing a solid base for more complex projects and\nshowcasing LangChain’s capabilities, which we’ll further explore in later\nchapters.\nBefore we start building, we’ll look at different summarization techniques,\neach suited to specific scenarios like large documents, content consolidation,\nand handling structured data. Since you’ve already worked with summarizing\nsmall documents using a PromptTemplate in section 1.3.2, we’ll skip that and\nfocus on more complex examples.\n\n\n3.1 Summarizing a document bigger than context\nwindow\nAs mentioned in chapter 2, each LLM has a maximum prompt size, also\nreferred to as the \"context window.\"\nDEFINITION\nThe \"LLM context window\" represents the maximum size of the prompt\nprovided to an LLM, comprising instructions and context. Different LLMs\nhave varying token limits for the context window: GPT-3.5 used to accept up\nto 16K tokens, GPT-4 and Claude-3 up to 100K, and Gemini-1.5 up to 1M.\nAs the context window for popular LLMs continues to grow, you may still\nencounter situations where a document exceeds the token limit of your\nchosen model. In these cases, you can use a map-reduce approach, as\nillustrated in Figure 3.1.\nFigure 3.1 Summarizing a document bigger than the LLM’s context window: this involves\nsplitting the document into smaller chunks, summarizing each chunk, and then summarizing the\ncombined chunk summaries.\n\n\nLarge doc Large doc\n‘Summarizer LM\nBi sa\n\nsummarize\n\nThe large document\n\nissplitinto =\ndocument chunks |\ncocccettescedttteceltttescessvace Doe chunks !\n(return) 5 5 5 |\nThe map operation map(summarize) :\ntransforms document |\nnk summ\nchunks into a a\ncorresponding summaries\n(tum chunk summaries\nonsen E E\na }\nfinal summary\nF) i, 7\nreturn\n\ntransforms all the chunk\nSummaries into a single\ncondensed summary",
      "page_number": 76,
      "chapter_number": 10,
      "summary": "Before closing this chapter, let's try to extract a standard prompt format from\nour examination of various use cases up to this point Key topics include prompts, summarization, and summarize.",
      "keywords": [
        "LLM",
        "LLMs",
        "prompt",
        "Thought",
        "Context",
        "Tree of Thought",
        "context window",
        "Chain of Thought",
        "Thread of Thought",
        "summarization",
        "text",
        "LLM context window",
        "language models",
        "models",
        "language"
      ],
      "concepts": [
        "prompts",
        "summarization",
        "summarize",
        "llm",
        "contexts",
        "documents",
        "document",
        "chunk",
        "improves",
        "improving"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 14,
          "title": "Segment 14 (pages 113-121)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 7,
          "title": "Segment 7 (pages 50-57)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 21,
          "title": "Segment 21 (pages 417-434)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 27,
          "title": "Segment 27 (pages 233-241)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 21,
          "title": "Segment 21 (pages 168-175)",
          "relevance_score": 0.6,
          "method": "api"
        }
      ]
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 84-95)",
      "start_page": 84,
      "end_page": 95,
      "detection_method": "topic_boundary",
      "content": "This workflow involves splitting the document into smaller segments,\nsummarizing each one, and then summarizing the combined summaries.\nTo start, you need to split the text into chunks using a tokenizer. A tokenizer\nreads the text and breaks it into tokens, which are the smallest units of text,\noften parts of words. After tokenizing the document, the tokens are grouped\ninto chunks of a specific size. This lets you control the content size being\nprocessed by the LLM and ensures the token count stays within your LLM's\nprompt limit. I'll show you how to use TokenTextSplitter, part of the\ntiktoken package, a tokenizer developed by OpenAI.\nBegin by opening a terminal and creating a new folder named ch03 for this\nchapter’s code. Then, create and activate a virtual environment:\nC:\\Github\\building-llm-applications>md ch03\nC:\\Github\\building-llm-applications>cd ch03\nC:\\Github\\building-llm-applications\\ch03>python -m venv env_ch03\nC:\\Github\\building-llm-applications\\ch03>.\\env_ch03\\Scripts\\activ\n(env_ch03) C:\\Github\\building-llm-applications\\ch03>\nNext, install the required packages—tiktoken, notebook, and langchain. If\nyou’ve cloned the repo from GitHub, use pip install -r requirements.txt.\nOtherwise, install them as follows:\n(env_ch03) C:\\Github\\building-llm-applications\\ch03>pip install t\nOnce the installation is complete, start the Jupyter notebook:\n(env_ch03) C:\\Github\\Building-llm-applications\\ch03>jupyter noteb\nNow, open or create a notebook and name it 03-\nsummarization_examples.ipynb, then save it.\n3.1.1 Chunking the text into Document objects\nLet's summarize the book \"Moby Dick\" using its text file, Moby-Dick.txt,\ndownloaded from The Project Gutenberg. You can locate the Moby-Dick.txt\nfile in the chapter's subfolder on my Github page (https://github.com/roberto-\ninf/building-llm-applications/tree/main/ch03). Place this file in your ch03\n\n\nfolder and load the text into a variable:\nwith open(\"./Moby-Dick.txt\", 'r', encoding='utf-8') as f:\n   moby_dick_book = f.read()\nIMPORTANT\nKeep in mind that running the code on the full \"Moby Dick\" text can get\nexpensive. The provided \"Moby-Dick.txt\" file is a shorter version, containing\nonly 5 chapters and about 18,000 tokens. Running the code a few times with\nthis version shouldn’t cost much. However, if you plan to run many tests, you\nmay want to reduce the file size even more to save money. Each time you\nexecute a chain with an LLMChain block, you’ll be charged. If budget\nallows, you can use the full version of the book, labeled \"Moby-\nDick_ORIGINAL_EXPENSIVE.txt.\" The entire \"Moby Dick\" text has\naround 300,000 words, or about 350,000 tokens. Using GPT-4o, which costs\n$2.50 per million tokens, processing the full text would cost about $0.75.\nRunning it multiple times will add up. If you switch to GPT-4o-mini, which\ncosts $0.15 per million tokens, the cost drops to around $0.05, making it\nmuch cheaper.\nI’ll cover chunking strategies in detail—like chunking by size and content\nstructure—in chapter 8. For now, let's split the text into chunks of about\n3,000 tokens each to simulate a context window shorter than the GPT4o-mini\nmodel we’ll be using.\n3.1.2 Split\nFirst, import the necessary libraries:\nfrom langchain_openai import ChatOpenAI\nfrom langchain_text_splitters import TokenTextSplitter\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableLambda, RunnablePara\nimport getpass\nNext, retrieve your OpenAI API key using the getpass function:\nOPENAI_API_KEY = getpass.getpass('Enter your OPENAI_API_KEY')\n\n\nYou'll be prompted to enter your OPENAI_API_KEY. After that, instantiate the\nOpenAI model in a new notebook cell::\nllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,model_name=\"gpt-4o\nNow you're ready to set up the first chain, which will break the document into\nchunks of the specified size. In this chapter, you'll learn the basics of LCEL,\nwith a more detailed exploration in the next chapter:\ntext_chunks_chain = (\n   RunnableLambda(lambda x: \n       [\n           {\n               'chunk': text_chunk, \n           }\n           for text_chunk in \n              TokenTextSplitter(chunk_size=3000, chunk_overlap=1\n       ]\n   )\n)\nNote\nLangChain chains are made up of components that implement the Runnable\ninterface, an abstract Python class that defines how a component takes input,\nprocesses it, and returns output. Any class implementing Runnable can be\npart of a chain. RunnableLambda lets you turn any Python callable into a\n`Runnable`, making it easy to include custom functions in a LangChain\nchain. It's similar to Python's lambda expression, where you run code with a\nparameter and optionally return output without defining a full function. With\nRunnableLambda, you can create a chain component without writing a\nseparate class to implement the Runnable interface. In this example, the code\nwrapped by RunnableLambda takes input text as a string through the x\nparameter and passes it to the split_text() function, which breaks the text\ninto chunks.\n3.1.3 Map\nThe next step is to set up the \"map\" chain, which will run a summarization\nprompt for each document chunk. If you're unfamiliar with \"map-reduce,\"\n\n\nrefer to the sidebar for more details.\nsummarize_chunk_prompt_template = \"\"\"\nWrite a concise summary of the following text, and include the ma\nText: {chunk}\n\"\"\"\nsummarize_chunk_prompt = PromptTemplate.from_template(summarize_c\nsummarize_chunk_chain = summarize_chunk_prompt | llm\nsummarize_map_chain = (\n   RunnableParallel (\n       {\n           'summary': summarize_chunk_chain | StrOutputParser()  \n       }\n   )\n) \nNote\nIn this chain, I’ve used RunnableParallel, which is similar to\nRunnableLambda, but it operates on a sequence, processing each element in\nparallel. In this case, we’ll feed the sequence of text chunks to the\nsummarize_map_chain, and each chunk will be summarized in parallel by the\ninner summarize_map_chain.\nMap-reduce\nMap-reduce is a programming model for processing large datasets in two\nsteps. First, the \"map\" operation splits the data into smaller subsets, each\nprocessed independently by the same function. This step typically returns a\nlist of results, grouped by a key. Next is the \"reduce\" operation, where results\nfor each key are aggregated into a single outcome. The final output is a list of\nkey-value pairs, where the keys come from the map step and the values are\nthe result of aggregating the mapped data in the reduce step.\n3.1.4 Reduce\nSetting up the reduce chain, which summarizes the summaries from each\ndocument chunk, follows a process similar to the map chain but requires a bit\n\n\nmore setup. Start by defining the prompt template:\nsummarize_summaries_prompt_template = \"\"\"\nWrite a coincise summary of the following text, which joins sever\nText: {summaries}\n\"\"\"\nsummarize_summaries_prompt = PromptTemplate.from_template(summari\nNext, you can configure the reduce chain:\nsummarize_reduce_chain = (\n   RunnableLambda(lambda x: \n       {\n           'summaries': '\\n'.join([i['summary'] for i in x]), \n       })\n   | summarize_summaries_prompt \n   | llm \n   | StrOutputParser()\n)\nThe reduce chain includes a lambda function that combines the summaries\nfrom the map chain into a single string. This string is then processed by the\nsummarize_summaries_prompt prompt, which generates a final summary of\nthe combined content.\n3.1.5 Map-reduce combined chain\nFinally, we combine the document-splitting chain, the map chain, and the\nreduce chain into a single map-reduce chain:\nmap_reduce_chain = (\n  text_chunks_chain  #A\n  | summarize_map_chain.map()  #B\n  | summarize_reduce_chain  #C\n) \nThis setup efficiently splits the input document in chunks, summarizes each\nchunk, and then compiles those summaries into a final summary. The map()\nfunction on summarize_map_chain is essential to enable parallel processing\nof the chunks.\n\n\n3.1.6 Map-reduce execution\nEverything is set up. Start the map-reduce summarization of the large\ndocument with this command (if you are on the OpenAI free-tier, this might\nfail with a 429 Rate Limit error):\nsummary = map_reduce_chain.invoke(moby_dick_book)\nIf you run print(summary), you'll get output similar to the following:\nThe introduction to the Project Gutenberg eBook of \"Moby-Dick; or \nAs Ishmael shares a bed with Queequeg, whom he initially fears to \nWARNING\nAs previously mentioned, running the map_reduce_chain will incur costs; the\nlonger the text you wish to summarize, the higher the cost. Therefore,\nconsider further shortening the input text (in our case, the Moby-Dick.txt\nebook file) if you want to limit expenses. Additionally, ensure your OpenAI\nAPI credit balance remains positive to avoid errors such as: RateLimitError:\nError code: 429 - {'error': {'message': 'You exceeded your current quota,\nplease check your plan and billing details [...]. If necessary, log in to the\nOpenAI API page, navigate to Settings > Billing, and set a credit of at least\n$5.\nLet's now proceed to the next use case: summarizing across documents.\n3.2 Summarizing across documents\nYou can easily learn how to summarize information across various data\nsources, such as Wikipedia or local files in Microsoft Word, PDF, and text\nformats. This process, as shown in Figure 3.2, is similar to the map-reduce\ntechnique used in the previous section.\nFigure 3.2 Summarizing across documents using the Map-Reduce technique seen earlier: in this\nmethod, each document chunk undergoes a map operation to generate a summary. These\nindividual summaries are then further condensed into a single summary through the reduce\noperation.\n\n\nIn the sequence diagram in Figure 3.2, content from each raw text source is\nloaded into a corresponding Document instance. During the \"map\" operation,\nthese Document objects are converted into individual summaries, which are\n\n\nthen combined into a single summary during the \"reduce\" operation.\nNext, I’ll introduce you to an alternative technique called \"Refine,\" illustrated\nin Figure 3.3.\nFigure 3.3 Summarizing across documents using the Refine technique: with this approach, a final\nsummary is constructed incrementally by iteratively summarizing the combination of the current\nfinal summary and one of the document chunks. This process continues until all document\nchunks have been processed, resulting in the completion of the final summary.\n\n\nWeb Word PDF Web Word POF\n\npage file _—fle i ot Sewer |= file — file ‘ [we | uw |\n5 BE 5 = BBE 5 a\n\ni\n'\ni\n'\ni\n\nsummarize !\ni The raw text !\nWeb Word POF tt sources are loaded |\nB B B B Document into Documents 3\n(return) Web Document !\nThe Document fF !\npopulated from the summarize\nweb site is\nsummarized\n5 Summary\nio haan eerie\nThe summary is :\n\"refined\" with the refine( summarize)\ncontent from the Word\ndocument = Summary\n(oun)\nThe sunumary is further [Summary + PDF, ...]\n\"refined\" with the\ncontent from the PDF Summary + tet document\nand txt Documents | fe\nrefine(summarize)\nfinal summary\n8 =| Summary\nca sides ccs ariallesbisaulcaaietuid as aed eae aac\n\nIn this method, you progressively build the final summary by refining it with\neach step. Each document is sent to the LLM for summarization, along with\nthe current draft of the summary. This continues until all documents are\nprocessed, leading to the final summary. Map-reduce works well for\nsummarizing large volumes of text, where some content loss is acceptable to\nmanage the processing load. In contrast, the refine technique is better when\nyou want to ensure that the essence of each part is fully captured in the final\nsummary.\n3.2.1 Creating a list of Document objects\nWhen summarizing a large document, you typically start by breaking it into\nsmaller chunks, treating each chunk as a separate document. In this case,\nwe're beginning with a set of existing documents, so there's no need to split\nanything. How you create each Document object will depend on the source of\nthe text. I’ll show you how to summarize content from four different sources:\na Wikipedia page and a set of files in various formats (txt, docx, PDF) stored\nin a local folder. All the content is related to Paestum, a Greek colony on the\nCilento coast in southern Italy around 500 BC. You'll use the appropriate\nDocumentLoader for each data source, selecting from the many options\nintroduced earlier in section 1.6 on LangChain’s Document object model..\n3.2.2 Wikipedia content\nLet's begin with the Wikipedia content. While you can create a document\nfrom web-based data content using the WebBaseLoader, specific loaders are\ncustomized to retrieve content from particular websites, such as the\nIMSDbLoader for the Internet Movie Script Database (IMSDb) website, the\nAZLyricsLoader for the AZ Lyrics website, and the WikipediaLoader for the\nWikipedia website.\nFirstly, install the package for the loaders, including the ones for\nDocx2txtLoader (for Word files) and the PyPDFLoader (for PDFs). You can\ninstall them all at once to avoid stopping and restarting the Jupyter notebook\nmultiple times. Alternatively, you can open a separate OS shell and install the\npackages there after activating your virtual environment:\n\n\n(env_ch03) C:\\Github\\Building-llm-applications\\ch03>pip install w\nIf you cloned the repository from Github and run pip install -r\nrequirements.txt after activating the virtual environment, all the required\npackages should already be installed.\nOnce everything is installed, start the Jupyter notebook as usual:\n(env_ch03) C:\\Github\\Building-llm-applications\\ch03>jupyter noteb\nNow, import the content from the Paestum Wikipedia page:\nfrom langchain.document_loaders import WikipediaLoader\nwikipedia_loader = WikipediaLoader(query=\"Paestum\", load_max_docs\nwikipedia_docs = wikipedia_loader.load()\nNOTE\nThe WikipediaLoader may load content from other Wikipedia hyperlinks\nreferenced in the requested article. For example, the Paestum article\nreferences the National Archeological Museum of Paestum, the Lucania\nregion, Lucanians, and the temples of Hera and Athena, resulting in\nadditional content loaded. Thus, it returns a Document list rather than a single\nDocument object. I've set the maximum number of documents returned to 2\nto save on summarization costs, but you can adjust it as needed.\n3.2.3 File based content\nTo get started, download or pull the Paestum folder from GitHub and place it\nin your local ch03 directory (if you didn’t clone the entire repository). The\nPaestum subfolder within ch03 contains three files:\nPaestum-Britannica.docx: Content sourced from the Encyclopedia\nBritannica website.\nPaestumRevisited.pdf: An excerpt from \"Paestum Revisited,\" a master\nthesis submitted at Stockholm University. The extract comprises only 4\npages, but you have the option to use the full document located in the\nsame folder (PaestumRevisited-StocholmsUniversitet.pdf).\n\n\nPaestum-Encyclopedia.txt: Content taken from Encyclopedia.com.\nBelow is the process to load these files into corresponding documents:\nfrom langchain.document_loaders import Docx2txtLoader\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.document_loaders import TextLoader\nword_loader = Docx2txtLoader(\"Paestum/Paestum-Britannica.docx\")\nword_docs = word_loader.load()\npdf_loader = PyPDFLoader(\"Paestum/PaestumRevisited.pdf\")\npdf_docs = pdf_loader.load()\ntxt_loader = TextLoader(\"Paestum/Paestum-Encyclopedia.txt\")\ntxt_docs = txt_loader.load()\nThe document variables (word_docs, pdf_docs, txt_docs) are in plural mode\nbecause a loader always returns a list of documents, even if the list contains\nonly one item.\nNOTE\nYou may have noticed the direct creation of a Document object from\nPaestum-Encyclopedia.txt using a TextLoader. You might wonder why the\nMoby-Dick.txt file was read with the Python file reader previously. The\nreason is that in that case, the intention was to split the content into a specific\nnumber of tokens to fit the LLM prompt, requiring manual creation of a\nDocument for each.\n3.2.4 Creating the Document list\nYou can now merge all the documents from various sources into a single\nDocument list:\nall_docs = wikipedia_docs + word_docs + pdf_docs + txt_docs\nWith everything compiled, you're ready to summarize the content using the\n\"refine\" technique. Before you move forward, take a moment to check out the\nsidebar on Document Loaders for alternative ways to create your document\n",
      "page_number": 84,
      "chapter_number": 11,
      "summary": "This chapter covers segment 11 (pages 84-95). Key topics include summaries, summary, and document. Begin by opening a terminal and creating a new folder named ch03 for this\nchapter’s code.",
      "keywords": [
        "document",
        "text",
        "chain",
        "summary",
        "summarize",
        "Github",
        "documents",
        "content",
        "chunk",
        "final summary",
        "Map",
        "document chunk",
        "Paestum",
        "reduce",
        "OpenAI"
      ],
      "concepts": [
        "summaries",
        "summary",
        "document",
        "documents",
        "content",
        "important",
        "summarizing",
        "summarize",
        "summarized",
        "chunks"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 59,
          "title": "Segment 59 (pages 499-500)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 8,
          "title": "Segment 8 (pages 127-144)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 1,
          "title": "Segment 1 (pages 1-18)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 12,
          "title": "Segment 12 (pages 199-216)",
          "relevance_score": 0.68,
          "method": "api"
        }
      ]
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 96-103)",
      "start_page": 96,
      "end_page": 103,
      "detection_method": "topic_boundary",
      "content": "list..\nDocument Loaders\nWhile the sections above have introduced document loaders for specific data\nsources, I encourage you to explore the UnstructuredLoader as well. It\nenables you to import content from various file types, including Word, PDF,\nand txt files, among others.\nAnother option is the DirectoryLoader, which utilizes the\nUnstructuredLoader internally. It allows you to load content from files of\ndifferent formats located in the same folder in a single operation.\nAs an exercise, I recommend recreating the documents from the Word, PDF,\nand txt Paestum content using either the UnstructuredLoader or the\nDirectoryLoader. If you choose to do so, you'll need to install the related\npackage and refer to the documentation on the LangChain website:\npip install \"unstructured[all-docs]\"\nThe LangChain framework provides numerous loaders for retrieving content\nfrom diverse data sources. I highly encourage you to explore the list and\nexperiment with any loaders that pique your interest:\nhttps://python.langchain.com/docs/integrations/document_loaders\n3.2.5 Progressively refining the final summary\nNow that everything is set up, you can create a chain to generate the final\nsummary step by step. Begin by importing the necessary modules:\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import PromptTemplate\nimport getpass\nNext, capture the OPENAI_API_KEY and set up the LLM model as before:\nOPENAI_API_KEY = getpass.getpass('Enter your OPENAI_API_KEY')\n\n\nllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,model_name=\"gpt-4o\nNow, define the chain, with related prompt, for summarizing individual\ndocuments:\ndoc_summary_template = \"\"\"Write a concise summary of the followin\n{text}\nDOC SUMMARY:\"\"\"\ndoc_summary_prompt = PromptTemplate.from_template(doc_summary_tem\ndoc_summary_chain = doc_summary_prompt | llm\nNext, set up the chain for refining the summary by iteratively combining the\ncurrent summary with the summary of an additional document:\nrefine_summary_template = \"\"\"\nYour must produce a final summary from the current refined summar\nwhich has been generated so far and from the content of an additi\nThis is the current refined summary generated so far: {current_re\nThis is the content of the additional document: {text}\nOnly use the content of the additional document if it is useful, \notherwise return the current full summary as it is.\"\"\"\nrefine_summary_prompt = PromptTemplate.from_template(refine_summa\nrefine_chain = refine_summary_prompt | llm | StrOutputParser()\nFinally, define a function that loops over each document, summarizes it using\nthe doc_summary_chain, and refines the overall summary using the\nrefine_chain:\ndef refine_summary(docs):\n   intermediate_steps = []\n   current_refined_summary = ''\n   for doc in docs:\n       intermediate_step = \\\n          {\"current_refined_summary\": current_refined_summary, \n           \"text\": doc.page_content}\n       intermediate_steps.append(intermediate_step)\n       \n       current_refined_summary = refine_chain.invoke(intermediat\n       \n   return {\"final_summary\": current_refined_summary,\n           \"intermediate_steps\": intermediate_steps}\n\n\nYou can now start the summarization process by calling refine_summary()\non your prepared document list:\nfull_summary = refine_summary(all_docs)\nPrinting the full_summary object will show the final summary under\nfinal_summary and the intermediate steps under intermediate_steps.\nAlthough the results steps are shortened for convenience, I encourage you to\nobserve how the summary evolves at each stage:\nprint(full_summary )\nHere is an excerpt from the output:\n{'final_summary': \"**Final Summary:**\\n\\nPaestum, an ancient Gree\nWe have covered a few summarization techniques, so let’s pause briefly to\nreflect on what we have learnt.\n3.3 Summarization flowchart\nTo wrap up this chapter, I’ve included a flowchart to help you choose the\nmost appropriate summarization technique for your specific needs:\nFigure 3.4 Summarization flowchart This flowchart guides you in selecting the right\nsummarization approach based on whether you need to summarize multiple unrelated documents\nand whether the input text fits within the context window\n\n\nThe first decision point\nmore documents 15 on Whether you need\nto summarize one or\nmultiple documents\n\nMute \\ No\n\nultiole |\n\nDocuments D> Split + Map Reduce\nThe second decision point 1~‘fag | Yes\n\ni$ on Whether the content\nto be summarized fits the\ncontext window\nNo\nMap Reduce or\na\n\nYes\n\n7,\n\nStuff prompt\n\nAs shown in the flowchart in Figure 3.4, the first key decision is whether\nyou're summarizing one or multiple documents. If it's just one document and\nit fits within the context window, you can \"stuff\" the entire document into a\nsingle prompt for summarization. If it doesn't fit, use the map-reduce method.\nFor multiple documents, if they all fit within the context window, you can\nalso stuff them into a single prompt. If not, use map-reduce for a large\nnumber of documents, or the refine technique if you want to ensure the core\nof each document is included in the final summary.\n3.4 Summary\nSummarization is a great starting point for building LLM applications\ndue to its straightforwardness.\nThe choice of technique depends on whether you're summarizing a\nsingle document or multiple documents and if the text fits within the\ncontext window.\nTypically, the first step in summarization involves loading each raw text\nsource into a LangChain Document object.\nFor many summarization tasks, a map-reduce approach is effective. This\nmethod summarizes each document or chunk individually in the \"map\"\nstage, then combines these summaries in the \"reduce\" stage to create a\nfinal summary.\nWhen summarizing multiple documents, load each document with the\nappropriate loader, summarize them individually, and progressively\nrefine the summary by integrating each document’s summary until all\nare included.\nMap-reduce is suited for large text summaries where some content loss\nis acceptable to handle the processing load, while the refine technique\nensures the essence of each part is fully retained in the final summary.\n\n\n4 Building a research\nsummarization engine\nThis chapter covers\nExplaining what a research summarization engine is.\nOrganizing core functionality, such as web searching and scraping\nfunctions.\nUsing prompt engineering for creating web searches and summarizing\nresults.\nStructuring the process into individual LangChain chains.\nIntegrating various sub-chains into a comprehensive main chain.\nApplying advanced LCEL features for parallel processing.\nBuilding on the content summarization techniques from chapter 3, this\nchapter will guide you through creating a research summarization engine.\nThis LLM application will process user queries, perform web searches, and\ncompile a comprehensive summary of the findings. We'll develop this project\nstep by step, starting with the basics and gradually increasing in complexity.\nAlong the way, you will deepen your knowledge of LangChain as I introduce\ncreating LLM chains with LangChain Expression Language (LCEL).\n4.1 Overview of a research summarization engine\nImagine you're researching various topics, like an NBA player, a tourist\ndestination, or whether to invest in a stock. Manually, you'd perform a web\nsearch, sift through results, read related pages, take notes, and compile a\nsummary. A modern approach is to let an LLM handle this work. You could\ncopy text from each web page, paste it into a ChatGPT prompt for\nsummarization, and repeat for multiple pages. Then, combine these\nsummaries into a final prompt for a consolidated summary (see figure 4.1).\nFigure 4.1 Semi-automated summarization with LLM: You prompt ChatGPT to summarize each\n\n\nweb search result and then compile them into a consolidated summary.\nA more efficient method is to develop a fully automated research\nsummarization engine. This engine can perform web searches, summarize the\n\n\nresults, and compile a final report automatically (see figure 4.2). It's a\nvaluable tool for handling any research query.\nFigure 4.2 Automated research summarization engine: Ask a question, and the engine performs a\nweb search, returns URLs, scrapes and summarizes web pages, and compiles a research report\nfor you.\n",
      "page_number": 96,
      "chapter_number": 12,
      "summary": "If you choose to do so, you'll need to install the related\npackage and refer to the documentation on the LangChain website:\npip install \"unstructured[all-docs]\"\nThe LangChain framework provides numerous loaders for retrieving content\nfrom diverse data sources Key topics include summarizes, summarized, and document.",
      "keywords": [
        "summary",
        "final summary",
        "summarization",
        "Document",
        "final",
        "documents",
        "refine",
        "LLM",
        "research summarization engine",
        "prompt",
        "research summarization",
        "content",
        "summarization engine",
        "current",
        "multiple documents"
      ],
      "concepts": [
        "summarizes",
        "summarized",
        "document",
        "documents",
        "summaries",
        "text",
        "llm",
        "langchain",
        "prompt",
        "step"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 20,
          "title": "Segment 20 (pages 165-173)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 22,
          "title": "Segment 22 (pages 176-183)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 19,
          "title": "Segment 19 (pages 157-164)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 25,
          "title": "Segment 25 (pages 216-223)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 23,
          "title": "Segment 23 (pages 190-197)",
          "relevance_score": 0.55,
          "method": "api"
        }
      ]
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 104-111)",
      "start_page": 104,
      "end_page": 111,
      "detection_method": "topic_boundary",
      "content": "We'll build this engine using LangChain. First, we'll implement web\nsearching and scraping, then set up the OpenAI LLM model for\nsummarization, and finally integrate all components into a Python\napplication. Initially, it will run as an executable and later if you wish, you\ncould expose it as a REST API.\n4.2 Setting up the project\nI am assuming you're using Visual Studio Code with the free Python\nextension and running on Windows, though you can opt for alternative\nPython IDEs like PyCharm. If you're new to Visual Studio Code, get it set up\nby following the sidebar instructions.\nInstalling Visual Studio Code and the Python extension\nDownload and install the appropriate version of Visual Studio Code for your\nOS from the official website:\nhttps://code.visualstudio.com/download\nFigure 4.3 Screenshot of the Extensions marketplace panel: 1) Select the Extensions menu; 2)\nenter python in the search box; 3) Select Python (from Microsoft) and click Install.\n\n\nOnce installed, open Visual Studio Code, and click the Extensions icon on\nthe left-hand menu. Then, search for Python, select the Python extension\n(from Microsoft), and click install.\nI'll briefly guide you through setting up a Python project in Visual Studio\n\n\nCode, creating a virtual environment, activating it, and installing necessary\npackages.\nUsing your file explorer or shell, create an empty folder named ch04 in your\nsource code area. For example:\nC:\\Github\\building-llm-applications\\ch04\nOpen Visual Studio Code, then choose File > Open Folder, navigate to the\nch04 folder, and click Select Folder.\nOpen a terminal within Visual Studio Code by selecting Terminal > New\nTerminal.\nThe terminal should display the path to the folder you just created. On\nWindows, you'll see something like this:\nPS C:\\Github\\building-llm-applications\\ch04>\nIf you've just installed Visual Studio Code or you're new to it, enable the\nterminal by running this command (you only need to do it once):\nPS C:\\Github\\building-llm-applications\\ch04> Set-ExecutionPolicy \nWithin this terminal, as usual, create a virtual environment, activate it, and\ninstall the required Python packages (I'm omitting the full path to ch04 for\nconvenience):\n... ch04> python -m venv env_ch04\n... ch04> .\\env_ch04\\Scripts\\activate\nIf you activate the virtual environment in PowerShell using activate.ps1,\nyou may see a prompt asking: “Do you want to run software from this\nuntrusted publisher?” Respond by typing A (Always run) to proceed.\nIf you have cloned this repository from GitHub, you can install the required\npackages with pip install -r requirements.txt, otherwise:\n(env_ch04) ... ch04> pip install langchain langchain_openai langc\n\n\nOnce the required packages are installed, I recommend creating a custom Run\nconfiguration to ensure you're running and debugging your code within the\nenv_ch04 virtual environment you just set up.\nTo create the configuration:\n1. In the top menu, go to Run > Open Configurations.\n2. This will open the launch.json file in the editor.\n3. Replace or add the following configuration:\n{\n   \"version\": \"0.2.0\",\n   \"configurations\": [\n       {\n           \"name\": \"Python Debugger: Current File\",\n           \"type\": \"debugpy\",\n           \"request\": \"launch\",\n           \"program\": \"${file}\",\n           \"console\": \"integratedTerminal\",\n           \"python\": \"${workspaceFolder}/env_ch04/Scripts/python\n       }\n   ]\n}\n\"${workspaceFolder}/env_ch04/bin/python\"\nNote\nIf you're using macOS or Linux, replace the path in the \"python\" field with:\nThis custom Run configuration ensures your code runs with the correct\ninterpreter and environment. You'll use it later for debugging. With\neverything in place, you're now ready to start coding!\n4.3 Implementing the core functionality\nLooking again at figure 4.2, it's clear that our research summarization engine\ndepends on two key capabilities we must provide: one for conducting web\nsearches and another for extracting text from related web pages. Additionally,\nyou'll need a utility function to initialize an instance of the LLM client you'd\nlike to use.\n\n\n4.3.1 Implementing web searching\nWe'll use the LangChain wrapper for the DuckDuckGo search engine to\nperform web searches. Its results method returns a list of objects, each\ncontaining the result URL in a property called \"link\".\nAdd a new empty file named web_searching.py to the project and fill it with\nthe code below:\nfrom langchain_community.utilities import DuckDuckGoSearchAPIWrap\nfrom typing import List\ndef web_search(web_query: str, num_results: int) -> List[str]:\n   return [r[\"link\"] for r in DuckDuckGoSearchAPIWrapper().resul\nCreate a separate Python file, such as web_searching_try.py, to test the\nsearch function:\nfrom web_searching import web_search\nresult = web_search(web_query=\"How many titles did Michael Jordan \nprint(result)\nNOTE\nIf you're unfamiliar with Visual Studio Code, you can execute the code above\nby pressing F5 and then selecting Python Debugger > Python File. You can\nalso set breakpoints and run the code step by step by pressing F10 on each\nline.\nIn the terminal, you'll get a list of URLs like the following ones, representing\nthe results of your search.\n['https://en.wikipedia.org/wiki/List_of_career_achievements_by_Mi\nNote\nOther web search engine wrappers provided by LangChain are\nTavilySearchResults and GoogleSearchAPIWrapper. Both require an API\nkey, so I chose DuckDuckGoSearchAPIWrapper because it doesn't.\n\n\n4.3.2 Implementing web scraping\nWe'll scrape the web pages from the result list using Beautiful Soup, which is\na web scraper library. Place the code shown in the listing below in a file\nnamed web_scraping.py:\nListing 4.1 web_scraping.py\nimport requests\nfrom bs4 import BeautifulSoup\ndef web_scrape(url: str) -> str:\n   try:\n       response = requests.get(url)\n       if response.status_code == 200:\n           soup = BeautifulSoup(response.text, \"html.parser\")\n           page_text = soup.get_text(separator=\" \", strip=True)\n           return page_text\n       else:\n           return f\"Could not retrieve the webpage: Status code \n   except Exception as e:\n       print(e)\n       return f\"Could not retrieve the webpage: {e}\"\nLet's try out the function by placing the code below in a file named\nweb_scraping_try.py:\nfrom web_scraping import web_scrape\nresult = web_scrape('https://en.wikipedia.org/wiki/List_of_career_\nprint(result)\nAfter you run this script, the output will be similar to the following excerpt:\nList of career achievements by Michael Jordan - Wikipedia Jump to \nAs you can see, much of the scraped content is not relevant, but don't worry\nfor now; the LLM will extract the relevant bits we're interested in.\n4.3.3 Instantiating the LLM client\n\n\nFor this use case, we'll use the OpenAI GPT-4o-mini model instead of GPT-\n4o. Create a function to instantiate the OpenAI client as follows and place it\nin a file named llm_models.py (replace 'YOUR_OPENAI_API_KEY' with\nyour OpenAI key):\nfrom langchain_openai import ChatOpenAI\nopenai_api_key = 'YOUR_OPENAI_API_KEY'\ndef get_llm():\n   return ChatOpenAI(openai_api_key=openai_api_key,\n                model_name=\"gpt-4o-mini\")\nNote\nThere are various ways to store secrets like the OpenAI key in Visual Studio\nCode, each with its pros and cons. I'm hardcoding the OpenAI key inline\nhere, but you're free to implement it with your preferred approach.\n4.3.4 JSON to Python object converter\nLet's make a utility function that converts JSON text from the LLM into a\nPython object, usually a dictionary or sometimes a list. If the JSON is\nmalformed, it will return an empty dictionary. Add the following code to a\nfile called utilities.py:\nimport json \ndef to_obj(s):\n   try:\n       return json.loads(s)\n   except Exception:\n       return {}\nTo recap: we've set up a Visual Studio Code project and implemented the\ncore capabilities we need. Before building the engine to orchestrate web\nsearching, web scraping, and summarization requests to the LLM, let's step\nback and take a second look at the big picture.\n4.4 Enhancing the architecture with query\nrewriting\n\n\nLet's revisit the diagram in Figure 4.2, which I've included again for\nconvenience as Figure 4.4 below\nFigure 4.4 Automated research summarization engine: Ask a question, and the engine performs a\nweb search, returns URLs, scrapes and summarizes web pages, and compiles a research report\nfor you.\n",
      "page_number": 104,
      "chapter_number": 13,
      "summary": "This chapter covers segment 13 (pages 104-111). Key topics include python, code, and coding. First, we'll implement web\nsearching and scraping, then set up the OpenAI LLM model for\nsummarization, and finally integrate all components into a Python\napplication.",
      "keywords": [
        "Visual Studio Code",
        "Visual Studio",
        "Studio Code",
        "open Visual Studio",
        "Code",
        "Python",
        "web",
        "Studio",
        "Visual",
        "Python extension",
        "installed Visual Studio",
        "file",
        "OpenAI",
        "Studio Code project",
        "LLM"
      ],
      "concepts": [
        "python",
        "code",
        "coding",
        "returns",
        "file",
        "installing",
        "creating",
        "create",
        "llm",
        "search"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Effective-Python",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 2,
          "title": "Segment 2 (pages 20-40)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "Segment 28 (pages 232-240)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 6,
          "title": "Segment 6 (pages 41-49)",
          "relevance_score": 0.65,
          "method": "api"
        }
      ]
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 112-119)",
      "start_page": 112,
      "end_page": 119,
      "detection_method": "topic_boundary",
      "content": "If you look closely at the architecture diagram in Figure 4.4, you might notice\na potential improvement: instead of sending the original research request\ndirectly to the web search engine, you can use the LLM to create multiple\nsearch queries. This technique, called \"query rewriting\" or \"multiple query\ngeneration,\" is similar to a method used to enhance RAG searches, which I'll\ncover in later chapters. Rewriting the original question into multiple queries\noffers several advantages. It can clarify ambiguous or unclear queries, fix\ngrammar or syntax errors that could confuse the search engine, and add\ncontext to short queries to improve search results. For complex queries,\nbreaking them into simpler queries provides useful context for better answers.\nThis is the key reason for rewriting a single query into multiple targeted\nsearches.\nFor instance, rather than querying the search engine \"How many titles did\nMichael Jordan win?\", you can prompt ChatGPT as follows:\nGenerate three web search queries to research the following topic, aiming for\na comprehensive report: \"How many titles did Michael Jordan win?\"\nYou'll receive queries like these:\n \n1. \"Michael Jordan NBA championships total\"\n2. \"List of NBA titles won by Michael Jordan\"\n3. \"Michael Jordan basketball career championships count\"\nConsequently, the engine will conduct research by performing these three\nweb searches and gathering results from each. This updated workflow is\ndepicted in the diagram in figure 4.5.\nFigure 4.5 Revised system architecture diagram, incorporating query rewriting: The process\nbegins by tasking the LLM to generate a specified number of queries based on the user's research\nquestion. These queries are then submitted to the search engine. The subsequent processing\nremains consistent with the illustration in figure 4.2.\n\n\n4. Web searches\n\nThe engine performs\nthe rewritten web\nsearches\n\n)\n\n>\n1, Research request\n\nA\n\n—\n\nUser 10. Research repo\n\nsummarization engine\n\n™~\n\nAutomated\nSummarization\nEngine\n\n5a. Search #1 results\n\nABB\n\n5b. Search #n results\n\nO0\n\n3, Rewritlen web searches\n\n2. Request query rewrite\n\n§. Summarize search results\n\n7. Summaries\n\nAE EEE)\n\n————————— +}\n} 8. Compile research report\nqe\n\n9 Research report\n\ns The LLM compiles a\n\nresearch report from the\nsearch summaries\n\nThe engine requests the\nLLM to rewrite the user's\nresearch request into\nmultiple web searches\n\nE}SIEMEIEIE)\nBARBER\n\nLLM client\n\nTechnically, the updated architecture is more complex, which could raise\nconcerns about performance due to the additional web searches. If processed\nsequentially, the response time would increase linearly; for instance, running\nfour web searches would take four times as long. However, you can speed up\nprocessing through parallelization, which I’ll cover in later sections. For now,\nwe’ll start with a sequential processing approach and explore parallelization\nlater.\n4.5 Prompt engineering\nBefore we assemble the building blocks from section 4.3, we need to focus\non some prompt engineering. We’ll create prompts for generating web search\nqueries, summarizing individual web pages, and producing the final report.\nLet’s tackle each of these tasks step by step.\n4.5.1 Crafting Web Search Prompts\nImagine a user asks a finance-related question like, \"Should I invest in Apple\nstocks?\" When guiding the LLM to generate web search queries based on this\nquestion, it's helpful to specify the persona that should frame these queries.\nFor example, you might begin the prompt with: \"You are an experienced\nfinance analyst AI assistant. Your objective is to create detailed, insightful,\nunbiased, and well-structured financial reports based on the provided data\nand trends.\"\nTo generate these instructions dynamically based on the research question,\nyou'll use a dedicated prompt. Start by designing a prompt that selects a\nsuitable research assistant and provides instructions tailored to the user's\nquery.\nCreate a file named prompts.py and begin with the following code:\nListing 4.2 prompts.py: Prompts to select and generate assistant instructions\nfrom langchain.prompts import PromptTemplate\nASSISTANT_SELECTION_INSTRUCTIONS = \"\"\"\n\n\nYou are skilled at assigning a research question to the correct r\nThere are various research assistants available, each specialized \nEach assistant is identified by a specific type. Each assistant h\nHow to select the correct assistant: you must select the relevant \n------\nHere are some examples on how to return the correct assistant inf\nExamples:\nQuestion: \"Should I invest in Apple stocks?\"\nResponse: \n{{\n   \"assistant_type\": \"Financial analyst assistant\",\n   \"assistant_instructions\": \"You are a seasoned finance analyst \n   \"user_question\": {user_question}\n}}\nQuestion: \"what are the most interesting sites in Tel Aviv?\"\nResponse: \n{{\n   \"assistant_type\": \"Tour guide assistant\",\n   \"assistant_instructions\": \"You are a world-travelled AI tour \n   \"user_question\": \"{user_question}\"\n}}\nQuestion: \"Is Messi a good soccer player?\"\nResponse: \n{{\n   \"assistant_type\": \"Sport expert assistant\",\n   \"assistant_instructions\": \"You are an experienced AI sport as\n   \"user_question\": \"{user_question}\"\n}}\n------\nNow that you have understood all the above, select the correct re\nQuestion: {user_question}\nResponse:\n\"\"\" \nASSISTANT_SELECTION_PROMPT_TEMPLATE = PromptTemplate.from_templat\n   template=ASSISTANT_SELECTION_INSTRUCTIONS\n)\nThis prompt template has a couple of key features. First, it's a \"few-shot\nprompt,\" as demonstrated by the three examples included, which help the\nLLM grasp our specific requirements. Second, it directs the LLM to return\n\n\nresults in JSON format, making it straightforward to convert the output into a\nPython dictionary for further processing.\nWith this prompt in place to select the appropriate assistant type and provide\ninstructions, you can now proceed to create the prompt for generating web\nsearches based on the user's question, as shown in the following listing.\nListing 4.3 prompts.py: Prompt to rewrite the user query into multiple web searches\nWEB_SEARCH_INSTRUCTIONS = \"\"\"\n{assistant_instructions}\nWrite {num_search_queries} web search queries to gather as much i\non the following question: {user_question}. Your objective is to w\nYou must respond with a list of queries such as query1, query2, q\n[\n   {{\"search_query\": \"query1\", \"user_question\": \"{user_question}\n   {{\"search_query\": \"query2\", \"user_question\": \"{user_question}\n   {{\"search_query\": \"query3\", \"user_question\": \"{user_question}\n]\n\"\"\"\nWEB_SEARCH_PROMPT_TEMPLATE = PromptTemplate.from_template(\n   template=WEB_SEARCH_INSTRUCTIONS\n)\nThe {assistaint_instructions} placeholder will be filled with the\n\"assistant_instructions\" output from the previous assistant selection\nprompt you saw. This prompt also returns results in JSON format, making it\neasy to convert the output into a list of Python dictionaries for further\nprocessing.\nIncluding the user question in the output ensures continuity throughout the\nprocess, allowing us to use it as needed, especially in the chain-based code\nthat follows.\nWith the web search prompt completed, let's move on to creating the\nsummarization prompts.\n4.5.2 Crafting Summarization Prompts\n\n\nThe prompt for summarizing result pages closely resembles prompts from the\nprevious chapter on summarization:\nListing 4.4 prompts.py: Prompt for summarizing result pages\nSUMMARY_INSTRUCTIONS = \"\"\"\nRead the following text:\nText: {search_result_text} \n-----------\nUsing the above text, answer in short the following question.\nQuestion: {search_query}\nIf you cannot answer the question above using the text provided a\nInclude all factual information, numbers, stats etc if available.\n\"\"\"\nSUMMARY_PROMPT_TEMPLATE = PromptTemplate.from_template(\n   template=SUMMARY_INSTRUCTIONS\n)\n4.5.3 Research Report prompt\nSimilarly, the prompt for generating the research report is straightforward:\nListing 4.5 prompts.py: Prompt for generating the research report\n# Research Report prompts adapted from https://github.com/assafel\nRESEARCH_REPORT_INSTRUCTIONS = \"\"\"\nYou are an AI critical thinker research assistant. Your sole purp\nInformation: \n--------\n{research_summary}\n--------\nUsing the above information, answer the following question or top\nThe report should focus on the answer to the question, should be w\nin depth, with facts and numbers if available and a minimum of 1,\nYou should strive to write the report as long as you can using al\nYou must write the report with markdown syntax.\nYou MUST determine your own concrete and valid opinion based on t\n\n\nWrite all used source urls at the end of the report, and make sur\nYou must write the report in apa format.\nPlease do your best, this is very important to my career.\"\"\" \nRESEARCH_REPORT_PROMPT_TEMPLATE = PromptTemplate.from_template(\n   template=RESEARCH_REPORT_INSTRUCTIONS\n)\n4.6 Initial implementation\nIn this section, I'll walk you through the initial implementation of our\nresearch summarization engine, following the steps laid out in the\narchitectural diagram in Figure 4.3.\n4.6.1 Importing Functions and Prompt Templates\nTo kick things off, create a Python file named research_engine_seq.py and\nbegin importing the functions and prompt templates you've crafted in the\npreceding sections.\nfrom web_searching import web_search\nfrom web_scraping import web_scrape\nfrom llm_models import get_llm\nfrom utilities import to_obj\nfrom prompts import (\n   ASSISTANT_SELECTION_PROMPT_TEMPLATE,\n   WEB_SEARCH_PROMPT_TEMPLATE,\n   SUMMARY_PROMPT_TEMPLATE,\n   RESEARCH_REPORT_PROMPT_TEMPLATE\n)\n4.6.2 Setting constants and input variables\nNow, let's establish some constants to configure the application. The\naccuracy and diversity of the report will depend on the number of web\nsearches and results per search you set. However, it's important to consider\nyour budget when setting these values. For instance, configuring 4 web\nsearches with 5 results per search would entail summarizing 20 web pages,\nwhich means roughly 2000 tokens per page, for a total of 20 x 2000 = 40,000\ntokens. With the OpenAI GPT4o Mini model costing around $0.0006 per\n\n\n1,000 output tokens, this would amount to approximately $0.01 per research\nrequest. I'll set lower configuration numbers, but feel free to adjust them as\nneeded:\nNUM_SEARCH_QUERIES = 2\nNUM_SEARCH_RESULTS_PER_QUERY = 3\nRESULT_TEXT_MAX_CHARACTERS = 10000\nThe sole input variable in the application is the one that captures the research\nquestion from the user:\nquestion = 'What can I see and do in the Spanish town of Astorga?\n4.6.3 Instantiating the LLM client\nInstantiating the LLM client is straightforward. You can do it simply as\nfollows:\nllm = get_llm()\n4.6.4 Generating the web searches and collecting the results\nIn the process of generating web searches and collecting results, the first step\nis to execute the LLM prompt to determine the correct research assistant and\nrelated instructions based on the user's research question:\nassistant_selection_prompt = ASSISTANT_SELECTION_PROMPT_TEMPLATE.\nassistant_instructions = llm.invoke(assistant_selection_prompt)\nTo execute this code, place a breakpoint on the line llm = get_llm(). If\nyou're new to Visual Studio Code, you can set a breakpoint by clicking to the\nleft of the line number where you want the debugger to pause.\nNext, start the debugger by clicking the Run & Debug icon on the left sidebar\n(or use the shortcut Ctrl+Shift+D on Windows). From the dropdown at the\ntop, select the Run configuration named \"Python Debugger: Current File\",\nwhich you created at the end of Section 4.2. Then, click the play button next\nto it to start debugging.\n",
      "page_number": 112,
      "chapter_number": 14,
      "summary": "For complex queries,\nbreaking them into simpler queries provides useful context for better answers Key topics include queries, query, and prompt. This technique, called \"query rewriting\" or \"multiple query\ngeneration,\" is similar to a method used to enhance RAG searches, which I'll\ncover in later chapters.",
      "keywords": [
        "prompt",
        "web search queries",
        "search",
        "web",
        "web searches",
        "research",
        "question",
        "assistant",
        "web search",
        "user",
        "LLM",
        "instructions",
        "research report",
        "TEMPLATE",
        "Michael Jordan"
      ],
      "concepts": [
        "queries",
        "query",
        "prompt",
        "searches",
        "assistant",
        "research",
        "important",
        "importing",
        "summarization",
        "summarize"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 39,
          "title": "Segment 39 (pages 357-366)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 28,
          "title": "Segment 28 (pages 555-576)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 48,
          "title": "Segment 48 (pages 973-990)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 27,
          "title": "Segment 27 (pages 536-554)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 58,
          "title": "Segment 58 (pages 491-498)",
          "relevance_score": 0.54,
          "method": "api"
        }
      ]
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 120-128)",
      "start_page": 120,
      "end_page": 128,
      "detection_method": "topic_boundary",
      "content": "Once the code runs and hits the breakpoint, inspect the value of the\nassistant_instructions variable in the \"Variables\" panel at the top left of\nthe screen. Alternatively, you can print the value manually in the “Debug\nConsole” panel at the bottom of the VS Code screen):\nprint(assistant_instructions)\nYou will observe the following output:\ncontent='{\\n    \"assistant_type\": \"Tour guide assistant\",\\n    \"a\nThe relevant information is in the content property. To convert it into a\nPython object, you can proceed as follows:\nassistant_instructions_dict = to_obj(assistant_instructions.conte\nPrinting the assistant_instructions_dict variable will yield the following\noutput:\n{'assistant_type': 'Tour guide assistant', 'assistant_instruction\nNow, you can execute the prompt to generate web searches based on the\noriginal user research question:\nweb_search_prompt = WEB_SEARCH_PROMPT_TEMPLATE.format(assistant_i\n                                                     num_search_\n                                                     user_questi\nweb_search_queries = llm.invoke(web_search_prompt)\nweb_search_queries_list = to_obj(web_search_queries.content.repla\nThe primary input for this prompt is the assistant_instructions output\nfrom the previous step. If you were to execute the code you’ve just written,\nupon printing web_search_queries_list, you would obtain a list of search\nqueries like this:\n[{'search_query': 'Astorga attractions', 'user_question': 'What c\nsee and do in the Spanish town of Astorga?'}]\nHere's how you can fetch web searches using the web_search() function:\nsearches_and_result_urls = [{'result_urls': web_search(web_query=w\n                                    num_results=NUM_SEARCH_RESUL\n\n\n                          'search_query': wq['search_query']} \n                          for wq in web_search_queries_list]\nExecuting up to this point, the searches_and_result_urls variable would\nhold a list of Python dictionaries like this:\n[{'result_urls': ['https://igotospain.com/one-day-in-astorga-on-t\nEach dictionary shows a search query and its corresponding result URLs (3\nfor each query here). The next step is to flatten the results, so each dictionary\ncontains a search query and just one result URL:\nsearch_query_and_result_url_list = []\nfor qr in searches_and_result_urls:\n   search_query_and_result_url_list.extend([{'search_query': qr[\n                                   'result_url': r\n                                   } for r in qr['result_urls']]\nNow, search_query_and_result_url_list has 9 dictionaries, just as expected:\n[{'search_query': 'Astorga attractions', 'result_url': 'https://ww\nWith the web search queries and all related result URLs ready, the next move\nis to start scraping the web pages linked to these URLs.\n4.6.5 Scraping the web results\nUse the web_scrape() function to pull text from web pages linked through\nyour search results:\nresult_text_list = [ {'result_text': web_scrape(url=re['result_ur\n                    'result_url': re['result_url'],\n                    'search_query': re['search_query']}\n                  for re in search_query_and_result_url_list]\nThis code populates result_text_list with 9 dictionaries, each carrying\ntext from a web page:\n[{'result_text': 'Astorga, Spain - WorldAtlas Astorga, Spain Asto\n{'result_text': \"Astorga, Spain: Uncovering the Jewels of a Hidde\nMuseum and Gaudi's Palace… ', 'result_url': 'https://citiesandatt\n\n\nThe next move is to summarize the information collected from each webpage.\n4.6.6 Summarizing the web results\nYou'll ask the language model to summarize the text from each web page\nusing the SUMMARY_PROMPT_TEMPLATE you set up earlier. This\nsummary will also keep the original search queries and URLs, which you'll\nneed later:\nresult_text_summary_list = []\nfor rt in result_text_list: \n   summary_prompt = SUMMARY_PROMPT_TEMPLATE.format(\n       search_result_text=rt['result_text'], \n       search_query=rt['search_query'])\n   \n   text_summary = llm.invoke(summary_prompt)\n   result_text_summary_list.append({'text_summary': text_summary\n                        'result_url': rt['result_url'],\n                        'search_query': rt['search_query']})\nThis process results in result_text_summary_list, a list of 9 dictionaries.\nEach contains a summary of the scraped text, the URL where it was found,\nand the search query used to find it:\n[{'text_summary': '\\nAstorga is a municipality in Northwestern Sp\nWith these summaries, you've compiled all the information needed for the\nfinal research report.\n4.6.7 Generating the research report\nLet's review the prompt used to create the final report:\nRESEARCH_REPORT_INSTRUCTIONS = \"\"\"\nYou are an AI critical thinker research assistant. Your sole purp\nInformation: \n--------\n{research_summary}\n--------\n\n\nUsing the above information, answer the …\n…\n\"\"\"\nLet's prepare the final report by combining summaries and URLs into a\nformat the prompt template expects. Here’s how.\nFirst, transform each dictionary into a string with the summary and its source\nURL:\nstringified_summary_list = [f'Source URL: {sr[\"result_url\"]}\\nSum\n                           for sr in result_text_summary_list]\nInspecting stringified_summary_list, you’ll find entries like these:\n['Source URL: https://www.worldatlas.com/cities/astorga-spain.htm\nthe Maragatería … pilgrims on the Camino de Santiago. ', 'Source \nNext, combine all the summary strings into one:\nappended_result_summaries = '\\n'.join(stringified_summary_list)\nThis gives you a single text block with all summaries and URLs:\nSource URL: https://www.worldatlas.com/cities/astorga-spain.html\nSummary:\nAstorga is a municipality in Northwestern Spain with a population \nSource URL: https://citiesandattractions.com/spain/astorga-spain-\nSummary:\nSome of the main attractions in Astorga, Spain include the Episco\nNow, use this content with the RESEARCH_REPORT_INSTRUCTIONS prompt\ntemplate to get the final research report:\nresearch_report_prompt = RESEARCH_REPORT_PROMPT_TEMPLATE.format(\n   research_summary=appended_result_summaries,\n   user_question=question\n)\nresearch_report = llm.invoke(research_report_prompt)\nprint(f'strigified_summary_list={stringified_summary_list}')\nprint(f'merged_result_summaries={appended_result_summaries}')\nprint(f'research_report={research_report}')\n\n\nIf you run the entire research_engine_seq.py script, you’ll receive a\ncomplete research report like the one below, based on web summaries after\nabout a minute:\n# Introduction\nAstorga is a charming town located in the northwestern region of \n# Historical and Cultural Significance\nAstorga is a town with a long and rich history, dating back to th\n[… SHORTENED]\nGreat job on completing your first automated web research! I recommend\ngoing through the code for this initial implementation again. If you didn't\nhave time to type and run it, be sure to check it out in the GitHub repository.\nYou'll likely understand it right away, but a review will help ensure\neverything is clear.\nThis initial version works well and generates the expected research report.\nHowever, it's a bit slow because it processes tasks sequentially. If we had set\nit up to handle 10 web searches with 10 results each, the time required would\nhave significantly increased.\nCan we make it faster? Absolutely. In the next section, we'll explore how to\ndo that by introducing the LangChain Expression Language (LCEL).\n4.7 Reimplementing the research summary engine\nin LCEL\nThe LangChain Expression Language (LCEL) offers a structured approach to\norganizing the core components of your LLM applications—such as web\nsearch, page scraping, and summarization—into an efficient chain or\npipeline. This framework not only simplifies the creation of complex\nworkflows from simple elements but also enhances them with advanced\nfeatures like streaming, parallel execution, and logging. While you got an\ninitial glimpse of LCEL in Chapter 4, this sidebar will provide a more in-\ndepth look at its full capabilities.\nLangChain Expression Language (LCEL)\n\n\nFor those developing LLM applications, using LCEL is highly recommended.\nIt allows you to interact with LLMs and chat models efficiently by creating\nand executing chains, providing several benefits:\nFallback: Enables adding a fallback action for error handling.\nParallel Execution: Executes independent chain components\nsimultaneously to boost performance.\nExecution Modes: Supports developing in synchronous mode and then\nswitching to streaming, batch, or asynchronous execution modes as\nneeded.\nLangSmith Tracing: Automatically logs execution steps when upgrading\nto LangSmith, facilitating debugging and monitoring.\nA chain follows the Runnable protocol, meaning it requires the\nimplementation of specific methods like invoke(), stream(), and batch(),\nincluding their asynchronous versions. LangChain's framework ensures that\nits components, such as PromptTemplate and JsonOutputFunctionsParser,\nadhere to these standards.\nFor detailed information and examples, visiting the official LCEL\ndocumentation is recommended, especially the How To and Cookbook\nsections: https://python.langchain.com/docs/expression_language/\nLCEL streamlines complex chain creation by offering a unified interface (the\nRunnable protocol), composition tools, and the ability to easily parallelize\nprocesses. Though mastering LCEL may require some practice, the effort is\nrewarding as it significantly enhances application performance and\nscalability.\nMy chain implementation strategy, shown in figure 4.6, involves constructing\na mini-chain for each processing step seen earlier and integrating these into a\nmaster Web Research chain.\nFigure 4.6 Architecture of the chain-based research summarization engine: Each step of the\nprocess is re-implemented as a mini-chain; all mini-chains are assembled into a master Web\nResearch chain.\n\n\nThis master Web Research chain handles the entire process, as shown in\nfigure 4.5, which illustrates the architecture of the chain-based research\nsummarization engine. Each processing step is implemented as a mini-chain,\nall integrated into the master Web Research chain:\nAssistant Instructions chain: This chain selects the best research\nassistant from available options to answer the user's question. It also\ncreates the system prompt that defines the assistant's skills and purpose.\nWeb Searches chain: This chain generates multiple web searches based\non the user's question. It provides context from different perspectives or\nbreaks down complex queries into simpler ones.\nSearch and Summarization chain: This chain performs web searches,\nretrieves URLs from search results, scrapes the relevant web pages, and\nsummarizes the content of each page.\nResearch Report chain: The final chain synthesizes the answer using the\noriginal question and the summaries generated from the search results.\nThe Search and Summarization chain, detailed in figure 4.7, is itself a\ncomposite of three chains: one for web search, one for scraping and\nsummarizing web pages, and one for compiling summaries into a single text\nblock.\nFigure 4.7 The Search and Summarization chain is made up of three chains: a web search chain,\n\n\na scraping and summarization chain, and a summary compilation chain.\nFigure 4.7 shows the Search and Summarization chain, which combines three\nseparate chains: one for web searches, another for scraping and summarizing\nweb content, and a third for merging these summaries into a single text block.\nReimplementing this process into chains, as shown in Figure 4.8, allows for\nparallel execution, significantly improving efficiency compared to the\nsequential approach in Figure 4.6.\nFigure 4.8 Chain architectural diagram showing how parallelization is applied in the research\nsummarization engine, highlighting the execution of separate chain instances in parallel for\nefficiency.\n\n\nFigure 4.7 highlights parallelization at two key stages:\n1. A separate instance of the Search and Summarization chain is created\nand executed simultaneously for each web search initiated by the Web\nSearches chain.\n2. For each search result generated by the Search Result URLs chain, an\nindividual Search Result Text and Summary chain instance is launched\nto run in parallel.\nWith this overview of the chain-based approach, we'll now explore the\nindividual \"mini chains,\" starting with the \"Assistant Instructions\" chain.\n4.7.1 Assistant Instructions chain\n",
      "page_number": 120,
      "chapter_number": 15,
      "summary": "This chapter covers segment 15 (pages 120-128). Key topics include chain, research, and likely. I recommend\ngoing through the code for this initial implementation again.",
      "keywords": [
        "search",
        "web",
        "result",
        "research",
        "web search",
        "chain",
        "SUMMARY",
        "URL",
        "text",
        "Web Research chain",
        "assistant",
        "list",
        "query",
        "urls",
        "Search Result URLs"
      ],
      "concepts": [
        "chain",
        "research",
        "likely",
        "result",
        "execute",
        "executing",
        "execution",
        "spain",
        "search",
        "summary"
      ],
      "similar_chapters": [
        {
          "book": "Microservice APIs Using Python Flask FastAPI",
          "chapter": 15,
          "title": "Segment 15 (pages 136-143)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "A Philosophy of Software Design",
          "chapter": 10,
          "title": "Segment 10 (pages 77-87)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 57,
          "title": "Segment 57 (pages 518-522)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 26,
          "title": "Segment 26 (pages 518-535)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 39,
          "title": "Segment 39 (pages 317-324)",
          "relevance_score": 0.62,
          "method": "api"
        }
      ]
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 129-139)",
      "start_page": 129,
      "end_page": 139,
      "detection_method": "topic_boundary",
      "content": "To begin processing a research question, the first task is to determine the\nmost relevant research assistant and their prompt instructions. This is handled\nby the assistant_instructions_chain, as shown below. Place this code in\na file named chain_1_1.py:\nfrom llm_models import get_llm\nfrom prompts import (\n   ASSISTANT_SELECTION_PROMPT_TEMPLATE, \n)\nfrom langchain.schema.output_parser import StrOutputParser\nassistant_instructions_chain = (\n   ASSISTANT_SELECTION_PROMPT_TEMPLATE | get_llm() \n)\nFor those new to LCEL syntax, the flow here is straightforward: the selection\nprompt feeds into the LLM, which then selects a research assistant based on\nthe user's question.\nTo test this setup, use the following script, saved as chain_try_1_1.py:\nfrom chain_1_1 import assistant_instructions_chain\nquestion = 'What can I see and do in the Spanish town of Astorga?\nassistant_instructions = assistant_instructions_chain.invoke(ques\nprint(assistant_instructions)\nRunning this code will produce output similar to what was shown in section\n4.6.4, detailing the assistant type, instructions, and the user question (the\noutput will come after a few seconds, and the metadata might look slightly\ndifferent from what I have reported here):\ncontent='{\\n    \"assistant_type\": \"Tour guide assistant\",\\n    \"a\nWhile you could manually extract the needed output from the content\nproperty, using LCEL offers a more efficient approach. By adding a\nStrOutputParser() block to your chain, you can automatically extract the\nLLM's response directly from the content property. Update the chain\naccordingly and save it as chain_1_2.py.\nfrom llm_models import get_llm\n\n\nfrom utilities import to_obj\nfrom prompts import (\n   ASSISTANT_SELECTION_PROMPT_TEMPLATE, \n)\nfrom langchain.schema.runnable import RunnablePassthrough\nfrom langchain.schema.output_parser import StrOutputParser\nassistant_instructions_chain = (\n   {'user_question': RunnablePassthrough()} \n   | ASSISTANT_SELECTION_PROMPT_TEMPLATE | get_llm() | StrOutput\n)\nThis updated version introduces several enhancements:\nWhile the ASSISTANT_SELECTION_PROMPT_TEMPLATE automatically\nmatches the input text question to the {user_question} field (refer back\nto section 4.5.1 for a refresh on {user_question}), it's safer to explicitly\nmap the input question to a user_question property in a Python\ndictionary using the RunnablePassthrough() function, which binds the\nexternal input to the user_question variable through an unnamed\nparameter. This iis also beneficial as this piece of information is\nimportant for subsequent steps.\nThe StrOutputParser() block extracts text from the LLM's content\nproperty, simplifying output handling.\nThe response is converted to a Python dictionary by to_obj(), making it\neasier to work with in the chain.\nTest the revised chain with this script, saved as chain_try_1_2.py:\nfrom chain_1_2 import assistant_instructions_chain\nquestion = 'What can I see and do in the Spanish town of Astorga?\nassistant_instructions_dict = assistant_instructions_chain.invoke\nprint(assistant_instructions_dict)\nRunning this should produce the expected output:\n{'assistant_type': 'Tour guide assistant', 'assistant_instruction\n4.7.2 Web Searches chain\n\n\nAfter the LLM selects the appropriate research assistant and details their role,\nyou can prompt the LLM to generate web searches related to the user's query.\nUse the code below, saved as chain_2_1.py, for this step:\nListing 4.6 Web Searches chain for rewriting the user query into web searches\nfrom llm_models import get_llm\nfrom utilities import to_obj\nfrom prompts import (\n   WEB_SEARCH_PROMPT_TEMPLATE\n)\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain.schema.runnable import RunnableLambda\nNUM_SEARCH_QUERIES = 2\nweb_searches_chain = (\n   RunnableLambda(lambda x:\n       {\n           'assistant_instructions': x['assistant_instructions']\n           'num_search_queries': NUM_SEARCH_QUERIES,\n           'user_question': x['user_question']\n       }\n   )\n   | WEB_SEARCH_PROMPT_TEMPLATE | get_llm() | StrOutputParser() \n)\nThis implementation uses a RunnableLambda block to process input from the\nprevious chain, transforming it into the format needed by\nWEB_SEARCH_PROMPT_TEMPLATE. The chain then continues similarly to\nprevious examples.\nTo test this chain, you can use the following script, saved as chain_try_2_1:\nListing 4.7 Script to test the Web Searches chain\nfrom utilities import to_obj\nfrom chain_2_1 import web_searches_chain\n# test chain invocation\nassistant_instruction_str = '{\"assistant_type\": \"Tour guide assis\nassistant_instruction_dict = to_obj(assistant_instruction_str)\nweb_searches_list = web_searches_chain.invoke(assistant_instructi\nprint(web_searches_list)\n\n\nThis script uses a mock input in assistant_instruction_str to mimic the\noutput from the Assistant Instructions chain, testing the chain's response to\nthis input.\nExpected output:\n[{'search_query': 'Things to do in Astorga Spain', 'user_question\nWith the web searches now generated, you can now proceed to the Search\nand Summarization chain.\n4.7.3 Search and Summarization chain\nThe Search and Summarization chain is designed to perform a web search\nbased on a query from the previous chain, retrieve URLs from the search\nresults, scrape the corresponding web pages, and then summarize each page.\nAs shown in Figure 4.5, this process is broken down into smaller sub-chains:\nSearch result URLs chain\nSearch result Text and Summary chain\nJoined summary chain\nWe'll begin by building these sub-chains, starting with the Search Result\nURLs chain.\nSearch result URLs chain\nThis sub-chain carries out a web search, retrieving a specific number of\nURLs from the search results. Save the following code as chain_3_1.py:\nListing 4.8 Search and Summarization chain\nfrom web_searching import web_search\nfrom langchain.schema.runnable import RunnableLambda\nNUM_SEARCH_RESULTS_PER_QUERY = 3\nsearch_result_urls_chain = (\n   RunnableLambda(lambda x: \n\n\n       [\n           {\n               'result_url': url, \n               'search_query': x['search_query'],\n               'user_question': x['user_question']\n           }\n           for url in web_search(web_query=x['search_query'], \n                                 num_results=NUM_SEARCH_RESULTS_\n       ]\n   )\n)\nA couple of key points to highlight:\nA lambda function is used to pass the web_query parameter to the\nweb_search() function. This function also formats the output as a list of\ndictionaries, each containing the resulting URL along with data from the\nprevious chain, such as the search query and the original user question.\nThese elements will be useful in subsequent stages.\nThe lambda function receives its input from the output of the previous\nchain, specifically the Web Searches chain.\nTo test this sub-chain, we'll use simulated input that mirrors what would\ncome from the Web Searches chain. Save the testing code in a file named\nchain_try_3_1.py:\nListing 4.9 Script to test the Search and Summarization chain\nfrom utilities import to_obj\nfrom chain_3_1 import search_result_urls_chain\n# test chain invocation\nweb_search_str = '{\"search_query\": \"Astorga Spain attractions\", \"\nweb_search_dict = to_obj(web_search_str)\nresult_urls_list = search_result_urls_chain.invoke(web_search_dic\nprint(result_urls_list)\nAfter executing it, you should see output similar to this:\n[{'result_url': 'https://loveatfirstadventure.com/astorga-spain/'\nSearch Result Text and Summary chain\n\n\nThe Search Result Text and Summary sub-chain handles a URL from the\nprevious sub-chain by:\nScraping the webpage's text using the provided URL.\nGenerating a summary of the scraped text.\nIncorporating the source URL into the summary.\nThese steps are effectively implemented in the following code, which should\nbe saved as chain_4_1.py:\nListing 4.10 Search Result Text and Summary chain\nfrom llm_models import get_llm\nfrom web_scraping import web_scrape\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain.schema.runnable import RunnableLambda, RunnablePar\nfrom prompts import (\n   SUMMARY_PROMPT_TEMPLATE\n)\nRESULT_TEXT_MAX_CHARACTERS = 10000\nsearch_result_text_and_summary_chain = (\n   RunnableLambda(lambda x:\n       {\n           'search_result_text': web_scrape(url=x['result_url'])\n           'result_url': x['result_url'], \n           'search_query': x['search_query'],\n           'user_question': x['user_question']\n       }\n   )\n   | RunnableParallel (\n       {\n           'text_summary': SUMMARY_PROMPT_TEMPLATE | get_llm() | \n           'result_url': lambda x: x['result_url'],\n           'user_question': lambda x: x['user_question']         \n       }\n   )\n   | RunnableLambda(lambda x: \n       {\n           'summary': f\"Source Url: {x['result_url']}\\nSummary: \n           'user_question': x['user_question']\n       }\n   ) \n)\n\n\nThe code follows the conventions established in previous chains, and you\nmight recall the purpose of RunnableParallel from Chapter 3. The\nRunnableParallel block allows for the simultaneous execution of an inner\nchain (next to text_summary) along with other operations (in this case, two\nlambda functions) that depend on the same input, which comes from the\ninitial RunnableLambda block.\nTo execute this chain, save the following script as chain_try_4_1.py:\nListing 4.11 Script to test the Search Result Text and Summary chain\nfrom utilities import to_obj\nfrom chain_4_1 import search_result_text_and_summary_chain\n# test chain invocation\nresult_url_str = '{\"result_url\": \"https://citiesandattractions.co\nresult_url_dict = to_obj(result_url_str)\nsearch_text_summary = search_result_text_and_summary_chain.invoke\nprint(search_text_summary)\nYou will get output similar to this:\n{'summary': 'Source Url: https://citiesandattractions.com/spain/a\nAssembling the Search and Summarization chain\nWe've now assembled all the key components to build the Search and\nSummarization chain. Before diving into the LCEL implementation, take a\nmoment to review Figure 4.9. This diagram provides a clearer understanding\nof the process compared to the earlier figure 4.7.\nFigure 4.9 Enhanced Search and Summarization chain diagram: The Search Result URLs chain\ngenerates multiple URLs; each URL initiates an instance of the Result Text and Summary chain,\nwhich in turn produces a corresponding summary. These summaries are then consolidated into a\nsingle text string by the Joined Summary chain.\n\n\nAs illustrated in figure 4.9, the Search Result URLs chain generates multiple\nURLs, each of which triggers an instance of the Result Text and Summary\nchain. These instances run in parallel, each producing a summary for its\nrespective web page. The Joined Summary chain then consolidates these\nsummaries into a single text block.\nTo implement this process, place the necessary code in a file named\nchain_5_1.py:\nListing 4.12 Search and Summarization chain\nfrom llm_models import get_llm\nfrom prompts import (\n   RESEARCH_REPORT_PROMPT_TEMPLATE\n)\nfrom chain_1_2 import assistant_instructions_chain\nfrom chain_2_1 import web_searches_chain\n\n\nfrom chain_3_1 import search_result_urls_chain\nfrom chain_4_1 import search_result_text_and_summary_chain\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain.schema.runnable import RunnableLambda\nsearch_and_summarization_chain = (\n   search_result_urls_chain \n   | search_result_text_and_summary_chain.map() # parallelize fo\n   | RunnableLambda(lambda x: \n       {\n           'summary': '\\n'.join([i['summary'] for i in x]), \n           'user_question': x[0]['user_question'] if len(x) > 0 \n       })\n)\nThe map() operator triggers multiple instances of the Result Text and\nSummary chain, one for each dictionary from the Search Result URLs chain\ncontaining a URL. This allows each instance to run simultaneously.\nAdditionally, the Joined Summaries sub-chain is integrated directly within\nthe larger Search and Summarization chain rather than as a separate entity.\nThis sub-chain merges summaries from each instance of the Result Text and\nSummary chain, functioning as a core part of the overall process.\nWith these components in place, you're ready to complete the Web Research\nchain.\n4.7.4 Web Research chain\nBefore diving into the implementation, let's take a look at Figure 4.10 for a\ndetailed overview of the Web Research chain, building on what we\nintroduced in Figure 4.8, similar to our approach with the Search and\nSummarization chain.\nFigure 4.10 Web Research chain: This illustrates how each web search initiated by the Web\nSearches chain triggers a separate instance of the Search and Summarization chain. These\ninstances operate concurrently, each producing a summary. These summaries are then compiled\nby the Research Report chain into the comprehensive final research report.\n\n\nAs shown in Figure 4.10, web searches from the Web Searches chain trigger\nmultiple instances of the Search and Summarization chains to run in parallel.\nEach chain generates a summary for a specific web search, and these\nsummaries are then combined by the Research Report chain to create the final\nresearch report.\nThe LCEL implementation of this process is outlined below, to be added to\nthe chain_5_1.py file:\nListing 4.13 Web Research chain\nweb_research_chain = (\n   assistant_instructions_chain \n   | web_searches_chain \n   | search_and_summarization_chain.map() # parallelize for each \n   | RunnableLambda(lambda x:\n      {\n          'research_summary': '\\n\\n'.join([i['summary'] for i in \n\n\n          'user_question': x[0]['user_question'] if len(x) > 0 e\n       })\n   | RESEARCH_REPORT_PROMPT_TEMPLATE | get_llm() | StrOutputPars\n)\nThis setup follows the same logic as the Search and Summarization chain.\nThe map() operator is used here to initiate multiple instances of the Search\nand Summarization sub-chain, allowing them to run concurrently. The\nResearch Report chain is then integrated as part of the overall Web Research\nchain.\nTo test the Web Research chain, use the following script, saving it as\nchain_try_5_1.py:\nListing 4.14 Script to test Web Research chain\nfrom chain_5_1 import web_research_chain\n# test chain invocation\nquestion = 'What can I see and do in the Spanish town of Astorga?\nweb_research_report = web_research_chain.invoke(question)\nprint(web_research_report)\nAfter running this, you'll get a research report formatted in Markdown, as\nspecified by the prompt. Note, the example report is shortened here for\nbrevity:\n# Introduction\nAstorga, a small town in northwestern Spain, may not be on everyo\nstunning architecture, delicious local cuisine, and natural beaut\nalong with practical information for planning a trip to this char\n# History of Astorga\nAstorga's history dates back to the ancient Roman settlement of A\nChristians. It has also been the site of many violent campaigns, \n# Top Attractions in Astorga\n## Episcopal Palace\nOne of the most iconic …\nCongratulations! You've built an LCEL-based chain that integrates sub-\nchains, inline chains, lambda functions, and parallelization. This hands-on\nexperience will make it easier for you to create your own LCEL chains. I\n",
      "page_number": 129,
      "chapter_number": 16,
      "summary": "This chapter covers segment 16 (pages 129-139). Key topics include chain, important. The response is converted to a Python dictionary by to_obj(), making it\neasier to work with in the chain.",
      "keywords": [
        "SEARCH",
        "Search result Text",
        "chain",
        "Search result URLs",
        "Web Research chain",
        "Search result",
        "Web",
        "Web Searches chain",
        "result",
        "result URLs chain",
        "Summarization chain",
        "result Text",
        "Summary",
        "Summary chain",
        "Web Searches"
      ],
      "concepts": [
        "chain",
        "important",
        "web",
        "searches",
        "search",
        "urls",
        "url",
        "llm",
        "output",
        "research"
      ],
      "similar_chapters": [
        {
          "book": "Effective-Python",
          "chapter": 40,
          "title": "Segment 40 (pages 420-428)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 2,
          "title": "Segment 2 (pages 20-40)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Effective-Python",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Effective-Python",
          "chapter": 2,
          "title": "Segment 2 (pages 10-18)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.6,
          "method": "api"
        }
      ]
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 140-148)",
      "start_page": 140,
      "end_page": 148,
      "detection_method": "topic_boundary",
      "content": "recommend taking some time to experiment with the different chains in this\napplication. Adjust the prompts, change the number of queries generated, or\ntailor the application to a specific type of web research you have in mind.\nNote\nThe research summarization engine you've built draws inspiration from an\nopen-source project named GPT Researcher. Exploring its GitHub repository\n(https://github.com/assafelovic/gpt-researcher) will show you a robust\nplatform supporting various search engines and LLMs, with features like\nmemory and compression. While it doesn’t use LCEL, adapting its core\nfunctionalities into an LCEL-based framework was intended to clarify several\ntechnical aspects for you. I recommend exploring the GPT Researcher’s\ncodebase and running the project on your computer if possible. It’s an\nexcellent way to learn about building an LLM application, including aspects\nlike web UI integration, beyond just the engine itself.\n4.8 Summary\nA research summarization engine is built around an LLM for efficient\nprocessing.\nThe engine's workflow involves:\nReceiving a research question from the user.\nGenerating web searches to gather relevant information with LLM\nassistance.\nSummarizing the content from each search result using the LLM.\nCombining these summaries into a detailed report.\nThis engine can be developed using Visual Studio Code.\nKey components of the engine include web searching, web scraping, and\nLLM interaction.\nPrompt engineering is critical in three areas:\nCrafting web searches from the user's question.\nSummarizing search results.\nCreating the final report.\nLangChain Expression Language (LCEL) helps organize the workflow\ninto a main chain with secondary chains.\nVarious techniques can parallelize the execution of these secondary\n\n\nchains for efficiency.\n\n\n5 Agentic Workflows with\nLangGraph\nThis chapter covers\nOverview of agentic workflows and agents\nLangGraph fundamentals and state management\nTransition from LangChain chains to an agentic workflow\nLarge language models are driving a new generation of applications that\nrequire more than simple prompt-response exchanges. As applications\nbecome more complex, agentic workflows have become essential—a pattern\nwhere the LLM orchestrates a structured, multi-step process using predefined\ncomponents and explicit state management. Unlike fully autonomous agents,\nagentic workflows follow a predictable sequence of steps and do not\ndynamically select tools or adapt to context in real time. Instead, they offer\nreliability, transparency, and modularity by guiding the application through a\nset flow, using the LLM to make decisions within fixed boundaries.\n5.1 Understanding Agentic Workflows and Agents\nLLM-powered agent-based systems typically follow one of two core design\npatterns: agentic workflows and agents. Each pattern shapes how the\napplication operates, as illustrated in figure 5.1. Because these terms are often\nused interchangeably—but have important differences—it’s essential to\nunderstand exactly what is meant by “agentic workflow” and “agent” before\ndiving deeper..\nFigure 5.1 Workflows and agents: workflows use the LLM to choose the next step from a fixed set\nof options, such as routing a request to a SQL database or a REST API and synthetizing the\nanswer with the related results. Agents, however, dynamically select and combine tools to achieve\ntheir objectives.\n\n\nWorkflow\n\n2a. text-to-Sql processor\n\nUser 1, Router 3, Answer synthetizer\n\nStatic flow 2b. text-to-api processor\n\n4. answer\n\nAgent\n\nAgentic workflows—often called simply workflows—guide an application\nthrough a fixed sequence of predetermined steps. The LLM is used to select\namong predefined options, helping the system complete tasks and manage the\noverall flow. In contrast, agents use language models for more than just task\nexecution: they reason, make decisions, and dynamically determine the next\nsteps based on available tools and evolving context. Here, a \"tool\" typically\nrefers to a function that returns or processes data. While both patterns rely on\nthe LLM to drive application behavior, workflows maintain a structured and\npredictable path, whereas agents can adapt in real time based on new\ninformation and shifting goals.\n5.1.1 Workflows\nWorkflows use the LLM to pick the next step from a limited set of choices.\nThey typically implement patterns such as Controller-Worker or Router, as\nillustrated in figure 5.2.\nFigure 5.2 Common Workflows patterns: the Controller-Worker pattern uses the LLM in the\ncontroller orchestrates the flow by assigning various tasks to workers following a certain\nsequence. In the Router pattern, the LLM simply directs the task to the appropriate worker\nbased on the context.\n\n\nController-Worker\nWorkflow\n\nStatic flow\n\nAnswer synthesis worker\n\nRouting Workflow\n\n2a. text-to-sql processor\n(either)\n\nUser 1. Router 3. Answer synthetizer\n\nStatic flow\n\n2b. text-to-api processor\n\n4. answer\n\nIn the Controller-Worker pattern, the controller spawns tasks for workers\nfollowing a certain sequence. In the Router pattern, the LLM simply directs\nthe task to the proper processor (or worker).\n5.1.2 Agents\nLLM agents use language models to perceive data, reason about it, decide on\nactions, and achieve goals. Advanced agents learn from feedback, retain\nmemory of past interactions, and build dynamic workflows with branching\nlogic. Unlike fixed prompt-response systems, agents generate new flows\nbased on real-time data and available tools.\n5.1.3 When to Use Agent-Based Architectures\nThe concepts of LLM-based workflows and agents are closely related and\noften overlap, with no sharp dividing line between them. In this chapter, our\nfocus is on agentic workflows; we’ll explore agents in greater depth later in\nthe book. Both approaches are most valuable when your application needs to\nbreak complex tasks into smaller steps, make decisions based on previous\nresults, access external tools or data, or maintain context throughout extended\ninteractions. It’s best to adopt agentic workflows or agents when your use\ncase genuinely benefits from explicit state management and dynamic control\n—balancing the added power against the increased complexity.\nTIP\nI highly recommend Anthropic's article \"Building Effective AI Agents\" for a\ndeeper understanding of workflows, agents, and when to use them. You can\nread it here: https://www.anthropic.com/engineering/building-effective-\nagents\n5.1.4 Agent Development Frameworks\nA variety of frameworks are available for building agent-based systems, each\nwith its own focus and trade-offs. LangGraph is designed for stateful,\npersistent agentic workflows using graph-based execution, making it\n\n\npowerful for complex applications—though it may require more technical\nexpertise. AutoGPT emphasizes fully autonomous, goal-driven agents with\nminimal supervision, but can face challenges with task consistency.\nLlamaIndex stands out in knowledge retrieval, though its scope is narrower\nthan broader agent frameworks. Microsoft Autogen supports highly\ncustomizable multi-agent conversations, but comes with a steeper learning\ncurve. n8n provides a visual interface and extensive integrations, making it\naccessible to non-developers, though advanced reasoning may require\nadditional components. Microsoft Semantic Kernel prioritizes memory and\nplanning and integrates well with Azure services. CrewAI enables\ncollaborative, multi-agent systems for specialized teams, but is a newer tool\nwith a smaller community.\n5.2 LangGraph Basics\nLangGraph builds on LangChain to manage more complex agentic\nworkflows with branching paths, stateful processing, and clear transitions\nbetween steps. It’s a framework for building stateful, multi-step AI\napplications using a graph-based structure. In LangGraph, nodes represent\nindividual tasks, such as generating text, calling an API, or analyzing data.\nEdges define the paths that connect these tasks. The state is information that\nmoves between nodes and updates at each step.\nThis setup is better than traditional chains when you need to make decisions,\nmanage state, or handle complex agentic workflows.\nNote\nLangGraph isn’t a replacement for LangChain but an extension. Think of\nLangChain as providing the building blocks and LangGraph as offering a\nblueprint to connect those parts into a complex system. LangChain gives you\ncomponents like LLMs, embeddings, and retrievers, while LangGraph helps\nyou organize those components into a structured, stateful workflow.\nTo use LangGraph effectively, it's important to understand a few key\nconcepts—like the core components of a graph (nodes and edges), how state\nflows through the graph, and how conditional edges control its behavior.\n\n\nThese are the building blocks you’ll explore in the next section.\n5.3 Moving from LangChain Chains to LangGraph\nLangChain’s simple, linear chains work for straightforward tasks but have\nlimits when your applications get more complex. A typical LangChain setup\noften looks like this:\nchain = (\n   prompt_template \n   | llm \n   | output_parser\n)\nThis setup struggles when tasks need to split into different paths, when you\nneed to repeat steps based on new information, or when you want to manage\nstate across multiple steps. It also falls short when multiple processes need to\nhappen in parallel.\nLangGraph solves these problems by offering better state management,\nconditional branching, and support for cyclical workflows. With explicit state\nmanagement, you can define and track data consistently across the workflow,\nwhich is essential for memory and reasoning. Conditional branching allows\nagents to take different paths based on previous results, making decision-\nmaking smoother. Cyclical workflows let agents repeat tasks until they meet\nspecific conditions, which helps refine results.\nLangGraph also makes it easier to understand and debug complex workflows.\nIts graph-based structure offers a clearer view of how data flows through the\nsystem, which helps when you need to trace or fix problems.\nThis graph-based approach works well for a range of use cases, such as\nmulti-step reasoning, task planning, managing context in long conversations,\ncoordinating research tasks, and automating business processes. As your\napplications get more complex, the benefits of using LangGraph’s agent-\nbased architecture become clearer. It gives you the control and flexibility\nneeded to build smart, multi-step systems that can adapt and make decisions\non their own.\n",
      "page_number": 140,
      "chapter_number": 17,
      "summary": "This chapter covers segment 17 (pages 140-148). Key topics include agents, workflow, and based. While it doesn’t use LCEL, adapting its core\nfunctionalities into an LCEL-based framework was intended to clarify several\ntechnical aspects for you.",
      "keywords": [
        "Agentic Workflows",
        "Workflows",
        "LLM",
        "agents",
        "Agentic",
        "LangGraph",
        "complex agentic workflows",
        "tasks",
        "GPT Researcher",
        "application",
        "state",
        "complex",
        "LangChain",
        "based",
        "chains"
      ],
      "concepts": [
        "agents",
        "workflow",
        "based",
        "tasks",
        "llm",
        "complex",
        "steps",
        "chains",
        "data",
        "result"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 30,
          "title": "Segment 30 (pages 253-261)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "Segment 28 (pages 232-240)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 5,
          "title": "Segment 5 (pages 33-40)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 8,
          "title": "Segment 8 (pages 61-70)",
          "relevance_score": 0.53,
          "method": "api"
        }
      ]
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 149-157)",
      "start_page": 149,
      "end_page": 157,
      "detection_method": "topic_boundary",
      "content": "5.4 LangGraph Core Components\nLangGraph provides a robust framework for building stateful, multi-step AI\napplications. Figure 5.3 presents the core components that form LangGraph\napplication's foundation.\nFigure 5.3 LangGraph core Components: a strongly typed state (in this example, modelled with\nResearchState) flows through the workflow. Nodes, usually Python functions (like def\nselect_assistant), perform tasks, and edges create directed data flows between nodes, in some\ncases with conditional paths.\n\n\ndef select_assistant\n\nResearch State\n\ndef\n\ngenerate_search_queries\n\nResearch State\ndef Research State\nperform_web_ searches\n\nResearch State\n\ndef evaluate_relevance\n\nResearch State\n\ndef write_report\n\nAt the heart of every LangGraph application is a state object — in our\nexample, ResearchState — which defines a clear and strongly typed state\nfor the entire workflow. This state is typically defined as a Python\nTypedDict, ensuring that data passed between components is well-structured\nand type-checked.\nIn a LangGraph, each node functions as a processing unit. Nodes can handle\ntasks such as generating search queries, calling external APIs, summarizing\nresults, or transforming data. These nodes are usually implemented as Python\nfunctions. The edges between nodes determine the directed flow of data,\ndefining how information moves through the graph.\nOne of LangGraph’s powerful features is its conditional edges, which allow\nyou to define dynamic execution paths based on the runtime state. Combined\nwith entry points and end conditions, this gives you full control over where\nthe graph begins, how it progresses, and when it completes.\nThe following sections walk through how to define and connect these\ncomponents, enabling you to build systems that can handle complex\nworkflows and adaptive decision-making.\n5.4.1 StateGraph Structure\nA core utility in LangGraph is the StateGraph class, which you use to define\nthe graph that models your application's workflow. For example:\nfrom langgraph.graph import StateGraph\nfrom typing import TypedDict\nclass ResearchState(TypedDict): #A\n   input_query: str\n   intermediate_result: str\n   final_output: str\ngraph = StateGraph(ResearchState) #B\n5.4.2 State Management and Typing\n\n\nState management is central to LangGraph applications. Unlike chain-based\nmethods that rely on implicit or loosely typed state, LangGraph enforces\nexplicit, strongly typed state, making workflows more robust and predictable.\nHere’s an extended version of the ResearchState that adds more detail:\nfrom typing import TypedDict, Optional, List\nclass ResearchState(TypedDict):\n   user_question: str\n   assistant_info: Optional[dict]\n   search_queries: Optional[List[dict]]\n   search_results: Optional[List[dict]]\n   research_summary: Optional[str]\n   final_report: Optional[str]\nEach node receives the current state and returns updates that merge into the\noverall state:\ndef process_node(state: dict) -> dict:\n   result = do_something(state[\"input_data\"]) #A\n   \n   return {\"output_data\": result} #B\n5.4.3 Node Functions and Edge Definitions\nNodes represent processing steps. Each node is a function that takes the\ncurrent state and returns updates. For instance:\ndef generate_search_queries(state: dict) -> dict:\n   \"\"\"Generate search queries based on user question.\"\"\"\n   question = state[\"user_question\"]\n   # Generate queries using an LLM\n   queries = llm_generate_queries(question)\n   return {\"search_queries\": queries}\ngraph.add_node(\"generate_queries\", generate_search_queries) #A\nEdges define valid transitions between nodes. A simple linear edge looks like\nthis:\ngraph.add_edge(\"generate_queries\", \"perform_searches\")\n\n\nA conditional edge uses a function to choose the next node based on the state:\ndef should_refine_queries(state: dict) -> str:\n   if len(state[\"search_results\"]) < 2:\n       return \"refine_queries\"\n   else:\n       return \"summarize_results\"\ngraph.add_conditional_edge(\"perform_searches\", should_refine_quer\n5.4.4 Entry Points and End Conditions\nEvery graph needs a starting point and clear end conditions. For example:\ngraph.set_entry_point(\"parse_question\") #A\nfrom langgraph.graph import END\ngraph.add_edge(\"write_final_report\", END) #B\nLet's explore a practical application of LangGraph.\n5.5 Turning the Web Research Assistant into an AI\nAgent\nTo demonstrate how LangGraph works, I’ll show you how to transform the\nweb research assistant from chapter 4—originally built with LangChain—\ninto an agent-based system. This upgrade allows the application to assess the\nrelevance of summaries from web page results and, if less than 50% of them\nare relevant, redirect the flow back to generating new search queries. If\nenough summaries are relevant, the application can proceed to write the final\nreport as usual. Achieving this level of dynamic control would be very\ncomplex with plain LangChain, which justifies the move to an agent-based\napproach with LangGraph. This case study guides you through each step,\nhighlighting the benefits of explicit state management and modular design.\n5.5.1 Original LangChain Implementation Overview\nOur original web research assistant used LangChain's sequential chains. The\nprocess followed these steps:\n\n\n1. Choose the appropriate research assistant based on the user's question.\n2. Generate search queries.\n3. Perform web searches and collect URLs.\n4. Scrape and summarize each search result.\n5. Compile a final research report.\nEach step fed its output into the next step, as you can see in the extract from\nthe original implementation in listing 5.1:\nListing 5.1 Original LangChain implementation of the Web Research Assistant\nassistant_instructions_chain = (\n   {'user_question': RunnablePassthrough()} \n   | ASSISTANT_SELECTION_PROMPT_TEMPLATE \n   | get_llm() \n   | StrOutputParser() \n   | to_obj\n)\nweb_searches_chain = (\n   # ...input processing...\n   | WEB_SEARCH_PROMPT_TEMPLATE \n   | get_llm() \n   | StrOutputParser() \n   | to_obj\n)\nweb_research_chain = ( #A\n   assistant_instructions_chain \n   | web_searches_chain \n   | search_and_summarization_chain.map()\n   | RunnableLambda(lambda x: # ...process results...)\n   | RESEARCH_REPORT_PROMPT_TEMPLATE \n   | get_llm() \n   | StrOutputParser()\n)\nThis approach works but has clear limitations:\nThe flow is rigid and linear, making it difficult to adapt dynamically\nbased on intermediate results. For instance, a conditional flow that\nredirects the application to generate new search queries if less than 50%\nof the summaries are relevant would be cumbersome to implement.\n\n\nError handling is challenging, as the lack of explicit state makes it hard\nto track and manage failures effectively.\nState is not explicitly managed, which complicates maintaining context\nacross multiple steps.\nDebugging becomes difficult when issues arise, especially with complex\nflows, since it is unclear which part of the chain failed or why.\n5.5.2 Identifying Components for Conversion\nTo convert the web research assistant to LangGraph, I first identify the key\ncomponents that will serve as nodes. Each node handles a specific part of the\nprocess:\nAssistant Selector: Determines which type of research assistant to use\nbased on the user’s question.\nQuery Generator: Creates search queries derived from the user's input.\nWeb Searcher: Conducts searches and gathers URLs based on the\ngenerated queries.\nContent Summarizer: Scrapes and summarizes the content of web pages.\nRelevance Evaluator: Assesses if the summaries are relevant enough to\nproceed or if new search queries are needed.\nReport Writer: Compiles the final research report using the relevant\nsummaries.\nUnlike a simple linear flow, this setup introduces a conditional element. After\nevaluating the relevance of the summaries, the flow can either proceed to the\nReport Writer if enough content is relevant or redirect back to the Query\nGenerator to create new search queries. This decision is based on a defined\nthreshold (for example, if less than 50% of summaries are relevant) and can\nrepeat up to a maximum of three iterations to avoid infinite loops.\nThe flow control is managed by a conditional routing function, the\nroute_based_on_relevance, which checks the relevance of the search results\nand the current iteration count. If the relevance is insufficient and the\nmaximum number of iterations has not been reached, the application\ngenerates new queries and repeats the search and evaluation steps. If the\nmaximum iteration count is reached, the application proceeds to compile a\n\n\nreport using the available results, regardless of their relevance.\nFor each component, I define:\nThe input state: The data each node requires to function.\nThe processing: The tasks each node performs.\nThe state updates: The information each node returns to update the\noverall state.\nThis modular and conditional approach makes the system flexible and\nadaptive, which would be cumbersome to achieve with plain LangChain’s\nlinear chains.\n5.5.3 Step-by-Step Transformation Process\nNow, I’ll guide you through the process of converting a LangChain\napplication to LangGraph. The following is a simplified version of the actual\ncode, which you can find in the GitHub repository.\nStep 1: Define the State\nThe first step is to design the state structure that will flow through the graph.\nA well-defined state helps you keep track of data across all nodes. In our case\nwe’ll model a composite state using inner types, as shown in listing 5.2.\nListing 5.2 State type of the LangGraph based Research assistant\nfrom typing import TypedDict, List, Optional\nclass AssistantInfo(TypedDict): #A\n   assistant_type: str\n   assistant_instructions: str\n   user_question: str\nclass SearchQuery(TypedDict): #A\n   search_query: str\n   user_question: str\nclass SearchResult(TypedDict): #A\n   result_url: str\n\n\n   search_query: str\n   user_question: str\n   is_fallback: Optional[bool]\nclass SearchSummary(TypedDict): #A\n   summary: str\n   result_url: str\n   user_question: str\n   is_fallback: Optional[bool]\nclass ResearchReport(TypedDict): #A\n   report: str\nclass ResearchState(TypedDict): #B\n   user_question: str\n   assistant_info: Optional[AssistantInfo]\n   search_queries: Optional[List[SearchQuery]]\n   search_results: Optional[List[SearchResult]]\n   search_summaries: Optional[List[SearchSummary]]\n   research_summary: Optional[str]\n   final_report: Optional[str]\n   used_fallback_search: Optional[bool]\n   relevance_evaluation: Optional[Dict[str, Any]]\n   should_regenerate_queries: Optional[bool]\n   iteration_count: Optional[int]\nThis state structure clearly defines the data available at each stage, reducing\nambiguity and simplifying debugging.\nStep 2: Convert Components to Node Functions\nNext, I convert each component into a node function. Each function takes the\ncurrent state, processes it, and returns updated state information, as you can\nsee in listing 5.3.\nListing 5.3 Node functions\ndef select_assistant(state: dict) -> dict:\n   \"\"\"Select the appropriate research assistant.\"\"\"\n   user_question = state[\"user_question\"]\n   \n   # Use the LLM to select an assistant\n   prompt = ASSISTANT_SELECTION_PROMPT_TEMPLATE.format(\n       user_question=user_question\n",
      "page_number": 149,
      "chapter_number": 18,
      "summary": "5.4 LangGraph Core Components\nLangGraph provides a robust framework for building stateful, multi-step AI\napplications Key topics include steps, optional, and nodes.",
      "keywords": [
        "state",
        "Optional",
        "Web Research Assistant",
        "Research State def",
        "Research",
        "search",
        "str",
        "Research Assistant",
        "Research State",
        "queries",
        "assistant",
        "search queries",
        "node",
        "Web Research",
        "question"
      ],
      "concepts": [
        "steps",
        "optional",
        "nodes",
        "graph",
        "based",
        "results",
        "queries",
        "query",
        "processing",
        "process"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "Segment 32 (pages 270-277)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 12,
          "title": "Segment 12 (pages 96-103)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 7,
          "title": "Segment 7 (pages 123-143)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 29,
          "title": "Segment 29 (pages 241-252)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 25,
          "title": "Segment 25 (pages 216-223)",
          "relevance_score": 0.56,
          "method": "api"
        }
      ]
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 158-165)",
      "start_page": 158,
      "end_page": 165,
      "detection_method": "topic_boundary",
      "content": "   )\n   response = get_llm().invoke(prompt)\n   \n   assistant_info = parse_assistant_info(response.content) #A\n   \n   return {\"assistant_info\": assistant_info} #B\ndef generate_search_queries(state: dict) -> dict:\n   \"\"\"Generate search queries based on the question.\"\"\"\n   assistant_info = state[\"assistant_info\"]\n   user_question = state[\"user_question\"]\n   \n   prompt = WEB_SEARCH_PROMPT_TEMPLATE.format( #C\n       assistant_instructions=assistant_info[\"assistant_instruct\n       user_question=user_question,\n       num_search_queries=3\n   )\n   response = get_llm().invoke(prompt)\n   \n   search_queries = parse_search_queries(response.content) #D\n   \n   return {\"search_queries\": search_queries} #E\nAdditional node functions follow the same pattern, each handling its own\ntask.\nStep 3: Define the Graph Structure\nWith node functions in place, I create the graph and define how the nodes\nconnect, establishing the execution order and data flow, as shown in Listing\n5.4. Unlike a simple linear chain, this version of the graph introduces a new\nnode for relevance evaluation and a conditional edge that dynamically alters\nthe flow based on the relevance of the search results.\nListing 5.4 Graph structure\nfrom langgraph.graph import StateGraph, END\ngraph = StateGraph(ResearchState)  #A\ngraph.add_node(\"select_assistant\", select_assistant)  #B\ngraph.add_node(\"generate_search_queries\", generate_search_queries\ngraph.add_node(\"perform_web_searches\", perform_web_searches)  #B\n\n\ngraph.add_node(\"summarize_search_results\", summarize_search_resul\ngraph.add_node(\"evaluate_search_relevance\", evaluate_search_relev\ngraph.add_node(\"write_research_report\", write_research_report)  #\ndef route_based_on_relevance(state): #C\n   iteration_count = state.get(\"iteration_count\", 0) + 1\n   state[\"iteration_count\"] = iteration_count\n   if iteration_count >= 3:\n       return \"write_research_report\"\n   if state.get(\"should_regenerate_queries\", False):\n       return \"generate_search_queries\"\n   return \"write_research_report\"\ngraph.add_edge(\"select_assistant\", \"generate_search_queries\")  #D\ngraph.add_edge(\"generate_search_queries\", \"perform_web_searches\") \ngraph.add_edge(\"perform_web_searches\", \"summarize_search_results\"\ngraph.add_edge(\"summarize_search_results\", \"evaluate_search_relev\ngraph.add_edge(\"write_research_report\", END)  #D\ngraph.add_conditional_edges( #E\n   \"evaluate_search_relevance\",\n   route_based_on_relevance,\n   {\n       \"generate_search_queries\": \"generate_search_queries\",\n       \"write_research_report\": \"write_research_report\"\n   }\n)\ngraph.set_entry_point(\"select_assistant\")  #F\nThe new relevance evaluation node checks if enough of the summarized\nresults are relevant. If less than 50% of them meet the criteria, the graph\nredirects the flow back to the Query Generator to refine the search. If the\nsummaries are sufficient or if the maximum of three iterations is reached, it\nmoves forward to compile the final report. This conditional flow is a\nsignificant enhancement over the rigid linear chains of LangChain, allowing\nthe system to adapt dynamically based on intermediate results.\nStep 4: Compile and Run the Graph\nAfter defining the graph, I compile it and run it using an initial state, as\n\n\nshown in listing 5.5. This step involves setting up an initial state with all\nrequired fields, including additional parameters for controlling the\nconditional flow, such as should_regenerate_queries and\niteration_count.\nListing 5.5 Running the graph\napp = graph.compile() #A\ninitial_state = { #B\n   \"user_question\": \" What can you tell me about Astorga's roman \n   \"assistant_info\": None,\n   \"search_queries\": None,\n   \"search_results\": None,\n   \"search_summaries\": None,\n   \"research_summary\": None,\n   \"final_report\": None,\n   \"used_fallback_search\": False,\n   \"relevance_evaluation\": None,\n   \"should_regenerate_queries\": None,\n   \"iteration_count\": 0\n}\nresult = app.invoke(initial_state) #C\nfinal_report = result[\"final_report\"] #D\nBy introducing conditional edges and relevance evaluation, this step-by-step\nprocess transforms a rigid, linear chain into a flexible, stateful, and adaptive\nagent-based workflow. The system can now evaluate its own results, adapt by\nrefining search queries if needed, and ensure that the final report is based on\nsufficiently relevant information. This adaptability would be cumbersome to\nimplement in plain LangChain, justifying the shift to LangGraph for complex\nLLM applications.\n5.5.4 Code Comparison and Benefits Realized\nThe LangGraph approach offers significant benefits:\nExplicit State Management: The state is clearly defined and passed\nthrough each node, making data handling transparent and reliable.\n\n\nModular Components: Each node handles a single task, simplifying\ntesting, debugging, and maintenance.\nClear Flow Control: The graph structure visually represents the\nexecution order and data flow, making it easier to trace and understand\ncomplex processes.\nEasier Debugging: With well-defined nodes and edges, it's\nstraightforward to identify where errors occur and what data caused\nthem.\nEnhanced Error Handling: Each node can implement specific error-\nhandling strategies without affecting the rest of the system.\nConditional Flow Control: The introduction of conditional edges based\non relevance evaluation allows the application to dynamically alter its\npath—either refining search queries if results are insufficient or\nproceeding to report writing. This adaptability ensures that the\napplication can respond intelligently to intermediate results, which\nwould be cumbersome to implement in plain LangChain.\nFuture Extensibility: Adding or modifying nodes requires minimal\nchanges to the overall system, allowing for smooth upgrades and new\ncapabilities.\nThis case study demonstrates how LangGraph enhances flexibility, control,\nand adaptability in complex, multi-step AI applications compared to\ntraditional LangChain chains. The ability to implement conditional flows\nbased on runtime evaluations makes LangGraph a powerful choice for\nbuilding smart, context-aware agent-based systems..\n5.6 Summary\nAgentic workflows follow fixed, predictable steps, while agents use\nLLMs for reasoning, dynamic tool selection, and real-time adaptation.\nMoving from basic LangChain apps to agentic workflows with\nLangGraph gives greater control and flexibility for complex tasks.\nLangGraph extends LangChain with stateful, multi-step workflows\norganized as nodes, edges, and explicit state updates.\nReplacing linear chains with LangGraph enables explicit state tracking\nand conditional branching, simplifying debugging and error handling.\nConditional flows based on runtime evaluations let applications adapt,\n\n\nsuch as re-running searches when results are insufficient.\nCore components include the StateGraph, node functions, and well-\ndefined edges and entry points for maintainable, extensible designs.\nThe case study shows converting a web research assistant from a rigid\nchain to a flexible agent-based system with modular design and clear\nstate management.\nLangGraph boosts control, modularity, and future scalability, enabling\ncomplex decision-making and better overall performance.\n\n\n6 RAG fundamentals with Chroma\nDB\nThis chapter covers\nImplementing semantic search using the RAG architecture\nUnderstanding vector stores and their functionality\nImplementing RAG with Chroma DB and OpenAI\nIn this chapter, you’ll dive into two essential concepts: semantic search and\nRetrieval Augmented Generation (RAG). You’ll explore how large language\nmodels (LLMs) are used for semantic search through a chatbot, enabling you\nto query a system for information across multiple documents and retrieve the\nfragments that best match the meaning of your question, rather than just\nmatching keywords. This approach is also known as Q&A over documents or\nquerying a knowledge base.\nIn earlier chapters, you learned about summarization, a typical use case for\nlarge language models. Now, I'll walk you through the basics of building a\nQ&A chatbot that searches across multiple documents. You'll interact with\nthe LLM to find the answers you're looking for.\nThis chapter focuses on RAG, the design pattern that powers semantic search\nsystems, with a particular emphasis on the vector store—a key component of\nthese systems. You’ll learn the technical terminology related to Q&A and\nRAG systems and understand how terms like \"semantic search\" and \"Q&A\"\nare often used interchangeably.\nBy the end of this chapter, you’ll have implemented a basic RAG-based\narchitecture using the APIs of an LLM (OpenAI) and a vector store (Chroma\nDB). In chapter 6, you'll build on this foundation to create Q&A chatbots\nusing RAG architecture. There's a lot to cover, so let's get started.\n\n\n6.1 Semantic Search\nSemantic search is a popular use case for LLMs, alongside summarization\nand code generation. It's one of the key applications driving the LLM and\nGenerative AI boom.\nDefinition\nSemantic search means searching for information by focusing on its meaning.\nThis involves understanding a query's meaning, retrieving relevant document\nfragments from a document store that closely match the query's meaning, and\noptionally generating a natural language answer.\nSemantic search differs from traditional keyword-based searches, which fail\nto find information if exact words don’t match. Semantic search produces\nrelevant results even if the query and result don’t share a single word.\nBefore diving into the code, I want to give you a clear understanding of a\nsemantic search chatbot's architecture. I'll start with a simple example to ease\nyou in, but by the end of this section, you'll grasp the architecture of a real-\nworld Q&A chatbot.\n6.1.1 A Basic Q&A Chatbot Over a Single Document\nI’ll start with a simple scenario to help you understand how a Q&A chatbot\nworks and to familiarize you with its components. The first chatbot example\nanswers questions about a single document, as shown in figure 6.1.\nFigure 6.1 A simple Q&A chatbot process: 1) The user sends a prompt containing a document\n(context) and a question to the chatbot; 2) The chatbot returns an initial answer; 3) The user\nfollows up with a more detailed question; 4) The chatbot provides a more accurate answer.\n\n\nThe main elements of this basic setup are:\nDocument: Contains the text for semantic search or information\nextraction.\nPrompt: Encapsulates the user’s question (semantic search) and the\ncontext (the document) with the information needed for the answer.\nLLM-based Chatbot: Sends the prompt to the LLM, which understands\nthe question and context, selects relevant information, and formulates an\nanswer for the user.\nLet’s break down these concepts further.\n",
      "page_number": 158,
      "chapter_number": 19,
      "summary": "Unlike a simple linear chain, this version of the graph introduces a new\nnode for relevance evaluation and a conditional edge that dynamically alters\nthe flow based on the relevance of the search results Key topics include graph, node, and based.",
      "keywords": [
        "search",
        "semantic search",
        "queries",
        "graph.add",
        "state",
        "Graph",
        "assistant",
        "node",
        "semantic",
        "report",
        "generate",
        "results",
        "chatbot",
        "search queries",
        "flow"
      ],
      "concepts": [
        "graph",
        "node",
        "based",
        "base",
        "state",
        "searches",
        "searching",
        "langchain",
        "relevant",
        "step"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "Segment 32 (pages 270-277)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 34,
          "title": "Segment 34 (pages 321-328)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 25,
          "title": "Segment 25 (pages 495-517)",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 31,
          "title": "Segment 31 (pages 262-269)",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 25,
          "title": "Segment 25 (pages 216-223)",
          "relevance_score": 0.5,
          "method": "api"
        }
      ]
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 166-173)",
      "start_page": 166,
      "end_page": 173,
      "detection_method": "topic_boundary",
      "content": "Definition\nContext is the text or information in the prompt, along with the user’s\nquestion, used to formulate an answer.\nDefinition\nSynthesize means to generate an answer from the question and context\nprovided.\nYou don't need to write any code to implement this initial setup. Just log into\nChatGPT or an alternative LLM-based chatbot like Gemini or Claude, and\nyou're ready to go. Let’s try a simple Q&A interaction using text about\nPaestum from britannica.com. Submit this prompt to ChatGPT (you can find\nthis prompt in a text file of the Github repository):\nRead the following text and let me know how many temples are in Paestum,\nwho constructed them, and what architectural style they are:\n---\nPaestum, Greek Poseidonia, ancient city in southern Italy near the west coast,\n22 miles (35 km) southeast of modern Salerno and 5 miles (8 km) south of\nthe Sele (ancient Silarus) River. Paestum is noted for its splendidly preserved\nGreek temples.\nVisit the ruins of the ancient Greek colony of Paestum and discover its\nhistory, culture, and society. See all videos for this article\nPoseidonia was probably founded about 600 BC by Greek colonists from\nSybaris, along the Gulf of Taranto, and it had become a flourishing town by\n540, judging from its temples. After many years’ resistance the city came\nunder the domination of the Lucanians (an indigenous Italic people)\nsometime before 400 BC, after which its name was changed to Paestum.\nAlexander, the king of Epirus, defeated the Lucanians at Paestum about 332\nBC, but the city remained Lucanian until 273, when it came under Roman\nrule and a Latin colony was founded there. The city supported Rome during\nthe Second Punic War. The locality was still prosperous during the early\n\n\nyears of the Roman Empire, but the gradual silting up of the mouth of the\nSilarus River eventually created a malarial swamp, and Paestum was finally\ndeserted after being sacked by Muslim raiders in AD 871. The abandoned\nsite’s remains were rediscovered in the 18th century.\nTemple ruins of columns and statures at Karnak, Egypt (Egyptian\narchitecture; Egyptian archaelogy; Egyptian history)\nThe ancient Greek part of Paestum consists of two sacred areas containing\nthree Doric temples in a remarkable state of preservation. During the ensuing\nRoman period a typical forum and town layout grew up between the two\nancient Greek sanctuaries. Of the three temples, the Temple of Athena (the\nso-called Temple of Ceres) and the Temple of Hera I (the so-called Basilica)\ndate from the 6th century BC, while the Temple of Hera II (the so-called\nTemple of Neptune) was probably built about 460 BC and is the best\npreserved of the three. The Temple of Peace in the forum is a Corinthian-\nDoric building begun perhaps in the 2nd century BC. Traces of a Roman\namphitheatre and other buildings, as well as intersecting main streets, have\nalso been found. The circuit of the town walls, which are built of travertine\nblocks and are 15–20 feet (5–6 m) thick, is about 3 miles (5 km) in\ncircumference. In July 1969 a farmer uncovered an ancient Lucanian tomb\nthat contained Greek frescoes painted in the early classical style. Paestum’s\narchaeological museum contains these and other treasures from the site.\nHere’s the answer I got (keep in mind, you might get a slightly different\nresult because of the stochastic nature of LLMs, but it's still likely to be\nincomplete):\nThere are three temples in Paestum, and they are constructed in the Doric\narchitectural style. These temples are:\nThe Temple of Athena (also known as the Temple of Ceres), from the\n6th century BC.\nThe Temple of Hera I (also known as the Basilica), also from the 6th\ncentury BC.\nThe Temple of Hera II (also known as the Temple of Neptune), built\n\n\naround 460 BC and the best-preserved of the three.\nThese temples are remarkable for their well-preserved Doric architecture and\nare important historical landmarks.\nThis is almost what I wanted, but it’s missing who built the temples. If this\nhad been a Q&A engine instead of a chatbot, the client system wouldn’t have\nreceived any answer about who built the temples.\nNote\nA Q&A engine, like a summarization engine, is called by a client system and\nreturns one answer. A chatbot, used by a person, allows for multiple\nquestions so you can refine the answer through interaction.\nNext, I send this prompt:\nThis answer is almost what I expected, but you forgot to answer something:\nwho built the temples?\nChatGPT responds:\nI apologize for missing that part. The temples in Paestum were built by Greek\ncolonists from Sybaris who founded the city of Poseidonia (later Paestum)\naround 600 BC. So, the Greek colonists constructed these temples in the\nDoric architectural style.\nThis shows the advantage of a chatbot over an engine: after a couple of\nrounds of questions, ChatGPT answered all my questions about the temples\nin Paestum. Importantly, I didn’t have to resubmit the full text; ChatGPT\nremembered the initial text.\nNote\nA chatbot like ChatGPT creates a stateful session that remembers the\nexchange by passing the history of the conversation as an additional context,\n\n\nallowing the user to refine the answer without resending the initial text.\nLet’s try another prompt:\nIs the archaeological site in Paestum recognized by UNESCO?\nChatGPT answered:\nYes, the archaeological site of Paestum is recognized by UNESCO. In 1998,\nit was designated a UNESCO World Heritage Site. This designation\nacknowledges the historical and architectural significance of the well-\npreserved Greek temples and ancient ruins in Paestum.\nHold on! The original text didn’t include this information. ChatGPT used its\nown knowledge, likely from Wikipedia or other sources, to answer correctly.\nThis eagerness to answer questions using unprovided content can be helpful,\nas shown in the example, but it can also be dangerous, as you will see shortly\nLet’s try another question:\nHow many columns do the three temples have in total?\nChatGPT answered:\nThe three temples in Paestum have a total of 94 columns:\nThe Temple of Athena (Temple of Ceres) has 6 columns on its shorter\nsides and 13 columns on its longer sides, for a total of 32 columns.\nThe Temple of Hera I (Basilica) has 6 columns on its shorter sides and 9\ncolumns on its longer sides, for a total of 30 columns.\nThe Temple of Hera II (Temple of Neptune) has 6 columns on its shorter\nsides and 14 columns on its longer sides, for a total of 32 columns.\nChatGPT tried to answer, but the answer is wrong. The Temple of Hera I has\n50 columns, not 30, as you can check yourself on the Wikipedia.\n\n\nWarning\nWhen a chatbot can't find the information in the provided text, it might rely\non its own knowledge, which can lead to incorrect or made-up information,\nknown as \"hallucination.\" You can limit this by forcing the LLM to only use\nthe information in the prompt and admit when it doesn't know the answer.\nBeyond prompt engineering, you can reduce hallucinations by testing the\napplication using an evaluation framework, which I'll cover in later chapters.\nIn one instance, ChatGPT provided accurate information about UNESCO, yet\nin another, it gave an incorrect answer about the number of columns in a\nstructure. This raises an important question: how can you tell when ChatGPT\nis providing reliable information and when it’s not? As a general rule, you\nshould be cautious about trusting ChatGPT for facts or details you haven’t\nindependently verified or supplied yourself.\nTip\nTo avoid this issue, instruct ChatGPT to use only the information you\nprovided and indicate if it doesn’t know the answer.\nYou can try to address the previous issue with this prompt:\nHow many columns do the three temples have in total? Use only the\ninformation I gave you in the text. If you don’t know the answer, just say so.\nYou’ll get a more accurate response:\nI apologize for the error in my previous response. The text you provided\ndoesn’t mention the total number of columns in the three temples in Paestum.\nSpoiler Alert\nDesigning safe prompts for Q&A chatbots reduces the chance of\nhallucinations. You’ll learn more about this in the rest of the book.\n\n\nNow, let’s move to a more complex use case.\n6.1.2 A More Complex Q&A Chatbot Over a Knowledge Base\nLet’s summarize the design of the basic LLM-based Q&A chatbot, like\nChatGPT, which processes a single piece of text:\n1. You send a prompt to the chatbot with the text you want to search for\nand the question you want to ask.\n2. The prompt should instruct the chatbot to formulate an answer using\nonly the provided text.\n3. The chatbot should create a session, retaining the conversation history to\nrefine answers.\nThe approach described so far works well for searching within a single text.\nBut what if your chatbot needs to answer questions about company\ninformation scattered across multiple sources—such as intranet pages, shared\nfolders, and documents in various formats like PDF, Word, TXT, or\nPowerPoint? That’s the challenge we’ll tackle in the next chapters.\nWhen designing an enterprise Q&A chatbot, one of the main obstacles is that\nyou can’t include all of the company’s content in the prompt along with the\nuser’s question—especially when dealing with large documents, this would\nquickly exceed the model’s context window. In practice, narrowing down the\ncontext to what’s most relevant improves speed, cost-efficiency, and\naccuracy. In fact, providing less—but more focused—context often yields\nbetter results than overloading the model with loosely related information.\nTo achieve this, the chatbot must be able to access the company’s knowledge\nbase and retrieve only the specific content needed to answer a given question.\nIdeally, it should “understand” the knowledge base well enough that you can\nask a question naturally—without manually supplying background context\neach time. This concept is illustrated in Figure 6.2.\nFigure 6.2 Hypothetical design for an enterprise Q&A chatbot: the knowledge of the chatbot is\nexpanded with various data sources: the enterprise intranet, knowledge bases and documents\n\n\nNow, you might wonder: how can you connect the ChatGPT chatbot (or\nGemini chat, or Claude chat) to company intranet, knowledge bases and\ndocuments, as shown in figure 6.2 above, so you can send just the question in\nthe prompt? Unfortunately, you can’t connect ChatGPT to your local data\ndirectly. The above solution doesn’t work with standard ChatGPT. The\nclosest alternative is to use ChatGPT Plus and configure a custom version\nthrough OpenAI's My GPTs offering, allowing you to upload documents for\nlookup by later queries. This is convenient for simple use cases. However, for\nmore control over how the chatbot interacts with text sources and the LLM,\nyou need a different approach. Enter the RAG design pattern.\n\n\n6.1.3 The RAG Design Pattern\nThe Retrieval Augmented Generation (RAG) design pattern is a classic\nsolution for building a Q&A chatbot. Let's break down what RAG stands for:\nRetrieval: This step involves retrieving context from a pre-prepared data\nsource, typically a vector store optimized for semantic search. Retrieval\nis a key part of the RAG architecture.\nAugmented: This means the answer is improved or enhanced by the\ncontext provided during the retrieval step.\nGeneration: This refers to generating the answer to your question. Since\nthis book focuses on LLMs and generative AI, the answer generation is\nperformed by an LLM, requiring the chatbot to interact with it.\nYou might wonder where this information is retrieved from and how. The\nkey is preparing the information so your custom chatbot can easily access it\nand then use it to augment the LLM's generated answer.\nThe RAG design pattern has two stages:\n1. Content Ingestion Stage (Indexing Stage): All the content users will\nquery is stored in a “special” database and indexed in a “special” format\nfor efficient retrieval (I will clarify what “special” means shortly).\n2. Question-Answering Stage (Retrieval and Generation Stage): The\nchatbot takes a user's question, retrieves relevant information from the\nspecial database, and feeds it along with the user's question to the LLM.\nThe LLM generates and returns the augmented answer.\nLet's dive into each phase.\nContent Ingestion Stage (Indexing Stage)\nBefore users can query the Q&A chatbot, you need to store relevant content,\nsuch as enterprise documents from various sources and formats, into a vector\nstore, a database optimized for quick search and retrieval, as shown in figure\n6.3.\n",
      "page_number": 166,
      "chapter_number": 20,
      "summary": "This chapter covers segment 20 (pages 166-173). Key topics include greek, temples, and text. You don't need to write any code to implement this initial setup.",
      "keywords": [
        "temples",
        "Paestum",
        "answer",
        "Temple of Hera",
        "chatbot",
        "Greek temples",
        "ChatGPT",
        "columns",
        "Greek",
        "text",
        "preserved Greek temples",
        "temples in Paestum",
        "information",
        "question",
        "prompt"
      ],
      "concepts": [
        "greek",
        "temples",
        "text",
        "answer",
        "answered",
        "question",
        "questions",
        "context",
        "content",
        "retrieve"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 26,
          "title": "Segment 26 (pages 211-218)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 15,
          "title": "Segment 15 (pages 122-129)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "Segment 32 (pages 270-277)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "makinggames",
          "chapter": 7,
          "title": "Segment 7 (pages 51-59)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 37,
          "title": "Segment 37 (pages 334-341)",
          "relevance_score": 0.44,
          "method": "api"
        }
      ]
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 174-182)",
      "start_page": 174,
      "end_page": 182,
      "detection_method": "topic_boundary",
      "content": "Figure 6.3 RAG Ingestion Stage: Documents are extracted from sources, split into chunks, and\nconverted into embeddings while being stored in a vector database, which stores a copy of the\noriginal chunks and their embeddings (vector form).\n\n\n\\ i |\nWie):\n= | Word docs |\nIL ! Chunks | Embeddings\nFile servers i\nPDFJ :\n\nPOF docs |\n\nys | Chunks : Embeddings\n= ii \\i—\nHTML :\n\nIntranet HTML websites Chunks | Embeddings\n\n/\n\nVector\n\nMarkdown ws Embeddings\n‘Chunks Embeddings\nKvowedgeO8s — Phintet Chunks | Embedcngs\n\nDuring the content ingestion stage, text is extracted from the sources, split\ninto small chunks, and each chunk is transformed into an \"embedding,\" a\nnumerical vector representation of the text. Splitting content into chunks is\ncrucial because embedding models work on finite sizes, and you want the\nsearch to target small, relevant content pieces instead of large, mixed-\nrelevance sections. You can create embeddings using the vector store’s\nproprietary model, an LLM provider's model (like OpenAI), or a dedicated\nembedding library. The embeddings and content chunks are then stored in a\nvector database. The purpose of the embeddings is to index the content for\nefficient lookup during the Q&A stage. This means the text in the user’s\nquestion doesn’t need to match the text in the results exactly to produce\nrelevant answers. For example, querying the vector store for \"feline animals\"\nwill return chunks mentioning \"cat,\" \"lion,\" and \"tiger,\" even if the word\n\"feline\" isn’t in any document chunk.\nQuestion & Answer Stage (Retrieval & Generation Stage)\nOnce the information has been split into small chunks, transformed into\nembeddings, and stored in a vector store, users can query your Q&A chatbot.\nLet's walk through the Q&A stage workflow, as shown in Figure 6.4.\nFigure 6.4 RAG Question & Answer Stage: 1) The chatbot uses a retriever to transform the user\nquestion into embeddings; 2) then the retriever uses the embedding to perform a similarity\nsearch in the vector store. 3) The vector store returns several relevant text chunks; 4) The\nretrieved content is fed into a prompt as a \"context\" along with the original user question. 5) The\nchatbot sends the prompt to the LLM which synthesizes a response and returns it to the user.\n\n\n' Chathot :\n\nee\n\nuser question\n\n1.The retriever transforms\nthe user question into the\ncorresponding embedding\n\n‘Nn vector DB returns text\n\n' chunks relevant to the user\n\nrelevant chunks using the | + Question 5.The LLM accepts the\nquestion embedding = | __4,The chatbot creates a prompt and synthetizes @\n! ! prompt with the user question : corresponding completion\n} Vector ' __ and the retrieved chunks (the : (the response)\n| 0B ‘context |\n\nWhen the chatbot receives a user question, a retriever converts the natural\nlanguage query into its vector representation using an embedding model. This\nstep must use the same embedding model as the ingestion stage. The chatbot\nthen uses the vectorized question to query the vector store, which understands\nthe query's semantic meaning and returns document chunks with embeddings\nsimilar to the query. It performs a vector distance calculation between the\nvectorized question and the items in the vector index stored in the database.\nThe chatbot can retrieve the closest text chunk or a set number of the closest\nchunks (ordered by distance).\nOnce the relevant content chunks are retrieved, the chatbot sends the LLM a\nprompt that includes the initial question and a \"context\" incorporating the\nretrieved document chunks. The LLM then generates (or \"synthesizes\" in\nRAG terminology) the answer and returns it to the chatbot, which then\ndelivers it to the user.\nThis final step is similar to the basic \"Q&A over a single document\" use case.\nYou provide the LLM with the initial question and a \"context\" (previously\nthe entire input text in the simple scenario) that provides the information for\nthe answer. Now, the \"context\" is represented by chunks retrieved from the\nvector store. The main difference between Q&A over a single document and\nover a range of documents in a vector database is the additional components:\nthe vector store provides the information for the answer. The role ChatGPT\nplayed in the basic chatbot use case is now split between an orchestrating\nchatbot (which accepts the query and retrieves the information) and an LLM\n(which synthesizes the answer).\nNow that you understand the high-level architecture of the RAG design\npattern, let's examine one of its key components: the vector store. After that,\nyou'll be ready to attempt your first RAG implementation.\n6.2 Vector Stores\nI have mentioned vector stores several times, but only briefly. In this section,\nI will explain what they are, their purpose, what they offer, and name a few\nexamples.\n\n\n6.2.1 What’s a Vector Store?\nA vector store is a storage system designed to efficiently store and query\nhigh-dimensional vectors. Vectors are key in AI because embeddings—\nnumerical representations of text, images, sounds, or videos—are built using\nthem. In short, embeddings are vectors that capture the meaning of words in\ntheir dimensions.\nThe main use of vector stores in LLM and ML applications is to store\nembeddings that act as indexes for text chunks (or chunks of video, image, or\naudio). Searches in vector stores are \"similarity searches,\" which measure the\ndistance between the embeddings of the query and those of the stored chunks.\nThe result is either the closest vector or a list of the closest vectors. This\n\"semantic similarity\" reflects how close the meanings of the text chunks are.\n6.2.2 How Do Vector Stores Work?\nVector distance calculations use common functions like Euclidean distance,\nCosine distance, and Hamming distance. These are used in ML algorithms\nsuch as k-Nearest Neighbors (KNN) and the more scalable Approximate\nNearest Neighbor (ANN) search, which is the standard algorithm for\nsimilarity searches.\nTip\nI won't cover distance metrics or similarity search algorithms here. If you're\ninterested, check out this academic paper by Yikun Han et al.:\nhttps://arxiv.org/pdf/2310.11703.pdf or this informal article by Erika\nCardenas: https://weaviate.io/blog/distance-metrics-in-vector-search.\nThe first vector stores, like Milvus, appeared in 2019 to support dense vector\nsearch, mainly for image recognition. These stores efficiently stored and\ncompared image embeddings. This is called \"dense vector\" search because\nthe vectors, or embeddings, have most dimensions with non-zero values.\nEarlier search techniques, like Term Frequency-Inverse Document Frequency\n(TF-IDF), used sparse vectors, where most values are zero. These were used\n\n\nfor lexical search, focusing on exact word matches and implemented in\nsystems like Lucene or BM25.\nMilvus was initially built for image-based embeddings, where the vectors\nrepresented the meaning of an image. Later, vector stores expanded to tasks\nlike product recommendations, and with the rise of large language models\n(LLMs), new vector stores emerged specializing in text-based semantic\nsimilarity search.\n6.2.3 Vector Libraries vs. Vector Databases\nThe first vector stores, known as \"vector libraries,\" like Faiss (developed by\nMeta), offered minimal functionality to keep things simple. They stored\nembeddings in memory using immutable data structures and provided\nefficient similarity search capabilities. However, as LLM adoption grew,\nthese libraries revealed several limitations:\nHandling Underlying Text: Vector libraries only stored embeddings,\nrequiring you to store the original data, such as text or images,\nelsewhere. This meant creating a unique identifier for each piece of data\nto synchronize the original text and its embeddings, complicating\nimplementation and maintenance.\nNo Updates: Vector libraries used immutable data structures, preventing\nupdates. This made them unsuitable for use cases with frequently\nchanging data, especially on an intraday basis.\nLimited Querying During Data Ingestion: Vector libraries didn’t support\nquerying during data ingestion due to the risk of simultaneous read and\nwrite operations, which could impact performance and scalability.\nTo address these issues, vendors like Pinecone developed \"vector databases,\"\noffering more features:\nHandling Text and Embeddings: Vector databases store both the text\nand related embeddings, simplifying the client application's workflow.\nMany can even handle embedding creation. They also allow storing\nmetadata associated with the text, such as provenance and lineage\ninformation.\n\n\nFull Create/Read/Update/Delete (CRUD) Capabilities: Vector databases\nsupport updating data, making them suitable for use cases with frequent\ndata changes.\nQuerying During Import: Vector databases allow similarity searches\nwhile importing new data, enhancing scalability and performance.\nVector databases soon introduced features like caching, sharding, and\npartitioning, which improved scalability, performance, robustness, and\ndurability, similar to traditional relational and NoSQL databases. Meanwhile,\nrelational databases like PostgreSQL and NoSQL databases like MongoDB,\nwhich already offered these benefits, adapted by adding support for vector\ntypes. This allows embeddings to be stored alongside text in the same record\nor document, making it easy to link text with its corresponding embedding.\nFor the rest of the book, I will use \"vector store\" and \"vector database\"\ninterchangeably, as they have converged in meaning.\n6.2.4 Most Popular Vector Stores\nCompiling a table summarizing the characteristics of the most popular vector\nstores is challenging due to their rapid evolution and convergence. However,\ntable 6.1 gives a rough idea of what's available in the market at the time of\npublication, providing a starting point for your exploration.\nTable 6.1 Most popular vector stores and related characteristics\nVector\nstore\nType\nWebsite\nFaiss\nVector\nlibrary\nhttps://github.com/facebookresearch/faiss/wiki/\nMilvus\nVector\ndatabase\nhttps://milvus.io\nQdrant\nVector\ndatabase\nhttps://qdrant.tech\nChroma\nVector\ndatabase\nhttps://www.trychroma.com\nWeaviate\nVector\ndatabase\nhttps://weaviate.io\n\n\nPinecone\nVector\ndatabase\nhttps://www.pinecone.io\nVald\nVector\ndatabase\nhttps://vald.vdaas.org\nScann\nVector\nlibrary\nhttps://github.com/google-research/google-\nresearch/tree/master/scann\nKDB\nTime series\nDB\nhttps://kdb.ai\nElastic\nSearch\nSearch\nengine\nhttps://www.elastic.co\nOpenSearch Fork of\nElasticSearch\nhttps://opensearch.org\nPgVector\nPostgresSQL\nextension\nhttps://github.com/pgvector/pgvector\nMongoDB\nAtlas\nMongoDB\nextension\nhttps://www.mongodb.com/\n6.2.5 Storing Text and Performing a Semantic Search Using\nChroma\nBefore guiding you through building an enterprise Q&A chatbot, let's first\nlearn how to store text in a vector store and perform semantic searches. This\nwill help you understand the fundamentals.\nWe'll use Chroma, a vector database that's easy to set up and use. You just\nneed to install the related Python package with pip. Let’s get started!\nSetting up Chroma DB\nFirst, set up a Chroma Jupyter notebook environment as explained in the\nsidebar below.\nSidebar: Setting up a Chroma Jupyter Notebook Environment\nCreate the virtual environment for Chapter 6's code. Open a terminal, create a\nch06 folder, navigate into it, and run:\n",
      "page_number": 174,
      "chapter_number": 21,
      "summary": "This chapter covers segment 21 (pages 174-182). Key topics include vector, embeddings, and stored. The embeddings and content chunks are then stored in a\nvector database.",
      "keywords": [
        "vector",
        "vector store",
        "vector database",
        "embeddings",
        "chunks",
        "Popular Vector Stores",
        "store",
        "text",
        "Vector databases store",
        "Vector Libraries",
        "question",
        "LLM",
        "text chunks",
        "user question",
        "database"
      ],
      "concepts": [
        "vector",
        "embeddings",
        "stored",
        "stores",
        "chunks",
        "search",
        "searches",
        "text",
        "data",
        "database"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 23,
          "title": "Segment 23 (pages 184-192)",
          "relevance_score": 0.79,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 17,
          "title": "Segment 17 (pages 139-148)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 25,
          "title": "Segment 25 (pages 495-517)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 24,
          "title": "Segment 24 (pages 208-215)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 16,
          "title": "Segment 16 (pages 141-148)",
          "relevance_score": 0.65,
          "method": "api"
        }
      ]
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 183-190)",
      "start_page": 183,
      "end_page": 190,
      "detection_method": "topic_boundary",
      "content": "C:\\Github\\building-llm-applications\\ch06>python -m venv env_ch06 \nActivate the virtual environment:\nC:\\Github\\building-llm-applications\\ch06>.\\env_ch06\\Scripts\\activ\nYou should see:\n(env_ch06) C:\\Github\\building-llm-applications\\ch06>\nInstall the necessary packages either with pip install -r requirements.txt if you\nhave cloned the Github repo, or as follows (note the chromadb version):\npip install notebook chromadb==0.5.3 openai\nIf you cloned the Github repo, start the Jupyter notebook with jupyter\nnotebook 06-chromadb-ingestion-and-querying.ipynb, otherwise create it\nfrom scratch:\njupyter notebook\nCreate a notebook with File > New > Notebook and save it as: 06-chromadb-\ningestion-and-querying.ipynb.\nOn In your notebook, import the chromadb module and create an in-memory\nclient for the vector database. Keep in mind that the in-memory client will\nlose all data when the notebook session ends. (For other ways to set up a\nChromaDB database, see the related sidebar at the end of this section:)\nimport chromadb\nchroma_client = chromadb.Client()\nNext, create a collection to store the content on Paestum from the Britannica\nwebsite. A collection is like a \"bucket\" where you store documents and their\nrelated embeddings.\ntourism_collection = chroma_client.create_collection(name=\"touris\nInserting the Content\n\n\nAdd the Paestum content to the collection, splitting it manually into a list of\nsmaller chunks (or “documents”). Chroma will then generate embeddings\nfrom the text you provide, using its default embeddings model unless you\nspecify a different one. I have shortened the text for convenience, but you can\nfind the full version in the Paestum-Britannica.txt file on my GitHub repo\nor check my notebook (also on GitHub) if needed. When storing the text, it's\nuseful to include metadata such as the source of each document and an\nassociated ID, as shown in listing 6.1.\nListing 6.1 Creating and populating a Chroma DB collection\ntourism_collection.add(\n   documents=[\n       \"Paestum, Greek Poseidonia, …[shortened] … preserved Gree\n       \"Poseidonia was probably …[shortened] … in the 18th centu\n       \"The ancient Greek part of …[shortened] … from the site.\"\n   ],\n   metadatas=[\n       {\"source\": \"https://www.britannica.com/place/Paestum\"}, \n       {\"source\": \"https://www.britannica.com/place/Paestum\"},\n       {\"source\": \"https://www.britannica.com/place/Paestum\"}\n   ],\n   ids=[\"paestum-br-01\", \"paestum-br-02\", \"paestum-br-03\"]\n)\nAfter running this code, you are ready to perform a search on the vector store.\nPerforming a Semantic Search\nLet's perform a query similar to the one you executed against ChatGPT. Ask\nfor the number of Doric temples in Paestum, specifying that you only want\nthe closest result:\nresults = tourism_collection.query(\n   query_texts=[\"How many Doric temples are in Paestum\"],\n   n_results=1\n)\nprint(results)\nHere's a shortened version of the result:\n\n\n{'ids': [['paestum-br-03']], 'distances': [[0.7664762139320374]], \nChroma understands the query's meaning and returns the correct text chunk\ncontaining the answer, along with metadata about the source and the distance\nbetween the query and answer embeddings.\nNote\nUnlike querying ChatGPT, where you had to provide the question and the full\ntext, querying Chroma only requires sending the question, as the content is\nalready stored in the Chroma DB.\nChecking Semantic Proximity\nTo see how close the returned text chunk (paestum-br-03) is to the question\ncompared to the other text chunks (paestum-br-01 and paestum-br-02),\nrequest three results:\nresults = tourism_collection.query(\n   query_texts=[\"How many Doric temples are in Paestum\"],\n   n_results=3\n)\nprint(results)\nYou should see:\n{'ids': [['paestum-br-03', 'paestum-br-01', 'paestum-br-02']], 'd\nThe embeddings for paestum-br-03 are the closest to the question’s\nembeddings, with a distance of 0.76. The chunk paestum-br-02 is the\nfarthest, with a distance of 1.33, proving that Chroma identified the most\nrelevant chunk correctly.\nNote\nThe vector database doesn’t generate an answer as ChatGPT does. It returns\nthe semantically closest text chunks to your query. For a properly formulated\nanswer, you still need an LLM model to process the original question and the\nretrieved text chunks. This approach saves costs since LLM vendors like\n\n\nOpenAI charge based on the number of tokens processed.\nThis section has given you a glimpse of what you can do with Chroma. For\nmore details, check out the official documentation at\nhttps://docs.trychroma.com/, especially the “Usage Guide” to learn how to\nrun Chroma in client/server mode if you prefer it to run on a separate host\nfrom your LLM solution.\nInstantiating ChromaDB in Different Ways\nSo far, you've worked with a local in-memory instance of ChromaDB. You\ncan also create a client for a local on-disk instance like this:\ncollection = client.create_collection(\"my_persistent_collection\")\nAlternatively, you can set up an HTTP client on the same computer or a\ndifferent one. To do this, open a command shell and run the following\ncommand (assuming you've installed ChromaDB via pip):\nchroma run --port 8010\nNext, instantiate the HTTP client in your notebook or application like this:\nclient = chromadb.HttpClient(host=\"http://localhost\", port=8010)\nOnce the client is set up, you can interact with it in the same way you do with\nthe in-memory client.\nNow that you know how to query the vector store, you can attempt to\nimplement the full RAG pattern, including generating complete answers.\n6.3 Implementing RAG from Scratch\nLet’s implement RAG by building a chatbot that uses the gpt-4o-mini model\nand a vector database. We will then ask it the same question about Paestum's\ntemples that you asked ChatGPT in section 6.1.1. When using ChatGPT, you\nhad to send a prompt with both the question and the full text on Paestum from\nBritannica. Once you build your own chatbot, you will only need to ask the\n\n\nquestion, as shown in figure 6.5 below.\nFigure 6.5 RAG Architecture including Chroma DB and GPT-4o-mini model\nAs you can see in the architectural diagram in figure 6.5, the chatbot will\nquery Chroma, retrieve the content, and feed it to GPT-4o-mini with the\noriginal question to get the full answer.\nWe'll build the chatbot step by step by implementing a few functions. First,\nimport the OpenAI library and set the OpenAI API key (assuming you're\nusing the same notebook from the previous section):\nfrom openai import OpenAI\n\n\nimport getpass\nOPENAI_API_KEY = getpass.getpass('Enter your OPENAI_API_KEY')\nNow instantiate the OpenAI client:\nopenai_client = OpenAI(api_key=OPENAI_API_KEY)\n6.3.1 Retrieving Content from the Vector Database\nAs you already know how to perform a semantic search against the vector\nstore, let’s wrap that code into a reusable function:\ndef query_vector_database(question):\n   results = tourism_collection.query(\n       query_texts=[question],\n       n_results=1\n   )\n   results_text = results['documents'][0][0]\n   return results_text\nLet’s try out this function against the same question we asked previously:\nresults_text = query_vector_database(\"How many Doric temples are \nprint(results_text)\nYou will see output like this:\nThe ancient Greek part of Paestum consists of two sacred areas co\nThis is the result we expected: notice the retrieved chunk correctly contains\nthe text “three Doric temples”.\n6.3.2 Invoking the LLM\nWe need to craft a prompt that combines the user’s question with the context\nretrieved from the vector database, and then submit it to the LLM. To get\nstarted, we’ll use a simple prompt and encapsulate the code for calling the\nLLM in a new function, as shown in Listing 6.2. For this example, I’ve set\nthe temperature to 0.7 to introduce some controlled randomness in the\nresponses.\n\n\nListing 6.2 Functions to define and execute a prompt\ndef prompt_template(question, context):\n   return f'Read the following text and answer this question: {q\ndef execute_llm_prompt(prompt_input):\n   prompt_response = openai_client.chat.completions.create(\n       model='gpt-4o-mini',\n       messages=[\n           {\"role\": \"system\", \"content\": \"You are an assistant f\n           {\"role\": \"user\", \"content\": prompt_input}\n       ],\n       temperature=0.7\n   )\n   return prompt_response\nUsing a simple Q&A prompt\nLet’s test the functions with the question that made ChatGPT hallucinate:\ntrick_question = \"How many columns have the three temples got in \ntq_result_text = query_vector_database(trick_question)\ntq_prompt = prompt_template(trick_question, tq_result_text)\ntq_prompt_response = execute_llm_prompt(tq_prompt)\nprint(tq_prompt_response)\nWe get:\nChatCompletion(id='chatcmpl-9nCGTTSUBJUmAOZWVIGbF8OE7nLF5', choic\nThe gpt-4o-mini model did not hallucinate. It correctly recognized it didn't\nhave enough information to answer the question. A few months ago, when I\nran the same code against the gpt-3.5-turbo model, it gave a wrong answer\nof 24 columns, with incorrect assumptions about the number of columns in\neach temple. I'll show you below how I fixed the issue.\nUsing a Safer Q&A Prompt\nHallucinations can be mitigated in general by using a well-known prompt for\nQ&A, available on the Langchain Hub web page (part of LangSmith).\n\n\nTip\nLangchain Hub (https://smith.langchain.com/hub) is a popular LLM resource,\nconstantly updated with open-source models, prompts, and advice on use\ncases. I highly recommend checking it out.\nHere is the recommended hallucination-safe RAG prompt from\nhttps://smith.langchain.com/hub/rlm/rag-prompt:\nUse the following pieces of retrieved context to answer the question. If you\ndon't know the answer, just say that you don't know. Use three sentences\nmaximum and keep the answer concise.\nQuestion: {question}\nContext: {context}\nAnswer:\nLet's update the prompt template function accordingly:\ndef prompt_template(question, text):\n   return f'Use the following pieces of retrieved context to answ\nNow let’s re-submit the trick question:\ntrick_question = \"How many columns have the three temples got in \ntq_result_text = query_vector_database(trick_question)\ntq_prompt = prompt_template(trick_question, tq_result_text)\ntq_prompt_response = execute_llm_prompt(tq_prompt)\nprint(tq_prompt_response)\nWe get this:\nChatCompletion(id='chatcmpl-9nCco9P3xSdArsptotrmJEjtd2N5D', choic\nWell done! You have prevented the LLM from hallucinating. Your chatbot\nwill now only use the knowledge stored in the vector database or admit\nexplicitly it doesn't know the answer.\n",
      "page_number": 183,
      "chapter_number": 22,
      "summary": "This chapter covers segment 22 (pages 183-190). Key topics include chromadb, question, and chroma. On In your notebook, import the chromadb module and create an in-memory\nclient for the vector database.",
      "keywords": [
        "question",
        "prompt",
        "text",
        "chroma",
        "Github",
        "Paestum",
        "LLM",
        "Activate the virtual",
        "openai",
        "vector",
        "results",
        "notebook",
        "vector database",
        "answer",
        "Github repo"
      ],
      "concepts": [
        "chromadb",
        "question",
        "chroma",
        "openai",
        "prompt",
        "client",
        "llm",
        "query",
        "model",
        "answer"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 25,
          "title": "Segment 25 (pages 495-517)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 23,
          "title": "Segment 23 (pages 184-192)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 61-81)",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 22,
          "title": "Segment 22 (pages 176-183)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 24,
          "title": "Segment 24 (pages 193-201)",
          "relevance_score": 0.49,
          "method": "api"
        }
      ]
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 191-199)",
      "start_page": 191,
      "end_page": 199,
      "detection_method": "topic_boundary",
      "content": "6.3.3 Building the Chatbot\nWe can now implement the chatbot with a single function, using the code\nwe’ve covered in this section:\ndef my_chatbot(question):\n   results_text = query_vector_database(question)  #A\n   prompt_input = prompt_template(question, results_text)  #B\n   prompt_output = execute_llm_prompt(prompt_input)  #C\n   return prompt_output\nLet's test it with the original question:\nquestion = \"Let me know how many temples there are in Paestum, wh\nresult = my_chatbot(question)\nprint(result)\nWe get:\nChatCompletion(id='chatcmpl-9nCmZjXJHXVEZLjNpQdAlrnm3K691', choic\nThe synthetized response is comprehensive as it answers all the questions we\nasked.\nYou should be proud of what you have achieved so far! You’ve implemented\na basic chatbot that can answer questions based on text imported into the\nvector database and provide additional information if needed. It won't return\ninformation not in the vector database, so it won't hallucinate or make up\nanswers.\nThe key takeaway is that you now understand the internals of a Q&A LLM-\nbased system, the components of the RAG design pattern, and its workflow.\nThis knowledge will help you when using frameworks like LangChain,\nLlamaIndex, and Semantic Kernel, which might hide their implementation\ndetails. You’ll be better equipped to troubleshoot problems and understand\nwhat’s going on behind the scenes.\nBefore re-implementing RAG with LangChain, let’s recap the RAG\nterminology you’ve learned so far.\n\n\n6.3.4 Recap on RAG Terminology\nThroughout this chapter, you've been learning and refining RAG\nterminology. Some terms may have similar meanings to ones you've seen\nearlier. The table below will help consolidate your understanding, especially\nfor concepts that can be expressed with different terms.\nTable 6.2 RAG Glossary\nTerm\nDefinition\nAlternative\nTerms\nRetrieval\nAugmented\nGeneration\n(RAG)\nUse case involving the generation of\ntext (typically an answer) augmented\nwith information retrieved from a\ncontent store optimized for semantic\nsearches, typically a vector store\nQ&A\nText chunk\nA fragment of text from a document.\nDocuments are split into chunks for\nmore effective searching, especially\nwhen stored in specialized unstructured\ntext stores like vector stores\nText fragment,\nchunk, text\nnode, node\nEmbeddings\nNumerical (vector) representation of a\npiece of text, used to index text chunks\nfor semantic searches\nVector\nRAG content\ningestion stage\nPhase in the RAG design where text is\nimported and indexed into a context\nstore for efficient retrieval against a\nnatural language question. In a vector\nstore, text is broken into chunks and\nindexed through their associated\nembeddings\nText indexing,\ntext\nvectorization\nVector store\nIn-memory store or specialized database\nholding text chunks and their related\nembeddings, which serve as their index\nVector\ndatabase\nSemantic\nsimilarity\nComparing pieces of text based on their\nmeaning, typically by calculating the\ndistance between the embeddings of the\n \n\n\ntext pieces. This can be done using\ncosine distance or Euclidean distance |\nvector similarity, cosine similarity\nSemantic\nsearch\nSearching for information based on its\nmeaning. This involves performing\nsemantic similarity between the\nembeddings of the search question and\nthe text chunks in a vector store\nQ&A, vector\nsearch\nContext\nText (or information) provided in the\nprompt along with the user question,\nused to formulate an answer. This can\nbe a full document or a list of text\nchunks retrieved from a vector store\nthrough semantic search\n \nSynthesize\nGenerate an answer, typically from a\nuser question and a \"context\" that\nprovides the necessary information\nGenerate\nRAG Question\n& Answer\nstage\nPhase in the RAG design where a user\nasks a search question, the application\nperforms a semantic search against a\ncontent store (typically a vector store),\nand feeds the LLM the original question\nalong with the \"context\" retrieved from\nthe store. The LLM then synthesizes\nand returns the answer to the\napplication, which passes it on to the\nuser\nRAG Question-\nAnswering\nstage\nYou are now ready to re-implement RAG with LangChain, which you will do\nin the next chapter.\n6.4 Summary\nImplement a minimalistic Q&A chatbot by feeding a question and\nsupporting document to an LLM like ChatGPT or Claude.\nFor answering questions over a knowledge base, use RAG, which\n\n\nincludes a vector store, a retriever, and an LLM to synthesize responses.\nThe RAG design pattern has two stages: ingestion (populating the vector\nstore with text and embeddings) and Q&A (retrieving relevant\ndocuments and generating responses).\nA vector store is a specialized database optimized for storing and\nretrieving text through related embeddings.\nMany vector stores evolved from in-memory libraries to full-fledged\npersistent databases.\nYou can implement a simple RAG system by directly using the APIs of\nthe LLM (such as the OpenAI API) and the vector store (such as the\nChroma DB API).\n\n\n7 Q&A chatbots with LangChain\nand LangSmith\nThis chapter covers\nImplementing RAG with LangChain\nQ&A across multiple documents\nTracing RAG chain execution with LangSmith\nAlternative implementation using LangChain Q&A specialized\nfunctionality\nNow that you understand the RAG design pattern, implementing it with\nLangChain should be straightforward. In this chapter, I’ll show you how to\nuse the LangChain object model to abstract interaction with source\ndocuments, the vector store, and the LLM.\nWe'll also explore LangSmith’s tracing capabilities for monitoring and\ntroubleshooting the chatbot workflow. Additionally, I’ll demonstrate\nalternative chatbot implementations using LangChain’s specialized Q&A\nclasses and functions.\nBy the end of this chapter, you’ll be equipped with the skills to build a\nsearch-enabled chatbot that can connect to private data sources—a complete\nversion of which you’ll construct in Chapter 12.\nBefore we start implementing the chatbot with LangChain, let’s review the\nLangChain classes that support the Q&A chatbot use case.\n7.1 LangChain object model for Q&A chatbots\nAs discussed earlier, the key benefit of using LangChain for your LLM-based\napplication is its ability to handle communication between components like\ndata loaders, vector stores, and LLMs. Instead of working with each API\n\n\ndirectly, LangChain abstracts these interactions. This allows you to swap out\nany component with a different provider without changing the overall design\nof your application.\nAdditionally, LangChain provides out-of-the-box implementations for\ncommon requirements when building LLM applications, such as splitting\nsource text, retrieving relevant context from a vector store, generating\nprompts, handling the prompt context window, and more.\nQ&A is a core use case for LangChain. The components needed for this\nworkflow are grouped by stage as follows:\nContent Ingestion (Indexing) Stage\nQ&A (Retrieval and Generation) Stage\nLet's examine these component groups in detail.\n7.1.1 Content Ingestion (Indexing) Stage\nFigure 7.1 summarizes the object model for the content ingestion stage of the\nQ&A use case.\nFigure 7.1 Object model associated with the content ingestion stage.\n\n\nDocument loaders\nparse external text\ncontent into a typed\nDocument object\n\nDocument transformers\ntransform a Document\ninto one or more\nDocument objects. For\nexample, a Text Splitter\nbreaks down raw text\ninto chunks to improve\nthe accuracy of vector-\nbased search.\n\n«create\nlist of\nobjects»\n\nA Document, considered\nthe core entity in\nLangChain, encapsulates\nboth the content and\nmetadata of unstructured\nor structured data\n\nseeeseee\n\nEmbeddings are actually\n\"embedding models”\nwhich convert\nDocuments into their\nvector representations\n\nAvector store contains for fast retrieval\n\nDocuments (and text\nchunks) and the related\nembeddings, vector\nrepresentations used for\nsemantic searches\n\nHere are the components shown in the figure:\nDocument: Models the text content and related metadata.\nBaseLoader: Loads text from external sources into the document model.\nTextSplitter: Splits documents into smaller chunks for efficient\nprocessing.\nVector Store: Stores text chunks and related embeddings for efficient\nretrieval.\nEmbedding Model: Converts text into embeddings (vector\nrepresentations).\nThis is a static view of how LangChain class families connect. For a dynamic\nview, see the sequence diagram in figure 7.2, which shows the typical content\ningestion process.\nFigure 7.2 Content Ingestion Sequence Diagram: the data ingestion orchestrator initializes a\nLoader to import text, which is parsed into a Document. The Document is transformed into\nsmaller chunks by a Text Splitter. An Embedding model generates embeddings for the chunks,\nand both the chunks and embeddings are stored in the vector store.\n\n\nHere’s how LangChain classes interact during content ingestion:\n1. The data ingestion orchestrator initializes a specific Loader to import\ntext from an external source, like CsvLoader for CSV files.\n2. The Loader parses the text and converts it into a Document object.\n3. The Document is passed to a DocumentTransformer, usually a\nTextSplitter, to divide it into smaller chunks.\n4. The chunks are processed by an Embeddings model to create\nembeddings.\n5. Both the chunks and embeddings are sent to the VectorStore for\nstorage.\n7.1.2 Q&A (Retrieval and Generation) Stage\n",
      "page_number": 191,
      "chapter_number": 23,
      "summary": "6.3.4 Recap on RAG Terminology\nThroughout this chapter, you've been learning and refining RAG\nterminology Key topics include text, document. You’ve implemented\na basic chatbot that can answer questions based on text imported into the\nvector database and provide additional information if needed.",
      "keywords": [
        "vector store",
        "text",
        "RAG",
        "vector",
        "store",
        "document",
        "Embeddings",
        "LangChain",
        "content ingestion",
        "chunks",
        "question",
        "RAG design",
        "text chunks",
        "content",
        "llm"
      ],
      "concepts": [
        "text",
        "rag",
        "document",
        "documents",
        "vectorization",
        "question",
        "questions",
        "langchain",
        "stored",
        "stores"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 16,
          "title": "Segment 16 (pages 130-138)",
          "relevance_score": 0.76,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 19,
          "title": "Segment 19 (pages 157-164)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 20,
          "title": "Segment 20 (pages 165-173)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 15,
          "title": "Segment 15 (pages 133-140)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 25,
          "title": "Segment 25 (pages 495-517)",
          "relevance_score": 0.61,
          "method": "api"
        }
      ]
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 200-208)",
      "start_page": 200,
      "end_page": 208,
      "detection_method": "topic_boundary",
      "content": "Figure 6.3 summarizes the object model for the retrieval and generation stage\nof the Q&A use case.\nFigure 7.3 Object model associated with the retrieval and generation stage.\n\n\n«abstract» «create object» «create object» —\nee ee ee e\nBasePromptTempiate MessagePromptTemplate\n\n«abstract» «abstract» «abstract» «abstract»\nSuing Pipeline Base ChatMessage MessagesPlaceholder\nPromptTempiate PromptTempiate ChatPromptTempiate PromptTempiate\nHumanMessage\nPromptTemplate ChatPromptTemplate PromptTemplate\n\nAlMessage\nPromptTemplate\nSystemMessage\nPromptTemplate\n\nA MessagePromptTemplate\n\nBo is designed to generate a\nPromptvalue instance from a\ntemplate structure including\nmessages from various roles\n(human, system, Al) and\n\ninput parameters\n\nA PromptTemplate is\ndesigned to generate a\nPromptValue instance\nfrom template structures\nand input parameters\n\nALLM model is\nsuitable for one-off\ninstructions\ninvoked by a client\nquery engine\n\n«use object»\n\nME ~~ we ee ee ee eee eee eee\n\nA Chat model is suitable\nfrom a chatbox which\n\n(concrete LLM\n\n' «return object.\nmodels ...) accepts interactive : a\ninstructions and keeps an A Retriever\nhistory for reference ‘ efficiently fetch\n\n«use object» data, often a list of\nwabstracts Pe «abstract» Documents,\nVOCOOTSIONO Fann ees «abstract» BaseRetriever typically from a\nTomeeeeeees \"use object» Embeddi vector store\nB indexed with\nAvector store contains embeddings\n\nEmbeddings are actually\n\nembeddings, vector\nrepresentations used for\nsemantic searches\n\nDocuments (and text 5 e\nchunks) and the related embedding models «abstract»\nwhich convert VectorStoreRetriever\n\nDocuments into their\nvector representations\nfor fast retrieval\n\nFAIS SRetriever\n(many others ...)\n\n(other retrievers...)\n\n(many others ...)\n\nThe figure includes the following components:\nVectorStore: Stores and retrieves relevant text chunks.\nRetriever: Retrieves relevant text chunks from the Vector Store based\non the similarity between the query's embedding and the stored text\nembeddings.\nEmbeddings (Embedding model): Ensures consistent embeddings for\nqueries and documents.\nPrompt / PromptTemplate: Constructs the input for the language model,\ntypically using the user question and a context made of retrieved text\nchunks.\nLanguageModel: Generates answers using the provided context and\nquery.\nFor a dynamic view of the Q&A workflow, see the sequence diagram in\nfigure 7.4.\nFigure 7.4 Q&A Sequence Diagram: the Q&A orchestrator sends the user’s question to the\nvector store Retriever, which generates an embedding using an Embeddings model. The\nRetriever searches for similar embeddings in the vector store and returns relevant documents.\nThe Q&A orchestrator combines the documents and the question into a prompt with a\nPromptTemplate, then sends it to the Language Model, which provides the answer.\n\n\nHere’s how LangChain classes interact during the Q&A process:\n1. The Q&A orchestrator sends the user's question to the vector store\nRetriever.\n2. The Retriever generates the question's embedding using an Embeddings\nmodel.\n3. The Retriever searches the vector store for documents with similar\nembeddings and returns them to the Q&A orchestrator.\n4. The Q&A orchestrator combines the retrieved documents and the user’s\nquestion into a prompt using a PromptTemplate.\n\n\n5. The orchestrator sends the prompt to the LanguageModel, which returns\nthe answer.\nNow that you understand the Q&A object model, let's get started with the\nimplementation!\n7.2 Vector Store Content Ingestion\nBefore querying your documents, you must store them in a vector database.\nTo make it more interesting, we'll import content about Paestum from various\nsources and in different formats, rather than just using the small Britannica\narticle on Paestum as before.\nFirst, install the required packages for loading, splitting documents, and\ncreating embeddings. We'll use the open-source embeddings model from the\nsentence-transformers package, which includes the Chroma embeddings\nmodel all-MiniLM-L6-v2. This helps save on costs, but feel free to use the\nOpenAI embeddings model if cost isn't an issue.\nOpen a new OS shell, navigate to the folder for this chapter, create and\nactivate the virtual environment for Chapter 7, and install the required\npackages (as usual, if you have cloned the repository from Github you can\ninstall the packages with pip install -r requirements.txt instead, after\nactivating the virtual environment):\nC:\\Github\\building-llm-applications\\ch07>python -m venv env_ch07\nC:\\Github\\building-llm-applications\\ch07>.\\env_ch07\\Scripts\\activ\n(env_ch07) C:\\Github\\building-llm-applications\\ch07>pip install w\nOnce the installation is complete, start a Jupyter notebook, as usual:\n(env_ch07) C:\\Github\\building-llm-applications\\ch07>jupyter noteb\nCreate a new notebook with File > New > Notebook, give it a name (e.g.,\nch07-QA_across_documents), and save it.\nImport the necessary libraries on the notebook:\n\n\nfrom langchain_community.document_loaders import WikipediaLoader, \nfrom langchain_chroma import Chroma\nfrom langchain_text_splitters import RecursiveCharacterTextSplitt\nfrom langchain_openai import OpenAIEmbeddings\nCapture the OpenAI API key as usual:\nimport getpass\nOPENAI_API_KEY = getpass.getpass('Enter your OPENAI_API_KEY')\n7.2.1 Splitting and Storing the Documents\nTo make the content more searchable:\n1. Split each document into chunks of about 500 characters. You can add\noverlap, usually around 10%, but for now, set the overlap to 0. The size\nof chunks and overlap considerations will be covered in Chapter 8 on\nadvanced indexing techniques;\n2. calculate the embeddings of the document chunks and store them in the\nChroma vector database.\nInstantiate the splitter, embeddings model, and vector database client as\nfollows:\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, ch\nembeddings_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY\nvector_db = Chroma(\"tourist_info\", embeddings_model) \nProcess each document by loading it, splitting it into chunks, and storing the\nchunks with the related embeddings into the vector database:\nwikipedia_loader = WikipediaLoader(query=\"Paestum\")\nwikipedia_chunks = text_splitter.split_documents(wikipedia_loader\nvector_db.add_documents(wikipedia_chunks)\nOnce the content has been split and stored in the vector database, you will see\nan output similar to this:\n['d3197373-7df1-4c24-8a0a-0145c176042c',\n'435add06-6b85-421a-8ad8-be88b7defe08',\n'7fbd7575-5f56-4fed-9642-34f587325699',\n\n\n'6a798aca-9dcb-433c-b0d3-196acfd83b0e',\n… \n]\nNote\nThe WikipediaLoader also loads content from other Wikipedia hyperlinks\nreferenced in the requested article. For example, the Paestum article\nreferences the National Archaeological Museum of Paestum, the Lucania\nregion, Lucanians, and the temples of Hera and Athena. As a result, it will\nload these related articles, providing more content than you might expect.\nFor other document formats:\nword_loader = Docx2txtLoader(\"Paestum/Paestum-Britannica.docx\")\nword_chunks = text_splitter.split_documents(word_loader.load())\nvector_db.add_documents(word_chunks)\npdf_loader = PyPDFLoader(\"Paestum/PaestumRevisited.pdf\")\npdf_chunks = text_splitter.split_documents(pdf_loader.load())\nvector_db.add_documents(pdf_chunks)\ntxt_loader = TextLoader(\"Paestum/Paestum-Encyclopedia.txt\")\ntxt_chunks = text_splitter.split_documents(txt_loader.load())\nvector_db.add_documents(txt_chunks)\nNote\nIf you prefer, you can process the full PaestumRevisited-\nStockholmsUniversitet.pdf document instead of its shorter version,\nPaestumRevisited.pdf. This will take considerably longer, but it will provide\ninformation for a wider range of questions.\nRemoving duplication\nYou might have noticed that the code above has a lot of duplication. Let's\nextract some common functionality into a function:\ndef split_and_import(loader):\n    chunks = text_splitter.split_documents(loader.load())\n    vector_db.add_documents(chunks)\n\n\n    print(f\"Ingested chunks created by {loader}\")\nNow you can call this function after instantiating each loader, as shown in\nListing 7.1:\nListing 7.1 Refactored ingestion of different file types\nwikipedia_loader = WikipediaLoader(query=\"Paestum\")\nsplit_and_import(wikipedia_loader)\nword_loader = Docx2txtLoader(\"Paestum/Paestum-Britannica.docx\")\nsplit_and_import(word_loader)\npdf_loader = PyPDFLoader(\"Paestum/PaestumRevisited.pdf\")\nsplit_and_import(pdf_loader)\ntxt_loader = TextLoader(\"Paestum/Paestum-Encyclopedia.txt\")\nsplit_and_import(txt_loader)\nAfter running this code, you will see output like this:\nIngested chunks created by <langchain_community.document_loaders.w\nIngested chunks created by [… SHORTENED] \n7.2.2 Ingesting Multiple Documents from a Folder\nI have just shown you how to ingest different types of documents (web, MS\nWord, PDF, txt) using a specialized document loader for each type. This\napproach works fine for a few documents, but if you need to load many\ndocuments, a more efficient method is needed. After placing all the\ndocuments in a folder (e.g., /CilentoTouristInfo), there are a couple of\nways to achieve this:\n1. Iterating over the files in the folder and calling the relevant loader.\n2. Using the purpose-built DirectoryLoader.\nLet's explore both approaches.\nIterating over all files in a folder\n\n\nYou can ingest all the files located in a folder into a vector store by iterating\nover the files, identifying the file type, and using the relevant loader as\ndescribed in the previous section. Before iterating over the files, create a\nloader factory as shown below:\nListing 7.2 Loader factory: instantiating the relevant extension-specific loader\nloader_classes = {\n   'docx': WordLoader,\n   'pdf': PdfLoader,\n   'txt': TxtLoader\n}\nimport os\ndef get_loader(filename):\n   _, file_extension = os.path.splitext(filename)\n   file_extension = file_extension.lstrip('.')\n   \n   loader_class = loader_classes.get(file_extension)\n   \n   if loader_class:\n       return loader_class(filename) \n   else:\n       raise ValueError(f\"No loader available for file extension \nExercise\nI leave you with an exercise to iterate over the files in /CilentoTouristInfo\nand load them into the vector store using get_loader() and\nsplit_and_import(). If you're unsure, check the section \"Ingesting Multiple\nDocuments from a Folder\" in my Python notebook ch07-\nQA_across_documents.ipynb on GitHub.\nIngesting all files with DirectoryLoader\nAn alternative method for loading all files in a folder into the vector store is\nby using the DirectoryLoader. This loader, part of a third-party package by\nUnstructured (Unstructured offers a platform and tools for ingesting and\nprocessing unstructured documents for RAG and fine-tuning), takes a folder\n",
      "page_number": 200,
      "chapter_number": 24,
      "summary": "The Q&A orchestrator combines the documents and the question into a prompt with a\nPromptTemplate, then sends it to the Language Model, which provides the answer Key topics include documents, document, and embeddings.",
      "keywords": [
        "Documents",
        "vector store",
        "vector",
        "loader",
        "embeddings",
        "chunks",
        "Embeddings model",
        "Paestum",
        "model",
        "store",
        "text",
        "object",
        "vector database",
        "vector store Retriever",
        "folder"
      ],
      "concepts": [
        "documents",
        "document",
        "embeddings",
        "models",
        "loader",
        "ingestion",
        "ingested",
        "ingest",
        "retrieval",
        "retriever"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 25,
          "title": "Segment 25 (pages 495-517)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 14,
          "title": "Segment 14 (pages 113-121)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 27,
          "title": "Segment 27 (pages 233-241)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 61-81)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 19,
          "title": "Segment 19 (pages 157-164)",
          "relevance_score": 0.57,
          "method": "api"
        }
      ]
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 209-219)",
      "start_page": 209,
      "end_page": 219,
      "detection_method": "topic_boundary",
      "content": "path and a glob pattern (a string with wildcard characters to specify a set of\nfilenames).\nFirst, install the unstructured package or the langchain-unstructured\nwrapper along with its dependencies. Follow the instructions for your\noperating system in the LangChain documentation\n(https://python.langchain.com/)> Integrations > Providers table >\nUnstructured) or the Unstructured documentation\n(https://docs.unstructured.io/).\nIn your Jupyter notebook, import the DirectoryLoader:\nfrom langchain_community.document_loaders import DirectoryLoader\nYou can now load and ingest the files into the vector store with the following\ncode:\nfolder_path = \"CilentoTouristInfo\"\npattern = \"**/*.{docx,pdf,txt}\"  #A\ndirectory_loader = DirectoryLoader(folder_path, pattern)  #B\nsplit_and_import(directory_loader)\nNOTE\nWhile the code above is included in the GitHub repository, its execution\ndepends on successfully installing the unstructured or langchain-unstructured\npackage. Because setup can vary across operating systems, the code may not\nrun consistently in all environments. I've added comments in the code to\nclarify this.\n7.3 Q&A Across Stored Documents\nNow that you have stored all the content about Paestum, let’s query the\nvector store directly to see which documents are retrieved.\n7.3.1 Querying the vector store directly\n\n\nLet's query the vector store to see what documents are retrieved for a question\nthat requires information from different sources:\nquery = \"Where was Poseidonia and who renamed it to Paestum?\"\nresults = vector_db.similarity_search(query, 4)  #A\nprint(results)\nHere is an excerpt from the results, showing that the most relevant chunks\nreturned by the vector store come from different sources, as you can verify in\nthe related metadata:\n[Document(metadata={'source': 'Paestum/Paestum-Britannica.docx'}, \nAlthough you now know which documents the vector store returns for your\nquery, the next step is to get a well-formulated answer.\n7.3.2 Asking a Question through a LangChain Chain\nUnlike when you implemented RAG using the OpenAI gpt-4o-mini model\ndirectly in section 5.3, with LangChain, you don't need to manually craft a\nprompt and configure it with the original question and the context from the\nvector store. You can set up a RAG chain, which will automatically\ninstantiate and execute a prompt template, as shown in Figure 7.5 below.\nFigure 7.5 LangChain RAG chain, LangChain RAG chain, packaging the prompt parameters\ninto a prompt instance, which is then sent to the chat or LLM model.\n\n\nSet up the prompt using a template from LangChain Hub\n(https://smith.langchain.com/hub). For clarity I have simply copied it and set\nit explicitly below:\nfrom langchain_core.prompts import PromptTemplate\nrag_prompt_template = \"\"\"Use the following pieces of context to a\nIf you don't know the answer, just say that you don't know, don't \nUse three sentences maximum and keep the answer as concise as pos\n{context}\nQuestion: {question}\nHelpful Answer:\"\"\"\nrag_prompt = PromptTemplate.from_template(rag_prompt_template)\nAlternatively, you can pull the prompt instance directly from LangChain\nHub:\nfrom langchain import hub\nrag_prompt = hub.pull(\"rlm/rag-prompt\")\n7.3.3 Completing the RAG Chain Setup\nTo complete the setup of the RAG chain, we need a few more components, as\nshown in Figure 7.6 below.\n\n\nFigure 7.6 Amended RAG chain, including runnable passthrough and retriever\nFigure 7.6 shows your RAG chain, which includes:\nRetriever: Retrieves relevant text content from the vector store and\ninjects it into the \"context\" parameter of the prompt.\n\n\nQuestion Feeder: Implemented as a simple “pass-through” component, it\nfeeds the user's question through the Runnable interface (an abstract\nclass on which every LangChain component is based).\nChat Model: Processes the prompt to generate the answer.\nInstantiate the retriever, question feeder, and chat model:\nretriever = vector_db.as_retriever()\nfrom langchain_core.runnables import RunnablePassthrough\nquestion_feeder = RunnablePassthrough()\nfrom langchain_openai import ChatOpenAI\nchatbot = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model_name=\"g\nSet up the RAG chain. As mentioned earlier, each block in a LangChain\nchain implements the Runnable interface and accepts a dictionary as input.\nThis is why the first block in the chain is a dictionary:\nrag_chain = {\"context\": retriever, \"question\": question_feeder} | \nCreate a utility function to execute the chain:\ndef execute_chain(chain, question):\n   answer = chain.invoke(question)\n   return answer\nAsk a question:\nquestion = \"Where was Poseidonia and who renamed it to Paestum? A\nanswer = execute_chain(rag_chain, question)\nprint(answer.content)\nThe answer you will get should be similar to:\nPoseidonia was located on the coast of the Tyrrhenian Sea in Magn\nInspect the full answer object if needed:\nprint(answer)\nYou should see something like:\n\n\ncontent='Poseidonia was located on the coast of the Tyrrhenian Se\nFollow-up Question\nLet’s find out if the chatbot can sustain the conversation by asking a follow-\nup question. I'll make it deliberately vague to see if the chatbot can\nunderstand what I'm after:\nQuestion = \"And then, what did they do? Also tell me the source\" \nanswer = execute_chain(rag_chain, question)\nprint(answer.content)\nThe chatbot returns:\nI don't know. The source is not provided in the context.\nIt seems the chatbot is a bit lost and doesn't understand that \"they\" refers to\nthe Romans. This is because the chatbot has no memory of previous\nquestions and answers. Currently, it is stateless and simply passes questions\nfrom the user to the LLM and back, without retaining any memory of the\nconversation flow.\nLet's see how we can fix this.\n7.4 Chatbot memory of message history\nOne of the most useful features of an LLM-based chatbot, compared to an\nLLM-based engine, is its ability to remember previous questions and\nresponses, allowing you to continue querying until you get a satisfactory\nanswer. This capability is crucial for providing context to each new prompt.\nLet's see how to incorporate message history into the RAG setup we finalized\nin the previous section, which I have summarized in listing 7.3 for\nconvenience:\nListing 7.3 Initial RAG Setup Before Incorporating Message History\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.prompts import PromptTemplate\n\n\nfrom langchain_openai import ChatOpenAI\nrag_prompt_template = \"\"\"Use the following pieces of context to a\nIf you don't know the answer, just say that you don't know, don't \nUse three sentences maximum and keep the answer as concise as pos\n{context}\nQuestion: {question}\nHelpful Answer:\"\"\"\nrag_prompt = PromptTemplate.from_template(rag_prompt_template)\nretriever = vector_db.as_retriever()\nquestion_feeder = RunnablePassthrough()\nchatbot = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model_name=\"g\nrag_chain = {\"context\": retriever, \"question\": question_feeder} | \ndef execute_chain(chain, question):\n   answer = rag_chain.invoke(question)\n   return answer\nFirst, we should amend the prompt to include message history.\n7.4.1 Amending the Prompt\nThe original RAG prompt doesn't account for message history, so we need to\nmodify it. Since message history is a core feature of the memory-enabled\nRAG design, we should use a different prompt helper than\nPromptTemplate.from_template. This helper was based on a template\nparameterized by a user question ({question}) and the context ({context})\npulled by the retriever from the vector store. LangChain provides a more\nsuitable prompt helper, ChatPromptTemplate.from_messages, which creates\na prompt from a list of chat messages.\nChat Messages\nMost chat-oriented LLMs use a standard format for messages and roles\nassociated with chat messages. A chat message is typically a key-value pair\nlike this:\n\n\n(\"role\": \"message text\")\nWhere:\n\"role\" can be \"system\", \"human\", or \"ai\" (see table 7.1 for\ndescriptions).\n\"message text\" stands for the text of the exchanged message.\nTable 7.1 Chat Message Roles\nRole\nDescription\nSample message\nSystem\nThis role represents the chatbot\napplication. You typically\ncreate one system message at\nstartup to instruct the chatbot\non its persona.\nYou are a world-class expert\nin Roman and Greek history,\nespecially in towns located\nin southern Italy. Provide\ninteresting insights on local\nhistory and recommend\nplaces to visit with\nknowledgeable and\nengaging answers.\nHuman\nThis message comes from a\nuser, typically a question.\nCan you please recommend\nsome attractions around\nPaestum and give me some\ninformation on them?\nAI\nThis is the synthesized\nresponse from the LLM.\nThe best attractions in\nPaestum are the three Greek\ntemples built around 500\nBC.\nCustom\nYou can use other non-\nstandard roles to incorporate\nmessages containing useful\ninformation. For example,\n\"Context\" for text retrieved\nfrom the vector store, or \"Chat\nHistory\" for the entire chat\nhistory.\n \n\n\nA chat history is a list of such messages:\nchat_history = [\n   (\"system\", \"You are a world-class expert in Roman and Greek h\n   (\"human\", \"Can you please recommend some attractions around P\n   (\"context\", \"Paestum was a major ancient Greek city on the co\n   (\"ai\", \"The best attractions in Paestum are the three Greek t\n]\nChat-Based Prompt\nNow that you understand how to model chat messages, you can create a\nmessage-based prompt:\nfrom langchain_core.prompts import ChatPromptTemplate\nrag_prompt = ChatPromptTemplate.from_messages(\n   [\n       (\"system\", \"You are a helpful assistant, world-class expe\n       (\"chat_history\", \"{chat_history_messages}\"),\n       (\"context\", \"{retrieved_context}\"),\n       (\"human\", \"{question}\"),\n   ]\n)\nYou will re-instantiate this prompt at each interaction with the user.\nSpecifically, after the initial interaction, which starts with an empty\nchat_history, you will feed the new {question}, the newly\n{retrieved_context}, and the accumulated {chat_history_memory}.\nYou already know how to feed a new question and a newly retrieved context:\nit’s the same as before, but the prompt is now message-based.\nIn the next section, I will show you how to update the message history at\neach user interaction.\n7.4.2 Updating the Chat Message History\nLangChain provides the ChatMessageHistory class to model chat message\nhistory. We can instantiate the chat_history_memory variable as a\nChatMessageHistory type to hold the chat history:\n\n\nfrom langchain_community.chat_message_histories import ChatMessag\nchat_history_memory = ChatMessageHistory()\nYou can add messages for Human (user question) and AI (LLM response) to\nthe chat history using the ChatMessageHistory convenience methods listed\nin Table 7.2.\nTable 7.2 ChatMessageHistory Convenience Methods for Standard Roles\nRole\nConvenience Method\nHuman\nadd_user_message(user_question)\nAI\nadd_ai_message(llm_response)\nYou don't need to add messages associated with the System and Context roles\nto the chat message history, as they are already part of the prompt and won't\nprovide the LLM with additional information. The only messages that need\ntracking are those encapsulating user questions and LLM responses.\nUpdate the chat history in the execute_chain() function as follows:\ndef execute_chain_with_memory(chain, question):\n   chat_history_memory.add_user_message(question)\n   answer = chain.invoke(question)\n   chat_history_memory.add_ai_message(answer)\n   print(f'Full chat message history: {chat_history_memory.messa\n   return answer\nWhen the chat_history_memory object is updated with the latest Human or\nAI messages, you can retrieve the entire message history using the messages\nproperty of the ChatMessageHistory class:\nfull_message_history = chat_history_memory.messages\nYou will inject this into the prompt each time the user asks a new question.\n7.4.3 Feeding the Chat History to the RAG Chain\nAfter updating your code to include message history in the prompt and\nupdating it after each user question and LLM response, you need to feed the\nupdated message history to the RAG chain. You can do this by redefining the\n\n\nchain with LCEL:\nrag_chain = {\n   \"retrieved_context\": retriever, \n   \"question\": question_feeder,\n   \"chat_history_messages\": chat_history_memory.messages\n} | rag_prompt | chatbot\nIn this setup, the chat_history_messages prompt parameter is fed through\nthe corresponding chat_history_messages property above.\n7.4.4 Putting Everything Together\nTo clarify the changes made for integrating chatbot memory into the RAG\nchain, here's the complete code in Listing 7.4.\nListing 7.4 RAG Chain with Chatbot Memory\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_community.chat_message_histories import ChatMessag\nfrom langchain_core.runnables import RunnableLambda\nrag_prompt = ChatPromptTemplate.from_messages(\n   [\n       (\"system\", \"You are a helpful assistant, world-class expe\n       (\"placeholder\", \"{chat_history_messages}\"),\n       (\"assistant\", \"{retrieved_context}\"),\n       (\"human\", \"{question}\"),\n   ]\n)\nretriever = vector_db.as_retriever()\nquestion_feeder = RunnablePassthrough()\nchatbot = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model_name=\"g\nchat_history_memory = ChatMessageHistory()\ndef get_messages(x):\n   return chat_history_memory.messages\nrag_chain = {\n   \"retrieved_context\": retriever, \n   \"question\": question_feeder,\n",
      "page_number": 209,
      "chapter_number": 25,
      "summary": "This chapter covers segment 25 (pages 209-219). Key topics include messages, question, and questions. Follow the instructions for your\noperating system in the LangChain documentation\n(https://python.langchain.com/)> Integrations > Providers table >\nUnstructured) or the Unstructured documentation\n(https://docs.unstructured.io/).",
      "keywords": [
        "RAG chain",
        "RAG",
        "question",
        "history",
        "message history",
        "chat",
        "Chat Message History",
        "Chain",
        "message",
        "prompt",
        "LangChain RAG chain",
        "LangChain",
        "chat message",
        "chat history",
        "context"
      ],
      "concepts": [
        "messages",
        "question",
        "questions",
        "answer",
        "prompt",
        "langchain",
        "chain",
        "context",
        "retrieved",
        "retrieves"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 15,
          "title": "Segment 15 (pages 122-129)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 13,
          "title": "Segment 13 (pages 104-112)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 20,
          "title": "Segment 20 (pages 165-173)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "Segment 32 (pages 270-277)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "Segment 28 (pages 232-240)",
          "relevance_score": 0.58,
          "method": "api"
        }
      ]
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 220-229)",
      "start_page": 220,
      "end_page": 229,
      "detection_method": "topic_boundary",
      "content": "   \"chat_history_messages\": RunnableLambda(get_messages)\n} | rag_prompt | chatbot\ndef execute_chain_with_memory(chain, question):\n   chat_history_memory.add_user_message(question)\n   answer = chain.invoke(question)\n   chat_history_memory.add_ai_message(answer)\n   print(f'Full chat message history: {chat_history_memory.messa\n   return answer\nTesting the Amended Chain\nNow, let's test the updated chain with the same question we asked previously:\nquestion = \"Where was Poseidonia and who renamed it to Paestum? A\nanswer = execute_chain_with_memory(rag_chain, question)\nprint(answer.content)\nHere is the chat message history accumulated so far, followed by the answer:\nFull chat message history: [HumanMessage(content='Where was Posei\nPoseidonia was an ancient Greek city located on the coast of the Tyrrhenian\nSea in what is now southern Italy. It was renamed Paestum by the Romans\nafter they took control of the city in 273 BC, following its conquest by the\nLucanians. The source of this information is from Wikipedia.\nThe answer is similar to what you got with the memoryless chatbot. Let’s see\nwhat happens if we now ask the same follow up-question we asked\npreviously:\nquestion = \"And then what did they do? Also tell me the source\" \nanswer = execute_chain_with_memory(rag_chain, question)\nprint(answer.content)\nNow we get the following response:\nFull chat message history: [HumanMessage(content='Where was Posei\n\n\nAfter the Romans renamed Poseidonia to Paestum, they developed the city\nfurther, enhancing its infrastructure and economy, particularly through\nagriculture and trade. They also constructed significant buildings, including\ntemples dedicated to Greek gods, which demonstrate the city's cultural\ncontinuity. The source of this information is from historical texts on Roman\nhistory and archaeology.\nAs you can see, the chat history now contains all the exchanged questions\nand answers. Most importantly, the chatbot understands that \"they\" refers to\nthe Romans and provides a coherent response. Congratulations! You have\nnow completed a fully functional chatbot that remembers previous messages\nand can sustain a proper conversation.\nBefore considering the chatbot complete, I would like to cover another\nimportant topic: tracing its chain execution with LangSmith.\n7.5 Tracing Execution with LangSmith\nLangSmith is a comprehensive developer platform for every stage of the\nLLM-based application lifecycle, whether you’re using LangChain or not. It\nhelps you debug, collaborate on, test, and monitor your LLM applications.\nLLM applications can be unpredictable due to their natural language inputs,\nwhich can create edge cases that are hard to reproduce and debug with\ntraditional tools. LangSmith addresses these challenges throughout the LLM\napplication development lifecycle:\nDevelopment and Debugging: LangSmith's tracing feature ensures your\nLLM application workflow executes as expected and helps troubleshoot\ndeviations. Its Hub provides standard, battle-tested prompts for common\nuse cases, speeding up implementation.\nEvaluation and Testing: LangSmith supports testing with built-in\nevaluators for relevance, correctness, and sensitivity of LLM\ncompletions. It also provides tools to build datasets from various\nsources, including production data, for continuous and regression\ntesting.\nMonitoring: LangSmith's tracing capabilities allow you to monitor the\nreal-time status of your LLM production application. It supports\n\n\nfeedback through human labels and annotations, enabling investigation\nand correction when the application deviates from the expected path.\nTo enable LangSmith's tracing, you just need to set up the LangSmith API\nkey and some tracing configurations at the top of your application. For more\ndetails on LangSmith and how to set up the API key, refer to the sidebar\nbelow.\nSidebar: Setting up LangSmith’s API Key\nSet up a LangSmith API key as follows:\n1. Register at LangSmith: https://smith.langchain.com/. Click the Sign-Up\nbutton to create an account using GitHub, Google, Discord, or an email\naddress.\n2. Log in to the LangSmith website, click the Settings cog-wheel on the\nleft-hand menu, and then click the Api Keys menu to access the Api\nKeys page.\nApi Keys screen - Access this screen by clicking the Settings cog-wheel and then the Api Keys\nmenu; you can create a new API key here.\n\n\n3. Click the Create API Key button in the top-right corner. Provide a\ndescription for the key (e.g., LangChain in Action examples) and select\nPersonal Access Token as the key type. Then click Create API Key. Be\n\n\nsure to copy the key and store it in a secure location, as you won’t be\nable to view it again on the LangSmith site—only the first and last few\ncharacters will remain visible.\nWith the LangSmith API key, you’ll be able to use it in your projects.\nThe easiest way to enable tracing through LangSmith is by setting a few\nenvironment variables before launching your Jupyter notebook. If your\nnotebook is already running, close it first. Then, follow these steps (I am\nassuming you are using in a Windows cmd shell):\n1. Navigate to the notebook folder and activate your virtual environment:\nC:\\Github\\building-llm-applications\\ch07> env_ch07\\Scripts\\activa\n2. Set the following environment veriables, which activate and configure\ntracing through LangSmith (I assume you are in C:\\Github\\building-llm-\napplications\\ch07, so I have shortened the folder name below):\n(env_ch07) C:\\...\\ch07>set LANGSMITH_TRACING=true\n(env_ch07) C:\\...\\ch07>set LANGSMITH_ENDPOINT=https://api.smith.l\n(env_ch07) C:\\...\\ch07>set LANGSMITH_PROJECT=Q & A chatbot\n(env_ch07) C:\\...\\ch07>set LANGSMITH_API_KEY=<YOUR_LANGSMITH_API_\n3. Restart the Jupyter notebook:\n(env_ch07) C:\\...\\ch07>jupyter notebook 07-QA_across_documents.ipynb\n4. Re-run the notebook cells up to the end of Section 7.3 (Q&A Across\nDocuments) or optionally through Section 7.4 (Chatbot Memory).\nOnce you’ve completed these steps, LangSmith will have captured tracing\ninformation for your notebook execution. You can view and analyze this\ntrace data directly in the LangSmith dashboard.\n7.5.1 Inspecting the LangSmith Traces\nGo to the LangSmith website (smith.langchain.com) > Projects > Q&A\nChatbot. Click the latest trace, as shown in the screenshot below:\n\n\nFigure 7.7 LangSmith high-level trace: click the latest trace to get high-level details of the chain\nexecution.\nThe current view shows all the traces associated with your Q&A Chatbot\nproject (a project, in LangSmith’s terminology is the collection of all the\ntraces associated to an application or to a part of an application), ordered by\nexecution time, with the most recent at the top. When you click any trace,\nsuch as the latest one at the top, you will get more details. I have split the\nthree panels into two figures for clarity. Figure 7.8 shows the left-hand panel\nand the middle panel.\nFigure 7.8 LangSmith trace details obtained by clicking one of the traces, for example, the latest\none at the top. You will see three panels: 1) The left panel shows all traces of the Chatbot Q&A\nproject; 2) the middle panel shows the runs of the selected trace.\n\n\nFigure 7.9 shows the middle panel and the right-hand panel.\nFigure 7.9 Middle and right-hand panels you get when clicking the latest trace; 1) the middle\npanel shows the runs of the selected trace; 2) the right panel shows the input to the selected trace\nand its output.\n\n\nLet's go through the three panels shown in the figures above that you see\nwhen clicking the latest trace on the Q&A Chatbot project webpage:\nLeft Panel: Shows the list of all traces associated with your project\n(Chatbot Q&A), ordered from the latest at the top to the oldest at the\nbottom. Each trace can be expanded into its inner steps. Click the latest\n\n\ntrace, which is the top one.\nMiddle Panel: Graphically displays the chain execution runs for the\nselected trace, including the vector-store Retriever and the ChatOpenAI\nLLM client. Each run represents the execution details of a chain\ncomponent within the trace. These runs are ordered by execution time\nand include their duration down to the centisecond.\nRight Panel: Shows the trace input (the user question) and its output (the\nsynthetized response).\nYou can get further details by clicking one of the runs in the middle panel, as\nshown in figure 7.10 below:\nFigure 7.10 Run details; when you click the Retriever run, its details appear in the right-hand\npanel, showing its input (the user question) and its output (the documents retrieved from the\nvector store).\n\n\nFor instance, clicking the Retriever run in the middle panel shows its details\nin the right-hand panel. You will see the original query in the Input section\nand the documents retrieved from the vector store in the Output section.\nThis is just a glimpse of LangSmith's tracing capabilities using a simple\n",
      "page_number": 220,
      "chapter_number": 26,
      "summary": "Most importantly, the chatbot understands that \"they\" refers to\nthe Romans and provides a coherent response Key topics include tracing, trace, and question.",
      "keywords": [
        "LangSmith API key",
        "LangSmith",
        "API key",
        "chat message history",
        "API",
        "Create API Key",
        "LangSmith API",
        "panel",
        "Api Keys",
        "trace",
        "key",
        "middle panel",
        "LangSmith tracing",
        "chatbot",
        "panel shows"
      ],
      "concepts": [
        "tracing",
        "trace",
        "question",
        "questions",
        "setting",
        "applications",
        "keys",
        "answers",
        "panels",
        "chain"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 26,
          "title": "Segment 26 (pages 211-218)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 15,
          "title": "Segment 15 (pages 122-129)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "Segment 32 (pages 270-277)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 13,
          "title": "Segment 13 (pages 103-110)",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 23,
          "title": "Segment 23 (pages 190-197)",
          "relevance_score": 0.47,
          "method": "api"
        }
      ]
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 230-240)",
      "start_page": 230,
      "end_page": 240,
      "detection_method": "topic_boundary",
      "content": "chain. I encourage you to examine each chain step in detail. Start by clicking\na trace substep in the left panel and then inspect its associated runs by\nselecting the one you want to examine in the middle panel. If you're eager to\nexperiment, create a new LangSmith project to trace the execution of the\nresearch summarization engine we built in chapter 4. This will help you\nappreciate a more complex trace that captures a wider range of chain\ncomponents.\nBefore closing the chapter, I want to mention a useful LangChain class you\nmight consider for simple Q&A chains.\n7.6 Creating a Q&A Chain with RetrievalQA\nDepending on your specific use case, you might set up a Q&A chain more\neffectively with the RetrievalQA chain utility function. This is similar to the\nMapReduceDocumentsChain and load_summarize_chain functions used for\nsummarization chains in the previous chapter:\nfrom langchain.chains import RetrievalQA\nrag_chain = RetrievalQA.from_chain_type(\n  llm=chatbot,  #A\n  chain_type=\"stuff\",  #B\n  retriever=retriever,  #C\n  return_source_documents=False  #D\n)\nYou can now execute it with the run() method:\nquestion = \"Where was Poseidonia and who renamed it to Paestum? A\nwith trace(\"RetrievalQA\", \"chain\", project_name=\"Q&A chatbot\", in\n   answer = execute_chain(rag_chain, question)\n   print(answer)\n   rt.end(outputs={\"output\": answer})\nThis returns the following result, similar to what we got earlier. Notice the\ninput and output properties are called “query” and “result” respectively. Feel\nfree to inspect the RetrievalQA trace in detail on the LangSmith website.\n{'query': 'Where was Poseidonia and who renamed it to Paestum? Al\n\n\nBefore moving on, check out the sidebar on how to configure the retriever for\nother types of searches.\nConfiguring the Retriever\nYou can configure the retriever more accurately by setting its parameters:\nsearch_type: Specifies the search algorithm to retrieve from the vector\ndatabase.\nsearch_kwargs: Specifies the parameters associated with the search\nalgorithm. In Python, kwargs refers to a special syntax (**kwargs) that\nallows you to pass a variable number of keyword arguments to a\nfunction. It collects these arguments into a dictionary, making it easy to\nhandle optional or dynamic function parameters.\nBy default, the retriever is configured for a plain Similarity Search (with the\nimplicit configuration of `search_type = \"similarity\"`). However, you can\nexplicitly configure it for a Similarity Search with a Threshold using\n`search_type=\"similarity_score_threshold\"` or for a Max Marginal Relevance\nSearch with `search_type = \"mmr\"`. You can provide additional\nconfiguration details through the `search_kwargs` parameter.\nFor example, to perform a similarity search and exclude any result with a\nsimilarity score lower than 0.8, while returning 3 results, you can configure\nthe retriever as follows:\nretriever = vector_db.as_retriever(\n   search_type=\"similarity_score_threshold\",\n   search_kwargs={'score_threshold': 0.8, 'k': 3}\n)\nConsult the official documentation for more information on search\nparameters.\n7.7 Summary\nLangChain abstracts core components of RAG, making the design\nmodular and flexible.\n\n\nLangChain's object model includes classes that simplify and abstract the\nRAG content ingestion phase, such as:\nBaseLoader: Imports a text source into one or more Document\nobjects.\nBaseDocumentTransformer: Transforms a loaded Document, for\nexample, by splitting it into smaller Document chunks.\nVectorStore: Efficiently stores Document objects optimized for\nsemantic searching.\nEmbedding Models: Index Document objects with their\nembeddings (vector representations) when storing them in the\nvector store.\nThe Q&A or retrieval stage of the RAG solution is supported by\nLangChain abstractions, including:\nBasePromptTemplate: Models a generic prompt and produces a\nspecific PromptValue instance after parameters are fed to it.\nBaseRetriever: Abstracts the retrieval process from the vector store.\nBaseLanguageModel: Abstracts clients to LLMs.\nA typical RAG chain includes a passthrough component, a vector store\nwith a retriever, a prompt template, and an LLM or chat client.\nLLM-based chatbots retain memory of previous questions and\nresponses, preserving context for ongoing queries until a satisfactory\nanswer is achieved.\nUse LangChain's ChatPromptTemplate.from_messages to create\nprompts and ChatMessageHistory to manage chat history, feeding it into\nthe RAG chain for each interaction between user and LLM.\nLangSmith allows you to analyze execution runs and traces, drilling\ndown into each step to examine the input and output of each component.\nLangChain provides additional specific classes for building Q&A\nchatbots tailored to simpler use cases.\n\n\n8 Advanced indexing\nThis chapter covers\nAdvanced RAG techniques for more effective retrieval\nSelecting the optimal chunk splitting strategy for your use case\nUsing multiple embeddings to enhance coarse chunk retrieval\nExpanding granular chunks to add context during retrieval\nIndexing strategies for semi-structured and multi-modal content\nIn chapter 7, you learned about the basics of the RAG (Retrieval-Augmented\nGeneration) architecture, a common pattern for building LLM-based\napplications. To simplify things, I introduced a stripped-down version. While\nthis was useful for learning, using this basic version in a real-world scenario\nwill often produce poor results. You might notice inaccurate answers, even\nwhen your content store (usually a vector store) has relevant data. These\nissues often arise from poorly phrased queries, inefficient text indexing, or\nineffective use of metadata attached to text chunks.\nIn this chapter, I’ll show you how to avoid these issues and use advanced\nmethods to improve accuracy. When working with LLM applications using\nLangChain, most of your time will be spent refining your design to get the\nbest results. This involves selecting advanced RAG techniques,\nexperimenting, and refining prompts. You can call yourself proficient only\nwhen you understand and apply these techniques effectively.\nHere, we’ll cover advanced indexing strategies, including creating multiple\nembeddings for larger text chunks stored in the vector database. Using this\nmethod boosts search precision and provides better context for generating\nhigh-quality responses.\n8.1 Improving RAG Accuracy\nTo improve RAG (Retrieval-Augmented Generation) accuracy, analyze each\n\n\nstep in both the content ingestion and Q&A workflows. Each stage may pose\nspecific issues, but it also opens up opportunities for improvement. Let's start\nwith the content ingestion stage.\n8.1.1 Content Ingestion Stage\nRetrieval accuracy can be improved through an optimized content ingestion\nprocess that aligns with the specific features of each content store. Relying\nonly on basic indexing can reduce indexing depth and weaken retrieval\nperformance. Figure 8.1 shows two key areas for improvement in the\ningestion stage: refining embedding calculations and optimizing how\nembeddings are linked to related text chunks in the vector store.\nFigure 8.1 Common accuracy issues in the ingestion stage of a simple RAG architecture are often\ndue to inadequate indexing that only uses basic embeddings for each text chunk. Advanced\nindexing techniques involve generating multiple embeddings for each chunk, enhancing\nsearchability.\n\n\nRAG Ingestion phase\n\n1, request the embedding service to calculate\nthe embeddings of the document chunks\n\nae 2 retum embeddings Embeddings\nText Ingestion script service\n\n3, store documents chunks and related\nembeddings into Vector store\n\nAE\n\nTypically we only calculate the\nembedding of the content of the\ntext chunk, However we can\ncalculate multiple embeddings to\nmake the chunk more searchable\n\nVector DB\n\nAdvanced indexing\n\nEven if a question is clear, retrieval can fail with overly simple indexing\nstrategies. In vector indexing, chunk size and overlap length are crucial:\nsmaller chunks may work well for precise questions but fail with broader\nqueries, while larger chunks may lack detail for specific questions. To\naddress this, you can add additional embeddings based on distinct chunk\nfeatures, as illustrated in the figure above. This multi-faceted approach helps\nmake each text chunk more adaptable and searchable. We’ll explore\nadvanced indexing techniques in this chapter.\n8.1.2 Question Answering Stage\nThe effectiveness of the question-answering stage depends largely on how\naccurately the system interprets and processes user queries. Many potential\nissues, highlighted in Figure 8.2, can disrupt this process.\nFigure 8.2 Common Issues in the Q&A stage of the naive RAG architecture and their solutions:1)\nHandle poorly formulated questions with question transformation; 2) Enhance retrieval accuracy\nby transforming the original user questions into more suitable vector DB search queries; 3)\nInclude relevant data sources by adding structured data content stores, such as relational DBs; 4)\nGenerate DB queries for structured data content stores; 5) Filter out irrelevant context retrieved\nfrom the content stores.\n\n\nRAG Q8A phase\n\nQuestions can be imprecise,\nincorrect or too vague. They can be\nreformulated as better questions\n\nThe context retrieved from the\ncontent stores might not be entirely\n\nrelevant for the user question,\n\nYou should filter out irrelevant\ncontext,\n\n1) Question transformation\n5) Remove irrelevant context\n\n4 Prompt witha question\n1. User question and context (Search results)\n———>\nChatbot —\n\nee\n\n—_—_.___{.\nUser\n\n6. Chatbot answer 5 LLM completion\n\nChatbot UI LLM\n\n2. User wi 3, Search results\n\nChatbot UI\nThe original question\nmight not be suitable for\nthe retrieval step. It The vector store might have limited\nmight need to be adapted as\n\na more suitable vector DB search\n\nor incomplete information to\nquery\n\nanswer the question. You might have\nto include additional content stores\nand generate the related structured DB\nqueries from the original user question.\n\n2) Query generation for retrieval Vector DB\n\n3) Include additional content stores\nRelational 0B such as relational databases\n\n4) generate structured DB queries from\nthe natural language user question\n\nWalking through the workflow of the Q&A stage of the RAG architecture,\nshown in the diagram above, you’ll encounter several pitfalls that can lead to\nsuboptimal answers. Each issue has a targeted solution:\nPoor question formulation: If a user’s question is unclear, the vector\nstore may return weak context, leading to poor results. The LLM\nstruggles when working with unclear queries and subpar context. A fix\nis to rephrase the question into a clearer, more detailed form before\npassing it to the retrieval system.\nIneffective question for retrieval: Using the original question for both\nretrieval and generation can fail, especially when the query is broad or\nabstract. Broad questions may not pinpoint relevant content, resulting in\nless accurate context. You can address this by breaking down broad\nquestions into specific sub-questions to retrieve more precise\ninformation.\nLimited data relevance in the content store: Most RAG systems rely\nonly on vector stores, but adding structured data sources, like relational\ndatabases, tables, or graph databases, can improve results. Route queries\nto the appropriate content store based on the type of data needed to\nenhance answer accuracy.\nLimited querying capabilities against structured data: While vector\nstores and LLMs excel with natural language, relational and graph\ndatabases don’t process it directly, creating a barrier to using structured\ncontent. Use an LLM to generate structured queries (e.g., SQL) tailored\nto each data source to overcome this.\nIrrelevant search results fed to the LLM: Even with clearer questions\nand better indexing, irrelevant data can sometimes slip through, adding\nnoise to the answer. Reduce this by applying filtering or post-processing\nsteps to keep only the most relevant results.\nInsufficient improvement in answer accuracy: Sometimes fixing\nindividual issues doesn’t yield expected improvements. In these cases,\nboost accuracy by combining multiple techniques—such as advanced\nindexing, question transformations, and multi-store routing—into an\nensemble strategy that maximizes precision.\nThe table below summarizes these issues and their solutions.\n\n\nTable 8.1 Common issues in Naïve RAG architecture and recommended solutions\nIssue\nSolution\nRetrieval returns the wrong content\nchunks\nAdvanced document indexing\ntechniques\nPoor question formulation\nQuestion transformations\nIneffective question for retrieval\nQuestion transformations\nLimited data relevance in content\nstore\nRouting to multiple content stores\nLimited querying capabilities for\nstructured data\nContent store query generation\nIrrelevant retrieved results fed to\nLLM\nRetrieval post-processing\nThis chapter, along with the next two, will dive into each problem and its\nsolution in detail.\n8.2 Advanced Document Indexing\nFor an LLM to generate high-quality answers, the relevance and accuracy of\nthe text chunks retrieved from the vector store (or any document store) are\ncritical. The quality of these chunks depends on several key factors:\nSplitting Strategy: The granularity of document chunks impacts retrieval\naccuracy. Smaller chunks yield more precise results for specific queries\nbut lack broader context. Larger chunks offer richer context but may\nmiss fine details. Choosing the right split size is essential and can be\noptimized using various techniques. Additional factors—such as chunk\noverlap and document hierarchy—also play a crucial role in balancing\ncontext and relevance.\nEmbedding Strategy: How you index each chunk is equally important.\nYou can use embeddings, metadata, or a combination. Advanced\nstrategies use multiple indexes—such as child document embeddings,\nsummaries, or hypothetical questions associated with a chunk—to\ncapture both fine-grained details and broader context.\nSentence Expansion: One method to deliver larger chunks without\nlosing detail is to expand smaller chunks by including surrounding\n\n\nsentences during retrieval. This approach provides additional context\nwithout sacrificing specificity.\nIndexing Structured and Semi-Structured Data: Retrieving structured\ndata (e.g., database tables or multimedia content) using unstructured\nqueries requires specialized techniques. This can include generating\nembeddings for database rows, images, or even audio files.\nThis chapter will cover each of these methods in detail, starting with\nstrategies for optimal document splitting.\n8.3 Splitting Strategy\nDuring the ingestion phase of RAG, documents are split into chunks before\nbeing stored in a vector database or document store. Each chunk is indexed\nusing embeddings and sometimes metadata (e.g., by tagging it with relevant\nkeywords). Vector similarity searches rely on the embeddings index, while\nmetadata searches use the keyword index.\nThe easiest way to improve the relevance of document chunk retrieval is to\nchoose the right document splitting strategy for your use case. Ideally, the\ndocument store should return all relevant chunks that provide the LLM with\nenough context to generate accurate answers. The size of these chunks plays\na critical role in retrieval performance, as shown in figure 8.3.\nFigure 8.3 Impact of Chunk Size on Answer Accuracy. Coarse chunks provide more context but\nless focus, and they are suitable for broader questions. Granular chunks provide less context but\nmore focus and they are suitable for more detailed questions.\n",
      "page_number": 230,
      "chapter_number": 27,
      "summary": "This chapter covers segment 27 (pages 230-240). Key topics include question, questions, and retriever. Before closing the chapter, I want to mention a useful LangChain class you\nmight consider for simple Q&A chains.",
      "keywords": [
        "rag",
        "question",
        "search",
        "vector store",
        "vector",
        "content",
        "chunks",
        "store",
        "Document",
        "chain",
        "retrieval",
        "content stores",
        "llm",
        "embeddings",
        "context"
      ],
      "concepts": [
        "question",
        "questions",
        "retriever",
        "retrieve",
        "retrieval",
        "chunks",
        "query",
        "queries",
        "index",
        "indexes"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 20,
          "title": "Segment 20 (pages 165-173)",
          "relevance_score": 0.72,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 19,
          "title": "Segment 19 (pages 157-164)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Effective_Modern_C++",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 26,
          "title": "Segment 26 (pages 518-535)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 25,
          "title": "Segment 25 (pages 216-223)",
          "relevance_score": 0.65,
          "method": "api"
        }
      ]
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 241-248)",
      "start_page": 241,
      "end_page": 248,
      "detection_method": "topic_boundary",
      "content": "Smaller, more granular chunks are better suited for answering detailed\nquestions because they focus on specific topics. However, they contain less\n\n\nsurrounding context, which can reduce effectiveness for broader queries. In\ncontrast, larger chunks are more effective for general questions since they\nprovide more context but lose focus on fine-grained details.\nThe challenge is to balance chunk size based on expected question types.\nSmall chunks work well when queries are precise, as the vector\nrepresentation of the question will match closely with those of the relevant\nchunks. But for broader questions, small chunks might miss context, making\nretrieval less accurate. Larger chunks help cover broader topics but at the cost\nof losing detailed semantic information. The added context in these larger\nchunks, however, can be valuable during answer generation, as it provides\nthe LLM with more background data to work with.\nFinding the right balance between granular and coarse chunks depends on\nyour use case. Ask yourself: are the questions likely to be detailed or broad?\nThe chunk size should match the expected query type.\nSplitting strategies\nThe size of the chunks isn’t the only factor. You also need to decide how to\nsplit the document. There are two main approaches:\n1. Splitting by Document Hierarchy: This approach respects the natural\nstructure of the document (e.g., chapters, sections, paragraphs). It works\nwell when the document is organized by topics, as chunks represent\ncoherent subtopics. Tools like HTMLHeaderTextSplitter and\nMarkdownHeaderTextSplitter in LangChain target specific document\ntypes and maintain semantic accuracy. However, chunk sizes can vary\ngreatly.\n2. Splitting by Absolute Size: You can define chunk size by characters,\ntokens, sentences, or words. This results in more consistent chunk sizes,\nbut context might be lost if chunks split mid-sentence.\nCharacterTextSplitter and its variants support different granularity\nlevels, but you’ll need to test for optimal size. Evaluating a range of\nfixed sizes is necessary to find what works best for your use case.\nFactors to Consider\n\n\nFor each splitting strategy, keep in mind:\nDocument Type: If you’re working with mixed content (e.g., text, tables,\nimages), maintaining related content within the same chunk is crucial. In\nthis case, splitting by document hierarchy is more effective than a fixed-\nsize approach.\nSearch Type: if you are planning to use metadata search, keywords can\nbe refined depending on the chunk granularity. You can also attach a\nmix of broad and detailed tags to each chunk to increase retrieval\nflexibility.\nChoosing the Right Strategy\nSelecting the best strategy often requires a mix of trial and error, but\nexperience will eventually guide you to the right balance between document\nhierarchy and absolute size. Table 8.2 summarizes the pros and cons of each\nstrategy and provides relevant LangChain classes to implement them.\nTable 8.2 Splitting Strategies, Pros and Cons, and Related LangChain Classes\nSplitting\nStrategy\nPros\nCons\nLangChain Classes\nDocument\nHierarchy\nMore\naccurate\nsemantic\nmeaning\nChunk size\nvaries\nsignificantly\nHTMLHeaderTextSplitter,\nMarkdownHeaderTextSplitter\nBy Size -\nNumber\nof Tokens\nConsistent\nchunk size\nIncomplete\nsentences at\nboundaries\nCharacterTextSplitter.\nfrom_tiktoken_encoder\nBy Size -\nNumber\nof\nCharacters\nConsistent\nchunk size\nIncomplete\nsentences\nmay reduce\nsemantic\nvalue\nCharacterTextSplitter\nBy\nSentence,\nParagraph,\nRetains\nsemantic\nmeaning\nSmall\nchunks may\nlack enough\nRecursiveCharacterTextSplitter\n\n\nor Word\nin most\ncases\ncontext\nEach method has its use case, and the choice depends on your specific needs\nand document structure. In the following sections, I’ll cover these strategies\nin detail, starting with document hierarchy splitting.\n8.3.1 Splitting by HTML Header\nIn this section, I’ll show you how to split documents using the\nHTMLHeaderTextSplitter class on various online documents from\nWikivoyage.org, a travel-focused site from the same group as Wikipedia.\nWe’ll explore how different levels of splitting granularity affect the accuracy\nof responses.\nFirst, set up your environment by creating a new folder and virtual\nenvironment, and then install the required packages. If you’ve cloned the\nrepository from GitHub, you can install dependencies directly with the\nprovided requirements file:\nC:\\Github\\building-llm-applications\\ch08> python -m venv env_ch08\nC:\\Github\\building-llm-applications\\ch08> env_ch08\\Scripts\\activa\n(env_ch08) C:\\Github\\building-llm-applications\\ch08> pip install \nNext, start a new Jupyter notebook or open the existing one in the cloned\nrepository:\n(env_ch08) C:\\Github\\building-llm-applications\\ch08> jupyter note\nSave the notebook as 08-advanced_indexing.ipynb and import the required\nlibraries:\nfrom langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nimport getpass\nOPENAI_API_KEY = getpass.getpass('Enter your OPENAI_API_KEY')\nSetting Up Chroma DB Collections\n\n\nNow, create a Chroma DB collection to store the more granular chunks:\ncornwall_granular_collection = Chroma(  #A\n   collection_name=\"cornwall_granular\",\n   embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_\n)\ncornwall_granular_collection.reset_collection()  #B\nThis will initialize a new Chroma collection called\ncornwall_granular_collection. If the collection already exists, it will be\nreset to start fresh.\nNext, set up a second collection for coarser chunks:\ncornwall_coarse_collection = Chroma(  #A\n   collection_name=\"cornwall_coarse\",\n   embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_\n)\ncornwall_coarse_collection.reset_collection()  #B\nLoading HTML Content with AsyncHtmlLoader\nThe final step is to ingest some content about Cornwall, a region in the UK\nknown for its stunning seaside resorts, using an HTML loader:\nfrom langchain_community.document_loaders import AsyncHtmlLoader\ndestination_url = \"https://en.wikivoyage.org/wiki/Cornwall\"\nhtml_loader = AsyncHtmlLoader(destination_url)\ndocs = html_loader.load()\nThis snippet fetches the Cornwall page content, which we’ll use to create\nboth granular and coarse chunks. In the following steps, I’ll show how to split\nthe content based on HTML headers and analyze the impact on retrieval\naccuracy.\nSplitting Content into Granular Chunks Using HTMLSectionSplitter\nBefore storing the content from the `docs` object into the vector database,\ndecide on a splitting strategy. Since this content is from an HTML page, you\n\n\ncan split it by H1 and H2 tags, which separate the content into sections. This\nwill generate more granular chunks, as shown in listing 8.1.\nListing 8.1 Splitting content with the HTML Section splitter\nfrom langchain_text_splitters import HTMLSectionSplitter\nheaders_to_split_on = [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2\")]\nhtml_section_splitter = HTMLSectionSplitter(headers_to_split_on=h\ndef split_docs_into_granular_chunks(docs):\n   all_chunks = []\n   for doc in docs:\n       html_string = doc.page_content  #A\n       temp_chunks = html_section_splitter.split_text(html_strin\n       all_chunks.extend(temp_chunks)\n   return all_chunks\nYou can now generate the granular chunks:\ngranular_chunks = split_docs_into_granular_chunks(docs)\nNow, insert the granular chunks into the Chroma collection:\ncornwall_granular_collection.add_documents(documents=granular_chu\nSearching Granular Chunks\nYou can now run a search for specific content within the granular chunks:\nresults = cornwall_granular_collection.similarity_search(query=\"E\nfor doc in results:\n   print(doc)\nSplitting Content into Coarse Chunks Using\nRecursiveCharacterTextSplitter\nFor larger, coarser chunks, use the RecursiveCharacterTextSplitter class.\nStart by creating the necessary objects:\nfrom langchain_community.document_transformers import Html2TextTr\n\n\nfrom langchain_text_splitters import RecursiveCharacterTextSplitt\nhtml2text_transformer = Html2TextTransformer()\ntext_splitter = RecursiveCharacterTextSplitter(\n   chunk_size=3000, chunk_overlap=300\n)\nNext, define a function to split the content into coarse chunks:\ndef split_docs_into_coarse_chunks(docs):\n   text_docs = html2text_transformer.transform_documents(docs)  \n   coarse_chunks = text_splitter.split_documents(text_docs)  #B\n   return coarse_chunks\nThen generate the coarse chunks:\ncoarse_chunks = split_docs_into_coarse_chunks(docs)\nInsert these chunks into the corresponding Chroma collection:\ncornwall_coarse_collection.add_documents(documents=coarse_chunks)\nSearching Coarse Chunks\nYou can now search for more general content within the coarse chunks:\nresults = cornwall_coarse_collection.similarity_search(query=\"Eve\nfor doc in results:\n   print(doc)\nIngesting Content from Multiple URLs\nTo make the searches more comprehensive, load additional content into the\ncollections. Listing 8.2 shows how to set up new granular and coarse\ncollections for various UK destinations and ingest the related content chunks.\nIf you'd like to minimize processing costs, consider reducing the size of the\nuk_destinations list.\nListing 8.2 Creating collections for multiple UK destinations\nuk_granular_collection = Chroma(  #A\n\n\n   collection_name=\"uk_granular\",\n   embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_\n)\nuk_granular_collection.reset_collection()  #B\nuk_coarse_collection = Chroma(  #A\n   collection_name=\"uk_coarse\",\n   embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_\n)\nuk_coarse_collection.reset_collection()  #B\nuk_destinations = [\n   \"Cornwall\", \"North_Cornwall\", \"South_Cornwall\", \"West_Cornwal\n   \"Tintagel\", \"Bodmin\", \"Wadebridge\", \"Penzance\", \"Newquay\",\n   \"St_Ives\", \"Port_Isaac\", \"Looe\", \"Polperro\", \"Porthleven\",\n   \"East_Sussex\", \"Brighton\", \"Battle\", \"Hastings_(England)\", \n   \"Rye_(England)\", \"Seaford\", \"Ashdown_Forest\"\n]\nwikivoyage_root_url = https://en.wikivoyage.org/wiki\nuk_destination_urls = [f'{wikivoyage_root_url}/{d}' for d in uk_d\nfor destination_url in uk_destination_urls:\n   html_loader = AsyncHtmlLoader(destination_url)  #C\n   docs = html_loader.load()  #D\n   \n   granular_chunks = split_docs_into_granular_chunks(docs)\n   uk_granular_collection.add_documents(documents=granular_chunk\n   coarse_chunks = split_docs_into_coarse_chunks(docs)\n   uk_coarse_collection.add_documents(documents=coarse_chunks)\nYou can now perform both granular and coarse searches:\ngranular_results = uk_granular_collection.similarity_search(query\nfor doc in granular_results:\n   print(doc)\ncoarse_results = uk_coarse_collection.similarity_search(query=\"Ev\nfor doc in coarse_results:\n   print(doc)\nTry experimenting with different queries, like \"Beaches in Cornwall\", to\nsee how the results vary between granular and coarse chunks. This approach\nwill help you fine-tune the balance between detailed and general content\n",
      "page_number": 241,
      "chapter_number": 28,
      "summary": "The added context in these larger\nchunks, however, can be valuable during answer generation, as it provides\nthe LLM with more background data to work with Key topics include docs, split.",
      "keywords": [
        "chunks",
        "granular",
        "coarse",
        "granular chunks",
        "content",
        "coarse chunks",
        "docs",
        "cornwall",
        "collection",
        "size",
        "chunk size",
        "chroma",
        "HTML",
        "Splitting",
        "API"
      ],
      "concepts": [
        "docs",
        "doc",
        "split",
        "sizes",
        "html",
        "collections",
        "collection",
        "document",
        "documents",
        "content"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 26,
          "title": "Segment 26 (pages 518-535)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 25,
          "title": "Segment 25 (pages 216-223)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 21,
          "title": "Segment 21 (pages 189-197)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 21,
          "title": "Segment 21 (pages 193-203)",
          "relevance_score": 0.58,
          "method": "api"
        }
      ]
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 249-256)",
      "start_page": 249,
      "end_page": 256,
      "detection_method": "topic_boundary",
      "content": "retrieval.\n8.4 Embedding Strategy\nPreviously, I discussed how keyword searches are more flexible than vector\nsearches because you can tag a document with multiple keywords. The same\nidea can be applied to embeddings—you can store multiple vectors per\ndocument, which increases the flexibility and accuracy of vector searches. I’ll\nwalk through various multi-vector strategies in the following sections, mainly\nfocusing on how to use LangChain’s MultiVectorRetriever.\nThe key to these strategies is a two-layer chunk structure. The top layer\nincludes synthesis chunks—the chunks fed into the LLM to generate answers.\nThe lower layer consists of retrieval chunks, smaller segments that create\nprecise embeddings for retrieving the synthesis chunks. I suggest testing all\nmulti-vector retriever techniques for your use case, as they usually yield\nsimilar performance improvements. However, since the structure of your text\nmay favor one technique over another, experimenting with each will help you\nidentify the best fit.\n8.4.1 Embedding Child Chunks with ParentDocumentRetriever\nA common challenge with chunk size is balancing between context and\ndetail. Large chunks work for broad questions but struggle with detailed\nqueries. Small chunks, while supporting detailed queries, often lack the\ncontext needed for generating comprehensive answers. This creates a\ntradeoff: if chunks are too small, the response might be incomplete; if they’re\ntoo large, the retrieval may be less precise.\nTo solve this, split the document into larger parent chunks and create smaller\nchild chunks within each parent. Use the child chunks solely for generating\nmore granular embeddings, which are then stored against the parent chunk.\nThis hybrid approach allows each document to have embeddings for both\nbroad and detailed queries, as illustrated in figure 8.4.\nFigure 8.4 Child Chunk embeddings—A coarse document chunk is indexed with its own\nembedding and the embeddings generated from its smaller child chunks. This allows it to match\n\n\neffectively with both broad and detailed queries.\n\n\nCoarse chunking strategy = =\n\nCoarse chunks\n\nCoarse chunk\n\nChild chunk #1 embedding\n\nCoarse chunk embedding Child chunk #4\n\nChild chunk #2 embedding\n\nx Child chunk #3 embedding\nChild chunk #3\n\nA detailed question is likely to hit\nthe embedding of a child chunk of\na coarse chunk\n\nA broad question is likely to hit\nthe embedding of a coarse\nchunk\n\nBroad question\n\nThe advantage of this approach is that when a broad query is made, the parent\nchunk is retrieved, providing rich context. For more detailed queries, the\nchild embeddings ensure precise matching which still lead to the retrieval of\nthe context-rich parent chunk. This structure helps the LLM generate more\naccurate and contextually relevant answers.\nI’ll show you how to implement the technique using\nParentDocumentRetriever. Start by importing the necessary libraries:\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.document_loaders import AsyncHtmlLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitt\nSetting Up the ParentDocumentRetriever\nThe approach begins by splitting content into large, coarse chunks for\nsynthesis, then further dividing each into smaller child chunks for retrieval.\nListing 8.3 demonstrates how to configure the splitters and set up the\nretriever. As shown, documents are stored in an InMemoryStore—a general-\npurpose, in-memory key-value store designed to hold serializable Python\nobjects such as strings, lists, and dictionaries. It’s particularly useful for\ncaching and intermediate data storage.\nListing 8.3 Setting Up Parent and Child Splitters for Coarse and Granular Chunks\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=3000) \nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=500)  \nchild_chunks_collection = Chroma(  #C\n   collection_name=\"uk_child_chunks\",\n   embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_\n)\nchild_chunks_collection.reset_collection()  #D\ndoc_store = InMemoryStore()  #E\n\n\nparent_doc_retriever = ParentDocumentRetriever(  #F\n   vectorstore=child_chunks_collection,\n   docstore=doc_store,\n   child_splitter=child_splitter,\n   parent_splitter=parent_splitter\n)\nIngesting Content into Document and Vector Stores\nNow, generate both coarse and granular chunks using the configured splitters\nand add them to the respective stores.\nfor destination_url in uk_destination_urls:\n   html_loader = AsyncHtmlLoader(destination_url)  #A\n   html_docs =  html_loader.load()  #B\n   text_docs = html2text_transformer.transform_documents(html_do\n   print(f'Ingesting {destination_url}')\n   parent_doc_retriever.add_documents(text_docs, ids=None)  #D\nVerifying the In-Memory Document Store\nYou can check if the coarse chunks have been correctly stored:\nlist(doc_store.yield_keys()) \nPerforming a Search on Granular Information\nNow perform a search on the child chunks using the\nParentDocumentRetriever:\nretrieved_docs = parent_doc_retriever.invoke(\"Cornwall Ranger\")\nThe first document retrieved (retrieved_docs[0]) will contain rich\ncontextual information:\nDocument(metadata={'source': 'https://en.wikivoyage.org/wiki/Sout\nComparing with Direct Semantic Search on Child Chunks\n\n\nNow, compare the results by directly searching only the child chunks:\nchild_docs_only = child_chunks_collection.similarity_search(\"Cornw\nThe first result is much shorter and lacks context:\nDocument(metadata={'doc_id': '34645d23-ed05-4a53-b3af-c8ab21e3f51\npage_content='The **Cornwall Ranger** ticket allows unlimited tra\nThe result we have obtained with the ParentDocumentRetriever is\nparticularly useful when used as context for LLM synthesis, as it provides\nbroader details around the specific information.\nNext, I’ll introduce another technique that leverages embedding strategies to\nfurther optimize RAG retrieval accuracy.\n8.4.2 Embedding Child Chunks with MultiVectorRetriever\nAn alternative method for embedding child chunks and linking them to the\nlarger parent chunks used in synthesis is to use the MultiVectorRetriever.\nBegin by importing the necessary libraries, including InMemoryByteStore,\nwhich is specifically designed for storing binary data. In this store, keys are\nstrings and values are bytes, making it ideal for use cases involving\nembeddings, models, or files where raw byte storage is preferred or required:\nfrom langchain.retrievers.multi_vector import MultiVectorRetrieve\nfrom langchain.storage import InMemoryByteStore\nfrom langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.document_loaders import AsyncHtmlLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitt\nimport uuid\nSetting Up the MultiVectorRetriever\nYou can use a similar approach as with the ParentDocumentRetriever by\ndefining parent and child splitters and injecting them into\nMultiVectorRetriever, as shown in Listing 8.4.\nListing 8.4 Configuring Parent and Child Splitters for MultiVectorRetriever\n\n\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=3000) \nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=500)  \nchild_chunks_collection = Chroma(  #C\n   collection_name=\"uk_child_chunks\",\n   embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_\n)\nchild_chunks_collection.reset_collection()  #D\ndoc_byte_store = InMemoryByteStore()  #E\ndoc_key = \"doc_id\"\nmulti_vector_retriever = MultiVectorRetriever(  #F\n   vectorstore=child_chunks_collection,\n   byte_store=doc_byte_store\n)\nIngesting Content into Document and Vector Stores\nThe next step is to load and ingest content into the MultiVectorRetriever,\nas shown in listing 8.5. In this case, the document store is handled internally\nby the retriever, unlike the ParentDocumentRetriever, where you configured\nthe store separately. While ingestion may feel slower, this is expected due to\nthe added complexity of managing multiple vector representations.\nListing 8.5 Ingesting Content into Document and Vector Stores\nfor destination_url in uk_destination_urls:\n   html_loader = AsyncHtmlLoader(destination_url)  #A\n   html_docs =  html_loader.load()  #B\n   text_docs = html2text_transformer.transform_documents(html_do\n   coarse_chunks = parent_splitter.split_documents(text_docs)  #\n   coarse_chunks_ids = [str(uuid.uuid4()) for _ in coarse_chunks\n   all_granular_chunks = []\n   for i, coarse_chunk in enumerate(coarse_chunks):  #E\n       \n       coarse_chunk_id = coarse_chunks_ids[i]\n           \n       granular_chunks = child_splitter.split_documents([coarse_\n       for granular_chunk in granular_chunks:\n\n\n           granular_chunk.metadata[doc_key] = coarse_chunk_id  #\n       all_granular_chunks.extend(granular_chunks)\n   print(f'Ingesting {destination_url}')\n   multi_vector_retriever.vectorstore.add_documents(all_granular_\n   multi_vector_retriever.docstore.mset(list(zip(coarse_chunks_i\nPerforming a search on granular information\nNow, perform a search using MultiVectorRetriever, just like you did with\nthe ParentDocumentRetriever:\nretrieved_docs = multi_vector_retriever.invoke(\"Cornwall Ranger\")\nPrinting the first result shows that the retrieved document contains rich,\ndetailed information:\nDocument(metadata={'source': 'https://en.wikivoyage.org/wiki/Sout\npage_content=\"Trains from London take about 3 hr 20 min to Plymou\nComparing with Direct Semantic Search on Child Chunks\nFor comparison, run the same search directly on the child chunk collection:\nchild_docs_only = child_chunks_collection.similarity_search(\"Cornw\nThe first document retrieved from the child collection (child_docs_only[0])\nis more concise and lacks the broader context:\nDocument(metadata={'doc_id': '04c7f88e-e090-4057-af5b-ea584e777b3\npage_content='The **Cornwall Ranger** ticket allows unlimited tra\nThis result is similar to what you observed with the\nParentDocumentRetriever. The broader parent chunks provide more useful\ncontext for synthesis, making them a better fit for complex or detailed\nqueries.\nIn the next section, I’ll cover additional strategies for improving RAG\naccuracy using advanced embedding techniques.\n",
      "page_number": 249,
      "chapter_number": 29,
      "summary": "This chapter covers segment 29 (pages 249-256). Key topics include chunk, embedding, and importing.",
      "keywords": [
        "Child",
        "chunks",
        "Child Chunks",
        "Embedding Child Chunks",
        "coarse",
        "document",
        "parent",
        "Embedding Child",
        "Embedding",
        "smaller child chunks",
        "doc",
        "docs",
        "Coarse chunk",
        "Coarse chunk Child",
        "Child Splitters"
      ],
      "concepts": [
        "chunk",
        "embedding",
        "importing",
        "retrieval",
        "retrieving",
        "store",
        "stored",
        "listing",
        "document",
        "documents"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 26,
          "title": "Segment 26 (pages 518-535)",
          "relevance_score": 0.72,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 59,
          "title": "Segment 59 (pages 499-500)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 30,
          "title": "Segment 30 (pages 523-540)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 32,
          "title": "Segment 32 (pages 559-576)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 1,
          "title": "Segment 1 (pages 1-18)",
          "relevance_score": 0.68,
          "method": "api"
        }
      ]
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 257-264)",
      "start_page": 257,
      "end_page": 264,
      "detection_method": "topic_boundary",
      "content": "8.4.3 Embedding Document Summaries\nEmbeddings from coarse chunks are often ineffective because they capture\ntoo much irrelevant content. A large chunk may include filler text or minor\ndetails that dilute the semantic value of the embeddings, making them less\nfocused and less useful.\nTo address this, you can create a summary of the coarse chunk and generate\nembeddings from it. These summary embeddings are then stored alongside\nthe original chunk embeddings, as shown in figure 8.5. Because the summary\nis more concise and relevant, the resulting embeddings are denser and more\neffective for retrieval, reducing noise and improving search precision.\nFigure 8.5 Chunk Summary Embedding—A coarse chunk is indexed with its own embedding and\nan additional embedding from its summary. This allows for more accurate retrieval when\nanswering detailed questions.\n\n\nCoarse chunking strategy = =\n\nCoarse chunks\n\nCoarse chunk\n\nCoarse chunk summary\n\nCoarse chunk embedding\n\nSummary embedding\n\nA broad question is likely to hit\n\nthe embedding of a coarse\n\nchunk\nA detailed question is likely to hit\nthe embedding of the summary\nof a coarse chunk\n\nBroad question Detailed question\n\nTo start, import the required libraries for building the summarization and\nretrieval setup:\nfrom langchain.retrievers.multi_vector import MultiVectorRetrieve\nfrom langchain.storage import InMemoryByteStore\nfrom langchain_chroma import Chroma\nfrom langchain_openai import ChatOpenAI\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.document_loaders import AsyncHtmlLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitt\nfrom langchain_core.documents import Document\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nimport uuid \nSetting Up the MultiVectorRetriever\nFirst, create a collection to store the summaries and set up the document store\n(InMemoryByteStore). Then, configure the MultiVectorRetriever to use\nthese components, as shown in Listing 8.6.\nListing 8.6 Injecting summary collection and document store into MultiVectorRetriever\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=3000) \nsummaries_collection = Chroma(  #B\n   collection_name=\"uk_summaries\",\n   embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_\n)\nsummaries_collection.reset_collection()  #C\ndoc_byte_store = InMemoryByteStore()  #D\ndoc_key = \"doc_id\"\nmulti_vector_retriever = MultiVectorRetriever(  #E\n   vectorstore=summaries_collection,\n   byte_store=doc_byte_store\n)\nSetting Up the Summarization Chain\n\n\nUse an LLM to generate summaries of the coarse chunks. Define a\nsummarization chain that extracts the content, prompts the LLM, and parses\nthe response into a usable format:\nllm = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=OPENAI_API_K\nsummarization_chain = (\n   {\"document\": lambda x: x.page_content}  #A\n   | ChatPromptTemplate.from_template(\"Summarize the following d\n   | llm  #C\n   | StrOutputParser())  #D\nIngesting Coarse Chunks and Summaries into Stores\nNext, load the content, split it into coarse chunks, generate summaries, and\nstore both in the appropriate collections, as shown in listing 8.7.\nListing 8.7 Ingesting Coarse Chunks and Their Summaries\nfor destination_url in uk_destination_urls:\n   html_loader = AsyncHtmlLoader(destination_url)  #A\n   html_docs =  html_loader.load()  #B\n   text_docs = html2text_transformer.transform_documents(html_do\n   coarse_chunks = parent_splitter.split_documents(text_docs)  #\n   coarse_chunks_ids = [str(uuid.uuid4()) for _ in coarse_chunks\n   all_summaries = []\n   for i, coarse_chunk in enumerate(coarse_chunks):  #E\n       \n       coarse_chunk_id = coarse_chunks_ids[i]\n           \n       summary_text =  summarization_chain.invoke(coarse_chunk)  \n       summary_doc = Document(page_content=summary_text, metadat\n       all_summaries.append(summary_doc)  #G\n   print(f'Ingesting {destination_url}')\n   multi_vector_retriever.vectorstore.add_documents(all_summarie\n   multi_vector_retriever.docstore.mset(list(zip(coarse_chunks_i\nWhen running the code in listing 8.6, you may notice that processing is\nslower compared to using child embeddings. This slowdown is due to the\n\n\ntime required to submit each coarse chunk for summarization to the LLM,\nwhich is a more computationally intensive step.\nPerforming a Search Using the MultiVectorRetriever\nOnce the ingestion is complete, you can perform a search using the\nMultiVectorRetriever, which now utilizes the summaries for each travel\ndestination:\nretrieved_docs = multi_vector_retriever.invoke(\"Cornwall travel\")\nIf you print the first result (retrieved_docs_only[0]), you’ll see a large\nchunk similar to those retrieved when using child embeddings. These larger\nchunks provide more context, making them effective when passed as input to\nthe LLM.\nComparing with Direct Semantic Search on Summaries\nFor comparison, perform a direct search on the summaries alone:\nsummary_docs_only = summaries_collection.similarity_search(\"Cornw\nprint(summary_docs_only[0])\nThe first result from the summary search is concise and lacks broader\ncontext:\nDocument(metadata={'doc_id': 'ee55d250-bc53-46ce-9204-8fd2c1a0566\nThis result confirms a pattern observed earlier: directly searching against\nsummaries or child chunks retrieves focused but context-limited information,\nwhile using a multi-vector approach retrieves broader context that is more\nuseful for synthesis.\nLet’s explore one more advanced multi-vector embedding technique in the\nfollowing section.\n8.4.4 Embedding Hypothetical Questions\n\n\nWhen querying a vector store, your natural language question is converted\ninto a vector, and the system calculates its similarity (e.g., cosine distance) to\nthe stored vectors. The documents linked to the closest vectors are then\nreturned. This approach works well if the question is semantically similar to\nthe ideal answer. But often, the wording of the question and the phrasing of\nthe ideal answer may not match closely enough, causing the search to miss\nrelevant documents.\nTo address this, you can generate hypothetical questions that each chunk is\nlikely to answer, and then store the chunk using embeddings derived from\nthese questions, as shown in figure 8.6. This method increases the chances\nthat the stored vectors will align more closely with a user’s query, making it\nmore likely to retrieve relevant information, even if the original document’s\nembeddings aren’t a perfect match for the question.\nFigure 8.6 Hypothetical Question embeddings—A document chunk is indexed with its own\nembedding and additional embeddings generated from hypothetical questions that it can answer.\nThis allows for more accurate matching with user queries.\n\n\nCoarse chunking strategy\n\nsafes\n\nCoarse chunks\n\nHypothetical question #1 embedding\nHypothetical\nquestion #1 —__ A A\n\nHypothetical question #2 embedding\n\npOthetical question #3 embedding\n\nCoarse chunk\n\nCoarse chunk embedding\n\nHypothetical questions which\n\ncan be answered with the content / A detailed question is likely to hit\nA tion is likely to hit of the the chunk the embedding of an hypothetical question\nhaptoeipl . pe which can be answered by a coarse chunk\n\nchunk\n\nBroad question\n\nDetailed question\n\nTo begin, import all the required libraries to set up the\nMultiVectorRetriever with hypothetical question embeddings:\nfrom langchain.retrievers.multi_vector import MultiVectorRetrieve\nfrom langchain.storage import InMemoryByteStore\nfrom langchain_chroma import Chroma\nfrom langchain_openai import ChatOpenAI\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.document_loaders import AsyncHtmlLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitt\nfrom langchain_core.documents import Document\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nimport uuid\nfrom typing import List\nfrom pydantic import BaseModel, Field\nSetting Up the MultiVectorRetriever\nSet up the MultiVectorRetriever similarly to how it was configured for\nsummary embeddings, but this time use a vector store specifically for storing\nhypothetical questions, as shown in Listing 8.8.\nListing 8.8 Configuring MultiVectorRetriever with Hypothetical Question Collection\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=3000) \nhypothetical_questions_collection = Chroma(  #B\n   collection_name=\"uk_hypothetical_questions\",\n   embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_\n)\nhypothetical_questions_collection.reset_collection()  #C\ndoc_byte_store = InMemoryByteStore()  #D\ndoc_key = \"doc_id\"\nmulti_vector_retriever = MultiVectorRetriever(  #E\n   vectorstore=hypothetical_questions_collection,\n   byte_store=doc_byte_store\n)\n",
      "page_number": 257,
      "chapter_number": 30,
      "summary": "This chapter covers segment 30 (pages 257-264). Key topics include embedding, chunk, and document.",
      "keywords": [
        "coarse chunk",
        "coarse",
        "chunk",
        "Coarse chunk embedding",
        "question",
        "Hypothetical Question",
        "Embedding",
        "Hypothetical",
        "summary",
        "Summaries",
        "langchain",
        "Embedding Hypothetical Questions",
        "Hypothetical Question embeddings",
        "Ingesting Coarse Chunks"
      ],
      "concepts": [
        "embedding",
        "chunk",
        "document",
        "documents",
        "retrieval",
        "retrieved",
        "retrieves",
        "summaries",
        "summary",
        "questions"
      ],
      "similar_chapters": [
        {
          "book": "Effective_Modern_C++",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 59,
          "title": "Segment 59 (pages 499-500)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 20,
          "title": "Segment 20 (pages 343-360)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 30,
          "title": "Segment 30 (pages 523-540)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 19,
          "title": "Segment 19 (pages 157-164)",
          "relevance_score": 0.59,
          "method": "api"
        }
      ]
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 265-272)",
      "start_page": 265,
      "end_page": 272,
      "detection_method": "topic_boundary",
      "content": "Setting Up the Hypothetical Question Generation Chain\nCreate a chain to generate hypothetical questions for each document chunk.\nUse structured output from the LLM to ensure the generated questions are\nreturned as a list of strings:\nclass HypotheticalQuestions(BaseModel):\n   \"\"\"Generate hypothetical questions for given text.\"\"\"\n   questions: List[str] = Field(..., description=\"List of hypoth\nllm_with_structured_output = ChatOpenAI(model=\"gpt-4o-mini\", \n       openai_api_key=OPENAI_API_KEY).with_structured_output(\n       HypotheticalQuestions\n)\nYou can see the full question generation chain in listing 8.9.\nListing 8.9 Chain for Generating Hypothetical Questions from Text\nhypothetical_questions_chain = (\n   {\"document_text\": lambda x: x.page_content}  #A\n   | ChatPromptTemplate.from_template(  #B\n       \"Generate a list of exactly 4 hypothetical questions that \n   )\n   | llm_with_structured_output  #C\n   | (lambda x: x.questions)  #D\n)\nIngesting Coarse Chunks and Related Hypothetical Questions\nNow, generate coarse chunks, create the hypothetical questions for each, and\nstore them in the respective collections, as shown in listing 8.10.\nListing 8.10 Ingesting Coarse Chunks and Related Hypothetical Questions\nfor destination_url in uk_destination_urls:\n   html_loader = AsyncHtmlLoader(destination_url)  #A\n   html_docs =  html_loader.load()  #B\n   text_docs = html2text_transformer.transform_documents(html_do\n   coarse_chunks = parent_splitter.split_documents(text_docs)  #\n\n\n   coarse_chunks_ids = [str(uuid.uuid4()) for _ in coarse_chunks\n   all_hypothetical_questions = []\n   for i, coarse_chunk in enumerate(coarse_chunks):  #E\n       \n       coarse_chunk_id = coarse_chunks_ids[i]\n           \n       hypothetical_questions = hypothetical_questions_chain.inv\n       hypothetical_questions_docs = [Document(page_content=ques\n                                             for question in hyp\n       all_hypothetical_questions.extend(hypothetical_questions_\n   print(f'Ingesting {destination_url}')\n   multi_vector_retriever.vectorstore.add_documents(all_hypothet\n   multi_vector_retriever.docstore.mset(list(zip(coarse_chunks_i\nPerforming a Search Using the MultiVectorRetriever\nAfter ingestion, perform a search using the MultiVectorRetriever, which\nnow uses the stored hypothetical question embeddings:\nretrieved_docs = multi_vector_retriever.invoke(\"How can you go to \nThe first retrieved document (retrieved_docs[0]) is a detailed response\nwith rich contextual information:\n[Document(metadata={'source': 'https://en.wikivoyage.org/wiki/Bri\nComparing with Direct Search on Hypothetical Questions\nNow, run a search directly on the hypothetical questions collection for\ncomparison:\nhypothetical_question_docs_only = hypothetical_questions_collecti\nThe results show hypothetical questions closely matching the query:\n[Document(metadata={'doc_id': 'af848894-8591-4c28-8295-f3b833ffaa\nDocument(metadata={'doc_id': '7fa14e56-270c-4461-88ab-9b546afb07\nDocument(metadata={'doc_id': '7fa14e56-270c-4461-88ab-9b546afb07\nDocument(metadata={'doc_id': '7fa14e56-270c-4461-88ab-9b546afb07\n\n\nThis pattern is consistent with earlier techniques: using hypothetical question\nembeddings helps the search engine focus on the specific intent behind the\nquery, making it easier to retrieve relevant documents even when the exact\nwording varies.\nIn the next section, I’ll explore another technique to further improve\ndocument retrieval accuracy.\n8.5 Granular Chunk Expansion\nAs previously discussed, the main drawback of splitting a document into\nsmall granular chunks is that, while these chunks are effective for detailed\nquestions, they often lack the context needed to generate complete answers.\nOne way to address this is through chunk expansion, as shown in Figure 8.7.\nFigure 8.7 Sentence Expansion—A granular chunk can be enhanced by including the content\nfrom its preceding and following chunks to provide additional context.\n\n\nARBRE\n—— FRERIRE\n\nCoarse chunking strategy\n\nGranular chunks\n\nThe granular chunk associated with\nGranular chunks the embedding hit by the question\nembeddings is sent to the LLM within in the prompt LLM\n\nas a context with the previous and\n\nA detailed question its the subsequent granular chunk\n\nembedding of a granular chunk\n\nDetailed question\n\nThe idea is to store an expanded version of each chunk that includes content\nfrom the chunks immediately before and after it. This expanded version is\nstored in a separate document store. So, when the vector store retrieves a\nrelevant granular chunk, the linked expanded chunk is returned instead,\noffering a richer context for the LLM to produce a more complete answer.\nThis technique can be easily implemented using the MultiVectorRetriever.\nLet's look at how to set this up.\nSetting Up the MultiVectorRetriever for Chunk Expansion\nFirst, configure the MultiVectorRetriever by creating a collection to hold\ngranular chunks and an in-memory document store for the expanded chunks,\nas shown in listing 8.11:\nListing 8.11 Setting up a multi vector retriever for granular chunk expansion\nfrom langchain.retrievers.multi_vector import MultiVectorRetrieve\nfrom langchain.storage import InMemoryByteStore\nfrom langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.document_loaders import AsyncHtmlLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitt\nimport uuid\ngranular_chunk_splitter = RecursiveCharacterTextSplitter(chunk_si\ngranular_chunks_collection = Chroma(  #B\n   collection_name=\"uk_granular_chunks\",\n   embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_\n)\ngranular_chunks_collection.reset_collection()  #C\nexpanded_chunk_store = InMemoryByteStore()  #D\ndoc_key = \"doc_id\"\nmulti_vector_retriever = MultiVectorRetriever(  #E\n   vectorstore=granular_chunks_collection,\n   byte_store=expanded_chunk_store\n)\n\n\nIngesting Granular and Expanded Chunks\nNow, generate expanded chunks by including the content from adjacent\nchunks, as shown in listing 8.12.\nListing 8.12 Generating and Storing Expanded Chunks\nfor destination_url in uk_destination_urls:\n   html_loader = AsyncHtmlLoader(destination_url)  #A\n   html_docs =  html_loader.load()  #B\n   text_docs = html2text_transformer.transform_documents(html_do\n   granular_chunks = granular_chunk_splitter.split_documents(tex\n   expanded_chunk_store_items = []\n   for i, granular_chunk in enumerate(granular_chunks):  #E\n       this_chunk_num = i  #F\n       previous_chunk_num = i-1  #F\n       next_chunk_num = i+1  #F\n       \n       if i==0:  #F\n           previous_chunk_num = None\n       elif i==(len(granular_chunks)-1):  #F\n           next_chunk_num = None\n       expanded_chunk_text = \"\"  #G\n       if previous_chunk_num:  #G\n           expanded_chunk_text += granular_chunks[previous_chunk_\n           expanded_chunk_text += \"\\n\"\n       expanded_chunk_text += granular_chunks[this_chunk_num].pa\n       expanded_chunk_text += \"\\n\"\n       if next_chunk_num:  #G\n           expanded_chunk_text += granular_chunks[next_chunk_num\n           expanded_chunk_text += \"\\n\"\n       expanded_chunk_id = str(uuid.uuid4())  #H\n       expanded_chunk_doc = Document(page_content=expanded_chunk_\n       expanded_chunk_store_item = (expanded_chunk_id, expanded_\n       expanded_chunk_store_items.append(expanded_chunk_store_it\n       granular_chunk.metadata[doc_key] = expanded_chunk_id  #J\n\n\n           \n   print(f'Ingesting {destination_url}')\n   multi_vector_retriever.vectorstore.add_documents(granular_chu\n   multi_vector_retriever.docstore.mset(expanded_chunk_store_ite\nPerforming a Search Using the MultiVectorRetriever\nAfter the ingestion step, run a search using the MultiVectorRetriever,\nwhich now uses expanded chunks for a more complete context:\nretrieved_docs = multi_vector_retriever.invoke(\"Cornwall Ranger\")\nThe first document retrieved will include content from surrounding chunks,\ngiving the LLM more context to generate a richer response:\nDocument(page_content=\"Buses only serve designated stops when in \nComparing with Direct Semantic Search on Granular Chunks\nFor comparison, run a search directly on the granular chunks without\nexpansion:\nchild_docs_only = granular_chunks_collection.similarity_search(\"C\nThe result will likely be more concise and lack the broader context:\nDocument(metadata={'doc_id': '04c7f88e-e090-4057-af5b-ea584e777b3\nThis smaller chunk lacks the surrounding details and may not provide enough\ncontext to synthesize a complete answer.\nChunk expansion offers a way to improve the effectiveness of granular\nembeddings by attaching broader context. The next section will cover how to\nefficiently handle mixed structured and unstructured content.\n8.6 Semi-Structured Content\nWhen dealing with documents that mix unstructured text and structured data\n(e.g., tables), it’s essential to handle each type separately. You should extract\n\n\nstructured content, like tables, and generate embeddings for their summaries\n—just as you would for text chunks, as discussed in the Embedding\nDocument Summaries section.\nStore the coarse text chunks and the full tables in a document store and place\nthe embeddings for the summaries of both text and tables in the vector store\nusing a MultiVectorRetriever, as shown in figure 8.8. This setup allows\nseamless retrieval of both structured and unstructured content.\nFigure 8.8 Embedding Structured and Unstructured Content—Structured data, such as tables,\nshould be summarized and embedded just like unstructured text chunks. This ensures that the\nembeddings match detailed questions as effectively as text embeddings. Use the\nMultiVectorRetriever to manage both types of content.\n",
      "page_number": 265,
      "chapter_number": 31,
      "summary": "This chapter covers segment 31 (pages 265-272). Key topics include chunk, document, and documents.",
      "keywords": [
        "hypothetical questions",
        "chunk",
        "Hypothetical",
        "Granular",
        "questions",
        "Related Hypothetical Questions",
        "expanded",
        "Question Generation Chain",
        "generate hypothetical questions",
        "Granular Chunk",
        "hypothetical question embeddings",
        "document",
        "Hypothetical Question Generation",
        "Text",
        "Coarse"
      ],
      "concepts": [
        "chunk",
        "document",
        "documents",
        "embeddings",
        "granular",
        "context",
        "structured",
        "generation",
        "generate",
        "generated"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 61-81)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 21,
          "title": "Segment 21 (pages 189-197)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 59,
          "title": "Segment 59 (pages 499-500)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "C++ Templates_ The Complete Guide",
          "chapter": 34,
          "title": "Segment 34 (pages 1080-1114)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.56,
          "method": "api"
        }
      ]
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 273-281)",
      "start_page": 273,
      "end_page": 281,
      "detection_method": "topic_boundary",
      "content": "Coarse chunking strategy\n\nText coarse chunks ‘Table coarse chunks\n\nText coarse chunk\nCoarse chunk embedding\n\nta)\n\nText chunk summary\n\nSummary embedding\n\nSummary embedding\n\nTable coarse chunk\n\nTable chunk summary\n\nA broad question is likely to hit\n\nthe embedding of a coarse\n\nchunk\nA detailed question might hit\nthe embedding of the summary\nof a table chunk\n\nBroad question Detailed question\n\nWhen a search hits the embedding of a table’s summary, the entire table\n(stored in the document store) is returned to the LLM for synthesis, providing\nthe necessary context to generate a complete response.\nFor a practical implementation of this technique, refer to\nhttps://github.com/langchain-\nai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb?\nref=blog.langchain.dev.\n8.7 Multi-Modal RAG\nYou’ve likely come across the term “multi-modal” LLMs. Models like GPT-\n4V extend traditional LLMs to handle not just text, but also images and\naudio. This opens the door for extending the RAG architecture to support\nmulti-modal data.\nThe approach is similar to handling semi-structured content. During the data\npreparation stage, you can use a multi-modal LLM to generate a summary of\nan image, just as you would for a table. Then, create embeddings for the\nimage summary and link these embeddings to the raw image stored in a\ndocument store, as shown in Figure 8.9.\nFigure 8.9 Multi-Modal RAG Workflow—1) Data Ingestion: Use a multi-modal LLM to generate\nan image summary, store the summary embeddings in a vector store, and keep the raw image in\na document store. 2) Retrieval: If the summary embeddings match a query, the raw image is\nreturned by the MultiVectorRetriever and fed into the LLM for synthesis.\n\n\nDuring retrieval, if the summary’s embeddings match a user query, the\nMultiVectorRetriever returns the raw image—just as it would return a table\nfor semi-structured text. The image is then passed to the multi-modal LLM\n\n\nalong with its summary, providing a rich context for generating a response.\nNote\nThis book does not cover multi-modal RAG in detail, as it is an advanced\ntopic that would require in-depth explanations beyond the intended scope.\nHowever, with what you’ve learned so far, you should have the foundation to\nexplore it on your own. For more information, I recommend the article\n“Multimodality and Large Multimodal Models (LMMs)” by Chip Huyen:\nhttps://huyenchip.com/2023/10/10/multimodal.html.\n8.8 Summary\nA basic RAG implementation often produces low-quality results,\nmaking it unsuitable for real-world applications.\nYou can improve answer quality using techniques like advanced\nindexing, multiple query generation, query transformation, content-\nspecific queries, retrieval post-processing (e.g., result reranking), and\nensemble methods.\nAdvanced indexing may include refined document splitting strategies,\nmultiple embeddings for coarse chunks, and expanded context for\ngranular chunks.\nChoose a splitting strategy based on content type: by size (e.g.,\nparagraphs, sentences, words, or characters) or by structure (e.g.,\nchapters or sections).\nStructured formats like HTML or Markdown benefit from splitting\nbased on document hierarchy.\nFor coarse chunks, enhance retrieval accuracy by using multiple\nembeddings generated from child chunks, summaries, or hypothetical\nquestions about the content.\nEmbed child chunks using the ParentDocumentRetriever. Use the\nMultiVectorRetriever to embed chunk summaries, hypothetical\nquestions, or child chunks.\nSince it’s hard to predict which multi-vector retrieval technique will\nperform best, experiment with each and compare results to find the\noptimal approach.\nFor granular chunks, expand retrieval context by including adjacent\n\n\nchunks for a fuller context.\nFor semi-structured content (e.g., text with tables) or multi-modal\ncontent (e.g., images and audio), use specialized indexing techniques for\noptimal results.\n\n\n9 Question transformations\nThis chapter covers\nRewrite user questions with \"Rewrite-Retrieve-Read\" for better\nembedding alignment.\nUse \"step-back\" queries to retrieve higher-level context.\nGenerate hypothetical documents to align questions with embeddings.\nDecompose complex queries into single or multi-step sequences.\nIn some cases, you might spend a lot of time preparing RAG data—collecting\ndocuments, splitting them into chunks, and generating embeddings for\nsynthesis and retrieval (as covered earlier). Yet, you may still see low-quality\nresults from the vector store. This problem might not come from missing\nrelevant content in the vector store, but from issues in the user's question\nitself.\nFor instance, the question might be poorly phrased, unclear, or overly\ncomplex. Questions that aren’t clearly and simply stated can confuse both the\nvector store and the LLM, leading to weaker retrieval results. In this section,\nI'll show you techniques to refine the user's question, making it easier for the\nquery engine and the LLM to understand. By improving the question, you’ll\nlikely see better retrieval performance, providing more relevant context for\nthe LLM to deliver a solid answer.\nLet's begin with a straightforward method: using the LLM to help rephrase\nthe question.\n9.1 Rewrite-Retrieve-Read\nTo improve a poorly worded question, one effective method is to have an\nLLM rewrite it into a clearer form. This approach, covered in the paper\n“Query Rewriting for Retrieval-Augmented Large Language Models” by\nXinbei Ma et al. (https://arxiv.org/pdf/2305.14283.pdf), inspired the diagram\n\n\nin figure 9.1.\nFigure 9.1 In the standard Retrieve-and-Read setup, the retriever processes the user question\ndirectly, delivering results to the LLM for synthesis. In the “Rewrite-Retrieve-Read” approach,\nan initial Rewrite step uses an LLM to rephrase the query before it reaches the retriever,\nenhancing the clarity of the retrieval process (diagram adapted from the paper “Query Rewriting\nfor Retrieval-Augmented Large Language Models”, https://arxiv.org/pdf/2305.14283.pdf).\n\n\nDocuments\n\nRetrieve-and-Read\n\nRewrite-Retrieve-Read\n\nIn the standard “Retrieve-and-Read” workflow, the retriever processes the\noriginal question directly and sends results to the LLM for synthesis. By\nadding a Rewrite step upfront, a rewriter (often an LLM) reformulates the\nquestion before passing it to the retriever. This improved workflow is known\nas “Rewrite-Retrieve-Read.”\nI usually apply this technique to create a tailored search query for the vector\nstore, keeping the original question in the RAG prompt. This approach allows\nthe rewritten query to optimize retrieval, particularly for semantic searches in\nthe vector database, while preserving the original question for the LLM to\nsynthesize the answer. See the workflow in figure 9.2, which is an amended\nversion of the RAG diagram you saw in figure 5.2.\nFigure 9.2 Using query rewriting to generate an optimized vector store query, while preserving\nthe original question for answer synthesis via the chatbot prompt.\n",
      "page_number": 273,
      "chapter_number": 32,
      "summary": "This chapter covers segment 32 (pages 273-281). Key topics include retrieval, retrieve, and retriever. This opens the door for extending the RAG architecture to support\nmulti-modal data.",
      "keywords": [
        "LLM",
        "question",
        "summary",
        "Summary embedding Table",
        "Table chunk summary",
        "multi-modal LLM",
        "table chunk Broad",
        "RAG",
        "chunks",
        "query",
        "embedding Table coarse",
        "embeddings",
        "Multi-Modal RAG",
        "Table coarse chunks"
      ],
      "concepts": [
        "retrieval",
        "retrieve",
        "retriever",
        "question",
        "questions",
        "chunks",
        "query",
        "queries",
        "rag",
        "likely"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 40,
          "title": "Segment 40 (pages 367-374)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 15,
          "title": "Segment 15 (pages 133-140)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 38,
          "title": "Segment 38 (pages 344-356)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 22,
          "title": "Segment 22 (pages 198-207)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 25,
          "title": "Segment 25 (pages 495-517)",
          "relevance_score": 0.61,
          "method": "api"
        }
      ]
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 282-289)",
      "start_page": 282,
      "end_page": 289,
      "detection_method": "topic_boundary",
      "content": "1b. The user's question is also included in the\nchatbot prompt sent to the LLM for answer\n\nuser question\n\nPTIrtiiiitiiiiii titi iit tt\n\n1a. The user's question is\n\nSSSSSSSSSSSER SESE EERE RED\n\nPrTiiiiiiiy) tiiitiii ie\n\npassed to a rewriter,\nUser which uses an LLM to\ngenerate a revised query\noptimized for vector store\nsearch\n2.The reever transforms retrieved\nthe user question into the chunks\ncoresponding embedding\n3.The retriever searches\nforrelevanttext usingthe qs\"; }4j7\nquestion embedding 5 The chatbot creates a\nprompt with the user\nquestion and the retrieved\nchunks (the context)\n4,The vector DB retums\ntext chunks relevant to 6.The LLIM accepts the\nthe user question prompt and synthetizes a\ncorresponding\ncompletion (the\nresponse)\n\nA prompt for rewriting the query can be as simple as the one below, adapted\nfrom a popular prompt in the Langchain Hub:\nRevise the original question to make it more refined and precise \nOriginal question: {user_question}  \nRevised Chroma DB query:\nTo apply the Rewrite-Retrieve-Read technique and use the prompt above to\nrewrite the original user question, open a new OS shell, navigate to the\nChapter 9 code folder, and set up your environment.:\nC:\\Github\\building-llm-applications\\ch09>\nC:\\Github\\building-llm-applications\\ch09>python -m venv env_ch09\nC:\\Github\\building-llm-applications\\ch09>pip install notebook lan\nC:\\Github\\building-llm-applications\\ch09>env_ch09\\Scripts\\activat\n(env_ch09) C:\\Github\\building-llm-applications\\ch09>jupyter noteb\nAfter launching Jupyter Notebook, create a new notebook named 09-\nquestion_transformations.ipynb. Then, re-import the UK tourist\ndestination content from Wikivoyage as in the previous chapter. For\nconvenience, I’ve included the code below.\nListing 9.1 Splitting and Ingesting Content from URLs\nfrom langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import HTMLSectionSplitter\nfrom langchain_community.document_loaders import AsyncHtmlLoader\nimport getpass\nOPENAI_API_KEY = getpass.getpass('Enter your OPENAI_API_KEY')\nuk_granular_collection = Chroma(\n   collection_name=\"uk_granular\",\n   embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_\n)\nuk_granular_collection.reset_collection()  #A\nuk_destinations = [\n   \"Cornwall\", \"North_Cornwall\", \"South_Cornwall\", \"West_Cornwal\n   \"Tintagel\", \"Bodmin\", \"Wadebridge\", \"Penzance\", \"Newquay\",\n\n\n   \"St_Ives\", \"Port_Isaac\", \"Looe\", \"Polperro\", \"Porthleven\",\n   \"East_Sussex\", \"Brighton\", \"Battle\", \"Hastings_(England)\", \n   \"Rye_(England)\", \"Seaford\", \"Ashdown_Forest\"\n]\nwikivoyage_root_url = \"https://en.wikivoyage.org/wiki\"\nuk_destination_urls = [f'{wikivoyage_root_url}/{d}' for d in uk_d\nheaders_to_split_on = [(\"h1\", \"Header 1\"),(\"h2\", \"Header 2\")]\nhtml_section_splitter = HTMLSectionSplitter(headers_to_split_on=h\ndef split_docs_into_granular_chunks(docs):\n   all_chunks = []\n   for doc in docs:\n       html_string = doc.page_content  #B\n       temp_chunks = html_section_splitter.split_text(html_strin\n       h2_temp_chunks = [chunk for chunk in temp_chunks if \"Head\n       all_chunks.extend(h2_temp_chunks) \n   return all_chunks\nfor destination_url in uk_destination_urls:\n   html_loader = AsyncHtmlLoader(destination_url)  #E\n   docs =  html_loader.load()  #F\n   \n   for doc in docs:\n       print(doc.metadata)\n       granular_chunks = split_docs_into_granular_chunks(docs)\n       uk_granular_collection.add_documents(documents=granular_c\nThis setup prepares your data for effective query rewriting and retrieval using\nthe Rewrite-Retrieve-Read workflow. The Rewrite step will help craft refined\nsearch queries, improving retrieval results and the overall quality of the\nresponses generated.\n9.1.1 Retrieving Content Using the Original User Question\nLet’s start by performing a search with the original user question:\nuser_question = \"Tell me some fun things I can enjoy in Cornwall\"\ninitial_results = uk_granular_collection.similarity_search(query=\nfor doc in initial_results:\n   print(doc)\n\n\nThe output will look something like this (I have reduced it in various places):\npage_content='Do \n[ edit ] \nCornwall, in particular Newquay, is the UK's  surfing  capital, w\nThe  South West Coast Path  runs [REDUCED…] \nThe  Camel Trail  is an  18-mile (29   km)  [REDUCED…]\nThe  Cornish Film Festival  is held annually [REDUCED…]\nThe  Royal Cornwall Show  is an agricultural show [REDUCED…]  \nCamel Creek Adventure Park , Tredinnick,  Wadebridge  offers grea\nFestivals \n[ edit ] \nMummer's Day , or \"Darkie Day\" as it is sometimes known [REDUCED\n' Obby 'Oss  is held annually on May Day (1 May), mainly in  Pad\nSt Piran's Day  (Cornish:  Gool Peran ) is the national day of C\npage_content='Do \n[ edit ] \nThe  South West Coast Path  runs [REDUCED…]  \nThe  Camel Trail  is an  18-mile (29   km)  off-road cycle-track \nThe  Cornish Film Festival  is held annually [REDUCED…]  \nCornwall, in particular Newquay, is the UK's  surfing  capital, [\nCricket:   Cornwall CCC  play in the National Counties [REDUCED…\npage_content='Buy \n[ edit ] \nIn the village centre you will find the usual [REDUCED…] ' metad\npage_content='Do \n[ edit ] \nThe  South West Coast Path  runs along [REDUCED…]  \nSt Piran's Day  (Cornish:  Gool Peran ) is the national day of C\nThis output provides some information on fun activities, but it appears\nsomehow limited. This might be due to the way the question has been\nworded.\n9.1.2 Setting Up the Query Rewriter Chain\nTo refine the user question into a more effective query for Chroma DB, we’ll\nuse the LLM to rewrite it into a format better suited for semantic search.\n\n\nSetting up a query rewriter chain will help us automate this transformation.\nStart by importing the necessary libraries:\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nllm = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=OPENAI_API_K\nNext, define the prompt that instructs the LLM to rewrite the user question:\nrewriter_prompt_template = \"\"\"\nGenerate search query for the Chroma DB vector store from a user \nJust return the revised Chroma DB query, with quotes around it. \nUser question: {user_question}\nRevised Chroma DB query:\n\"\"\"\nrewriter_prompt = ChatPromptTemplate.from_template(rewriter_promp\nNow, construct the chain to execute the rewriting process:\nrewriter_chain = rewriter_prompt | llm | StrOutputParser()\nThis setup allows you to pass a user question to the rewriter chain, which\ngenerates a tailored query optimized for Chroma DB, enhancing retrieval\naccuracy.\n9.1.3 Retrieving Content with the Rewritten Query\nNow, let’s use the rewriter chain to create a more targeted query and see if it\nreturns more accurate results compared to the original question.\nFirst, generate the rewritten query:\nuser_question =\"Tell me some fun things I can do in Cornwall\"\nsearch_query = rewriter_chain.invoke({\"user_question\": user_quest\nprint(search_query)\nIf you print search_query, you should see something like:\n\n\n\"fun activities to do in Cornwall\"\nNow, use this refined query to perform the vector store search:\nimproved_results = uk_granular_collection.similarity_search(query\nFinally, print the results to review their relevance:\nfor doc in improved_results:\n   print(doc)\nYou will get the following output, which I have reduced to save space:\npage_content='Do \n[ edit ] \nCornwall, in particular Newquay, is the UK's  surfing  capital, \nThe  South West Coast Path  runs along the coastline  [REDUCED…]\nThe Cornish section is supposed to be the most  [REDUCED…]\nThe  Camel Trail  is an  18-mile (29   km)  off-road cycle-track \nThe  Cornish Film Festival  is held annually each November aroun\nThe  Royal Cornwall Show  is an agricultural show  [REDUCED…] \nCamel Creek Adventure Park , Tredinnick,  Wadebridge  offers  [RE\nFestivals \n[ edit ] \nMummer's Day , or \"Darkie Day\" as it is sometimes  [REDUCED…]\n' Obby 'Oss  is held annually on May Day (1 May), [REDUCED…]\nSt Piran's Day  (Cornish:  Gool Peran ) is the national day  [RE\npage_content='Do \n[ edit ] \nThe  South West Coast Path  runs  [REDUCED…]\nThe  Camel Trail  is an  18-mile (29   km)  off-road cycle- [RED\nThe  Cornish Film Festival  is held annually each November aroun\nCornwall, in particular Newquay, is the UK's  surfing  capital, \nCricket:   Cornwall CCC  play in the National Counties Cricket  \npage_content='Do \n[ edit ] \nHelford River  is an idyllic river estuary between Falmouth and \nThe  South West Coast Path  runs along the coastline  [REDUCED…]\nFestivals \n[ edit ] \nAllantide  (Cornish:  Kalan Gwav  or  Nos Kalan Gwav ) ia a Corn\n\n\nFurry Dance , also known as Flora Day, takes place in   [REDUCED\nGolowan , sometimes also  Goluan  or  Gol-Jowan , is the Cornish \nGuldize  is an ancient harvest festival in Autumn, [REDUCED…]\nMontol Festival  is an annual heritage, arts and community  [RED\nNickanan Night  is traditionally held on the Monday before Lent. \nSt Piran's Day  (Cornish:  Gool Peran ) is the national day of C\nThis approach should yield results that align more closely with your original\nintent, displaying a broader selection of content on activities available in\nCornwall. Using the rewritten question, the vector store retriever provides a\nmore diverse set of text chunks compared to the results from the original\nquestion.\n9.1.4 Combining Everything into a Single RAG Chain\nNow, you can build a complete workflow that transforms the initial user\nquestion into a search query for vector retrieval. The original question is\nretained in the prompt to generate the final answer. Listing 9.2 shows the full\nRAG chain, including the query rewriting step.\nListing 9.2 Combined RAG Chain with Query Rewriting\nfrom langchain_core.runnables import RunnablePassthrough\nretriever = uk_granular_collection.as_retriever()\nrag_prompt_template = \"\"\"\nGiven a question and some context, answer the question.\nIf you do not know the answer, just say I do not know.\nContext: {context}\nQuestion: {question}\n\"\"\"\nrag_prompt = ChatPromptTemplate.from_template(rag_prompt_template\nrewrite_retrieve_read_rag_chain = (\n   {\n       \"context\": {\"user_question\": RunnablePassthrough()} | rew\n       \"question\": RunnablePassthrough(),  #B\n   }\n   | rag_prompt\n   | llm\n   | StrOutputParser()\n\n\n)\nNow, run the complete workflow:\nuser_question = \"Tell me some fun things I can do in Cornwall\"\nanswer = rewrite_retrieve_read_rag_chain.invoke(user_question)\nprint(answer) \nWhen you print the answer, you should see a response like this:\nIn Cornwall, you can enjoy a variety of fun activities such as:\n1. Surfing in Newquay, known as the UK's surfing capital, where y\n2. Walking along the scenic South West Coast Path, which offers b\n3. Cycling on the Camel Trail, an 18-mile off-road cycle track th\n4. Attending the Cornish Film Festival held annually in November \n5. Exploring the Helford River, where you can take a ferry ride, \n6. Participating in local festivals such as St Piran's Day or the \n7. Visiting Camel Creek Adventure Park for family-friendly entert\n8. Enjoying various agricultural shows, like the Royal Cornwall S\nThere are plenty of options for both adventure and relaxation in \nThis output demonstrates a satisfying answer based on the combined\nRewrite-Retrieve-Read workflow.\nNext, I’ll show you further ways to refine the “Rewrite-Retrieve-Read”\nprocess.\n9.2 Generating Multiple Queries\nThe “Rewrite-Retrieve-Read” approach assumes the original user question\nwas poorly phrased. But if the question is well-formed and contains multiple\nimplicit questions, rewriting it as a single improved query may not be\neffective. In these cases, it’s better to have the LLM break down the original\nquestion into multiple explicit questions. Each question can be executed\nseparately against the vector store, with the answers then synthesized into a\ncomprehensive response. Figure 9.3 illustrates this workflow.\nFigure 9.3 Workflow for Multiple Query Generation: The LLM application reformulates the\noriginal question into multiple explicit questions, which are executed against the vector store to\nproduce the context, and synthesizes the answer from a prompt with the original question and the\n",
      "page_number": 282,
      "chapter_number": 33,
      "summary": "This chapter covers segment 33 (pages 282-289). Key topics include reduced, question, and questions. For\nconvenience, I’ve included the code below.",
      "keywords": [
        "South West Coast",
        "West Coast Path",
        "user question",
        "question",
        "Cornish Film Festival",
        "original user question",
        "reduced",
        "South West",
        "user",
        "Cornwall",
        "Coast Path runs",
        "West Coast",
        "query",
        "original question",
        "Coast Path"
      ],
      "concepts": [
        "reduced",
        "question",
        "questions",
        "retrieved",
        "retrieval",
        "query",
        "queries",
        "importing",
        "cornwall",
        "chunks"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 6",
          "chapter": 30,
          "title": "Segment 30 (pages 523-540)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 32,
          "title": "Segment 32 (pages 559-576)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 33,
          "title": "Segment 33 (pages 577-594)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 34,
          "title": "Segment 34 (pages 595-602)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 20,
          "title": "Segment 20 (pages 343-360)",
          "relevance_score": 0.62,
          "method": "api"
        }
      ]
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 290-300)",
      "start_page": 290,
      "end_page": 300,
      "detection_method": "topic_boundary",
      "content": "produced context.\n\n\nIs LAwarmer than Miami\n\nin Apri?\n\nls LAwarmer than Miami\n\nUser generate 3 queries from the original question\n\nQuestion: Is LA warmer than Miami in April?\nContext: C1 +02 +03\n\nIn the figure 9.3, the LLM application reformulates the original question into\nmultiple explicit questions, executes them against the vector store to gather\ncontext, and then synthesizes the answer using a prompt that includes both\nthe original question and the retrieved context.\nFor example, if you prompt ChatGPT with the following:\nReformulate the following question into multiple explicit questio\nIs LA warmer than Miami in April?\nYou might receive the following questions, which can be executed\nindividually against the vector store:\nWhat is the average temperature in Los Angeles during April?\nWhat is the average temperature in Miami during April?\nCan you compare the April temperatures in Los Angeles and Miami a\nIs there a notable difference in temperature between Los Angeles \nCould you provide insights into how the temperatures in Los Angel\nThis approach is especially useful when designing generic LLM applications.\nYou can automatically generate multiple queries for any user question using a\nprompt like this, adapted from a LangChain example:\nQUERY_PROMPT = PromptTemplate(\n   input_variables=[\"question\"],\n   template=\"\"\"You are an AI language model assistant. Your task \n   different versions of the given user question to retrieve rel\n   database. By generating multiple perspectives on the user que\n   the user overcome some of the limitations of the distance-bas\n   Provide these alternative questions separated by newlines.\n   Original question: {question}\"\"\",\n)\nLangChain’s MultiQueryRetriever class allows you to use a prompt, an\nLLM reference, and a retriever (e.g., derived from a vector database). When a\nMultiQueryRetriever instance processes a user query, it executes the entire\nmulti-query workflow automatically, as shown in Figure 7.10. This approach\ncombines multi-retrieval with answer synthesis.\nNext, I’ll show you how to implement a custom MultiQueryRetriever.\n\n\n9.2.1 Setting Up the Chain for Generating Multiple Queries\nFirst, import the necessary libraries:\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom typing import List\nfrom langchain_core.output_parsers import BaseOutputParser\nfrom pydantic import BaseModel, Field\nBegin by setting up the prompt shown earlier to instruct the LLM to generate\nmultiple variations of a single user question:\nmulti_query_gen_prompt_template = \"\"\"\nYou are an AI language model assistant. Your task is to generate \ndifferent versions of the given user question to retrieve relevan\ndatabase. By generating multiple perspectives on the user questio\nthe user overcome some of the limitations of the distance-based s\nProvide these alternative questions separated by newlines.\nOriginal question: {question}\n\"\"\"\nmulti_query_gen_prompt = ChatPromptTemplate.from_template(multi_q\nSince the LLM is generating five alternative questions, it’s useful to format\nthe output as a list of strings, with each string representing one question. This\nlets you process each question independently. To do this, implement a custom\nresult parser instead of the standard StrOutputParser:\nclass LineListOutputParser(BaseOutputParser[List[str]]):\n   \"\"\"Parse out a question from each output line.\"\"\"\n   def parse(self, text: str) -> List[str]:\n       lines = text.strip().split(\"\\n\")\n       return list(filter(None, lines))  \nquestions_parser = LineListOutputParser() \nWith these components, you can set up the chain to generate multiple queries:\nllm = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=OPENAI_API_K\nmulti_query_gen_chain = multi_query_gen_prompt | llm | questions_\n\n\nNow, try running this chain:\nuser_question = \" Tell me some fun things I can do in Cornwall.\"\nmultiple_queries = multi_query_gen_chain.invoke(user_question)\nWhen you print multiple_queries, you should get a list of questions similar\nto this:\n['What are some enjoyable activities to explore in Cornwall?  ',\n'Can you suggest interesting attractions or events in Cornwall f\n'What are the top leisure activities to try out while visiting C\n'What fun experiences or adventures does Cornwall have to offer? \n'Could you recommend some entertaining things to do while in Cor\nOne final step remains to complete the setup.\n9.2.2 Setting Up a Custom Multi-Query Retriever\nTo configure a custom multi-query retriever, start by creating a standard\nretriever and then embedding it within the multi-query retriever, as shown\nbelow:\nbasic_retriever = uk_granular_collection.as_retriever()\nmulti_query_retriever = MultiQueryRetriever(\n   retriever=basic_retriever, llm_chain=multi_query_gen_chain, \n   parser_key=\"lines\"  #A\n)  \nNow, test it with an example query:\nuser_question = \"Tell me some fun things I can do in Cornwall\"\nretrieved_docs = multi_query_retriever.invoke(user_question)\nWhen you print retrieved_docs, you should see output similar to this:\n[Document(metadata={'Header 2': 'Do'}, page_content='Do \\n [ edit \nDocument(metadata={'Header 2': 'Do'}, page_content='Do \\n [ edit \nDocument(metadata={'Header 2': 'Contents'}, page_content='Conten\nDocument(metadata={'Header 2': 'Do'}, page_content=\"Do \\n [ edit \nDocument(metadata={'Header 2': 'Festivals'}, page_content='Festi\n\n\nDocument(metadata={'Header 2': 'Contents'}, page_content='Conten\nDocument(metadata={'Header 2': 'See'}, page_content=\"See \\n [ ed\n9.2.3 Using a Standard MultiQueryRetriever Instance\nFor straightforward use cases, you can set up multi-query generation with a\nstandard MultiQueryRetriever instance. First, instantiate the multi-query\nretriever:\nstd_multi_query_retriever = MultiQueryRetriever.from_llm(\n   retriever=basic_retriever, llm=llm\n)\nNow, test it with the same question:\nuser_question = \" Tell me some fun things I can do in Cornwall\"\nretrieved_docs = multi_query_retriever.invoke(user_question)\nThe output in retrieved_docs will be similar to the results you saw\npreviously.\nIn some cases, rewriting the original question or breaking it into multiple\nexplicit questions may not yield the desired results. Continue reading to\nexplore a technique that can improve accuracy in these situations.\n9.3 Step-Back Question\nWhen you send a highly detailed question directly to the vector store—\nassuming your documents are split into small, specific chunks—you might\nretrieve information that’s too focused, missing broader context. This can\nlimit the LLM’s ability to generate a comprehensive answer.\nAs discussed in section 7.4, one solution is to create two sets of document\nchunks: coarse chunks for synthesis and fine-grained chunks for detailed\nretrieval. Another solution is to adjust the user question rather than the\ndocument chunks, using an approach called a “step-back question.”\nIn this approach, you start with the user’s detailed question but then create a\n\n\nbroader question to retrieve a more generalized context. This “step-back\ncontext” provides a higher-level view than the “original context” derived\nfrom the specific question. You then provide the LLM with both the detailed\ncontext and the broader context to enable a fuller response, as illustrated in\nfigure 9.4.\nFigure 9.4 Step-Back Question Workflow: The LLM application first sends the detailed question\n(Q_D) to the vector store to retrieve a detailed context (C_D). It then prompts the LLM to\ngenerate a more abstract question (Q_A) based on Q_D, which is also executed on the vector\nstore to obtain an abstract context (C_A). Finally, the LLM application combines Q_D, C_D, and\nC_A into a single prompt, enabling the LLM to synthesize a comprehensive answer.\n\n\nUser ooo\n\n|\nQuestion: Detailed question Q_D\nContext C_D+C_A\n\nThe LLM application sends the original detailed question to the vector store\nto retrieve detailed context, then generates and executes a broader question to\nobtain abstract context. It combines both contexts with the original question\nto enable the LLM to create a comprehensive answer.\nThis technique, developed by Huaixiu Steven Zheng et al., is explained in\ntheir paper “Take a Step Back: Evoking Reasoning via Abstraction in Large\nLanguage Models” (https://arxiv.org/pdf/2310.06117.pdf?\nref=blog.langchain.dev).\nTo implement this, use a prompt like the following to generate the step-back\nquestion:\nGenerate a less specific question (aka Step-back question) for th\nDetailed question: {detailed_question}\nStep-back question:\nFor example, if you input the prompt with the detailed question, “Can you\ngive me some tips for a trip to Brighton?” the more abstract (step-back)\nquestion might look like:\nStep-back question: \"What should I know before visiting a popular \nThis broader question helps retrieve more general information, which,\ncombined with the detailed context, allows the LLM to produce a well-\nrounded answer.\n9.3.1 Setting Up the Chain to Generate a Step-Back Question\nImplementing the step-back question technique is straightforward: it involves\ncrafting an effective prompt to generate a broader question and then\nfollowing a standard RAG workflow. Here’s a sample implementation, which\nclosely resembles the pattern used for the rewrite-retrieve-read technique.\nStart by setting up the prompt in your Jupyter notebook:\nllm = ChatOpenAI(model=\"4o-mini\", openai_api_key=OPENAI_API_KEY)\nstep_back_prompt_template = \"\"\"\n\n\nGenerate a less specific question (aka Step-back question) for th\nDetailed question: {detailed_question}\nStep-back question:\n\"\"\"\nstep_back_prompt = ChatPromptTemplate.from_template(step_back_pro\nNote\nI’ve chosen to use 4o-mini over gpt-4o and gpt-4o-mini because it tends to\ngenerate more abstract yet contextually relevant queries, along with more\ncoherent and well-synthesized final answers. That said, I encourage you to\nexperiment with different models to see how their outputs vary.\nNow, create the chain:\nstep_back_question_gen_chain = step_back_prompt | llm | StrOutput\nTry out the chain with a sample question:\nuser_question = \"Can you give me some tips for a trip to Brighton\nstep_back_question = step_back_question_gen_chain.invoke(user_que\nWhen you print step_back_question, you should get a response like:\n'What are some general tips for planning a successful trip to a c\nThis generated step-back question can then be used within a RAG\narchitecture to retrieve broader context from the vector store, which can\nsubsequently be provided to the LLM to help synthesize a more complete\nanswer.\n9.3.2 Incorporating Step-Back Question Generation into the\nRAG Chain\nYou can integrate the step-back question generation chain into a RAG\nworkflow, as shown in Listing 9.3.\nListing 9.3 Integrating Step-Back Question Generation within a RAG Architecture\n\n\nretriever = uk_granular_collection.as_retriever()\nrag_prompt_template = \"\"\"\nGiven a question and some context, answer the question.\nIf you do not know the answer, just say I do not know.\nContext: {context}\nQuestion: {question}\n\"\"\"\nrag_prompt = ChatPromptTemplate.from_template(rag_prompt_template\nstep_back_question_rag_chain = (\n   {\n       \"context\": {\"detailed_question\": RunnablePassthrough()} | \n       \"question\": RunnablePassthrough(),  #B\n   }\n   | rag_prompt\n   | llm\n   | StrOutputParser()\n)\nNow, try running the chain:\nuser_question = \"Can you give me some tips for a trip to Brighton\nanswer = step_back_question_rag_chain.invoke(user_question)\nprint(answer)\nYou should see a synthesized response like this:\nHere are some tips for a trip to Brighton:\n1. **Stay Safe**: While Brighton is generally safe, be cautious i\n2. **Watch for Traffic**: Be mindful of traffic, especially in bu\n3. **Valuables**: Take standard precautions with your valuables t\n4. **Homelessness**: Be aware that there may be homeless individu\n5. **Beaches**: Lifeguards patrol the beaches from late May to ea\n6. **Emergency Contacts**: In case of emergencies related to the \n7. **Explore Local Venues**: Enjoy local venues favored by reside\n8. **Cultural Areas**: Visit areas like The Lanes and North Laine \n9. **Stay Informed**: Keep an eye on your surroundings, especiall\nEnjoy your trip to Brighton!\nThis technique offers an alternative to embedding-focused methods,\n",
      "page_number": 290,
      "chapter_number": 34,
      "summary": "This chapter covers segment 34 (pages 290-300). Key topics include question, questions, and retrieved. You can automatically generate multiple queries for any user question using a\nprompt like this, adapted from a LangChain example:\nQUERY_PROMPT = PromptTemplate(\n   input_variables=[\"question\"],\n   template=\"\"\"You are an AI language model assistant.",
      "keywords": [
        "question",
        "LLM",
        "Step-Back Question",
        "User",
        "detailed question",
        "prompt",
        "context",
        "Step-Back",
        "user question",
        "original question",
        "detailed",
        "retriever",
        "Chain",
        "questions",
        "generate"
      ],
      "concepts": [
        "question",
        "questions",
        "retrieved",
        "retrieve",
        "retrieval",
        "context",
        "llm",
        "answer",
        "step",
        "prompt"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 58,
          "title": "Segment 58 (pages 491-498)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 22,
          "title": "Segment 22 (pages 435-456)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 48,
          "title": "Segment 48 (pages 973-990)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 27,
          "title": "Segment 27 (pages 536-554)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 28,
          "title": "Segment 28 (pages 555-576)",
          "relevance_score": 0.58,
          "method": "api"
        }
      ]
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 301-308)",
      "start_page": 301,
      "end_page": 308,
      "detection_method": "topic_boundary",
      "content": "enhancing retrieval by broadening the question itself to improve context.\nNext, I’ll introduce another technique that also optimizes retrieval through\nquestion transformation.\n9.4 Hypothetical Document Embeddings (HyDE)\nAs discussed in section 7.2.2, embedding hypothetical questions can enhance\nRAG retrieval by indexing document chunks with additional embeddings that\nrepresent questions answerable by the content in each chunk. This approach\nmakes embeddings of these hypothetical questions more semantically similar\nto the user’s question than embeddings of the raw chunk text alone.\nA similar effect can be achieved with Hypothetical Document Embeddings\n(HyDE), a technique that keeps the original chunk embeddings unchanged\nwhile generating hypothetical documents based on the user’s question, as\nshown in figure 9.5.\nFigure 9.5 Hypothetical Document Embeddings (HyDE): In this approach, the LLM generates\nhypothetical documents that would answer the user’s question. Rather than querying the\ndocument store with the user’s original question, these generated documents are used. Because\nthese hypothetical documents are semantically closer to the document chunk text, they improve\nthe relevance of retrieved content.\n\n\nUser\n\nQuestion: Original question 0_Q\nContext: C1 +C02+C3\n\nSSS SSS SSS SSS SSS SSS SESS SSS SESE EES\n\nAs shown in the sequence diagram in Figure 9.5, the generated hypothetical\ndocuments are used to retrieve relevant content, aiming to increase the\nsemantic similarity between the user’s question and the document chunk\nembeddings. This technique was introduced by Luyu Gao et al. in their paper,\n“Precise Zero-Shot Dense Retrieval without Relevance Labels”\n(https://arxiv.org/pdf/2212.10496v1.pdf). Now, let’s proceed to implement\nthis workflow.\nLet’s move on to implementing this workflow.\n9.4.1 Generating an Hypothetical Document for the User\nQuestion\nImplementing HyDE follows a familiar pattern: you design a chain to\ngenerate a hypothetical document that could answer the user’s question, and\nthen use this generated document as input for retrieval within a larger RAG\nworkflow.\nFirst, set up the prompt:\nllm = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=OPENAI_API_K\nhyde_prompt_template = \"\"\"\nWrite one sentence that could answer the provided question. Do no\nQuestion: {question}\nSentence:\n\"\"\"\nhyde_prompt = ChatPromptTemplate.from_template(hyde_prompt_templa\nNext, build the HyDE chain:\nhyde_chain = hyde_prompt | llm | StrOutputParser()\nTest it with a sample question:\nuser_question = \"What are the best beaches in Cornwall?\"\nhypotetical_document = hyde_chain.invoke(user_question)\n\n\nWhen you print hypothetical_document, you should see output like:\nSome of the best beaches in Cornwall include Fistral Beach, Porth\nThis generated hypothetical document can then be used in the RAG chain to\nretrieve relevant content, improving the alignment between the user’s\nquestion and the document chunks in the vector store. Let’s look at\nintegrating this step into the broader RAG workflow next.\n9.4.2 Integrating the HyDE Chain into the RAG Chain\nYou can incorporate the HyDE chain into a RAG workflow, as shown in\nListing 9.4.\nListing 9.4 Integrating a HyDE Chain within a RAG Workflow\nretriever = uk_granular_collection.as_retriever()\nrag_prompt_template = \"\"\"\nGiven a question and some context, answer the question.\nOnly use the provided context to answer the question.\nIf you do not know the answer, just say I do not know. \nContext: {context}\nQuestion: {question}\n\"\"\"\nrag_prompt = ChatPromptTemplate.from_template(rag_prompt_template\nhyde_rag_chain = (\n   {\n       \"context\": {\"question\": RunnablePassthrough()} | hyde_cha\n       \"question\": RunnablePassthrough(),  #B\n   }\n   | rag_prompt\n   | llm\n   | StrOutputParser()\n)\nNow, try the complete RAG chain:\nuser_question = \"What are the best beaches in Cornwall?\"\n\n\nanswer = hyde_rag_chain.invoke(user_question)\nprint(answer)\nYou should see a synthesized answer similar to this:\nThe best beaches in Cornwall mentioned in the context include Bud\nThis concludes the integration of HyDE into the RAG chain, enhancing the\nretrieval process by using a hypothetical document. Before moving on, I’ll\nbriefly revisit the multi-query generation technique we discussed earlier to\nrefine the retrieval focus.\n9.5 Single-Step and Multi-Step Decomposition\nIn the “multiple questions” or “sub-questions” method covered in section 9.2,\nthe original user question contained several implicit questions. For that case,\nwe instructed the LLM to generate a set of explicit, independent questions,\neach of which could be executed separately (or in parallel) on the vector\nstore. This approach, called “single-step decomposition,” is effective when\nthe questions are independent, and the original “complex question” can be\nsplit into simple, single-step questions.\nHowever, if the original question includes several interdependent questions, a\ndifferent approach is required. For example, if your data store contains tourist\ninformation, consider the question: “What is the average August temperature\nat the most popular sandy beach in Cornwall?” This question requires a\nsequence of dependent queries, where each answer informs the next question.\nIn this case, you could instruct the LLM to generate a strategic plan for\nbreaking down the question into a sequence of interdependent queries. Each\nquery would contain a parameter, filled with information from the previous\nanswer. Once the LLM returns this sequence of parameterized questions, you\ncan execute them step-by-step, storing each answer and feeding it into the\nsubsequent query. This process continues until you reach the final answer, as\nshown in figure 9.6.\nFigure 9.6 Multi-Step Decomposition Workflow: In this workflow, the original complex question\nis sent to the LLM, which generates a sequence of parameterized questions. Each question is then\nexecuted on the vector store, using answers from previous steps as parameters. Once all questions\n\n\nare answered, the LLM synthesizes a final response based on the collected information.\n\n\nUser\n\nQuestion: Original question 0_Q\nContext: C14 C2 +03\n\nSSS SSS SSS SSS SSS SSS SESS SSS SESE EES\n\nIn the workflow illustrated in the sequence diagram, the original complex\nquestion is sent to the LLM, which breaks it down into a series of\nparameterized questions. Each question is executed on the vector store, using\nprevious answers as parameters. After all questions are answered, the LLM\ncombines the collected information to generate a final response.\nTo prompt the LLM to generate this question sequence, use a template like\nthis:\nBreak down the following question into multiple dependent steps t\nFor each step, include the question and the corresponding paramet\n- Question: The text of the question.\n- Parameter: The parameter to be filled with the previous answer.\n—---\nOriginal question: “What is the average August temperature at the \nMultiple dependent questions:\nTo illustrate what such a sequence of questions might look like, you can try\nrunning the prompt above in ChatGPT. Replacing the user question with\n“What is the average August temperature at the most popular sandy beach in\nCornwall?” will produce a response similar to the one below:\n[\n {\n   \"Question\": \"What is the most popular sandy beach in Cornwall\n   \"Parameter\": \"None (initial query to the full data source)\"\n },\n {\n   \"Question\": \"What are the recorded daily temperatures in Augu\n   \"Parameter\": \"Beach Name from the previous answer\"\n },\n {\n   \"Question\": \"What is the average temperature from the followi\n   \"Parameter\": \"Daily Temperatures from the previous answer\"\n }\n]\nEach question can then be executed against the vector store or SQL database,\nusing the answer from the previous step as its parameter value. After\nprocessing the final question, you will have the context needed to answer the\noriginal question with the LLM.\n",
      "page_number": 301,
      "chapter_number": 35,
      "summary": "This chapter covers segment 35 (pages 301-308). Key topics include question, questions, and retrieval.",
      "keywords": [
        "question",
        "Hypothetical Document Embeddings",
        "Hypothetical Document",
        "SSS",
        "questions",
        "Document",
        "RAG",
        "LLM",
        "Hypothetical",
        "user",
        "original question",
        "answer",
        "HyDE",
        "User Question",
        "Document Embeddings"
      ],
      "concepts": [
        "question",
        "questions",
        "retrieval",
        "retrieved",
        "retrieve",
        "parameter",
        "answerable",
        "answer",
        "context",
        "embeddings"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 59,
          "title": "Segment 59 (pages 499-500)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 24,
          "title": "Segment 24 (pages 208-215)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 30,
          "title": "Segment 30 (pages 523-540)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Effective_Modern_C++",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 20,
          "title": "Segment 20 (pages 343-360)",
          "relevance_score": 0.63,
          "method": "api"
        }
      ]
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 309-316)",
      "start_page": 309,
      "end_page": 316,
      "detection_method": "topic_boundary",
      "content": "Note\nIf using a SQL database, you could simplify this with native SQL functions\nlike AVG. However, this example illustrates the concept rather than the\noptimal implementation.\nWhile I won’t provide a full implementation here, this method builds on\nadvanced RAG techniques, such as:\nRouting a natural language question to the relevant content store.\nGenerating a query for specific content stores, like SQL databases, from\na natural language question.\nWhile LangChain doesn’t offer a dedicated class for multi-step question\ndecomposition, you may find inspiration in LlamaIndex’s\nMultiStepQueryEngine class, which is worth exploring for further ideas.\nThis concludes our exploration of question transformation techniques for\nenhancing retrieval effectiveness. In the next chapter, you’ll learn additional\nmethods to improve RAG performance.\n9.6 Summary\nThe quality of LLM answers depends on well-structured questions, as\nvague or complex queries can mislead the vector store and LLM,\nresulting in weaker results.\nPoorly phrased questions lead to less accurate retrievals and answers,\nmaking clear and concise questions essential.\nThe \"Rewrite-Retrieve-Read\" approach improves answers by refining\nthe original question, helping retrieve better context and enhancing LLM\nresponses.\nSplitting questions into explicit sub-questions, executed separately,\nensures better retrieval, and LangChain’s MultiQueryRetriever can\ncombine these answers into a comprehensive response.\nHighly detailed queries can return overly specific results, missing the\nbroader context needed for a complete answer.\nUsing coarse chunks for synthesis and fine-grained chunks for detail, or\n\n\nemploying “step-back questions” to broaden queries, helps capture high-\nlevel context.\nEmbedding hypothetical questions alongside document chunks improves\nretrieval by aligning them with potential user questions.\nGenerating hypothetical documents with HyDE retrieves semantically\naligned content based on user questions without altering chunk\nembeddings.\nDecomposing questions into independent sub-questions allows for\nindividual retrieval and synthesis.\nGenerating a sequence of parameterized questions for interdependent\nqueries ensures that each step builds on the previous answer,\nsystematically resolving dependencies for a final answer.\n\n\n10 Query generation, routing and\nretrieval post-processing\nThis chapter covers\nGenerate metadata queries directly from user questions\nConvert user questions into database-specific queries (e.g., SQL,\nSPARQL)\nRoute questions to the appropriate handler based on intent\nEnhance result relevance using Reciprocal Rank Fusion (RRF)\nIn chapters 8 and 9, you improved RAG answer accuracy using advanced\nindexing and query transformations. Indexing strengthens embedding\neffectiveness for broader chunks, adding richer context, while query\ntransformations boost the precision of vector store retrieval.\nNow, you’ll dive into three more advanced RAG techniques. First, you’ll\nlearn to generate queries specific to the type of content store in use. For\ninstance, you’ll see how to generate SQL from a user's natural language\nquestion to retrieve data from a relational database. Your setup might include\nseveral types of content stores—such as vector stores, a relational database,\nor even a knowledge graph database. You’ll use the LLM to direct the user’s\nquestion to the right content store.\nFinally, you’ll refine the retrieved results to send only the most relevant\ncontent for synthesis, filtering out unnecessary data to maintain clarity and\nrelevance.\n10.1 Content Database Query Generation\nTo give an LLM the best information possible for answering user questions,\nyou often need to access other databases beyond your vector store. Many of\nthese databases hold structured data and only accept structured queries. You\n\n\nmight wonder if the difference between unstructured user questions and the\nstructured queries required for these databases poses a problem. This gap can\nbe bridged with an LLM’s help. Let’s look at common content stores in LLM\napplications and typical ways to retrieve data from them.\nVector Store (or Vector Database): Vector Store (or Vector Database):\nYou’re already familiar with vector stores, which hold document chunks\nalong with their embeddings in a vector-based index to enable semantic\nretrieval. This approach, known as dense retrieval, uses embeddings—\ncompact vectors with hundreds or thousands of dimensions that capture\nthe semantic meaning of text. Similarity between a user query and stored\nchunks is computed based on the distance between these dense vectors.\nAn alternative is sparse retrieval (also called lexical retrieval), which\nmany vector databases support, though it doesn’t require one. In the\nindexing phase, each document chunk is tokenized, and an inverted\nindex is built to map each unique token to the list of chunks in which it\nappears. During querying, the user’s question is tokenized in the same\nway, and each token is matched against the inverted index to retrieve\nrelevant chunks. These are then ranked using relevance scoring methods\nlike BM25 or TF–IDF, based on how well the term statistics of each\nchunk align with the query. Sparse search excels at precise, keyword-\ndriven queries, supports Boolean logic (such as “must” and “must not”),\nand offers strong explainability by directly linking results to matching\nterms in the query.\nRelational (SQL) Database: An LLM application can connect to a\nrelational database, which stores structured facts in tables. Data is\ntypically retrieved using SQL queries. For example, a database might\ncontain seasonal temperatures at tourist resorts or lists of available\nhotels and car rentals by location. LLMs can assist by generating SQL\nqueries from natural language questions—a technique known as text-to-\nSQL. This approach is gaining popularity as a way to make databases\naccessible to non-technical users.\nDocument, Key-Value, or Object Databases: These databases store data\nas documents or objects, typically in JSON or BSON format. Because\nmany LLMs have been extensively trained on JSON structures, they can\naccurately convert user questions into JSON-based queries that align\nwith the database schema. Notably, many document databases have\n\n\nrecently been rebranded as vector databases after introducing support for\nvector fields—used to store embeddings—and adding vector search and\nsimilarity capabilities.\nKnowledge Graph Databases: Knowledge graphs represent data as a\ngraph, where nodes correspond to entities and edges define their\nrelationships. Originally popularized by companies like Facebook and\nLinkedIn to infer connections in social networks, graph databases are\nnow increasingly used in LLM applications. LLMs can transform\nunstructured text into structured knowledge graphs—a process often\nreferred to as Knowledge Graph Enhanced RAG—resulting in a more\ncompact and structured representation compared to vector stores. Once\ndata is stored in a graph database, it can be queried using graph-specific\nlanguages like SPARQL or Cypher, enabling complex reasoning and\ninference that go beyond simple similarity search. This forms the basis\nof GraphRAG. We’ll explore how LLMs can assist in building these\ngraph structures from raw text, generating SPARQL or Cypher queries,\nand finally converting query results back into natural language\nresponses.\nNow, let’s dig into the process of transforming a user’s question into a\nstructured query for different databases, starting with retrieving document\nchunks from a vector store using metadata.\n10.2 Self-Querying (Metadata Query Enrichment)\nA vector store typically indexes document chunks by embedding for dense\nsearch, but it can also use keyword-based indexing in a few ways:\n1. Explicit Metadata Tags: You can add metadata to each chunk, such as\nthe timestamp, filename or URL, topic, and keywords. These keywords\ncan come from user input or ones you assign manually.\n2. Keyword Extraction via Algorithm: Use algorithms like TF-IDF (Term\nFrequency-Inverse Document Frequency) or its extension, BM25, to\nidentify relevant keywords for each chunk based on word frequency and\nimportance.\n3. Keyword Suggestions from the LLM: You can ask the LLM to generate\nkeywords for tagging each chunk.\n\n\nWith keywords attached to chunks through any of these methods, you can\nperform a semantic search, focusing only on chunks filtered by a keyword-\nbased (or “sparse”) search. If your chatbot’s UI allows users to filter results\ndirectly—such as with dropdown options—your vector store query can\nexplicitly include a metadata filter based on those selections. However, more\ncommonly, you’ll automate this filtering by inferring relevant metadata from\nthe user’s question. This technique, known as “self-querying” or “self-\nmetadata querying,” enables your application to automatically generate a\nquery enriched with metadata filters based on the user’s question.\nIn a self-querying flow, the user’s original question is transformed into an\nenriched query with both a metadata filter and a semantic component,\nenabling a combined dense and sparse search. This process, illustrated in\nfigure 10.1, resembles the setup shown in figure 8.\nFigure 10.1 Self-querying workflow: the original question turns into a semantic query with an\nembedded metadata filter. When this enriched query runs on the vector store, it first selects\nchunks matching the metadata filter, then applies semantic search on this refined set.\n\n\n1b. The user's question is also included in\nthe chatbot prompt sent to the LLM for\n\nanswer synthesis\nuser question SSeeresssseeeesesescccecssssnnssesssssssssssscsssshssd\nser question\nSelf-que a\n= vain Rewitlen query + metadata ey —_ (\n1a. The user question is PTTTI Titi\nfed to the self-query\n\nUser generator which uses the (7\nLLM to generate a Rewritten query +] Y;\nmetadata enriched query | \"etadata query\n\nLLM\n2. The retriever transforms i\nthe rewritten query into the Retriever |\ncorresponding embedding !\n+ Chatbot —\n8.The retrever searches , Prompt\nfor relevant tet chunks = f !\nusing the rewritten que |\nonteshg and ty . = iinet cease 5 The chatbo creates d\nembeddi i question and the retrieved\nchunks (the context)\n4.The vector DB retums\naan 0 6 The LLM acteps the\n4 prompt and synthesizes a\ncorresponding completion\nVector (the response)\n\nNow that you understand the basics of self-metadata querying, let’s go\nthrough the steps to implement it. We’ll start with the ingestion phase, where\nyou’ll tag each chunk with relevant metadata keywords. Then, in the Q&A\nphase, I’ll show you two methods for generating self-metadata queries: one\nusing the built-in SelfQueryRetriever and another that uses LLM function\ncalling.\n10.2.1 Ingestion: Metadata Enrichment\nTo use metadata effectively, start by re-importing the UK tourist destination\ndata into a new collection, this time storing metadata for each chunk. Here’s\nhow to set up the environment:\nInitialize the Environment\nOpen a new OS shell, navigate to the chapter 10 code folder, activate the\nvirtual environment, and create a new Jupyter notebook.\nC:\\Github\\building-llm-applications\\ch10>\nC:\\Github\\building-llm-applications\\ch10>python -m venv env_ch10\nC:\\Github\\building-llm-applications\\ch10>pip install notebook lan\nc:\\GitHub\\building-llm-applications\\ch10>env_ch10\\Scripts\\activat\nThen, in Jupyter Notebook, go to File > New > Notebook, and save it as 10-\nquery_generation.ipynb.\nDefine Metadata\nIdentify keywords to tag each chunk, such as:\nsource: URL of the original content\ndestination: the tourist destination referenced\nregion: the UK region of the destination\nManually define mappings for destination and region, and dynamically\ngenerate the source URL for each chunk.\n",
      "page_number": 309,
      "chapter_number": 36,
      "summary": "This chapter covers segment 36 (pages 309-316). Key topics include query, queries, and queried. While I won’t provide a full implementation here, this method builds on\nadvanced RAG techniques, such as:\nRouting a natural language question to the relevant content store.",
      "keywords": [
        "vector store",
        "LLM",
        "user questions",
        "vector",
        "question",
        "user",
        "query",
        "questions",
        "metadata",
        "store",
        "databases",
        "chunks",
        "SQL",
        "queries",
        "’ll"
      ],
      "concepts": [
        "query",
        "queries",
        "queried",
        "question",
        "questions",
        "retrievals",
        "retrieve",
        "retrieved",
        "database",
        "chunks"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 25,
          "title": "Segment 25 (pages 495-517)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 25,
          "title": "Segment 25 (pages 216-223)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 26,
          "title": "Segment 26 (pages 518-535)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 40,
          "title": "Segment 40 (pages 367-374)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 19,
          "title": "Segment 19 (pages 157-164)",
          "relevance_score": 0.61,
          "method": "api"
        }
      ]
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 317-324)",
      "start_page": 317,
      "end_page": 324,
      "detection_method": "topic_boundary",
      "content": "Set Up Chroma DB Collection\nConfigure the Chroma DB collection with the code shown in the following\nlisting:\nListing 10.1 Setting up the ChromaDB collection\nfrom langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nimport getpass\nOPENAI_API_KEY = getpass.getpass('Enter your OPENAI_API_KEY')\nuk_with_metadata_collection = Chroma(\n   collection_name=\"uk_with_metadata_collection\",\n   embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_\nuk_with_metadata_collection.reset_collection()  #A\nDefine Ingestion Content and Splitting Strategy\nOutline the content and define a text splitting strategy to process the\ndocuments. Listing 10.2 shows how to set this up:\nListing 10.2 Defining content to be ingested and text splitting strategy\nfrom langchain_community.document_loaders import AsyncHtmlLoader\nfrom langchain_community.document_transformers import Html2TextTr\nfrom langchain_text_splitters import RecursiveCharacterTextSplitt\nfrom langchain_core.documents import Document\nhtml2text_transformer = Html2TextTransformer()\ntext_splitter = RecursiveCharacterTextSplitter(  #A\n   chunk_size=1000, chunk_overlap=100\n)\ndef split_docs_into_chunks(docs):\n   text_docs = html2text_transformer.transform_documents(docs)  \n   chunks = text_splitter.split_documents(text_docs)\n   return chunks\n\n\nuk_destinations = [\n   (\"Cornwall\", \"Cornwall\"), (\"North_Cornwall\", \"Cornwall\"), \n   (\"South_Cornwall\", \"Cornwall\"), (\"West_Cornwall\", \"Cornwall\")\n   (\"Tintagel\", \"Cornwall\"), (\"Bodmin\", \"Cornwall\"), (\"Wadebridg\n   (\"Penzance\", \"Cornwall\"), (\"Newquay\", \"Cornwall\"), (\"St_Ives\"\n   (\"Port_Isaac\", \"Cornwall\"), (\"Looe\", \"Cornwall\"), (\"Polperro\"\n   (\"Porthleven\", \"Cornwall\"),\n   (\"East_Sussex\", \"East_Sussex\"), (\"Brighton\", \"East_Sussex\"),\n   (\"Battle\", \"East_Sussex\"), (\"Hastings_(England)\", \"East_Susse\n   (\"Rye_(England)\", \"East_Sussex\"), (\"Seaford\", \"East_Sussex\"), \n   (\"Ashdown_Forest\", \"East_Sussex\")\n]\nwikivoyage_root_url = \"https://en.wikivoyage.org/wiki\"\nuk_destination_url_with_metadata = [  #C\n   ( f'{wikivoyage_root_url}/{destination}', destination, region\n   for destination, region in uk_destinations]\nThe next step is to ingest the content and the related metadata.\nIngest Content with Metadata\nEnrich the content with metadata by processing each document chunk as\nshown in listing 10.3.\nListing 10.3 Enriching chunks with related metadata\nfor (url, destination, region) in uk_destination_url_with_metadat\n   html_loader = AsyncHtmlLoader(url)  #A\n   docs =  html_loader.load()  #B\n   \n   docs_with_metadata = [\n       Document(page_content=d.page_content,\n       metadata = {\n           'source': url,\n           'destination': destination,\n           'region': region})\n       for d in docs]\n            \n   chunks = split_docs_into_chunks(docs_with_metadata)\n   print(f'Importing: {destination}')\n   uk_with_metadata_collection.add_documents(documents=chunks)\n\n\nNow your collection is ready, with each document chunk enriched with\nmetadata. You can query this content and apply metadata filters to refine\nsearch results based on keywords like destination, region, or source.\n10.2.2 Q & A on a Metadata-Enriched Collection\nThere are three ways to query metadata-enriched content:\n1. Explicit Metadata Filters: Specify the metadata filter manually.\n2. SelfQueryRetriever: Automatically generate the metadata filter using the\nSelfQueryRetriever.\n3. Structured LLM Function Call: Infer the metadata filter with a structured\ncall to an LLM function.\nLet’s explore these methods, starting with explicit filtering.\nQuerying with an Explicit Metadata Filter\nYou can leverage the metadata attached to each chunk by explicitly adding a\nfilter to the retriever. Here’s an example:\nquestion =  \"Events or festivals\"\nmetadata_retriever = uk_with_metadata_collection.as_retriever(sea\nresult_docs = metadata_retriever.invoke(question)\nWhen you print result_docs, you’ll see that only chunks tagged with\n`destination: Newquay` are returned, confirming that the filter is working\ncorrectly:\n[Document(metadata={'destination': 'Newquay', 'region': 'Cornwall\nDocument(metadata={'destination': 'Newquay', 'region': 'Cornwall\nTo adjust the filter, instantiate a new retriever with the updated parameters.\nAutomatically Generating Metadata Filters with SelfQueryRetriever\nYou can also generate metadata filters automatically with\n\n\nSelfQueryRetriever. This tool interprets the user’s question to infer the\nappropriate filter. First, import the necessary libraries, as shown in the\nfollowing listing:\nListing 10.4 Setting up metadata field information\nfrom langchain.chains.query_constructor.base import AttributeInfo\nfrom langchain.retrievers.self_query.base import SelfQueryRetriev\nfrom langchain_openai import ChatOpenAI\nNext, define the metadata attributes to infer from the question:\nmetadata_field_info = [\n   AttributeInfo(\n       name=\"destination\",\n       description=\"The specific UK destination to be searched\",\n       type=\"string\",\n   ),\n   AttributeInfo(\n       name=\"region\",\n       description=\"The name of the UK region to be searched\",\n       type=\"string\",\n   )\n]\nNow, set up the SelfQueryRetriever with the question, without specifying a\nmanual filter:\nquestion = \"Tell me about events or festivals in the UK town of N\nllm = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=OPENAI_API_K\nself_query_retriever = SelfQueryRetriever.from_llm(\n   llm, uk_with_metadata_collection, question, metadata_field_in\n)\nInvoke the retriever with the question:\nresult_docs = self_query_retriever.invoke(question)\nPrinting result_docs will confirm that only chunks related to Newquay are\nretrieved, matching the inferred filter:\n\n\n[Document(metadata={'destination': 'Newquay', 'region': 'Cornwall\nDocument(metadata={'destination': 'Newquay', 'region': 'Cornwall\nDocument(metadata={'destination': 'Newquay', 'region': 'Cornwall\nflexibility.\nGenerating Metadata Filters with an LLM Function Call\nYou can also infer metadata filters by having the LLM map the question to a\npredefined metadata template with attributes you stored during ingestion.\nThis approach offers greater flexibility than the SelfQueryRetriever but\nrequires more setup.\nFirst, import the libraries necessary to create a structured query with specific\nfilters:\nListing 10.5 Importing libraries\nimport datetime\nfrom typing import Literal, Optional, Tuple, List\nfrom pydantic import BaseModel, Field\nfrom langchain.chains.query_constructor.ir import (\n   Comparator,\n   Comparison,\n   Operation,\n   Operator,\n   StructuredQuery,\n)\nfrom langchain.retrievers.self_query.chroma import ChromaTranslat\nThe DestinationSearch class translates the user question into a structured\nobject with a content_search field containing the question (minus filtering\ndetails) and fields for inferred search filters. Listing 10.6 shows this setup:\nListing 10.6 Strongly-typed structured question including inferred filter attributes\nclass DestinationSearch(BaseModel):\n   \"\"\"Search over a vector database of tourist destinations.\"\"\"\n   content_search: str = Field(\n       \"\",\n\n\n       description=\"Similarity search query applied to tourist d\n   )\n   destination: str = Field(\n       ...,\n       description=\"The specific UK destination to be searched.\"\n   )\n   region: str = Field(\n       ...,\n       description=\"The name of the UK region to be searched.\",\n   )\n   def pretty_print(self) -> None:\n       for field in self.__fields__:\n           if getattr(self, field) is not None and getattr(self, \n               self.__fields__[field], \"default\", None\n           ):\n               print(f\"{field}: {getattr(self, field)}\")\nBuild a Chroma DB Filter Statement from the Structured Query\nNext, create a function to convert a `DestinationSearch` object into a filter\ncompatible with Chroma DB, as shown in listing 10.7:\nListing 10.7 Building a ChromaDB specific filter statement from a structured query object\ndef build_filter(destination_search: DestinationSearch):\n   comparisons = []\n   destination = destination_search.destination  #A\n   region = destination_search.region  #A\n   \n   if destination and destination != '':  #B\n       comparisons.append(\n           Comparison(\n               comparator=Comparator.EQ,\n               attribute=\"destination\",\n               value=destination,\n           )\n       )\n   if region and region != '':  #C\n       comparisons.append(\n           Comparison(\n               comparator=Comparator.EQ,\n               attribute=\"region\",\n               value=region,\n\n\n           )\n       )    \n   search_filter = Operation(operator=Operator.AND, arguments=co\n   chroma_filter = ChromaTranslator().visit_operation(search_fil\n       \n   return chroma_filter\nBuild a Query Chain to Convert the Question into a Structured Query\nNow, define the query generator chain to convert the user question into a\nstructured query with metadata filters:\nListing 10.8 Query generator chain\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nsystem_message = \"\"\"You are an expert at converting user question\nYou have access to a database of tourist destinations. \\\nGiven a question, return a database query optimized to retrieve t\nIf there are acronyms or words you are not familiar with, do not \nprompt = ChatPromptTemplate.from_messages(\n   [\n       (\"system\", system_message),\n       (\"human\", \"{question}\"),\n   ]\n)\nllm = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=OPENAI_API_K\nstructured_llm = llm.with_structured_output(DestinationSearch, me\nquery_generator = prompt | structured_llm\nLet’s try out the chain with the same question used earlier:\nquestion = \"Tell me about events or festivals in the UK town of N\nstructured_query =query_generator.invoke(question)\nPrinting structured_query shows the question converted into a structured\nobject:\nDestinationSearch(content_search='events festivals', destination=\n\n\nWith the structured query created, generate a Chroma DB-compatible search\nfilter:\nsearch_filter = build_filter(structured_query)\nThe search_filter result will look like this:\n{'$and': [{'destination': {'$eq': 'Newquay'}},\n {'region': {'$eq': 'Cornwall'}}]}\nPerform the vector search using the generated structured query and Chroma\nDB filter:\nsearch_query = structured_query.content_search\nmetadata_retriever = uk_with_metadata_collection.as_retriever(sea\nanswer = metadata_retriever.invoke(search_query)\nThe answer should closely match the output from the SelfQueryRetriever,\nreturning chunks associated with Newquay:\n[Document(metadata={'destination': 'Newquay', 'region': 'Cornwall\nSo far, you’ve focused on generating metadata-enriched queries for vector\nstores. In the next section, you’ll learn how to generate SQL queries from\nnatural language questions, enabling retrieval of structured data from\nrelational databases.\n10.3 Generating a Structured SQL Query\nMany LLMs can transform user questions into SQL queries, enabling access\nto relational databases directly from LLM applications. While LLMs are\ncontinually improving in generating accurate SQL, challenges remain,\nespecially when working with complex schemas or specific database\nstructures. LangChain enhances these capabilities with evolving Text-to-SQL\nfeatures, but there are some common issues you should consider.\nA helpful reference here is the paper \"Evaluating the Text-to-SQL\nCapabilities of Large Language Models\" by Nitarshan Rajkumar et al.\n",
      "page_number": 317,
      "chapter_number": 37,
      "summary": "This chapter covers segment 37 (pages 317-324). Key topics include importing, cornwall, and question. Ingest Content with Metadata\nEnrich the content with metadata by processing each document chunk as\nshown in listing 10.3.",
      "keywords": [
        "metadata",
        "Cornwall",
        "filter",
        "destination",
        "metadata filters",
        "query",
        "Structured",
        "question",
        "East",
        "region",
        "structured query",
        "Newquay",
        "Sussex",
        "Content",
        "Chroma"
      ],
      "concepts": [
        "importing",
        "cornwall",
        "question",
        "questions",
        "filters",
        "filtering",
        "destinations",
        "metadata",
        "query",
        "queries"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 26,
          "title": "Segment 26 (pages 518-535)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 30,
          "title": "Segment 30 (pages 523-540)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 7,
          "title": "Segment 7 (pages 52-61)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 25,
          "title": "Segment 25 (pages 433-450)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 32,
          "title": "Segment 32 (pages 559-576)",
          "relevance_score": 0.62,
          "method": "api"
        }
      ]
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 325-336)",
      "start_page": 325,
      "end_page": 336,
      "detection_method": "topic_boundary",
      "content": "(https://arxiv.org/pdf/2204.00498.pdf). Though dated, this paper offers\npractical insights into common pitfalls and solutions. The main finding was\nthat \"hallucinations\"—incorrect table and column names—can often be\nreduced by using few-shot prompts that include the schema and sample\nrecords for the target table. An example schema from the paper looks like\nthis:\nCREATE TABLE \"state\" (\n   \"state_name\" TEXT,\n   \"population\" INT DEFAULT NULL,\n   \"area\" DOUBLE DEFAULT NULL,\n   \"country_name\" VARCHAR(3) NOT NULL DEFAULT '',\n   \"capital\" TEXT,\n   \"density\" DOUBLE DEFAULT NULL\n);\n/* example rows\nstate_name     population     area      country_name     capital  \nalabama        3894000        51700.0   usa              montgome\nalaska         401800         591000.0  usa              juneau   \narizona        2718000        114000.0  usa              phoenix  \n*/\n-- Answer the following question using the above table schema:\n-- {user_question}\nUsing the CREATE TABLE command along with sample data helps the LLM\nbetter understand the structure and constraints, minimizing incorrect column\nand table references.\n10.3.1 Installing SQLite\nSQLite does not require full installation. Unzip the package, place it in a\nfolder, and add the folder to your system’s Path environment variable. Refer\nto Appendix D for setup instructions on Windows. For other operating\nsystems, consult the SQLite documentation.\n10.3.2 Setting Up and Connecting to the Database\nLet’s create a booking database called UkBooking to store UK destinations,\naccommodations, and special offers.\nHere is the relational diagram for the UKBooking database. Each table shows\n\n\nits primary key (PK) and foreign key (FK) columns, with relationships\nmarked by arrows connecting related tables. This setup visually represents\nthe structure and relationships within the database.\nFigure 10.2 Entity-Relationship diagram of the UkBooking DB\n\n\nDestination\n\n+Offerld\n+Description\n+DiscountRate\n+StartDate\n+EndDate\n+Accommodationid\n\nUK Booking DB Entity-Relationships\n\nAccommodation\n\n+Accommodationld\n+Name\n+Description\n+Address\n\n4 +Destinationld\n+AccommodationTypeld\n\n+CheckinDate\n+CheckoutDate\n+Accommodationid\n+Customerld\n\nAccommodation Type\n\n+AccommodationTypeld\n+TypeName\n\n+CustomerlD\n+FirstName\n+LastNAme\n\n+Email\n+Phone\n+Address\n\nThe database schema, shown in figure 10.2, includes the relationships\nbetween tables for destinations, accommodations, and offers.\nOpen your OS shell, navigate to the code folder, and enter the following\ncommand to create the UkBooking database:\nC:\\Github\\building-llm-applications\\ch10>sqlite3 UkBooking.db\nThis opens the SQLite terminal:\nSQLite version 3.46.1 2024-08-13 09:16:08 (UTF-16 console I/O)\nEnter \".help\" for usage hints.\nsqlite>\nIn the SQLite terminal, load the SQL scripts to create and populate the\nUkBooking database. Ensure these files are in C:\\Github\\building-llm-\napplications\\ch10. Download them from GitHub if necessary:\nsqlite> .read CreateUkBooking.sql\nsqlite> .read PopulateUkBooking.sql\nTo confirm the setup, check for records in the Offer table:\nsqlite> SELECT * FROM Offer;\nYou should see output similar to this:\n1|1|Summer Special|0.15|2024-06-01|2024-08-31\n2|2|Weekend Getaway|0.1|2024-09-01|2024-12-31\n3|3|Early Bird Discount|0.2|2024-05-01|2024-06-30\n4|4|Stay 3 Nights, Get 1 Free|0.25|2024-01-01|2024-03-31\n… \nNow the UkBooking database is ready for use with LangChain.\nReturn to the Jupyter notebook and import the libraries needed for SQL\ndatabase connections:\nfrom langchain_community.utilities import SQLDatabase\nfrom langchain_community.tools import QuerySQLDataBaseTool\nfrom langchain.chains import create_sql_query_chain\n\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nimport getpass\nimport os\nUse the following code to connect to the database and list available tables:\ndb = SQLDatabase.from_uri(\"sqlite:///UkBooking.db\")\nprint(db.get_usable_table_names())\nYou should see a list of table names:\n['Accommodation', 'AccommodationType', 'Booking', 'Customer', 'De\nRun a sample query to verify the connection:\ndb.run(\"SELECT * FROM Offer;\")\nThe output should display entries from the Offer table:\n\"[(1, 1, 'Summer Special', 0.15, '2024-06-01', '2024-08-31'), (2, \nNow, you’re set up to query the UkBooking database programmatically using\nLangChain.\n10.3.3 Generating SQL Queries from Natural Language\nNow that the setup is complete, you can start generating SQL queries directly\nfrom natural language questions. Here’s how to test a simple query:\nllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4o-min\nsql_query_gen_chain = create_sql_query_chain(llm, db)\nresponse = sql_query_gen_chain.invoke({\"question\": \"Give me some \nPrinting response will show the generated SQL query:\n'```sql\\nSELECT \"Offer\".\"OfferDescription\", \"Offer\".\"DiscountRate\nHowever, if you attempt to execute this SQL directly against the database,\nyou’ll encounter an error due to the backticks (```), which are non-SQL\ncharacters:\n\n\ndb.run(response)\n'Error: (sqlite3.OperationalError) near \"```sql\\nSELECT \"Offer\".\"\nTo clean up the SQL formatting, you can use the LLM to strip unnecessary\ncharacters and output a properly formatted SQL statement. Here’s a simple\nchain setup for this:\nListing 10.9 Chain to fix the formatting of the generated SQL\nclean_sql_prompt_template = \"\"\"You are an expert in SQL Lite. You \nwhich might contain unneded prefixes or suffixes. Given the follow\ntransform it to a clean, executable SQL statement for SQL lite.\nOnly return an executable SQL statement which terminates with a s\nDo not include the language name or symbols like ```.\nUnclean SQL: {unclean_sql}\"\"\"\nclean_sql_prompt = ChatPromptTemplate.from_template(clean_sql_pro\nclean_sql_chain = clean_sql_prompt | llm\nfull_sql_gen_chain = sql_query_gen_chain | clean_sql_chain | StrO\nLet’s try out this full chain with a sample question and verify the output:\nquestion = \"Give me some offers for Cardiff, including the accomo\nresponse = full_sql_gen_chain.invoke({\"question\": question})\nprint(response)\nThe output should be a clean SQL statement:\n\"SELECT Offer.OfferDescription, Offer.DiscountRate, Accommodation\nThis approach ensures the SQL statement is correctly formatted and ready to\nexecute against the database.\n10.3.4 Executing the SQL Query\nNow, let’s create a chain to generate and execute SQL queries.\n\n\nsql_query_exec_chain = QuerySQLDataBaseTool(db=db)\nsql_query_gen_and_exec_chain = full_sql_gen_chain | sql_query_exe\nresponse = sql_query_gen_and_exec_chain.invoke({\"question\":questi\nPrinting response should show the following output:\n\"[('Early Bird Discount', 0.2, 'Cardiff Camping')]\"\nThis setup allows you to retrieve data from a relational database by using a\ncombined chain (sql_query_gen_and_exec_chain) that handles both SQL\ngeneration and execution. You can easily integrate this chain within a broader\nRAG (Retrieval-Augmented Generation) setup, as discussed in earlier\nsections. Try extending this integration as an exercise.\nFigure 10.3 RAG with SQL Workflow: The LLM converts the natural language question into a\nSQL query, which is executed on the SQL database. The database returns records that are then\nprocessed by the LLM to generate the final answer.\n\n\nUser\n\nSSPE S ESSE ESSE ESSE EERSTE SEER EE EEE\n\nOriginal question 0.Q\n\n|\n|\n\ngenerate SQL from CREATE + examples |\n$$\n\nReturn SQL|SELECT ... FROM... WHERE},\n\nReturn\nSQL records:\n\nQuestion: Original question 0.Q\n\nContext: SQL records\n\nThe sequence diagram in figure 10.3 can give you a visual idea of what the\nfull RAG with SQL workflow would look like.\nTIP\nLangChain’s SQLDatabaseChain class provides a streamlined way to\ngenerate SQL queries directly from user questions. This tool uses an LLM\nand your database connection to automatically create “few-shot” prompts,\nsimilar to those recommended in the Rajkumar paper. Experimenting with\nSQLDatabaseChain can be highly beneficial if you plan to incorporate\nrelational databases into your RAG setup. A sequence diagram (Figure 10.3)\nshows a high-level overview of this complete RAG workflow.\n10.4 Generating a Semantic SQL Query\nIn the previous section, you learned how to generate SQL queries from\nnatural language. However, these queries rely on “strict” SQL, meaning they\ndepend on exact matching and traditional relational operations. Relational\ndatabases operate on record sets using operations like SELECT, JOIN, WHERE,\nand GROUP BY, where filters are based on exact string matches or numeric\ncomparisons.\nBut what if you want to expand the SQL search to include results that are\nsimilar in meaning to what the user intended? This requires a shift from\nstandard SQL to a semantic SQL search. In this section, I’ll provide an\noverview of how to implement semantic SQL search, a topic that continues to\nevolve.\n10.4.1 Standard SQL Query\nA standard SQL query filters based on exact matches. For example, to find\nusers with the first name “Roberto,” you would use:\nSELECT first_name, last_name FROM user WHERE first_name = ‘Robert\nThis query returns only users named “Roberto.” It won’t return records for\n\n\nnames like Robert, Rob, Robbie, Roby, Robin, Roe, Bobby, Bob, or Bert.\nYou can loosen this search slightly with the LIKE operator for partial\nmatching. For instance, to find users with names that start with “Rob”:\nSELECT first_name, last_name FROM user WHERE first_name LIKE ‘Rob%\nThis query will return names like Roberto, Robert, Rob, Robbie, Roby, and\nRobin but still won’t catch variations like Roe, Bobby, Bob, or Bert, as they\ndon’t contain the string “Rob.”\n10.4.2 Semantic SQL Query\nWith the rise of large language models (LLMs), several relational databases\nnow support semantic search, which enables searches based on embeddings\ninstead of exact matches. An example is PGVector, an extension for\nPostgreSQL that allows vector-based similarity searches using metrics such\nas L2, inner product, and cosine distance. This approach enables you to\nperform searches that return results based on meaning rather than exact text\nmatches.\nIn this section, I’ll refer to this approach as semantic SQL search or SQL\nsimilarity search interchangeably.\n10.4.3 Creating the Embeddings\nTo extend the “traditional SQL” approach with PGVector’s similarity search,\nyou’ll need to add vector-based embeddings for any columns you want to use\nin semantic searches. Here’s how to do it.\n1. Add a Vector Column: First, add a VECTOR type column to the table for\neach field you want to search by similarity. For example, to enable\nsimilarity search on first_name, add a column called\nfirst_name_embedding:\nALTER TABLE user  ADD COLUMN first_name_embedding VECTOR\n2. Calculate Embeddings: Next, compute the embedding values for each\n\n\nfirst_name. You can do this directly within PostgreSQL if you have a\nfunction to generate embeddings, or you can compute embeddings\nexternally using an API client like LangChain.\nIn-Database Calculation: If PostgreSQL has a custom function like\ncalculate_my_embedding() available, you can update the\nembeddings in-place with SQL:\nUPDATE user\nSET first_name_embedding = calculate_my_embedding(first_name)\nExternal Calculation with LangChain: If you’re using a pre-built\nembedding function (e.g., OpenAI’s), calculate embeddings\nexternally and store them using the PGVector API. In listing 10.10\nyou can see an example of using LangChain’s OpenAIEmbeddings\nwrapper to generate embeddings for first_name values and update\nthe database (library imports are omitted for brevity).\nListing 10.10 Using LangChain’s OpenAIEmbeddings wrapper to generate embeddings\ndb = SQLDatabase.from_uri(YOUR_DB_CONNECTION_STRING)  #A\nembeddings_model = OpenAIEmbeddings()  #A\nfirst_names_resultset_str = db.run('SELECT first_name FROM user')\nfirst_names = [fn[0] for fn in eval(first_names_resultset_str)]  \nfirst_names_embeddings = embeddings_model.embed_documents(first_n\nfn_emb = zip(first_names, first_names_embeddings)  #D\nfor fn, emb in fn_emb:\n   sql = f'UPDATE user SET first_name_embeddings = ARRAY{emb} WH\n   db.run(sql)\nBy following these steps, you’ll enable semantic search on first_name or\nother fields, allowing PGVector to retrieve records based on similarity, rather\nthan exact matches.\n10.4.4 Performing a Semantic SQL Search\nAfter setting up the embeddings, you can perform a similarity search as\nfollows:\n\n\nembedded_query= embeddings_model.embed_query(\"Roberto\")\nquery = (\n   'SELECT first_name FROM user WHERE first_name_embeddings IS N\n)\ndb.run(query)\nThis query returns variations of “Roberto,” including Roe, Bobby, Bob, and\nBert, by ordering results based on similarity.\n10.4.5 Automating Semantic SQL Search\nNow that you understand how to generate embeddings and perform similarity\nsearches in a SQL database, the final step is to create a prompt that can\nautomatically generate SQL similarity queries. This process is similar to what\nwe covered for generating traditional SQL queries. Once you design,\nimplement, and test this prompt—and integrate it into a full chain within\nLCEL—your LLM application will be capable of generating semantic\nsearches on PGVector or any SQL database that supports ARRAY (or\nsimilar) data types, seamlessly feeding the results to the LLM for synthesis.\n10.4.6 Benefits of Semantic SQL Search\nThe simple example here only scratches the surface of semantic SQL’s\ncapabilities. You can combine semantic filtering with exact matching or use\nmultiple semantic filters, especially powerful in multi-table queries using\njoins. This approach allows highly nuanced searches, especially when\ncombined with traditional SQL filtering.\nLater, I’ll show you how to combine metadata and semantic filtering in a\nvector store, which can achieve similar results. However, using multiple\nsemantic filters in SQL offers greater flexibility, particularly for complex\nqueries.\n10.5 Generating Queries for a Graph Database\nGraph databases are designed to store, navigate, and query data in a graph\nformat, with nodes representing entities and edges defining relationships, as\nshown in Figure 10.4. They are well-suited for building knowledge graphs,\n",
      "page_number": 325,
      "chapter_number": 38,
      "summary": "This setup visually represents\nthe structure and relationships within the database Key topics include tables, database, and embeddings. Unzip the package, place it in a\nfolder, and add the folder to your system’s Path environment variable.",
      "keywords": [
        "SQL",
        "semantic SQL search",
        "Semantic SQL",
        "SQL query",
        "SQL search",
        "Semantic SQL Query",
        "DOUBLE DEFAULT NULL",
        "SQL Queries",
        "query",
        "SQL database",
        "Database",
        "generate SQL",
        "SQL statement",
        "DEFAULT NULL",
        "Semantic"
      ],
      "concepts": [
        "tables",
        "database",
        "embeddings",
        "imports",
        "query",
        "queries",
        "offers",
        "generating",
        "generate",
        "generation"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 20,
          "title": "Segment 20 (pages 181-194)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 7,
          "title": "Segment 7 (pages 52-61)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 16,
          "title": "Segment 16 (pages 133-143)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 13,
          "title": "Segment 13 (pages 119-126)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 57,
          "title": "Segment 57 (pages 549-558)",
          "relevance_score": 0.52,
          "method": "api"
        }
      ]
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 337-346)",
      "start_page": 337,
      "end_page": 346,
      "detection_method": "topic_boundary",
      "content": "making them ideal for specialized domains that require advanced reasoning,\ninference, and explainability.\nFigure 10.4 Graph Representation of data: Nodes represent entities such as \"Roberto\" and\n\"InterMilan,\" while edges like `hasOccupation` and `isFanOf` depict their relationships.\nUnlike relational databases, graph databases do not follow a universal\nstandard. Some use the Resource Description Framework (RDF) to represent\ndata as “triples”—a Subject-Predicate-Object format. For instance, in RDF:\n(Roberto, hasOccupation, SoftwareDeveloper)\n(Roberto, isFanOf, InterMilan)\n(InterMilan, playsIn, SerieA)\nThis triple structure might look like:\n@prefix ex: <http://example.org/> .\n@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\nex:Roberto rdf:type ex:Person .\nex:Roberto ex:hasOccupation ex:SoftwareDevelopment .\nex:Roberto ex:isFanOf ex:InterMilan .\n\n\nex:InterMilan rdf:type ex:SoccerTeam .\nex:InterMilan ex:playsIn ex:SerieA .\nex:SerieA rdf:type ex:SoccerLeague .\nHowever, only some graph DBs use RDF. Others use proprietary graph\nrepresentations and query languages like Cypher (for Neo4j) or Gremlin,\nrather than RDF and SPARQL.\nAs you can see in the figure 10.4, the graph data structure enables powerful,\nflexible representations of relationships that are difficult to capture with\ntraditional databases.\nThis versatility makes graph DBs ideal for knowledge-intensive applications\nwhere deep understanding of relationships and reasoning is essential.\nGraph databases have been around since the early 2000s, with Neo4j among\nthe first. While they offer powerful, flexible ways to represent and query\ninformation, their complexity can be a hurdle. Recently, large language\nmodels (LLMs) have made graph DBs more accessible by enhancing several\nkey functions:\nEntity and Relationship Extraction: LLMs can pull entities,\nrelationships, and even full graphs from unstructured text, allowing you\nto store this data directly in a graph DB.\nAutomated Query Generation: LLMs can generate complex Cypher or\nSPARQL queries from natural language questions, easing a task\ntraditionally challenging for less experienced developers. To accomplish\nthis, use a carefully crafted few-shot prompt with examples, which\nworks best with a high-accuracy LLM like GPT-4o.\nNatural Language Answers: LLMs can convert query results (e.g., in\nRDF) into natural language responses. This process involves feeding the\ninitial question, the generated Cypher or SPARQL query, and the results\ninto a dedicated prompt, which a lower-cost LLM like GPT-4o-mini can\nhandle effectively.\nFigure 10.5 below shows the resulting Knowledge Graph RAG architecture,\nalso known as KG-RAG, which closely resembles the setup used for vector\n\n\nstore based RAG.\nFigure 10.5 Knowledge Graph RAG Architecture (KG-RAG): Similar to vector-store-based RAG\nsetups, but using a SPARQL generator. The generator converts a natural language question into\nSPARQL, which is executed on the Knowledge Graph database. The retrieved graph data is then\nprovided to the LLM, along with the original question, to synthesize the answer.\n\n\n1b. The user question is also fed to the\nprompt sent to the LLM to synthesize the\nanswer\n\ner i laser\n\n§PARQL\ngenerator\n\nuser question\n\n1a, The user question is\n\nfed to. a SPARQL\n\nUser generator which uses an\nLLM to generate a SPARQL\nknowledge graph query sag\n\n2. The retriever executes\nthe SPARQL query on |\nthe Knowledge Graph QL trtsesetsesesenes '  4,The chatbot creates a\nDB prompt with the user\nquestion and the retrieved\ngraphs (the context)\n\n3. The Knowledge Graph\n\nDB retuns relevant 9. The LLM accepts the\n\nbranches of the graph prompt and synthetizes a\ncorresponding\ncompletion (the\nresponse)\n\nLangChain supports several graph DBs, including Neo4j and Amazon\nNeptune. While this book does not cover Knowledge Graph RAG in detail, I\nrecommend consulting LangChain’s documentation and examples.\nLangChain also offers practical blog posts, such as a collaborative piece with\nNeo4j on building DevOps RAG applications with knowledge graphs:\nhttps://blog.langchain.dev/using-a-knowledge-graph-to-implement-a-devops-\nrag-application/.\nBelow is a sample prompt template for generating Cypher queries, taken\nfrom LangChain’s Neo4j QA Chain:\nCYPHER_GENERATION_TEMPLATE = \"\"\"Task:Generate Cypher statement to \nInstructions:\nUse only the provided relationship types and properties in the sc\nDo not use any other relationship types or properties that are no\nSchema:\n{schema}\nNote: Do not include any explanations or apologies in your respon\nDo not respond to any questions that might ask anything else than \nDo not include any text except the generated Cypher statement.\nExamples: Here are a few examples of generated Cypher statements \n# How many people played in Top Gun?\nMATCH (m:Movie {{title:\"Top Gun\"}})<-[:ACTED_IN]-()\nRETURN count(*) AS numberOfActors\nThe question is:\n{question}\"\"\"\nGraph DBs are evolving to meet new LLM-driven use cases, giving rise to\nKnowledge Graph Embeddings. This approach enriches knowledge graphs\nwith textual descriptions and embeddings, supporting semantic search as a\ncomplement to traditional graph queries. For further reading on this\ntechnique, see this Nature article: https://www.nature.com/articles/s41598-\n023-38857-5. For a comprehensive guide, I recommend Knowledge Graph\nEnhanced RAG by Tomaž Bratanič and Oskar Hane.\nThese tools allow you to leverage the structured knowledge of graph DBs,\ncombined with the adaptability of LLMs, for powerful retrieval-augmented\ngeneration solutions.\n10.6 Chain Routing\n\n\nAn application’s content might reside in multiple storage types, not just\nvector stores for unstructured text. You may also use relational databases for\nstructured data, document databases for semi-structured content, and\nknowledge graph databases for entity relationships. Additionally, the\napplication might need to connect to different LLMs depending on the task,\nas some LLMs are optimized or more cost-effective for specific functions. As\na result, the RAG architecture often branches into a tree structure, with each\nbranch tailored to specific types of queries or tasks, as shown in figure 10.6.\nFigure 10.6 Complex RAG architecture with branching pathways, each optimized for specific\ntasks, such as answering questions about tourist Destinations (from a vector store) or\nAccommodation offers (from a relational SQL DB).\n\n\n1b. The user question is also fed to the\nprompt sent to the LLM to synthetize the\nanswer\n\nuser question S050 cee seeeseseseescescsssssnsucssessesssssecccece}acd\nser question\nQuestion\nrouter a (\n1a. The user question is\nfedtoa question router, scbsccessccsbossscessaszazecs al mada (ue ssscdeosseocssed >\nUser\nrelevant retriever ! ‘cer nae\n2.The selected question\nhandler generates the en SOL Generator\nfelated wey\nPrompt\n3.The chosen retriever\nwl ae Vector store 4.The chosen'retriever\nteioiarl Wiereaiia retriever feeds context to the prompt\nsent to the LLM to\nsynthetize the answer\n\n9. The LLM accepts the\n\nprompt and synthetizes a\ncorresponding\nrelevant completion (the\ncontext response)\n\nSen tk\n\nThis figure illustrates a RAG setup with several branches, each designed to\nhandle different application tasks. For example, one branch could address\nfactual questions about tourist destinations, while another branch handles\nquestions about available accommodation offers.\nSuppose a user asks about a tourist destination. In that case, you’d likely\nroute this query to a RAG chain based on a vector store. If the user’s question\nis about accommodation offers, you might route it to a RAG chain connected\nto the UkBooking database introduced earlier.\nTo route each question to the correct chain, you use a routing chain. This\nchain analyzes the question to determine the best-suited chain for handling it.\nLet’s go over the implementation of a routing chain for this purpose.\n10.6.1 Setting Up Data Retrievers\nTo streamline setup, we’ll reuse the vector store and relational database\nconfigurations from previous sections. Import the necessary libraries:\nfrom typing import Literal\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom pydantic import BaseModel, Field\nfrom langchain.schema.runnable import RunnableLambda\nNow create the corresponding retriever chains:\ntourist_info_retriever_chain = RunnableLambda(lambda x: x['questi\nuk_accommodation_retriever_chain =  full_sql_gen_chain | sql_quer\nThese retriever chains direct questions to the appropriate data source. Next,\nwe’ll build a router to direct user questions to one of these retriever chains.\n10.6.2 Setting Up the Query Router\nWe’ll implement a question router using an LLM. The LLM will analyze\neach question and determine the best retriever chain based on its content. The\n\n\nprompt will specify the function of each retriever: the vector store for general\ntourist information and the relational database for accommodation bookings.\nThe router function, defined in listing 10.11, binds the LLM’s response to a\ntyped object, instantiating the datasource attribute with either\n\"tourist_info_store\" or \"uk_booking_db\" depending on the question’s\nintent.\nListing 10.11 Routing the query to the correct retriever\nclass RouteQuery(BaseModel):\n   \"\"\"Route a user question to the most relevant datasource.\"\"\"\n   datasource: Literal[\"tourist_info_store\", \"uk_booking_db\"] = \n       ...,\n       description=\"Given a user question, route it either to a \n   )\nllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4o-min\nstructured_llm_router = llm.with_structured_output(RouteQuery)  #A\nsystem = \"\"\"You are an expert at routing a user question to a tou\nor to an UK accommodation booking relational database.\nThe vector store contains tourist information about UK destinatio\nUse the vectorstore for general tourist information questions on \nFor questions about accommodation availability or booking, use th\nroute_prompt = ChatPromptTemplate.from_messages(\n   [\n       (\"system\", system),\n       (\"human\", \"{question}\"),\n   ]\n)\nquestion_router = route_prompt | structured_llm_router\nThis setup enables the LLM to intelligently route each question to the\nappropriate data source, improving response accuracy based on question\nintent.\nTesting the Router Chain\nLet’s test the router chain with a question about tourist information and\nanother about accommodation booking:\n\n\nselected_data_source = question_router.invoke(\n   {\"question\": \"Have you got any offers in Brighton?\"}\n)\nprint(selected_data_source)\nExpected output:\ndatasource='uk_booking_db'\nThen test with a tourist-related question:\nselected_data_source = question_router.invoke(\n   {\"question\": \"Where are the best beaches in Cornwall?\"}\n)\nprint(selected_data_source)\nExpected output:\ndatasource='tourist_info_store'\nThe router correctly identifies the appropriate data source!\nSetting Up the Retriever Chooser\nNow, let’s implement a function to select the correct retriever based on the\nchosen data source ('uk_booking_db' or 'tourist_info_store').\nListing 10.12 Retriever Chooser Function\nretriever_chains = {\n   'tourist_info_store': tourist_info_retriever_chain,\n   'uk_booking_db': uk_accommodation_retriever_chain\n}\ndef retriever_chooser(question):\n   selected_data_source = question_router.invoke(\n       {\"question\": question})\n   return retriever_chains[selected_data_source.datasource]\nLet’s test the retriever chooser function with a sample question:\n",
      "page_number": 337,
      "chapter_number": 39,
      "summary": "This chapter covers segment 39 (pages 337-346). Key topics include questions, question, and retrieved. Figure 10.4 Graph Representation of data: Nodes represent entities such as \"Roberto\" and\n\"InterMilan,\" while edges like `hasOccupation` and `isFanOf` depict their relationships.",
      "keywords": [
        "Knowledge Graph RAG",
        "Knowledge Graph",
        "question",
        "Graph",
        "user question",
        "LLM",
        "retriever",
        "SPARQL knowledge graph",
        "Graph RAG",
        "RAG",
        "Graph RAG architecture",
        "Knowledge",
        "Chain",
        "graph DBs",
        "data"
      ],
      "concepts": [
        "questions",
        "question",
        "retrieved",
        "retrieval",
        "graph",
        "chain",
        "generation",
        "generate",
        "generated",
        "generator"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "Segment 32 (pages 270-277)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 11,
          "title": "Segment 11 (pages 88-98)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Microservice APIs Using Python Flask FastAPI",
          "chapter": 25,
          "title": "Segment 25 (pages 240-252)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Software Architecture",
          "chapter": 20,
          "title": "Segment 20 (pages 181-194)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 10,
          "title": "Segment 10 (pages 81-88)",
          "relevance_score": 0.61,
          "method": "api"
        }
      ]
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 347-354)",
      "start_page": 347,
      "end_page": 354,
      "detection_method": "topic_boundary",
      "content": "chosen = retriever_chooser('Tell me about events or festivals in \nprint(chosen)\nExpected output:\nfirst=RunnableLambda(lambda x: x['question']) last=VectorStoreRet\nThe output confirms that the correct retriever chain instance is selected based\non the question's intent. This setup ensures the question is routed to the best-\nmatched data source for accurate results.\n10.6.3 Integrating the Chain Router into a Full RAG Chain\nThe final step is to integrate the chain router into a complete RAG chain,\nenabling the workflow of retriever selection, query execution, and answer\nsynthesis. See listing 10.13 for an example.\nListing 10.13 Full RAG Chain for Routing, Retrieval, and Answer Synthesis\nfrom langchain_core.runnables import RunnablePassthrough\nrag_prompt_template = \"\"\"\nGiven a question and some context, answer the question.\nIf you get a structured context, like a tuple, try to infer the m\ntypically they refer to accommodation offers, and the number is a \nIf you do not know the answer, just say I do not know.\nContext: {context}\nQuestion: {question}\n\"\"\"\nrag_prompt = ChatPromptTemplate.from_template(rag_prompt_template\ndef execute_rag_chain(question, chosen_retriever):\n   full_rag_chain = (\n       {\n           \"context\": {\"question\": RunnablePassthrough()} | chos\n           \"question\": RunnablePassthrough(),  #B\n       }\n       | rag_prompt\n       | llm\n\n\n       | StrOutputParser()\n   )\n   return full_rag_chain.invoke(question)\nLet’s test the RAG chain with both an accommodation query and a tourist\ninformation query.\nExample: Asking about Accommodation Offers\nquestion = 'Give me some offers for Cardiff, including the accomm\nchosen_retriever = retriever_chooser(question)\nanswer = execute_rag_chain(question, chosen_retriever)\nExpected output:\nOne offer for Cardiff is the \"Early Bird Discount\" at Cardiff Cam\nExample: Asking about Tourist Information\nquestion_2 = 'Tell me about events or festivals in the UK town of \nchosen_retriever_2 = retriever_chooser(question_2)\nanswer2 = execute_rag_chain(question_2, chosen_retriever_2)\nExpected output:\nIn Newquay, the **Cornish Film Festival** is held annually each N\nIn both cases, the RAG chain correctly routes each question to the\nappropriate retriever, performs the retrieval, and synthesizes a coherent\nanswer by feeding the context and original question to the LLM.\nWe’re nearing the end of our exploration of advanced RAG techniques. One\nmore topic remains: retrieval post-processing, which focuses on refining the\nchunks sent as context to the LLM, further optimizing the relevance and\nclarity of responses.\n\n\n10.7 Retrieval Post-Processing\nAfter using the techniques discussed so far to improve the effectiveness and\naccuracy of RAG retrieval, you’ll likely have a list of document chunks (or\nnodes) from the content store. Before passing these to the LLM for answer\nsynthesis, you may want to perform post-processing to filter out less relevant\ncontent, ensuring the LLM produces a concise and accurate response, as\nshown in figure 10.7.\nFigure 10.7 Retrieval Post-processing: Retrieved chunks from the vector store are filtered to\nremove irrelevant content, ensuring only high-quality chunks are sent to the LLM for answering\nthe user’s question.\n\n\nIn the following sections, I’ll introduce some key post-processing techniques.\n10.7.1 Similarity Postprocessor\nA straightforward way to reduce the number of chunks returned by a\nsimilarity retriever (often a vector-based retriever using semantic distance) is\nto apply a cutoff to similarity scores. Chunks below a specific similarity\nscore, or above a certain distance, are discarded.\n\n\nIn LangChain, you can set this similarity threshold before executing the\nsearch by instantiating a “score threshold” similarity retriever from the vector\nstore. Set the search_type to \"similarity_score_threshold\" and specify\nthe threshold in search_kwargs:\nscore_threshold_similartity_retriever = vector_store.as_retriever\n   search_type=\"similarity_score_threshold\", search_kwargs={\"sco\n)\nAfter instantiating this retriever, you can execute the search with its\nget_relevant_documents() method:\ndoc_chunks = score_threshold_similartity_retriever.get_relevant_d\nThis retrieves only documents with similarity scores above the specified\nthreshold, ensuring that only highly relevant content is passed to the LLM.\n10.7.2 Keyword Postprocessors\nAnother post-processing approach is to filter retrieved document chunks by\nkeywords. This allows you to include or exclude chunks based on the\npresence of specific terms.\nWhile LangChain does not provide a built-in keyword post-processor, you\ncan implement one in Python as follows (for demonstration purposes):\nselected_chunks = [c for c in chunks \n            if set(c.split()).intersection(required_keywords)\n           and not set(c.split()).intersection(excluded_keywords\nThis code filters chunks to include only those containing required_keywords\nand excludes any that contain excluded_keywords.\n10.7.3 Time Weighting\nYou might also want to prioritize chunks based on how recently they were\naccessed. To do this, include a last_accessed_at timestamp in each chunk’s\nmetadata and update it with each access:\n[Document(page_content='this is some content of a chunk', metadat\n\n\nOnce timestamps are in place, you can use a\nTimeWeightedVectorStoreRetriever, which applies a time decay factor to\nrank chunks by both similarity score and recency:\nretriever = TimeWeightedVectorStoreRetriever(\n   vectorstore=vectorstore, \n   decay_rate=1e-10, k=3\n)\nThe TimeWeightedVectorStoreRetriever adjusts similarity scores based on\nrecency:\nadjusted_similarity_score = similarity_score + (1.0 - decay_rate) \nThis adjusted score allows recent, relevant content to rank higher, ensuring\nmore timely responses. Next, we’ll discuss one of the most essential post-\nprocessing techniques.\n10.7.4 RAG Fusion (Reciprocal Rank Fusion)\nIn the previous chapter, we discussed “multiple query generation,” where\nmultiple queries are generated from a user’s question, and a subset of relevant\nresults is selected from the combined results across all queries. Using\nLangChain’s MultiQueryRetriever, we automated this process to select the\nmost relevant answers. However, if you want finer control over result\nranking, consider an approach called Reciprocal Rank Fusion (RRF).\nRRF or Reciprocal Rank Fusion, detailed by Cormack, Clarke, and Buttcher\nin their paper \"Reciprocal Rank Fusion Outperforms Condorcet and\nIndividual Rank Learning Methods\"\n(https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf), reranks\nretrieved documents based on a specific scoring formula:\nrrfscore = 1 / (rank + k)\nwhere:\nrank: the document's current rank based on similarity or relevance.\nk: a smoothing constant to control the weight of existing ranks.\n\n\nAs each document is processed, its RRF score accumulates across all\ngenerated queries. Once all scores are calculated, documents are reranked by\ntheir cumulative RRF scores, and the top-ranked results are sent to the LLM\nfor answer synthesis.\nYou can get a visual idea of how the overall process work by following\nthrough figure 10.8.\nFigure 10.8 Reciprocal Rank Fusion Workflow: Multiple queries are generated from the initial\nuser question. Each query retrieves a set of results, such as from a vector store. All results are\nthen reranked using RRF scores, with only the top results sent to the LLM for final answer\nsynthesis.\n\n\nAt a high level, implementing RAG fusion involves generating multiple\nqueries, as shown in chapter 9, and ranking the results with the RRF\nalgorithm. This retrieval workflow can then be embedded within a larger\nRAG chain, as demonstrated in previous examples.\nGenerating Multiple Queries\n",
      "page_number": 347,
      "chapter_number": 40,
      "summary": "One\nmore topic remains: retrieval post-processing, which focuses on refining the\nchunks sent as context to the LLM, further optimizing the relevance and\nclarity of responses Key topics include retrieval, retrieved, and retrieves.",
      "keywords": [
        "Reciprocal Rank Fusion",
        "RAG",
        "RAG Chain",
        "Full RAG Chain",
        "question",
        "Rank Fusion",
        "chain",
        "retriever",
        "Reciprocal Rank",
        "llm",
        "rank",
        "Similarity",
        "chunks",
        "score",
        "RAG Fusion"
      ],
      "concepts": [
        "retrieval",
        "retrieved",
        "retrieves",
        "question",
        "rank",
        "chunks",
        "scores",
        "scoring",
        "rag",
        "fusion"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 40,
          "title": "Segment 40 (pages 367-374)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 19,
          "title": "Segment 19 (pages 157-164)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 25,
          "title": "Segment 25 (pages 495-517)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 38,
          "title": "Segment 38 (pages 344-356)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 26,
          "title": "Segment 26 (pages 518-535)",
          "relevance_score": 0.61,
          "method": "api"
        }
      ]
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 355-362)",
      "start_page": 355,
      "end_page": 362,
      "detection_method": "topic_boundary",
      "content": "You can generate multiple queries from an initial question using a chain, as\nshown in Chapter 8. For convenience, the setup is repeated here in Listing\n10.14.\nListing 10.14 Multi-Query Generation\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom typing import List\nfrom langchain_core.output_parsers import BaseOutputParser\nfrom pydantic import BaseModel, Field\nmulti_query_gen_prompt_template = \"\"\"\nYou are an AI language model assistant. Your task is to generate \ndifferent versions of the given user question to retrieve relevan\ndatabase. By generating multiple perspectives on the user questio\nthe user overcome some of the limitations of the distance-based s\nProvide these alternative questions separated by newlines.\nOriginal question: {question}\n\"\"\"\nmulti_query_gen_prompt = ChatPromptTemplate.from_template(multi_q\nclass LineListOutputParser(BaseOutputParser[List[str]]):\n   \"\"\"Parse out a question from each output line.\"\"\"\n   def parse(self, text: str) -> List[str]:\n       lines = text.strip().split(\"\\n\")\n       return list(filter(None, lines))  \nquestions_parser = LineListOutputParser()\nllm = ChatOpenAI(model=\"gpt-4o\", openai_api_key=OPENAI_API_KEY)\nmulti_query_gen_chain = multi_query_gen_prompt | llm | questions_\nWith this setup, you can now generate multiple alternative queries from a\nsingle question, helping to capture varied perspectives and nuances that\nimprove document retrieval accuracy.\nNote\nI’ve chosen to use gpt-4o instead of gpt-4o-mini, as it is more likely to\n\n\nproduce higher-quality queries and generate more accurate, well-synthesized\nresponses\nNow that multiple queries can be generated, the next step is to implement a\nranking mechanism to sort and prioritize the retrieved results.\nReciprocal Rank Fusion Algorithm\nThe core of this workflow is the Reciprocal Rank Fusion (RRF) algorithm,\nwhich assigns scores to documents retrieved by multiple queries. Using the\nRRF formula, each document is scored based on its rank, then reranked by\ntotal RRF score. See Listing 10.15 for implementation details.\nListing 10.15 Reciprocal Rank Fusion (RRF) Algorithm\ndef reciprocal_rank_fusion(results_groups: list[list], k=60):  #A\n   \"\"\" Reciprocal_rank_fusion that takes multiple groups of rank\n       and an optional parameter k used in the Reciprocal Rank F\n   indexed_results = {}  #B\n   \n   for group_id, results_group in enumerate(results_groups):  #C\n       for local_rank, doc in enumerate(results_group):\n           indexed_results[(group_id, local_rank)] = doc\n   \n   fused_scores = {}  #D\n   \n   for key, doc in indexed_results.items():  #E\n       group_id, local_rank = key\n       if key not in fused_scores:\n           fused_scores[key] = 0  #F\n       \n       doc_current_score = fused_scores[key]        \n       fused_scores[key] += 1 / (local_rank + k)  #G\n   reranked_results = [  #H\n       (indexed_results[key], score)\n       for key, score in sorted(fused_scores.items(), key=lambda \n   ]\n   return reranked_results\n\n\nWith the RRF algorithm in place, let’s create a RAG Fusion retrieval chain,\nas shown below:\nretriever = uk_with_metadata_collection.as_retriever(search_kwarg\ntop_three_results = RunnableLambda(lambda x: x[0:3])  #A\nrag_fusion_retrieval_chain = multi_query_gen_chain | retriever.ma\n       \ndocs = rag_fusion_retrieval_chain.invoke({\"question\": question})  \nlen(docs)\nThe final step is to integrate this RAG Fusion retrieval chain into a larger\nRAG chain for end-to-end question routing, retrieval, and answer synthesis.\nIncorporating RAG Fusion into the RAG Chain\nAs we’ve seen before, integrating a retrieval chain into a broader RAG chain\nis straightforward. For completeness, Listing 10.16 shows how to incorporate\nthe RAG Fusion retrieval chain within a RAG chain.\nListing 10.16 Integrating the RAG Fusion Retrieval Chain into a RAG Chain\nrag_prompt_template = \"\"\"\nGiven a question and some context, answer the question.\nIf you do not know the answer, just say I do not know.\nContext: {context}\nQuestion: {question}\n\"\"\"\nrag_prompt = ChatPromptTemplate.from_template(rag_prompt_template\nrag_chain = (\n   {\n       \"context\": {\"question\": RunnablePassthrough()} | rag_fusi\n       \"question\": RunnablePassthrough(),  #B\n   }\n   | rag_prompt\n   | llm\n   | StrOutputParser()\n)\nNow, let’s test the complete RAG chain with an example question:\n\n\nuser_question = \"Can you give me some tips for a trip to Brighton\nanswer = rag_chain.invoke(user_question)\nprint(answer)\nExpected output:\nHere are some tips for a trip to Brighton:\n1. **Visit During Festivals**: If you can, plan your visit in May \n2. **Enjoy the Beach**: Brighton boasts a beautiful stretch of sh\n3. **Explore Local Culture**: Brighton has a vibrant cultural sce\n4. **Transportation**: Brighton is well-connected by train, makin\n5. **Seasonal Work Opportunities**: If you're looking for tempora\n6. **Plan for All Budgets**: Whether you're looking for budget op\n7. **Stay Safe**: Like any city, it's important to stay aware of \n8. **Local Council Resources**: Check out the Brighton and Hove C\nEnjoy your trip!\nCongratulations! You’ve completed a comprehensive guide to advanced\nRAG techniques, making you well-equipped to tackle complex RAG tasks.\n10.8 Summary\nUse structured data from SQL and document databases to enhance LLM\nresponses, complementing unstructured data in vector stores.\nStructured queries are often required for these databases, and LLMs can\ngenerate them from natural language inputs.\nVector stores support dense searches with embeddings and sparse\nsearches using keyword-based indexing.\nKeywords for sparse searches can be added via metadata, extracted with\nalgorithms like TF-IDF, or generated by the LLM.\nLLMs can also generate metadata queries directly from user questions.\nMany LLMs can translate user questions into SQL queries, enabling\n\n\ndirect access to relational databases.\nSQL-based retrievals rely on exact matching, but semantic SQL expands\nsearches to find approximate matches aligned with user intent.\nGraph databases represent data as nodes and relationships, making them\nideal for complex, interconnected information.\nGraph databases excel in LLM use cases requiring advanced reasoning\nand relationship understanding, as they can be used to create knowledge\ngraphs with the help of LLMs or answer questions based on knowledge\ngraphs through KG-RAG.\nContent for LLM applications often spans vector stores, relational\ndatabases, document stores, and knowledge graphs.\nRouting chains ensure queries are directed to the appropriate storage\ntype for optimized retrieval.\n- Reciprocal Rank Fusion (RRF) improves query relevance by reranking\ndocuments from multiple queries using cumulative scores.\nRRF enhances answer quality by feeding the most relevant results to the\nLLM for synthesis.\n\n\n11 Building Tool-based Agents with\nLangGraph\nThis chapter covers\nBuilding LLM-powered agents using LangGraph\nRegistering and using tools for dynamic agent execution\nDebugging agent execution and tool calls\nSimplifying agents with pre-built LangGraph components\nObserving agent execution with LangSmith\nIn chapter 5, you explored the distinction between agentic workflows and\nagents. You learned that agentic workflows are fundamentally deterministic:\ntheir logic is based on flows with conditional paths that depend on the current\napplication state. These workflows can be elegantly modeled using node-\nbased graphs in LangGraph, and you saw a complete, hands-on example of\nsuch a system.\nAgents, however, operate differently. Rather than following a predetermined\nflow, agents rely on dynamic, context-sensitive decision-making. With the\nhelp of a language model (LLM), an agent chooses which tools to use—and\nin what order—based on the evolving context of the task at hand. These\ndecisions aren’t pre-scripted; instead, they unfold step by step, as the agent\ncontinually evaluates the outputs of previous actions and adapts accordingly.\nIn this chapter, you’ll put these ideas into practice by building a multi-tool\ntravel information agent. We’ll begin simply, implementing an agent that\nprovides destination information using a single tool. From there, we’ll extend\nit into a true multi-tool agent, able to answer questions about both travel\ndestinations and their current weather conditions.\nAs we progress, I’ll introduce you to the core concepts necessary for\nconstructing multi-tool agents, with particular focus on the tool-calling\nprotocol. You’ll first implement this protocol from scratch to grasp every\n\n\ndetail, then see how LangGraph’s built-in capabilities can streamline and\nsimplify your agent’s architecture.\nThis chapter’s multi-tool agent will serve as the foundation for the more\nadvanced, multi-agent systems you’ll build in the chapters ahead.\nLet’s dive in—there’s a lot to discover.\n11.1 Starting Simple: Building a Single-Tool Travel\nInfo Agent\nIn this section, we'll lay the groundwork for our agent-based applications by\nbuilding a straightforward travel information agent. This first agent will use\njust one tool: a vector store retriever that answers questions about Cornwall’s\ndestinations and resorts, leveraging content from wikivoyage.org. The\ncontent is split into chunks and stored in a vector store for efficient retrieval.\nIf you’ve followed the advanced RAG (Retrieval-Augmented Generation)\nchapters earlier in this book, you should already be comfortable with\nsourcing content and populating a vector store. Here, we’ll build on that\nfoundation, keeping the focus on agent mechanics.\n11.1.1 Project Setup\nLet’s set up a new Python project using VS Code (this works seamlessly with\nCursor as well).\nCreate a virtual environment and install dependencies\nFirst, set up your Python virtual environment and install all necessary\ndependencies. You’ll find the requirements.txt file either on the Manning\nwebsite for this book or in the cloned GitHub repository that accompanies\nchapter 11.\nOpen a new PowerShell terminal in VS Code (from the menu: Terminal >\nNew Terminal), navigate to the ch11 project folder, then create and activate a\n\n\nnew virtual environment:\nPS C:\\Github\\building-llm-applications\\ch11> python -m venv env_c\nPS C:\\Github\\building-llm-applications\\ch11> .\\env_ch11\\Scripts\\a\n(env_ch11) PS C:\\Github\\building-llm-applications\\ch11>\nOnce your environment is activated, install the required dependencies:\n(env_ch11) PS C:\\Github\\building-llm-applications\\ch11> pip insta\nYour project environment is now ready for development.\nAdd your OpenAI API key\nCreate a .env file in your project root and add your OpenAI API key:\nOPENAI_API_KEY=<Your OPENAI_API_KEY>\nConfigure VS Code debugging\nFor smooth debugging, add the following launch.json to your .vscode\ndirectory:\n{\n   \"version\": \"0.2.0\",\n   \"configurations\": [\n       {\n           \"name\": \"Python Debugger: Current File\",\n           \"type\": \"debugpy\",\n           \"request\": \"launch\",\n           \"program\": \"${file}\",\n           \"console\": \"integratedTerminal\"\n       }\n   ]\n}\nOrganize your implementation files:\nWe’ll use the naming convention main_x_y.py for our scripts. The “x”\nrepresents the feature (e.g., main_1_1.py for the initial travel info agent,\nmain_2_1.py when adding weather), while “y” is the version of that feature\n",
      "page_number": 355,
      "chapter_number": 41,
      "summary": "This chapter covers segment 41 (pages 355-362). Key topics include agents, question, and questions. Your task is to generate \ndifferent versions of the given user question to retrieve relevan\ndatabase.",
      "keywords": [
        "Reciprocal Rank Fusion",
        "RAG Fusion retrieval",
        "Fusion retrieval chain",
        "RAG",
        "RAG Fusion",
        "RAG chain",
        "Rank Fusion",
        "Reciprocal Rank",
        "Fusion",
        "chain",
        "Rank",
        "question",
        "Fusion retrieval",
        "agent",
        "retrieval chain"
      ],
      "concepts": [
        "agents",
        "question",
        "questions",
        "retrieve",
        "retrieval",
        "retrieved",
        "llm",
        "listing",
        "based",
        "rag"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 25,
          "title": "Segment 25 (pages 495-517)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 19,
          "title": "Segment 19 (pages 157-164)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 20,
          "title": "Segment 20 (pages 165-173)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 48,
          "title": "Segment 48 (pages 973-990)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 27,
          "title": "Segment 27 (pages 233-241)",
          "relevance_score": 0.53,
          "method": "api"
        }
      ]
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 363-371)",
      "start_page": 363,
      "end_page": 371,
      "detection_method": "topic_boundary",
      "content": "as we iterate. This will make it easy for you to compare versions and follow\nthe progression of the implementation.\nNote\nThe code in these examples is intentionally simplified to focus on core\nfunctionality. Error handling and defensive programming are omitted for\nclarity and learning purposes.\n11.1.2 Loading Environment Variables\nOnce your .env file is ready, you can load your API key at the top of your\nscript as follows (see Listing 11.1):\nListing 11.1 Loading environment variables\nload_dotenv() #A\n11.1.3 Preparing the Travel Information Vector Store\nTo enable our travel information agent to answer queries about Cornwall\ndestinations, we first need a way to store and efficiently retrieve relevant\ninformation. The approach here draws on techniques you saw in the\nRetrieval-Augmented Generation (RAG) section, but streamlines them for\nthis agent-centric context. At a high level, we’ll download travel content for a\nset of Cornwall destinations from WikiVoyage, break the text into\nmanageable chunks, embed those chunks into vector representations, and\nthen store everything in a Chroma vector store. We’ll also encapsulate the\ninitialization logic in a singleton pattern to ensure the vector store is only\nbuilt once during the agent’s lifetime.\nThe following code (listing 11.2) sets up this vector store, making it easy to\nretrieve relevant travel information as the agent operates.\nListing 11.2 Preparing the Travel Information Vector Store\nUK_DESTINATIONS = [ #A\n   \"Cornwall\",\n\n\n   \"North_Cornwall\",\n   \"South_Cornwall\",\n   \"West_Cornwall\",\n]\nasync def build_vectorstore(destinations: Sequence[str]) -> Chrom\n   \"\"\"Download WikiVoyage pages and create a Chroma vector store\n   urls = [f\"https://en.wikivoyage.org/wiki/{slug}\" for slug in \n   loader = AsyncHtmlLoader(urls) #C\n   print(\"Downloading destination pages ...\") #C\n   docs = await loader.aload() #C\n   splitter = RecursiveCharacterTextSplitter(chunk_size=1024, ch\n   chunks = sum([splitter.split_documents([d]) for d in docs], [\n   print(f\"Embedding {len(chunks)} chunks ...\") #E\n   vectordb_client = Chroma.from_documents(chunks, embedding=Ope\n   print(\"Vector store ready.\\n\")\n   return vectordb_client #F\n# Singleton pattern (build once)\n_ti_vectorstore_client: Chroma | None = None #G\ndef get_travel_info_vectorstore() -> Chroma: #H\n   global _ti_vectorstore_client\n   if _ti_vectorstore_client is None:\n       if not os.environ.get(\"OPENAI_API_KEY\"):\n           raise RuntimeError(\"Set the OPENAI_API_KEY env variab\n       _ti_vectorstore_client = asyncio.run(build_vectorstore(UK_\n   return _ti_vectorstore_client #I\nti_vectorstore_client = get_travel_info_vectorstore() #J\nti_retriever = ti_vectorstore_client.as_retriever() #K\nThis setup starts by defining a list of Cornwall-related destinations that serve\nas the basis for information retrieval. The build_vectorstore()\nasynchronous function constructs URLs for each destination and uses an\nasynchronous loader to fetch the corresponding WikiVoyage pages. Once the\npages are downloaded, the text is split into overlapping chunks to ensure that\ninformation remains contextually meaningful. These chunks are then\nembedded using OpenAI’s embedding models and stored in a Chroma vector\nstore, making them quickly searchable by semantic similarity.\nTo prevent unnecessary recomputation and data downloads, the singleton\n\n\npattern is used for the vector store client. The\nget_travel_info_vectorstore() function ensures that the vector store is\nonly built once and is reused for all future retrievals. At the end, the vector\nstore client is instantiated, and a retriever object is created from it—this\nretriever will be the agent’s interface for accessing Cornwall travel\ninformation. This foundation allows the agent to efficiently answer user\nqueries about destinations, using up-to-date knowledge sourced directly from\nWikiVoyage.\n11.2 Enabling Agents to Call Tools\nNow that we have our vector store retriever ready, the next step is to expose\nthis retrieval capability as a tool the agent can use. This is where the concept\nof tool calling—a major advance in modern agent frameworks—enters the\npicture.\n11.2.1 From Function Calling to Tool Calling\nLLMs like those from OpenAI initially introduced function calling: a\nmechanism that allows the model to request specific functions, passing\nstructured arguments, based on the needs of a given prompt. This concept\nquickly evolved into the more general tool calling protocol, now widely\nsupported by major LLM providers. With tool calling, models can invoke not\njust custom functions but also a variety of built-in or external “tools,”\nhandling everything from code execution to external API lookups.\nThis evolution has made agent implementations dramatically simpler and\nmore robust, particularly when using the ReAct (Reasoning and Acting)\ndesign pattern. The ReAct pattern, introduced by Yao et al. in their 2022\npaper \"ReAct: Synergizing Reasoning and Acting in Language Models\",\nenables LLMs to interleave reasoning steps (“thoughts”) with tool\ninvocations (“actions”). In effect, the model can break a task into smaller\npieces, use tools where appropriate, and synthesize an answer step by step.\nHere’s a simplified view of the ReAct pattern as a process diagram (see\nfigure 11.1):\n\n\nFigure 11.1. The ReAct pattern alternates between reasoning (“Re”) and action (“Act”), enabling\nthe agent to process a user question by thinking, calling tools as needed, and following up with\nfurther reasoning before delivering the final answer.\n\n\nThe ReAct pattern, as illustrated in the diagram, begins with a user question\nthat initiates the process. The agent first enters a Reasoning phase (labeled as\n“Re: Reasoning - Thought”), where the language model considers the user’s\ninput and determines the necessary next steps. If additional information or\nactions are needed, the agent transitions to the Action phase (“Act: Action -\nTool Call”), where it calls one or more tools—such as performing a semantic\nsearch or invoking an API.\nAfter the tool call is completed, the agent returns to another Reasoning phase\n(“Re: Reasoning - Follows Up”), in which it incorporates the results from the\ntool into its ongoing line of reasoning. This may lead to further tool calls, or\nthe agent may determine that it now has enough information to formulate a\nresponse. Finally, the process concludes with the agent providing a Final\nAnswer to the user.\nThis stepwise interplay between reasoning (“Re”) and acting (“Act”)\nembodies the core of the ReAct pattern, allowing agents to dynamically\ncombine thought and action as they work toward solving the user’s query.\nNote\nWith the introduction of the OpenAI Responses API, the transition from\nfunction calling to tool calling has accelerated. The Responses API is\ndesigned specifically for structured tool use, and has largely superseded the\nolder Completion API for agentic applications.\n11.2.2 How Tool Calling Works with LLMs\nOpenAI’s tool calling supports several types of tools—including user-defined\nfunctions, code interpreters, and native capabilities like web browsing. At a\nhigh level, when you register a tool with the LLM, you expose both the\nfunction signature (name and parameters) and a textual description. The\nmodel then decides, at runtime, when and how to use each tool based on the\nuser's input and the context of the conversation.\nIn LangGraph, tools can be registered using either a class-based or decorator-\nbased approach. We’ll use the decorator-based approach for clarity and\n\n\nconciseness.\nLet’s implement our semantic search tool as a function decorated with @tool,\nmaking it available to the agent for tool calling, as shown in listing 11.3:\nListing 11.3 LangGraph attribute-based tool definition\n@tool #A\ndef search_travel_info(query: str) -> str: #B\n   \"\"\"Search embedded WikiVoyage content for information about d\n   docs = ti_retriever.invoke(query) #C\n   top = docs[:4] if isinstance(docs, list) else docs #C\n   return \"\\n---\\n\".join(d.page_content for d in top) #D\nThis decorated function, search_travel_info(), is now recognized as a\ntool: it takes a user query, searches the vector store for relevant WikiVoyage\ncontent, and returns up to four top results as a single string. The @tool\ndecorator ensures that the function’s name, description, and parameter\nschema are all available to the LLM for tool calling.\n11.2.3 Registering Tools with the LLM\nTo enable the agent to use our semantic search tool, we must register it with\nthe LLM so that the model knows both the function's signature and its\nintended purpose. In LangChain, this is achieved using the bind_tools\nprotocol, but it is instructive to see how this works at the OpenAI API level\nfirst.\nWith the introduction of the OpenAI Responses API, the way tools (and\nfunctions) are registered has become more standardized and explicit. Now,\nyou provide a structured definition of your tool—specifying its name,\ndescription, and parameter schema. The model can then return responses that\nexplicitly request tool invocations, passing arguments as needed. For further\ndetails, you can consult the OpenAI function calling documentation\n(https://platform.openai.com/docs/guides/function-calling?api-\nmode=responses).\nExample: Manual Tool Registration with OpenAI API\n\n\nSuppose you wanted to expose our search_travel_info function directly to\nthe OpenAI API (without using LangChain). You would define the tool’s\nschema as shown in Listing 11.4. Note that this example is for illustration\nonly and is not included in the book’s provided source code.\nListing 11.4 Manual tool registration with OpenAI API\nsearch_travel_info_tool = {\n   \"type\": \"function\",  #A\n   \"function\": {\n       \"name\": \"search_travel_info\",  #B\n       \"description\": \"Search embedded WikiVoyage content for in\n       \"parameters\": {  #D\n           \"type\": \"object\",  #E\n           \"properties\": {\n               \"query\": {  #F\n                   \"type\": \"string\",\n                   \"description\": \"A natural language query abou\n               }\n           },\n           \"required\": [\"query\"]  #H\n       }\n   }\n}\n# Register the tool (function) when making a chat completion requ\nresponse = client.chat.completions.create(\n   model=\"gpt-4o\",  #I\n   messages=[\n       {\"role\": \"user\", \"content\": \"Tell me about surfing in Cor\n   ],\n   tools=[search_travel_info_tool],  #K\n   tool_choice=\"auto\",  # let the model decide when to use the t\n)\nIn this example, you explicitly define the tool’s metadata and parameters, and\nthen pass it in the tools argument of your API call. When the model decides it\nneeds to call search_travel_info, it will return a structured tool call in its\nresponse. Your application must then handle the invocation of the Python\nfunction, passing the model-generated arguments, and send the results back\nto the LLM if the conversation continues.\nRegistering Tools in LangChain\n\n\nLangChain automates much of this process. You simply define your tool as a\ndecorated Python function, and then register it with the LLM. You can see in\nlisting 11.5 how this looks in code:\nListing 11.5 Registering tools in LangChain\nTOOLS = [search_travel_info] #A\nllm_model = ChatOpenAI(temperature=0, model=\"gpt-4.1-mini\", #B\n                      use_responses_api=True) #B\nllm_with_tools = llm_model.bind_tools(TOOLS) #C\nHere, we list the available tools (currently just search_travel_info),\ninstantiate the GPT-4.1-mini chat model (with Responses API support), and\nuse .bind_tools(TOOLS) to expose those tools to the model for tool calling.\nLangChain handles the translation between your Python code and the\nOpenAI function/tool-calling protocol, including automatically generating the\nappropriate JSON schema from your function signature and docstring.\nThis setup ensures that whenever the model receives a user message, it can\nautonomously decide if and when to call any registered tool, structuring its\nresponses according to the tool calling protocol.\nNote\nIf you’re using a model other than OpenAI, or you prefer not to use the\nResponses API, tool calling can still work—though the capabilities and\nstructure of the responses may differ. The older Completion API is still\navailable, but for most agentic use cases, the newer Responses API is\nrecommended for its clarity, power, and alignment with evolving best\npractices.\n11.2.4 Agent State: Tracking the Conversation\nIn our implementation, the agent’s state is simply a collection of LLM\nmessages that track the entire conversation. This is defined as follows:\nclass AgentState(TypedDict):\n",
      "page_number": 363,
      "chapter_number": 42,
      "summary": "This chapter covers segment 42 (pages 363-371). Key topics include tools, functionality, and function. Covers function. Note\nThe code in these examples is intentionally simplified to focus on core\nfunctionality.",
      "keywords": [
        "tool",
        "tool calling",
        "Vector Store",
        "API",
        "Chroma vector store",
        "Responses API",
        "OpenAI Responses API",
        "agent",
        "Travel",
        "function",
        "calling",
        "OpenAI API",
        "Vector",
        "Store",
        "Information Vector Store"
      ],
      "concepts": [
        "tools",
        "functionality",
        "function",
        "functions",
        "agent",
        "models",
        "api",
        "queries",
        "query",
        "calling"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 27,
          "title": "Segment 27 (pages 222-231)",
          "relevance_score": 0.72,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 23,
          "title": "Segment 23 (pages 197-207)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 18,
          "title": "Segment 18 (pages 147-158)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 49,
          "title": "Segment 49 (pages 993-1010)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Effective-Python",
          "chapter": 11,
          "title": "Segment 11 (pages 99-114)",
          "relevance_score": 0.66,
          "method": "api"
        }
      ]
    },
    {
      "number": 43,
      "title": "Segment 43 (pages 372-379)",
      "start_page": 372,
      "end_page": 379,
      "detection_method": "topic_boundary",
      "content": "   messages: Annotated[Sequence[BaseMessage], operator.add]\nHere, AgentState only contains the sequence of messages exchanged\nbetween the user, the agent, and any tool responses. This keeps the design\nsimple.\n11.2.5 Executing Tool Calls\nThe core of the tool execution logic is implemented in a node that examines\nthe LLM’s most recent message, extracts any tool calls requested by the\nmodel, and invokes the corresponding functions with the provided arguments.\nEach tool’s output is then wrapped in a message and appended to the\nconversation state.\nA simplified implementation is shown in listing 11.6.\nListing 11.6 Tool execution implemented from scratch\nclass ToolsExecutionNode: #A\n   \"\"\"Execute tools requested by the LLM in the last AIMessage.\"\n   def __init__(self, tools: Sequence): #B\n       self._tools_by_name = {t.name: t for t in tools}\n   def __call__(self, state: dict): #C\n       messages: Sequence[BaseMessage] = state.get(\"messages\", [\n       last_msg = messages[-1] #D\n       tool_messages: list[ToolMessage] = [] #E\n       tool_calls = getattr(last_msg, \"tool_calls\", []) #F\n       \n       for tool_call in tool_calls: #G\n           tool_name = tool_call[\"name\"] #H\n           tool_args = tool_call[\"args\"] #I\n           tool = self._tools_by_name[tool_name] #J\n           result = tool.invoke(tool_args) #K\n           tool_messages.append(\n               ToolMessage(\n                   content=json.dumps(result), #L\n                   name=tool_name,\n                   tool_call_id=tool_call[\"id\"],\n               )\n           )\n\n\n       return {\"messages\": tool_messages} #M\n   \ntools_execution_node = ToolsExecutionNode(TOOLS) #N\nThis pattern allows the agent to handle multiple tool calls in a single step.\nThe tools execution node inspects the LLM’s latest response, invokes each\nrequested tool by name, collects the results, and formats them for the next\nstage in the conversation. In a typical agent workflow, these tool results are\nsent back to the LLM, which incorporates the new information to either\nreason further or generate a direct answer to the user’s query.\nAlthough I’ve shown you how to implement tool calling from scratch for\nlearning purposes, you usually won’t need to write this logic yourself in\npractice, as shown in the sidebar.\nThe Built-in ToolNode Class\nLangGraph provides a built-in class, ToolNode, which performs the same\nfunction as our custom ToolsExecutionNode. For most applications, you can\nuse:\ntools_execution_node = ToolNode(TOOLS)\nThis saves you from having to implement the tools execution logic manually,\nstreamlining your agent development process.\nNow that you understand how tool execution is managed, let’s explore how\nthe agent, guided by the LLM, determines which tools to call—or whether it\nalready has enough information to generate the final answer.\n11.2.6 The LLM Node: Coordinating Reasoning and Action\nNext, we add an LLM node to our LangGraph workflow, as shown in listing\n11.7.\nListing 11.7 LLM Node\ndef llm_node(state: AgentState): #A    \n   \"\"\"LLM node that decides whether to call the search tool.\"\"\"\n\n\n   current_messages = state[\"messages\"] #B\n   respose_message = llm_with_tools.invoke(current_messages) #C\n   return {\"messages\": [respose_message]} #D\nThis node takes the agent state (a list of messages) and forwards them to the\nLLM for processing. Because we are using an LLM with tool calling enabled\n(llm_with_tools), the model automatically understands when to issue tool\ncalls and when to produce a final answer:\nIf the last message is a user question, the LLM can decide to request a\ntool call (or multiple calls) to retrieve relevant information.\nIf the last message(s) are tool results, the LLM integrates those\nresponses, reasons further, and may either produce a final answer or\nrequest additional tool calls if needed.\nThe model returns either an AIMessage with a final answer in the content\nfield, or additional tool calls in the tool_calls field, depending on the\nsituation. This flexible, reactive loop is at the heart of all modern agent\nimplementations—and it’s what makes today’s LLM-based agents so\ncapable.\n11.3 Assembling the Agent Graph\nWith our LLM node and tool execution node ready, the next step is to\nassemble them into a working agent graph. In LangGraph, an agent is\nstructured as a directed graph, where each node represents a component (such\nas an LLM or a tool executor), and edges represent the possible flow of data\nand control between these nodes.\nThe code in Listing 11.8 demonstrates how we assemble our single-tool\ntravel information agent as a graph. Each node in the graph corresponds to\neither the LLM (language model) or the tool execution logic.\nListing 11.8 Graph of the tool-based agent\nbuilder = StateGraph(AgentState) #A\nbuilder.add_node(\"llm_node\", llm_node) #B\nbuilder.add_node(\"tools\", tools_execution_node) #B\n\n\nbuilder.add_conditional_edges(\"llm_node\", tools_condition) #C\nbuilder.add_edge(\"tools\", \"llm_node\") #D\nbuilder.set_entry_point(\"llm_node\") #E\ntravel_info_agent = builder.compile() #F\n11.3.1 Understanding the Agent Graph Structure\nOur agent graph consists of two main nodes, as shown in figure 11.2: the\nLLM node, responsible for reasoning and generating tool calls, and the tools\nnode, responsible for executing the requested tools and returning results. The\nflow of the conversation alternates between these two nodes, reflecting the\nReAct pattern described earlier in this chapter.\nFigure 11.2 Conditional graph logic routes each user query through the LLM node, then\ndynamically directs flow to the tools node or ends the process, depending on whether tool calls\nare required\n\n\nA critical aspect of this setup is the conditional edge that connects the LLM\nnode to the next step. This is controlled by the tools_condition function—a\nprebuilt utility in LangGraph. This function examines the latest message from\nthe LLM:\nIf the message contains the tool_calls property (meaning the LLM is\nrequesting one or more tool invocations), the graph routes the flow to\nthe node named \"tools\".\nIf there are no tool calls present, the flow is directed to the END node,\nterminating the graph and producing a final answer for the user.\nThis mechanism lets the agent dynamically decide, at each turn, whether to\ncontinue reasoning, take action, or conclude the interaction.\nWe explicitly set the entry point of the graph to \"llm_node\", ensuring that\neach user question is first processed by the language model. (As an\nalternative, you could achieve the same effect by adding an edge from the\nSTART node to \"llm_node\" with graph_builder.add_edge(START,\n\"llm_node\").)\nBy compiling the graph with builder.compile(), we finalize our travel\ninformation agent, ready to receive queries and intelligently use its tool to\nfind relevant travel information.\n11.4 Running the Agent Chatbot: The REPL Loop\nThe final step in building our agent-powered chatbot is to implement the user\ninterface—a simple loop that continuously accepts user questions and returns\nanswers, until the user chooses to exit. This classic pattern, known as a REPL\n(Read-Eval-Print Loop), is the main bridge between the user and your travel\ninformation agent.\nAt a high level, the chat loop listens for input, wraps the user’s question in a\nmessage structure, invokes the agent graph, and prints the assistant’s reply.\nThis interaction continues indefinitely, enabling a true conversational\nexperience.\n\n\nListing 11.9 shows how you can implement this chat loop in Python:\nListing 11.9 Chatbot REPL loop\ndef chat_loop(): #A\n   print(\"UK Travel Assistant (type 'exit' to quit)\")\n   while True:\n       user_input = input(\"You: \").strip() #B\n       if user_input.lower() in {\"exit\", \"quit\"}: #C\n           break\n       state = {\"messages\": [HumanMessage(content=user_input)]} \n       result = travel_info_agent.invoke(state) #E\n       response_msg = result[\"messages\"][-1] #F\n       print(f\"Assistant: {response_msg.content}\\n\") #G\n           \nif __name__ == \"__main__\":\n   chat_loop()            \nThis loop welcomes the user, waits for their question, and continues\nprocessing until the user types “exit” or “quit.” Each input is packaged as a\nHumanMessage and passed to the travel information agent you just built. The\nagent’s reply is extracted from the graph’s output and displayed back to the\nuser.\nWith this in place, you are now ready to run your first agent-based chatbot.\nTry asking it about destinations or activities in Cornwall—and experience\nhow the agent reasons, retrieves information, and converses, all within the\nframework you’ve just constructed, as we are about to see.\n11.5 Executing a Request\nNow that your travel information agent is ready, let’s step through the agent\nin action by running and debugging your implementation. This hands-on\nwalkthrough will show you how the agent orchestrates the flow between the\nLLM and the tool, helping you see the LangGraph framework in motion.\n11.5.1 Step-by-Step Debugging\nBegin by opening your main_01_01.py file and running it in debug mode,\nusing the Python Debug configuration you set up in your launch.json\n\n\nearlier. To trace the agent’s flow, place breakpoints at the beginning of the\nfollowing functions:\nsearch_travel_info()\nToolsExecutionNode.__call__()\nllm_node()\nReady? Hit F5 (or click the play icon in your IDE), and let’s walk through the\nagent’s workflow together.\nVector Store Creation\nWhen you start the script, you’ll see the vector store being created and\npopulated with travel information. In your debug console, you’ll see the\noutput similar to that in figure 11.3.\nFigure 11.3 Output at startup, during vector store creation\nChatbot Loop Launch\nNext, chatbot loop starts up, waiting for your input:\nUK Travel Assistant (type 'exit' to quit)\nYou:\nEnter your question and press Enter:\n",
      "page_number": 372,
      "chapter_number": 43,
      "summary": "11.2.5 Executing Tool Calls\nThe core of the tool execution logic is implemented in a node that examines\nthe LLM’s most recent message, extracts any tool calls requested by the\nmodel, and invokes the corresponding functions with the provided arguments Key topics include tool, agent, and messages.",
      "keywords": [
        "LLM",
        "tool",
        "LLM Node",
        "node",
        "Tool Calls",
        "agent",
        "travel information agent",
        "Calls",
        "Graph",
        "Agent Graph",
        "user",
        "tool execution",
        "messages",
        "loop",
        "information agent"
      ],
      "concepts": [
        "tool",
        "agent",
        "messages",
        "node",
        "loop",
        "listing",
        "user",
        "state",
        "builder",
        "graph"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "Segment 32 (pages 270-277)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "Segment 28 (pages 232-240)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 15,
          "title": "Segment 15 (pages 119-130)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 26,
          "title": "Segment 26 (pages 214-221)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 29,
          "title": "Segment 29 (pages 241-252)",
          "relevance_score": 0.59,
          "method": "api"
        }
      ]
    },
    {
      "number": 44,
      "title": "Segment 44 (pages 380-388)",
      "start_page": 380,
      "end_page": 388,
      "detection_method": "topic_boundary",
      "content": "You: Suggest three towns with a nice beach in Cornwall\nLLM Node Activation\nYour breakpoint inside llm_node() will trigger. Inspect the state.messages\nproperty:\nHumanMessage(content='Suggest three towns with a nice beach in Co\nStep over (F10) to the next line to send this message to the LLM. Since the\nLLM is configured for tool calling, examine the resulting response_message:\n  AIMessage(content=[],\n...\ntool_calls=[\n{'name': 'search_travel_info', 'args': {'query': 'beach towns in \n{'name': 'search_travel_info', 'args': {'query': 'best beaches in \n{'name': 'search_travel_info', 'args': {'query': 'top seaside tow\n...)\nHere, the LLM has generated three tool calls—each targeting your semantic\nsearch tool with a slightly different query. Notice how the model rewrites the\nqueries to maximize information coverage, as discussed in the Read-Rewrite\npattern from the Advanced RAG section. You do not need to manually\nhandle query rewriting: the LLM handles it.\nTools Execution Node\nContinue execution (F5). The list of tool calls is added to the message list.\nThe conditional edge’s tools_condition detects tool calls, routing execution to\nToolsExecutionNode.__call__(). Your breakpoint will trigger there.\nInspect state.messages:\n  [\nHumanMessage(content='Suggest three towns with a nice beach in Co\nAIMessage(content=[], ... ,\ntool_calls=[\n{'name': 'search_travel_info', 'args': {'query': 'beach towns in \n{'name': 'search_travel_info', 'args': {'query': 'best beaches in \n{'name': 'search_travel_info', 'args': {'query': 'top seaside tow\n\n\n]\nThe last message (from the LLM) has no content (the LLM hasn’t answered\nyet because it needs more information), but it contains the tool calls that must\nbe executed. The node extracts these, then iterates through each one,\nextracting the tool name and arguments, and invokes the corresponding tool:\nresult = tool.invoke(tool_args)\nStep through this to watch search_travel_info() execute. Each semantic\nsearch result (a document returned from the vector store) is collected as a\nToolMessage and added to the list for the next LLM step.\nTool Call Results Passed Back to LLM\nContinue (F5), and you’ll return to llm_node(). Now, state.messages\ncontains your original question, the LLM’s tool call instructions, and the\nresults from each tool execution:\n  \\[HumanMessage(content='Suggest three towns with a nice beach \n  AIMessage(content=\\[], ...\n  tool\\_calls=\\[{'name': 'search\\_travel\\_info', 'args': {'query\n  ToolMessage(content='...', name='search\\_travel\\_info', tool\\_\n  ToolMessage(content='...', name='search\\_travel\\_info', tool\\_\n  ToolMessage(content='...', name='search\\_travel\\_info', tool\\_\nThe content of each tool message gives the LLM the facts it needs to\nsynthesize a final answer. Step over the following line:\nresponse\\_message = llm\\_with\\_tools.invoke(current\\_messages)\nInspect response_message:\nAIMessage(content=\\[{'type': 'text', 'text': \"Three towns in Cornw\nNow, the content field is populated with the LLM’s answer. The\ntool_calls field is gone—the LLM no longer needs external tools and has\nsynthesized a response.\nCompleting the Request\n\n\nAs execution leaves llm_node(), the tools_condition on the conditional\nedge checks for further tool calls. Finding none, it ends the conversation.\nWhen your main invocation returns:\nresult = travel\\_info\\_agent.invoke(state)\nThe result.messages list concludes with the final AIMessage containing the\nanswer:\n  {'messages': [\n  HumanMessage(content='Suggest three towns with a nice beach in \n  AIMessage(content=[],  tool_calls=[...]),\n  ToolMessage(...), ToolMessage(...), ToolMessage(...),\n  AIMessage(content=[{'type': 'text', 'text': \"Three towns in Co\n  ]\nThe chatbot will now display the answer and prompt for your next question:\nUK Travel Assistant (type 'exit' to quit)\nYou: Suggest three towns with a nice beach in Cornwall\nAssistant: [{'type': 'text', 'text': \"Three towns in Cornwall wit\nYou:\nYou have now observed, step by step, how your agent processes a user\nrequest: interpreting the question, generating tool calls, executing them, and\nsynthesizing a final, well-informed answer. This debug-driven walkthrough\noffers deep insight into the mechanics of agentic reasoning and action in\nLangGraph.\n11.6 Expanding Your Agent: Adding a Weather\nForecast Tool\nSo far, our agent has been able to answer travel-related queries using\nsemantic search over curated travel content. But real-world travel advice\noften depends on dynamic, real-time information—like the weather! In this\nsection, you’ll learn how to extend your agent by adding a second tool,\nenabling it to respond with context-aware answers based on current\nconditions.\n\n\n11.6.1 Implementing a Mock Weather Service\nTo illustrate the process, start by copying your main_01_01.py file to a new\nscript called main_02_01.py. This will help you track each evolutionary step\nin your agent’s development.\nFirst, let’s introduce a mock weather service. This service will simulate real-\ntime weather data for any given town, returning both a weather condition\n(such as “sunny” or “rainy”) and a temperature. You can see the\nimplementation of the WeatherForecastService in listing 11.10:\nListing 11.10 WeatherForecastService\nclass WeatherForecast(TypedDict):\n   town: str\n   weather: Literal[\"sunny\", \"foggy\", \"rainy\", \"windy\"]\n   temperature: int\nclass WeatherForecastService:\n   _weather_options = [\"sunny\", \"foggy\", \"rainy\", \"windy\"]\n   _temp_min = 18\n   _temp_max = 31\n   @classmethod\n   def get_forecast(cls, town: str) -> Optional[WeatherForecast]\n       weather = random.choice(cls._weather_options)\n       temperature = random.randint(cls._temp_min, cls._temp_max\n       return WeatherForecast(town=town, weather=weather, temper\nThis mock service chooses a random weather condition and temperature\nwithin a typical summer range for Cornwall. Later, you can swap this out for\na real-world weather API if desired.\n11.6.2 Creating the Weather Forecast Tool\nWith the mock service in place, the next step is to create a tool that wraps it,\nmaking weather data available to the agent. Here’s how you can define the\ntool function and add it to your agent’s toolkit:\n@tool\n\n\ndef weather_forecast(town: str) -> dict:\n   \"\"\"Get a mock weather forecast for a given town. Returns a We\n   forecast = WeatherForecastService.get_forecast(town)\n   if forecast is None:\n       return {\"error\": f\"No weather data available for '{town}'\n   return forecast\nThe weather_forecast tool provides a mock weather forecast for a given\ntown. When called with a town name, it returns a dictionary containing the\nweather conditions (such as sunny, foggy, rainy, or windy) and temperature\nfor that location. If no data is available, it returns an error message instead.\nThis tool allows the agent to incorporate simulated real-time weather\ninformation into its responses.\n11.6.3 Updating the Agent for Multi-Tool Support\nFinally, make sure your LLM model is set up to use tool calling and\nrecognizes both available tools:\nTOOLS = [search_travel_info, weather_forecast] #A\nllm_model = ChatOpenAI(\n   temperature=0,\n   model=\"gpt-4.1-mini\",   #B\n   use_responses_api=True  #B\n)\nllm_with_tools = llm_model.bind_tools(TOOLS) #C\nWith this setup, your agent is now equipped to handle both travel queries and\nweather checks, giving it the foundation to provide much more accurate and\nuseful responses. In the next sections, you’ll see how to guide the LLM to use\nboth tools effectively, and observe the agent’s new capabilities in action.\nThis workflow not only shows how to extend your agent’s toolset, but also\ndemonstrates the modular, incremental approach that makes agentic systems\nwith LangGraph and LangChain so powerful.\n11.7 Executing the Multi-Tool Agent\nWith your weather forecast tool registered, your agent can now synthesize\n\n\nanswers that combine travel information and real-time weather conditions.\nThe beauty of this modular approach is that adding a new tool requires no\nchanges to your agent’s graph structure. All orchestration is handled by the\nLLM and the tool calling protocol.\n11.7.1 Running the Multi-Tool Agent (Initial Behavior)\nLet’s put the new capabilities to the test. Set the same breakpoints as before\n—especially in the tool execution and LLM nodes—and add a breakpoint at\nthe start of weather_forecast().\nRun main_02_01.py in debug mode (F5) and enter the following prompt:\nYou: Suggest two Cornwall beach towns with nice weather\nIncorrect Behavior: LLM Defaults to Internal Knowledge\nAfter submitting your query, your first breakpoint will hit in llm_node().\nStep through to the last line and inspect the value of\nresponse_message.tool_calls:\n[{'name': 'weather_forecast', 'args': {'town': 'Newquay'}, 'id': \n{'name': 'weather_forecast', 'args': {'town': 'Falmouth'}, 'id': \nHere, the LLM has simply picked two towns it “knows” (Newquay and\nFalmouth) and asked for their weather, without consulting your semantic\nsearch tool at all. This is typical of a model using its internal knowledge base\nrather than the tools provided—something we want to avoid for reliability\nand accuracy.\nWhy does this happen? The LLM is acting on its pre-training and defaulting\nto what it already “knows” about Cornwall, rather than querying your up-to-\ndate data.\n11.7.2 Improving LLM Tool Usage with System Guidance\nTo nudge the LLM towards tool use and away from hallucinations, let’s make\nits instructions and tool descriptions clearer. Make a copy of your script as\n\n\nmain_02_02.py, and update the tool definitions:\n@tool(description=\"Search travel information about destinations i\ndef search_travel_info(query: str) -> str: \n   ...\n@tool(description=\"Get the weather forecast, given a town name.\")\ndef weather_forecast(town: str) -> dict:\n   ...\nNext, introduce a guiding SystemMessage in your llm_node():\nsystem_message = SystemMessage(content=\"You are a helpful assista\ncurrent_messages.append(system_message)\nYou can see the amended llm_node() function in listing 11.11:\nListing 11.11 Guiding tool selection\ndef llm_node(state: AgentState): #A    \n   \"\"\"LLM node that decides whether to call the search tool.\"\"\"\n   current_messages = state[\"messages\"] #B\n   system_message = SystemMessage(content=\"You are a helpful ass\n   current_messages.append(system_message) #D\n   respose_message = llm_with_tools.invoke(current_messages) #E\n   return {\"messages\": [respose_message]} #F\nRestart your application in debug mode and ask:\nYou: Suggest two Cornwall beach towns with nice weather\nAt your first breakpoint in llm_node(), examine current_messages before\nand after appending the system message. Then, check respose_message—\nnow, you’ll see the following tool call:\n{'name': 'search_travel_info', 'args': {'query': 'beach towns in \nWhat does this mean?\nThe LLM is now forced to use your semantic search tool for candidate beach\ntowns, and won’t pick them “from its own knowledge.” This minimizes\n\n\nhallucinations and guarantees the data used comes from your knowledge\nbase.\nContinue stepping through the code (F5). Right before you submit the\ncurrent_messages to the LLM again, your messages will look like:\n[HumanMessage(content='Suggest 2 Cornwall beach towns with nice w\nSystemMessage(content='You are a helpful assistant that can searc\nAIMessage(content=[], ..., tool_calls=[{'name': 'search_travel_in\nToolMessage(content='\"<p id=\\\\\"mwrg\\\\\">Cornwall, in particular New\nSystemMessage(content='You are a helpful assistant that can searc\nFrom the ToolMessage, the LLM now receives a list of possible beach towns\nfrom your vector store.\nNext, as you step through and reach llm_node() again, the LLM will issue\ncalls for the weather forecast in two of these towns:\nAIMessage(content=[], ..., tool_calls=[\n{'name': 'weather_forecast', 'args': {'town': 'Newquay'}, 'id': '\n{'name': 'weather_forecast', 'args': {'town': 'St Ives'}, 'id': '\nThe towns selected by the LLM might differ in your run, but they’ll always\ncome from the results of your semantic search tool.\nStep through the weather tool calls—each ToolMessage you inspect should\nlook like:\nToolMessage(content='{\"town\": \"Newquay\", \"weather\": \"foggy\", \"tem\nToolMessage(content='{\"town\": \"St Ives\", \"weather\": \"windy\", \"tem\nWhat If the Weather Isn’t Good?\nSuppose the weather is less than ideal in those towns. Continue to the next\nLLM response and you’ll see:\nAIMessage(content=[], ...,  \ntool_calls=[\n{'name': 'weather_forecast', 'args': {'town': 'Perranporth'}, 'id\n{'name': 'weather_forecast', 'args': {'town': 'Falmouth'}, 'id': \nHere, the LLM is asking for the weather in two more towns, likely because\n\n\nthe previous results didn’t meet the “nice weather” requirement. This process\ncontinues until the agent finds two towns with suitable conditions.\nAfter the final tool calls, the LLM generates a synthesized, fact-based\nanswer:\nAIMessage(content=[{'type': 'text', 'text': 'Two beach towns in C\nWhich appears to the user as:\nUK Travel Assistant (type 'exit' to quit)\nYou: Suggest two Cornwall beach towns with nice weather\nAssistant: [{'type': 'text', 'text': 'Two beach towns in Cornwall \nIn summary, by enhancing tool descriptions and providing explicit\ninstructions via system prompts, you can guide the LLM to chain tool use in a\nmulti-step, fact-grounded workflow—first searching for beach towns, then\nfiltering by real-time weather—producing answers that are both dynamic and\nreliable.\nExercise\nTry replacing the mock weather tool with a real API, like OpenWeatherMap,\nusing LangChain’s OpenWeatherMap integration. This will make your agent\ntruly real-time!\n11.8 Using Pre-Built Components for Rapid\nDevelopment\nUp to this point, you’ve built an agent from the ground up: wiring together\nthe graph, orchestrating tool calls, and stepping through every detail in the\ndebugger. You now have a solid grasp of how tool calling works under the\nhood.\nHowever, for most production scenarios, you’ll want to reduce boilerplate\nand move faster—while still retaining transparency and observability when\nneeded. The LangGraph library provides pre-built agent components, such as\nthe react agent, that encapsulate much of this orchestration logic for you.\n",
      "page_number": 380,
      "chapter_number": 44,
      "summary": "This chapter covers segment 44 (pages 380-388). Key topics include tool, weather, and message.",
      "keywords": [
        "LLM",
        "tool",
        "Weather",
        "towns",
        "Cornwall beach towns",
        "beach towns",
        "content",
        "tool calls",
        "search",
        "agent",
        "Forecast",
        "travel",
        "calls",
        "Weather Forecast",
        "LLM Node Activation"
      ],
      "concepts": [
        "tool",
        "weather",
        "message",
        "towns",
        "agent",
        "llm",
        "travel",
        "query",
        "queries",
        "calling"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents In Action",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 467-474)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "Segment 28 (pages 232-240)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 31,
          "title": "Segment 31 (pages 262-269)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "Segment 32 (pages 270-277)",
          "relevance_score": 0.6,
          "method": "api"
        }
      ]
    },
    {
      "number": 45,
      "title": "Segment 45 (pages 389-396)",
      "start_page": 389,
      "end_page": 396,
      "detection_method": "topic_boundary",
      "content": "Let’s see how dramatically you can simplify your agent by switching to a\npre-built approach.\n11.8.1 Refactoring to Use the LangGraph React Agent\nStart by copying your previous script (main_02_02.py) to a new file:\nmain_03_01.py.\nImport the RemainingSteps Utility\nAt the top of your script, add the following import:\nfrom langgraph.managed.is_last_step import RemainingSteps\nRemove Manual Tool Binding\nYou can now delete the line where you bound tools to the LLM:\nllm_with_tools = llm_model.bind_tools(TOOLS)\nThe pre-built agent will handle this for you internally.\nUpdate the AgentState\nModify your AgentState definition to include a remaining_steps field. This\nfield allows the agent to manage how many tool-calling rounds are left in a\ncontrolled way:\nclass AgentState(TypedDict): \n   messages: Annotated[Sequence[BaseMessage], operator.add]\n   remaining_steps: RemainingSteps \nRemove Node and Graph Construction\nNow for the biggest simplification: delete your custom ToolsExecutionNode,\nllm_node, and any explicit graph wiring code.\n\n\nReplace it all with a single instantiation of the built-in LangGraph React\nagent:\ntravel_info_agent = create_react_agent(\n   model=llm_model,\n   tools=TOOLS,\n   state_schema=AgentState,\n   prompt=\"You are a helpful assistant that can search travel in\nThat’s it! The React agent now orchestrates the flow, tool calling, and\nsynthesis for you.\n11.8.2 Running the Pre-Built Agent\nWhen you run main_03_01.py and ask your usual test question:\nYou: Suggest two Cornwall beach towns with nice weather\nAssistant: [{'type': 'text', 'text': 'Two beach towns in Cornwall \nYou get the correct, grounded answer—with much less code.\n11.8.3 Observing and Debugging with LangSmith\nA common concern when switching to high-level abstractions is loss of\nvisibility: How do you know the agent is actually following the right\nreasoning steps?\nWhile you can still debug tool functions directly, the flow inside the agent\nitself is less exposed.\nThis is where LangSmith comes in. LangSmith enables full tracing and\ninspection of agent behavior, including tool calls, LLM reasoning, and\nintermediate states.\nEnabling LangSmith Tracing\nTo enable tracing, add the following lines to your .env file:\nLANGSMITH_TRACING=true\n\n\nLANGSMITH_ENDPOINT=\"https://api.smith.langchain.com\"\nLANGSMITH_API_KEY=\"<your-langsmith-api-key>\"\nLANGSMITH_PROJECT=\"langchain-in-action-react-agent\"\nAfter re-running your application and submitting a question, you can log into\nLangSmith https://smith.langchain.com):\n1. Select Tracing projects in the Observability menu (left-hand sidebar).\n2. Click your project (langchain-in-action-react-agent) in the right-\nhand panel.\n3. Open the latest trace.\nYou’ll see the full execution trace, as shown in figure 11.4.\nFigure 11.4 LangSmith agent execution trace: this trace visualizes each step of the agent’s\nworkflow—including LLM calls, tool invocations, and message flow—when answering the\nquestion, “Suggest two Cornwall beach towns with nice weather.”\n\n\nTRACE 0 (A) LangCraph 70\n= Waterfall @ ,*\nRun Feedback Metadata\n[A LangGraph ©\n0582s ©1267\nInput v\nBD agent 282s\nBD call.model 2.8% —\nG Punnablesequence 2.79s V Suggest two Cornwall beach towns with nice weather\nBD Prompt 0.00s\nG) chatopenal gpt4:-mini. 278s\nOutput v\nQ should continue 0,00s\nB tons 067s ¥ HUMAN\nQ search_travel_info 0.66s Vv Suggest two Cornwall beach towns with nice weather\nVectorStoreRetriever 065s\n@ too's o02s vow\nBD weather forecast 0.003\nB tools 002s Vv —-gearch travel_info call_gnt74CvMudg}k9bsLUOFSBUS\nA] weather forecast 0.00s\nquery: beach towns in Cornwall\nBD agent 222s v\nB) call_model 2.21 VY _weather_forecast call_oj6ckdc4LtmegewsvLe2bMwU\nQ Runnablesequence 2.21 v\ntown: St Ives\nB} Prompt 0.005\nGB chatopenal gota 22 weather forecast call_avw57JluKLHw)i3sVxX)kUz9\n\nQ should_continue 0,00s\n\nThe LangSmith graphical trace shows every tool call, every LLM step, and\nthe flow of messages—so even when you use a pre-built agent, you retain the\nability to audit, debug, and understand exactly what the agent is doing at\nevery stage.\nIn summary, by combining pre-built LangGraph agent components with\nLangSmith for observability, you can quickly build robust, production-ready\nagentic applications—while still maintaining full transparency and control\nwhen needed. This workflow has become the foundation for many modern AI\nagent systems in real-world use today.\n11.9 Summary\nLangGraph enables you to build both deterministic workflows and\ndynamic LLM agents using node-based and tool-based architectures.\nYou can register tools—like semantic search and weather APIs—so that\nLLMs can choose and call them step by step.\nBy stepping through code with breakpoints, you can trace agent\nreasoning, tool invocation, and message flow for debugging and\nunderstanding.\nClear tool descriptions and system messages help control LLM behavior\nand reduce hallucination.\nMulti-tool agents can chain together tool outputs (e.g., finding towns,\nthen filtering by weather), all orchestrated by the LLM.\nUsing pre-built agent components, like the LangGraph React agent,\ndramatically reduces boilerplate and speeds up development.\nLangSmith provides rich observability, showing each LLM step and tool\ncall even for high-level agent abstractions.\nThis agent-centric pattern lays the groundwork for more advanced agent\nand multi-agent systems explored in later chapters.\n\n\n12 Multi-agent Systems\nThis chapter covers\nConnecting tools to data sources\nComposing multi-agent systems using router and supervisor patterns\nDebugging, testing, and tracing multi-agent interactions\nIn chapter 11, we explored the foundations of building AI agents by creating\na travel information agent capable of answering user queries about\ndestinations, routes, and transportation options. While a single, specialized\nagent can be powerful, real-world applications often require the coordination\nof multiple agents, each handling a distinct area of expertise. In this chapter,\nwe’ll embark on that journey—transforming our travel information agent into\na robust, multi-agent travel assistant system.\nImagine planning a trip where you not only need up-to-date travel\ninformation but also want to seamlessly book your accommodation. Our\nenhanced multi-agent travel assistant will do just that: it will be able to\nanswer travel questions and help you reserve hotels or bed and breakfasts in\nyour chosen destination. To achieve this, we’ll begin by building a new agent\n—the accommodation booking agent.\nThe accommodation booking agent will empower users to book lodgings\nfrom two different sources. First, it will interface with a local accommodation\ndatabase, which mainly features hotel deals and is exposed via a dedicated\ntool. Second, it will connect to an external B&B REST API, providing access\nto a wider selection of bed and breakfast options, also accessible through its\nown tool. Depending on user requests, the agent will use one or both of these\ntools to deliver relevant accommodation options.\nOnce we have our new agent in place, we’ll combine it with the travel\ninformation agent from the previous chapter. The result will be a unified,\nmulti-agent travel assistant capable of fielding a wide variety of travel-related\nqueries, handling both information requests and accommodation bookings,\n\n\nand even combining both for a more streamlined experience.\nLet’s begin by constructing our new accommodation booking agent.\n12.1 Building an Accommodation Booking Agent\nTo build a practical, helpful travel assistant, we need more than just\ninformation retrieval—we need the ability to act. In this section, we’ll\ndevelop an accommodation booking agent from the ground up, starting by\nbuilding the tools it needs: one for hotel bookings based on a local room\navailability database, and another for B&B bookings from an external REST\nAPI. By the end of this section, you’ll have a ReAct-style agent that can\ncheck and book both hotel and B&B rooms in Cornwall.\n12.1.1 Hotel Booking Tool\nLet’s start by creating the hotel booking tool. To enable our agent to retrieve\nhotel offers and availability, we’ll use the LangChain SQL Database Toolkit,\nwhich exposes a SQL database as a set of agent tools. This toolkit makes it\nstraightforward for an agent to run queries, retrieve hotel details, and check\nroom availability—all through tool calls, without needing to write raw SQL\nin your prompts.\nThe hotel data, including current offers and availability, is stored in a local\nSQLite database cornwall_hotels.db which is kept up-to-date by our\nbackend partners. We don’t need to worry about how the data is pushed—just\ntrust that it’s there and refreshed as needed.\nFirst, copy the latest script, main_03_01.py, to a new script, main_04_01.py.\nThen, prepare your environment:\n1. Create a folder named hotel_db.\n2. Place the provided SQL schema file cornwall_hotels_schema.sql into\nthat folder.\n3. Open a terminal (inside VS Code or standalone), navigate to the folder,\nand create the database with (I have omitted the root of the ch11 folder\nfor convenience):\n\n\n\\ch11>cd hotel_db\n\\ch11>sqlite3 cornwall_hotels.db < cornwall_hotels_schema.sql\nNow, let’s check that the database is working. Open the SQLite shell:\n\\ch11>sqlite3 cornwall_hotels.db\nWithin the SQLite shell, run these queries to verify your setup:\nsqlite> .tables\nsqlite> SELECT * FROM hotels;\nsqlite> SELECT * FROM hotel_room_offers;\nWith the database ready, let’s move on to the Python implementation.\nImport the necessary LangChain SQL integration libraries:\nfrom langchain_community.utilities.sql_database import SQLDatabas\nfrom langchain_community.agent_toolkits import SQLDatabaseToolkit\nInstantiate the SQLite database:\nhotel_db = SQLDatabase.from_uri(\"sqlite:///hotel_db/cornwall_hote\nNow, create an instance of the SQL Database toolkit:\nhotel_db_toolkit = SQLDatabaseToolkit(db=hotel_db, llm=llm_model)\nThat’s it! Now you can access the toolkit’s tools with:\nhotel_db_toolkit_tools = hotel_db_toolkit.get_tools()\n12.1.2 B&B Booking Tool\nNext, let’s create a Bed & Breakfast booking tool. This tool will retrieve\nB&B room availability from a REST service. For development and testing,\nwe’ll mock this service.\nFirst, we’ll define the return type for our tool, and then create a mock\nimplementation of the BnB booking service, as shown in Listing 12.1. (For\n",
      "page_number": 389,
      "chapter_number": 45,
      "summary": "This chapter covers segment 45 (pages 389-396). Key topics include agent, tool, and tracing. While you can still debug tool functions directly, the flow inside the agent\nitself is less exposed.",
      "keywords": [
        "agent",
        "accommodation booking agent",
        "Tool",
        "Cornwall",
        "LLM",
        "hotel",
        "LangSmith",
        "booking agent",
        "React Agent",
        "LangGraph React Agent",
        "travel",
        "booking",
        "Cornwall beach towns",
        "database",
        "Booking Tool"
      ],
      "concepts": [
        "agent",
        "tool",
        "tracing",
        "trace",
        "book",
        "hotels",
        "llm",
        "travel",
        "weather",
        "database"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents In Action",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "Segment 28 (pages 232-240)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 15,
          "title": "Segment 15 (pages 119-130)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 23,
          "title": "Segment 23 (pages 197-207)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 27,
          "title": "Segment 27 (pages 222-231)",
          "relevance_score": 0.52,
          "method": "api"
        }
      ]
    },
    {
      "number": 46,
      "title": "Segment 46 (pages 397-404)",
      "start_page": 397,
      "end_page": 404,
      "detection_method": "topic_boundary",
      "content": "convenience, the example here uses a reduced set of mock data. You can find\nthe complete implementation in the code files provided with this book.).\nListing 12.1 BnB Booking Service\nclass BnBBookingService: #A\n   @staticmethod\n   def get_offers_near_town(town: str, num_rooms: int) -> List[B\n       # Mocked REST API response: multiple BnBs per destination\n       mock_bnb_offers = [ #C\n           # Newquay\n           {\"bnb_id\": 1, \"bnb_name\": \"Seaside BnB\", \"town\": \"New\n           {\"bnb_id\": 2, \"bnb_name\": \"Surfside Guesthouse\", \"tow\n           # Falmouth\n           {\"bnb_id\": 3, \"bnb_name\": \"Harbour View BnB\", \"town\": \n           {\"bnb_id\": 4, \"bnb_name\": \"Seafarer's Rest\", \"town\": \n           # St Austell\n           {\"bnb_id\": 5, \"bnb_name\": \"Garden Gate BnB\", \"town\": \n           {\"bnb_id\": 6, \"bnb_name\": \"Coastal Cottage BnB\", \"tow\n                  ... \n           # Port Isaac\n           {\"bnb_id\": 27, \"bnb_name\": \"Port Isaac View BnB\", \"tow\n           {\"bnb_id\": 28, \"bnb_name\": \"Fisherman's Cottage BnB\", \n           # Fowey\n           {\"bnb_id\": 29, \"bnb_name\": \"Fowey Quay BnB\", \"town\": \n           {\"bnb_id\": 30, \"bnb_name\": \"Riverside Rest BnB\", \"tow\n       ]\n       offers = [offer for offer in mock_bnb_offers if offer[\"tow\n       return offers\nNow we can define the check_bnb_availability tool:\nListing 12.2 BnB Availability tool\n@tool(description=\"Check BnB room availability and price for a de\ndef check_bnb_availability(destination: str, num_rooms: int) -> L\n   \"\"\"Check BnB room availability and price for the requested de\n   offers = BnBBookingService.get_offers_near_town(destination, \n   if not offers:\n       return [{\"error\": f\"No available BnBs found in {destinati\n   return offers\n12.1.3 ReAct Accommodation Booking Agent\n\n\nWith both the hotel and B&B booking tools ready, it’s time to build the\nReAct accommodation booking agent. This agent will use both tools in\nresponse to user requests. If the user doesn’t specify a preference, the agent\nwill search both hotels and B&Bs for available rooms.\nBOOKING_TOOLS = hotel_db_toolkit_tools + [check_bnb_availability] \naccommodation_booking_agent = create_react_agent( #B\n   model=llm_model,\n   tools=BOOKING_TOOLS,\n   state_schema=AgentState,\n   prompt=\"You are a helpful assistant that can check hotel and \n)\nYou can now try out the agent by replacing the agent line in chat_loop()\nwith:\n...\nresult = accommodation_booking_agent.invoke(state) \n...\nLet’s run the main_04_01.py script in debug mode and ask the following\nquestion:\nUK Travel Assistant (type 'exit' to quit)\nYou: Are there any rooms available in Penzance?\nAfter you hit ENTER, you might see an answer similar to the following one:\nI have found Penzance Pier BnB with available rooms at £95 per ro\nFor hotels, Penzance Palace has 3 available rooms with prices of \nAs you can see, the agent used both tools to retrieve results from both the\nhotel database and the mock B&B service.\nAt this point, your accommodation booking agent is working as expected. It’s\nstrongly recommended to debug the execution and inspect the LangSmith\ntraces to better understand how the agent is reasoning and acting step by step.\nAlthough you now have both a travel information agent and an\naccommodation booking agent, they are still disconnected. You can use\n\n\neither one or the other, but not both in a unified experience. In the next\nsection, we’ll build a multi-agent travel assistant that brings these capabilities\ntogether, providing a seamless experience for travel planning and\naccommodation booking.\n12.2 Building a router-based Travel assistant\nSo far, we have developed two independent agents: a travel information agent\nand an accommodation booking agent, each with specialized capabilities.\nWhile this modular approach is powerful, it raises an essential design\nquestion: how can we combine these agents to deliver a seamless user\nexperience—one that can answer travel information queries and handle\naccommodation bookings in a single conversation?\nA common and effective solution is to introduce a router agent. This agent\nacts as an intelligent entry point: it receives the user’s message, determines\nwhich specialized agent should handle the request, and dispatches the task\naccordingly.\n12.2.1 Designing the Router Agent\nTo implement our multi-agent travel assistant, begin by copying your\nprevious script, main_04_01.py, to main_05_01.py. Next, we need to bring in\nsome extra libraries to support graph-based workflows:\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.types import Command\nThe next step is to clearly define the set of agents available for routing. We\ndo this by declaring an enumeration for the two agent types:\nclass AgentType(str, Enum):\n   travel_info_agent = \"travel_info_agent\"\n   accommodation_booking_agent = \"accommodation_booking_agent\"\nTo ensure our router agent receives clear and structured decisions from the\nLLM, we define a Pydantic model that captures the LLM’s output—\nspecifying which agent should handle each query:\n\n\nclass AgentTypeOutput(BaseModel): \n   agent: AgentType = Field(..., description=\"Which agent should \nBy configuring the OpenAI LLM client to produce responses in this\nstructured format, we eliminate any need for string parsing or manual post-\nprocessing. The router will always produce a result of either\n\"travel_info_agent\" or \"accommodation_booking_agent\".\n12.2.2 Routing Logic\nThe heart of the router agent is its system prompt, which concisely instructs\nthe LLM how to classify each user request:\nROUTER_SYSTEM_PROMPT = (\n   \"You are a router. Given the following user message, decide i\n   \"or an accommodation booking question (about hotels, BnBs, ro\n   \"If it is a travel information question, respond with 'travel_\n   \"If it is an accommodation booking question, respond with 'ac\n)\nWith this system prompt, the router agent evaluates each user input and\ndecides which specialist agent should take over. The router is implemented as\nthe entry node of our LangGraph workflow, with the travel information agent\nand the accommodation booking agent as subsequent nodes. You can see the\nrouter implementation in listing 12.2.\nListing 12.3 Router Agent node\ndef router_agent_node(state: AgentState) -> Command[AgentType]:\n   \"\"\"Router node: decides which agent should handle the user qu\n   messages = state[\"messages\"] #A\n   last_msg = messages[-1] if messages else None #B\n   if isinstance(last_msg, HumanMessage): #C\n       user_input = last_msg.content #D\n       router_messages = [ #E\n           SystemMessage(content=ROUTER_SYSTEM_PROMPT),\n           HumanMessage(content=user_input)\n       ]\n       router_response = llm_router.invoke(router_messages) #F\n       agent_name = router_response.agent.value #G\n       return Command(update=state, goto=agent_name) #H\n   \n\n\n   return Command(update=state, goto=AgentType.travel_info_agent\nIf you examine the implementation in listing 12.2, you’ll notice that the\nrouter extracts the user’s message and submits it to the LLM along with the\nsystem prompt. The LLM returns a structured output of type\nAgentTypeOutput, which contains the agent name to which the request\nshould be routed. The router then uses a Command to redirect the conversation\nflow to the selected agent node in the graph. In simple workflows, the\nCommand can hand off the unchanged state to the new node, but it also allows\nfor state updates in more complex flows.\n12.2.3 Building the Multi-Agent Graph\nAt this point, you have all the components needed to assemble the graph-\nbased multi-agent system. You can see the graph implementation in listing\n12.3.\nListing 12.4 Router-based multi-agent travel assistant graph\ngraph = StateGraph(AgentState) #A\ngraph.add_node(\"router_agent\", router_agent_node) #B\ngraph.add_node(\"travel_info_agent\", travel_info_agent) #C\ngraph.add_node(\"accommodation_booking_agent\", accommodation_booki\ngraph.add_edge(\"travel_info_agent\", END) #E\ngraph.add_edge(\"accommodation_booking_agent\", END) #F\ngraph.set_entry_point(\"router_agent\") #G\ntravel_assistant = graph.compile() #H\nThe workflow graph connects the router agent to the two specialized agents.\nNotably, the only explicit edges you define are those from the travel\ninformation agent and the accommodation booking agent to the end of the\nworkflow. The connection from the router to the specialized agents is\ndetermined dynamically at runtime by the LLM’s response and is handled via\nCommand.\nBelow is a graphical representation of the current multi-agent travel assistant\ngraph:\n\n\nFigure 12.1 Router-based multi-agent Travel assistant: the router agent dispatches user queries\nto either the travel information agent or the accommodation booking agent, each equipped with\ntheir own specialized tools\nAs the diagram in figure 12.1 shows, this is a hybrid architecture. At the top,\nthe system exhibits a deterministic, workflow-driven routing logic. At the\n\n\nlower level, each specialist agent uses its own set of tools (such as travel data\nAPIs or accommodation booking interfaces) and follows an agentic, tool-\nbased decision process, which is inherently more flexible and dynamic.\n12.2.4 Trying Out the Router Agent\nTo see the system in action, run your multi-agent travel assistant by starting\nmain_05_01.py in debug mode and try the following two user queries:\nWhat are the main attractions in St Ives?\nAre there any rooms available in Penzance this weekend?\nOne important thing to note with this design is that each user question is\nrouted to a single agent for handling—in other words, each query takes a\n\"one-way ticket\" through the workflow. The router makes a clean and\nunambiguous handoff, and the selected agent responds directly to the user\nbefore the workflow ends.\nFor example, when you ask:\nWhat are the main attractions in St Ives?\nthe request is routed to the travel information agent. You can see the related\nLangSmith execution trace in figure 12.2.\nFigure 12.2 LangSmith execution trace of a travel information question\n\n\n(&) LangGraph ©\n© 12.72s © 1,390\n( router_agent 9.295\n©} RunnabieSequence 2.88s\n©) chatOpenal gpt-4.1-mini 2.875\n(Q RunnableLambda 0.00s\n(QB travel_info_agent 3.32s\nLangGraph 3.32s\n(2B agent 0.96s\n(BQ calt_mode! 0.96s\n©} RunnadieSequence 0.95s\nPrompt 0.00s\n©) ChatOpenal gpt-4.1-mini_ 0.95s\nshould_continue 0.00s\n(B tools 117s\nsearch_travel_info 1.17s\nVectorStoreRetriever 1.16s\nagent 1.18s\n( cali_model 118s\n| RunnableSequence 1.18s\nPrompt 0.00s\n©) chatOpenal gpt-4.1-mini 1.175\nshould_continue 0.00s",
      "page_number": 397,
      "chapter_number": 46,
      "summary": "This chapter covers segment 46 (pages 397-404). Key topics include agent, graph, and user. You can find\nthe complete implementation in the code files provided with this book.).",
      "keywords": [
        "Agent",
        "Accommodation Booking Agent",
        "BnB",
        "router agent",
        "Accommodation Booking",
        "travel information agent",
        "Travel",
        "router",
        "Booking Agent",
        "Booking",
        "Accommodation",
        "Travel Assistant",
        "travel information",
        "multi-agent travel assistant",
        "information agent"
      ],
      "concepts": [
        "agent",
        "graph",
        "user",
        "tool",
        "router",
        "book",
        "travel",
        "state",
        "question",
        "returns"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "Segment 32 (pages 270-277)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "Segment 28 (pages 232-240)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 6,
          "title": "Segment 6 (pages 59-67)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 31,
          "title": "Segment 31 (pages 262-269)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.53,
          "method": "api"
        }
      ]
    },
    {
      "number": 47,
      "title": "Segment 47 (pages 405-412)",
      "start_page": 405,
      "end_page": 412,
      "detection_method": "topic_boundary",
      "content": "If you ask:\nAre there any rooms available in Penzance this weekend?\nthe system dispatches the query to the accommodation booking agent.\nIn both cases, the workflow (which can be followed in figure 12.2) is clear:\nthe router agent evaluates the intent and hands the query off to the most\nappropriate specialist agent, which then handles the response and ends the\nsession.\nThis modular, graph-based design provides a strong foundation for more\nadvanced workflows. In later sections, you’ll see how you can evolve this\nsystem to handle more complex, multi-step, or even collaborative agentic\nscenarios.\n12.3 Handling Multi-Agent Requests with a\nSupervisor\nThe workflow-based multi-agent architecture we developed in the previous\nsection works well for simple, single-purpose queries—questions that can be\nclearly routed to either the travel information agent or the accommodation\nbooking agent. But what happens when a user asks for something that spans\nboth domains? Consider a query like:\n\"Can you find a nice seaside Cornwall town with good weather right now and\nfind availability and price for one double hotel room in that town?\"\nWith our previous router-based design, such a question cannot be answered\neffectively, as it requires both agents to work together and share intermediate\nresults.\nTo solve this, we need to shift our architecture towards a more flexible,\ncollaborative agent system—one where multiple specialized agents can be\ncoordinated as “sub-tools” under a higher-level manager. In LangGraph, this\nis exactly the use case for the Supervisor: a built-in component designed to\n\n\norchestrate multiple agents, allowing them to collaborate on complex\nrequests.\n12.3.1 The Supervisor Pattern: An Agent of Agents\nConceptually, the Supervisor is an “agent of agents.” It acts as an\norchestrator, managing a collection of other agents (which themselves may\nuse tools) and deciding which agent to activate, possibly multiple times in a\nsingle workflow. Each agent acts as a specialized tool that the Supervisor can\ninvoke as needed.\nLet’s see how to set up this pattern in your multi-agent travel assistant.\nStart by copying one of your previous implementation, main_04_01.py, to a\nnew script, main_06_01.py. Next, install the necessary package:\npip install langgraph-supervisor\nThen import the Supervisor:\nfrom langgraph_supervisor.supervisor import create_supervisor\nWhen defining agents to be managed by the Supervisor, it’s important to\nassign each a unique name. You can see how you instantiate your agents with\nnames in listing 12.4.\nListing 12.5 Setting up the leaf agents\ntravel_info_agent = create_react_agent(\n   model=llm_model,\n   tools=TOOLS,\n   state_schema=AgentState,\n   name=\"travel_info_agent\",\n   prompt=\"You are a helpful assistant that can search travel in\n)\naccommodation_booking_agent = create_react_agent(\n   model=llm_model,\n   tools=BOOKING_TOOLS,\n   state_schema=AgentState,\n   name=\"accommodation_booking_agent\",\n\n\n   prompt=\"You are a helpful assistant that can check hotel and \n)\nNow you can implement your travel assistant as a Supervisor, as shown in\nlisting 12.5:\nListing 12.6 Setting up the Supervisor agent\ntravel_assistant = create_supervisor( #A\n   agents=[travel_info_agent, accommodation_booking_agent], #B\n   model= ChatOpenAI(model=\"gpt-4.1\", use_responses_api=True), #\n   supervisor_name=\"travel_assistant\",\n   prompt=( #D\n       \"You are a supervisor that manages two agents: a travel i\n       \"You can answer user questions that might require calling \n       \"Decide which agent(s) to use for each user request and c\n   )\n).compile() #E\nYou’ll notice that configuring a Supervisor is much like setting up a ReAct\nagent, but instead of passing a list of tools, you provide a list of agents. Since\nthe Supervisor needs to analyze complex multi-step requests and coordinate\nseveral agents, it’s best to use a more powerful LLM model (like gpt-4.1 or\neven o3) to maximize accuracy and task decomposition.\nTip\nTry experimenting with different models for the Supervisor, such as gpt-4.1,\no3 or o4-mini, and compare how well the assistant handles increasingly\ncomplex, multi-faceted questions.\nAs in previous designs, simply update your chat loop to invoke the\nsupervisor-based travel assistant:\nresult = travel_assistant.invoke(state)\n12.3.2 From “One-Way” to “Return Ticket” Interactions\nUnlike the workflow-based router—where every user question was routed\nonce and only once to a specific agent (a “one-way ticket”)—the Supervisor\n\n\nenables a much richer interaction. The Supervisor can invoke each agent as\nneeded, potentially revisiting agents multiple times (“return tickets”) in a\nsingle session to collect, combine, and reason over intermediate results. This\nenables the system to handle more sophisticated, open-ended, and multi-part\nqueries.\nBelow, you can see a diagram representing this supervisor-based architecture:\nFigure 12.3 Router-based multi-agent Travel assistant: the router agent dispatches user queries\nto either the travel information agent or the accommodation booking agent, each equipped with\ntheir own specialized tools\n\n\nAs shown in the diagram in figure 12.3, both the high-level (supervisor) and\nlow-level (agent/tool) orchestration follow a tool-based approach,\nmaximizing flexibility and composability. The Supervisor becomes the\ncentral decision-maker, ensuring the right agent (or sequence of agents) is\nactivated for every complex travel request.\n\n\nThis Supervisor-driven architecture unlocks a new level of multi-agent\ncollaboration, laying the groundwork for more advanced, open-ended AI\ntravel assistants capable of addressing real-world, multi-step needs.\n12.3.3 Trying out the Supervisor agent\nNow, run the travel assistant by starting main_06_01.py in debug mode (with\nLangSmith tracing enabled), and try entering a complex, multi-part question\nsuch as:\nUK Travel Assistant (type 'exit' to quit)\nYou: Can you find a nice seaside Cornwall town with good weather \nWhen you examine the LangSmith trace, you’ll notice a more intricate agent\nutilization trajectory, similar to that in figure 12.4, in which the\ntravel_assistant is the supervisor agent.\nFigure 12.4 LangSmith execution trace of a combine travel information and booking question\n\n\nLangGraph —\n@® 2389s => 1216\nGs travel_assistant 2-795\ne& travel_info_agent 76<s\nGQ cat_agent 76<=s\nGs travel_info_agent 76<=s\nGa agent oss\nGy tools OS's\n| | search_travel_info O.30s\nGa agent 353s\nGz tools 0-025\nGy weather_forecast 0.005\nGy tcols 003s\nGy tools 0-025\nGa tcols 0:07\nGa agent ov2s\nGz tools 0-235\nG_ agent 1795s\nGB travel_assistant 2-29=\n\necc = booking agent 917s\ncallagent 917s\naccommodsation_booking_agent 917s\n\nGay agent 2-245\nGy tools 0.07\nGay check_bnb_availability 0.005\nEy tools 002s\nGs salt_db_query ©.00s\nagent oO.7is\ntools O.OTs\negent OS89s\ntools 0.035\nagent 2.61s\ntools O-.O2s\nagent 193s\nGB travel_assistant 1695\n\nI have summarized below the key steps from the execution trace below, so\nyou can understand the flow better (remember the travel_assistant is the\nsupervisor):\ntravel assistant\ntools\ntransfer_to_travel_info_agent\ntravel_info_agent\ntools\nsearch_travel_info\ntools\nweather_forecast\ntravel_assistant\ntools\ntransfer_to_accommodation_booking_agent\naccommodation_booking_agent\ntools\ncheck_bnb_availability\ntools\nsql_db_query\nThis shows that the supervisor-based travel assistant was able to coordinate\nboth agents, using each one and its underlying tools as needed to fully answer\nthe user's question. Technically, agents are now orchestrated as tools, and the\nSupervisor manages this collaboration dynamically.\n12.4 Summary\nBy integrating external tools—such as SQL databases and REST APIs—\nagents can retrieve real-time data and perform actions beyond simple\ninformation retrieval.\nRouter-based agent design enable modularity by directing each user\nquery to the most appropriate specialist agent, simplifying single-task\nscenarios.\nThe supervisor pattern goes further, orchestrating collaboration among\nagents, so multiple agents can be used together to answer complex,\n",
      "page_number": 405,
      "chapter_number": 47,
      "summary": "This modular, graph-based design provides a strong foundation for more\nadvanced workflows Key topics include agent, tools, and supervisor.",
      "keywords": [
        "agent",
        "Supervisor",
        "travel",
        "tools",
        "travel assistant",
        "assistant",
        "Supervisor agent",
        "Supervisor agent travel",
        "booking",
        "booking agent",
        "accommodation booking agent",
        "Supervisor Pattern",
        "info",
        "accommodation",
        "agent travel"
      ],
      "concepts": [
        "agent",
        "tools",
        "supervisor",
        "travel",
        "assistant",
        "model",
        "level",
        "design",
        "single",
        "user"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "Segment 32 (pages 270-277)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 29,
          "title": "Segment 29 (pages 241-252)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 13,
          "title": "Segment 13 (pages 217-234)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 11,
          "title": "Segment 11 (pages 87-95)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 9,
          "title": "Segment 9 (pages 71-80)",
          "relevance_score": 0.5,
          "method": "api"
        }
      ]
    },
    {
      "number": 48,
      "title": "Segment 48 (pages 413-424)",
      "start_page": 413,
      "end_page": 424,
      "detection_method": "topic_boundary",
      "content": "multi-part questions.\nTracing and debugging with LangSmith provides valuable visibility into\nagent decisions and tool usage, making it easier to optimize and extend\nyour system.\n\n\n13 Building and consuming MCP\nservers\nThis chapter covers\nThe purpose and architecture behind the Model Context Protocol (MCP)\nBuilding and exposing your own MCP server, with a practical weather\ntool example\nTesting and consuming MCP servers and related tools in applications\nIntegrating remote MCP tools into agents alongside local tools\nBuilding AI agents that can effectively access and use external context is a\ncentral challenge for application developers. Previously, integrating context\nfrom different sources meant wrapping each one as a tool, following specific\nprotocols‚Äîa time-consuming and repetitive process repeated by countless\nteams. Imagine if, instead, data and service providers could expose their\nresources as ready-made tools, instantly available for any agent or\napplication. This is the promise of the Model Context Protocol (MCP),\nintroduced by Anthropic.\nMCP defines a standardized way for services to expose ‚Äútools‚Äù through\nMCP servers. Agents, or ‚ÄúMCP hosts,‚Äù connect to these servers via\nMCP clients, discovering and using remote tools as easily as local ones. This\napproach moves much of the integration work to where it belongs‚Äîat the\nsource‚Äîallowing developers to focus on building more capable agents\nrather than reinventing the same plumbing. Once the connection is set up,\ntools from MCP servers work seamlessly with existing agent architectures.\nSince its release in late 2024, MCP has rapidly become a de-facto standard.\nMajor LLM providers like OpenAI and Google have adopted MCP in their\nAPIs and SDKs, while a growing number of companies and communities are\nmaking services available as MCP servers. Today, thousands of tools are\nlisted on public MCP server portals, ready to be integrated into new or\nexisting projects with minimal effort.\n\n\nIn this chapter, you‚Äôll get hands-on experience with MCP. We‚Äôll\nintroduce the core protocol and architecture, highlight the growing\necosystem, and show you where to find both official and community MCP\nservers. Then, you‚Äôll learn to build your own MCP server‚Äîstarting with\na practical weather data example using AccuWeather‚Äîtest it, and integrate\nit into an agent application. You‚Äôll also see how to combine remote MCP\ntools with local logic, configure clients, and follow best practices for robust\nintegration.\nAs the MCP ecosystem grows, understanding how to build and consume\nMCP servers is becoming essential for developers and service providers\nalike. Let‚Äôs dive in and explore how MCP is shaping modern, context-rich\nAI applications.\n13.1 Introduction to MCP Servers\nBuilding advanced AI agents increasingly involves giving them access to\nexternal sources of context, such as databases, APIs, or other services.\nTraditionally, this is achieved by wrapping each data source as a ‚Äútool‚Äù\nthat agents can call using a protocol supported by most large language\nmodels (LLMs). However, this approach quickly leads to repeated\nwork‚Äîevery developer has to build and maintain similar wrappers for\ncommon sources, resulting in duplicated effort across the ecosystem.\nWhat if there was a better way? Instead of every developer reinventing the\nwheel, what if the providers of context could directly expose their data and\nservices as tools, ready to be consumed by any agent? This is the central\nmotivation behind the Model Context Protocol (MCP), an open standard\nintroduced by Anthropic. MCP allows for context and capabilities to be\nexposed in a unified, tool-based way, as you can appreciate in figure\n13.1‚Äîeliminating much of the friction in agent development and\nencouraging a richer, more reusable ecosystem.\nFigure 13.1 MCP host process connecting to multiple local and remote MCP servers via MCP\nclients, each exposing tools backed by various resources. This architecture enables agents to\nflexibly access both local documents and online services through standardized tool interfaces\n\n\nMCP Host process\n\nme ce Local MCP Server 1 (on\n\nMCP client 3\nAl Agent or\nAl Application\nLocal Documents\n\nOnline resources\n\nRemote MCP Server 3 fen\n\nRemote Hotel DB\n\nRemote MCP Server 2\nOnline\n\nWeather Service\n\n13.1.1 The Problem: Context Integration at Scale\nAs we saw in previous chapters, LLM agents typically consume external data\nvia tools injected into their requests. Each new context source‚Äîa weather\nAPI, a document database, a search service‚Äîmeans creating yet another\nwrapper tool and integrating it with the agent, often following slightly\ndifferent conventions and protocols. This process is not only repetitive but\nalso results in the same work being duplicated by countless teams.\nImagine every agent developer spending time wrapping the same set of\npublic APIs, or every organization hand-rolling integrations for standard\nservices. This approach simply doesn‚Äôt scale, especially as the range of\npossible tools and context sources grows.\n13.1.2 The Solution: The Model Context Protocol (MCP)\nMCP addresses this challenge by providing a standardized way for data and\nservice providers to expose tools via MCP servers. Instead of each agent\nindividually wrapping every service, developers can simply ‚Äúsubscribe‚Äù\nto MCP servers and use the tools they expose, with minimal additional work.\nThe protocol defines a classic client-server architecture, as you can see in\nfigure 13.1.\nHere, the MCP host process‚Äîthe agent or application‚Äîconnects to one or\nmore MCP servers using an MCP client. Each MCP server can expose a\ncollection of tools, and the agent subscribes only to the tools it needs. This\narchitecture is familiar from that of REST APIs and WebSockets, which\nexpose endpoints for consumption through a client, and makes it easy to add\nor swap out new sources of context as requirements change.\nAs you can also see on figure 13.1, MCP servers can be either remote\n(accessed over StreamableHTTP, usually in production) or local (accessed\nvia STDIO, typically for development or for accessing local resources such\nas files). In most cases, and throughout this chapter, we‚Äôll assume you are\nworking with remote MCP servers.\n\n\nOnce configured, tools from MCP servers integrate with your agent in\nexactly the same way as local tools, adhering to the same tool calling\nprotocols you learned in earlier chapters. If your agent is modular and well-\narchitected, you often don‚Äôt need to change any logic at all to support\nremote MCP tools.\nNote\nMCP goes beyond just tools‚Äîit can also standardize how prompts, files, and\nother resources are shared. For our purposes, we‚Äôll focus on tool exposure,\nbut for a deeper dive, see the original MCP article by Anthropic\n(https://www.anthropic.com/news/model-context-protocol).\n13.1.3 The MCP Ecosystem\nSince its introduction in late 2024, MCP has rapidly gained traction. Leading\nLLM providers like OpenAI and Google have both adopted MCP, integrating\nsupport into their APIs and agent SDKs. On the provider side, companies are\nincreasingly wrapping their services and data with MCP servers, making\nthem instantly ‚ÄúAI-ready‚Äù for a wide range of agents and applications.\nA number of public MCP server portals have emerged, making it easy to find\ntools for almost any need. Table 13.1 highlights some of the most prominent\ndirectories:\nTable 13.1 MCP server portals\nPortal\nDescription\nhttps://github.com/modelcontextprotocol/servers Anthropic's official MCP\nportal, listing both\nofficial and community\nservers\nhttps://mcp.so/\nA community-driven\ndirectory featuring over\n16,000 servers\nhttps://smithery.ai/\nA portal with over 5,000\ntools, mostly MCP-\n\n\ncompliant\nhttps://mcpservers.org/\nA collection of around\n1,500 servers\nWith such a broad and growing ecosystem, agents can increasingly draw on a\nshared library of tools‚Äîwhether built in-house, offered by major companies,\nor shared by the community.\nAfter this overview of the motivation, architecture, and ecosystem behind\nMCP servers, we are ready to look at how to build, expose, and consume\nthese tools in real-world applications. In the next sections, we‚Äôll explore\nhow to create your own MCP server and how to integrate MCP tools\nseamlessly into your agents.\n13.2 How to Build MCP Servers\nWith a clear understanding of what MCP servers are and their role in the\nmodern AI agent ecosystem, the next step is to learn how to actually build,\ndeploy, and integrate MCP servers. This section will guide you through the\nkey resources, official tools, and best practices to help you create robust MCP\nservers and make them available to agents and applications.\n13.2.1 Essential Resources for MCP Server Development\nBefore diving into code, it‚Äôs important to familiarize yourself with the\nfoundational documentation and tools for MCP development. The official\nhub for the protocol is modelcontextprotocol.io, which provides a wealth of\ninformation on the architecture, design principles, tutorials, and the complete\nprotocol specification. Whether you are new to MCP or looking for advanced\nfeatures, this site should be your starting point.\nTip\nPay particular attention to the MCP protocol specification\n(https://modelcontextprotocol.io/specification ), which is regularly updated\nand details every aspect of the protocol‚Äîincluding transport mechanisms,\nsecurity considerations, and best practices. The specification itself is\n\n\ntechnology-agnostic, making it valuable no matter which language or\nplatform you use.\nWhile understanding the protocol is crucial, implementing MCP from scratch\nin your own application is not recommended. Doing so would mean\nduplicating a significant amount of effort, and you‚Äôd likely end up\nreimplementing features already solved in mature SDKs. Instead, you should\nleverage one of the official language-specific SDKs available for MCP.\n13.2.2 Official Language-Specific MCP SDKs\nThe official MCP GitHub repository aggregates a variety of resources, with\nparticular emphasis on the official SDKs for several major programming\nlanguages‚Äîincluding Python, JavaScript, Java, Kotlin, and C#. This book\nwill primarily focus on Python, but the same general principles apply to the\nother supported languages.\nFastMCP 1\nThe initial Python SDK, often referred to as FastMCP 1, is available at\nhttps://github.com/modelcontextprotocol/python-sdk . This library provided\nthe community with the first robust framework for building and consuming\nMCP servers in Python.\nFastMCP 2\nBuilding on the experience with FastMCP 1, the MCP community released\nFastMCP 2‚Äîa major upgrade that addresses limitations of the original\nimplementation and aligns more closely with the latest protocol\nspecifications. FastMCP 2 offers significant improvements, such as:\nEasier deployment and server composition\nEnhanced security features\nImproved client connectivity and advanced capabilities, like dynamic\ntool rewriting\nBuilt-in testing utilities and integration hooks for other libraries\n\n\nFastMCP 2 is actively maintained at https://github.com/jlowin/fastmcp, and\nyou‚Äôll find comprehensive documentation and tutorials at\nhttps://gofastmcp.com/getting-started/welcome. We‚Äôll use FastMCP 2 for\nhands-on examples, so keep these resources handy as you follow along.\n13.2.3 Consuming MCP Servers in LLM Applications and\nAgents\nWhile the previous sections focused on building MCP servers, it‚Äôs equally\nimportant to understand how to consume these servers in your own AI-\npowered applications. Thanks to wide adoption, integrating MCP tools has\nbecome remarkably straightforward‚Äîespecially with leading platforms like\nOpenAI.\nOpenAI‚Äôs API natively supports tools provided by public MCP servers via\nthe Responses API. Not only can you discover and reference these tools, but\nOpenAI will also execute them for you‚Äîeliminating the need for manual\nclient code in many cases.\nTip\nReview the OpenAI documentation on remote MCP tool integration. The\nprocess is simple, but it‚Äôs recommended to carefully consider your\nauthorization strategy‚Äîdeciding whether to approve such calls\nautomatically or interactively.\nIn many enterprise environments, MCP servers might be available only\nwithin organizational boundaries, so you cannot expect the OpenAI\nResponses API to execute the remote tools for you. For these scenarios, there\nare two primary approaches:\n1. Use the FastMCP Client: The official FastMCP SDK provides client\nfacilities to connect, authenticate, and consume tools from MCP servers\ndirectly.\n2. Take advantage of LangChain/LangGraph Integrations: If you are\ndeveloping agents with LangGraph or LangChain, you can use the\nLangChain MCP client library‚Äîparticularly the\n\n\nMultiServerMCPClient class‚Äîto easily aggregate and consume tools\nfrom multiple MCP servers through a simple configuration interface.\nIn the following sections, we‚Äôll demonstrate both approaches in practice.\nYou‚Äôll learn how to build a practical MCP server, test it, and integrate it\ninto an agent workflow‚Äîwhether you‚Äôre working directly with SDKs or\nleveraging modern agent frameworks like LangChain.\n13.3 Building a Weather MCP Server\nAfter learning about what MCP Servers are, their purpose, and the available\nlibraries and ecosystem, it is time to build one! In this section, we'll replace\nthe mockup weather tool that you used to build agents in previous chapters\nwith a real-world weather MCP server based on the AccuWeather REST API.\nWe‚Äôll also integrate this server into one of the agent-based solutions built\nearlier. Step by step, you‚Äôll see how to build, test, and connect the MCP\nserver to your agent.\n13.3.1 Implementing the MCP Server\nWe begin by replacing our previous mock weather tool with a proper MCP\nserver that exposes live weather data from AccuWeather.\nBefore implementing the code, go to the AccuWeather developer portal and\nregister (for free at: https://developer.accuweather.com/user/register) to claim\nan API key.\nAfter registering, navigate to the My Apps tab and click \"Add a new\nApp\".\nFill in the details, such as:\nApp name: accu-mcp-server\nProducts: Core Weather Limited Trial\nWhere will the app be used: Desktop Website\nWhat will you be creating with this API: Weather App\nProgramming language: Python\nChoose other settings as desired, then click Create App.\nOnce your new app is registered, locate the API key in the My Apps\n\n\ndashboard and copy it.\nAdd the API key to your .env file in your Python project, as shown\nbelow (replace the API key accordingly):\nACCUWEATHER_API_KEY=<Your API key>\nYou are now ready to implement a real MCP server that exposes the weather\nservice. Create a folder named mcp within the ch11 folder, and create an\nempty Python script called accuweather_mcp.py.\nYou can see the MCP server implementation in listing 14.1, adapted from\nthis GitHub repository: https://github.com/adhikasp/mcp-weather.\nListing 13.1 Accuweather MCP server\nimport os\nimport json\nfrom typing import Dict\nfrom fastmcp import FastMCP\nfrom dotenv import load_dotenv\nfrom aiohttp import ClientSession\nload_dotenv() #A\nmcp = FastMCP(\"mcp-accuweather\") #B\n@mcp.tool(description=\"Get weather conditions for a location.\") #\nasync def get_weather_conditions(location: str) -> Dict:\n   \"\"\"Get weather conditions for a location.\"\"\"\n   api_key = os.getenv(\"ACCUWEATHER_API_KEY\") #D\n   base_url = \"http://dataservice.accuweather.com\"\n   async with ClientSession() as session:\n       location_search_url = f\"{base_url}/locations/v1/cities/se\n       params = { #F\n           \"apikey\": api_key,\n           \"q\": location,\n       }\n       async with session.get(location_search_url, params=params\n           locations = await response.json() #G\n           if response.status != 200:\n               raise Exception(f\"Error fetching location data: {\n           if not locations or len(locations) == 0:\n               raise Exception(\"Location not found\")\n\n\n       location_key = locations[0][\"Key\"] #H\n       # Get current conditions \n       current_conditions_url = f\"{base_url}/currentconditions/v\n       params = { #J\n           \"apikey\": api_key,\n           \"details\": \"true\"\n       }\n       async with session.get(current_conditions_url, params=par\n           current_conditions = await response.json() #K\n           \n       # Format current conditions\n       if current_conditions and len(current_conditions) > 0:\n           current = current_conditions[0] #L\n           current_data = {\n               \"temperature\": {\n                   \"value\": current[\"Temperature\"][\"Metric\"][\"Va\n                   \"unit\": current[\"Temperature\"][\"Metric\"][\"Uni\n               },\n               \"weather_text\": current[\"WeatherText\"],\n               \"relative_humidity\": current.get(\"RelativeHumidit\n               \"precipitation\": current.get(\"HasPrecipitation\", \n               \"observation_time\": current[\"LocalObservationDate\n           }\n       else:\n           current_data = \"No current conditions available\" #M\n       return { #N\n           \"location\": locations[0][\"LocalizedName\"], \n           \"location_key\": location_key, \n           \"country\": locations[0][\"Country\"][\"LocalizedName\"], \n           \"current_conditions\": current_data,\n       }\nif __name__ == \"__main__\":\n   mcp.run(transport=\"streamable-http\", host=\"127.0.0.1\", #O\n           port=8020, path=\"/accu-mcp-server\")\nThis implementation relies on the fastmcp package (which is FastMCP 2).\nMake sure to install the required package in your virtual environment (or add\nfastmcp to your requirements.txt and run pip install -r\nrequirements.txt).\nThe core logic of the implementation is simple: it searches the underlying\nAccuWeather locations REST endpoint to resolve the user‚Äôs input, then\n",
      "page_number": 413,
      "chapter_number": 48,
      "summary": "Tracing and debugging with LangSmith provides valuable visibility into\nagent decisions and tool usage, making it easier to optimize and extend\nyour system Key topics include tool, agent, and server.",
      "keywords": [
        "MCP servers",
        "MCP",
        "Weather MCP Server",
        "remote MCP",
        "remote MCP servers",
        "consuming MCP servers",
        "MCP server portals",
        "servers",
        "remote MCP tools",
        "MCP tools",
        "public MCP server",
        "tools",
        "building MCP servers",
        "Local MCP Server",
        "Accuweather MCP server"
      ],
      "concepts": [
        "tool",
        "agent",
        "server",
        "locate",
        "location",
        "locations",
        "weather",
        "clients",
        "current",
        "important"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 8",
          "chapter": 32,
          "title": "Segment 32 (pages 300-308)",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 54,
          "title": "Segment 54 (pages 513-523)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 5,
          "title": "Segment 5 (pages 33-40)",
          "relevance_score": 0.45,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 22,
          "title": "Segment 22 (pages 188-196)",
          "relevance_score": 0.44,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.44,
          "method": "api"
        }
      ]
    },
    {
      "number": 49,
      "title": "Segment 49 (pages 425-432)",
      "start_page": 425,
      "end_page": 432,
      "detection_method": "topic_boundary",
      "content": "queries current weather conditions using AccuWeather‚Äôs API.\n(env_ch11) C:\\Github\\building-llm-applications\\ch11> pip install \nNow, open a new terminal (within VS Code or otherwise), activate your\nvirtual environment, move into the mcp folder, and run the server:\nC:\\Github\\building-llm-applications\\ch11>env_ch11\\Scripts\\activat\n(env_ch11) C:\\Github\\building-llm-applications\\ch11>cd mcp\n(env_ch11) C:\\Github\\building-llm-applications\\ch11\\mcp>python ac\nYou will see the MCP server starting up, and finally you'll see this output:\n‚Üê[32mINFO‚Üê[0m:     Started server process [‚Üê[36m20712‚Üê[0m\n‚Üê[32mINFO‚Üê[0m:     Waiting for application startup.\n‚Üê[32mINFO‚Üê[0m:     Application startup complete.\n‚Üê[32mINFO‚Üê[0m:     Uvicorn running on ‚Üê[1mhttp://0.0.0.0:80\nCongratulations! Your first MCP server is up and running!\n13.3.2 Trying out the MCP Server with MCP Inspector\nOne of the fastest ways to interactively test your newly created MCP server is\nby using the MCP Inspector. This tool provides a user-friendly interface that\nlets you connect to any MCP server, explore its tools, and run live queries\nwithout needing to write any client code. The process is straightforward, and\nthe Inspector is a great way to build confidence before integrating the server\ninto your applications.\nInstalling MCP Inspector\nTo get started, you‚Äôll need to install the MCP Inspector on your computer.\nMCP Inspector is a Node.js application, so ensure you have Node.js installed.\nIf not, you can download and install it from nodejs.org.\nOnce Node.js is ready, open a new command prompt or terminal. It‚Äôs a\ngood practice to keep your tools organized, so create a new folder under your\nproject directory‚Äîsuch as mcp-inspector inside ch11. Then run the\nfollowing command to launch MCP Inspector using npx:\n\n\nc:\\Github\\building-llm-applications\\ch11\\mcp-inspector>npx @model\nDuring installation, you will be asked to confirm before proceeding:\nNeed to install the following packages:\n@modelcontextprotocol/inspector@0.16.212\nOk to proceed? (y) y\nAfter confirmation, the installation will proceed, and MCP Inspector will\nautomatically launch in your browser:\n\n\n14 Productionizing AI Agents:\nmemory, guardrails, and beyond\nThis chapter covers:\nAdding short-term memory with LangGraph checkpoints\nImplementing guardrails at multiple workflow stages\nAdditional considerations for production deployment\nBuilding AI agents that behave reliably in real-world environments is about\nmore than just connecting a language model to some tools. Production\nsystems need to maintain context across turns, respect application boundaries,\nhandle edge cases gracefully, and keep operating even when something\nunexpected happens. Without these safeguards, even the most capable model\nwill eventually produce errors, off-topic answers, or inconsistent behavior\nthat undermines user trust.\nIn this chapter, we’ll focus on two of the most important capabilities for\nmaking AI agents production-ready: memory and guardrails. Memory allows\nan agent to “remember” past interactions, enabling it to hold natural\nconversations, answer follow-up questions, and recover from interruptions.\nGuardrails keep the agent within its intended scope and policy framework,\nfiltering out irrelevant or unsafe requests before they reach the model—and,\nif needed, catching inappropriate responses after the model has generated\nthem.\nWe’ll explore how these features work in LangGraph, using our travel\ninformation assistant as the running example. You’ll see how to persist\nconversation state using checkpoints, enforce scope restrictions at both the\nrouter and agent level, and extend the design to include post-model checks\nand human-in-the-loop review. By the end of the chapter, you’ll have the\ntools to build assistants that are not only smart, but also safe, focused, and\nresilient.\n\n\n14.1 Memory\nWhen designing AI agent-based systems—especially those exposed via\nchatbots—one of the most important steps towards production readiness is\nadding memory. Memory allows the system to maintain context between user\ninteractions, enabling stateful workflows and natural conversations.\nWithout memory, each interaction between the user and the LLM starts fresh,\nwith no knowledge of what was said before. For simple Q&A this might be\nacceptable, but for anything that requires follow-up, refinement, or reference\nto past turns, it quickly becomes frustrating.\nLangGraph provides a powerful and flexible mechanism for implementing\nmemory: checkpoints. In this section we will explore short-term memory\nusing checkpoints, and see how to integrate them into our existing\ntravel_assistant agent.\n14.1.1 Types of Memory\nIn AI agents, memory can exist at different scopes:\nShort-term memory — Context retained during a single session between\na user and the LLM. This is ideal for ongoing conversations until the\nuser achieves their goal. Typically stored in-memory or in a session-\nscoped store.\nLong-term user memory — Persistent across multiple sessions for the\nsame user, enabling the system to remember preferences or past\nactivities.\nLong-term application-level memory — Persistent across all users and\nsessions, storing general knowledge useful for everyone (e.g., current\nexchange rates).\nLong-term user and application memory tend to be highly system-specific, so\nin this section we will focus exclusively on short-term conversational\nmemory.\n14.1.2 Why Short-Term Memory is Needed\n\n\nIn a conversational application that uses the tool-calling protocol (as\ndescribed in earlier chapters), a typical interaction works like this:\n1. The user sends a message.\n2. The LLM may issue tool calls.\n3. The application executes these tools and returns results to the LLM.\n4. The LLM synthesises a final answer.\nIf the user follows up with a clarifying or related question, a stateless system\nwould lose all prior context, forcing the conversation to restart. This is both\ninefficient and unnatural.\nThe solution is to store the entire conversation history after each interaction\nand feed it back to the LLM on subsequent turns. This makes the system\nstateful and allows the model to resolve references like “same town” or “that\nhotel” based on prior turns.\n14.1.3 Checkpoints in LangGraph\nAlthough you could implement short-term memory simply by storing the list\nof exchanged messages after each LLM response, LangGraph provides a\nmore powerful and generalized mechanism: the checkpointer.\nA checkpoint is a snapshot of the graph’s execution state taken at a specific\nmoment in the flow.\nIn practice, LangGraph takes these snapshots at each node in the graph—\nstarting from the entry point (also called the START node) and continuing\nuntil the exit point (END node).\nThis approach is more flexible than storing messages alone, because it\npreserves all aspects of the graph state, not just the conversation text.\nThat flexibility enables a range of use cases beyond chat memory:\nState rehydration after failure — If an execution fails partway through,\nyou can resume from the last successful checkpoint without re-running\nthe entire process. This is especially valuable when some steps are\n\n\nexpensive or slow.\nHuman-in-the-loop workflows — You can pause at a checkpoint to\ncollect manual input or approval, then resume exactly where you left off\nusing the stored state.\nMulti-turn conversational context — For chatbots, checkpoints ensure\nthe conversation history is preserved across turns without having to\nreconstruct it manually.\nIn this chapter we will focus solely on the last use case—maintaining\nconversational state for the entire duration of a user–LLM session—while\nkeeping in mind that the same mechanism is equally capable of powering the\nothers.\nWhat is a Checkpoint?\nA checkpoint represents the saved state of the graph at a given super-step,\nwhere:\nA super-step corresponds to the execution of a single graph node.\nParallel nodes processed within the same step are grouped together.\nBy saving a checkpoint at each super-step, we can:\nReuse the current execution state in a follow-up question.\nReplay the persisted execution up to a specific point—useful when\nrecovering from errors or resuming after manual intervention.\nHow Checkpoints Work in LangGraph\nLangGraph’s checkpointing system is built around two main concepts:\n1. Checkpointer — The component responsible for capturing and storing\nstate snapshots. After each super-step (i.e., completion of a node), the\ncheckpointer records the current graph state, as illustrated in figure 14.1.\nThis snapshot can include conversation history, tool outputs,\nintermediate variables, and execution metadata—ensuring the workflow\ncan be resumed or inspected at any point.\n\n\n2. Configuration — Defines the session that checkpoints belong to. In\nLangGraph, a session is called a thread.\na. Each thread is identified by a thread ID (a unique value, typically a\nUUID).\nb. A single user may have multiple threads (sessions) active or saved\nat different times.\nc. The configuration links the checkpoint to a specific thread so the\nsystem knows which session state to load later.\nFigure 14.1 Sequence diagram of the travel assistant with checkpoints saved after each node\nexecution, allowing state restoration across turns using a shared thread_id.\n\n\nRouter Agent Agents Checkpointer\ncra re us er\n\nMessage\n(Turn 1)\nSave checkpoint\n(START — router)\na\nRoute +\nprompt\nrx— oc\nalt [Needs agent]\nAgent\nselection\nLp covccccccecccsccccscccccccccccdeccccoccccccoccseccossccocsccce 4)\nExecute\nselected agent\nSave checkpoint\n(after agent execution)\nAgent\nresults\nPee eee\nSynthesize\nanswer\nmi OOOO\nAssistant\nresponse\nMB ccc cnacdcscostackUsnsdsedcencndeconnccéecenussucecssasssusueus 9)\nSave checkpoint\n(router — END)\nOh\nFinal\nanswer\nanendereesssonseceeunnsenoucencenenas @\nMessage\n\n(Turn 2: “same town?”)\n\nLoad last checkpoint\nby thread_id\nOm\nContinue with prior\nstate (history rehydrated)\n\nAssistant\n\nSave checkpoint\n(after ag\n\nAgent Agents heckpoi\n(LangGraph) fetes oti 5 / LLM (thread_id)",
      "page_number": 425,
      "chapter_number": 49,
      "summary": "This tool provides a user-friendly interface that\nlets you connect to any MCP server, explore its tools, and run live queries\nwithout needing to write any client code Key topics include checkpoint, memory, and agents.",
      "keywords": [
        "MCP Inspector",
        "mcp",
        "MCP server",
        "memory",
        "Inspector",
        "Äôs API",
        "Github",
        "checkpoint",
        "agent",
        "LLM",
        "launch MCP Inspector",
        "user",
        "Installing MCP Inspector",
        "state",
        "server"
      ],
      "concepts": [
        "checkpoint",
        "memory",
        "agents",
        "conversational",
        "user",
        "application",
        "applications",
        "executes",
        "execution",
        "turns"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 5,
          "title": "Segment 5 (pages 33-40)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 60,
          "title": "Segment 60 (pages 591-598)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 27,
          "title": "Segment 27 (pages 216-223)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 15,
          "title": "Segment 15 (pages 122-129)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 8,
          "title": "Segment 8 (pages 59-66)",
          "relevance_score": 0.49,
          "method": "api"
        }
      ]
    },
    {
      "number": 50,
      "title": "Segment 50 (pages 433-440)",
      "start_page": 433,
      "end_page": 440,
      "detection_method": "topic_boundary",
      "content": "When you pass a thread ID to the graph on subsequent invocations,\nLangGraph retrieves the stored state from the last checkpoint in that thread\nand resumes execution from there—or, in a conversational context, uses it to\nprovide the LLM with the correct history.\n14.1.4 Adding Short-Term Memory to Our Travel Assistant\nTo demonstrate how LangGraph’s persistence and checkpointing work, we’ll\nenhance our router-based travel_assistant from section 12.2 with short-\nterm memory.\nFor clarity, we’ll walk through the changes step-by-step.\nStart by making a copy of your existing main_05_01.py and name it\nmain_08_01.py.\nWe’ll add persistence features to this copy so you can compare with the\noriginal.\nStep 1 — The Original Stateless Chat Loop\nFirst, let’s revisit the existing chat loop. In this version, each time the user\ninteracts, we only pass the new message as the state to travel_assistant. There\nis no concept of session or continuity—each turn is treated as completely\nseparate.\nListing 14.1 Original chat loop from router based agent solution\ndef chat_loop(): #A\n   print(\"UK Travel Assistant (type 'exit' to quit)\")\n   while True:\n       user_input = input(\"You: \").strip() #B\n       if user_input.lower() in {\"exit\", \"quit\"}: #C\n           break\n       state = {\"messages\": [HumanMessage(content=user_input)]} \n       result = travel_assistant.invoke(state) #E\n       response_msg = result[\"messages\"][-1] #F\n       print(f\"Assistant: {response_msg.content}\\n\") #G\n\n\nIn this approach, travel_assistant.invoke() receives only the state—a\nfresh message each time:\nstate = {\"messages\": [HumanMessage(content=user_input)]}\nresult = travel_assistant.invoke(state)\nStep 2 — Introducing a Thread ID\nTo persist state across turns, we need a way to uniquely identify the\nconversation.\nLangGraph does this using a thread ID, passed in a RunnableConfig.\nWe can generate one at the start of the chat loop:\nimport uuid\nthread_id = uuid.uuid1()\nconfig = {\"configurable\": {\"thread_id\": thread_id}}\nListing 14.2 shows the revised chat loop implementation, where the thread ID\nis created once, printed, and passed with every invoke() call\nListing 14.2 Revised chat loop implementation with thread_id\ndef chat_loop(): #A\n   thread_id=uuid.uuid1() #B\n   print(f'Thread ID: {thread_id}') \n   config={\"configurable\": {\"thread_id\": thread_id}} #B\n   print(\"UK Travel Assistant (type 'exit' to quit)\")\n   while True:\n       user_input = input(\"You: \").strip() #C\n       if user_input.lower() in {\"exit\", \"quit\"}: #D\n           break\n       state = {\"messages\": [HumanMessage(content=user_input)]} \n       result = travel_assistant.invoke(state, config=config) #F\n       response_msg = result[\"messages\"][-1] #G\n       print(f\"Assistant: {response_msg.content}\\n\") #H\nNow, instead of just passing state, we pass two arguments:\nresult = travel_assistant.invoke(state, config)\n\n\nThe config ensures that all checkpoints and state belong to this specific\nsession (thread_id).\nStep 3 — Adding a Checkpointer\nWith the thread ID in place, we can add an in-memory checkpointer to store\nstate snapshots.\nfrom langgraph.checkpoint.memory import InMemorySaver\ncheckpointer = InMemorySaver()\nNote\nInMemorySaver is ideal for development, testing, and quick proof-of-\nconcepts. In a production environment, you should use a persistent storage\nbackend to ensure state survives restarts and can be shared across multiple\napplication instances. LangGraph provides built-in options such as a\nSqliteSaver (from the langgraph-checkpoint-sqlite package, backed by\na SQLite database) and a PostgresSaver (from the langgraph-checkpoint-\npostgres package, backed by PostgreSQL). It also provide the async\nversions, SqliteSaverAsync and PostgresSaverAsync (from the same\npackages). For production deployments, the PostgreSQL-based checkpointer\nis generally recommended for its scalability, reliability, and concurrency\nsupport.\nWhen compiling the graph, pass the checkpointer, as shown in Listing 14.3.\nListing 14.3 Graph enriched with checkpointer\ngraph = StateGraph(AgentState) #A\ngraph.add_node(\"router_agent\", router_agent_node) #B\ngraph.add_node(\"travel_info_agent\", travel_info_agent) #C\ngraph.add_node(\"accommodation_booking_agent\", accommodation_booki\ngraph.add_edge(\"travel_info_agent\", END) #E\ngraph.add_edge(\"accommodation_booking_agent\", END) #F\ngraph.set_entry_point(\"router_agent\") #G\ncheckpointer = InMemorySaver() #H\n\n\ntravel_assistant = graph.compile(checkpointer=checkpointer) #I\nThis enables the graph to save a snapshot after every node execution, so it\ncan resume with full context on the next turn.\nStep 4 — Configuring the LLM for Conversation Continuity\nFinally, we need to configure the LLM to reference previous turns without\nresending the entire conversation history every time.\nOpenAI’s Responses API allows this using the use_previous_response_id\nflag.\nllm_model = ChatOpenAI(model=\"gpt-5\", #A\n                      use_responses_api=True, #B\n                      use_previous_response_id=True) #C\nWith use_previous_response_id=True, the LangChain ChatOpenAI\nwrapper sends only the ID of the previous response, and OpenAI rehydrates\nthe full history internally.\nImportant\nIf you enable use_responses_api=True but not\nuse_previous_response_id=True, LangChain will try to resend the full\nhistory on every turn. The Responses API treats this as a duplicate\nsubmission and returns an error. When using LangGraph memory with the\nResponses API, enabling use_previous_response_id is mandatory.\n14.1.5 Executing the Checkpointer-Enabled Assistant\nWith the checkpointer integrated into our travel_assistant, we can now\nsee short-term conversational memory in action.\nRun the updated router-based assistant and enter a typical question:\nThread ID: e683b337-752b-11f0-84a9-34f39a8d3195\nYou: What's the weather like in Penzance?\nAssistant: [{'type': 'text', 'text': 'The weather in Penzance is \n\n\nNow, enter a follow-up question that refers to the previous turn:\nYou: What's the weather in the same town now?\nAssistant: [{'type': 'text', 'text': 'Current weather in Penzance\nNotice how the assistant correctly resolves “same town” to Penzance—it was\nable to do this because the LangGraph checkpointer supplied the LLM with\nthe entire conversation history, not just the latest user input.\nCongratulations—you’ve just implemented a stateful conversational chatbot!\nAlthough this is satisfying, it’s worth taking a deeper look at what’s\nhappening under the bonnet so you can understand exactly how the\ncheckpointer maintains short-term memory.\n14.1.6 Rewinding the State to a Past Checkpoint\nTo better understand how LangGraph manages conversational memory, we’ll\nsimulate what happens internally when restoring from a checkpoint. This\nexercise will:\n1. Ask the chatbot a question.\n2. Retrieve the last checkpoint from the checkpointer.\n3. Rehydrate the graph state to that checkpoint.\n4. Ask a follow-up question that depends on that restored context.\nThis is effectively what LangGraph does automatically when you pass the\nsame thread_id on subsequent turns.\nStep 1 — Updating the Chat Loop for State Inspection\nCreate a copy of main_08_01.py and name it main_08_02.py.\nReplace the chat_loop() with the implementation shown in listing 14.4.\nListing 14.4 Step-by-step state inspection\ndef chat_loop(): \n\n\n   thread_id=uuid.uuid1() #A\n   print(f'Thread ID: {thread_id}') \n   config={\"configurable\": {\"thread_id\": thread_id}} #B\n   user_input = input(\"You: \").strip() #C\n   question = {\"messages\": [HumanMessage(content=user_input)]} #\n   result = travel_assistant.invoke(question, config=config) #E\n   response_msg = result[\"messages\"][-1] #F\n   print(f\"Assistant: {response_msg.content}\\n\") #G\n   state_history = travel_assistant.get_state_history(config) #H\n   state_history_list = list(state_history) #I\n   print(f'State history: {state_history_list}') #I\n   last_snapshot = list(state_history_list)[0] #J\n   print(f'Last snapshot: {last_snapshot.config}')\n   thread_id = last_snapshot.config[\"configurable\"][\"thread_id\"] \n   last_checkpoint_id = last_snapshot.config[\"configurable\"][\"ch\n   new_config = {\"configurable\": #M\n             {\"thread_id\": thread_id, \n              \"checkpoint_id\": last_checkpoint_id}}\n   \n   retrieved_snapshot = travel_assistant.get_state(new_config) #\n   print(f'Retrieved snapshot: {retrieved_snapshot}') #O\n   travel_assistant.invoke(None, config=new_config) #P\n   new_question = {\"messages\": [HumanMessage(content=\"What is th\n   result = travel_assistant.invoke(new_question, config=new_con\n   response_msg = result[\"messages\"][-1] #R\n   print(f\"Assistant: {response_msg.content}\\n\") #S\nStep 2 — Running the Example and Viewing State History\nRun main_08_02.py in debug mode and enter the usual question:\nYou: What's the weather like in Penzance?\nAssistant: [{'type': 'text', 'text': 'It’s currently foggy in Pen\nRetrieve the state history:\n\n\nstate_history = travel_assistant.get_state_history(config)\nstate_history_list = list(state_history)\nprint(f'State history: {state_history_list}')\nYou’ll see multiple StateSnapshot entries, each representing a checkpoint,\nstarting from the most recent and moving backwards to the initial START\nnode. Each snapshot contains:\nThe messages exchanged so far.\nTool call information (if any).\nMetadata about the graph execution step.\nStep 3 — Rehydrating from a Specific Checkpoint\nFrom the most recent snapshot:\nlast_snapshot = list(state_history_list)[0]\nprint(f'Last snapshot: {last_snapshot.config}')\nExtract the thread_id and checkpoint_id:\nthread_id = last_snapshot.config[\"configurable\"][\"thread_id\"]\nlast_checkpoint_id = last_snapshot.config[\"configurable\"][\"checkp\nBuild a new config pointing to that checkpoint:\nnew_config = {\"configurable\": \n             {\"thread_id\": thread_id, \n              \"checkpoint_id\": last_checkpoint_id}}\nRetrieve the state at this checkpoint to confirm it matches expectations:\nretrieved_snapshot = travel_assistant.get_state(new_config)\nprint(f'Retrieved snapshot: {retrieved_snapshot}')\nYou should see the full conversation history up to that checkpoint, including\nuser messages, tool outputs, and assistant responses.\nStep 4 — Resuming from the Restored State\n\n\nTo rewind the graph to that point:\ntravel_assistant.invoke(None, config=new_config)\nNow ask a follow-up that depends on that past context:\nnew_question = {\"messages\": [HumanMessage(content=\"What is the we\nresult = travel_assistant.invoke(new_question, config=new_config)\nresponse_msg = result[\"messages\"][-1]\nYou might see output like:\nAssistant: [{'type': 'text', 'text': 'In Penzance it’s currently w\nThe assistant correctly infers “same town” as Penzance, confirming that the\nrehydrated state was passed back to the LLM.\nBy stepping through this manual rewind, you now have a low-level\nunderstanding of how LangGraph’s checkpointer powers short-term\nconversational memory—storing the entire execution context at each node,\nand restoring it later to continue exactly where the conversation left off.\n14.2 Guardrails\nGuardrails are application-level mechanisms that keep an AI agent operating\nwithin a defined scope, policy framework, and intended purpose. They serve\nas the “rules of the road” for agent behavior, inspecting and validating both\ninputs and outputs at critical points to ensure the system stays safe, relevant,\ncompliant, and efficient. Without guardrails, an agent might drift into\nunrelated subject matter, produce unsafe or non-compliant responses, or\nwaste processing resources on unnecessary actions.\nA strong guardrail design can stop a travel assistant from giving stock market\ntips, prevent a support bot from revealing confidential information, or block\npoorly formed requests before they reach an expensive language model. In\npractice, guardrails often fall into three broad categories:\nRule-based — explicit conditions or regular expressions that catch\nprohibited patterns or topics.\n",
      "page_number": 433,
      "chapter_number": 50,
      "summary": "LangGraph provides built-in options such as a\nSqliteSaver (from the langgraph-checkpoint-sqlite package, backed by\na SQLite database) and a PostgresSaver (from the langgraph-checkpoint-\npostgres package, backed by PostgreSQL) Key topics include checkpoint, state, and step.",
      "keywords": [
        "state",
        "thread",
        "Travel",
        "Assistant",
        "history",
        "Chat Loop",
        "config",
        "response",
        "Travel Assistant",
        "checkpoint",
        "Chat",
        "Loop",
        "result",
        "messages",
        "snapshot"
      ],
      "concepts": [
        "checkpoint",
        "state",
        "step",
        "stepping",
        "graph",
        "message",
        "conversational",
        "assistant",
        "listing",
        "history"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 15,
          "title": "Segment 15 (pages 122-129)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "Segment 32 (pages 270-277)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 12,
          "title": "Segment 12 (pages 96-103)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 27,
          "title": "Segment 27 (pages 219-226)",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 18,
          "title": "Segment 18 (pages 147-158)",
          "relevance_score": 0.5,
          "method": "api"
        }
      ]
    },
    {
      "number": 51,
      "title": "Segment 51 (pages 441-449)",
      "start_page": 441,
      "end_page": 449,
      "detection_method": "topic_boundary",
      "content": "Retrieval-based — checks against approved data sources to confirm that\na request is relevant and in scope.\nModel-based — compact classification or moderation models that assess\nintent, safety, or adherence to policy.\nThese controls can be introduced at multiple points in the agent workflow:\nPre-model checks — reject invalid or irrelevant queries before the LLM\nruns.\nPost-model checks — verify generated responses against policy or safety\nguidelines.\nRouting-stage checks — decide whether a query should trigger certain\ntools or branches.\nTool-level checks — block unsafe or unauthorized tool actions.\nFunctionally, guardrails act like validation layers—if a check fails, the\nagent’s normal flow is adjusted, whether by refusing the request, asking for\nclarification, or redirecting to a safer response path.\nIn this section, we’ll integrate custom guardrails into the router node of the\ntravel information assistant we built earlier (now enhanced with memory) so\nit can immediately reject irrelevant or out-of-scope requests—such as non-\ntravel queries. We’ll also explore LangGraph’s pre-model hook, which lets us\nenforce guardrails before any LLM call, ensuring that both the travel and\nweather agents remain within coverage boundaries—for example, only\nhandling destinations in Cornwall.\n14.2.1 Implementing Guardrails to Reject Non-Travel-Related\nQuestions\nThe first and most obvious guardrail for our UK travel information assistant\nis a domain relevance check—a pre-filter that screens the user’s question\nbefore any agent reasoning occurs. If the question falls outside the assistant’s\nremit, we intercept it early and politely refuse to answer. This prevents the\nsystem from attempting to handle queries about unrelated topics such as\nsports results, financial markets, or celebrity gossip.\n\n\nIntroducing this guardrail delivers two key benefits:\n1. Improved Accuracy — Our agents are trained and configured only for\ntravel and weather information. If we attempt to answer unrelated\nquestions, the results will almost certainly contain inaccuracies or\nhallucinations. By rejecting irrelevant queries outright, we keep the\nconversation aligned with the agent’s actual capabilities.\n2. Cost Control — Without this filter, some users might deliberately use\nour assistant as a free gateway to an expensive LLM, bypassing\nsubscription costs for unrelated questions. By blocking non-travel topics\nearly, we prevent this form of resource abuse and avoid unnecessary\nprocessing costs.\nDefining the Guardrail Policy\nOur first task is to clearly define what qualifies as in-scope for this assistant.\nThis ensures the guardrail has unambiguous decision criteria.\nTo begin, make a copy of the main_08_01.py script from the previous section\nand save it as main_09_01.py. Then, implement the guardrail policy as shown\nin Listing 14.5.\nListing 14.5 Guardrail policy to restrict questions to travel-related topics\nclass GuardrailDecision(BaseModel): #A\n   is_travel: bool = Field(\n       ...,\n       description=(\n           \"True if the user question is about travel informatio\n           \"lodging (hotels/BnBs), prices, availability, or weat\n       ),\n   )\n   reason: str = Field(..., description=\"Brief justification for \nGUARDRAIL_SYSTEM_PROMPT = ( #B\n   \"You are a strict classifier. Given the user's last message, \n   \"travel-related. Travel-related queries include destinations, \n   \"room availability, prices, or weather in Cornwall/England.\"\n)\nREFUSAL_INSTRUCTION = ( #C\n\n\n   \"You can only help with travel-related questions (destination\n   \"availability, or weather in Cornwall/England). The user's re\n   \"Politely refuse and briefly explain what topics you can help \n)\nllm_guardrail = llm_model.with_structured_output(GuardrailDecisio\nThis snippet defines a guardrail policy that uses a lightweight LLM classifier\nto determine whether a user’s question is travel-related.\nGuardrailDecision is a Pydantic model that structures the classification\noutput. The is_travel flag indicates whether the request falls within the\ntravel domain, and reason provides a brief justification for the decision.\nGUARDRAIL_SYSTEM_PROMPT instructs the model to classify strictly, giving a\nprecise definition of “travel-related” for our purposes.\nREFUSAL_INSTRUCTION contains a fixed, polite message to explain to the user\nwhy their question can’t be answered.\nllm_guardrail wraps the base LLM with structured output formatting,\nenabling fast, consistent decision-making before the main routing logic runs.\nUpdating the Router Graph\nAt a high level, we want irrelevant queries to exit the workflow immediately\n—without touching any of the downstream agents. That means introducing a\ndedicated guardrail_refusal node in our LangGraph routing structure. This\nnode simply redirects to the graph’s END without performing any work.\nThe updated workflow is shown in Figure 14.2.\nFigure 14.2 Updated travel assistant workflow: A guardrail checks if the user’s question is in\nscope. Irrelevant queries are routed to guardrail_refusal, which sends them directly to END,\nbypassing all other agents.\n\n\nThe router agent now checks whether a question is in scope. If it is, the query\nis routed to either the travel information agent or the weather agent as before.\nIf it isn’t, the router sends it to the new guardrail_refusal node, which is\nlinked directly to the END node.\nThe corresponding graph definition is shown in Listing 14.6.\n\n\nListing 14.6 Adding a guardrail_refusal node to the router graph\ndef guardrail_refusal_node(state: AgentState): #A\n   return {}\ngraph = StateGraph(AgentState) \ngraph.add_node(\"router_agent\", router_agent_node) \ngraph.add_node(\"travel_info_agent\", travel_info_agent) \ngraph.add_node(\"accommodation_booking_agent\", accommodation_booki\ngraph.add_node(\"guardrail_refusal\", guardrail_refusal_node) #B\ngraph.add_edge(\"travel_info_agent\", END) \ngraph.add_edge(\"accommodation_booking_agent\", END) \ngraph.add_edge(\"guardrail_refusal\", END) #C\ngraph.set_entry_point(\"router_agent\") \ncheckpointer = InMemorySaver() \ntravel_assistant = graph.compile(checkpointer=checkpointer) \nAs you can see, the guardrail_refusal node is intentionally a no-op—its\nonly role is to create a clean shortcut to END.\nUpdating the Router Agent\nWith the guardrail policy defined and the graph updated, the final step is to\nimplement the logic that enforces it. Previously, our router agent only\ndecided whether to send a query to the travel or weather agent. Now it must\nfirst run the guardrail check, as illustrated in the diagram of figure 14.3.\nFigure 14.3 Flowchart of updated router logic with guardrail check: queries first pass a relevance\nfilter, and irrelevant ones are sent with a refusal message directly to the guardrail_refusal node.\n\n\nIncoming user query\n\nRouter guardrail\n(is travel-related?)\n\nRouter agent\n(classify: travel vs\nweather)\n\nguardrail_refusal node\n(no-op)\n\nTravel info Weather info\n\nIf the guardrail check fails—meaning the question is judged irrelevant to the\ntravel assistant—the router generates a refusal message and routes execution\ndirectly to the guardrail_refusal node, as shown in Figure 14.3.\nThis logic is implemented in Listing 14.7.\nListing 14.7 Router agent with guardrail enforcement\ndef router_agent_node(state: AgentState) -> Command[AgentType]:\n   \"\"\"Router node: decides which agent should handle the user qu\n   messages = state[\"messages\"] \n   last_msg = messages[-1] if messages else None \n   if isinstance(last_msg, HumanMessage):\n       user_input = last_msg.content \n       classifier_messages = [ \n           SystemMessage(content=GUARDRAIL_SYSTEM_PROMPT), #A\n           HumanMessage(content=user_input),\n       ]\n       decision = llm_guardrail.invoke(classifier_messages) #B\n       if not decision.is_travel: #C\n           refusal_text = ( #D\n               \"Sorry, I can only help with travel-related quest\n               \"lodging, prices, availability, or weather in Cor\n               \"Please rephrase your request to be travel-relate\n           )\n           return Command( #E\n               update={\"messages\": [AIMessage(content=refusal_te\n               goto=\"guardrail_refusal\",\n           ) \n       router_messages = [ \n           SystemMessage(content=ROUTER_SYSTEM_PROMPT),\n           HumanMessage(content=user_input)\n       ]\n       router_response = llm_router.invoke(router_messages) \n       agent_name = router_response.agent.value \n       return Command(update=state, goto=agent_name) \n   \n   return Command(update=state, goto=AgentType.travel_info_agent\nIn this revised logic:\nThe router first invokes llm_guardrail with the latest user query.\n\n\nIf the classification result indicates the question is not travel-related, the\nrouter constructs a fixed refusal message, stores it in the state, and sends\nexecution to the guardrail_refusal node—bypassing all normal routing.\nIf the check passes, the router continues with its usual process of\nselecting the most appropriate agent.\nTesting the Guardrail\nTo see it in action, run main_09_01.py in debug mode and place a breakpoint\non the line where llm_guardrail is invoked. Then enter the following query:\nUK Travel Assistant (type 'exit' to quit)\nYou: Can you give me the latest results of Inter Milan?\nWhen execution stops at the breakpoint, inspect decision.is_travel. You\nshould see False, since football scores are outside the allowed travel domain.\nResuming execution (F5) will produce:\nAssistant: Sorry, I can only help with travel-related questions (\nCongratulations—you’ve successfully implemented your first guardrail!\nHowever, our work isn’t done. Remember, one of our agents only has\ncoverage for Cornwall, not the whole of the UK. This means we’ll want to\nimplement further scope restrictions at the agent level, which we’ll tackle in\nthe next section.\n14.2.2 Implementing More Restrictive Guardrails at Agent\nLevel\nIn traditional software development, it’s considered best practice for each\nclass or component to validate its own data rather than relying solely on\nvalidations at higher levels such as the UI. The same principle applies to\nagent-based systems: each agent should enforce its own input guardrails,\neven if broader checks are already in place at the chatbot entry point.\nThese agent-level guardrails are often more restrictive than system-wide\nones, because they can account for the specific capabilities and data scope of\n\n\nthe individual agent.\nIn our case:\nThe travel information agent can only handle queries about Cornwall,\nbecause its vector store contains data exclusively from that region.\nThe accommodation booking agent will also be limited to Cornwall for\nnow, to keep the assistant’s scope consistent.\nWhy have two levels of guardrails?\nRouter-level guardrail — Acts as an early fail-fast filter before any\nagent logic or tool invocation.\nAgent-level guardrails — Provide a “belt-and-suspenders” safeguard to\ncatch out-of-scope requests if the agent is ever called directly or reused\nin a different context.\nDefining the Cornwall-Restricted Guardrail Policy\nWe start by explicitly defining the policy: only travel-related questions about\nCornwall are permitted. This ensures that both travel and accommodation\nbooking agents reject queries for other regions or countries.\nMake a copy of main_09_01.py and rename it main_09_02.py. Then add the\nfollowing system prompts to define the classification and refusal behavior.\nListing 14.8 System prompts for classification and refusal behavior\nAGENT_GUARDRAIL_SYSTEM_PROMPT = ( \n   \"You are a strict classifier. Given the user's last message, \n   \"travel-related. Travel-related queries include destinations, \n   \"room availability, prices, or weather in Cornwall/England.\"\n   \"Only accept travel-related questions covering Cornwall (Engl\n   \"from other areas in England and from other countries\"\n)\nAGENT_REFUSAL_INSTRUCTION = ( \n   \"You can only help with travel-related questions (destination\n   \"availability, or weather in Cornwall/England). The user's re\n   \"Or it might be a travel related question but not focusing on \n",
      "page_number": 441,
      "chapter_number": 51,
      "summary": "This chapter covers segment 51 (pages 441-449). Key topics include agent, travel, and guardrails. Model-based — compact classification or moderation models that assess\nintent, safety, or adherence to policy.",
      "keywords": [
        "guardrail",
        "agent",
        "router",
        "travel",
        "REFUSAL",
        "refusal node",
        "node",
        "router agent",
        "Guardrail Policy",
        "user",
        "LLM",
        "Cornwall",
        "travel-related",
        "question",
        "weather"
      ],
      "concepts": [
        "agent",
        "travel",
        "guardrails",
        "graph",
        "checks",
        "query",
        "decision",
        "routing",
        "message",
        "defining"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 42,
          "title": "Segment 42 (pages 853-871)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 31,
          "title": "Segment 31 (pages 262-269)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "Segment 32 (pages 270-277)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 35,
          "title": "Segment 35 (pages 301-308)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 6,
          "title": "Segment 6 (pages 59-67)",
          "relevance_score": 0.52,
          "method": "api"
        }
      ]
    },
    {
      "number": 52,
      "title": "Segment 52 (pages 450-457)",
      "start_page": 450,
      "end_page": 457,
      "detection_method": "topic_boundary",
      "content": "   \"Politely refuse and briefly explain what topics you can help \n)\nCreating the Agent-Level Guardrail Function\nThe agent guardrail is implemented as a Python function that takes the\ncurrent graph state and returns either an unchanged state (for valid input) or\none modified to instruct the LLM to issue a refusal.\nListing 14.9 Agent-level guardrail function\ndef pre_model_guardrail(state: dict):\n   messages = state.get(\"messages\", [])\n   last_msg = messages[-1] if messages else None\n   if not isinstance(last_msg, HumanMessage): #A\n       return {}\n   user_input = last_msg.content\n   classifier_messages = [ #B\n       SystemMessage(content=AGENT_GUARDRAIL_SYSTEM_PROMPT),\n       HumanMessage(content=user_input),\n   ]\n   decision = llm_guardrail.invoke(classifier_messages)\n   if decision.is_travel: #C\n       # Allow normal flow; do not modify inputs\n       return {}\n       return {\"llm_input_messages\": [SystemMessage(content=AGEN\nThe pre_model_guardrail() function works as a pre-processing filter before\nthe LLM sees the user’s query:\nIt verifies that the latest message is indeed from the user.\nIt sends the query, along with a strict classification system prompt, to\nthe guardrail LLM.\nIf the query is in-scope (travel-related and Cornwall-specific), it passes\nthrough unchanged.\nOtherwise, the function prepends a refusal instruction so the agent\npolitely declines the request.\n\n\nInjecting the Guardrail into the Agents\nLangGraph’s ReAct agents support pre-model hooks (pre_model_hook) and\npost-model hooks (post_model_hook), allowing you to intercept and\nmanipulate inputs or outputs. While these hooks can be used for tasks like\nsummarizing long inputs or sanitizing outputs, here we’ll focus solely on\ninput-side guardrails.\nTo enable the Cornwall restriction, we simply pass pre_model_guardrail to\nboth the travel information agent and the accommodation booking agent.\nListing 14.10 Travel information agent with Cornwall guardrail\ntravel_info_agent = create_react_agent(\n   model=llm_model,\n   tools=TOOLS,\n   state_schema=AgentState,\n   prompt=\"You are a helpful assistant that can search travel in\n   pre_model_hook=pre_model_guardrail, #A\n)\nListing 14.11 Accommodation booking agent with Cornwall guardrail\naccommodation_booking_agent = create_react_agent( #A\n   model=llm_model,\n   tools=BOOKING_TOOLS,\n   state_schema=AgentState,\n   prompt=\"You are a helpful assistant that can check hotel and \n   pre_model_hook=pre_model_guardrail,\n)\nTesting the Cornwall Guardrail\nRun main_09_02.py in debug mode, placing a breakpoint on the\nllm_guardrail invocation inside pre_model_guardrail(). Then try:\nUK Travel Assistant (type 'exit' to quit)\nYou: Can you give me some travel tips for Liverpool (UK)?\nWhen paused at the breakpoint, inspect decision.is_travel—it should be False\nbecause the query is not Cornwall-specific. Execution will then prepend the\n\n\nrefusal instruction, resulting in output like:\nAssistant: [{'type': 'text', 'text': 'Sorry—I can only help with \nWith this, we now have two layers of defense:\n1. A router-level guardrail that quickly rejects any non-travel queries.\n2. Agent-level guardrails that enforce Cornwall-specific scope for travel\nand accommodation requests.\nOur entire agentic workflow is now protected from both irrelevant and out-\nof-coverage queries, making the system safer, more reliable, and more cost-\nefficient.\n14.3 Beyond this chapter\nBy this point, you’ve seen how to equip AI agents with memory and\nguardrails—two of the most critical building blocks for making them\nproduction-ready. But depending on your application’s domain, scale, and\ncompliance requirements, there are additional considerations you may need to\naddress before deploying to real users.\nSome of these areas have been mentioned in passing throughout the book but\nnot explored in depth, either because they are highly domain-specific or\nbecause they deserve their own dedicated treatment. Below are a few\ndirections worth exploring further.\n14.3.1 Long-term user and application memory\nIn this chapter, we focused on short- to medium-term memory—keeping\ntrack of relevant state within a single conversation or across a limited session\nhistory.\nHowever, production agents often benefit from persistent, long-term memory\nthat stores user preferences, past interactions, and contextual information\nacross weeks, months, or even years. This might involve:\nDedicated vector stores for each user.\n\n\nPeriodic summarization and pruning to keep memory manageable.\nPrivacy and compliance controls for personally identifiable information\n(PII).\nLong-term memory can dramatically improve personalization, but it also\nbrings engineering, scaling, and regulatory challenges you’ll need to\nplan for.\nTable 14.1 summarizes the various types of memory your AI agent-based\nsystem might need to accommodate user needs.\nTable 14.1 Type of memory in AI agents\nMemory\ntype\nScope\nPersistence\nExample\n(Travel\nAssistant)\nChallenges\nShort-term\nSingle user\nsession\nUntil\nsession ends\nRemembering\n“same town” in\na weather\nfollow-up\nwithin one\nconversation\nLimited\ncontinuity;\nlost when\nsession\ncloses\nLong-term\n(user)\nAcross\nmultiple\nsessions for\none user\nWeeks,\nmonths, or\nyears\nRemembering a\nuser’s preferred\nCornwall\ndestinations or\naccommodation\ntypes\nPrivacy\ncompliance\nLong-term\n(application)\nAcross all\nusers and\nsessions\nOngoing\nStoring general\ntravel updates\n(e.g., Cornwall\nevent calendar,\nseasonal\nattraction\nschedules)\nKeeping\ndata fresh;\navoiding\noutdated or\nincorrect\ninfo\n14.3.2 Human-in-the-loop (HITL)\n\n\nEven a well-designed travel information assistant will encounter situations\nwhere automated handling is not enough—cases where the query is\nambiguous, the available data is incomplete, or a decision could have a\nsignificant real-world impact. A human-in-the-loop approach allows such\nrequests to be escalated to a human travel expert for review before a response\nis sent.\nFor our Cornwall-focused travel assistant, common HITL scenarios could\ninclude:\nRequests for personalized itineraries involving unusual combinations of\nactivities where safety, feasibility, or timing is uncertain.\nQueries about real-time disruptions—such as severe weather, transport\nstrikes, or event cancellations—where up-to-date human judgment is\nneeded.\nSpecial accommodation or accessibility requests that require confirming\ndetails with local providers.\nIn early production deployments, HITL is especially valuable: it helps ensure\naccuracy, prevents reputational damage, and provides real-world feedback to\nrefine the assistant’s automated policies. Over time, insights from human\nreviews can be incorporated into improved guardrails, better prompts, or\nexpanded knowledge bases—reducing the number of cases that need\nescalation.\n14.3.3 Post-model guardrails\nIn this chapter, we focused on guardrails that run before invoking the LLM—\nscreening queries for relevance and redirecting out-of-scope requests. In a\nproduction travel information assistant, you may also want post-model\nguardrails that inspect the model’s output before it is shown to the user or\nsent to a downstream service (such as a booking API).\nFor our Cornwall-focused assistant, post-model guardrails could include:\nFiltering outdated or incorrect details—for example, removing\nreferences to attractions that have permanently closed or events that\nhave already passed.\n\n\nRedacting sensitive information—such as private contact numbers for\nsmall BnBs that should only be shared after confirmed bookings.\nEnforcing brand tone and style—ensuring that all travel advice is given\nin a warm, welcoming, and concise manner consistent with the\nassistant’s persona.\nVerifying structured output—making sure any booking\nrecommendations, price quotes, or itineraries follow the correct format\nexpected by other systems.\nPost-model guardrails act as a final safety net, catching cases where the\nLLM’s answer might look reasonable but contains factual errors, tone\nmismatches, or details that are inappropriate for immediate user delivery.\n14.3.4 Evaluation of AI agents and applications\nOne of the most overlooked—but absolutely essential—steps in preparing an\nagent for production is systematic evaluation. This is a broad and evolving\ndiscipline, complex enough to deserve an entire book of its own, but it is\ncritical for ensuring that your travel assistant remains accurate, safe, and\nefficient once deployed.\nFor our Cornwall-focused travel information assistant, evaluation might\ninclude:\nFunctional testing – Verifying that the assistant provides correct,\nrelevant, and complete answers across a wide set of test queries, such as\n“Best family-friendly beaches in Cornwall” or “Current weather forecast\nfor St Ives.” This ensures it stays within scope and retrieves accurate,\nup-to-date information.\nBehavioral testing – Confirming that the assistant follows policy and\nsafety rules, avoids giving irrelevant or unsafe travel advice, and\nmaintains a consistent, friendly tone suitable for tourism and customer\nservice.\nPerformance testing – Measuring latency and API costs under realistic\nuser loads, such as peak summer tourist season when requests might\nsurge.\nRegression testing – Ensuring the assistant’s reliability is not\n\n\ncompromised when prompts are refined, tools are updated, or the\nunderlying LLM is replaced.\nWhile this book does not cover evaluation frameworks and methodologies in\ndetail, you should treat evaluation as a core part of your pre-production\nchecklist and as an ongoing process after launch. Continuous evaluation helps\nyou catch issues before users do, adapt to changes in local events or services,\nand maintain the trustworthiness of your travel assistant over time.\nEquipping your agents with memory and guardrails is an important\nmilestone, but it’s only part of the journey. True production readiness\nrequires a holistic approach that includes safety, compliance, reliability, and\ncontinuous evaluation. The good news is that the foundations you’ve built in\nthis chapter will make it much easier to layer in these additional capabilities\nas your applications grow in scope and complexity.\n14.3.5 Deployment on LangGraph Platform and Open Agent\nPlatform (OAP)\nThe final step in preparing agents for production is deployment. How you\ndeploy depends heavily on your organization’s infrastructure strategy—\nwhether applications run on-premises or in the cloud, and on IT policies\naround privacy, security, and compliance. It also reflects organizational\nchoices: some teams favor local DevOps and SRE-driven deployments, while\nothers rely on SaaS-based hosting for simplicity and scalability.\nOnce you’ve developed your LangGraph-based multi-agent system, a natural\npath is to deploy it on the LangGraph Platform. This is LangChain’s fully\nmanaged hosting solution for agentic applications. It provides built-in\nfeatures such as horizontal scalability, persistent state management, and end-\nto-end monitoring through the familiar LangSmith dashboard. The platform\nabstracts away much of the operational overhead, letting you move from\nprototype to production with minimal friction while still retaining\nobservability and fine-grained debugging tools.\nAnother powerful option is to integrate your agents into the Agent Open\nPlatform (OAP). OAP is a flexible runtime and orchestration layer for AI\n\n\nagents that comes with a set of prebuilt agent patterns—including the multi-\ntool agent and the supervisor agent. These can be customized and extended to\nfit enterprise use cases, whether by plugging into MCP servers, local vector\nstores, or other enterprise data sources. OAP is designed to act as a bridge\nbetween bespoke LangGraph agents and a broader ecosystem of composable,\ninteroperable agents, making it especially valuable for organizations planning\nto run multiple agents in coordination.\nBoth LangGraph Platform and OAP are available as fully managed SaaS\nofferings, but can also be deployed into a client’s own cloud environment for\nteams that need to maintain tighter control over data residency and\ncompliance. This dual deployment model means you can start quickly with\nmanaged hosting and later migrate to a private setup if regulatory or\noperational needs demand it.\nTogether, these deployment paths provide a smooth continuum—from\ndevelopment on your laptop, to scalable production hosting, to enterprise-\nwide agent orchestration—allowing you to choose the right trade-off between\ncontrol, convenience, and operational complexity.\n14.4 Summary\nMemory enables natural conversation — Short-term conversational\nmemory preserves context across turns, allowing the assistant to handle\nfollow-up questions without repeating information.\nCheckpoints store full execution state — LangGraph saves snapshots\nafter each node, enabling both conversation continuity and recovery\nfrom failures.\nGuardrails maintain scope and safety — They filter or redirect requests\nthat fall outside the assistant’s domain or policy guidelines.\nMultiple guardrail layers are valuable — Use router-level checks for fast\nrejection and agent-level checks for specific data or domain constraints.\nPost-model guardrails act as a final safety net — They verify output\ncorrectness, tone, and format before the user sees it or a downstream\nsystem processes it.\nHuman-in-the-loop improves reliability — Escalating certain cases to\nhuman review can prevent costly mistakes and provide training data for\n",
      "page_number": 450,
      "chapter_number": 52,
      "summary": "14.3 Beyond this chapter\nBy this point, you’ve seen how to equip AI agents with memory and\nguardrails—two of the most critical building blocks for making them\nproduction-ready Key topics include agent, travel, and guardrail.",
      "keywords": [
        "Guardrail",
        "agent",
        "travel",
        "assistant",
        "Agent-Level Guardrail Function",
        "LLM",
        "model",
        "Travel Assistant",
        "travel information assistant",
        "Cornwall guardrail",
        "user",
        "Cornwall guardrail travel",
        "Post-model guardrails",
        "travel information agent",
        "memory"
      ],
      "concepts": [
        "agent",
        "travel",
        "guardrail",
        "user",
        "assistant",
        "information",
        "production",
        "state",
        "memory",
        "evaluation"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 29,
          "title": "Segment 29 (pages 577-594)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 5,
          "title": "Segment 5 (pages 33-40)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 13,
          "title": "Segment 13 (pages 217-234)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 25,
          "title": "Segment 25 (pages 240-248)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.55,
          "method": "api"
        }
      ]
    },
    {
      "number": 53,
      "title": "Segment 53 (pages 458-465)",
      "start_page": 458,
      "end_page": 465,
      "detection_method": "topic_boundary",
      "content": "future automation.\nEvaluation is ongoing, not one-off — Regular functional, behavioral,\nperformance, and regression testing ensures the assistant remains\ntrustworthy over time.\nProduction readiness is holistic — Combining memory, guardrails,\nhuman review, and evaluation sets the foundation for safe, effective, and\nscalable AI agents.\n\n\nAppendix A. Trying out LangChain\nIn this appendix, you will get a first hands-on experience with LangChain by\nperforming simple sentence completions and experimenting with basic\nprompt engineering in a Jupyter Notebook.\n\n\nA.1 Trying out LangChain in a Jupyter Notebook\nenvironment\nWe’ll begin with a straightforward example: completing a sentence using an\nOpenAI model and refining its output through prompt engineering. OpenAI’s\nmodels are a convenient starting point because they are accessible through a\npublic REST API and require no local infrastructure setup.\nIf you prefer, you can use an open-source LLM inference engine such as\nOllama. However, since many readers may not yet have run an open-source\nLLM locally, we will cover that setup in detail in Appendix E. For now, we\nwill keep things simple and use the OpenAI REST API.\nBefore you proceed, ensure that you have Python 3.11 or later installed on\nyour local machine and that the following prerequisites are met:\n1. Already own or generate an OpenAI key.\n2. Know how to set up a Python Jupyter notebook environment.\nIf you're unfamiliar with these tasks, check the sidebar for guidance on\n\"Creating an OpenAI key\" and refer to Appendix B for instructions on setting\nup a Jupyter notebook environment. Since most readers use Windows, I'll be\nproviding instructions specifically for Windows and a Python virtual\nenvironment. If you're using Linux or Anaconda, I assume you're advanced\nenough to adapt these instructions to your setup. If you prefer, you can also\nrun these examples in an online notebook environment like Google Colab, as\nlong as you install the required packages.\nCreating an OpenAI key\nAssuming you've registered with OpenAI, which is necessary to explore the\nChatGPT examples discussed at the beginning of the chapter:\nLog in to your OpenAI account at https://platform.openai.com/ and\nnavigate to the API section.\nAccess the API Keys by clicking your profile icon > Your profile > User\n\n\nAPI key (tab).\nCreate a new secret key by clicking the \"Create Secret Key\" button,\nnaming it (e.g., BuildingLLMApps), and confirming.\nSafely save the key, for example in a secure local file or a password\nvault tool, as retrieval is not possible later.\nAdditionally, set a spending limit (e.g., $10) to control costs. Configure this\nunder Settings > Limits in the left-hand menu based on your usage\npreferences.\nTo establish the virtual environment for the code in this chapter, follow the\nsteps below. Open a new terminal in your operating system, create a folder\nlike c:\\Github\\building-llm-applications\\ch01 navigate into it, and\nexecute:\nC:\\>cd Github\\building-llm-applications\\ch01\nC:\\Github\\building-llm-applications\\ch01>python -m venv env_ch01\nC:\\Github\\building-llm-applications\\ch01>.\\env_ch01\\Scripts\\activ\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>\nNOTE\nI am running these commands on a Windows cmd shell. If you are on a Linux\ncomputer, you might have to adapt things slightly. For example, you can\nactivate a virtual environment with: ./env_ch01/bin/activate or\n./env_ch01/Scripts/activate. If you are using Powershell, you should use\nActivate.ps1.\nNow install the notebook, langchain (and indirectly openai) packages:\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>pip install n\nNote\nAlternatively, if you have cloned the Github code from the repo associated\nwith this book, you can install all the packages in this way:\n\n\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>pip install -r\nrequirements.txt\nOnce the installation is complete, start up a Jupyter notebook:\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>jupyter noteb\nCreate a notebook with: File > New > Notebook, then rename it with File >\nRename … to 01-langchain_examples.ipynb.\nIf you have cloned the Github repository and want to use the notebook\ndirectly, start it up as follows:\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>jupyter noteb\nA.1.1 Sentence completion example\nNow, you are ready to execute LangChain code in the notebook. Start by\nimporting the LangChain library and configuring an OpenAI Language\nModel (LLM) instance.\nIn the initial cell of your notebook, start by importing the necessary libraries,\nby running the cell:\nfrom langchain_openai import ChatOpenAI\nimport getpass\nNOTE\nIf you are unfamiliar with Jupyter notebooks, you can run a cell by pressing\nShift+Enter or clicking the play button located next to the top menu.\nNow add and execute a cell to grab your OpenAI API key (just hit Enter after\ninserting the key):\nOPENAI_API_KEY = getpass.getpass('Enter your OPENAI_API_KEY')\nSetting the OpenAI API key with an environment variable\n\n\nAnother way to set the OpenAI key is by using an environment variable. For\nexample, in Windows, you can set it in the command shell like this, before\nlaunching the notebook:\nset OPENAI_API_KEY=your_openai_api_key\nThen, in your Python code, retrieve it with (make sure you import os):\napi_key = os.getenv(\"OPENAI_API_KEY\")\nSince this method depends on the operating system and some readers may not\nbe familiar with it, I'll focus on the previous approach. However, if you're\ncomfortable using environment variables, feel free to use them.\nOnce you have entered your OpenAI key, add and execute this cell:\nllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,\n                model_name=\"gpt-4o-mini\")\nllm.invoke(\"It's a hot day, I would like to go to the…\")\nYou will see output similar to the example below, which represents a return\nmessage from the LLM:\nAIMessage(content=\"…beach to cool off and relax in the refreshing \nAs you can see in the content property, the completion generated by the\nLLM is \"…beach to cool off [...]\".\nA.1.2 Prompt engineering examples\nI've mentioned prompts several times, but I haven't shown you any examples\nyet. A prompt is the instruction you give the LLM to complete a task and\ngenerate a response. It's a key part of any LLM application, so much so that\ndeveloping LLM applications often involves spending a lot of time designing\nand refining prompts through trial and error. Various patterns and techniques\nare already forming around prompts, leading to the emergence of a discipline\ncalled prompt engineering. This field focuses on crafting prompts that yield\nthe best possible answers. I'll dedicate the next chapter to teaching you the\nfundamentals of prompt engineering. For now, let's start with a\n\n\nstraightforward prompt.:\nprompt_input = \"\"\"Write a short message to remind users to be \nvigilant about phishing attacks.\"\"\"\nresponse = llm.invoke(prompt_input)\nprint(response.content)\nOutput:\nJust a friendly reminder to stay vigilant against phishing attack\nPrompt template\nIn Chapter 2 on prompt engineering, you'll learn about prompt templates.\nThey're structured prompts that allow you to run various versions of the same\ntheme. LangChain offers a class called PromptTemplate for this. Its job is to\ngenerate prompts from template structures and input parameters. Below is an\nexample of how to create and execute a prompt from a template, which you\ncan place in a single notebook cell:\nListing 1.1 Creating a prompt from a PromptTemplate\nfrom langchain_core.prompts import PromptTemplate\nsegovia_aqueduct_text = \"The Aqueduct of Segovia (Spanish: Acuedu\nprompt_template = PromptTemplate.from_template(\"You are an experi\nprompt_input = prompt_template.format(\n   text=segovia_aqueduct_text, \n   num_words=20, \n   tone=\"knowledgeable and engaging\")\nresponse = llm.invoke(prompt_input)\nprint(response.content)\nWhen executing the code above you should get this output or something\nsimilar:\nThe Aqueduct of Segovia, a Roman marvel in Spain, dates back to t\nIn the next chapter, I'll show you how to replicate the examples we've just\n\n\nimplemented in LangChain using the plain OpenAI REST API, so you can\nsee the differences.\nA.1.3 Creating chains and executing them with LCEL\nOne of the benefits of using LangChain is its processing technique built\naround the concept of a \"chain.\" A chain is a pipeline of components put\ntogether to achieve a particular outcome. For example, to illustrate (but do\nnot execute this code snippet), you could create a chain to scrape the latest\nnews from a website, summarize it, and email it to someone, like this:\nchain = web_scraping | prompt | llm_model | email_text\nThis declarative, intuitive, and readable method of defining a chain\nshowcases the LangChain Expression Language (LCEL), which I'll cover\nextensively in a later chapter. For now, let's walk through an example by\nreimplementing the previous summarization task using LCEL.\nFirst, set up the chain in a new notebook cell as follows:\nprompt_template = PromptTemplate.from_template(\"You are an experi\nllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,\n                model_name=\"gpt-4o-mini\")\nchain = prompt_template | llm\nNow, this chain is ready to accept any text, target number of words, and\ntarget tone. Execute it as shown below:\nresponse = chain.invoke({\"text\": segovia_aqueduct_text, \n             \"num_words\": 20, \n             \"tone\": \"knowledgeable and engaging\"})\nprint(response.content)\nYou'll get this output, or similar:\nThe Aqueduct of Segovia: A Roman marvel channeling water to the c\nAs you can see, this way of setting up and executing the processing is\nsomewhat simpler than the original imperative approach. The chain pipeline\nworks because both PromptTemplate and ChatOpenAI objects implement a\n",
      "page_number": 458,
      "chapter_number": 53,
      "summary": "However, since many readers may not yet have run an open-source\nLLM locally, we will cover that setup in detail in Appendix E Key topics include prompt, notebook.",
      "keywords": [
        "OpenAI API key",
        "OpenAI",
        "API",
        "OpenAI REST API",
        "key",
        "Jupyter Notebook environment",
        "OpenAI key",
        "Jupyter Notebook",
        "prompt",
        "Notebook",
        "API key",
        "Github",
        "LLM",
        "REST API",
        "OpenAI API"
      ],
      "concepts": [
        "prompt",
        "llm",
        "notebook",
        "key",
        "keys",
        "chains",
        "langchain",
        "local",
        "locally",
        "content"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 7,
          "title": "Segment 7 (pages 53-60)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 6,
          "title": "Segment 6 (pages 41-49)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 47,
          "title": "Segment 47 (pages 398-406)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 37,
          "title": "Segment 37 (pages 322-331)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "Segment 28 (pages 232-240)",
          "relevance_score": 0.56,
          "method": "api"
        }
      ]
    },
    {
      "number": 54,
      "title": "Segment 54 (pages 466-473)",
      "start_page": 466,
      "end_page": 473,
      "detection_method": "topic_boundary",
      "content": "common interface (Runnable) that allows them to be linked. The | syntax is\nsugar for creating a Chain object behind the scenes.\nSo far, I've provided a light introduction to LangChain. My goal was to\nquickly show you what LangChain is, its object model, and how to code with\nit, knowing you're eager to dive in. I've also shared examples of common\nLLM applications to give you an idea of what people are building with\nLLMs. However, if you're not familiar with LLMs, you might want some\nbackground information before moving forward. I'll cover that in the next\nsection. If you already have this knowledge, feel free to skip to the next\nchapter.\n\n\nAppendix B. Setting up a Jupyter\nNotebook environment\nIf you're just starting with Python Jupyter notebooks, think of it as an\ninteractive space where you can type and run code, see the results, and tweak\nthe code to get the outcomes you want in real-time. The code snippets I'll be\nsharing aren't exclusive to a particular Python version, but for the smoothest\nexperience, it's suggested to run them on Python 3.11 or a newer version if\nyou can.\nInstalling the Python interpreter or a Python distribution\nIf you haven't actively used Python lately, I suggest installing the latest\nversion of the Python 3 interpreter. You have a few options:\n1. Standalone Python Interpreter: Download and install Python 3 from\npython.org. Choose the installer appropriate for your operating system\nand follow the setup instructions. You can find OS-specific guidance in\nthe official documentation here:\nhttps://docs.python.org/3/using/index.html (you may need to scroll to\nlocate your platform). This option is ideal if you prefer a lightweight\nPython installation without the additional libraries and tools bundled\nwith distributions like Anaconda or Miniconda.\n2. Anaconda: Download Anaconda from anaconda.com/download. It\ncomes with Python and a wide range of data science libraries, including\ntools for visualization, data analysis, and numerical/statistical work. This\noption is perfect for data science and machine learning projects if you\nhave enough disk space. Anaconda also makes it easy to create virtual\nenvironments for specific projects. It includes Anaconda Navigator, a\nuser-friendly graphical interface, for those who prefer not to use\ncommand-line tools.\n3. Miniconda: Miniconda is a lighter alternative to Anaconda. It allows you\nto manage virtual environments for specific applications without taking\n\n\nup much disk space. It only includes essential data science libraries.\nLearn more at: docs.conda.io/projects/miniconda/.\nFor the remainder of this appendix, let's assume you have Python installed (as\ndescribed in option 1 above). If you've chosen Miniconda or Anaconda, I\nassume you're familiar with Python and can adapt my instructions as needed.\nNow, you're ready to set up a virtual environment using venv, which is a tool\nfor managing virtual environments, and to update pip, the Python package\ninstaller.\nCreating a virtual environment with venv and upgrading pip\nOpen the operating system terminal shell (e.g., cmd on Windows), create a\nproject folder, and navigate into it:\nC:\\Github\\building-llm-applications\\ch01>\nCreate a virtual environment with venv. A virtual environment serves as a\nself-contained Python installation, specifically for the ch01 folder you've\nrecently created. This ensures that package version conflicts with other\nprojects on your machine are avoided. Create a virtual environment for ch01\nusing the following command:\nC:\\Github\\building-llm-applications\\ch01>python -m venv env_ch01\nYou've successfully set up a virtual environment named \"env_ch01.\" Now,\nactivate it using the following command:\nC:\\Github\\building-llm-applications\\ch01>.\\env_ch01\\Scripts\\activ\nYou should now observe the updated operating system prompt, displaying the\nenvironment name in front as \"(env_ch01)\":\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>\nBefore proceeding further, it is useful to upgrade pip, the Python package\nmanagement tool, as follows, so you will be able to install the necessary\nPython packages with no issues:\n\n\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>python -m pip \nFor additional details on the steps you've taken, refer to the following\ndocumentation on the python.org website:\nhttps://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-\nenvironments/\nSetting up a Jupyter notebook\nHaving activated the virtual environment, you're now prepared to configure a\nJupyter notebook for executing prompts with OpenAI models. Proceed to\ninstall Jupyter and the LangChain packages using the following steps:\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>pip install n\nAfter about a minute, the installation of the notebook and LangChain\npackages should be finished. If you'd like, you can confirm it by using the\nfollowing command:\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>pip list\nYou can start the Jupyter notebook by executing the following command:\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>jupyter noteb\nAfter a few seconds, you should see this output in the terminal:\n[I 2023-10-23 22:43:26.870 ServerApp] Jupyter Server 2.8.0 is run\n[I 2023-10-23 22:43:26.870 ServerApp] http://localhost:8888/tree?\n[I 2023-10-23 22:43:26.871 ServerApp]     http://127.0.0.1:8888/t\n[I 2023-10-23 22:43:26.872 ServerApp] Use Control-C to stop this \n[C 2023-10-23 22:43:26.978 ServerApp]\nSubsequently a browser window will open up at this URL:\nhttp://localhost:8888/tree\nOnce you have created the notebook, select: File > Rename â€¦ and name the\nnotebook file: prompt_examples.ipynb.\nFigure B.1 Creating a new Jupyter notebook: create the notebook with the menu File > New >\nNotebook and then rename the file to langchain_examples.ipynb\n\n\nNow, you're prepared to input code into the notebook cells.\nTIP\nIf you are unfamiliar with Jupyter notebook, remember to press Ctrl + Enter\nto execute the code in each cell.\n\n\nAppendix C. Choosing an LLM\nC.1 Popular Large Language Models\nThis appendix outlines the key features of the most popular large language\nmodels available at the time of writing. It also highlights the criteria to\nconsider when selecting the most suitable LLM for your project.\nC.1.1 OpenAI GPT-4o and GPT-4.1 series\nOpenAI’s GPT-4, launched in March 2023, marked a significant milestone as\nthe first “frontier model” to demonstrate advanced reasoning capabilities. It\nwas also one of the earliest multi-modal models, capable of processing text,\nimages, and later video, enabling diverse applications. While its architecture\ndetails remain undisclosed, GPT-4 was rumored to feature 1.8 trillion\nparameters and employed a “Mixture of Experts” design to enhance accuracy\nacross various use cases.\nThe GPT-4o series succeeded GPT-4, offering specialized options for\ndifferent tasks. GPT-4o was optimized for complex, high-accuracy tasks like\nmulti-step reasoning, while GPT-4o-mini targeted simpler applications such\nas summarization, editing, and translation, replacing GPT-3.5 for these use\ncases.\nThe GPT-4.1 series, released in mid-2025, followed GPT-4o and introduced\nrefinements in performance, speed, and cost-efficiency. It retained GPT-4o’s\nstrength in complex reasoning while offering better responsiveness and\nadaptability across a wide range of tasks. With improved balance between\naccuracy and resource usage, GPT-4.1 became a strong fit for both advanced\nand general-purpose applications.\nLangChain integrates seamlessly with OpenAI models through the\nlangchain-openai package and also supports open-source models using\ninference engines compatible with the OpenAI REST API, as detailed in\n\n\nChapter 11.\nC.1.2 OpenAI o1 and o3 series\nIn September 2024, OpenAI introduced o1, designed for advanced reasoning\nand multi-step problem-solving with higher accuracy but at significantly\nhigher costs — over 30 times that of GPT-4o-mini.\nIn December 2024, OpenAI announced the upcoming release of o3 and GPT-\no3-mini, scheduled for early 2025, as an evolution of the o1 model with\nenhanced performance and capabilities.\no3 demonstrates exceptional results across various benchmarks. On the ARC-\nAGI test, which evaluates “general AI” capabilities, it scored 87.5%,\nsurpassing the 87% achieved by top human performers. For comparison,\nGPT-3 scored 0%, and GPT-4o scored around 5%. In the Frontier Math\nbenchmark, o3 solved 25% of complex problems, a ground-breaking\nachievement since no prior LLM managed more than 2%. Its programming\nabilities were equally remarkable, achieving 99% on a competitive\nprogramming benchmark where only top human programmers reach 100%.\no3 is positioned as the first LLM nearing AGI-level (Artificial General\nIntelligence) capabilities, setting new standards in reasoning, problem-\nsolving, and programming.\nC.1.3 Gemini\nIn December 2023, Google introduced Gemini 1.5, a family of multimodal\nmodels capable of processing text, audio, images, and code. The lineup\nranged from Gemini Ultra, designed for advanced tasks using a Mixture of\nExperts architecture, to Gemini Nano, optimized for lightweight mobile\napplications. Gemini Ultra surpasses PaLM-2 in benchmarks, excelling in\nreasoning, math comprehension, and code generation.\nGemini Ultra’s standout feature is its large context window, accommodating\nvast data inputs: up to 1 hour of video, 11 hours of audio, codebases\nexceeding 30,000 lines, or over 700,000 words. Research tests extended to 10\n\n\nmillion tokens, demonstrating its capacity for extensive workloads.\nGemini 2.0, introduced in December 2024, marks a significant advancement\nin AI, enabling sophisticated reasoning for tasks like multimodal queries,\ncoding, and complex math, powered by Trillium TPUs. Its leading model,\nGemini 2.0 Flash, offers fast, high-performance capabilities with support for\nmultimodal input and output, including text, images, video, audio, and\nsteerable multilingual TTS. Enhancing its predecessor, 2.0 Flash operates at\ntwice the speed of 1.5 Pro, integrates native tools, and sets new standards in\nAI performance.\nGemini 2.5, launched in May 2025, builds on the foundation of Gemini 2.0\nwith enhanced performance, efficiency, and broader multimodal capabilities.\nDesigned for both advanced and real-time applications, Gemini 2.5 Pro offers\nimproved reasoning, longer context handling, and faster response times\nacross text, image, audio, and video inputs. It introduces better integration\nwith Google's ecosystem, tighter tool use, and more reliable multilingual\nsupport, positioning it as a competitive alternative to other state-of-the-art\nmodels.\nLangChain offers seamless access to Gemini models through the langchain-\ngoogle-genai package, enabling developers to integrate these powerful\nmodels into their applications effortlessly.\nC.1.4 Gemma\nIn February 2024, Google rolled out Gemma, an open-source counterpart to\nGemini, designed with a focus on lightweight functionality. Built on the same\nresearch and tech-stack as Gemini, Gemma offers model weight files at 2B\nand 7B parameters, freely accessible for use. The models are optimized to run\non NVIDIA GPUs and Google Cloud TPUs. Additionally, Gemma is\nprovided with a toolkit to facilitate effective utilization for both fine-tuning\nand inference tasks.\nC.1.5 Claude\nIn March 2023, Anthropic and Google launched Claude, a language model\n",
      "page_number": 466,
      "chapter_number": 54,
      "summary": "This chapter covers segment 54 (pages 466-473). Key topics include model, python, and gemini. The | syntax is\nsugar for creating a Chain object behind the scenes.",
      "keywords": [
        "Python",
        "Gemini",
        "virtual environment",
        "Jupyter notebook",
        "Python Jupyter notebooks",
        "Jupyter",
        "Github",
        "Notebook",
        "models",
        "virtual",
        "environment",
        "Anaconda",
        "Gemini Ultra",
        "env",
        "Python interpreter"
      ],
      "concepts": [
        "model",
        "python",
        "gemini",
        "openai",
        "installing",
        "offering",
        "offers",
        "notebook",
        "package",
        "reasoning"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Effective-Python",
          "chapter": 39,
          "title": "Segment 39 (pages 411-419)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Effective-Python",
          "chapter": 40,
          "title": "Segment 40 (pages 420-428)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "Segment 28 (pages 232-240)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 41,
          "title": "Segment 41 (pages 342-350)",
          "relevance_score": 0.6,
          "method": "api"
        }
      ]
    },
    {
      "number": 55,
      "title": "Segment 55 (pages 474-483)",
      "start_page": 474,
      "end_page": 483,
      "detection_method": "topic_boundary",
      "content": "designed with a strong emphasis on honesty and safety. Claude excels in\nsummarization, coding, writing, and chatbot-based question answering.\nAlthough Anthropic has not shared details about its architecture or training,\nClaude set a new standard by supporting 100,000 tokens, which at the time\nwas the biggest context window, enabling it to handle complex tasks. To\nimprove speed, Anthropic released Claude Instant, optimized for faster\nperformance.\nSubsequent updates, including Claude 2 in July 2023, Claude 3 in March\n2024, Claude 3.5 in October 2024, and Claude 3.7 in February 2025,\nenhanced accuracy, safety, and versatility across diverse language tasks.\nClaude 3.7 is available in the bigger accurate version, called Sonnet and the\nsmaller faster version called Haiku. Claude 3.7 Sonnet is in the same segment\nas GPT-4.1 and Gemini 2.5 Pro. Haiku can be considered in the same group\nas GPT-4o-mini.\nAnthropic models, including the Claude family, are accessible by installing\nthe langchain-anthropic package.\nC.1.6 Cohere\nCohere, backed by former Google Brain employees, including Aidan Gomez,\nco-author of the influential \"Attention Is All You Need\" paper, focuses\nexclusively on enterprise needs. Known for precision and consistency,\nCohere offers models ranging from 6 billion to 52 billion parameters,\nallowing organizations to tailor their approach.\nCohere has achieved the highest accuracy among large language models at\nsome point, making it a dependable choice for businesses. Major corporations\nlike Jasper and Spotify have adopted Cohere for advanced natural language\nprocessing, highlighting its practical applicability. However, it's worth noting\nthat Cohere's technology comes at a higher cost compared to more widely\nrecognized models from OpenAI. You can access Cohere models from\nLangChain by installing the langchain-cohere package.\nC.1.7 Llama\n\n\nMeta's Llama series of large language models has played a major role in\nadvancing open-access AI. First introduced in 2023 with a 70B-parameter\nmodel, Llama quickly evolved into a flexible, open-source platform with\nsmaller variants to suit different computational needs. Built on transformer\narchitecture and trained on diverse public datasets, Llama inspired derivative\nmodels like Vicuna and Orca. Over 2023 and 2024, Meta released Llama-2\nand Llama-3, including a 405B-parameter model and the introduction of\nvision capabilities. In April 2025, Meta launched Llama 4, featuring major\narchitectural improvements. The Llama 4 lineup includes Scout (109B\nparameters, multimodal, 10M token context), Maverick (400B parameters,\noptimized for reasoning), and Behemoth (a 2T-parameter model still in\ntraining, aimed at outperforming current benchmarks). While open-weight,\nthese models come with licensing constraints for commercial use. Llama’s\nscalability, openness, and compatibility with tools like LangChain—via\nwrappers such as GPT4All and Ollama—make it a powerful resource for\nboth researchers and developers in AI and NLP.\nC.1.8 Falcon\nDeveloped by the Technology Innovation Institute, Falcon is a transformer-\nbased, causal decoder-only model designed for natural language processing\ntasks.\nFalcon 3, released in December 2024, demonstrates strong performance\ncompared to its peers. For instance, Falcon 3 10B achieves results\ncomparable to Qwen 2.5 7B and Gemma 9B across benchmarks like MUSR\nand MMLU_PRO, while outperforming them on the MATH benchmark. This\npositions Falcon 3 as a competitive option among contemporary open-source\nLLMs.\nC.1.9 Mistral\nFounded in 2023, Mistral AI has rapidly become a leader in open-source\nlarge language models, known for its efficient and high-performing designs.\nIt introduced the Mistral 7B model, followed by the Mixtral 8x7B and 8x22B\nmodels—both using Sparse Mixture-of-Experts (MoE) architectures that\nactivate only a subset of parameters per token to improve cost-efficiency and\n\n\nperformance. In May 2025, the company released Mistral Medium 3, a mid-\nsized model optimized for enterprise use, offering strong performance at\nlower cost and supporting hybrid deployments. Mistral's ongoing innovation\nand open-source approach position it as a major competitor in the AI space,\nbalancing scalability, affordability, and flexibility across a range of\napplications.\nC.1.10 Qwen\nQwen, open-sourced on GitHub in August 2023, is a family of LLMs\ndeveloped by Alibaba Cloud. The models range from 1.8 billion to 72 billion\nparameters, trained on datasets between 2.2 trillion and 3 trillion tokens.\nWhile Qwen models are designed for general purposes, there are fine-tuned\nversions for specific tasks, such as Code-Qwen for coding and Math-Qwen\nfor mathematics. Qwen-Chat has been fine-tuned using Reinforcement\nLearning from Human Feedback (RLHF), similar to ChatGPT. The models\nsupport a context length of around 30,000 tokens and perform particularly\nwell in English and Chinese.\nC.1.11 Grok\nGrok, developed by xAI, has rapidly advanced since its debut in late 2023.\nInitially focused on conversational tasks and integrated with the X platform,\nGrok used a mixture-of-experts architecture and supported up to 128,000\ntokens. Subsequent versions—Grok-2 and Grok-2 Mini—added features like\nimage generation and improved performance and speed. In February 2025,\nxAI released Grok 3, significantly upgraded with 10x more compute, new\nreasoning modes (\"Think\" and \"Big Brain\"), real-time data retrieval via\n\"DeepSearch,\" and image editing capabilities. In May 2025, Grok 3.5\nlaunched in beta with enhanced technical reasoning and a redesigned RAG\nsystem. Microsoft also partnered with xAI to host Grok 3 and Grok 3 Mini on\nAzure AI Foundry, bringing the models to a broader enterprise audience.\nC.1.12 Phi\nThe Phi-3 family comprises small language models (SLMs) designed to\nprovide many capabilities of large language models while being more\n\n\nresource-efficient. These models outperform others of the same and next size\nup in benchmarks for language, coding, and math tasks, thanks to Microsoft’s\ninnovative training methods. The Phi-3-mini (3.8 billion parameters) delivers\nexceptional performance, rivaling models twice its size, with future releases\nincluding Phi-3-small (7 billion parameters) and Phi-3-medium (14 billion\nparameters). The models are accessible through Microsoft Azure AI Model\nCatalog, Hugging Face, Ollama for local deployment, and as NVIDIA NIM\nmicroservices with standard API interfaces.\nPhi-3.5-MoE, the latest addition to the Phi family, is a lightweight, state-of-\nthe-art open model optimized for reasoning-intensive tasks such as code,\nmath, and logic. It supports multilingual applications with a 128K token\ncontext length. Developed using high-quality datasets, it incorporates\nadvanced fine-tuning and optimization techniques for precise instruction\nadherence and robust safety. Designed for memory-constrained and latency-\nsensitive environments, Phi-3.5-MoE powers general-purpose AI systems and\nis accessible through Azure AI Studio and GitHub via a serverless API,\noffering scalable and cost-efficient deployment.\nC.1.13 DeepSeek\nDeepSeek, a Chinese AI company founded in 2023 by Liang Wenfeng, has\ndeveloped open-source large language models (LLMs) that rival leading\nsystems in performance and cost-efficiency. Its DeepSeek-V3 model, a\nMixture-of-Experts (MoE) architecture with 671 billion parameters\n(activating 37 billion per token), was trained on 14.8 trillion tokens using\nsupervised fine-tuning and reinforcement learning. Despite its scale, training\nrequired only 2.788 million GPU hours on Nvidia H800 chips, costing less\nthan $6 million. It outperforms other open-source models and matches top\nproprietary systems, excelling in mathematics, programming, reasoning, and\nmultilingual tasks, with its AI assistant surpassing ChatGPT as the top free\napp on Apple’s App Store.\nDeepSeek-R1 also competes with high-end LLMs like OpenAI’s o1,\nspecializing in complex reasoning and coding through \"chain-of-thought\"\nreasoning. Trained with 2,000 Nvidia H800 chips at a cost of $5.6 million,\nR1’s efficiency sparked debates on sustainable AI training. While both\n\n\nmodels are open-source, they avoid politically sensitive topics, raising\nprivacy concerns and prompting global discussions about China’s growing\ninfluence in AI and the shifting balance of technological power.\nTable C.1 provides a summary of the key characteristics of the LLMs\ndiscussed in this appendix.\nTable C.1 Comparison among LLM models (*sizes with an asterisk are estimated)\nModel\nDeveloper\nLaunch\nNum\nparams\nMax\ntokens\nOpen\nsource\nGPT-4o\nOpenAI\nMay 24\n200B*\n128K\nNo\nGPT-4.1\nOpenAI\nApr 25\nN/A\n1M\nNo\no1\nOpenAI\nOct 24\n400B*\n128K\nNo\no3\nOpenAI\nDec 24\n200B*\n128K\nNo\nGemini 2.5\nPro\nGoogle\nMay 25\nN/A\n1M\nNo\nGemma-3\nGoogle\nMar 25\n1B-27B\n128K\nYes\nClaude\nSonnet-3.7\nAnthropic\nFeb 25\nN/A\n200K\nNo\nCommand R+\nCohere\nAug 24\n100B\n128K\nNo\nLlama-4\nMeta\nSep 24\n90B\n128K\nYes\nFalcon3-10B-\nBase\nTII-UAE\nDec 24\n10B\n32K\nYes\nMixtral-\n8x22B-\nInstruct-v0.1\nMistral AI\nMar 24\n141B\n64K\nYes\nQwen2.5 72B\nAlibaba\nCloud\nSep 24\n72B\n128K\nYes\nGrok 3.5\nxAI\nFeb 25\n314B\n1M\nNo\nDeepSeek-V3\nDeepseek\nAI\nDec 24\n671B\n128K\nYes\nDeepSeek-R1\nDeepseek\nAI\nJan 25\n671B\n132K\nYes\nPhi-4.0\nMicrosoft\nDec 24\n14B\n128K\nYes\n\n\nC.2 How to choose a model\nEach use case has its unique requirements. Selecting the ideal LLM for your\nspecific needs can be a complex task. It often involves testing multiple\nmodels to identify the most suitable one. However, you can employ certain\ncriteria to streamline the selection process.\nSeveral key factors to consider include:\nModel Purpose\nProprietary vs. Open Source\nModel size\nContext Window size\nSupported Languages\nAccuracy vs. Speed\nCost and Hardware Requirements\nTask suitability\nSafety and Bias\nIn the following sections, I’ll explore these factors in greater depth to help\nyou make a well-informed decision.\nC.2.1 Model Purpose\nSome LLMs are flexible and can manage various tasks, while others are\ntailored for specific functions. For instance, OpenAI's GPT-4 or Gemini 1.5\nPro are versatile and adaptable models, suitable for a range of tasks, while\nCode Llama, or Qwen Coder as their names suggest, are specialized for\ngenerating programming code.\nSpecialized models are usually smaller than their more general counterparts.\nIf you have a clear use case in mind, it's a good idea to choose an LLM\nspecifically created and trained for that purpose. There are common LLM\nspecializations, ranging from simpler tasks like text classification and\nsentiment analysis to more advanced functions like text summarization,\ntranslation, code analysis, text generation, question-answering, and logic and\nreasoning.\n\n\nMany foundational LLMs can handle combinations of these functions,\noffering a wide range of possibilities to meet your specific needs.\nC.2.2 Proprietary vs. Open-Source\nMany language models are proprietary, meaning their developers keep the\ninternal details private. Information such as architecture, parameter count\n(discussed in the next section), or training specifics is often undisclosed.\nProprietary models from providers like OpenAI, Gemini, Cohere, Anthropic\nand Stability AI are typically offered as cloud-based subscription services\naccessed through REST APIs. Pricing depends on the accuracy of the model\nused and the number of tokens processed. This approach provides\nconvenience, allowing users to access ready-to-use services without\nmanaging hardware or infrastructure. However, data submitted to these\nservices is retained and potentially processed by the provider, which can be a\nconcern for sensitive data.\nIn contrast, open-source models offer full transparency, providing access to\ntheir underlying implementation as well as detailed information about their\narchitecture and training processes. This transparency brings two key\nadvantages. First, it enables you to deploy a fully private solution, avoiding\nthe need to send sensitive data to third-party vendors—an important\nconsideration for privacy and security. Second, it allows for fine-tuning on\ndomain-specific datasets, giving you the flexibility to adapt the model to your\nunique needs. The trade-off, however, is that you typically need to host and\nmanage the infrastructure yourself, which involves provisioning GPUs and\ndedicating IT resources to maintain and scale the service. As an alternative,\nyou can opt for managed LLM hosting platforms such as IBM Watsonx,\nAmazon Bedrock, Azure AI Studio, or Google Vertex AI. These services\nprovide a scalable, secure environment for running both open-source and\nproprietary models—eliminating much of the operational complexity\nassociated with self-hosting.\nLangChain supports both proprietary models, such as OpenAI, Gemini, and\nCohere, and open-source models through inference engine wrappers like\nOllama.\n\n\nC.2.3 Model size (Number of Parameters)\nThe A model's parameters represent its internal variables, specifically the\nweights in the artificial neural network. These weights are adjusted during\ntraining and enable the model to learn patterns from data. More parameters\nallow a model to capture greater complexity and nuance, potentially\nincreasing accuracy. However, larger models with more parameters require\nsubstantial hardware resources—such as increased memory and high-\nperformance GPUs—and often result in higher latency during inference.\nCompression techniques can reduce a model's size without major loss of\naccuracy, which we’ll discuss in the chapter on open-source LLMs.\nLanguage models vary widely in parameter count, from trillions in GPT-4\nand Gemini 1.5 Ultra to a few billion in Mistral and Gemma. Parameters\nform the foundation of a model’s ability to process text tasks.\nC.2.4 Context Window size\nThe number of input tokens allowed in Large Language Models (LLMs)\ndirectly impacts their functionality and suitability for different tasks. Token\nlimits vary across models, influencing the complexity of prompts they can\nhandle.\nModels with smaller token limits are well-suited for concise prompts and\nstraightforward interactions. These limitations often stem from design\nchoices or computational constraints, and such models are typically\nspecialized for a narrow set of tasks. In contrast, LLMs with higher token\ncapacities—often designed as generalists—can handle more detailed, context-\nrich inputs, making them better suited for complex, multi-step tasks.\nChoosing the right token limit depends on the task. For short, straightforward\ninputs, smaller token allowances may suffice. For applications requiring\ndetailed interactions or extended context, models with larger token limits are\nmore appropriate.\nIn summary, token capacity is a key consideration when selecting an LLM.\nAligning the token limit with your project's requirements ensures effective\n\n\nuse of the model’s capabilities.\nC.2.5 Multilingual Support\nWhen considering an LLM for multilingual support, it's essential to research\nwhich one aligns with your requirements. Some LLMs are proficient in\nmultiple languages, including ancient ones like Latin, ancient Greek, and\neven Phoenician, while others are primarily English-focused. Typically,\nLLMs excel with languages that have extensive training data, like English,\nChinese, and Spanish. If your needs involve a less common language, you\nmight need to seek a specialized LLM or even undertake fine-tuning yourself.\nIt's crucial to match your language requirements with the capabilities of the\nchosen LLM for optimal results.\nC.2.6 Accuracy vs. Speed\nSelecting Choosing an LLM often requires balancing accuracy and\nprocessing speed. Larger models with more parameters deliver higher\nprecision and nuanced responses, especially for complex language tasks.\nHowever, this accuracy comes at the cost of slower processing and\nsignificant computational requirements.\nSmaller models, with fewer parameters, are faster and better suited for real-\ntime applications. While they sacrifice some accuracy, they excel in tasks\nrequiring quick responses. The decision between a large or small model\ndepends on the specific needs of the application—detailed comprehension\nfavors larger models, while speed-sensitive tasks benefit from smaller ones.\nAdvances in compression techniques, discussed in Chapter 10 on open-\nsource models, have bridged this gap. Compact models with lower parameter\ncounts now achieve accuracy comparable to much larger models, making the\ntrade-off between speed and precision less of a limitation.\nC.2.7 Cost and Hardware Requirements\nCost and hardware requirements are key factors when deploying Large\nLanguage Models (LLMs). Organizations must carefully weigh financial and\n\n\ntechnical considerations to ensure effective use of these models.\nProprietary LLMs are typically priced based on their accuracy and the\nnumber of tokens processed. While they deliver high precision, enhanced\ncapabilities result in higher costs, requiring organizations to balance accuracy\nagainst budget constraints.\nOpen-source LLMs, though more affordable in terms of licensing, shift the\ncost burden to hardware and infrastructure. Deploying these models demands\npowerful GPUs, sufficient RAM, and reliable virtual machines. Additionally,\norganizations need skilled IT staff to manage deployment, maintenance, and\nsupport.\nThe choice between proprietary and open-source LLMs depends on\norganizational goals and resources. Proprietary models offer convenience and\nperformance for a fee, while open-source models provide flexibility and cost\nsavings at the expense of hardware and IT investment. Careful evaluation\nensures the solution aligns with both objectives and available resources.\nC.2.8 Task suitability (standard benchmarks)\nThe effectiveness of a language model for specific tasks depends on its\narchitecture, size, training data, and fine-tuning. Different models are\noptimized for different use cases, and standardized benchmarks help evaluate\ntheir performance across various tasks.\nLeaderboards like the Hugging Face Open LLM Leaderboard and LMSYS\nChatbot Arena Leaderboard offer a centralized way to compare models across\nbenchmarks, streamlining the evaluation process. Below are some widely\nrecognized benchmarks, with descriptions drawn from the corresponding\nresearch publications.\nInstruction-Following Evaluation (IFEval): IFEval is an evaluation\nbenchmark for Large Language Models (LLMs) designed to assess their\nability to follow natural language instructions. It uses 25 types of\n\"verifiable instructions,\" such as word count or keyword mentions, and\nincludes 500 prompts. IFEval provides a simple, reproducible alternative\nto human evaluations, which are costly and inconsistent, and LLM-\n",
      "page_number": 474,
      "chapter_number": 55,
      "summary": "This chapter covers segment 55 (pages 474-483). Key topics include models, tasks, and tokens.",
      "keywords": [
        "models",
        "large language models",
        "language models",
        "LLMs",
        "language",
        "Claude",
        "tasks",
        "large language",
        "LLM",
        "parameters",
        "Cohere",
        "open-source",
        "open-source models",
        "token",
        "Proprietary models"
      ],
      "concepts": [
        "models",
        "tasks",
        "tokens",
        "parameters",
        "open",
        "required",
        "requirements",
        "require",
        "llms",
        "language"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.76,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.7,
          "method": "api"
        }
      ]
    },
    {
      "number": 56,
      "title": "Segment 56 (pages 484-491)",
      "start_page": 484,
      "end_page": 491,
      "detection_method": "topic_boundary",
      "content": "based evaluations, which may be biased. Paper:\nhttps://arxiv.org/abs/2311.07911.\nBig Bench Hard (BBH): The BIG-Bench Hard (BBH) benchmark is a\nsubset of 23 challenging tasks from the BIG-Bench evaluation suite,\ndesigned to test areas where language models have historically\nunderperformed compared to average human raters. These tasks often\nrequire multi-step reasoning and highlight the limitations of language\nmodels without advanced prompting techniques. Using chain-of-thought\n(CoT) prompting, models like PaLM and Codex demonstrated\nsignificant improvements, surpassing human performance on 10 and 17\ntasks, respectively. The benchmark emphasizes that few-shot prompting\nalone underestimates the potential of language models, and CoT\nprompting reveals their advanced reasoning capabilities, particularly as\nmodel scale increases. Paper: https://arxiv.org/abs/2210.09261.\nMathematics Aptitude Test for Euristics - Level 5 (MATH, L5): The\nMATH benchmark is a dataset of 12,500 challenging mathematics\nproblems designed to evaluate the mathematical problem-solving\nabilities of machine learning models. Each problem includes a detailed\nstep-by-step solution, enabling models to learn answer derivations and\nexplanations. Alongside MATH, an auxiliary pretraining dataset is\nprovided to help models grasp fundamental mathematical concepts.\nDespite improvements, model accuracy on MATH remains low,\nindicating that scaling Transformer models alone is insufficient for\nstrong mathematical reasoning. Advancing this capability will likely\nrequire novel algorithms and research breakthroughs. Paper:\nhttps://arxiv.org/abs/2103.03874.\nGoogle Proof Q&A (GPQA): GPQA is a dataset of 448 multiple-choice\nquestions in biology, physics, and chemistry, created by domain experts\nto be exceptionally difficult. Expert participants with advanced degrees\nachieve 65% accuracy (74% excluding identified errors), while non-\nexperts with unrestricted web access average only 34%, making the\nquestions effectively \"Google-proof.\" GPQA is designed to aid in\ndeveloping scalable oversight methods, enabling humans to supervise\nand extract reliable, truthful information from AI systems, even those\nsurpassing human capabilities. Paper: https://arxiv.org/abs/2311.12022.\nMultistep Soft Reasoning (MuSR): MuSR is a benchmark designed to\nevaluate LLMs on multistep reasoning tasks framed within natural\n\n\nlanguage narratives. It features complex reasoning scenarios, such as\n1000-word murder mysteries, generated using a neurosymbolic\nsynthetic-to-natural algorithm. These tasks are challenging for advanced\nmodels and can scale to match future LLM advancements. MuSR\nemphasizes real-world reasoning, offering narratives that are more\nrealistic and difficult than typical synthetic benchmarks while remaining\naccessible for human annotation. The benchmark highlights limitations\nin current reasoning techniques, such as chain-of-thought prompting,\nand helps identify areas for improvement in robust reasoning\ncapabilities.Paper: https://arxiv.org/abs/2310.16049.\nMore Robust Massive Multitask Language Understanding (MMLU-\nPRO): MMLU-Pro is an advanced benchmark that builds on the\nMassive Multitask Language Understanding (MMLU) dataset by\nintroducing more challenging, reasoning-focused questions and\nexpanding answer options from four to ten. It removes trivial and noisy\nquestions, resulting in a dataset that better evaluates complex reasoning\ncapabilities. Compared to MMLU, MMLU-Pro decreases model\naccuracy by 16% to 33% and reduces sensitivity to prompt variations,\nimproving stability. Models using Chain of Thought (CoT) reasoning\nperform better on MMLU-Pro, highlighting its emphasis on complex\nreasoning over direct answering. This benchmark provides a more\neffective tool for tracking advancements in AI reasoning and\ncomprehension. Paper: https://arxiv.org/abs/2406.01574.\nThe table below provides a snapshot of how leading LLMs perform across\nkey benchmarks at the time of publication. For the most accurate insights,\ncheck for updated scores on the latest versions of the models you plan to use.\nTable C.2 Benchmark scores (%) of the most popular LLMs\nModel\nIfEval BBH\nMATH-\nL5\nGPQA\nMuSR\nMMLU-\nPRO\nGPT-4o\n85.60\n83.10\n76.60\n53.60\nN/A\n74.68\nGemini\n2.0 Flash\nN/A\n86.80\n89.70\n62.10\nN/A\n76.40\nClaude 3.5\nSonnet\n88.00\nN/A\n71.1\n59.40\nN/A\n78.00\n\n\nCommand\nR+\nN/A\nN/A\n44.0\nN/A\nN/A\nN/A\nGrok 2\n75.50\nN/A\n76.1\nN/A\nN/A\nN/A\nDeepSeek-\nV3\n86.10\nN/A\nN/A\n59.10\nN/A\n75.90\nQwen2.5-\n72B-\nInstruct\n86.38\n61.87\n1.21\n16.67\n11.74\n51.40\nQwen2.5-\nCoder-\n32B\n43.63\n48.51\n30.59\n12.86\n15.87\n47.81\nQwen2.5-\nMath-7B\n24.60\n22.01\n30.51\n5.82\n5.00\n19.09\nLlama-\n3.3-70B-\nInstruct\n89.98\n56.56\n0.23\n10.51\n15.57\n48.13\nMixtral-\n8x22B-\nInstruct-\nv0.1\n71.84\n44.11\n18.73\n16.44\n13.49\n38.70\nMistral-\n7B-v0.3\n22.66\n23.95\n3.02\n5.59\n8.36\n21.70\nGemma-2-\n27b-it\n79.78\n49.27\n0.76\n16.67\n9.11\n38.35\nFalcon3-\n10B-Base\n36.48\n41.38\n24.77\n12.75\n14.17\n36.00\nPhi-3.5-\nMoE-\ninstruct\n69.25\n48.77\n22.66\n14.09\n17.33\n40.64\nC.2.9 Safety and Bias\nEnsuring the safety and fairness of Large Language Models (LLMs) is\nessential for ethical and responsible AI use. The focus is on preventing\nharmful, offensive, or biased outputs that could negatively impact individuals\nor reinforce stereotypes.\n\n\nTo improve safety, LLMs must avoid generating content that is sexist, racist,\nor hateful. Since these models learn from extensive internet text, which can\ncontain harmful material, implementing robust filtering and monitoring is\ncritical to align outputs with ethical standards.\nReducing bias is equally important. Without careful oversight, LLMs can\nunintentionally perpetuate stereotypes. For example, assuming doctors are\nmale and nurses are female reinforces outdated gender biases, undermining\ninclusivity and fairness.\nThis challenge is especially important in fields like healthcare, customer\nservice, and education, where unbiased and accurate responses are vital.\nRegularly refining training data and fine-tuning models are necessary steps to\nensure fairness and inclusivity.\nIn summary, promoting safety and reducing bias in LLMs requires\ncontinuous effort and a commitment to ethical AI practices. As you explore\nand implement AI solutions, prioritize testing and refinement to address\npotential issues, as covered in the next section.\nC.2.10 A practical example\nLet’s integrate all the key factors and decisions involved in selecting an LLM\nfor a specific application, using a practical example. Imagine implementing a\ncode assistant chatbot to generate Python snippets for stock trading strategies.\nThe target users are software developers supporting traders in a financial\norganization.\nThe code generator must utilize proprietary in-house trading libraries, making\nconfidentiality a critical concern. Company policy strictly prohibits\nsubmitting any code referencing proprietary trading libraries to external\nsystems. The assistant must generate code quickly while maintaining high\naccuracy to minimize bugs and avoid delays, especially during volatile\ntrading periods. As this is for a financial company, cost is not a limiting\nfactor.\nThe flowchart below illustrates the decision-making workflow for selecting\nan LLM for this use case.\n\n\nFigure C.1 Flowchart for selecting an LLM for a Python coding assistant designed to generate\ncode for trading strategies. The process begins with choosing between a general-purpose and a\nspecialized model, with a preference for LLMs tailored for code generation, particularly in\nPython. The next step focuses on selecting open-source models to comply with strict IT policies\nrequiring all proprietary code to remain confidential. The final decision balances speed and\naccuracy, leading to the selection of Qwen2.5 Coder 32b based on its strong performance,\nincluding a high Python HumanEval score for accurate Python code generation.\n\n\nApp designer Choosing an LLM to generate Python\ncode for proprietary stock trading models\nrequires a focus on confidentiality, high\nperformance, and precision. Speed and\naccuracy are equally important, while\n\ncost i not a limiting factor.\n\nDefine\nRequirements\n\nGeneral Specialized:\n\ncoding in Python\n\nPublic and\nproprietary\n\nMore accurate:\nFaster and cheaper: big model (more\nSmall model expensive)\n\nDeepseek Coder 7b\nCode Llama 7b Python\nStarCoder2 3b\n\nCode Llama 70b Instruct\nCode Llama 70b Python\n\nQwen2.5 Coder 32b\nPhind CodeLlama 34b\n\nFast & accurate:\nmedium model\n(medium cost)\n\nThe process begins with defining requirements and progresses through the\nfollowing decision points:\n1. General-purpose vs. Specialized Models: The focus is on specialized\nLLMs optimized for code generation, particularly for Python. These\nmodels are better suited to the task than general-purpose models.\n2. Privacy and Confidentiality: Given the sensitivity of the proprietary\ntrading libraries, company policy mandates that the code cannot be\nexposed externally. This requires selecting an open-source model to\nensure data remains within the organization.\n3. Model Size: While cost is not a concern, the model must balance\naccuracy and speed. A medium-sized model is ideal to achieve this\ntrade-off. The selection process narrows the options to a shortlist of\nseven LLMs, including general-purpose code generators and Python-\nspecific models.\n4. Accuracy Evaluation: The shortlisted models are evaluated using the\nPython HumanEval score (refer to Table C.3, sourced from Hugging\nFace leaderboard at: https://huggingface.co/spaces/bigcode/bigcode-\nmodels-leaderboard), which measures Python code generation accuracy.\nQwen2.5-Coder-32b is chosen for its superior balance of accuracy and\nspeed compared to other options.\nTable C.3 Python HumanEval rankings, sourced from the Hugging Face leaderboard\nModel\nhumaneval-python\nQwen2.5-Coder-32B-Instruct\n83.20\nDeepSeek-Coder-7b-instruct\n80.22\nCodeLlama-70b-Instruct\n75.60\nPhind-CodeLlama-34B-v2\n71.95\nCodeLlama-70b-Python\n55.49\nCodeLlama-7b-Python\n40.48\nStarCoder2-7B\n34.09\nAlthough Qwen2.5-Coder-32b has been selected as the top candidate, it is\nvaluable to evaluate other strong alternatives, including Phind CodeLlama\n34b, Deepseek Coder 7b, and Code Llama 7b Python.\n\n\nC.3 A Word of Caution\nLLMs Large Language Models (LLMs) have revolutionized natural language\nprocessing, but they come with limitations and risks you need to understand.\nBias: LLMs can inherit and reproduce biases present in their training\ndata, which may result in harmful stereotypes or unfair outcomes.\nAddressing these issues is crucial for building ethical and trustworthy AI\nsystems. If your application involves sensitive user profiles, it's\nimportant to implement guardrails—such as content filtering, human\noversight, or prompt engineering—and to evaluate multiple LLMs to\nidentify the one that exhibits the least bias in your specific use case.\nPrivacy and Security: Proprietary LLMs may use prompt data for model\nimprovement, which can raise privacy and confidentiality concerns. To\nmitigate this risk, consider deploying open-source models in a private\nenvironment where you retain full control over data handling. Some\nproviders, such as OpenAI, offer enterprise plans that guarantee prompt\ndata will not be used for training. However, it ultimately comes down to\nwhether you trust the provider to uphold their data privacy\ncommitments.\nHallucinations: LLMs sometimes produce incorrect or fabricated\nanswers, known as hallucinations. Transparency and explainability are\nkey when users question an output. Tools like LangChain’s evaluation\nframework help reduce hallucinations by improving response accuracy.\nResponsibility and Liability: The legal landscape for LLMs remains\nunclear. Determining who is responsible for errors or harm caused by\nLLM outputs is complex. Providers often include disclaimers in their\nterms and chatbot interfaces to limit liability. When deploying an LLM,\nconsider accountability standards and define clear usage guidelines.\nIntellectual Property (IP) Rights: LLMs trained on unrestricted data can\ninadvertently generate content that violates IP laws. Some models\naddress this by training exclusively on public domain material to avoid\nlegal issues. If your application requires strict IP compliance, verify that\nyour chosen LLM avoids proprietary or copyrighted content.\nUnderstanding these challenges ensures you deploy LLMs responsibly\nand avoid unintended risks.\n",
      "page_number": 484,
      "chapter_number": 56,
      "summary": "This chapter covers segment 56 (pages 484-491). Key topics include model, llms, and reasoning. Using chain-of-thought\n(CoT) prompting, models like PaLM and Codex demonstrated\nsignificant improvements, surpassing human performance on 10 and 17\ntasks, respectively.",
      "keywords": [
        "models",
        "LLMs",
        "Python",
        "code",
        "language models",
        "reasoning",
        "language",
        "Python code",
        "Code Llama",
        "LLM",
        "Large Language Models",
        "Multitask Language Understanding",
        "accuracy",
        "Python code generation",
        "benchmark"
      ],
      "concepts": [
        "model",
        "llms",
        "reasoning",
        "python",
        "prompt",
        "require",
        "requiring",
        "requirements",
        "biased",
        "biases"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 8,
          "title": "Segment 8 (pages 61-70)",
          "relevance_score": 0.72,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 21,
          "title": "Segment 21 (pages 168-175)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "Segment 5 (pages 34-43)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 32,
          "title": "Segment 32 (pages 292-299)",
          "relevance_score": 0.62,
          "method": "api"
        }
      ]
    },
    {
      "number": 57,
      "title": "Segment 57 (pages 492-499)",
      "start_page": 492,
      "end_page": 499,
      "detection_method": "topic_boundary",
      "content": "Appendix D. Installing SQLite on\nWindows\nSQLite does not require a full installation. Simply unzip the package, place it\nin a folder, and add the folder to your system's Path environment variable.\nFollow these steps for Windows setup. For other operating systems, refer to\nthe SQLite documentation.\n1. Download SQLite:\nGo to the SQLite download page:\nhttps://www.sqlite.org/download.html.\nDownload the latest zipped tools package, e.g., sqlite-tools-\nwin-x64-3460100.zip.\n2. Extract Files:\nUnzip the downloaded file to a folder, for instance, C:\\sqlite.\nAfter unzipping, you should see the files, including the SQLite\nexecutable, in C:\\sqlite\\sqlite-tools-win-x64-3460100.\n3. Add SQLite to the System Path:\nOpen the Start menu, go to Control Panel, and search for “edit\nsystem environment variables.”\nIn System Properties, click the Environment Variables button.\nIn the System variables section, select Path and click Edit.\nAdd C:\\sqlite\\sqlite-tools-win-x64-3460100 at the end of the\nlist, then click OK to close all dialog boxes.\n4. Verify Installation:\nOpen a new command shell and type sqlite3. If everything is set\nup correctly, you’ll enter the SQLite prompt where you can start\ncreating and managing databases.\nWith SQLite installed and configured, you can now create the database for\nthe examples in Chapter 9.\n\n\nAppendix E. Open-source LLMs\nThis appendix covers\nAdvantages of open-source LLMs: flexibility, transparency, and control.\nPerformance benchmarks and key features of leading open-source\nLLMs.\nChallenges of local deployment and strategies to address them.\nSelecting an optimal inference engine for your use case.\nIn earlier chapters, you worked with OpenAI's public REST API. It’s a\nstraightforward way to build LLM applications since you don’t need to set up\na local LLM host. After signing up with OpenAI and generating an API key,\nyou can send requests to their endpoints and access LLM capabilities. This\nquick setup lets you work with state-of-the-art models like GPT-4o, 4o-mini,\nor o1 and o3 efficiently. The main drawback is cost—running examples like\nsummarization might cost a few cents or even dollars. If you're working on\nprojects for your company, privacy might also be a concern. Some employers\nblock OpenAI entirely to avoid the risk of leaking sensitive or proprietary\ndata.\nThis chapter introduces open-source LLMs, a practical solution for reducing\ncosts and addressing privacy concerns. These models are especially appealing\nto individuals and organizations that prioritize data confidentiality or are new\nto AI. I’ll guide you through the most popular open-source LLM families,\ntheir features, and the advantages they offer. The focus will be on running\nthese models, ranging from high-performance, advanced setups to user-\nfriendly tools ideal for learning and experimentation.\nFinally, I’ll show you how to transition the summarization and QA systems\nyou built earlier to a local open-source LLM. By the end of this chapter,\nyou’ll understand open-source LLMs well and feel confident using them\nwhen they’re the right choice.\n\n\nE.1 Benefits of open-source LLMs\nOpen-source Large Language Models (LLMs) offer clear advantages in cost,\nprivacy, and flexibility. They provide control over data, lower costs by\navoiding licensing fees, and allow customization. Community-driven\ndevelopment fuels innovation, making these models competitive with\nproprietary ones. This section explores the key benefits of open-source\nLLMs.\nE.1.1 Transparency\nClosed-source models often function as \"black boxes,\" making them difficult\nto understand and potentially problematic for compliance, especially in\nindustries like healthcare and finance. Their lack of transparency limits\ncustomizability and creates challenges for auditing and accountability.\nOpen-source LLMs, by contrast, are transparent in their architecture, training\ndata, and methods. Organizations can inspect, validate, and modify the code\nto suit specific needs, ensuring compliance and fostering trust. Developers\ngain flexibility to extend and adapt models for unique applications,\nimproving control and oversight.\nTransparent origins enhance trust in the model's integrity, offering verifiable\nassurances instead of relying on blind faith. This clarity also helps address\npotential privacy risks. However, with transparency comes responsibility—\nusers bear accountability for flaws and financial impacts. Open access can\npose cybersecurity risks if not adequately protected.\nE.1.2 Privacy\nPrivacy and security are critical, particularly in regulated industries like\nfinance and healthcare. Leaks of sensitive data or Personally Identifiable\nInformation (PII) can harm trust and reputation. Using proprietary public-\ncloud LLMs may raise concerns about security and intellectual property risks,\nespecially for companies processing sensitive data.\nOpen-source LLMs mitigate these concerns by enabling on-premises or\n\n\nprivate cloud deployment. Data stays within the corporate network, reducing\nexposure risks. These models can be customized to meet privacy needs,\nincluding implementing security protocols, content filtering, and data\nanonymization. They also support compliance with privacy laws and industry\nstandards.\nAs privacy regulations grow stricter, open-source LLMs become increasingly\nappealing to organizations seeking control over their data.\nE.1.3 Community driven\nProprietary LLMs are typically developed with a fixed vision by a single\norganization. Open-source LLMs, however, benefit from contributions by a\ndiverse community of developers, ranging from individuals to enterprises.\nThese contributions improve features, provide flexibility, and foster growth\nthrough public forks and private customizations.\nThe open-source community produces a wide variety of models and training\nmethods, advancing the field rapidly. While prominent contributors influence\ndirection, the open nature allows anyone to contribute, driving collaboration\nand innovation. This dynamic reduces the performance gap with proprietary\nmodels as advancements in architecture, datasets, and training methods\nevolve.\nHowever, community projects can face challenges, including disputes or\nmismanagement, which may slow progress. Despite these risks, open-source\nmodels empower smaller organizations to compete with industry leaders,\nleveling the playing field.\nChoosing between proprietary and open-source LLMs depends on your\npriorities. If control, customization, and community engagement are essential,\nopen-source models are ideal. If strict SLAs or turnkey solutions are more\ncritical, proprietary models might be better suited. The decision depends on\nyour project’s needs and goals.\nE.1.4 Cost Savings\n\n\nOpen-source LLMs are typically more cost-effective than proprietary models\ndue to the lack of licensing fees. However, deploying them on-premises may\ninvolve significant upfront capital costs, and running them in the cloud can\nincur ongoing operational expenses. Even with these factors, the total cost of\nownership (TCO) for open-source models is usually lower over the medium\nto long term compared to the recurring fees of proprietary LLMs. The right\nchoice depends on your use case, expected text processing volume, and\nreadiness to handle initial deployment costs.\nFor low initial usage, a pay-as-you-go proprietary model might be more\npractical. As usage grows and the client base justifies investment in\ninfrastructure, transitioning to an open-source model can save money. If you\nalready have the skills to deploy and manage an open-source LLM, starting\nlocally might be cost-effective. However, you must consider hidden costs,\nsuch as time spent by staff on setup and maintenance.\nThe cost-effectiveness of an LLM also depends on its application. Proprietary\nvendors charge based on the number of tokens processed, which can become\nexpensive for tasks like summarizing large amounts of text. In such cases, an\nopen-source model may reduce costs. On the other hand, for applications\nusing efficient strategies like Retrieval Augmented Generation (RAG) with\nminimal text processing, proprietary models might be more economical.\nAn additional advantage of open-source LLMs is the ability to fine-tune them\nthrough specialized training, creating a custom model. However, this\ninvolves expenses such as consultancy fees, staff time, computational\nresources, and extended timelines. In cases where no proprietary model suits\nyour specific domain, building and fine-tuning an open-source LLM may be\nthe only option, albeit at your own expense.\nE.2 Popular open-source LLMs\nThe world of open-source Large Language Models (LLMs) is moving fast,\nmaking it tricky to figure out which ones stand out. To help, I’ve pulled\ntogether key details about popular open-source LLMs into a series of tables,\nstarting with Table E.1. This table gives a straightforward overview of some\nof the most interesting open-source LLMs available just before this book was\n\n\npublished. Models are grouped by their type, listed under the “Model Type”\ncolumn:\nFoundation: General-purpose models trained on raw data. Great for\nresearch, experimenting, or fine-tuning, but not usually ready for direct\nuse with end-users.\nInstruction: Models fine-tuned to follow instructions or handle question\nanswering.\nChat: Models tailored for interactive conversations, similar to ChatGPT.\nCode: Built to generate, explain, or debug code.\nDomain-Specific: Designed for specialized industries or business needs.\nThis setup makes it easier to see what each type of model is good at and how\nit fits your needs.\nTable E.1 Most popular open-source LLMs at the time of publication\nModel\nDeveloper HuggingFace\nURL\nModel\nType\nSize\nContext\nwindow\nDeepSeek-\nV3\nDeepseek\nAI\n/deepseek-\nai/DeepSeek-V3 \nFoundation 671B\n128K\nQwen2.5-\n72B-\nInstruct\nQwen\n/Qwen/Qwen2.5-\n72B-Instruct\nInstruct\n72B\n128K\nQwen2.5-\nCoder-\n32B\nQwen\n/Qwen/Qwen2.5-\nCoder-32B\nCode\n32B\n32K\nQwen2.5-\nMath-7B\nQwen\nQwen/Qwen2.5-\nMath-7B\nDomain\nspecific\n(Math)\n7B\n32K\nLlama-\n3.3-70B-\nInstruct\nMeta\nLlama\n/meta-\nllama/Llama-3.3-\n70B-Instruct\nInstruct\n70B\n128K\nMixtral-\n8x22B-\nInstruct-\nv0.1\nMistral AI /mistralai/Mixtral-\n8x22B-Instruct-\nv0.1\nInstruct\n141B\n64K\n\n\nMistral-\n7B-v0.3\nMistral AI mistralai/Mistral-\n7B-v0.3\nFoundation 7B\n32K\ngemma-2-\n27b-it\nGoogle\ngoogle/gemma-2-\n27b-it\nFoundation 27B\n8K\nFalcon3-\n10B-Base\nTII-UAE\ntiiuae/Falcon3-\n10B-Base\nFoundation 10B\n32K\nPhi-3.5-\nMoE-\ninstruct\nMicrosoft\nmicrosoft/Phi-3.5-\nMoE-instruct\nInstruct\n41.9B 128K\nTable E.2 shows the performance of these models based on standard\nbenchmarks, fully defined in Appendix C.\nTable E.2 Extract from the HuggingFace Open LLM Leaderboard, showing standard\nperformance benchmarks on the most popular open-source LLMs at the time of publication:\nhttps://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\nModel\nIfEval\nBBH\nMATH-\nL5\nGPQA\nMuSR\nMMLU-\nPRO\nDeepSeek-\nV3\n86.10\nN/A\nN/A\n59.10\nN/A\n75.90\nQwen2.5-\n72B-\nInstruct\n86.38\n61.87\n1.21\n16.67\n11.74\n51.40\nQwen2.5-\nCoder-32B\n43.63\n48.51\n30.59\n12.86\n15.87\n47.81\nQwen2.5-\nMath-7B\n24.60\n22.01\n30.51\n5.82\n5.00\n19.09\nLlama-3.3-\n70B-\nInstruct\n89.98\n56.56\n0.23\n11.51\n15.57\n48.13\nMixtral-\n8x22B-\nInstruct-\nv0.1\n71.84\n44.11\n18.73\n16.44\n13.49\n38.70\nMistral-7B-\n22.66\n23.95\n3.02\n5.59\n8.36\n21.70\n\n\nv0.3\nGemma-2-\n27b-it\n79.78\n49.27\n0.76\n16.67\n9.11\n38.35\nFalcon3-\n10B-Base\n36.48\n41.38\n24.77\n12.75\n14.17\n36.00\nPhi-3.5-\nMoE-\ninstruct\n69.25\n48.77\n22.66\n14.09\n17.33\n40.64\nAs shown in the table table E.2, models from the same family can vary in\nsuitability depending on the use case. For instance, if you need an LLM for\nan instruction-based chatbot, you should look for one with a high IFEval\nscore. In that case, Qwen2.5-72B-Instruct is a strong option with an\nimpressive IFEval score of 86.38. On the other hand, if your focus is solving\nmath problems, Qwen2.5-Math-7B might be a better choice due to its high\nMATH score of 30.51, combined with the advantage of being smaller in size.\nNow that you know about popular open-source LLMs and their features, I’ll\nshow you how to run these models on your own computer. This will let you\nexperiment and explore their capabilities firsthand.\nE.3 Considerations on running open-source LLMs\nlocally\nLarge language models operate in two main phases: training and inference.\nDuring training, the model learns patterns from the training set, adjusting its\nweights (parameters). These weights are then used during inference to make\npredictions or respond to new inputs. Open-source LLM weights are usually\neasy to access, often available on platforms like Hugging Face or through\ntools like LM Studio or Ollama. Running the inference phase locally requires\nsuitable hardware. This section covers hardware requirements and how\nmodern techniques such as quantization make it feasible to run models even\non consumer-grade machines. An important consideration when hosting an\nLLM locally is whether it provides an OpenAI-compatible API, such as a\nREST API. Using OpenAI during the Proof of Concept (PoC) phase and\ntransitioning to an open-source LLM later can reduce costs or address privacy\n",
      "page_number": 492,
      "chapter_number": 57,
      "summary": "This chapter covers segment 57 (pages 492-499). Key topics include model, open, and data. Simply unzip the package, place it\nin a folder, and add the folder to your system's Path environment variable.",
      "keywords": [
        "Open-source LLMs",
        "Open-source",
        "Popular open-source LLMs",
        "LLMs",
        "models",
        "LLM",
        "popular open-source",
        "open-source models",
        "Installing SQLite",
        "SQLite",
        "Instruct",
        "proprietary",
        "proprietary models",
        "Large Language Models",
        "Savings Open-source LLMs"
      ],
      "concepts": [
        "model",
        "open",
        "data",
        "llm",
        "llms",
        "cost",
        "privacy",
        "proprietary",
        "base",
        "based"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "Segment 28 (pages 232-240)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 37,
          "title": "Segment 37 (pages 308-315)",
          "relevance_score": 0.51,
          "method": "api"
        }
      ]
    },
    {
      "number": 58,
      "title": "Segment 58 (pages 500-510)",
      "start_page": 500,
      "end_page": 510,
      "detection_method": "topic_boundary",
      "content": "concerns. If the inference engine supports an OpenAI-compatible API, you\ncan switch seamlessly to production without rewriting or retesting code.\nE.3.1 Limitations of consumer hardware\nHosting a large LLM in production demands powerful hardware. For\nexample, Llama-70B requires 140GB of RAM and disk space when loaded in\nsingle-precision (float16). High-end GPUs, such as NVIDIA’s A100 (priced\nat $10,000 or more), are essential for efficient inference. Cloud options are\navailable, but they are expensive, costing about $1.50 per hour for A100\nrentals.\nFor local experimentation, however, consumer hardware can still be viable.\nTechniques like quantization and specialized inference engines reduce the\nhardware burden, enabling LLMs to run on laptops or desktops with as little\nas 16GB or 32GB of RAM and no dedicated GPU. These methods make it\npossible to explore LLM functionality on modest machines.\nE.3.2 Quantization\nQuantization reduces LLM size by lowering the precision of weights, trading\nsome accuracy for faster and more efficient inference. Large models typically\nuse 16-bit or 32-bit floating-point precision (2 to 4 bytes per parameter).\nQuantization compresses this to 4-bit integer precision (0.5 bytes per\nparameter). Quantization offers a balance between size, speed, and accuracy.\nFor example, LLAMA-2-7B, originally 13.5GB with 16-bit precision, can be\nreduced to 3.9GB with 4-bit quantization, allowing it to run on modest\nlaptops.\nThe most common quantization techniques used are:\nPost-Training Quantization (PTQ): Applied after training; simpler but\nmay reduce accuracy.\nQuantization-Aware Training (QAT): Integrated during training for\nbetter accuracy but more complex.\nGPTQ: Combines GPT with PTQ techniques.\nNF4: Uses 4-bit Normal Float precision, often outperforming standard\n\n\n4-bit integer quantization.\nGGML and GGUF: Tools by Georgi Gerganov for running models on\nCPUs. GGUF, an enhanced version, supports a wider range of open-\nsource models.\nSome quantization tools like bitsandbytes (by Tim Dettmers) are available\nthrough Python’s pip install and are well-documented on Hugging Face.\nQuantized models can be found on Hugging Face by searching terms like\nGGML or GGUF, or directly through inference engines such as Ollama or\nLM Studio.\nTIP\nFor a deeper dive into quantization, check out the following blog posts “What\nare Quantized LLMs\" by Miguel Carreira Neves\n(https://www.tensorops.ai/post/what-are-quantized-llms) and “How is\nllama.cpp possible?\" by Finbarr Timbers (https://finbarr.ca/how-is-llama-cpp-\npossible)\nModern advancements in quantization and optimized engines have\nsignificantly lowered the barrier to running open-source LLMs locally,\nmaking them accessible even for users with limited hardware, especially for\nlearning and experimentation.\nE.3.3 OpenAI REST API compatibility\nMany inference engines include an embedded HTTP server that accepts\nrequests through endpoints compatible with the OpenAI REST API. This\ncompatibility allows immediate use with standard OpenAI libraries,\nsimplifying the engine’s codebase and supporting quick adoption. For users,\nthe benefits include minimal learning requirements compared to learning\nproprietary APIs and the ability to swap engines seamlessly. With this\nstandardization, you can efficiently test multiple engines using the same\nclient code or replace an engine with one better suited to your needs.\nIn this section, I’ll provide code snippets which you can run against each\ninference engine I will introduce later. These snippets require minimal\n\n\nmodifications—often just adjusting the port number. They demonstrate how\nto make requests using raw HTTP (e.g., curl), the OpenAI Python library\nand LangChain. Let’s start with direct HTTP requests.\nDirect curl invocation\nThe easiest way to test an OpenAI-compatible REST API endpoint is by\nusing the /chat/completions endpoint. Below is an example with OpenAI’s\npublic service. Open a shell (on Windows you can open a command line\nwindow or a Git Bash shell) and replace YOUR-OPENAI-KEY with your actual\nOpenAI API key:\n$ curl https://api.openai.com/v1/chat/completions   -H \"Content-T\n   \"model\": \"gpt-4o-mini\",\n   \"messages\": [\n   { \"role\": \"system\", \"content\": \"You are an helpful AI assista\n   { \"role\": \"user\", \"content\": \"How many Greek temples are in P\n   ],\n   \"temperature\": 0.7\n }'\nThe response might look like this:\n % Total    % Received % Xferd  Average Speed   Time    Time     \n                                Dload  Upload   Total   Spent    \n100   810  100   578  100   232    209     83  0:00:02  0:00:02 -\n{\n   \"id\": \"chatcmpl-AjANT94Lel2oZIOz8kmF0PKWfRuGa\",\n   \"object\": \"chat.completion\",\n   \"created\": 1735328015,\n   \"model\": \"gpt-4o-mini-2024-07-18\",\n   \"choices\": [\n       {\n           \"index\": 0,\n           \"message\": {\n               \"role\": \"assistant\",\n               \"content\": \"Paestum, an ancient Greek city locate\n               \"refusal\": null\n           },\n           \"logprobs\": null,\n           \"finish_reason\": \"stop\"\n       }\n   ],\n\n\n   \"usage\": {\n       \"prompt_tokens\": 28,\n       \"completion_tokens\": 163,\n       \"total_tokens\": 191,\n       \"prompt_tokens_details\": {\n           \"cached_tokens\": 0,\n           \"audio_tokens\": 0\n       },\n       \"completion_tokens_details\": {\n           \"reasoning_tokens\": 0,\n           \"audio_tokens\": 0,\n           \"accepted_prediction_tokens\": 0,\n           \"rejected_prediction_tokens\": 0\n       }\n   },\n   \"system_fingerprint\": \"fp_0aa8d3e20b\"\n}\nTo make the same request to a local open-source LLM, adjust the command\nas follows:\ncurl https://localhost:8000/v1/chat/completions   -H \"Content-Typ\n   \"model\": \"mistralai/Mistral-7B-v0.3\",\n   \"messages\": [\n   { \"role\": \"system\", \"content\": \"You are a helpful AI assistan\n   { \"role\": \"user\", \"content\": \"How many Greek temples are ther\n   ],\n   \"temperature\": 0.7\n }'\nNote\nFor local inference engines, you do not need to include an OpenAI key in the\nrequest header, so it has been omitted in the example. However, you need to\nadjust two key details in the `curl` request based on the inference engine and\nLLM you are using:\n1. Port Number: Each inference engine’s local HTTP server is set to a\ndefault port, which is often different from 8000. Refer to the engine's\ndocumentation and update the port number as needed.\n2. Model Name: Inference engines use specific naming conventions for\nmodels. Some follow Hugging Face conventions (e.g., 'mistralai/Mistral-\n7B-v0.3'), while others may not. Check the engine's documentation to\n\n\nensure you provide the correct model name.\nPython openai library\nTo use the OpenAI library for requests, create a virtual environment, install\nthe library with pip install openai, create a Jupyter notebook, as you have\nseen in the previous chapters, and use the code from listing E.1 (after\nreplacing YOUR-OPENAI-KEY with your OpenAI key).\nListing E.1 Calling the OpenAI completions endpoint\nimport getpass\nfrom openai import OpenAI\nOPENAI_API_KEY = getpass.getpass('Enter your OPENAI_API_KEY')\nclient = OpenAI(\n   api_key = OPENAI_API_KEY\n)\ncompletion = client.chat.completions.create(\n model=\"gpt-4o-mini\",\n messages=[\n   { \"role\": \"system\", \"content\": \"You are a helpful AI assistan\n   { \"role\": \"user\", \"content\": \"How many Greek temples are ther\n ],\n temperature=0.7\n)\nprint(completion.choices[0].message.content)\nYou will get output similar to this:\nPaestum, an ancient Greek city located in present-day Italy, is r\nIn order to run the code above against a local open-source LLM, you need to\nadapt it as follows (note the extra base_url parameter when instantiating the\nOpenAI client):\nfrom openai import OpenAI\nport_number = '8080' #A\nclient = OpenAI(\n\n\n   base_url=f'http://localhost:{port_number}/v1', \n   api_key = \"NO_KEY_NEEDED\"\n)\ncompletion = client.chat.completions.create(\n model=\"mistral\",\n messages=[\n   { \"role\": \"system\", \"content\": \"You are a helpful AI assistan\n   { \"role\": \"user\", \"content\": \"How many Greek temples are ther\n ],\n temperature=0.7\n)\nprint(completion.choices[0].message.content)\nNote\nTo adapt the example above for your inference engine and local open-source\nLLM, update the port number to match the engine's local HTTP server and\nset the model name according to the engine's specific naming convention. For\nexample, if using LM studio, use port 8080.\nLangChain's openai wrapper\nFor LangChain, direct its OpenAI wrapper to the local inference engine by\nupdating the base URL:\nfrom langchain_openai import ChatOpenAI\nport_number = '8080'\nllm = ChatOpenAI(openai_api_base=f'http://localhost:{port_number}\nresponse = llm.invoke(\"How many Greek temples are there in Paestu\nprint(response.content)\nNOTE\nEnsure the port matches the local engine configuration.\nNow, let's explore the essential component required to execute open-source\nmodels on regular consumer hardware: local inference engines.\n\n\nE.4 Local inference engines\nRunning an open-source LLM on consumer hardware is most practical with\nan inference engine. These engines host the local model and handle requests\nfrom client applications, often through native bindings for languages like\nPython, JavaScript, Java, C++, Go, or Rust. Many inference engines also\ninclude a local HTTP server with OpenAI REST API-compatible endpoints,\nallowing you to use familiar OpenAI libraries or frameworks like LangChain\nwithout significant changes. This flexibility lets you switch between local\nopen-source LLMs and OpenAI’s public service with minimal effort.\nIn the following sections, I will introduce several inference engines,\nbeginning with foundational tools like llama.cpp, optimized for high\nperformance, and moving to user-friendly options like Ollama and\nproduction-grade solutions such as vLLM. I will also cover consumer-\nfocused alternatives, including LocalAI (a simplified wrapper for engines like\nllama.cpp or vLLM), GPT4All, and LM Studio, which are notable for their\nintuitive user interfaces. These options will help you select the best engine\nbased on your hardware, experience, and project requirements.\nAs shown in Figure E.1, which serves as a guide for this section, llama.cpp\nand vLLM are foundational backends for many inference engines. Higher-\nlevel tools like Ollama, llamafile, and LocalAI, along with user-friendly\nengines such as GPT4All and LM Studio, are built on llama.cpp. Meanwhile,\nvLLM functions independently, with LocalAI being the only tool currently\nbuilding on it.\nFigure E.1 Lineage and functionality of local inference engines\n\n\nLet’s start our tour with llama.cpp.\nE.4.1 Llama.cpp\nllama.cpp was one of the first engines designed to run open-source models\nefficiently on consumer hardware. Initially developed to support the LLaMA\nmodel using 4-bit integer quantization for Apple silicon GPUs, it has since\nexpanded to support Linux, Windows (x86 processors), and even Raspberry\nPi. It now handles a wide range of quantized models, including Mistral,\nGemma, Phi and Falcon. It supports 2-bit to 8-bit integer quantization and\noffers bindings for Python, Java, C#, Go, Scala, and Ruby.\n\n\nSetting up llama.cpp\nTo use llama.cpp, follow these steps:\n1. Build the Executable: Download the source code from GitHub and build\nit using your preferred strategy, such as make, CMake, Zig, or gmake.\nAdvanced build options include Metal, MPI, and BLAS for enhanced\nperformance.\n2. Prepare the Model Weights: Obtain a quantized version of the model\n(e.g., Mistral-7B-Instruct-v0.2-GGUF) from Hugging Face or\ngenerate it using GitHub instructions. Ensure the model fits within your\nsystem's disk and RAM capacity.\n3. Run Inference: Execute the inference command, pointing to the\nquantized model file:\n  ./main -m ./models/mistral-7b-instruct-v0.2.Q2_K.gguf -p \"How \nNOTE\nFor detailed instructions, refer to the official GitHub page:\nhttps://github.com/ggerganov/llama.cpp.\nPython bindings\nIf you prefer a higher-level API, links to relevant bindings are available on\nthe llama.cpp GitHub page. Before installation, review both the llama.cpp\ndocumentation and the selected bindings' documentation to prevent redundant\nsetup.\nTo install the Python bindings from PyPI, use the following command:\npip install llama-cpp-python\nThis command not only downloads the library but also tries to build\nllama.cpp from source using CMake and your system's C compiler. For GPU\nsupport, follow additional instructions and configurations.\nAfter setup, you can use Python to interact with your local quantized LLM\n\n\ninstance. Listing E.2 shows an adapted example from the official llama-cpp-\npython bindings documentation.\nListing E.2 Executing prompts against a local quantized Mistral-7B-Instruct instance\n### Adapted from official documentation at\n### https://github.com/abetlen/llama-cpp-python\nfrom llama_cpp import Llama\nllm = Llama(\n     model_path=\"./models/mistral-7b-instruct-v0.2.Q2_K.gguf\"\n)\noutput = llm(\n     \"Q: What are the planets in the solar system? A: \", # Promp\n     max_tokens=32, # Generate up to 32 tokens, set to None to g\n     stop=[\"Q:\", \"\\n\"], # Stop generating just before the model w\n     echo=True # Echo the prompt back in the output\n) \nprint(output)\nYou get the following output:\n{\n \"id\": \"cmpl-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n \"object\": \"text_completion\",\n \"created\": 1679561337,\n \"model\": \"./models/mistral-7b-instruct-v0.2.Q2_K.gguf\",\n \"choices\": [\n   {\n     \"text\": \"Q: Name the planets in the solar system? A: Mercur\n     \"index\": 0,\n     \"logprobs\": None,\n     \"finish_reason\": \"stop\"\n   }\n ],\n \"usage\": {\n   \"prompt_tokens\": 14,\n   \"completion_tokens\": 28,\n   \"total_tokens\": 42\n }\n}\nLangChain integration\nLangChain’s LlamaCpp client can connect directly to a quantized local\n\n\ninstance of Llama-based models, provided you have installed the llama-cpp-\npython library (https://github.com/abetlen/llama-cpp-python):\nfrom langchain_community.llms import LlamaCpp\nllm = LlamaCpp(model_path=\"./models/llama-2-7b-chat.ggmlv3.q2_K.b\nOpenAI API compatible endpoints\nllama.cpp includes a local HTTP server that provides OpenAI REST API-\ncompatible endpoints. To try it out, you can use the `curl` script or Python\nexamples from section 11.3.3, ensuring the port number and model name are\ncorrectly configured.\nWhile detailed installation and setup instructions for llama.cpp are not\nincluded here, it is important to understand its basics. Many of the local\ninference engines discussed later are built on llama.cpp or draw inspiration\nfrom its design. Next, I’ll introduce Ollama.\nE.4.2 Ollama\nOllama is a hosting environment for open-source LLMs, available for\nWindows, macOS, and Linux. Unlike llama.cpp, Ollama does not require\nyou to build executables from source; installation packages are readily\navailable.\nInteractive mode\nAfter downloading and installing Ollama from the Ollama.ai homepage\n(https://ollama.ai), launch the Ollama client (in Windows, launch it from the\nStart menu). This opens a terminal (or PowerShell window on Windows),\nwhere you can immediately run an LLM using the following command:\nollama run mistral\nThis command downloads a quantized version of the selected model (Mistral\nin this example) from the remote Ollama library. The terminal will display\nthe download progress, as shown in Figure 11.2.\n",
      "page_number": 500,
      "chapter_number": 58,
      "summary": "This chapter covers segment 58 (pages 500-510). Key topics include models, engines, and openai. If the inference engine supports an OpenAI-compatible API, you\ncan switch seamlessly to production without rewriting or retesting code.",
      "keywords": [
        "inference engines",
        "local inference engines",
        "OpenAI",
        "OpenAI REST API",
        "inference",
        "model",
        "engines",
        "local",
        "API",
        "local inference",
        "LLM",
        "tokens",
        "REST API",
        "quantization",
        "llama.cpp"
      ],
      "concepts": [
        "models",
        "engines",
        "openai",
        "llm",
        "uses",
        "hardware",
        "local",
        "locally",
        "include",
        "including"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 43,
          "title": "Segment 43 (pages 393-400)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 17,
          "title": "Segment 17 (pages 136-143)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 7,
          "title": "Segment 7 (pages 53-60)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 18,
          "title": "Segment 18 (pages 355-374)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.56,
          "method": "api"
        }
      ]
    },
    {
      "number": 59,
      "title": "Segment 59 (pages 511-519)",
      "start_page": 511,
      "end_page": 519,
      "detection_method": "topic_boundary",
      "content": "Figure 11.2 Screenshot of ollama terminal while installing the Mistral LLM\nAt this stage, you can send a prompt and receive a response, as shown below\n(note the response may be slow):\n>>> How many Greek temples are in Paestum?\nThere are three Greek temples in Paestum, which is a town on the \n1. Temple of Hera I (also known as the Basilica) - This temple wa\ngoddess Hera. It is the largest and most impressive of the three \n2. Temple of Neptune - This temple was also built around 550 BC a\nNeptune in Roman mythology). It is smaller than the Temple of Her\n3. Temple of Athena - This temple was built around 460 BC and is \nsmallest of the three temples and is incomplete due to damage sus\nAll three temples are well-preserved and are a popular tourist de\nSite.\nOllama natively supports several popular open-source LLMs, including\nLlama, Phi, and Gemma. You can browse the available models on the library\npage: https://ollama.com/library. To run a GGUF model from another source,\nsuch as Hugging Face, copy the model to a local folder and import it\n\n\nfollowing the instructions in the Ollama documentation.\nServer mode\nOnce the server is running, it exposes a REST API on port 11434, ready to\naccept requests for LLM interactions.\nNote\nOllama’s REST API endpoints are proprietary and not compatible with\nOpenAI’s specifications. Code examples from earlier sections cannot be used\ndirectly.\nBelow is an examples using Ollama’s API from curl (or Postman, if you\nprefer).\ncurl http://localhost:11434/api/generate -d '{\n \"model\": \"mistral\", \n \"prompt\":\"How many Greek temples are in Paestum?\"\n}’\nBy default, the output streams one word at a time, each included in a separate\nJSON object, as shown below:\n{\n   \"model\": \"mistral\",\n   \"created_at\": \"2024-12-28T11:52:26.1375731Z\",\n   \"response\": \" There\",\n   \"done\": false\n}\n{\n   \"model\": \"mistral\",\n   \"created_at\": \"2024-12-28T11:52:26.5276658Z\",\n   \"response\": \" are\",\n   \"done\": false\n}\n{\n   \"model\": \"mistral\",\n   \"created_at\": \"2024-12-28T11:52:26.9259504Z\",\n   \"response\": \" three\",\n   \"done\": false\n}\n\n\n…\nIf you prefer to wait for the full response, you must set the “stream” attribute\nto false:\ncurl http://localhost:11434/api/generate -d '{\n \"model\": \"mistral\", \n \"prompt\":\"How many Greek temples are in Paestum?\",\n \"prompt\":false\n}’\nYou will get a single response object:\n{\n   \"model\": \"mistral\",\n   \"created_at\": \"2024-12-28T13:07:03.1862002Z\",\n   \"response\": \" There are three Doric temples in Paestum, locat\n   \"done\": true,\n   \"done_reason\": \"stop\",\n   \"context\": [\n       3,\n       29473,\n      … SHORTENED,\n       1102,\n       29499\n   ],\n   \"total_duration\": 40777816100,\n   \"load_duration\": 13459239600,\n   \"prompt_eval_count\": 16,\n   \"prompt_eval_duration\": 1454000000,\n   \"eval_count\": 123,\n   \"eval_duration\": 25791000000\n}\nThis is a request to the /chat endpoint, using a payload format compatible\nwith OpenAI's corresponding request structure:\ncurl http://localhost:11434/api/chat -d '{\n \"model\": \"mistral\",\n \"messages\": [\n   { \"role\": \"system\", \"content\": \"You are a helpful AI assistan\n   { \"role\": \"user\", \"content\": \"How many Greek temples are ther\n ],\n \"temperature\":0.7\n}’\n\n\nThese API calls allow interaction with the LLM using any programming\nlanguage. However, for Python or JavaScript, you can use libraries provided\nby Ollama that wrap the low-level REST API, simplifying client\ndevelopment.\nNative Python library\nTo get started, install the Ollama Python library:\npip install ollama\nYou can then interact with the LLM's /chat endpoint as shown:\nimport ollama\nresponse = ollama.chat(model='mistral', messages=[\n   { \"role\": \"system\", \"content\": \"You are an helpful AI assista\n   { \"role\": \"user\", \"content\": \"How many Greek temples are in P\n])\nprint(response['message']['content'])\nAlternatively, you can send instructions to the /generate endpoint:\nollama.generate(model='mistral', prompt=’How many Greek temples a\nTo enable streaming responses, set stream=True in the chat or generate\ncall.\nIf you need to specify a custom host or timeout, create a Client object:\nfrom ollama import Client\nclient = Client(host='http://mylocalserver:8000')\nresponse = client.chat(model='mistral', messages=[\n   { \"role\": \"system\", \"content\": \"You are an helpful AI assista\n   { \"role\": \"user\", \"content\": \"How many Greek temples are in P\n])\nFor asynchronous execution, use AsyncClient with asyncio:\nimport asyncio\nfrom ollama import AsyncClient\nasync def chat():\n\n\n   message = {'role': 'user', 'content': ’How many Greek temples \n   response = await AsyncClient().chat(model='mistral', messages\n   print(response['message']['content'])\nasyncio.run(chat())\nLangChain integration\nLangChain provides a wrapper for the Ollama Python library, integrating it\nwith LangChain interfaces such as the Runnable interface. Below is an\nexample of re-implementing a basic synchronous LLM invocation using\nLangChain:\nfrom langchain_community.llms import Ollama\nollama = Ollama(model='mistral')\nquery = 'How many Greek temples are in Paestum?'\nresponse  = llm.invoke(query)\nprint(response)\nTo handle streaming responses, you can use the following approach:\nfor chunks in llm.stream(query):\n   print(chunks)\nFor more information, I recommend consulting the Ollama documentation on\nGitHub. It includes community-driven integrations, such as web and desktop\nUIs, terminal plugins for Emacs and Vim, and other useful extensions.\nOllama is designed to help you explore open-source models locally. If you\ndecide to move beyond a proof of concept on a Linux system and build a\nproduction-grade solution using non-quantized LLMs on powerful GPU\nhardware, potentially in the cloud, consider exploring vLLM. While vLLM is\nnot specifically for consumer-grade hardware, it demonstrates the capabilities\nof inference engines designed to host open-source models of any size on\nvarious types of hardware.\nE.4.3 vLLM\nvLLM is a high-performance Python library for LLM inference, built on a\n\n\nC++ core and CUDA binaries. It targets Linux systems and high-grade GPUs,\nincluding V100, T4, RTX20xx, A100, L4, and H100. According to the\nofficial web page (https://docs.vllm.ai), vLLM offers the following advanced\nperformance features:\nHigh-Throughput Serving: State-of-the-art inference speed.\nPagedAttention: Efficient memory management for attention key and\nvalue data.\nContinuous Batching: Dynamically batches incoming requests for better\nefficiency.\nCUDA/HIP Graph Execution: Speeds up model execution.\nQuantization Support: Includes GPTQ, AWQ, and SqueezeLLM\nmethods.\nvLLM is designed with flexibility and usability in mind. Key features include:\nHuggingFace Integration: Seamless support for popular HuggingFace\nmodels.\nAdvanced Decoding Algorithms: Supports parallel sampling, beam\nsearch, and other methods.\nTensor Parallelism: Enables distributed inference across multiple GPUs.\nStreaming Outputs: Provides real-time response generation.\nOpenAI-Compatible API: Allows easy integration with existing\napplications.\nGPU Support: Works with both NVIDIA and AMD GPUs.\nvLLM supports a variety of architectures, including BLOOM, Falcon, GPT-J,\nGPT-NeoX, Vicuna, LLaMA, and Mistral. While it can handle quantized\nmodels, vLLM is optimized for larger model versions, such as LLaMA-2-\n70B-hf and Falcon-180B. For a detailed list of supported models, refer to the\nSupported Models section of the vLLM documentation.\nInstallation\nTo install vLLM, ensure you are using Linux with one of the recommended\nhigh-grade GPUs. It is best to use a dedicated virtual environment created\nwith venv. Install the library with:\n\n\npip install vllm\nOnce installed, you can activate the server mode.\nServer mode\nStart a local OpenAI-compatible HTTP server with the following command\nin a separate terminal:\n$ python -m vllm.entrypoints.openai.api_server --model mistralai/\nThis will download the model from Hugging Face (if not already\ndownloaded) and activate an HTTP server on port 8000, exposing OpenAI-\ncompatible endpoints.\nOpenAI REST API compatible endpoints\nRefer to section E.3.3 for examples of curl, Python, and LangChain code\nthat you can use with vLLM. Ensure you set the port to 8000 and use the\ncorrect model name. Consult the vLLM documentation for further details.\nOffline batched inference\nvLLM offers a unique capability for high-performance offline batched\ninference, allowing you to process multiple requests in a single operation.\nThis feature sets it apart from other local inference engines. To use it, import\nthe LLM and SamplingParams modules, create a list of prompts, and process\nthem using an LLM instance. The results can be iterated and reviewed, as\nshown in Listing E.3.\nListing E.3 vLLM offline batched inference\nfrom vllm import LLM, SamplingParams\nprompts = [ #A\n   \"How many temples are there in Paestum?\",\n   \"Who built the aqueduct in Segovia?\",\n   \"Is LeBron James better than Michael Jordan?\",\n   \"Summarize the Lord of The Rings in three sentences.\",\n\n\n]\nsampling_params = SamplingParams(temperature=0.7, top_p=0.95) #B\nllm = LLM(model=\"mistralai/Mistral-7B-v0.1\") #C\noutputs = llm.generate(prompts, sampling_params) #D\nfor output in outputs: #E\n   prompt = output.prompt\n   llm_response = output.outputs[0].text\n   print(f\"Prompt: {prompt!r}, LLM response: {llm_response!r}\")\nAfter presenting the advanced capabilities of vLLM, designed for deploying\nopen-source LLMs in professional production environments, I will now\nreturn to exploring local inference engines built for consumer-grade\nhardware.\nE.4.4 llamafile\nOne of the simplest ways to run an LLM locally is with llamafile. According\nto its GitHub description, llamafile allows you to \"distribute and run an LLM\nwith a single file.\" This executable combines the quantized weights of an\nLLM in GGUF format with the llama.cpp C++ executable, packaged using\nCosmopolitan-Libc. Cosmopolitan-Libc, described as \"making C a build-\nonce, run-anywhere language,\" enables the llamafile to run on macOS, Linux,\nand Windows (with a .exe extension) without requiring any installation. This\napproach sets an incredibly low barrier for experimenting with local LLMs.\nThe llamafile GitHub page offers a variety of prebuilt LLMs, including recent\nversions of LLaMA, Gemma, and Phi.\nServer mode\nRunning an LLM with llamafile is straightforward. Follow these steps (refer\nto the GitHub project for detailed instructions):\n4. Download a llamafile: Obtain a prebuilt file, such as mistral-7b-\ninstruct-v0.2.Q5_K_M.llamafile, from the GitHub project page\n(https://github.com/Mozilla-Ocho/llamafile).\n\n\n5. Place the File in a Folder: Copy the llamafile to a directory on your\nsystem.\n6. Set Permissions:\nOn macOS or Linux: Grant execute permissions using:\n    chmod +x mistral-7b-instruct-v0.2.Q5_K_M.llamafile\nOn Windows: Rename the file to include the .exe extension:\n    mistral-7b-instruct-v0.2.Q5_K_M.llamafile.exe\n4. Run the File: Launch the llamafile. For example, on Windows:\n  c:\\temp>mistral-7b-instruct-v0.2.Q5_K_M.llamafile.exe -ngl 999\nThe llamafile will start a web server at http://127.0.0.1:8080 and open a\nbrowser pointing to a chat web interface at the same address. You can begin\ninteracting with the LLM immediately, as shown in Figure E.3.\nFigure E.3 llamafile chat web UI pointing to the local web server communicating with the local\nopen-source LLM\n",
      "page_number": 511,
      "chapter_number": 59,
      "summary": "This chapter covers segment 59 (pages 511-519). Key topics include models, vllm. Code examples from earlier sections cannot be used\ndirectly.",
      "keywords": [
        "Greek temples",
        "LLM",
        "ollama",
        "model",
        "temples",
        "Greek",
        "response",
        "vLLM",
        "REST API",
        "API",
        "Mistral",
        "Greek temples response",
        "Ollama Python library",
        "Paestum",
        "Python"
      ],
      "concepts": [
        "models",
        "llm",
        "vllm",
        "response",
        "included",
        "includes",
        "execution",
        "executable",
        "execute",
        "chat"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents In Action",
          "chapter": 6,
          "title": "Segment 6 (pages 41-49)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 38,
          "title": "Segment 38 (pages 316-325)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "Segment 28 (pages 232-240)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 26,
          "title": "Segment 26 (pages 211-218)",
          "relevance_score": 0.54,
          "method": "api"
        }
      ]
    },
    {
      "number": 60,
      "title": "Segment 60 (pages 520-527)",
      "start_page": 520,
      "end_page": 527,
      "detection_method": "topic_boundary",
      "content": "For example you can enter the same prompt you entered into Ollama\npreviously:\n>>> How many Greek temples are in Paestum?\nOpenAI API compatible endpoints\n\n\nThe web server also includes an OpenAI API-compatible\n/chat/completions endpoint. Use the examples from section E.3.3 for curl,\nPython, or LangChain, ensuring you set the port to 8080.\nTIP\nTo learn more about llamafile, a project by Justine Tunney in collaboration\nwith Mozilla, visit the GitHub page. For additional insights, check out\nJustine’s blog post \"Bash One-Liners for LLMs\"\n(https://justine.lol/oneliners/).\nE.4.5 LM Studio\nLM Studio is a user-focused local inference engine and a direct competitor to\nGPT4All. Executables for macOS, Windows, and Linux can be downloaded\nfrom the LM Studio website (https://lmstudio.ai). Once launched, LM Studio\nprovides a seamless experience for searching, selecting, and downloading\nGGUF models, thanks to its integration with Hugging Face. Figure E.4 shows\nthe model search interface.\nFigure E.4 GGUF Search Screen: 1) search for a model using the search box (in our case,\nMistral), 2) select a model variant from the left panel, and download a specific quantized version\nfrom the right panel by clicking the \"Download\" button (not shown in the screenshot). After\ndownloading a model, you can either chat directly through the chat screen or activate the\nbackend server for programmatic interaction.\n\n\nIn the figure above, you can select a model by entering its name in the search\n\n\nbox, choosing a variant from the left panel, and then selecting a specific\nquantized version (in our case we select a 2-bit quantization, which has very\nlow fidelity) from the right panel by clicking the Download button.\nOnce downloaded, you can either interact with the model directly through the\nchat screen or activate the backend server for programmatic communication.\nChat screen\nTo send interactive prompts, open the Chat menu on the left bar, select the\ndownloaded model from the dropdown at the top, and type your prompt in\nthe text box at the bottom, as shown in Figure E.5.\nFigure E.5 Chat Screen: Select the chat menu on the left, choose the model from the dropdown at\nthe top, and type a prompt in the text box at the bottom.\n\n\nThe response time will vary based on the model, its quantization level, and\nyour computer's hardware. Expect a delay of a few seconds before the LLM\nprovides an answer, as shown in Figure E.6.\nFigure E.6 Prompt Response: The time to receive an answer depends on the model, its\nquantization, and your computer’s hardware.\n\n\nOnce you’ve explored the UI, it’s time to examine the server API mode.\nServer mode\nTo activate the local HTTP server and expose OpenAI-compatible REST API\nendpoints:\n1. Click the Server icon in the left-hand menu.\n2. Set the desired port number (e.g., 8080).\n3. Click the Start Server button, as shown in Figure E.7.\nFigure E.7 Activating the Local HTTP Server: click the Server menu on the left, enter (or accept)\nthe port number and click the Start Server button.\n\n\nAfter starting the server, the logs will appear in the terminal panel,\nconfirming the server is running. Example logs:\n[2024-12-30 13:42:41.709] [INFO] [LM STUDIO SERVER] Verbose serve\n[2024-12-30 13:42:41.721] [INFO] [LM STUDIO SERVER] Success! HTTP \n[2024-12-30 13:42:41.721] [INFO] [LM STUDIO SERVER] Supported end\n[2024-12-30 13:42:41.721] [INFO] [LM STUDIO SERVER] ->    GET  ht\n\n\n[2024-12-30 13:42:41.722] [INFO] [LM STUDIO SERVER] ->    POST ht\n[2024-12-30 13:42:41.722] [INFO] [LM STUDIO SERVER] ->    POST ht\n[2024-12-30 13:42:41.722] [INFO] [LM STUDIO SERVER] ->    POST ht\n[2024-12-30 13:42:41.723] [INFO] [LM STUDIO SERVER] Model loaded: \n[2024-12-30 13:42:41.723] [INFO] [LM STUDIO SERVER] Logs are save\nTo test the server, copy the sample curl request provided in the left-hand\npanel and paste it into a terminal (on Windows, use Git Bash for easier\nhandling of escaping), or alternatively run on Postman against the specified\nURL. As the terminal begins processing the request, the LM Studio logs\npanel will display activity, providing real-time updates during the request's\nexecution:\n[2024-12-30 13:46:12.782] [INFO] Received POST request to /v1/cha\n \"model\": \"TheBloke/Mistral-7B-v0.1-GGUF\",\n \"messages\": [\n   {\n     \"role\": \"system\",\n     \"content\": \"Always answer in rhymes.\"\n   },\n   {\n     \"role\": \"user\",\n     \"content\": \"Introduce yourself.\"\n   }\n ],\n \"temperature\": 0.7,\n \"max_tokens\": -1,\n \"stream\": true\n}\n[2024-12-30 13:46:12.783] [INFO] [LM STUDIO SERVER] Context Overf\n[2024-12-30 13:46:12.783] [INFO] [LM STUDIO SERVER] Streaming res\n[2024-12-30 13:46:20.630] [INFO] [LM STUDIO SERVER] First token g\n[2024-12-30 13:46:52.409] [INFO] Finished streaming response\nYou will now be able to interact with the OpenAI compatible REST API\nendpoints. As usual, refer to section E.3.3 for code examples and make sure\nyou use the correct port number and model name.\nE.4.6 LocalAI\nLocalAI is a free inference engine designed to run OpenAI REST API-\ncompatible LLMs on consumer hardware, including systems with plain CPUs\nor low-grade GPUs. It supports various quantized open-source LLMs and can\n",
      "page_number": 520,
      "chapter_number": 60,
      "summary": "Once launched, LM Studio\nprovides a seamless experience for searching, selecting, and downloading\nGGUF models, thanks to its integration with Hugging Face Key topics include server, info, and models.",
      "keywords": [
        "STUDIO SERVER",
        "server",
        "Studio",
        "INFO",
        "Ollama previously",
        "entered into Ollama",
        "Greek temples",
        "model",
        "local HTTP server",
        "chat screen",
        "Start Server button",
        "HTTP server",
        "chat",
        "REST API",
        "Start Server"
      ],
      "concepts": [
        "server",
        "info",
        "models",
        "panel",
        "chat",
        "post",
        "stream",
        "openai",
        "port",
        "searching"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 53,
          "title": "Segment 53 (pages 448-456)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 6,
          "title": "Segment 6 (pages 41-49)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 7,
          "title": "Segment 7 (pages 53-60)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "Segment 28 (pages 232-240)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 9,
          "title": "Segment 9 (pages 71-78)",
          "relevance_score": 0.51,
          "method": "api"
        }
      ]
    },
    {
      "number": 61,
      "title": "Segment 61 (pages 528-535)",
      "start_page": 528,
      "end_page": 535,
      "detection_method": "topic_boundary",
      "content": "also handle audio-to-text, text-to-audio, and multi-modal models. Here, the\nfocus is on its text LLM capabilities.\nWritten in Go, LocalAI serves as a higher-level inference engine that routes\nOpenAI-like REST API calls to back-end engines like llama.cpp or vLLM.\nFigure E.1, adapted from the LocalAI documentation, illustrates this\narchitecture.\nFigure E.1: LocalAI Architecture: LocalAI routes OpenAI-like REST API calls to inference\nengines like llama.cpp, vLLM, or other custom backends.\n\n\nServer mode\nThe primary distribution method for LocalAI is through container images,\nwhich can be deployed using Docker, Podman, or Kubernetes. Popular\nmodels are automatically downloaded when starting the container.\nFor example, to run Mistral-OpenOrca on CPU:\n\n\ndocker run -ti -p 8080:8080 localai/localai:v2.7.0-ffmpeg-core mi\nIf you have CUDA-12 GPU you can run the related image with the -gpu\noption:\ndocker run -ti -p 8080:8080 --gpus all localai/localai:v2.7.0-cub\nContainer images for CPU, CUDA-11, and CUDA-12 are available on\nDocker Hub and Quay.io. The LocalAI documentation\n(https://localai.io/docs) provides the appropriate Docker commands for each\nmodel and hardware configuration.\nIf you have a custom quantized model in GGUF format, you can place it in a\nlocal folder (e.g., local_models) and reference it when starting the container:\ndocker run -p 8080:8080 -v $PWD/local_models:/local_models -ti --\nOpenAI REST API compatible endpoints\nWhen a model is started using Docker, LocalAI launches an HTTP server on\nport 8080 with OpenAI-compatible endpoints. Refer to section 10.3.3 for\nexamples of curl, Python, and LangChain code you can use with LocalAI.\nEnsure the port is set to 8080 and verify the model name in the LocalAI\ndocumentation for accurate configuration.\nE.4.7 GPT4All\nGPT4All is an inference engine built on llama.cpp that allows you to run\nGGUF quantized LLM models on consumer hardware, including CPUs and\nlow-grade GPUs. It improves on llamafile with a more user-friendly\ngraphical interface, making it accessible to non-technical users.\nInstallation packages for Windows, macOS, and Ubuntu Linux are available\non the GPT4All website (https://gpt4all.io). After installation, you’ll see a\ndesktop client shown in figure E.2, with an interface which allows you to\ndownload a model, chat with it and also upload your documents to enable\nout-of the box RAG Q&A.\n\n\nFigure E.2 GPT4All desktop application home screen\nGPT4All includes the following components, visible and hidden:\n1. Backend Inference Engine: Built on llama.cpp, the engine supports\nGGUF quantized LLM models (typically under 8GB) for architectures\nlike Falcon, LLAMA, MPT, GPT-J, and Mistral.\n2. Language-Specific Bindings: High-level API libraries are available for\n\n\nC++, Python, Go, NodeJS, and more, enabling programmatic access to\nthe inference engine.\n3. Local Web Server: The desktop application can start a local web server\nthat exposes chat completions through OpenAI-compatible REST API\nendpoints.\n4. Desktop GUI: The graphical client lets you interact with an LLM\nthrough a user-friendly interface. Models can be selected from a\ndropdown or added from the Model Explorer on the GPT4All homepage\nor Hugging Face.\n5. LocalDocs Plugin (Optional): This plugin allows you to import files\ncontaining data or unstructured text and chat with the content. It\nimplements a local Retrieval-Augmented Generation (RAG) architecture\nusing SBERT and an embedded vector database, enabling basic Q&A\nfunctionality with zero coding.\nServer mode\nTo enable the local HTTP server for REST API access (port 4891), navigate\nto GPT4All > Settings and enable the web server option.\nOpenAI REST API compatible endpoints\nThe server exposes OpenAI-compatible endpoints. Use the examples from\nsection 10.3.3 to send requests with curl, Python, or LangChain. Ensure the\nport is set to 4891, and verify the correct model name in the GPT4All\ndocumentation.\nGPT4All Python bindings\nYou can create a Python client using the GPT4All Python generation API.\nInstall the package from PyPI:\npip install gpt4all\nThen you can invoke the generate() function as follows:\nfrom gpt4all import GPT4All\n\n\nmodel = GPT4All('mistral-7b-instruct-v0.1.Q4_0.gguf')\noutput = model.generate('How many Greek temples are in Paestum?', \nprint(output)\nWhen you instantiate the model, the GGUF file will download automatically\nto a local directory if it is not already available. The Python API also supports\nstreaming responses.\nLangChain GPT4all python library\nLangChain integrates with the native GPT4All Python bindings, providing an\nadvantage by implementing standard LangChain Python interfaces, such as\nRunnable. This allows you to build LCEL-based solutions, as demonstrated\nin previous chapters.\nFirst, install the gpt4all package:\npip install gpt4all\nNext, test the GPT4All LangChain integration by running the code in Listing\nE.1. Use a dedicated environment created with venv for best practice.\nNOTE\nBefore running the code, ensure you download a GGUF quantized model of\nyour choice into the specified local directory: open the GPT4All application\nand navigate to the Models section in the left-hand menu. Click on + Add\nModel to browse the available models, then search for those labeled with the\n.gguf extension. Once you find the model you want, simply click Download\nto save it to your device.\nListing E.1 Connecting to a local open-source model via LangChain’s GPT4All Wrapper\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_community.llms import GPT4All\nprompt = ChatPromptTemplate.from_messages([ #A\n   (\"system\", \"You are a helpful AI assistant.\"),\n   (\"user\", \"{input}\")\n])\n\n\nmodel_path = ('./models/mistral-7b-instruct-v0.1.Q4_0.gguf') #B\nllm = GPT4All(model=model_path)\nchain = prompt | llm #C\nresponse = chain.invoke({\"input\": \"How many Greek temples are the\nprint(response)\nThis code demonstrates how to access a local open-source model through\nGPT4All using LangChain’s wrapper. With this integration, you can\nseamlessly implement complex workflows.\nE.4.8 Comparing local inference engines\nBefore closing this section, I want to provide a comparison of the local\ninference engines discussed.\nSome engines are optimized for consumer-grade hardware, while others\nsupport advanced configurations, including high-end GPUs. Most engines\ninclude an OpenAI-compatible HTTP server, but only a few provide\ndedicated bindings for specific programming languages. A summary of the\nmain characteristics of each engine is shown in Table E.3.\nTable E.3 Summary of the characteristics of local LLM inference engines\nInference\nengine\nBackend\nengine\nInstaller\navailable\nSupported\nOS\nOpenAI\nREST API\ncompatibility\nNative\nbindings\nD\nU\nllama.cpp llama.cpp\nNo\n(must\nbuild\nfrom\nsource)\nMacOS,\nLinux,\nWindows\nYes\nPython,\nJava, C#,\nGo, Scala,\nRuby,\nRust,\nScala,\nJavaScript,\nnode.js\nN\nollama\nllama.cpp\nYes\nMacOS,\nLinux;\nYes\nPython,\nJavascript\nN\n\n\nWindows \nvLLM\nvLLM\nYes\n(pip)\nLinux\nYes\nPython\nN\nllamafile\nllama.cpp\nYes\nMacOS,\nLinux,\nWindows\nYes\nN/A\nN\nLocalAI\nLlama.cpp\nand vLLM\nDocker\nimage\nAny\nhardware\nrunning\nDocker\nYes\nN/A\nN\nGPT4All\nLlama.cpp Yes\nMacOS,\nLinux\nUbuntu,\nWindows\nYes\nPython\nY\nLM\nStudio\nLlama.cpp Yes\nMacOS,\nLinux\nUbuntu,\nWindows\nYes\nPython\nY\nBy now, you should have a clear understanding of how to use local inference\nengines to run open-source LLMs. However, inference engines are not the\nonly option for serving local LLMs. In the next section, I’ll briefly explain\nhow to perform inference using the Hugging Face Transformers library.\nE.4.9 Choosing a local inference engine\nIf you are new to local LLM inference engines, start with a simple option like\nllamafile. It offers an easy way to experiment with lightweight models. Once\nyou’re comfortable, consider moving to ollama, which simplifies\ndownloading, configuring, and testing additional models.\nIf you prefer a graphical user interface, consider upgrading to LM Studio or\nGPT4All instead.\nFor production-grade solutions, vLLM is a strong choice. If you need\nmaximum control over all aspects of hosting an LLM, consider using\nllama.cpp. It provides advanced customization options but requires a deeper\n",
      "page_number": 528,
      "chapter_number": 61,
      "summary": "The LocalAI documentation\n(https://localai.io/docs) provides the appropriate Docker commands for each\nmodel and hardware configuration Key topics include models, local, and python.",
      "keywords": [
        "REST API",
        "local",
        "Python",
        "model",
        "API",
        "LocalAI",
        "inference",
        "inference engines",
        "REST API calls",
        "OpenAI-like REST API",
        "LLM",
        "local inference engines",
        "OpenAI REST API",
        "Docker",
        "REST"
      ],
      "concepts": [
        "models",
        "local",
        "python",
        "localai",
        "docker",
        "llm",
        "api",
        "llama",
        "option",
        "optional"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents In Action",
          "chapter": 6,
          "title": "Segment 6 (pages 41-49)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 37,
          "title": "Segment 37 (pages 322-331)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 7,
          "title": "Segment 7 (pages 53-60)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 32,
          "title": "Segment 32 (pages 264-271)",
          "relevance_score": 0.48,
          "method": "api"
        }
      ]
    },
    {
      "number": 62,
      "title": "Segment 62 (pages 536-543)",
      "start_page": 536,
      "end_page": 543,
      "detection_method": "topic_boundary",
      "content": "understanding of the hosting process.\nE.5 Inference via the HuggingFace Transformers\nlibrary\nFor full control over a model's architecture and the ability to modify it, you\ncan run a pre-trained model using the Hugging Face Transformers library.\nThis library is not only designed for experimentation and configuration but\nalso supports fine-tuning models, enabling you to share your improvements\nwith the Hugging Face community. Built on deep learning frameworks like\nJAX, PyTorch, and TensorFlow, it allows you to use one framework for\ntraining and another for inference.\nE.5.1 Hugging Face Transformers library\nBefore installing the transformers package, set up a virtual environment\nwith venv and install one or more of the following backends: Flax, PyTorch,\nor TensorFlow. Backend installation can vary depending on your hardware,\nso it is not covered here. Assuming the backends are installed, you can install\ntransformers via PyPI:\npip install transformers\nOnce installed, you can interact with pre-trained models stored locally. For\nexample, with the PyTorch backend, you can run inference on a quantized 4-\nbit version of Mistral Instruct as shown in Listing E.4 (which I have adapted\nfrom the Hugging Face documentation):\nListing E.4 Performing inference through the Hugging Face transformers library\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" #A\ntokenizer = AutoTokenizer.from_pretrained(model_id) #B\nmodel = AutoModelForCausalLM.from_pretrained(model_id, load_in_4b\ntext = \"Hello my name is\"\n\n\ninputs = tokenizer(text, return_tensors=\"pt\").to(0) #C\noutputs = model.generate(**inputs, max_new_tokens=20) #D\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nWhen using the Transformers library, you must explicitly handle\ntokenization. While this approach provides significant flexibility, it requires a\ndeeper understanding of transformer architecture, making it well-suited for\nadvanced use cases, research, and experimentation.\nLet’s see how you’d write the same code with LangChain.\nE.5.2 LangChain’s HuggingFace Pipeline\nLangChain offers a wrapper for the Hugging Face Transformers pipeline,\nsimplifying integration into LangChain applications. If you want to use\nHugging Face with LangChain, you can implement the code shown in listing\nE.5.\nListing E.5 HuggingFace transformers via LangChain\nfrom langchain_community.llms.huggingface_pipeline import Hugging\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pip\nfrom langchain.prompts import PromptTemplate\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" #A\ntokenizer = AutoTokenizer.from_pretrained(model_id) #B\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\npipe = pipeline(\"text-generation\", \n     model=model, tokenizer=tokenizer, max_new_tokens=50) #C\nhf_pipeline = HuggingFacePipeline(pipeline=pipe)\nprompt_template = \"\"\"Question: {question}\nAnswer: Let's think step by step.\"\"\" #D\nprompt = PromptTemplate.from_template(prompt_template)\nllm_chain = prompt | hf #E\nquestion = \"How many Greek temples are there in Paestum?\"\n\n\nprint(llm_chain.invoke({\"question\": question})) #F\nNow that I’ve covered how to run an open-source LLM locally, the next step\nis to explore practical applications. In the following section, I’ll show how to\nredirect the research summarization engine we built in Chapter 4 from public\nOpenAI endpoints to a local open-source LLM like Mistral.\nE.6 Building a local summarization engine\nRevisit the research summarization engine from Chapter 4 to compare the\noriginal OpenAI-based solution with a local open-source LLM. To do this,\nduplicate the Visual Studio Code project folder by copying the ch04 folder\nand renaming it apE. This setup allows you to test both versions side by side,\nmaking it easier to evaluate accuracy and performance.\nKeep in mind that processing with a local LLM will take significantly longer\ncompared to OpenAI's API. However, this exercise will provide valuable\ninsights into how hardware affects inference speed. It will also give you\nhands-on experience in evaluating whether and how to implement a project\nusing a local open-source LLM.\nE.6.1 Choosing the inference engine\nFirst, decide which inference engine to use. Options like GPT4All and LM\nStudio are user-friendly and suitable for those with basic LLM experience.\nTo minimize code changes, avoid rewriting the application with native\nPython bindings like those offered by GPT4All. Instead, use the OpenAI-\ncompatible REST API endpoints provided by both GPT4All and LM Studio.\nChoosing between them is a matter of preference since both meet the\nrequirements of usability and OpenAI compatibility. For this example, I’ll\nuse LM Studio.\nE.6.2 Starting up the OpenAI compatible server\nIf LM Studio is not already installed, follow the installation steps covered\nearlier. Open the Server menu, select a port (e.g., 8080), and start the server.\n\n\nLM Studio will now accept OpenAI-compatible requests.\nE.6.3 Modifying the original solution\nSince requests will be routed through an OpenAI-compatible endpoint, you\nonly need to make a small change in the llm_models.py file, shown in\nListing E.6.\nListing E.6 Original llm_models.py code\nfrom langchain.llms.openai import OpenAI\nopenai_api_key = 'YOUR_OPENAI_API_KEY'\ndef get_llm():\n   return OpenAI(openai_api_key=openai_api_key, model=\"gpt-4o-mi\nYou simply need to replace the implementation of get_llm() as shown in\nlisting E.7:\nListing E.7 Modified llm_models.py code, using a local open-source LLM\ndef get_llm():\n  port_number = '8080'\n  client = OpenAI(\n      base_url=f'http://localhost:{port_number}/v1', \n      api_key = ‘NO-KEY-NEEDED’,\n      model=’mistral' \n  )\n  return client \nThis approach avoids changes to the rest of the code. However, for flexibility,\nyou could make get_llm() configurable, as shown in Listing E.8.\nListing E.8 configurable get_llm()\ndef get_llm(is_llm_local=False):\n   port_number = '8080'\n   if is_llm_local:\n\n\n       client = OpenAI(\n           base_url=f'http://localhost:{port_number}/v1', \n           api_key='NO-KEY-NEEDED',\n           model='mistral'\n       )\n   else:\n       client = OpenAI(openai_api_key=openai_api_key, model=\"gpt\n   return client\nThis version allows you to switch between OpenAI and local LLMs. Keep in\nmind that some inference engines have fixed port numbers or specific model\nnaming conventions, which may require you to further generalize the code\nand make it more configurable.\nE.6.4 Running the summarization engine through the local\nLLM\nTo test the summarization engine, execute chain_try_5_1.py. The LM\nStudio server logs panel should display activity as the request is processed.\nWhen the output is generated (this may take some time, as noted earlier), it\nshould closely match the results from the original OpenAI-based\nsummarization.\nE.6.5 Comparison between OpenAI and local LLM\nWhen running the summarization engine with a local LLM, you’ll notice:\n1. Accuracy: The output is similar for this use case (web scraping and\nsummarization).\n2. Performance: Local LLM inference is slower, especially on CPUs.\nWithout a GPU, processing times can be significantly longer.\nThis suggests that while a quantized local open-source LLM is sufficient for\ndevelopment, it may not meet production requirements. Consider deploying\nLM Studio on hardware with an NVIDIA GPU or using a more advanced\ninference engine like vLLM. vLLM supports unquantized models and high-\nend GPUs, offering better performance for demanding use cases.\n\n\nE.7 Summary\nOpen-source LLMs provide potential cost savings, privacy control, and\nflexibility, with community-driven innovation ensuring competitive\nperformance and customization compared to proprietary models. These\nfeatures make open-source LLMs an appealing option for both\nexperimentation and production.\nTransparency in architecture, training data, and methods allows open-\nsource LLMs to be validated, modified, and tailored for compliance and\ntrust. This clarity ensures businesses can meet regulatory requirements\nwhile customizing models for specific needs.\nOpen-source LLMs empower organizations with lower total cost of\nownership and fine-tuning capabilities, enabling tailored solutions while\nmaintaining long-term affordability. This combination helps businesses\nachieve both financial efficiency and functional scalability.\nComparative tables simplify the evaluation of open-source LLMs,\nshowcasing their unique features and optimal use scenarios. By\ncomparing models side-by-side, you can quickly identify the best fit for\nyour project goals.\nOpen-source LLM weights enable local inference with accessible tools\nand modern techniques like quantization, supporting seamless OpenAI-\ncompatible API integration for cost-effective and private deployments.\nThese methods make running powerful models on consumer hardware\nmore practical and affordable.\nStart with a simple inference engine like llamafile for lightweight\nmodels, progress to tools like Ollama or LM Studio for ease of use, and\nchoose llama.cpp or vLLM for advanced, production-grade\ncustomization. This progression helps you build confidence though\nsimple user interfaces and scale capabilities as your project grows.\nApplications built for OpenAI can be adapted to open-source LLMs\nusing compatible REST APIs, offering insights into hardware impacts\non inference speed and feasibility. This adaptation enables cost-effective\nexperimentation and highlights areas for optimization.\n\n\nwelcome\nDear Reader,\nThank you so much for purchasing \"AI Agents and Applications\". Your\ninterest in this cutting-edge topic means a great deal to me, and I am excited\nto guide you through the expansive world of large language models (LLMs).\nThis book emerged from my professional journey as a software developer,\nwhere my main focus has been quantitative development in finance. Over the\nyears, I occasionally delved into AI projects aimed at tackling problems\nwhere traditional mathematical computations fell short. My fascination with\nLLMs began when I encountered ChatGPT in November 2022. Its\ncapabilities inspired me to explore further, leading me to experiment with the\nOpenAI APIs, delve into prompt engineering, and design applications using\nRetrieval Augmented Generation (RAG). My involvement with LangChain,\nan open-source LLM application framework, significantly accelerated my\nlearning and allowed me to witness its rapid adoption within the community.\n\"AI Agents and Applications\" is structured to cater both to beginners and\nseasoned professionals. It offers a comprehensive look into the foundational\ntechnologies and advanced techniques in the realm of LLMs. Whether you\nare new to programming or an experienced developer, you will find the\ncontent approachable yet enriching, especially with practical code examples\nto enhance your engagement.\nThe insights in this book are drawn from my real-world experiences and\ncontinuous learning in a field that evolves almost daily. I cover a range of\ntopics from running open source LLMs locally to advanced RAG techniques\nand fine-tuning methods. My goal is to provide a resource that I wish had\nbeen available to me—an accessible, practical guide that empowers you to\nnavigate and excel in this emerging domain.\nAs this field is still in its infancy, your feedback is incredibly valuable. I\nencourage you to actively participate in our online discussion forum at\n\n\nliveBook. Your questions, comments, and suggestions not only help improve\nthis book but also contribute to the broader community exploring LLM\napplications.\nThank you once again for joining me on this journey. I am eager to hear\nabout your experiences and look forward to any insights you might share.\nWarm regards,\n— Roberto Infante\nIn this book\nwelcome 1 Introduction to AI Agents and Applications 2 Executing prompts\nprogrammatically 3 Summarizing text using LangChain 4 Building a research\nsummarization engine 5 Agentic Workflows with LangGraph 6 RAG\nfundamentals with Chroma DB 7 Q&A chatbots with LangChain and\nLangSmith 8 Advanced indexing 9 Question transformations 10 Query\ngeneration, routing and retrieval post-processing 11 Building Tool-based\nAgents with LangGraph 12 Multi-agent Systems 13 Building and consuming\nMCP servers 14 Productionizing AI Agents: memory, guardrails, and beyond\nAppendix A. Trying out LangChain Appendix B. Setting up a Jupyter\nNotebook environment Appendix C. Choosing an LLM Appendix D.\nInstalling SQLite on Windows Appendix E. Open-source LLMs\n",
      "page_number": 536,
      "chapter_number": 62,
      "summary": "While this approach provides significant flexibility, it requires a\ndeeper understanding of transformer architecture, making it well-suited for\nadvanced use cases, research, and experimentation Key topics include model, openai.",
      "keywords": [
        "Hugging Face Transformers",
        "Hugging Face",
        "llm",
        "local open-source LLM",
        "Face Transformers library",
        "open-source LLM",
        "Face Transformers",
        "OpenAI",
        "Transformers",
        "LLMs",
        "Transformers library",
        "local",
        "local LLM",
        "open-source",
        "Hugging"
      ],
      "concepts": [
        "model",
        "llm",
        "openai",
        "llms",
        "inference",
        "open",
        "code",
        "applications",
        "application",
        "transformers"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 48,
          "title": "Segment 48 (pages 407-414)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 17,
          "title": "Segment 17 (pages 136-143)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 15,
          "title": "Segment 15 (pages 119-127)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 53,
          "title": "Segment 53 (pages 448-456)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 50,
          "title": "Segment 50 (pages 423-432)",
          "relevance_score": 0.47,
          "method": "api"
        }
      ]
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "chapter": null,
      "content": "tS andy\n\nRoberto Infante\n\nMEAP.\n\nEd MHANWING",
      "content_length": 44,
      "extraction_method": "OCR"
    },
    {
      "page_number": 2,
      "chapter": null,
      "content": "With LangChain, LangGraph, and MCP\n\nRoberto Infante\n\ni HANHIHG",
      "content_length": 62,
      "extraction_method": "OCR"
    },
    {
      "page_number": 3,
      "chapter": null,
      "content": "AI Agents and Applications\n1. welcome\n2. 1_Introduction_to_AI_Agents_and_Applications\n3. 2_Executing_prompts_programmatically\n4. 3_Summarizing_text_using_LangChain\n5. 4_Building_a_research_summarization_engine\n6. 5_Agentic_Workflows_with_LangGraph\n7. 6_RAG_fundamentals_with_Chroma_DB\n8. 7_Q&A_chatbots_with_LangChain_and_LangSmith\n9. 8_Advanced_indexing\n10. 9_Question_transformations\n11. 10_Query_generation,_routing_and_retrieval_post-processing\n12. 11_Building_Tool-based_Agents_with_LangGraph\n13. 12_Multi-agent_Systems\n14. 13_Building_and_consuming_MCP_servers\n15. 14_Productionizing_AI_Agents:_memory,_guardrails,_and_beyond\n16. Appendix_A._Trying_out_LangChain\n17. Appendix_B._Setting_up_a_Jupyter_Notebook_environment\n18. Appendix_C._Choosing_an_LLM\n19. Appendix_D._Installing_SQLite_on_Windows\n20. Appendix_E._Open-source_LLMs\n21. index\n",
      "content_length": 847,
      "extraction_method": "Direct"
    },
    {
      "page_number": 4,
      "chapter": null,
      "content": "welcome\nDear Reader,\nThank you so much for purchasing \"AI Agents and Applications\". Your\ninterest in this cutting-edge topic means a great deal to me, and I am excited\nto guide you through the expansive world of large language models (LLMs).\nThis book emerged from my professional journey as a software developer,\nwhere my main focus has been quantitative development in finance. Over the\nyears, I occasionally delved into AI projects aimed at tackling problems\nwhere traditional mathematical computations fell short. My fascination with\nLLMs began when I encountered ChatGPT in November 2022. Its\ncapabilities inspired me to explore further, leading me to experiment with the\nOpenAI APIs, delve into prompt engineering, and design applications using\nRetrieval Augmented Generation (RAG). My involvement with LangChain,\nan open-source LLM application framework, significantly accelerated my\nlearning and allowed me to witness its rapid adoption within the community.\n\"AI Agents and Applications\" is structured to cater both to beginners and\nseasoned professionals. It offers a comprehensive look into the foundational\ntechnologies and advanced techniques in the realm of LLMs. Whether you\nare new to programming or an experienced developer, you will find the\ncontent approachable yet enriching, especially with practical code examples\nto enhance your engagement.\nThe insights in this book are drawn from my real-world experiences and\ncontinuous learning in a field that evolves almost daily. I cover a range of\ntopics from running open source LLMs locally to advanced RAG techniques\nand fine-tuning methods. My goal is to provide a resource that I wish had\nbeen available to me—an accessible, practical guide that empowers you to\nnavigate and excel in this emerging domain.\nAs this field is still in its infancy, your feedback is incredibly valuable. I\nencourage you to actively participate in our online discussion forum at\n",
      "content_length": 1925,
      "extraction_method": "Direct"
    },
    {
      "page_number": 5,
      "chapter": null,
      "content": "liveBook. Your questions, comments, and suggestions not only help improve\nthis book but also contribute to the broader community exploring LLM\napplications.\nThank you once again for joining me on this journey. I am eager to hear\nabout your experiences and look forward to any insights you might share.\nWarm regards,\n— Roberto Infante\nIn this book\nwelcome 1 Introduction to AI Agents and Applications 2 Executing prompts\nprogrammatically 3 Summarizing text using LangChain 4 Building a research\nsummarization engine 5 Agentic Workflows with LangGraph 6 RAG\nfundamentals with Chroma DB 7 Q&A chatbots with LangChain and\nLangSmith 8 Advanced indexing 9 Question transformations 10 Query\ngeneration, routing and retrieval post-processing 11 Building Tool-based\nAgents with LangGraph 12 Multi-agent Systems 13 Building and consuming\nMCP servers 14 Productionizing AI Agents: memory, guardrails, and beyond\nAppendix A. Trying out LangChain Appendix B. Setting up a Jupyter\nNotebook environment Appendix C. Choosing an LLM Appendix D.\nInstalling SQLite on Windows Appendix E. Open-source LLMs\n",
      "content_length": 1086,
      "extraction_method": "Direct"
    },
    {
      "page_number": 6,
      "chapter": null,
      "content": "1 Introduction to AI Agents and\nApplications\nThis chapter covers\nCore challenges in building LLM-powered applications\nLangChain’s modular architecture and components\nPatterns for engines, chatbots, and agents\nFoundations of prompt engineering and RAG\nAfter the launch of ChatGPT in late 2022, developers quickly began\nexperimenting with applications powered by large language models (LLMs).\nSince then, LLMs have moved from novelty to necessity, becoming a new\nstaple in the application development toolbox—much like databases or web\ninterfaces in earlier eras. They give applications the ability to answer\ncomplex questions, generate tailored content, summarize long documents,\nand coordinate actions across systems. More importantly, LLMs have\nunlocked a new class of applications: AI agents. Agents take input in natural\nlanguage, decide which tools or services to call, orchestrate multi-step\nworkflows, and return results in a clear, human-friendly format.\nYou could build such systems entirely from scratch, but you would quickly\nencounter recurring challenges: how to ingest and manage data, how to\nstructure prompts, how to chain model calls reliably, and how to integrate\nexternal APIs and services. Reinventing these patterns each time is slow and\nerror-prone. This is where frameworks come in. LangChain, LangGraph, and\nLangSmith provide modular building blocks that eliminate boilerplate,\npromote best practices, and let you focus on application logic instead of low-\nlevel wiring. Throughout this book, we will use these frameworks as practical\ntools—not as the goal, but as the means—to learn how to design, build, and\nscale real LLM-based applications and agents.\nIn this opening chapter, we will set the foundation. You’ll get an overview of\nthe main problems LLM applications aim to solve, explore the architecture\n",
      "content_length": 1832,
      "extraction_method": "Direct"
    },
    {
      "page_number": 7,
      "chapter": null,
      "content": "and object model that LangChain provides, and examine the three primary\nfamilies of LLM-powered applications: engines, chatbots, and AI agents. We\nwill also introduce the key patterns—such as Retrieval-Augmented\nGeneration (RAG) and prompt engineering—that you’ll return to throughout\nthe book. By the end of this chapter, you will have a clear picture of the\nchallenges in building LLM applications, the patterns that solve them, and the\nframeworks we will use to bring those solutions to life.\n1.1 Introducing LangChain\nImagine you’ve been asked to build a chatbot that can answer customer\nquestions from your company’s documentation, or a search engine that\nretrieves precise answers from thousands of internal reports. You quickly\ndiscover the same pain points:\nHow do you get your own data into the model reliably, without pasting\nentire documents into prompts?\nHow do you keep prompts, chains, and integrations maintainable as your\nfeatures grow?\nHow do you handle context limits and costs while still preserving\naccuracy?\nHow do you orchestrate multi-step workflows and API calls without\nfragile glue code?\nAnd once your app is live, how do you evaluate, debug, and monitor its\nbehavior?\nWithout a framework, developers end up re-implementing the same plumbing\n—load data, split it, embed it, store it, retrieve it, prompt it—again and again.\nIt works for a demo, but it doesn’t scale. LangChain addresses these\nproblems by standardizing common patterns into modular, composable\ncomponents. With loaders, splitters, embeddings, retrievers, vector store\nretrievers, and prompt templates, you can focus on application logic instead\nof boilerplate. The LangChain Expression Language (LCEL) and the\nRunnable interface then let you chain these pieces together consistently,\nmaking pipelines easier to build, debug, and maintain.\nLangChain has also evolved rapidly, fueled by an active open-source\n",
      "content_length": 1899,
      "extraction_method": "Direct"
    },
    {
      "page_number": 8,
      "chapter": null,
      "content": "community. It continues to keep pace with advances in LLM architectures,\nnew data sources, and retrieval technologies, while helping to establish\nshared best practices for building and deploying LLM-based systems.\nThree principles guide its design: modularity, composability, and\nextensibility. Components follow standard interfaces, so you can swap an\nLLM, change a vector store, or add a new data connector without rewriting\nyour entire application. Real-world tasks can be composed from multiple\ncomponents, forming chains or agent workflows that dynamically select the\nright tools for the job. And while default implementations exist, you can\nalways extend or replace them with custom logic or third-party integrations,\navoiding lock-in and promoting interoperability.\nBy learning LangChain, you not only gain the ability to build production-\ngrade LLM applications, but also acquire transferable skills. Competing\nframeworks solve similar problems in similar ways, so once you understand\nthese patterns, you can adapt them to whatever stack you choose in the future.\n1.1.1 LangChain architecture\nLangChain’s documentation is thorough, but the best way to really\nunderstand how it works is by building with it. This section gives you a high-\nlevel overview that we’ll keep coming back to in later chapters. Think of it as\nthe map you’ll use while we dive into the details and code examples later on.\nMost LLM frameworks follow a similar overall pattern, but each one defines\nits own components. In LangChain, the workflow looks roughly like what\nyou see in Figure 1.1. You start by pulling in text from different sources—\nfiles, databases, or websites—and wrapping it into Document objects. Those\ndocuments are often split into smaller chunks so they’re easier to handle.\nNext, each chunk is passed through an embedding model, which turns the\ntext into vectors that capture its meaning. Both the raw chunks and their\nembeddings are stored in a vector store, which lets you quickly retrieve the\nmost relevant pieces of text based on similarity search.\nFigure 1.1 LangChain architecture: The Document Loader imports data, which the Text Splitter\ndivides into chunks. These are vectorized by an Embedding Model, stored in a Vector Store, and\nretrieved through a Retriever for the LLM. The LLM Cache checks for prior requests to return\n",
      "content_length": 2336,
      "extraction_method": "Direct"
    },
    {
      "page_number": 9,
      "chapter": null,
      "content": "cached responses, while the Output Parser formats the LLM's final response.\n",
      "content_length": 76,
      "extraction_method": "Direct"
    },
    {
      "page_number": 10,
      "chapter": null,
      "content": "databases and web pages\n\nLLM applications process unstructured\ntext coming from a variety of sources,\nsuch as document and text files,\n\nA Splitter splits\nthe raw text into txt PDF. docx, em!\nalist of smaller unstructured text fles\n\nDatabases Websites CSV, JSON, XML\nstructured files\n\ndocuments An Output Parser\nCalled chunks transforms the LLM\nmo _—— response into a\neasily than the The Document say CY mt\noriginal text Loader parses SON\nextemal text content\ninto a typed\n\nDocument object\n\nLarge Language\nModel (LLM) or\nchat model\n\nDocument\nchunks\n\nA prompt encapsulates\nthe question asked to\nthe LLM and the context\nrepresented by chunks\nretrieved from a vector\nstore or a knowledge\ngraph database\n\nAn embedding model\ntransforms chunks into\nvectors representing the\nsemantic meaning of the\nunderlying information",
      "content_length": 813,
      "extraction_method": "OCR"
    },
    {
      "page_number": 11,
      "chapter": null,
      "content": "When an LLM app runs a task—say summarization or semantic search—it\nbuilds a prompt that combines the user’s question with extra context. That\ncontext usually comes from document chunks pulled out of a vector store.\nSometimes, though, you’ll also want to bring in information from a graph\ndatabase. Vector stores are still the backbone of most retrieval-augmented\ngeneration (RAG) workflows, but graph databases are becoming more\ncommon in apps that need to represent and reason about relationships\nbetween entities. LangChain already integrates with popular options like\nNeo4j, and it lets you use graph-based memory or planning components—\nfeatures that are showing up more and more in advanced agent architectures.\nTo make all of this easier to wire together, LangChain introduced the\nRunnable interface and the LangChain Expression Language (LCEL). These\ngive you a clean, consistent way to chain components without writing piles of\nglue code. We’ll go deeper into both later in this chapter.\nIt’s also worth keeping in mind that LangChain’s workflow isn’t locked into\na simple pipeline. You can arrange components as graphs to handle more\ncomplex, branching flows—a capability formalized in the LangGraph\nmodule. We’ll dig into these patterns and architectural variations in the next\nsection, where the bigger picture of LLM application design comes into\nfocus.\nA detailed description of each component follows below (it references the\nnumbering in figure 1.1):\nDocument loaders (1): in LangChain, document loaders play a pivotal\nrole by extracting data from various sources and transforming it into\nDocument objects. These Documents serve as the fundamental entities\nprocessed within the system.\nText splitters (2): As we'll discuss in upcoming chapters, text splitters\nare crucial for breaking down text from a data source into smaller\nDocument instances or \"chunks.\" This approach helps overcome the\nlimitation of the maximum prompt size, also known as “context\nwindow”. It's especially relevant in the ingestion phase of the Question\n& Answer use case, where the original text is split before being indexed\nwith embeddings and stored in the vector database.\n",
      "content_length": 2167,
      "extraction_method": "Direct"
    },
    {
      "page_number": 12,
      "chapter": null,
      "content": "Document: In LangChain, a Document is a fundamental data structure that\nencapsulates both content and metadata. It represents a unit of text—\nstructured or unstructured—along with relevant contextual information.\nFor example, the text extracted from a PDF file can be wrapped in a\nDocument object, with metadata storing details like the source file name\nor page number. When working with larger texts, a Text Splitter can\ndivide them into smaller segments, each represented as an individual\nDocument, making it easier to process them in chunks.\nEmbedding models (3): LangChain provides support for the most\npopular embedding models through convenient wrappers. These models\nare designed to capture the semantic meaning of text and convert it into\nnumerical vectors.\nVector stores (4): vector stores operate as specialized databases designed\nfor the efficient retrieval of Document objects. These objects typically\nrepresent fragments or \"chunks\" of the original document, indexed based\non their associated embeddings or vector-based semantic\nrepresentations. This indexing allows for search queries to match the\nembeddings of the chunks against those of the query, retrieving the most\nsimilar embeddings and their related chunks. By ingesting Documents\nalong with their associated embeddings, the vector store can serve as an\noffline knowledge base for proprietary data. LangChain is compatible\nwith various popular vector stores.\nKnowledge Graph databases: Although not a key component of the\narchitecture, LangChain offers client wrappers for leading graph\ndatabases to facilitate Knowledge Graph functionality. These databases\nstore entities and their relationships in a graph form.\nRetrievers (5): In LangChain, retrievers efficiently fetch data, often a list\nof Documents containing unstructured text, from indexed databases like\nvector stores. LangChain provides support for various retrievers,\ncatering not only to vector stores but also to relational databases and\nmore complex data stores, such as knowledge graph databases like\nNeo4j.\nPrompts (6): LangChain provides tools for defining prompt templates—\nreusable structures that can be dynamically filled with input to create\nprompt instances. These inputs may come from users or external sources\nsuch as vector stores. Once a prompt instance is fully constructed, it is\nsent to the Language Model (LLM) for processing. To improve prompt\n",
      "content_length": 2398,
      "extraction_method": "Direct"
    },
    {
      "page_number": 13,
      "chapter": null,
      "content": "quality and guide the model’s behavior, you can enhance templates by\nincluding examples, a technique often used in few-shot learning.\nLLM Cache (7): An optional component that improves performance and\nreduces costs by minimizing calls to the underlying LLM. It provides\nimmediate answers to previously asked questions.\nLLM / ChatModel (8): This component acts as an interface to an LLM\nor a chat model built on an LLM. LangChain supports multiple LLMs\nand associated chat models, including those from OpenAI, Cohere, and\nHuggingFace. Additionally, LangChain supports a Fake LLM for unit\ntesting purposes.\nOutput Parser (9): This component transforms an LLM's natural\nlanguage response into a structured format, such as JSON, making it\neasier for the LLM application to process.\nThe components above can be organized into a chain or structured around\nagents:\nChain: A composite arrangement guiding LangChain's processing\nworkflow, customized for specific use cases and based on a sequence of\nthe described components.\nAgent: This component manages a dynamic workflow, extending a\nsequential chain. The agent's processing is flexible and can adapt based\non user input or component output. Resources in the dynamic workflow\nare called \"tools\" or \"plugins\" in most frameworks' documentation, and\nthe collection of all tools is referred to as the \"toolkit.\"\nLangChain's comprehensive design supports three primary application types,\nwhich we’ll examine in detail shortly: summarization and query services,\nchatbots, and agents. Although the framework can seem complex at first\nglance, the detailed explanations in upcoming chapters will break down each\ncomponent and clear up any uncertainties.\n1.2 LangChain core object model\nWith a good understanding of LangChain’s high-level architecture, we can\nnow turn to its core object model for a clearer picture of how the framework\noperates. Gaining familiarity with the key objects and their interactions will\n",
      "content_length": 1951,
      "extraction_method": "Direct"
    },
    {
      "page_number": 14,
      "chapter": null,
      "content": "significantly improve your ability to use LangChain effectively.\nThe object model is organized into class hierarchies, beginning with abstract\nbase classes from which multiple concrete implementations are derived.\nFigure 1.2 provides a visual overview of the main class families used across\nvarious tasks, all centered around the Document entity. It illustrates how\nloaders generate Document objects, how splitters divide them into smaller\nsegments, and how these are then passed into vector stores and retrievers for\ndownstream processing.\nAs covered in section 1.1.1 on LangChain architecture, the primary classes\ninclude:\nDocument\nDocumentLoader\nTextSplitter\nVectorStore\nRetriever\nLangChain integrates with a wide variety of third-party tools and services\nacross these components, offering great flexibility. You can find a full list of\nsupported integrations in the official documentation. In addition, LangChain\nprovides the LangChain Hub, a community-driven repository for discovering\nand sharing reusable components such as prompts, chains, and tools.\nA key architectural feature shared by many of these components—from\nloaders to LLMs—is the Runnable interface. This common interface allows\nobjects to be composed and chained together consistently, enabling highly\nmodular workflows. We'll dive deeper into this feature, and into the\nLangChain Expression Language (LCEL), in a later section focused on\nbuilding composable and expressive LLM pipelines.\nFigure 1.2 Object model of classes associated with the Document core entity, including Document\nloaders (which create Document objects), splitters (which create a list of Document objects),\nvector stores (which store Document objects in vector stores) and retrievers (which retrieve\nDocument objects from vector stores and other sources)\n",
      "content_length": 1798,
      "extraction_method": "Direct"
    },
    {
      "page_number": 15,
      "chapter": null,
      "content": "Document loaders\nparse external text\ncontent into a typed\nDocument object\n\nDocument transformers\ntransform a Document\ninto one or more\nDocument objects. For\nexample, a Splitter splits :\nthe raw text into a list of +\nsmaller documents called\nchunks thatcanbe\nprocessed more easily\nthan the original text\n\nA Document, considered\nthe core entity in\nLangChain, encapsulates\nboth the content and\nmetadata of unstructured\nor structured data\n\nA vector store contains\n\nDocuments (and text\nchunks) and the related\nvector representations\nused for semantic\nsearches\n\n: listof\nobjects»\n\nceeeeee\n\nA Retriever efficiently fetch\ndata, often a list of\nDocuments containing\nunstructured text, from\nindexed databases like\nvector stores, but also\nfrom other sources\n\n«create\n\neeeeeee",
      "content_length": 764,
      "extraction_method": "OCR"
    },
    {
      "page_number": 16,
      "chapter": null,
      "content": "In figure 1.3, you can see the object model related to Language Models,\nincluding PromptTemplate and PromptValue. This figure illustrates how\nthese classes connect to the LLM interface, exposing a somewhat more\ncomplex hierarchy than the classes presented earlier.\nFigure 1.3 Object model of classes associated with Language Models, including Prompt\nTemplates and Prompt Values\n",
      "content_length": 378,
      "extraction_method": "Direct"
    },
    {
      "page_number": 17,
      "chapter": null,
      "content": "A PromptTemplate is A MessagePromptTempiate\ndesigned to generate a iS designed to generate a\nPromptValue instance PromptValue instance from a\n\nfrom template structures template structure including\nand input parameters messages from various roles\n(human, system, Al) and\ninput parameters\n«abstract»\nBaseLanguageModel\nALLM model is suitable 1 AChat model is suitable\nfor one-off instructions from a chatbox which\ninvoked by a client query abstract» «abstracts accepts interactive\nengine BaseChatModel instructions and keeps an\n: . history for reference\n\n«abstracts\nLLM\n\nChatOpenAl\n\n(many others ...) SimpleChatModel\n\n(many others ...)\n\ni\n\n(many others ...)",
      "content_length": 654,
      "extraction_method": "OCR"
    },
    {
      "page_number": 18,
      "chapter": null,
      "content": "Now that you've reviewed LangChain's architecture and object model, let's\npause and explore the types of applications you can build with it.\n1.3 Building LLM applications and AI agents\nLLMs are great at handling natural language—they can understand text,\ngenerate it, and pull out information when you need it. That makes them\nuseful for all kinds of applications: summarization, translation, sentiment\nanalysis, semantic search, chatbots and\ncode generation. Because of this range, you’ll now see LLMs showing up in\nfields as varied as education, healthcare, law, and finance.\nEven with all these different use cases, most LLM apps end up looking pretty\nsimilar under the hood. They take in natural language input, work with\nunstructured data, pull in extra context from one or more sources, and then\npackage everything into a prompt for the model to process.\nFrom a high level, these applications generally fall into three main categories:\nLLM-based applications or engines – Systems that provide a specific\ncapability such as summarization, search, or content generation.\nChatbots – Conversational interfaces that maintain context over multiple\nexchanges.\nAI agents – Autonomous or semi-autonomous systems that use LLMs to\nplan and execute multi-step tasks, often interacting with external tools or\nAPIs.\nWe’ll examine each of these categories in turn before exploring how\nLangChain supports their development—starting with LLM-based\napplications and engines.\n1.3.1 LLM-based applications: summarization and Q&A\nengines\nAn LLM-based engine acts as a backend tool that handles specific natural\n",
      "content_length": 1596,
      "extraction_method": "Direct"
    },
    {
      "page_number": 19,
      "chapter": null,
      "content": "language requests for other systems. For example, a summarization engine\ncondenses lengthy text passages into concise summaries. These summaries\ncan be returned immediately to the client or stored in a database for later use\nby other applications, as shown in Figure 1.4.\nFigure 1.4 A summarization engine efficiently summarizes and stores content from large volumes\nof text and can be invoked by other systems through REST API.\nSummarization engines are often deployed as shared services, typically\nexposed through a REST API so multiple systems can call them on demand.\nAnother common type is the Question & Answer (Q&A) engine, which\nanswers user queries against a knowledge base. A Q&A engine works in two\nphases: ingestion and query.\nIn the ingestion phase, the engine builds its knowledge base by pulling in\n",
      "content_length": 814,
      "extraction_method": "Direct"
    },
    {
      "page_number": 20,
      "chapter": null,
      "content": "text, splitting it into chunks, and converting those chunks into embeddings—\nmathematical vectors that capture meaning. Both the embeddings and the\noriginal chunks are stored in a vector store for efficient retrieval. Don’t worry\nif “embedding model” or “vector store” sound unfamiliar; we’ll cover them\nin detail later. For now, just think of this step as transforming raw text into a\nsearchable representation.\nDefinition\nEmbeddings are vector representations of words, tokens, or larger text units\n—such as sentences, paragraphs, or document chunks—mapped into a\ncontinuous, high-dimensional space (typically with hundreds or thousands of\ndimensions). These vectors capture semantic and syntactic relationships,\nallowing language models to understand meaning, context, and similarity.\nEmbeddings are learned during the pre-training phase, where models are\ntrained on large-scale text corpora to predict tokens based on surrounding\ncontext. By encoding both individual words and broader semantic meaning,\nembeddings enable more effective reasoning, retrieval, and language\nunderstanding.\nIn the query phase, the engine takes a user’s question, turns it into an\nembedding using the same model, and performs a semantic search over the\nvector store. The most relevant chunks are retrieved and combined with the\noriginal question to form a prompt, which is then sent to the LLM. The model\nuses both the question and the retrieved context to generate an accurate,\ngrounded answer.\nThis workflow is known as Retrieval-Augmented Generation (RAG), shown\nin Figure 1.5. RAG has quickly become a cornerstone of modern LLM\napplications, and we’ll spend three full chapters (8, 9, and 10) diving into\nadvanced techniques for making it more powerful and reliable.\nDefinition\nRetrieval-Augmented Generation (RAG) is a design pattern where the LLM’s\ntext “generation” is “augmented” by incorporating additional context,\n“retrieved” from a local knowledge base—often stored in a vector store—at\n",
      "content_length": 1981,
      "extraction_method": "Direct"
    },
    {
      "page_number": 21,
      "chapter": null,
      "content": "query time.\nFigure 1.5 A Q&A engine implemented with RAG design: an LLM query engine stores domain-\nspecific document information in a vector store. When an external system sends a query, it\nconverts the natural language question into its embeddings (or vector) representation, retrieves\nthe related documents from the vector store, and then gives the LLM the information it needs to\ncraft a natural language response.\nLangChain makes it straightforward to build engines like question-answering\nsystems by giving you modular components out of the box: document\nloaders, text splitters, embedding models, vector stores, and retrievers. You\n",
      "content_length": 639,
      "extraction_method": "Direct"
    },
    {
      "page_number": 22,
      "chapter": null,
      "content": "can snap these together with minimal boilerplate, which means you spend\nmore time on your application’s logic and less time wrestling with low-level\ndetails. Later in the book, we’ll look at how to evaluate and choose among\ndifferent embedding models and vector stores. LangChain supports many\npopular options, so you can mix and match without getting locked into a\nsingle vendor.\nEngines aren’t limited to Q&A. They can also call external tools by running\npredefined sequences of steps, often called chains. For example, an engine\nhandling a user request might convert natural language instructions into API\ncalls, pull data from outside systems, and then use an LLM to interpret and\npresent the results in a clean, human-readable format.\nAt their core, these engines are designed for system-level work: automating\nprocesses, processing data intelligently, and stitching together different\nplatforms. They simplify workflows that involve natural language by\nhandling the messy parts—retrieval, transformation, orchestration—so you\ndon’t have to.\nBut once a human is directly interacting with the system in real time, the\ngame changes. At that point, the engine takes on a conversational role,\nshifting from pure automation into dialogue. This is where chatbots come\ninto play.\n1.3.2 LLM-based chatbots\nAn LLM-based chatbot acts as an intelligent assistant, enabling ongoing,\nnatural conversations with a language model. Unlike simple question–answer\nscripts, these systems aim to keep interactions both useful and safe. They rely\nheavily on prompt design: clear instructions shape the model’s behavior and\nhelp prevent irrelevant, inaccurate, or unsafe replies. Modern chat APIs—\nsuch as those from OpenAI—support role-based messaging formats (system,\nuser, assistant), which let you define an assistant’s persona and enforce\nconsistent behavior across a conversation.\nTo improve accuracy, chatbots often pull in factual context from local\nknowledge sources like vector stores. This lets them blend conversational\n",
      "content_length": 2015,
      "extraction_method": "Direct"
    },
    {
      "page_number": 23,
      "chapter": null,
      "content": "fluency with domain-specific grounding, so the answers aren’t just smooth\nbut also relevant and reliable.\nA key strength of chatbots is conversation memory. By tracking earlier turns,\nthey can keep responses coherent and personalized. This memory is limited\nby the model’s context window. Larger windows help, but many systems still\ncompress or summarize conversation history to stay within limits—and to\nmanage cost.\nLLM-based chatbots are usually specialized for tasks such as summarization,\nquestion answering, or translation. They can either respond directly to user\ninput or combine it with stored knowledge. For example, the architecture of a\nsummarization chatbot (figure 1.6) builds on a basic summarization engine,\nbut adds dialogue management and context-awareness layers.\nFigure 1.6 A summarization chatbot has some similarities with a summarization engine, but it\noffers an interactive experience where the LLM and the user can work together to fine-tune and\nimprove the results.\n",
      "content_length": 992,
      "extraction_method": "Direct"
    },
    {
      "page_number": 24,
      "chapter": null,
      "content": "The crucial difference between a summarization engine and a summarization\nchatbot is interactivity. A chatbot lets you refine responses in real time: if you\nwant a shorter or more casual summary, you can just ask, as illustrated in the\nsequence diagram in Figure 1.7.\nFigure 1.7 Sequence diagram that outlines how a user interacts with an LLM through a chatbot\nto create a more concise summary.\n",
      "content_length": 395,
      "extraction_method": "Direct"
    },
    {
      "page_number": 25,
      "chapter": null,
      "content": "pom}\n\nChatbot\nREST API\n\n—— — em mm a a ee — ———",
      "content_length": 47,
      "extraction_method": "OCR"
    },
    {
      "page_number": 26,
      "chapter": null,
      "content": "This back-and-forth makes the process collaborative—producing answers\nthat feel more tailored and context-aware. In the next chapter, we’ll explore\ntechniques like role instructions, few-shot examples, and advanced prompt\nengineering to give you more control over chatbot behavior and output.\n1.3.3 AI agents\nAn AI agent is a system that works with a large language model (LLM) to\ncarry out multi-step tasks—often involving multiple data sources, branching\nlogic, and adaptive decision-making. Unlike simple pipelines, agents can\noperate with a degree of independence: they follow the constraints you set,\nbut they make choices about what to do next.\nAt each step, the agent consults the LLM to decide which tools to use, runs\nthose tools, and processes the results before moving on. This loop continues\nuntil the agent produces a complete solution. In practice, that might mean\npulling information from both structured sources (like databases or APIs) and\nunstructured ones (like documents or web pages), combining the results, and\npresenting them in a coherent format.\nConsider this example: a tour operator uses an AI agent to generate holiday\npackages based on natural language requests from a booking website. As\nshown in Figure 1.8, the process could look like this:\n1. The agent sends a prompt to the LLM asking it to choose the most\nrelevant tools for the request—for example, flight, hotel, and car rental\nproviders; weather forecast services; and internal holiday deals\ndatabases.\n2. Guided by a developer-crafted prompt listing available tools and their\ndescriptions, the LLM selects the appropriate ones and generates the\nrequired queries—such as SQL for the holiday deals database or REST\nAPI calls for external providers.\n3. The agent executes the queries, gathers the results, and sends another\nprompt to the LLM containing both the original holiday request and the\ncollected data.\n4. The LLM responds with a summarized holiday plan that includes all\n",
      "content_length": 1966,
      "extraction_method": "Direct"
    },
    {
      "page_number": 27,
      "chapter": null,
      "content": "bookings, which the agent then returns to the booking website.\nFigure 1.8 Workflow of an AI agent tasked with assembling holiday packages: An external client\nsystem sends a customized holiday request in natural language. The agent prompts the LLM to\nselect tools and formulate queries in technology-specific formats. The agent executes the queries,\ngathers the results, and sends them back to the LLM, along with the original request, to obtain a\ncomprehensive holiday package summary. Finally, the agent forwards the summarized package\nto the client system.\nNote\n",
      "content_length": 564,
      "extraction_method": "Direct"
    },
    {
      "page_number": 28,
      "chapter": null,
      "content": "The workflow can involve multiple iterations between the agent and the LLM\nbefore producing a final output—such as a complete holiday plan. Also, the\narchitecture shown in figure 1.8 is only one of the possible ones. An\nalternative design could be based on a set of more granular agents\ncoordinated by a supervisor agent at the top. We’ll explore these designs in\nchapters 11, 12 and 13. In high-stakes domains like finance or healthcare, it's\ncommon to include a human-in-the-loop step. This ensures that a human can\nreview and validate critical actions—such as approving a financial\ntransaction or confirming a complex medical recommendation—before\nfinalizing them. In the holiday planning example, the agent could be\nprogrammed to pause and request human approval of the proposed itinerary\nbefore sending it to the client, adding an extra layer of oversight and trust.\nLangChain’s agent framework also allows developers to incorporate human\nvalidation steps as part of the toolchain.\nThere’s been a surge of interest in AI agents, and major players like OpenAI,\nGoogle, and Amazon have all released agent SDKs to encourage developers\nand companies to adopt the approach. LangChain’s agent framework—\nespecially with the introduction of LangGraph—is built to handle these\nmulti-step, multi-tool workflows in a modular, controlled way.\nLangGraph provides pre-built agent and orchestrator classes, along with\nready-to-use tool integrations, so you don’t have to reinvent the wheel.\nBetween Chapters 11 and 13, you’ll get hands-on experience building\nadvanced agents with LangGraph, learning how to design, orchestrate, and\nrefine them in practice.\nIn many ways, an AI agent represents the most advanced form of LLM\napplication. It leverages the full spectrum of LLM capabilities—\nunderstanding, generating, and reasoning over text—to drive complex,\nautomated workflows. Agents dynamically select and use multiple tools,\nguided by prompts you design, making them powerful across industries like\nfinance, healthcare, and logistics where multi-step reasoning and data\nintegration are essential.\nInterest in agents has accelerated with the introduction of the Model Context\nProtocol (MCP) by Anthropic. MCP defines a standard way for services to\nexpose tools through MCP servers, which agents can then access via MCP\n",
      "content_length": 2313,
      "extraction_method": "Direct"
    },
    {
      "page_number": 29,
      "chapter": null,
      "content": "clients as easily as if they were local components. This shifts the integration\nburden to the service itself, letting developers focus on building capable\nagents rather than writing and maintaining custom connectors. Since its\nrelease in late 2024, MCP has quickly become a de facto standard—adopted\nby major LLM providers like OpenAI and Google—and thousands of tools\nare now available through public MCP portals.\nWe’ll dive into MCP in Chapter 13, where you’ll learn the protocol, explore\nthe ecosystem in practice, and build your own MCP server that can plug\ndirectly into an agent application.\nNow that you’ve seen LangChain’s purpose, architecture, and the core\napplication types it supports, I encourage you to try out LangChain using the\nJupyter notebook described in Appendix A. It’s a quick way to get a feel for\nthe framework before we dive deeper!\n1.4 Typical LLM use cases\nLLMs are applied across a wide range of tasks, from text classification to\ncode generation and logical reasoning. Below are some of the most common\nuse cases, along with real-world examples for further exploration:\nText Classification and Sentiment Analysis: This includes categorizing\nnews articles or recommending stocks based on sentiment analysis. For\nexample, GoDaddy uses LLMs to automatically classify support tickets,\nas described in the article “LLM From the Trenches: 10 Lessons\nLearned Operationalizing Models at GoDaddy.”\nNatural Language Understanding and Generation: LLMs can identify\nmain topics in a text and generate summaries tailored by length, tone, or\nterminology. Duolingo uses AI to accelerate lesson creation, detailed in\ntheir blog post “How Duolingo Uses AI to Create Lessons Faster.”\nSummarization is explored in depth in Chapters 3 and 4.\nSemantic Search: This involves querying a knowledge base based on the\nintent and context of a question, rather than relying on simple keywords.\nThe Picnic supermarket app uses LLMs to enhance recipe search, as\nexplained in the Medium post “Enhancing Search Retrieval with Large\nLanguage Models (LLMs).” We’ll explore this extensively in the\n",
      "content_length": 2093,
      "extraction_method": "Direct"
    },
    {
      "page_number": 30,
      "chapter": null,
      "content": "context of Q&A chatbot development in Chapters 6 and 7, with deeper\ndives into advanced techniques in Chapters 8 to 10.\nAutonomous Reasoning and Workflow Execution: LLMs can handle\ntasks like planning a complete holiday package by understanding\nrequests and managing each step of the process. We’ll discuss building\nLLM-powered agents using LangGraph in Chapter 12.\nStructured Data Extraction: This involves pulling structured data—like\nentities and their relationships—from unstructured text, such as financial\nreports or news articles.\nCode Understanding and Generation: LLMs can analyze code to identify\nissues, suggest improvements, or generate new code components—\nranging from simple functions and classes to entire applications—based\non user instructions. This capability powers popular IDE extensions like\nGitHub Copilot and Cline AI, and emerging AI-driven coding assistants\nsuch as Cursor and Windsurf, which provide real-time code suggestions\nand error detection as you work. In addition, CLI-based coding tools like\nAnthropic’s Claude Code and OpenAI Codex allow developers to\ninteract with AI through the command line, enabling code generation,\nrefactoring, and debugging directly from terminal environments.\nPersonalized Education and Tutoring: LLMs are increasingly used as\ninteractive tutors, providing personalized help and feedback. For\ninstance, Khan Academy’s “Khanmigo” uses an LLM to assist students\nwith interactive learning.\nThese use cases assume the LLM can competently handle user requests.\nHowever, real-world tasks often involve specific domains or scenarios that\nextend beyond the LLM’s initial training. How can you ensure your LLM\nmeets user needs effectively in these cases? That’s exactly what you’ll learn\nin the next section.\n1.5 How to adapt an LLM to your needs\nThere are several techniques to enhance the LLM's ability to respond to user\nrequests, even without prior knowledge or training in a specific domain.\nThese include:\nPrompt engineering\n",
      "content_length": 1984,
      "extraction_method": "Direct"
    },
    {
      "page_number": 31,
      "chapter": null,
      "content": "Retrieval Augmented Generation\nFine-tuning\nLet's begin with the most basic approach: prompt-engineering.\n1.5.1 Prompt engineering\nPrompts for large language models (LLMs) can be as simple as a single\ncommand or as complex as a block of instructions enriched with examples\nand context. Prompt engineering is the practice of designing these inputs so\nthat the model understands the task and produces useful, accurate responses.\nDone well, it allows developers to guide the model’s behavior, adapt it to\ndomain-specific needs, and even handle problems the model wasn’t explicitly\ntrained on. A common technique here is in-context learning, where the model\ninfers patterns from examples embedded directly in the prompt—no fine-\ntuning required.\nIn practice, prompts are often organized as templates: a fixed instruction with\nvariable fields that accept dynamic input. This makes prompts reusable and\neasier to manage across different parts of an application. A widely used\npattern is few-shot prompting, where you include a handful of examples to\nteach the model how to generalize to similar inputs.\nWell-crafted prompts are surprisingly powerful. They can coax high-quality,\ndomain-aware output from an LLM without needing extra training data. For\ninstance, chatbots often embed recent conversation turns into the prompt so\nthe model maintains context and produces coherent, multi-turn replies.\nPrompt engineering can often take you very far, making it a lightweight yet\neffective tool for many real-world applications.\nLangChain builds on this idea with abstractions for managing and reusing\nprompts consistently, which we’ll explore in the next chapter. That said,\nprompt engineering alone has limits—especially when applications need to\nground answers in user-specific or enterprise data. In those cases, the natural\nnext step is Retrieval-Augmented Generation (RAG), which augments\nprompts with knowledge pulled dynamically from external sources.\n",
      "content_length": 1948,
      "extraction_method": "Direct"
    },
    {
      "page_number": 32,
      "chapter": null,
      "content": "1.5.2 Retrieval Augmented Generation (RAG)\nOne of the most effective ways to improve LLM responses is to ground them\nin your own data. Instead of relying only on what the model learned during\npretraining, you can retrieve relevant context from a local knowledge base—\nusually stored in a vector database—and add it to the prompt. This approach,\ncalled Retrieval-Augmented Generation (RAG), has become a core pattern in\nmodern LLM applications.\nThe workflow begins with building a knowledge base. Documents are\ningested, split into smaller chunks, and converted into vector representations\nusing an embedding model. These embeddings capture semantic meaning,\nmaking it possible to compare text by similarity rather than exact wording.\nLangChain provides tools to load documents in many formats, split them\nefficiently, and generate embeddings, then store everything in a vector\ndatabase.\nFigure 1.9 A collection of documents is split into text chunks and transformed into vector-based\nembeddings. Both text chunks and related embeddings are then stored in a vector store.\n",
      "content_length": 1071,
      "extraction_method": "Direct"
    },
    {
      "page_number": 33,
      "chapter": null,
      "content": "1. feed a text document\nto the ingestion script\n\n2. request LLM provider to calculate the\nembeddings of the document chunks\n\n7 ; 3, return embeddings\nText ingestion script d Embeddings model\n\nLLM provider\n\n4. store documents chunks and related\nembeddings into Vector store\n\nVector store",
      "content_length": 286,
      "extraction_method": "OCR"
    },
    {
      "page_number": 34,
      "chapter": null,
      "content": "RAG offers multiple benefits:\n1. Efficiency: Instead of passing an entire document to the model, you\nretrieve only the key chunks—keeping inputs concise, reducing token\ncosts, and working within context limits.\n2. Accuracy: Responses are grounded on real data, reducing the risk of\nhallucinations. In Chapter 6, you’ll learn techniques for having the LLM\ncite its sources, further improving transparency and trust.\n3. Flexibility: By swapping embedding models, retrievers, or vector stores,\nyou can adapt the same pattern to different domains and requirements.\nDefinition\n\"Grounding\" an LLM involves crafting prompts that include context pulled\nfrom a trusted knowledge source—often stored in a vector store. This ensures\nthat the LLM generates its response based on verified facts rather than relying\nsolely on its pre-trained knowledge, which may include outdated or\nunreliable data.\nDefinition\nA “hallucination” is when a large language model generates an incorrect,\nmisleading, or fabricated response. This happens when the model draws from\npoor-quality data during training or lacks sufficient information to answer\naccurately. Due to their auto-regressive nature, LLMs will try to generate a\nresponse even when relevant content is missing—leading to hallucinations as\nthey “fill in the gaps” with plausible-sounding but incorrect information.\nTo make RAG reliable, prompts should explicitly instruct the LLM to rely\nonly on the retrieved context. LangChain also supports guardrails and\nvalidators to help enforce safe behavior. In high-stakes cases, human-in-the-\nloop review remains the best safeguard.\nIn short, RAG bridges the gap between static pretrained models and dynamic,\ndomain-specific applications. By teaching your app to “speak the same\nlanguage” as the LLM—vectors—you unlock a practical way to deliver\ngrounded, verifiable, and cost-efficient answers. If prompt engineering and\n",
      "content_length": 1899,
      "extraction_method": "Direct"
    },
    {
      "page_number": 35,
      "chapter": null,
      "content": "RAG still don’t meet your needs, the next step is fine-tuning the model,\nwhich we’ll revisit later in the book.\n1.5.3 Fine-tuning\nFine-tuning is the process of adapting a pre-trained LLM to perform better in\na specific task or domain. This is done by training the model on a curated\ndataset of examples that capture the style, terminology, and reasoning\npatterns you want it to master. Traditionally, fine-tuning required specialized\nML frameworks and access to powerful hardware, but today many platforms\n—including OpenAI’s fine-tuning API and several open-source toolkits—\nmake it possible with just a dataset upload and minimal configuration.\nThe main benefit of fine-tuning is efficiency: once a model has absorbed\ndomain-specific knowledge, you don’t need to stuff every prompt with long\ninstructions or examples. Instead, the model “knows” how to respond in your\ncontext. The trade-offs, however, are real. Preparing high-quality datasets\ntakes time and expertise, and training runs can be costly since they often\nrequire GPUs.\nRecent advances like Low-Rank Adaptation (LoRA) and other parameter-\nefficient fine-tuning methods have lowered both cost and complexity, making\nfine-tuning more accessible. Related techniques, such as instruction tuning\nand reinforcement learning from human feedback (RLHF), push models to\nfollow directions more reliably, though they typically demand more\nengineering effort and infrastructure.\nWhether fine-tuning is truly necessary is an ongoing debate. General-purpose\nLLMs are surprisingly strong out-of-the-box, especially when paired with\nRetrieval-Augmented Generation (RAG). In fact, research by Heydar\nSoudani, Evangelos Kanoulas, and Faegheh Hasibi (“Fine-Tuning vs.\nRetrieval-Augmented Generation for Less Popular Knowledge”) shows that\nRAG often outperforms fine-tuning by letting you provide context\ndynamically at runtime, reducing both costs and retraining needs.\nThat said, in highly specialized domains—such as medicine, law, or finance\n—fine-tuning remains invaluable. It allows models to capture domain-\n",
      "content_length": 2060,
      "extraction_method": "Direct"
    },
    {
      "page_number": 36,
      "chapter": null,
      "content": "specific jargon and workflows in ways generic models struggle to match.\nWell-known examples include:\nBioMistral (biology and life sciences)\nLexisNexis’s legal-domain LLM (“LexiGPT”)\nBloombergGPT (finance)\nAnthropic’s Claude Code and similar code-focused models\nIn summary, fine-tuning customizes an LLM for domain expertise and\nspecialized accuracy, but it comes at a cost in time, money, and complexity.\nAs a developer, you’ll need to weigh when it’s truly worth it versus when\nRAG or prompt engineering will get you there faster.\n1.6 Which LLMs to choose\nWhen developing LLM-based applications, you’ll find a wide range of\nmodels to choose from. Some are proprietary, accessible via subscription or\npay-as-you-go APIs, while others are open source and can be self-hosted.\nMost modern LLMs offer REST APIs for easy integration and user-friendly\nchat interfaces. Many also come in different size variants, letting you balance\nperformance, speed, and cost.\nLangChain makes it simple to integrate with a variety of LLMs. Thanks to its\nstandardized interface, you can switch models with minimal code changes—\nan essential feature in today’s fast-evolving LLM landscape.Below are key\nconsiderations for choosing the right LLM for a specific task.\nModel Purpose:\nFor general tasks like summarization, translation, classification, or\nsentiment analysis, most large model families (GPT, Gemini,\nClaude, Llama, Mistral) handle these as standard capabilities.\nFor specialized tasks—such as code generation—seek out models\nthat are optimized or fine-tuned for that purpose. For instance,\nClaude Sonnet and Meta’s Code Llama variants are well-regarded\nfor coding tasks.\nContext Window size:\nLarger A larger context window lets your application handle longer\n",
      "content_length": 1747,
      "extraction_method": "Direct"
    },
    {
      "page_number": 37,
      "chapter": null,
      "content": "prompts and documents. For example, some models support up to 2\nmillion tokens, while many others offer 128K–256K tokens.\nKeep in mind: larger windows can increase both performance\noverhead and cost, especially for commercial APIs that charge per\ntoken.\nMultilingual Support:\nIf your app needs to handle multiple languages, opt for models with\nstrong multilingual training. Qwen and Llama are especially\ncapable in both Western and Asian languages. Some models may\nspecialize—for example, certain Gemma releases are optimized for\nJapanese.\nModel size:\nLLMs range from small (1B parameters) to very large (trillions of\nparameters). Smaller models can be more cost-effective and faster,\noften sufficient for simpler tasks.\nFor example, OpenAI mini and nano versions offer impressive\naccuracy at a lower cost and latency than its full-size counterpart.\nSpeed:\nSmaller and mid-size models typically respond faster. If\nresponsiveness is critical (e.g., in chatbots), benchmark both\nperformance and accuracy.\nInstruction vs Reasoning:\nA key distinction in LLM behavior is between instruction models\nand reasoning models.\nInstruction models, such as OpenAI’s GPT-4 series or Google\nGemini Pro, are best suited when you already know exactly how a\ntask should be performed and want the LLM to follow your\ndetailed directions. They excel at accurately executing well-\ndescribed instructions. You can think of them as reliable junior\nassistants: fast, precise, and effective when the steps are clear.\nReasoning models, such as OpenAI’s o-series or Google Gemini\nThinking, are designed for situations where the task is defined but\nthe method is not. These models not only execute work but also\ndecide how to approach it. They are closer to senior problem-\nsolvers—capable of planning, adapting, and filling in missing\ndetails.\nThe trade-offs are cost and speed: reasoning models are generally\n",
      "content_length": 1881,
      "extraction_method": "Direct"
    },
    {
      "page_number": 38,
      "chapter": null,
      "content": "slower and more expensive, while instruction models are faster and\nmore economical. Choosing between them depends on whether you\nwant the model to simply follow a plan you provide or to figure out\nthe plan itself.\nOpen-Source vs. Proprietary:\nOpen-source models (Llama, Mistral, Qwen, Falcon) provide\nstronger data privacy and allow on-premise or private cloud\ndeployment.\nProprietary models are easier to set up via API and may deliver\nbest-in-class performance out of the box—but long-term costs can\nbe high. Many teams start with commercial APIs, then migrate to\nopen-source hosting for cost or compliance reasons.\nBy weighing these factors, you can select an LLM that aligns with your\nproject’s goals, technical requirements, and budget. Keep in mind that your\napplication may involve multiple tasks, each with its own demands for\naccuracy, speed, and cost-efficiency. In many cases, it’s wise to configure\nyour system to use different models for different tasks. For example, in one\nrecent project, I used GPT-4o Mini for summarization and sentiment\nanalysis, o3 to interpret and route user queries to the appropriate agents, and\nGPT-4.1 for answer synthesis—balancing performance and cost across the\nworkflow.\n1.7 What You'll Learn from this Book\nIf you’ve read this far, you already recognize the value of building\napplications powered by large language models. LLM interaction is rapidly\nbecoming a core feature of modern software—similar to how websites, in the\nearly 2000s, opened up a new communication channel for server-based\napplications. And this shift is only accelerating.\nWe’ll begin with prompt engineering, the foundational skill for effective\ninteraction with LLMs. You’ll start by experimenting interactively with\nChatGPT, then move to programmatic access via REST APIs. These\nexercises will establish the backbone for many projects in this book.\nFrom there, we’ll use LangChain to build two categories of applications:\n",
      "content_length": 1942,
      "extraction_method": "Direct"
    },
    {
      "page_number": 39,
      "chapter": null,
      "content": "custom engines (such as summarization and Q&A systems) and chatbots that\ncombine conversational fluency with knowledge retrieval. Each project will\nbe small and self-contained, but all will share a common background theme\nin the travel industry, so you can see how the ideas fit together in a coherent\ndomain.\nOnce those foundations are in place, we’ll move to LangGraph to build AI\nagents—applications that can orchestrate multi-step workflows, coordinate\ntools, and make adaptive decisions. This will be your introduction to the most\nadvanced class of LLM-powered systems, where engines and tools come\ntogether into dynamic, autonomous applications. To make this approachable,\nI’ll start with a simple Python script and then extend it step by step. Each\nextension will branch into a different capability—such as tool use, planning,\nor memory—so you can see how agents grow in sophistication without being\noverwhelmed all at once.\nWe’ll also take a deep dive into Retrieval-Augmented Generation (RAG), the\narchitectural pattern behind many real-world systems. RAG concepts will be\nintroduced through short, focused scripts—some independent, others\nprogressively refined into more advanced workflows—so you can master\nboth fundamentals and cutting-edge techniques.\nAlthough the examples reference OpenAI models for accessibility and quick\nwins, you’ll also learn how to use open-source models through the inference\nengines listed in Appendix E. This way, you’ll gain the skills to build and\ndeploy applications that are cost-effective, privacy-conscious, and fully under\nyour control.\nBeyond building, you’ll explore the entire lifecycle of LLM applications:\ndebugging, monitoring, and refining with LangSmith; orchestrating complex\nworkflows with LangGraph; and applying best practices for production\ndeployment to ensure scalability and maintainability.\nBy the end of this book, you’ll have built a portfolio of working applications,\nlearned the core architectural patterns, and developed the skills to design and\nimplement LLM-powered systems with confidence. You won’t just\nunderstand how these applications work—you’ll be ready to keep innovating\nand pushing the boundaries of what’s possible with LangChain, LangGraph,\n",
      "content_length": 2225,
      "extraction_method": "Direct"
    },
    {
      "page_number": 40,
      "chapter": null,
      "content": "and large language models.\n1.8 Recap on LLM terminology\nIf you're new to LangChain, LLMs, and LLM applications, you'll encounter\nmany terms in this chapter. The following table will help clarify these\nconcepts.\nTable 1.1 LLM Application Glossary\nTerm\nDefinition\nLLM\nA Large Language Model trained on vast text datasets to\nperform tasks like understanding, generating, and\nsummarizing text.\nEmbedding\nNumerical vector representations of words or tokens in a\nhigh-dimensional space, capturing their semantic and\nsyntactic meaning.\nEmbedding\nModel\nModels that convert text into vectors, capturing the\nmeaning for efficient processing and retrieval in\nLangChain.\nVector Store\nA specialized database for storing and retrieving vector-\nbased representations of documents, supporting efficient\nsearch and retrieval.\nPrompt\nA request to guide the LLM, often created from\ntemplates and user inputs, for processing specific tasks.\nPrompt\nEngineering\nThe craft of designing effective prompts to guide the\nLLM in generating accurate responses, often through\ntrial and refinement.\nCompletion\nThe response generated by an LLM when given a\nprompt, based on the model's predictive text abilities.\nLLM-Based\nEngine\nA backend service using LLMs for tasks like\nsummarization or analysis, powering other applications\nwith language processing.\nLLM-Based\nChatbot\nA chatbot using an LLM, optimized with prompts and\ninstructions to guide interactions and maintain safety\nand relevance.\n",
      "content_length": 1462,
      "extraction_method": "Direct"
    },
    {
      "page_number": 41,
      "chapter": null,
      "content": "RAG\nRetrieval Augmented Generation combines LLM\noutputs with context retrieved from a local knowledge\nbase for better accuracy.\nGrounding\nSupplying the LLM with verified context from a trusted\nsource to ensure reliable responses.\nHallucination\nThe model's tendency to generate incorrect or fabricated\ninformation when lacking sufficient context or relying\non low-quality data.\nLLM-Based\nAgent\nAn autonomous system that orchestrates complex\nworkflows by coordinating across multiple tools,\nincluding remote tools hosted by MCP servers, and data\nsources, executing multi-step tasks through interactions\nwith a language model.\nFine-Tuning\nAdapting a pre-trained LLM for specific tasks by\ntraining it on examples, enhancing its domain-specific\nknowledge.\nEach term has been introduced in the chapter to support your understanding\nof LangChain and its applications.\n1.9 Summary\nLLMs have rapidly evolved into core building blocks for modern\napplications, enabling tasks like summarization, semantic search, and\nconversational assistants.\nWithout frameworks, teams often reinvent the wheel—managing\ningestion, embeddings, retrieval, and orchestration with brittle, one-off\ncode. LangChain addresses this by standardizing these patterns into\nmodular, reusable components.\nLangChain’s modular architecture builds on loaders, splitters,\nembedding models, retrievers, and vector stores, making it\nstraightforward to build engines such as summarization and Q&A\nsystems.\nConversational use cases demand more than static pipelines. LLM-based\nchatbots extend engines with dialogue management and memory,\nallowing adaptive, multi-turn interactions.\nBeyond chatbots, AI agents represent the most advanced type of LLM\n",
      "content_length": 1701,
      "extraction_method": "Direct"
    },
    {
      "page_number": 42,
      "chapter": null,
      "content": "application. Agents orchestrate multi-step workflows and tools under\nLLM guidance, with frameworks like LangGraph designed to make this\npractical and maintainable.\nRetrieval-Augmented Generation (RAG) is a foundational pattern that\ngrounds LLM outputs in external knowledge, improving accuracy while\nreducing hallucinations and token costs.\nPrompt engineering remains a critical skill for shaping LLM behavior,\nbut when prompts alone aren’t enough, RAG or even fine-tuning can\nextend capabilities further.\n",
      "content_length": 506,
      "extraction_method": "Direct"
    },
    {
      "page_number": 43,
      "chapter": null,
      "content": "2 Executing prompts\nprogrammatically\nThis chapter covers\nUnderstanding prompts and prompt engineering\nDifferent kinds of prompts and how they're structured\nEnhancing prompt responses using one, two, or a few-shot learning\nExamples of using prompts with ChatGPT and the OpenAI API\nApplications built with LangChain interact with LLMs primarily through\nprompts—structured inputs that guide the model’s behavior. Think of it as\ngiving instructions to a highly capable but inexperienced colleague: the\nclearer and more specific your instructions, the better the outcome. To\ngenerate accurate and relevant responses, your prompts must be well-crafted\nand tailored to the task at hand. In LangChain, prompt design plays a central\nrole in determining how your application performs.\nPrompt engineering—the practice of designing and refining prompts to steer\nthe LLM’s output—is a core skill in LLM application development. You’ll\nspend a significant amount of time creating, testing, and iterating on prompts\nto ensure your system consistently delivers high-quality results. LangChain\nrecognizes this and places prompt engineering at the heart of its design,\noffering a suite of tools to support the process. Whether you're writing simple\ninstructions or building sophisticated templates that require advanced\nreasoning and dynamic input handling, LangChain provides the infrastructure\nto manage and scale your prompt-driven workflows effectively.\nIn this chapter, you'll begin with the basics of prompt design and gradually\nmove to more sophisticated techniques, such as Chain of Thought.\nLangChain's suite of prompt engineering tools, including PromptTemplate\nand FewShotPromptTemplate, will be your key resources as you learn to\nharness the full power of LLMs in your applications.\n",
      "content_length": 1777,
      "extraction_method": "Direct"
    },
    {
      "page_number": 44,
      "chapter": null,
      "content": "2.1 Running prompts programmatically\nLangChain applications rely on well-crafted prompts to generate\ncompletions, which are then passed to the next component in the chain.\nUnlike prompts entered manually in interfaces like ChatGPT, LangChain\nprompts are typically constructed and sent to the LLM programmatically as\npart of a larger workflow. In the following sections, we’ll start by setting up\nand executing prompts using the OpenAI API directly, and then explore how\nto run and manage prompts within LangChain.\nBefore diving in, make sure you’ve covered these basics:\n1. You have an OpenAI API key.\n2. You know how to create a Python Jupyter notebook environment.\nIf you're not familiar with these tasks, check the sidebar for guidance on\n\"Creating an OpenAI key\" in Section 1.7 and follow the instructions in\nAppendix B titled \"Setting up a Jupyter notebook environment.\".\n2.1.1 Setting up an OpenAI Jupyter Notebook environment\nAssuming you have cleared the prerequisites, open the operating system\nterminal shell (e.g., cmd on Windows), create a project folder, and navigate\ninto it:\nC:\\Github\\building-llm-applications\\ch02>\nCreate and activate a virtual environment with venv:\nC:\\Github\\building-llm-applications\\ch02>python -m venv env_ch02\nC:\\Github\\building-llm-applications\\ch02>.\\env_ch02\\Scripts\\activ\nYou should now observe the updated operating system prompt, displaying the\nenvironment name in front as (env_ch02):\n(env_ch02) C:\\Github\\building-llm-applications\\ch02>\nHaving activated the virtual environment, you're now prepared to implement\na Jupyter notebook for executing prompts with OpenAI models. If you have\n",
      "content_length": 1633,
      "extraction_method": "Direct"
    },
    {
      "page_number": 45,
      "chapter": null,
      "content": "cloned the repository from Github, you can install Jupyter and the OpenAI\npackages as follows:\n(env_ch02) C:\\Github\\building-llm-applications\\ch02>pip install -\nThen you can start the notebook:\n(env_ch02) C:\\Github\\building-llm-applications\\ch02>jupyter noteb\nOtherwise, if you are creating everything from scratch locally, install the\npackages in this way:\n(env_ch02) C:\\Github\\building-llm-applications\\ch02>pip install n\nAfter about a minute, the installation of the notebook and OpenAI packages\nshould be finished. You can start the Jupyter notebook by executing the\nfollowing command:\n(env_ch02) C:\\Github\\building-llm-applications\\ch02>jupyter noteb\nAfter a few seconds, you should see some output in the terminal.\nSubsequently a browser window will open up at this URL:\nhttp://localhost:8888/tree.\nOnce you have created the notebook, go to the Jupyter menu, select: File >\nRename … and name the notebook file: 02-prompt_examples.ipynb.\nFigure 2.1 Creating a new Jupyter notebook: create the notebook with the menu File > New >\nNotebook and then rename the file to prompt_examples.ipynb\n",
      "content_length": 1093,
      "extraction_method": "Direct"
    },
    {
      "page_number": 46,
      "chapter": null,
      "content": "Now, you're prepared to input code into the notebook cells (or simply execute\nthem if you got the notebook from Github).\nTIP\nIf you are unfamiliar with Jupyter notebook, remember to press Shift + Enter\nto execute the code in each cell.\nThroughout the remaining chapter, I assume you've set up a virtual\nenvironment, installed the OpenAI library, and launched a Jupyter notebook\ninstance, as outlined in the previous section.\n",
      "content_length": 425,
      "extraction_method": "Direct"
    },
    {
      "page_number": 47,
      "chapter": null,
      "content": "2.1.2 Minimal prompt execution\nIn the first cell of your notebook, import the required libraries and grab the\nOpenAI API key in a secure way (never hardcode it as it might get misused):\nfrom openai import OpenAI\nimport getpass\nOPENAI_API_KEY = getpass.getpass('Enter your OPENAI_API_KEY')\nAfter entering your OpenAI API key (you only need to hit Enter, without\nShift), set up the OpenAI client as follows:\nclient = OpenAI(api_key=OPENAI_API_KEY)\nIn the next notebook cell, enter and execute the following code. It's based on\none of the prompts discussed in the \"prompt basics\" section, and you can find\ndetails about the chat.completions.create function in the accompanying\nsidebar.\nprompt_input = \"\"\"Write a short message to remind users to be vig\nresponse = client.chat.completions.create(\n model=\"gpt-4o-mini\",\n messages=[\n   {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n   {\"role\": \"user\", \"content\": prompt_input}\n ],\n temperature= 0.7,\n max_tokens= 400  \n)\nprint(response)\nThe output will look like this (I have shortened it for convenience):\nChatCompletion(id='chatcmpl-8Wr77Lor0Xusp7V7iM2NURV8dbwVj', choic\nFor a clearer output, explore the attributes and properties of the response\nobject:\nprint(response.choices[0].message.content)\n",
      "content_length": 1259,
      "extraction_method": "Direct"
    },
    {
      "page_number": 48,
      "chapter": null,
      "content": "You will see an output similar to the following, though it may vary slightly\ndue to the non-deterministic nature of LLMs—especially with the\ntemperature parameter set above zero, which increases randomness:\nAttention users,\nPlease be vigilant and stay alert regarding phishing attacks. Cyb\nStay safe and secure online!\nBest regards,\n[Your Name/Company Name]\nNote\nYou may not be familiar with the three double quotes (\"\"\") I have used to\nformulate the prompt and may wonder about their purpose. This allows you\nto enter a block of text spanning multiple lines in a very readable way,\nwithout having to introduce newline characters (\\n), which is ideal for\ncapturing prompts.\nchat.completions.create\nThe chat.completions.create function is essential for interacting with the\nOpenAI LLM, so it's crucial to understand its signature in detail:\nclient.chat.completions.create(\n model=\"gpt-4o-mini\",\n messages=[\n   {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n   {\"role\": \"user\", \"content\": prompt_input}\n ],\n temperature= 0.7,\n max_tokens= 400  \n)\nHere is a description of the parameters:\nmodel: This refers to the OpenAI model to be used. Some model\nfamilies, such as GPT-4o and GPT-4o-mini, have generic capabilities,\nunderstanding and generating text. Other models, like the \"moderation\"\n",
      "content_length": 1303,
      "extraction_method": "Direct"
    },
    {
      "page_number": 49,
      "chapter": null,
      "content": "family, are more specialized. Within each family, there are variants that\ncan accept bigger or smaller prompts or are newer, and therefore\nrecommended over more obsolete variants. Learn more on this page on\nOpenAI Models: https://platform.openai.com/docs/models.\nmessages: A prompt can be composed of a list of messages (when\nfollowing the OpenAI convention), each impersonated by a “role” and\nhaving some “content” with the instructions to be followed.\ntemperature: This is a \"creativity\" or \"entropy\" coefficient, ranging from\n0 (no creativity, meaning the output will always be the same given the\nsame prompt) to 2 (the output will vary considerably at each execution\nof the same prompt).\nmax_tokens: This limits the number of output tokens, generally to\nreduce costs, as you are charged for the number of tokens read and\ngenerated.\nNow that you've grasped the fundamentals of programmatically submitting\nprompts, you can proceed to learn and work with more intricate prompts.\n2.2 Running prompts with LangChain\nBefore moving to more advanced prompts, let me show you how to replicate\nthe same example in LangChain. This will help you familiarize yourself with\nits object model. In the following sections, I'll demonstrate how LangChain\nsimplifies the implementation of more complex prompts that would be more\nchallenging to implement from scratch.\nBefore adding LangChain code to your notebook, open a new OS shell and\nimport the LangChain packages as you did in chapter 1 to avoid stopping the\nrunning OS shell (you do not need to do this if you installed the dependencies\nfrom requirements.txt, as explained earlier):\n(env_ch02) c:\\Github\\building-llm-applications\\ch02>pip install l\nOnce the installation is complete, add a new cell in the notebook and import\nthe LangChain OpenAI wrapper library. Then, instantiate a connection to the\nLLM:\nfrom langchain_openai import ChatOpenAI\n",
      "content_length": 1888,
      "extraction_method": "Direct"
    },
    {
      "page_number": 50,
      "chapter": null,
      "content": "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,\n                model_name=\"gpt-4o-mini\")\nYou can now instantiate and execute the prompt you saw earlier as follows:\nprompt_input = \"\"\"Write a coincise message to remind users to be \nresponse = llm.invoke(prompt_input)\nprint(response.content)\nThis will return the following output similar to:\n\"Stay alert and protect your information - beware of phishing attacks.\"\n2.3 Prompt templates\nWhen building LLM applications, creating flexible prompt templates that\nincorporate user input through parameters is crucial. LangChain makes this\neasier with its PromptTemplate class, which lets you manage and reuse\nparametrized prompts without the need for custom functions. This not only\nstreamlines the creation of dynamic prompts but also enhances the efficiency\nand adaptability of your LLM interactions, as I’ll demonstrate shortly.\n2.3.1 Implementing a prompt template with a Python function\nTo illustrate the concept of a template, let's create a text summarization\ntemplate that requests the text, desired summary length, and preferred tone.\nYou could implement this using a simple Python function:\ndef generate_text_summary_prompt(text, num_words, tone):\n   return f\"You are an experienced copywriter. Write a {num_word\nLet’s use the prompt template to generate a prompt and then execute it\nthrough LangChain’s ChatOpenAI wrapper as usual:\nsegovia_aqueduct_text = \"The Aqueduct of Segovia (Spanish: Acuedu\n",
      "content_length": 1451,
      "extraction_method": "Direct"
    },
    {
      "page_number": 51,
      "chapter": null,
      "content": "input_prompt = generate_text_summary_prompt(text=segovia_aqueduct_\nresponse = llm.invoke(input_prompt)\nprint(response.content) \nYou should be output similar to:\nThe Aqueduct of Segovia, built in the 1st century AD, is a well-p\n2.3.2 Using LangChain’s PromptTemplate\nWith LangChain, you don't need to implement a prompt template function\nmanually. Instead, you can use the convenient PromptTemplate class to\nhandle parametrized templates, as I showed in chapter 1. Here's how you can\nuse it:\nfrom langchain_core.prompts import PromptTemplate\nprompt_template = PromptTemplate.from_template(\"You are an experi\nTo use the prompt template, create a prompt instance and format it with your\nparameters:\nprompt = prompt_template.format(text=segovia_aqueduct_text, num_w\nThen, invoke the ChatOpenAI client with the formatted prompt:\nresponse = llm.invoke(prompt)\nprint(response.content)\nThis will generate output similar to what’s shown here (as you now\nunderstand, the exact output may differ from what’s printed in the book, so I\nwon’t repeat this point on the next page):\nThe Aqueduct of Segovia, a Roman marvel built in the 1st century A\nLet’s take a short break from coding with LangChain. I want to delve deeper\ninto prompt engineering. In the upcoming pages, you'll see that a good\nprompt can have various elements, depending on your goal, task complexity,\nand desired accuracy. By learning from different prompt variations, you'll be\n",
      "content_length": 1433,
      "extraction_method": "Direct"
    },
    {
      "page_number": 52,
      "chapter": null,
      "content": "able to handle both simple and complex prompts, adjusting their complexity\nto fit specific cases.\n2.4 Types of prompt\nCreating effective prompts is crucial for getting the best results in LangChain\napplications. Whether your app is focused on text classification, sentiment\nanalysis, summarization, text generation, or question answering, each task\nrequires a carefully designed prompt. In the following sections, we'll dive\ninto how to craft prompts tailored to these specific tasks, ensuring your\napplication functions as intended.\nAlthough these prompting techniques are key to developing LangChain\napplications, we'll use ChatGPT for now to keep things simple and explain\nprompt engineering concepts more clearly. The patterns you learn here will\ndirectly apply to the LLM apps we build with LangChain in the upcoming\nchapters.\n2.4.1 Text classification\nIn classification, the aim is to place an input text into one of several\npredefined categories, as demonstrated in the following prompt:\nInstruction: Classify the following text into one of these categories: history,\ntech, gardening.\nText: Headphones provide immersive audio experiences for music lovers and\ngamers alike.\nOutput:\nThe text \"Headphones provide immersive audio experiences for music lovers\nand gamers alike.\" should be classified as \"tech\" because it discusses\ntechnology-related products and their functionality.\n",
      "content_length": 1386,
      "extraction_method": "Direct"
    },
    {
      "page_number": 53,
      "chapter": null,
      "content": "The output is overly detailed. Let's revise the prompt and specify to only\ndisplay the category.\nInstruction: Classify the following text into one of these categories: history,\ntech, gardening.\nText: Headphones provide immersive audio experiences for music lovers and\ngamers alike.\nOutput: only the category\nOutput:\nTech\nNow, categorize another sentence by applying the same prompt and only\naltering the text:\nInstruction: Classify the following text into one of these categories: history,\ntech, gardening.\nText: Julius Caesar was a Roman general and statesman who played a pivotal\nrole in the demise of the Roman Republic and the rise of the Roman Empire.\nOutput: only the category\nOutput:\nHistory\nIn summary, in a standard text classification prompt, you find three\ncomponents: an Instruction, the input Text, and an Output specification.\nNow, let's delve into a slightly specialized text classification: sentiment\nanalysis.\n",
      "content_length": 927,
      "extraction_method": "Direct"
    },
    {
      "page_number": 54,
      "chapter": null,
      "content": "2.4.2 Sentiment analysis\nSentiment analysis is a specific type of text classification that aims to\ndetermine whether a given text is perceived as positive, neutral, or negative.\nBelow are three sample prompts you can experiment with independently.\nFirst prompt example:\nInstruction: Classify the following text as positive, neutral or negative\nText: Apple have announced stellar quarterly results\nExpected output:\nPositive\nSecond prompt example:\nInstruction: Classify the following text as positive, neutral or negative\nText: this is the worst movie I have watched this month\nExpected output:\nNegative\nThird prompt example:\nInstruction: Classify the following text as positive, neutral or negative\nText: The customer bought a green dress\nExpected output:\n",
      "content_length": 755,
      "extraction_method": "Direct"
    },
    {
      "page_number": 55,
      "chapter": null,
      "content": "Neutral\nIf you have a list of texts and wish to analyze their sentiments collectively,\nyou can input them all into a single prompt:\nInstruction: classify the sentiment of following stock reports as positive,\nneutral or negative\nStock 1: Apple: the launch of the new iPhone has been a success\nStock 2: Nvidia: sales are being propelled by consumer demand on LLMs\nStock 3: GX oil: demand of carbon based energy is dropping thanks to the\nincrease of renewable energy\nOutput: output a table with the columns \"stock name\", \"sentiment\"\nExpected output:\n| Stock Name | Sentiment |\n|-------------|-----------|\n| Apple | positive |\n| Nvidia | positive |\n| GX oil | negative |\nAs you observe, in this scenario, the output is formatted in a table and can be\nreadily processed programmatically.\nNow, let's create a prompt for one of the most common LLM use cases: text\nsummarization.\n2.4.3 Text summarization\nCreating a text summarization prompt is simple: you only need to ask for a\n",
      "content_length": 972,
      "extraction_method": "Direct"
    },
    {
      "page_number": 56,
      "chapter": null,
      "content": "summary of an input text with your preferred length.\nInstruction: write a 30 word summary for the following text\nText: Home PCs from the 1980s were iconic pioneers of personal computing.\nComputers like the Apple II, IBM PC, and Commodore 64 revolutionized\nhow we lived and worked. They introduced us to word processing, gaming,\nand the possibilities of home computing. The graphical user interface, as seen\nin the Apple Macintosh, changed the way we interacted with machines. These\nPCs, though less powerful by today's standards, ignited a digital revolution,\nshaping the technology landscape we know today. They hold a special place\nin history, symbolizing the dawn of a new era in computing that laid the\nfoundation for the modern digital age.\nOutput:\n1980s home PCs like Apple II, IBM PC, Commodore 64 pioneered personal\ncomputing, introducing word processing, gaming, and GUIs. They shaped\ntoday's tech, symbolizing a digital era's dawn.\nYou can also ask the LLM to provide a summary for a text it's already\nfamiliar with:\nInstruction: Write a one sentence summary of the Lord of the Rings\nOutput:\n\"The Lord of the Rings\" follows the epic journey of a diverse group of\ncharacters as they seek to destroy a powerful ring and save Middle-earth from\nthe dark lord Sauron.\n2.4.4 Composing text\nLLMs are part of the AI category called \"generative AI.\" While LLMs can\n",
      "content_length": 1366,
      "extraction_method": "Direct"
    },
    {
      "page_number": 57,
      "chapter": null,
      "content": "handle audio and video, with text they generate new content, like a paragraph\nor sentence, based on a list of facts, for example:\nWrite a piece on the diver watches, mentioning the following facts:\nthe Rolex Oyster Perpetual was among the first to be water resistant\nOmega produced the Seamaster in 1948 as the first purpose designed\ndiver watch was the Seamaster\nthe Seamaster set a world record of immersion of 62.5m in 1955.\nRolex produced the first Submariner in 1953\nthe Submariner became popular also because of the association with\nJames Bond until 1995\nJames Bond started to wear the Seamaster in 1995\nOutput:\nDiver Watches: A Journey into Time's Abyss\nDiver watches have a storied history, encapsulating the spirit of adventure\nand resilience against the unforgiving depths of the oceans. These timepieces\nnot only serve as reliable companions for professional divers but have also\nbecome iconic symbols of style and innovation. Here, we delve into the\nfascinating world of diver watches, highlighting key milestones and\nlegendary timepieces that have defined this genre.\nThe Rolex Oyster Perpetual, introduced in the early 20th century, played a\npioneering role in the evolution of diver watches. Among its notable features\nwas its remarkable water-resistant design, which set the stage for the\ndevelopment of truly waterproof timepieces. This innovation marked a\ncrucial step in ensuring that watches could withstand the rigors of underwater\nexploration.\nOmega, a respected Swiss watchmaker, made its mark on the diver watch\nscene in 1948 with the introduction of the Seamaster. This timepiece was a\ngame-changer as it was purposefully designed for divers, combining style and\nfunctionality. It was a clear indication that dive watches were evolving\n",
      "content_length": 1761,
      "extraction_method": "Direct"
    },
    {
      "page_number": 58,
      "chapter": null,
      "content": "beyond mere timekeeping instruments into specialized tools for underwater\nadventurers.\nThe Seamaster didn't stop at design; it also showcased its capabilities in the\ndepths. In 1955, it set a remarkable world record for immersion, descending\nto an astonishing 62.5 meters underwater. This achievement solidified the\nSeamaster's reputation as a reliable and robust diver's companion.\nNot to be outdone, Rolex joined the fray by producing the first Submariner in\n1953. This iconic watch quickly became synonymous with underwater\nexploration and adventure. Its success was further boosted by its association\nwith the fictional British spy, James Bond. The Submariner's appearance on\nthe wrist of the suave secret agent added a layer of mystique and\nsophistication to this already legendary timepiece. It's a partnership that\nendured until 1995, firmly establishing the Submariner as a symbol of\ntimeless style and rugged functionality.\nHowever, the tides of popularity shifted in 1995 when James Bond, portrayed\nby Pierce Brosnan, made a bold move by strapping on an Omega Seamaster\nin \"GoldenEye.\" This marked the beginning of a new era for the Seamaster,\nas it became the watch of choice for the world's most famous secret agent.\nThe Seamaster seamlessly blended style, innovation, and precision, making it\na perfect match for 007's daring missions.\nIn conclusion, diver watches have come a long way since their inception. The\nRolex Oyster Perpetual and Omega Seamaster stand as pioneers, shaping the\npath for future innovations. These watches not only broke records but also\ncaptured the imaginations of generations. The Rolex Submariner and the\nOmega Seamaster, each with its unique charm, continue to be revered in the\nworld of horology, leaving an indelible mark on the wrists of adventurers and\nstyle enthusiasts alike.\nYou may notice that ChatGPT has used the limited information to create a\nquite lengthy article. On the other hand, you might have expected a shorter\npiece and may not be a fan of the complex vocabulary or somewhat formal\ntone it used. In such a situation, try restructuring the prompt more\nsystematically to get the generated text to match your preferred length,\nvocabulary, and tone:\n",
      "content_length": 2209,
      "extraction_method": "Direct"
    },
    {
      "page_number": 59,
      "chapter": null,
      "content": "Persona: you are a watch enthusiast, experienced in writing high quality\narticles for watch magazines for a general audience\nInstruction: Write a piece on the diver watches\nFacts to mention:\nthe Rolex Oyster Perpetual was among the first to be water resistant\nOmega produced the Seamaster in 1948 as the first purpose designed\ndiver watch was the Seamaster\nthe Seamaster set a world record of immersion of 62.5m in 1955.\nRolex produced the first Submariner in 1953\nthe Submariner became popular also because of the association with\nJames Bond until 1995\nJames Bond started to wear the Seamaster in 1995\nTone: use a sober, knowledgeable but not too formal tone and use everyday\nvocabulary\nOutput: the length of the article should be 150 words at most\nOutput:\n**Diver Watches: Timepieces of Exploration and Style**\nDiver watches have long been cherished for their blend of functionality and\naesthetics, a perfect companion for both the adventurous spirit and the style-\nconscious.\nRolex deserves recognition for pioneering water resistance with the Oyster\nPerpetual, setting the stage for a new era in watchmaking.\nOmega, however, made a significant splash in 1948 with the Seamaster, the\nworld's first purpose-designed diver watch. Its 1955 record dive of 62.5\nmeters underlines its prowess in the deep.\nRolex didn't lag behind, introducing the Submariner in 1953, a watch that\n",
      "content_length": 1377,
      "extraction_method": "Direct"
    },
    {
      "page_number": 60,
      "chapter": null,
      "content": "became synonymous with exploration, partly due to its affiliation with James\nBond on-screen.\nSpeaking of James Bond, it's worth noting that in 1995, the iconic secret\nagent started sporting the Omega Seamaster, a move that reinforced the\nSeamaster's reputation as a sophisticated yet rugged timepiece for those who\ndare to explore both land and sea. These diver watches are more than\ninstruments; they're symbols of adventure and style.\nBy adding more details to the prompt, the article has been generated exactly\nas you wanted it.\nnote\nCreating a well-structured prompt with defined sections (Persona, Instruction,\nFacts, Tone, Output Format) helps the LLM understand your requirements\nbetter and generate the expected text.\n2.4.5 Question answering\nThe ability of LLMs to \"understand\" text makes them suitable not only for\nsummarizing but also for retrieving specific information, known as the\n\"Question answering\" use case.\nNow, let's go back to the same text we used in the summarization example\nand ask focused questions about its content.\nInstruction: read the following text, and tell me which home computers were\npopular in the 80s.\nText: Home PCs from the 1980s were iconic pioneers of personal computing.\nComputers like the Apple II, IBM PC, and Commodore 64 revolutionized\nhow we lived and worked. They introduced us to word processing, gaming,\nand the possibilities of home computing. The graphical user interface, as seen\nin the Apple Macintosh, changed the way we interacted with machines. These\nPCs, though less powerful by today's standards, ignited a digital revolution,\nshaping the technology landscape we know today. They hold a special place\nin history, symbolizing the dawn of a new era in computing that laid the\n",
      "content_length": 1735,
      "extraction_method": "Direct"
    },
    {
      "page_number": 61,
      "chapter": null,
      "content": "foundation for the modern digital age.\nOutput: just output the names of the computers in a list\nOutput:\nThe popular home computers in the 1980s were:\n1. Apple II\n2. IBM PC\n3. Commodore 64\n4. Apple Macintosh\nQuestion answering is such a common use case most LLMs understand Q\nand A as shorthand for Question and Answer. For example:\nText: Java is a popular programming language that compiles code into\nbytecode, which is executed by the Java Virtual Machine (JVM) for\nplatform-independent application development.\nQ: Where is Java code executed?\nA:\nOutput:\nJava code is executed by the Java Virtual Machine (JVM).\nLLMs are skilled not just at finding requested information in a text but also at\nmaking logical deductions. This will become apparent in the next section.\n2.4.6 Reasoning\n",
      "content_length": 784,
      "extraction_method": "Direct"
    },
    {
      "page_number": 62,
      "chapter": null,
      "content": "While reasoning is a powerful capability, it's an area where LLMs aren't\nexceptional yet. Therefore, you might need more advanced prompt\nengineering techniques to get the results you want.\nHere's a simple example involving mathematical reasoning:\nInstruction: Add the square numbers in this sequence: 19, 13, 1, 17, 4, 64,\n900\nOutput:\nThe square numbers in the sequence 19, 13, 1, 17, 4, 64, and 900 are 1, 4, 64,\nand 900. To find their sum:\n1 + 4 + 64 + 900 = 969\nSo, the sum of the square numbers in this sequence is 969.\nChatGPT identified the square numbers, picked them out, and then figured\nout their total. The next example is more challenging, involving palindromes\n—numbers that read the same forward and backward, such as 1661 or 27872:\nInstruction:\nSum the palindromes in this sequence: 13, 1331, 121, 73, 99, 56, 232, 7\nI got this output:\nThe palindromes in the sequence 13, 1331, 121, 73, 99, 56, 232, and 7 are\n1331, 121, 232, and 7. To find their sum:\n1331 + 121 + 232 + 7 = 1691\nSo, the sum of the palindromes in this sequence is 1691.\n",
      "content_length": 1052,
      "extraction_method": "Direct"
    },
    {
      "page_number": 63,
      "chapter": null,
      "content": "This is incorrect! A palindrome is a number that reads the same forward and\nbackward. Therefore, 99 should have been considered a palindrome, but it\nwasn't.\nI'll be frank with you: the output above is what I encountered in the early\nstages of writing this book. However, you should now see a different and,\nmore importantly, accurate output. It's still valuable to explain how I\ncorrected ChatGPT's mistake before OpenAI addressed this in a later version\nof the model.\nSo, how did I fix it? Since the LLM didn’t fully grasp the concept of a\npalindrome, I “taught” ChatGPT to correctly identify all relevant cases, as I'll\nexplain in the next section. Before moving on, take a moment to review the\ntypes of prompts you've learned. I've summarized them in Table 2.1 below\nfor your reference.\nTable 2.1 Prompt types and examples\nPrompt type\nPrompt example\nText classification\nClassify the following text into one of these categories:\nhistory, tech, gardening.\nSentiment\nanalysis\nClassify the following text as positive, neutral or\nnegative\nText\nsummarization\nWrite a 30 word summary for the following text\nComposing Text\nWrite a piece on the diver watches, mentioning the\nfollowing facts:\n- the Rolex Oyster Perpetual was among the first to be\nwater resistant\n- Omega produced the Seamaster in 1948 as the first\npurpose designed diver watch was the Seamaster\n…\nQuestion\nanswering\nRead the following text, and tell me which home\ncomputers were popular in the 80s.\nReasoning\nAdd the square numbers in this sequence: 19, 13, 1, 17,\n4, 64, 900\n",
      "content_length": 1537,
      "extraction_method": "Direct"
    },
    {
      "page_number": 64,
      "chapter": null,
      "content": "2.5 Reasoning in detail\nWhen developing LLM applications, particularly those requiring complex\nreasoning, it's important to understand the limitations of different prompt\ntechniques. For instance, \"zero-shot learning\" is a method where you provide\nno examples, expecting the LLM to grasp and perform the task based purely\non the instructions given. While this approach can work well in some\nscenarios, it doesn't always yield accurate results for more intricate tasks,\nsuch as teaching an LLM to identify palindromes. In the following sections,\nI'll demonstrate how to go beyond zero-shot learning to refine LLM prompts\nfor more reliable performance in reasoning tasks.\n2.5.1 One-shot learning\nWhen you provide just one example in the prompt, it's called \"one-shot\nlearning.\" Here's how I attempted to address the palindrome problem by\ngiving a single example:\nInstruction: Sum the palindromes in this sequence: 13, 1331, 121, 73, 99, 56,\n232, 7\nExample: 33 is a palindrome\nI got this output:\nI apologize for the previous oversight. Let's correctly identify and sum the\npalindromes in this sequence: 13, 1331, 121, 73, 99, 56, 232, and 7.\nThe palindromes in the sequence are 1331, 121, 232, and 7. To find their\nsum:\n1331 + 121 + 232 + 7 = 1691\nSo, the sum of the palindromes in this sequence is indeed 1691.\nUnfortunately, even after using one-shot learning, the issue persisted. So, I\n",
      "content_length": 1387,
      "extraction_method": "Direct"
    },
    {
      "page_number": 65,
      "chapter": null,
      "content": "made adjustments to the prompt and included two examples.\n2.5.2 Two-shot learning\nHere is the amended prompt I used:\nInstruction: Sum the palindromes in this sequence: 13, 1331, 121, 73, 99, 56,\n232, 7\nExamples:\n33 is a palindrome\n44 is a palindrome\nThis is the output I got:\nCertainly, let's identify and sum the palindromes in the sequence: 13, 1331,\n121, 73, 99, 56, 232, and 7.\nThe palindromes in this sequence are 1331, 121, 232, and 7. To find their\nsum:\n1331 + 121 + 232 + 7 = 1691\nSo, the sum of the palindromes in this sequence is 1691.\nStill incorrect: 99 got overlooked once again.\nFollowing another unsuccessful attempt, I explored different approaches and\nfound a standard solution. It suggested breaking down the problem into\n\"steps\" and providing clear instructions for each step. This is similar to\nguiding a colleague who may be struggling with a specific step in a\nprocedure.\n2.5.3 Providing steps\n",
      "content_length": 916,
      "extraction_method": "Direct"
    },
    {
      "page_number": 66,
      "chapter": null,
      "content": "This is the amended prompt I entered:\nInstruction: Sum the palindromes in this sequence: 13, 1331, 121, 73, 99, 56\n232, 7\nExamples:\n33 is a palindrome\n44 is a palindrome\nSteps: 1) identify the palindromes; 2) add them up\nI got this output:\nCertainly, let's follow your instructions step by step:\nStep 1: Identify the palindromes in the sequence.\nPalindromes in the sequence:\n1331\n121\n99\n232\n7\nStep 2: Add them up.\n1331 + 121 + 99 + 232 + 7 = 1790\nSo, the sum of the palindromes in the sequence is 1790.\nFinally, I nailed it!\nInterestingly, it appears that ChatGPT has undergone a similar training\nprocess (on a much larger scale, of course) to fix this issue. It's likely that\nother users encountered the same problem.\n",
      "content_length": 719,
      "extraction_method": "Direct"
    },
    {
      "page_number": 67,
      "chapter": null,
      "content": "If you enter the original prompt:\nInstruction:\nSum the palindromes in this sequence: 13, 1331, 121, 73, 99, 56, 232, 7\nYou'll now see the output below.\nLet's identify the palindromes in the given sequence:\n1331 is a palindrome\n121 is a palindrome\n99 is a palindrome\n232 is a palindrome\n7 is a palindrome\nNow, let's add these palindromes together:\n1331 + 121 + 99 + 232 + 7 = 1790\nTherefore, the sum of the palindromes in the sequence is 1790.\nIt appears ChatGPT is addressing the problem step by step, much like how I\nresolved the issue.\nI found it interesting to share an example that's no longer relevant. This is\nsomething you might experience while working with LLMs: you could invest\ntime in teaching the LLM certain tasks using advanced prompt engineering\ntechniques, only to find that a newer model version can handle the task\nwithout any additional training. For instance, OpenAI's o1 model performed\nreasoning tasks with minimal prompts, which previously required more\ncomplex prompts like the few-shot techniques you're about to learn.\n",
      "content_length": 1046,
      "extraction_method": "Direct"
    },
    {
      "page_number": 68,
      "chapter": null,
      "content": "2.5.4 Few-shot learning\nIn some cases, providing only two examples may not be enough to teach the\nLLM a new concept. In such instances, you need to provide several\nexamples, known as \"few-shot learning.\" To demonstrate this, let me create a\ngame where a number can be classified as Abra, Kadabra, or Abra Kadabra.\nLet's see what happens when no information about the classification rules is\nprovided.\nClassify the following numbers as Abra, Kadabra or Abra Kadabra:\n3, 5, 8, 2, 7, 12, 13\nOutput:\nThe classification of numbers into \"Abra,\" \"Kadabra,\" or \"Abra Kadabra\"\ndoesn't follow a conventional mathematical pattern. These are just words, not\nrelated to numbers. Therefore, it's not possible to classify the numbers you\nprovided as \"Abra,\" \"Kadabra,\" or \"Abra Kadabra\" based on any\nmathematical or logical criteria.\nChatGPT hasn't experienced any hallucinations but rightfully pointed out that\nyou haven't clarified the classification rules.\nI'll show you a prompt that, instead of directly explaining the rules, offers an\ninterpretation based on a specific list of numbers, different from the ones I\noriginally asked to be addressed:\nClassify the following numbers as Abra, Kadabra or Abra Kadabra:\n3, 4, 5, 7, 8, 10, 11, 13, 35\nExamples:\n6 // not divisible by 5, not divisible by 7 // None\n",
      "content_length": 1295,
      "extraction_method": "Direct"
    },
    {
      "page_number": 69,
      "chapter": null,
      "content": "15 // divisible by 5, not divisible by 7 // Abra\n12 // not divisible by 5, not divisible by 7 // None\n21 // not divisible by 5, divisible by 7 // Kadabra\n70 // divisible by 5, divisible by 7 // Abra Kadabra\nObserve the format I've employed for every example: I've delineated each\nfield with a double forward slash (//), although I haven't explicitly outlined\nthe meaning of each field. Trusting the LLM to decipher it.\nHere is the output:\nLet's classify the numbers as Abra, Kadabra, or Abra Kadabra based on\nwhether they are divisible by 5 and/or 7:\n3: Not divisible by 5, not divisible by 7 - None\n4: Not divisible by 5, not divisible by 7 - None\n5: Divisible by 5, not divisible by 7 - Abra\n7: Not divisible by 5, divisible by 7 - Kadabra\n8: Not divisible by 5, not divisible by 7 - None\n10: Divisible by 5, not divisible by 7 - Abra\n11: Not divisible by 5, not divisible by 7 - None\n13: Not divisible by 5, not divisible by 7 - None\n35: Divisible by 5, divisible by 7 - Abra Kadabra\nSo, here's the classification:\n",
      "content_length": 1018,
      "extraction_method": "Direct"
    },
    {
      "page_number": 70,
      "chapter": null,
      "content": "Abra: 5, 10, 35\nKadabra: 7\nAbra Kadabra: 35\nNone: 3, 4, 8, 11, 13\nChatGPT not only provided accurate results but also explained the reasoning\nbehind them. It successfully deduced the general rules from the given\nexamples and articulated them well. Quite impressive, isn't it?\nNote\nYou may be familiar with a similar \"number classification game\" or\n“Divisibility Test Algorithm”\n(https://stackoverflow.com/questions/20486231/divisibility-test-algorithm)\nwhere a number divisible by 3 is labeled \"Foo,\" a number divisible by 5 is\nlabeled \"Bar,\" and a number divisible by both 3 and 5 is labeled \"Foo Bar.\" I\nhaven't used this as an example because ChatGPT already knows it and can\ncorrectly classify numbers without examples, using \"zero-shot\" learning.\n2.5.5 Implementing few-shot learning with LangChain\nLet’s implement the AbraKadabra game with LangChain. First, establish a\nconnection to the LLM, as you did previously:\nfrom langchain_openai import ChatOpenAI\nllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,\n                model_name=\"gpt-4o-mini\")\nNow, define and run the same few-shot prompt you executed previously\ndirectly against ChatGPT:\nprompt_input = \"\"\"Classify the following numbers as Abra, Kadabra \n3, 4, 5, 7, 8, 10, 11, 13, 35\nExamples: \n",
      "content_length": 1255,
      "extraction_method": "Direct"
    },
    {
      "page_number": 71,
      "chapter": null,
      "content": "6 // not divisible by 5, not divisible by 7 // None\n15 // divisible by 5, not divisible by 7 // Abra\n12 // not divisible by 5, not divisible by 7 // None\n21 // not divisible by 5, divisible by 7 // Kadabra\n70 // divisible by 5, divisible by 7 // Abra Kadabra\n\"\"\"\nresponse = llm.invoke(prompt_input)\nprint(response.content)\nThe output, is, as expected:\n3 // not divisible by 5, not divisible by 7 // None\n4 // not divisible by 5, not divisible by 7 // None\n5 // divisible by 5, not divisible by 7 // Abra\n7 // not divisible by 5, divisible by 7 // Kadabra\n8 // not divisible by 5, not divisible by 7 // None\n10 // divisible by 5, not divisible by 7 // Abra\n11 // not divisible by 5, not divisible by 7 // None\n13 // not divisible by 5, not divisible by 7 // None\n35 // divisible by 5, divisible by 7 // Abra Kadabra\nThe output is accurate, but the implementation isn't ideal because the\nexamples are hardcoded in the prompt. LangChain provides a cleaner\nsolution for creating a few-shot prompt. It lets you separate the training\nexamples from the prompt template and inject them later, as shown in listing\n2.1.\nListing 2.1 Few-shot prompt using FewShotPromptTemplate\nfrom langchain_core.prompts.few_shot import FewShotPromptTemplate\nfrom langchain_core.prompts.prompt import PromptTemplate\nexamples = [\n {\n     \"number\": 6,\n     \"reasoning\": \"not divisible by 5 nor by 7\",\n     \"result\": \"None\"\n },\n {\n     \"number\": 15,\n     \"reasoning\": \"divisible by 5 but not by 7\",\n     \"result\": \"Abra\"\n },\n {\n",
      "content_length": 1498,
      "extraction_method": "Direct"
    },
    {
      "page_number": 72,
      "chapter": null,
      "content": "     \"number\": 12,\n     \"reasoning\": \"not divisible by 5 nor by 7\",\n     \"result\": \"None\"\n },\n {\n     \"number\": 21,\n     \"reasoning\": \"divisible by 7 but not by 5\",\n     \"result\": \"Kadabra\"\n },\n {\n     \"number\": 70,\n     \"reasoning\": \"divisible by 5 and by 7\",\n     \"result\": \"Abra Kadabra\"\n } ]\nexample_prompt = PromptTemplate(input_variables=[\"number\", \"reaso\nfew_shot_prompt = FewShotPromptTemplate(\n   examples=examples,\n   example_prompt=example_prompt,\n   suffix=\"Classify the following numbers as Abra, Kadabra or Ab\n   input_variables=[\"comma_delimited_input_numbers\"]\n)\nprompt_input = few_shot_prompt.format(comma_delimited_input_numbe\nresponse = llm.invoke(prompt_input)\nprint(response.content)\nAnd the result is still correct:\n3 \\ not divisible by 5 nor by 7 \\ None\n4 \\ not divisible by 5 nor by 7 \\ None\n5 \\ divisible by 5 but not by 7 \\ Abra\n7 \\ divisible by 7 but not by 5 \\ Kadabra\n8 \\ not divisible by 5 nor by 7 \\ None\n10 \\ divisible by 5 but not by 7 \\ Abra\n11 \\ not divisible by 5 nor by 7 \\ None\n13 \\ not divisible by 5 nor by 7 \\ None\n35 \\ divisible by 5 and by 7 \\ Abra Kadabra\nBefore concluding this section, I want to introduce you to a technique known\nas \"chain of thought,\" which blends \"providing logical steps\" with \"few-shot\nlearning.\"\n2.5.6 Chain of Thought (CoT)\n",
      "content_length": 1294,
      "extraction_method": "Direct"
    },
    {
      "page_number": 73,
      "chapter": null,
      "content": "I'm making up a new mathematical definition: I'll name a sequence of\nnumbers \"Strange\" if it contains at least two odd numbers, and the sum of all\nodd numbers is divisible by 3.\nInitially, I check whether the LLM is unfamiliar with the rules of \"Strange\"\nsequences:\nIs the following sequence \"Strange\"?\n1, 3, 4, 5, 7, 10, 15, 18, 22, 24\nOutput:\nThe sequence you provided appears to be increasing, but it doesn't follow a\ncommon mathematical pattern like arithmetic or geometric progressions.\nWhether it's considered \"strange\" depends on the context or criteria you're\nusing to define \"strange.\" In a mathematical sense, it might be considered an\nunusual or non-standard sequence. If you have specific criteria or rules that\ndefine what a \"strange\" sequence is, please provide more details for a more\nprecise classification.\nAs expected, ChatGPT doesn't seem to understand the concept of a \"Strange\nsequence,\" reaffirming that, luckily, it hasn't developed psychic abilities…\nyet.\nI will now create a prompt using the Chain of Thought technique, as\nintroduced by Wei et al (https://arxiv.org/abs/2201.11903). This prompt, an\nextension of a few-shot learning prompt, will include:\n1. Several sample sequences for few-shot learning\n2. For each sample sequence, an explanation of the logical steps to\ndetermine if a sequence is Strange\nQ: Is the following sequence \"Strange\"?\n3, 4, 5, 7, 10, 18, 22, 24\n",
      "content_length": 1399,
      "extraction_method": "Direct"
    },
    {
      "page_number": 74,
      "chapter": null,
      "content": "A:\nExamples:\nQ: is the following sequence strange: 1, 4, 6, 8, 20\nA: 1 is an odd number; I need at least two odd numbers // Not Strange\nQ: is the following sequence strange: 5, 6, 7, 8, 20\nA: 5 and 7 are odd numbers; the sum of 5 and 7 is 12; 12 is divisible by 3 //\nStrange\nQ: is the following sequence strange: 1, 5, 6, 7, 8, 20\nA: 1, 5 and 7 are odd numbers; the sum of 1, 5 and 7 is 13; 13 is not divisible\nby 3 // Not Strange\nQ: is the following sequence strange: 5, 6, 7, 8, 9, 20\nA: 5, 7, 9 are odd numbers; the sum of 5, 7 and 9 is 21; 21 is divisible by 3 //\nStrange\nOutput:\nThank you for providing the examples and clarifications. Based on the\ncriteria you've presented:\nThe sequence \"3, 4, 5, 7, 10, 18, 22, 24\" has 3 odd numbers: 3, 5, and 7. The\nsum of these odd numbers is 15. 15 is divisible by 3. So, according to the\ncriteria, this sequence is **Strange**.\nChatGPT gave the right answer and explained its thought process. To\nreinforce your understanding, try using a similar prompt with a different set\nof numbers. Also, observe how it responds when the sequence contains just\none odd number.\n",
      "content_length": 1110,
      "extraction_method": "Direct"
    },
    {
      "page_number": 75,
      "chapter": null,
      "content": "In essence, in this section you've discovered how to introduce ChatGPT to a\nnew task by offering various examples or outlining the necessary steps. This\napproach is known as \"in-context\" learning, as you're instructing the LLM\nusing information provided within the context of a prompt.\nDefinition\n“In-context learning” refers to any method that instructs an LLM on an\nunfamiliar task or instruction requested by the user, utilizing examples within\nthe prompt context. Common techniques include one-shot, two-shot, or few-\nshot learning, as well as providing step-by-step guidance, often referred to as\na \"chain of thought.\" Compared to fine-tuning, in-context learning is a more\ncost-effective and less resource-intensive approach, as it doesn't demand\nhigh-end hardware resources like GPUs or in-depth knowledge of\ntransformer architectures.\nThis section covered a range of in-context learning prompts, which I have\nsummarized, for convenience, in table 2.2.\nTable 2.2 In-context learning prompts\nIn-Context learning\ntechnique\nExplanation\nZero-shot learning\nNo example is provided in the prompt\nOne-shot learning\nOne example is provided in the prompt\nTwo-shot learning\nTwo examples are provided in the prompt\nFew-shot learning\nA number of examples are provided in the\nprompt\nChain of Thought\nA number of examples are provided in the\nprompt, and for each example, all the logical\nsteps to achieve an objective are clearly\nexplained\nIf you want to explore advanced techniques beyond Chain of Thought, check\nthe related sidebar. It includes detailed pointers on Tree of Thought and\nThread of Thought, two powerful methods that enhance reasoning and\nproblem-solving in language models.\n",
      "content_length": 1683,
      "extraction_method": "Direct"
    },
    {
      "page_number": 76,
      "chapter": null,
      "content": "Beyond Chain of Thought\nChain of Thought improves how language models handle complex reasoning\nby breaking problems into smaller, logical steps. However, it has limitations,\nsuch as missing deeper exploration or struggling with messy contexts. Two\nadvanced techniques address these gaps: Tree of Thought (ToT) and Thread\nof Thought (ThoT):\nTree of Thought (ToT): ToT improves problem-solving by allowing\nlanguage models to explore multiple reasoning paths. Traditional models\nmake token-by-token decisions, which limits their ability to plan ahead\nor revisit earlier choices. ToT structures the process into coherent steps,\nenabling models to evaluate and refine options as they go. This method\n(https://arxiv.org/abs/2305.10601) is highly effective in tasks requiring\nstrategic thinking, such as the Game of 24, creative writing, and mini\ncrosswords. For example, ToT helped GPT-4 solve 74% of Game of 24\nproblems, compared to just 4% using Chain of Thought. This approach\nimproves deliberate decision-making, backtracking, and strategic\nforesight.\nThread of Thought (ThoT): ThoT addresses challenges with chaotic\ncontexts, where irrelevant details distract models or lead to errors.\nInspired by human cognition, ThoT (https://arxiv.org/abs/2311.08734)\nsystematically breaks down and analyzes large chunks of information\nwhile focusing on relevant details. It works as a modular add-on that\nintegrates with various models and prompts. Experiments on datasets\nlike PopQA, EntityQ, and a Multi-Turn Conversation Response dataset\nshow ThoT significantly enhances reasoning accuracy. It excels in\nfiltering noise and maintaining focus in complex contexts.\nBoth techniques push language models beyond simple response generation,\nimproving reasoning, planning, and decision-making in structured and\nchaotic scenarios. For technical details and examples, refer to the published\nresearch and code repositories.\nBefore closing this chapter, let's try to extract a standard prompt format from\nour examination of various use cases up to this point.\n",
      "content_length": 2039,
      "extraction_method": "Direct"
    },
    {
      "page_number": 77,
      "chapter": null,
      "content": "2.6 Prompt structure\nCombining all the prompt elements from earlier sections results in the\nfollowing generalized structure:\nPersona: Specify the role you want the language model (LLM) to\nembody. Whether a skilled marketing manager, an award-winning sci-fi\nauthor, or a proficient copy editor for a computing magazine, set the\nexpectations high.\nContext: Provide detailed background information to help the LLM\ncomprehend the context surrounding your request. Precision enhances\nthe likelihood of aligning with your expectations.\nInstruction: Clearly define the action you want the LLM to take with\nyour input text. Whether it's translation, writing, rewriting, improving,\nclassifying, ordering, or finding, be explicit.\nInput: This refers to specific context details, which could take the form\nof a paragraph, a query, or a list of key points. You may choose to label\nit accordingly, such as \"Text\" or \"Question.\", etc\nSteps: Outline the processing steps the LLM should follow to generate\nthe output.\nTone: Specify the desired tone of the LLM's answer—formal, informal,\nwitty, enthusiastic, sober, friendly, etc. Combinations are possible.\nOutput Format: Optionally specify the desired output format. If not\nspecified, the LLM assumes a text answer. However, you can request\nformats like a table, CSV file (with column names), JSON (with\nattribute names), or other structured formats.\nExamples: In cases where the LLM may lack sufficient training for\ncertain tasks, especially in text classification and reasoning, provide\nmini-inline training through examples. Even a few examples\nsignificantly enhance answer quality.\nThis list acts as a guide or checklist for organizing prompts. Customize it\nbased on your requirements, choosing relevant sections or making\nmodifications. Here's a prompt that includes many of these elements,\nstructured with clear sections:\nPersona: You are an experienced Large Language Model (LLM) developer\n",
      "content_length": 1932,
      "extraction_method": "Direct"
    },
    {
      "page_number": 78,
      "chapter": null,
      "content": "and renowned speaker.\nContext: You have been invited to give a keynote speech for a LLM event.\nInstruction: Write the punch lines for the speech.\nInput: Include the following facts:\nLLMs have become mainstream with the launch of ChatGPT in\nNovember 2022\nmany popular LLMs and LLM based chatbots have been launched since\nthen, such as LLAMA-`2, Falcon180B, Bard.\nLLMs becoming as popular as search engines\nmany companies want to integrate LLMs in their applications\nTone: Use a witty but entertaining tone.\nOutput format: Present the text in two paragraphs of 5 lines each.\nOutput:\nLadies and gentlemen, as we embark on this linguistic odyssey, remember\nthis: In November 2022, ChatGPT didn't just enter the scene; it was the\nlinguistic equivalent of a rockstar's encore, setting the stage for an epic word-\ntastic journey! Today, with LLAMA-2, Falcon180B, and Bard, it's safe to say\nthat LLMs have become the Beatles of the digital era, making search engines\nfeel like yesterday's news.\nBut here's the kicker – companies are now falling over themselves,\ndesperately trying to get these LLMs into their applications. It's like the gold\nrush, but instead of panning for precious metal, we're sifting through bytes of\nbrilliant language. So, my friends, in this age where words wield the power,\nand LLMs are the mighty pens, we're not just scripting the future; we're\npenning a linguistic saga that's bound to be a best-seller! Welcome to the age\nof LLMs, where we're rewriting the rules, one sentence at a time, and words\nare the currency of change!\n",
      "content_length": 1548,
      "extraction_method": "Direct"
    },
    {
      "page_number": 79,
      "chapter": null,
      "content": "NOTE\nStudies (for example “The Prompt Report: A Systematic Survey of\nPrompting Techniques”, https://arxiv.org/abs/2406.06608) have found that\nexplicitly naming different parts of a prompt tends to improve results.\nHowever, you don't have to name every section. Most LLMs can figure out\nthe purpose of the text in the prompt on their own. So, you can mix things up\nby naming some parts (like Question or Examples) and leaving out names for\nothers (like Context or Tone). Experiment and see what works best for your\ncase and the results you're getting.\nIf you want to delve deeper into prompt engineering, I highly recommend the\nfollowing resources:\nhttps://www.promptingguide.ai/techniques\nhttps://github.com/dair-ai/Prompt-Engineering-Guide\nhttps://github.com/promptslab/Awesome-Prompt-Engineering\n2.7 Summary\nA prompt is a specific request that guides the LLM, giving it instructions\nbased on provided background information.\nDifferent types of prompts are tailored for specific tasks, such as text\nclassification, sentiment analysis, text summarization, text generation, or\nquestion and answering.\nIf a prompt doesn't produce the expected result, it means the underlying\nLLM isn't familiar with the requested task and needs training.\nTraining involves enhancing the prompt with examples, categorized as\none-shot, two-shot, or few-shot learning based on the number of\nexamples given.\nAs you create various prompts, you might notice a common prompt\nstructure with sections like persona, context, instruction, input, steps,\nexamples, etc.\nYou should adapt the prompt structure to your use case by choosing\nrelevant sections or adding custom ones. You can also drop some\nexplicit section names\nWhile users often create and execute prompts directly through a chat\n",
      "content_length": 1761,
      "extraction_method": "Direct"
    },
    {
      "page_number": 80,
      "chapter": null,
      "content": "LLM UI like ChatGPT, software developers typically automate prompt\ngeneration programmatically using directly the LLM API or LangChain.\nLangChain supports prompt engineering with a range of classes, from\nPromptTemplate to the more advanced FewShotPromptTemplate\nIn LLM application development, it's crucial to design parameterized\nprompts, also known as prompt templates.\n",
      "content_length": 372,
      "extraction_method": "Direct"
    },
    {
      "page_number": 81,
      "chapter": null,
      "content": "3 Summarizing text using\nLangChain\nThis chapter covers\nSummarization of large documents exceeding the LLM’s context\nwindow\nSummarization across multiple documents\nSummarization of structured data\nIn Chapter 1, you explored three major LLM application types:\nsummarization engines, chatbots, and autonomous agents. In this chapter,\nyou'll begin building practical summarization chains using LangChain, with a\nparticular focus on the LangChain Expression Language (LCEL) to handle\nvarious real-world scenarios. A chain is a sequence of connected operations\nwhere the output of one step becomes the input for the next—ideal for\nautomating tasks like summarization. This work lays the foundation for\nconstructing a more advanced summarization engine in the next chapter.\nSummarization engines are essential for automating the summarization of\nlarge document volumes, a task that would be impractical and costly to\nhandle manually, even with tools like ChatGPT. Starting with a\nsummarization engine is a practical entry point for developing LLM\napplications, providing a solid base for more complex projects and\nshowcasing LangChain’s capabilities, which we’ll further explore in later\nchapters.\nBefore we start building, we’ll look at different summarization techniques,\neach suited to specific scenarios like large documents, content consolidation,\nand handling structured data. Since you’ve already worked with summarizing\nsmall documents using a PromptTemplate in section 1.3.2, we’ll skip that and\nfocus on more complex examples.\n",
      "content_length": 1530,
      "extraction_method": "Direct"
    },
    {
      "page_number": 82,
      "chapter": null,
      "content": "3.1 Summarizing a document bigger than context\nwindow\nAs mentioned in chapter 2, each LLM has a maximum prompt size, also\nreferred to as the \"context window.\"\nDEFINITION\nThe \"LLM context window\" represents the maximum size of the prompt\nprovided to an LLM, comprising instructions and context. Different LLMs\nhave varying token limits for the context window: GPT-3.5 used to accept up\nto 16K tokens, GPT-4 and Claude-3 up to 100K, and Gemini-1.5 up to 1M.\nAs the context window for popular LLMs continues to grow, you may still\nencounter situations where a document exceeds the token limit of your\nchosen model. In these cases, you can use a map-reduce approach, as\nillustrated in Figure 3.1.\nFigure 3.1 Summarizing a document bigger than the LLM’s context window: this involves\nsplitting the document into smaller chunks, summarizing each chunk, and then summarizing the\ncombined chunk summaries.\n",
      "content_length": 898,
      "extraction_method": "Direct"
    },
    {
      "page_number": 83,
      "chapter": null,
      "content": "Large doc Large doc\n‘Summarizer LM\nBi sa\n\nsummarize\n\nThe large document\n\nissplitinto =\ndocument chunks |\ncocccettescedttteceltttescessvace Doe chunks !\n(return) 5 5 5 |\nThe map operation map(summarize) :\ntransforms document |\nnk summ\nchunks into a a\ncorresponding summaries\n(tum chunk summaries\nonsen E E\na }\nfinal summary\nF) i, 7\nreturn\n\ntransforms all the chunk\nSummaries into a single\ncondensed summary",
      "content_length": 405,
      "extraction_method": "OCR"
    },
    {
      "page_number": 84,
      "chapter": null,
      "content": "This workflow involves splitting the document into smaller segments,\nsummarizing each one, and then summarizing the combined summaries.\nTo start, you need to split the text into chunks using a tokenizer. A tokenizer\nreads the text and breaks it into tokens, which are the smallest units of text,\noften parts of words. After tokenizing the document, the tokens are grouped\ninto chunks of a specific size. This lets you control the content size being\nprocessed by the LLM and ensures the token count stays within your LLM's\nprompt limit. I'll show you how to use TokenTextSplitter, part of the\ntiktoken package, a tokenizer developed by OpenAI.\nBegin by opening a terminal and creating a new folder named ch03 for this\nchapter’s code. Then, create and activate a virtual environment:\nC:\\Github\\building-llm-applications>md ch03\nC:\\Github\\building-llm-applications>cd ch03\nC:\\Github\\building-llm-applications\\ch03>python -m venv env_ch03\nC:\\Github\\building-llm-applications\\ch03>.\\env_ch03\\Scripts\\activ\n(env_ch03) C:\\Github\\building-llm-applications\\ch03>\nNext, install the required packages—tiktoken, notebook, and langchain. If\nyou’ve cloned the repo from GitHub, use pip install -r requirements.txt.\nOtherwise, install them as follows:\n(env_ch03) C:\\Github\\building-llm-applications\\ch03>pip install t\nOnce the installation is complete, start the Jupyter notebook:\n(env_ch03) C:\\Github\\Building-llm-applications\\ch03>jupyter noteb\nNow, open or create a notebook and name it 03-\nsummarization_examples.ipynb, then save it.\n3.1.1 Chunking the text into Document objects\nLet's summarize the book \"Moby Dick\" using its text file, Moby-Dick.txt,\ndownloaded from The Project Gutenberg. You can locate the Moby-Dick.txt\nfile in the chapter's subfolder on my Github page (https://github.com/roberto-\ninf/building-llm-applications/tree/main/ch03). Place this file in your ch03\n",
      "content_length": 1869,
      "extraction_method": "Direct"
    },
    {
      "page_number": 85,
      "chapter": null,
      "content": "folder and load the text into a variable:\nwith open(\"./Moby-Dick.txt\", 'r', encoding='utf-8') as f:\n   moby_dick_book = f.read()\nIMPORTANT\nKeep in mind that running the code on the full \"Moby Dick\" text can get\nexpensive. The provided \"Moby-Dick.txt\" file is a shorter version, containing\nonly 5 chapters and about 18,000 tokens. Running the code a few times with\nthis version shouldn’t cost much. However, if you plan to run many tests, you\nmay want to reduce the file size even more to save money. Each time you\nexecute a chain with an LLMChain block, you’ll be charged. If budget\nallows, you can use the full version of the book, labeled \"Moby-\nDick_ORIGINAL_EXPENSIVE.txt.\" The entire \"Moby Dick\" text has\naround 300,000 words, or about 350,000 tokens. Using GPT-4o, which costs\n$2.50 per million tokens, processing the full text would cost about $0.75.\nRunning it multiple times will add up. If you switch to GPT-4o-mini, which\ncosts $0.15 per million tokens, the cost drops to around $0.05, making it\nmuch cheaper.\nI’ll cover chunking strategies in detail—like chunking by size and content\nstructure—in chapter 8. For now, let's split the text into chunks of about\n3,000 tokens each to simulate a context window shorter than the GPT4o-mini\nmodel we’ll be using.\n3.1.2 Split\nFirst, import the necessary libraries:\nfrom langchain_openai import ChatOpenAI\nfrom langchain_text_splitters import TokenTextSplitter\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableLambda, RunnablePara\nimport getpass\nNext, retrieve your OpenAI API key using the getpass function:\nOPENAI_API_KEY = getpass.getpass('Enter your OPENAI_API_KEY')\n",
      "content_length": 1728,
      "extraction_method": "Direct"
    },
    {
      "page_number": 86,
      "chapter": null,
      "content": "You'll be prompted to enter your OPENAI_API_KEY. After that, instantiate the\nOpenAI model in a new notebook cell::\nllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,model_name=\"gpt-4o\nNow you're ready to set up the first chain, which will break the document into\nchunks of the specified size. In this chapter, you'll learn the basics of LCEL,\nwith a more detailed exploration in the next chapter:\ntext_chunks_chain = (\n   RunnableLambda(lambda x: \n       [\n           {\n               'chunk': text_chunk, \n           }\n           for text_chunk in \n              TokenTextSplitter(chunk_size=3000, chunk_overlap=1\n       ]\n   )\n)\nNote\nLangChain chains are made up of components that implement the Runnable\ninterface, an abstract Python class that defines how a component takes input,\nprocesses it, and returns output. Any class implementing Runnable can be\npart of a chain. RunnableLambda lets you turn any Python callable into a\n`Runnable`, making it easy to include custom functions in a LangChain\nchain. It's similar to Python's lambda expression, where you run code with a\nparameter and optionally return output without defining a full function. With\nRunnableLambda, you can create a chain component without writing a\nseparate class to implement the Runnable interface. In this example, the code\nwrapped by RunnableLambda takes input text as a string through the x\nparameter and passes it to the split_text() function, which breaks the text\ninto chunks.\n3.1.3 Map\nThe next step is to set up the \"map\" chain, which will run a summarization\nprompt for each document chunk. If you're unfamiliar with \"map-reduce,\"\n",
      "content_length": 1613,
      "extraction_method": "Direct"
    },
    {
      "page_number": 87,
      "chapter": null,
      "content": "refer to the sidebar for more details.\nsummarize_chunk_prompt_template = \"\"\"\nWrite a concise summary of the following text, and include the ma\nText: {chunk}\n\"\"\"\nsummarize_chunk_prompt = PromptTemplate.from_template(summarize_c\nsummarize_chunk_chain = summarize_chunk_prompt | llm\nsummarize_map_chain = (\n   RunnableParallel (\n       {\n           'summary': summarize_chunk_chain | StrOutputParser()  \n       }\n   )\n) \nNote\nIn this chain, I’ve used RunnableParallel, which is similar to\nRunnableLambda, but it operates on a sequence, processing each element in\nparallel. In this case, we’ll feed the sequence of text chunks to the\nsummarize_map_chain, and each chunk will be summarized in parallel by the\ninner summarize_map_chain.\nMap-reduce\nMap-reduce is a programming model for processing large datasets in two\nsteps. First, the \"map\" operation splits the data into smaller subsets, each\nprocessed independently by the same function. This step typically returns a\nlist of results, grouped by a key. Next is the \"reduce\" operation, where results\nfor each key are aggregated into a single outcome. The final output is a list of\nkey-value pairs, where the keys come from the map step and the values are\nthe result of aggregating the mapped data in the reduce step.\n3.1.4 Reduce\nSetting up the reduce chain, which summarizes the summaries from each\ndocument chunk, follows a process similar to the map chain but requires a bit\n",
      "content_length": 1425,
      "extraction_method": "Direct"
    },
    {
      "page_number": 88,
      "chapter": null,
      "content": "more setup. Start by defining the prompt template:\nsummarize_summaries_prompt_template = \"\"\"\nWrite a coincise summary of the following text, which joins sever\nText: {summaries}\n\"\"\"\nsummarize_summaries_prompt = PromptTemplate.from_template(summari\nNext, you can configure the reduce chain:\nsummarize_reduce_chain = (\n   RunnableLambda(lambda x: \n       {\n           'summaries': '\\n'.join([i['summary'] for i in x]), \n       })\n   | summarize_summaries_prompt \n   | llm \n   | StrOutputParser()\n)\nThe reduce chain includes a lambda function that combines the summaries\nfrom the map chain into a single string. This string is then processed by the\nsummarize_summaries_prompt prompt, which generates a final summary of\nthe combined content.\n3.1.5 Map-reduce combined chain\nFinally, we combine the document-splitting chain, the map chain, and the\nreduce chain into a single map-reduce chain:\nmap_reduce_chain = (\n  text_chunks_chain  #A\n  | summarize_map_chain.map()  #B\n  | summarize_reduce_chain  #C\n) \nThis setup efficiently splits the input document in chunks, summarizes each\nchunk, and then compiles those summaries into a final summary. The map()\nfunction on summarize_map_chain is essential to enable parallel processing\nof the chunks.\n",
      "content_length": 1239,
      "extraction_method": "Direct"
    },
    {
      "page_number": 89,
      "chapter": null,
      "content": "3.1.6 Map-reduce execution\nEverything is set up. Start the map-reduce summarization of the large\ndocument with this command (if you are on the OpenAI free-tier, this might\nfail with a 429 Rate Limit error):\nsummary = map_reduce_chain.invoke(moby_dick_book)\nIf you run print(summary), you'll get output similar to the following:\nThe introduction to the Project Gutenberg eBook of \"Moby-Dick; or \nAs Ishmael shares a bed with Queequeg, whom he initially fears to \nWARNING\nAs previously mentioned, running the map_reduce_chain will incur costs; the\nlonger the text you wish to summarize, the higher the cost. Therefore,\nconsider further shortening the input text (in our case, the Moby-Dick.txt\nebook file) if you want to limit expenses. Additionally, ensure your OpenAI\nAPI credit balance remains positive to avoid errors such as: RateLimitError:\nError code: 429 - {'error': {'message': 'You exceeded your current quota,\nplease check your plan and billing details [...]. If necessary, log in to the\nOpenAI API page, navigate to Settings > Billing, and set a credit of at least\n$5.\nLet's now proceed to the next use case: summarizing across documents.\n3.2 Summarizing across documents\nYou can easily learn how to summarize information across various data\nsources, such as Wikipedia or local files in Microsoft Word, PDF, and text\nformats. This process, as shown in Figure 3.2, is similar to the map-reduce\ntechnique used in the previous section.\nFigure 3.2 Summarizing across documents using the Map-Reduce technique seen earlier: in this\nmethod, each document chunk undergoes a map operation to generate a summary. These\nindividual summaries are then further condensed into a single summary through the reduce\noperation.\n",
      "content_length": 1719,
      "extraction_method": "Direct"
    },
    {
      "page_number": 90,
      "chapter": null,
      "content": "In the sequence diagram in Figure 3.2, content from each raw text source is\nloaded into a corresponding Document instance. During the \"map\" operation,\nthese Document objects are converted into individual summaries, which are\n",
      "content_length": 225,
      "extraction_method": "Direct"
    },
    {
      "page_number": 91,
      "chapter": null,
      "content": "then combined into a single summary during the \"reduce\" operation.\nNext, I’ll introduce you to an alternative technique called \"Refine,\" illustrated\nin Figure 3.3.\nFigure 3.3 Summarizing across documents using the Refine technique: with this approach, a final\nsummary is constructed incrementally by iteratively summarizing the combination of the current\nfinal summary and one of the document chunks. This process continues until all document\nchunks have been processed, resulting in the completion of the final summary.\n",
      "content_length": 521,
      "extraction_method": "Direct"
    },
    {
      "page_number": 92,
      "chapter": null,
      "content": "Web Word PDF Web Word POF\n\npage file _—fle i ot Sewer |= file — file ‘ [we | uw |\n5 BE 5 = BBE 5 a\n\ni\n'\ni\n'\ni\n\nsummarize !\ni The raw text !\nWeb Word POF tt sources are loaded |\nB B B B Document into Documents 3\n(return) Web Document !\nThe Document fF !\npopulated from the summarize\nweb site is\nsummarized\n5 Summary\nio haan eerie\nThe summary is :\n\"refined\" with the refine( summarize)\ncontent from the Word\ndocument = Summary\n(oun)\nThe sunumary is further [Summary + PDF, ...]\n\"refined\" with the\ncontent from the PDF Summary + tet document\nand txt Documents | fe\nrefine(summarize)\nfinal summary\n8 =| Summary\nca sides ccs ariallesbisaulcaaietuid as aed eae aac",
      "content_length": 658,
      "extraction_method": "OCR"
    },
    {
      "page_number": 93,
      "chapter": null,
      "content": "In this method, you progressively build the final summary by refining it with\neach step. Each document is sent to the LLM for summarization, along with\nthe current draft of the summary. This continues until all documents are\nprocessed, leading to the final summary. Map-reduce works well for\nsummarizing large volumes of text, where some content loss is acceptable to\nmanage the processing load. In contrast, the refine technique is better when\nyou want to ensure that the essence of each part is fully captured in the final\nsummary.\n3.2.1 Creating a list of Document objects\nWhen summarizing a large document, you typically start by breaking it into\nsmaller chunks, treating each chunk as a separate document. In this case,\nwe're beginning with a set of existing documents, so there's no need to split\nanything. How you create each Document object will depend on the source of\nthe text. I’ll show you how to summarize content from four different sources:\na Wikipedia page and a set of files in various formats (txt, docx, PDF) stored\nin a local folder. All the content is related to Paestum, a Greek colony on the\nCilento coast in southern Italy around 500 BC. You'll use the appropriate\nDocumentLoader for each data source, selecting from the many options\nintroduced earlier in section 1.6 on LangChain’s Document object model..\n3.2.2 Wikipedia content\nLet's begin with the Wikipedia content. While you can create a document\nfrom web-based data content using the WebBaseLoader, specific loaders are\ncustomized to retrieve content from particular websites, such as the\nIMSDbLoader for the Internet Movie Script Database (IMSDb) website, the\nAZLyricsLoader for the AZ Lyrics website, and the WikipediaLoader for the\nWikipedia website.\nFirstly, install the package for the loaders, including the ones for\nDocx2txtLoader (for Word files) and the PyPDFLoader (for PDFs). You can\ninstall them all at once to avoid stopping and restarting the Jupyter notebook\nmultiple times. Alternatively, you can open a separate OS shell and install the\npackages there after activating your virtual environment:\n",
      "content_length": 2093,
      "extraction_method": "Direct"
    },
    {
      "page_number": 94,
      "chapter": null,
      "content": "(env_ch03) C:\\Github\\Building-llm-applications\\ch03>pip install w\nIf you cloned the repository from Github and run pip install -r\nrequirements.txt after activating the virtual environment, all the required\npackages should already be installed.\nOnce everything is installed, start the Jupyter notebook as usual:\n(env_ch03) C:\\Github\\Building-llm-applications\\ch03>jupyter noteb\nNow, import the content from the Paestum Wikipedia page:\nfrom langchain.document_loaders import WikipediaLoader\nwikipedia_loader = WikipediaLoader(query=\"Paestum\", load_max_docs\nwikipedia_docs = wikipedia_loader.load()\nNOTE\nThe WikipediaLoader may load content from other Wikipedia hyperlinks\nreferenced in the requested article. For example, the Paestum article\nreferences the National Archeological Museum of Paestum, the Lucania\nregion, Lucanians, and the temples of Hera and Athena, resulting in\nadditional content loaded. Thus, it returns a Document list rather than a single\nDocument object. I've set the maximum number of documents returned to 2\nto save on summarization costs, but you can adjust it as needed.\n3.2.3 File based content\nTo get started, download or pull the Paestum folder from GitHub and place it\nin your local ch03 directory (if you didn’t clone the entire repository). The\nPaestum subfolder within ch03 contains three files:\nPaestum-Britannica.docx: Content sourced from the Encyclopedia\nBritannica website.\nPaestumRevisited.pdf: An excerpt from \"Paestum Revisited,\" a master\nthesis submitted at Stockholm University. The extract comprises only 4\npages, but you have the option to use the full document located in the\nsame folder (PaestumRevisited-StocholmsUniversitet.pdf).\n",
      "content_length": 1677,
      "extraction_method": "Direct"
    },
    {
      "page_number": 95,
      "chapter": null,
      "content": "Paestum-Encyclopedia.txt: Content taken from Encyclopedia.com.\nBelow is the process to load these files into corresponding documents:\nfrom langchain.document_loaders import Docx2txtLoader\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.document_loaders import TextLoader\nword_loader = Docx2txtLoader(\"Paestum/Paestum-Britannica.docx\")\nword_docs = word_loader.load()\npdf_loader = PyPDFLoader(\"Paestum/PaestumRevisited.pdf\")\npdf_docs = pdf_loader.load()\ntxt_loader = TextLoader(\"Paestum/Paestum-Encyclopedia.txt\")\ntxt_docs = txt_loader.load()\nThe document variables (word_docs, pdf_docs, txt_docs) are in plural mode\nbecause a loader always returns a list of documents, even if the list contains\nonly one item.\nNOTE\nYou may have noticed the direct creation of a Document object from\nPaestum-Encyclopedia.txt using a TextLoader. You might wonder why the\nMoby-Dick.txt file was read with the Python file reader previously. The\nreason is that in that case, the intention was to split the content into a specific\nnumber of tokens to fit the LLM prompt, requiring manual creation of a\nDocument for each.\n3.2.4 Creating the Document list\nYou can now merge all the documents from various sources into a single\nDocument list:\nall_docs = wikipedia_docs + word_docs + pdf_docs + txt_docs\nWith everything compiled, you're ready to summarize the content using the\n\"refine\" technique. Before you move forward, take a moment to check out the\nsidebar on Document Loaders for alternative ways to create your document\n",
      "content_length": 1517,
      "extraction_method": "Direct"
    },
    {
      "page_number": 96,
      "chapter": null,
      "content": "list..\nDocument Loaders\nWhile the sections above have introduced document loaders for specific data\nsources, I encourage you to explore the UnstructuredLoader as well. It\nenables you to import content from various file types, including Word, PDF,\nand txt files, among others.\nAnother option is the DirectoryLoader, which utilizes the\nUnstructuredLoader internally. It allows you to load content from files of\ndifferent formats located in the same folder in a single operation.\nAs an exercise, I recommend recreating the documents from the Word, PDF,\nand txt Paestum content using either the UnstructuredLoader or the\nDirectoryLoader. If you choose to do so, you'll need to install the related\npackage and refer to the documentation on the LangChain website:\npip install \"unstructured[all-docs]\"\nThe LangChain framework provides numerous loaders for retrieving content\nfrom diverse data sources. I highly encourage you to explore the list and\nexperiment with any loaders that pique your interest:\nhttps://python.langchain.com/docs/integrations/document_loaders\n3.2.5 Progressively refining the final summary\nNow that everything is set up, you can create a chain to generate the final\nsummary step by step. Begin by importing the necessary modules:\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import PromptTemplate\nimport getpass\nNext, capture the OPENAI_API_KEY and set up the LLM model as before:\nOPENAI_API_KEY = getpass.getpass('Enter your OPENAI_API_KEY')\n",
      "content_length": 1483,
      "extraction_method": "Direct"
    },
    {
      "page_number": 97,
      "chapter": null,
      "content": "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,model_name=\"gpt-4o\nNow, define the chain, with related prompt, for summarizing individual\ndocuments:\ndoc_summary_template = \"\"\"Write a concise summary of the followin\n{text}\nDOC SUMMARY:\"\"\"\ndoc_summary_prompt = PromptTemplate.from_template(doc_summary_tem\ndoc_summary_chain = doc_summary_prompt | llm\nNext, set up the chain for refining the summary by iteratively combining the\ncurrent summary with the summary of an additional document:\nrefine_summary_template = \"\"\"\nYour must produce a final summary from the current refined summar\nwhich has been generated so far and from the content of an additi\nThis is the current refined summary generated so far: {current_re\nThis is the content of the additional document: {text}\nOnly use the content of the additional document if it is useful, \notherwise return the current full summary as it is.\"\"\"\nrefine_summary_prompt = PromptTemplate.from_template(refine_summa\nrefine_chain = refine_summary_prompt | llm | StrOutputParser()\nFinally, define a function that loops over each document, summarizes it using\nthe doc_summary_chain, and refines the overall summary using the\nrefine_chain:\ndef refine_summary(docs):\n   intermediate_steps = []\n   current_refined_summary = ''\n   for doc in docs:\n       intermediate_step = \\\n          {\"current_refined_summary\": current_refined_summary, \n           \"text\": doc.page_content}\n       intermediate_steps.append(intermediate_step)\n       \n       current_refined_summary = refine_chain.invoke(intermediat\n       \n   return {\"final_summary\": current_refined_summary,\n           \"intermediate_steps\": intermediate_steps}\n",
      "content_length": 1649,
      "extraction_method": "Direct"
    },
    {
      "page_number": 98,
      "chapter": null,
      "content": "You can now start the summarization process by calling refine_summary()\non your prepared document list:\nfull_summary = refine_summary(all_docs)\nPrinting the full_summary object will show the final summary under\nfinal_summary and the intermediate steps under intermediate_steps.\nAlthough the results steps are shortened for convenience, I encourage you to\nobserve how the summary evolves at each stage:\nprint(full_summary )\nHere is an excerpt from the output:\n{'final_summary': \"**Final Summary:**\\n\\nPaestum, an ancient Gree\nWe have covered a few summarization techniques, so let’s pause briefly to\nreflect on what we have learnt.\n3.3 Summarization flowchart\nTo wrap up this chapter, I’ve included a flowchart to help you choose the\nmost appropriate summarization technique for your specific needs:\nFigure 3.4 Summarization flowchart This flowchart guides you in selecting the right\nsummarization approach based on whether you need to summarize multiple unrelated documents\nand whether the input text fits within the context window\n",
      "content_length": 1032,
      "extraction_method": "Direct"
    },
    {
      "page_number": 99,
      "chapter": null,
      "content": "The first decision point\nmore documents 15 on Whether you need\nto summarize one or\nmultiple documents\n\nMute \\ No\n\nultiole |\n\nDocuments D> Split + Map Reduce\nThe second decision point 1~‘fag | Yes\n\ni$ on Whether the content\nto be summarized fits the\ncontext window\nNo\nMap Reduce or\na\n\nYes\n\n7,\n\nStuff prompt",
      "content_length": 305,
      "extraction_method": "OCR"
    },
    {
      "page_number": 100,
      "chapter": null,
      "content": "As shown in the flowchart in Figure 3.4, the first key decision is whether\nyou're summarizing one or multiple documents. If it's just one document and\nit fits within the context window, you can \"stuff\" the entire document into a\nsingle prompt for summarization. If it doesn't fit, use the map-reduce method.\nFor multiple documents, if they all fit within the context window, you can\nalso stuff them into a single prompt. If not, use map-reduce for a large\nnumber of documents, or the refine technique if you want to ensure the core\nof each document is included in the final summary.\n3.4 Summary\nSummarization is a great starting point for building LLM applications\ndue to its straightforwardness.\nThe choice of technique depends on whether you're summarizing a\nsingle document or multiple documents and if the text fits within the\ncontext window.\nTypically, the first step in summarization involves loading each raw text\nsource into a LangChain Document object.\nFor many summarization tasks, a map-reduce approach is effective. This\nmethod summarizes each document or chunk individually in the \"map\"\nstage, then combines these summaries in the \"reduce\" stage to create a\nfinal summary.\nWhen summarizing multiple documents, load each document with the\nappropriate loader, summarize them individually, and progressively\nrefine the summary by integrating each document’s summary until all\nare included.\nMap-reduce is suited for large text summaries where some content loss\nis acceptable to handle the processing load, while the refine technique\nensures the essence of each part is fully retained in the final summary.\n",
      "content_length": 1615,
      "extraction_method": "Direct"
    },
    {
      "page_number": 101,
      "chapter": null,
      "content": "4 Building a research\nsummarization engine\nThis chapter covers\nExplaining what a research summarization engine is.\nOrganizing core functionality, such as web searching and scraping\nfunctions.\nUsing prompt engineering for creating web searches and summarizing\nresults.\nStructuring the process into individual LangChain chains.\nIntegrating various sub-chains into a comprehensive main chain.\nApplying advanced LCEL features for parallel processing.\nBuilding on the content summarization techniques from chapter 3, this\nchapter will guide you through creating a research summarization engine.\nThis LLM application will process user queries, perform web searches, and\ncompile a comprehensive summary of the findings. We'll develop this project\nstep by step, starting with the basics and gradually increasing in complexity.\nAlong the way, you will deepen your knowledge of LangChain as I introduce\ncreating LLM chains with LangChain Expression Language (LCEL).\n4.1 Overview of a research summarization engine\nImagine you're researching various topics, like an NBA player, a tourist\ndestination, or whether to invest in a stock. Manually, you'd perform a web\nsearch, sift through results, read related pages, take notes, and compile a\nsummary. A modern approach is to let an LLM handle this work. You could\ncopy text from each web page, paste it into a ChatGPT prompt for\nsummarization, and repeat for multiple pages. Then, combine these\nsummaries into a final prompt for a consolidated summary (see figure 4.1).\nFigure 4.1 Semi-automated summarization with LLM: You prompt ChatGPT to summarize each\n",
      "content_length": 1594,
      "extraction_method": "Direct"
    },
    {
      "page_number": 102,
      "chapter": null,
      "content": "web search result and then compile them into a consolidated summary.\nA more efficient method is to develop a fully automated research\nsummarization engine. This engine can perform web searches, summarize the\n",
      "content_length": 208,
      "extraction_method": "Direct"
    },
    {
      "page_number": 103,
      "chapter": null,
      "content": "results, and compile a final report automatically (see figure 4.2). It's a\nvaluable tool for handling any research query.\nFigure 4.2 Automated research summarization engine: Ask a question, and the engine performs a\nweb search, returns URLs, scrapes and summarizes web pages, and compiles a research report\nfor you.\n",
      "content_length": 316,
      "extraction_method": "Direct"
    },
    {
      "page_number": 104,
      "chapter": null,
      "content": "We'll build this engine using LangChain. First, we'll implement web\nsearching and scraping, then set up the OpenAI LLM model for\nsummarization, and finally integrate all components into a Python\napplication. Initially, it will run as an executable and later if you wish, you\ncould expose it as a REST API.\n4.2 Setting up the project\nI am assuming you're using Visual Studio Code with the free Python\nextension and running on Windows, though you can opt for alternative\nPython IDEs like PyCharm. If you're new to Visual Studio Code, get it set up\nby following the sidebar instructions.\nInstalling Visual Studio Code and the Python extension\nDownload and install the appropriate version of Visual Studio Code for your\nOS from the official website:\nhttps://code.visualstudio.com/download\nFigure 4.3 Screenshot of the Extensions marketplace panel: 1) Select the Extensions menu; 2)\nenter python in the search box; 3) Select Python (from Microsoft) and click Install.\n",
      "content_length": 963,
      "extraction_method": "Direct"
    },
    {
      "page_number": 105,
      "chapter": null,
      "content": "Once installed, open Visual Studio Code, and click the Extensions icon on\nthe left-hand menu. Then, search for Python, select the Python extension\n(from Microsoft), and click install.\nI'll briefly guide you through setting up a Python project in Visual Studio\n",
      "content_length": 260,
      "extraction_method": "Direct"
    },
    {
      "page_number": 106,
      "chapter": null,
      "content": "Code, creating a virtual environment, activating it, and installing necessary\npackages.\nUsing your file explorer or shell, create an empty folder named ch04 in your\nsource code area. For example:\nC:\\Github\\building-llm-applications\\ch04\nOpen Visual Studio Code, then choose File > Open Folder, navigate to the\nch04 folder, and click Select Folder.\nOpen a terminal within Visual Studio Code by selecting Terminal > New\nTerminal.\nThe terminal should display the path to the folder you just created. On\nWindows, you'll see something like this:\nPS C:\\Github\\building-llm-applications\\ch04>\nIf you've just installed Visual Studio Code or you're new to it, enable the\nterminal by running this command (you only need to do it once):\nPS C:\\Github\\building-llm-applications\\ch04> Set-ExecutionPolicy \nWithin this terminal, as usual, create a virtual environment, activate it, and\ninstall the required Python packages (I'm omitting the full path to ch04 for\nconvenience):\n... ch04> python -m venv env_ch04\n... ch04> .\\env_ch04\\Scripts\\activate\nIf you activate the virtual environment in PowerShell using activate.ps1,\nyou may see a prompt asking: “Do you want to run software from this\nuntrusted publisher?” Respond by typing A (Always run) to proceed.\nIf you have cloned this repository from GitHub, you can install the required\npackages with pip install -r requirements.txt, otherwise:\n(env_ch04) ... ch04> pip install langchain langchain_openai langc\n",
      "content_length": 1444,
      "extraction_method": "Direct"
    },
    {
      "page_number": 107,
      "chapter": null,
      "content": "Once the required packages are installed, I recommend creating a custom Run\nconfiguration to ensure you're running and debugging your code within the\nenv_ch04 virtual environment you just set up.\nTo create the configuration:\n1. In the top menu, go to Run > Open Configurations.\n2. This will open the launch.json file in the editor.\n3. Replace or add the following configuration:\n{\n   \"version\": \"0.2.0\",\n   \"configurations\": [\n       {\n           \"name\": \"Python Debugger: Current File\",\n           \"type\": \"debugpy\",\n           \"request\": \"launch\",\n           \"program\": \"${file}\",\n           \"console\": \"integratedTerminal\",\n           \"python\": \"${workspaceFolder}/env_ch04/Scripts/python\n       }\n   ]\n}\n\"${workspaceFolder}/env_ch04/bin/python\"\nNote\nIf you're using macOS or Linux, replace the path in the \"python\" field with:\nThis custom Run configuration ensures your code runs with the correct\ninterpreter and environment. You'll use it later for debugging. With\neverything in place, you're now ready to start coding!\n4.3 Implementing the core functionality\nLooking again at figure 4.2, it's clear that our research summarization engine\ndepends on two key capabilities we must provide: one for conducting web\nsearches and another for extracting text from related web pages. Additionally,\nyou'll need a utility function to initialize an instance of the LLM client you'd\nlike to use.\n",
      "content_length": 1389,
      "extraction_method": "Direct"
    },
    {
      "page_number": 108,
      "chapter": null,
      "content": "4.3.1 Implementing web searching\nWe'll use the LangChain wrapper for the DuckDuckGo search engine to\nperform web searches. Its results method returns a list of objects, each\ncontaining the result URL in a property called \"link\".\nAdd a new empty file named web_searching.py to the project and fill it with\nthe code below:\nfrom langchain_community.utilities import DuckDuckGoSearchAPIWrap\nfrom typing import List\ndef web_search(web_query: str, num_results: int) -> List[str]:\n   return [r[\"link\"] for r in DuckDuckGoSearchAPIWrapper().resul\nCreate a separate Python file, such as web_searching_try.py, to test the\nsearch function:\nfrom web_searching import web_search\nresult = web_search(web_query=\"How many titles did Michael Jordan \nprint(result)\nNOTE\nIf you're unfamiliar with Visual Studio Code, you can execute the code above\nby pressing F5 and then selecting Python Debugger > Python File. You can\nalso set breakpoints and run the code step by step by pressing F10 on each\nline.\nIn the terminal, you'll get a list of URLs like the following ones, representing\nthe results of your search.\n['https://en.wikipedia.org/wiki/List_of_career_achievements_by_Mi\nNote\nOther web search engine wrappers provided by LangChain are\nTavilySearchResults and GoogleSearchAPIWrapper. Both require an API\nkey, so I chose DuckDuckGoSearchAPIWrapper because it doesn't.\n",
      "content_length": 1353,
      "extraction_method": "Direct"
    },
    {
      "page_number": 109,
      "chapter": null,
      "content": "4.3.2 Implementing web scraping\nWe'll scrape the web pages from the result list using Beautiful Soup, which is\na web scraper library. Place the code shown in the listing below in a file\nnamed web_scraping.py:\nListing 4.1 web_scraping.py\nimport requests\nfrom bs4 import BeautifulSoup\ndef web_scrape(url: str) -> str:\n   try:\n       response = requests.get(url)\n       if response.status_code == 200:\n           soup = BeautifulSoup(response.text, \"html.parser\")\n           page_text = soup.get_text(separator=\" \", strip=True)\n           return page_text\n       else:\n           return f\"Could not retrieve the webpage: Status code \n   except Exception as e:\n       print(e)\n       return f\"Could not retrieve the webpage: {e}\"\nLet's try out the function by placing the code below in a file named\nweb_scraping_try.py:\nfrom web_scraping import web_scrape\nresult = web_scrape('https://en.wikipedia.org/wiki/List_of_career_\nprint(result)\nAfter you run this script, the output will be similar to the following excerpt:\nList of career achievements by Michael Jordan - Wikipedia Jump to \nAs you can see, much of the scraped content is not relevant, but don't worry\nfor now; the LLM will extract the relevant bits we're interested in.\n4.3.3 Instantiating the LLM client\n",
      "content_length": 1261,
      "extraction_method": "Direct"
    },
    {
      "page_number": 110,
      "chapter": null,
      "content": "For this use case, we'll use the OpenAI GPT-4o-mini model instead of GPT-\n4o. Create a function to instantiate the OpenAI client as follows and place it\nin a file named llm_models.py (replace 'YOUR_OPENAI_API_KEY' with\nyour OpenAI key):\nfrom langchain_openai import ChatOpenAI\nopenai_api_key = 'YOUR_OPENAI_API_KEY'\ndef get_llm():\n   return ChatOpenAI(openai_api_key=openai_api_key,\n                model_name=\"gpt-4o-mini\")\nNote\nThere are various ways to store secrets like the OpenAI key in Visual Studio\nCode, each with its pros and cons. I'm hardcoding the OpenAI key inline\nhere, but you're free to implement it with your preferred approach.\n4.3.4 JSON to Python object converter\nLet's make a utility function that converts JSON text from the LLM into a\nPython object, usually a dictionary or sometimes a list. If the JSON is\nmalformed, it will return an empty dictionary. Add the following code to a\nfile called utilities.py:\nimport json \ndef to_obj(s):\n   try:\n       return json.loads(s)\n   except Exception:\n       return {}\nTo recap: we've set up a Visual Studio Code project and implemented the\ncore capabilities we need. Before building the engine to orchestrate web\nsearching, web scraping, and summarization requests to the LLM, let's step\nback and take a second look at the big picture.\n4.4 Enhancing the architecture with query\nrewriting\n",
      "content_length": 1354,
      "extraction_method": "Direct"
    },
    {
      "page_number": 111,
      "chapter": null,
      "content": "Let's revisit the diagram in Figure 4.2, which I've included again for\nconvenience as Figure 4.4 below\nFigure 4.4 Automated research summarization engine: Ask a question, and the engine performs a\nweb search, returns URLs, scrapes and summarizes web pages, and compiles a research report\nfor you.\n",
      "content_length": 297,
      "extraction_method": "Direct"
    },
    {
      "page_number": 112,
      "chapter": null,
      "content": "If you look closely at the architecture diagram in Figure 4.4, you might notice\na potential improvement: instead of sending the original research request\ndirectly to the web search engine, you can use the LLM to create multiple\nsearch queries. This technique, called \"query rewriting\" or \"multiple query\ngeneration,\" is similar to a method used to enhance RAG searches, which I'll\ncover in later chapters. Rewriting the original question into multiple queries\noffers several advantages. It can clarify ambiguous or unclear queries, fix\ngrammar or syntax errors that could confuse the search engine, and add\ncontext to short queries to improve search results. For complex queries,\nbreaking them into simpler queries provides useful context for better answers.\nThis is the key reason for rewriting a single query into multiple targeted\nsearches.\nFor instance, rather than querying the search engine \"How many titles did\nMichael Jordan win?\", you can prompt ChatGPT as follows:\nGenerate three web search queries to research the following topic, aiming for\na comprehensive report: \"How many titles did Michael Jordan win?\"\nYou'll receive queries like these:\n \n1. \"Michael Jordan NBA championships total\"\n2. \"List of NBA titles won by Michael Jordan\"\n3. \"Michael Jordan basketball career championships count\"\nConsequently, the engine will conduct research by performing these three\nweb searches and gathering results from each. This updated workflow is\ndepicted in the diagram in figure 4.5.\nFigure 4.5 Revised system architecture diagram, incorporating query rewriting: The process\nbegins by tasking the LLM to generate a specified number of queries based on the user's research\nquestion. These queries are then submitted to the search engine. The subsequent processing\nremains consistent with the illustration in figure 4.2.\n",
      "content_length": 1822,
      "extraction_method": "Direct"
    },
    {
      "page_number": 113,
      "chapter": null,
      "content": "4. Web searches\n\nThe engine performs\nthe rewritten web\nsearches\n\n)\n\n>\n1, Research request\n\nA\n\n—\n\nUser 10. Research repo\n\nsummarization engine\n\n™~\n\nAutomated\nSummarization\nEngine\n\n5a. Search #1 results\n\nABB\n\n5b. Search #n results\n\nO0\n\n3, Rewritlen web searches\n\n2. Request query rewrite\n\n§. Summarize search results\n\n7. Summaries\n\nAE EEE)\n\n————————— +}\n} 8. Compile research report\nqe\n\n9 Research report\n\ns The LLM compiles a\n\nresearch report from the\nsearch summaries\n\nThe engine requests the\nLLM to rewrite the user's\nresearch request into\nmultiple web searches\n\nE}SIEMEIEIE)\nBARBER\n\nLLM client",
      "content_length": 595,
      "extraction_method": "OCR"
    },
    {
      "page_number": 114,
      "chapter": null,
      "content": "Technically, the updated architecture is more complex, which could raise\nconcerns about performance due to the additional web searches. If processed\nsequentially, the response time would increase linearly; for instance, running\nfour web searches would take four times as long. However, you can speed up\nprocessing through parallelization, which I’ll cover in later sections. For now,\nwe’ll start with a sequential processing approach and explore parallelization\nlater.\n4.5 Prompt engineering\nBefore we assemble the building blocks from section 4.3, we need to focus\non some prompt engineering. We’ll create prompts for generating web search\nqueries, summarizing individual web pages, and producing the final report.\nLet’s tackle each of these tasks step by step.\n4.5.1 Crafting Web Search Prompts\nImagine a user asks a finance-related question like, \"Should I invest in Apple\nstocks?\" When guiding the LLM to generate web search queries based on this\nquestion, it's helpful to specify the persona that should frame these queries.\nFor example, you might begin the prompt with: \"You are an experienced\nfinance analyst AI assistant. Your objective is to create detailed, insightful,\nunbiased, and well-structured financial reports based on the provided data\nand trends.\"\nTo generate these instructions dynamically based on the research question,\nyou'll use a dedicated prompt. Start by designing a prompt that selects a\nsuitable research assistant and provides instructions tailored to the user's\nquery.\nCreate a file named prompts.py and begin with the following code:\nListing 4.2 prompts.py: Prompts to select and generate assistant instructions\nfrom langchain.prompts import PromptTemplate\nASSISTANT_SELECTION_INSTRUCTIONS = \"\"\"\n",
      "content_length": 1729,
      "extraction_method": "Direct"
    },
    {
      "page_number": 115,
      "chapter": null,
      "content": "You are skilled at assigning a research question to the correct r\nThere are various research assistants available, each specialized \nEach assistant is identified by a specific type. Each assistant h\nHow to select the correct assistant: you must select the relevant \n------\nHere are some examples on how to return the correct assistant inf\nExamples:\nQuestion: \"Should I invest in Apple stocks?\"\nResponse: \n{{\n   \"assistant_type\": \"Financial analyst assistant\",\n   \"assistant_instructions\": \"You are a seasoned finance analyst \n   \"user_question\": {user_question}\n}}\nQuestion: \"what are the most interesting sites in Tel Aviv?\"\nResponse: \n{{\n   \"assistant_type\": \"Tour guide assistant\",\n   \"assistant_instructions\": \"You are a world-travelled AI tour \n   \"user_question\": \"{user_question}\"\n}}\nQuestion: \"Is Messi a good soccer player?\"\nResponse: \n{{\n   \"assistant_type\": \"Sport expert assistant\",\n   \"assistant_instructions\": \"You are an experienced AI sport as\n   \"user_question\": \"{user_question}\"\n}}\n------\nNow that you have understood all the above, select the correct re\nQuestion: {user_question}\nResponse:\n\"\"\" \nASSISTANT_SELECTION_PROMPT_TEMPLATE = PromptTemplate.from_templat\n   template=ASSISTANT_SELECTION_INSTRUCTIONS\n)\nThis prompt template has a couple of key features. First, it's a \"few-shot\nprompt,\" as demonstrated by the three examples included, which help the\nLLM grasp our specific requirements. Second, it directs the LLM to return\n",
      "content_length": 1449,
      "extraction_method": "Direct"
    },
    {
      "page_number": 116,
      "chapter": null,
      "content": "results in JSON format, making it straightforward to convert the output into a\nPython dictionary for further processing.\nWith this prompt in place to select the appropriate assistant type and provide\ninstructions, you can now proceed to create the prompt for generating web\nsearches based on the user's question, as shown in the following listing.\nListing 4.3 prompts.py: Prompt to rewrite the user query into multiple web searches\nWEB_SEARCH_INSTRUCTIONS = \"\"\"\n{assistant_instructions}\nWrite {num_search_queries} web search queries to gather as much i\non the following question: {user_question}. Your objective is to w\nYou must respond with a list of queries such as query1, query2, q\n[\n   {{\"search_query\": \"query1\", \"user_question\": \"{user_question}\n   {{\"search_query\": \"query2\", \"user_question\": \"{user_question}\n   {{\"search_query\": \"query3\", \"user_question\": \"{user_question}\n]\n\"\"\"\nWEB_SEARCH_PROMPT_TEMPLATE = PromptTemplate.from_template(\n   template=WEB_SEARCH_INSTRUCTIONS\n)\nThe {assistaint_instructions} placeholder will be filled with the\n\"assistant_instructions\" output from the previous assistant selection\nprompt you saw. This prompt also returns results in JSON format, making it\neasy to convert the output into a list of Python dictionaries for further\nprocessing.\nIncluding the user question in the output ensures continuity throughout the\nprocess, allowing us to use it as needed, especially in the chain-based code\nthat follows.\nWith the web search prompt completed, let's move on to creating the\nsummarization prompts.\n4.5.2 Crafting Summarization Prompts\n",
      "content_length": 1578,
      "extraction_method": "Direct"
    },
    {
      "page_number": 117,
      "chapter": null,
      "content": "The prompt for summarizing result pages closely resembles prompts from the\nprevious chapter on summarization:\nListing 4.4 prompts.py: Prompt for summarizing result pages\nSUMMARY_INSTRUCTIONS = \"\"\"\nRead the following text:\nText: {search_result_text} \n-----------\nUsing the above text, answer in short the following question.\nQuestion: {search_query}\nIf you cannot answer the question above using the text provided a\nInclude all factual information, numbers, stats etc if available.\n\"\"\"\nSUMMARY_PROMPT_TEMPLATE = PromptTemplate.from_template(\n   template=SUMMARY_INSTRUCTIONS\n)\n4.5.3 Research Report prompt\nSimilarly, the prompt for generating the research report is straightforward:\nListing 4.5 prompts.py: Prompt for generating the research report\n# Research Report prompts adapted from https://github.com/assafel\nRESEARCH_REPORT_INSTRUCTIONS = \"\"\"\nYou are an AI critical thinker research assistant. Your sole purp\nInformation: \n--------\n{research_summary}\n--------\nUsing the above information, answer the following question or top\nThe report should focus on the answer to the question, should be w\nin depth, with facts and numbers if available and a minimum of 1,\nYou should strive to write the report as long as you can using al\nYou must write the report with markdown syntax.\nYou MUST determine your own concrete and valid opinion based on t\n",
      "content_length": 1345,
      "extraction_method": "Direct"
    },
    {
      "page_number": 118,
      "chapter": null,
      "content": "Write all used source urls at the end of the report, and make sur\nYou must write the report in apa format.\nPlease do your best, this is very important to my career.\"\"\" \nRESEARCH_REPORT_PROMPT_TEMPLATE = PromptTemplate.from_template(\n   template=RESEARCH_REPORT_INSTRUCTIONS\n)\n4.6 Initial implementation\nIn this section, I'll walk you through the initial implementation of our\nresearch summarization engine, following the steps laid out in the\narchitectural diagram in Figure 4.3.\n4.6.1 Importing Functions and Prompt Templates\nTo kick things off, create a Python file named research_engine_seq.py and\nbegin importing the functions and prompt templates you've crafted in the\npreceding sections.\nfrom web_searching import web_search\nfrom web_scraping import web_scrape\nfrom llm_models import get_llm\nfrom utilities import to_obj\nfrom prompts import (\n   ASSISTANT_SELECTION_PROMPT_TEMPLATE,\n   WEB_SEARCH_PROMPT_TEMPLATE,\n   SUMMARY_PROMPT_TEMPLATE,\n   RESEARCH_REPORT_PROMPT_TEMPLATE\n)\n4.6.2 Setting constants and input variables\nNow, let's establish some constants to configure the application. The\naccuracy and diversity of the report will depend on the number of web\nsearches and results per search you set. However, it's important to consider\nyour budget when setting these values. For instance, configuring 4 web\nsearches with 5 results per search would entail summarizing 20 web pages,\nwhich means roughly 2000 tokens per page, for a total of 20 x 2000 = 40,000\ntokens. With the OpenAI GPT4o Mini model costing around $0.0006 per\n",
      "content_length": 1535,
      "extraction_method": "Direct"
    },
    {
      "page_number": 119,
      "chapter": null,
      "content": "1,000 output tokens, this would amount to approximately $0.01 per research\nrequest. I'll set lower configuration numbers, but feel free to adjust them as\nneeded:\nNUM_SEARCH_QUERIES = 2\nNUM_SEARCH_RESULTS_PER_QUERY = 3\nRESULT_TEXT_MAX_CHARACTERS = 10000\nThe sole input variable in the application is the one that captures the research\nquestion from the user:\nquestion = 'What can I see and do in the Spanish town of Astorga?\n4.6.3 Instantiating the LLM client\nInstantiating the LLM client is straightforward. You can do it simply as\nfollows:\nllm = get_llm()\n4.6.4 Generating the web searches and collecting the results\nIn the process of generating web searches and collecting results, the first step\nis to execute the LLM prompt to determine the correct research assistant and\nrelated instructions based on the user's research question:\nassistant_selection_prompt = ASSISTANT_SELECTION_PROMPT_TEMPLATE.\nassistant_instructions = llm.invoke(assistant_selection_prompt)\nTo execute this code, place a breakpoint on the line llm = get_llm(). If\nyou're new to Visual Studio Code, you can set a breakpoint by clicking to the\nleft of the line number where you want the debugger to pause.\nNext, start the debugger by clicking the Run & Debug icon on the left sidebar\n(or use the shortcut Ctrl+Shift+D on Windows). From the dropdown at the\ntop, select the Run configuration named \"Python Debugger: Current File\",\nwhich you created at the end of Section 4.2. Then, click the play button next\nto it to start debugging.\n",
      "content_length": 1506,
      "extraction_method": "Direct"
    },
    {
      "page_number": 120,
      "chapter": null,
      "content": "Once the code runs and hits the breakpoint, inspect the value of the\nassistant_instructions variable in the \"Variables\" panel at the top left of\nthe screen. Alternatively, you can print the value manually in the “Debug\nConsole” panel at the bottom of the VS Code screen):\nprint(assistant_instructions)\nYou will observe the following output:\ncontent='{\\n    \"assistant_type\": \"Tour guide assistant\",\\n    \"a\nThe relevant information is in the content property. To convert it into a\nPython object, you can proceed as follows:\nassistant_instructions_dict = to_obj(assistant_instructions.conte\nPrinting the assistant_instructions_dict variable will yield the following\noutput:\n{'assistant_type': 'Tour guide assistant', 'assistant_instruction\nNow, you can execute the prompt to generate web searches based on the\noriginal user research question:\nweb_search_prompt = WEB_SEARCH_PROMPT_TEMPLATE.format(assistant_i\n                                                     num_search_\n                                                     user_questi\nweb_search_queries = llm.invoke(web_search_prompt)\nweb_search_queries_list = to_obj(web_search_queries.content.repla\nThe primary input for this prompt is the assistant_instructions output\nfrom the previous step. If you were to execute the code you’ve just written,\nupon printing web_search_queries_list, you would obtain a list of search\nqueries like this:\n[{'search_query': 'Astorga attractions', 'user_question': 'What c\nsee and do in the Spanish town of Astorga?'}]\nHere's how you can fetch web searches using the web_search() function:\nsearches_and_result_urls = [{'result_urls': web_search(web_query=w\n                                    num_results=NUM_SEARCH_RESUL\n",
      "content_length": 1710,
      "extraction_method": "Direct"
    },
    {
      "page_number": 121,
      "chapter": null,
      "content": "                          'search_query': wq['search_query']} \n                          for wq in web_search_queries_list]\nExecuting up to this point, the searches_and_result_urls variable would\nhold a list of Python dictionaries like this:\n[{'result_urls': ['https://igotospain.com/one-day-in-astorga-on-t\nEach dictionary shows a search query and its corresponding result URLs (3\nfor each query here). The next step is to flatten the results, so each dictionary\ncontains a search query and just one result URL:\nsearch_query_and_result_url_list = []\nfor qr in searches_and_result_urls:\n   search_query_and_result_url_list.extend([{'search_query': qr[\n                                   'result_url': r\n                                   } for r in qr['result_urls']]\nNow, search_query_and_result_url_list has 9 dictionaries, just as expected:\n[{'search_query': 'Astorga attractions', 'result_url': 'https://ww\nWith the web search queries and all related result URLs ready, the next move\nis to start scraping the web pages linked to these URLs.\n4.6.5 Scraping the web results\nUse the web_scrape() function to pull text from web pages linked through\nyour search results:\nresult_text_list = [ {'result_text': web_scrape(url=re['result_ur\n                    'result_url': re['result_url'],\n                    'search_query': re['search_query']}\n                  for re in search_query_and_result_url_list]\nThis code populates result_text_list with 9 dictionaries, each carrying\ntext from a web page:\n[{'result_text': 'Astorga, Spain - WorldAtlas Astorga, Spain Asto\n{'result_text': \"Astorga, Spain: Uncovering the Jewels of a Hidde\nMuseum and Gaudi's Palace… ', 'result_url': 'https://citiesandatt\n",
      "content_length": 1698,
      "extraction_method": "Direct"
    },
    {
      "page_number": 122,
      "chapter": null,
      "content": "The next move is to summarize the information collected from each webpage.\n4.6.6 Summarizing the web results\nYou'll ask the language model to summarize the text from each web page\nusing the SUMMARY_PROMPT_TEMPLATE you set up earlier. This\nsummary will also keep the original search queries and URLs, which you'll\nneed later:\nresult_text_summary_list = []\nfor rt in result_text_list: \n   summary_prompt = SUMMARY_PROMPT_TEMPLATE.format(\n       search_result_text=rt['result_text'], \n       search_query=rt['search_query'])\n   \n   text_summary = llm.invoke(summary_prompt)\n   result_text_summary_list.append({'text_summary': text_summary\n                        'result_url': rt['result_url'],\n                        'search_query': rt['search_query']})\nThis process results in result_text_summary_list, a list of 9 dictionaries.\nEach contains a summary of the scraped text, the URL where it was found,\nand the search query used to find it:\n[{'text_summary': '\\nAstorga is a municipality in Northwestern Sp\nWith these summaries, you've compiled all the information needed for the\nfinal research report.\n4.6.7 Generating the research report\nLet's review the prompt used to create the final report:\nRESEARCH_REPORT_INSTRUCTIONS = \"\"\"\nYou are an AI critical thinker research assistant. Your sole purp\nInformation: \n--------\n{research_summary}\n--------\n",
      "content_length": 1348,
      "extraction_method": "Direct"
    },
    {
      "page_number": 123,
      "chapter": null,
      "content": "Using the above information, answer the …\n…\n\"\"\"\nLet's prepare the final report by combining summaries and URLs into a\nformat the prompt template expects. Here’s how.\nFirst, transform each dictionary into a string with the summary and its source\nURL:\nstringified_summary_list = [f'Source URL: {sr[\"result_url\"]}\\nSum\n                           for sr in result_text_summary_list]\nInspecting stringified_summary_list, you’ll find entries like these:\n['Source URL: https://www.worldatlas.com/cities/astorga-spain.htm\nthe Maragatería … pilgrims on the Camino de Santiago. ', 'Source \nNext, combine all the summary strings into one:\nappended_result_summaries = '\\n'.join(stringified_summary_list)\nThis gives you a single text block with all summaries and URLs:\nSource URL: https://www.worldatlas.com/cities/astorga-spain.html\nSummary:\nAstorga is a municipality in Northwestern Spain with a population \nSource URL: https://citiesandattractions.com/spain/astorga-spain-\nSummary:\nSome of the main attractions in Astorga, Spain include the Episco\nNow, use this content with the RESEARCH_REPORT_INSTRUCTIONS prompt\ntemplate to get the final research report:\nresearch_report_prompt = RESEARCH_REPORT_PROMPT_TEMPLATE.format(\n   research_summary=appended_result_summaries,\n   user_question=question\n)\nresearch_report = llm.invoke(research_report_prompt)\nprint(f'strigified_summary_list={stringified_summary_list}')\nprint(f'merged_result_summaries={appended_result_summaries}')\nprint(f'research_report={research_report}')\n",
      "content_length": 1508,
      "extraction_method": "Direct"
    },
    {
      "page_number": 124,
      "chapter": null,
      "content": "If you run the entire research_engine_seq.py script, you’ll receive a\ncomplete research report like the one below, based on web summaries after\nabout a minute:\n# Introduction\nAstorga is a charming town located in the northwestern region of \n# Historical and Cultural Significance\nAstorga is a town with a long and rich history, dating back to th\n[… SHORTENED]\nGreat job on completing your first automated web research! I recommend\ngoing through the code for this initial implementation again. If you didn't\nhave time to type and run it, be sure to check it out in the GitHub repository.\nYou'll likely understand it right away, but a review will help ensure\neverything is clear.\nThis initial version works well and generates the expected research report.\nHowever, it's a bit slow because it processes tasks sequentially. If we had set\nit up to handle 10 web searches with 10 results each, the time required would\nhave significantly increased.\nCan we make it faster? Absolutely. In the next section, we'll explore how to\ndo that by introducing the LangChain Expression Language (LCEL).\n4.7 Reimplementing the research summary engine\nin LCEL\nThe LangChain Expression Language (LCEL) offers a structured approach to\norganizing the core components of your LLM applications—such as web\nsearch, page scraping, and summarization—into an efficient chain or\npipeline. This framework not only simplifies the creation of complex\nworkflows from simple elements but also enhances them with advanced\nfeatures like streaming, parallel execution, and logging. While you got an\ninitial glimpse of LCEL in Chapter 4, this sidebar will provide a more in-\ndepth look at its full capabilities.\nLangChain Expression Language (LCEL)\n",
      "content_length": 1709,
      "extraction_method": "Direct"
    },
    {
      "page_number": 125,
      "chapter": null,
      "content": "For those developing LLM applications, using LCEL is highly recommended.\nIt allows you to interact with LLMs and chat models efficiently by creating\nand executing chains, providing several benefits:\nFallback: Enables adding a fallback action for error handling.\nParallel Execution: Executes independent chain components\nsimultaneously to boost performance.\nExecution Modes: Supports developing in synchronous mode and then\nswitching to streaming, batch, or asynchronous execution modes as\nneeded.\nLangSmith Tracing: Automatically logs execution steps when upgrading\nto LangSmith, facilitating debugging and monitoring.\nA chain follows the Runnable protocol, meaning it requires the\nimplementation of specific methods like invoke(), stream(), and batch(),\nincluding their asynchronous versions. LangChain's framework ensures that\nits components, such as PromptTemplate and JsonOutputFunctionsParser,\nadhere to these standards.\nFor detailed information and examples, visiting the official LCEL\ndocumentation is recommended, especially the How To and Cookbook\nsections: https://python.langchain.com/docs/expression_language/\nLCEL streamlines complex chain creation by offering a unified interface (the\nRunnable protocol), composition tools, and the ability to easily parallelize\nprocesses. Though mastering LCEL may require some practice, the effort is\nrewarding as it significantly enhances application performance and\nscalability.\nMy chain implementation strategy, shown in figure 4.6, involves constructing\na mini-chain for each processing step seen earlier and integrating these into a\nmaster Web Research chain.\nFigure 4.6 Architecture of the chain-based research summarization engine: Each step of the\nprocess is re-implemented as a mini-chain; all mini-chains are assembled into a master Web\nResearch chain.\n",
      "content_length": 1812,
      "extraction_method": "Direct"
    },
    {
      "page_number": 126,
      "chapter": null,
      "content": "This master Web Research chain handles the entire process, as shown in\nfigure 4.5, which illustrates the architecture of the chain-based research\nsummarization engine. Each processing step is implemented as a mini-chain,\nall integrated into the master Web Research chain:\nAssistant Instructions chain: This chain selects the best research\nassistant from available options to answer the user's question. It also\ncreates the system prompt that defines the assistant's skills and purpose.\nWeb Searches chain: This chain generates multiple web searches based\non the user's question. It provides context from different perspectives or\nbreaks down complex queries into simpler ones.\nSearch and Summarization chain: This chain performs web searches,\nretrieves URLs from search results, scrapes the relevant web pages, and\nsummarizes the content of each page.\nResearch Report chain: The final chain synthesizes the answer using the\noriginal question and the summaries generated from the search results.\nThe Search and Summarization chain, detailed in figure 4.7, is itself a\ncomposite of three chains: one for web search, one for scraping and\nsummarizing web pages, and one for compiling summaries into a single text\nblock.\nFigure 4.7 The Search and Summarization chain is made up of three chains: a web search chain,\n",
      "content_length": 1310,
      "extraction_method": "Direct"
    },
    {
      "page_number": 127,
      "chapter": null,
      "content": "a scraping and summarization chain, and a summary compilation chain.\nFigure 4.7 shows the Search and Summarization chain, which combines three\nseparate chains: one for web searches, another for scraping and summarizing\nweb content, and a third for merging these summaries into a single text block.\nReimplementing this process into chains, as shown in Figure 4.8, allows for\nparallel execution, significantly improving efficiency compared to the\nsequential approach in Figure 4.6.\nFigure 4.8 Chain architectural diagram showing how parallelization is applied in the research\nsummarization engine, highlighting the execution of separate chain instances in parallel for\nefficiency.\n",
      "content_length": 679,
      "extraction_method": "Direct"
    },
    {
      "page_number": 128,
      "chapter": null,
      "content": "Figure 4.7 highlights parallelization at two key stages:\n1. A separate instance of the Search and Summarization chain is created\nand executed simultaneously for each web search initiated by the Web\nSearches chain.\n2. For each search result generated by the Search Result URLs chain, an\nindividual Search Result Text and Summary chain instance is launched\nto run in parallel.\nWith this overview of the chain-based approach, we'll now explore the\nindividual \"mini chains,\" starting with the \"Assistant Instructions\" chain.\n4.7.1 Assistant Instructions chain\n",
      "content_length": 556,
      "extraction_method": "Direct"
    },
    {
      "page_number": 129,
      "chapter": null,
      "content": "To begin processing a research question, the first task is to determine the\nmost relevant research assistant and their prompt instructions. This is handled\nby the assistant_instructions_chain, as shown below. Place this code in\na file named chain_1_1.py:\nfrom llm_models import get_llm\nfrom prompts import (\n   ASSISTANT_SELECTION_PROMPT_TEMPLATE, \n)\nfrom langchain.schema.output_parser import StrOutputParser\nassistant_instructions_chain = (\n   ASSISTANT_SELECTION_PROMPT_TEMPLATE | get_llm() \n)\nFor those new to LCEL syntax, the flow here is straightforward: the selection\nprompt feeds into the LLM, which then selects a research assistant based on\nthe user's question.\nTo test this setup, use the following script, saved as chain_try_1_1.py:\nfrom chain_1_1 import assistant_instructions_chain\nquestion = 'What can I see and do in the Spanish town of Astorga?\nassistant_instructions = assistant_instructions_chain.invoke(ques\nprint(assistant_instructions)\nRunning this code will produce output similar to what was shown in section\n4.6.4, detailing the assistant type, instructions, and the user question (the\noutput will come after a few seconds, and the metadata might look slightly\ndifferent from what I have reported here):\ncontent='{\\n    \"assistant_type\": \"Tour guide assistant\",\\n    \"a\nWhile you could manually extract the needed output from the content\nproperty, using LCEL offers a more efficient approach. By adding a\nStrOutputParser() block to your chain, you can automatically extract the\nLLM's response directly from the content property. Update the chain\naccordingly and save it as chain_1_2.py.\nfrom llm_models import get_llm\n",
      "content_length": 1643,
      "extraction_method": "Direct"
    },
    {
      "page_number": 130,
      "chapter": null,
      "content": "from utilities import to_obj\nfrom prompts import (\n   ASSISTANT_SELECTION_PROMPT_TEMPLATE, \n)\nfrom langchain.schema.runnable import RunnablePassthrough\nfrom langchain.schema.output_parser import StrOutputParser\nassistant_instructions_chain = (\n   {'user_question': RunnablePassthrough()} \n   | ASSISTANT_SELECTION_PROMPT_TEMPLATE | get_llm() | StrOutput\n)\nThis updated version introduces several enhancements:\nWhile the ASSISTANT_SELECTION_PROMPT_TEMPLATE automatically\nmatches the input text question to the {user_question} field (refer back\nto section 4.5.1 for a refresh on {user_question}), it's safer to explicitly\nmap the input question to a user_question property in a Python\ndictionary using the RunnablePassthrough() function, which binds the\nexternal input to the user_question variable through an unnamed\nparameter. This iis also beneficial as this piece of information is\nimportant for subsequent steps.\nThe StrOutputParser() block extracts text from the LLM's content\nproperty, simplifying output handling.\nThe response is converted to a Python dictionary by to_obj(), making it\neasier to work with in the chain.\nTest the revised chain with this script, saved as chain_try_1_2.py:\nfrom chain_1_2 import assistant_instructions_chain\nquestion = 'What can I see and do in the Spanish town of Astorga?\nassistant_instructions_dict = assistant_instructions_chain.invoke\nprint(assistant_instructions_dict)\nRunning this should produce the expected output:\n{'assistant_type': 'Tour guide assistant', 'assistant_instruction\n4.7.2 Web Searches chain\n",
      "content_length": 1552,
      "extraction_method": "Direct"
    },
    {
      "page_number": 131,
      "chapter": null,
      "content": "After the LLM selects the appropriate research assistant and details their role,\nyou can prompt the LLM to generate web searches related to the user's query.\nUse the code below, saved as chain_2_1.py, for this step:\nListing 4.6 Web Searches chain for rewriting the user query into web searches\nfrom llm_models import get_llm\nfrom utilities import to_obj\nfrom prompts import (\n   WEB_SEARCH_PROMPT_TEMPLATE\n)\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain.schema.runnable import RunnableLambda\nNUM_SEARCH_QUERIES = 2\nweb_searches_chain = (\n   RunnableLambda(lambda x:\n       {\n           'assistant_instructions': x['assistant_instructions']\n           'num_search_queries': NUM_SEARCH_QUERIES,\n           'user_question': x['user_question']\n       }\n   )\n   | WEB_SEARCH_PROMPT_TEMPLATE | get_llm() | StrOutputParser() \n)\nThis implementation uses a RunnableLambda block to process input from the\nprevious chain, transforming it into the format needed by\nWEB_SEARCH_PROMPT_TEMPLATE. The chain then continues similarly to\nprevious examples.\nTo test this chain, you can use the following script, saved as chain_try_2_1:\nListing 4.7 Script to test the Web Searches chain\nfrom utilities import to_obj\nfrom chain_2_1 import web_searches_chain\n# test chain invocation\nassistant_instruction_str = '{\"assistant_type\": \"Tour guide assis\nassistant_instruction_dict = to_obj(assistant_instruction_str)\nweb_searches_list = web_searches_chain.invoke(assistant_instructi\nprint(web_searches_list)\n",
      "content_length": 1508,
      "extraction_method": "Direct"
    },
    {
      "page_number": 132,
      "chapter": null,
      "content": "This script uses a mock input in assistant_instruction_str to mimic the\noutput from the Assistant Instructions chain, testing the chain's response to\nthis input.\nExpected output:\n[{'search_query': 'Things to do in Astorga Spain', 'user_question\nWith the web searches now generated, you can now proceed to the Search\nand Summarization chain.\n4.7.3 Search and Summarization chain\nThe Search and Summarization chain is designed to perform a web search\nbased on a query from the previous chain, retrieve URLs from the search\nresults, scrape the corresponding web pages, and then summarize each page.\nAs shown in Figure 4.5, this process is broken down into smaller sub-chains:\nSearch result URLs chain\nSearch result Text and Summary chain\nJoined summary chain\nWe'll begin by building these sub-chains, starting with the Search Result\nURLs chain.\nSearch result URLs chain\nThis sub-chain carries out a web search, retrieving a specific number of\nURLs from the search results. Save the following code as chain_3_1.py:\nListing 4.8 Search and Summarization chain\nfrom web_searching import web_search\nfrom langchain.schema.runnable import RunnableLambda\nNUM_SEARCH_RESULTS_PER_QUERY = 3\nsearch_result_urls_chain = (\n   RunnableLambda(lambda x: \n",
      "content_length": 1235,
      "extraction_method": "Direct"
    },
    {
      "page_number": 133,
      "chapter": null,
      "content": "       [\n           {\n               'result_url': url, \n               'search_query': x['search_query'],\n               'user_question': x['user_question']\n           }\n           for url in web_search(web_query=x['search_query'], \n                                 num_results=NUM_SEARCH_RESULTS_\n       ]\n   )\n)\nA couple of key points to highlight:\nA lambda function is used to pass the web_query parameter to the\nweb_search() function. This function also formats the output as a list of\ndictionaries, each containing the resulting URL along with data from the\nprevious chain, such as the search query and the original user question.\nThese elements will be useful in subsequent stages.\nThe lambda function receives its input from the output of the previous\nchain, specifically the Web Searches chain.\nTo test this sub-chain, we'll use simulated input that mirrors what would\ncome from the Web Searches chain. Save the testing code in a file named\nchain_try_3_1.py:\nListing 4.9 Script to test the Search and Summarization chain\nfrom utilities import to_obj\nfrom chain_3_1 import search_result_urls_chain\n# test chain invocation\nweb_search_str = '{\"search_query\": \"Astorga Spain attractions\", \"\nweb_search_dict = to_obj(web_search_str)\nresult_urls_list = search_result_urls_chain.invoke(web_search_dic\nprint(result_urls_list)\nAfter executing it, you should see output similar to this:\n[{'result_url': 'https://loveatfirstadventure.com/astorga-spain/'\nSearch Result Text and Summary chain\n",
      "content_length": 1489,
      "extraction_method": "Direct"
    },
    {
      "page_number": 134,
      "chapter": null,
      "content": "The Search Result Text and Summary sub-chain handles a URL from the\nprevious sub-chain by:\nScraping the webpage's text using the provided URL.\nGenerating a summary of the scraped text.\nIncorporating the source URL into the summary.\nThese steps are effectively implemented in the following code, which should\nbe saved as chain_4_1.py:\nListing 4.10 Search Result Text and Summary chain\nfrom llm_models import get_llm\nfrom web_scraping import web_scrape\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain.schema.runnable import RunnableLambda, RunnablePar\nfrom prompts import (\n   SUMMARY_PROMPT_TEMPLATE\n)\nRESULT_TEXT_MAX_CHARACTERS = 10000\nsearch_result_text_and_summary_chain = (\n   RunnableLambda(lambda x:\n       {\n           'search_result_text': web_scrape(url=x['result_url'])\n           'result_url': x['result_url'], \n           'search_query': x['search_query'],\n           'user_question': x['user_question']\n       }\n   )\n   | RunnableParallel (\n       {\n           'text_summary': SUMMARY_PROMPT_TEMPLATE | get_llm() | \n           'result_url': lambda x: x['result_url'],\n           'user_question': lambda x: x['user_question']         \n       }\n   )\n   | RunnableLambda(lambda x: \n       {\n           'summary': f\"Source Url: {x['result_url']}\\nSummary: \n           'user_question': x['user_question']\n       }\n   ) \n)\n",
      "content_length": 1355,
      "extraction_method": "Direct"
    },
    {
      "page_number": 135,
      "chapter": null,
      "content": "The code follows the conventions established in previous chains, and you\nmight recall the purpose of RunnableParallel from Chapter 3. The\nRunnableParallel block allows for the simultaneous execution of an inner\nchain (next to text_summary) along with other operations (in this case, two\nlambda functions) that depend on the same input, which comes from the\ninitial RunnableLambda block.\nTo execute this chain, save the following script as chain_try_4_1.py:\nListing 4.11 Script to test the Search Result Text and Summary chain\nfrom utilities import to_obj\nfrom chain_4_1 import search_result_text_and_summary_chain\n# test chain invocation\nresult_url_str = '{\"result_url\": \"https://citiesandattractions.co\nresult_url_dict = to_obj(result_url_str)\nsearch_text_summary = search_result_text_and_summary_chain.invoke\nprint(search_text_summary)\nYou will get output similar to this:\n{'summary': 'Source Url: https://citiesandattractions.com/spain/a\nAssembling the Search and Summarization chain\nWe've now assembled all the key components to build the Search and\nSummarization chain. Before diving into the LCEL implementation, take a\nmoment to review Figure 4.9. This diagram provides a clearer understanding\nof the process compared to the earlier figure 4.7.\nFigure 4.9 Enhanced Search and Summarization chain diagram: The Search Result URLs chain\ngenerates multiple URLs; each URL initiates an instance of the Result Text and Summary chain,\nwhich in turn produces a corresponding summary. These summaries are then consolidated into a\nsingle text string by the Joined Summary chain.\n",
      "content_length": 1576,
      "extraction_method": "Direct"
    },
    {
      "page_number": 136,
      "chapter": null,
      "content": "As illustrated in figure 4.9, the Search Result URLs chain generates multiple\nURLs, each of which triggers an instance of the Result Text and Summary\nchain. These instances run in parallel, each producing a summary for its\nrespective web page. The Joined Summary chain then consolidates these\nsummaries into a single text block.\nTo implement this process, place the necessary code in a file named\nchain_5_1.py:\nListing 4.12 Search and Summarization chain\nfrom llm_models import get_llm\nfrom prompts import (\n   RESEARCH_REPORT_PROMPT_TEMPLATE\n)\nfrom chain_1_2 import assistant_instructions_chain\nfrom chain_2_1 import web_searches_chain\n",
      "content_length": 637,
      "extraction_method": "Direct"
    },
    {
      "page_number": 137,
      "chapter": null,
      "content": "from chain_3_1 import search_result_urls_chain\nfrom chain_4_1 import search_result_text_and_summary_chain\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain.schema.runnable import RunnableLambda\nsearch_and_summarization_chain = (\n   search_result_urls_chain \n   | search_result_text_and_summary_chain.map() # parallelize fo\n   | RunnableLambda(lambda x: \n       {\n           'summary': '\\n'.join([i['summary'] for i in x]), \n           'user_question': x[0]['user_question'] if len(x) > 0 \n       })\n)\nThe map() operator triggers multiple instances of the Result Text and\nSummary chain, one for each dictionary from the Search Result URLs chain\ncontaining a URL. This allows each instance to run simultaneously.\nAdditionally, the Joined Summaries sub-chain is integrated directly within\nthe larger Search and Summarization chain rather than as a separate entity.\nThis sub-chain merges summaries from each instance of the Result Text and\nSummary chain, functioning as a core part of the overall process.\nWith these components in place, you're ready to complete the Web Research\nchain.\n4.7.4 Web Research chain\nBefore diving into the implementation, let's take a look at Figure 4.10 for a\ndetailed overview of the Web Research chain, building on what we\nintroduced in Figure 4.8, similar to our approach with the Search and\nSummarization chain.\nFigure 4.10 Web Research chain: This illustrates how each web search initiated by the Web\nSearches chain triggers a separate instance of the Search and Summarization chain. These\ninstances operate concurrently, each producing a summary. These summaries are then compiled\nby the Research Report chain into the comprehensive final research report.\n",
      "content_length": 1712,
      "extraction_method": "Direct"
    },
    {
      "page_number": 138,
      "chapter": null,
      "content": "As shown in Figure 4.10, web searches from the Web Searches chain trigger\nmultiple instances of the Search and Summarization chains to run in parallel.\nEach chain generates a summary for a specific web search, and these\nsummaries are then combined by the Research Report chain to create the final\nresearch report.\nThe LCEL implementation of this process is outlined below, to be added to\nthe chain_5_1.py file:\nListing 4.13 Web Research chain\nweb_research_chain = (\n   assistant_instructions_chain \n   | web_searches_chain \n   | search_and_summarization_chain.map() # parallelize for each \n   | RunnableLambda(lambda x:\n      {\n          'research_summary': '\\n\\n'.join([i['summary'] for i in \n",
      "content_length": 694,
      "extraction_method": "Direct"
    },
    {
      "page_number": 139,
      "chapter": null,
      "content": "          'user_question': x[0]['user_question'] if len(x) > 0 e\n       })\n   | RESEARCH_REPORT_PROMPT_TEMPLATE | get_llm() | StrOutputPars\n)\nThis setup follows the same logic as the Search and Summarization chain.\nThe map() operator is used here to initiate multiple instances of the Search\nand Summarization sub-chain, allowing them to run concurrently. The\nResearch Report chain is then integrated as part of the overall Web Research\nchain.\nTo test the Web Research chain, use the following script, saving it as\nchain_try_5_1.py:\nListing 4.14 Script to test Web Research chain\nfrom chain_5_1 import web_research_chain\n# test chain invocation\nquestion = 'What can I see and do in the Spanish town of Astorga?\nweb_research_report = web_research_chain.invoke(question)\nprint(web_research_report)\nAfter running this, you'll get a research report formatted in Markdown, as\nspecified by the prompt. Note, the example report is shortened here for\nbrevity:\n# Introduction\nAstorga, a small town in northwestern Spain, may not be on everyo\nstunning architecture, delicious local cuisine, and natural beaut\nalong with practical information for planning a trip to this char\n# History of Astorga\nAstorga's history dates back to the ancient Roman settlement of A\nChristians. It has also been the site of many violent campaigns, \n# Top Attractions in Astorga\n## Episcopal Palace\nOne of the most iconic …\nCongratulations! You've built an LCEL-based chain that integrates sub-\nchains, inline chains, lambda functions, and parallelization. This hands-on\nexperience will make it easier for you to create your own LCEL chains. I\n",
      "content_length": 1612,
      "extraction_method": "Direct"
    },
    {
      "page_number": 140,
      "chapter": null,
      "content": "recommend taking some time to experiment with the different chains in this\napplication. Adjust the prompts, change the number of queries generated, or\ntailor the application to a specific type of web research you have in mind.\nNote\nThe research summarization engine you've built draws inspiration from an\nopen-source project named GPT Researcher. Exploring its GitHub repository\n(https://github.com/assafelovic/gpt-researcher) will show you a robust\nplatform supporting various search engines and LLMs, with features like\nmemory and compression. While it doesn’t use LCEL, adapting its core\nfunctionalities into an LCEL-based framework was intended to clarify several\ntechnical aspects for you. I recommend exploring the GPT Researcher’s\ncodebase and running the project on your computer if possible. It’s an\nexcellent way to learn about building an LLM application, including aspects\nlike web UI integration, beyond just the engine itself.\n4.8 Summary\nA research summarization engine is built around an LLM for efficient\nprocessing.\nThe engine's workflow involves:\nReceiving a research question from the user.\nGenerating web searches to gather relevant information with LLM\nassistance.\nSummarizing the content from each search result using the LLM.\nCombining these summaries into a detailed report.\nThis engine can be developed using Visual Studio Code.\nKey components of the engine include web searching, web scraping, and\nLLM interaction.\nPrompt engineering is critical in three areas:\nCrafting web searches from the user's question.\nSummarizing search results.\nCreating the final report.\nLangChain Expression Language (LCEL) helps organize the workflow\ninto a main chain with secondary chains.\nVarious techniques can parallelize the execution of these secondary\n",
      "content_length": 1766,
      "extraction_method": "Direct"
    },
    {
      "page_number": 141,
      "chapter": null,
      "content": "chains for efficiency.\n",
      "content_length": 23,
      "extraction_method": "Direct"
    },
    {
      "page_number": 142,
      "chapter": null,
      "content": "5 Agentic Workflows with\nLangGraph\nThis chapter covers\nOverview of agentic workflows and agents\nLangGraph fundamentals and state management\nTransition from LangChain chains to an agentic workflow\nLarge language models are driving a new generation of applications that\nrequire more than simple prompt-response exchanges. As applications\nbecome more complex, agentic workflows have become essential—a pattern\nwhere the LLM orchestrates a structured, multi-step process using predefined\ncomponents and explicit state management. Unlike fully autonomous agents,\nagentic workflows follow a predictable sequence of steps and do not\ndynamically select tools or adapt to context in real time. Instead, they offer\nreliability, transparency, and modularity by guiding the application through a\nset flow, using the LLM to make decisions within fixed boundaries.\n5.1 Understanding Agentic Workflows and Agents\nLLM-powered agent-based systems typically follow one of two core design\npatterns: agentic workflows and agents. Each pattern shapes how the\napplication operates, as illustrated in figure 5.1. Because these terms are often\nused interchangeably—but have important differences—it’s essential to\nunderstand exactly what is meant by “agentic workflow” and “agent” before\ndiving deeper..\nFigure 5.1 Workflows and agents: workflows use the LLM to choose the next step from a fixed set\nof options, such as routing a request to a SQL database or a REST API and synthetizing the\nanswer with the related results. Agents, however, dynamically select and combine tools to achieve\ntheir objectives.\n",
      "content_length": 1583,
      "extraction_method": "Direct"
    },
    {
      "page_number": 143,
      "chapter": null,
      "content": "Workflow\n\n2a. text-to-Sql processor\n\nUser 1, Router 3, Answer synthetizer\n\nStatic flow 2b. text-to-api processor\n\n4. answer\n\nAgent",
      "content_length": 130,
      "extraction_method": "OCR"
    },
    {
      "page_number": 144,
      "chapter": null,
      "content": "Agentic workflows—often called simply workflows—guide an application\nthrough a fixed sequence of predetermined steps. The LLM is used to select\namong predefined options, helping the system complete tasks and manage the\noverall flow. In contrast, agents use language models for more than just task\nexecution: they reason, make decisions, and dynamically determine the next\nsteps based on available tools and evolving context. Here, a \"tool\" typically\nrefers to a function that returns or processes data. While both patterns rely on\nthe LLM to drive application behavior, workflows maintain a structured and\npredictable path, whereas agents can adapt in real time based on new\ninformation and shifting goals.\n5.1.1 Workflows\nWorkflows use the LLM to pick the next step from a limited set of choices.\nThey typically implement patterns such as Controller-Worker or Router, as\nillustrated in figure 5.2.\nFigure 5.2 Common Workflows patterns: the Controller-Worker pattern uses the LLM in the\ncontroller orchestrates the flow by assigning various tasks to workers following a certain\nsequence. In the Router pattern, the LLM simply directs the task to the appropriate worker\nbased on the context.\n",
      "content_length": 1191,
      "extraction_method": "Direct"
    },
    {
      "page_number": 145,
      "chapter": null,
      "content": "Controller-Worker\nWorkflow\n\nStatic flow\n\nAnswer synthesis worker\n\nRouting Workflow\n\n2a. text-to-sql processor\n(either)\n\nUser 1. Router 3. Answer synthetizer\n\nStatic flow\n\n2b. text-to-api processor\n\n4. answer",
      "content_length": 207,
      "extraction_method": "OCR"
    },
    {
      "page_number": 146,
      "chapter": null,
      "content": "In the Controller-Worker pattern, the controller spawns tasks for workers\nfollowing a certain sequence. In the Router pattern, the LLM simply directs\nthe task to the proper processor (or worker).\n5.1.2 Agents\nLLM agents use language models to perceive data, reason about it, decide on\nactions, and achieve goals. Advanced agents learn from feedback, retain\nmemory of past interactions, and build dynamic workflows with branching\nlogic. Unlike fixed prompt-response systems, agents generate new flows\nbased on real-time data and available tools.\n5.1.3 When to Use Agent-Based Architectures\nThe concepts of LLM-based workflows and agents are closely related and\noften overlap, with no sharp dividing line between them. In this chapter, our\nfocus is on agentic workflows; we’ll explore agents in greater depth later in\nthe book. Both approaches are most valuable when your application needs to\nbreak complex tasks into smaller steps, make decisions based on previous\nresults, access external tools or data, or maintain context throughout extended\ninteractions. It’s best to adopt agentic workflows or agents when your use\ncase genuinely benefits from explicit state management and dynamic control\n—balancing the added power against the increased complexity.\nTIP\nI highly recommend Anthropic's article \"Building Effective AI Agents\" for a\ndeeper understanding of workflows, agents, and when to use them. You can\nread it here: https://www.anthropic.com/engineering/building-effective-\nagents\n5.1.4 Agent Development Frameworks\nA variety of frameworks are available for building agent-based systems, each\nwith its own focus and trade-offs. LangGraph is designed for stateful,\npersistent agentic workflows using graph-based execution, making it\n",
      "content_length": 1738,
      "extraction_method": "Direct"
    },
    {
      "page_number": 147,
      "chapter": null,
      "content": "powerful for complex applications—though it may require more technical\nexpertise. AutoGPT emphasizes fully autonomous, goal-driven agents with\nminimal supervision, but can face challenges with task consistency.\nLlamaIndex stands out in knowledge retrieval, though its scope is narrower\nthan broader agent frameworks. Microsoft Autogen supports highly\ncustomizable multi-agent conversations, but comes with a steeper learning\ncurve. n8n provides a visual interface and extensive integrations, making it\naccessible to non-developers, though advanced reasoning may require\nadditional components. Microsoft Semantic Kernel prioritizes memory and\nplanning and integrates well with Azure services. CrewAI enables\ncollaborative, multi-agent systems for specialized teams, but is a newer tool\nwith a smaller community.\n5.2 LangGraph Basics\nLangGraph builds on LangChain to manage more complex agentic\nworkflows with branching paths, stateful processing, and clear transitions\nbetween steps. It’s a framework for building stateful, multi-step AI\napplications using a graph-based structure. In LangGraph, nodes represent\nindividual tasks, such as generating text, calling an API, or analyzing data.\nEdges define the paths that connect these tasks. The state is information that\nmoves between nodes and updates at each step.\nThis setup is better than traditional chains when you need to make decisions,\nmanage state, or handle complex agentic workflows.\nNote\nLangGraph isn’t a replacement for LangChain but an extension. Think of\nLangChain as providing the building blocks and LangGraph as offering a\nblueprint to connect those parts into a complex system. LangChain gives you\ncomponents like LLMs, embeddings, and retrievers, while LangGraph helps\nyou organize those components into a structured, stateful workflow.\nTo use LangGraph effectively, it's important to understand a few key\nconcepts—like the core components of a graph (nodes and edges), how state\nflows through the graph, and how conditional edges control its behavior.\n",
      "content_length": 2022,
      "extraction_method": "Direct"
    },
    {
      "page_number": 148,
      "chapter": null,
      "content": "These are the building blocks you’ll explore in the next section.\n5.3 Moving from LangChain Chains to LangGraph\nLangChain’s simple, linear chains work for straightforward tasks but have\nlimits when your applications get more complex. A typical LangChain setup\noften looks like this:\nchain = (\n   prompt_template \n   | llm \n   | output_parser\n)\nThis setup struggles when tasks need to split into different paths, when you\nneed to repeat steps based on new information, or when you want to manage\nstate across multiple steps. It also falls short when multiple processes need to\nhappen in parallel.\nLangGraph solves these problems by offering better state management,\nconditional branching, and support for cyclical workflows. With explicit state\nmanagement, you can define and track data consistently across the workflow,\nwhich is essential for memory and reasoning. Conditional branching allows\nagents to take different paths based on previous results, making decision-\nmaking smoother. Cyclical workflows let agents repeat tasks until they meet\nspecific conditions, which helps refine results.\nLangGraph also makes it easier to understand and debug complex workflows.\nIts graph-based structure offers a clearer view of how data flows through the\nsystem, which helps when you need to trace or fix problems.\nThis graph-based approach works well for a range of use cases, such as\nmulti-step reasoning, task planning, managing context in long conversations,\ncoordinating research tasks, and automating business processes. As your\napplications get more complex, the benefits of using LangGraph’s agent-\nbased architecture become clearer. It gives you the control and flexibility\nneeded to build smart, multi-step systems that can adapt and make decisions\non their own.\n",
      "content_length": 1764,
      "extraction_method": "Direct"
    },
    {
      "page_number": 149,
      "chapter": null,
      "content": "5.4 LangGraph Core Components\nLangGraph provides a robust framework for building stateful, multi-step AI\napplications. Figure 5.3 presents the core components that form LangGraph\napplication's foundation.\nFigure 5.3 LangGraph core Components: a strongly typed state (in this example, modelled with\nResearchState) flows through the workflow. Nodes, usually Python functions (like def\nselect_assistant), perform tasks, and edges create directed data flows between nodes, in some\ncases with conditional paths.\n",
      "content_length": 507,
      "extraction_method": "Direct"
    },
    {
      "page_number": 150,
      "chapter": null,
      "content": "def select_assistant\n\nResearch State\n\ndef\n\ngenerate_search_queries\n\nResearch State\ndef Research State\nperform_web_ searches\n\nResearch State\n\ndef evaluate_relevance\n\nResearch State\n\ndef write_report",
      "content_length": 197,
      "extraction_method": "OCR"
    },
    {
      "page_number": 151,
      "chapter": null,
      "content": "At the heart of every LangGraph application is a state object — in our\nexample, ResearchState — which defines a clear and strongly typed state\nfor the entire workflow. This state is typically defined as a Python\nTypedDict, ensuring that data passed between components is well-structured\nand type-checked.\nIn a LangGraph, each node functions as a processing unit. Nodes can handle\ntasks such as generating search queries, calling external APIs, summarizing\nresults, or transforming data. These nodes are usually implemented as Python\nfunctions. The edges between nodes determine the directed flow of data,\ndefining how information moves through the graph.\nOne of LangGraph’s powerful features is its conditional edges, which allow\nyou to define dynamic execution paths based on the runtime state. Combined\nwith entry points and end conditions, this gives you full control over where\nthe graph begins, how it progresses, and when it completes.\nThe following sections walk through how to define and connect these\ncomponents, enabling you to build systems that can handle complex\nworkflows and adaptive decision-making.\n5.4.1 StateGraph Structure\nA core utility in LangGraph is the StateGraph class, which you use to define\nthe graph that models your application's workflow. For example:\nfrom langgraph.graph import StateGraph\nfrom typing import TypedDict\nclass ResearchState(TypedDict): #A\n   input_query: str\n   intermediate_result: str\n   final_output: str\ngraph = StateGraph(ResearchState) #B\n5.4.2 State Management and Typing\n",
      "content_length": 1527,
      "extraction_method": "Direct"
    },
    {
      "page_number": 152,
      "chapter": null,
      "content": "State management is central to LangGraph applications. Unlike chain-based\nmethods that rely on implicit or loosely typed state, LangGraph enforces\nexplicit, strongly typed state, making workflows more robust and predictable.\nHere’s an extended version of the ResearchState that adds more detail:\nfrom typing import TypedDict, Optional, List\nclass ResearchState(TypedDict):\n   user_question: str\n   assistant_info: Optional[dict]\n   search_queries: Optional[List[dict]]\n   search_results: Optional[List[dict]]\n   research_summary: Optional[str]\n   final_report: Optional[str]\nEach node receives the current state and returns updates that merge into the\noverall state:\ndef process_node(state: dict) -> dict:\n   result = do_something(state[\"input_data\"]) #A\n   \n   return {\"output_data\": result} #B\n5.4.3 Node Functions and Edge Definitions\nNodes represent processing steps. Each node is a function that takes the\ncurrent state and returns updates. For instance:\ndef generate_search_queries(state: dict) -> dict:\n   \"\"\"Generate search queries based on user question.\"\"\"\n   question = state[\"user_question\"]\n   # Generate queries using an LLM\n   queries = llm_generate_queries(question)\n   return {\"search_queries\": queries}\ngraph.add_node(\"generate_queries\", generate_search_queries) #A\nEdges define valid transitions between nodes. A simple linear edge looks like\nthis:\ngraph.add_edge(\"generate_queries\", \"perform_searches\")\n",
      "content_length": 1423,
      "extraction_method": "Direct"
    },
    {
      "page_number": 153,
      "chapter": null,
      "content": "A conditional edge uses a function to choose the next node based on the state:\ndef should_refine_queries(state: dict) -> str:\n   if len(state[\"search_results\"]) < 2:\n       return \"refine_queries\"\n   else:\n       return \"summarize_results\"\ngraph.add_conditional_edge(\"perform_searches\", should_refine_quer\n5.4.4 Entry Points and End Conditions\nEvery graph needs a starting point and clear end conditions. For example:\ngraph.set_entry_point(\"parse_question\") #A\nfrom langgraph.graph import END\ngraph.add_edge(\"write_final_report\", END) #B\nLet's explore a practical application of LangGraph.\n5.5 Turning the Web Research Assistant into an AI\nAgent\nTo demonstrate how LangGraph works, I’ll show you how to transform the\nweb research assistant from chapter 4—originally built with LangChain—\ninto an agent-based system. This upgrade allows the application to assess the\nrelevance of summaries from web page results and, if less than 50% of them\nare relevant, redirect the flow back to generating new search queries. If\nenough summaries are relevant, the application can proceed to write the final\nreport as usual. Achieving this level of dynamic control would be very\ncomplex with plain LangChain, which justifies the move to an agent-based\napproach with LangGraph. This case study guides you through each step,\nhighlighting the benefits of explicit state management and modular design.\n5.5.1 Original LangChain Implementation Overview\nOur original web research assistant used LangChain's sequential chains. The\nprocess followed these steps:\n",
      "content_length": 1538,
      "extraction_method": "Direct"
    },
    {
      "page_number": 154,
      "chapter": null,
      "content": "1. Choose the appropriate research assistant based on the user's question.\n2. Generate search queries.\n3. Perform web searches and collect URLs.\n4. Scrape and summarize each search result.\n5. Compile a final research report.\nEach step fed its output into the next step, as you can see in the extract from\nthe original implementation in listing 5.1:\nListing 5.1 Original LangChain implementation of the Web Research Assistant\nassistant_instructions_chain = (\n   {'user_question': RunnablePassthrough()} \n   | ASSISTANT_SELECTION_PROMPT_TEMPLATE \n   | get_llm() \n   | StrOutputParser() \n   | to_obj\n)\nweb_searches_chain = (\n   # ...input processing...\n   | WEB_SEARCH_PROMPT_TEMPLATE \n   | get_llm() \n   | StrOutputParser() \n   | to_obj\n)\nweb_research_chain = ( #A\n   assistant_instructions_chain \n   | web_searches_chain \n   | search_and_summarization_chain.map()\n   | RunnableLambda(lambda x: # ...process results...)\n   | RESEARCH_REPORT_PROMPT_TEMPLATE \n   | get_llm() \n   | StrOutputParser()\n)\nThis approach works but has clear limitations:\nThe flow is rigid and linear, making it difficult to adapt dynamically\nbased on intermediate results. For instance, a conditional flow that\nredirects the application to generate new search queries if less than 50%\nof the summaries are relevant would be cumbersome to implement.\n",
      "content_length": 1322,
      "extraction_method": "Direct"
    },
    {
      "page_number": 155,
      "chapter": null,
      "content": "Error handling is challenging, as the lack of explicit state makes it hard\nto track and manage failures effectively.\nState is not explicitly managed, which complicates maintaining context\nacross multiple steps.\nDebugging becomes difficult when issues arise, especially with complex\nflows, since it is unclear which part of the chain failed or why.\n5.5.2 Identifying Components for Conversion\nTo convert the web research assistant to LangGraph, I first identify the key\ncomponents that will serve as nodes. Each node handles a specific part of the\nprocess:\nAssistant Selector: Determines which type of research assistant to use\nbased on the user’s question.\nQuery Generator: Creates search queries derived from the user's input.\nWeb Searcher: Conducts searches and gathers URLs based on the\ngenerated queries.\nContent Summarizer: Scrapes and summarizes the content of web pages.\nRelevance Evaluator: Assesses if the summaries are relevant enough to\nproceed or if new search queries are needed.\nReport Writer: Compiles the final research report using the relevant\nsummaries.\nUnlike a simple linear flow, this setup introduces a conditional element. After\nevaluating the relevance of the summaries, the flow can either proceed to the\nReport Writer if enough content is relevant or redirect back to the Query\nGenerator to create new search queries. This decision is based on a defined\nthreshold (for example, if less than 50% of summaries are relevant) and can\nrepeat up to a maximum of three iterations to avoid infinite loops.\nThe flow control is managed by a conditional routing function, the\nroute_based_on_relevance, which checks the relevance of the search results\nand the current iteration count. If the relevance is insufficient and the\nmaximum number of iterations has not been reached, the application\ngenerates new queries and repeats the search and evaluation steps. If the\nmaximum iteration count is reached, the application proceeds to compile a\n",
      "content_length": 1956,
      "extraction_method": "Direct"
    },
    {
      "page_number": 156,
      "chapter": null,
      "content": "report using the available results, regardless of their relevance.\nFor each component, I define:\nThe input state: The data each node requires to function.\nThe processing: The tasks each node performs.\nThe state updates: The information each node returns to update the\noverall state.\nThis modular and conditional approach makes the system flexible and\nadaptive, which would be cumbersome to achieve with plain LangChain’s\nlinear chains.\n5.5.3 Step-by-Step Transformation Process\nNow, I’ll guide you through the process of converting a LangChain\napplication to LangGraph. The following is a simplified version of the actual\ncode, which you can find in the GitHub repository.\nStep 1: Define the State\nThe first step is to design the state structure that will flow through the graph.\nA well-defined state helps you keep track of data across all nodes. In our case\nwe’ll model a composite state using inner types, as shown in listing 5.2.\nListing 5.2 State type of the LangGraph based Research assistant\nfrom typing import TypedDict, List, Optional\nclass AssistantInfo(TypedDict): #A\n   assistant_type: str\n   assistant_instructions: str\n   user_question: str\nclass SearchQuery(TypedDict): #A\n   search_query: str\n   user_question: str\nclass SearchResult(TypedDict): #A\n   result_url: str\n",
      "content_length": 1284,
      "extraction_method": "Direct"
    },
    {
      "page_number": 157,
      "chapter": null,
      "content": "   search_query: str\n   user_question: str\n   is_fallback: Optional[bool]\nclass SearchSummary(TypedDict): #A\n   summary: str\n   result_url: str\n   user_question: str\n   is_fallback: Optional[bool]\nclass ResearchReport(TypedDict): #A\n   report: str\nclass ResearchState(TypedDict): #B\n   user_question: str\n   assistant_info: Optional[AssistantInfo]\n   search_queries: Optional[List[SearchQuery]]\n   search_results: Optional[List[SearchResult]]\n   search_summaries: Optional[List[SearchSummary]]\n   research_summary: Optional[str]\n   final_report: Optional[str]\n   used_fallback_search: Optional[bool]\n   relevance_evaluation: Optional[Dict[str, Any]]\n   should_regenerate_queries: Optional[bool]\n   iteration_count: Optional[int]\nThis state structure clearly defines the data available at each stage, reducing\nambiguity and simplifying debugging.\nStep 2: Convert Components to Node Functions\nNext, I convert each component into a node function. Each function takes the\ncurrent state, processes it, and returns updated state information, as you can\nsee in listing 5.3.\nListing 5.3 Node functions\ndef select_assistant(state: dict) -> dict:\n   \"\"\"Select the appropriate research assistant.\"\"\"\n   user_question = state[\"user_question\"]\n   \n   # Use the LLM to select an assistant\n   prompt = ASSISTANT_SELECTION_PROMPT_TEMPLATE.format(\n       user_question=user_question\n",
      "content_length": 1366,
      "extraction_method": "Direct"
    },
    {
      "page_number": 158,
      "chapter": null,
      "content": "   )\n   response = get_llm().invoke(prompt)\n   \n   assistant_info = parse_assistant_info(response.content) #A\n   \n   return {\"assistant_info\": assistant_info} #B\ndef generate_search_queries(state: dict) -> dict:\n   \"\"\"Generate search queries based on the question.\"\"\"\n   assistant_info = state[\"assistant_info\"]\n   user_question = state[\"user_question\"]\n   \n   prompt = WEB_SEARCH_PROMPT_TEMPLATE.format( #C\n       assistant_instructions=assistant_info[\"assistant_instruct\n       user_question=user_question,\n       num_search_queries=3\n   )\n   response = get_llm().invoke(prompt)\n   \n   search_queries = parse_search_queries(response.content) #D\n   \n   return {\"search_queries\": search_queries} #E\nAdditional node functions follow the same pattern, each handling its own\ntask.\nStep 3: Define the Graph Structure\nWith node functions in place, I create the graph and define how the nodes\nconnect, establishing the execution order and data flow, as shown in Listing\n5.4. Unlike a simple linear chain, this version of the graph introduces a new\nnode for relevance evaluation and a conditional edge that dynamically alters\nthe flow based on the relevance of the search results.\nListing 5.4 Graph structure\nfrom langgraph.graph import StateGraph, END\ngraph = StateGraph(ResearchState)  #A\ngraph.add_node(\"select_assistant\", select_assistant)  #B\ngraph.add_node(\"generate_search_queries\", generate_search_queries\ngraph.add_node(\"perform_web_searches\", perform_web_searches)  #B\n",
      "content_length": 1472,
      "extraction_method": "Direct"
    },
    {
      "page_number": 159,
      "chapter": null,
      "content": "graph.add_node(\"summarize_search_results\", summarize_search_resul\ngraph.add_node(\"evaluate_search_relevance\", evaluate_search_relev\ngraph.add_node(\"write_research_report\", write_research_report)  #\ndef route_based_on_relevance(state): #C\n   iteration_count = state.get(\"iteration_count\", 0) + 1\n   state[\"iteration_count\"] = iteration_count\n   if iteration_count >= 3:\n       return \"write_research_report\"\n   if state.get(\"should_regenerate_queries\", False):\n       return \"generate_search_queries\"\n   return \"write_research_report\"\ngraph.add_edge(\"select_assistant\", \"generate_search_queries\")  #D\ngraph.add_edge(\"generate_search_queries\", \"perform_web_searches\") \ngraph.add_edge(\"perform_web_searches\", \"summarize_search_results\"\ngraph.add_edge(\"summarize_search_results\", \"evaluate_search_relev\ngraph.add_edge(\"write_research_report\", END)  #D\ngraph.add_conditional_edges( #E\n   \"evaluate_search_relevance\",\n   route_based_on_relevance,\n   {\n       \"generate_search_queries\": \"generate_search_queries\",\n       \"write_research_report\": \"write_research_report\"\n   }\n)\ngraph.set_entry_point(\"select_assistant\")  #F\nThe new relevance evaluation node checks if enough of the summarized\nresults are relevant. If less than 50% of them meet the criteria, the graph\nredirects the flow back to the Query Generator to refine the search. If the\nsummaries are sufficient or if the maximum of three iterations is reached, it\nmoves forward to compile the final report. This conditional flow is a\nsignificant enhancement over the rigid linear chains of LangChain, allowing\nthe system to adapt dynamically based on intermediate results.\nStep 4: Compile and Run the Graph\nAfter defining the graph, I compile it and run it using an initial state, as\n",
      "content_length": 1735,
      "extraction_method": "Direct"
    },
    {
      "page_number": 160,
      "chapter": null,
      "content": "shown in listing 5.5. This step involves setting up an initial state with all\nrequired fields, including additional parameters for controlling the\nconditional flow, such as should_regenerate_queries and\niteration_count.\nListing 5.5 Running the graph\napp = graph.compile() #A\ninitial_state = { #B\n   \"user_question\": \" What can you tell me about Astorga's roman \n   \"assistant_info\": None,\n   \"search_queries\": None,\n   \"search_results\": None,\n   \"search_summaries\": None,\n   \"research_summary\": None,\n   \"final_report\": None,\n   \"used_fallback_search\": False,\n   \"relevance_evaluation\": None,\n   \"should_regenerate_queries\": None,\n   \"iteration_count\": 0\n}\nresult = app.invoke(initial_state) #C\nfinal_report = result[\"final_report\"] #D\nBy introducing conditional edges and relevance evaluation, this step-by-step\nprocess transforms a rigid, linear chain into a flexible, stateful, and adaptive\nagent-based workflow. The system can now evaluate its own results, adapt by\nrefining search queries if needed, and ensure that the final report is based on\nsufficiently relevant information. This adaptability would be cumbersome to\nimplement in plain LangChain, justifying the shift to LangGraph for complex\nLLM applications.\n5.5.4 Code Comparison and Benefits Realized\nThe LangGraph approach offers significant benefits:\nExplicit State Management: The state is clearly defined and passed\nthrough each node, making data handling transparent and reliable.\n",
      "content_length": 1449,
      "extraction_method": "Direct"
    },
    {
      "page_number": 161,
      "chapter": null,
      "content": "Modular Components: Each node handles a single task, simplifying\ntesting, debugging, and maintenance.\nClear Flow Control: The graph structure visually represents the\nexecution order and data flow, making it easier to trace and understand\ncomplex processes.\nEasier Debugging: With well-defined nodes and edges, it's\nstraightforward to identify where errors occur and what data caused\nthem.\nEnhanced Error Handling: Each node can implement specific error-\nhandling strategies without affecting the rest of the system.\nConditional Flow Control: The introduction of conditional edges based\non relevance evaluation allows the application to dynamically alter its\npath—either refining search queries if results are insufficient or\nproceeding to report writing. This adaptability ensures that the\napplication can respond intelligently to intermediate results, which\nwould be cumbersome to implement in plain LangChain.\nFuture Extensibility: Adding or modifying nodes requires minimal\nchanges to the overall system, allowing for smooth upgrades and new\ncapabilities.\nThis case study demonstrates how LangGraph enhances flexibility, control,\nand adaptability in complex, multi-step AI applications compared to\ntraditional LangChain chains. The ability to implement conditional flows\nbased on runtime evaluations makes LangGraph a powerful choice for\nbuilding smart, context-aware agent-based systems..\n5.6 Summary\nAgentic workflows follow fixed, predictable steps, while agents use\nLLMs for reasoning, dynamic tool selection, and real-time adaptation.\nMoving from basic LangChain apps to agentic workflows with\nLangGraph gives greater control and flexibility for complex tasks.\nLangGraph extends LangChain with stateful, multi-step workflows\norganized as nodes, edges, and explicit state updates.\nReplacing linear chains with LangGraph enables explicit state tracking\nand conditional branching, simplifying debugging and error handling.\nConditional flows based on runtime evaluations let applications adapt,\n",
      "content_length": 1999,
      "extraction_method": "Direct"
    },
    {
      "page_number": 162,
      "chapter": null,
      "content": "such as re-running searches when results are insufficient.\nCore components include the StateGraph, node functions, and well-\ndefined edges and entry points for maintainable, extensible designs.\nThe case study shows converting a web research assistant from a rigid\nchain to a flexible agent-based system with modular design and clear\nstate management.\nLangGraph boosts control, modularity, and future scalability, enabling\ncomplex decision-making and better overall performance.\n",
      "content_length": 478,
      "extraction_method": "Direct"
    },
    {
      "page_number": 163,
      "chapter": null,
      "content": "6 RAG fundamentals with Chroma\nDB\nThis chapter covers\nImplementing semantic search using the RAG architecture\nUnderstanding vector stores and their functionality\nImplementing RAG with Chroma DB and OpenAI\nIn this chapter, you’ll dive into two essential concepts: semantic search and\nRetrieval Augmented Generation (RAG). You’ll explore how large language\nmodels (LLMs) are used for semantic search through a chatbot, enabling you\nto query a system for information across multiple documents and retrieve the\nfragments that best match the meaning of your question, rather than just\nmatching keywords. This approach is also known as Q&A over documents or\nquerying a knowledge base.\nIn earlier chapters, you learned about summarization, a typical use case for\nlarge language models. Now, I'll walk you through the basics of building a\nQ&A chatbot that searches across multiple documents. You'll interact with\nthe LLM to find the answers you're looking for.\nThis chapter focuses on RAG, the design pattern that powers semantic search\nsystems, with a particular emphasis on the vector store—a key component of\nthese systems. You’ll learn the technical terminology related to Q&A and\nRAG systems and understand how terms like \"semantic search\" and \"Q&A\"\nare often used interchangeably.\nBy the end of this chapter, you’ll have implemented a basic RAG-based\narchitecture using the APIs of an LLM (OpenAI) and a vector store (Chroma\nDB). In chapter 6, you'll build on this foundation to create Q&A chatbots\nusing RAG architecture. There's a lot to cover, so let's get started.\n",
      "content_length": 1567,
      "extraction_method": "Direct"
    },
    {
      "page_number": 164,
      "chapter": null,
      "content": "6.1 Semantic Search\nSemantic search is a popular use case for LLMs, alongside summarization\nand code generation. It's one of the key applications driving the LLM and\nGenerative AI boom.\nDefinition\nSemantic search means searching for information by focusing on its meaning.\nThis involves understanding a query's meaning, retrieving relevant document\nfragments from a document store that closely match the query's meaning, and\noptionally generating a natural language answer.\nSemantic search differs from traditional keyword-based searches, which fail\nto find information if exact words don’t match. Semantic search produces\nrelevant results even if the query and result don’t share a single word.\nBefore diving into the code, I want to give you a clear understanding of a\nsemantic search chatbot's architecture. I'll start with a simple example to ease\nyou in, but by the end of this section, you'll grasp the architecture of a real-\nworld Q&A chatbot.\n6.1.1 A Basic Q&A Chatbot Over a Single Document\nI’ll start with a simple scenario to help you understand how a Q&A chatbot\nworks and to familiarize you with its components. The first chatbot example\nanswers questions about a single document, as shown in figure 6.1.\nFigure 6.1 A simple Q&A chatbot process: 1) The user sends a prompt containing a document\n(context) and a question to the chatbot; 2) The chatbot returns an initial answer; 3) The user\nfollows up with a more detailed question; 4) The chatbot provides a more accurate answer.\n",
      "content_length": 1494,
      "extraction_method": "Direct"
    },
    {
      "page_number": 165,
      "chapter": null,
      "content": "The main elements of this basic setup are:\nDocument: Contains the text for semantic search or information\nextraction.\nPrompt: Encapsulates the user’s question (semantic search) and the\ncontext (the document) with the information needed for the answer.\nLLM-based Chatbot: Sends the prompt to the LLM, which understands\nthe question and context, selects relevant information, and formulates an\nanswer for the user.\nLet’s break down these concepts further.\n",
      "content_length": 454,
      "extraction_method": "Direct"
    },
    {
      "page_number": 166,
      "chapter": null,
      "content": "Definition\nContext is the text or information in the prompt, along with the user’s\nquestion, used to formulate an answer.\nDefinition\nSynthesize means to generate an answer from the question and context\nprovided.\nYou don't need to write any code to implement this initial setup. Just log into\nChatGPT or an alternative LLM-based chatbot like Gemini or Claude, and\nyou're ready to go. Let’s try a simple Q&A interaction using text about\nPaestum from britannica.com. Submit this prompt to ChatGPT (you can find\nthis prompt in a text file of the Github repository):\nRead the following text and let me know how many temples are in Paestum,\nwho constructed them, and what architectural style they are:\n---\nPaestum, Greek Poseidonia, ancient city in southern Italy near the west coast,\n22 miles (35 km) southeast of modern Salerno and 5 miles (8 km) south of\nthe Sele (ancient Silarus) River. Paestum is noted for its splendidly preserved\nGreek temples.\nVisit the ruins of the ancient Greek colony of Paestum and discover its\nhistory, culture, and society. See all videos for this article\nPoseidonia was probably founded about 600 BC by Greek colonists from\nSybaris, along the Gulf of Taranto, and it had become a flourishing town by\n540, judging from its temples. After many years’ resistance the city came\nunder the domination of the Lucanians (an indigenous Italic people)\nsometime before 400 BC, after which its name was changed to Paestum.\nAlexander, the king of Epirus, defeated the Lucanians at Paestum about 332\nBC, but the city remained Lucanian until 273, when it came under Roman\nrule and a Latin colony was founded there. The city supported Rome during\nthe Second Punic War. The locality was still prosperous during the early\n",
      "content_length": 1731,
      "extraction_method": "Direct"
    },
    {
      "page_number": 167,
      "chapter": null,
      "content": "years of the Roman Empire, but the gradual silting up of the mouth of the\nSilarus River eventually created a malarial swamp, and Paestum was finally\ndeserted after being sacked by Muslim raiders in AD 871. The abandoned\nsite’s remains were rediscovered in the 18th century.\nTemple ruins of columns and statures at Karnak, Egypt (Egyptian\narchitecture; Egyptian archaelogy; Egyptian history)\nThe ancient Greek part of Paestum consists of two sacred areas containing\nthree Doric temples in a remarkable state of preservation. During the ensuing\nRoman period a typical forum and town layout grew up between the two\nancient Greek sanctuaries. Of the three temples, the Temple of Athena (the\nso-called Temple of Ceres) and the Temple of Hera I (the so-called Basilica)\ndate from the 6th century BC, while the Temple of Hera II (the so-called\nTemple of Neptune) was probably built about 460 BC and is the best\npreserved of the three. The Temple of Peace in the forum is a Corinthian-\nDoric building begun perhaps in the 2nd century BC. Traces of a Roman\namphitheatre and other buildings, as well as intersecting main streets, have\nalso been found. The circuit of the town walls, which are built of travertine\nblocks and are 15–20 feet (5–6 m) thick, is about 3 miles (5 km) in\ncircumference. In July 1969 a farmer uncovered an ancient Lucanian tomb\nthat contained Greek frescoes painted in the early classical style. Paestum’s\narchaeological museum contains these and other treasures from the site.\nHere’s the answer I got (keep in mind, you might get a slightly different\nresult because of the stochastic nature of LLMs, but it's still likely to be\nincomplete):\nThere are three temples in Paestum, and they are constructed in the Doric\narchitectural style. These temples are:\nThe Temple of Athena (also known as the Temple of Ceres), from the\n6th century BC.\nThe Temple of Hera I (also known as the Basilica), also from the 6th\ncentury BC.\nThe Temple of Hera II (also known as the Temple of Neptune), built\n",
      "content_length": 2002,
      "extraction_method": "Direct"
    },
    {
      "page_number": 168,
      "chapter": null,
      "content": "around 460 BC and the best-preserved of the three.\nThese temples are remarkable for their well-preserved Doric architecture and\nare important historical landmarks.\nThis is almost what I wanted, but it’s missing who built the temples. If this\nhad been a Q&A engine instead of a chatbot, the client system wouldn’t have\nreceived any answer about who built the temples.\nNote\nA Q&A engine, like a summarization engine, is called by a client system and\nreturns one answer. A chatbot, used by a person, allows for multiple\nquestions so you can refine the answer through interaction.\nNext, I send this prompt:\nThis answer is almost what I expected, but you forgot to answer something:\nwho built the temples?\nChatGPT responds:\nI apologize for missing that part. The temples in Paestum were built by Greek\ncolonists from Sybaris who founded the city of Poseidonia (later Paestum)\naround 600 BC. So, the Greek colonists constructed these temples in the\nDoric architectural style.\nThis shows the advantage of a chatbot over an engine: after a couple of\nrounds of questions, ChatGPT answered all my questions about the temples\nin Paestum. Importantly, I didn’t have to resubmit the full text; ChatGPT\nremembered the initial text.\nNote\nA chatbot like ChatGPT creates a stateful session that remembers the\nexchange by passing the history of the conversation as an additional context,\n",
      "content_length": 1370,
      "extraction_method": "Direct"
    },
    {
      "page_number": 169,
      "chapter": null,
      "content": "allowing the user to refine the answer without resending the initial text.\nLet’s try another prompt:\nIs the archaeological site in Paestum recognized by UNESCO?\nChatGPT answered:\nYes, the archaeological site of Paestum is recognized by UNESCO. In 1998,\nit was designated a UNESCO World Heritage Site. This designation\nacknowledges the historical and architectural significance of the well-\npreserved Greek temples and ancient ruins in Paestum.\nHold on! The original text didn’t include this information. ChatGPT used its\nown knowledge, likely from Wikipedia or other sources, to answer correctly.\nThis eagerness to answer questions using unprovided content can be helpful,\nas shown in the example, but it can also be dangerous, as you will see shortly\nLet’s try another question:\nHow many columns do the three temples have in total?\nChatGPT answered:\nThe three temples in Paestum have a total of 94 columns:\nThe Temple of Athena (Temple of Ceres) has 6 columns on its shorter\nsides and 13 columns on its longer sides, for a total of 32 columns.\nThe Temple of Hera I (Basilica) has 6 columns on its shorter sides and 9\ncolumns on its longer sides, for a total of 30 columns.\nThe Temple of Hera II (Temple of Neptune) has 6 columns on its shorter\nsides and 14 columns on its longer sides, for a total of 32 columns.\nChatGPT tried to answer, but the answer is wrong. The Temple of Hera I has\n50 columns, not 30, as you can check yourself on the Wikipedia.\n",
      "content_length": 1453,
      "extraction_method": "Direct"
    },
    {
      "page_number": 170,
      "chapter": null,
      "content": "Warning\nWhen a chatbot can't find the information in the provided text, it might rely\non its own knowledge, which can lead to incorrect or made-up information,\nknown as \"hallucination.\" You can limit this by forcing the LLM to only use\nthe information in the prompt and admit when it doesn't know the answer.\nBeyond prompt engineering, you can reduce hallucinations by testing the\napplication using an evaluation framework, which I'll cover in later chapters.\nIn one instance, ChatGPT provided accurate information about UNESCO, yet\nin another, it gave an incorrect answer about the number of columns in a\nstructure. This raises an important question: how can you tell when ChatGPT\nis providing reliable information and when it’s not? As a general rule, you\nshould be cautious about trusting ChatGPT for facts or details you haven’t\nindependently verified or supplied yourself.\nTip\nTo avoid this issue, instruct ChatGPT to use only the information you\nprovided and indicate if it doesn’t know the answer.\nYou can try to address the previous issue with this prompt:\nHow many columns do the three temples have in total? Use only the\ninformation I gave you in the text. If you don’t know the answer, just say so.\nYou’ll get a more accurate response:\nI apologize for the error in my previous response. The text you provided\ndoesn’t mention the total number of columns in the three temples in Paestum.\nSpoiler Alert\nDesigning safe prompts for Q&A chatbots reduces the chance of\nhallucinations. You’ll learn more about this in the rest of the book.\n",
      "content_length": 1543,
      "extraction_method": "Direct"
    },
    {
      "page_number": 171,
      "chapter": null,
      "content": "Now, let’s move to a more complex use case.\n6.1.2 A More Complex Q&A Chatbot Over a Knowledge Base\nLet’s summarize the design of the basic LLM-based Q&A chatbot, like\nChatGPT, which processes a single piece of text:\n1. You send a prompt to the chatbot with the text you want to search for\nand the question you want to ask.\n2. The prompt should instruct the chatbot to formulate an answer using\nonly the provided text.\n3. The chatbot should create a session, retaining the conversation history to\nrefine answers.\nThe approach described so far works well for searching within a single text.\nBut what if your chatbot needs to answer questions about company\ninformation scattered across multiple sources—such as intranet pages, shared\nfolders, and documents in various formats like PDF, Word, TXT, or\nPowerPoint? That’s the challenge we’ll tackle in the next chapters.\nWhen designing an enterprise Q&A chatbot, one of the main obstacles is that\nyou can’t include all of the company’s content in the prompt along with the\nuser’s question—especially when dealing with large documents, this would\nquickly exceed the model’s context window. In practice, narrowing down the\ncontext to what’s most relevant improves speed, cost-efficiency, and\naccuracy. In fact, providing less—but more focused—context often yields\nbetter results than overloading the model with loosely related information.\nTo achieve this, the chatbot must be able to access the company’s knowledge\nbase and retrieve only the specific content needed to answer a given question.\nIdeally, it should “understand” the knowledge base well enough that you can\nask a question naturally—without manually supplying background context\neach time. This concept is illustrated in Figure 6.2.\nFigure 6.2 Hypothetical design for an enterprise Q&A chatbot: the knowledge of the chatbot is\nexpanded with various data sources: the enterprise intranet, knowledge bases and documents\n",
      "content_length": 1923,
      "extraction_method": "Direct"
    },
    {
      "page_number": 172,
      "chapter": null,
      "content": "Now, you might wonder: how can you connect the ChatGPT chatbot (or\nGemini chat, or Claude chat) to company intranet, knowledge bases and\ndocuments, as shown in figure 6.2 above, so you can send just the question in\nthe prompt? Unfortunately, you can’t connect ChatGPT to your local data\ndirectly. The above solution doesn’t work with standard ChatGPT. The\nclosest alternative is to use ChatGPT Plus and configure a custom version\nthrough OpenAI's My GPTs offering, allowing you to upload documents for\nlookup by later queries. This is convenient for simple use cases. However, for\nmore control over how the chatbot interacts with text sources and the LLM,\nyou need a different approach. Enter the RAG design pattern.\n",
      "content_length": 717,
      "extraction_method": "Direct"
    },
    {
      "page_number": 173,
      "chapter": null,
      "content": "6.1.3 The RAG Design Pattern\nThe Retrieval Augmented Generation (RAG) design pattern is a classic\nsolution for building a Q&A chatbot. Let's break down what RAG stands for:\nRetrieval: This step involves retrieving context from a pre-prepared data\nsource, typically a vector store optimized for semantic search. Retrieval\nis a key part of the RAG architecture.\nAugmented: This means the answer is improved or enhanced by the\ncontext provided during the retrieval step.\nGeneration: This refers to generating the answer to your question. Since\nthis book focuses on LLMs and generative AI, the answer generation is\nperformed by an LLM, requiring the chatbot to interact with it.\nYou might wonder where this information is retrieved from and how. The\nkey is preparing the information so your custom chatbot can easily access it\nand then use it to augment the LLM's generated answer.\nThe RAG design pattern has two stages:\n1. Content Ingestion Stage (Indexing Stage): All the content users will\nquery is stored in a “special” database and indexed in a “special” format\nfor efficient retrieval (I will clarify what “special” means shortly).\n2. Question-Answering Stage (Retrieval and Generation Stage): The\nchatbot takes a user's question, retrieves relevant information from the\nspecial database, and feeds it along with the user's question to the LLM.\nThe LLM generates and returns the augmented answer.\nLet's dive into each phase.\nContent Ingestion Stage (Indexing Stage)\nBefore users can query the Q&A chatbot, you need to store relevant content,\nsuch as enterprise documents from various sources and formats, into a vector\nstore, a database optimized for quick search and retrieval, as shown in figure\n6.3.\n",
      "content_length": 1705,
      "extraction_method": "Direct"
    },
    {
      "page_number": 174,
      "chapter": null,
      "content": "Figure 6.3 RAG Ingestion Stage: Documents are extracted from sources, split into chunks, and\nconverted into embeddings while being stored in a vector database, which stores a copy of the\noriginal chunks and their embeddings (vector form).\n",
      "content_length": 239,
      "extraction_method": "Direct"
    },
    {
      "page_number": 175,
      "chapter": null,
      "content": "\\ i |\nWie):\n= | Word docs |\nIL ! Chunks | Embeddings\nFile servers i\nPDFJ :\n\nPOF docs |\n\nys | Chunks : Embeddings\n= ii \\i—\nHTML :\n\nIntranet HTML websites Chunks | Embeddings\n\n/\n\nVector\n\nMarkdown ws Embeddings\n‘Chunks Embeddings\nKvowedgeO8s — Phintet Chunks | Embedcngs",
      "content_length": 267,
      "extraction_method": "OCR"
    },
    {
      "page_number": 176,
      "chapter": null,
      "content": "During the content ingestion stage, text is extracted from the sources, split\ninto small chunks, and each chunk is transformed into an \"embedding,\" a\nnumerical vector representation of the text. Splitting content into chunks is\ncrucial because embedding models work on finite sizes, and you want the\nsearch to target small, relevant content pieces instead of large, mixed-\nrelevance sections. You can create embeddings using the vector store’s\nproprietary model, an LLM provider's model (like OpenAI), or a dedicated\nembedding library. The embeddings and content chunks are then stored in a\nvector database. The purpose of the embeddings is to index the content for\nefficient lookup during the Q&A stage. This means the text in the user’s\nquestion doesn’t need to match the text in the results exactly to produce\nrelevant answers. For example, querying the vector store for \"feline animals\"\nwill return chunks mentioning \"cat,\" \"lion,\" and \"tiger,\" even if the word\n\"feline\" isn’t in any document chunk.\nQuestion & Answer Stage (Retrieval & Generation Stage)\nOnce the information has been split into small chunks, transformed into\nembeddings, and stored in a vector store, users can query your Q&A chatbot.\nLet's walk through the Q&A stage workflow, as shown in Figure 6.4.\nFigure 6.4 RAG Question & Answer Stage: 1) The chatbot uses a retriever to transform the user\nquestion into embeddings; 2) then the retriever uses the embedding to perform a similarity\nsearch in the vector store. 3) The vector store returns several relevant text chunks; 4) The\nretrieved content is fed into a prompt as a \"context\" along with the original user question. 5) The\nchatbot sends the prompt to the LLM which synthesizes a response and returns it to the user.\n",
      "content_length": 1745,
      "extraction_method": "Direct"
    },
    {
      "page_number": 177,
      "chapter": null,
      "content": "' Chathot :\n\nee\n\nuser question\n\n1.The retriever transforms\nthe user question into the\ncorresponding embedding\n\n‘Nn vector DB returns text\n\n' chunks relevant to the user\n\nrelevant chunks using the | + Question 5.The LLM accepts the\nquestion embedding = | __4,The chatbot creates a prompt and synthetizes @\n! ! prompt with the user question : corresponding completion\n} Vector ' __ and the retrieved chunks (the : (the response)\n| 0B ‘context |",
      "content_length": 442,
      "extraction_method": "OCR"
    },
    {
      "page_number": 178,
      "chapter": null,
      "content": "When the chatbot receives a user question, a retriever converts the natural\nlanguage query into its vector representation using an embedding model. This\nstep must use the same embedding model as the ingestion stage. The chatbot\nthen uses the vectorized question to query the vector store, which understands\nthe query's semantic meaning and returns document chunks with embeddings\nsimilar to the query. It performs a vector distance calculation between the\nvectorized question and the items in the vector index stored in the database.\nThe chatbot can retrieve the closest text chunk or a set number of the closest\nchunks (ordered by distance).\nOnce the relevant content chunks are retrieved, the chatbot sends the LLM a\nprompt that includes the initial question and a \"context\" incorporating the\nretrieved document chunks. The LLM then generates (or \"synthesizes\" in\nRAG terminology) the answer and returns it to the chatbot, which then\ndelivers it to the user.\nThis final step is similar to the basic \"Q&A over a single document\" use case.\nYou provide the LLM with the initial question and a \"context\" (previously\nthe entire input text in the simple scenario) that provides the information for\nthe answer. Now, the \"context\" is represented by chunks retrieved from the\nvector store. The main difference between Q&A over a single document and\nover a range of documents in a vector database is the additional components:\nthe vector store provides the information for the answer. The role ChatGPT\nplayed in the basic chatbot use case is now split between an orchestrating\nchatbot (which accepts the query and retrieves the information) and an LLM\n(which synthesizes the answer).\nNow that you understand the high-level architecture of the RAG design\npattern, let's examine one of its key components: the vector store. After that,\nyou'll be ready to attempt your first RAG implementation.\n6.2 Vector Stores\nI have mentioned vector stores several times, but only briefly. In this section,\nI will explain what they are, their purpose, what they offer, and name a few\nexamples.\n",
      "content_length": 2070,
      "extraction_method": "Direct"
    },
    {
      "page_number": 179,
      "chapter": null,
      "content": "6.2.1 What’s a Vector Store?\nA vector store is a storage system designed to efficiently store and query\nhigh-dimensional vectors. Vectors are key in AI because embeddings—\nnumerical representations of text, images, sounds, or videos—are built using\nthem. In short, embeddings are vectors that capture the meaning of words in\ntheir dimensions.\nThe main use of vector stores in LLM and ML applications is to store\nembeddings that act as indexes for text chunks (or chunks of video, image, or\naudio). Searches in vector stores are \"similarity searches,\" which measure the\ndistance between the embeddings of the query and those of the stored chunks.\nThe result is either the closest vector or a list of the closest vectors. This\n\"semantic similarity\" reflects how close the meanings of the text chunks are.\n6.2.2 How Do Vector Stores Work?\nVector distance calculations use common functions like Euclidean distance,\nCosine distance, and Hamming distance. These are used in ML algorithms\nsuch as k-Nearest Neighbors (KNN) and the more scalable Approximate\nNearest Neighbor (ANN) search, which is the standard algorithm for\nsimilarity searches.\nTip\nI won't cover distance metrics or similarity search algorithms here. If you're\ninterested, check out this academic paper by Yikun Han et al.:\nhttps://arxiv.org/pdf/2310.11703.pdf or this informal article by Erika\nCardenas: https://weaviate.io/blog/distance-metrics-in-vector-search.\nThe first vector stores, like Milvus, appeared in 2019 to support dense vector\nsearch, mainly for image recognition. These stores efficiently stored and\ncompared image embeddings. This is called \"dense vector\" search because\nthe vectors, or embeddings, have most dimensions with non-zero values.\nEarlier search techniques, like Term Frequency-Inverse Document Frequency\n(TF-IDF), used sparse vectors, where most values are zero. These were used\n",
      "content_length": 1870,
      "extraction_method": "Direct"
    },
    {
      "page_number": 180,
      "chapter": null,
      "content": "for lexical search, focusing on exact word matches and implemented in\nsystems like Lucene or BM25.\nMilvus was initially built for image-based embeddings, where the vectors\nrepresented the meaning of an image. Later, vector stores expanded to tasks\nlike product recommendations, and with the rise of large language models\n(LLMs), new vector stores emerged specializing in text-based semantic\nsimilarity search.\n6.2.3 Vector Libraries vs. Vector Databases\nThe first vector stores, known as \"vector libraries,\" like Faiss (developed by\nMeta), offered minimal functionality to keep things simple. They stored\nembeddings in memory using immutable data structures and provided\nefficient similarity search capabilities. However, as LLM adoption grew,\nthese libraries revealed several limitations:\nHandling Underlying Text: Vector libraries only stored embeddings,\nrequiring you to store the original data, such as text or images,\nelsewhere. This meant creating a unique identifier for each piece of data\nto synchronize the original text and its embeddings, complicating\nimplementation and maintenance.\nNo Updates: Vector libraries used immutable data structures, preventing\nupdates. This made them unsuitable for use cases with frequently\nchanging data, especially on an intraday basis.\nLimited Querying During Data Ingestion: Vector libraries didn’t support\nquerying during data ingestion due to the risk of simultaneous read and\nwrite operations, which could impact performance and scalability.\nTo address these issues, vendors like Pinecone developed \"vector databases,\"\noffering more features:\nHandling Text and Embeddings: Vector databases store both the text\nand related embeddings, simplifying the client application's workflow.\nMany can even handle embedding creation. They also allow storing\nmetadata associated with the text, such as provenance and lineage\ninformation.\n",
      "content_length": 1873,
      "extraction_method": "Direct"
    },
    {
      "page_number": 181,
      "chapter": null,
      "content": "Full Create/Read/Update/Delete (CRUD) Capabilities: Vector databases\nsupport updating data, making them suitable for use cases with frequent\ndata changes.\nQuerying During Import: Vector databases allow similarity searches\nwhile importing new data, enhancing scalability and performance.\nVector databases soon introduced features like caching, sharding, and\npartitioning, which improved scalability, performance, robustness, and\ndurability, similar to traditional relational and NoSQL databases. Meanwhile,\nrelational databases like PostgreSQL and NoSQL databases like MongoDB,\nwhich already offered these benefits, adapted by adding support for vector\ntypes. This allows embeddings to be stored alongside text in the same record\nor document, making it easy to link text with its corresponding embedding.\nFor the rest of the book, I will use \"vector store\" and \"vector database\"\ninterchangeably, as they have converged in meaning.\n6.2.4 Most Popular Vector Stores\nCompiling a table summarizing the characteristics of the most popular vector\nstores is challenging due to their rapid evolution and convergence. However,\ntable 6.1 gives a rough idea of what's available in the market at the time of\npublication, providing a starting point for your exploration.\nTable 6.1 Most popular vector stores and related characteristics\nVector\nstore\nType\nWebsite\nFaiss\nVector\nlibrary\nhttps://github.com/facebookresearch/faiss/wiki/\nMilvus\nVector\ndatabase\nhttps://milvus.io\nQdrant\nVector\ndatabase\nhttps://qdrant.tech\nChroma\nVector\ndatabase\nhttps://www.trychroma.com\nWeaviate\nVector\ndatabase\nhttps://weaviate.io\n",
      "content_length": 1595,
      "extraction_method": "Direct"
    },
    {
      "page_number": 182,
      "chapter": null,
      "content": "Pinecone\nVector\ndatabase\nhttps://www.pinecone.io\nVald\nVector\ndatabase\nhttps://vald.vdaas.org\nScann\nVector\nlibrary\nhttps://github.com/google-research/google-\nresearch/tree/master/scann\nKDB\nTime series\nDB\nhttps://kdb.ai\nElastic\nSearch\nSearch\nengine\nhttps://www.elastic.co\nOpenSearch Fork of\nElasticSearch\nhttps://opensearch.org\nPgVector\nPostgresSQL\nextension\nhttps://github.com/pgvector/pgvector\nMongoDB\nAtlas\nMongoDB\nextension\nhttps://www.mongodb.com/\n6.2.5 Storing Text and Performing a Semantic Search Using\nChroma\nBefore guiding you through building an enterprise Q&A chatbot, let's first\nlearn how to store text in a vector store and perform semantic searches. This\nwill help you understand the fundamentals.\nWe'll use Chroma, a vector database that's easy to set up and use. You just\nneed to install the related Python package with pip. Let’s get started!\nSetting up Chroma DB\nFirst, set up a Chroma Jupyter notebook environment as explained in the\nsidebar below.\nSidebar: Setting up a Chroma Jupyter Notebook Environment\nCreate the virtual environment for Chapter 6's code. Open a terminal, create a\nch06 folder, navigate into it, and run:\n",
      "content_length": 1145,
      "extraction_method": "Direct"
    },
    {
      "page_number": 183,
      "chapter": null,
      "content": "C:\\Github\\building-llm-applications\\ch06>python -m venv env_ch06 \nActivate the virtual environment:\nC:\\Github\\building-llm-applications\\ch06>.\\env_ch06\\Scripts\\activ\nYou should see:\n(env_ch06) C:\\Github\\building-llm-applications\\ch06>\nInstall the necessary packages either with pip install -r requirements.txt if you\nhave cloned the Github repo, or as follows (note the chromadb version):\npip install notebook chromadb==0.5.3 openai\nIf you cloned the Github repo, start the Jupyter notebook with jupyter\nnotebook 06-chromadb-ingestion-and-querying.ipynb, otherwise create it\nfrom scratch:\njupyter notebook\nCreate a notebook with File > New > Notebook and save it as: 06-chromadb-\ningestion-and-querying.ipynb.\nOn In your notebook, import the chromadb module and create an in-memory\nclient for the vector database. Keep in mind that the in-memory client will\nlose all data when the notebook session ends. (For other ways to set up a\nChromaDB database, see the related sidebar at the end of this section:)\nimport chromadb\nchroma_client = chromadb.Client()\nNext, create a collection to store the content on Paestum from the Britannica\nwebsite. A collection is like a \"bucket\" where you store documents and their\nrelated embeddings.\ntourism_collection = chroma_client.create_collection(name=\"touris\nInserting the Content\n",
      "content_length": 1317,
      "extraction_method": "Direct"
    },
    {
      "page_number": 184,
      "chapter": null,
      "content": "Add the Paestum content to the collection, splitting it manually into a list of\nsmaller chunks (or “documents”). Chroma will then generate embeddings\nfrom the text you provide, using its default embeddings model unless you\nspecify a different one. I have shortened the text for convenience, but you can\nfind the full version in the Paestum-Britannica.txt file on my GitHub repo\nor check my notebook (also on GitHub) if needed. When storing the text, it's\nuseful to include metadata such as the source of each document and an\nassociated ID, as shown in listing 6.1.\nListing 6.1 Creating and populating a Chroma DB collection\ntourism_collection.add(\n   documents=[\n       \"Paestum, Greek Poseidonia, …[shortened] … preserved Gree\n       \"Poseidonia was probably …[shortened] … in the 18th centu\n       \"The ancient Greek part of …[shortened] … from the site.\"\n   ],\n   metadatas=[\n       {\"source\": \"https://www.britannica.com/place/Paestum\"}, \n       {\"source\": \"https://www.britannica.com/place/Paestum\"},\n       {\"source\": \"https://www.britannica.com/place/Paestum\"}\n   ],\n   ids=[\"paestum-br-01\", \"paestum-br-02\", \"paestum-br-03\"]\n)\nAfter running this code, you are ready to perform a search on the vector store.\nPerforming a Semantic Search\nLet's perform a query similar to the one you executed against ChatGPT. Ask\nfor the number of Doric temples in Paestum, specifying that you only want\nthe closest result:\nresults = tourism_collection.query(\n   query_texts=[\"How many Doric temples are in Paestum\"],\n   n_results=1\n)\nprint(results)\nHere's a shortened version of the result:\n",
      "content_length": 1581,
      "extraction_method": "Direct"
    },
    {
      "page_number": 185,
      "chapter": null,
      "content": "{'ids': [['paestum-br-03']], 'distances': [[0.7664762139320374]], \nChroma understands the query's meaning and returns the correct text chunk\ncontaining the answer, along with metadata about the source and the distance\nbetween the query and answer embeddings.\nNote\nUnlike querying ChatGPT, where you had to provide the question and the full\ntext, querying Chroma only requires sending the question, as the content is\nalready stored in the Chroma DB.\nChecking Semantic Proximity\nTo see how close the returned text chunk (paestum-br-03) is to the question\ncompared to the other text chunks (paestum-br-01 and paestum-br-02),\nrequest three results:\nresults = tourism_collection.query(\n   query_texts=[\"How many Doric temples are in Paestum\"],\n   n_results=3\n)\nprint(results)\nYou should see:\n{'ids': [['paestum-br-03', 'paestum-br-01', 'paestum-br-02']], 'd\nThe embeddings for paestum-br-03 are the closest to the question’s\nembeddings, with a distance of 0.76. The chunk paestum-br-02 is the\nfarthest, with a distance of 1.33, proving that Chroma identified the most\nrelevant chunk correctly.\nNote\nThe vector database doesn’t generate an answer as ChatGPT does. It returns\nthe semantically closest text chunks to your query. For a properly formulated\nanswer, you still need an LLM model to process the original question and the\nretrieved text chunks. This approach saves costs since LLM vendors like\n",
      "content_length": 1396,
      "extraction_method": "Direct"
    },
    {
      "page_number": 186,
      "chapter": null,
      "content": "OpenAI charge based on the number of tokens processed.\nThis section has given you a glimpse of what you can do with Chroma. For\nmore details, check out the official documentation at\nhttps://docs.trychroma.com/, especially the “Usage Guide” to learn how to\nrun Chroma in client/server mode if you prefer it to run on a separate host\nfrom your LLM solution.\nInstantiating ChromaDB in Different Ways\nSo far, you've worked with a local in-memory instance of ChromaDB. You\ncan also create a client for a local on-disk instance like this:\ncollection = client.create_collection(\"my_persistent_collection\")\nAlternatively, you can set up an HTTP client on the same computer or a\ndifferent one. To do this, open a command shell and run the following\ncommand (assuming you've installed ChromaDB via pip):\nchroma run --port 8010\nNext, instantiate the HTTP client in your notebook or application like this:\nclient = chromadb.HttpClient(host=\"http://localhost\", port=8010)\nOnce the client is set up, you can interact with it in the same way you do with\nthe in-memory client.\nNow that you know how to query the vector store, you can attempt to\nimplement the full RAG pattern, including generating complete answers.\n6.3 Implementing RAG from Scratch\nLet’s implement RAG by building a chatbot that uses the gpt-4o-mini model\nand a vector database. We will then ask it the same question about Paestum's\ntemples that you asked ChatGPT in section 6.1.1. When using ChatGPT, you\nhad to send a prompt with both the question and the full text on Paestum from\nBritannica. Once you build your own chatbot, you will only need to ask the\n",
      "content_length": 1611,
      "extraction_method": "Direct"
    },
    {
      "page_number": 187,
      "chapter": null,
      "content": "question, as shown in figure 6.5 below.\nFigure 6.5 RAG Architecture including Chroma DB and GPT-4o-mini model\nAs you can see in the architectural diagram in figure 6.5, the chatbot will\nquery Chroma, retrieve the content, and feed it to GPT-4o-mini with the\noriginal question to get the full answer.\nWe'll build the chatbot step by step by implementing a few functions. First,\nimport the OpenAI library and set the OpenAI API key (assuming you're\nusing the same notebook from the previous section):\nfrom openai import OpenAI\n",
      "content_length": 525,
      "extraction_method": "Direct"
    },
    {
      "page_number": 188,
      "chapter": null,
      "content": "import getpass\nOPENAI_API_KEY = getpass.getpass('Enter your OPENAI_API_KEY')\nNow instantiate the OpenAI client:\nopenai_client = OpenAI(api_key=OPENAI_API_KEY)\n6.3.1 Retrieving Content from the Vector Database\nAs you already know how to perform a semantic search against the vector\nstore, let’s wrap that code into a reusable function:\ndef query_vector_database(question):\n   results = tourism_collection.query(\n       query_texts=[question],\n       n_results=1\n   )\n   results_text = results['documents'][0][0]\n   return results_text\nLet’s try out this function against the same question we asked previously:\nresults_text = query_vector_database(\"How many Doric temples are \nprint(results_text)\nYou will see output like this:\nThe ancient Greek part of Paestum consists of two sacred areas co\nThis is the result we expected: notice the retrieved chunk correctly contains\nthe text “three Doric temples”.\n6.3.2 Invoking the LLM\nWe need to craft a prompt that combines the user’s question with the context\nretrieved from the vector database, and then submit it to the LLM. To get\nstarted, we’ll use a simple prompt and encapsulate the code for calling the\nLLM in a new function, as shown in Listing 6.2. For this example, I’ve set\nthe temperature to 0.7 to introduce some controlled randomness in the\nresponses.\n",
      "content_length": 1308,
      "extraction_method": "Direct"
    },
    {
      "page_number": 189,
      "chapter": null,
      "content": "Listing 6.2 Functions to define and execute a prompt\ndef prompt_template(question, context):\n   return f'Read the following text and answer this question: {q\ndef execute_llm_prompt(prompt_input):\n   prompt_response = openai_client.chat.completions.create(\n       model='gpt-4o-mini',\n       messages=[\n           {\"role\": \"system\", \"content\": \"You are an assistant f\n           {\"role\": \"user\", \"content\": prompt_input}\n       ],\n       temperature=0.7\n   )\n   return prompt_response\nUsing a simple Q&A prompt\nLet’s test the functions with the question that made ChatGPT hallucinate:\ntrick_question = \"How many columns have the three temples got in \ntq_result_text = query_vector_database(trick_question)\ntq_prompt = prompt_template(trick_question, tq_result_text)\ntq_prompt_response = execute_llm_prompt(tq_prompt)\nprint(tq_prompt_response)\nWe get:\nChatCompletion(id='chatcmpl-9nCGTTSUBJUmAOZWVIGbF8OE7nLF5', choic\nThe gpt-4o-mini model did not hallucinate. It correctly recognized it didn't\nhave enough information to answer the question. A few months ago, when I\nran the same code against the gpt-3.5-turbo model, it gave a wrong answer\nof 24 columns, with incorrect assumptions about the number of columns in\neach temple. I'll show you below how I fixed the issue.\nUsing a Safer Q&A Prompt\nHallucinations can be mitigated in general by using a well-known prompt for\nQ&A, available on the Langchain Hub web page (part of LangSmith).\n",
      "content_length": 1436,
      "extraction_method": "Direct"
    },
    {
      "page_number": 190,
      "chapter": null,
      "content": "Tip\nLangchain Hub (https://smith.langchain.com/hub) is a popular LLM resource,\nconstantly updated with open-source models, prompts, and advice on use\ncases. I highly recommend checking it out.\nHere is the recommended hallucination-safe RAG prompt from\nhttps://smith.langchain.com/hub/rlm/rag-prompt:\nUse the following pieces of retrieved context to answer the question. If you\ndon't know the answer, just say that you don't know. Use three sentences\nmaximum and keep the answer concise.\nQuestion: {question}\nContext: {context}\nAnswer:\nLet's update the prompt template function accordingly:\ndef prompt_template(question, text):\n   return f'Use the following pieces of retrieved context to answ\nNow let’s re-submit the trick question:\ntrick_question = \"How many columns have the three temples got in \ntq_result_text = query_vector_database(trick_question)\ntq_prompt = prompt_template(trick_question, tq_result_text)\ntq_prompt_response = execute_llm_prompt(tq_prompt)\nprint(tq_prompt_response)\nWe get this:\nChatCompletion(id='chatcmpl-9nCco9P3xSdArsptotrmJEjtd2N5D', choic\nWell done! You have prevented the LLM from hallucinating. Your chatbot\nwill now only use the knowledge stored in the vector database or admit\nexplicitly it doesn't know the answer.\n",
      "content_length": 1251,
      "extraction_method": "Direct"
    },
    {
      "page_number": 191,
      "chapter": null,
      "content": "6.3.3 Building the Chatbot\nWe can now implement the chatbot with a single function, using the code\nwe’ve covered in this section:\ndef my_chatbot(question):\n   results_text = query_vector_database(question)  #A\n   prompt_input = prompt_template(question, results_text)  #B\n   prompt_output = execute_llm_prompt(prompt_input)  #C\n   return prompt_output\nLet's test it with the original question:\nquestion = \"Let me know how many temples there are in Paestum, wh\nresult = my_chatbot(question)\nprint(result)\nWe get:\nChatCompletion(id='chatcmpl-9nCmZjXJHXVEZLjNpQdAlrnm3K691', choic\nThe synthetized response is comprehensive as it answers all the questions we\nasked.\nYou should be proud of what you have achieved so far! You’ve implemented\na basic chatbot that can answer questions based on text imported into the\nvector database and provide additional information if needed. It won't return\ninformation not in the vector database, so it won't hallucinate or make up\nanswers.\nThe key takeaway is that you now understand the internals of a Q&A LLM-\nbased system, the components of the RAG design pattern, and its workflow.\nThis knowledge will help you when using frameworks like LangChain,\nLlamaIndex, and Semantic Kernel, which might hide their implementation\ndetails. You’ll be better equipped to troubleshoot problems and understand\nwhat’s going on behind the scenes.\nBefore re-implementing RAG with LangChain, let’s recap the RAG\nterminology you’ve learned so far.\n",
      "content_length": 1463,
      "extraction_method": "Direct"
    },
    {
      "page_number": 192,
      "chapter": null,
      "content": "6.3.4 Recap on RAG Terminology\nThroughout this chapter, you've been learning and refining RAG\nterminology. Some terms may have similar meanings to ones you've seen\nearlier. The table below will help consolidate your understanding, especially\nfor concepts that can be expressed with different terms.\nTable 6.2 RAG Glossary\nTerm\nDefinition\nAlternative\nTerms\nRetrieval\nAugmented\nGeneration\n(RAG)\nUse case involving the generation of\ntext (typically an answer) augmented\nwith information retrieved from a\ncontent store optimized for semantic\nsearches, typically a vector store\nQ&A\nText chunk\nA fragment of text from a document.\nDocuments are split into chunks for\nmore effective searching, especially\nwhen stored in specialized unstructured\ntext stores like vector stores\nText fragment,\nchunk, text\nnode, node\nEmbeddings\nNumerical (vector) representation of a\npiece of text, used to index text chunks\nfor semantic searches\nVector\nRAG content\ningestion stage\nPhase in the RAG design where text is\nimported and indexed into a context\nstore for efficient retrieval against a\nnatural language question. In a vector\nstore, text is broken into chunks and\nindexed through their associated\nembeddings\nText indexing,\ntext\nvectorization\nVector store\nIn-memory store or specialized database\nholding text chunks and their related\nembeddings, which serve as their index\nVector\ndatabase\nSemantic\nsimilarity\nComparing pieces of text based on their\nmeaning, typically by calculating the\ndistance between the embeddings of the\n \n",
      "content_length": 1508,
      "extraction_method": "Direct"
    },
    {
      "page_number": 193,
      "chapter": null,
      "content": "text pieces. This can be done using\ncosine distance or Euclidean distance |\nvector similarity, cosine similarity\nSemantic\nsearch\nSearching for information based on its\nmeaning. This involves performing\nsemantic similarity between the\nembeddings of the search question and\nthe text chunks in a vector store\nQ&A, vector\nsearch\nContext\nText (or information) provided in the\nprompt along with the user question,\nused to formulate an answer. This can\nbe a full document or a list of text\nchunks retrieved from a vector store\nthrough semantic search\n \nSynthesize\nGenerate an answer, typically from a\nuser question and a \"context\" that\nprovides the necessary information\nGenerate\nRAG Question\n& Answer\nstage\nPhase in the RAG design where a user\nasks a search question, the application\nperforms a semantic search against a\ncontent store (typically a vector store),\nand feeds the LLM the original question\nalong with the \"context\" retrieved from\nthe store. The LLM then synthesizes\nand returns the answer to the\napplication, which passes it on to the\nuser\nRAG Question-\nAnswering\nstage\nYou are now ready to re-implement RAG with LangChain, which you will do\nin the next chapter.\n6.4 Summary\nImplement a minimalistic Q&A chatbot by feeding a question and\nsupporting document to an LLM like ChatGPT or Claude.\nFor answering questions over a knowledge base, use RAG, which\n",
      "content_length": 1361,
      "extraction_method": "Direct"
    },
    {
      "page_number": 194,
      "chapter": null,
      "content": "includes a vector store, a retriever, and an LLM to synthesize responses.\nThe RAG design pattern has two stages: ingestion (populating the vector\nstore with text and embeddings) and Q&A (retrieving relevant\ndocuments and generating responses).\nA vector store is a specialized database optimized for storing and\nretrieving text through related embeddings.\nMany vector stores evolved from in-memory libraries to full-fledged\npersistent databases.\nYou can implement a simple RAG system by directly using the APIs of\nthe LLM (such as the OpenAI API) and the vector store (such as the\nChroma DB API).\n",
      "content_length": 596,
      "extraction_method": "Direct"
    },
    {
      "page_number": 195,
      "chapter": null,
      "content": "7 Q&A chatbots with LangChain\nand LangSmith\nThis chapter covers\nImplementing RAG with LangChain\nQ&A across multiple documents\nTracing RAG chain execution with LangSmith\nAlternative implementation using LangChain Q&A specialized\nfunctionality\nNow that you understand the RAG design pattern, implementing it with\nLangChain should be straightforward. In this chapter, I’ll show you how to\nuse the LangChain object model to abstract interaction with source\ndocuments, the vector store, and the LLM.\nWe'll also explore LangSmith’s tracing capabilities for monitoring and\ntroubleshooting the chatbot workflow. Additionally, I’ll demonstrate\nalternative chatbot implementations using LangChain’s specialized Q&A\nclasses and functions.\nBy the end of this chapter, you’ll be equipped with the skills to build a\nsearch-enabled chatbot that can connect to private data sources—a complete\nversion of which you’ll construct in Chapter 12.\nBefore we start implementing the chatbot with LangChain, let’s review the\nLangChain classes that support the Q&A chatbot use case.\n7.1 LangChain object model for Q&A chatbots\nAs discussed earlier, the key benefit of using LangChain for your LLM-based\napplication is its ability to handle communication between components like\ndata loaders, vector stores, and LLMs. Instead of working with each API\n",
      "content_length": 1324,
      "extraction_method": "Direct"
    },
    {
      "page_number": 196,
      "chapter": null,
      "content": "directly, LangChain abstracts these interactions. This allows you to swap out\nany component with a different provider without changing the overall design\nof your application.\nAdditionally, LangChain provides out-of-the-box implementations for\ncommon requirements when building LLM applications, such as splitting\nsource text, retrieving relevant context from a vector store, generating\nprompts, handling the prompt context window, and more.\nQ&A is a core use case for LangChain. The components needed for this\nworkflow are grouped by stage as follows:\nContent Ingestion (Indexing) Stage\nQ&A (Retrieval and Generation) Stage\nLet's examine these component groups in detail.\n7.1.1 Content Ingestion (Indexing) Stage\nFigure 7.1 summarizes the object model for the content ingestion stage of the\nQ&A use case.\nFigure 7.1 Object model associated with the content ingestion stage.\n",
      "content_length": 874,
      "extraction_method": "Direct"
    },
    {
      "page_number": 197,
      "chapter": null,
      "content": "Document loaders\nparse external text\ncontent into a typed\nDocument object\n\nDocument transformers\ntransform a Document\ninto one or more\nDocument objects. For\nexample, a Text Splitter\nbreaks down raw text\ninto chunks to improve\nthe accuracy of vector-\nbased search.\n\n«create\nlist of\nobjects»\n\nA Document, considered\nthe core entity in\nLangChain, encapsulates\nboth the content and\nmetadata of unstructured\nor structured data\n\nseeeseee\n\nEmbeddings are actually\n\"embedding models”\nwhich convert\nDocuments into their\nvector representations\n\nAvector store contains for fast retrieval\n\nDocuments (and text\nchunks) and the related\nembeddings, vector\nrepresentations used for\nsemantic searches",
      "content_length": 683,
      "extraction_method": "OCR"
    },
    {
      "page_number": 198,
      "chapter": null,
      "content": "Here are the components shown in the figure:\nDocument: Models the text content and related metadata.\nBaseLoader: Loads text from external sources into the document model.\nTextSplitter: Splits documents into smaller chunks for efficient\nprocessing.\nVector Store: Stores text chunks and related embeddings for efficient\nretrieval.\nEmbedding Model: Converts text into embeddings (vector\nrepresentations).\nThis is a static view of how LangChain class families connect. For a dynamic\nview, see the sequence diagram in figure 7.2, which shows the typical content\ningestion process.\nFigure 7.2 Content Ingestion Sequence Diagram: the data ingestion orchestrator initializes a\nLoader to import text, which is parsed into a Document. The Document is transformed into\nsmaller chunks by a Text Splitter. An Embedding model generates embeddings for the chunks,\nand both the chunks and embeddings are stored in the vector store.\n",
      "content_length": 916,
      "extraction_method": "Direct"
    },
    {
      "page_number": 199,
      "chapter": null,
      "content": "Here’s how LangChain classes interact during content ingestion:\n1. The data ingestion orchestrator initializes a specific Loader to import\ntext from an external source, like CsvLoader for CSV files.\n2. The Loader parses the text and converts it into a Document object.\n3. The Document is passed to a DocumentTransformer, usually a\nTextSplitter, to divide it into smaller chunks.\n4. The chunks are processed by an Embeddings model to create\nembeddings.\n5. Both the chunks and embeddings are sent to the VectorStore for\nstorage.\n7.1.2 Q&A (Retrieval and Generation) Stage\n",
      "content_length": 570,
      "extraction_method": "Direct"
    },
    {
      "page_number": 200,
      "chapter": null,
      "content": "Figure 6.3 summarizes the object model for the retrieval and generation stage\nof the Q&A use case.\nFigure 7.3 Object model associated with the retrieval and generation stage.\n",
      "content_length": 175,
      "extraction_method": "Direct"
    },
    {
      "page_number": 201,
      "chapter": null,
      "content": "«abstract» «create object» «create object» —\nee ee ee e\nBasePromptTempiate MessagePromptTemplate\n\n«abstract» «abstract» «abstract» «abstract»\nSuing Pipeline Base ChatMessage MessagesPlaceholder\nPromptTempiate PromptTempiate ChatPromptTempiate PromptTempiate\nHumanMessage\nPromptTemplate ChatPromptTemplate PromptTemplate\n\nAlMessage\nPromptTemplate\nSystemMessage\nPromptTemplate\n\nA MessagePromptTemplate\n\nBo is designed to generate a\nPromptvalue instance from a\ntemplate structure including\nmessages from various roles\n(human, system, Al) and\n\ninput parameters\n\nA PromptTemplate is\ndesigned to generate a\nPromptValue instance\nfrom template structures\nand input parameters\n\nALLM model is\nsuitable for one-off\ninstructions\ninvoked by a client\nquery engine\n\n«use object»\n\nME ~~ we ee ee ee eee eee eee\n\nA Chat model is suitable\nfrom a chatbox which\n\n(concrete LLM\n\n' «return object.\nmodels ...) accepts interactive : a\ninstructions and keeps an A Retriever\nhistory for reference ‘ efficiently fetch\n\n«use object» data, often a list of\nwabstracts Pe «abstract» Documents,\nVOCOOTSIONO Fann ees «abstract» BaseRetriever typically from a\nTomeeeeeees \"use object» Embeddi vector store\nB indexed with\nAvector store contains embeddings\n\nEmbeddings are actually\n\nembeddings, vector\nrepresentations used for\nsemantic searches\n\nDocuments (and text 5 e\nchunks) and the related embedding models «abstract»\nwhich convert VectorStoreRetriever\n\nDocuments into their\nvector representations\nfor fast retrieval\n\nFAIS SRetriever\n(many others ...)\n\n(other retrievers...)\n\n(many others ...)",
      "content_length": 1562,
      "extraction_method": "OCR"
    },
    {
      "page_number": 202,
      "chapter": null,
      "content": "The figure includes the following components:\nVectorStore: Stores and retrieves relevant text chunks.\nRetriever: Retrieves relevant text chunks from the Vector Store based\non the similarity between the query's embedding and the stored text\nembeddings.\nEmbeddings (Embedding model): Ensures consistent embeddings for\nqueries and documents.\nPrompt / PromptTemplate: Constructs the input for the language model,\ntypically using the user question and a context made of retrieved text\nchunks.\nLanguageModel: Generates answers using the provided context and\nquery.\nFor a dynamic view of the Q&A workflow, see the sequence diagram in\nfigure 7.4.\nFigure 7.4 Q&A Sequence Diagram: the Q&A orchestrator sends the user’s question to the\nvector store Retriever, which generates an embedding using an Embeddings model. The\nRetriever searches for similar embeddings in the vector store and returns relevant documents.\nThe Q&A orchestrator combines the documents and the question into a prompt with a\nPromptTemplate, then sends it to the Language Model, which provides the answer.\n",
      "content_length": 1066,
      "extraction_method": "Direct"
    },
    {
      "page_number": 203,
      "chapter": null,
      "content": "Here’s how LangChain classes interact during the Q&A process:\n1. The Q&A orchestrator sends the user's question to the vector store\nRetriever.\n2. The Retriever generates the question's embedding using an Embeddings\nmodel.\n3. The Retriever searches the vector store for documents with similar\nembeddings and returns them to the Q&A orchestrator.\n4. The Q&A orchestrator combines the retrieved documents and the user’s\nquestion into a prompt using a PromptTemplate.\n",
      "content_length": 464,
      "extraction_method": "Direct"
    },
    {
      "page_number": 204,
      "chapter": null,
      "content": "5. The orchestrator sends the prompt to the LanguageModel, which returns\nthe answer.\nNow that you understand the Q&A object model, let's get started with the\nimplementation!\n7.2 Vector Store Content Ingestion\nBefore querying your documents, you must store them in a vector database.\nTo make it more interesting, we'll import content about Paestum from various\nsources and in different formats, rather than just using the small Britannica\narticle on Paestum as before.\nFirst, install the required packages for loading, splitting documents, and\ncreating embeddings. We'll use the open-source embeddings model from the\nsentence-transformers package, which includes the Chroma embeddings\nmodel all-MiniLM-L6-v2. This helps save on costs, but feel free to use the\nOpenAI embeddings model if cost isn't an issue.\nOpen a new OS shell, navigate to the folder for this chapter, create and\nactivate the virtual environment for Chapter 7, and install the required\npackages (as usual, if you have cloned the repository from Github you can\ninstall the packages with pip install -r requirements.txt instead, after\nactivating the virtual environment):\nC:\\Github\\building-llm-applications\\ch07>python -m venv env_ch07\nC:\\Github\\building-llm-applications\\ch07>.\\env_ch07\\Scripts\\activ\n(env_ch07) C:\\Github\\building-llm-applications\\ch07>pip install w\nOnce the installation is complete, start a Jupyter notebook, as usual:\n(env_ch07) C:\\Github\\building-llm-applications\\ch07>jupyter noteb\nCreate a new notebook with File > New > Notebook, give it a name (e.g.,\nch07-QA_across_documents), and save it.\nImport the necessary libraries on the notebook:\n",
      "content_length": 1631,
      "extraction_method": "Direct"
    },
    {
      "page_number": 205,
      "chapter": null,
      "content": "from langchain_community.document_loaders import WikipediaLoader, \nfrom langchain_chroma import Chroma\nfrom langchain_text_splitters import RecursiveCharacterTextSplitt\nfrom langchain_openai import OpenAIEmbeddings\nCapture the OpenAI API key as usual:\nimport getpass\nOPENAI_API_KEY = getpass.getpass('Enter your OPENAI_API_KEY')\n7.2.1 Splitting and Storing the Documents\nTo make the content more searchable:\n1. Split each document into chunks of about 500 characters. You can add\noverlap, usually around 10%, but for now, set the overlap to 0. The size\nof chunks and overlap considerations will be covered in Chapter 8 on\nadvanced indexing techniques;\n2. calculate the embeddings of the document chunks and store them in the\nChroma vector database.\nInstantiate the splitter, embeddings model, and vector database client as\nfollows:\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, ch\nembeddings_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY\nvector_db = Chroma(\"tourist_info\", embeddings_model) \nProcess each document by loading it, splitting it into chunks, and storing the\nchunks with the related embeddings into the vector database:\nwikipedia_loader = WikipediaLoader(query=\"Paestum\")\nwikipedia_chunks = text_splitter.split_documents(wikipedia_loader\nvector_db.add_documents(wikipedia_chunks)\nOnce the content has been split and stored in the vector database, you will see\nan output similar to this:\n['d3197373-7df1-4c24-8a0a-0145c176042c',\n'435add06-6b85-421a-8ad8-be88b7defe08',\n'7fbd7575-5f56-4fed-9642-34f587325699',\n",
      "content_length": 1546,
      "extraction_method": "Direct"
    },
    {
      "page_number": 206,
      "chapter": null,
      "content": "'6a798aca-9dcb-433c-b0d3-196acfd83b0e',\n… \n]\nNote\nThe WikipediaLoader also loads content from other Wikipedia hyperlinks\nreferenced in the requested article. For example, the Paestum article\nreferences the National Archaeological Museum of Paestum, the Lucania\nregion, Lucanians, and the temples of Hera and Athena. As a result, it will\nload these related articles, providing more content than you might expect.\nFor other document formats:\nword_loader = Docx2txtLoader(\"Paestum/Paestum-Britannica.docx\")\nword_chunks = text_splitter.split_documents(word_loader.load())\nvector_db.add_documents(word_chunks)\npdf_loader = PyPDFLoader(\"Paestum/PaestumRevisited.pdf\")\npdf_chunks = text_splitter.split_documents(pdf_loader.load())\nvector_db.add_documents(pdf_chunks)\ntxt_loader = TextLoader(\"Paestum/Paestum-Encyclopedia.txt\")\ntxt_chunks = text_splitter.split_documents(txt_loader.load())\nvector_db.add_documents(txt_chunks)\nNote\nIf you prefer, you can process the full PaestumRevisited-\nStockholmsUniversitet.pdf document instead of its shorter version,\nPaestumRevisited.pdf. This will take considerably longer, but it will provide\ninformation for a wider range of questions.\nRemoving duplication\nYou might have noticed that the code above has a lot of duplication. Let's\nextract some common functionality into a function:\ndef split_and_import(loader):\n    chunks = text_splitter.split_documents(loader.load())\n    vector_db.add_documents(chunks)\n",
      "content_length": 1441,
      "extraction_method": "Direct"
    },
    {
      "page_number": 207,
      "chapter": null,
      "content": "    print(f\"Ingested chunks created by {loader}\")\nNow you can call this function after instantiating each loader, as shown in\nListing 7.1:\nListing 7.1 Refactored ingestion of different file types\nwikipedia_loader = WikipediaLoader(query=\"Paestum\")\nsplit_and_import(wikipedia_loader)\nword_loader = Docx2txtLoader(\"Paestum/Paestum-Britannica.docx\")\nsplit_and_import(word_loader)\npdf_loader = PyPDFLoader(\"Paestum/PaestumRevisited.pdf\")\nsplit_and_import(pdf_loader)\ntxt_loader = TextLoader(\"Paestum/Paestum-Encyclopedia.txt\")\nsplit_and_import(txt_loader)\nAfter running this code, you will see output like this:\nIngested chunks created by <langchain_community.document_loaders.w\nIngested chunks created by [… SHORTENED] \n7.2.2 Ingesting Multiple Documents from a Folder\nI have just shown you how to ingest different types of documents (web, MS\nWord, PDF, txt) using a specialized document loader for each type. This\napproach works fine for a few documents, but if you need to load many\ndocuments, a more efficient method is needed. After placing all the\ndocuments in a folder (e.g., /CilentoTouristInfo), there are a couple of\nways to achieve this:\n1. Iterating over the files in the folder and calling the relevant loader.\n2. Using the purpose-built DirectoryLoader.\nLet's explore both approaches.\nIterating over all files in a folder\n",
      "content_length": 1332,
      "extraction_method": "Direct"
    },
    {
      "page_number": 208,
      "chapter": null,
      "content": "You can ingest all the files located in a folder into a vector store by iterating\nover the files, identifying the file type, and using the relevant loader as\ndescribed in the previous section. Before iterating over the files, create a\nloader factory as shown below:\nListing 7.2 Loader factory: instantiating the relevant extension-specific loader\nloader_classes = {\n   'docx': WordLoader,\n   'pdf': PdfLoader,\n   'txt': TxtLoader\n}\nimport os\ndef get_loader(filename):\n   _, file_extension = os.path.splitext(filename)\n   file_extension = file_extension.lstrip('.')\n   \n   loader_class = loader_classes.get(file_extension)\n   \n   if loader_class:\n       return loader_class(filename) \n   else:\n       raise ValueError(f\"No loader available for file extension \nExercise\nI leave you with an exercise to iterate over the files in /CilentoTouristInfo\nand load them into the vector store using get_loader() and\nsplit_and_import(). If you're unsure, check the section \"Ingesting Multiple\nDocuments from a Folder\" in my Python notebook ch07-\nQA_across_documents.ipynb on GitHub.\nIngesting all files with DirectoryLoader\nAn alternative method for loading all files in a folder into the vector store is\nby using the DirectoryLoader. This loader, part of a third-party package by\nUnstructured (Unstructured offers a platform and tools for ingesting and\nprocessing unstructured documents for RAG and fine-tuning), takes a folder\n",
      "content_length": 1417,
      "extraction_method": "Direct"
    },
    {
      "page_number": 209,
      "chapter": null,
      "content": "path and a glob pattern (a string with wildcard characters to specify a set of\nfilenames).\nFirst, install the unstructured package or the langchain-unstructured\nwrapper along with its dependencies. Follow the instructions for your\noperating system in the LangChain documentation\n(https://python.langchain.com/)> Integrations > Providers table >\nUnstructured) or the Unstructured documentation\n(https://docs.unstructured.io/).\nIn your Jupyter notebook, import the DirectoryLoader:\nfrom langchain_community.document_loaders import DirectoryLoader\nYou can now load and ingest the files into the vector store with the following\ncode:\nfolder_path = \"CilentoTouristInfo\"\npattern = \"**/*.{docx,pdf,txt}\"  #A\ndirectory_loader = DirectoryLoader(folder_path, pattern)  #B\nsplit_and_import(directory_loader)\nNOTE\nWhile the code above is included in the GitHub repository, its execution\ndepends on successfully installing the unstructured or langchain-unstructured\npackage. Because setup can vary across operating systems, the code may not\nrun consistently in all environments. I've added comments in the code to\nclarify this.\n7.3 Q&A Across Stored Documents\nNow that you have stored all the content about Paestum, let’s query the\nvector store directly to see which documents are retrieved.\n7.3.1 Querying the vector store directly\n",
      "content_length": 1320,
      "extraction_method": "Direct"
    },
    {
      "page_number": 210,
      "chapter": null,
      "content": "Let's query the vector store to see what documents are retrieved for a question\nthat requires information from different sources:\nquery = \"Where was Poseidonia and who renamed it to Paestum?\"\nresults = vector_db.similarity_search(query, 4)  #A\nprint(results)\nHere is an excerpt from the results, showing that the most relevant chunks\nreturned by the vector store come from different sources, as you can verify in\nthe related metadata:\n[Document(metadata={'source': 'Paestum/Paestum-Britannica.docx'}, \nAlthough you now know which documents the vector store returns for your\nquery, the next step is to get a well-formulated answer.\n7.3.2 Asking a Question through a LangChain Chain\nUnlike when you implemented RAG using the OpenAI gpt-4o-mini model\ndirectly in section 5.3, with LangChain, you don't need to manually craft a\nprompt and configure it with the original question and the context from the\nvector store. You can set up a RAG chain, which will automatically\ninstantiate and execute a prompt template, as shown in Figure 7.5 below.\nFigure 7.5 LangChain RAG chain, LangChain RAG chain, packaging the prompt parameters\ninto a prompt instance, which is then sent to the chat or LLM model.\n",
      "content_length": 1194,
      "extraction_method": "Direct"
    },
    {
      "page_number": 211,
      "chapter": null,
      "content": "Set up the prompt using a template from LangChain Hub\n(https://smith.langchain.com/hub). For clarity I have simply copied it and set\nit explicitly below:\nfrom langchain_core.prompts import PromptTemplate\nrag_prompt_template = \"\"\"Use the following pieces of context to a\nIf you don't know the answer, just say that you don't know, don't \nUse three sentences maximum and keep the answer as concise as pos\n{context}\nQuestion: {question}\nHelpful Answer:\"\"\"\nrag_prompt = PromptTemplate.from_template(rag_prompt_template)\nAlternatively, you can pull the prompt instance directly from LangChain\nHub:\nfrom langchain import hub\nrag_prompt = hub.pull(\"rlm/rag-prompt\")\n7.3.3 Completing the RAG Chain Setup\nTo complete the setup of the RAG chain, we need a few more components, as\nshown in Figure 7.6 below.\n",
      "content_length": 797,
      "extraction_method": "Direct"
    },
    {
      "page_number": 212,
      "chapter": null,
      "content": "Figure 7.6 Amended RAG chain, including runnable passthrough and retriever\nFigure 7.6 shows your RAG chain, which includes:\nRetriever: Retrieves relevant text content from the vector store and\ninjects it into the \"context\" parameter of the prompt.\n",
      "content_length": 248,
      "extraction_method": "Direct"
    },
    {
      "page_number": 213,
      "chapter": null,
      "content": "Question Feeder: Implemented as a simple “pass-through” component, it\nfeeds the user's question through the Runnable interface (an abstract\nclass on which every LangChain component is based).\nChat Model: Processes the prompt to generate the answer.\nInstantiate the retriever, question feeder, and chat model:\nretriever = vector_db.as_retriever()\nfrom langchain_core.runnables import RunnablePassthrough\nquestion_feeder = RunnablePassthrough()\nfrom langchain_openai import ChatOpenAI\nchatbot = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model_name=\"g\nSet up the RAG chain. As mentioned earlier, each block in a LangChain\nchain implements the Runnable interface and accepts a dictionary as input.\nThis is why the first block in the chain is a dictionary:\nrag_chain = {\"context\": retriever, \"question\": question_feeder} | \nCreate a utility function to execute the chain:\ndef execute_chain(chain, question):\n   answer = chain.invoke(question)\n   return answer\nAsk a question:\nquestion = \"Where was Poseidonia and who renamed it to Paestum? A\nanswer = execute_chain(rag_chain, question)\nprint(answer.content)\nThe answer you will get should be similar to:\nPoseidonia was located on the coast of the Tyrrhenian Sea in Magn\nInspect the full answer object if needed:\nprint(answer)\nYou should see something like:\n",
      "content_length": 1302,
      "extraction_method": "Direct"
    },
    {
      "page_number": 214,
      "chapter": null,
      "content": "content='Poseidonia was located on the coast of the Tyrrhenian Se\nFollow-up Question\nLet’s find out if the chatbot can sustain the conversation by asking a follow-\nup question. I'll make it deliberately vague to see if the chatbot can\nunderstand what I'm after:\nQuestion = \"And then, what did they do? Also tell me the source\" \nanswer = execute_chain(rag_chain, question)\nprint(answer.content)\nThe chatbot returns:\nI don't know. The source is not provided in the context.\nIt seems the chatbot is a bit lost and doesn't understand that \"they\" refers to\nthe Romans. This is because the chatbot has no memory of previous\nquestions and answers. Currently, it is stateless and simply passes questions\nfrom the user to the LLM and back, without retaining any memory of the\nconversation flow.\nLet's see how we can fix this.\n7.4 Chatbot memory of message history\nOne of the most useful features of an LLM-based chatbot, compared to an\nLLM-based engine, is its ability to remember previous questions and\nresponses, allowing you to continue querying until you get a satisfactory\nanswer. This capability is crucial for providing context to each new prompt.\nLet's see how to incorporate message history into the RAG setup we finalized\nin the previous section, which I have summarized in listing 7.3 for\nconvenience:\nListing 7.3 Initial RAG Setup Before Incorporating Message History\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.prompts import PromptTemplate\n",
      "content_length": 1478,
      "extraction_method": "Direct"
    },
    {
      "page_number": 215,
      "chapter": null,
      "content": "from langchain_openai import ChatOpenAI\nrag_prompt_template = \"\"\"Use the following pieces of context to a\nIf you don't know the answer, just say that you don't know, don't \nUse three sentences maximum and keep the answer as concise as pos\n{context}\nQuestion: {question}\nHelpful Answer:\"\"\"\nrag_prompt = PromptTemplate.from_template(rag_prompt_template)\nretriever = vector_db.as_retriever()\nquestion_feeder = RunnablePassthrough()\nchatbot = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model_name=\"g\nrag_chain = {\"context\": retriever, \"question\": question_feeder} | \ndef execute_chain(chain, question):\n   answer = rag_chain.invoke(question)\n   return answer\nFirst, we should amend the prompt to include message history.\n7.4.1 Amending the Prompt\nThe original RAG prompt doesn't account for message history, so we need to\nmodify it. Since message history is a core feature of the memory-enabled\nRAG design, we should use a different prompt helper than\nPromptTemplate.from_template. This helper was based on a template\nparameterized by a user question ({question}) and the context ({context})\npulled by the retriever from the vector store. LangChain provides a more\nsuitable prompt helper, ChatPromptTemplate.from_messages, which creates\na prompt from a list of chat messages.\nChat Messages\nMost chat-oriented LLMs use a standard format for messages and roles\nassociated with chat messages. A chat message is typically a key-value pair\nlike this:\n",
      "content_length": 1441,
      "extraction_method": "Direct"
    },
    {
      "page_number": 216,
      "chapter": null,
      "content": "(\"role\": \"message text\")\nWhere:\n\"role\" can be \"system\", \"human\", or \"ai\" (see table 7.1 for\ndescriptions).\n\"message text\" stands for the text of the exchanged message.\nTable 7.1 Chat Message Roles\nRole\nDescription\nSample message\nSystem\nThis role represents the chatbot\napplication. You typically\ncreate one system message at\nstartup to instruct the chatbot\non its persona.\nYou are a world-class expert\nin Roman and Greek history,\nespecially in towns located\nin southern Italy. Provide\ninteresting insights on local\nhistory and recommend\nplaces to visit with\nknowledgeable and\nengaging answers.\nHuman\nThis message comes from a\nuser, typically a question.\nCan you please recommend\nsome attractions around\nPaestum and give me some\ninformation on them?\nAI\nThis is the synthesized\nresponse from the LLM.\nThe best attractions in\nPaestum are the three Greek\ntemples built around 500\nBC.\nCustom\nYou can use other non-\nstandard roles to incorporate\nmessages containing useful\ninformation. For example,\n\"Context\" for text retrieved\nfrom the vector store, or \"Chat\nHistory\" for the entire chat\nhistory.\n \n",
      "content_length": 1094,
      "extraction_method": "Direct"
    },
    {
      "page_number": 217,
      "chapter": null,
      "content": "A chat history is a list of such messages:\nchat_history = [\n   (\"system\", \"You are a world-class expert in Roman and Greek h\n   (\"human\", \"Can you please recommend some attractions around P\n   (\"context\", \"Paestum was a major ancient Greek city on the co\n   (\"ai\", \"The best attractions in Paestum are the three Greek t\n]\nChat-Based Prompt\nNow that you understand how to model chat messages, you can create a\nmessage-based prompt:\nfrom langchain_core.prompts import ChatPromptTemplate\nrag_prompt = ChatPromptTemplate.from_messages(\n   [\n       (\"system\", \"You are a helpful assistant, world-class expe\n       (\"chat_history\", \"{chat_history_messages}\"),\n       (\"context\", \"{retrieved_context}\"),\n       (\"human\", \"{question}\"),\n   ]\n)\nYou will re-instantiate this prompt at each interaction with the user.\nSpecifically, after the initial interaction, which starts with an empty\nchat_history, you will feed the new {question}, the newly\n{retrieved_context}, and the accumulated {chat_history_memory}.\nYou already know how to feed a new question and a newly retrieved context:\nit’s the same as before, but the prompt is now message-based.\nIn the next section, I will show you how to update the message history at\neach user interaction.\n7.4.2 Updating the Chat Message History\nLangChain provides the ChatMessageHistory class to model chat message\nhistory. We can instantiate the chat_history_memory variable as a\nChatMessageHistory type to hold the chat history:\n",
      "content_length": 1461,
      "extraction_method": "Direct"
    },
    {
      "page_number": 218,
      "chapter": null,
      "content": "from langchain_community.chat_message_histories import ChatMessag\nchat_history_memory = ChatMessageHistory()\nYou can add messages for Human (user question) and AI (LLM response) to\nthe chat history using the ChatMessageHistory convenience methods listed\nin Table 7.2.\nTable 7.2 ChatMessageHistory Convenience Methods for Standard Roles\nRole\nConvenience Method\nHuman\nadd_user_message(user_question)\nAI\nadd_ai_message(llm_response)\nYou don't need to add messages associated with the System and Context roles\nto the chat message history, as they are already part of the prompt and won't\nprovide the LLM with additional information. The only messages that need\ntracking are those encapsulating user questions and LLM responses.\nUpdate the chat history in the execute_chain() function as follows:\ndef execute_chain_with_memory(chain, question):\n   chat_history_memory.add_user_message(question)\n   answer = chain.invoke(question)\n   chat_history_memory.add_ai_message(answer)\n   print(f'Full chat message history: {chat_history_memory.messa\n   return answer\nWhen the chat_history_memory object is updated with the latest Human or\nAI messages, you can retrieve the entire message history using the messages\nproperty of the ChatMessageHistory class:\nfull_message_history = chat_history_memory.messages\nYou will inject this into the prompt each time the user asks a new question.\n7.4.3 Feeding the Chat History to the RAG Chain\nAfter updating your code to include message history in the prompt and\nupdating it after each user question and LLM response, you need to feed the\nupdated message history to the RAG chain. You can do this by redefining the\n",
      "content_length": 1642,
      "extraction_method": "Direct"
    },
    {
      "page_number": 219,
      "chapter": null,
      "content": "chain with LCEL:\nrag_chain = {\n   \"retrieved_context\": retriever, \n   \"question\": question_feeder,\n   \"chat_history_messages\": chat_history_memory.messages\n} | rag_prompt | chatbot\nIn this setup, the chat_history_messages prompt parameter is fed through\nthe corresponding chat_history_messages property above.\n7.4.4 Putting Everything Together\nTo clarify the changes made for integrating chatbot memory into the RAG\nchain, here's the complete code in Listing 7.4.\nListing 7.4 RAG Chain with Chatbot Memory\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_community.chat_message_histories import ChatMessag\nfrom langchain_core.runnables import RunnableLambda\nrag_prompt = ChatPromptTemplate.from_messages(\n   [\n       (\"system\", \"You are a helpful assistant, world-class expe\n       (\"placeholder\", \"{chat_history_messages}\"),\n       (\"assistant\", \"{retrieved_context}\"),\n       (\"human\", \"{question}\"),\n   ]\n)\nretriever = vector_db.as_retriever()\nquestion_feeder = RunnablePassthrough()\nchatbot = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model_name=\"g\nchat_history_memory = ChatMessageHistory()\ndef get_messages(x):\n   return chat_history_memory.messages\nrag_chain = {\n   \"retrieved_context\": retriever, \n   \"question\": question_feeder,\n",
      "content_length": 1355,
      "extraction_method": "Direct"
    },
    {
      "page_number": 220,
      "chapter": null,
      "content": "   \"chat_history_messages\": RunnableLambda(get_messages)\n} | rag_prompt | chatbot\ndef execute_chain_with_memory(chain, question):\n   chat_history_memory.add_user_message(question)\n   answer = chain.invoke(question)\n   chat_history_memory.add_ai_message(answer)\n   print(f'Full chat message history: {chat_history_memory.messa\n   return answer\nTesting the Amended Chain\nNow, let's test the updated chain with the same question we asked previously:\nquestion = \"Where was Poseidonia and who renamed it to Paestum? A\nanswer = execute_chain_with_memory(rag_chain, question)\nprint(answer.content)\nHere is the chat message history accumulated so far, followed by the answer:\nFull chat message history: [HumanMessage(content='Where was Posei\nPoseidonia was an ancient Greek city located on the coast of the Tyrrhenian\nSea in what is now southern Italy. It was renamed Paestum by the Romans\nafter they took control of the city in 273 BC, following its conquest by the\nLucanians. The source of this information is from Wikipedia.\nThe answer is similar to what you got with the memoryless chatbot. Let’s see\nwhat happens if we now ask the same follow up-question we asked\npreviously:\nquestion = \"And then what did they do? Also tell me the source\" \nanswer = execute_chain_with_memory(rag_chain, question)\nprint(answer.content)\nNow we get the following response:\nFull chat message history: [HumanMessage(content='Where was Posei\n",
      "content_length": 1417,
      "extraction_method": "Direct"
    },
    {
      "page_number": 221,
      "chapter": null,
      "content": "After the Romans renamed Poseidonia to Paestum, they developed the city\nfurther, enhancing its infrastructure and economy, particularly through\nagriculture and trade. They also constructed significant buildings, including\ntemples dedicated to Greek gods, which demonstrate the city's cultural\ncontinuity. The source of this information is from historical texts on Roman\nhistory and archaeology.\nAs you can see, the chat history now contains all the exchanged questions\nand answers. Most importantly, the chatbot understands that \"they\" refers to\nthe Romans and provides a coherent response. Congratulations! You have\nnow completed a fully functional chatbot that remembers previous messages\nand can sustain a proper conversation.\nBefore considering the chatbot complete, I would like to cover another\nimportant topic: tracing its chain execution with LangSmith.\n7.5 Tracing Execution with LangSmith\nLangSmith is a comprehensive developer platform for every stage of the\nLLM-based application lifecycle, whether you’re using LangChain or not. It\nhelps you debug, collaborate on, test, and monitor your LLM applications.\nLLM applications can be unpredictable due to their natural language inputs,\nwhich can create edge cases that are hard to reproduce and debug with\ntraditional tools. LangSmith addresses these challenges throughout the LLM\napplication development lifecycle:\nDevelopment and Debugging: LangSmith's tracing feature ensures your\nLLM application workflow executes as expected and helps troubleshoot\ndeviations. Its Hub provides standard, battle-tested prompts for common\nuse cases, speeding up implementation.\nEvaluation and Testing: LangSmith supports testing with built-in\nevaluators for relevance, correctness, and sensitivity of LLM\ncompletions. It also provides tools to build datasets from various\nsources, including production data, for continuous and regression\ntesting.\nMonitoring: LangSmith's tracing capabilities allow you to monitor the\nreal-time status of your LLM production application. It supports\n",
      "content_length": 2027,
      "extraction_method": "Direct"
    },
    {
      "page_number": 222,
      "chapter": null,
      "content": "feedback through human labels and annotations, enabling investigation\nand correction when the application deviates from the expected path.\nTo enable LangSmith's tracing, you just need to set up the LangSmith API\nkey and some tracing configurations at the top of your application. For more\ndetails on LangSmith and how to set up the API key, refer to the sidebar\nbelow.\nSidebar: Setting up LangSmith’s API Key\nSet up a LangSmith API key as follows:\n1. Register at LangSmith: https://smith.langchain.com/. Click the Sign-Up\nbutton to create an account using GitHub, Google, Discord, or an email\naddress.\n2. Log in to the LangSmith website, click the Settings cog-wheel on the\nleft-hand menu, and then click the Api Keys menu to access the Api\nKeys page.\nApi Keys screen - Access this screen by clicking the Settings cog-wheel and then the Api Keys\nmenu; you can create a new API key here.\n",
      "content_length": 887,
      "extraction_method": "Direct"
    },
    {
      "page_number": 223,
      "chapter": null,
      "content": "3. Click the Create API Key button in the top-right corner. Provide a\ndescription for the key (e.g., LangChain in Action examples) and select\nPersonal Access Token as the key type. Then click Create API Key. Be\n",
      "content_length": 211,
      "extraction_method": "Direct"
    },
    {
      "page_number": 224,
      "chapter": null,
      "content": "sure to copy the key and store it in a secure location, as you won’t be\nable to view it again on the LangSmith site—only the first and last few\ncharacters will remain visible.\nWith the LangSmith API key, you’ll be able to use it in your projects.\nThe easiest way to enable tracing through LangSmith is by setting a few\nenvironment variables before launching your Jupyter notebook. If your\nnotebook is already running, close it first. Then, follow these steps (I am\nassuming you are using in a Windows cmd shell):\n1. Navigate to the notebook folder and activate your virtual environment:\nC:\\Github\\building-llm-applications\\ch07> env_ch07\\Scripts\\activa\n2. Set the following environment veriables, which activate and configure\ntracing through LangSmith (I assume you are in C:\\Github\\building-llm-\napplications\\ch07, so I have shortened the folder name below):\n(env_ch07) C:\\...\\ch07>set LANGSMITH_TRACING=true\n(env_ch07) C:\\...\\ch07>set LANGSMITH_ENDPOINT=https://api.smith.l\n(env_ch07) C:\\...\\ch07>set LANGSMITH_PROJECT=Q & A chatbot\n(env_ch07) C:\\...\\ch07>set LANGSMITH_API_KEY=<YOUR_LANGSMITH_API_\n3. Restart the Jupyter notebook:\n(env_ch07) C:\\...\\ch07>jupyter notebook 07-QA_across_documents.ipynb\n4. Re-run the notebook cells up to the end of Section 7.3 (Q&A Across\nDocuments) or optionally through Section 7.4 (Chatbot Memory).\nOnce you’ve completed these steps, LangSmith will have captured tracing\ninformation for your notebook execution. You can view and analyze this\ntrace data directly in the LangSmith dashboard.\n7.5.1 Inspecting the LangSmith Traces\nGo to the LangSmith website (smith.langchain.com) > Projects > Q&A\nChatbot. Click the latest trace, as shown in the screenshot below:\n",
      "content_length": 1699,
      "extraction_method": "Direct"
    },
    {
      "page_number": 225,
      "chapter": null,
      "content": "Figure 7.7 LangSmith high-level trace: click the latest trace to get high-level details of the chain\nexecution.\nThe current view shows all the traces associated with your Q&A Chatbot\nproject (a project, in LangSmith’s terminology is the collection of all the\ntraces associated to an application or to a part of an application), ordered by\nexecution time, with the most recent at the top. When you click any trace,\nsuch as the latest one at the top, you will get more details. I have split the\nthree panels into two figures for clarity. Figure 7.8 shows the left-hand panel\nand the middle panel.\nFigure 7.8 LangSmith trace details obtained by clicking one of the traces, for example, the latest\none at the top. You will see three panels: 1) The left panel shows all traces of the Chatbot Q&A\nproject; 2) the middle panel shows the runs of the selected trace.\n",
      "content_length": 858,
      "extraction_method": "Direct"
    },
    {
      "page_number": 226,
      "chapter": null,
      "content": "Figure 7.9 shows the middle panel and the right-hand panel.\nFigure 7.9 Middle and right-hand panels you get when clicking the latest trace; 1) the middle\npanel shows the runs of the selected trace; 2) the right panel shows the input to the selected trace\nand its output.\n",
      "content_length": 271,
      "extraction_method": "Direct"
    },
    {
      "page_number": 227,
      "chapter": null,
      "content": "Let's go through the three panels shown in the figures above that you see\nwhen clicking the latest trace on the Q&A Chatbot project webpage:\nLeft Panel: Shows the list of all traces associated with your project\n(Chatbot Q&A), ordered from the latest at the top to the oldest at the\nbottom. Each trace can be expanded into its inner steps. Click the latest\n",
      "content_length": 356,
      "extraction_method": "Direct"
    },
    {
      "page_number": 228,
      "chapter": null,
      "content": "trace, which is the top one.\nMiddle Panel: Graphically displays the chain execution runs for the\nselected trace, including the vector-store Retriever and the ChatOpenAI\nLLM client. Each run represents the execution details of a chain\ncomponent within the trace. These runs are ordered by execution time\nand include their duration down to the centisecond.\nRight Panel: Shows the trace input (the user question) and its output (the\nsynthetized response).\nYou can get further details by clicking one of the runs in the middle panel, as\nshown in figure 7.10 below:\nFigure 7.10 Run details; when you click the Retriever run, its details appear in the right-hand\npanel, showing its input (the user question) and its output (the documents retrieved from the\nvector store).\n",
      "content_length": 766,
      "extraction_method": "Direct"
    },
    {
      "page_number": 229,
      "chapter": null,
      "content": "For instance, clicking the Retriever run in the middle panel shows its details\nin the right-hand panel. You will see the original query in the Input section\nand the documents retrieved from the vector store in the Output section.\nThis is just a glimpse of LangSmith's tracing capabilities using a simple\n",
      "content_length": 304,
      "extraction_method": "Direct"
    },
    {
      "page_number": 230,
      "chapter": null,
      "content": "chain. I encourage you to examine each chain step in detail. Start by clicking\na trace substep in the left panel and then inspect its associated runs by\nselecting the one you want to examine in the middle panel. If you're eager to\nexperiment, create a new LangSmith project to trace the execution of the\nresearch summarization engine we built in chapter 4. This will help you\nappreciate a more complex trace that captures a wider range of chain\ncomponents.\nBefore closing the chapter, I want to mention a useful LangChain class you\nmight consider for simple Q&A chains.\n7.6 Creating a Q&A Chain with RetrievalQA\nDepending on your specific use case, you might set up a Q&A chain more\neffectively with the RetrievalQA chain utility function. This is similar to the\nMapReduceDocumentsChain and load_summarize_chain functions used for\nsummarization chains in the previous chapter:\nfrom langchain.chains import RetrievalQA\nrag_chain = RetrievalQA.from_chain_type(\n  llm=chatbot,  #A\n  chain_type=\"stuff\",  #B\n  retriever=retriever,  #C\n  return_source_documents=False  #D\n)\nYou can now execute it with the run() method:\nquestion = \"Where was Poseidonia and who renamed it to Paestum? A\nwith trace(\"RetrievalQA\", \"chain\", project_name=\"Q&A chatbot\", in\n   answer = execute_chain(rag_chain, question)\n   print(answer)\n   rt.end(outputs={\"output\": answer})\nThis returns the following result, similar to what we got earlier. Notice the\ninput and output properties are called “query” and “result” respectively. Feel\nfree to inspect the RetrievalQA trace in detail on the LangSmith website.\n{'query': 'Where was Poseidonia and who renamed it to Paestum? Al\n",
      "content_length": 1646,
      "extraction_method": "Direct"
    },
    {
      "page_number": 231,
      "chapter": null,
      "content": "Before moving on, check out the sidebar on how to configure the retriever for\nother types of searches.\nConfiguring the Retriever\nYou can configure the retriever more accurately by setting its parameters:\nsearch_type: Specifies the search algorithm to retrieve from the vector\ndatabase.\nsearch_kwargs: Specifies the parameters associated with the search\nalgorithm. In Python, kwargs refers to a special syntax (**kwargs) that\nallows you to pass a variable number of keyword arguments to a\nfunction. It collects these arguments into a dictionary, making it easy to\nhandle optional or dynamic function parameters.\nBy default, the retriever is configured for a plain Similarity Search (with the\nimplicit configuration of `search_type = \"similarity\"`). However, you can\nexplicitly configure it for a Similarity Search with a Threshold using\n`search_type=\"similarity_score_threshold\"` or for a Max Marginal Relevance\nSearch with `search_type = \"mmr\"`. You can provide additional\nconfiguration details through the `search_kwargs` parameter.\nFor example, to perform a similarity search and exclude any result with a\nsimilarity score lower than 0.8, while returning 3 results, you can configure\nthe retriever as follows:\nretriever = vector_db.as_retriever(\n   search_type=\"similarity_score_threshold\",\n   search_kwargs={'score_threshold': 0.8, 'k': 3}\n)\nConsult the official documentation for more information on search\nparameters.\n7.7 Summary\nLangChain abstracts core components of RAG, making the design\nmodular and flexible.\n",
      "content_length": 1519,
      "extraction_method": "Direct"
    },
    {
      "page_number": 232,
      "chapter": null,
      "content": "LangChain's object model includes classes that simplify and abstract the\nRAG content ingestion phase, such as:\nBaseLoader: Imports a text source into one or more Document\nobjects.\nBaseDocumentTransformer: Transforms a loaded Document, for\nexample, by splitting it into smaller Document chunks.\nVectorStore: Efficiently stores Document objects optimized for\nsemantic searching.\nEmbedding Models: Index Document objects with their\nembeddings (vector representations) when storing them in the\nvector store.\nThe Q&A or retrieval stage of the RAG solution is supported by\nLangChain abstractions, including:\nBasePromptTemplate: Models a generic prompt and produces a\nspecific PromptValue instance after parameters are fed to it.\nBaseRetriever: Abstracts the retrieval process from the vector store.\nBaseLanguageModel: Abstracts clients to LLMs.\nA typical RAG chain includes a passthrough component, a vector store\nwith a retriever, a prompt template, and an LLM or chat client.\nLLM-based chatbots retain memory of previous questions and\nresponses, preserving context for ongoing queries until a satisfactory\nanswer is achieved.\nUse LangChain's ChatPromptTemplate.from_messages to create\nprompts and ChatMessageHistory to manage chat history, feeding it into\nthe RAG chain for each interaction between user and LLM.\nLangSmith allows you to analyze execution runs and traces, drilling\ndown into each step to examine the input and output of each component.\nLangChain provides additional specific classes for building Q&A\nchatbots tailored to simpler use cases.\n",
      "content_length": 1552,
      "extraction_method": "Direct"
    },
    {
      "page_number": 233,
      "chapter": null,
      "content": "8 Advanced indexing\nThis chapter covers\nAdvanced RAG techniques for more effective retrieval\nSelecting the optimal chunk splitting strategy for your use case\nUsing multiple embeddings to enhance coarse chunk retrieval\nExpanding granular chunks to add context during retrieval\nIndexing strategies for semi-structured and multi-modal content\nIn chapter 7, you learned about the basics of the RAG (Retrieval-Augmented\nGeneration) architecture, a common pattern for building LLM-based\napplications. To simplify things, I introduced a stripped-down version. While\nthis was useful for learning, using this basic version in a real-world scenario\nwill often produce poor results. You might notice inaccurate answers, even\nwhen your content store (usually a vector store) has relevant data. These\nissues often arise from poorly phrased queries, inefficient text indexing, or\nineffective use of metadata attached to text chunks.\nIn this chapter, I’ll show you how to avoid these issues and use advanced\nmethods to improve accuracy. When working with LLM applications using\nLangChain, most of your time will be spent refining your design to get the\nbest results. This involves selecting advanced RAG techniques,\nexperimenting, and refining prompts. You can call yourself proficient only\nwhen you understand and apply these techniques effectively.\nHere, we’ll cover advanced indexing strategies, including creating multiple\nembeddings for larger text chunks stored in the vector database. Using this\nmethod boosts search precision and provides better context for generating\nhigh-quality responses.\n8.1 Improving RAG Accuracy\nTo improve RAG (Retrieval-Augmented Generation) accuracy, analyze each\n",
      "content_length": 1684,
      "extraction_method": "Direct"
    },
    {
      "page_number": 234,
      "chapter": null,
      "content": "step in both the content ingestion and Q&A workflows. Each stage may pose\nspecific issues, but it also opens up opportunities for improvement. Let's start\nwith the content ingestion stage.\n8.1.1 Content Ingestion Stage\nRetrieval accuracy can be improved through an optimized content ingestion\nprocess that aligns with the specific features of each content store. Relying\nonly on basic indexing can reduce indexing depth and weaken retrieval\nperformance. Figure 8.1 shows two key areas for improvement in the\ningestion stage: refining embedding calculations and optimizing how\nembeddings are linked to related text chunks in the vector store.\nFigure 8.1 Common accuracy issues in the ingestion stage of a simple RAG architecture are often\ndue to inadequate indexing that only uses basic embeddings for each text chunk. Advanced\nindexing techniques involve generating multiple embeddings for each chunk, enhancing\nsearchability.\n",
      "content_length": 927,
      "extraction_method": "Direct"
    },
    {
      "page_number": 235,
      "chapter": null,
      "content": "RAG Ingestion phase\n\n1, request the embedding service to calculate\nthe embeddings of the document chunks\n\nae 2 retum embeddings Embeddings\nText Ingestion script service\n\n3, store documents chunks and related\nembeddings into Vector store\n\nAE\n\nTypically we only calculate the\nembedding of the content of the\ntext chunk, However we can\ncalculate multiple embeddings to\nmake the chunk more searchable\n\nVector DB\n\nAdvanced indexing",
      "content_length": 426,
      "extraction_method": "OCR"
    },
    {
      "page_number": 236,
      "chapter": null,
      "content": "Even if a question is clear, retrieval can fail with overly simple indexing\nstrategies. In vector indexing, chunk size and overlap length are crucial:\nsmaller chunks may work well for precise questions but fail with broader\nqueries, while larger chunks may lack detail for specific questions. To\naddress this, you can add additional embeddings based on distinct chunk\nfeatures, as illustrated in the figure above. This multi-faceted approach helps\nmake each text chunk more adaptable and searchable. We’ll explore\nadvanced indexing techniques in this chapter.\n8.1.2 Question Answering Stage\nThe effectiveness of the question-answering stage depends largely on how\naccurately the system interprets and processes user queries. Many potential\nissues, highlighted in Figure 8.2, can disrupt this process.\nFigure 8.2 Common Issues in the Q&A stage of the naive RAG architecture and their solutions:1)\nHandle poorly formulated questions with question transformation; 2) Enhance retrieval accuracy\nby transforming the original user questions into more suitable vector DB search queries; 3)\nInclude relevant data sources by adding structured data content stores, such as relational DBs; 4)\nGenerate DB queries for structured data content stores; 5) Filter out irrelevant context retrieved\nfrom the content stores.\n",
      "content_length": 1306,
      "extraction_method": "Direct"
    },
    {
      "page_number": 237,
      "chapter": null,
      "content": "RAG Q8A phase\n\nQuestions can be imprecise,\nincorrect or too vague. They can be\nreformulated as better questions\n\nThe context retrieved from the\ncontent stores might not be entirely\n\nrelevant for the user question,\n\nYou should filter out irrelevant\ncontext,\n\n1) Question transformation\n5) Remove irrelevant context\n\n4 Prompt witha question\n1. User question and context (Search results)\n———>\nChatbot —\n\nee\n\n—_—_.___{.\nUser\n\n6. Chatbot answer 5 LLM completion\n\nChatbot UI LLM\n\n2. User wi 3, Search results\n\nChatbot UI\nThe original question\nmight not be suitable for\nthe retrieval step. It The vector store might have limited\nmight need to be adapted as\n\na more suitable vector DB search\n\nor incomplete information to\nquery\n\nanswer the question. You might have\nto include additional content stores\nand generate the related structured DB\nqueries from the original user question.\n\n2) Query generation for retrieval Vector DB\n\n3) Include additional content stores\nRelational 0B such as relational databases\n\n4) generate structured DB queries from\nthe natural language user question",
      "content_length": 1074,
      "extraction_method": "OCR"
    },
    {
      "page_number": 238,
      "chapter": null,
      "content": "Walking through the workflow of the Q&A stage of the RAG architecture,\nshown in the diagram above, you’ll encounter several pitfalls that can lead to\nsuboptimal answers. Each issue has a targeted solution:\nPoor question formulation: If a user’s question is unclear, the vector\nstore may return weak context, leading to poor results. The LLM\nstruggles when working with unclear queries and subpar context. A fix\nis to rephrase the question into a clearer, more detailed form before\npassing it to the retrieval system.\nIneffective question for retrieval: Using the original question for both\nretrieval and generation can fail, especially when the query is broad or\nabstract. Broad questions may not pinpoint relevant content, resulting in\nless accurate context. You can address this by breaking down broad\nquestions into specific sub-questions to retrieve more precise\ninformation.\nLimited data relevance in the content store: Most RAG systems rely\nonly on vector stores, but adding structured data sources, like relational\ndatabases, tables, or graph databases, can improve results. Route queries\nto the appropriate content store based on the type of data needed to\nenhance answer accuracy.\nLimited querying capabilities against structured data: While vector\nstores and LLMs excel with natural language, relational and graph\ndatabases don’t process it directly, creating a barrier to using structured\ncontent. Use an LLM to generate structured queries (e.g., SQL) tailored\nto each data source to overcome this.\nIrrelevant search results fed to the LLM: Even with clearer questions\nand better indexing, irrelevant data can sometimes slip through, adding\nnoise to the answer. Reduce this by applying filtering or post-processing\nsteps to keep only the most relevant results.\nInsufficient improvement in answer accuracy: Sometimes fixing\nindividual issues doesn’t yield expected improvements. In these cases,\nboost accuracy by combining multiple techniques—such as advanced\nindexing, question transformations, and multi-store routing—into an\nensemble strategy that maximizes precision.\nThe table below summarizes these issues and their solutions.\n",
      "content_length": 2143,
      "extraction_method": "Direct"
    },
    {
      "page_number": 239,
      "chapter": null,
      "content": "Table 8.1 Common issues in Naïve RAG architecture and recommended solutions\nIssue\nSolution\nRetrieval returns the wrong content\nchunks\nAdvanced document indexing\ntechniques\nPoor question formulation\nQuestion transformations\nIneffective question for retrieval\nQuestion transformations\nLimited data relevance in content\nstore\nRouting to multiple content stores\nLimited querying capabilities for\nstructured data\nContent store query generation\nIrrelevant retrieved results fed to\nLLM\nRetrieval post-processing\nThis chapter, along with the next two, will dive into each problem and its\nsolution in detail.\n8.2 Advanced Document Indexing\nFor an LLM to generate high-quality answers, the relevance and accuracy of\nthe text chunks retrieved from the vector store (or any document store) are\ncritical. The quality of these chunks depends on several key factors:\nSplitting Strategy: The granularity of document chunks impacts retrieval\naccuracy. Smaller chunks yield more precise results for specific queries\nbut lack broader context. Larger chunks offer richer context but may\nmiss fine details. Choosing the right split size is essential and can be\noptimized using various techniques. Additional factors—such as chunk\noverlap and document hierarchy—also play a crucial role in balancing\ncontext and relevance.\nEmbedding Strategy: How you index each chunk is equally important.\nYou can use embeddings, metadata, or a combination. Advanced\nstrategies use multiple indexes—such as child document embeddings,\nsummaries, or hypothetical questions associated with a chunk—to\ncapture both fine-grained details and broader context.\nSentence Expansion: One method to deliver larger chunks without\nlosing detail is to expand smaller chunks by including surrounding\n",
      "content_length": 1746,
      "extraction_method": "Direct"
    },
    {
      "page_number": 240,
      "chapter": null,
      "content": "sentences during retrieval. This approach provides additional context\nwithout sacrificing specificity.\nIndexing Structured and Semi-Structured Data: Retrieving structured\ndata (e.g., database tables or multimedia content) using unstructured\nqueries requires specialized techniques. This can include generating\nembeddings for database rows, images, or even audio files.\nThis chapter will cover each of these methods in detail, starting with\nstrategies for optimal document splitting.\n8.3 Splitting Strategy\nDuring the ingestion phase of RAG, documents are split into chunks before\nbeing stored in a vector database or document store. Each chunk is indexed\nusing embeddings and sometimes metadata (e.g., by tagging it with relevant\nkeywords). Vector similarity searches rely on the embeddings index, while\nmetadata searches use the keyword index.\nThe easiest way to improve the relevance of document chunk retrieval is to\nchoose the right document splitting strategy for your use case. Ideally, the\ndocument store should return all relevant chunks that provide the LLM with\nenough context to generate accurate answers. The size of these chunks plays\na critical role in retrieval performance, as shown in figure 8.3.\nFigure 8.3 Impact of Chunk Size on Answer Accuracy. Coarse chunks provide more context but\nless focus, and they are suitable for broader questions. Granular chunks provide less context but\nmore focus and they are suitable for more detailed questions.\n",
      "content_length": 1465,
      "extraction_method": "Direct"
    },
    {
      "page_number": 241,
      "chapter": null,
      "content": "Smaller, more granular chunks are better suited for answering detailed\nquestions because they focus on specific topics. However, they contain less\n",
      "content_length": 147,
      "extraction_method": "Direct"
    },
    {
      "page_number": 242,
      "chapter": null,
      "content": "surrounding context, which can reduce effectiveness for broader queries. In\ncontrast, larger chunks are more effective for general questions since they\nprovide more context but lose focus on fine-grained details.\nThe challenge is to balance chunk size based on expected question types.\nSmall chunks work well when queries are precise, as the vector\nrepresentation of the question will match closely with those of the relevant\nchunks. But for broader questions, small chunks might miss context, making\nretrieval less accurate. Larger chunks help cover broader topics but at the cost\nof losing detailed semantic information. The added context in these larger\nchunks, however, can be valuable during answer generation, as it provides\nthe LLM with more background data to work with.\nFinding the right balance between granular and coarse chunks depends on\nyour use case. Ask yourself: are the questions likely to be detailed or broad?\nThe chunk size should match the expected query type.\nSplitting strategies\nThe size of the chunks isn’t the only factor. You also need to decide how to\nsplit the document. There are two main approaches:\n1. Splitting by Document Hierarchy: This approach respects the natural\nstructure of the document (e.g., chapters, sections, paragraphs). It works\nwell when the document is organized by topics, as chunks represent\ncoherent subtopics. Tools like HTMLHeaderTextSplitter and\nMarkdownHeaderTextSplitter in LangChain target specific document\ntypes and maintain semantic accuracy. However, chunk sizes can vary\ngreatly.\n2. Splitting by Absolute Size: You can define chunk size by characters,\ntokens, sentences, or words. This results in more consistent chunk sizes,\nbut context might be lost if chunks split mid-sentence.\nCharacterTextSplitter and its variants support different granularity\nlevels, but you’ll need to test for optimal size. Evaluating a range of\nfixed sizes is necessary to find what works best for your use case.\nFactors to Consider\n",
      "content_length": 1976,
      "extraction_method": "Direct"
    },
    {
      "page_number": 243,
      "chapter": null,
      "content": "For each splitting strategy, keep in mind:\nDocument Type: If you’re working with mixed content (e.g., text, tables,\nimages), maintaining related content within the same chunk is crucial. In\nthis case, splitting by document hierarchy is more effective than a fixed-\nsize approach.\nSearch Type: if you are planning to use metadata search, keywords can\nbe refined depending on the chunk granularity. You can also attach a\nmix of broad and detailed tags to each chunk to increase retrieval\nflexibility.\nChoosing the Right Strategy\nSelecting the best strategy often requires a mix of trial and error, but\nexperience will eventually guide you to the right balance between document\nhierarchy and absolute size. Table 8.2 summarizes the pros and cons of each\nstrategy and provides relevant LangChain classes to implement them.\nTable 8.2 Splitting Strategies, Pros and Cons, and Related LangChain Classes\nSplitting\nStrategy\nPros\nCons\nLangChain Classes\nDocument\nHierarchy\nMore\naccurate\nsemantic\nmeaning\nChunk size\nvaries\nsignificantly\nHTMLHeaderTextSplitter,\nMarkdownHeaderTextSplitter\nBy Size -\nNumber\nof Tokens\nConsistent\nchunk size\nIncomplete\nsentences at\nboundaries\nCharacterTextSplitter.\nfrom_tiktoken_encoder\nBy Size -\nNumber\nof\nCharacters\nConsistent\nchunk size\nIncomplete\nsentences\nmay reduce\nsemantic\nvalue\nCharacterTextSplitter\nBy\nSentence,\nParagraph,\nRetains\nsemantic\nmeaning\nSmall\nchunks may\nlack enough\nRecursiveCharacterTextSplitter\n",
      "content_length": 1436,
      "extraction_method": "Direct"
    },
    {
      "page_number": 244,
      "chapter": null,
      "content": "or Word\nin most\ncases\ncontext\nEach method has its use case, and the choice depends on your specific needs\nand document structure. In the following sections, I’ll cover these strategies\nin detail, starting with document hierarchy splitting.\n8.3.1 Splitting by HTML Header\nIn this section, I’ll show you how to split documents using the\nHTMLHeaderTextSplitter class on various online documents from\nWikivoyage.org, a travel-focused site from the same group as Wikipedia.\nWe’ll explore how different levels of splitting granularity affect the accuracy\nof responses.\nFirst, set up your environment by creating a new folder and virtual\nenvironment, and then install the required packages. If you’ve cloned the\nrepository from GitHub, you can install dependencies directly with the\nprovided requirements file:\nC:\\Github\\building-llm-applications\\ch08> python -m venv env_ch08\nC:\\Github\\building-llm-applications\\ch08> env_ch08\\Scripts\\activa\n(env_ch08) C:\\Github\\building-llm-applications\\ch08> pip install \nNext, start a new Jupyter notebook or open the existing one in the cloned\nrepository:\n(env_ch08) C:\\Github\\building-llm-applications\\ch08> jupyter note\nSave the notebook as 08-advanced_indexing.ipynb and import the required\nlibraries:\nfrom langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nimport getpass\nOPENAI_API_KEY = getpass.getpass('Enter your OPENAI_API_KEY')\nSetting Up Chroma DB Collections\n",
      "content_length": 1429,
      "extraction_method": "Direct"
    },
    {
      "page_number": 245,
      "chapter": null,
      "content": "Now, create a Chroma DB collection to store the more granular chunks:\ncornwall_granular_collection = Chroma(  #A\n   collection_name=\"cornwall_granular\",\n   embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_\n)\ncornwall_granular_collection.reset_collection()  #B\nThis will initialize a new Chroma collection called\ncornwall_granular_collection. If the collection already exists, it will be\nreset to start fresh.\nNext, set up a second collection for coarser chunks:\ncornwall_coarse_collection = Chroma(  #A\n   collection_name=\"cornwall_coarse\",\n   embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_\n)\ncornwall_coarse_collection.reset_collection()  #B\nLoading HTML Content with AsyncHtmlLoader\nThe final step is to ingest some content about Cornwall, a region in the UK\nknown for its stunning seaside resorts, using an HTML loader:\nfrom langchain_community.document_loaders import AsyncHtmlLoader\ndestination_url = \"https://en.wikivoyage.org/wiki/Cornwall\"\nhtml_loader = AsyncHtmlLoader(destination_url)\ndocs = html_loader.load()\nThis snippet fetches the Cornwall page content, which we’ll use to create\nboth granular and coarse chunks. In the following steps, I’ll show how to split\nthe content based on HTML headers and analyze the impact on retrieval\naccuracy.\nSplitting Content into Granular Chunks Using HTMLSectionSplitter\nBefore storing the content from the `docs` object into the vector database,\ndecide on a splitting strategy. Since this content is from an HTML page, you\n",
      "content_length": 1502,
      "extraction_method": "Direct"
    },
    {
      "page_number": 246,
      "chapter": null,
      "content": "can split it by H1 and H2 tags, which separate the content into sections. This\nwill generate more granular chunks, as shown in listing 8.1.\nListing 8.1 Splitting content with the HTML Section splitter\nfrom langchain_text_splitters import HTMLSectionSplitter\nheaders_to_split_on = [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2\")]\nhtml_section_splitter = HTMLSectionSplitter(headers_to_split_on=h\ndef split_docs_into_granular_chunks(docs):\n   all_chunks = []\n   for doc in docs:\n       html_string = doc.page_content  #A\n       temp_chunks = html_section_splitter.split_text(html_strin\n       all_chunks.extend(temp_chunks)\n   return all_chunks\nYou can now generate the granular chunks:\ngranular_chunks = split_docs_into_granular_chunks(docs)\nNow, insert the granular chunks into the Chroma collection:\ncornwall_granular_collection.add_documents(documents=granular_chu\nSearching Granular Chunks\nYou can now run a search for specific content within the granular chunks:\nresults = cornwall_granular_collection.similarity_search(query=\"E\nfor doc in results:\n   print(doc)\nSplitting Content into Coarse Chunks Using\nRecursiveCharacterTextSplitter\nFor larger, coarser chunks, use the RecursiveCharacterTextSplitter class.\nStart by creating the necessary objects:\nfrom langchain_community.document_transformers import Html2TextTr\n",
      "content_length": 1314,
      "extraction_method": "Direct"
    },
    {
      "page_number": 247,
      "chapter": null,
      "content": "from langchain_text_splitters import RecursiveCharacterTextSplitt\nhtml2text_transformer = Html2TextTransformer()\ntext_splitter = RecursiveCharacterTextSplitter(\n   chunk_size=3000, chunk_overlap=300\n)\nNext, define a function to split the content into coarse chunks:\ndef split_docs_into_coarse_chunks(docs):\n   text_docs = html2text_transformer.transform_documents(docs)  \n   coarse_chunks = text_splitter.split_documents(text_docs)  #B\n   return coarse_chunks\nThen generate the coarse chunks:\ncoarse_chunks = split_docs_into_coarse_chunks(docs)\nInsert these chunks into the corresponding Chroma collection:\ncornwall_coarse_collection.add_documents(documents=coarse_chunks)\nSearching Coarse Chunks\nYou can now search for more general content within the coarse chunks:\nresults = cornwall_coarse_collection.similarity_search(query=\"Eve\nfor doc in results:\n   print(doc)\nIngesting Content from Multiple URLs\nTo make the searches more comprehensive, load additional content into the\ncollections. Listing 8.2 shows how to set up new granular and coarse\ncollections for various UK destinations and ingest the related content chunks.\nIf you'd like to minimize processing costs, consider reducing the size of the\nuk_destinations list.\nListing 8.2 Creating collections for multiple UK destinations\nuk_granular_collection = Chroma(  #A\n",
      "content_length": 1325,
      "extraction_method": "Direct"
    },
    {
      "page_number": 248,
      "chapter": null,
      "content": "   collection_name=\"uk_granular\",\n   embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_\n)\nuk_granular_collection.reset_collection()  #B\nuk_coarse_collection = Chroma(  #A\n   collection_name=\"uk_coarse\",\n   embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_\n)\nuk_coarse_collection.reset_collection()  #B\nuk_destinations = [\n   \"Cornwall\", \"North_Cornwall\", \"South_Cornwall\", \"West_Cornwal\n   \"Tintagel\", \"Bodmin\", \"Wadebridge\", \"Penzance\", \"Newquay\",\n   \"St_Ives\", \"Port_Isaac\", \"Looe\", \"Polperro\", \"Porthleven\",\n   \"East_Sussex\", \"Brighton\", \"Battle\", \"Hastings_(England)\", \n   \"Rye_(England)\", \"Seaford\", \"Ashdown_Forest\"\n]\nwikivoyage_root_url = https://en.wikivoyage.org/wiki\nuk_destination_urls = [f'{wikivoyage_root_url}/{d}' for d in uk_d\nfor destination_url in uk_destination_urls:\n   html_loader = AsyncHtmlLoader(destination_url)  #C\n   docs = html_loader.load()  #D\n   \n   granular_chunks = split_docs_into_granular_chunks(docs)\n   uk_granular_collection.add_documents(documents=granular_chunk\n   coarse_chunks = split_docs_into_coarse_chunks(docs)\n   uk_coarse_collection.add_documents(documents=coarse_chunks)\nYou can now perform both granular and coarse searches:\ngranular_results = uk_granular_collection.similarity_search(query\nfor doc in granular_results:\n   print(doc)\ncoarse_results = uk_coarse_collection.similarity_search(query=\"Ev\nfor doc in coarse_results:\n   print(doc)\nTry experimenting with different queries, like \"Beaches in Cornwall\", to\nsee how the results vary between granular and coarse chunks. This approach\nwill help you fine-tune the balance between detailed and general content\n",
      "content_length": 1637,
      "extraction_method": "Direct"
    },
    {
      "page_number": 249,
      "chapter": null,
      "content": "retrieval.\n8.4 Embedding Strategy\nPreviously, I discussed how keyword searches are more flexible than vector\nsearches because you can tag a document with multiple keywords. The same\nidea can be applied to embeddings—you can store multiple vectors per\ndocument, which increases the flexibility and accuracy of vector searches. I’ll\nwalk through various multi-vector strategies in the following sections, mainly\nfocusing on how to use LangChain’s MultiVectorRetriever.\nThe key to these strategies is a two-layer chunk structure. The top layer\nincludes synthesis chunks—the chunks fed into the LLM to generate answers.\nThe lower layer consists of retrieval chunks, smaller segments that create\nprecise embeddings for retrieving the synthesis chunks. I suggest testing all\nmulti-vector retriever techniques for your use case, as they usually yield\nsimilar performance improvements. However, since the structure of your text\nmay favor one technique over another, experimenting with each will help you\nidentify the best fit.\n8.4.1 Embedding Child Chunks with ParentDocumentRetriever\nA common challenge with chunk size is balancing between context and\ndetail. Large chunks work for broad questions but struggle with detailed\nqueries. Small chunks, while supporting detailed queries, often lack the\ncontext needed for generating comprehensive answers. This creates a\ntradeoff: if chunks are too small, the response might be incomplete; if they’re\ntoo large, the retrieval may be less precise.\nTo solve this, split the document into larger parent chunks and create smaller\nchild chunks within each parent. Use the child chunks solely for generating\nmore granular embeddings, which are then stored against the parent chunk.\nThis hybrid approach allows each document to have embeddings for both\nbroad and detailed queries, as illustrated in figure 8.4.\nFigure 8.4 Child Chunk embeddings—A coarse document chunk is indexed with its own\nembedding and the embeddings generated from its smaller child chunks. This allows it to match\n",
      "content_length": 2018,
      "extraction_method": "Direct"
    },
    {
      "page_number": 250,
      "chapter": null,
      "content": "effectively with both broad and detailed queries.\n",
      "content_length": 50,
      "extraction_method": "Direct"
    },
    {
      "page_number": 251,
      "chapter": null,
      "content": "Coarse chunking strategy = =\n\nCoarse chunks\n\nCoarse chunk\n\nChild chunk #1 embedding\n\nCoarse chunk embedding Child chunk #4\n\nChild chunk #2 embedding\n\nx Child chunk #3 embedding\nChild chunk #3\n\nA detailed question is likely to hit\nthe embedding of a child chunk of\na coarse chunk\n\nA broad question is likely to hit\nthe embedding of a coarse\nchunk\n\nBroad question",
      "content_length": 361,
      "extraction_method": "OCR"
    },
    {
      "page_number": 252,
      "chapter": null,
      "content": "The advantage of this approach is that when a broad query is made, the parent\nchunk is retrieved, providing rich context. For more detailed queries, the\nchild embeddings ensure precise matching which still lead to the retrieval of\nthe context-rich parent chunk. This structure helps the LLM generate more\naccurate and contextually relevant answers.\nI’ll show you how to implement the technique using\nParentDocumentRetriever. Start by importing the necessary libraries:\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.document_loaders import AsyncHtmlLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitt\nSetting Up the ParentDocumentRetriever\nThe approach begins by splitting content into large, coarse chunks for\nsynthesis, then further dividing each into smaller child chunks for retrieval.\nListing 8.3 demonstrates how to configure the splitters and set up the\nretriever. As shown, documents are stored in an InMemoryStore—a general-\npurpose, in-memory key-value store designed to hold serializable Python\nobjects such as strings, lists, and dictionaries. It’s particularly useful for\ncaching and intermediate data storage.\nListing 8.3 Setting Up Parent and Child Splitters for Coarse and Granular Chunks\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=3000) \nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=500)  \nchild_chunks_collection = Chroma(  #C\n   collection_name=\"uk_child_chunks\",\n   embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_\n)\nchild_chunks_collection.reset_collection()  #D\ndoc_store = InMemoryStore()  #E\n",
      "content_length": 1743,
      "extraction_method": "Direct"
    },
    {
      "page_number": 253,
      "chapter": null,
      "content": "parent_doc_retriever = ParentDocumentRetriever(  #F\n   vectorstore=child_chunks_collection,\n   docstore=doc_store,\n   child_splitter=child_splitter,\n   parent_splitter=parent_splitter\n)\nIngesting Content into Document and Vector Stores\nNow, generate both coarse and granular chunks using the configured splitters\nand add them to the respective stores.\nfor destination_url in uk_destination_urls:\n   html_loader = AsyncHtmlLoader(destination_url)  #A\n   html_docs =  html_loader.load()  #B\n   text_docs = html2text_transformer.transform_documents(html_do\n   print(f'Ingesting {destination_url}')\n   parent_doc_retriever.add_documents(text_docs, ids=None)  #D\nVerifying the In-Memory Document Store\nYou can check if the coarse chunks have been correctly stored:\nlist(doc_store.yield_keys()) \nPerforming a Search on Granular Information\nNow perform a search on the child chunks using the\nParentDocumentRetriever:\nretrieved_docs = parent_doc_retriever.invoke(\"Cornwall Ranger\")\nThe first document retrieved (retrieved_docs[0]) will contain rich\ncontextual information:\nDocument(metadata={'source': 'https://en.wikivoyage.org/wiki/Sout\nComparing with Direct Semantic Search on Child Chunks\n",
      "content_length": 1185,
      "extraction_method": "Direct"
    },
    {
      "page_number": 254,
      "chapter": null,
      "content": "Now, compare the results by directly searching only the child chunks:\nchild_docs_only = child_chunks_collection.similarity_search(\"Cornw\nThe first result is much shorter and lacks context:\nDocument(metadata={'doc_id': '34645d23-ed05-4a53-b3af-c8ab21e3f51\npage_content='The **Cornwall Ranger** ticket allows unlimited tra\nThe result we have obtained with the ParentDocumentRetriever is\nparticularly useful when used as context for LLM synthesis, as it provides\nbroader details around the specific information.\nNext, I’ll introduce another technique that leverages embedding strategies to\nfurther optimize RAG retrieval accuracy.\n8.4.2 Embedding Child Chunks with MultiVectorRetriever\nAn alternative method for embedding child chunks and linking them to the\nlarger parent chunks used in synthesis is to use the MultiVectorRetriever.\nBegin by importing the necessary libraries, including InMemoryByteStore,\nwhich is specifically designed for storing binary data. In this store, keys are\nstrings and values are bytes, making it ideal for use cases involving\nembeddings, models, or files where raw byte storage is preferred or required:\nfrom langchain.retrievers.multi_vector import MultiVectorRetrieve\nfrom langchain.storage import InMemoryByteStore\nfrom langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.document_loaders import AsyncHtmlLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitt\nimport uuid\nSetting Up the MultiVectorRetriever\nYou can use a similar approach as with the ParentDocumentRetriever by\ndefining parent and child splitters and injecting them into\nMultiVectorRetriever, as shown in Listing 8.4.\nListing 8.4 Configuring Parent and Child Splitters for MultiVectorRetriever\n",
      "content_length": 1760,
      "extraction_method": "Direct"
    },
    {
      "page_number": 255,
      "chapter": null,
      "content": "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=3000) \nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=500)  \nchild_chunks_collection = Chroma(  #C\n   collection_name=\"uk_child_chunks\",\n   embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_\n)\nchild_chunks_collection.reset_collection()  #D\ndoc_byte_store = InMemoryByteStore()  #E\ndoc_key = \"doc_id\"\nmulti_vector_retriever = MultiVectorRetriever(  #F\n   vectorstore=child_chunks_collection,\n   byte_store=doc_byte_store\n)\nIngesting Content into Document and Vector Stores\nThe next step is to load and ingest content into the MultiVectorRetriever,\nas shown in listing 8.5. In this case, the document store is handled internally\nby the retriever, unlike the ParentDocumentRetriever, where you configured\nthe store separately. While ingestion may feel slower, this is expected due to\nthe added complexity of managing multiple vector representations.\nListing 8.5 Ingesting Content into Document and Vector Stores\nfor destination_url in uk_destination_urls:\n   html_loader = AsyncHtmlLoader(destination_url)  #A\n   html_docs =  html_loader.load()  #B\n   text_docs = html2text_transformer.transform_documents(html_do\n   coarse_chunks = parent_splitter.split_documents(text_docs)  #\n   coarse_chunks_ids = [str(uuid.uuid4()) for _ in coarse_chunks\n   all_granular_chunks = []\n   for i, coarse_chunk in enumerate(coarse_chunks):  #E\n       \n       coarse_chunk_id = coarse_chunks_ids[i]\n           \n       granular_chunks = child_splitter.split_documents([coarse_\n       for granular_chunk in granular_chunks:\n",
      "content_length": 1586,
      "extraction_method": "Direct"
    },
    {
      "page_number": 256,
      "chapter": null,
      "content": "           granular_chunk.metadata[doc_key] = coarse_chunk_id  #\n       all_granular_chunks.extend(granular_chunks)\n   print(f'Ingesting {destination_url}')\n   multi_vector_retriever.vectorstore.add_documents(all_granular_\n   multi_vector_retriever.docstore.mset(list(zip(coarse_chunks_i\nPerforming a search on granular information\nNow, perform a search using MultiVectorRetriever, just like you did with\nthe ParentDocumentRetriever:\nretrieved_docs = multi_vector_retriever.invoke(\"Cornwall Ranger\")\nPrinting the first result shows that the retrieved document contains rich,\ndetailed information:\nDocument(metadata={'source': 'https://en.wikivoyage.org/wiki/Sout\npage_content=\"Trains from London take about 3 hr 20 min to Plymou\nComparing with Direct Semantic Search on Child Chunks\nFor comparison, run the same search directly on the child chunk collection:\nchild_docs_only = child_chunks_collection.similarity_search(\"Cornw\nThe first document retrieved from the child collection (child_docs_only[0])\nis more concise and lacks the broader context:\nDocument(metadata={'doc_id': '04c7f88e-e090-4057-af5b-ea584e777b3\npage_content='The **Cornwall Ranger** ticket allows unlimited tra\nThis result is similar to what you observed with the\nParentDocumentRetriever. The broader parent chunks provide more useful\ncontext for synthesis, making them a better fit for complex or detailed\nqueries.\nIn the next section, I’ll cover additional strategies for improving RAG\naccuracy using advanced embedding techniques.\n",
      "content_length": 1504,
      "extraction_method": "Direct"
    },
    {
      "page_number": 257,
      "chapter": null,
      "content": "8.4.3 Embedding Document Summaries\nEmbeddings from coarse chunks are often ineffective because they capture\ntoo much irrelevant content. A large chunk may include filler text or minor\ndetails that dilute the semantic value of the embeddings, making them less\nfocused and less useful.\nTo address this, you can create a summary of the coarse chunk and generate\nembeddings from it. These summary embeddings are then stored alongside\nthe original chunk embeddings, as shown in figure 8.5. Because the summary\nis more concise and relevant, the resulting embeddings are denser and more\neffective for retrieval, reducing noise and improving search precision.\nFigure 8.5 Chunk Summary Embedding—A coarse chunk is indexed with its own embedding and\nan additional embedding from its summary. This allows for more accurate retrieval when\nanswering detailed questions.\n",
      "content_length": 857,
      "extraction_method": "Direct"
    },
    {
      "page_number": 258,
      "chapter": null,
      "content": "Coarse chunking strategy = =\n\nCoarse chunks\n\nCoarse chunk\n\nCoarse chunk summary\n\nCoarse chunk embedding\n\nSummary embedding\n\nA broad question is likely to hit\n\nthe embedding of a coarse\n\nchunk\nA detailed question is likely to hit\nthe embedding of the summary\nof a coarse chunk\n\nBroad question Detailed question",
      "content_length": 309,
      "extraction_method": "OCR"
    },
    {
      "page_number": 259,
      "chapter": null,
      "content": "To start, import the required libraries for building the summarization and\nretrieval setup:\nfrom langchain.retrievers.multi_vector import MultiVectorRetrieve\nfrom langchain.storage import InMemoryByteStore\nfrom langchain_chroma import Chroma\nfrom langchain_openai import ChatOpenAI\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.document_loaders import AsyncHtmlLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitt\nfrom langchain_core.documents import Document\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nimport uuid \nSetting Up the MultiVectorRetriever\nFirst, create a collection to store the summaries and set up the document store\n(InMemoryByteStore). Then, configure the MultiVectorRetriever to use\nthese components, as shown in Listing 8.6.\nListing 8.6 Injecting summary collection and document store into MultiVectorRetriever\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=3000) \nsummaries_collection = Chroma(  #B\n   collection_name=\"uk_summaries\",\n   embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_\n)\nsummaries_collection.reset_collection()  #C\ndoc_byte_store = InMemoryByteStore()  #D\ndoc_key = \"doc_id\"\nmulti_vector_retriever = MultiVectorRetriever(  #E\n   vectorstore=summaries_collection,\n   byte_store=doc_byte_store\n)\nSetting Up the Summarization Chain\n",
      "content_length": 1407,
      "extraction_method": "Direct"
    },
    {
      "page_number": 260,
      "chapter": null,
      "content": "Use an LLM to generate summaries of the coarse chunks. Define a\nsummarization chain that extracts the content, prompts the LLM, and parses\nthe response into a usable format:\nllm = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=OPENAI_API_K\nsummarization_chain = (\n   {\"document\": lambda x: x.page_content}  #A\n   | ChatPromptTemplate.from_template(\"Summarize the following d\n   | llm  #C\n   | StrOutputParser())  #D\nIngesting Coarse Chunks and Summaries into Stores\nNext, load the content, split it into coarse chunks, generate summaries, and\nstore both in the appropriate collections, as shown in listing 8.7.\nListing 8.7 Ingesting Coarse Chunks and Their Summaries\nfor destination_url in uk_destination_urls:\n   html_loader = AsyncHtmlLoader(destination_url)  #A\n   html_docs =  html_loader.load()  #B\n   text_docs = html2text_transformer.transform_documents(html_do\n   coarse_chunks = parent_splitter.split_documents(text_docs)  #\n   coarse_chunks_ids = [str(uuid.uuid4()) for _ in coarse_chunks\n   all_summaries = []\n   for i, coarse_chunk in enumerate(coarse_chunks):  #E\n       \n       coarse_chunk_id = coarse_chunks_ids[i]\n           \n       summary_text =  summarization_chain.invoke(coarse_chunk)  \n       summary_doc = Document(page_content=summary_text, metadat\n       all_summaries.append(summary_doc)  #G\n   print(f'Ingesting {destination_url}')\n   multi_vector_retriever.vectorstore.add_documents(all_summarie\n   multi_vector_retriever.docstore.mset(list(zip(coarse_chunks_i\nWhen running the code in listing 8.6, you may notice that processing is\nslower compared to using child embeddings. This slowdown is due to the\n",
      "content_length": 1633,
      "extraction_method": "Direct"
    },
    {
      "page_number": 261,
      "chapter": null,
      "content": "time required to submit each coarse chunk for summarization to the LLM,\nwhich is a more computationally intensive step.\nPerforming a Search Using the MultiVectorRetriever\nOnce the ingestion is complete, you can perform a search using the\nMultiVectorRetriever, which now utilizes the summaries for each travel\ndestination:\nretrieved_docs = multi_vector_retriever.invoke(\"Cornwall travel\")\nIf you print the first result (retrieved_docs_only[0]), you’ll see a large\nchunk similar to those retrieved when using child embeddings. These larger\nchunks provide more context, making them effective when passed as input to\nthe LLM.\nComparing with Direct Semantic Search on Summaries\nFor comparison, perform a direct search on the summaries alone:\nsummary_docs_only = summaries_collection.similarity_search(\"Cornw\nprint(summary_docs_only[0])\nThe first result from the summary search is concise and lacks broader\ncontext:\nDocument(metadata={'doc_id': 'ee55d250-bc53-46ce-9204-8fd2c1a0566\nThis result confirms a pattern observed earlier: directly searching against\nsummaries or child chunks retrieves focused but context-limited information,\nwhile using a multi-vector approach retrieves broader context that is more\nuseful for synthesis.\nLet’s explore one more advanced multi-vector embedding technique in the\nfollowing section.\n8.4.4 Embedding Hypothetical Questions\n",
      "content_length": 1356,
      "extraction_method": "Direct"
    },
    {
      "page_number": 262,
      "chapter": null,
      "content": "When querying a vector store, your natural language question is converted\ninto a vector, and the system calculates its similarity (e.g., cosine distance) to\nthe stored vectors. The documents linked to the closest vectors are then\nreturned. This approach works well if the question is semantically similar to\nthe ideal answer. But often, the wording of the question and the phrasing of\nthe ideal answer may not match closely enough, causing the search to miss\nrelevant documents.\nTo address this, you can generate hypothetical questions that each chunk is\nlikely to answer, and then store the chunk using embeddings derived from\nthese questions, as shown in figure 8.6. This method increases the chances\nthat the stored vectors will align more closely with a user’s query, making it\nmore likely to retrieve relevant information, even if the original document’s\nembeddings aren’t a perfect match for the question.\nFigure 8.6 Hypothetical Question embeddings—A document chunk is indexed with its own\nembedding and additional embeddings generated from hypothetical questions that it can answer.\nThis allows for more accurate matching with user queries.\n",
      "content_length": 1149,
      "extraction_method": "Direct"
    },
    {
      "page_number": 263,
      "chapter": null,
      "content": "Coarse chunking strategy\n\nsafes\n\nCoarse chunks\n\nHypothetical question #1 embedding\nHypothetical\nquestion #1 —__ A A\n\nHypothetical question #2 embedding\n\npOthetical question #3 embedding\n\nCoarse chunk\n\nCoarse chunk embedding\n\nHypothetical questions which\n\ncan be answered with the content / A detailed question is likely to hit\nA tion is likely to hit of the the chunk the embedding of an hypothetical question\nhaptoeipl . pe which can be answered by a coarse chunk\n\nchunk\n\nBroad question\n\nDetailed question",
      "content_length": 506,
      "extraction_method": "OCR"
    },
    {
      "page_number": 264,
      "chapter": null,
      "content": "To begin, import all the required libraries to set up the\nMultiVectorRetriever with hypothetical question embeddings:\nfrom langchain.retrievers.multi_vector import MultiVectorRetrieve\nfrom langchain.storage import InMemoryByteStore\nfrom langchain_chroma import Chroma\nfrom langchain_openai import ChatOpenAI\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.document_loaders import AsyncHtmlLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitt\nfrom langchain_core.documents import Document\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nimport uuid\nfrom typing import List\nfrom pydantic import BaseModel, Field\nSetting Up the MultiVectorRetriever\nSet up the MultiVectorRetriever similarly to how it was configured for\nsummary embeddings, but this time use a vector store specifically for storing\nhypothetical questions, as shown in Listing 8.8.\nListing 8.8 Configuring MultiVectorRetriever with Hypothetical Question Collection\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=3000) \nhypothetical_questions_collection = Chroma(  #B\n   collection_name=\"uk_hypothetical_questions\",\n   embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_\n)\nhypothetical_questions_collection.reset_collection()  #C\ndoc_byte_store = InMemoryByteStore()  #D\ndoc_key = \"doc_id\"\nmulti_vector_retriever = MultiVectorRetriever(  #E\n   vectorstore=hypothetical_questions_collection,\n   byte_store=doc_byte_store\n)\n",
      "content_length": 1514,
      "extraction_method": "Direct"
    },
    {
      "page_number": 265,
      "chapter": null,
      "content": "Setting Up the Hypothetical Question Generation Chain\nCreate a chain to generate hypothetical questions for each document chunk.\nUse structured output from the LLM to ensure the generated questions are\nreturned as a list of strings:\nclass HypotheticalQuestions(BaseModel):\n   \"\"\"Generate hypothetical questions for given text.\"\"\"\n   questions: List[str] = Field(..., description=\"List of hypoth\nllm_with_structured_output = ChatOpenAI(model=\"gpt-4o-mini\", \n       openai_api_key=OPENAI_API_KEY).with_structured_output(\n       HypotheticalQuestions\n)\nYou can see the full question generation chain in listing 8.9.\nListing 8.9 Chain for Generating Hypothetical Questions from Text\nhypothetical_questions_chain = (\n   {\"document_text\": lambda x: x.page_content}  #A\n   | ChatPromptTemplate.from_template(  #B\n       \"Generate a list of exactly 4 hypothetical questions that \n   )\n   | llm_with_structured_output  #C\n   | (lambda x: x.questions)  #D\n)\nIngesting Coarse Chunks and Related Hypothetical Questions\nNow, generate coarse chunks, create the hypothetical questions for each, and\nstore them in the respective collections, as shown in listing 8.10.\nListing 8.10 Ingesting Coarse Chunks and Related Hypothetical Questions\nfor destination_url in uk_destination_urls:\n   html_loader = AsyncHtmlLoader(destination_url)  #A\n   html_docs =  html_loader.load()  #B\n   text_docs = html2text_transformer.transform_documents(html_do\n   coarse_chunks = parent_splitter.split_documents(text_docs)  #\n",
      "content_length": 1491,
      "extraction_method": "Direct"
    },
    {
      "page_number": 266,
      "chapter": null,
      "content": "   coarse_chunks_ids = [str(uuid.uuid4()) for _ in coarse_chunks\n   all_hypothetical_questions = []\n   for i, coarse_chunk in enumerate(coarse_chunks):  #E\n       \n       coarse_chunk_id = coarse_chunks_ids[i]\n           \n       hypothetical_questions = hypothetical_questions_chain.inv\n       hypothetical_questions_docs = [Document(page_content=ques\n                                             for question in hyp\n       all_hypothetical_questions.extend(hypothetical_questions_\n   print(f'Ingesting {destination_url}')\n   multi_vector_retriever.vectorstore.add_documents(all_hypothet\n   multi_vector_retriever.docstore.mset(list(zip(coarse_chunks_i\nPerforming a Search Using the MultiVectorRetriever\nAfter ingestion, perform a search using the MultiVectorRetriever, which\nnow uses the stored hypothetical question embeddings:\nretrieved_docs = multi_vector_retriever.invoke(\"How can you go to \nThe first retrieved document (retrieved_docs[0]) is a detailed response\nwith rich contextual information:\n[Document(metadata={'source': 'https://en.wikivoyage.org/wiki/Bri\nComparing with Direct Search on Hypothetical Questions\nNow, run a search directly on the hypothetical questions collection for\ncomparison:\nhypothetical_question_docs_only = hypothetical_questions_collecti\nThe results show hypothetical questions closely matching the query:\n[Document(metadata={'doc_id': 'af848894-8591-4c28-8295-f3b833ffaa\nDocument(metadata={'doc_id': '7fa14e56-270c-4461-88ab-9b546afb07\nDocument(metadata={'doc_id': '7fa14e56-270c-4461-88ab-9b546afb07\nDocument(metadata={'doc_id': '7fa14e56-270c-4461-88ab-9b546afb07\n",
      "content_length": 1603,
      "extraction_method": "Direct"
    },
    {
      "page_number": 267,
      "chapter": null,
      "content": "This pattern is consistent with earlier techniques: using hypothetical question\nembeddings helps the search engine focus on the specific intent behind the\nquery, making it easier to retrieve relevant documents even when the exact\nwording varies.\nIn the next section, I’ll explore another technique to further improve\ndocument retrieval accuracy.\n8.5 Granular Chunk Expansion\nAs previously discussed, the main drawback of splitting a document into\nsmall granular chunks is that, while these chunks are effective for detailed\nquestions, they often lack the context needed to generate complete answers.\nOne way to address this is through chunk expansion, as shown in Figure 8.7.\nFigure 8.7 Sentence Expansion—A granular chunk can be enhanced by including the content\nfrom its preceding and following chunks to provide additional context.\n",
      "content_length": 835,
      "extraction_method": "Direct"
    },
    {
      "page_number": 268,
      "chapter": null,
      "content": "ARBRE\n—— FRERIRE\n\nCoarse chunking strategy\n\nGranular chunks\n\nThe granular chunk associated with\nGranular chunks the embedding hit by the question\nembeddings is sent to the LLM within in the prompt LLM\n\nas a context with the previous and\n\nA detailed question its the subsequent granular chunk\n\nembedding of a granular chunk\n\nDetailed question",
      "content_length": 341,
      "extraction_method": "OCR"
    },
    {
      "page_number": 269,
      "chapter": null,
      "content": "The idea is to store an expanded version of each chunk that includes content\nfrom the chunks immediately before and after it. This expanded version is\nstored in a separate document store. So, when the vector store retrieves a\nrelevant granular chunk, the linked expanded chunk is returned instead,\noffering a richer context for the LLM to produce a more complete answer.\nThis technique can be easily implemented using the MultiVectorRetriever.\nLet's look at how to set this up.\nSetting Up the MultiVectorRetriever for Chunk Expansion\nFirst, configure the MultiVectorRetriever by creating a collection to hold\ngranular chunks and an in-memory document store for the expanded chunks,\nas shown in listing 8.11:\nListing 8.11 Setting up a multi vector retriever for granular chunk expansion\nfrom langchain.retrievers.multi_vector import MultiVectorRetrieve\nfrom langchain.storage import InMemoryByteStore\nfrom langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.document_loaders import AsyncHtmlLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitt\nimport uuid\ngranular_chunk_splitter = RecursiveCharacterTextSplitter(chunk_si\ngranular_chunks_collection = Chroma(  #B\n   collection_name=\"uk_granular_chunks\",\n   embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_\n)\ngranular_chunks_collection.reset_collection()  #C\nexpanded_chunk_store = InMemoryByteStore()  #D\ndoc_key = \"doc_id\"\nmulti_vector_retriever = MultiVectorRetriever(  #E\n   vectorstore=granular_chunks_collection,\n   byte_store=expanded_chunk_store\n)\n",
      "content_length": 1588,
      "extraction_method": "Direct"
    },
    {
      "page_number": 270,
      "chapter": null,
      "content": "Ingesting Granular and Expanded Chunks\nNow, generate expanded chunks by including the content from adjacent\nchunks, as shown in listing 8.12.\nListing 8.12 Generating and Storing Expanded Chunks\nfor destination_url in uk_destination_urls:\n   html_loader = AsyncHtmlLoader(destination_url)  #A\n   html_docs =  html_loader.load()  #B\n   text_docs = html2text_transformer.transform_documents(html_do\n   granular_chunks = granular_chunk_splitter.split_documents(tex\n   expanded_chunk_store_items = []\n   for i, granular_chunk in enumerate(granular_chunks):  #E\n       this_chunk_num = i  #F\n       previous_chunk_num = i-1  #F\n       next_chunk_num = i+1  #F\n       \n       if i==0:  #F\n           previous_chunk_num = None\n       elif i==(len(granular_chunks)-1):  #F\n           next_chunk_num = None\n       expanded_chunk_text = \"\"  #G\n       if previous_chunk_num:  #G\n           expanded_chunk_text += granular_chunks[previous_chunk_\n           expanded_chunk_text += \"\\n\"\n       expanded_chunk_text += granular_chunks[this_chunk_num].pa\n       expanded_chunk_text += \"\\n\"\n       if next_chunk_num:  #G\n           expanded_chunk_text += granular_chunks[next_chunk_num\n           expanded_chunk_text += \"\\n\"\n       expanded_chunk_id = str(uuid.uuid4())  #H\n       expanded_chunk_doc = Document(page_content=expanded_chunk_\n       expanded_chunk_store_item = (expanded_chunk_id, expanded_\n       expanded_chunk_store_items.append(expanded_chunk_store_it\n       granular_chunk.metadata[doc_key] = expanded_chunk_id  #J\n",
      "content_length": 1515,
      "extraction_method": "Direct"
    },
    {
      "page_number": 271,
      "chapter": null,
      "content": "           \n   print(f'Ingesting {destination_url}')\n   multi_vector_retriever.vectorstore.add_documents(granular_chu\n   multi_vector_retriever.docstore.mset(expanded_chunk_store_ite\nPerforming a Search Using the MultiVectorRetriever\nAfter the ingestion step, run a search using the MultiVectorRetriever,\nwhich now uses expanded chunks for a more complete context:\nretrieved_docs = multi_vector_retriever.invoke(\"Cornwall Ranger\")\nThe first document retrieved will include content from surrounding chunks,\ngiving the LLM more context to generate a richer response:\nDocument(page_content=\"Buses only serve designated stops when in \nComparing with Direct Semantic Search on Granular Chunks\nFor comparison, run a search directly on the granular chunks without\nexpansion:\nchild_docs_only = granular_chunks_collection.similarity_search(\"C\nThe result will likely be more concise and lack the broader context:\nDocument(metadata={'doc_id': '04c7f88e-e090-4057-af5b-ea584e777b3\nThis smaller chunk lacks the surrounding details and may not provide enough\ncontext to synthesize a complete answer.\nChunk expansion offers a way to improve the effectiveness of granular\nembeddings by attaching broader context. The next section will cover how to\nefficiently handle mixed structured and unstructured content.\n8.6 Semi-Structured Content\nWhen dealing with documents that mix unstructured text and structured data\n(e.g., tables), it’s essential to handle each type separately. You should extract\n",
      "content_length": 1479,
      "extraction_method": "Direct"
    },
    {
      "page_number": 272,
      "chapter": null,
      "content": "structured content, like tables, and generate embeddings for their summaries\n—just as you would for text chunks, as discussed in the Embedding\nDocument Summaries section.\nStore the coarse text chunks and the full tables in a document store and place\nthe embeddings for the summaries of both text and tables in the vector store\nusing a MultiVectorRetriever, as shown in figure 8.8. This setup allows\nseamless retrieval of both structured and unstructured content.\nFigure 8.8 Embedding Structured and Unstructured Content—Structured data, such as tables,\nshould be summarized and embedded just like unstructured text chunks. This ensures that the\nembeddings match detailed questions as effectively as text embeddings. Use the\nMultiVectorRetriever to manage both types of content.\n",
      "content_length": 778,
      "extraction_method": "Direct"
    },
    {
      "page_number": 273,
      "chapter": null,
      "content": "Coarse chunking strategy\n\nText coarse chunks ‘Table coarse chunks\n\nText coarse chunk\nCoarse chunk embedding\n\nta)\n\nText chunk summary\n\nSummary embedding\n\nSummary embedding\n\nTable coarse chunk\n\nTable chunk summary\n\nA broad question is likely to hit\n\nthe embedding of a coarse\n\nchunk\nA detailed question might hit\nthe embedding of the summary\nof a table chunk\n\nBroad question Detailed question",
      "content_length": 390,
      "extraction_method": "OCR"
    },
    {
      "page_number": 274,
      "chapter": null,
      "content": "When a search hits the embedding of a table’s summary, the entire table\n(stored in the document store) is returned to the LLM for synthesis, providing\nthe necessary context to generate a complete response.\nFor a practical implementation of this technique, refer to\nhttps://github.com/langchain-\nai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb?\nref=blog.langchain.dev.\n8.7 Multi-Modal RAG\nYou’ve likely come across the term “multi-modal” LLMs. Models like GPT-\n4V extend traditional LLMs to handle not just text, but also images and\naudio. This opens the door for extending the RAG architecture to support\nmulti-modal data.\nThe approach is similar to handling semi-structured content. During the data\npreparation stage, you can use a multi-modal LLM to generate a summary of\nan image, just as you would for a table. Then, create embeddings for the\nimage summary and link these embeddings to the raw image stored in a\ndocument store, as shown in Figure 8.9.\nFigure 8.9 Multi-Modal RAG Workflow—1) Data Ingestion: Use a multi-modal LLM to generate\nan image summary, store the summary embeddings in a vector store, and keep the raw image in\na document store. 2) Retrieval: If the summary embeddings match a query, the raw image is\nreturned by the MultiVectorRetriever and fed into the LLM for synthesis.\n",
      "content_length": 1312,
      "extraction_method": "Direct"
    },
    {
      "page_number": 275,
      "chapter": null,
      "content": "During retrieval, if the summary’s embeddings match a user query, the\nMultiVectorRetriever returns the raw image—just as it would return a table\nfor semi-structured text. The image is then passed to the multi-modal LLM\n",
      "content_length": 219,
      "extraction_method": "Direct"
    },
    {
      "page_number": 276,
      "chapter": null,
      "content": "along with its summary, providing a rich context for generating a response.\nNote\nThis book does not cover multi-modal RAG in detail, as it is an advanced\ntopic that would require in-depth explanations beyond the intended scope.\nHowever, with what you’ve learned so far, you should have the foundation to\nexplore it on your own. For more information, I recommend the article\n“Multimodality and Large Multimodal Models (LMMs)” by Chip Huyen:\nhttps://huyenchip.com/2023/10/10/multimodal.html.\n8.8 Summary\nA basic RAG implementation often produces low-quality results,\nmaking it unsuitable for real-world applications.\nYou can improve answer quality using techniques like advanced\nindexing, multiple query generation, query transformation, content-\nspecific queries, retrieval post-processing (e.g., result reranking), and\nensemble methods.\nAdvanced indexing may include refined document splitting strategies,\nmultiple embeddings for coarse chunks, and expanded context for\ngranular chunks.\nChoose a splitting strategy based on content type: by size (e.g.,\nparagraphs, sentences, words, or characters) or by structure (e.g.,\nchapters or sections).\nStructured formats like HTML or Markdown benefit from splitting\nbased on document hierarchy.\nFor coarse chunks, enhance retrieval accuracy by using multiple\nembeddings generated from child chunks, summaries, or hypothetical\nquestions about the content.\nEmbed child chunks using the ParentDocumentRetriever. Use the\nMultiVectorRetriever to embed chunk summaries, hypothetical\nquestions, or child chunks.\nSince it’s hard to predict which multi-vector retrieval technique will\nperform best, experiment with each and compare results to find the\noptimal approach.\nFor granular chunks, expand retrieval context by including adjacent\n",
      "content_length": 1771,
      "extraction_method": "Direct"
    },
    {
      "page_number": 277,
      "chapter": null,
      "content": "chunks for a fuller context.\nFor semi-structured content (e.g., text with tables) or multi-modal\ncontent (e.g., images and audio), use specialized indexing techniques for\noptimal results.\n",
      "content_length": 188,
      "extraction_method": "Direct"
    },
    {
      "page_number": 278,
      "chapter": null,
      "content": "9 Question transformations\nThis chapter covers\nRewrite user questions with \"Rewrite-Retrieve-Read\" for better\nembedding alignment.\nUse \"step-back\" queries to retrieve higher-level context.\nGenerate hypothetical documents to align questions with embeddings.\nDecompose complex queries into single or multi-step sequences.\nIn some cases, you might spend a lot of time preparing RAG data—collecting\ndocuments, splitting them into chunks, and generating embeddings for\nsynthesis and retrieval (as covered earlier). Yet, you may still see low-quality\nresults from the vector store. This problem might not come from missing\nrelevant content in the vector store, but from issues in the user's question\nitself.\nFor instance, the question might be poorly phrased, unclear, or overly\ncomplex. Questions that aren’t clearly and simply stated can confuse both the\nvector store and the LLM, leading to weaker retrieval results. In this section,\nI'll show you techniques to refine the user's question, making it easier for the\nquery engine and the LLM to understand. By improving the question, you’ll\nlikely see better retrieval performance, providing more relevant context for\nthe LLM to deliver a solid answer.\nLet's begin with a straightforward method: using the LLM to help rephrase\nthe question.\n9.1 Rewrite-Retrieve-Read\nTo improve a poorly worded question, one effective method is to have an\nLLM rewrite it into a clearer form. This approach, covered in the paper\n“Query Rewriting for Retrieval-Augmented Large Language Models” by\nXinbei Ma et al. (https://arxiv.org/pdf/2305.14283.pdf), inspired the diagram\n",
      "content_length": 1601,
      "extraction_method": "Direct"
    },
    {
      "page_number": 279,
      "chapter": null,
      "content": "in figure 9.1.\nFigure 9.1 In the standard Retrieve-and-Read setup, the retriever processes the user question\ndirectly, delivering results to the LLM for synthesis. In the “Rewrite-Retrieve-Read” approach,\nan initial Rewrite step uses an LLM to rephrase the query before it reaches the retriever,\nenhancing the clarity of the retrieval process (diagram adapted from the paper “Query Rewriting\nfor Retrieval-Augmented Large Language Models”, https://arxiv.org/pdf/2305.14283.pdf).\n",
      "content_length": 479,
      "extraction_method": "Direct"
    },
    {
      "page_number": 280,
      "chapter": null,
      "content": "Documents\n\nRetrieve-and-Read\n\nRewrite-Retrieve-Read",
      "content_length": 51,
      "extraction_method": "OCR"
    },
    {
      "page_number": 281,
      "chapter": null,
      "content": "In the standard “Retrieve-and-Read” workflow, the retriever processes the\noriginal question directly and sends results to the LLM for synthesis. By\nadding a Rewrite step upfront, a rewriter (often an LLM) reformulates the\nquestion before passing it to the retriever. This improved workflow is known\nas “Rewrite-Retrieve-Read.”\nI usually apply this technique to create a tailored search query for the vector\nstore, keeping the original question in the RAG prompt. This approach allows\nthe rewritten query to optimize retrieval, particularly for semantic searches in\nthe vector database, while preserving the original question for the LLM to\nsynthesize the answer. See the workflow in figure 9.2, which is an amended\nversion of the RAG diagram you saw in figure 5.2.\nFigure 9.2 Using query rewriting to generate an optimized vector store query, while preserving\nthe original question for answer synthesis via the chatbot prompt.\n",
      "content_length": 927,
      "extraction_method": "Direct"
    },
    {
      "page_number": 282,
      "chapter": null,
      "content": "1b. The user's question is also included in the\nchatbot prompt sent to the LLM for answer\n\nuser question\n\nPTIrtiiiitiiiiii titi iit tt\n\n1a. The user's question is\n\nSSSSSSSSSSSER SESE EERE RED\n\nPrTiiiiiiiy) tiiitiii ie\n\npassed to a rewriter,\nUser which uses an LLM to\ngenerate a revised query\noptimized for vector store\nsearch\n2.The reever transforms retrieved\nthe user question into the chunks\ncoresponding embedding\n3.The retriever searches\nforrelevanttext usingthe qs\"; }4j7\nquestion embedding 5 The chatbot creates a\nprompt with the user\nquestion and the retrieved\nchunks (the context)\n4,The vector DB retums\ntext chunks relevant to 6.The LLIM accepts the\nthe user question prompt and synthetizes a\ncorresponding\ncompletion (the\nresponse)",
      "content_length": 741,
      "extraction_method": "OCR"
    },
    {
      "page_number": 283,
      "chapter": null,
      "content": "A prompt for rewriting the query can be as simple as the one below, adapted\nfrom a popular prompt in the Langchain Hub:\nRevise the original question to make it more refined and precise \nOriginal question: {user_question}  \nRevised Chroma DB query:\nTo apply the Rewrite-Retrieve-Read technique and use the prompt above to\nrewrite the original user question, open a new OS shell, navigate to the\nChapter 9 code folder, and set up your environment.:\nC:\\Github\\building-llm-applications\\ch09>\nC:\\Github\\building-llm-applications\\ch09>python -m venv env_ch09\nC:\\Github\\building-llm-applications\\ch09>pip install notebook lan\nC:\\Github\\building-llm-applications\\ch09>env_ch09\\Scripts\\activat\n(env_ch09) C:\\Github\\building-llm-applications\\ch09>jupyter noteb\nAfter launching Jupyter Notebook, create a new notebook named 09-\nquestion_transformations.ipynb. Then, re-import the UK tourist\ndestination content from Wikivoyage as in the previous chapter. For\nconvenience, I’ve included the code below.\nListing 9.1 Splitting and Ingesting Content from URLs\nfrom langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import HTMLSectionSplitter\nfrom langchain_community.document_loaders import AsyncHtmlLoader\nimport getpass\nOPENAI_API_KEY = getpass.getpass('Enter your OPENAI_API_KEY')\nuk_granular_collection = Chroma(\n   collection_name=\"uk_granular\",\n   embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_\n)\nuk_granular_collection.reset_collection()  #A\nuk_destinations = [\n   \"Cornwall\", \"North_Cornwall\", \"South_Cornwall\", \"West_Cornwal\n   \"Tintagel\", \"Bodmin\", \"Wadebridge\", \"Penzance\", \"Newquay\",\n",
      "content_length": 1655,
      "extraction_method": "Direct"
    },
    {
      "page_number": 284,
      "chapter": null,
      "content": "   \"St_Ives\", \"Port_Isaac\", \"Looe\", \"Polperro\", \"Porthleven\",\n   \"East_Sussex\", \"Brighton\", \"Battle\", \"Hastings_(England)\", \n   \"Rye_(England)\", \"Seaford\", \"Ashdown_Forest\"\n]\nwikivoyage_root_url = \"https://en.wikivoyage.org/wiki\"\nuk_destination_urls = [f'{wikivoyage_root_url}/{d}' for d in uk_d\nheaders_to_split_on = [(\"h1\", \"Header 1\"),(\"h2\", \"Header 2\")]\nhtml_section_splitter = HTMLSectionSplitter(headers_to_split_on=h\ndef split_docs_into_granular_chunks(docs):\n   all_chunks = []\n   for doc in docs:\n       html_string = doc.page_content  #B\n       temp_chunks = html_section_splitter.split_text(html_strin\n       h2_temp_chunks = [chunk for chunk in temp_chunks if \"Head\n       all_chunks.extend(h2_temp_chunks) \n   return all_chunks\nfor destination_url in uk_destination_urls:\n   html_loader = AsyncHtmlLoader(destination_url)  #E\n   docs =  html_loader.load()  #F\n   \n   for doc in docs:\n       print(doc.metadata)\n       granular_chunks = split_docs_into_granular_chunks(docs)\n       uk_granular_collection.add_documents(documents=granular_c\nThis setup prepares your data for effective query rewriting and retrieval using\nthe Rewrite-Retrieve-Read workflow. The Rewrite step will help craft refined\nsearch queries, improving retrieval results and the overall quality of the\nresponses generated.\n9.1.1 Retrieving Content Using the Original User Question\nLet’s start by performing a search with the original user question:\nuser_question = \"Tell me some fun things I can enjoy in Cornwall\"\ninitial_results = uk_granular_collection.similarity_search(query=\nfor doc in initial_results:\n   print(doc)\n",
      "content_length": 1605,
      "extraction_method": "Direct"
    },
    {
      "page_number": 285,
      "chapter": null,
      "content": "The output will look something like this (I have reduced it in various places):\npage_content='Do \n[ edit ] \nCornwall, in particular Newquay, is the UK's  surfing  capital, w\nThe  South West Coast Path  runs [REDUCED…] \nThe  Camel Trail  is an  18-mile (29   km)  [REDUCED…]\nThe  Cornish Film Festival  is held annually [REDUCED…]\nThe  Royal Cornwall Show  is an agricultural show [REDUCED…]  \nCamel Creek Adventure Park , Tredinnick,  Wadebridge  offers grea\nFestivals \n[ edit ] \nMummer's Day , or \"Darkie Day\" as it is sometimes known [REDUCED\n' Obby 'Oss  is held annually on May Day (1 May), mainly in  Pad\nSt Piran's Day  (Cornish:  Gool Peran ) is the national day of C\npage_content='Do \n[ edit ] \nThe  South West Coast Path  runs [REDUCED…]  \nThe  Camel Trail  is an  18-mile (29   km)  off-road cycle-track \nThe  Cornish Film Festival  is held annually [REDUCED…]  \nCornwall, in particular Newquay, is the UK's  surfing  capital, [\nCricket:   Cornwall CCC  play in the National Counties [REDUCED…\npage_content='Buy \n[ edit ] \nIn the village centre you will find the usual [REDUCED…] ' metad\npage_content='Do \n[ edit ] \nThe  South West Coast Path  runs along [REDUCED…]  \nSt Piran's Day  (Cornish:  Gool Peran ) is the national day of C\nThis output provides some information on fun activities, but it appears\nsomehow limited. This might be due to the way the question has been\nworded.\n9.1.2 Setting Up the Query Rewriter Chain\nTo refine the user question into a more effective query for Chroma DB, we’ll\nuse the LLM to rewrite it into a format better suited for semantic search.\n",
      "content_length": 1585,
      "extraction_method": "Direct"
    },
    {
      "page_number": 286,
      "chapter": null,
      "content": "Setting up a query rewriter chain will help us automate this transformation.\nStart by importing the necessary libraries:\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nllm = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=OPENAI_API_K\nNext, define the prompt that instructs the LLM to rewrite the user question:\nrewriter_prompt_template = \"\"\"\nGenerate search query for the Chroma DB vector store from a user \nJust return the revised Chroma DB query, with quotes around it. \nUser question: {user_question}\nRevised Chroma DB query:\n\"\"\"\nrewriter_prompt = ChatPromptTemplate.from_template(rewriter_promp\nNow, construct the chain to execute the rewriting process:\nrewriter_chain = rewriter_prompt | llm | StrOutputParser()\nThis setup allows you to pass a user question to the rewriter chain, which\ngenerates a tailored query optimized for Chroma DB, enhancing retrieval\naccuracy.\n9.1.3 Retrieving Content with the Rewritten Query\nNow, let’s use the rewriter chain to create a more targeted query and see if it\nreturns more accurate results compared to the original question.\nFirst, generate the rewritten query:\nuser_question =\"Tell me some fun things I can do in Cornwall\"\nsearch_query = rewriter_chain.invoke({\"user_question\": user_quest\nprint(search_query)\nIf you print search_query, you should see something like:\n",
      "content_length": 1417,
      "extraction_method": "Direct"
    },
    {
      "page_number": 287,
      "chapter": null,
      "content": "\"fun activities to do in Cornwall\"\nNow, use this refined query to perform the vector store search:\nimproved_results = uk_granular_collection.similarity_search(query\nFinally, print the results to review their relevance:\nfor doc in improved_results:\n   print(doc)\nYou will get the following output, which I have reduced to save space:\npage_content='Do \n[ edit ] \nCornwall, in particular Newquay, is the UK's  surfing  capital, \nThe  South West Coast Path  runs along the coastline  [REDUCED…]\nThe Cornish section is supposed to be the most  [REDUCED…]\nThe  Camel Trail  is an  18-mile (29   km)  off-road cycle-track \nThe  Cornish Film Festival  is held annually each November aroun\nThe  Royal Cornwall Show  is an agricultural show  [REDUCED…] \nCamel Creek Adventure Park , Tredinnick,  Wadebridge  offers  [RE\nFestivals \n[ edit ] \nMummer's Day , or \"Darkie Day\" as it is sometimes  [REDUCED…]\n' Obby 'Oss  is held annually on May Day (1 May), [REDUCED…]\nSt Piran's Day  (Cornish:  Gool Peran ) is the national day  [RE\npage_content='Do \n[ edit ] \nThe  South West Coast Path  runs  [REDUCED…]\nThe  Camel Trail  is an  18-mile (29   km)  off-road cycle- [RED\nThe  Cornish Film Festival  is held annually each November aroun\nCornwall, in particular Newquay, is the UK's  surfing  capital, \nCricket:   Cornwall CCC  play in the National Counties Cricket  \npage_content='Do \n[ edit ] \nHelford River  is an idyllic river estuary between Falmouth and \nThe  South West Coast Path  runs along the coastline  [REDUCED…]\nFestivals \n[ edit ] \nAllantide  (Cornish:  Kalan Gwav  or  Nos Kalan Gwav ) ia a Corn\n",
      "content_length": 1596,
      "extraction_method": "Direct"
    },
    {
      "page_number": 288,
      "chapter": null,
      "content": "Furry Dance , also known as Flora Day, takes place in   [REDUCED\nGolowan , sometimes also  Goluan  or  Gol-Jowan , is the Cornish \nGuldize  is an ancient harvest festival in Autumn, [REDUCED…]\nMontol Festival  is an annual heritage, arts and community  [RED\nNickanan Night  is traditionally held on the Monday before Lent. \nSt Piran's Day  (Cornish:  Gool Peran ) is the national day of C\nThis approach should yield results that align more closely with your original\nintent, displaying a broader selection of content on activities available in\nCornwall. Using the rewritten question, the vector store retriever provides a\nmore diverse set of text chunks compared to the results from the original\nquestion.\n9.1.4 Combining Everything into a Single RAG Chain\nNow, you can build a complete workflow that transforms the initial user\nquestion into a search query for vector retrieval. The original question is\nretained in the prompt to generate the final answer. Listing 9.2 shows the full\nRAG chain, including the query rewriting step.\nListing 9.2 Combined RAG Chain with Query Rewriting\nfrom langchain_core.runnables import RunnablePassthrough\nretriever = uk_granular_collection.as_retriever()\nrag_prompt_template = \"\"\"\nGiven a question and some context, answer the question.\nIf you do not know the answer, just say I do not know.\nContext: {context}\nQuestion: {question}\n\"\"\"\nrag_prompt = ChatPromptTemplate.from_template(rag_prompt_template\nrewrite_retrieve_read_rag_chain = (\n   {\n       \"context\": {\"user_question\": RunnablePassthrough()} | rew\n       \"question\": RunnablePassthrough(),  #B\n   }\n   | rag_prompt\n   | llm\n   | StrOutputParser()\n",
      "content_length": 1643,
      "extraction_method": "Direct"
    },
    {
      "page_number": 289,
      "chapter": null,
      "content": ")\nNow, run the complete workflow:\nuser_question = \"Tell me some fun things I can do in Cornwall\"\nanswer = rewrite_retrieve_read_rag_chain.invoke(user_question)\nprint(answer) \nWhen you print the answer, you should see a response like this:\nIn Cornwall, you can enjoy a variety of fun activities such as:\n1. Surfing in Newquay, known as the UK's surfing capital, where y\n2. Walking along the scenic South West Coast Path, which offers b\n3. Cycling on the Camel Trail, an 18-mile off-road cycle track th\n4. Attending the Cornish Film Festival held annually in November \n5. Exploring the Helford River, where you can take a ferry ride, \n6. Participating in local festivals such as St Piran's Day or the \n7. Visiting Camel Creek Adventure Park for family-friendly entert\n8. Enjoying various agricultural shows, like the Royal Cornwall S\nThere are plenty of options for both adventure and relaxation in \nThis output demonstrates a satisfying answer based on the combined\nRewrite-Retrieve-Read workflow.\nNext, I’ll show you further ways to refine the “Rewrite-Retrieve-Read”\nprocess.\n9.2 Generating Multiple Queries\nThe “Rewrite-Retrieve-Read” approach assumes the original user question\nwas poorly phrased. But if the question is well-formed and contains multiple\nimplicit questions, rewriting it as a single improved query may not be\neffective. In these cases, it’s better to have the LLM break down the original\nquestion into multiple explicit questions. Each question can be executed\nseparately against the vector store, with the answers then synthesized into a\ncomprehensive response. Figure 9.3 illustrates this workflow.\nFigure 9.3 Workflow for Multiple Query Generation: The LLM application reformulates the\noriginal question into multiple explicit questions, which are executed against the vector store to\nproduce the context, and synthesizes the answer from a prompt with the original question and the\n",
      "content_length": 1905,
      "extraction_method": "Direct"
    },
    {
      "page_number": 290,
      "chapter": null,
      "content": "produced context.\n",
      "content_length": 18,
      "extraction_method": "Direct"
    },
    {
      "page_number": 291,
      "chapter": null,
      "content": "Is LAwarmer than Miami\n\nin Apri?\n\nls LAwarmer than Miami\n\nUser generate 3 queries from the original question\n\nQuestion: Is LA warmer than Miami in April?\nContext: C1 +02 +03",
      "content_length": 173,
      "extraction_method": "OCR"
    },
    {
      "page_number": 292,
      "chapter": null,
      "content": "In the figure 9.3, the LLM application reformulates the original question into\nmultiple explicit questions, executes them against the vector store to gather\ncontext, and then synthesizes the answer using a prompt that includes both\nthe original question and the retrieved context.\nFor example, if you prompt ChatGPT with the following:\nReformulate the following question into multiple explicit questio\nIs LA warmer than Miami in April?\nYou might receive the following questions, which can be executed\nindividually against the vector store:\nWhat is the average temperature in Los Angeles during April?\nWhat is the average temperature in Miami during April?\nCan you compare the April temperatures in Los Angeles and Miami a\nIs there a notable difference in temperature between Los Angeles \nCould you provide insights into how the temperatures in Los Angel\nThis approach is especially useful when designing generic LLM applications.\nYou can automatically generate multiple queries for any user question using a\nprompt like this, adapted from a LangChain example:\nQUERY_PROMPT = PromptTemplate(\n   input_variables=[\"question\"],\n   template=\"\"\"You are an AI language model assistant. Your task \n   different versions of the given user question to retrieve rel\n   database. By generating multiple perspectives on the user que\n   the user overcome some of the limitations of the distance-bas\n   Provide these alternative questions separated by newlines.\n   Original question: {question}\"\"\",\n)\nLangChain’s MultiQueryRetriever class allows you to use a prompt, an\nLLM reference, and a retriever (e.g., derived from a vector database). When a\nMultiQueryRetriever instance processes a user query, it executes the entire\nmulti-query workflow automatically, as shown in Figure 7.10. This approach\ncombines multi-retrieval with answer synthesis.\nNext, I’ll show you how to implement a custom MultiQueryRetriever.\n",
      "content_length": 1899,
      "extraction_method": "Direct"
    },
    {
      "page_number": 293,
      "chapter": null,
      "content": "9.2.1 Setting Up the Chain for Generating Multiple Queries\nFirst, import the necessary libraries:\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom typing import List\nfrom langchain_core.output_parsers import BaseOutputParser\nfrom pydantic import BaseModel, Field\nBegin by setting up the prompt shown earlier to instruct the LLM to generate\nmultiple variations of a single user question:\nmulti_query_gen_prompt_template = \"\"\"\nYou are an AI language model assistant. Your task is to generate \ndifferent versions of the given user question to retrieve relevan\ndatabase. By generating multiple perspectives on the user questio\nthe user overcome some of the limitations of the distance-based s\nProvide these alternative questions separated by newlines.\nOriginal question: {question}\n\"\"\"\nmulti_query_gen_prompt = ChatPromptTemplate.from_template(multi_q\nSince the LLM is generating five alternative questions, it’s useful to format\nthe output as a list of strings, with each string representing one question. This\nlets you process each question independently. To do this, implement a custom\nresult parser instead of the standard StrOutputParser:\nclass LineListOutputParser(BaseOutputParser[List[str]]):\n   \"\"\"Parse out a question from each output line.\"\"\"\n   def parse(self, text: str) -> List[str]:\n       lines = text.strip().split(\"\\n\")\n       return list(filter(None, lines))  \nquestions_parser = LineListOutputParser() \nWith these components, you can set up the chain to generate multiple queries:\nllm = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=OPENAI_API_K\nmulti_query_gen_chain = multi_query_gen_prompt | llm | questions_\n",
      "content_length": 1704,
      "extraction_method": "Direct"
    },
    {
      "page_number": 294,
      "chapter": null,
      "content": "Now, try running this chain:\nuser_question = \" Tell me some fun things I can do in Cornwall.\"\nmultiple_queries = multi_query_gen_chain.invoke(user_question)\nWhen you print multiple_queries, you should get a list of questions similar\nto this:\n['What are some enjoyable activities to explore in Cornwall?  ',\n'Can you suggest interesting attractions or events in Cornwall f\n'What are the top leisure activities to try out while visiting C\n'What fun experiences or adventures does Cornwall have to offer? \n'Could you recommend some entertaining things to do while in Cor\nOne final step remains to complete the setup.\n9.2.2 Setting Up a Custom Multi-Query Retriever\nTo configure a custom multi-query retriever, start by creating a standard\nretriever and then embedding it within the multi-query retriever, as shown\nbelow:\nbasic_retriever = uk_granular_collection.as_retriever()\nmulti_query_retriever = MultiQueryRetriever(\n   retriever=basic_retriever, llm_chain=multi_query_gen_chain, \n   parser_key=\"lines\"  #A\n)  \nNow, test it with an example query:\nuser_question = \"Tell me some fun things I can do in Cornwall\"\nretrieved_docs = multi_query_retriever.invoke(user_question)\nWhen you print retrieved_docs, you should see output similar to this:\n[Document(metadata={'Header 2': 'Do'}, page_content='Do \\n [ edit \nDocument(metadata={'Header 2': 'Do'}, page_content='Do \\n [ edit \nDocument(metadata={'Header 2': 'Contents'}, page_content='Conten\nDocument(metadata={'Header 2': 'Do'}, page_content=\"Do \\n [ edit \nDocument(metadata={'Header 2': 'Festivals'}, page_content='Festi\n",
      "content_length": 1572,
      "extraction_method": "Direct"
    },
    {
      "page_number": 295,
      "chapter": null,
      "content": "Document(metadata={'Header 2': 'Contents'}, page_content='Conten\nDocument(metadata={'Header 2': 'See'}, page_content=\"See \\n [ ed\n9.2.3 Using a Standard MultiQueryRetriever Instance\nFor straightforward use cases, you can set up multi-query generation with a\nstandard MultiQueryRetriever instance. First, instantiate the multi-query\nretriever:\nstd_multi_query_retriever = MultiQueryRetriever.from_llm(\n   retriever=basic_retriever, llm=llm\n)\nNow, test it with the same question:\nuser_question = \" Tell me some fun things I can do in Cornwall\"\nretrieved_docs = multi_query_retriever.invoke(user_question)\nThe output in retrieved_docs will be similar to the results you saw\npreviously.\nIn some cases, rewriting the original question or breaking it into multiple\nexplicit questions may not yield the desired results. Continue reading to\nexplore a technique that can improve accuracy in these situations.\n9.3 Step-Back Question\nWhen you send a highly detailed question directly to the vector store—\nassuming your documents are split into small, specific chunks—you might\nretrieve information that’s too focused, missing broader context. This can\nlimit the LLM’s ability to generate a comprehensive answer.\nAs discussed in section 7.4, one solution is to create two sets of document\nchunks: coarse chunks for synthesis and fine-grained chunks for detailed\nretrieval. Another solution is to adjust the user question rather than the\ndocument chunks, using an approach called a “step-back question.”\nIn this approach, you start with the user’s detailed question but then create a\n",
      "content_length": 1571,
      "extraction_method": "Direct"
    },
    {
      "page_number": 296,
      "chapter": null,
      "content": "broader question to retrieve a more generalized context. This “step-back\ncontext” provides a higher-level view than the “original context” derived\nfrom the specific question. You then provide the LLM with both the detailed\ncontext and the broader context to enable a fuller response, as illustrated in\nfigure 9.4.\nFigure 9.4 Step-Back Question Workflow: The LLM application first sends the detailed question\n(Q_D) to the vector store to retrieve a detailed context (C_D). It then prompts the LLM to\ngenerate a more abstract question (Q_A) based on Q_D, which is also executed on the vector\nstore to obtain an abstract context (C_A). Finally, the LLM application combines Q_D, C_D, and\nC_A into a single prompt, enabling the LLM to synthesize a comprehensive answer.\n",
      "content_length": 766,
      "extraction_method": "Direct"
    },
    {
      "page_number": 297,
      "chapter": null,
      "content": "User ooo\n\n|\nQuestion: Detailed question Q_D\nContext C_D+C_A",
      "content_length": 59,
      "extraction_method": "OCR"
    },
    {
      "page_number": 298,
      "chapter": null,
      "content": "The LLM application sends the original detailed question to the vector store\nto retrieve detailed context, then generates and executes a broader question to\nobtain abstract context. It combines both contexts with the original question\nto enable the LLM to create a comprehensive answer.\nThis technique, developed by Huaixiu Steven Zheng et al., is explained in\ntheir paper “Take a Step Back: Evoking Reasoning via Abstraction in Large\nLanguage Models” (https://arxiv.org/pdf/2310.06117.pdf?\nref=blog.langchain.dev).\nTo implement this, use a prompt like the following to generate the step-back\nquestion:\nGenerate a less specific question (aka Step-back question) for th\nDetailed question: {detailed_question}\nStep-back question:\nFor example, if you input the prompt with the detailed question, “Can you\ngive me some tips for a trip to Brighton?” the more abstract (step-back)\nquestion might look like:\nStep-back question: \"What should I know before visiting a popular \nThis broader question helps retrieve more general information, which,\ncombined with the detailed context, allows the LLM to produce a well-\nrounded answer.\n9.3.1 Setting Up the Chain to Generate a Step-Back Question\nImplementing the step-back question technique is straightforward: it involves\ncrafting an effective prompt to generate a broader question and then\nfollowing a standard RAG workflow. Here’s a sample implementation, which\nclosely resembles the pattern used for the rewrite-retrieve-read technique.\nStart by setting up the prompt in your Jupyter notebook:\nllm = ChatOpenAI(model=\"4o-mini\", openai_api_key=OPENAI_API_KEY)\nstep_back_prompt_template = \"\"\"\n",
      "content_length": 1634,
      "extraction_method": "Direct"
    },
    {
      "page_number": 299,
      "chapter": null,
      "content": "Generate a less specific question (aka Step-back question) for th\nDetailed question: {detailed_question}\nStep-back question:\n\"\"\"\nstep_back_prompt = ChatPromptTemplate.from_template(step_back_pro\nNote\nI’ve chosen to use 4o-mini over gpt-4o and gpt-4o-mini because it tends to\ngenerate more abstract yet contextually relevant queries, along with more\ncoherent and well-synthesized final answers. That said, I encourage you to\nexperiment with different models to see how their outputs vary.\nNow, create the chain:\nstep_back_question_gen_chain = step_back_prompt | llm | StrOutput\nTry out the chain with a sample question:\nuser_question = \"Can you give me some tips for a trip to Brighton\nstep_back_question = step_back_question_gen_chain.invoke(user_que\nWhen you print step_back_question, you should get a response like:\n'What are some general tips for planning a successful trip to a c\nThis generated step-back question can then be used within a RAG\narchitecture to retrieve broader context from the vector store, which can\nsubsequently be provided to the LLM to help synthesize a more complete\nanswer.\n9.3.2 Incorporating Step-Back Question Generation into the\nRAG Chain\nYou can integrate the step-back question generation chain into a RAG\nworkflow, as shown in Listing 9.3.\nListing 9.3 Integrating Step-Back Question Generation within a RAG Architecture\n",
      "content_length": 1354,
      "extraction_method": "Direct"
    },
    {
      "page_number": 300,
      "chapter": null,
      "content": "retriever = uk_granular_collection.as_retriever()\nrag_prompt_template = \"\"\"\nGiven a question and some context, answer the question.\nIf you do not know the answer, just say I do not know.\nContext: {context}\nQuestion: {question}\n\"\"\"\nrag_prompt = ChatPromptTemplate.from_template(rag_prompt_template\nstep_back_question_rag_chain = (\n   {\n       \"context\": {\"detailed_question\": RunnablePassthrough()} | \n       \"question\": RunnablePassthrough(),  #B\n   }\n   | rag_prompt\n   | llm\n   | StrOutputParser()\n)\nNow, try running the chain:\nuser_question = \"Can you give me some tips for a trip to Brighton\nanswer = step_back_question_rag_chain.invoke(user_question)\nprint(answer)\nYou should see a synthesized response like this:\nHere are some tips for a trip to Brighton:\n1. **Stay Safe**: While Brighton is generally safe, be cautious i\n2. **Watch for Traffic**: Be mindful of traffic, especially in bu\n3. **Valuables**: Take standard precautions with your valuables t\n4. **Homelessness**: Be aware that there may be homeless individu\n5. **Beaches**: Lifeguards patrol the beaches from late May to ea\n6. **Emergency Contacts**: In case of emergencies related to the \n7. **Explore Local Venues**: Enjoy local venues favored by reside\n8. **Cultural Areas**: Visit areas like The Lanes and North Laine \n9. **Stay Informed**: Keep an eye on your surroundings, especiall\nEnjoy your trip to Brighton!\nThis technique offers an alternative to embedding-focused methods,\n",
      "content_length": 1453,
      "extraction_method": "Direct"
    },
    {
      "page_number": 301,
      "chapter": null,
      "content": "enhancing retrieval by broadening the question itself to improve context.\nNext, I’ll introduce another technique that also optimizes retrieval through\nquestion transformation.\n9.4 Hypothetical Document Embeddings (HyDE)\nAs discussed in section 7.2.2, embedding hypothetical questions can enhance\nRAG retrieval by indexing document chunks with additional embeddings that\nrepresent questions answerable by the content in each chunk. This approach\nmakes embeddings of these hypothetical questions more semantically similar\nto the user’s question than embeddings of the raw chunk text alone.\nA similar effect can be achieved with Hypothetical Document Embeddings\n(HyDE), a technique that keeps the original chunk embeddings unchanged\nwhile generating hypothetical documents based on the user’s question, as\nshown in figure 9.5.\nFigure 9.5 Hypothetical Document Embeddings (HyDE): In this approach, the LLM generates\nhypothetical documents that would answer the user’s question. Rather than querying the\ndocument store with the user’s original question, these generated documents are used. Because\nthese hypothetical documents are semantically closer to the document chunk text, they improve\nthe relevance of retrieved content.\n",
      "content_length": 1223,
      "extraction_method": "Direct"
    },
    {
      "page_number": 302,
      "chapter": null,
      "content": "User\n\nQuestion: Original question 0_Q\nContext: C1 +C02+C3\n\nSSS SSS SSS SSS SSS SSS SESS SSS SESE EES",
      "content_length": 100,
      "extraction_method": "OCR"
    },
    {
      "page_number": 303,
      "chapter": null,
      "content": "As shown in the sequence diagram in Figure 9.5, the generated hypothetical\ndocuments are used to retrieve relevant content, aiming to increase the\nsemantic similarity between the user’s question and the document chunk\nembeddings. This technique was introduced by Luyu Gao et al. in their paper,\n“Precise Zero-Shot Dense Retrieval without Relevance Labels”\n(https://arxiv.org/pdf/2212.10496v1.pdf). Now, let’s proceed to implement\nthis workflow.\nLet’s move on to implementing this workflow.\n9.4.1 Generating an Hypothetical Document for the User\nQuestion\nImplementing HyDE follows a familiar pattern: you design a chain to\ngenerate a hypothetical document that could answer the user’s question, and\nthen use this generated document as input for retrieval within a larger RAG\nworkflow.\nFirst, set up the prompt:\nllm = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=OPENAI_API_K\nhyde_prompt_template = \"\"\"\nWrite one sentence that could answer the provided question. Do no\nQuestion: {question}\nSentence:\n\"\"\"\nhyde_prompt = ChatPromptTemplate.from_template(hyde_prompt_templa\nNext, build the HyDE chain:\nhyde_chain = hyde_prompt | llm | StrOutputParser()\nTest it with a sample question:\nuser_question = \"What are the best beaches in Cornwall?\"\nhypotetical_document = hyde_chain.invoke(user_question)\n",
      "content_length": 1294,
      "extraction_method": "Direct"
    },
    {
      "page_number": 304,
      "chapter": null,
      "content": "When you print hypothetical_document, you should see output like:\nSome of the best beaches in Cornwall include Fistral Beach, Porth\nThis generated hypothetical document can then be used in the RAG chain to\nretrieve relevant content, improving the alignment between the user’s\nquestion and the document chunks in the vector store. Let’s look at\nintegrating this step into the broader RAG workflow next.\n9.4.2 Integrating the HyDE Chain into the RAG Chain\nYou can incorporate the HyDE chain into a RAG workflow, as shown in\nListing 9.4.\nListing 9.4 Integrating a HyDE Chain within a RAG Workflow\nretriever = uk_granular_collection.as_retriever()\nrag_prompt_template = \"\"\"\nGiven a question and some context, answer the question.\nOnly use the provided context to answer the question.\nIf you do not know the answer, just say I do not know. \nContext: {context}\nQuestion: {question}\n\"\"\"\nrag_prompt = ChatPromptTemplate.from_template(rag_prompt_template\nhyde_rag_chain = (\n   {\n       \"context\": {\"question\": RunnablePassthrough()} | hyde_cha\n       \"question\": RunnablePassthrough(),  #B\n   }\n   | rag_prompt\n   | llm\n   | StrOutputParser()\n)\nNow, try the complete RAG chain:\nuser_question = \"What are the best beaches in Cornwall?\"\n",
      "content_length": 1226,
      "extraction_method": "Direct"
    },
    {
      "page_number": 305,
      "chapter": null,
      "content": "answer = hyde_rag_chain.invoke(user_question)\nprint(answer)\nYou should see a synthesized answer similar to this:\nThe best beaches in Cornwall mentioned in the context include Bud\nThis concludes the integration of HyDE into the RAG chain, enhancing the\nretrieval process by using a hypothetical document. Before moving on, I’ll\nbriefly revisit the multi-query generation technique we discussed earlier to\nrefine the retrieval focus.\n9.5 Single-Step and Multi-Step Decomposition\nIn the “multiple questions” or “sub-questions” method covered in section 9.2,\nthe original user question contained several implicit questions. For that case,\nwe instructed the LLM to generate a set of explicit, independent questions,\neach of which could be executed separately (or in parallel) on the vector\nstore. This approach, called “single-step decomposition,” is effective when\nthe questions are independent, and the original “complex question” can be\nsplit into simple, single-step questions.\nHowever, if the original question includes several interdependent questions, a\ndifferent approach is required. For example, if your data store contains tourist\ninformation, consider the question: “What is the average August temperature\nat the most popular sandy beach in Cornwall?” This question requires a\nsequence of dependent queries, where each answer informs the next question.\nIn this case, you could instruct the LLM to generate a strategic plan for\nbreaking down the question into a sequence of interdependent queries. Each\nquery would contain a parameter, filled with information from the previous\nanswer. Once the LLM returns this sequence of parameterized questions, you\ncan execute them step-by-step, storing each answer and feeding it into the\nsubsequent query. This process continues until you reach the final answer, as\nshown in figure 9.6.\nFigure 9.6 Multi-Step Decomposition Workflow: In this workflow, the original complex question\nis sent to the LLM, which generates a sequence of parameterized questions. Each question is then\nexecuted on the vector store, using answers from previous steps as parameters. Once all questions\n",
      "content_length": 2122,
      "extraction_method": "Direct"
    },
    {
      "page_number": 306,
      "chapter": null,
      "content": "are answered, the LLM synthesizes a final response based on the collected information.\n",
      "content_length": 87,
      "extraction_method": "Direct"
    },
    {
      "page_number": 307,
      "chapter": null,
      "content": "User\n\nQuestion: Original question 0_Q\nContext: C14 C2 +03\n\nSSS SSS SSS SSS SSS SSS SESS SSS SESE EES",
      "content_length": 100,
      "extraction_method": "OCR"
    },
    {
      "page_number": 308,
      "chapter": null,
      "content": "In the workflow illustrated in the sequence diagram, the original complex\nquestion is sent to the LLM, which breaks it down into a series of\nparameterized questions. Each question is executed on the vector store, using\nprevious answers as parameters. After all questions are answered, the LLM\ncombines the collected information to generate a final response.\nTo prompt the LLM to generate this question sequence, use a template like\nthis:\nBreak down the following question into multiple dependent steps t\nFor each step, include the question and the corresponding paramet\n- Question: The text of the question.\n- Parameter: The parameter to be filled with the previous answer.\n—---\nOriginal question: “What is the average August temperature at the \nMultiple dependent questions:\nTo illustrate what such a sequence of questions might look like, you can try\nrunning the prompt above in ChatGPT. Replacing the user question with\n“What is the average August temperature at the most popular sandy beach in\nCornwall?” will produce a response similar to the one below:\n[\n {\n   \"Question\": \"What is the most popular sandy beach in Cornwall\n   \"Parameter\": \"None (initial query to the full data source)\"\n },\n {\n   \"Question\": \"What are the recorded daily temperatures in Augu\n   \"Parameter\": \"Beach Name from the previous answer\"\n },\n {\n   \"Question\": \"What is the average temperature from the followi\n   \"Parameter\": \"Daily Temperatures from the previous answer\"\n }\n]\nEach question can then be executed against the vector store or SQL database,\nusing the answer from the previous step as its parameter value. After\nprocessing the final question, you will have the context needed to answer the\noriginal question with the LLM.\n",
      "content_length": 1714,
      "extraction_method": "Direct"
    },
    {
      "page_number": 309,
      "chapter": null,
      "content": "Note\nIf using a SQL database, you could simplify this with native SQL functions\nlike AVG. However, this example illustrates the concept rather than the\noptimal implementation.\nWhile I won’t provide a full implementation here, this method builds on\nadvanced RAG techniques, such as:\nRouting a natural language question to the relevant content store.\nGenerating a query for specific content stores, like SQL databases, from\na natural language question.\nWhile LangChain doesn’t offer a dedicated class for multi-step question\ndecomposition, you may find inspiration in LlamaIndex’s\nMultiStepQueryEngine class, which is worth exploring for further ideas.\nThis concludes our exploration of question transformation techniques for\nenhancing retrieval effectiveness. In the next chapter, you’ll learn additional\nmethods to improve RAG performance.\n9.6 Summary\nThe quality of LLM answers depends on well-structured questions, as\nvague or complex queries can mislead the vector store and LLM,\nresulting in weaker results.\nPoorly phrased questions lead to less accurate retrievals and answers,\nmaking clear and concise questions essential.\nThe \"Rewrite-Retrieve-Read\" approach improves answers by refining\nthe original question, helping retrieve better context and enhancing LLM\nresponses.\nSplitting questions into explicit sub-questions, executed separately,\nensures better retrieval, and LangChain’s MultiQueryRetriever can\ncombine these answers into a comprehensive response.\nHighly detailed queries can return overly specific results, missing the\nbroader context needed for a complete answer.\nUsing coarse chunks for synthesis and fine-grained chunks for detail, or\n",
      "content_length": 1659,
      "extraction_method": "Direct"
    },
    {
      "page_number": 310,
      "chapter": null,
      "content": "employing “step-back questions” to broaden queries, helps capture high-\nlevel context.\nEmbedding hypothetical questions alongside document chunks improves\nretrieval by aligning them with potential user questions.\nGenerating hypothetical documents with HyDE retrieves semantically\naligned content based on user questions without altering chunk\nembeddings.\nDecomposing questions into independent sub-questions allows for\nindividual retrieval and synthesis.\nGenerating a sequence of parameterized questions for interdependent\nqueries ensures that each step builds on the previous answer,\nsystematically resolving dependencies for a final answer.\n",
      "content_length": 643,
      "extraction_method": "Direct"
    },
    {
      "page_number": 311,
      "chapter": null,
      "content": "10 Query generation, routing and\nretrieval post-processing\nThis chapter covers\nGenerate metadata queries directly from user questions\nConvert user questions into database-specific queries (e.g., SQL,\nSPARQL)\nRoute questions to the appropriate handler based on intent\nEnhance result relevance using Reciprocal Rank Fusion (RRF)\nIn chapters 8 and 9, you improved RAG answer accuracy using advanced\nindexing and query transformations. Indexing strengthens embedding\neffectiveness for broader chunks, adding richer context, while query\ntransformations boost the precision of vector store retrieval.\nNow, you’ll dive into three more advanced RAG techniques. First, you’ll\nlearn to generate queries specific to the type of content store in use. For\ninstance, you’ll see how to generate SQL from a user's natural language\nquestion to retrieve data from a relational database. Your setup might include\nseveral types of content stores—such as vector stores, a relational database,\nor even a knowledge graph database. You’ll use the LLM to direct the user’s\nquestion to the right content store.\nFinally, you’ll refine the retrieved results to send only the most relevant\ncontent for synthesis, filtering out unnecessary data to maintain clarity and\nrelevance.\n10.1 Content Database Query Generation\nTo give an LLM the best information possible for answering user questions,\nyou often need to access other databases beyond your vector store. Many of\nthese databases hold structured data and only accept structured queries. You\n",
      "content_length": 1516,
      "extraction_method": "Direct"
    },
    {
      "page_number": 312,
      "chapter": null,
      "content": "might wonder if the difference between unstructured user questions and the\nstructured queries required for these databases poses a problem. This gap can\nbe bridged with an LLM’s help. Let’s look at common content stores in LLM\napplications and typical ways to retrieve data from them.\nVector Store (or Vector Database): Vector Store (or Vector Database):\nYou’re already familiar with vector stores, which hold document chunks\nalong with their embeddings in a vector-based index to enable semantic\nretrieval. This approach, known as dense retrieval, uses embeddings—\ncompact vectors with hundreds or thousands of dimensions that capture\nthe semantic meaning of text. Similarity between a user query and stored\nchunks is computed based on the distance between these dense vectors.\nAn alternative is sparse retrieval (also called lexical retrieval), which\nmany vector databases support, though it doesn’t require one. In the\nindexing phase, each document chunk is tokenized, and an inverted\nindex is built to map each unique token to the list of chunks in which it\nappears. During querying, the user’s question is tokenized in the same\nway, and each token is matched against the inverted index to retrieve\nrelevant chunks. These are then ranked using relevance scoring methods\nlike BM25 or TF–IDF, based on how well the term statistics of each\nchunk align with the query. Sparse search excels at precise, keyword-\ndriven queries, supports Boolean logic (such as “must” and “must not”),\nand offers strong explainability by directly linking results to matching\nterms in the query.\nRelational (SQL) Database: An LLM application can connect to a\nrelational database, which stores structured facts in tables. Data is\ntypically retrieved using SQL queries. For example, a database might\ncontain seasonal temperatures at tourist resorts or lists of available\nhotels and car rentals by location. LLMs can assist by generating SQL\nqueries from natural language questions—a technique known as text-to-\nSQL. This approach is gaining popularity as a way to make databases\naccessible to non-technical users.\nDocument, Key-Value, or Object Databases: These databases store data\nas documents or objects, typically in JSON or BSON format. Because\nmany LLMs have been extensively trained on JSON structures, they can\naccurately convert user questions into JSON-based queries that align\nwith the database schema. Notably, many document databases have\n",
      "content_length": 2430,
      "extraction_method": "Direct"
    },
    {
      "page_number": 313,
      "chapter": null,
      "content": "recently been rebranded as vector databases after introducing support for\nvector fields—used to store embeddings—and adding vector search and\nsimilarity capabilities.\nKnowledge Graph Databases: Knowledge graphs represent data as a\ngraph, where nodes correspond to entities and edges define their\nrelationships. Originally popularized by companies like Facebook and\nLinkedIn to infer connections in social networks, graph databases are\nnow increasingly used in LLM applications. LLMs can transform\nunstructured text into structured knowledge graphs—a process often\nreferred to as Knowledge Graph Enhanced RAG—resulting in a more\ncompact and structured representation compared to vector stores. Once\ndata is stored in a graph database, it can be queried using graph-specific\nlanguages like SPARQL or Cypher, enabling complex reasoning and\ninference that go beyond simple similarity search. This forms the basis\nof GraphRAG. We’ll explore how LLMs can assist in building these\ngraph structures from raw text, generating SPARQL or Cypher queries,\nand finally converting query results back into natural language\nresponses.\nNow, let’s dig into the process of transforming a user’s question into a\nstructured query for different databases, starting with retrieving document\nchunks from a vector store using metadata.\n10.2 Self-Querying (Metadata Query Enrichment)\nA vector store typically indexes document chunks by embedding for dense\nsearch, but it can also use keyword-based indexing in a few ways:\n1. Explicit Metadata Tags: You can add metadata to each chunk, such as\nthe timestamp, filename or URL, topic, and keywords. These keywords\ncan come from user input or ones you assign manually.\n2. Keyword Extraction via Algorithm: Use algorithms like TF-IDF (Term\nFrequency-Inverse Document Frequency) or its extension, BM25, to\nidentify relevant keywords for each chunk based on word frequency and\nimportance.\n3. Keyword Suggestions from the LLM: You can ask the LLM to generate\nkeywords for tagging each chunk.\n",
      "content_length": 2007,
      "extraction_method": "Direct"
    },
    {
      "page_number": 314,
      "chapter": null,
      "content": "With keywords attached to chunks through any of these methods, you can\nperform a semantic search, focusing only on chunks filtered by a keyword-\nbased (or “sparse”) search. If your chatbot’s UI allows users to filter results\ndirectly—such as with dropdown options—your vector store query can\nexplicitly include a metadata filter based on those selections. However, more\ncommonly, you’ll automate this filtering by inferring relevant metadata from\nthe user’s question. This technique, known as “self-querying” or “self-\nmetadata querying,” enables your application to automatically generate a\nquery enriched with metadata filters based on the user’s question.\nIn a self-querying flow, the user’s original question is transformed into an\nenriched query with both a metadata filter and a semantic component,\nenabling a combined dense and sparse search. This process, illustrated in\nfigure 10.1, resembles the setup shown in figure 8.\nFigure 10.1 Self-querying workflow: the original question turns into a semantic query with an\nembedded metadata filter. When this enriched query runs on the vector store, it first selects\nchunks matching the metadata filter, then applies semantic search on this refined set.\n",
      "content_length": 1206,
      "extraction_method": "Direct"
    },
    {
      "page_number": 315,
      "chapter": null,
      "content": "1b. The user's question is also included in\nthe chatbot prompt sent to the LLM for\n\nanswer synthesis\nuser question SSeeresssseeeesesescccecssssnnssesssssssssssscsssshssd\nser question\nSelf-que a\n= vain Rewitlen query + metadata ey —_ (\n1a. The user question is PTTTI Titi\nfed to the self-query\n\nUser generator which uses the (7\nLLM to generate a Rewritten query +] Y;\nmetadata enriched query | \"etadata query\n\nLLM\n2. The retriever transforms i\nthe rewritten query into the Retriever |\ncorresponding embedding !\n+ Chatbot —\n8.The retrever searches , Prompt\nfor relevant tet chunks = f !\nusing the rewritten que |\nonteshg and ty . = iinet cease 5 The chatbo creates d\nembeddi i question and the retrieved\nchunks (the context)\n4.The vector DB retums\naan 0 6 The LLM acteps the\n4 prompt and synthesizes a\ncorresponding completion\nVector (the response)",
      "content_length": 846,
      "extraction_method": "OCR"
    },
    {
      "page_number": 316,
      "chapter": null,
      "content": "Now that you understand the basics of self-metadata querying, let’s go\nthrough the steps to implement it. We’ll start with the ingestion phase, where\nyou’ll tag each chunk with relevant metadata keywords. Then, in the Q&A\nphase, I’ll show you two methods for generating self-metadata queries: one\nusing the built-in SelfQueryRetriever and another that uses LLM function\ncalling.\n10.2.1 Ingestion: Metadata Enrichment\nTo use metadata effectively, start by re-importing the UK tourist destination\ndata into a new collection, this time storing metadata for each chunk. Here’s\nhow to set up the environment:\nInitialize the Environment\nOpen a new OS shell, navigate to the chapter 10 code folder, activate the\nvirtual environment, and create a new Jupyter notebook.\nC:\\Github\\building-llm-applications\\ch10>\nC:\\Github\\building-llm-applications\\ch10>python -m venv env_ch10\nC:\\Github\\building-llm-applications\\ch10>pip install notebook lan\nc:\\GitHub\\building-llm-applications\\ch10>env_ch10\\Scripts\\activat\nThen, in Jupyter Notebook, go to File > New > Notebook, and save it as 10-\nquery_generation.ipynb.\nDefine Metadata\nIdentify keywords to tag each chunk, such as:\nsource: URL of the original content\ndestination: the tourist destination referenced\nregion: the UK region of the destination\nManually define mappings for destination and region, and dynamically\ngenerate the source URL for each chunk.\n",
      "content_length": 1395,
      "extraction_method": "Direct"
    },
    {
      "page_number": 317,
      "chapter": null,
      "content": "Set Up Chroma DB Collection\nConfigure the Chroma DB collection with the code shown in the following\nlisting:\nListing 10.1 Setting up the ChromaDB collection\nfrom langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nimport getpass\nOPENAI_API_KEY = getpass.getpass('Enter your OPENAI_API_KEY')\nuk_with_metadata_collection = Chroma(\n   collection_name=\"uk_with_metadata_collection\",\n   embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_\nuk_with_metadata_collection.reset_collection()  #A\nDefine Ingestion Content and Splitting Strategy\nOutline the content and define a text splitting strategy to process the\ndocuments. Listing 10.2 shows how to set this up:\nListing 10.2 Defining content to be ingested and text splitting strategy\nfrom langchain_community.document_loaders import AsyncHtmlLoader\nfrom langchain_community.document_transformers import Html2TextTr\nfrom langchain_text_splitters import RecursiveCharacterTextSplitt\nfrom langchain_core.documents import Document\nhtml2text_transformer = Html2TextTransformer()\ntext_splitter = RecursiveCharacterTextSplitter(  #A\n   chunk_size=1000, chunk_overlap=100\n)\ndef split_docs_into_chunks(docs):\n   text_docs = html2text_transformer.transform_documents(docs)  \n   chunks = text_splitter.split_documents(text_docs)\n   return chunks\n",
      "content_length": 1315,
      "extraction_method": "Direct"
    },
    {
      "page_number": 318,
      "chapter": null,
      "content": "uk_destinations = [\n   (\"Cornwall\", \"Cornwall\"), (\"North_Cornwall\", \"Cornwall\"), \n   (\"South_Cornwall\", \"Cornwall\"), (\"West_Cornwall\", \"Cornwall\")\n   (\"Tintagel\", \"Cornwall\"), (\"Bodmin\", \"Cornwall\"), (\"Wadebridg\n   (\"Penzance\", \"Cornwall\"), (\"Newquay\", \"Cornwall\"), (\"St_Ives\"\n   (\"Port_Isaac\", \"Cornwall\"), (\"Looe\", \"Cornwall\"), (\"Polperro\"\n   (\"Porthleven\", \"Cornwall\"),\n   (\"East_Sussex\", \"East_Sussex\"), (\"Brighton\", \"East_Sussex\"),\n   (\"Battle\", \"East_Sussex\"), (\"Hastings_(England)\", \"East_Susse\n   (\"Rye_(England)\", \"East_Sussex\"), (\"Seaford\", \"East_Sussex\"), \n   (\"Ashdown_Forest\", \"East_Sussex\")\n]\nwikivoyage_root_url = \"https://en.wikivoyage.org/wiki\"\nuk_destination_url_with_metadata = [  #C\n   ( f'{wikivoyage_root_url}/{destination}', destination, region\n   for destination, region in uk_destinations]\nThe next step is to ingest the content and the related metadata.\nIngest Content with Metadata\nEnrich the content with metadata by processing each document chunk as\nshown in listing 10.3.\nListing 10.3 Enriching chunks with related metadata\nfor (url, destination, region) in uk_destination_url_with_metadat\n   html_loader = AsyncHtmlLoader(url)  #A\n   docs =  html_loader.load()  #B\n   \n   docs_with_metadata = [\n       Document(page_content=d.page_content,\n       metadata = {\n           'source': url,\n           'destination': destination,\n           'region': region})\n       for d in docs]\n            \n   chunks = split_docs_into_chunks(docs_with_metadata)\n   print(f'Importing: {destination}')\n   uk_with_metadata_collection.add_documents(documents=chunks)\n",
      "content_length": 1577,
      "extraction_method": "Direct"
    },
    {
      "page_number": 319,
      "chapter": null,
      "content": "Now your collection is ready, with each document chunk enriched with\nmetadata. You can query this content and apply metadata filters to refine\nsearch results based on keywords like destination, region, or source.\n10.2.2 Q & A on a Metadata-Enriched Collection\nThere are three ways to query metadata-enriched content:\n1. Explicit Metadata Filters: Specify the metadata filter manually.\n2. SelfQueryRetriever: Automatically generate the metadata filter using the\nSelfQueryRetriever.\n3. Structured LLM Function Call: Infer the metadata filter with a structured\ncall to an LLM function.\nLet’s explore these methods, starting with explicit filtering.\nQuerying with an Explicit Metadata Filter\nYou can leverage the metadata attached to each chunk by explicitly adding a\nfilter to the retriever. Here’s an example:\nquestion =  \"Events or festivals\"\nmetadata_retriever = uk_with_metadata_collection.as_retriever(sea\nresult_docs = metadata_retriever.invoke(question)\nWhen you print result_docs, you’ll see that only chunks tagged with\n`destination: Newquay` are returned, confirming that the filter is working\ncorrectly:\n[Document(metadata={'destination': 'Newquay', 'region': 'Cornwall\nDocument(metadata={'destination': 'Newquay', 'region': 'Cornwall\nTo adjust the filter, instantiate a new retriever with the updated parameters.\nAutomatically Generating Metadata Filters with SelfQueryRetriever\nYou can also generate metadata filters automatically with\n",
      "content_length": 1446,
      "extraction_method": "Direct"
    },
    {
      "page_number": 320,
      "chapter": null,
      "content": "SelfQueryRetriever. This tool interprets the user’s question to infer the\nappropriate filter. First, import the necessary libraries, as shown in the\nfollowing listing:\nListing 10.4 Setting up metadata field information\nfrom langchain.chains.query_constructor.base import AttributeInfo\nfrom langchain.retrievers.self_query.base import SelfQueryRetriev\nfrom langchain_openai import ChatOpenAI\nNext, define the metadata attributes to infer from the question:\nmetadata_field_info = [\n   AttributeInfo(\n       name=\"destination\",\n       description=\"The specific UK destination to be searched\",\n       type=\"string\",\n   ),\n   AttributeInfo(\n       name=\"region\",\n       description=\"The name of the UK region to be searched\",\n       type=\"string\",\n   )\n]\nNow, set up the SelfQueryRetriever with the question, without specifying a\nmanual filter:\nquestion = \"Tell me about events or festivals in the UK town of N\nllm = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=OPENAI_API_K\nself_query_retriever = SelfQueryRetriever.from_llm(\n   llm, uk_with_metadata_collection, question, metadata_field_in\n)\nInvoke the retriever with the question:\nresult_docs = self_query_retriever.invoke(question)\nPrinting result_docs will confirm that only chunks related to Newquay are\nretrieved, matching the inferred filter:\n",
      "content_length": 1298,
      "extraction_method": "Direct"
    },
    {
      "page_number": 321,
      "chapter": null,
      "content": "[Document(metadata={'destination': 'Newquay', 'region': 'Cornwall\nDocument(metadata={'destination': 'Newquay', 'region': 'Cornwall\nDocument(metadata={'destination': 'Newquay', 'region': 'Cornwall\nflexibility.\nGenerating Metadata Filters with an LLM Function Call\nYou can also infer metadata filters by having the LLM map the question to a\npredefined metadata template with attributes you stored during ingestion.\nThis approach offers greater flexibility than the SelfQueryRetriever but\nrequires more setup.\nFirst, import the libraries necessary to create a structured query with specific\nfilters:\nListing 10.5 Importing libraries\nimport datetime\nfrom typing import Literal, Optional, Tuple, List\nfrom pydantic import BaseModel, Field\nfrom langchain.chains.query_constructor.ir import (\n   Comparator,\n   Comparison,\n   Operation,\n   Operator,\n   StructuredQuery,\n)\nfrom langchain.retrievers.self_query.chroma import ChromaTranslat\nThe DestinationSearch class translates the user question into a structured\nobject with a content_search field containing the question (minus filtering\ndetails) and fields for inferred search filters. Listing 10.6 shows this setup:\nListing 10.6 Strongly-typed structured question including inferred filter attributes\nclass DestinationSearch(BaseModel):\n   \"\"\"Search over a vector database of tourist destinations.\"\"\"\n   content_search: str = Field(\n       \"\",\n",
      "content_length": 1390,
      "extraction_method": "Direct"
    },
    {
      "page_number": 322,
      "chapter": null,
      "content": "       description=\"Similarity search query applied to tourist d\n   )\n   destination: str = Field(\n       ...,\n       description=\"The specific UK destination to be searched.\"\n   )\n   region: str = Field(\n       ...,\n       description=\"The name of the UK region to be searched.\",\n   )\n   def pretty_print(self) -> None:\n       for field in self.__fields__:\n           if getattr(self, field) is not None and getattr(self, \n               self.__fields__[field], \"default\", None\n           ):\n               print(f\"{field}: {getattr(self, field)}\")\nBuild a Chroma DB Filter Statement from the Structured Query\nNext, create a function to convert a `DestinationSearch` object into a filter\ncompatible with Chroma DB, as shown in listing 10.7:\nListing 10.7 Building a ChromaDB specific filter statement from a structured query object\ndef build_filter(destination_search: DestinationSearch):\n   comparisons = []\n   destination = destination_search.destination  #A\n   region = destination_search.region  #A\n   \n   if destination and destination != '':  #B\n       comparisons.append(\n           Comparison(\n               comparator=Comparator.EQ,\n               attribute=\"destination\",\n               value=destination,\n           )\n       )\n   if region and region != '':  #C\n       comparisons.append(\n           Comparison(\n               comparator=Comparator.EQ,\n               attribute=\"region\",\n               value=region,\n",
      "content_length": 1429,
      "extraction_method": "Direct"
    },
    {
      "page_number": 323,
      "chapter": null,
      "content": "           )\n       )    \n   search_filter = Operation(operator=Operator.AND, arguments=co\n   chroma_filter = ChromaTranslator().visit_operation(search_fil\n       \n   return chroma_filter\nBuild a Query Chain to Convert the Question into a Structured Query\nNow, define the query generator chain to convert the user question into a\nstructured query with metadata filters:\nListing 10.8 Query generator chain\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nsystem_message = \"\"\"You are an expert at converting user question\nYou have access to a database of tourist destinations. \\\nGiven a question, return a database query optimized to retrieve t\nIf there are acronyms or words you are not familiar with, do not \nprompt = ChatPromptTemplate.from_messages(\n   [\n       (\"system\", system_message),\n       (\"human\", \"{question}\"),\n   ]\n)\nllm = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=OPENAI_API_K\nstructured_llm = llm.with_structured_output(DestinationSearch, me\nquery_generator = prompt | structured_llm\nLet’s try out the chain with the same question used earlier:\nquestion = \"Tell me about events or festivals in the UK town of N\nstructured_query =query_generator.invoke(question)\nPrinting structured_query shows the question converted into a structured\nobject:\nDestinationSearch(content_search='events festivals', destination=\n",
      "content_length": 1375,
      "extraction_method": "Direct"
    },
    {
      "page_number": 324,
      "chapter": null,
      "content": "With the structured query created, generate a Chroma DB-compatible search\nfilter:\nsearch_filter = build_filter(structured_query)\nThe search_filter result will look like this:\n{'$and': [{'destination': {'$eq': 'Newquay'}},\n {'region': {'$eq': 'Cornwall'}}]}\nPerform the vector search using the generated structured query and Chroma\nDB filter:\nsearch_query = structured_query.content_search\nmetadata_retriever = uk_with_metadata_collection.as_retriever(sea\nanswer = metadata_retriever.invoke(search_query)\nThe answer should closely match the output from the SelfQueryRetriever,\nreturning chunks associated with Newquay:\n[Document(metadata={'destination': 'Newquay', 'region': 'Cornwall\nSo far, you’ve focused on generating metadata-enriched queries for vector\nstores. In the next section, you’ll learn how to generate SQL queries from\nnatural language questions, enabling retrieval of structured data from\nrelational databases.\n10.3 Generating a Structured SQL Query\nMany LLMs can transform user questions into SQL queries, enabling access\nto relational databases directly from LLM applications. While LLMs are\ncontinually improving in generating accurate SQL, challenges remain,\nespecially when working with complex schemas or specific database\nstructures. LangChain enhances these capabilities with evolving Text-to-SQL\nfeatures, but there are some common issues you should consider.\nA helpful reference here is the paper \"Evaluating the Text-to-SQL\nCapabilities of Large Language Models\" by Nitarshan Rajkumar et al.\n",
      "content_length": 1518,
      "extraction_method": "Direct"
    },
    {
      "page_number": 325,
      "chapter": null,
      "content": "(https://arxiv.org/pdf/2204.00498.pdf). Though dated, this paper offers\npractical insights into common pitfalls and solutions. The main finding was\nthat \"hallucinations\"—incorrect table and column names—can often be\nreduced by using few-shot prompts that include the schema and sample\nrecords for the target table. An example schema from the paper looks like\nthis:\nCREATE TABLE \"state\" (\n   \"state_name\" TEXT,\n   \"population\" INT DEFAULT NULL,\n   \"area\" DOUBLE DEFAULT NULL,\n   \"country_name\" VARCHAR(3) NOT NULL DEFAULT '',\n   \"capital\" TEXT,\n   \"density\" DOUBLE DEFAULT NULL\n);\n/* example rows\nstate_name     population     area      country_name     capital  \nalabama        3894000        51700.0   usa              montgome\nalaska         401800         591000.0  usa              juneau   \narizona        2718000        114000.0  usa              phoenix  \n*/\n-- Answer the following question using the above table schema:\n-- {user_question}\nUsing the CREATE TABLE command along with sample data helps the LLM\nbetter understand the structure and constraints, minimizing incorrect column\nand table references.\n10.3.1 Installing SQLite\nSQLite does not require full installation. Unzip the package, place it in a\nfolder, and add the folder to your system’s Path environment variable. Refer\nto Appendix D for setup instructions on Windows. For other operating\nsystems, consult the SQLite documentation.\n10.3.2 Setting Up and Connecting to the Database\nLet’s create a booking database called UkBooking to store UK destinations,\naccommodations, and special offers.\nHere is the relational diagram for the UKBooking database. Each table shows\n",
      "content_length": 1641,
      "extraction_method": "Direct"
    },
    {
      "page_number": 326,
      "chapter": null,
      "content": "its primary key (PK) and foreign key (FK) columns, with relationships\nmarked by arrows connecting related tables. This setup visually represents\nthe structure and relationships within the database.\nFigure 10.2 Entity-Relationship diagram of the UkBooking DB\n",
      "content_length": 258,
      "extraction_method": "Direct"
    },
    {
      "page_number": 327,
      "chapter": null,
      "content": "Destination\n\n+Offerld\n+Description\n+DiscountRate\n+StartDate\n+EndDate\n+Accommodationid\n\nUK Booking DB Entity-Relationships\n\nAccommodation\n\n+Accommodationld\n+Name\n+Description\n+Address\n\n4 +Destinationld\n+AccommodationTypeld\n\n+CheckinDate\n+CheckoutDate\n+Accommodationid\n+Customerld\n\nAccommodation Type\n\n+AccommodationTypeld\n+TypeName\n\n+CustomerlD\n+FirstName\n+LastNAme\n\n+Email\n+Phone\n+Address",
      "content_length": 388,
      "extraction_method": "OCR"
    },
    {
      "page_number": 328,
      "chapter": null,
      "content": "The database schema, shown in figure 10.2, includes the relationships\nbetween tables for destinations, accommodations, and offers.\nOpen your OS shell, navigate to the code folder, and enter the following\ncommand to create the UkBooking database:\nC:\\Github\\building-llm-applications\\ch10>sqlite3 UkBooking.db\nThis opens the SQLite terminal:\nSQLite version 3.46.1 2024-08-13 09:16:08 (UTF-16 console I/O)\nEnter \".help\" for usage hints.\nsqlite>\nIn the SQLite terminal, load the SQL scripts to create and populate the\nUkBooking database. Ensure these files are in C:\\Github\\building-llm-\napplications\\ch10. Download them from GitHub if necessary:\nsqlite> .read CreateUkBooking.sql\nsqlite> .read PopulateUkBooking.sql\nTo confirm the setup, check for records in the Offer table:\nsqlite> SELECT * FROM Offer;\nYou should see output similar to this:\n1|1|Summer Special|0.15|2024-06-01|2024-08-31\n2|2|Weekend Getaway|0.1|2024-09-01|2024-12-31\n3|3|Early Bird Discount|0.2|2024-05-01|2024-06-30\n4|4|Stay 3 Nights, Get 1 Free|0.25|2024-01-01|2024-03-31\n… \nNow the UkBooking database is ready for use with LangChain.\nReturn to the Jupyter notebook and import the libraries needed for SQL\ndatabase connections:\nfrom langchain_community.utilities import SQLDatabase\nfrom langchain_community.tools import QuerySQLDataBaseTool\nfrom langchain.chains import create_sql_query_chain\n",
      "content_length": 1361,
      "extraction_method": "Direct"
    },
    {
      "page_number": 329,
      "chapter": null,
      "content": "from langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nimport getpass\nimport os\nUse the following code to connect to the database and list available tables:\ndb = SQLDatabase.from_uri(\"sqlite:///UkBooking.db\")\nprint(db.get_usable_table_names())\nYou should see a list of table names:\n['Accommodation', 'AccommodationType', 'Booking', 'Customer', 'De\nRun a sample query to verify the connection:\ndb.run(\"SELECT * FROM Offer;\")\nThe output should display entries from the Offer table:\n\"[(1, 1, 'Summer Special', 0.15, '2024-06-01', '2024-08-31'), (2, \nNow, you’re set up to query the UkBooking database programmatically using\nLangChain.\n10.3.3 Generating SQL Queries from Natural Language\nNow that the setup is complete, you can start generating SQL queries directly\nfrom natural language questions. Here’s how to test a simple query:\nllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4o-min\nsql_query_gen_chain = create_sql_query_chain(llm, db)\nresponse = sql_query_gen_chain.invoke({\"question\": \"Give me some \nPrinting response will show the generated SQL query:\n'```sql\\nSELECT \"Offer\".\"OfferDescription\", \"Offer\".\"DiscountRate\nHowever, if you attempt to execute this SQL directly against the database,\nyou’ll encounter an error due to the backticks (```), which are non-SQL\ncharacters:\n",
      "content_length": 1391,
      "extraction_method": "Direct"
    },
    {
      "page_number": 330,
      "chapter": null,
      "content": "db.run(response)\n'Error: (sqlite3.OperationalError) near \"```sql\\nSELECT \"Offer\".\"\nTo clean up the SQL formatting, you can use the LLM to strip unnecessary\ncharacters and output a properly formatted SQL statement. Here’s a simple\nchain setup for this:\nListing 10.9 Chain to fix the formatting of the generated SQL\nclean_sql_prompt_template = \"\"\"You are an expert in SQL Lite. You \nwhich might contain unneded prefixes or suffixes. Given the follow\ntransform it to a clean, executable SQL statement for SQL lite.\nOnly return an executable SQL statement which terminates with a s\nDo not include the language name or symbols like ```.\nUnclean SQL: {unclean_sql}\"\"\"\nclean_sql_prompt = ChatPromptTemplate.from_template(clean_sql_pro\nclean_sql_chain = clean_sql_prompt | llm\nfull_sql_gen_chain = sql_query_gen_chain | clean_sql_chain | StrO\nLet’s try out this full chain with a sample question and verify the output:\nquestion = \"Give me some offers for Cardiff, including the accomo\nresponse = full_sql_gen_chain.invoke({\"question\": question})\nprint(response)\nThe output should be a clean SQL statement:\n\"SELECT Offer.OfferDescription, Offer.DiscountRate, Accommodation\nThis approach ensures the SQL statement is correctly formatted and ready to\nexecute against the database.\n10.3.4 Executing the SQL Query\nNow, let’s create a chain to generate and execute SQL queries.\n",
      "content_length": 1364,
      "extraction_method": "Direct"
    },
    {
      "page_number": 331,
      "chapter": null,
      "content": "sql_query_exec_chain = QuerySQLDataBaseTool(db=db)\nsql_query_gen_and_exec_chain = full_sql_gen_chain | sql_query_exe\nresponse = sql_query_gen_and_exec_chain.invoke({\"question\":questi\nPrinting response should show the following output:\n\"[('Early Bird Discount', 0.2, 'Cardiff Camping')]\"\nThis setup allows you to retrieve data from a relational database by using a\ncombined chain (sql_query_gen_and_exec_chain) that handles both SQL\ngeneration and execution. You can easily integrate this chain within a broader\nRAG (Retrieval-Augmented Generation) setup, as discussed in earlier\nsections. Try extending this integration as an exercise.\nFigure 10.3 RAG with SQL Workflow: The LLM converts the natural language question into a\nSQL query, which is executed on the SQL database. The database returns records that are then\nprocessed by the LLM to generate the final answer.\n",
      "content_length": 869,
      "extraction_method": "Direct"
    },
    {
      "page_number": 332,
      "chapter": null,
      "content": "User\n\nSSPE S ESSE ESSE ESSE EERSTE SEER EE EEE\n\nOriginal question 0.Q\n\n|\n|\n\ngenerate SQL from CREATE + examples |\n$$\n\nReturn SQL|SELECT ... FROM... WHERE},\n\nReturn\nSQL records:\n\nQuestion: Original question 0.Q\n\nContext: SQL records",
      "content_length": 231,
      "extraction_method": "OCR"
    },
    {
      "page_number": 333,
      "chapter": null,
      "content": "The sequence diagram in figure 10.3 can give you a visual idea of what the\nfull RAG with SQL workflow would look like.\nTIP\nLangChain’s SQLDatabaseChain class provides a streamlined way to\ngenerate SQL queries directly from user questions. This tool uses an LLM\nand your database connection to automatically create “few-shot” prompts,\nsimilar to those recommended in the Rajkumar paper. Experimenting with\nSQLDatabaseChain can be highly beneficial if you plan to incorporate\nrelational databases into your RAG setup. A sequence diagram (Figure 10.3)\nshows a high-level overview of this complete RAG workflow.\n10.4 Generating a Semantic SQL Query\nIn the previous section, you learned how to generate SQL queries from\nnatural language. However, these queries rely on “strict” SQL, meaning they\ndepend on exact matching and traditional relational operations. Relational\ndatabases operate on record sets using operations like SELECT, JOIN, WHERE,\nand GROUP BY, where filters are based on exact string matches or numeric\ncomparisons.\nBut what if you want to expand the SQL search to include results that are\nsimilar in meaning to what the user intended? This requires a shift from\nstandard SQL to a semantic SQL search. In this section, I’ll provide an\noverview of how to implement semantic SQL search, a topic that continues to\nevolve.\n10.4.1 Standard SQL Query\nA standard SQL query filters based on exact matches. For example, to find\nusers with the first name “Roberto,” you would use:\nSELECT first_name, last_name FROM user WHERE first_name = ‘Robert\nThis query returns only users named “Roberto.” It won’t return records for\n",
      "content_length": 1624,
      "extraction_method": "Direct"
    },
    {
      "page_number": 334,
      "chapter": null,
      "content": "names like Robert, Rob, Robbie, Roby, Robin, Roe, Bobby, Bob, or Bert.\nYou can loosen this search slightly with the LIKE operator for partial\nmatching. For instance, to find users with names that start with “Rob”:\nSELECT first_name, last_name FROM user WHERE first_name LIKE ‘Rob%\nThis query will return names like Roberto, Robert, Rob, Robbie, Roby, and\nRobin but still won’t catch variations like Roe, Bobby, Bob, or Bert, as they\ndon’t contain the string “Rob.”\n10.4.2 Semantic SQL Query\nWith the rise of large language models (LLMs), several relational databases\nnow support semantic search, which enables searches based on embeddings\ninstead of exact matches. An example is PGVector, an extension for\nPostgreSQL that allows vector-based similarity searches using metrics such\nas L2, inner product, and cosine distance. This approach enables you to\nperform searches that return results based on meaning rather than exact text\nmatches.\nIn this section, I’ll refer to this approach as semantic SQL search or SQL\nsimilarity search interchangeably.\n10.4.3 Creating the Embeddings\nTo extend the “traditional SQL” approach with PGVector’s similarity search,\nyou’ll need to add vector-based embeddings for any columns you want to use\nin semantic searches. Here’s how to do it.\n1. Add a Vector Column: First, add a VECTOR type column to the table for\neach field you want to search by similarity. For example, to enable\nsimilarity search on first_name, add a column called\nfirst_name_embedding:\nALTER TABLE user  ADD COLUMN first_name_embedding VECTOR\n2. Calculate Embeddings: Next, compute the embedding values for each\n",
      "content_length": 1616,
      "extraction_method": "Direct"
    },
    {
      "page_number": 335,
      "chapter": null,
      "content": "first_name. You can do this directly within PostgreSQL if you have a\nfunction to generate embeddings, or you can compute embeddings\nexternally using an API client like LangChain.\nIn-Database Calculation: If PostgreSQL has a custom function like\ncalculate_my_embedding() available, you can update the\nembeddings in-place with SQL:\nUPDATE user\nSET first_name_embedding = calculate_my_embedding(first_name)\nExternal Calculation with LangChain: If you’re using a pre-built\nembedding function (e.g., OpenAI’s), calculate embeddings\nexternally and store them using the PGVector API. In listing 10.10\nyou can see an example of using LangChain’s OpenAIEmbeddings\nwrapper to generate embeddings for first_name values and update\nthe database (library imports are omitted for brevity).\nListing 10.10 Using LangChain’s OpenAIEmbeddings wrapper to generate embeddings\ndb = SQLDatabase.from_uri(YOUR_DB_CONNECTION_STRING)  #A\nembeddings_model = OpenAIEmbeddings()  #A\nfirst_names_resultset_str = db.run('SELECT first_name FROM user')\nfirst_names = [fn[0] for fn in eval(first_names_resultset_str)]  \nfirst_names_embeddings = embeddings_model.embed_documents(first_n\nfn_emb = zip(first_names, first_names_embeddings)  #D\nfor fn, emb in fn_emb:\n   sql = f'UPDATE user SET first_name_embeddings = ARRAY{emb} WH\n   db.run(sql)\nBy following these steps, you’ll enable semantic search on first_name or\nother fields, allowing PGVector to retrieve records based on similarity, rather\nthan exact matches.\n10.4.4 Performing a Semantic SQL Search\nAfter setting up the embeddings, you can perform a similarity search as\nfollows:\n",
      "content_length": 1603,
      "extraction_method": "Direct"
    },
    {
      "page_number": 336,
      "chapter": null,
      "content": "embedded_query= embeddings_model.embed_query(\"Roberto\")\nquery = (\n   'SELECT first_name FROM user WHERE first_name_embeddings IS N\n)\ndb.run(query)\nThis query returns variations of “Roberto,” including Roe, Bobby, Bob, and\nBert, by ordering results based on similarity.\n10.4.5 Automating Semantic SQL Search\nNow that you understand how to generate embeddings and perform similarity\nsearches in a SQL database, the final step is to create a prompt that can\nautomatically generate SQL similarity queries. This process is similar to what\nwe covered for generating traditional SQL queries. Once you design,\nimplement, and test this prompt—and integrate it into a full chain within\nLCEL—your LLM application will be capable of generating semantic\nsearches on PGVector or any SQL database that supports ARRAY (or\nsimilar) data types, seamlessly feeding the results to the LLM for synthesis.\n10.4.6 Benefits of Semantic SQL Search\nThe simple example here only scratches the surface of semantic SQL’s\ncapabilities. You can combine semantic filtering with exact matching or use\nmultiple semantic filters, especially powerful in multi-table queries using\njoins. This approach allows highly nuanced searches, especially when\ncombined with traditional SQL filtering.\nLater, I’ll show you how to combine metadata and semantic filtering in a\nvector store, which can achieve similar results. However, using multiple\nsemantic filters in SQL offers greater flexibility, particularly for complex\nqueries.\n10.5 Generating Queries for a Graph Database\nGraph databases are designed to store, navigate, and query data in a graph\nformat, with nodes representing entities and edges defining relationships, as\nshown in Figure 10.4. They are well-suited for building knowledge graphs,\n",
      "content_length": 1758,
      "extraction_method": "Direct"
    },
    {
      "page_number": 337,
      "chapter": null,
      "content": "making them ideal for specialized domains that require advanced reasoning,\ninference, and explainability.\nFigure 10.4 Graph Representation of data: Nodes represent entities such as \"Roberto\" and\n\"InterMilan,\" while edges like `hasOccupation` and `isFanOf` depict their relationships.\nUnlike relational databases, graph databases do not follow a universal\nstandard. Some use the Resource Description Framework (RDF) to represent\ndata as “triples”—a Subject-Predicate-Object format. For instance, in RDF:\n(Roberto, hasOccupation, SoftwareDeveloper)\n(Roberto, isFanOf, InterMilan)\n(InterMilan, playsIn, SerieA)\nThis triple structure might look like:\n@prefix ex: <http://example.org/> .\n@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\nex:Roberto rdf:type ex:Person .\nex:Roberto ex:hasOccupation ex:SoftwareDevelopment .\nex:Roberto ex:isFanOf ex:InterMilan .\n",
      "content_length": 867,
      "extraction_method": "Direct"
    },
    {
      "page_number": 338,
      "chapter": null,
      "content": "ex:InterMilan rdf:type ex:SoccerTeam .\nex:InterMilan ex:playsIn ex:SerieA .\nex:SerieA rdf:type ex:SoccerLeague .\nHowever, only some graph DBs use RDF. Others use proprietary graph\nrepresentations and query languages like Cypher (for Neo4j) or Gremlin,\nrather than RDF and SPARQL.\nAs you can see in the figure 10.4, the graph data structure enables powerful,\nflexible representations of relationships that are difficult to capture with\ntraditional databases.\nThis versatility makes graph DBs ideal for knowledge-intensive applications\nwhere deep understanding of relationships and reasoning is essential.\nGraph databases have been around since the early 2000s, with Neo4j among\nthe first. While they offer powerful, flexible ways to represent and query\ninformation, their complexity can be a hurdle. Recently, large language\nmodels (LLMs) have made graph DBs more accessible by enhancing several\nkey functions:\nEntity and Relationship Extraction: LLMs can pull entities,\nrelationships, and even full graphs from unstructured text, allowing you\nto store this data directly in a graph DB.\nAutomated Query Generation: LLMs can generate complex Cypher or\nSPARQL queries from natural language questions, easing a task\ntraditionally challenging for less experienced developers. To accomplish\nthis, use a carefully crafted few-shot prompt with examples, which\nworks best with a high-accuracy LLM like GPT-4o.\nNatural Language Answers: LLMs can convert query results (e.g., in\nRDF) into natural language responses. This process involves feeding the\ninitial question, the generated Cypher or SPARQL query, and the results\ninto a dedicated prompt, which a lower-cost LLM like GPT-4o-mini can\nhandle effectively.\nFigure 10.5 below shows the resulting Knowledge Graph RAG architecture,\nalso known as KG-RAG, which closely resembles the setup used for vector\n",
      "content_length": 1845,
      "extraction_method": "Direct"
    },
    {
      "page_number": 339,
      "chapter": null,
      "content": "store based RAG.\nFigure 10.5 Knowledge Graph RAG Architecture (KG-RAG): Similar to vector-store-based RAG\nsetups, but using a SPARQL generator. The generator converts a natural language question into\nSPARQL, which is executed on the Knowledge Graph database. The retrieved graph data is then\nprovided to the LLM, along with the original question, to synthesize the answer.\n",
      "content_length": 373,
      "extraction_method": "Direct"
    },
    {
      "page_number": 340,
      "chapter": null,
      "content": "1b. The user question is also fed to the\nprompt sent to the LLM to synthesize the\nanswer\n\ner i laser\n\n§PARQL\ngenerator\n\nuser question\n\n1a, The user question is\n\nfed to. a SPARQL\n\nUser generator which uses an\nLLM to generate a SPARQL\nknowledge graph query sag\n\n2. The retriever executes\nthe SPARQL query on |\nthe Knowledge Graph QL trtsesetsesesenes '  4,The chatbot creates a\nDB prompt with the user\nquestion and the retrieved\ngraphs (the context)\n\n3. The Knowledge Graph\n\nDB retuns relevant 9. The LLM accepts the\n\nbranches of the graph prompt and synthetizes a\ncorresponding\ncompletion (the\nresponse)",
      "content_length": 602,
      "extraction_method": "OCR"
    },
    {
      "page_number": 341,
      "chapter": null,
      "content": "LangChain supports several graph DBs, including Neo4j and Amazon\nNeptune. While this book does not cover Knowledge Graph RAG in detail, I\nrecommend consulting LangChain’s documentation and examples.\nLangChain also offers practical blog posts, such as a collaborative piece with\nNeo4j on building DevOps RAG applications with knowledge graphs:\nhttps://blog.langchain.dev/using-a-knowledge-graph-to-implement-a-devops-\nrag-application/.\nBelow is a sample prompt template for generating Cypher queries, taken\nfrom LangChain’s Neo4j QA Chain:\nCYPHER_GENERATION_TEMPLATE = \"\"\"Task:Generate Cypher statement to \nInstructions:\nUse only the provided relationship types and properties in the sc\nDo not use any other relationship types or properties that are no\nSchema:\n{schema}\nNote: Do not include any explanations or apologies in your respon\nDo not respond to any questions that might ask anything else than \nDo not include any text except the generated Cypher statement.\nExamples: Here are a few examples of generated Cypher statements \n# How many people played in Top Gun?\nMATCH (m:Movie {{title:\"Top Gun\"}})<-[:ACTED_IN]-()\nRETURN count(*) AS numberOfActors\nThe question is:\n{question}\"\"\"\nGraph DBs are evolving to meet new LLM-driven use cases, giving rise to\nKnowledge Graph Embeddings. This approach enriches knowledge graphs\nwith textual descriptions and embeddings, supporting semantic search as a\ncomplement to traditional graph queries. For further reading on this\ntechnique, see this Nature article: https://www.nature.com/articles/s41598-\n023-38857-5. For a comprehensive guide, I recommend Knowledge Graph\nEnhanced RAG by Tomaž Bratanič and Oskar Hane.\nThese tools allow you to leverage the structured knowledge of graph DBs,\ncombined with the adaptability of LLMs, for powerful retrieval-augmented\ngeneration solutions.\n10.6 Chain Routing\n",
      "content_length": 1846,
      "extraction_method": "Direct"
    },
    {
      "page_number": 342,
      "chapter": null,
      "content": "An application’s content might reside in multiple storage types, not just\nvector stores for unstructured text. You may also use relational databases for\nstructured data, document databases for semi-structured content, and\nknowledge graph databases for entity relationships. Additionally, the\napplication might need to connect to different LLMs depending on the task,\nas some LLMs are optimized or more cost-effective for specific functions. As\na result, the RAG architecture often branches into a tree structure, with each\nbranch tailored to specific types of queries or tasks, as shown in figure 10.6.\nFigure 10.6 Complex RAG architecture with branching pathways, each optimized for specific\ntasks, such as answering questions about tourist Destinations (from a vector store) or\nAccommodation offers (from a relational SQL DB).\n",
      "content_length": 829,
      "extraction_method": "Direct"
    },
    {
      "page_number": 343,
      "chapter": null,
      "content": "1b. The user question is also fed to the\nprompt sent to the LLM to synthetize the\nanswer\n\nuser question S050 cee seeeseseseescescsssssnsucssessesssssecccece}acd\nser question\nQuestion\nrouter a (\n1a. The user question is\nfedtoa question router, scbsccessccsbossscessaszazecs al mada (ue ssscdeosseocssed >\nUser\nrelevant retriever ! ‘cer nae\n2.The selected question\nhandler generates the en SOL Generator\nfelated wey\nPrompt\n3.The chosen retriever\nwl ae Vector store 4.The chosen'retriever\nteioiarl Wiereaiia retriever feeds context to the prompt\nsent to the LLM to\nsynthetize the answer\n\n9. The LLM accepts the\n\nprompt and synthetizes a\ncorresponding\nrelevant completion (the\ncontext response)\n\nSen tk",
      "content_length": 698,
      "extraction_method": "OCR"
    },
    {
      "page_number": 344,
      "chapter": null,
      "content": "This figure illustrates a RAG setup with several branches, each designed to\nhandle different application tasks. For example, one branch could address\nfactual questions about tourist destinations, while another branch handles\nquestions about available accommodation offers.\nSuppose a user asks about a tourist destination. In that case, you’d likely\nroute this query to a RAG chain based on a vector store. If the user’s question\nis about accommodation offers, you might route it to a RAG chain connected\nto the UkBooking database introduced earlier.\nTo route each question to the correct chain, you use a routing chain. This\nchain analyzes the question to determine the best-suited chain for handling it.\nLet’s go over the implementation of a routing chain for this purpose.\n10.6.1 Setting Up Data Retrievers\nTo streamline setup, we’ll reuse the vector store and relational database\nconfigurations from previous sections. Import the necessary libraries:\nfrom typing import Literal\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom pydantic import BaseModel, Field\nfrom langchain.schema.runnable import RunnableLambda\nNow create the corresponding retriever chains:\ntourist_info_retriever_chain = RunnableLambda(lambda x: x['questi\nuk_accommodation_retriever_chain =  full_sql_gen_chain | sql_quer\nThese retriever chains direct questions to the appropriate data source. Next,\nwe’ll build a router to direct user questions to one of these retriever chains.\n10.6.2 Setting Up the Query Router\nWe’ll implement a question router using an LLM. The LLM will analyze\neach question and determine the best retriever chain based on its content. The\n",
      "content_length": 1686,
      "extraction_method": "Direct"
    },
    {
      "page_number": 345,
      "chapter": null,
      "content": "prompt will specify the function of each retriever: the vector store for general\ntourist information and the relational database for accommodation bookings.\nThe router function, defined in listing 10.11, binds the LLM’s response to a\ntyped object, instantiating the datasource attribute with either\n\"tourist_info_store\" or \"uk_booking_db\" depending on the question’s\nintent.\nListing 10.11 Routing the query to the correct retriever\nclass RouteQuery(BaseModel):\n   \"\"\"Route a user question to the most relevant datasource.\"\"\"\n   datasource: Literal[\"tourist_info_store\", \"uk_booking_db\"] = \n       ...,\n       description=\"Given a user question, route it either to a \n   )\nllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4o-min\nstructured_llm_router = llm.with_structured_output(RouteQuery)  #A\nsystem = \"\"\"You are an expert at routing a user question to a tou\nor to an UK accommodation booking relational database.\nThe vector store contains tourist information about UK destinatio\nUse the vectorstore for general tourist information questions on \nFor questions about accommodation availability or booking, use th\nroute_prompt = ChatPromptTemplate.from_messages(\n   [\n       (\"system\", system),\n       (\"human\", \"{question}\"),\n   ]\n)\nquestion_router = route_prompt | structured_llm_router\nThis setup enables the LLM to intelligently route each question to the\nappropriate data source, improving response accuracy based on question\nintent.\nTesting the Router Chain\nLet’s test the router chain with a question about tourist information and\nanother about accommodation booking:\n",
      "content_length": 1585,
      "extraction_method": "Direct"
    },
    {
      "page_number": 346,
      "chapter": null,
      "content": "selected_data_source = question_router.invoke(\n   {\"question\": \"Have you got any offers in Brighton?\"}\n)\nprint(selected_data_source)\nExpected output:\ndatasource='uk_booking_db'\nThen test with a tourist-related question:\nselected_data_source = question_router.invoke(\n   {\"question\": \"Where are the best beaches in Cornwall?\"}\n)\nprint(selected_data_source)\nExpected output:\ndatasource='tourist_info_store'\nThe router correctly identifies the appropriate data source!\nSetting Up the Retriever Chooser\nNow, let’s implement a function to select the correct retriever based on the\nchosen data source ('uk_booking_db' or 'tourist_info_store').\nListing 10.12 Retriever Chooser Function\nretriever_chains = {\n   'tourist_info_store': tourist_info_retriever_chain,\n   'uk_booking_db': uk_accommodation_retriever_chain\n}\ndef retriever_chooser(question):\n   selected_data_source = question_router.invoke(\n       {\"question\": question})\n   return retriever_chains[selected_data_source.datasource]\nLet’s test the retriever chooser function with a sample question:\n",
      "content_length": 1050,
      "extraction_method": "Direct"
    },
    {
      "page_number": 347,
      "chapter": null,
      "content": "chosen = retriever_chooser('Tell me about events or festivals in \nprint(chosen)\nExpected output:\nfirst=RunnableLambda(lambda x: x['question']) last=VectorStoreRet\nThe output confirms that the correct retriever chain instance is selected based\non the question's intent. This setup ensures the question is routed to the best-\nmatched data source for accurate results.\n10.6.3 Integrating the Chain Router into a Full RAG Chain\nThe final step is to integrate the chain router into a complete RAG chain,\nenabling the workflow of retriever selection, query execution, and answer\nsynthesis. See listing 10.13 for an example.\nListing 10.13 Full RAG Chain for Routing, Retrieval, and Answer Synthesis\nfrom langchain_core.runnables import RunnablePassthrough\nrag_prompt_template = \"\"\"\nGiven a question and some context, answer the question.\nIf you get a structured context, like a tuple, try to infer the m\ntypically they refer to accommodation offers, and the number is a \nIf you do not know the answer, just say I do not know.\nContext: {context}\nQuestion: {question}\n\"\"\"\nrag_prompt = ChatPromptTemplate.from_template(rag_prompt_template\ndef execute_rag_chain(question, chosen_retriever):\n   full_rag_chain = (\n       {\n           \"context\": {\"question\": RunnablePassthrough()} | chos\n           \"question\": RunnablePassthrough(),  #B\n       }\n       | rag_prompt\n       | llm\n",
      "content_length": 1368,
      "extraction_method": "Direct"
    },
    {
      "page_number": 348,
      "chapter": null,
      "content": "       | StrOutputParser()\n   )\n   return full_rag_chain.invoke(question)\nLet’s test the RAG chain with both an accommodation query and a tourist\ninformation query.\nExample: Asking about Accommodation Offers\nquestion = 'Give me some offers for Cardiff, including the accomm\nchosen_retriever = retriever_chooser(question)\nanswer = execute_rag_chain(question, chosen_retriever)\nExpected output:\nOne offer for Cardiff is the \"Early Bird Discount\" at Cardiff Cam\nExample: Asking about Tourist Information\nquestion_2 = 'Tell me about events or festivals in the UK town of \nchosen_retriever_2 = retriever_chooser(question_2)\nanswer2 = execute_rag_chain(question_2, chosen_retriever_2)\nExpected output:\nIn Newquay, the **Cornish Film Festival** is held annually each N\nIn both cases, the RAG chain correctly routes each question to the\nappropriate retriever, performs the retrieval, and synthesizes a coherent\nanswer by feeding the context and original question to the LLM.\nWe’re nearing the end of our exploration of advanced RAG techniques. One\nmore topic remains: retrieval post-processing, which focuses on refining the\nchunks sent as context to the LLM, further optimizing the relevance and\nclarity of responses.\n",
      "content_length": 1211,
      "extraction_method": "Direct"
    },
    {
      "page_number": 349,
      "chapter": null,
      "content": "10.7 Retrieval Post-Processing\nAfter using the techniques discussed so far to improve the effectiveness and\naccuracy of RAG retrieval, you’ll likely have a list of document chunks (or\nnodes) from the content store. Before passing these to the LLM for answer\nsynthesis, you may want to perform post-processing to filter out less relevant\ncontent, ensuring the LLM produces a concise and accurate response, as\nshown in figure 10.7.\nFigure 10.7 Retrieval Post-processing: Retrieved chunks from the vector store are filtered to\nremove irrelevant content, ensuring only high-quality chunks are sent to the LLM for answering\nthe user’s question.\n",
      "content_length": 640,
      "extraction_method": "Direct"
    },
    {
      "page_number": 350,
      "chapter": null,
      "content": "In the following sections, I’ll introduce some key post-processing techniques.\n10.7.1 Similarity Postprocessor\nA straightforward way to reduce the number of chunks returned by a\nsimilarity retriever (often a vector-based retriever using semantic distance) is\nto apply a cutoff to similarity scores. Chunks below a specific similarity\nscore, or above a certain distance, are discarded.\n",
      "content_length": 385,
      "extraction_method": "Direct"
    },
    {
      "page_number": 351,
      "chapter": null,
      "content": "In LangChain, you can set this similarity threshold before executing the\nsearch by instantiating a “score threshold” similarity retriever from the vector\nstore. Set the search_type to \"similarity_score_threshold\" and specify\nthe threshold in search_kwargs:\nscore_threshold_similartity_retriever = vector_store.as_retriever\n   search_type=\"similarity_score_threshold\", search_kwargs={\"sco\n)\nAfter instantiating this retriever, you can execute the search with its\nget_relevant_documents() method:\ndoc_chunks = score_threshold_similartity_retriever.get_relevant_d\nThis retrieves only documents with similarity scores above the specified\nthreshold, ensuring that only highly relevant content is passed to the LLM.\n10.7.2 Keyword Postprocessors\nAnother post-processing approach is to filter retrieved document chunks by\nkeywords. This allows you to include or exclude chunks based on the\npresence of specific terms.\nWhile LangChain does not provide a built-in keyword post-processor, you\ncan implement one in Python as follows (for demonstration purposes):\nselected_chunks = [c for c in chunks \n            if set(c.split()).intersection(required_keywords)\n           and not set(c.split()).intersection(excluded_keywords\nThis code filters chunks to include only those containing required_keywords\nand excludes any that contain excluded_keywords.\n10.7.3 Time Weighting\nYou might also want to prioritize chunks based on how recently they were\naccessed. To do this, include a last_accessed_at timestamp in each chunk’s\nmetadata and update it with each access:\n[Document(page_content='this is some content of a chunk', metadat\n",
      "content_length": 1619,
      "extraction_method": "Direct"
    },
    {
      "page_number": 352,
      "chapter": null,
      "content": "Once timestamps are in place, you can use a\nTimeWeightedVectorStoreRetriever, which applies a time decay factor to\nrank chunks by both similarity score and recency:\nretriever = TimeWeightedVectorStoreRetriever(\n   vectorstore=vectorstore, \n   decay_rate=1e-10, k=3\n)\nThe TimeWeightedVectorStoreRetriever adjusts similarity scores based on\nrecency:\nadjusted_similarity_score = similarity_score + (1.0 - decay_rate) \nThis adjusted score allows recent, relevant content to rank higher, ensuring\nmore timely responses. Next, we’ll discuss one of the most essential post-\nprocessing techniques.\n10.7.4 RAG Fusion (Reciprocal Rank Fusion)\nIn the previous chapter, we discussed “multiple query generation,” where\nmultiple queries are generated from a user’s question, and a subset of relevant\nresults is selected from the combined results across all queries. Using\nLangChain’s MultiQueryRetriever, we automated this process to select the\nmost relevant answers. However, if you want finer control over result\nranking, consider an approach called Reciprocal Rank Fusion (RRF).\nRRF or Reciprocal Rank Fusion, detailed by Cormack, Clarke, and Buttcher\nin their paper \"Reciprocal Rank Fusion Outperforms Condorcet and\nIndividual Rank Learning Methods\"\n(https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf), reranks\nretrieved documents based on a specific scoring formula:\nrrfscore = 1 / (rank + k)\nwhere:\nrank: the document's current rank based on similarity or relevance.\nk: a smoothing constant to control the weight of existing ranks.\n",
      "content_length": 1532,
      "extraction_method": "Direct"
    },
    {
      "page_number": 353,
      "chapter": null,
      "content": "As each document is processed, its RRF score accumulates across all\ngenerated queries. Once all scores are calculated, documents are reranked by\ntheir cumulative RRF scores, and the top-ranked results are sent to the LLM\nfor answer synthesis.\nYou can get a visual idea of how the overall process work by following\nthrough figure 10.8.\nFigure 10.8 Reciprocal Rank Fusion Workflow: Multiple queries are generated from the initial\nuser question. Each query retrieves a set of results, such as from a vector store. All results are\nthen reranked using RRF scores, with only the top results sent to the LLM for final answer\nsynthesis.\n",
      "content_length": 629,
      "extraction_method": "Direct"
    },
    {
      "page_number": 354,
      "chapter": null,
      "content": "At a high level, implementing RAG fusion involves generating multiple\nqueries, as shown in chapter 9, and ranking the results with the RRF\nalgorithm. This retrieval workflow can then be embedded within a larger\nRAG chain, as demonstrated in previous examples.\nGenerating Multiple Queries\n",
      "content_length": 288,
      "extraction_method": "Direct"
    },
    {
      "page_number": 355,
      "chapter": null,
      "content": "You can generate multiple queries from an initial question using a chain, as\nshown in Chapter 8. For convenience, the setup is repeated here in Listing\n10.14.\nListing 10.14 Multi-Query Generation\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom typing import List\nfrom langchain_core.output_parsers import BaseOutputParser\nfrom pydantic import BaseModel, Field\nmulti_query_gen_prompt_template = \"\"\"\nYou are an AI language model assistant. Your task is to generate \ndifferent versions of the given user question to retrieve relevan\ndatabase. By generating multiple perspectives on the user questio\nthe user overcome some of the limitations of the distance-based s\nProvide these alternative questions separated by newlines.\nOriginal question: {question}\n\"\"\"\nmulti_query_gen_prompt = ChatPromptTemplate.from_template(multi_q\nclass LineListOutputParser(BaseOutputParser[List[str]]):\n   \"\"\"Parse out a question from each output line.\"\"\"\n   def parse(self, text: str) -> List[str]:\n       lines = text.strip().split(\"\\n\")\n       return list(filter(None, lines))  \nquestions_parser = LineListOutputParser()\nllm = ChatOpenAI(model=\"gpt-4o\", openai_api_key=OPENAI_API_KEY)\nmulti_query_gen_chain = multi_query_gen_prompt | llm | questions_\nWith this setup, you can now generate multiple alternative queries from a\nsingle question, helping to capture varied perspectives and nuances that\nimprove document retrieval accuracy.\nNote\nI’ve chosen to use gpt-4o instead of gpt-4o-mini, as it is more likely to\n",
      "content_length": 1503,
      "extraction_method": "Direct"
    },
    {
      "page_number": 356,
      "chapter": null,
      "content": "produce higher-quality queries and generate more accurate, well-synthesized\nresponses\nNow that multiple queries can be generated, the next step is to implement a\nranking mechanism to sort and prioritize the retrieved results.\nReciprocal Rank Fusion Algorithm\nThe core of this workflow is the Reciprocal Rank Fusion (RRF) algorithm,\nwhich assigns scores to documents retrieved by multiple queries. Using the\nRRF formula, each document is scored based on its rank, then reranked by\ntotal RRF score. See Listing 10.15 for implementation details.\nListing 10.15 Reciprocal Rank Fusion (RRF) Algorithm\ndef reciprocal_rank_fusion(results_groups: list[list], k=60):  #A\n   \"\"\" Reciprocal_rank_fusion that takes multiple groups of rank\n       and an optional parameter k used in the Reciprocal Rank F\n   indexed_results = {}  #B\n   \n   for group_id, results_group in enumerate(results_groups):  #C\n       for local_rank, doc in enumerate(results_group):\n           indexed_results[(group_id, local_rank)] = doc\n   \n   fused_scores = {}  #D\n   \n   for key, doc in indexed_results.items():  #E\n       group_id, local_rank = key\n       if key not in fused_scores:\n           fused_scores[key] = 0  #F\n       \n       doc_current_score = fused_scores[key]        \n       fused_scores[key] += 1 / (local_rank + k)  #G\n   reranked_results = [  #H\n       (indexed_results[key], score)\n       for key, score in sorted(fused_scores.items(), key=lambda \n   ]\n   return reranked_results\n",
      "content_length": 1466,
      "extraction_method": "Direct"
    },
    {
      "page_number": 357,
      "chapter": null,
      "content": "With the RRF algorithm in place, let’s create a RAG Fusion retrieval chain,\nas shown below:\nretriever = uk_with_metadata_collection.as_retriever(search_kwarg\ntop_three_results = RunnableLambda(lambda x: x[0:3])  #A\nrag_fusion_retrieval_chain = multi_query_gen_chain | retriever.ma\n       \ndocs = rag_fusion_retrieval_chain.invoke({\"question\": question})  \nlen(docs)\nThe final step is to integrate this RAG Fusion retrieval chain into a larger\nRAG chain for end-to-end question routing, retrieval, and answer synthesis.\nIncorporating RAG Fusion into the RAG Chain\nAs we’ve seen before, integrating a retrieval chain into a broader RAG chain\nis straightforward. For completeness, Listing 10.16 shows how to incorporate\nthe RAG Fusion retrieval chain within a RAG chain.\nListing 10.16 Integrating the RAG Fusion Retrieval Chain into a RAG Chain\nrag_prompt_template = \"\"\"\nGiven a question and some context, answer the question.\nIf you do not know the answer, just say I do not know.\nContext: {context}\nQuestion: {question}\n\"\"\"\nrag_prompt = ChatPromptTemplate.from_template(rag_prompt_template\nrag_chain = (\n   {\n       \"context\": {\"question\": RunnablePassthrough()} | rag_fusi\n       \"question\": RunnablePassthrough(),  #B\n   }\n   | rag_prompt\n   | llm\n   | StrOutputParser()\n)\nNow, let’s test the complete RAG chain with an example question:\n",
      "content_length": 1339,
      "extraction_method": "Direct"
    },
    {
      "page_number": 358,
      "chapter": null,
      "content": "user_question = \"Can you give me some tips for a trip to Brighton\nanswer = rag_chain.invoke(user_question)\nprint(answer)\nExpected output:\nHere are some tips for a trip to Brighton:\n1. **Visit During Festivals**: If you can, plan your visit in May \n2. **Enjoy the Beach**: Brighton boasts a beautiful stretch of sh\n3. **Explore Local Culture**: Brighton has a vibrant cultural sce\n4. **Transportation**: Brighton is well-connected by train, makin\n5. **Seasonal Work Opportunities**: If you're looking for tempora\n6. **Plan for All Budgets**: Whether you're looking for budget op\n7. **Stay Safe**: Like any city, it's important to stay aware of \n8. **Local Council Resources**: Check out the Brighton and Hove C\nEnjoy your trip!\nCongratulations! You’ve completed a comprehensive guide to advanced\nRAG techniques, making you well-equipped to tackle complex RAG tasks.\n10.8 Summary\nUse structured data from SQL and document databases to enhance LLM\nresponses, complementing unstructured data in vector stores.\nStructured queries are often required for these databases, and LLMs can\ngenerate them from natural language inputs.\nVector stores support dense searches with embeddings and sparse\nsearches using keyword-based indexing.\nKeywords for sparse searches can be added via metadata, extracted with\nalgorithms like TF-IDF, or generated by the LLM.\nLLMs can also generate metadata queries directly from user questions.\nMany LLMs can translate user questions into SQL queries, enabling\n",
      "content_length": 1481,
      "extraction_method": "Direct"
    },
    {
      "page_number": 359,
      "chapter": null,
      "content": "direct access to relational databases.\nSQL-based retrievals rely on exact matching, but semantic SQL expands\nsearches to find approximate matches aligned with user intent.\nGraph databases represent data as nodes and relationships, making them\nideal for complex, interconnected information.\nGraph databases excel in LLM use cases requiring advanced reasoning\nand relationship understanding, as they can be used to create knowledge\ngraphs with the help of LLMs or answer questions based on knowledge\ngraphs through KG-RAG.\nContent for LLM applications often spans vector stores, relational\ndatabases, document stores, and knowledge graphs.\nRouting chains ensure queries are directed to the appropriate storage\ntype for optimized retrieval.\n- Reciprocal Rank Fusion (RRF) improves query relevance by reranking\ndocuments from multiple queries using cumulative scores.\nRRF enhances answer quality by feeding the most relevant results to the\nLLM for synthesis.\n",
      "content_length": 955,
      "extraction_method": "Direct"
    },
    {
      "page_number": 360,
      "chapter": null,
      "content": "11 Building Tool-based Agents with\nLangGraph\nThis chapter covers\nBuilding LLM-powered agents using LangGraph\nRegistering and using tools for dynamic agent execution\nDebugging agent execution and tool calls\nSimplifying agents with pre-built LangGraph components\nObserving agent execution with LangSmith\nIn chapter 5, you explored the distinction between agentic workflows and\nagents. You learned that agentic workflows are fundamentally deterministic:\ntheir logic is based on flows with conditional paths that depend on the current\napplication state. These workflows can be elegantly modeled using node-\nbased graphs in LangGraph, and you saw a complete, hands-on example of\nsuch a system.\nAgents, however, operate differently. Rather than following a predetermined\nflow, agents rely on dynamic, context-sensitive decision-making. With the\nhelp of a language model (LLM), an agent chooses which tools to use—and\nin what order—based on the evolving context of the task at hand. These\ndecisions aren’t pre-scripted; instead, they unfold step by step, as the agent\ncontinually evaluates the outputs of previous actions and adapts accordingly.\nIn this chapter, you’ll put these ideas into practice by building a multi-tool\ntravel information agent. We’ll begin simply, implementing an agent that\nprovides destination information using a single tool. From there, we’ll extend\nit into a true multi-tool agent, able to answer questions about both travel\ndestinations and their current weather conditions.\nAs we progress, I’ll introduce you to the core concepts necessary for\nconstructing multi-tool agents, with particular focus on the tool-calling\nprotocol. You’ll first implement this protocol from scratch to grasp every\n",
      "content_length": 1716,
      "extraction_method": "Direct"
    },
    {
      "page_number": 361,
      "chapter": null,
      "content": "detail, then see how LangGraph’s built-in capabilities can streamline and\nsimplify your agent’s architecture.\nThis chapter’s multi-tool agent will serve as the foundation for the more\nadvanced, multi-agent systems you’ll build in the chapters ahead.\nLet’s dive in—there’s a lot to discover.\n11.1 Starting Simple: Building a Single-Tool Travel\nInfo Agent\nIn this section, we'll lay the groundwork for our agent-based applications by\nbuilding a straightforward travel information agent. This first agent will use\njust one tool: a vector store retriever that answers questions about Cornwall’s\ndestinations and resorts, leveraging content from wikivoyage.org. The\ncontent is split into chunks and stored in a vector store for efficient retrieval.\nIf you’ve followed the advanced RAG (Retrieval-Augmented Generation)\nchapters earlier in this book, you should already be comfortable with\nsourcing content and populating a vector store. Here, we’ll build on that\nfoundation, keeping the focus on agent mechanics.\n11.1.1 Project Setup\nLet’s set up a new Python project using VS Code (this works seamlessly with\nCursor as well).\nCreate a virtual environment and install dependencies\nFirst, set up your Python virtual environment and install all necessary\ndependencies. You’ll find the requirements.txt file either on the Manning\nwebsite for this book or in the cloned GitHub repository that accompanies\nchapter 11.\nOpen a new PowerShell terminal in VS Code (from the menu: Terminal >\nNew Terminal), navigate to the ch11 project folder, then create and activate a\n",
      "content_length": 1555,
      "extraction_method": "Direct"
    },
    {
      "page_number": 362,
      "chapter": null,
      "content": "new virtual environment:\nPS C:\\Github\\building-llm-applications\\ch11> python -m venv env_c\nPS C:\\Github\\building-llm-applications\\ch11> .\\env_ch11\\Scripts\\a\n(env_ch11) PS C:\\Github\\building-llm-applications\\ch11>\nOnce your environment is activated, install the required dependencies:\n(env_ch11) PS C:\\Github\\building-llm-applications\\ch11> pip insta\nYour project environment is now ready for development.\nAdd your OpenAI API key\nCreate a .env file in your project root and add your OpenAI API key:\nOPENAI_API_KEY=<Your OPENAI_API_KEY>\nConfigure VS Code debugging\nFor smooth debugging, add the following launch.json to your .vscode\ndirectory:\n{\n   \"version\": \"0.2.0\",\n   \"configurations\": [\n       {\n           \"name\": \"Python Debugger: Current File\",\n           \"type\": \"debugpy\",\n           \"request\": \"launch\",\n           \"program\": \"${file}\",\n           \"console\": \"integratedTerminal\"\n       }\n   ]\n}\nOrganize your implementation files:\nWe’ll use the naming convention main_x_y.py for our scripts. The “x”\nrepresents the feature (e.g., main_1_1.py for the initial travel info agent,\nmain_2_1.py when adding weather), while “y” is the version of that feature\n",
      "content_length": 1162,
      "extraction_method": "Direct"
    },
    {
      "page_number": 363,
      "chapter": null,
      "content": "as we iterate. This will make it easy for you to compare versions and follow\nthe progression of the implementation.\nNote\nThe code in these examples is intentionally simplified to focus on core\nfunctionality. Error handling and defensive programming are omitted for\nclarity and learning purposes.\n11.1.2 Loading Environment Variables\nOnce your .env file is ready, you can load your API key at the top of your\nscript as follows (see Listing 11.1):\nListing 11.1 Loading environment variables\nload_dotenv() #A\n11.1.3 Preparing the Travel Information Vector Store\nTo enable our travel information agent to answer queries about Cornwall\ndestinations, we first need a way to store and efficiently retrieve relevant\ninformation. The approach here draws on techniques you saw in the\nRetrieval-Augmented Generation (RAG) section, but streamlines them for\nthis agent-centric context. At a high level, we’ll download travel content for a\nset of Cornwall destinations from WikiVoyage, break the text into\nmanageable chunks, embed those chunks into vector representations, and\nthen store everything in a Chroma vector store. We’ll also encapsulate the\ninitialization logic in a singleton pattern to ensure the vector store is only\nbuilt once during the agent’s lifetime.\nThe following code (listing 11.2) sets up this vector store, making it easy to\nretrieve relevant travel information as the agent operates.\nListing 11.2 Preparing the Travel Information Vector Store\nUK_DESTINATIONS = [ #A\n   \"Cornwall\",\n",
      "content_length": 1493,
      "extraction_method": "Direct"
    },
    {
      "page_number": 364,
      "chapter": null,
      "content": "   \"North_Cornwall\",\n   \"South_Cornwall\",\n   \"West_Cornwall\",\n]\nasync def build_vectorstore(destinations: Sequence[str]) -> Chrom\n   \"\"\"Download WikiVoyage pages and create a Chroma vector store\n   urls = [f\"https://en.wikivoyage.org/wiki/{slug}\" for slug in \n   loader = AsyncHtmlLoader(urls) #C\n   print(\"Downloading destination pages ...\") #C\n   docs = await loader.aload() #C\n   splitter = RecursiveCharacterTextSplitter(chunk_size=1024, ch\n   chunks = sum([splitter.split_documents([d]) for d in docs], [\n   print(f\"Embedding {len(chunks)} chunks ...\") #E\n   vectordb_client = Chroma.from_documents(chunks, embedding=Ope\n   print(\"Vector store ready.\\n\")\n   return vectordb_client #F\n# Singleton pattern (build once)\n_ti_vectorstore_client: Chroma | None = None #G\ndef get_travel_info_vectorstore() -> Chroma: #H\n   global _ti_vectorstore_client\n   if _ti_vectorstore_client is None:\n       if not os.environ.get(\"OPENAI_API_KEY\"):\n           raise RuntimeError(\"Set the OPENAI_API_KEY env variab\n       _ti_vectorstore_client = asyncio.run(build_vectorstore(UK_\n   return _ti_vectorstore_client #I\nti_vectorstore_client = get_travel_info_vectorstore() #J\nti_retriever = ti_vectorstore_client.as_retriever() #K\nThis setup starts by defining a list of Cornwall-related destinations that serve\nas the basis for information retrieval. The build_vectorstore()\nasynchronous function constructs URLs for each destination and uses an\nasynchronous loader to fetch the corresponding WikiVoyage pages. Once the\npages are downloaded, the text is split into overlapping chunks to ensure that\ninformation remains contextually meaningful. These chunks are then\nembedded using OpenAI’s embedding models and stored in a Chroma vector\nstore, making them quickly searchable by semantic similarity.\nTo prevent unnecessary recomputation and data downloads, the singleton\n",
      "content_length": 1856,
      "extraction_method": "Direct"
    },
    {
      "page_number": 365,
      "chapter": null,
      "content": "pattern is used for the vector store client. The\nget_travel_info_vectorstore() function ensures that the vector store is\nonly built once and is reused for all future retrievals. At the end, the vector\nstore client is instantiated, and a retriever object is created from it—this\nretriever will be the agent’s interface for accessing Cornwall travel\ninformation. This foundation allows the agent to efficiently answer user\nqueries about destinations, using up-to-date knowledge sourced directly from\nWikiVoyage.\n11.2 Enabling Agents to Call Tools\nNow that we have our vector store retriever ready, the next step is to expose\nthis retrieval capability as a tool the agent can use. This is where the concept\nof tool calling—a major advance in modern agent frameworks—enters the\npicture.\n11.2.1 From Function Calling to Tool Calling\nLLMs like those from OpenAI initially introduced function calling: a\nmechanism that allows the model to request specific functions, passing\nstructured arguments, based on the needs of a given prompt. This concept\nquickly evolved into the more general tool calling protocol, now widely\nsupported by major LLM providers. With tool calling, models can invoke not\njust custom functions but also a variety of built-in or external “tools,”\nhandling everything from code execution to external API lookups.\nThis evolution has made agent implementations dramatically simpler and\nmore robust, particularly when using the ReAct (Reasoning and Acting)\ndesign pattern. The ReAct pattern, introduced by Yao et al. in their 2022\npaper \"ReAct: Synergizing Reasoning and Acting in Language Models\",\nenables LLMs to interleave reasoning steps (“thoughts”) with tool\ninvocations (“actions”). In effect, the model can break a task into smaller\npieces, use tools where appropriate, and synthesize an answer step by step.\nHere’s a simplified view of the ReAct pattern as a process diagram (see\nfigure 11.1):\n",
      "content_length": 1914,
      "extraction_method": "Direct"
    },
    {
      "page_number": 366,
      "chapter": null,
      "content": "Figure 11.1. The ReAct pattern alternates between reasoning (“Re”) and action (“Act”), enabling\nthe agent to process a user question by thinking, calling tools as needed, and following up with\nfurther reasoning before delivering the final answer.\n",
      "content_length": 247,
      "extraction_method": "Direct"
    },
    {
      "page_number": 367,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 368,
      "chapter": null,
      "content": "The ReAct pattern, as illustrated in the diagram, begins with a user question\nthat initiates the process. The agent first enters a Reasoning phase (labeled as\n“Re: Reasoning - Thought”), where the language model considers the user’s\ninput and determines the necessary next steps. If additional information or\nactions are needed, the agent transitions to the Action phase (“Act: Action -\nTool Call”), where it calls one or more tools—such as performing a semantic\nsearch or invoking an API.\nAfter the tool call is completed, the agent returns to another Reasoning phase\n(“Re: Reasoning - Follows Up”), in which it incorporates the results from the\ntool into its ongoing line of reasoning. This may lead to further tool calls, or\nthe agent may determine that it now has enough information to formulate a\nresponse. Finally, the process concludes with the agent providing a Final\nAnswer to the user.\nThis stepwise interplay between reasoning (“Re”) and acting (“Act”)\nembodies the core of the ReAct pattern, allowing agents to dynamically\ncombine thought and action as they work toward solving the user’s query.\nNote\nWith the introduction of the OpenAI Responses API, the transition from\nfunction calling to tool calling has accelerated. The Responses API is\ndesigned specifically for structured tool use, and has largely superseded the\nolder Completion API for agentic applications.\n11.2.2 How Tool Calling Works with LLMs\nOpenAI’s tool calling supports several types of tools—including user-defined\nfunctions, code interpreters, and native capabilities like web browsing. At a\nhigh level, when you register a tool with the LLM, you expose both the\nfunction signature (name and parameters) and a textual description. The\nmodel then decides, at runtime, when and how to use each tool based on the\nuser's input and the context of the conversation.\nIn LangGraph, tools can be registered using either a class-based or decorator-\nbased approach. We’ll use the decorator-based approach for clarity and\n",
      "content_length": 1993,
      "extraction_method": "Direct"
    },
    {
      "page_number": 369,
      "chapter": null,
      "content": "conciseness.\nLet’s implement our semantic search tool as a function decorated with @tool,\nmaking it available to the agent for tool calling, as shown in listing 11.3:\nListing 11.3 LangGraph attribute-based tool definition\n@tool #A\ndef search_travel_info(query: str) -> str: #B\n   \"\"\"Search embedded WikiVoyage content for information about d\n   docs = ti_retriever.invoke(query) #C\n   top = docs[:4] if isinstance(docs, list) else docs #C\n   return \"\\n---\\n\".join(d.page_content for d in top) #D\nThis decorated function, search_travel_info(), is now recognized as a\ntool: it takes a user query, searches the vector store for relevant WikiVoyage\ncontent, and returns up to four top results as a single string. The @tool\ndecorator ensures that the function’s name, description, and parameter\nschema are all available to the LLM for tool calling.\n11.2.3 Registering Tools with the LLM\nTo enable the agent to use our semantic search tool, we must register it with\nthe LLM so that the model knows both the function's signature and its\nintended purpose. In LangChain, this is achieved using the bind_tools\nprotocol, but it is instructive to see how this works at the OpenAI API level\nfirst.\nWith the introduction of the OpenAI Responses API, the way tools (and\nfunctions) are registered has become more standardized and explicit. Now,\nyou provide a structured definition of your tool—specifying its name,\ndescription, and parameter schema. The model can then return responses that\nexplicitly request tool invocations, passing arguments as needed. For further\ndetails, you can consult the OpenAI function calling documentation\n(https://platform.openai.com/docs/guides/function-calling?api-\nmode=responses).\nExample: Manual Tool Registration with OpenAI API\n",
      "content_length": 1750,
      "extraction_method": "Direct"
    },
    {
      "page_number": 370,
      "chapter": null,
      "content": "Suppose you wanted to expose our search_travel_info function directly to\nthe OpenAI API (without using LangChain). You would define the tool’s\nschema as shown in Listing 11.4. Note that this example is for illustration\nonly and is not included in the book’s provided source code.\nListing 11.4 Manual tool registration with OpenAI API\nsearch_travel_info_tool = {\n   \"type\": \"function\",  #A\n   \"function\": {\n       \"name\": \"search_travel_info\",  #B\n       \"description\": \"Search embedded WikiVoyage content for in\n       \"parameters\": {  #D\n           \"type\": \"object\",  #E\n           \"properties\": {\n               \"query\": {  #F\n                   \"type\": \"string\",\n                   \"description\": \"A natural language query abou\n               }\n           },\n           \"required\": [\"query\"]  #H\n       }\n   }\n}\n# Register the tool (function) when making a chat completion requ\nresponse = client.chat.completions.create(\n   model=\"gpt-4o\",  #I\n   messages=[\n       {\"role\": \"user\", \"content\": \"Tell me about surfing in Cor\n   ],\n   tools=[search_travel_info_tool],  #K\n   tool_choice=\"auto\",  # let the model decide when to use the t\n)\nIn this example, you explicitly define the tool’s metadata and parameters, and\nthen pass it in the tools argument of your API call. When the model decides it\nneeds to call search_travel_info, it will return a structured tool call in its\nresponse. Your application must then handle the invocation of the Python\nfunction, passing the model-generated arguments, and send the results back\nto the LLM if the conversation continues.\nRegistering Tools in LangChain\n",
      "content_length": 1597,
      "extraction_method": "Direct"
    },
    {
      "page_number": 371,
      "chapter": null,
      "content": "LangChain automates much of this process. You simply define your tool as a\ndecorated Python function, and then register it with the LLM. You can see in\nlisting 11.5 how this looks in code:\nListing 11.5 Registering tools in LangChain\nTOOLS = [search_travel_info] #A\nllm_model = ChatOpenAI(temperature=0, model=\"gpt-4.1-mini\", #B\n                      use_responses_api=True) #B\nllm_with_tools = llm_model.bind_tools(TOOLS) #C\nHere, we list the available tools (currently just search_travel_info),\ninstantiate the GPT-4.1-mini chat model (with Responses API support), and\nuse .bind_tools(TOOLS) to expose those tools to the model for tool calling.\nLangChain handles the translation between your Python code and the\nOpenAI function/tool-calling protocol, including automatically generating the\nappropriate JSON schema from your function signature and docstring.\nThis setup ensures that whenever the model receives a user message, it can\nautonomously decide if and when to call any registered tool, structuring its\nresponses according to the tool calling protocol.\nNote\nIf you’re using a model other than OpenAI, or you prefer not to use the\nResponses API, tool calling can still work—though the capabilities and\nstructure of the responses may differ. The older Completion API is still\navailable, but for most agentic use cases, the newer Responses API is\nrecommended for its clarity, power, and alignment with evolving best\npractices.\n11.2.4 Agent State: Tracking the Conversation\nIn our implementation, the agent’s state is simply a collection of LLM\nmessages that track the entire conversation. This is defined as follows:\nclass AgentState(TypedDict):\n",
      "content_length": 1651,
      "extraction_method": "Direct"
    },
    {
      "page_number": 372,
      "chapter": null,
      "content": "   messages: Annotated[Sequence[BaseMessage], operator.add]\nHere, AgentState only contains the sequence of messages exchanged\nbetween the user, the agent, and any tool responses. This keeps the design\nsimple.\n11.2.5 Executing Tool Calls\nThe core of the tool execution logic is implemented in a node that examines\nthe LLM’s most recent message, extracts any tool calls requested by the\nmodel, and invokes the corresponding functions with the provided arguments.\nEach tool’s output is then wrapped in a message and appended to the\nconversation state.\nA simplified implementation is shown in listing 11.6.\nListing 11.6 Tool execution implemented from scratch\nclass ToolsExecutionNode: #A\n   \"\"\"Execute tools requested by the LLM in the last AIMessage.\"\n   def __init__(self, tools: Sequence): #B\n       self._tools_by_name = {t.name: t for t in tools}\n   def __call__(self, state: dict): #C\n       messages: Sequence[BaseMessage] = state.get(\"messages\", [\n       last_msg = messages[-1] #D\n       tool_messages: list[ToolMessage] = [] #E\n       tool_calls = getattr(last_msg, \"tool_calls\", []) #F\n       \n       for tool_call in tool_calls: #G\n           tool_name = tool_call[\"name\"] #H\n           tool_args = tool_call[\"args\"] #I\n           tool = self._tools_by_name[tool_name] #J\n           result = tool.invoke(tool_args) #K\n           tool_messages.append(\n               ToolMessage(\n                   content=json.dumps(result), #L\n                   name=tool_name,\n                   tool_call_id=tool_call[\"id\"],\n               )\n           )\n",
      "content_length": 1552,
      "extraction_method": "Direct"
    },
    {
      "page_number": 373,
      "chapter": null,
      "content": "       return {\"messages\": tool_messages} #M\n   \ntools_execution_node = ToolsExecutionNode(TOOLS) #N\nThis pattern allows the agent to handle multiple tool calls in a single step.\nThe tools execution node inspects the LLM’s latest response, invokes each\nrequested tool by name, collects the results, and formats them for the next\nstage in the conversation. In a typical agent workflow, these tool results are\nsent back to the LLM, which incorporates the new information to either\nreason further or generate a direct answer to the user’s query.\nAlthough I’ve shown you how to implement tool calling from scratch for\nlearning purposes, you usually won’t need to write this logic yourself in\npractice, as shown in the sidebar.\nThe Built-in ToolNode Class\nLangGraph provides a built-in class, ToolNode, which performs the same\nfunction as our custom ToolsExecutionNode. For most applications, you can\nuse:\ntools_execution_node = ToolNode(TOOLS)\nThis saves you from having to implement the tools execution logic manually,\nstreamlining your agent development process.\nNow that you understand how tool execution is managed, let’s explore how\nthe agent, guided by the LLM, determines which tools to call—or whether it\nalready has enough information to generate the final answer.\n11.2.6 The LLM Node: Coordinating Reasoning and Action\nNext, we add an LLM node to our LangGraph workflow, as shown in listing\n11.7.\nListing 11.7 LLM Node\ndef llm_node(state: AgentState): #A    \n   \"\"\"LLM node that decides whether to call the search tool.\"\"\"\n",
      "content_length": 1529,
      "extraction_method": "Direct"
    },
    {
      "page_number": 374,
      "chapter": null,
      "content": "   current_messages = state[\"messages\"] #B\n   respose_message = llm_with_tools.invoke(current_messages) #C\n   return {\"messages\": [respose_message]} #D\nThis node takes the agent state (a list of messages) and forwards them to the\nLLM for processing. Because we are using an LLM with tool calling enabled\n(llm_with_tools), the model automatically understands when to issue tool\ncalls and when to produce a final answer:\nIf the last message is a user question, the LLM can decide to request a\ntool call (or multiple calls) to retrieve relevant information.\nIf the last message(s) are tool results, the LLM integrates those\nresponses, reasons further, and may either produce a final answer or\nrequest additional tool calls if needed.\nThe model returns either an AIMessage with a final answer in the content\nfield, or additional tool calls in the tool_calls field, depending on the\nsituation. This flexible, reactive loop is at the heart of all modern agent\nimplementations—and it’s what makes today’s LLM-based agents so\ncapable.\n11.3 Assembling the Agent Graph\nWith our LLM node and tool execution node ready, the next step is to\nassemble them into a working agent graph. In LangGraph, an agent is\nstructured as a directed graph, where each node represents a component (such\nas an LLM or a tool executor), and edges represent the possible flow of data\nand control between these nodes.\nThe code in Listing 11.8 demonstrates how we assemble our single-tool\ntravel information agent as a graph. Each node in the graph corresponds to\neither the LLM (language model) or the tool execution logic.\nListing 11.8 Graph of the tool-based agent\nbuilder = StateGraph(AgentState) #A\nbuilder.add_node(\"llm_node\", llm_node) #B\nbuilder.add_node(\"tools\", tools_execution_node) #B\n",
      "content_length": 1761,
      "extraction_method": "Direct"
    },
    {
      "page_number": 375,
      "chapter": null,
      "content": "builder.add_conditional_edges(\"llm_node\", tools_condition) #C\nbuilder.add_edge(\"tools\", \"llm_node\") #D\nbuilder.set_entry_point(\"llm_node\") #E\ntravel_info_agent = builder.compile() #F\n11.3.1 Understanding the Agent Graph Structure\nOur agent graph consists of two main nodes, as shown in figure 11.2: the\nLLM node, responsible for reasoning and generating tool calls, and the tools\nnode, responsible for executing the requested tools and returning results. The\nflow of the conversation alternates between these two nodes, reflecting the\nReAct pattern described earlier in this chapter.\nFigure 11.2 Conditional graph logic routes each user query through the LLM node, then\ndynamically directs flow to the tools node or ends the process, depending on whether tool calls\nare required\n",
      "content_length": 779,
      "extraction_method": "Direct"
    },
    {
      "page_number": 376,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 377,
      "chapter": null,
      "content": "A critical aspect of this setup is the conditional edge that connects the LLM\nnode to the next step. This is controlled by the tools_condition function—a\nprebuilt utility in LangGraph. This function examines the latest message from\nthe LLM:\nIf the message contains the tool_calls property (meaning the LLM is\nrequesting one or more tool invocations), the graph routes the flow to\nthe node named \"tools\".\nIf there are no tool calls present, the flow is directed to the END node,\nterminating the graph and producing a final answer for the user.\nThis mechanism lets the agent dynamically decide, at each turn, whether to\ncontinue reasoning, take action, or conclude the interaction.\nWe explicitly set the entry point of the graph to \"llm_node\", ensuring that\neach user question is first processed by the language model. (As an\nalternative, you could achieve the same effect by adding an edge from the\nSTART node to \"llm_node\" with graph_builder.add_edge(START,\n\"llm_node\").)\nBy compiling the graph with builder.compile(), we finalize our travel\ninformation agent, ready to receive queries and intelligently use its tool to\nfind relevant travel information.\n11.4 Running the Agent Chatbot: The REPL Loop\nThe final step in building our agent-powered chatbot is to implement the user\ninterface—a simple loop that continuously accepts user questions and returns\nanswers, until the user chooses to exit. This classic pattern, known as a REPL\n(Read-Eval-Print Loop), is the main bridge between the user and your travel\ninformation agent.\nAt a high level, the chat loop listens for input, wraps the user’s question in a\nmessage structure, invokes the agent graph, and prints the assistant’s reply.\nThis interaction continues indefinitely, enabling a true conversational\nexperience.\n",
      "content_length": 1772,
      "extraction_method": "Direct"
    },
    {
      "page_number": 378,
      "chapter": null,
      "content": "Listing 11.9 shows how you can implement this chat loop in Python:\nListing 11.9 Chatbot REPL loop\ndef chat_loop(): #A\n   print(\"UK Travel Assistant (type 'exit' to quit)\")\n   while True:\n       user_input = input(\"You: \").strip() #B\n       if user_input.lower() in {\"exit\", \"quit\"}: #C\n           break\n       state = {\"messages\": [HumanMessage(content=user_input)]} \n       result = travel_info_agent.invoke(state) #E\n       response_msg = result[\"messages\"][-1] #F\n       print(f\"Assistant: {response_msg.content}\\n\") #G\n           \nif __name__ == \"__main__\":\n   chat_loop()            \nThis loop welcomes the user, waits for their question, and continues\nprocessing until the user types “exit” or “quit.” Each input is packaged as a\nHumanMessage and passed to the travel information agent you just built. The\nagent’s reply is extracted from the graph’s output and displayed back to the\nuser.\nWith this in place, you are now ready to run your first agent-based chatbot.\nTry asking it about destinations or activities in Cornwall—and experience\nhow the agent reasons, retrieves information, and converses, all within the\nframework you’ve just constructed, as we are about to see.\n11.5 Executing a Request\nNow that your travel information agent is ready, let’s step through the agent\nin action by running and debugging your implementation. This hands-on\nwalkthrough will show you how the agent orchestrates the flow between the\nLLM and the tool, helping you see the LangGraph framework in motion.\n11.5.1 Step-by-Step Debugging\nBegin by opening your main_01_01.py file and running it in debug mode,\nusing the Python Debug configuration you set up in your launch.json\n",
      "content_length": 1666,
      "extraction_method": "Direct"
    },
    {
      "page_number": 379,
      "chapter": null,
      "content": "earlier. To trace the agent’s flow, place breakpoints at the beginning of the\nfollowing functions:\nsearch_travel_info()\nToolsExecutionNode.__call__()\nllm_node()\nReady? Hit F5 (or click the play icon in your IDE), and let’s walk through the\nagent’s workflow together.\nVector Store Creation\nWhen you start the script, you’ll see the vector store being created and\npopulated with travel information. In your debug console, you’ll see the\noutput similar to that in figure 11.3.\nFigure 11.3 Output at startup, during vector store creation\nChatbot Loop Launch\nNext, chatbot loop starts up, waiting for your input:\nUK Travel Assistant (type 'exit' to quit)\nYou:\nEnter your question and press Enter:\n",
      "content_length": 692,
      "extraction_method": "Direct"
    },
    {
      "page_number": 380,
      "chapter": null,
      "content": "You: Suggest three towns with a nice beach in Cornwall\nLLM Node Activation\nYour breakpoint inside llm_node() will trigger. Inspect the state.messages\nproperty:\nHumanMessage(content='Suggest three towns with a nice beach in Co\nStep over (F10) to the next line to send this message to the LLM. Since the\nLLM is configured for tool calling, examine the resulting response_message:\n  AIMessage(content=[],\n...\ntool_calls=[\n{'name': 'search_travel_info', 'args': {'query': 'beach towns in \n{'name': 'search_travel_info', 'args': {'query': 'best beaches in \n{'name': 'search_travel_info', 'args': {'query': 'top seaside tow\n...)\nHere, the LLM has generated three tool calls—each targeting your semantic\nsearch tool with a slightly different query. Notice how the model rewrites the\nqueries to maximize information coverage, as discussed in the Read-Rewrite\npattern from the Advanced RAG section. You do not need to manually\nhandle query rewriting: the LLM handles it.\nTools Execution Node\nContinue execution (F5). The list of tool calls is added to the message list.\nThe conditional edge’s tools_condition detects tool calls, routing execution to\nToolsExecutionNode.__call__(). Your breakpoint will trigger there.\nInspect state.messages:\n  [\nHumanMessage(content='Suggest three towns with a nice beach in Co\nAIMessage(content=[], ... ,\ntool_calls=[\n{'name': 'search_travel_info', 'args': {'query': 'beach towns in \n{'name': 'search_travel_info', 'args': {'query': 'best beaches in \n{'name': 'search_travel_info', 'args': {'query': 'top seaside tow\n",
      "content_length": 1542,
      "extraction_method": "Direct"
    },
    {
      "page_number": 381,
      "chapter": null,
      "content": "]\nThe last message (from the LLM) has no content (the LLM hasn’t answered\nyet because it needs more information), but it contains the tool calls that must\nbe executed. The node extracts these, then iterates through each one,\nextracting the tool name and arguments, and invokes the corresponding tool:\nresult = tool.invoke(tool_args)\nStep through this to watch search_travel_info() execute. Each semantic\nsearch result (a document returned from the vector store) is collected as a\nToolMessage and added to the list for the next LLM step.\nTool Call Results Passed Back to LLM\nContinue (F5), and you’ll return to llm_node(). Now, state.messages\ncontains your original question, the LLM’s tool call instructions, and the\nresults from each tool execution:\n  \\[HumanMessage(content='Suggest three towns with a nice beach \n  AIMessage(content=\\[], ...\n  tool\\_calls=\\[{'name': 'search\\_travel\\_info', 'args': {'query\n  ToolMessage(content='...', name='search\\_travel\\_info', tool\\_\n  ToolMessage(content='...', name='search\\_travel\\_info', tool\\_\n  ToolMessage(content='...', name='search\\_travel\\_info', tool\\_\nThe content of each tool message gives the LLM the facts it needs to\nsynthesize a final answer. Step over the following line:\nresponse\\_message = llm\\_with\\_tools.invoke(current\\_messages)\nInspect response_message:\nAIMessage(content=\\[{'type': 'text', 'text': \"Three towns in Cornw\nNow, the content field is populated with the LLM’s answer. The\ntool_calls field is gone—the LLM no longer needs external tools and has\nsynthesized a response.\nCompleting the Request\n",
      "content_length": 1569,
      "extraction_method": "Direct"
    },
    {
      "page_number": 382,
      "chapter": null,
      "content": "As execution leaves llm_node(), the tools_condition on the conditional\nedge checks for further tool calls. Finding none, it ends the conversation.\nWhen your main invocation returns:\nresult = travel\\_info\\_agent.invoke(state)\nThe result.messages list concludes with the final AIMessage containing the\nanswer:\n  {'messages': [\n  HumanMessage(content='Suggest three towns with a nice beach in \n  AIMessage(content=[],  tool_calls=[...]),\n  ToolMessage(...), ToolMessage(...), ToolMessage(...),\n  AIMessage(content=[{'type': 'text', 'text': \"Three towns in Co\n  ]\nThe chatbot will now display the answer and prompt for your next question:\nUK Travel Assistant (type 'exit' to quit)\nYou: Suggest three towns with a nice beach in Cornwall\nAssistant: [{'type': 'text', 'text': \"Three towns in Cornwall wit\nYou:\nYou have now observed, step by step, how your agent processes a user\nrequest: interpreting the question, generating tool calls, executing them, and\nsynthesizing a final, well-informed answer. This debug-driven walkthrough\noffers deep insight into the mechanics of agentic reasoning and action in\nLangGraph.\n11.6 Expanding Your Agent: Adding a Weather\nForecast Tool\nSo far, our agent has been able to answer travel-related queries using\nsemantic search over curated travel content. But real-world travel advice\noften depends on dynamic, real-time information—like the weather! In this\nsection, you’ll learn how to extend your agent by adding a second tool,\nenabling it to respond with context-aware answers based on current\nconditions.\n",
      "content_length": 1538,
      "extraction_method": "Direct"
    },
    {
      "page_number": 383,
      "chapter": null,
      "content": "11.6.1 Implementing a Mock Weather Service\nTo illustrate the process, start by copying your main_01_01.py file to a new\nscript called main_02_01.py. This will help you track each evolutionary step\nin your agent’s development.\nFirst, let’s introduce a mock weather service. This service will simulate real-\ntime weather data for any given town, returning both a weather condition\n(such as “sunny” or “rainy”) and a temperature. You can see the\nimplementation of the WeatherForecastService in listing 11.10:\nListing 11.10 WeatherForecastService\nclass WeatherForecast(TypedDict):\n   town: str\n   weather: Literal[\"sunny\", \"foggy\", \"rainy\", \"windy\"]\n   temperature: int\nclass WeatherForecastService:\n   _weather_options = [\"sunny\", \"foggy\", \"rainy\", \"windy\"]\n   _temp_min = 18\n   _temp_max = 31\n   @classmethod\n   def get_forecast(cls, town: str) -> Optional[WeatherForecast]\n       weather = random.choice(cls._weather_options)\n       temperature = random.randint(cls._temp_min, cls._temp_max\n       return WeatherForecast(town=town, weather=weather, temper\nThis mock service chooses a random weather condition and temperature\nwithin a typical summer range for Cornwall. Later, you can swap this out for\na real-world weather API if desired.\n11.6.2 Creating the Weather Forecast Tool\nWith the mock service in place, the next step is to create a tool that wraps it,\nmaking weather data available to the agent. Here’s how you can define the\ntool function and add it to your agent’s toolkit:\n@tool\n",
      "content_length": 1491,
      "extraction_method": "Direct"
    },
    {
      "page_number": 384,
      "chapter": null,
      "content": "def weather_forecast(town: str) -> dict:\n   \"\"\"Get a mock weather forecast for a given town. Returns a We\n   forecast = WeatherForecastService.get_forecast(town)\n   if forecast is None:\n       return {\"error\": f\"No weather data available for '{town}'\n   return forecast\nThe weather_forecast tool provides a mock weather forecast for a given\ntown. When called with a town name, it returns a dictionary containing the\nweather conditions (such as sunny, foggy, rainy, or windy) and temperature\nfor that location. If no data is available, it returns an error message instead.\nThis tool allows the agent to incorporate simulated real-time weather\ninformation into its responses.\n11.6.3 Updating the Agent for Multi-Tool Support\nFinally, make sure your LLM model is set up to use tool calling and\nrecognizes both available tools:\nTOOLS = [search_travel_info, weather_forecast] #A\nllm_model = ChatOpenAI(\n   temperature=0,\n   model=\"gpt-4.1-mini\",   #B\n   use_responses_api=True  #B\n)\nllm_with_tools = llm_model.bind_tools(TOOLS) #C\nWith this setup, your agent is now equipped to handle both travel queries and\nweather checks, giving it the foundation to provide much more accurate and\nuseful responses. In the next sections, you’ll see how to guide the LLM to use\nboth tools effectively, and observe the agent’s new capabilities in action.\nThis workflow not only shows how to extend your agent’s toolset, but also\ndemonstrates the modular, incremental approach that makes agentic systems\nwith LangGraph and LangChain so powerful.\n11.7 Executing the Multi-Tool Agent\nWith your weather forecast tool registered, your agent can now synthesize\n",
      "content_length": 1634,
      "extraction_method": "Direct"
    },
    {
      "page_number": 385,
      "chapter": null,
      "content": "answers that combine travel information and real-time weather conditions.\nThe beauty of this modular approach is that adding a new tool requires no\nchanges to your agent’s graph structure. All orchestration is handled by the\nLLM and the tool calling protocol.\n11.7.1 Running the Multi-Tool Agent (Initial Behavior)\nLet’s put the new capabilities to the test. Set the same breakpoints as before\n—especially in the tool execution and LLM nodes—and add a breakpoint at\nthe start of weather_forecast().\nRun main_02_01.py in debug mode (F5) and enter the following prompt:\nYou: Suggest two Cornwall beach towns with nice weather\nIncorrect Behavior: LLM Defaults to Internal Knowledge\nAfter submitting your query, your first breakpoint will hit in llm_node().\nStep through to the last line and inspect the value of\nresponse_message.tool_calls:\n[{'name': 'weather_forecast', 'args': {'town': 'Newquay'}, 'id': \n{'name': 'weather_forecast', 'args': {'town': 'Falmouth'}, 'id': \nHere, the LLM has simply picked two towns it “knows” (Newquay and\nFalmouth) and asked for their weather, without consulting your semantic\nsearch tool at all. This is typical of a model using its internal knowledge base\nrather than the tools provided—something we want to avoid for reliability\nand accuracy.\nWhy does this happen? The LLM is acting on its pre-training and defaulting\nto what it already “knows” about Cornwall, rather than querying your up-to-\ndate data.\n11.7.2 Improving LLM Tool Usage with System Guidance\nTo nudge the LLM towards tool use and away from hallucinations, let’s make\nits instructions and tool descriptions clearer. Make a copy of your script as\n",
      "content_length": 1645,
      "extraction_method": "Direct"
    },
    {
      "page_number": 386,
      "chapter": null,
      "content": "main_02_02.py, and update the tool definitions:\n@tool(description=\"Search travel information about destinations i\ndef search_travel_info(query: str) -> str: \n   ...\n@tool(description=\"Get the weather forecast, given a town name.\")\ndef weather_forecast(town: str) -> dict:\n   ...\nNext, introduce a guiding SystemMessage in your llm_node():\nsystem_message = SystemMessage(content=\"You are a helpful assista\ncurrent_messages.append(system_message)\nYou can see the amended llm_node() function in listing 11.11:\nListing 11.11 Guiding tool selection\ndef llm_node(state: AgentState): #A    \n   \"\"\"LLM node that decides whether to call the search tool.\"\"\"\n   current_messages = state[\"messages\"] #B\n   system_message = SystemMessage(content=\"You are a helpful ass\n   current_messages.append(system_message) #D\n   respose_message = llm_with_tools.invoke(current_messages) #E\n   return {\"messages\": [respose_message]} #F\nRestart your application in debug mode and ask:\nYou: Suggest two Cornwall beach towns with nice weather\nAt your first breakpoint in llm_node(), examine current_messages before\nand after appending the system message. Then, check respose_message—\nnow, you’ll see the following tool call:\n{'name': 'search_travel_info', 'args': {'query': 'beach towns in \nWhat does this mean?\nThe LLM is now forced to use your semantic search tool for candidate beach\ntowns, and won’t pick them “from its own knowledge.” This minimizes\n",
      "content_length": 1427,
      "extraction_method": "Direct"
    },
    {
      "page_number": 387,
      "chapter": null,
      "content": "hallucinations and guarantees the data used comes from your knowledge\nbase.\nContinue stepping through the code (F5). Right before you submit the\ncurrent_messages to the LLM again, your messages will look like:\n[HumanMessage(content='Suggest 2 Cornwall beach towns with nice w\nSystemMessage(content='You are a helpful assistant that can searc\nAIMessage(content=[], ..., tool_calls=[{'name': 'search_travel_in\nToolMessage(content='\"<p id=\\\\\"mwrg\\\\\">Cornwall, in particular New\nSystemMessage(content='You are a helpful assistant that can searc\nFrom the ToolMessage, the LLM now receives a list of possible beach towns\nfrom your vector store.\nNext, as you step through and reach llm_node() again, the LLM will issue\ncalls for the weather forecast in two of these towns:\nAIMessage(content=[], ..., tool_calls=[\n{'name': 'weather_forecast', 'args': {'town': 'Newquay'}, 'id': '\n{'name': 'weather_forecast', 'args': {'town': 'St Ives'}, 'id': '\nThe towns selected by the LLM might differ in your run, but they’ll always\ncome from the results of your semantic search tool.\nStep through the weather tool calls—each ToolMessage you inspect should\nlook like:\nToolMessage(content='{\"town\": \"Newquay\", \"weather\": \"foggy\", \"tem\nToolMessage(content='{\"town\": \"St Ives\", \"weather\": \"windy\", \"tem\nWhat If the Weather Isn’t Good?\nSuppose the weather is less than ideal in those towns. Continue to the next\nLLM response and you’ll see:\nAIMessage(content=[], ...,  \ntool_calls=[\n{'name': 'weather_forecast', 'args': {'town': 'Perranporth'}, 'id\n{'name': 'weather_forecast', 'args': {'town': 'Falmouth'}, 'id': \nHere, the LLM is asking for the weather in two more towns, likely because\n",
      "content_length": 1665,
      "extraction_method": "Direct"
    },
    {
      "page_number": 388,
      "chapter": null,
      "content": "the previous results didn’t meet the “nice weather” requirement. This process\ncontinues until the agent finds two towns with suitable conditions.\nAfter the final tool calls, the LLM generates a synthesized, fact-based\nanswer:\nAIMessage(content=[{'type': 'text', 'text': 'Two beach towns in C\nWhich appears to the user as:\nUK Travel Assistant (type 'exit' to quit)\nYou: Suggest two Cornwall beach towns with nice weather\nAssistant: [{'type': 'text', 'text': 'Two beach towns in Cornwall \nIn summary, by enhancing tool descriptions and providing explicit\ninstructions via system prompts, you can guide the LLM to chain tool use in a\nmulti-step, fact-grounded workflow—first searching for beach towns, then\nfiltering by real-time weather—producing answers that are both dynamic and\nreliable.\nExercise\nTry replacing the mock weather tool with a real API, like OpenWeatherMap,\nusing LangChain’s OpenWeatherMap integration. This will make your agent\ntruly real-time!\n11.8 Using Pre-Built Components for Rapid\nDevelopment\nUp to this point, you’ve built an agent from the ground up: wiring together\nthe graph, orchestrating tool calls, and stepping through every detail in the\ndebugger. You now have a solid grasp of how tool calling works under the\nhood.\nHowever, for most production scenarios, you’ll want to reduce boilerplate\nand move faster—while still retaining transparency and observability when\nneeded. The LangGraph library provides pre-built agent components, such as\nthe react agent, that encapsulate much of this orchestration logic for you.\n",
      "content_length": 1547,
      "extraction_method": "Direct"
    },
    {
      "page_number": 389,
      "chapter": null,
      "content": "Let’s see how dramatically you can simplify your agent by switching to a\npre-built approach.\n11.8.1 Refactoring to Use the LangGraph React Agent\nStart by copying your previous script (main_02_02.py) to a new file:\nmain_03_01.py.\nImport the RemainingSteps Utility\nAt the top of your script, add the following import:\nfrom langgraph.managed.is_last_step import RemainingSteps\nRemove Manual Tool Binding\nYou can now delete the line where you bound tools to the LLM:\nllm_with_tools = llm_model.bind_tools(TOOLS)\nThe pre-built agent will handle this for you internally.\nUpdate the AgentState\nModify your AgentState definition to include a remaining_steps field. This\nfield allows the agent to manage how many tool-calling rounds are left in a\ncontrolled way:\nclass AgentState(TypedDict): \n   messages: Annotated[Sequence[BaseMessage], operator.add]\n   remaining_steps: RemainingSteps \nRemove Node and Graph Construction\nNow for the biggest simplification: delete your custom ToolsExecutionNode,\nllm_node, and any explicit graph wiring code.\n",
      "content_length": 1036,
      "extraction_method": "Direct"
    },
    {
      "page_number": 390,
      "chapter": null,
      "content": "Replace it all with a single instantiation of the built-in LangGraph React\nagent:\ntravel_info_agent = create_react_agent(\n   model=llm_model,\n   tools=TOOLS,\n   state_schema=AgentState,\n   prompt=\"You are a helpful assistant that can search travel in\nThat’s it! The React agent now orchestrates the flow, tool calling, and\nsynthesis for you.\n11.8.2 Running the Pre-Built Agent\nWhen you run main_03_01.py and ask your usual test question:\nYou: Suggest two Cornwall beach towns with nice weather\nAssistant: [{'type': 'text', 'text': 'Two beach towns in Cornwall \nYou get the correct, grounded answer—with much less code.\n11.8.3 Observing and Debugging with LangSmith\nA common concern when switching to high-level abstractions is loss of\nvisibility: How do you know the agent is actually following the right\nreasoning steps?\nWhile you can still debug tool functions directly, the flow inside the agent\nitself is less exposed.\nThis is where LangSmith comes in. LangSmith enables full tracing and\ninspection of agent behavior, including tool calls, LLM reasoning, and\nintermediate states.\nEnabling LangSmith Tracing\nTo enable tracing, add the following lines to your .env file:\nLANGSMITH_TRACING=true\n",
      "content_length": 1196,
      "extraction_method": "Direct"
    },
    {
      "page_number": 391,
      "chapter": null,
      "content": "LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com\"\nLANGSMITH_API_KEY=\"<your-langsmith-api-key>\"\nLANGSMITH_PROJECT=\"langchain-in-action-react-agent\"\nAfter re-running your application and submitting a question, you can log into\nLangSmith https://smith.langchain.com):\n1. Select Tracing projects in the Observability menu (left-hand sidebar).\n2. Click your project (langchain-in-action-react-agent) in the right-\nhand panel.\n3. Open the latest trace.\nYou’ll see the full execution trace, as shown in figure 11.4.\nFigure 11.4 LangSmith agent execution trace: this trace visualizes each step of the agent’s\nworkflow—including LLM calls, tool invocations, and message flow—when answering the\nquestion, “Suggest two Cornwall beach towns with nice weather.”\n",
      "content_length": 752,
      "extraction_method": "Direct"
    },
    {
      "page_number": 392,
      "chapter": null,
      "content": "TRACE 0 (A) LangCraph 70\n= Waterfall @ ,*\nRun Feedback Metadata\n[A LangGraph ©\n0582s ©1267\nInput v\nBD agent 282s\nBD call.model 2.8% —\nG Punnablesequence 2.79s V Suggest two Cornwall beach towns with nice weather\nBD Prompt 0.00s\nG) chatopenal gpt4:-mini. 278s\nOutput v\nQ should continue 0,00s\nB tons 067s ¥ HUMAN\nQ search_travel_info 0.66s Vv Suggest two Cornwall beach towns with nice weather\nVectorStoreRetriever 065s\n@ too's o02s vow\nBD weather forecast 0.003\nB tools 002s Vv —-gearch travel_info call_gnt74CvMudg}k9bsLUOFSBUS\nA] weather forecast 0.00s\nquery: beach towns in Cornwall\nBD agent 222s v\nB) call_model 2.21 VY _weather_forecast call_oj6ckdc4LtmegewsvLe2bMwU\nQ Runnablesequence 2.21 v\ntown: St Ives\nB} Prompt 0.005\nGB chatopenal gota 22 weather forecast call_avw57JluKLHw)i3sVxX)kUz9\n\nQ should_continue 0,00s",
      "content_length": 821,
      "extraction_method": "OCR"
    },
    {
      "page_number": 393,
      "chapter": null,
      "content": "The LangSmith graphical trace shows every tool call, every LLM step, and\nthe flow of messages—so even when you use a pre-built agent, you retain the\nability to audit, debug, and understand exactly what the agent is doing at\nevery stage.\nIn summary, by combining pre-built LangGraph agent components with\nLangSmith for observability, you can quickly build robust, production-ready\nagentic applications—while still maintaining full transparency and control\nwhen needed. This workflow has become the foundation for many modern AI\nagent systems in real-world use today.\n11.9 Summary\nLangGraph enables you to build both deterministic workflows and\ndynamic LLM agents using node-based and tool-based architectures.\nYou can register tools—like semantic search and weather APIs—so that\nLLMs can choose and call them step by step.\nBy stepping through code with breakpoints, you can trace agent\nreasoning, tool invocation, and message flow for debugging and\nunderstanding.\nClear tool descriptions and system messages help control LLM behavior\nand reduce hallucination.\nMulti-tool agents can chain together tool outputs (e.g., finding towns,\nthen filtering by weather), all orchestrated by the LLM.\nUsing pre-built agent components, like the LangGraph React agent,\ndramatically reduces boilerplate and speeds up development.\nLangSmith provides rich observability, showing each LLM step and tool\ncall even for high-level agent abstractions.\nThis agent-centric pattern lays the groundwork for more advanced agent\nand multi-agent systems explored in later chapters.\n",
      "content_length": 1552,
      "extraction_method": "Direct"
    },
    {
      "page_number": 394,
      "chapter": null,
      "content": "12 Multi-agent Systems\nThis chapter covers\nConnecting tools to data sources\nComposing multi-agent systems using router and supervisor patterns\nDebugging, testing, and tracing multi-agent interactions\nIn chapter 11, we explored the foundations of building AI agents by creating\na travel information agent capable of answering user queries about\ndestinations, routes, and transportation options. While a single, specialized\nagent can be powerful, real-world applications often require the coordination\nof multiple agents, each handling a distinct area of expertise. In this chapter,\nwe’ll embark on that journey—transforming our travel information agent into\na robust, multi-agent travel assistant system.\nImagine planning a trip where you not only need up-to-date travel\ninformation but also want to seamlessly book your accommodation. Our\nenhanced multi-agent travel assistant will do just that: it will be able to\nanswer travel questions and help you reserve hotels or bed and breakfasts in\nyour chosen destination. To achieve this, we’ll begin by building a new agent\n—the accommodation booking agent.\nThe accommodation booking agent will empower users to book lodgings\nfrom two different sources. First, it will interface with a local accommodation\ndatabase, which mainly features hotel deals and is exposed via a dedicated\ntool. Second, it will connect to an external B&B REST API, providing access\nto a wider selection of bed and breakfast options, also accessible through its\nown tool. Depending on user requests, the agent will use one or both of these\ntools to deliver relevant accommodation options.\nOnce we have our new agent in place, we’ll combine it with the travel\ninformation agent from the previous chapter. The result will be a unified,\nmulti-agent travel assistant capable of fielding a wide variety of travel-related\nqueries, handling both information requests and accommodation bookings,\n",
      "content_length": 1908,
      "extraction_method": "Direct"
    },
    {
      "page_number": 395,
      "chapter": null,
      "content": "and even combining both for a more streamlined experience.\nLet’s begin by constructing our new accommodation booking agent.\n12.1 Building an Accommodation Booking Agent\nTo build a practical, helpful travel assistant, we need more than just\ninformation retrieval—we need the ability to act. In this section, we’ll\ndevelop an accommodation booking agent from the ground up, starting by\nbuilding the tools it needs: one for hotel bookings based on a local room\navailability database, and another for B&B bookings from an external REST\nAPI. By the end of this section, you’ll have a ReAct-style agent that can\ncheck and book both hotel and B&B rooms in Cornwall.\n12.1.1 Hotel Booking Tool\nLet’s start by creating the hotel booking tool. To enable our agent to retrieve\nhotel offers and availability, we’ll use the LangChain SQL Database Toolkit,\nwhich exposes a SQL database as a set of agent tools. This toolkit makes it\nstraightforward for an agent to run queries, retrieve hotel details, and check\nroom availability—all through tool calls, without needing to write raw SQL\nin your prompts.\nThe hotel data, including current offers and availability, is stored in a local\nSQLite database cornwall_hotels.db which is kept up-to-date by our\nbackend partners. We don’t need to worry about how the data is pushed—just\ntrust that it’s there and refreshed as needed.\nFirst, copy the latest script, main_03_01.py, to a new script, main_04_01.py.\nThen, prepare your environment:\n1. Create a folder named hotel_db.\n2. Place the provided SQL schema file cornwall_hotels_schema.sql into\nthat folder.\n3. Open a terminal (inside VS Code or standalone), navigate to the folder,\nand create the database with (I have omitted the root of the ch11 folder\nfor convenience):\n",
      "content_length": 1752,
      "extraction_method": "Direct"
    },
    {
      "page_number": 396,
      "chapter": null,
      "content": "\\ch11>cd hotel_db\n\\ch11>sqlite3 cornwall_hotels.db < cornwall_hotels_schema.sql\nNow, let’s check that the database is working. Open the SQLite shell:\n\\ch11>sqlite3 cornwall_hotels.db\nWithin the SQLite shell, run these queries to verify your setup:\nsqlite> .tables\nsqlite> SELECT * FROM hotels;\nsqlite> SELECT * FROM hotel_room_offers;\nWith the database ready, let’s move on to the Python implementation.\nImport the necessary LangChain SQL integration libraries:\nfrom langchain_community.utilities.sql_database import SQLDatabas\nfrom langchain_community.agent_toolkits import SQLDatabaseToolkit\nInstantiate the SQLite database:\nhotel_db = SQLDatabase.from_uri(\"sqlite:///hotel_db/cornwall_hote\nNow, create an instance of the SQL Database toolkit:\nhotel_db_toolkit = SQLDatabaseToolkit(db=hotel_db, llm=llm_model)\nThat’s it! Now you can access the toolkit’s tools with:\nhotel_db_toolkit_tools = hotel_db_toolkit.get_tools()\n12.1.2 B&B Booking Tool\nNext, let’s create a Bed & Breakfast booking tool. This tool will retrieve\nB&B room availability from a REST service. For development and testing,\nwe’ll mock this service.\nFirst, we’ll define the return type for our tool, and then create a mock\nimplementation of the BnB booking service, as shown in Listing 12.1. (For\n",
      "content_length": 1265,
      "extraction_method": "Direct"
    },
    {
      "page_number": 397,
      "chapter": null,
      "content": "convenience, the example here uses a reduced set of mock data. You can find\nthe complete implementation in the code files provided with this book.).\nListing 12.1 BnB Booking Service\nclass BnBBookingService: #A\n   @staticmethod\n   def get_offers_near_town(town: str, num_rooms: int) -> List[B\n       # Mocked REST API response: multiple BnBs per destination\n       mock_bnb_offers = [ #C\n           # Newquay\n           {\"bnb_id\": 1, \"bnb_name\": \"Seaside BnB\", \"town\": \"New\n           {\"bnb_id\": 2, \"bnb_name\": \"Surfside Guesthouse\", \"tow\n           # Falmouth\n           {\"bnb_id\": 3, \"bnb_name\": \"Harbour View BnB\", \"town\": \n           {\"bnb_id\": 4, \"bnb_name\": \"Seafarer's Rest\", \"town\": \n           # St Austell\n           {\"bnb_id\": 5, \"bnb_name\": \"Garden Gate BnB\", \"town\": \n           {\"bnb_id\": 6, \"bnb_name\": \"Coastal Cottage BnB\", \"tow\n                  ... \n           # Port Isaac\n           {\"bnb_id\": 27, \"bnb_name\": \"Port Isaac View BnB\", \"tow\n           {\"bnb_id\": 28, \"bnb_name\": \"Fisherman's Cottage BnB\", \n           # Fowey\n           {\"bnb_id\": 29, \"bnb_name\": \"Fowey Quay BnB\", \"town\": \n           {\"bnb_id\": 30, \"bnb_name\": \"Riverside Rest BnB\", \"tow\n       ]\n       offers = [offer for offer in mock_bnb_offers if offer[\"tow\n       return offers\nNow we can define the check_bnb_availability tool:\nListing 12.2 BnB Availability tool\n@tool(description=\"Check BnB room availability and price for a de\ndef check_bnb_availability(destination: str, num_rooms: int) -> L\n   \"\"\"Check BnB room availability and price for the requested de\n   offers = BnBBookingService.get_offers_near_town(destination, \n   if not offers:\n       return [{\"error\": f\"No available BnBs found in {destinati\n   return offers\n12.1.3 ReAct Accommodation Booking Agent\n",
      "content_length": 1758,
      "extraction_method": "Direct"
    },
    {
      "page_number": 398,
      "chapter": null,
      "content": "With both the hotel and B&B booking tools ready, it’s time to build the\nReAct accommodation booking agent. This agent will use both tools in\nresponse to user requests. If the user doesn’t specify a preference, the agent\nwill search both hotels and B&Bs for available rooms.\nBOOKING_TOOLS = hotel_db_toolkit_tools + [check_bnb_availability] \naccommodation_booking_agent = create_react_agent( #B\n   model=llm_model,\n   tools=BOOKING_TOOLS,\n   state_schema=AgentState,\n   prompt=\"You are a helpful assistant that can check hotel and \n)\nYou can now try out the agent by replacing the agent line in chat_loop()\nwith:\n...\nresult = accommodation_booking_agent.invoke(state) \n...\nLet’s run the main_04_01.py script in debug mode and ask the following\nquestion:\nUK Travel Assistant (type 'exit' to quit)\nYou: Are there any rooms available in Penzance?\nAfter you hit ENTER, you might see an answer similar to the following one:\nI have found Penzance Pier BnB with available rooms at £95 per ro\nFor hotels, Penzance Palace has 3 available rooms with prices of \nAs you can see, the agent used both tools to retrieve results from both the\nhotel database and the mock B&B service.\nAt this point, your accommodation booking agent is working as expected. It’s\nstrongly recommended to debug the execution and inspect the LangSmith\ntraces to better understand how the agent is reasoning and acting step by step.\nAlthough you now have both a travel information agent and an\naccommodation booking agent, they are still disconnected. You can use\n",
      "content_length": 1525,
      "extraction_method": "Direct"
    },
    {
      "page_number": 399,
      "chapter": null,
      "content": "either one or the other, but not both in a unified experience. In the next\nsection, we’ll build a multi-agent travel assistant that brings these capabilities\ntogether, providing a seamless experience for travel planning and\naccommodation booking.\n12.2 Building a router-based Travel assistant\nSo far, we have developed two independent agents: a travel information agent\nand an accommodation booking agent, each with specialized capabilities.\nWhile this modular approach is powerful, it raises an essential design\nquestion: how can we combine these agents to deliver a seamless user\nexperience—one that can answer travel information queries and handle\naccommodation bookings in a single conversation?\nA common and effective solution is to introduce a router agent. This agent\nacts as an intelligent entry point: it receives the user’s message, determines\nwhich specialized agent should handle the request, and dispatches the task\naccordingly.\n12.2.1 Designing the Router Agent\nTo implement our multi-agent travel assistant, begin by copying your\nprevious script, main_04_01.py, to main_05_01.py. Next, we need to bring in\nsome extra libraries to support graph-based workflows:\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.types import Command\nThe next step is to clearly define the set of agents available for routing. We\ndo this by declaring an enumeration for the two agent types:\nclass AgentType(str, Enum):\n   travel_info_agent = \"travel_info_agent\"\n   accommodation_booking_agent = \"accommodation_booking_agent\"\nTo ensure our router agent receives clear and structured decisions from the\nLLM, we define a Pydantic model that captures the LLM’s output—\nspecifying which agent should handle each query:\n",
      "content_length": 1719,
      "extraction_method": "Direct"
    },
    {
      "page_number": 400,
      "chapter": null,
      "content": "class AgentTypeOutput(BaseModel): \n   agent: AgentType = Field(..., description=\"Which agent should \nBy configuring the OpenAI LLM client to produce responses in this\nstructured format, we eliminate any need for string parsing or manual post-\nprocessing. The router will always produce a result of either\n\"travel_info_agent\" or \"accommodation_booking_agent\".\n12.2.2 Routing Logic\nThe heart of the router agent is its system prompt, which concisely instructs\nthe LLM how to classify each user request:\nROUTER_SYSTEM_PROMPT = (\n   \"You are a router. Given the following user message, decide i\n   \"or an accommodation booking question (about hotels, BnBs, ro\n   \"If it is a travel information question, respond with 'travel_\n   \"If it is an accommodation booking question, respond with 'ac\n)\nWith this system prompt, the router agent evaluates each user input and\ndecides which specialist agent should take over. The router is implemented as\nthe entry node of our LangGraph workflow, with the travel information agent\nand the accommodation booking agent as subsequent nodes. You can see the\nrouter implementation in listing 12.2.\nListing 12.3 Router Agent node\ndef router_agent_node(state: AgentState) -> Command[AgentType]:\n   \"\"\"Router node: decides which agent should handle the user qu\n   messages = state[\"messages\"] #A\n   last_msg = messages[-1] if messages else None #B\n   if isinstance(last_msg, HumanMessage): #C\n       user_input = last_msg.content #D\n       router_messages = [ #E\n           SystemMessage(content=ROUTER_SYSTEM_PROMPT),\n           HumanMessage(content=user_input)\n       ]\n       router_response = llm_router.invoke(router_messages) #F\n       agent_name = router_response.agent.value #G\n       return Command(update=state, goto=agent_name) #H\n   \n",
      "content_length": 1772,
      "extraction_method": "Direct"
    },
    {
      "page_number": 401,
      "chapter": null,
      "content": "   return Command(update=state, goto=AgentType.travel_info_agent\nIf you examine the implementation in listing 12.2, you’ll notice that the\nrouter extracts the user’s message and submits it to the LLM along with the\nsystem prompt. The LLM returns a structured output of type\nAgentTypeOutput, which contains the agent name to which the request\nshould be routed. The router then uses a Command to redirect the conversation\nflow to the selected agent node in the graph. In simple workflows, the\nCommand can hand off the unchanged state to the new node, but it also allows\nfor state updates in more complex flows.\n12.2.3 Building the Multi-Agent Graph\nAt this point, you have all the components needed to assemble the graph-\nbased multi-agent system. You can see the graph implementation in listing\n12.3.\nListing 12.4 Router-based multi-agent travel assistant graph\ngraph = StateGraph(AgentState) #A\ngraph.add_node(\"router_agent\", router_agent_node) #B\ngraph.add_node(\"travel_info_agent\", travel_info_agent) #C\ngraph.add_node(\"accommodation_booking_agent\", accommodation_booki\ngraph.add_edge(\"travel_info_agent\", END) #E\ngraph.add_edge(\"accommodation_booking_agent\", END) #F\ngraph.set_entry_point(\"router_agent\") #G\ntravel_assistant = graph.compile() #H\nThe workflow graph connects the router agent to the two specialized agents.\nNotably, the only explicit edges you define are those from the travel\ninformation agent and the accommodation booking agent to the end of the\nworkflow. The connection from the router to the specialized agents is\ndetermined dynamically at runtime by the LLM’s response and is handled via\nCommand.\nBelow is a graphical representation of the current multi-agent travel assistant\ngraph:\n",
      "content_length": 1708,
      "extraction_method": "Direct"
    },
    {
      "page_number": 402,
      "chapter": null,
      "content": "Figure 12.1 Router-based multi-agent Travel assistant: the router agent dispatches user queries\nto either the travel information agent or the accommodation booking agent, each equipped with\ntheir own specialized tools\nAs the diagram in figure 12.1 shows, this is a hybrid architecture. At the top,\nthe system exhibits a deterministic, workflow-driven routing logic. At the\n",
      "content_length": 373,
      "extraction_method": "Direct"
    },
    {
      "page_number": 403,
      "chapter": null,
      "content": "lower level, each specialist agent uses its own set of tools (such as travel data\nAPIs or accommodation booking interfaces) and follows an agentic, tool-\nbased decision process, which is inherently more flexible and dynamic.\n12.2.4 Trying Out the Router Agent\nTo see the system in action, run your multi-agent travel assistant by starting\nmain_05_01.py in debug mode and try the following two user queries:\nWhat are the main attractions in St Ives?\nAre there any rooms available in Penzance this weekend?\nOne important thing to note with this design is that each user question is\nrouted to a single agent for handling—in other words, each query takes a\n\"one-way ticket\" through the workflow. The router makes a clean and\nunambiguous handoff, and the selected agent responds directly to the user\nbefore the workflow ends.\nFor example, when you ask:\nWhat are the main attractions in St Ives?\nthe request is routed to the travel information agent. You can see the related\nLangSmith execution trace in figure 12.2.\nFigure 12.2 LangSmith execution trace of a travel information question\n",
      "content_length": 1082,
      "extraction_method": "Direct"
    },
    {
      "page_number": 404,
      "chapter": null,
      "content": "(&) LangGraph ©\n© 12.72s © 1,390\n( router_agent 9.295\n©} RunnabieSequence 2.88s\n©) chatOpenal gpt-4.1-mini 2.875\n(Q RunnableLambda 0.00s\n(QB travel_info_agent 3.32s\nLangGraph 3.32s\n(2B agent 0.96s\n(BQ calt_mode! 0.96s\n©} RunnadieSequence 0.95s\nPrompt 0.00s\n©) ChatOpenal gpt-4.1-mini_ 0.95s\nshould_continue 0.00s\n(B tools 117s\nsearch_travel_info 1.17s\nVectorStoreRetriever 1.16s\nagent 1.18s\n( cali_model 118s\n| RunnableSequence 1.18s\nPrompt 0.00s\n©) chatOpenal gpt-4.1-mini 1.175\nshould_continue 0.00s",
      "content_length": 501,
      "extraction_method": "OCR"
    },
    {
      "page_number": 405,
      "chapter": null,
      "content": "If you ask:\nAre there any rooms available in Penzance this weekend?\nthe system dispatches the query to the accommodation booking agent.\nIn both cases, the workflow (which can be followed in figure 12.2) is clear:\nthe router agent evaluates the intent and hands the query off to the most\nappropriate specialist agent, which then handles the response and ends the\nsession.\nThis modular, graph-based design provides a strong foundation for more\nadvanced workflows. In later sections, you’ll see how you can evolve this\nsystem to handle more complex, multi-step, or even collaborative agentic\nscenarios.\n12.3 Handling Multi-Agent Requests with a\nSupervisor\nThe workflow-based multi-agent architecture we developed in the previous\nsection works well for simple, single-purpose queries—questions that can be\nclearly routed to either the travel information agent or the accommodation\nbooking agent. But what happens when a user asks for something that spans\nboth domains? Consider a query like:\n\"Can you find a nice seaside Cornwall town with good weather right now and\nfind availability and price for one double hotel room in that town?\"\nWith our previous router-based design, such a question cannot be answered\neffectively, as it requires both agents to work together and share intermediate\nresults.\nTo solve this, we need to shift our architecture towards a more flexible,\ncollaborative agent system—one where multiple specialized agents can be\ncoordinated as “sub-tools” under a higher-level manager. In LangGraph, this\nis exactly the use case for the Supervisor: a built-in component designed to\n",
      "content_length": 1594,
      "extraction_method": "Direct"
    },
    {
      "page_number": 406,
      "chapter": null,
      "content": "orchestrate multiple agents, allowing them to collaborate on complex\nrequests.\n12.3.1 The Supervisor Pattern: An Agent of Agents\nConceptually, the Supervisor is an “agent of agents.” It acts as an\norchestrator, managing a collection of other agents (which themselves may\nuse tools) and deciding which agent to activate, possibly multiple times in a\nsingle workflow. Each agent acts as a specialized tool that the Supervisor can\ninvoke as needed.\nLet’s see how to set up this pattern in your multi-agent travel assistant.\nStart by copying one of your previous implementation, main_04_01.py, to a\nnew script, main_06_01.py. Next, install the necessary package:\npip install langgraph-supervisor\nThen import the Supervisor:\nfrom langgraph_supervisor.supervisor import create_supervisor\nWhen defining agents to be managed by the Supervisor, it’s important to\nassign each a unique name. You can see how you instantiate your agents with\nnames in listing 12.4.\nListing 12.5 Setting up the leaf agents\ntravel_info_agent = create_react_agent(\n   model=llm_model,\n   tools=TOOLS,\n   state_schema=AgentState,\n   name=\"travel_info_agent\",\n   prompt=\"You are a helpful assistant that can search travel in\n)\naccommodation_booking_agent = create_react_agent(\n   model=llm_model,\n   tools=BOOKING_TOOLS,\n   state_schema=AgentState,\n   name=\"accommodation_booking_agent\",\n",
      "content_length": 1354,
      "extraction_method": "Direct"
    },
    {
      "page_number": 407,
      "chapter": null,
      "content": "   prompt=\"You are a helpful assistant that can check hotel and \n)\nNow you can implement your travel assistant as a Supervisor, as shown in\nlisting 12.5:\nListing 12.6 Setting up the Supervisor agent\ntravel_assistant = create_supervisor( #A\n   agents=[travel_info_agent, accommodation_booking_agent], #B\n   model= ChatOpenAI(model=\"gpt-4.1\", use_responses_api=True), #\n   supervisor_name=\"travel_assistant\",\n   prompt=( #D\n       \"You are a supervisor that manages two agents: a travel i\n       \"You can answer user questions that might require calling \n       \"Decide which agent(s) to use for each user request and c\n   )\n).compile() #E\nYou’ll notice that configuring a Supervisor is much like setting up a ReAct\nagent, but instead of passing a list of tools, you provide a list of agents. Since\nthe Supervisor needs to analyze complex multi-step requests and coordinate\nseveral agents, it’s best to use a more powerful LLM model (like gpt-4.1 or\neven o3) to maximize accuracy and task decomposition.\nTip\nTry experimenting with different models for the Supervisor, such as gpt-4.1,\no3 or o4-mini, and compare how well the assistant handles increasingly\ncomplex, multi-faceted questions.\nAs in previous designs, simply update your chat loop to invoke the\nsupervisor-based travel assistant:\nresult = travel_assistant.invoke(state)\n12.3.2 From “One-Way” to “Return Ticket” Interactions\nUnlike the workflow-based router—where every user question was routed\nonce and only once to a specific agent (a “one-way ticket”)—the Supervisor\n",
      "content_length": 1529,
      "extraction_method": "Direct"
    },
    {
      "page_number": 408,
      "chapter": null,
      "content": "enables a much richer interaction. The Supervisor can invoke each agent as\nneeded, potentially revisiting agents multiple times (“return tickets”) in a\nsingle session to collect, combine, and reason over intermediate results. This\nenables the system to handle more sophisticated, open-ended, and multi-part\nqueries.\nBelow, you can see a diagram representing this supervisor-based architecture:\nFigure 12.3 Router-based multi-agent Travel assistant: the router agent dispatches user queries\nto either the travel information agent or the accommodation booking agent, each equipped with\ntheir own specialized tools\n",
      "content_length": 612,
      "extraction_method": "Direct"
    },
    {
      "page_number": 409,
      "chapter": null,
      "content": "As shown in the diagram in figure 12.3, both the high-level (supervisor) and\nlow-level (agent/tool) orchestration follow a tool-based approach,\nmaximizing flexibility and composability. The Supervisor becomes the\ncentral decision-maker, ensuring the right agent (or sequence of agents) is\nactivated for every complex travel request.\n",
      "content_length": 333,
      "extraction_method": "Direct"
    },
    {
      "page_number": 410,
      "chapter": null,
      "content": "This Supervisor-driven architecture unlocks a new level of multi-agent\ncollaboration, laying the groundwork for more advanced, open-ended AI\ntravel assistants capable of addressing real-world, multi-step needs.\n12.3.3 Trying out the Supervisor agent\nNow, run the travel assistant by starting main_06_01.py in debug mode (with\nLangSmith tracing enabled), and try entering a complex, multi-part question\nsuch as:\nUK Travel Assistant (type 'exit' to quit)\nYou: Can you find a nice seaside Cornwall town with good weather \nWhen you examine the LangSmith trace, you’ll notice a more intricate agent\nutilization trajectory, similar to that in figure 12.4, in which the\ntravel_assistant is the supervisor agent.\nFigure 12.4 LangSmith execution trace of a combine travel information and booking question\n",
      "content_length": 796,
      "extraction_method": "Direct"
    },
    {
      "page_number": 411,
      "chapter": null,
      "content": "LangGraph —\n@® 2389s => 1216\nGs travel_assistant 2-795\ne& travel_info_agent 76<s\nGQ cat_agent 76<=s\nGs travel_info_agent 76<=s\nGa agent oss\nGy tools OS's\n| | search_travel_info O.30s\nGa agent 353s\nGz tools 0-025\nGy weather_forecast 0.005\nGy tcols 003s\nGy tools 0-025\nGa tcols 0:07\nGa agent ov2s\nGz tools 0-235\nG_ agent 1795s\nGB travel_assistant 2-29=\n\necc = booking agent 917s\ncallagent 917s\naccommodsation_booking_agent 917s\n\nGay agent 2-245\nGy tools 0.07\nGay check_bnb_availability 0.005\nEy tools 002s\nGs salt_db_query ©.00s\nagent oO.7is\ntools O.OTs\negent OS89s\ntools 0.035\nagent 2.61s\ntools O-.O2s\nagent 193s\nGB travel_assistant 1695",
      "content_length": 636,
      "extraction_method": "OCR"
    },
    {
      "page_number": 412,
      "chapter": null,
      "content": "I have summarized below the key steps from the execution trace below, so\nyou can understand the flow better (remember the travel_assistant is the\nsupervisor):\ntravel assistant\ntools\ntransfer_to_travel_info_agent\ntravel_info_agent\ntools\nsearch_travel_info\ntools\nweather_forecast\ntravel_assistant\ntools\ntransfer_to_accommodation_booking_agent\naccommodation_booking_agent\ntools\ncheck_bnb_availability\ntools\nsql_db_query\nThis shows that the supervisor-based travel assistant was able to coordinate\nboth agents, using each one and its underlying tools as needed to fully answer\nthe user's question. Technically, agents are now orchestrated as tools, and the\nSupervisor manages this collaboration dynamically.\n12.4 Summary\nBy integrating external tools—such as SQL databases and REST APIs—\nagents can retrieve real-time data and perform actions beyond simple\ninformation retrieval.\nRouter-based agent design enable modularity by directing each user\nquery to the most appropriate specialist agent, simplifying single-task\nscenarios.\nThe supervisor pattern goes further, orchestrating collaboration among\nagents, so multiple agents can be used together to answer complex,\n",
      "content_length": 1164,
      "extraction_method": "Direct"
    },
    {
      "page_number": 413,
      "chapter": null,
      "content": "multi-part questions.\nTracing and debugging with LangSmith provides valuable visibility into\nagent decisions and tool usage, making it easier to optimize and extend\nyour system.\n",
      "content_length": 178,
      "extraction_method": "Direct"
    },
    {
      "page_number": 414,
      "chapter": null,
      "content": "13 Building and consuming MCP\nservers\nThis chapter covers\nThe purpose and architecture behind the Model Context Protocol (MCP)\nBuilding and exposing your own MCP server, with a practical weather\ntool example\nTesting and consuming MCP servers and related tools in applications\nIntegrating remote MCP tools into agents alongside local tools\nBuilding AI agents that can effectively access and use external context is a\ncentral challenge for application developers. Previously, integrating context\nfrom different sources meant wrapping each one as a tool, following specific\nprotocols‚Äîa time-consuming and repetitive process repeated by countless\nteams. Imagine if, instead, data and service providers could expose their\nresources as ready-made tools, instantly available for any agent or\napplication. This is the promise of the Model Context Protocol (MCP),\nintroduced by Anthropic.\nMCP defines a standardized way for services to expose ‚Äútools‚Äù through\nMCP servers. Agents, or ‚ÄúMCP hosts,‚Äù connect to these servers via\nMCP clients, discovering and using remote tools as easily as local ones. This\napproach moves much of the integration work to where it belongs‚Äîat the\nsource‚Äîallowing developers to focus on building more capable agents\nrather than reinventing the same plumbing. Once the connection is set up,\ntools from MCP servers work seamlessly with existing agent architectures.\nSince its release in late 2024, MCP has rapidly become a de-facto standard.\nMajor LLM providers like OpenAI and Google have adopted MCP in their\nAPIs and SDKs, while a growing number of companies and communities are\nmaking services available as MCP servers. Today, thousands of tools are\nlisted on public MCP server portals, ready to be integrated into new or\nexisting projects with minimal effort.\n",
      "content_length": 1794,
      "extraction_method": "Direct"
    },
    {
      "page_number": 415,
      "chapter": null,
      "content": "In this chapter, you‚Äôll get hands-on experience with MCP. We‚Äôll\nintroduce the core protocol and architecture, highlight the growing\necosystem, and show you where to find both official and community MCP\nservers. Then, you‚Äôll learn to build your own MCP server‚Äîstarting with\na practical weather data example using AccuWeather‚Äîtest it, and integrate\nit into an agent application. You‚Äôll also see how to combine remote MCP\ntools with local logic, configure clients, and follow best practices for robust\nintegration.\nAs the MCP ecosystem grows, understanding how to build and consume\nMCP servers is becoming essential for developers and service providers\nalike. Let‚Äôs dive in and explore how MCP is shaping modern, context-rich\nAI applications.\n13.1 Introduction to MCP Servers\nBuilding advanced AI agents increasingly involves giving them access to\nexternal sources of context, such as databases, APIs, or other services.\nTraditionally, this is achieved by wrapping each data source as a ‚Äútool‚Äù\nthat agents can call using a protocol supported by most large language\nmodels (LLMs). However, this approach quickly leads to repeated\nwork‚Äîevery developer has to build and maintain similar wrappers for\ncommon sources, resulting in duplicated effort across the ecosystem.\nWhat if there was a better way? Instead of every developer reinventing the\nwheel, what if the providers of context could directly expose their data and\nservices as tools, ready to be consumed by any agent? This is the central\nmotivation behind the Model Context Protocol (MCP), an open standard\nintroduced by Anthropic. MCP allows for context and capabilities to be\nexposed in a unified, tool-based way, as you can appreciate in figure\n13.1‚Äîeliminating much of the friction in agent development and\nencouraging a richer, more reusable ecosystem.\nFigure 13.1 MCP host process connecting to multiple local and remote MCP servers via MCP\nclients, each exposing tools backed by various resources. This architecture enables agents to\nflexibly access both local documents and online services through standardized tool interfaces\n",
      "content_length": 2108,
      "extraction_method": "Direct"
    },
    {
      "page_number": 416,
      "chapter": null,
      "content": "MCP Host process\n\nme ce Local MCP Server 1 (on\n\nMCP client 3\nAl Agent or\nAl Application\nLocal Documents\n\nOnline resources\n\nRemote MCP Server 3 fen\n\nRemote Hotel DB\n\nRemote MCP Server 2\nOnline\n\nWeather Service",
      "content_length": 208,
      "extraction_method": "OCR"
    },
    {
      "page_number": 417,
      "chapter": null,
      "content": "13.1.1 The Problem: Context Integration at Scale\nAs we saw in previous chapters, LLM agents typically consume external data\nvia tools injected into their requests. Each new context source‚Äîa weather\nAPI, a document database, a search service‚Äîmeans creating yet another\nwrapper tool and integrating it with the agent, often following slightly\ndifferent conventions and protocols. This process is not only repetitive but\nalso results in the same work being duplicated by countless teams.\nImagine every agent developer spending time wrapping the same set of\npublic APIs, or every organization hand-rolling integrations for standard\nservices. This approach simply doesn‚Äôt scale, especially as the range of\npossible tools and context sources grows.\n13.1.2 The Solution: The Model Context Protocol (MCP)\nMCP addresses this challenge by providing a standardized way for data and\nservice providers to expose tools via MCP servers. Instead of each agent\nindividually wrapping every service, developers can simply ‚Äúsubscribe‚Äù\nto MCP servers and use the tools they expose, with minimal additional work.\nThe protocol defines a classic client-server architecture, as you can see in\nfigure 13.1.\nHere, the MCP host process‚Äîthe agent or application‚Äîconnects to one or\nmore MCP servers using an MCP client. Each MCP server can expose a\ncollection of tools, and the agent subscribes only to the tools it needs. This\narchitecture is familiar from that of REST APIs and WebSockets, which\nexpose endpoints for consumption through a client, and makes it easy to add\nor swap out new sources of context as requirements change.\nAs you can also see on figure 13.1, MCP servers can be either remote\n(accessed over StreamableHTTP, usually in production) or local (accessed\nvia STDIO, typically for development or for accessing local resources such\nas files). In most cases, and throughout this chapter, we‚Äôll assume you are\nworking with remote MCP servers.\n",
      "content_length": 1945,
      "extraction_method": "Direct"
    },
    {
      "page_number": 418,
      "chapter": null,
      "content": "Once configured, tools from MCP servers integrate with your agent in\nexactly the same way as local tools, adhering to the same tool calling\nprotocols you learned in earlier chapters. If your agent is modular and well-\narchitected, you often don‚Äôt need to change any logic at all to support\nremote MCP tools.\nNote\nMCP goes beyond just tools‚Äîit can also standardize how prompts, files, and\nother resources are shared. For our purposes, we‚Äôll focus on tool exposure,\nbut for a deeper dive, see the original MCP article by Anthropic\n(https://www.anthropic.com/news/model-context-protocol).\n13.1.3 The MCP Ecosystem\nSince its introduction in late 2024, MCP has rapidly gained traction. Leading\nLLM providers like OpenAI and Google have both adopted MCP, integrating\nsupport into their APIs and agent SDKs. On the provider side, companies are\nincreasingly wrapping their services and data with MCP servers, making\nthem instantly ‚ÄúAI-ready‚Äù for a wide range of agents and applications.\nA number of public MCP server portals have emerged, making it easy to find\ntools for almost any need. Table 13.1 highlights some of the most prominent\ndirectories:\nTable 13.1 MCP server portals\nPortal\nDescription\nhttps://github.com/modelcontextprotocol/servers Anthropic's official MCP\nportal, listing both\nofficial and community\nservers\nhttps://mcp.so/\nA community-driven\ndirectory featuring over\n16,000 servers\nhttps://smithery.ai/\nA portal with over 5,000\ntools, mostly MCP-\n",
      "content_length": 1467,
      "extraction_method": "Direct"
    },
    {
      "page_number": 419,
      "chapter": null,
      "content": "compliant\nhttps://mcpservers.org/\nA collection of around\n1,500 servers\nWith such a broad and growing ecosystem, agents can increasingly draw on a\nshared library of tools‚Äîwhether built in-house, offered by major companies,\nor shared by the community.\nAfter this overview of the motivation, architecture, and ecosystem behind\nMCP servers, we are ready to look at how to build, expose, and consume\nthese tools in real-world applications. In the next sections, we‚Äôll explore\nhow to create your own MCP server and how to integrate MCP tools\nseamlessly into your agents.\n13.2 How to Build MCP Servers\nWith a clear understanding of what MCP servers are and their role in the\nmodern AI agent ecosystem, the next step is to learn how to actually build,\ndeploy, and integrate MCP servers. This section will guide you through the\nkey resources, official tools, and best practices to help you create robust MCP\nservers and make them available to agents and applications.\n13.2.1 Essential Resources for MCP Server Development\nBefore diving into code, it‚Äôs important to familiarize yourself with the\nfoundational documentation and tools for MCP development. The official\nhub for the protocol is modelcontextprotocol.io, which provides a wealth of\ninformation on the architecture, design principles, tutorials, and the complete\nprotocol specification. Whether you are new to MCP or looking for advanced\nfeatures, this site should be your starting point.\nTip\nPay particular attention to the MCP protocol specification\n(https://modelcontextprotocol.io/specification ), which is regularly updated\nand details every aspect of the protocol‚Äîincluding transport mechanisms,\nsecurity considerations, and best practices. The specification itself is\n",
      "content_length": 1733,
      "extraction_method": "Direct"
    },
    {
      "page_number": 420,
      "chapter": null,
      "content": "technology-agnostic, making it valuable no matter which language or\nplatform you use.\nWhile understanding the protocol is crucial, implementing MCP from scratch\nin your own application is not recommended. Doing so would mean\nduplicating a significant amount of effort, and you‚Äôd likely end up\nreimplementing features already solved in mature SDKs. Instead, you should\nleverage one of the official language-specific SDKs available for MCP.\n13.2.2 Official Language-Specific MCP SDKs\nThe official MCP GitHub repository aggregates a variety of resources, with\nparticular emphasis on the official SDKs for several major programming\nlanguages‚Äîincluding Python, JavaScript, Java, Kotlin, and C#. This book\nwill primarily focus on Python, but the same general principles apply to the\nother supported languages.\nFastMCP 1\nThe initial Python SDK, often referred to as FastMCP 1, is available at\nhttps://github.com/modelcontextprotocol/python-sdk . This library provided\nthe community with the first robust framework for building and consuming\nMCP servers in Python.\nFastMCP 2\nBuilding on the experience with FastMCP 1, the MCP community released\nFastMCP 2‚Äîa major upgrade that addresses limitations of the original\nimplementation and aligns more closely with the latest protocol\nspecifications. FastMCP 2 offers significant improvements, such as:\nEasier deployment and server composition\nEnhanced security features\nImproved client connectivity and advanced capabilities, like dynamic\ntool rewriting\nBuilt-in testing utilities and integration hooks for other libraries\n",
      "content_length": 1565,
      "extraction_method": "Direct"
    },
    {
      "page_number": 421,
      "chapter": null,
      "content": "FastMCP 2 is actively maintained at https://github.com/jlowin/fastmcp, and\nyou‚Äôll find comprehensive documentation and tutorials at\nhttps://gofastmcp.com/getting-started/welcome. We‚Äôll use FastMCP 2 for\nhands-on examples, so keep these resources handy as you follow along.\n13.2.3 Consuming MCP Servers in LLM Applications and\nAgents\nWhile the previous sections focused on building MCP servers, it‚Äôs equally\nimportant to understand how to consume these servers in your own AI-\npowered applications. Thanks to wide adoption, integrating MCP tools has\nbecome remarkably straightforward‚Äîespecially with leading platforms like\nOpenAI.\nOpenAI‚Äôs API natively supports tools provided by public MCP servers via\nthe Responses API. Not only can you discover and reference these tools, but\nOpenAI will also execute them for you‚Äîeliminating the need for manual\nclient code in many cases.\nTip\nReview the OpenAI documentation on remote MCP tool integration. The\nprocess is simple, but it‚Äôs recommended to carefully consider your\nauthorization strategy‚Äîdeciding whether to approve such calls\nautomatically or interactively.\nIn many enterprise environments, MCP servers might be available only\nwithin organizational boundaries, so you cannot expect the OpenAI\nResponses API to execute the remote tools for you. For these scenarios, there\nare two primary approaches:\n1. Use the FastMCP Client: The official FastMCP SDK provides client\nfacilities to connect, authenticate, and consume tools from MCP servers\ndirectly.\n2. Take advantage of LangChain/LangGraph Integrations: If you are\ndeveloping agents with LangGraph or LangChain, you can use the\nLangChain MCP client library‚Äîparticularly the\n",
      "content_length": 1692,
      "extraction_method": "Direct"
    },
    {
      "page_number": 422,
      "chapter": null,
      "content": "MultiServerMCPClient class‚Äîto easily aggregate and consume tools\nfrom multiple MCP servers through a simple configuration interface.\nIn the following sections, we‚Äôll demonstrate both approaches in practice.\nYou‚Äôll learn how to build a practical MCP server, test it, and integrate it\ninto an agent workflow‚Äîwhether you‚Äôre working directly with SDKs or\nleveraging modern agent frameworks like LangChain.\n13.3 Building a Weather MCP Server\nAfter learning about what MCP Servers are, their purpose, and the available\nlibraries and ecosystem, it is time to build one! In this section, we'll replace\nthe mockup weather tool that you used to build agents in previous chapters\nwith a real-world weather MCP server based on the AccuWeather REST API.\nWe‚Äôll also integrate this server into one of the agent-based solutions built\nearlier. Step by step, you‚Äôll see how to build, test, and connect the MCP\nserver to your agent.\n13.3.1 Implementing the MCP Server\nWe begin by replacing our previous mock weather tool with a proper MCP\nserver that exposes live weather data from AccuWeather.\nBefore implementing the code, go to the AccuWeather developer portal and\nregister (for free at: https://developer.accuweather.com/user/register) to claim\nan API key.\nAfter registering, navigate to the My Apps tab and click \"Add a new\nApp\".\nFill in the details, such as:\nApp name: accu-mcp-server\nProducts: Core Weather Limited Trial\nWhere will the app be used: Desktop Website\nWhat will you be creating with this API: Weather App\nProgramming language: Python\nChoose other settings as desired, then click Create App.\nOnce your new app is registered, locate the API key in the My Apps\n",
      "content_length": 1673,
      "extraction_method": "Direct"
    },
    {
      "page_number": 423,
      "chapter": null,
      "content": "dashboard and copy it.\nAdd the API key to your .env file in your Python project, as shown\nbelow (replace the API key accordingly):\nACCUWEATHER_API_KEY=<Your API key>\nYou are now ready to implement a real MCP server that exposes the weather\nservice. Create a folder named mcp within the ch11 folder, and create an\nempty Python script called accuweather_mcp.py.\nYou can see the MCP server implementation in listing 14.1, adapted from\nthis GitHub repository: https://github.com/adhikasp/mcp-weather.\nListing 13.1 Accuweather MCP server\nimport os\nimport json\nfrom typing import Dict\nfrom fastmcp import FastMCP\nfrom dotenv import load_dotenv\nfrom aiohttp import ClientSession\nload_dotenv() #A\nmcp = FastMCP(\"mcp-accuweather\") #B\n@mcp.tool(description=\"Get weather conditions for a location.\") #\nasync def get_weather_conditions(location: str) -> Dict:\n   \"\"\"Get weather conditions for a location.\"\"\"\n   api_key = os.getenv(\"ACCUWEATHER_API_KEY\") #D\n   base_url = \"http://dataservice.accuweather.com\"\n   async with ClientSession() as session:\n       location_search_url = f\"{base_url}/locations/v1/cities/se\n       params = { #F\n           \"apikey\": api_key,\n           \"q\": location,\n       }\n       async with session.get(location_search_url, params=params\n           locations = await response.json() #G\n           if response.status != 200:\n               raise Exception(f\"Error fetching location data: {\n           if not locations or len(locations) == 0:\n               raise Exception(\"Location not found\")\n",
      "content_length": 1510,
      "extraction_method": "Direct"
    },
    {
      "page_number": 424,
      "chapter": null,
      "content": "       location_key = locations[0][\"Key\"] #H\n       # Get current conditions \n       current_conditions_url = f\"{base_url}/currentconditions/v\n       params = { #J\n           \"apikey\": api_key,\n           \"details\": \"true\"\n       }\n       async with session.get(current_conditions_url, params=par\n           current_conditions = await response.json() #K\n           \n       # Format current conditions\n       if current_conditions and len(current_conditions) > 0:\n           current = current_conditions[0] #L\n           current_data = {\n               \"temperature\": {\n                   \"value\": current[\"Temperature\"][\"Metric\"][\"Va\n                   \"unit\": current[\"Temperature\"][\"Metric\"][\"Uni\n               },\n               \"weather_text\": current[\"WeatherText\"],\n               \"relative_humidity\": current.get(\"RelativeHumidit\n               \"precipitation\": current.get(\"HasPrecipitation\", \n               \"observation_time\": current[\"LocalObservationDate\n           }\n       else:\n           current_data = \"No current conditions available\" #M\n       return { #N\n           \"location\": locations[0][\"LocalizedName\"], \n           \"location_key\": location_key, \n           \"country\": locations[0][\"Country\"][\"LocalizedName\"], \n           \"current_conditions\": current_data,\n       }\nif __name__ == \"__main__\":\n   mcp.run(transport=\"streamable-http\", host=\"127.0.0.1\", #O\n           port=8020, path=\"/accu-mcp-server\")\nThis implementation relies on the fastmcp package (which is FastMCP 2).\nMake sure to install the required package in your virtual environment (or add\nfastmcp to your requirements.txt and run pip install -r\nrequirements.txt).\nThe core logic of the implementation is simple: it searches the underlying\nAccuWeather locations REST endpoint to resolve the user‚Äôs input, then\n",
      "content_length": 1800,
      "extraction_method": "Direct"
    },
    {
      "page_number": 425,
      "chapter": null,
      "content": "queries current weather conditions using AccuWeather‚Äôs API.\n(env_ch11) C:\\Github\\building-llm-applications\\ch11> pip install \nNow, open a new terminal (within VS Code or otherwise), activate your\nvirtual environment, move into the mcp folder, and run the server:\nC:\\Github\\building-llm-applications\\ch11>env_ch11\\Scripts\\activat\n(env_ch11) C:\\Github\\building-llm-applications\\ch11>cd mcp\n(env_ch11) C:\\Github\\building-llm-applications\\ch11\\mcp>python ac\nYou will see the MCP server starting up, and finally you'll see this output:\n‚Üê[32mINFO‚Üê[0m:     Started server process [‚Üê[36m20712‚Üê[0m\n‚Üê[32mINFO‚Üê[0m:     Waiting for application startup.\n‚Üê[32mINFO‚Üê[0m:     Application startup complete.\n‚Üê[32mINFO‚Üê[0m:     Uvicorn running on ‚Üê[1mhttp://0.0.0.0:80\nCongratulations! Your first MCP server is up and running!\n13.3.2 Trying out the MCP Server with MCP Inspector\nOne of the fastest ways to interactively test your newly created MCP server is\nby using the MCP Inspector. This tool provides a user-friendly interface that\nlets you connect to any MCP server, explore its tools, and run live queries\nwithout needing to write any client code. The process is straightforward, and\nthe Inspector is a great way to build confidence before integrating the server\ninto your applications.\nInstalling MCP Inspector\nTo get started, you‚Äôll need to install the MCP Inspector on your computer.\nMCP Inspector is a Node.js application, so ensure you have Node.js installed.\nIf not, you can download and install it from nodejs.org.\nOnce Node.js is ready, open a new command prompt or terminal. It‚Äôs a\ngood practice to keep your tools organized, so create a new folder under your\nproject directory‚Äîsuch as mcp-inspector inside ch11. Then run the\nfollowing command to launch MCP Inspector using npx:\n",
      "content_length": 1805,
      "extraction_method": "Direct"
    },
    {
      "page_number": 426,
      "chapter": null,
      "content": "c:\\Github\\building-llm-applications\\ch11\\mcp-inspector>npx @model\nDuring installation, you will be asked to confirm before proceeding:\nNeed to install the following packages:\n@modelcontextprotocol/inspector@0.16.212\nOk to proceed? (y) y\nAfter confirmation, the installation will proceed, and MCP Inspector will\nautomatically launch in your browser:\n",
      "content_length": 349,
      "extraction_method": "Direct"
    },
    {
      "page_number": 427,
      "chapter": null,
      "content": "14 Productionizing AI Agents:\nmemory, guardrails, and beyond\nThis chapter covers:\nAdding short-term memory with LangGraph checkpoints\nImplementing guardrails at multiple workflow stages\nAdditional considerations for production deployment\nBuilding AI agents that behave reliably in real-world environments is about\nmore than just connecting a language model to some tools. Production\nsystems need to maintain context across turns, respect application boundaries,\nhandle edge cases gracefully, and keep operating even when something\nunexpected happens. Without these safeguards, even the most capable model\nwill eventually produce errors, off-topic answers, or inconsistent behavior\nthat undermines user trust.\nIn this chapter, we’ll focus on two of the most important capabilities for\nmaking AI agents production-ready: memory and guardrails. Memory allows\nan agent to “remember” past interactions, enabling it to hold natural\nconversations, answer follow-up questions, and recover from interruptions.\nGuardrails keep the agent within its intended scope and policy framework,\nfiltering out irrelevant or unsafe requests before they reach the model—and,\nif needed, catching inappropriate responses after the model has generated\nthem.\nWe’ll explore how these features work in LangGraph, using our travel\ninformation assistant as the running example. You’ll see how to persist\nconversation state using checkpoints, enforce scope restrictions at both the\nrouter and agent level, and extend the design to include post-model checks\nand human-in-the-loop review. By the end of the chapter, you’ll have the\ntools to build assistants that are not only smart, but also safe, focused, and\nresilient.\n",
      "content_length": 1688,
      "extraction_method": "Direct"
    },
    {
      "page_number": 428,
      "chapter": null,
      "content": "14.1 Memory\nWhen designing AI agent-based systems—especially those exposed via\nchatbots—one of the most important steps towards production readiness is\nadding memory. Memory allows the system to maintain context between user\ninteractions, enabling stateful workflows and natural conversations.\nWithout memory, each interaction between the user and the LLM starts fresh,\nwith no knowledge of what was said before. For simple Q&A this might be\nacceptable, but for anything that requires follow-up, refinement, or reference\nto past turns, it quickly becomes frustrating.\nLangGraph provides a powerful and flexible mechanism for implementing\nmemory: checkpoints. In this section we will explore short-term memory\nusing checkpoints, and see how to integrate them into our existing\ntravel_assistant agent.\n14.1.1 Types of Memory\nIn AI agents, memory can exist at different scopes:\nShort-term memory — Context retained during a single session between\na user and the LLM. This is ideal for ongoing conversations until the\nuser achieves their goal. Typically stored in-memory or in a session-\nscoped store.\nLong-term user memory — Persistent across multiple sessions for the\nsame user, enabling the system to remember preferences or past\nactivities.\nLong-term application-level memory — Persistent across all users and\nsessions, storing general knowledge useful for everyone (e.g., current\nexchange rates).\nLong-term user and application memory tend to be highly system-specific, so\nin this section we will focus exclusively on short-term conversational\nmemory.\n14.1.2 Why Short-Term Memory is Needed\n",
      "content_length": 1592,
      "extraction_method": "Direct"
    },
    {
      "page_number": 429,
      "chapter": null,
      "content": "In a conversational application that uses the tool-calling protocol (as\ndescribed in earlier chapters), a typical interaction works like this:\n1. The user sends a message.\n2. The LLM may issue tool calls.\n3. The application executes these tools and returns results to the LLM.\n4. The LLM synthesises a final answer.\nIf the user follows up with a clarifying or related question, a stateless system\nwould lose all prior context, forcing the conversation to restart. This is both\ninefficient and unnatural.\nThe solution is to store the entire conversation history after each interaction\nand feed it back to the LLM on subsequent turns. This makes the system\nstateful and allows the model to resolve references like “same town” or “that\nhotel” based on prior turns.\n14.1.3 Checkpoints in LangGraph\nAlthough you could implement short-term memory simply by storing the list\nof exchanged messages after each LLM response, LangGraph provides a\nmore powerful and generalized mechanism: the checkpointer.\nA checkpoint is a snapshot of the graph’s execution state taken at a specific\nmoment in the flow.\nIn practice, LangGraph takes these snapshots at each node in the graph—\nstarting from the entry point (also called the START node) and continuing\nuntil the exit point (END node).\nThis approach is more flexible than storing messages alone, because it\npreserves all aspects of the graph state, not just the conversation text.\nThat flexibility enables a range of use cases beyond chat memory:\nState rehydration after failure — If an execution fails partway through,\nyou can resume from the last successful checkpoint without re-running\nthe entire process. This is especially valuable when some steps are\n",
      "content_length": 1694,
      "extraction_method": "Direct"
    },
    {
      "page_number": 430,
      "chapter": null,
      "content": "expensive or slow.\nHuman-in-the-loop workflows — You can pause at a checkpoint to\ncollect manual input or approval, then resume exactly where you left off\nusing the stored state.\nMulti-turn conversational context — For chatbots, checkpoints ensure\nthe conversation history is preserved across turns without having to\nreconstruct it manually.\nIn this chapter we will focus solely on the last use case—maintaining\nconversational state for the entire duration of a user–LLM session—while\nkeeping in mind that the same mechanism is equally capable of powering the\nothers.\nWhat is a Checkpoint?\nA checkpoint represents the saved state of the graph at a given super-step,\nwhere:\nA super-step corresponds to the execution of a single graph node.\nParallel nodes processed within the same step are grouped together.\nBy saving a checkpoint at each super-step, we can:\nReuse the current execution state in a follow-up question.\nReplay the persisted execution up to a specific point—useful when\nrecovering from errors or resuming after manual intervention.\nHow Checkpoints Work in LangGraph\nLangGraph’s checkpointing system is built around two main concepts:\n1. Checkpointer — The component responsible for capturing and storing\nstate snapshots. After each super-step (i.e., completion of a node), the\ncheckpointer records the current graph state, as illustrated in figure 14.1.\nThis snapshot can include conversation history, tool outputs,\nintermediate variables, and execution metadata—ensuring the workflow\ncan be resumed or inspected at any point.\n",
      "content_length": 1540,
      "extraction_method": "Direct"
    },
    {
      "page_number": 431,
      "chapter": null,
      "content": "2. Configuration — Defines the session that checkpoints belong to. In\nLangGraph, a session is called a thread.\na. Each thread is identified by a thread ID (a unique value, typically a\nUUID).\nb. A single user may have multiple threads (sessions) active or saved\nat different times.\nc. The configuration links the checkpoint to a specific thread so the\nsystem knows which session state to load later.\nFigure 14.1 Sequence diagram of the travel assistant with checkpoints saved after each node\nexecution, allowing state restoration across turns using a shared thread_id.\n",
      "content_length": 568,
      "extraction_method": "Direct"
    },
    {
      "page_number": 432,
      "chapter": null,
      "content": "Router Agent Agents Checkpointer\ncra re us er\n\nMessage\n(Turn 1)\nSave checkpoint\n(START — router)\na\nRoute +\nprompt\nrx— oc\nalt [Needs agent]\nAgent\nselection\nLp covccccccecccsccccscccccccccccdeccccoccccccoccseccossccocsccce 4)\nExecute\nselected agent\nSave checkpoint\n(after agent execution)\nAgent\nresults\nPee eee\nSynthesize\nanswer\nmi OOOO\nAssistant\nresponse\nMB ccc cnacdcscostackUsnsdsedcencndeconnccéecenussucecssasssusueus 9)\nSave checkpoint\n(router — END)\nOh\nFinal\nanswer\nanendereesssonseceeunnsenoucencenenas @\nMessage\n\n(Turn 2: “same town?”)\n\nLoad last checkpoint\nby thread_id\nOm\nContinue with prior\nstate (history rehydrated)\n\nAssistant\n\nSave checkpoint\n(after ag\n\nAgent Agents heckpoi\n(LangGraph) fetes oti 5 / LLM (thread_id)",
      "content_length": 729,
      "extraction_method": "OCR"
    },
    {
      "page_number": 433,
      "chapter": null,
      "content": "When you pass a thread ID to the graph on subsequent invocations,\nLangGraph retrieves the stored state from the last checkpoint in that thread\nand resumes execution from there—or, in a conversational context, uses it to\nprovide the LLM with the correct history.\n14.1.4 Adding Short-Term Memory to Our Travel Assistant\nTo demonstrate how LangGraph’s persistence and checkpointing work, we’ll\nenhance our router-based travel_assistant from section 12.2 with short-\nterm memory.\nFor clarity, we’ll walk through the changes step-by-step.\nStart by making a copy of your existing main_05_01.py and name it\nmain_08_01.py.\nWe’ll add persistence features to this copy so you can compare with the\noriginal.\nStep 1 — The Original Stateless Chat Loop\nFirst, let’s revisit the existing chat loop. In this version, each time the user\ninteracts, we only pass the new message as the state to travel_assistant. There\nis no concept of session or continuity—each turn is treated as completely\nseparate.\nListing 14.1 Original chat loop from router based agent solution\ndef chat_loop(): #A\n   print(\"UK Travel Assistant (type 'exit' to quit)\")\n   while True:\n       user_input = input(\"You: \").strip() #B\n       if user_input.lower() in {\"exit\", \"quit\"}: #C\n           break\n       state = {\"messages\": [HumanMessage(content=user_input)]} \n       result = travel_assistant.invoke(state) #E\n       response_msg = result[\"messages\"][-1] #F\n       print(f\"Assistant: {response_msg.content}\\n\") #G\n",
      "content_length": 1473,
      "extraction_method": "Direct"
    },
    {
      "page_number": 434,
      "chapter": null,
      "content": "In this approach, travel_assistant.invoke() receives only the state—a\nfresh message each time:\nstate = {\"messages\": [HumanMessage(content=user_input)]}\nresult = travel_assistant.invoke(state)\nStep 2 — Introducing a Thread ID\nTo persist state across turns, we need a way to uniquely identify the\nconversation.\nLangGraph does this using a thread ID, passed in a RunnableConfig.\nWe can generate one at the start of the chat loop:\nimport uuid\nthread_id = uuid.uuid1()\nconfig = {\"configurable\": {\"thread_id\": thread_id}}\nListing 14.2 shows the revised chat loop implementation, where the thread ID\nis created once, printed, and passed with every invoke() call\nListing 14.2 Revised chat loop implementation with thread_id\ndef chat_loop(): #A\n   thread_id=uuid.uuid1() #B\n   print(f'Thread ID: {thread_id}') \n   config={\"configurable\": {\"thread_id\": thread_id}} #B\n   print(\"UK Travel Assistant (type 'exit' to quit)\")\n   while True:\n       user_input = input(\"You: \").strip() #C\n       if user_input.lower() in {\"exit\", \"quit\"}: #D\n           break\n       state = {\"messages\": [HumanMessage(content=user_input)]} \n       result = travel_assistant.invoke(state, config=config) #F\n       response_msg = result[\"messages\"][-1] #G\n       print(f\"Assistant: {response_msg.content}\\n\") #H\nNow, instead of just passing state, we pass two arguments:\nresult = travel_assistant.invoke(state, config)\n",
      "content_length": 1384,
      "extraction_method": "Direct"
    },
    {
      "page_number": 435,
      "chapter": null,
      "content": "The config ensures that all checkpoints and state belong to this specific\nsession (thread_id).\nStep 3 — Adding a Checkpointer\nWith the thread ID in place, we can add an in-memory checkpointer to store\nstate snapshots.\nfrom langgraph.checkpoint.memory import InMemorySaver\ncheckpointer = InMemorySaver()\nNote\nInMemorySaver is ideal for development, testing, and quick proof-of-\nconcepts. In a production environment, you should use a persistent storage\nbackend to ensure state survives restarts and can be shared across multiple\napplication instances. LangGraph provides built-in options such as a\nSqliteSaver (from the langgraph-checkpoint-sqlite package, backed by\na SQLite database) and a PostgresSaver (from the langgraph-checkpoint-\npostgres package, backed by PostgreSQL). It also provide the async\nversions, SqliteSaverAsync and PostgresSaverAsync (from the same\npackages). For production deployments, the PostgreSQL-based checkpointer\nis generally recommended for its scalability, reliability, and concurrency\nsupport.\nWhen compiling the graph, pass the checkpointer, as shown in Listing 14.3.\nListing 14.3 Graph enriched with checkpointer\ngraph = StateGraph(AgentState) #A\ngraph.add_node(\"router_agent\", router_agent_node) #B\ngraph.add_node(\"travel_info_agent\", travel_info_agent) #C\ngraph.add_node(\"accommodation_booking_agent\", accommodation_booki\ngraph.add_edge(\"travel_info_agent\", END) #E\ngraph.add_edge(\"accommodation_booking_agent\", END) #F\ngraph.set_entry_point(\"router_agent\") #G\ncheckpointer = InMemorySaver() #H\n",
      "content_length": 1531,
      "extraction_method": "Direct"
    },
    {
      "page_number": 436,
      "chapter": null,
      "content": "travel_assistant = graph.compile(checkpointer=checkpointer) #I\nThis enables the graph to save a snapshot after every node execution, so it\ncan resume with full context on the next turn.\nStep 4 — Configuring the LLM for Conversation Continuity\nFinally, we need to configure the LLM to reference previous turns without\nresending the entire conversation history every time.\nOpenAI’s Responses API allows this using the use_previous_response_id\nflag.\nllm_model = ChatOpenAI(model=\"gpt-5\", #A\n                      use_responses_api=True, #B\n                      use_previous_response_id=True) #C\nWith use_previous_response_id=True, the LangChain ChatOpenAI\nwrapper sends only the ID of the previous response, and OpenAI rehydrates\nthe full history internally.\nImportant\nIf you enable use_responses_api=True but not\nuse_previous_response_id=True, LangChain will try to resend the full\nhistory on every turn. The Responses API treats this as a duplicate\nsubmission and returns an error. When using LangGraph memory with the\nResponses API, enabling use_previous_response_id is mandatory.\n14.1.5 Executing the Checkpointer-Enabled Assistant\nWith the checkpointer integrated into our travel_assistant, we can now\nsee short-term conversational memory in action.\nRun the updated router-based assistant and enter a typical question:\nThread ID: e683b337-752b-11f0-84a9-34f39a8d3195\nYou: What's the weather like in Penzance?\nAssistant: [{'type': 'text', 'text': 'The weather in Penzance is \n",
      "content_length": 1478,
      "extraction_method": "Direct"
    },
    {
      "page_number": 437,
      "chapter": null,
      "content": "Now, enter a follow-up question that refers to the previous turn:\nYou: What's the weather in the same town now?\nAssistant: [{'type': 'text', 'text': 'Current weather in Penzance\nNotice how the assistant correctly resolves “same town” to Penzance—it was\nable to do this because the LangGraph checkpointer supplied the LLM with\nthe entire conversation history, not just the latest user input.\nCongratulations—you’ve just implemented a stateful conversational chatbot!\nAlthough this is satisfying, it’s worth taking a deeper look at what’s\nhappening under the bonnet so you can understand exactly how the\ncheckpointer maintains short-term memory.\n14.1.6 Rewinding the State to a Past Checkpoint\nTo better understand how LangGraph manages conversational memory, we’ll\nsimulate what happens internally when restoring from a checkpoint. This\nexercise will:\n1. Ask the chatbot a question.\n2. Retrieve the last checkpoint from the checkpointer.\n3. Rehydrate the graph state to that checkpoint.\n4. Ask a follow-up question that depends on that restored context.\nThis is effectively what LangGraph does automatically when you pass the\nsame thread_id on subsequent turns.\nStep 1 — Updating the Chat Loop for State Inspection\nCreate a copy of main_08_01.py and name it main_08_02.py.\nReplace the chat_loop() with the implementation shown in listing 14.4.\nListing 14.4 Step-by-step state inspection\ndef chat_loop(): \n",
      "content_length": 1404,
      "extraction_method": "Direct"
    },
    {
      "page_number": 438,
      "chapter": null,
      "content": "   thread_id=uuid.uuid1() #A\n   print(f'Thread ID: {thread_id}') \n   config={\"configurable\": {\"thread_id\": thread_id}} #B\n   user_input = input(\"You: \").strip() #C\n   question = {\"messages\": [HumanMessage(content=user_input)]} #\n   result = travel_assistant.invoke(question, config=config) #E\n   response_msg = result[\"messages\"][-1] #F\n   print(f\"Assistant: {response_msg.content}\\n\") #G\n   state_history = travel_assistant.get_state_history(config) #H\n   state_history_list = list(state_history) #I\n   print(f'State history: {state_history_list}') #I\n   last_snapshot = list(state_history_list)[0] #J\n   print(f'Last snapshot: {last_snapshot.config}')\n   thread_id = last_snapshot.config[\"configurable\"][\"thread_id\"] \n   last_checkpoint_id = last_snapshot.config[\"configurable\"][\"ch\n   new_config = {\"configurable\": #M\n             {\"thread_id\": thread_id, \n              \"checkpoint_id\": last_checkpoint_id}}\n   \n   retrieved_snapshot = travel_assistant.get_state(new_config) #\n   print(f'Retrieved snapshot: {retrieved_snapshot}') #O\n   travel_assistant.invoke(None, config=new_config) #P\n   new_question = {\"messages\": [HumanMessage(content=\"What is th\n   result = travel_assistant.invoke(new_question, config=new_con\n   response_msg = result[\"messages\"][-1] #R\n   print(f\"Assistant: {response_msg.content}\\n\") #S\nStep 2 — Running the Example and Viewing State History\nRun main_08_02.py in debug mode and enter the usual question:\nYou: What's the weather like in Penzance?\nAssistant: [{'type': 'text', 'text': 'It’s currently foggy in Pen\nRetrieve the state history:\n",
      "content_length": 1572,
      "extraction_method": "Direct"
    },
    {
      "page_number": 439,
      "chapter": null,
      "content": "state_history = travel_assistant.get_state_history(config)\nstate_history_list = list(state_history)\nprint(f'State history: {state_history_list}')\nYou’ll see multiple StateSnapshot entries, each representing a checkpoint,\nstarting from the most recent and moving backwards to the initial START\nnode. Each snapshot contains:\nThe messages exchanged so far.\nTool call information (if any).\nMetadata about the graph execution step.\nStep 3 — Rehydrating from a Specific Checkpoint\nFrom the most recent snapshot:\nlast_snapshot = list(state_history_list)[0]\nprint(f'Last snapshot: {last_snapshot.config}')\nExtract the thread_id and checkpoint_id:\nthread_id = last_snapshot.config[\"configurable\"][\"thread_id\"]\nlast_checkpoint_id = last_snapshot.config[\"configurable\"][\"checkp\nBuild a new config pointing to that checkpoint:\nnew_config = {\"configurable\": \n             {\"thread_id\": thread_id, \n              \"checkpoint_id\": last_checkpoint_id}}\nRetrieve the state at this checkpoint to confirm it matches expectations:\nretrieved_snapshot = travel_assistant.get_state(new_config)\nprint(f'Retrieved snapshot: {retrieved_snapshot}')\nYou should see the full conversation history up to that checkpoint, including\nuser messages, tool outputs, and assistant responses.\nStep 4 — Resuming from the Restored State\n",
      "content_length": 1296,
      "extraction_method": "Direct"
    },
    {
      "page_number": 440,
      "chapter": null,
      "content": "To rewind the graph to that point:\ntravel_assistant.invoke(None, config=new_config)\nNow ask a follow-up that depends on that past context:\nnew_question = {\"messages\": [HumanMessage(content=\"What is the we\nresult = travel_assistant.invoke(new_question, config=new_config)\nresponse_msg = result[\"messages\"][-1]\nYou might see output like:\nAssistant: [{'type': 'text', 'text': 'In Penzance it’s currently w\nThe assistant correctly infers “same town” as Penzance, confirming that the\nrehydrated state was passed back to the LLM.\nBy stepping through this manual rewind, you now have a low-level\nunderstanding of how LangGraph’s checkpointer powers short-term\nconversational memory—storing the entire execution context at each node,\nand restoring it later to continue exactly where the conversation left off.\n14.2 Guardrails\nGuardrails are application-level mechanisms that keep an AI agent operating\nwithin a defined scope, policy framework, and intended purpose. They serve\nas the “rules of the road” for agent behavior, inspecting and validating both\ninputs and outputs at critical points to ensure the system stays safe, relevant,\ncompliant, and efficient. Without guardrails, an agent might drift into\nunrelated subject matter, produce unsafe or non-compliant responses, or\nwaste processing resources on unnecessary actions.\nA strong guardrail design can stop a travel assistant from giving stock market\ntips, prevent a support bot from revealing confidential information, or block\npoorly formed requests before they reach an expensive language model. In\npractice, guardrails often fall into three broad categories:\nRule-based — explicit conditions or regular expressions that catch\nprohibited patterns or topics.\n",
      "content_length": 1712,
      "extraction_method": "Direct"
    },
    {
      "page_number": 441,
      "chapter": null,
      "content": "Retrieval-based — checks against approved data sources to confirm that\na request is relevant and in scope.\nModel-based — compact classification or moderation models that assess\nintent, safety, or adherence to policy.\nThese controls can be introduced at multiple points in the agent workflow:\nPre-model checks — reject invalid or irrelevant queries before the LLM\nruns.\nPost-model checks — verify generated responses against policy or safety\nguidelines.\nRouting-stage checks — decide whether a query should trigger certain\ntools or branches.\nTool-level checks — block unsafe or unauthorized tool actions.\nFunctionally, guardrails act like validation layers—if a check fails, the\nagent’s normal flow is adjusted, whether by refusing the request, asking for\nclarification, or redirecting to a safer response path.\nIn this section, we’ll integrate custom guardrails into the router node of the\ntravel information assistant we built earlier (now enhanced with memory) so\nit can immediately reject irrelevant or out-of-scope requests—such as non-\ntravel queries. We’ll also explore LangGraph’s pre-model hook, which lets us\nenforce guardrails before any LLM call, ensuring that both the travel and\nweather agents remain within coverage boundaries—for example, only\nhandling destinations in Cornwall.\n14.2.1 Implementing Guardrails to Reject Non-Travel-Related\nQuestions\nThe first and most obvious guardrail for our UK travel information assistant\nis a domain relevance check—a pre-filter that screens the user’s question\nbefore any agent reasoning occurs. If the question falls outside the assistant’s\nremit, we intercept it early and politely refuse to answer. This prevents the\nsystem from attempting to handle queries about unrelated topics such as\nsports results, financial markets, or celebrity gossip.\n",
      "content_length": 1802,
      "extraction_method": "Direct"
    },
    {
      "page_number": 442,
      "chapter": null,
      "content": "Introducing this guardrail delivers two key benefits:\n1. Improved Accuracy — Our agents are trained and configured only for\ntravel and weather information. If we attempt to answer unrelated\nquestions, the results will almost certainly contain inaccuracies or\nhallucinations. By rejecting irrelevant queries outright, we keep the\nconversation aligned with the agent’s actual capabilities.\n2. Cost Control — Without this filter, some users might deliberately use\nour assistant as a free gateway to an expensive LLM, bypassing\nsubscription costs for unrelated questions. By blocking non-travel topics\nearly, we prevent this form of resource abuse and avoid unnecessary\nprocessing costs.\nDefining the Guardrail Policy\nOur first task is to clearly define what qualifies as in-scope for this assistant.\nThis ensures the guardrail has unambiguous decision criteria.\nTo begin, make a copy of the main_08_01.py script from the previous section\nand save it as main_09_01.py. Then, implement the guardrail policy as shown\nin Listing 14.5.\nListing 14.5 Guardrail policy to restrict questions to travel-related topics\nclass GuardrailDecision(BaseModel): #A\n   is_travel: bool = Field(\n       ...,\n       description=(\n           \"True if the user question is about travel informatio\n           \"lodging (hotels/BnBs), prices, availability, or weat\n       ),\n   )\n   reason: str = Field(..., description=\"Brief justification for \nGUARDRAIL_SYSTEM_PROMPT = ( #B\n   \"You are a strict classifier. Given the user's last message, \n   \"travel-related. Travel-related queries include destinations, \n   \"room availability, prices, or weather in Cornwall/England.\"\n)\nREFUSAL_INSTRUCTION = ( #C\n",
      "content_length": 1671,
      "extraction_method": "Direct"
    },
    {
      "page_number": 443,
      "chapter": null,
      "content": "   \"You can only help with travel-related questions (destination\n   \"availability, or weather in Cornwall/England). The user's re\n   \"Politely refuse and briefly explain what topics you can help \n)\nllm_guardrail = llm_model.with_structured_output(GuardrailDecisio\nThis snippet defines a guardrail policy that uses a lightweight LLM classifier\nto determine whether a user’s question is travel-related.\nGuardrailDecision is a Pydantic model that structures the classification\noutput. The is_travel flag indicates whether the request falls within the\ntravel domain, and reason provides a brief justification for the decision.\nGUARDRAIL_SYSTEM_PROMPT instructs the model to classify strictly, giving a\nprecise definition of “travel-related” for our purposes.\nREFUSAL_INSTRUCTION contains a fixed, polite message to explain to the user\nwhy their question can’t be answered.\nllm_guardrail wraps the base LLM with structured output formatting,\nenabling fast, consistent decision-making before the main routing logic runs.\nUpdating the Router Graph\nAt a high level, we want irrelevant queries to exit the workflow immediately\n—without touching any of the downstream agents. That means introducing a\ndedicated guardrail_refusal node in our LangGraph routing structure. This\nnode simply redirects to the graph’s END without performing any work.\nThe updated workflow is shown in Figure 14.2.\nFigure 14.2 Updated travel assistant workflow: A guardrail checks if the user’s question is in\nscope. Irrelevant queries are routed to guardrail_refusal, which sends them directly to END,\nbypassing all other agents.\n",
      "content_length": 1597,
      "extraction_method": "Direct"
    },
    {
      "page_number": 444,
      "chapter": null,
      "content": "The router agent now checks whether a question is in scope. If it is, the query\nis routed to either the travel information agent or the weather agent as before.\nIf it isn’t, the router sends it to the new guardrail_refusal node, which is\nlinked directly to the END node.\nThe corresponding graph definition is shown in Listing 14.6.\n",
      "content_length": 332,
      "extraction_method": "Direct"
    },
    {
      "page_number": 445,
      "chapter": null,
      "content": "Listing 14.6 Adding a guardrail_refusal node to the router graph\ndef guardrail_refusal_node(state: AgentState): #A\n   return {}\ngraph = StateGraph(AgentState) \ngraph.add_node(\"router_agent\", router_agent_node) \ngraph.add_node(\"travel_info_agent\", travel_info_agent) \ngraph.add_node(\"accommodation_booking_agent\", accommodation_booki\ngraph.add_node(\"guardrail_refusal\", guardrail_refusal_node) #B\ngraph.add_edge(\"travel_info_agent\", END) \ngraph.add_edge(\"accommodation_booking_agent\", END) \ngraph.add_edge(\"guardrail_refusal\", END) #C\ngraph.set_entry_point(\"router_agent\") \ncheckpointer = InMemorySaver() \ntravel_assistant = graph.compile(checkpointer=checkpointer) \nAs you can see, the guardrail_refusal node is intentionally a no-op—its\nonly role is to create a clean shortcut to END.\nUpdating the Router Agent\nWith the guardrail policy defined and the graph updated, the final step is to\nimplement the logic that enforces it. Previously, our router agent only\ndecided whether to send a query to the travel or weather agent. Now it must\nfirst run the guardrail check, as illustrated in the diagram of figure 14.3.\nFigure 14.3 Flowchart of updated router logic with guardrail check: queries first pass a relevance\nfilter, and irrelevant ones are sent with a refusal message directly to the guardrail_refusal node.\n",
      "content_length": 1314,
      "extraction_method": "Direct"
    },
    {
      "page_number": 446,
      "chapter": null,
      "content": "Incoming user query\n\nRouter guardrail\n(is travel-related?)\n\nRouter agent\n(classify: travel vs\nweather)\n\nguardrail_refusal node\n(no-op)\n\nTravel info Weather info",
      "content_length": 160,
      "extraction_method": "OCR"
    },
    {
      "page_number": 447,
      "chapter": null,
      "content": "If the guardrail check fails—meaning the question is judged irrelevant to the\ntravel assistant—the router generates a refusal message and routes execution\ndirectly to the guardrail_refusal node, as shown in Figure 14.3.\nThis logic is implemented in Listing 14.7.\nListing 14.7 Router agent with guardrail enforcement\ndef router_agent_node(state: AgentState) -> Command[AgentType]:\n   \"\"\"Router node: decides which agent should handle the user qu\n   messages = state[\"messages\"] \n   last_msg = messages[-1] if messages else None \n   if isinstance(last_msg, HumanMessage):\n       user_input = last_msg.content \n       classifier_messages = [ \n           SystemMessage(content=GUARDRAIL_SYSTEM_PROMPT), #A\n           HumanMessage(content=user_input),\n       ]\n       decision = llm_guardrail.invoke(classifier_messages) #B\n       if not decision.is_travel: #C\n           refusal_text = ( #D\n               \"Sorry, I can only help with travel-related quest\n               \"lodging, prices, availability, or weather in Cor\n               \"Please rephrase your request to be travel-relate\n           )\n           return Command( #E\n               update={\"messages\": [AIMessage(content=refusal_te\n               goto=\"guardrail_refusal\",\n           ) \n       router_messages = [ \n           SystemMessage(content=ROUTER_SYSTEM_PROMPT),\n           HumanMessage(content=user_input)\n       ]\n       router_response = llm_router.invoke(router_messages) \n       agent_name = router_response.agent.value \n       return Command(update=state, goto=agent_name) \n   \n   return Command(update=state, goto=AgentType.travel_info_agent\nIn this revised logic:\nThe router first invokes llm_guardrail with the latest user query.\n",
      "content_length": 1705,
      "extraction_method": "Direct"
    },
    {
      "page_number": 448,
      "chapter": null,
      "content": "If the classification result indicates the question is not travel-related, the\nrouter constructs a fixed refusal message, stores it in the state, and sends\nexecution to the guardrail_refusal node—bypassing all normal routing.\nIf the check passes, the router continues with its usual process of\nselecting the most appropriate agent.\nTesting the Guardrail\nTo see it in action, run main_09_01.py in debug mode and place a breakpoint\non the line where llm_guardrail is invoked. Then enter the following query:\nUK Travel Assistant (type 'exit' to quit)\nYou: Can you give me the latest results of Inter Milan?\nWhen execution stops at the breakpoint, inspect decision.is_travel. You\nshould see False, since football scores are outside the allowed travel domain.\nResuming execution (F5) will produce:\nAssistant: Sorry, I can only help with travel-related questions (\nCongratulations—you’ve successfully implemented your first guardrail!\nHowever, our work isn’t done. Remember, one of our agents only has\ncoverage for Cornwall, not the whole of the UK. This means we’ll want to\nimplement further scope restrictions at the agent level, which we’ll tackle in\nthe next section.\n14.2.2 Implementing More Restrictive Guardrails at Agent\nLevel\nIn traditional software development, it’s considered best practice for each\nclass or component to validate its own data rather than relying solely on\nvalidations at higher levels such as the UI. The same principle applies to\nagent-based systems: each agent should enforce its own input guardrails,\neven if broader checks are already in place at the chatbot entry point.\nThese agent-level guardrails are often more restrictive than system-wide\nones, because they can account for the specific capabilities and data scope of\n",
      "content_length": 1751,
      "extraction_method": "Direct"
    },
    {
      "page_number": 449,
      "chapter": null,
      "content": "the individual agent.\nIn our case:\nThe travel information agent can only handle queries about Cornwall,\nbecause its vector store contains data exclusively from that region.\nThe accommodation booking agent will also be limited to Cornwall for\nnow, to keep the assistant’s scope consistent.\nWhy have two levels of guardrails?\nRouter-level guardrail — Acts as an early fail-fast filter before any\nagent logic or tool invocation.\nAgent-level guardrails — Provide a “belt-and-suspenders” safeguard to\ncatch out-of-scope requests if the agent is ever called directly or reused\nin a different context.\nDefining the Cornwall-Restricted Guardrail Policy\nWe start by explicitly defining the policy: only travel-related questions about\nCornwall are permitted. This ensures that both travel and accommodation\nbooking agents reject queries for other regions or countries.\nMake a copy of main_09_01.py and rename it main_09_02.py. Then add the\nfollowing system prompts to define the classification and refusal behavior.\nListing 14.8 System prompts for classification and refusal behavior\nAGENT_GUARDRAIL_SYSTEM_PROMPT = ( \n   \"You are a strict classifier. Given the user's last message, \n   \"travel-related. Travel-related queries include destinations, \n   \"room availability, prices, or weather in Cornwall/England.\"\n   \"Only accept travel-related questions covering Cornwall (Engl\n   \"from other areas in England and from other countries\"\n)\nAGENT_REFUSAL_INSTRUCTION = ( \n   \"You can only help with travel-related questions (destination\n   \"availability, or weather in Cornwall/England). The user's re\n   \"Or it might be a travel related question but not focusing on \n",
      "content_length": 1656,
      "extraction_method": "Direct"
    },
    {
      "page_number": 450,
      "chapter": null,
      "content": "   \"Politely refuse and briefly explain what topics you can help \n)\nCreating the Agent-Level Guardrail Function\nThe agent guardrail is implemented as a Python function that takes the\ncurrent graph state and returns either an unchanged state (for valid input) or\none modified to instruct the LLM to issue a refusal.\nListing 14.9 Agent-level guardrail function\ndef pre_model_guardrail(state: dict):\n   messages = state.get(\"messages\", [])\n   last_msg = messages[-1] if messages else None\n   if not isinstance(last_msg, HumanMessage): #A\n       return {}\n   user_input = last_msg.content\n   classifier_messages = [ #B\n       SystemMessage(content=AGENT_GUARDRAIL_SYSTEM_PROMPT),\n       HumanMessage(content=user_input),\n   ]\n   decision = llm_guardrail.invoke(classifier_messages)\n   if decision.is_travel: #C\n       # Allow normal flow; do not modify inputs\n       return {}\n       return {\"llm_input_messages\": [SystemMessage(content=AGEN\nThe pre_model_guardrail() function works as a pre-processing filter before\nthe LLM sees the user’s query:\nIt verifies that the latest message is indeed from the user.\nIt sends the query, along with a strict classification system prompt, to\nthe guardrail LLM.\nIf the query is in-scope (travel-related and Cornwall-specific), it passes\nthrough unchanged.\nOtherwise, the function prepends a refusal instruction so the agent\npolitely declines the request.\n",
      "content_length": 1390,
      "extraction_method": "Direct"
    },
    {
      "page_number": 451,
      "chapter": null,
      "content": "Injecting the Guardrail into the Agents\nLangGraph’s ReAct agents support pre-model hooks (pre_model_hook) and\npost-model hooks (post_model_hook), allowing you to intercept and\nmanipulate inputs or outputs. While these hooks can be used for tasks like\nsummarizing long inputs or sanitizing outputs, here we’ll focus solely on\ninput-side guardrails.\nTo enable the Cornwall restriction, we simply pass pre_model_guardrail to\nboth the travel information agent and the accommodation booking agent.\nListing 14.10 Travel information agent with Cornwall guardrail\ntravel_info_agent = create_react_agent(\n   model=llm_model,\n   tools=TOOLS,\n   state_schema=AgentState,\n   prompt=\"You are a helpful assistant that can search travel in\n   pre_model_hook=pre_model_guardrail, #A\n)\nListing 14.11 Accommodation booking agent with Cornwall guardrail\naccommodation_booking_agent = create_react_agent( #A\n   model=llm_model,\n   tools=BOOKING_TOOLS,\n   state_schema=AgentState,\n   prompt=\"You are a helpful assistant that can check hotel and \n   pre_model_hook=pre_model_guardrail,\n)\nTesting the Cornwall Guardrail\nRun main_09_02.py in debug mode, placing a breakpoint on the\nllm_guardrail invocation inside pre_model_guardrail(). Then try:\nUK Travel Assistant (type 'exit' to quit)\nYou: Can you give me some travel tips for Liverpool (UK)?\nWhen paused at the breakpoint, inspect decision.is_travel—it should be False\nbecause the query is not Cornwall-specific. Execution will then prepend the\n",
      "content_length": 1476,
      "extraction_method": "Direct"
    },
    {
      "page_number": 452,
      "chapter": null,
      "content": "refusal instruction, resulting in output like:\nAssistant: [{'type': 'text', 'text': 'Sorry—I can only help with \nWith this, we now have two layers of defense:\n1. A router-level guardrail that quickly rejects any non-travel queries.\n2. Agent-level guardrails that enforce Cornwall-specific scope for travel\nand accommodation requests.\nOur entire agentic workflow is now protected from both irrelevant and out-\nof-coverage queries, making the system safer, more reliable, and more cost-\nefficient.\n14.3 Beyond this chapter\nBy this point, you’ve seen how to equip AI agents with memory and\nguardrails—two of the most critical building blocks for making them\nproduction-ready. But depending on your application’s domain, scale, and\ncompliance requirements, there are additional considerations you may need to\naddress before deploying to real users.\nSome of these areas have been mentioned in passing throughout the book but\nnot explored in depth, either because they are highly domain-specific or\nbecause they deserve their own dedicated treatment. Below are a few\ndirections worth exploring further.\n14.3.1 Long-term user and application memory\nIn this chapter, we focused on short- to medium-term memory—keeping\ntrack of relevant state within a single conversation or across a limited session\nhistory.\nHowever, production agents often benefit from persistent, long-term memory\nthat stores user preferences, past interactions, and contextual information\nacross weeks, months, or even years. This might involve:\nDedicated vector stores for each user.\n",
      "content_length": 1547,
      "extraction_method": "Direct"
    },
    {
      "page_number": 453,
      "chapter": null,
      "content": "Periodic summarization and pruning to keep memory manageable.\nPrivacy and compliance controls for personally identifiable information\n(PII).\nLong-term memory can dramatically improve personalization, but it also\nbrings engineering, scaling, and regulatory challenges you’ll need to\nplan for.\nTable 14.1 summarizes the various types of memory your AI agent-based\nsystem might need to accommodate user needs.\nTable 14.1 Type of memory in AI agents\nMemory\ntype\nScope\nPersistence\nExample\n(Travel\nAssistant)\nChallenges\nShort-term\nSingle user\nsession\nUntil\nsession ends\nRemembering\n“same town” in\na weather\nfollow-up\nwithin one\nconversation\nLimited\ncontinuity;\nlost when\nsession\ncloses\nLong-term\n(user)\nAcross\nmultiple\nsessions for\none user\nWeeks,\nmonths, or\nyears\nRemembering a\nuser’s preferred\nCornwall\ndestinations or\naccommodation\ntypes\nPrivacy\ncompliance\nLong-term\n(application)\nAcross all\nusers and\nsessions\nOngoing\nStoring general\ntravel updates\n(e.g., Cornwall\nevent calendar,\nseasonal\nattraction\nschedules)\nKeeping\ndata fresh;\navoiding\noutdated or\nincorrect\ninfo\n14.3.2 Human-in-the-loop (HITL)\n",
      "content_length": 1098,
      "extraction_method": "Direct"
    },
    {
      "page_number": 454,
      "chapter": null,
      "content": "Even a well-designed travel information assistant will encounter situations\nwhere automated handling is not enough—cases where the query is\nambiguous, the available data is incomplete, or a decision could have a\nsignificant real-world impact. A human-in-the-loop approach allows such\nrequests to be escalated to a human travel expert for review before a response\nis sent.\nFor our Cornwall-focused travel assistant, common HITL scenarios could\ninclude:\nRequests for personalized itineraries involving unusual combinations of\nactivities where safety, feasibility, or timing is uncertain.\nQueries about real-time disruptions—such as severe weather, transport\nstrikes, or event cancellations—where up-to-date human judgment is\nneeded.\nSpecial accommodation or accessibility requests that require confirming\ndetails with local providers.\nIn early production deployments, HITL is especially valuable: it helps ensure\naccuracy, prevents reputational damage, and provides real-world feedback to\nrefine the assistant’s automated policies. Over time, insights from human\nreviews can be incorporated into improved guardrails, better prompts, or\nexpanded knowledge bases—reducing the number of cases that need\nescalation.\n14.3.3 Post-model guardrails\nIn this chapter, we focused on guardrails that run before invoking the LLM—\nscreening queries for relevance and redirecting out-of-scope requests. In a\nproduction travel information assistant, you may also want post-model\nguardrails that inspect the model’s output before it is shown to the user or\nsent to a downstream service (such as a booking API).\nFor our Cornwall-focused assistant, post-model guardrails could include:\nFiltering outdated or incorrect details—for example, removing\nreferences to attractions that have permanently closed or events that\nhave already passed.\n",
      "content_length": 1818,
      "extraction_method": "Direct"
    },
    {
      "page_number": 455,
      "chapter": null,
      "content": "Redacting sensitive information—such as private contact numbers for\nsmall BnBs that should only be shared after confirmed bookings.\nEnforcing brand tone and style—ensuring that all travel advice is given\nin a warm, welcoming, and concise manner consistent with the\nassistant’s persona.\nVerifying structured output—making sure any booking\nrecommendations, price quotes, or itineraries follow the correct format\nexpected by other systems.\nPost-model guardrails act as a final safety net, catching cases where the\nLLM’s answer might look reasonable but contains factual errors, tone\nmismatches, or details that are inappropriate for immediate user delivery.\n14.3.4 Evaluation of AI agents and applications\nOne of the most overlooked—but absolutely essential—steps in preparing an\nagent for production is systematic evaluation. This is a broad and evolving\ndiscipline, complex enough to deserve an entire book of its own, but it is\ncritical for ensuring that your travel assistant remains accurate, safe, and\nefficient once deployed.\nFor our Cornwall-focused travel information assistant, evaluation might\ninclude:\nFunctional testing – Verifying that the assistant provides correct,\nrelevant, and complete answers across a wide set of test queries, such as\n“Best family-friendly beaches in Cornwall” or “Current weather forecast\nfor St Ives.” This ensures it stays within scope and retrieves accurate,\nup-to-date information.\nBehavioral testing – Confirming that the assistant follows policy and\nsafety rules, avoids giving irrelevant or unsafe travel advice, and\nmaintains a consistent, friendly tone suitable for tourism and customer\nservice.\nPerformance testing – Measuring latency and API costs under realistic\nuser loads, such as peak summer tourist season when requests might\nsurge.\nRegression testing – Ensuring the assistant’s reliability is not\n",
      "content_length": 1850,
      "extraction_method": "Direct"
    },
    {
      "page_number": 456,
      "chapter": null,
      "content": "compromised when prompts are refined, tools are updated, or the\nunderlying LLM is replaced.\nWhile this book does not cover evaluation frameworks and methodologies in\ndetail, you should treat evaluation as a core part of your pre-production\nchecklist and as an ongoing process after launch. Continuous evaluation helps\nyou catch issues before users do, adapt to changes in local events or services,\nand maintain the trustworthiness of your travel assistant over time.\nEquipping your agents with memory and guardrails is an important\nmilestone, but it’s only part of the journey. True production readiness\nrequires a holistic approach that includes safety, compliance, reliability, and\ncontinuous evaluation. The good news is that the foundations you’ve built in\nthis chapter will make it much easier to layer in these additional capabilities\nas your applications grow in scope and complexity.\n14.3.5 Deployment on LangGraph Platform and Open Agent\nPlatform (OAP)\nThe final step in preparing agents for production is deployment. How you\ndeploy depends heavily on your organization’s infrastructure strategy—\nwhether applications run on-premises or in the cloud, and on IT policies\naround privacy, security, and compliance. It also reflects organizational\nchoices: some teams favor local DevOps and SRE-driven deployments, while\nothers rely on SaaS-based hosting for simplicity and scalability.\nOnce you’ve developed your LangGraph-based multi-agent system, a natural\npath is to deploy it on the LangGraph Platform. This is LangChain’s fully\nmanaged hosting solution for agentic applications. It provides built-in\nfeatures such as horizontal scalability, persistent state management, and end-\nto-end monitoring through the familiar LangSmith dashboard. The platform\nabstracts away much of the operational overhead, letting you move from\nprototype to production with minimal friction while still retaining\nobservability and fine-grained debugging tools.\nAnother powerful option is to integrate your agents into the Agent Open\nPlatform (OAP). OAP is a flexible runtime and orchestration layer for AI\n",
      "content_length": 2095,
      "extraction_method": "Direct"
    },
    {
      "page_number": 457,
      "chapter": null,
      "content": "agents that comes with a set of prebuilt agent patterns—including the multi-\ntool agent and the supervisor agent. These can be customized and extended to\nfit enterprise use cases, whether by plugging into MCP servers, local vector\nstores, or other enterprise data sources. OAP is designed to act as a bridge\nbetween bespoke LangGraph agents and a broader ecosystem of composable,\ninteroperable agents, making it especially valuable for organizations planning\nto run multiple agents in coordination.\nBoth LangGraph Platform and OAP are available as fully managed SaaS\nofferings, but can also be deployed into a client’s own cloud environment for\nteams that need to maintain tighter control over data residency and\ncompliance. This dual deployment model means you can start quickly with\nmanaged hosting and later migrate to a private setup if regulatory or\noperational needs demand it.\nTogether, these deployment paths provide a smooth continuum—from\ndevelopment on your laptop, to scalable production hosting, to enterprise-\nwide agent orchestration—allowing you to choose the right trade-off between\ncontrol, convenience, and operational complexity.\n14.4 Summary\nMemory enables natural conversation — Short-term conversational\nmemory preserves context across turns, allowing the assistant to handle\nfollow-up questions without repeating information.\nCheckpoints store full execution state — LangGraph saves snapshots\nafter each node, enabling both conversation continuity and recovery\nfrom failures.\nGuardrails maintain scope and safety — They filter or redirect requests\nthat fall outside the assistant’s domain or policy guidelines.\nMultiple guardrail layers are valuable — Use router-level checks for fast\nrejection and agent-level checks for specific data or domain constraints.\nPost-model guardrails act as a final safety net — They verify output\ncorrectness, tone, and format before the user sees it or a downstream\nsystem processes it.\nHuman-in-the-loop improves reliability — Escalating certain cases to\nhuman review can prevent costly mistakes and provide training data for\n",
      "content_length": 2083,
      "extraction_method": "Direct"
    },
    {
      "page_number": 458,
      "chapter": null,
      "content": "future automation.\nEvaluation is ongoing, not one-off — Regular functional, behavioral,\nperformance, and regression testing ensures the assistant remains\ntrustworthy over time.\nProduction readiness is holistic — Combining memory, guardrails,\nhuman review, and evaluation sets the foundation for safe, effective, and\nscalable AI agents.\n",
      "content_length": 336,
      "extraction_method": "Direct"
    },
    {
      "page_number": 459,
      "chapter": null,
      "content": "Appendix A. Trying out LangChain\nIn this appendix, you will get a first hands-on experience with LangChain by\nperforming simple sentence completions and experimenting with basic\nprompt engineering in a Jupyter Notebook.\n",
      "content_length": 220,
      "extraction_method": "Direct"
    },
    {
      "page_number": 460,
      "chapter": null,
      "content": "A.1 Trying out LangChain in a Jupyter Notebook\nenvironment\nWe’ll begin with a straightforward example: completing a sentence using an\nOpenAI model and refining its output through prompt engineering. OpenAI’s\nmodels are a convenient starting point because they are accessible through a\npublic REST API and require no local infrastructure setup.\nIf you prefer, you can use an open-source LLM inference engine such as\nOllama. However, since many readers may not yet have run an open-source\nLLM locally, we will cover that setup in detail in Appendix E. For now, we\nwill keep things simple and use the OpenAI REST API.\nBefore you proceed, ensure that you have Python 3.11 or later installed on\nyour local machine and that the following prerequisites are met:\n1. Already own or generate an OpenAI key.\n2. Know how to set up a Python Jupyter notebook environment.\nIf you're unfamiliar with these tasks, check the sidebar for guidance on\n\"Creating an OpenAI key\" and refer to Appendix B for instructions on setting\nup a Jupyter notebook environment. Since most readers use Windows, I'll be\nproviding instructions specifically for Windows and a Python virtual\nenvironment. If you're using Linux or Anaconda, I assume you're advanced\nenough to adapt these instructions to your setup. If you prefer, you can also\nrun these examples in an online notebook environment like Google Colab, as\nlong as you install the required packages.\nCreating an OpenAI key\nAssuming you've registered with OpenAI, which is necessary to explore the\nChatGPT examples discussed at the beginning of the chapter:\nLog in to your OpenAI account at https://platform.openai.com/ and\nnavigate to the API section.\nAccess the API Keys by clicking your profile icon > Your profile > User\n",
      "content_length": 1745,
      "extraction_method": "Direct"
    },
    {
      "page_number": 461,
      "chapter": null,
      "content": "API key (tab).\nCreate a new secret key by clicking the \"Create Secret Key\" button,\nnaming it (e.g., BuildingLLMApps), and confirming.\nSafely save the key, for example in a secure local file or a password\nvault tool, as retrieval is not possible later.\nAdditionally, set a spending limit (e.g., $10) to control costs. Configure this\nunder Settings > Limits in the left-hand menu based on your usage\npreferences.\nTo establish the virtual environment for the code in this chapter, follow the\nsteps below. Open a new terminal in your operating system, create a folder\nlike c:\\Github\\building-llm-applications\\ch01 navigate into it, and\nexecute:\nC:\\>cd Github\\building-llm-applications\\ch01\nC:\\Github\\building-llm-applications\\ch01>python -m venv env_ch01\nC:\\Github\\building-llm-applications\\ch01>.\\env_ch01\\Scripts\\activ\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>\nNOTE\nI am running these commands on a Windows cmd shell. If you are on a Linux\ncomputer, you might have to adapt things slightly. For example, you can\nactivate a virtual environment with: ./env_ch01/bin/activate or\n./env_ch01/Scripts/activate. If you are using Powershell, you should use\nActivate.ps1.\nNow install the notebook, langchain (and indirectly openai) packages:\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>pip install n\nNote\nAlternatively, if you have cloned the Github code from the repo associated\nwith this book, you can install all the packages in this way:\n",
      "content_length": 1450,
      "extraction_method": "Direct"
    },
    {
      "page_number": 462,
      "chapter": null,
      "content": "(env_ch01) C:\\Github\\building-llm-applications\\ch01>pip install -r\nrequirements.txt\nOnce the installation is complete, start up a Jupyter notebook:\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>jupyter noteb\nCreate a notebook with: File > New > Notebook, then rename it with File >\nRename … to 01-langchain_examples.ipynb.\nIf you have cloned the Github repository and want to use the notebook\ndirectly, start it up as follows:\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>jupyter noteb\nA.1.1 Sentence completion example\nNow, you are ready to execute LangChain code in the notebook. Start by\nimporting the LangChain library and configuring an OpenAI Language\nModel (LLM) instance.\nIn the initial cell of your notebook, start by importing the necessary libraries,\nby running the cell:\nfrom langchain_openai import ChatOpenAI\nimport getpass\nNOTE\nIf you are unfamiliar with Jupyter notebooks, you can run a cell by pressing\nShift+Enter or clicking the play button located next to the top menu.\nNow add and execute a cell to grab your OpenAI API key (just hit Enter after\ninserting the key):\nOPENAI_API_KEY = getpass.getpass('Enter your OPENAI_API_KEY')\nSetting the OpenAI API key with an environment variable\n",
      "content_length": 1218,
      "extraction_method": "Direct"
    },
    {
      "page_number": 463,
      "chapter": null,
      "content": "Another way to set the OpenAI key is by using an environment variable. For\nexample, in Windows, you can set it in the command shell like this, before\nlaunching the notebook:\nset OPENAI_API_KEY=your_openai_api_key\nThen, in your Python code, retrieve it with (make sure you import os):\napi_key = os.getenv(\"OPENAI_API_KEY\")\nSince this method depends on the operating system and some readers may not\nbe familiar with it, I'll focus on the previous approach. However, if you're\ncomfortable using environment variables, feel free to use them.\nOnce you have entered your OpenAI key, add and execute this cell:\nllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,\n                model_name=\"gpt-4o-mini\")\nllm.invoke(\"It's a hot day, I would like to go to the…\")\nYou will see output similar to the example below, which represents a return\nmessage from the LLM:\nAIMessage(content=\"…beach to cool off and relax in the refreshing \nAs you can see in the content property, the completion generated by the\nLLM is \"…beach to cool off [...]\".\nA.1.2 Prompt engineering examples\nI've mentioned prompts several times, but I haven't shown you any examples\nyet. A prompt is the instruction you give the LLM to complete a task and\ngenerate a response. It's a key part of any LLM application, so much so that\ndeveloping LLM applications often involves spending a lot of time designing\nand refining prompts through trial and error. Various patterns and techniques\nare already forming around prompts, leading to the emergence of a discipline\ncalled prompt engineering. This field focuses on crafting prompts that yield\nthe best possible answers. I'll dedicate the next chapter to teaching you the\nfundamentals of prompt engineering. For now, let's start with a\n",
      "content_length": 1732,
      "extraction_method": "Direct"
    },
    {
      "page_number": 464,
      "chapter": null,
      "content": "straightforward prompt.:\nprompt_input = \"\"\"Write a short message to remind users to be \nvigilant about phishing attacks.\"\"\"\nresponse = llm.invoke(prompt_input)\nprint(response.content)\nOutput:\nJust a friendly reminder to stay vigilant against phishing attack\nPrompt template\nIn Chapter 2 on prompt engineering, you'll learn about prompt templates.\nThey're structured prompts that allow you to run various versions of the same\ntheme. LangChain offers a class called PromptTemplate for this. Its job is to\ngenerate prompts from template structures and input parameters. Below is an\nexample of how to create and execute a prompt from a template, which you\ncan place in a single notebook cell:\nListing 1.1 Creating a prompt from a PromptTemplate\nfrom langchain_core.prompts import PromptTemplate\nsegovia_aqueduct_text = \"The Aqueduct of Segovia (Spanish: Acuedu\nprompt_template = PromptTemplate.from_template(\"You are an experi\nprompt_input = prompt_template.format(\n   text=segovia_aqueduct_text, \n   num_words=20, \n   tone=\"knowledgeable and engaging\")\nresponse = llm.invoke(prompt_input)\nprint(response.content)\nWhen executing the code above you should get this output or something\nsimilar:\nThe Aqueduct of Segovia, a Roman marvel in Spain, dates back to t\nIn the next chapter, I'll show you how to replicate the examples we've just\n",
      "content_length": 1331,
      "extraction_method": "Direct"
    },
    {
      "page_number": 465,
      "chapter": null,
      "content": "implemented in LangChain using the plain OpenAI REST API, so you can\nsee the differences.\nA.1.3 Creating chains and executing them with LCEL\nOne of the benefits of using LangChain is its processing technique built\naround the concept of a \"chain.\" A chain is a pipeline of components put\ntogether to achieve a particular outcome. For example, to illustrate (but do\nnot execute this code snippet), you could create a chain to scrape the latest\nnews from a website, summarize it, and email it to someone, like this:\nchain = web_scraping | prompt | llm_model | email_text\nThis declarative, intuitive, and readable method of defining a chain\nshowcases the LangChain Expression Language (LCEL), which I'll cover\nextensively in a later chapter. For now, let's walk through an example by\nreimplementing the previous summarization task using LCEL.\nFirst, set up the chain in a new notebook cell as follows:\nprompt_template = PromptTemplate.from_template(\"You are an experi\nllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,\n                model_name=\"gpt-4o-mini\")\nchain = prompt_template | llm\nNow, this chain is ready to accept any text, target number of words, and\ntarget tone. Execute it as shown below:\nresponse = chain.invoke({\"text\": segovia_aqueduct_text, \n             \"num_words\": 20, \n             \"tone\": \"knowledgeable and engaging\"})\nprint(response.content)\nYou'll get this output, or similar:\nThe Aqueduct of Segovia: A Roman marvel channeling water to the c\nAs you can see, this way of setting up and executing the processing is\nsomewhat simpler than the original imperative approach. The chain pipeline\nworks because both PromptTemplate and ChatOpenAI objects implement a\n",
      "content_length": 1678,
      "extraction_method": "Direct"
    },
    {
      "page_number": 466,
      "chapter": null,
      "content": "common interface (Runnable) that allows them to be linked. The | syntax is\nsugar for creating a Chain object behind the scenes.\nSo far, I've provided a light introduction to LangChain. My goal was to\nquickly show you what LangChain is, its object model, and how to code with\nit, knowing you're eager to dive in. I've also shared examples of common\nLLM applications to give you an idea of what people are building with\nLLMs. However, if you're not familiar with LLMs, you might want some\nbackground information before moving forward. I'll cover that in the next\nsection. If you already have this knowledge, feel free to skip to the next\nchapter.\n",
      "content_length": 645,
      "extraction_method": "Direct"
    },
    {
      "page_number": 467,
      "chapter": null,
      "content": "Appendix B. Setting up a Jupyter\nNotebook environment\nIf you're just starting with Python Jupyter notebooks, think of it as an\ninteractive space where you can type and run code, see the results, and tweak\nthe code to get the outcomes you want in real-time. The code snippets I'll be\nsharing aren't exclusive to a particular Python version, but for the smoothest\nexperience, it's suggested to run them on Python 3.11 or a newer version if\nyou can.\nInstalling the Python interpreter or a Python distribution\nIf you haven't actively used Python lately, I suggest installing the latest\nversion of the Python 3 interpreter. You have a few options:\n1. Standalone Python Interpreter: Download and install Python 3 from\npython.org. Choose the installer appropriate for your operating system\nand follow the setup instructions. You can find OS-specific guidance in\nthe official documentation here:\nhttps://docs.python.org/3/using/index.html (you may need to scroll to\nlocate your platform). This option is ideal if you prefer a lightweight\nPython installation without the additional libraries and tools bundled\nwith distributions like Anaconda or Miniconda.\n2. Anaconda: Download Anaconda from anaconda.com/download. It\ncomes with Python and a wide range of data science libraries, including\ntools for visualization, data analysis, and numerical/statistical work. This\noption is perfect for data science and machine learning projects if you\nhave enough disk space. Anaconda also makes it easy to create virtual\nenvironments for specific projects. It includes Anaconda Navigator, a\nuser-friendly graphical interface, for those who prefer not to use\ncommand-line tools.\n3. Miniconda: Miniconda is a lighter alternative to Anaconda. It allows you\nto manage virtual environments for specific applications without taking\n",
      "content_length": 1806,
      "extraction_method": "Direct"
    },
    {
      "page_number": 468,
      "chapter": null,
      "content": "up much disk space. It only includes essential data science libraries.\nLearn more at: docs.conda.io/projects/miniconda/.\nFor the remainder of this appendix, let's assume you have Python installed (as\ndescribed in option 1 above). If you've chosen Miniconda or Anaconda, I\nassume you're familiar with Python and can adapt my instructions as needed.\nNow, you're ready to set up a virtual environment using venv, which is a tool\nfor managing virtual environments, and to update pip, the Python package\ninstaller.\nCreating a virtual environment with venv and upgrading pip\nOpen the operating system terminal shell (e.g., cmd on Windows), create a\nproject folder, and navigate into it:\nC:\\Github\\building-llm-applications\\ch01>\nCreate a virtual environment with venv. A virtual environment serves as a\nself-contained Python installation, specifically for the ch01 folder you've\nrecently created. This ensures that package version conflicts with other\nprojects on your machine are avoided. Create a virtual environment for ch01\nusing the following command:\nC:\\Github\\building-llm-applications\\ch01>python -m venv env_ch01\nYou've successfully set up a virtual environment named \"env_ch01.\" Now,\nactivate it using the following command:\nC:\\Github\\building-llm-applications\\ch01>.\\env_ch01\\Scripts\\activ\nYou should now observe the updated operating system prompt, displaying the\nenvironment name in front as \"(env_ch01)\":\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>\nBefore proceeding further, it is useful to upgrade pip, the Python package\nmanagement tool, as follows, so you will be able to install the necessary\nPython packages with no issues:\n",
      "content_length": 1647,
      "extraction_method": "Direct"
    },
    {
      "page_number": 469,
      "chapter": null,
      "content": "(env_ch01) C:\\Github\\building-llm-applications\\ch01>python -m pip \nFor additional details on the steps you've taken, refer to the following\ndocumentation on the python.org website:\nhttps://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-\nenvironments/\nSetting up a Jupyter notebook\nHaving activated the virtual environment, you're now prepared to configure a\nJupyter notebook for executing prompts with OpenAI models. Proceed to\ninstall Jupyter and the LangChain packages using the following steps:\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>pip install n\nAfter about a minute, the installation of the notebook and LangChain\npackages should be finished. If you'd like, you can confirm it by using the\nfollowing command:\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>pip list\nYou can start the Jupyter notebook by executing the following command:\n(env_ch01) C:\\Github\\building-llm-applications\\ch01>jupyter noteb\nAfter a few seconds, you should see this output in the terminal:\n[I 2023-10-23 22:43:26.870 ServerApp] Jupyter Server 2.8.0 is run\n[I 2023-10-23 22:43:26.870 ServerApp] http://localhost:8888/tree?\n[I 2023-10-23 22:43:26.871 ServerApp]     http://127.0.0.1:8888/t\n[I 2023-10-23 22:43:26.872 ServerApp] Use Control-C to stop this \n[C 2023-10-23 22:43:26.978 ServerApp]\nSubsequently a browser window will open up at this URL:\nhttp://localhost:8888/tree\nOnce you have created the notebook, select: File > Rename â€¦ and name the\nnotebook file: prompt_examples.ipynb.\nFigure B.1 Creating a new Jupyter notebook: create the notebook with the menu File > New >\nNotebook and then rename the file to langchain_examples.ipynb\n",
      "content_length": 1666,
      "extraction_method": "Direct"
    },
    {
      "page_number": 470,
      "chapter": null,
      "content": "Now, you're prepared to input code into the notebook cells.\nTIP\nIf you are unfamiliar with Jupyter notebook, remember to press Ctrl + Enter\nto execute the code in each cell.\n",
      "content_length": 174,
      "extraction_method": "Direct"
    },
    {
      "page_number": 471,
      "chapter": null,
      "content": "Appendix C. Choosing an LLM\nC.1 Popular Large Language Models\nThis appendix outlines the key features of the most popular large language\nmodels available at the time of writing. It also highlights the criteria to\nconsider when selecting the most suitable LLM for your project.\nC.1.1 OpenAI GPT-4o and GPT-4.1 series\nOpenAI’s GPT-4, launched in March 2023, marked a significant milestone as\nthe first “frontier model” to demonstrate advanced reasoning capabilities. It\nwas also one of the earliest multi-modal models, capable of processing text,\nimages, and later video, enabling diverse applications. While its architecture\ndetails remain undisclosed, GPT-4 was rumored to feature 1.8 trillion\nparameters and employed a “Mixture of Experts” design to enhance accuracy\nacross various use cases.\nThe GPT-4o series succeeded GPT-4, offering specialized options for\ndifferent tasks. GPT-4o was optimized for complex, high-accuracy tasks like\nmulti-step reasoning, while GPT-4o-mini targeted simpler applications such\nas summarization, editing, and translation, replacing GPT-3.5 for these use\ncases.\nThe GPT-4.1 series, released in mid-2025, followed GPT-4o and introduced\nrefinements in performance, speed, and cost-efficiency. It retained GPT-4o’s\nstrength in complex reasoning while offering better responsiveness and\nadaptability across a wide range of tasks. With improved balance between\naccuracy and resource usage, GPT-4.1 became a strong fit for both advanced\nand general-purpose applications.\nLangChain integrates seamlessly with OpenAI models through the\nlangchain-openai package and also supports open-source models using\ninference engines compatible with the OpenAI REST API, as detailed in\n",
      "content_length": 1700,
      "extraction_method": "Direct"
    },
    {
      "page_number": 472,
      "chapter": null,
      "content": "Chapter 11.\nC.1.2 OpenAI o1 and o3 series\nIn September 2024, OpenAI introduced o1, designed for advanced reasoning\nand multi-step problem-solving with higher accuracy but at significantly\nhigher costs — over 30 times that of GPT-4o-mini.\nIn December 2024, OpenAI announced the upcoming release of o3 and GPT-\no3-mini, scheduled for early 2025, as an evolution of the o1 model with\nenhanced performance and capabilities.\no3 demonstrates exceptional results across various benchmarks. On the ARC-\nAGI test, which evaluates “general AI” capabilities, it scored 87.5%,\nsurpassing the 87% achieved by top human performers. For comparison,\nGPT-3 scored 0%, and GPT-4o scored around 5%. In the Frontier Math\nbenchmark, o3 solved 25% of complex problems, a ground-breaking\nachievement since no prior LLM managed more than 2%. Its programming\nabilities were equally remarkable, achieving 99% on a competitive\nprogramming benchmark where only top human programmers reach 100%.\no3 is positioned as the first LLM nearing AGI-level (Artificial General\nIntelligence) capabilities, setting new standards in reasoning, problem-\nsolving, and programming.\nC.1.3 Gemini\nIn December 2023, Google introduced Gemini 1.5, a family of multimodal\nmodels capable of processing text, audio, images, and code. The lineup\nranged from Gemini Ultra, designed for advanced tasks using a Mixture of\nExperts architecture, to Gemini Nano, optimized for lightweight mobile\napplications. Gemini Ultra surpasses PaLM-2 in benchmarks, excelling in\nreasoning, math comprehension, and code generation.\nGemini Ultra’s standout feature is its large context window, accommodating\nvast data inputs: up to 1 hour of video, 11 hours of audio, codebases\nexceeding 30,000 lines, or over 700,000 words. Research tests extended to 10\n",
      "content_length": 1783,
      "extraction_method": "Direct"
    },
    {
      "page_number": 473,
      "chapter": null,
      "content": "million tokens, demonstrating its capacity for extensive workloads.\nGemini 2.0, introduced in December 2024, marks a significant advancement\nin AI, enabling sophisticated reasoning for tasks like multimodal queries,\ncoding, and complex math, powered by Trillium TPUs. Its leading model,\nGemini 2.0 Flash, offers fast, high-performance capabilities with support for\nmultimodal input and output, including text, images, video, audio, and\nsteerable multilingual TTS. Enhancing its predecessor, 2.0 Flash operates at\ntwice the speed of 1.5 Pro, integrates native tools, and sets new standards in\nAI performance.\nGemini 2.5, launched in May 2025, builds on the foundation of Gemini 2.0\nwith enhanced performance, efficiency, and broader multimodal capabilities.\nDesigned for both advanced and real-time applications, Gemini 2.5 Pro offers\nimproved reasoning, longer context handling, and faster response times\nacross text, image, audio, and video inputs. It introduces better integration\nwith Google's ecosystem, tighter tool use, and more reliable multilingual\nsupport, positioning it as a competitive alternative to other state-of-the-art\nmodels.\nLangChain offers seamless access to Gemini models through the langchain-\ngoogle-genai package, enabling developers to integrate these powerful\nmodels into their applications effortlessly.\nC.1.4 Gemma\nIn February 2024, Google rolled out Gemma, an open-source counterpart to\nGemini, designed with a focus on lightweight functionality. Built on the same\nresearch and tech-stack as Gemini, Gemma offers model weight files at 2B\nand 7B parameters, freely accessible for use. The models are optimized to run\non NVIDIA GPUs and Google Cloud TPUs. Additionally, Gemma is\nprovided with a toolkit to facilitate effective utilization for both fine-tuning\nand inference tasks.\nC.1.5 Claude\nIn March 2023, Anthropic and Google launched Claude, a language model\n",
      "content_length": 1892,
      "extraction_method": "Direct"
    },
    {
      "page_number": 474,
      "chapter": null,
      "content": "designed with a strong emphasis on honesty and safety. Claude excels in\nsummarization, coding, writing, and chatbot-based question answering.\nAlthough Anthropic has not shared details about its architecture or training,\nClaude set a new standard by supporting 100,000 tokens, which at the time\nwas the biggest context window, enabling it to handle complex tasks. To\nimprove speed, Anthropic released Claude Instant, optimized for faster\nperformance.\nSubsequent updates, including Claude 2 in July 2023, Claude 3 in March\n2024, Claude 3.5 in October 2024, and Claude 3.7 in February 2025,\nenhanced accuracy, safety, and versatility across diverse language tasks.\nClaude 3.7 is available in the bigger accurate version, called Sonnet and the\nsmaller faster version called Haiku. Claude 3.7 Sonnet is in the same segment\nas GPT-4.1 and Gemini 2.5 Pro. Haiku can be considered in the same group\nas GPT-4o-mini.\nAnthropic models, including the Claude family, are accessible by installing\nthe langchain-anthropic package.\nC.1.6 Cohere\nCohere, backed by former Google Brain employees, including Aidan Gomez,\nco-author of the influential \"Attention Is All You Need\" paper, focuses\nexclusively on enterprise needs. Known for precision and consistency,\nCohere offers models ranging from 6 billion to 52 billion parameters,\nallowing organizations to tailor their approach.\nCohere has achieved the highest accuracy among large language models at\nsome point, making it a dependable choice for businesses. Major corporations\nlike Jasper and Spotify have adopted Cohere for advanced natural language\nprocessing, highlighting its practical applicability. However, it's worth noting\nthat Cohere's technology comes at a higher cost compared to more widely\nrecognized models from OpenAI. You can access Cohere models from\nLangChain by installing the langchain-cohere package.\nC.1.7 Llama\n",
      "content_length": 1869,
      "extraction_method": "Direct"
    },
    {
      "page_number": 475,
      "chapter": null,
      "content": "Meta's Llama series of large language models has played a major role in\nadvancing open-access AI. First introduced in 2023 with a 70B-parameter\nmodel, Llama quickly evolved into a flexible, open-source platform with\nsmaller variants to suit different computational needs. Built on transformer\narchitecture and trained on diverse public datasets, Llama inspired derivative\nmodels like Vicuna and Orca. Over 2023 and 2024, Meta released Llama-2\nand Llama-3, including a 405B-parameter model and the introduction of\nvision capabilities. In April 2025, Meta launched Llama 4, featuring major\narchitectural improvements. The Llama 4 lineup includes Scout (109B\nparameters, multimodal, 10M token context), Maverick (400B parameters,\noptimized for reasoning), and Behemoth (a 2T-parameter model still in\ntraining, aimed at outperforming current benchmarks). While open-weight,\nthese models come with licensing constraints for commercial use. Llama’s\nscalability, openness, and compatibility with tools like LangChain—via\nwrappers such as GPT4All and Ollama—make it a powerful resource for\nboth researchers and developers in AI and NLP.\nC.1.8 Falcon\nDeveloped by the Technology Innovation Institute, Falcon is a transformer-\nbased, causal decoder-only model designed for natural language processing\ntasks.\nFalcon 3, released in December 2024, demonstrates strong performance\ncompared to its peers. For instance, Falcon 3 10B achieves results\ncomparable to Qwen 2.5 7B and Gemma 9B across benchmarks like MUSR\nand MMLU_PRO, while outperforming them on the MATH benchmark. This\npositions Falcon 3 as a competitive option among contemporary open-source\nLLMs.\nC.1.9 Mistral\nFounded in 2023, Mistral AI has rapidly become a leader in open-source\nlarge language models, known for its efficient and high-performing designs.\nIt introduced the Mistral 7B model, followed by the Mixtral 8x7B and 8x22B\nmodels—both using Sparse Mixture-of-Experts (MoE) architectures that\nactivate only a subset of parameters per token to improve cost-efficiency and\n",
      "content_length": 2031,
      "extraction_method": "Direct"
    },
    {
      "page_number": 476,
      "chapter": null,
      "content": "performance. In May 2025, the company released Mistral Medium 3, a mid-\nsized model optimized for enterprise use, offering strong performance at\nlower cost and supporting hybrid deployments. Mistral's ongoing innovation\nand open-source approach position it as a major competitor in the AI space,\nbalancing scalability, affordability, and flexibility across a range of\napplications.\nC.1.10 Qwen\nQwen, open-sourced on GitHub in August 2023, is a family of LLMs\ndeveloped by Alibaba Cloud. The models range from 1.8 billion to 72 billion\nparameters, trained on datasets between 2.2 trillion and 3 trillion tokens.\nWhile Qwen models are designed for general purposes, there are fine-tuned\nversions for specific tasks, such as Code-Qwen for coding and Math-Qwen\nfor mathematics. Qwen-Chat has been fine-tuned using Reinforcement\nLearning from Human Feedback (RLHF), similar to ChatGPT. The models\nsupport a context length of around 30,000 tokens and perform particularly\nwell in English and Chinese.\nC.1.11 Grok\nGrok, developed by xAI, has rapidly advanced since its debut in late 2023.\nInitially focused on conversational tasks and integrated with the X platform,\nGrok used a mixture-of-experts architecture and supported up to 128,000\ntokens. Subsequent versions—Grok-2 and Grok-2 Mini—added features like\nimage generation and improved performance and speed. In February 2025,\nxAI released Grok 3, significantly upgraded with 10x more compute, new\nreasoning modes (\"Think\" and \"Big Brain\"), real-time data retrieval via\n\"DeepSearch,\" and image editing capabilities. In May 2025, Grok 3.5\nlaunched in beta with enhanced technical reasoning and a redesigned RAG\nsystem. Microsoft also partnered with xAI to host Grok 3 and Grok 3 Mini on\nAzure AI Foundry, bringing the models to a broader enterprise audience.\nC.1.12 Phi\nThe Phi-3 family comprises small language models (SLMs) designed to\nprovide many capabilities of large language models while being more\n",
      "content_length": 1952,
      "extraction_method": "Direct"
    },
    {
      "page_number": 477,
      "chapter": null,
      "content": "resource-efficient. These models outperform others of the same and next size\nup in benchmarks for language, coding, and math tasks, thanks to Microsoft’s\ninnovative training methods. The Phi-3-mini (3.8 billion parameters) delivers\nexceptional performance, rivaling models twice its size, with future releases\nincluding Phi-3-small (7 billion parameters) and Phi-3-medium (14 billion\nparameters). The models are accessible through Microsoft Azure AI Model\nCatalog, Hugging Face, Ollama for local deployment, and as NVIDIA NIM\nmicroservices with standard API interfaces.\nPhi-3.5-MoE, the latest addition to the Phi family, is a lightweight, state-of-\nthe-art open model optimized for reasoning-intensive tasks such as code,\nmath, and logic. It supports multilingual applications with a 128K token\ncontext length. Developed using high-quality datasets, it incorporates\nadvanced fine-tuning and optimization techniques for precise instruction\nadherence and robust safety. Designed for memory-constrained and latency-\nsensitive environments, Phi-3.5-MoE powers general-purpose AI systems and\nis accessible through Azure AI Studio and GitHub via a serverless API,\noffering scalable and cost-efficient deployment.\nC.1.13 DeepSeek\nDeepSeek, a Chinese AI company founded in 2023 by Liang Wenfeng, has\ndeveloped open-source large language models (LLMs) that rival leading\nsystems in performance and cost-efficiency. Its DeepSeek-V3 model, a\nMixture-of-Experts (MoE) architecture with 671 billion parameters\n(activating 37 billion per token), was trained on 14.8 trillion tokens using\nsupervised fine-tuning and reinforcement learning. Despite its scale, training\nrequired only 2.788 million GPU hours on Nvidia H800 chips, costing less\nthan $6 million. It outperforms other open-source models and matches top\nproprietary systems, excelling in mathematics, programming, reasoning, and\nmultilingual tasks, with its AI assistant surpassing ChatGPT as the top free\napp on Apple’s App Store.\nDeepSeek-R1 also competes with high-end LLMs like OpenAI’s o1,\nspecializing in complex reasoning and coding through \"chain-of-thought\"\nreasoning. Trained with 2,000 Nvidia H800 chips at a cost of $5.6 million,\nR1’s efficiency sparked debates on sustainable AI training. While both\n",
      "content_length": 2259,
      "extraction_method": "Direct"
    },
    {
      "page_number": 478,
      "chapter": null,
      "content": "models are open-source, they avoid politically sensitive topics, raising\nprivacy concerns and prompting global discussions about China’s growing\ninfluence in AI and the shifting balance of technological power.\nTable C.1 provides a summary of the key characteristics of the LLMs\ndiscussed in this appendix.\nTable C.1 Comparison among LLM models (*sizes with an asterisk are estimated)\nModel\nDeveloper\nLaunch\nNum\nparams\nMax\ntokens\nOpen\nsource\nGPT-4o\nOpenAI\nMay 24\n200B*\n128K\nNo\nGPT-4.1\nOpenAI\nApr 25\nN/A\n1M\nNo\no1\nOpenAI\nOct 24\n400B*\n128K\nNo\no3\nOpenAI\nDec 24\n200B*\n128K\nNo\nGemini 2.5\nPro\nGoogle\nMay 25\nN/A\n1M\nNo\nGemma-3\nGoogle\nMar 25\n1B-27B\n128K\nYes\nClaude\nSonnet-3.7\nAnthropic\nFeb 25\nN/A\n200K\nNo\nCommand R+\nCohere\nAug 24\n100B\n128K\nNo\nLlama-4\nMeta\nSep 24\n90B\n128K\nYes\nFalcon3-10B-\nBase\nTII-UAE\nDec 24\n10B\n32K\nYes\nMixtral-\n8x22B-\nInstruct-v0.1\nMistral AI\nMar 24\n141B\n64K\nYes\nQwen2.5 72B\nAlibaba\nCloud\nSep 24\n72B\n128K\nYes\nGrok 3.5\nxAI\nFeb 25\n314B\n1M\nNo\nDeepSeek-V3\nDeepseek\nAI\nDec 24\n671B\n128K\nYes\nDeepSeek-R1\nDeepseek\nAI\nJan 25\n671B\n132K\nYes\nPhi-4.0\nMicrosoft\nDec 24\n14B\n128K\nYes\n",
      "content_length": 1076,
      "extraction_method": "Direct"
    },
    {
      "page_number": 479,
      "chapter": null,
      "content": "C.2 How to choose a model\nEach use case has its unique requirements. Selecting the ideal LLM for your\nspecific needs can be a complex task. It often involves testing multiple\nmodels to identify the most suitable one. However, you can employ certain\ncriteria to streamline the selection process.\nSeveral key factors to consider include:\nModel Purpose\nProprietary vs. Open Source\nModel size\nContext Window size\nSupported Languages\nAccuracy vs. Speed\nCost and Hardware Requirements\nTask suitability\nSafety and Bias\nIn the following sections, I’ll explore these factors in greater depth to help\nyou make a well-informed decision.\nC.2.1 Model Purpose\nSome LLMs are flexible and can manage various tasks, while others are\ntailored for specific functions. For instance, OpenAI's GPT-4 or Gemini 1.5\nPro are versatile and adaptable models, suitable for a range of tasks, while\nCode Llama, or Qwen Coder as their names suggest, are specialized for\ngenerating programming code.\nSpecialized models are usually smaller than their more general counterparts.\nIf you have a clear use case in mind, it's a good idea to choose an LLM\nspecifically created and trained for that purpose. There are common LLM\nspecializations, ranging from simpler tasks like text classification and\nsentiment analysis to more advanced functions like text summarization,\ntranslation, code analysis, text generation, question-answering, and logic and\nreasoning.\n",
      "content_length": 1423,
      "extraction_method": "Direct"
    },
    {
      "page_number": 480,
      "chapter": null,
      "content": "Many foundational LLMs can handle combinations of these functions,\noffering a wide range of possibilities to meet your specific needs.\nC.2.2 Proprietary vs. Open-Source\nMany language models are proprietary, meaning their developers keep the\ninternal details private. Information such as architecture, parameter count\n(discussed in the next section), or training specifics is often undisclosed.\nProprietary models from providers like OpenAI, Gemini, Cohere, Anthropic\nand Stability AI are typically offered as cloud-based subscription services\naccessed through REST APIs. Pricing depends on the accuracy of the model\nused and the number of tokens processed. This approach provides\nconvenience, allowing users to access ready-to-use services without\nmanaging hardware or infrastructure. However, data submitted to these\nservices is retained and potentially processed by the provider, which can be a\nconcern for sensitive data.\nIn contrast, open-source models offer full transparency, providing access to\ntheir underlying implementation as well as detailed information about their\narchitecture and training processes. This transparency brings two key\nadvantages. First, it enables you to deploy a fully private solution, avoiding\nthe need to send sensitive data to third-party vendors—an important\nconsideration for privacy and security. Second, it allows for fine-tuning on\ndomain-specific datasets, giving you the flexibility to adapt the model to your\nunique needs. The trade-off, however, is that you typically need to host and\nmanage the infrastructure yourself, which involves provisioning GPUs and\ndedicating IT resources to maintain and scale the service. As an alternative,\nyou can opt for managed LLM hosting platforms such as IBM Watsonx,\nAmazon Bedrock, Azure AI Studio, or Google Vertex AI. These services\nprovide a scalable, secure environment for running both open-source and\nproprietary models—eliminating much of the operational complexity\nassociated with self-hosting.\nLangChain supports both proprietary models, such as OpenAI, Gemini, and\nCohere, and open-source models through inference engine wrappers like\nOllama.\n",
      "content_length": 2134,
      "extraction_method": "Direct"
    },
    {
      "page_number": 481,
      "chapter": null,
      "content": "C.2.3 Model size (Number of Parameters)\nThe A model's parameters represent its internal variables, specifically the\nweights in the artificial neural network. These weights are adjusted during\ntraining and enable the model to learn patterns from data. More parameters\nallow a model to capture greater complexity and nuance, potentially\nincreasing accuracy. However, larger models with more parameters require\nsubstantial hardware resources—such as increased memory and high-\nperformance GPUs—and often result in higher latency during inference.\nCompression techniques can reduce a model's size without major loss of\naccuracy, which we’ll discuss in the chapter on open-source LLMs.\nLanguage models vary widely in parameter count, from trillions in GPT-4\nand Gemini 1.5 Ultra to a few billion in Mistral and Gemma. Parameters\nform the foundation of a model’s ability to process text tasks.\nC.2.4 Context Window size\nThe number of input tokens allowed in Large Language Models (LLMs)\ndirectly impacts their functionality and suitability for different tasks. Token\nlimits vary across models, influencing the complexity of prompts they can\nhandle.\nModels with smaller token limits are well-suited for concise prompts and\nstraightforward interactions. These limitations often stem from design\nchoices or computational constraints, and such models are typically\nspecialized for a narrow set of tasks. In contrast, LLMs with higher token\ncapacities—often designed as generalists—can handle more detailed, context-\nrich inputs, making them better suited for complex, multi-step tasks.\nChoosing the right token limit depends on the task. For short, straightforward\ninputs, smaller token allowances may suffice. For applications requiring\ndetailed interactions or extended context, models with larger token limits are\nmore appropriate.\nIn summary, token capacity is a key consideration when selecting an LLM.\nAligning the token limit with your project's requirements ensures effective\n",
      "content_length": 1974,
      "extraction_method": "Direct"
    },
    {
      "page_number": 482,
      "chapter": null,
      "content": "use of the model’s capabilities.\nC.2.5 Multilingual Support\nWhen considering an LLM for multilingual support, it's essential to research\nwhich one aligns with your requirements. Some LLMs are proficient in\nmultiple languages, including ancient ones like Latin, ancient Greek, and\neven Phoenician, while others are primarily English-focused. Typically,\nLLMs excel with languages that have extensive training data, like English,\nChinese, and Spanish. If your needs involve a less common language, you\nmight need to seek a specialized LLM or even undertake fine-tuning yourself.\nIt's crucial to match your language requirements with the capabilities of the\nchosen LLM for optimal results.\nC.2.6 Accuracy vs. Speed\nSelecting Choosing an LLM often requires balancing accuracy and\nprocessing speed. Larger models with more parameters deliver higher\nprecision and nuanced responses, especially for complex language tasks.\nHowever, this accuracy comes at the cost of slower processing and\nsignificant computational requirements.\nSmaller models, with fewer parameters, are faster and better suited for real-\ntime applications. While they sacrifice some accuracy, they excel in tasks\nrequiring quick responses. The decision between a large or small model\ndepends on the specific needs of the application—detailed comprehension\nfavors larger models, while speed-sensitive tasks benefit from smaller ones.\nAdvances in compression techniques, discussed in Chapter 10 on open-\nsource models, have bridged this gap. Compact models with lower parameter\ncounts now achieve accuracy comparable to much larger models, making the\ntrade-off between speed and precision less of a limitation.\nC.2.7 Cost and Hardware Requirements\nCost and hardware requirements are key factors when deploying Large\nLanguage Models (LLMs). Organizations must carefully weigh financial and\n",
      "content_length": 1848,
      "extraction_method": "Direct"
    },
    {
      "page_number": 483,
      "chapter": null,
      "content": "technical considerations to ensure effective use of these models.\nProprietary LLMs are typically priced based on their accuracy and the\nnumber of tokens processed. While they deliver high precision, enhanced\ncapabilities result in higher costs, requiring organizations to balance accuracy\nagainst budget constraints.\nOpen-source LLMs, though more affordable in terms of licensing, shift the\ncost burden to hardware and infrastructure. Deploying these models demands\npowerful GPUs, sufficient RAM, and reliable virtual machines. Additionally,\norganizations need skilled IT staff to manage deployment, maintenance, and\nsupport.\nThe choice between proprietary and open-source LLMs depends on\norganizational goals and resources. Proprietary models offer convenience and\nperformance for a fee, while open-source models provide flexibility and cost\nsavings at the expense of hardware and IT investment. Careful evaluation\nensures the solution aligns with both objectives and available resources.\nC.2.8 Task suitability (standard benchmarks)\nThe effectiveness of a language model for specific tasks depends on its\narchitecture, size, training data, and fine-tuning. Different models are\noptimized for different use cases, and standardized benchmarks help evaluate\ntheir performance across various tasks.\nLeaderboards like the Hugging Face Open LLM Leaderboard and LMSYS\nChatbot Arena Leaderboard offer a centralized way to compare models across\nbenchmarks, streamlining the evaluation process. Below are some widely\nrecognized benchmarks, with descriptions drawn from the corresponding\nresearch publications.\nInstruction-Following Evaluation (IFEval): IFEval is an evaluation\nbenchmark for Large Language Models (LLMs) designed to assess their\nability to follow natural language instructions. It uses 25 types of\n\"verifiable instructions,\" such as word count or keyword mentions, and\nincludes 500 prompts. IFEval provides a simple, reproducible alternative\nto human evaluations, which are costly and inconsistent, and LLM-\n",
      "content_length": 2016,
      "extraction_method": "Direct"
    },
    {
      "page_number": 484,
      "chapter": null,
      "content": "based evaluations, which may be biased. Paper:\nhttps://arxiv.org/abs/2311.07911.\nBig Bench Hard (BBH): The BIG-Bench Hard (BBH) benchmark is a\nsubset of 23 challenging tasks from the BIG-Bench evaluation suite,\ndesigned to test areas where language models have historically\nunderperformed compared to average human raters. These tasks often\nrequire multi-step reasoning and highlight the limitations of language\nmodels without advanced prompting techniques. Using chain-of-thought\n(CoT) prompting, models like PaLM and Codex demonstrated\nsignificant improvements, surpassing human performance on 10 and 17\ntasks, respectively. The benchmark emphasizes that few-shot prompting\nalone underestimates the potential of language models, and CoT\nprompting reveals their advanced reasoning capabilities, particularly as\nmodel scale increases. Paper: https://arxiv.org/abs/2210.09261.\nMathematics Aptitude Test for Euristics - Level 5 (MATH, L5): The\nMATH benchmark is a dataset of 12,500 challenging mathematics\nproblems designed to evaluate the mathematical problem-solving\nabilities of machine learning models. Each problem includes a detailed\nstep-by-step solution, enabling models to learn answer derivations and\nexplanations. Alongside MATH, an auxiliary pretraining dataset is\nprovided to help models grasp fundamental mathematical concepts.\nDespite improvements, model accuracy on MATH remains low,\nindicating that scaling Transformer models alone is insufficient for\nstrong mathematical reasoning. Advancing this capability will likely\nrequire novel algorithms and research breakthroughs. Paper:\nhttps://arxiv.org/abs/2103.03874.\nGoogle Proof Q&A (GPQA): GPQA is a dataset of 448 multiple-choice\nquestions in biology, physics, and chemistry, created by domain experts\nto be exceptionally difficult. Expert participants with advanced degrees\nachieve 65% accuracy (74% excluding identified errors), while non-\nexperts with unrestricted web access average only 34%, making the\nquestions effectively \"Google-proof.\" GPQA is designed to aid in\ndeveloping scalable oversight methods, enabling humans to supervise\nand extract reliable, truthful information from AI systems, even those\nsurpassing human capabilities. Paper: https://arxiv.org/abs/2311.12022.\nMultistep Soft Reasoning (MuSR): MuSR is a benchmark designed to\nevaluate LLMs on multistep reasoning tasks framed within natural\n",
      "content_length": 2380,
      "extraction_method": "Direct"
    },
    {
      "page_number": 485,
      "chapter": null,
      "content": "language narratives. It features complex reasoning scenarios, such as\n1000-word murder mysteries, generated using a neurosymbolic\nsynthetic-to-natural algorithm. These tasks are challenging for advanced\nmodels and can scale to match future LLM advancements. MuSR\nemphasizes real-world reasoning, offering narratives that are more\nrealistic and difficult than typical synthetic benchmarks while remaining\naccessible for human annotation. The benchmark highlights limitations\nin current reasoning techniques, such as chain-of-thought prompting,\nand helps identify areas for improvement in robust reasoning\ncapabilities.Paper: https://arxiv.org/abs/2310.16049.\nMore Robust Massive Multitask Language Understanding (MMLU-\nPRO): MMLU-Pro is an advanced benchmark that builds on the\nMassive Multitask Language Understanding (MMLU) dataset by\nintroducing more challenging, reasoning-focused questions and\nexpanding answer options from four to ten. It removes trivial and noisy\nquestions, resulting in a dataset that better evaluates complex reasoning\ncapabilities. Compared to MMLU, MMLU-Pro decreases model\naccuracy by 16% to 33% and reduces sensitivity to prompt variations,\nimproving stability. Models using Chain of Thought (CoT) reasoning\nperform better on MMLU-Pro, highlighting its emphasis on complex\nreasoning over direct answering. This benchmark provides a more\neffective tool for tracking advancements in AI reasoning and\ncomprehension. Paper: https://arxiv.org/abs/2406.01574.\nThe table below provides a snapshot of how leading LLMs perform across\nkey benchmarks at the time of publication. For the most accurate insights,\ncheck for updated scores on the latest versions of the models you plan to use.\nTable C.2 Benchmark scores (%) of the most popular LLMs\nModel\nIfEval BBH\nMATH-\nL5\nGPQA\nMuSR\nMMLU-\nPRO\nGPT-4o\n85.60\n83.10\n76.60\n53.60\nN/A\n74.68\nGemini\n2.0 Flash\nN/A\n86.80\n89.70\n62.10\nN/A\n76.40\nClaude 3.5\nSonnet\n88.00\nN/A\n71.1\n59.40\nN/A\n78.00\n",
      "content_length": 1949,
      "extraction_method": "Direct"
    },
    {
      "page_number": 486,
      "chapter": null,
      "content": "Command\nR+\nN/A\nN/A\n44.0\nN/A\nN/A\nN/A\nGrok 2\n75.50\nN/A\n76.1\nN/A\nN/A\nN/A\nDeepSeek-\nV3\n86.10\nN/A\nN/A\n59.10\nN/A\n75.90\nQwen2.5-\n72B-\nInstruct\n86.38\n61.87\n1.21\n16.67\n11.74\n51.40\nQwen2.5-\nCoder-\n32B\n43.63\n48.51\n30.59\n12.86\n15.87\n47.81\nQwen2.5-\nMath-7B\n24.60\n22.01\n30.51\n5.82\n5.00\n19.09\nLlama-\n3.3-70B-\nInstruct\n89.98\n56.56\n0.23\n10.51\n15.57\n48.13\nMixtral-\n8x22B-\nInstruct-\nv0.1\n71.84\n44.11\n18.73\n16.44\n13.49\n38.70\nMistral-\n7B-v0.3\n22.66\n23.95\n3.02\n5.59\n8.36\n21.70\nGemma-2-\n27b-it\n79.78\n49.27\n0.76\n16.67\n9.11\n38.35\nFalcon3-\n10B-Base\n36.48\n41.38\n24.77\n12.75\n14.17\n36.00\nPhi-3.5-\nMoE-\ninstruct\n69.25\n48.77\n22.66\n14.09\n17.33\n40.64\nC.2.9 Safety and Bias\nEnsuring the safety and fairness of Large Language Models (LLMs) is\nessential for ethical and responsible AI use. The focus is on preventing\nharmful, offensive, or biased outputs that could negatively impact individuals\nor reinforce stereotypes.\n",
      "content_length": 886,
      "extraction_method": "Direct"
    },
    {
      "page_number": 487,
      "chapter": null,
      "content": "To improve safety, LLMs must avoid generating content that is sexist, racist,\nor hateful. Since these models learn from extensive internet text, which can\ncontain harmful material, implementing robust filtering and monitoring is\ncritical to align outputs with ethical standards.\nReducing bias is equally important. Without careful oversight, LLMs can\nunintentionally perpetuate stereotypes. For example, assuming doctors are\nmale and nurses are female reinforces outdated gender biases, undermining\ninclusivity and fairness.\nThis challenge is especially important in fields like healthcare, customer\nservice, and education, where unbiased and accurate responses are vital.\nRegularly refining training data and fine-tuning models are necessary steps to\nensure fairness and inclusivity.\nIn summary, promoting safety and reducing bias in LLMs requires\ncontinuous effort and a commitment to ethical AI practices. As you explore\nand implement AI solutions, prioritize testing and refinement to address\npotential issues, as covered in the next section.\nC.2.10 A practical example\nLet’s integrate all the key factors and decisions involved in selecting an LLM\nfor a specific application, using a practical example. Imagine implementing a\ncode assistant chatbot to generate Python snippets for stock trading strategies.\nThe target users are software developers supporting traders in a financial\norganization.\nThe code generator must utilize proprietary in-house trading libraries, making\nconfidentiality a critical concern. Company policy strictly prohibits\nsubmitting any code referencing proprietary trading libraries to external\nsystems. The assistant must generate code quickly while maintaining high\naccuracy to minimize bugs and avoid delays, especially during volatile\ntrading periods. As this is for a financial company, cost is not a limiting\nfactor.\nThe flowchart below illustrates the decision-making workflow for selecting\nan LLM for this use case.\n",
      "content_length": 1953,
      "extraction_method": "Direct"
    },
    {
      "page_number": 488,
      "chapter": null,
      "content": "Figure C.1 Flowchart for selecting an LLM for a Python coding assistant designed to generate\ncode for trading strategies. The process begins with choosing between a general-purpose and a\nspecialized model, with a preference for LLMs tailored for code generation, particularly in\nPython. The next step focuses on selecting open-source models to comply with strict IT policies\nrequiring all proprietary code to remain confidential. The final decision balances speed and\naccuracy, leading to the selection of Qwen2.5 Coder 32b based on its strong performance,\nincluding a high Python HumanEval score for accurate Python code generation.\n",
      "content_length": 634,
      "extraction_method": "Direct"
    },
    {
      "page_number": 489,
      "chapter": null,
      "content": "App designer Choosing an LLM to generate Python\ncode for proprietary stock trading models\nrequires a focus on confidentiality, high\nperformance, and precision. Speed and\naccuracy are equally important, while\n\ncost i not a limiting factor.\n\nDefine\nRequirements\n\nGeneral Specialized:\n\ncoding in Python\n\nPublic and\nproprietary\n\nMore accurate:\nFaster and cheaper: big model (more\nSmall model expensive)\n\nDeepseek Coder 7b\nCode Llama 7b Python\nStarCoder2 3b\n\nCode Llama 70b Instruct\nCode Llama 70b Python\n\nQwen2.5 Coder 32b\nPhind CodeLlama 34b\n\nFast & accurate:\nmedium model\n(medium cost)",
      "content_length": 583,
      "extraction_method": "OCR"
    },
    {
      "page_number": 490,
      "chapter": null,
      "content": "The process begins with defining requirements and progresses through the\nfollowing decision points:\n1. General-purpose vs. Specialized Models: The focus is on specialized\nLLMs optimized for code generation, particularly for Python. These\nmodels are better suited to the task than general-purpose models.\n2. Privacy and Confidentiality: Given the sensitivity of the proprietary\ntrading libraries, company policy mandates that the code cannot be\nexposed externally. This requires selecting an open-source model to\nensure data remains within the organization.\n3. Model Size: While cost is not a concern, the model must balance\naccuracy and speed. A medium-sized model is ideal to achieve this\ntrade-off. The selection process narrows the options to a shortlist of\nseven LLMs, including general-purpose code generators and Python-\nspecific models.\n4. Accuracy Evaluation: The shortlisted models are evaluated using the\nPython HumanEval score (refer to Table C.3, sourced from Hugging\nFace leaderboard at: https://huggingface.co/spaces/bigcode/bigcode-\nmodels-leaderboard), which measures Python code generation accuracy.\nQwen2.5-Coder-32b is chosen for its superior balance of accuracy and\nspeed compared to other options.\nTable C.3 Python HumanEval rankings, sourced from the Hugging Face leaderboard\nModel\nhumaneval-python\nQwen2.5-Coder-32B-Instruct\n83.20\nDeepSeek-Coder-7b-instruct\n80.22\nCodeLlama-70b-Instruct\n75.60\nPhind-CodeLlama-34B-v2\n71.95\nCodeLlama-70b-Python\n55.49\nCodeLlama-7b-Python\n40.48\nStarCoder2-7B\n34.09\nAlthough Qwen2.5-Coder-32b has been selected as the top candidate, it is\nvaluable to evaluate other strong alternatives, including Phind CodeLlama\n34b, Deepseek Coder 7b, and Code Llama 7b Python.\n",
      "content_length": 1715,
      "extraction_method": "Direct"
    },
    {
      "page_number": 491,
      "chapter": null,
      "content": "C.3 A Word of Caution\nLLMs Large Language Models (LLMs) have revolutionized natural language\nprocessing, but they come with limitations and risks you need to understand.\nBias: LLMs can inherit and reproduce biases present in their training\ndata, which may result in harmful stereotypes or unfair outcomes.\nAddressing these issues is crucial for building ethical and trustworthy AI\nsystems. If your application involves sensitive user profiles, it's\nimportant to implement guardrails—such as content filtering, human\noversight, or prompt engineering—and to evaluate multiple LLMs to\nidentify the one that exhibits the least bias in your specific use case.\nPrivacy and Security: Proprietary LLMs may use prompt data for model\nimprovement, which can raise privacy and confidentiality concerns. To\nmitigate this risk, consider deploying open-source models in a private\nenvironment where you retain full control over data handling. Some\nproviders, such as OpenAI, offer enterprise plans that guarantee prompt\ndata will not be used for training. However, it ultimately comes down to\nwhether you trust the provider to uphold their data privacy\ncommitments.\nHallucinations: LLMs sometimes produce incorrect or fabricated\nanswers, known as hallucinations. Transparency and explainability are\nkey when users question an output. Tools like LangChain’s evaluation\nframework help reduce hallucinations by improving response accuracy.\nResponsibility and Liability: The legal landscape for LLMs remains\nunclear. Determining who is responsible for errors or harm caused by\nLLM outputs is complex. Providers often include disclaimers in their\nterms and chatbot interfaces to limit liability. When deploying an LLM,\nconsider accountability standards and define clear usage guidelines.\nIntellectual Property (IP) Rights: LLMs trained on unrestricted data can\ninadvertently generate content that violates IP laws. Some models\naddress this by training exclusively on public domain material to avoid\nlegal issues. If your application requires strict IP compliance, verify that\nyour chosen LLM avoids proprietary or copyrighted content.\nUnderstanding these challenges ensures you deploy LLMs responsibly\nand avoid unintended risks.\n",
      "content_length": 2209,
      "extraction_method": "Direct"
    },
    {
      "page_number": 492,
      "chapter": null,
      "content": "Appendix D. Installing SQLite on\nWindows\nSQLite does not require a full installation. Simply unzip the package, place it\nin a folder, and add the folder to your system's Path environment variable.\nFollow these steps for Windows setup. For other operating systems, refer to\nthe SQLite documentation.\n1. Download SQLite:\nGo to the SQLite download page:\nhttps://www.sqlite.org/download.html.\nDownload the latest zipped tools package, e.g., sqlite-tools-\nwin-x64-3460100.zip.\n2. Extract Files:\nUnzip the downloaded file to a folder, for instance, C:\\sqlite.\nAfter unzipping, you should see the files, including the SQLite\nexecutable, in C:\\sqlite\\sqlite-tools-win-x64-3460100.\n3. Add SQLite to the System Path:\nOpen the Start menu, go to Control Panel, and search for “edit\nsystem environment variables.”\nIn System Properties, click the Environment Variables button.\nIn the System variables section, select Path and click Edit.\nAdd C:\\sqlite\\sqlite-tools-win-x64-3460100 at the end of the\nlist, then click OK to close all dialog boxes.\n4. Verify Installation:\nOpen a new command shell and type sqlite3. If everything is set\nup correctly, you’ll enter the SQLite prompt where you can start\ncreating and managing databases.\nWith SQLite installed and configured, you can now create the database for\nthe examples in Chapter 9.\n",
      "content_length": 1319,
      "extraction_method": "Direct"
    },
    {
      "page_number": 493,
      "chapter": null,
      "content": "Appendix E. Open-source LLMs\nThis appendix covers\nAdvantages of open-source LLMs: flexibility, transparency, and control.\nPerformance benchmarks and key features of leading open-source\nLLMs.\nChallenges of local deployment and strategies to address them.\nSelecting an optimal inference engine for your use case.\nIn earlier chapters, you worked with OpenAI's public REST API. It’s a\nstraightforward way to build LLM applications since you don’t need to set up\na local LLM host. After signing up with OpenAI and generating an API key,\nyou can send requests to their endpoints and access LLM capabilities. This\nquick setup lets you work with state-of-the-art models like GPT-4o, 4o-mini,\nor o1 and o3 efficiently. The main drawback is cost—running examples like\nsummarization might cost a few cents or even dollars. If you're working on\nprojects for your company, privacy might also be a concern. Some employers\nblock OpenAI entirely to avoid the risk of leaking sensitive or proprietary\ndata.\nThis chapter introduces open-source LLMs, a practical solution for reducing\ncosts and addressing privacy concerns. These models are especially appealing\nto individuals and organizations that prioritize data confidentiality or are new\nto AI. I’ll guide you through the most popular open-source LLM families,\ntheir features, and the advantages they offer. The focus will be on running\nthese models, ranging from high-performance, advanced setups to user-\nfriendly tools ideal for learning and experimentation.\nFinally, I’ll show you how to transition the summarization and QA systems\nyou built earlier to a local open-source LLM. By the end of this chapter,\nyou’ll understand open-source LLMs well and feel confident using them\nwhen they’re the right choice.\n",
      "content_length": 1747,
      "extraction_method": "Direct"
    },
    {
      "page_number": 494,
      "chapter": null,
      "content": "E.1 Benefits of open-source LLMs\nOpen-source Large Language Models (LLMs) offer clear advantages in cost,\nprivacy, and flexibility. They provide control over data, lower costs by\navoiding licensing fees, and allow customization. Community-driven\ndevelopment fuels innovation, making these models competitive with\nproprietary ones. This section explores the key benefits of open-source\nLLMs.\nE.1.1 Transparency\nClosed-source models often function as \"black boxes,\" making them difficult\nto understand and potentially problematic for compliance, especially in\nindustries like healthcare and finance. Their lack of transparency limits\ncustomizability and creates challenges for auditing and accountability.\nOpen-source LLMs, by contrast, are transparent in their architecture, training\ndata, and methods. Organizations can inspect, validate, and modify the code\nto suit specific needs, ensuring compliance and fostering trust. Developers\ngain flexibility to extend and adapt models for unique applications,\nimproving control and oversight.\nTransparent origins enhance trust in the model's integrity, offering verifiable\nassurances instead of relying on blind faith. This clarity also helps address\npotential privacy risks. However, with transparency comes responsibility—\nusers bear accountability for flaws and financial impacts. Open access can\npose cybersecurity risks if not adequately protected.\nE.1.2 Privacy\nPrivacy and security are critical, particularly in regulated industries like\nfinance and healthcare. Leaks of sensitive data or Personally Identifiable\nInformation (PII) can harm trust and reputation. Using proprietary public-\ncloud LLMs may raise concerns about security and intellectual property risks,\nespecially for companies processing sensitive data.\nOpen-source LLMs mitigate these concerns by enabling on-premises or\n",
      "content_length": 1837,
      "extraction_method": "Direct"
    },
    {
      "page_number": 495,
      "chapter": null,
      "content": "private cloud deployment. Data stays within the corporate network, reducing\nexposure risks. These models can be customized to meet privacy needs,\nincluding implementing security protocols, content filtering, and data\nanonymization. They also support compliance with privacy laws and industry\nstandards.\nAs privacy regulations grow stricter, open-source LLMs become increasingly\nappealing to organizations seeking control over their data.\nE.1.3 Community driven\nProprietary LLMs are typically developed with a fixed vision by a single\norganization. Open-source LLMs, however, benefit from contributions by a\ndiverse community of developers, ranging from individuals to enterprises.\nThese contributions improve features, provide flexibility, and foster growth\nthrough public forks and private customizations.\nThe open-source community produces a wide variety of models and training\nmethods, advancing the field rapidly. While prominent contributors influence\ndirection, the open nature allows anyone to contribute, driving collaboration\nand innovation. This dynamic reduces the performance gap with proprietary\nmodels as advancements in architecture, datasets, and training methods\nevolve.\nHowever, community projects can face challenges, including disputes or\nmismanagement, which may slow progress. Despite these risks, open-source\nmodels empower smaller organizations to compete with industry leaders,\nleveling the playing field.\nChoosing between proprietary and open-source LLMs depends on your\npriorities. If control, customization, and community engagement are essential,\nopen-source models are ideal. If strict SLAs or turnkey solutions are more\ncritical, proprietary models might be better suited. The decision depends on\nyour project’s needs and goals.\nE.1.4 Cost Savings\n",
      "content_length": 1779,
      "extraction_method": "Direct"
    },
    {
      "page_number": 496,
      "chapter": null,
      "content": "Open-source LLMs are typically more cost-effective than proprietary models\ndue to the lack of licensing fees. However, deploying them on-premises may\ninvolve significant upfront capital costs, and running them in the cloud can\nincur ongoing operational expenses. Even with these factors, the total cost of\nownership (TCO) for open-source models is usually lower over the medium\nto long term compared to the recurring fees of proprietary LLMs. The right\nchoice depends on your use case, expected text processing volume, and\nreadiness to handle initial deployment costs.\nFor low initial usage, a pay-as-you-go proprietary model might be more\npractical. As usage grows and the client base justifies investment in\ninfrastructure, transitioning to an open-source model can save money. If you\nalready have the skills to deploy and manage an open-source LLM, starting\nlocally might be cost-effective. However, you must consider hidden costs,\nsuch as time spent by staff on setup and maintenance.\nThe cost-effectiveness of an LLM also depends on its application. Proprietary\nvendors charge based on the number of tokens processed, which can become\nexpensive for tasks like summarizing large amounts of text. In such cases, an\nopen-source model may reduce costs. On the other hand, for applications\nusing efficient strategies like Retrieval Augmented Generation (RAG) with\nminimal text processing, proprietary models might be more economical.\nAn additional advantage of open-source LLMs is the ability to fine-tune them\nthrough specialized training, creating a custom model. However, this\ninvolves expenses such as consultancy fees, staff time, computational\nresources, and extended timelines. In cases where no proprietary model suits\nyour specific domain, building and fine-tuning an open-source LLM may be\nthe only option, albeit at your own expense.\nE.2 Popular open-source LLMs\nThe world of open-source Large Language Models (LLMs) is moving fast,\nmaking it tricky to figure out which ones stand out. To help, I’ve pulled\ntogether key details about popular open-source LLMs into a series of tables,\nstarting with Table E.1. This table gives a straightforward overview of some\nof the most interesting open-source LLMs available just before this book was\n",
      "content_length": 2249,
      "extraction_method": "Direct"
    },
    {
      "page_number": 497,
      "chapter": null,
      "content": "published. Models are grouped by their type, listed under the “Model Type”\ncolumn:\nFoundation: General-purpose models trained on raw data. Great for\nresearch, experimenting, or fine-tuning, but not usually ready for direct\nuse with end-users.\nInstruction: Models fine-tuned to follow instructions or handle question\nanswering.\nChat: Models tailored for interactive conversations, similar to ChatGPT.\nCode: Built to generate, explain, or debug code.\nDomain-Specific: Designed for specialized industries or business needs.\nThis setup makes it easier to see what each type of model is good at and how\nit fits your needs.\nTable E.1 Most popular open-source LLMs at the time of publication\nModel\nDeveloper HuggingFace\nURL\nModel\nType\nSize\nContext\nwindow\nDeepSeek-\nV3\nDeepseek\nAI\n/deepseek-\nai/DeepSeek-V3 \nFoundation 671B\n128K\nQwen2.5-\n72B-\nInstruct\nQwen\n/Qwen/Qwen2.5-\n72B-Instruct\nInstruct\n72B\n128K\nQwen2.5-\nCoder-\n32B\nQwen\n/Qwen/Qwen2.5-\nCoder-32B\nCode\n32B\n32K\nQwen2.5-\nMath-7B\nQwen\nQwen/Qwen2.5-\nMath-7B\nDomain\nspecific\n(Math)\n7B\n32K\nLlama-\n3.3-70B-\nInstruct\nMeta\nLlama\n/meta-\nllama/Llama-3.3-\n70B-Instruct\nInstruct\n70B\n128K\nMixtral-\n8x22B-\nInstruct-\nv0.1\nMistral AI /mistralai/Mixtral-\n8x22B-Instruct-\nv0.1\nInstruct\n141B\n64K\n",
      "content_length": 1224,
      "extraction_method": "Direct"
    },
    {
      "page_number": 498,
      "chapter": null,
      "content": "Mistral-\n7B-v0.3\nMistral AI mistralai/Mistral-\n7B-v0.3\nFoundation 7B\n32K\ngemma-2-\n27b-it\nGoogle\ngoogle/gemma-2-\n27b-it\nFoundation 27B\n8K\nFalcon3-\n10B-Base\nTII-UAE\ntiiuae/Falcon3-\n10B-Base\nFoundation 10B\n32K\nPhi-3.5-\nMoE-\ninstruct\nMicrosoft\nmicrosoft/Phi-3.5-\nMoE-instruct\nInstruct\n41.9B 128K\nTable E.2 shows the performance of these models based on standard\nbenchmarks, fully defined in Appendix C.\nTable E.2 Extract from the HuggingFace Open LLM Leaderboard, showing standard\nperformance benchmarks on the most popular open-source LLMs at the time of publication:\nhttps://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\nModel\nIfEval\nBBH\nMATH-\nL5\nGPQA\nMuSR\nMMLU-\nPRO\nDeepSeek-\nV3\n86.10\nN/A\nN/A\n59.10\nN/A\n75.90\nQwen2.5-\n72B-\nInstruct\n86.38\n61.87\n1.21\n16.67\n11.74\n51.40\nQwen2.5-\nCoder-32B\n43.63\n48.51\n30.59\n12.86\n15.87\n47.81\nQwen2.5-\nMath-7B\n24.60\n22.01\n30.51\n5.82\n5.00\n19.09\nLlama-3.3-\n70B-\nInstruct\n89.98\n56.56\n0.23\n11.51\n15.57\n48.13\nMixtral-\n8x22B-\nInstruct-\nv0.1\n71.84\n44.11\n18.73\n16.44\n13.49\n38.70\nMistral-7B-\n22.66\n23.95\n3.02\n5.59\n8.36\n21.70\n",
      "content_length": 1062,
      "extraction_method": "Direct"
    },
    {
      "page_number": 499,
      "chapter": null,
      "content": "v0.3\nGemma-2-\n27b-it\n79.78\n49.27\n0.76\n16.67\n9.11\n38.35\nFalcon3-\n10B-Base\n36.48\n41.38\n24.77\n12.75\n14.17\n36.00\nPhi-3.5-\nMoE-\ninstruct\n69.25\n48.77\n22.66\n14.09\n17.33\n40.64\nAs shown in the table table E.2, models from the same family can vary in\nsuitability depending on the use case. For instance, if you need an LLM for\nan instruction-based chatbot, you should look for one with a high IFEval\nscore. In that case, Qwen2.5-72B-Instruct is a strong option with an\nimpressive IFEval score of 86.38. On the other hand, if your focus is solving\nmath problems, Qwen2.5-Math-7B might be a better choice due to its high\nMATH score of 30.51, combined with the advantage of being smaller in size.\nNow that you know about popular open-source LLMs and their features, I’ll\nshow you how to run these models on your own computer. This will let you\nexperiment and explore their capabilities firsthand.\nE.3 Considerations on running open-source LLMs\nlocally\nLarge language models operate in two main phases: training and inference.\nDuring training, the model learns patterns from the training set, adjusting its\nweights (parameters). These weights are then used during inference to make\npredictions or respond to new inputs. Open-source LLM weights are usually\neasy to access, often available on platforms like Hugging Face or through\ntools like LM Studio or Ollama. Running the inference phase locally requires\nsuitable hardware. This section covers hardware requirements and how\nmodern techniques such as quantization make it feasible to run models even\non consumer-grade machines. An important consideration when hosting an\nLLM locally is whether it provides an OpenAI-compatible API, such as a\nREST API. Using OpenAI during the Proof of Concept (PoC) phase and\ntransitioning to an open-source LLM later can reduce costs or address privacy\n",
      "content_length": 1824,
      "extraction_method": "Direct"
    },
    {
      "page_number": 500,
      "chapter": null,
      "content": "concerns. If the inference engine supports an OpenAI-compatible API, you\ncan switch seamlessly to production without rewriting or retesting code.\nE.3.1 Limitations of consumer hardware\nHosting a large LLM in production demands powerful hardware. For\nexample, Llama-70B requires 140GB of RAM and disk space when loaded in\nsingle-precision (float16). High-end GPUs, such as NVIDIA’s A100 (priced\nat $10,000 or more), are essential for efficient inference. Cloud options are\navailable, but they are expensive, costing about $1.50 per hour for A100\nrentals.\nFor local experimentation, however, consumer hardware can still be viable.\nTechniques like quantization and specialized inference engines reduce the\nhardware burden, enabling LLMs to run on laptops or desktops with as little\nas 16GB or 32GB of RAM and no dedicated GPU. These methods make it\npossible to explore LLM functionality on modest machines.\nE.3.2 Quantization\nQuantization reduces LLM size by lowering the precision of weights, trading\nsome accuracy for faster and more efficient inference. Large models typically\nuse 16-bit or 32-bit floating-point precision (2 to 4 bytes per parameter).\nQuantization compresses this to 4-bit integer precision (0.5 bytes per\nparameter). Quantization offers a balance between size, speed, and accuracy.\nFor example, LLAMA-2-7B, originally 13.5GB with 16-bit precision, can be\nreduced to 3.9GB with 4-bit quantization, allowing it to run on modest\nlaptops.\nThe most common quantization techniques used are:\nPost-Training Quantization (PTQ): Applied after training; simpler but\nmay reduce accuracy.\nQuantization-Aware Training (QAT): Integrated during training for\nbetter accuracy but more complex.\nGPTQ: Combines GPT with PTQ techniques.\nNF4: Uses 4-bit Normal Float precision, often outperforming standard\n",
      "content_length": 1804,
      "extraction_method": "Direct"
    },
    {
      "page_number": 501,
      "chapter": null,
      "content": "4-bit integer quantization.\nGGML and GGUF: Tools by Georgi Gerganov for running models on\nCPUs. GGUF, an enhanced version, supports a wider range of open-\nsource models.\nSome quantization tools like bitsandbytes (by Tim Dettmers) are available\nthrough Python’s pip install and are well-documented on Hugging Face.\nQuantized models can be found on Hugging Face by searching terms like\nGGML or GGUF, or directly through inference engines such as Ollama or\nLM Studio.\nTIP\nFor a deeper dive into quantization, check out the following blog posts “What\nare Quantized LLMs\" by Miguel Carreira Neves\n(https://www.tensorops.ai/post/what-are-quantized-llms) and “How is\nllama.cpp possible?\" by Finbarr Timbers (https://finbarr.ca/how-is-llama-cpp-\npossible)\nModern advancements in quantization and optimized engines have\nsignificantly lowered the barrier to running open-source LLMs locally,\nmaking them accessible even for users with limited hardware, especially for\nlearning and experimentation.\nE.3.3 OpenAI REST API compatibility\nMany inference engines include an embedded HTTP server that accepts\nrequests through endpoints compatible with the OpenAI REST API. This\ncompatibility allows immediate use with standard OpenAI libraries,\nsimplifying the engine’s codebase and supporting quick adoption. For users,\nthe benefits include minimal learning requirements compared to learning\nproprietary APIs and the ability to swap engines seamlessly. With this\nstandardization, you can efficiently test multiple engines using the same\nclient code or replace an engine with one better suited to your needs.\nIn this section, I’ll provide code snippets which you can run against each\ninference engine I will introduce later. These snippets require minimal\n",
      "content_length": 1739,
      "extraction_method": "Direct"
    },
    {
      "page_number": 502,
      "chapter": null,
      "content": "modifications—often just adjusting the port number. They demonstrate how\nto make requests using raw HTTP (e.g., curl), the OpenAI Python library\nand LangChain. Let’s start with direct HTTP requests.\nDirect curl invocation\nThe easiest way to test an OpenAI-compatible REST API endpoint is by\nusing the /chat/completions endpoint. Below is an example with OpenAI’s\npublic service. Open a shell (on Windows you can open a command line\nwindow or a Git Bash shell) and replace YOUR-OPENAI-KEY with your actual\nOpenAI API key:\n$ curl https://api.openai.com/v1/chat/completions   -H \"Content-T\n   \"model\": \"gpt-4o-mini\",\n   \"messages\": [\n   { \"role\": \"system\", \"content\": \"You are an helpful AI assista\n   { \"role\": \"user\", \"content\": \"How many Greek temples are in P\n   ],\n   \"temperature\": 0.7\n }'\nThe response might look like this:\n % Total    % Received % Xferd  Average Speed   Time    Time     \n                                Dload  Upload   Total   Spent    \n100   810  100   578  100   232    209     83  0:00:02  0:00:02 -\n{\n   \"id\": \"chatcmpl-AjANT94Lel2oZIOz8kmF0PKWfRuGa\",\n   \"object\": \"chat.completion\",\n   \"created\": 1735328015,\n   \"model\": \"gpt-4o-mini-2024-07-18\",\n   \"choices\": [\n       {\n           \"index\": 0,\n           \"message\": {\n               \"role\": \"assistant\",\n               \"content\": \"Paestum, an ancient Greek city locate\n               \"refusal\": null\n           },\n           \"logprobs\": null,\n           \"finish_reason\": \"stop\"\n       }\n   ],\n",
      "content_length": 1472,
      "extraction_method": "Direct"
    },
    {
      "page_number": 503,
      "chapter": null,
      "content": "   \"usage\": {\n       \"prompt_tokens\": 28,\n       \"completion_tokens\": 163,\n       \"total_tokens\": 191,\n       \"prompt_tokens_details\": {\n           \"cached_tokens\": 0,\n           \"audio_tokens\": 0\n       },\n       \"completion_tokens_details\": {\n           \"reasoning_tokens\": 0,\n           \"audio_tokens\": 0,\n           \"accepted_prediction_tokens\": 0,\n           \"rejected_prediction_tokens\": 0\n       }\n   },\n   \"system_fingerprint\": \"fp_0aa8d3e20b\"\n}\nTo make the same request to a local open-source LLM, adjust the command\nas follows:\ncurl https://localhost:8000/v1/chat/completions   -H \"Content-Typ\n   \"model\": \"mistralai/Mistral-7B-v0.3\",\n   \"messages\": [\n   { \"role\": \"system\", \"content\": \"You are a helpful AI assistan\n   { \"role\": \"user\", \"content\": \"How many Greek temples are ther\n   ],\n   \"temperature\": 0.7\n }'\nNote\nFor local inference engines, you do not need to include an OpenAI key in the\nrequest header, so it has been omitted in the example. However, you need to\nadjust two key details in the `curl` request based on the inference engine and\nLLM you are using:\n1. Port Number: Each inference engine’s local HTTP server is set to a\ndefault port, which is often different from 8000. Refer to the engine's\ndocumentation and update the port number as needed.\n2. Model Name: Inference engines use specific naming conventions for\nmodels. Some follow Hugging Face conventions (e.g., 'mistralai/Mistral-\n7B-v0.3'), while others may not. Check the engine's documentation to\n",
      "content_length": 1484,
      "extraction_method": "Direct"
    },
    {
      "page_number": 504,
      "chapter": null,
      "content": "ensure you provide the correct model name.\nPython openai library\nTo use the OpenAI library for requests, create a virtual environment, install\nthe library with pip install openai, create a Jupyter notebook, as you have\nseen in the previous chapters, and use the code from listing E.1 (after\nreplacing YOUR-OPENAI-KEY with your OpenAI key).\nListing E.1 Calling the OpenAI completions endpoint\nimport getpass\nfrom openai import OpenAI\nOPENAI_API_KEY = getpass.getpass('Enter your OPENAI_API_KEY')\nclient = OpenAI(\n   api_key = OPENAI_API_KEY\n)\ncompletion = client.chat.completions.create(\n model=\"gpt-4o-mini\",\n messages=[\n   { \"role\": \"system\", \"content\": \"You are a helpful AI assistan\n   { \"role\": \"user\", \"content\": \"How many Greek temples are ther\n ],\n temperature=0.7\n)\nprint(completion.choices[0].message.content)\nYou will get output similar to this:\nPaestum, an ancient Greek city located in present-day Italy, is r\nIn order to run the code above against a local open-source LLM, you need to\nadapt it as follows (note the extra base_url parameter when instantiating the\nOpenAI client):\nfrom openai import OpenAI\nport_number = '8080' #A\nclient = OpenAI(\n",
      "content_length": 1159,
      "extraction_method": "Direct"
    },
    {
      "page_number": 505,
      "chapter": null,
      "content": "   base_url=f'http://localhost:{port_number}/v1', \n   api_key = \"NO_KEY_NEEDED\"\n)\ncompletion = client.chat.completions.create(\n model=\"mistral\",\n messages=[\n   { \"role\": \"system\", \"content\": \"You are a helpful AI assistan\n   { \"role\": \"user\", \"content\": \"How many Greek temples are ther\n ],\n temperature=0.7\n)\nprint(completion.choices[0].message.content)\nNote\nTo adapt the example above for your inference engine and local open-source\nLLM, update the port number to match the engine's local HTTP server and\nset the model name according to the engine's specific naming convention. For\nexample, if using LM studio, use port 8080.\nLangChain's openai wrapper\nFor LangChain, direct its OpenAI wrapper to the local inference engine by\nupdating the base URL:\nfrom langchain_openai import ChatOpenAI\nport_number = '8080'\nllm = ChatOpenAI(openai_api_base=f'http://localhost:{port_number}\nresponse = llm.invoke(\"How many Greek temples are there in Paestu\nprint(response.content)\nNOTE\nEnsure the port matches the local engine configuration.\nNow, let's explore the essential component required to execute open-source\nmodels on regular consumer hardware: local inference engines.\n",
      "content_length": 1167,
      "extraction_method": "Direct"
    },
    {
      "page_number": 506,
      "chapter": null,
      "content": "E.4 Local inference engines\nRunning an open-source LLM on consumer hardware is most practical with\nan inference engine. These engines host the local model and handle requests\nfrom client applications, often through native bindings for languages like\nPython, JavaScript, Java, C++, Go, or Rust. Many inference engines also\ninclude a local HTTP server with OpenAI REST API-compatible endpoints,\nallowing you to use familiar OpenAI libraries or frameworks like LangChain\nwithout significant changes. This flexibility lets you switch between local\nopen-source LLMs and OpenAI’s public service with minimal effort.\nIn the following sections, I will introduce several inference engines,\nbeginning with foundational tools like llama.cpp, optimized for high\nperformance, and moving to user-friendly options like Ollama and\nproduction-grade solutions such as vLLM. I will also cover consumer-\nfocused alternatives, including LocalAI (a simplified wrapper for engines like\nllama.cpp or vLLM), GPT4All, and LM Studio, which are notable for their\nintuitive user interfaces. These options will help you select the best engine\nbased on your hardware, experience, and project requirements.\nAs shown in Figure E.1, which serves as a guide for this section, llama.cpp\nand vLLM are foundational backends for many inference engines. Higher-\nlevel tools like Ollama, llamafile, and LocalAI, along with user-friendly\nengines such as GPT4All and LM Studio, are built on llama.cpp. Meanwhile,\nvLLM functions independently, with LocalAI being the only tool currently\nbuilding on it.\nFigure E.1 Lineage and functionality of local inference engines\n",
      "content_length": 1623,
      "extraction_method": "Direct"
    },
    {
      "page_number": 507,
      "chapter": null,
      "content": "Let’s start our tour with llama.cpp.\nE.4.1 Llama.cpp\nllama.cpp was one of the first engines designed to run open-source models\nefficiently on consumer hardware. Initially developed to support the LLaMA\nmodel using 4-bit integer quantization for Apple silicon GPUs, it has since\nexpanded to support Linux, Windows (x86 processors), and even Raspberry\nPi. It now handles a wide range of quantized models, including Mistral,\nGemma, Phi and Falcon. It supports 2-bit to 8-bit integer quantization and\noffers bindings for Python, Java, C#, Go, Scala, and Ruby.\n",
      "content_length": 556,
      "extraction_method": "Direct"
    },
    {
      "page_number": 508,
      "chapter": null,
      "content": "Setting up llama.cpp\nTo use llama.cpp, follow these steps:\n1. Build the Executable: Download the source code from GitHub and build\nit using your preferred strategy, such as make, CMake, Zig, or gmake.\nAdvanced build options include Metal, MPI, and BLAS for enhanced\nperformance.\n2. Prepare the Model Weights: Obtain a quantized version of the model\n(e.g., Mistral-7B-Instruct-v0.2-GGUF) from Hugging Face or\ngenerate it using GitHub instructions. Ensure the model fits within your\nsystem's disk and RAM capacity.\n3. Run Inference: Execute the inference command, pointing to the\nquantized model file:\n  ./main -m ./models/mistral-7b-instruct-v0.2.Q2_K.gguf -p \"How \nNOTE\nFor detailed instructions, refer to the official GitHub page:\nhttps://github.com/ggerganov/llama.cpp.\nPython bindings\nIf you prefer a higher-level API, links to relevant bindings are available on\nthe llama.cpp GitHub page. Before installation, review both the llama.cpp\ndocumentation and the selected bindings' documentation to prevent redundant\nsetup.\nTo install the Python bindings from PyPI, use the following command:\npip install llama-cpp-python\nThis command not only downloads the library but also tries to build\nllama.cpp from source using CMake and your system's C compiler. For GPU\nsupport, follow additional instructions and configurations.\nAfter setup, you can use Python to interact with your local quantized LLM\n",
      "content_length": 1395,
      "extraction_method": "Direct"
    },
    {
      "page_number": 509,
      "chapter": null,
      "content": "instance. Listing E.2 shows an adapted example from the official llama-cpp-\npython bindings documentation.\nListing E.2 Executing prompts against a local quantized Mistral-7B-Instruct instance\n### Adapted from official documentation at\n### https://github.com/abetlen/llama-cpp-python\nfrom llama_cpp import Llama\nllm = Llama(\n     model_path=\"./models/mistral-7b-instruct-v0.2.Q2_K.gguf\"\n)\noutput = llm(\n     \"Q: What are the planets in the solar system? A: \", # Promp\n     max_tokens=32, # Generate up to 32 tokens, set to None to g\n     stop=[\"Q:\", \"\\n\"], # Stop generating just before the model w\n     echo=True # Echo the prompt back in the output\n) \nprint(output)\nYou get the following output:\n{\n \"id\": \"cmpl-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n \"object\": \"text_completion\",\n \"created\": 1679561337,\n \"model\": \"./models/mistral-7b-instruct-v0.2.Q2_K.gguf\",\n \"choices\": [\n   {\n     \"text\": \"Q: Name the planets in the solar system? A: Mercur\n     \"index\": 0,\n     \"logprobs\": None,\n     \"finish_reason\": \"stop\"\n   }\n ],\n \"usage\": {\n   \"prompt_tokens\": 14,\n   \"completion_tokens\": 28,\n   \"total_tokens\": 42\n }\n}\nLangChain integration\nLangChain’s LlamaCpp client can connect directly to a quantized local\n",
      "content_length": 1207,
      "extraction_method": "Direct"
    },
    {
      "page_number": 510,
      "chapter": null,
      "content": "instance of Llama-based models, provided you have installed the llama-cpp-\npython library (https://github.com/abetlen/llama-cpp-python):\nfrom langchain_community.llms import LlamaCpp\nllm = LlamaCpp(model_path=\"./models/llama-2-7b-chat.ggmlv3.q2_K.b\nOpenAI API compatible endpoints\nllama.cpp includes a local HTTP server that provides OpenAI REST API-\ncompatible endpoints. To try it out, you can use the `curl` script or Python\nexamples from section 11.3.3, ensuring the port number and model name are\ncorrectly configured.\nWhile detailed installation and setup instructions for llama.cpp are not\nincluded here, it is important to understand its basics. Many of the local\ninference engines discussed later are built on llama.cpp or draw inspiration\nfrom its design. Next, I’ll introduce Ollama.\nE.4.2 Ollama\nOllama is a hosting environment for open-source LLMs, available for\nWindows, macOS, and Linux. Unlike llama.cpp, Ollama does not require\nyou to build executables from source; installation packages are readily\navailable.\nInteractive mode\nAfter downloading and installing Ollama from the Ollama.ai homepage\n(https://ollama.ai), launch the Ollama client (in Windows, launch it from the\nStart menu). This opens a terminal (or PowerShell window on Windows),\nwhere you can immediately run an LLM using the following command:\nollama run mistral\nThis command downloads a quantized version of the selected model (Mistral\nin this example) from the remote Ollama library. The terminal will display\nthe download progress, as shown in Figure 11.2.\n",
      "content_length": 1543,
      "extraction_method": "Direct"
    },
    {
      "page_number": 511,
      "chapter": null,
      "content": "Figure 11.2 Screenshot of ollama terminal while installing the Mistral LLM\nAt this stage, you can send a prompt and receive a response, as shown below\n(note the response may be slow):\n>>> How many Greek temples are in Paestum?\nThere are three Greek temples in Paestum, which is a town on the \n1. Temple of Hera I (also known as the Basilica) - This temple wa\ngoddess Hera. It is the largest and most impressive of the three \n2. Temple of Neptune - This temple was also built around 550 BC a\nNeptune in Roman mythology). It is smaller than the Temple of Her\n3. Temple of Athena - This temple was built around 460 BC and is \nsmallest of the three temples and is incomplete due to damage sus\nAll three temples are well-preserved and are a popular tourist de\nSite.\nOllama natively supports several popular open-source LLMs, including\nLlama, Phi, and Gemma. You can browse the available models on the library\npage: https://ollama.com/library. To run a GGUF model from another source,\nsuch as Hugging Face, copy the model to a local folder and import it\n",
      "content_length": 1048,
      "extraction_method": "Direct"
    },
    {
      "page_number": 512,
      "chapter": null,
      "content": "following the instructions in the Ollama documentation.\nServer mode\nOnce the server is running, it exposes a REST API on port 11434, ready to\naccept requests for LLM interactions.\nNote\nOllama’s REST API endpoints are proprietary and not compatible with\nOpenAI’s specifications. Code examples from earlier sections cannot be used\ndirectly.\nBelow is an examples using Ollama’s API from curl (or Postman, if you\nprefer).\ncurl http://localhost:11434/api/generate -d '{\n \"model\": \"mistral\", \n \"prompt\":\"How many Greek temples are in Paestum?\"\n}’\nBy default, the output streams one word at a time, each included in a separate\nJSON object, as shown below:\n{\n   \"model\": \"mistral\",\n   \"created_at\": \"2024-12-28T11:52:26.1375731Z\",\n   \"response\": \" There\",\n   \"done\": false\n}\n{\n   \"model\": \"mistral\",\n   \"created_at\": \"2024-12-28T11:52:26.5276658Z\",\n   \"response\": \" are\",\n   \"done\": false\n}\n{\n   \"model\": \"mistral\",\n   \"created_at\": \"2024-12-28T11:52:26.9259504Z\",\n   \"response\": \" three\",\n   \"done\": false\n}\n",
      "content_length": 1001,
      "extraction_method": "Direct"
    },
    {
      "page_number": 513,
      "chapter": null,
      "content": "…\nIf you prefer to wait for the full response, you must set the “stream” attribute\nto false:\ncurl http://localhost:11434/api/generate -d '{\n \"model\": \"mistral\", \n \"prompt\":\"How many Greek temples are in Paestum?\",\n \"prompt\":false\n}’\nYou will get a single response object:\n{\n   \"model\": \"mistral\",\n   \"created_at\": \"2024-12-28T13:07:03.1862002Z\",\n   \"response\": \" There are three Doric temples in Paestum, locat\n   \"done\": true,\n   \"done_reason\": \"stop\",\n   \"context\": [\n       3,\n       29473,\n      … SHORTENED,\n       1102,\n       29499\n   ],\n   \"total_duration\": 40777816100,\n   \"load_duration\": 13459239600,\n   \"prompt_eval_count\": 16,\n   \"prompt_eval_duration\": 1454000000,\n   \"eval_count\": 123,\n   \"eval_duration\": 25791000000\n}\nThis is a request to the /chat endpoint, using a payload format compatible\nwith OpenAI's corresponding request structure:\ncurl http://localhost:11434/api/chat -d '{\n \"model\": \"mistral\",\n \"messages\": [\n   { \"role\": \"system\", \"content\": \"You are a helpful AI assistan\n   { \"role\": \"user\", \"content\": \"How many Greek temples are ther\n ],\n \"temperature\":0.7\n}’\n",
      "content_length": 1092,
      "extraction_method": "Direct"
    },
    {
      "page_number": 514,
      "chapter": null,
      "content": "These API calls allow interaction with the LLM using any programming\nlanguage. However, for Python or JavaScript, you can use libraries provided\nby Ollama that wrap the low-level REST API, simplifying client\ndevelopment.\nNative Python library\nTo get started, install the Ollama Python library:\npip install ollama\nYou can then interact with the LLM's /chat endpoint as shown:\nimport ollama\nresponse = ollama.chat(model='mistral', messages=[\n   { \"role\": \"system\", \"content\": \"You are an helpful AI assista\n   { \"role\": \"user\", \"content\": \"How many Greek temples are in P\n])\nprint(response['message']['content'])\nAlternatively, you can send instructions to the /generate endpoint:\nollama.generate(model='mistral', prompt=’How many Greek temples a\nTo enable streaming responses, set stream=True in the chat or generate\ncall.\nIf you need to specify a custom host or timeout, create a Client object:\nfrom ollama import Client\nclient = Client(host='http://mylocalserver:8000')\nresponse = client.chat(model='mistral', messages=[\n   { \"role\": \"system\", \"content\": \"You are an helpful AI assista\n   { \"role\": \"user\", \"content\": \"How many Greek temples are in P\n])\nFor asynchronous execution, use AsyncClient with asyncio:\nimport asyncio\nfrom ollama import AsyncClient\nasync def chat():\n",
      "content_length": 1277,
      "extraction_method": "Direct"
    },
    {
      "page_number": 515,
      "chapter": null,
      "content": "   message = {'role': 'user', 'content': ’How many Greek temples \n   response = await AsyncClient().chat(model='mistral', messages\n   print(response['message']['content'])\nasyncio.run(chat())\nLangChain integration\nLangChain provides a wrapper for the Ollama Python library, integrating it\nwith LangChain interfaces such as the Runnable interface. Below is an\nexample of re-implementing a basic synchronous LLM invocation using\nLangChain:\nfrom langchain_community.llms import Ollama\nollama = Ollama(model='mistral')\nquery = 'How many Greek temples are in Paestum?'\nresponse  = llm.invoke(query)\nprint(response)\nTo handle streaming responses, you can use the following approach:\nfor chunks in llm.stream(query):\n   print(chunks)\nFor more information, I recommend consulting the Ollama documentation on\nGitHub. It includes community-driven integrations, such as web and desktop\nUIs, terminal plugins for Emacs and Vim, and other useful extensions.\nOllama is designed to help you explore open-source models locally. If you\ndecide to move beyond a proof of concept on a Linux system and build a\nproduction-grade solution using non-quantized LLMs on powerful GPU\nhardware, potentially in the cloud, consider exploring vLLM. While vLLM is\nnot specifically for consumer-grade hardware, it demonstrates the capabilities\nof inference engines designed to host open-source models of any size on\nvarious types of hardware.\nE.4.3 vLLM\nvLLM is a high-performance Python library for LLM inference, built on a\n",
      "content_length": 1493,
      "extraction_method": "Direct"
    },
    {
      "page_number": 516,
      "chapter": null,
      "content": "C++ core and CUDA binaries. It targets Linux systems and high-grade GPUs,\nincluding V100, T4, RTX20xx, A100, L4, and H100. According to the\nofficial web page (https://docs.vllm.ai), vLLM offers the following advanced\nperformance features:\nHigh-Throughput Serving: State-of-the-art inference speed.\nPagedAttention: Efficient memory management for attention key and\nvalue data.\nContinuous Batching: Dynamically batches incoming requests for better\nefficiency.\nCUDA/HIP Graph Execution: Speeds up model execution.\nQuantization Support: Includes GPTQ, AWQ, and SqueezeLLM\nmethods.\nvLLM is designed with flexibility and usability in mind. Key features include:\nHuggingFace Integration: Seamless support for popular HuggingFace\nmodels.\nAdvanced Decoding Algorithms: Supports parallel sampling, beam\nsearch, and other methods.\nTensor Parallelism: Enables distributed inference across multiple GPUs.\nStreaming Outputs: Provides real-time response generation.\nOpenAI-Compatible API: Allows easy integration with existing\napplications.\nGPU Support: Works with both NVIDIA and AMD GPUs.\nvLLM supports a variety of architectures, including BLOOM, Falcon, GPT-J,\nGPT-NeoX, Vicuna, LLaMA, and Mistral. While it can handle quantized\nmodels, vLLM is optimized for larger model versions, such as LLaMA-2-\n70B-hf and Falcon-180B. For a detailed list of supported models, refer to the\nSupported Models section of the vLLM documentation.\nInstallation\nTo install vLLM, ensure you are using Linux with one of the recommended\nhigh-grade GPUs. It is best to use a dedicated virtual environment created\nwith venv. Install the library with:\n",
      "content_length": 1615,
      "extraction_method": "Direct"
    },
    {
      "page_number": 517,
      "chapter": null,
      "content": "pip install vllm\nOnce installed, you can activate the server mode.\nServer mode\nStart a local OpenAI-compatible HTTP server with the following command\nin a separate terminal:\n$ python -m vllm.entrypoints.openai.api_server --model mistralai/\nThis will download the model from Hugging Face (if not already\ndownloaded) and activate an HTTP server on port 8000, exposing OpenAI-\ncompatible endpoints.\nOpenAI REST API compatible endpoints\nRefer to section E.3.3 for examples of curl, Python, and LangChain code\nthat you can use with vLLM. Ensure you set the port to 8000 and use the\ncorrect model name. Consult the vLLM documentation for further details.\nOffline batched inference\nvLLM offers a unique capability for high-performance offline batched\ninference, allowing you to process multiple requests in a single operation.\nThis feature sets it apart from other local inference engines. To use it, import\nthe LLM and SamplingParams modules, create a list of prompts, and process\nthem using an LLM instance. The results can be iterated and reviewed, as\nshown in Listing E.3.\nListing E.3 vLLM offline batched inference\nfrom vllm import LLM, SamplingParams\nprompts = [ #A\n   \"How many temples are there in Paestum?\",\n   \"Who built the aqueduct in Segovia?\",\n   \"Is LeBron James better than Michael Jordan?\",\n   \"Summarize the Lord of The Rings in three sentences.\",\n",
      "content_length": 1359,
      "extraction_method": "Direct"
    },
    {
      "page_number": 518,
      "chapter": null,
      "content": "]\nsampling_params = SamplingParams(temperature=0.7, top_p=0.95) #B\nllm = LLM(model=\"mistralai/Mistral-7B-v0.1\") #C\noutputs = llm.generate(prompts, sampling_params) #D\nfor output in outputs: #E\n   prompt = output.prompt\n   llm_response = output.outputs[0].text\n   print(f\"Prompt: {prompt!r}, LLM response: {llm_response!r}\")\nAfter presenting the advanced capabilities of vLLM, designed for deploying\nopen-source LLMs in professional production environments, I will now\nreturn to exploring local inference engines built for consumer-grade\nhardware.\nE.4.4 llamafile\nOne of the simplest ways to run an LLM locally is with llamafile. According\nto its GitHub description, llamafile allows you to \"distribute and run an LLM\nwith a single file.\" This executable combines the quantized weights of an\nLLM in GGUF format with the llama.cpp C++ executable, packaged using\nCosmopolitan-Libc. Cosmopolitan-Libc, described as \"making C a build-\nonce, run-anywhere language,\" enables the llamafile to run on macOS, Linux,\nand Windows (with a .exe extension) without requiring any installation. This\napproach sets an incredibly low barrier for experimenting with local LLMs.\nThe llamafile GitHub page offers a variety of prebuilt LLMs, including recent\nversions of LLaMA, Gemma, and Phi.\nServer mode\nRunning an LLM with llamafile is straightforward. Follow these steps (refer\nto the GitHub project for detailed instructions):\n4. Download a llamafile: Obtain a prebuilt file, such as mistral-7b-\ninstruct-v0.2.Q5_K_M.llamafile, from the GitHub project page\n(https://github.com/Mozilla-Ocho/llamafile).\n",
      "content_length": 1584,
      "extraction_method": "Direct"
    },
    {
      "page_number": 519,
      "chapter": null,
      "content": "5. Place the File in a Folder: Copy the llamafile to a directory on your\nsystem.\n6. Set Permissions:\nOn macOS or Linux: Grant execute permissions using:\n    chmod +x mistral-7b-instruct-v0.2.Q5_K_M.llamafile\nOn Windows: Rename the file to include the .exe extension:\n    mistral-7b-instruct-v0.2.Q5_K_M.llamafile.exe\n4. Run the File: Launch the llamafile. For example, on Windows:\n  c:\\temp>mistral-7b-instruct-v0.2.Q5_K_M.llamafile.exe -ngl 999\nThe llamafile will start a web server at http://127.0.0.1:8080 and open a\nbrowser pointing to a chat web interface at the same address. You can begin\ninteracting with the LLM immediately, as shown in Figure E.3.\nFigure E.3 llamafile chat web UI pointing to the local web server communicating with the local\nopen-source LLM\n",
      "content_length": 769,
      "extraction_method": "Direct"
    },
    {
      "page_number": 520,
      "chapter": null,
      "content": "For example you can enter the same prompt you entered into Ollama\npreviously:\n>>> How many Greek temples are in Paestum?\nOpenAI API compatible endpoints\n",
      "content_length": 153,
      "extraction_method": "Direct"
    },
    {
      "page_number": 521,
      "chapter": null,
      "content": "The web server also includes an OpenAI API-compatible\n/chat/completions endpoint. Use the examples from section E.3.3 for curl,\nPython, or LangChain, ensuring you set the port to 8080.\nTIP\nTo learn more about llamafile, a project by Justine Tunney in collaboration\nwith Mozilla, visit the GitHub page. For additional insights, check out\nJustine’s blog post \"Bash One-Liners for LLMs\"\n(https://justine.lol/oneliners/).\nE.4.5 LM Studio\nLM Studio is a user-focused local inference engine and a direct competitor to\nGPT4All. Executables for macOS, Windows, and Linux can be downloaded\nfrom the LM Studio website (https://lmstudio.ai). Once launched, LM Studio\nprovides a seamless experience for searching, selecting, and downloading\nGGUF models, thanks to its integration with Hugging Face. Figure E.4 shows\nthe model search interface.\nFigure E.4 GGUF Search Screen: 1) search for a model using the search box (in our case,\nMistral), 2) select a model variant from the left panel, and download a specific quantized version\nfrom the right panel by clicking the \"Download\" button (not shown in the screenshot). After\ndownloading a model, you can either chat directly through the chat screen or activate the\nbackend server for programmatic interaction.\n",
      "content_length": 1246,
      "extraction_method": "Direct"
    },
    {
      "page_number": 522,
      "chapter": null,
      "content": "In the figure above, you can select a model by entering its name in the search\n",
      "content_length": 79,
      "extraction_method": "Direct"
    },
    {
      "page_number": 523,
      "chapter": null,
      "content": "box, choosing a variant from the left panel, and then selecting a specific\nquantized version (in our case we select a 2-bit quantization, which has very\nlow fidelity) from the right panel by clicking the Download button.\nOnce downloaded, you can either interact with the model directly through the\nchat screen or activate the backend server for programmatic communication.\nChat screen\nTo send interactive prompts, open the Chat menu on the left bar, select the\ndownloaded model from the dropdown at the top, and type your prompt in\nthe text box at the bottom, as shown in Figure E.5.\nFigure E.5 Chat Screen: Select the chat menu on the left, choose the model from the dropdown at\nthe top, and type a prompt in the text box at the bottom.\n",
      "content_length": 738,
      "extraction_method": "Direct"
    },
    {
      "page_number": 524,
      "chapter": null,
      "content": "The response time will vary based on the model, its quantization level, and\nyour computer's hardware. Expect a delay of a few seconds before the LLM\nprovides an answer, as shown in Figure E.6.\nFigure E.6 Prompt Response: The time to receive an answer depends on the model, its\nquantization, and your computer’s hardware.\n",
      "content_length": 321,
      "extraction_method": "Direct"
    },
    {
      "page_number": 525,
      "chapter": null,
      "content": "Once you’ve explored the UI, it’s time to examine the server API mode.\nServer mode\nTo activate the local HTTP server and expose OpenAI-compatible REST API\nendpoints:\n1. Click the Server icon in the left-hand menu.\n2. Set the desired port number (e.g., 8080).\n3. Click the Start Server button, as shown in Figure E.7.\nFigure E.7 Activating the Local HTTP Server: click the Server menu on the left, enter (or accept)\nthe port number and click the Start Server button.\n",
      "content_length": 466,
      "extraction_method": "Direct"
    },
    {
      "page_number": 526,
      "chapter": null,
      "content": "After starting the server, the logs will appear in the terminal panel,\nconfirming the server is running. Example logs:\n[2024-12-30 13:42:41.709] [INFO] [LM STUDIO SERVER] Verbose serve\n[2024-12-30 13:42:41.721] [INFO] [LM STUDIO SERVER] Success! HTTP \n[2024-12-30 13:42:41.721] [INFO] [LM STUDIO SERVER] Supported end\n[2024-12-30 13:42:41.721] [INFO] [LM STUDIO SERVER] ->    GET  ht\n",
      "content_length": 384,
      "extraction_method": "Direct"
    },
    {
      "page_number": 527,
      "chapter": null,
      "content": "[2024-12-30 13:42:41.722] [INFO] [LM STUDIO SERVER] ->    POST ht\n[2024-12-30 13:42:41.722] [INFO] [LM STUDIO SERVER] ->    POST ht\n[2024-12-30 13:42:41.722] [INFO] [LM STUDIO SERVER] ->    POST ht\n[2024-12-30 13:42:41.723] [INFO] [LM STUDIO SERVER] Model loaded: \n[2024-12-30 13:42:41.723] [INFO] [LM STUDIO SERVER] Logs are save\nTo test the server, copy the sample curl request provided in the left-hand\npanel and paste it into a terminal (on Windows, use Git Bash for easier\nhandling of escaping), or alternatively run on Postman against the specified\nURL. As the terminal begins processing the request, the LM Studio logs\npanel will display activity, providing real-time updates during the request's\nexecution:\n[2024-12-30 13:46:12.782] [INFO] Received POST request to /v1/cha\n \"model\": \"TheBloke/Mistral-7B-v0.1-GGUF\",\n \"messages\": [\n   {\n     \"role\": \"system\",\n     \"content\": \"Always answer in rhymes.\"\n   },\n   {\n     \"role\": \"user\",\n     \"content\": \"Introduce yourself.\"\n   }\n ],\n \"temperature\": 0.7,\n \"max_tokens\": -1,\n \"stream\": true\n}\n[2024-12-30 13:46:12.783] [INFO] [LM STUDIO SERVER] Context Overf\n[2024-12-30 13:46:12.783] [INFO] [LM STUDIO SERVER] Streaming res\n[2024-12-30 13:46:20.630] [INFO] [LM STUDIO SERVER] First token g\n[2024-12-30 13:46:52.409] [INFO] Finished streaming response\nYou will now be able to interact with the OpenAI compatible REST API\nendpoints. As usual, refer to section E.3.3 for code examples and make sure\nyou use the correct port number and model name.\nE.4.6 LocalAI\nLocalAI is a free inference engine designed to run OpenAI REST API-\ncompatible LLMs on consumer hardware, including systems with plain CPUs\nor low-grade GPUs. It supports various quantized open-source LLMs and can\n",
      "content_length": 1727,
      "extraction_method": "Direct"
    },
    {
      "page_number": 528,
      "chapter": null,
      "content": "also handle audio-to-text, text-to-audio, and multi-modal models. Here, the\nfocus is on its text LLM capabilities.\nWritten in Go, LocalAI serves as a higher-level inference engine that routes\nOpenAI-like REST API calls to back-end engines like llama.cpp or vLLM.\nFigure E.1, adapted from the LocalAI documentation, illustrates this\narchitecture.\nFigure E.1: LocalAI Architecture: LocalAI routes OpenAI-like REST API calls to inference\nengines like llama.cpp, vLLM, or other custom backends.\n",
      "content_length": 491,
      "extraction_method": "Direct"
    },
    {
      "page_number": 529,
      "chapter": null,
      "content": "Server mode\nThe primary distribution method for LocalAI is through container images,\nwhich can be deployed using Docker, Podman, or Kubernetes. Popular\nmodels are automatically downloaded when starting the container.\nFor example, to run Mistral-OpenOrca on CPU:\n",
      "content_length": 262,
      "extraction_method": "Direct"
    },
    {
      "page_number": 530,
      "chapter": null,
      "content": "docker run -ti -p 8080:8080 localai/localai:v2.7.0-ffmpeg-core mi\nIf you have CUDA-12 GPU you can run the related image with the -gpu\noption:\ndocker run -ti -p 8080:8080 --gpus all localai/localai:v2.7.0-cub\nContainer images for CPU, CUDA-11, and CUDA-12 are available on\nDocker Hub and Quay.io. The LocalAI documentation\n(https://localai.io/docs) provides the appropriate Docker commands for each\nmodel and hardware configuration.\nIf you have a custom quantized model in GGUF format, you can place it in a\nlocal folder (e.g., local_models) and reference it when starting the container:\ndocker run -p 8080:8080 -v $PWD/local_models:/local_models -ti --\nOpenAI REST API compatible endpoints\nWhen a model is started using Docker, LocalAI launches an HTTP server on\nport 8080 with OpenAI-compatible endpoints. Refer to section 10.3.3 for\nexamples of curl, Python, and LangChain code you can use with LocalAI.\nEnsure the port is set to 8080 and verify the model name in the LocalAI\ndocumentation for accurate configuration.\nE.4.7 GPT4All\nGPT4All is an inference engine built on llama.cpp that allows you to run\nGGUF quantized LLM models on consumer hardware, including CPUs and\nlow-grade GPUs. It improves on llamafile with a more user-friendly\ngraphical interface, making it accessible to non-technical users.\nInstallation packages for Windows, macOS, and Ubuntu Linux are available\non the GPT4All website (https://gpt4all.io). After installation, you’ll see a\ndesktop client shown in figure E.2, with an interface which allows you to\ndownload a model, chat with it and also upload your documents to enable\nout-of the box RAG Q&A.\n",
      "content_length": 1628,
      "extraction_method": "Direct"
    },
    {
      "page_number": 531,
      "chapter": null,
      "content": "Figure E.2 GPT4All desktop application home screen\nGPT4All includes the following components, visible and hidden:\n1. Backend Inference Engine: Built on llama.cpp, the engine supports\nGGUF quantized LLM models (typically under 8GB) for architectures\nlike Falcon, LLAMA, MPT, GPT-J, and Mistral.\n2. Language-Specific Bindings: High-level API libraries are available for\n",
      "content_length": 368,
      "extraction_method": "Direct"
    },
    {
      "page_number": 532,
      "chapter": null,
      "content": "C++, Python, Go, NodeJS, and more, enabling programmatic access to\nthe inference engine.\n3. Local Web Server: The desktop application can start a local web server\nthat exposes chat completions through OpenAI-compatible REST API\nendpoints.\n4. Desktop GUI: The graphical client lets you interact with an LLM\nthrough a user-friendly interface. Models can be selected from a\ndropdown or added from the Model Explorer on the GPT4All homepage\nor Hugging Face.\n5. LocalDocs Plugin (Optional): This plugin allows you to import files\ncontaining data or unstructured text and chat with the content. It\nimplements a local Retrieval-Augmented Generation (RAG) architecture\nusing SBERT and an embedded vector database, enabling basic Q&A\nfunctionality with zero coding.\nServer mode\nTo enable the local HTTP server for REST API access (port 4891), navigate\nto GPT4All > Settings and enable the web server option.\nOpenAI REST API compatible endpoints\nThe server exposes OpenAI-compatible endpoints. Use the examples from\nsection 10.3.3 to send requests with curl, Python, or LangChain. Ensure the\nport is set to 4891, and verify the correct model name in the GPT4All\ndocumentation.\nGPT4All Python bindings\nYou can create a Python client using the GPT4All Python generation API.\nInstall the package from PyPI:\npip install gpt4all\nThen you can invoke the generate() function as follows:\nfrom gpt4all import GPT4All\n",
      "content_length": 1398,
      "extraction_method": "Direct"
    },
    {
      "page_number": 533,
      "chapter": null,
      "content": "model = GPT4All('mistral-7b-instruct-v0.1.Q4_0.gguf')\noutput = model.generate('How many Greek temples are in Paestum?', \nprint(output)\nWhen you instantiate the model, the GGUF file will download automatically\nto a local directory if it is not already available. The Python API also supports\nstreaming responses.\nLangChain GPT4all python library\nLangChain integrates with the native GPT4All Python bindings, providing an\nadvantage by implementing standard LangChain Python interfaces, such as\nRunnable. This allows you to build LCEL-based solutions, as demonstrated\nin previous chapters.\nFirst, install the gpt4all package:\npip install gpt4all\nNext, test the GPT4All LangChain integration by running the code in Listing\nE.1. Use a dedicated environment created with venv for best practice.\nNOTE\nBefore running the code, ensure you download a GGUF quantized model of\nyour choice into the specified local directory: open the GPT4All application\nand navigate to the Models section in the left-hand menu. Click on + Add\nModel to browse the available models, then search for those labeled with the\n.gguf extension. Once you find the model you want, simply click Download\nto save it to your device.\nListing E.1 Connecting to a local open-source model via LangChain’s GPT4All Wrapper\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_community.llms import GPT4All\nprompt = ChatPromptTemplate.from_messages([ #A\n   (\"system\", \"You are a helpful AI assistant.\"),\n   (\"user\", \"{input}\")\n])\n",
      "content_length": 1498,
      "extraction_method": "Direct"
    },
    {
      "page_number": 534,
      "chapter": null,
      "content": "model_path = ('./models/mistral-7b-instruct-v0.1.Q4_0.gguf') #B\nllm = GPT4All(model=model_path)\nchain = prompt | llm #C\nresponse = chain.invoke({\"input\": \"How many Greek temples are the\nprint(response)\nThis code demonstrates how to access a local open-source model through\nGPT4All using LangChain’s wrapper. With this integration, you can\nseamlessly implement complex workflows.\nE.4.8 Comparing local inference engines\nBefore closing this section, I want to provide a comparison of the local\ninference engines discussed.\nSome engines are optimized for consumer-grade hardware, while others\nsupport advanced configurations, including high-end GPUs. Most engines\ninclude an OpenAI-compatible HTTP server, but only a few provide\ndedicated bindings for specific programming languages. A summary of the\nmain characteristics of each engine is shown in Table E.3.\nTable E.3 Summary of the characteristics of local LLM inference engines\nInference\nengine\nBackend\nengine\nInstaller\navailable\nSupported\nOS\nOpenAI\nREST API\ncompatibility\nNative\nbindings\nD\nU\nllama.cpp llama.cpp\nNo\n(must\nbuild\nfrom\nsource)\nMacOS,\nLinux,\nWindows\nYes\nPython,\nJava, C#,\nGo, Scala,\nRuby,\nRust,\nScala,\nJavaScript,\nnode.js\nN\nollama\nllama.cpp\nYes\nMacOS,\nLinux;\nYes\nPython,\nJavascript\nN\n",
      "content_length": 1248,
      "extraction_method": "Direct"
    },
    {
      "page_number": 535,
      "chapter": null,
      "content": "Windows \nvLLM\nvLLM\nYes\n(pip)\nLinux\nYes\nPython\nN\nllamafile\nllama.cpp\nYes\nMacOS,\nLinux,\nWindows\nYes\nN/A\nN\nLocalAI\nLlama.cpp\nand vLLM\nDocker\nimage\nAny\nhardware\nrunning\nDocker\nYes\nN/A\nN\nGPT4All\nLlama.cpp Yes\nMacOS,\nLinux\nUbuntu,\nWindows\nYes\nPython\nY\nLM\nStudio\nLlama.cpp Yes\nMacOS,\nLinux\nUbuntu,\nWindows\nYes\nPython\nY\nBy now, you should have a clear understanding of how to use local inference\nengines to run open-source LLMs. However, inference engines are not the\nonly option for serving local LLMs. In the next section, I’ll briefly explain\nhow to perform inference using the Hugging Face Transformers library.\nE.4.9 Choosing a local inference engine\nIf you are new to local LLM inference engines, start with a simple option like\nllamafile. It offers an easy way to experiment with lightweight models. Once\nyou’re comfortable, consider moving to ollama, which simplifies\ndownloading, configuring, and testing additional models.\nIf you prefer a graphical user interface, consider upgrading to LM Studio or\nGPT4All instead.\nFor production-grade solutions, vLLM is a strong choice. If you need\nmaximum control over all aspects of hosting an LLM, consider using\nllama.cpp. It provides advanced customization options but requires a deeper\n",
      "content_length": 1231,
      "extraction_method": "Direct"
    },
    {
      "page_number": 536,
      "chapter": null,
      "content": "understanding of the hosting process.\nE.5 Inference via the HuggingFace Transformers\nlibrary\nFor full control over a model's architecture and the ability to modify it, you\ncan run a pre-trained model using the Hugging Face Transformers library.\nThis library is not only designed for experimentation and configuration but\nalso supports fine-tuning models, enabling you to share your improvements\nwith the Hugging Face community. Built on deep learning frameworks like\nJAX, PyTorch, and TensorFlow, it allows you to use one framework for\ntraining and another for inference.\nE.5.1 Hugging Face Transformers library\nBefore installing the transformers package, set up a virtual environment\nwith venv and install one or more of the following backends: Flax, PyTorch,\nor TensorFlow. Backend installation can vary depending on your hardware,\nso it is not covered here. Assuming the backends are installed, you can install\ntransformers via PyPI:\npip install transformers\nOnce installed, you can interact with pre-trained models stored locally. For\nexample, with the PyTorch backend, you can run inference on a quantized 4-\nbit version of Mistral Instruct as shown in Listing E.4 (which I have adapted\nfrom the Hugging Face documentation):\nListing E.4 Performing inference through the Hugging Face transformers library\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" #A\ntokenizer = AutoTokenizer.from_pretrained(model_id) #B\nmodel = AutoModelForCausalLM.from_pretrained(model_id, load_in_4b\ntext = \"Hello my name is\"\n",
      "content_length": 1583,
      "extraction_method": "Direct"
    },
    {
      "page_number": 537,
      "chapter": null,
      "content": "inputs = tokenizer(text, return_tensors=\"pt\").to(0) #C\noutputs = model.generate(**inputs, max_new_tokens=20) #D\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nWhen using the Transformers library, you must explicitly handle\ntokenization. While this approach provides significant flexibility, it requires a\ndeeper understanding of transformer architecture, making it well-suited for\nadvanced use cases, research, and experimentation.\nLet’s see how you’d write the same code with LangChain.\nE.5.2 LangChain’s HuggingFace Pipeline\nLangChain offers a wrapper for the Hugging Face Transformers pipeline,\nsimplifying integration into LangChain applications. If you want to use\nHugging Face with LangChain, you can implement the code shown in listing\nE.5.\nListing E.5 HuggingFace transformers via LangChain\nfrom langchain_community.llms.huggingface_pipeline import Hugging\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pip\nfrom langchain.prompts import PromptTemplate\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" #A\ntokenizer = AutoTokenizer.from_pretrained(model_id) #B\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\npipe = pipeline(\"text-generation\", \n     model=model, tokenizer=tokenizer, max_new_tokens=50) #C\nhf_pipeline = HuggingFacePipeline(pipeline=pipe)\nprompt_template = \"\"\"Question: {question}\nAnswer: Let's think step by step.\"\"\" #D\nprompt = PromptTemplate.from_template(prompt_template)\nllm_chain = prompt | hf #E\nquestion = \"How many Greek temples are there in Paestum?\"\n",
      "content_length": 1522,
      "extraction_method": "Direct"
    },
    {
      "page_number": 538,
      "chapter": null,
      "content": "print(llm_chain.invoke({\"question\": question})) #F\nNow that I’ve covered how to run an open-source LLM locally, the next step\nis to explore practical applications. In the following section, I’ll show how to\nredirect the research summarization engine we built in Chapter 4 from public\nOpenAI endpoints to a local open-source LLM like Mistral.\nE.6 Building a local summarization engine\nRevisit the research summarization engine from Chapter 4 to compare the\noriginal OpenAI-based solution with a local open-source LLM. To do this,\nduplicate the Visual Studio Code project folder by copying the ch04 folder\nand renaming it apE. This setup allows you to test both versions side by side,\nmaking it easier to evaluate accuracy and performance.\nKeep in mind that processing with a local LLM will take significantly longer\ncompared to OpenAI's API. However, this exercise will provide valuable\ninsights into how hardware affects inference speed. It will also give you\nhands-on experience in evaluating whether and how to implement a project\nusing a local open-source LLM.\nE.6.1 Choosing the inference engine\nFirst, decide which inference engine to use. Options like GPT4All and LM\nStudio are user-friendly and suitable for those with basic LLM experience.\nTo minimize code changes, avoid rewriting the application with native\nPython bindings like those offered by GPT4All. Instead, use the OpenAI-\ncompatible REST API endpoints provided by both GPT4All and LM Studio.\nChoosing between them is a matter of preference since both meet the\nrequirements of usability and OpenAI compatibility. For this example, I’ll\nuse LM Studio.\nE.6.2 Starting up the OpenAI compatible server\nIf LM Studio is not already installed, follow the installation steps covered\nearlier. Open the Server menu, select a port (e.g., 8080), and start the server.\n",
      "content_length": 1823,
      "extraction_method": "Direct"
    },
    {
      "page_number": 539,
      "chapter": null,
      "content": "LM Studio will now accept OpenAI-compatible requests.\nE.6.3 Modifying the original solution\nSince requests will be routed through an OpenAI-compatible endpoint, you\nonly need to make a small change in the llm_models.py file, shown in\nListing E.6.\nListing E.6 Original llm_models.py code\nfrom langchain.llms.openai import OpenAI\nopenai_api_key = 'YOUR_OPENAI_API_KEY'\ndef get_llm():\n   return OpenAI(openai_api_key=openai_api_key, model=\"gpt-4o-mi\nYou simply need to replace the implementation of get_llm() as shown in\nlisting E.7:\nListing E.7 Modified llm_models.py code, using a local open-source LLM\ndef get_llm():\n  port_number = '8080'\n  client = OpenAI(\n      base_url=f'http://localhost:{port_number}/v1', \n      api_key = ‘NO-KEY-NEEDED’,\n      model=’mistral' \n  )\n  return client \nThis approach avoids changes to the rest of the code. However, for flexibility,\nyou could make get_llm() configurable, as shown in Listing E.8.\nListing E.8 configurable get_llm()\ndef get_llm(is_llm_local=False):\n   port_number = '8080'\n   if is_llm_local:\n",
      "content_length": 1046,
      "extraction_method": "Direct"
    },
    {
      "page_number": 540,
      "chapter": null,
      "content": "       client = OpenAI(\n           base_url=f'http://localhost:{port_number}/v1', \n           api_key='NO-KEY-NEEDED',\n           model='mistral'\n       )\n   else:\n       client = OpenAI(openai_api_key=openai_api_key, model=\"gpt\n   return client\nThis version allows you to switch between OpenAI and local LLMs. Keep in\nmind that some inference engines have fixed port numbers or specific model\nnaming conventions, which may require you to further generalize the code\nand make it more configurable.\nE.6.4 Running the summarization engine through the local\nLLM\nTo test the summarization engine, execute chain_try_5_1.py. The LM\nStudio server logs panel should display activity as the request is processed.\nWhen the output is generated (this may take some time, as noted earlier), it\nshould closely match the results from the original OpenAI-based\nsummarization.\nE.6.5 Comparison between OpenAI and local LLM\nWhen running the summarization engine with a local LLM, you’ll notice:\n1. Accuracy: The output is similar for this use case (web scraping and\nsummarization).\n2. Performance: Local LLM inference is slower, especially on CPUs.\nWithout a GPU, processing times can be significantly longer.\nThis suggests that while a quantized local open-source LLM is sufficient for\ndevelopment, it may not meet production requirements. Consider deploying\nLM Studio on hardware with an NVIDIA GPU or using a more advanced\ninference engine like vLLM. vLLM supports unquantized models and high-\nend GPUs, offering better performance for demanding use cases.\n",
      "content_length": 1542,
      "extraction_method": "Direct"
    },
    {
      "page_number": 541,
      "chapter": null,
      "content": "E.7 Summary\nOpen-source LLMs provide potential cost savings, privacy control, and\nflexibility, with community-driven innovation ensuring competitive\nperformance and customization compared to proprietary models. These\nfeatures make open-source LLMs an appealing option for both\nexperimentation and production.\nTransparency in architecture, training data, and methods allows open-\nsource LLMs to be validated, modified, and tailored for compliance and\ntrust. This clarity ensures businesses can meet regulatory requirements\nwhile customizing models for specific needs.\nOpen-source LLMs empower organizations with lower total cost of\nownership and fine-tuning capabilities, enabling tailored solutions while\nmaintaining long-term affordability. This combination helps businesses\nachieve both financial efficiency and functional scalability.\nComparative tables simplify the evaluation of open-source LLMs,\nshowcasing their unique features and optimal use scenarios. By\ncomparing models side-by-side, you can quickly identify the best fit for\nyour project goals.\nOpen-source LLM weights enable local inference with accessible tools\nand modern techniques like quantization, supporting seamless OpenAI-\ncompatible API integration for cost-effective and private deployments.\nThese methods make running powerful models on consumer hardware\nmore practical and affordable.\nStart with a simple inference engine like llamafile for lightweight\nmodels, progress to tools like Ollama or LM Studio for ease of use, and\nchoose llama.cpp or vLLM for advanced, production-grade\ncustomization. This progression helps you build confidence though\nsimple user interfaces and scale capabilities as your project grows.\nApplications built for OpenAI can be adapted to open-source LLMs\nusing compatible REST APIs, offering insights into hardware impacts\non inference speed and feasibility. This adaptation enables cost-effective\nexperimentation and highlights areas for optimization.\n",
      "content_length": 1956,
      "extraction_method": "Direct"
    },
    {
      "page_number": 542,
      "chapter": null,
      "content": "welcome\nDear Reader,\nThank you so much for purchasing \"AI Agents and Applications\". Your\ninterest in this cutting-edge topic means a great deal to me, and I am excited\nto guide you through the expansive world of large language models (LLMs).\nThis book emerged from my professional journey as a software developer,\nwhere my main focus has been quantitative development in finance. Over the\nyears, I occasionally delved into AI projects aimed at tackling problems\nwhere traditional mathematical computations fell short. My fascination with\nLLMs began when I encountered ChatGPT in November 2022. Its\ncapabilities inspired me to explore further, leading me to experiment with the\nOpenAI APIs, delve into prompt engineering, and design applications using\nRetrieval Augmented Generation (RAG). My involvement with LangChain,\nan open-source LLM application framework, significantly accelerated my\nlearning and allowed me to witness its rapid adoption within the community.\n\"AI Agents and Applications\" is structured to cater both to beginners and\nseasoned professionals. It offers a comprehensive look into the foundational\ntechnologies and advanced techniques in the realm of LLMs. Whether you\nare new to programming or an experienced developer, you will find the\ncontent approachable yet enriching, especially with practical code examples\nto enhance your engagement.\nThe insights in this book are drawn from my real-world experiences and\ncontinuous learning in a field that evolves almost daily. I cover a range of\ntopics from running open source LLMs locally to advanced RAG techniques\nand fine-tuning methods. My goal is to provide a resource that I wish had\nbeen available to me—an accessible, practical guide that empowers you to\nnavigate and excel in this emerging domain.\nAs this field is still in its infancy, your feedback is incredibly valuable. I\nencourage you to actively participate in our online discussion forum at\n",
      "content_length": 1925,
      "extraction_method": "Direct"
    },
    {
      "page_number": 543,
      "chapter": null,
      "content": "liveBook. Your questions, comments, and suggestions not only help improve\nthis book but also contribute to the broader community exploring LLM\napplications.\nThank you once again for joining me on this journey. I am eager to hear\nabout your experiences and look forward to any insights you might share.\nWarm regards,\n— Roberto Infante\nIn this book\nwelcome 1 Introduction to AI Agents and Applications 2 Executing prompts\nprogrammatically 3 Summarizing text using LangChain 4 Building a research\nsummarization engine 5 Agentic Workflows with LangGraph 6 RAG\nfundamentals with Chroma DB 7 Q&A chatbots with LangChain and\nLangSmith 8 Advanced indexing 9 Question transformations 10 Query\ngeneration, routing and retrieval post-processing 11 Building Tool-based\nAgents with LangGraph 12 Multi-agent Systems 13 Building and consuming\nMCP servers 14 Productionizing AI Agents: memory, guardrails, and beyond\nAppendix A. Trying out LangChain Appendix B. Setting up a Jupyter\nNotebook environment Appendix C. Choosing an LLM Appendix D.\nInstalling SQLite on Windows Appendix E. Open-source LLMs\n",
      "content_length": 1086,
      "extraction_method": "Direct"
    }
  ],
  "enrichment": {
    "version": "1.0.0",
    "generated_by": "generate_chapter_metadata.py",
    "contains": [
      "keywords",
      "concepts",
      "summary"
    ]
  }
}